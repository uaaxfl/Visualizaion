2015.eamt-1.45,E06-1032,0,\N,Missing
2015.eamt-1.45,W10-1751,0,\N,Missing
2015.eamt-1.45,W14-3301,0,\N,Missing
2015.eamt-1.45,P02-1040,0,\N,Missing
2015.eamt-1.45,W14-3319,1,\N,Missing
2015.eamt-1.45,P11-1105,0,\N,Missing
2015.eamt-1.45,P10-2041,0,\N,Missing
2015.eamt-1.45,W05-0909,0,\N,Missing
2015.eamt-1.45,P07-2045,0,\N,Missing
2015.eamt-1.45,W07-0718,0,\N,Missing
2015.eamt-1.45,C14-1111,0,\N,Missing
2015.eamt-1.45,P12-3005,0,\N,Missing
2015.eamt-1.45,2012.eamt-1.67,1,\N,Missing
2015.eamt-1.45,2014.eamt-1.4,1,\N,Missing
2015.eamt-1.45,W14-3320,0,\N,Missing
2015.eamt-1.45,2005.mtsummit-papers.11,0,\N,Missing
2015.eamt-1.45,ljubesic-etal-2014-tweetcat,1,\N,Missing
2015.eamt-1.45,W15-3036,1,\N,Missing
2015.eamt-1.45,rubino-etal-2014-quality,1,\N,Missing
2015.eamt-1.45,W15-3022,1,\N,Missing
2015.eamt-1.45,W15-4903,1,\N,Missing
2015.eamt-1.45,2015.eamt-1.4,1,\N,Missing
2015.eamt-1.45,espla-gomis-etal-2014-comparing,1,\N,Missing
2015.eamt-1.45,W15-3001,0,\N,Missing
2015.eamt-1.45,ljubesic-toral-2014-cawac,1,\N,Missing
2015.eamt-1.45,W14-0405,1,\N,Missing
2015.eamt-1.45,2005.iwslt-1.8,0,\N,Missing
2015.eamt-1.45,W16-3421,1,\N,Missing
2015.eamt-1.45,D07-1078,0,\N,Missing
2015.eamt-1.45,W08-0509,0,\N,Missing
2015.eamt-1.45,W11-2123,0,\N,Missing
2015.eamt-1.45,P14-1129,0,\N,Missing
2015.eamt-1.45,W16-2347,0,\N,Missing
2015.eamt-1.45,W16-2375,1,\N,Missing
2015.eamt-1.45,W16-2367,1,\N,Missing
2015.eamt-1.45,W16-3423,1,\N,Missing
2020.lrec-1.409,hnatkova-etal-2014-syn,0,0.0735679,"Missing"
2020.lrec-1.409,L16-1242,1,0.826954,"into formats that facilitate digital processing (Bušta et al., 2017). 3. Corpus Annotation Linguistic annotation of the Gigafida 2.0 corpus was performed on the following levels: (1) tokenization and sentence segmentation, (2) annotation with morphosyntactic descriptions (MSDs) and (3) lemmatization. At the time this corpus was finalized, there were two major pipelines available for the Slovenian language: the first was developed within the SSJ project (Obeliks, Grcar et al., 2012, hereafter “the SSJ pipeline”), and the one developed within the CLARIN.SI research infrastructure (relditagger, Ljubešić and Erjavec, 2016, hereafter “CLARIN.SI pipeline”), each with their own strengths and weaknesses. While the SSJ pipeline includes explicit linguistic postprocessing rules, the CLARIN.SI pipeline excels in overall annotation accuracy. This was the reason why a &quot;metatagging&quot; approach was chosen to make use of the strong sides of each pipeline. For the first task of tokenization and sentence segmentation, the rule-based Obeliks4J tool was applied as it follows the segmentation rules applied both in the previous versions of the reference corpus, as well as the manually annotated training data. The remaining two an"
2020.lrec-1.409,R15-1049,1,0.825877,"egree possible for a number of reasons. Firstly, more reliable sources have since been compiled for the analysis of nonstandard Slovene, in particular the Janes Corpus of Internet Slovene (Fišer et al., 2018). Secondly, the number of nonstandard texts in Gigafida 1.0 was relatively small to begin with and thus not representative for the analysis of nonstandard linguistic features. Finally, corpus texts were not assigned any metadata on their degree of standardness, which made it impossible to exclude them from queries. Non-standard texts were identified automatically using a regression model (Ljubešić et al., 2015), where paragraphs were first assigned standardness values between 1 (standard) and 3 (highly non-standard). This provided a more precise overview of the distribution of nonstandardness within a text. Each text was thus assigned a vector of values, one for each paragraph. This distribution of values for each text was then compared with the distribution of values within the entire corpus by applying the Kolmogorov-Smirnov test. All texts with a statistically significant deviation from the distribution in the entire corpus and with a mean value of linguistic nonstandardness higher than the corpu"
2020.lrec-1.720,N19-1423,0,0.0238089,"es; covers not only discrete differences in word sense but more subtle, graded changes in meaning; and covers not only a well-resourced language (English) but a number of less-resourced languages. We define the task and evaluation metrics, outline the dataset collection methodology, and describe the status of the dataset so far. Keywords: corpus, annotation, semantics, similarity, context, salience, context-dependence 1. Introduction Recent work in language modelling and word embeddings has led to a sharp increase in use of context-dependent models such as ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019). These models, by providing representations of words which depend on the surrounding context, allow us to take account of the effects not only of discrete differences in word sense but of the more graded effects of context. However, evaluation of these models has generally been in terms of either their performance as language models, or their effect on downstream tasks such as sentiment classification (Peters et al., 2018): there are few resources available which allow evaluation in terms of the properties of the embeddings themselves, or in terms of their ability to model human perceptions o"
2020.lrec-1.720,J15-4004,0,0.742447,"s of context. However, evaluation of these models has generally been in terms of either their performance as language models, or their effect on downstream tasks such as sentiment classification (Peters et al., 2018): there are few resources available which allow evaluation in terms of the properties of the embeddings themselves, or in terms of their ability to model human perceptions of meaning. There are established methods to evaluate word embedding models intrinsically via their ability to reflect human similarity judgements (see e.g. WordSim-353 (Finkelstein et al., 2002) and SimLex-999 (Hill et al., 2015)) or model analogies (Mikolov et al., 2013); however, these have generally ignored context and treated words in isolation. The few that do provide context (e.g. SCWS (Huang et al., 2012) and WiC (Pilehvar and Camacho-Collados, 2019)) focus on word sense and discrete effects, thus missing some of the effects that context has on words in general, and some of the benefits of context-dependent models. To evaluate current models, we need a way to evaluate their ability to reflect similarity judgements in context: how well do they model the effects that context has on word meaning? In this paper we"
2020.lrec-1.720,P12-1092,0,0.926591,"ification (Peters et al., 2018): there are few resources available which allow evaluation in terms of the properties of the embeddings themselves, or in terms of their ability to model human perceptions of meaning. There are established methods to evaluate word embedding models intrinsically via their ability to reflect human similarity judgements (see e.g. WordSim-353 (Finkelstein et al., 2002) and SimLex-999 (Hill et al., 2015)) or model analogies (Mikolov et al., 2013); however, these have generally ignored context and treated words in isolation. The few that do provide context (e.g. SCWS (Huang et al., 2012) and WiC (Pilehvar and Camacho-Collados, 2019)) focus on word sense and discrete effects, thus missing some of the effects that context has on words in general, and some of the benefits of context-dependent models. To evaluate current models, we need a way to evaluate their ability to reflect similarity judgements in context: how well do they model the effects that context has on word meaning? In this paper we present our ongoing efforts to define and build a new dataset that tries to fill that gap: CoSimLex (Armendariz et al., 2020). CoSimLex builds on the familiar pairwise, graded similarity"
2020.lrec-1.720,Q17-1022,0,0.0793333,"Missing"
2020.lrec-1.720,N18-1202,0,0.0456083,"ext-dependent similarity measures; covers not only discrete differences in word sense but more subtle, graded changes in meaning; and covers not only a well-resourced language (English) but a number of less-resourced languages. We define the task and evaluation metrics, outline the dataset collection methodology, and describe the status of the dataset so far. Keywords: corpus, annotation, semantics, similarity, context, salience, context-dependence 1. Introduction Recent work in language modelling and word embeddings has led to a sharp increase in use of context-dependent models such as ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019). These models, by providing representations of words which depend on the surrounding context, allow us to take account of the effects not only of discrete differences in word sense but of the more graded effects of context. However, evaluation of these models has generally been in terms of either their performance as language models, or their effect on downstream tasks such as sentiment classification (Peters et al., 2018): there are few resources available which allow evaluation in terms of the properties of the embeddings themselves, or in terms of their abili"
2020.lrec-1.720,N19-1128,0,0.151127,"8): there are few resources available which allow evaluation in terms of the properties of the embeddings themselves, or in terms of their ability to model human perceptions of meaning. There are established methods to evaluate word embedding models intrinsically via their ability to reflect human similarity judgements (see e.g. WordSim-353 (Finkelstein et al., 2002) and SimLex-999 (Hill et al., 2015)) or model analogies (Mikolov et al., 2013); however, these have generally ignored context and treated words in isolation. The few that do provide context (e.g. SCWS (Huang et al., 2012) and WiC (Pilehvar and Camacho-Collados, 2019)) focus on word sense and discrete effects, thus missing some of the effects that context has on words in general, and some of the benefits of context-dependent models. To evaluate current models, we need a way to evaluate their ability to reflect similarity judgements in context: how well do they model the effects that context has on word meaning? In this paper we present our ongoing efforts to define and build a new dataset that tries to fill that gap: CoSimLex (Armendariz et al., 2020). CoSimLex builds on the familiar pairwise, graded similarity task of SimLex-999, but extends it to pairs o"
2020.lrec-1.720,W17-0228,0,0.068048,"nses and cannot therefore capture continuous effects of context in the judgements of similarity between different words. These datasets are also available only in English, and do not allow models to be evaluated across different languages. 3. Dataset and Task Design CoSimLex will be based on pairs of words from SimLex999 (Hill et al., 2015); the reliability and common use of this dataset makes it a good starting point and allows comparison of judgements and model outputs to the contextindependent case. For Croatian and Finnish we use existing translations of Simlex-999 (Mrkˇsi´c et al., 2017; Venekoski and Vankka, 2017; Kittask, 2019). In the case of Slovene, we have produced our own new translation (Pollak et al., 2020), following the methodology used by Mrkˇsi´c et al. (2017) for Croatian. The English dataset consists of 333 pairs; the Croatian, Finnish and Slovene datasets of 111 pairs each. Each pair is rated within two different contexts, giving a total of 1554 scores of contextual similarity. This poses a difficult task: to find suitable, organically occurring contexts for each pair; this task is more pronounced for languages with less resources, and as a result the selection of pairs is different for"
2020.peoples-1.15,2020.acl-main.112,0,0.0407795,"Missing"
2020.peoples-1.15,W18-6218,1,0.860007,"Missing"
2020.peoples-1.15,S18-1001,0,0.0303931,"ovene) and two topics (migrants and LGBT). We show significant and consistent improvements in automatic classification across all languages and topics, as well as consistent (and expected) emotion distributions across all languages and topics, proving for the manually corrected lexicons to be a useful addition to the severely lacking area of emotion lexicons, the crucial resource for emotive analysis of text. 1 Introduction Emotion lexicons are rather scarce resources for most languages (Buechel et al., 2020), although they are a very important ingredient for robust emotion detection in text (Mohammad et al., 2018). They are mostly differentiated between depending on the way they encode emotions – either via continuous or discrete representations (Calvo and Mac Kim, 2013). In this work, we present emotion lexicons for three languages – Croatian, Dutch and Slovene, developed by manually correcting automatic translations that are part of the NRC Emotion Lexicon (Mohammad and Turney, 2013). In that lexicon, sentiment (positive or negative) and a discrete model of emotion covering anger, anticipation, disgust, fear, joy, sadness, surprise and trust, are encoded via a binary variable for each emotion. The si"
2020.semeval-1.3,C18-1139,0,0.0425101,"imLex-999 non contextualised similarity scores. The approach, even if very successful, seems to rely on having out of context human annotations, perhaps not realistic in the general case. The fact that the system did very poorly in Subtask 1, which asked to predict change, seems to indicate much of the success is coming from the human annotations. A related strategy could perhaps be used with embeddings or computed predictions instead of human scores. The next group focused on testing a variety of models and parameters. BRUMS (Hettiarachchi and Ranasinghe, 2020) worked with ELMo, BERT, Flair (Akbik et al., 2018), Transformer-XL (Dai et al., 2019) and XLNet (Yang et al., 2019). Their final submission made use of stacked embeddings proposed by Akbik et al. (2018). They won the Finnish Subtask 2, ended second in the two Slovene ones and performed very well in the two English ones. The Hitachi team (Morishita et al., 2020) looked at BERT and XML-R. Their main insight was that for every language, the layers from the center to the end where always the best performing ones, however while BERT performed best in the last layer, XLM-R did in the center one, suggesting their inner structure is organised differe"
2020.semeval-1.3,2020.semeval-1.37,0,0.0306792,"h Subtask 2, ended second in the two Slovene ones and performed very well in the two English ones. The Hitachi team (Morishita et al., 2020) looked at BERT and XML-R. Their main insight was that for every language, the layers from the center to the end where always the best performing ones, however while BERT performed best in the last layer, XLM-R did in the center one, suggesting their inner structure is organised differently. They won the Slovene Subtask 1, finished second in the two Croatian subtasks and performed competitively in the English ones. To conclude with this group JUSTMasters (Al-Khdour et al., 2020) tested several models, parameters and their own strategy to combine models. They achieved very good performance, especially in the English Subtask 2. However, in order to optimise their system, they made many more submissions than allowed in the competition; we therefore leave them out of the official ranking. With a more multilingual approach, BabelEncoding (Costella Pessutto et al., 2020) proposed a solution in which they translated the contexts and target words to many languages and then used a weighted combination of monolingual pretrained non contextualised embeddings and BERT embeddings"
2020.semeval-1.3,2020.lrec-1.720,1,0.566046,"Missing"
2020.semeval-1.3,2020.semeval-1.38,0,0.0192666,"l at predicting the change between contexts, but surprisingly poorly at predicting similarity itself, ending last in the English Subtask 2 and second from the last in Croatian and Slovene. The starting point of CitiusNLP (Gamallo, 2020) was the idea that, even if BERT seems to be able to encode syntactic structure, it doesn’t seem to make use of it. They created a linguistically motivated system that relied in dependency to create predictions. However, its performance was considerably worse than BERT’s and their actual submissions are based on a standard BERT model. Finally, the Will_Go team (Bao et al., 2020) looked at different ways to measure similarity between embeddings, mixing euclidean distance with the most common cosine similarity and several others not 44 described in their paper. The combination works well, they achieved a second place in the English Subtask 1 and won the Finnish Subtask 1. 8 Conclusion We resented the SemEval-2020 Task on Graded Word Similarity in Context and introduced our new dataset CoSimLex. We provided the motivation behind their design choices and described the annotation process. The task received a good number of submissions and system description papers (15 and"
2020.semeval-1.3,S17-2002,1,0.894409,"me word, and labelled as to whether the word sense in the two examples/contexts is the same or different. This forces engagement with the context; it also creates a task in which context-independent models like word2vec “would perform no better than a random baseline”; and inter-rater agreement scores are much more healthy. However, as the dataset focuses on discrete word senses, it cannot capture graded effects of context. These datasets are also available only in English. Multi-lingual similarity datasets exist: in SemEval2017 Task 2: Multilingual and Cross-lingual Semantic Word Similarity, Camacho-Collados et al. (2017) used five different languages, and even used pairs in which each word was presented in a different language. A more recent Multi-SimLex dataset (Vuli´c et al., 2020) comprises similarity ratings for 1,888 concept pairs aligned across 13 typologically diverse languages. However, the pairs in both datasets were annotated out of context, preventing analysis of contextual effects. 38 3 Task Description Our dataset is based on pairs of words from SimLex-999 (Hill et al., 2015). Each instance is a naturallyoccurring context, taken from Wikipedia, in which both words in the pair appear, labelled wit"
2020.semeval-1.3,2020.semeval-1.35,0,0.0334679,"al influence. As an example, ukWaC-subs was created by substituting target words by either: a correct substitute; a word that could be the right substitute in other circumstances but it is not in this context; or a random word. The datasets included WiC, which when used to fine tune the model resulted in the best performance for Subtask1, giving them a third place. The approach works very well, giving a very consistent performance in all categories, and significantly improving the non fine-tuned model from a ρ=0.715 and 0.661 per subtask, to a ρ=0.760 and 0.718 respectively. Ferryman’s focus (Chen et al., 2020) was clearly the English Subtask 1, which they won with a modification of BERT in which they fed the TF-IDF score of the words to the model, thus incorporating information about the general importance of words. The system does very well at predicting the change between contexts, but surprisingly poorly at predicting similarity itself, ending last in the English Subtask 2 and second from the last in Croatian and Slovene. The starting point of CitiusNLP (Gamallo, 2020) was the idea that, even if BERT seems to be able to encode syntactic structure, it doesn’t seem to make use of it. They created"
2020.semeval-1.3,P19-4007,1,0.829844,"e to optimise their system with more than the competition’s limit of 9 submissions. neither filled the form nor submitted a system description paper do not appear in the official rankings (Tables 2 and 3). We will discuss here the results of the remaining 11 systems. First, we describe a group of systems designed around sense embeddings created using WordNet (Miller, 1995) as a guide. The most successful was the submission by LMMS. They employed a similar strategy to the one set out in (Loureiro and Jorge, 2019), creating pretrained embeddings for each sense in WordNet, this time using XLM-R (Conneau et al., 2019) and SemCor augmented with their own UWA dataset (Loureiro and Camacho-Collados, 2020). This approach achieved second place in the English Subtask 1 and fourth in the English Subtask 2. UZH (Tang, 2020) submitted (after the competition had ended) a system based on the original BERT sense embeddings created for (Loureiro and Jorge, 2019) but improved their performance by combining them with contextualised embeddings. Finally for this group AlexU-AUX-BERT (Mahmoud and Torki, 2020) created new sense embeddings for the competition 43 target words. In order to do so they sourced additional contexts"
2020.semeval-1.3,2020.semeval-1.5,0,0.204677,"Missing"
2020.semeval-1.3,P19-1285,0,0.02456,"ty scores. The approach, even if very successful, seems to rely on having out of context human annotations, perhaps not realistic in the general case. The fact that the system did very poorly in Subtask 1, which asked to predict change, seems to indicate much of the success is coming from the human annotations. A related strategy could perhaps be used with embeddings or computed predictions instead of human scores. The next group focused on testing a variety of models and parameters. BRUMS (Hettiarachchi and Ranasinghe, 2020) worked with ELMo, BERT, Flair (Akbik et al., 2018), Transformer-XL (Dai et al., 2019) and XLNet (Yang et al., 2019). Their final submission made use of stacked embeddings proposed by Akbik et al. (2018). They won the Finnish Subtask 2, ended second in the two Slovene ones and performed very well in the two English ones. The Hitachi team (Morishita et al., 2020) looked at BERT and XML-R. Their main insight was that for every language, the layers from the center to the end where always the best performing ones, however while BERT performed best in the last layer, XLM-R did in the center one, suggesting their inner structure is organised differently. They won the Slovene Subtask"
2020.semeval-1.3,N19-1423,0,0.0961329,"w dataset (CoSimLex) was created for evaluation in this task: it contains pairs of words, each annotated within two short text passages. Systems beat the baselines by significant margins, but few did well in more than one language or subtask. Almost every system employed a Transformer model, but with many variations in the details: WordNet sense embeddings, translation of contexts, TF-IDF weightings, and the automatic creation of datasets for fine-tuning were all used to good effect. 1 Introduction Contextualised word embeddings, produced by models such as ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019), have quickly become the standard in NLP systems. They deliver impressive performance in language modeling and downstream tasks; but there are few resources available which allow intrinsic evaluation in terms of the properties of the embeddings themselves, or their ability to model human perception of meaning, and how these depend on context. For non-contextualised models, resources like WordSim-353 (Finkelstein et al., 2002) and SimLex-999 (Hill et al., 2015) were instrumental to evaluate their ability to reflect human similarity judgements. However these datasets treat pairs of words in iso"
2020.semeval-1.3,J13-3003,0,0.173635,"luation in terms of the properties of the embeddings themselves, or their ability to model human perception of meaning, and how these depend on context. For non-contextualised models, resources like WordSim-353 (Finkelstein et al., 2002) and SimLex-999 (Hill et al., 2015) were instrumental to evaluate their ability to reflect human similarity judgements. However these datasets treat pairs of words in isolation, and thus cannot tell us much about the effect of context. The few resources that work with context, like SCWS (Huang et al., 2012), WiC (Pilehvar and Camacho-Collados, 2019), and WSim (Erk et al., 2013), focus on word sense and discrete effects, thus missing the more graded effects that context has on words in general, and that approaches like ELMo and BERT would seem well suited to model. Further, USim (Erk et al., 2013) focuses on separate sentential contexts only in the English language. The goal of SemEval-2020 Task 3: Graded Word Similarity in Context, was to move towards filling that gap. We created a new dataset, CoSimLex (Armendariz et al., 2020), which builds on the familiar pairwise, graded similarity task of SimLex-999, but extends it to pairs of words as they occur in context; sp"
2020.semeval-1.3,2020.semeval-1.34,0,0.0283968,"roving the non fine-tuned model from a ρ=0.715 and 0.661 per subtask, to a ρ=0.760 and 0.718 respectively. Ferryman’s focus (Chen et al., 2020) was clearly the English Subtask 1, which they won with a modification of BERT in which they fed the TF-IDF score of the words to the model, thus incorporating information about the general importance of words. The system does very well at predicting the change between contexts, but surprisingly poorly at predicting similarity itself, ending last in the English Subtask 2 and second from the last in Croatian and Slovene. The starting point of CitiusNLP (Gamallo, 2020) was the idea that, even if BERT seems to be able to encode syntactic structure, it doesn’t seem to make use of it. They created a linguistically motivated system that relied in dependency to create predictions. However, its performance was considerably worse than BERT’s and their actual submissions are based on a standard BERT model. Finally, the Will_Go team (Bao et al., 2020) looked at different ways to measure similarity between embeddings, mixing euclidean distance with the most common cosine similarity and several others not 44 described in their paper. The combination works well, they a"
2020.semeval-1.3,2020.semeval-1.17,0,0.0328571,"nally for this group AlexU-AUX-BERT (Mahmoud and Torki, 2020) created new sense embeddings for the competition 43 target words. In order to do so they sourced additional contexts for the top WordNet synsets. Their system scored third in the English Subtask 2. The pretrained WordNet sense embedding proved highly successful in this task, especially in Subtask 2, predicting the similarity scores themselves. The biggest weakness of the approach is their reliance on linguistic resources that don’t exist for most languages other than English. Related to these systems, the submission by MineriaUNAM (Gomez-Adorno et al., 2020) won the English Subtask 2. They proposed a system in which they calculated K-Means inspired centroids from the words in the context and used them to modify the original SimLex-999 non contextualised similarity scores. The approach, even if very successful, seems to rely on having out of context human annotations, perhaps not realistic in the general case. The fact that the system did very poorly in Subtask 1, which asked to predict change, seems to indicate much of the success is coming from the human annotations. A related strategy could perhaps be used with embeddings or computed prediction"
2020.semeval-1.3,2020.semeval-1.16,0,0.0416405,"rom the words in the context and used them to modify the original SimLex-999 non contextualised similarity scores. The approach, even if very successful, seems to rely on having out of context human annotations, perhaps not realistic in the general case. The fact that the system did very poorly in Subtask 1, which asked to predict change, seems to indicate much of the success is coming from the human annotations. A related strategy could perhaps be used with embeddings or computed predictions instead of human scores. The next group focused on testing a variety of models and parameters. BRUMS (Hettiarachchi and Ranasinghe, 2020) worked with ELMo, BERT, Flair (Akbik et al., 2018), Transformer-XL (Dai et al., 2019) and XLNet (Yang et al., 2019). Their final submission made use of stacked embeddings proposed by Akbik et al. (2018). They won the Finnish Subtask 2, ended second in the two Slovene ones and performed very well in the two English ones. The Hitachi team (Morishita et al., 2020) looked at BERT and XML-R. Their main insight was that for every language, the layers from the center to the end where always the best performing ones, however while BERT performed best in the last layer, XLM-R did in the center one, su"
2020.semeval-1.3,J15-4004,0,0.527307,"used to good effect. 1 Introduction Contextualised word embeddings, produced by models such as ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019), have quickly become the standard in NLP systems. They deliver impressive performance in language modeling and downstream tasks; but there are few resources available which allow intrinsic evaluation in terms of the properties of the embeddings themselves, or their ability to model human perception of meaning, and how these depend on context. For non-contextualised models, resources like WordSim-353 (Finkelstein et al., 2002) and SimLex-999 (Hill et al., 2015) were instrumental to evaluate their ability to reflect human similarity judgements. However these datasets treat pairs of words in isolation, and thus cannot tell us much about the effect of context. The few resources that work with context, like SCWS (Huang et al., 2012), WiC (Pilehvar and Camacho-Collados, 2019), and WSim (Erk et al., 2013), focus on word sense and discrete effects, thus missing the more graded effects that context has on words in general, and that approaches like ELMo and BERT would seem well suited to model. Further, USim (Erk et al., 2013) focuses on separate sentential"
2020.semeval-1.3,P12-1092,0,0.573882,"eam tasks; but there are few resources available which allow intrinsic evaluation in terms of the properties of the embeddings themselves, or their ability to model human perception of meaning, and how these depend on context. For non-contextualised models, resources like WordSim-353 (Finkelstein et al., 2002) and SimLex-999 (Hill et al., 2015) were instrumental to evaluate their ability to reflect human similarity judgements. However these datasets treat pairs of words in isolation, and thus cannot tell us much about the effect of context. The few resources that work with context, like SCWS (Huang et al., 2012), WiC (Pilehvar and Camacho-Collados, 2019), and WSim (Erk et al., 2013), focus on word sense and discrete effects, thus missing the more graded effects that context has on words in general, and that approaches like ELMo and BERT would seem well suited to model. Further, USim (Erk et al., 2013) focuses on separate sentential contexts only in the English language. The goal of SemEval-2020 Task 3: Graded Word Similarity in Context, was to move towards filling that gap. We created a new dataset, CoSimLex (Armendariz et al., 2020), which builds on the familiar pairwise, graded similarity task of S"
2020.semeval-1.3,2020.emnlp-main.283,0,0.0166095,"submissions. neither filled the form nor submitted a system description paper do not appear in the official rankings (Tables 2 and 3). We will discuss here the results of the remaining 11 systems. First, we describe a group of systems designed around sense embeddings created using WordNet (Miller, 1995) as a guide. The most successful was the submission by LMMS. They employed a similar strategy to the one set out in (Loureiro and Jorge, 2019), creating pretrained embeddings for each sense in WordNet, this time using XLM-R (Conneau et al., 2019) and SemCor augmented with their own UWA dataset (Loureiro and Camacho-Collados, 2020). This approach achieved second place in the English Subtask 1 and fourth in the English Subtask 2. UZH (Tang, 2020) submitted (after the competition had ended) a system based on the original BERT sense embeddings created for (Loureiro and Jorge, 2019) but improved their performance by combining them with contextualised embeddings. Finally for this group AlexU-AUX-BERT (Mahmoud and Torki, 2020) created new sense embeddings for the competition 43 target words. In order to do so they sourced additional contexts for the top WordNet synsets. Their system scored third in the English Subtask 2. The"
2020.semeval-1.3,P19-1569,0,0.0310581,"notator against the average of the rest. JUSTMasters is not part of the official ranking since they were able to optimise their system with more than the competition’s limit of 9 submissions. neither filled the form nor submitted a system description paper do not appear in the official rankings (Tables 2 and 3). We will discuss here the results of the remaining 11 systems. First, we describe a group of systems designed around sense embeddings created using WordNet (Miller, 1995) as a guide. The most successful was the submission by LMMS. They employed a similar strategy to the one set out in (Loureiro and Jorge, 2019), creating pretrained embeddings for each sense in WordNet, this time using XLM-R (Conneau et al., 2019) and SemCor augmented with their own UWA dataset (Loureiro and Camacho-Collados, 2020). This approach achieved second place in the English Subtask 1 and fourth in the English Subtask 2. UZH (Tang, 2020) submitted (after the competition had ended) a system based on the original BERT sense embeddings created for (Loureiro and Jorge, 2019) but improved their performance by combining them with contextualised embeddings. Finally for this group AlexU-AUX-BERT (Mahmoud and Torki, 2020) created new"
2020.semeval-1.3,2020.semeval-1.33,0,0.0312792,"set out in (Loureiro and Jorge, 2019), creating pretrained embeddings for each sense in WordNet, this time using XLM-R (Conneau et al., 2019) and SemCor augmented with their own UWA dataset (Loureiro and Camacho-Collados, 2020). This approach achieved second place in the English Subtask 1 and fourth in the English Subtask 2. UZH (Tang, 2020) submitted (after the competition had ended) a system based on the original BERT sense embeddings created for (Loureiro and Jorge, 2019) but improved their performance by combining them with contextualised embeddings. Finally for this group AlexU-AUX-BERT (Mahmoud and Torki, 2020) created new sense embeddings for the competition 43 target words. In order to do so they sourced additional contexts for the top WordNet synsets. Their system scored third in the English Subtask 2. The pretrained WordNet sense embedding proved highly successful in this task, especially in Subtask 2, predicting the similarity scores themselves. The biggest weakness of the approach is their reliance on linguistic resources that don’t exist for most languages other than English. Related to these systems, the submission by MineriaUNAM (Gomez-Adorno et al., 2020) won the English Subtask 2. They pr"
2020.semeval-1.3,2020.semeval-1.36,0,0.0172085,"ss is coming from the human annotations. A related strategy could perhaps be used with embeddings or computed predictions instead of human scores. The next group focused on testing a variety of models and parameters. BRUMS (Hettiarachchi and Ranasinghe, 2020) worked with ELMo, BERT, Flair (Akbik et al., 2018), Transformer-XL (Dai et al., 2019) and XLNet (Yang et al., 2019). Their final submission made use of stacked embeddings proposed by Akbik et al. (2018). They won the Finnish Subtask 2, ended second in the two Slovene ones and performed very well in the two English ones. The Hitachi team (Morishita et al., 2020) looked at BERT and XML-R. Their main insight was that for every language, the layers from the center to the end where always the best performing ones, however while BERT performed best in the last layer, XLM-R did in the center one, suggesting their inner structure is organised differently. They won the Slovene Subtask 1, finished second in the two Croatian subtasks and performed competitively in the English ones. To conclude with this group JUSTMasters (Al-Khdour et al., 2020) tested several models, parameters and their own strategy to combine models. They achieved very good performance, esp"
2020.semeval-1.3,Q17-1022,1,0.867881,"Missing"
2020.semeval-1.3,N18-1202,0,0.17532,"system description papers. A new dataset (CoSimLex) was created for evaluation in this task: it contains pairs of words, each annotated within two short text passages. Systems beat the baselines by significant margins, but few did well in more than one language or subtask. Almost every system employed a Transformer model, but with many variations in the details: WordNet sense embeddings, translation of contexts, TF-IDF weightings, and the automatic creation of datasets for fine-tuning were all used to good effect. 1 Introduction Contextualised word embeddings, produced by models such as ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019), have quickly become the standard in NLP systems. They deliver impressive performance in language modeling and downstream tasks; but there are few resources available which allow intrinsic evaluation in terms of the properties of the embeddings themselves, or their ability to model human perception of meaning, and how these depend on context. For non-contextualised models, resources like WordSim-353 (Finkelstein et al., 2002) and SimLex-999 (Hill et al., 2015) were instrumental to evaluate their ability to reflect human similarity judgements. However these datas"
2020.semeval-1.3,N19-1128,1,0.870958,"ew resources available which allow intrinsic evaluation in terms of the properties of the embeddings themselves, or their ability to model human perception of meaning, and how these depend on context. For non-contextualised models, resources like WordSim-353 (Finkelstein et al., 2002) and SimLex-999 (Hill et al., 2015) were instrumental to evaluate their ability to reflect human similarity judgements. However these datasets treat pairs of words in isolation, and thus cannot tell us much about the effect of context. The few resources that work with context, like SCWS (Huang et al., 2012), WiC (Pilehvar and Camacho-Collados, 2019), and WSim (Erk et al., 2013), focus on word sense and discrete effects, thus missing the more graded effects that context has on words in general, and that approaches like ELMo and BERT would seem well suited to model. Further, USim (Erk et al., 2013) focuses on separate sentential contexts only in the English language. The goal of SemEval-2020 Task 3: Graded Word Similarity in Context, was to move towards filling that gap. We created a new dataset, CoSimLex (Armendariz et al., 2020), which builds on the familiar pairwise, graded similarity task of SimLex-999, but extends it to pairs of words"
2020.semeval-1.3,2020.semeval-1.18,0,0.0318793,"languages and then used a weighted combination of monolingual pretrained non contextualised embeddings and BERT embeddings. Their idea is that the translation not only brings new resources but the process itself can produce useful information, for example to disambiguate. The approach works very well for the less resourced languages, being clearly the best system in that category, in both Subtask 1 and 2. Their system won Subtask 1 and 2 for Croatian (by a healthy margin) and 2 for Slovene, ending third in the Slovene Subtask 1 and third and second in the two Finnish ones. The MultiSem team (Soler and Apidianaki, 2020) collected 5 different datasets in order to fine-tune their BERT models, most of them automatically generated from previous datasets to increase contextual influence. As an example, ukWaC-subs was created by substituting target words by either: a correct substitute; a word that could be the right substitute in other circumstances but it is not in this context; or a random word. The datasets included WiC, which when used to fine tune the model resulted in the best performance for Subtask1, giving them a third place. The approach works very well, giving a very consistent performance in all categ"
2020.semeval-1.3,2020.semeval-1.19,0,0.0366991,"l discuss here the results of the remaining 11 systems. First, we describe a group of systems designed around sense embeddings created using WordNet (Miller, 1995) as a guide. The most successful was the submission by LMMS. They employed a similar strategy to the one set out in (Loureiro and Jorge, 2019), creating pretrained embeddings for each sense in WordNet, this time using XLM-R (Conneau et al., 2019) and SemCor augmented with their own UWA dataset (Loureiro and Camacho-Collados, 2020). This approach achieved second place in the English Subtask 1 and fourth in the English Subtask 2. UZH (Tang, 2020) submitted (after the competition had ended) a system based on the original BERT sense embeddings created for (Loureiro and Jorge, 2019) but improved their performance by combining them with contextualised embeddings. Finally for this group AlexU-AUX-BERT (Mahmoud and Torki, 2020) created new sense embeddings for the competition 43 target words. In order to do so they sourced additional contexts for the top WordNet synsets. Their system scored third in the English Subtask 2. The pretrained WordNet sense embedding proved highly successful in this task, especially in Subtask 2, predicting the si"
2020.semeval-1.3,2020.lrec-1.582,1,0.835923,"Missing"
2020.semeval-1.3,W17-0228,0,0.0235541,"ilarity of words and the effect that context has on it. Good context-independent models could theoretically give reasonably competitive results in this task, however we still expect context-dependent models to have a considerable advantage. 4 Dataset CoSimLex (Armendariz et al., 2020) is based on pairs of words from SimLex-999 (Hill et al., 2015); the reliability and common use of SimLex makes it a good starting point and allows comparison of judgements and model outputs to the context-independent case. For Croatian and Finnish we use existing translations of SimLex-999 (Mrkši´c et al., 2017; Venekoski and Vankka, 2017; Kittask, 2019). In the case of Slovene, we have produced our own new translation,1 following Mrkši´c et al. (2017)’s methodology for Croatian. The dataset consists of 340 pairs in English, 112 in Croatian, 111 in Slovene and 24 in Finnish. Each pair is rated within two different contexts, giving a total of 1174 scores of contextual similarity. This poses a difficult task: to find suitable, organically occurring contexts; this task is even more challenging for languages with less resources, and as a result the selection of pairs is different for each language. Each line of CoSimLex is made of"
2020.semeval-1.3,2020.cl-4.5,1,0.887547,"Missing"
2020.vardial-1.1,2020.vardial-1.24,0,0.254959,"Missing"
2020.vardial-1.1,2020.vardial-1.26,0,0.535455,"y stages of the COVID-19 pandemic. Lock downs and restrictive measures in many countries during this period have impacted universities and research centers worldwide causing significant disruption. We believe that this situation is very likely to have discouraged more teams to participate in this year’s evaluation campaign. 2 Team RDI Akanksha Anumit¸i CUBoulder-UBC Phlyers HeLju NRC Piyush Mishra SUKI The Linguistadors T¨ubingen UAIC UnibucKernel UPB ZHAW-InIT Total SMG ULI System Description Paper X X (Popa and S, tef˘anescu, 2020) * (Ceolin and Zhang, 2020) (Scherrer and Ljubeˇsi´c, 2020) (Bernier-Colborne and Goutte, 2020) (Mishra, 2020) (Jauhiainen et al., 2020a) X X X X X X X X X X X (C¸o¨ ltekin, 2020) (Rebeja and Cristea, 2020) (G˘aman and Ionescu, 2020a) (Zaharia et al., 2020) (Benites et al., 2020) X X X 8 7 1 11 Table 1: The teams that participated in the VarDial Evaluation Campaign 2020 along with their system description papers. *The system description paper by team CUBoulder-UBC does not appear in the VarDial workshop proceedings. CUBoulder-UBC reused a system described in Hulden et al. (2015). 4 Romanian Dialect Identification RDI 4.1 Dataset The training data is composed of news articles from the Mo"
2020.vardial-1.1,W19-1402,0,0.3829,"Missing"
2020.vardial-1.1,W18-3909,1,0.890114,"Missing"
2020.vardial-1.1,P19-1068,1,0.744799,"Commons Attribution 4.0 International License. //creativecommons.org/licenses/by/4.0/. 1 https://sites.google.com/view/vardial2020/evaluation-campaign 2 For recent surveys on these topics see Zampieri et al. (2020) and Jauhiainen et al. (2019c). License details: http: 1 Proceedings of the 7th VarDial Workshop on NLP for Similar Languages, Varieties and Dialects, pages 1–14 Barcelona, Spain (Online), December 13, 2020 2 Shared Tasks at VarDial 2020 Romanian Dialect Identification (RDI): In the Romanian Dialect Identification (RDI) shared task, we provided participants with the MOROCO data set (Butnaru and Ionescu, 2019) for training, which contains Moldavian (MD) and Romanian (RO) samples of text collected from the news domain. The task was a binary classification by dialect, in which a classification model is required to discriminate between the Moldavian (MD) and the Romanian (RO) dialects. The task was closed, therefore, participants are not allowed to use external data to train their models. The test set contained newly collected text samples from a different domain, not previously included in MOROCO, resulting in a cross-domain dialect identification task. Social Media Variety Geolocation (SMG): In cont"
2020.vardial-1.1,2020.vardial-1.25,0,0.269727,"Missing"
2020.vardial-1.1,2020.vardial-1.17,0,0.072284,"Missing"
2020.vardial-1.1,N19-1423,0,0.024395,"ainen et al. (2020b). 6.2 Participants and Approaches Unfortunately, the ULI shared task had only one team submitting results to the tracks. The NRC team submitted three runs for each of the shared task tracks. All the runs used BERT-related deep neural networks taking sequences of characters as input similar to what the NRC team used when they won the CLI shared task (Jauhiainen et al., 2019b) in the previous VarDial Evaluation Campaign (BernierColborne et al., 2019). The encoders of the networks were pre-trained on masked language modeling (MLM) and sentence pair classification (SPC) tasks (Devlin et al., 2019). The third run on each track was using only the information on the training set as opposed to the second run, in which the MLM was also done on the unlabeled test set in order to adapt the model. The first run on each track was a plurality voting ensemble of the six models used in the second and third runs of all the tracks. 6.3 Results For the baseline, we used an implementation of the HeLI method equal to the one we used when evaluating language identification methods for 285 languages (Jauhiainen et al., 2017). The baseline and the NRC teams results are listed in Tables 8, 9, and 10. Rank"
2020.vardial-1.1,2020.findings-emnlp.387,0,0.274164,"Missing"
2020.vardial-1.1,goldhahn-etal-2012-building,0,0.217587,"same data format and evaluation methodology. Both constrained and unconstrained submissions were allowed, but only one participating team made use of the latter. Uralic Language Identification (ULI): This shared task focused on discriminating between endangered languages of the Uralic group. In addition to 29 Uralic minority languages, the shared task also featured 149 non-relevant languages. For training, we provided texts from the Wanca 2016 corpora (Jauhiainen et al., 2019a) for the relevant languages while the texts for the non-relevant languages came from the Leipzig corpora collection (Goldhahn et al., 2012). The test set for the relevant languages included sentences from the forthcoming Wanca 2017 corpora (Jauhiainen et al., 2020b) that were not present in the Wanca 2016 corpora. The sentences for the non-relevant languages were from the Leipzig corpora collection. The ULI shared task was divided into three separate tracks using the same training and test data. The difference between the tracks was based on how the submissions were scored: track 1 focused on macro-averaged F-score for the 29 relevant languages, track 2 on micro-averaged F-score for the relevant languages, and track 3 on macro-av"
2020.vardial-1.1,L16-1284,1,0.891978,"Missing"
2020.vardial-1.1,2020.vardial-1.23,1,0.814174,"Missing"
2020.vardial-1.1,D18-1469,1,0.402749,"e SMG task is split into three subtasks covering different language areas: the BCMS subtask is focused on geolocated tweets published in the area of Croatia, Bosnia and Herzegovina, Montenegro and Serbia in the HBS macro-language (Ljubeˇsi´c et al., 2016); the DE-AT subtask focuses on conversations from the microblogging platform Jodel initiated in Germany and Austria, which are written in standard German but commonly contain regional and dialectal forms; the CH subtask is based on Jodel conversations initiated in Switzerland, which were found to be held majoritarily in Swiss German dialects (Hovy and Purschke, 2018). All three subtasks used the same data format and evaluation methodology. Both constrained and unconstrained submissions were allowed, but only one participating team made use of the latter. Uralic Language Identification (ULI): This shared task focused on discriminating between endangered languages of the Uralic group. In addition to 29 Uralic minority languages, the shared task also featured 149 non-relevant languages. For training, we provided texts from the Wanca 2016 corpora (Jauhiainen et al., 2019a) for the relevant languages while the texts for the non-relevant languages came from the"
2020.vardial-1.1,W17-1225,1,0.926999,"Missing"
2020.vardial-1.1,J16-3005,1,0.860747,"h quantile loss. SUKI. This approach divides each geographic area into a fixed grid with 81 areas and uses a n-gram language model to predict the most likely area (Jauhiainen et al., 2020a). The Linguistadors. These submissions are based on classic regression methods (linear regression, lasso regression, and ridge regression) and rely on TF-IDF weighted input features. UnibucKernel. The UnibucKernel team (G˘aman and Ionescu, 2020a) submitted two single systems, a character-level CNN (Zhang et al., 2015) with double regression output, and a Nu-SVR model trained on top of n-gram string kernels (Ionescu et al., 2016). The third system is an ensemble approach based on XGBoost, trained on the predictions provided by the two previously mentioned systems and an LSTMbased one. The LSTM is trained on top of fine-tuned German BERT embeddings. ZHAW-InIT. The ZHAW-InIT team (Benites et al., 2020) uses unsupervised k-means clustering to infer a set of dialect classes which are then used in a classification architecture. Their systems are based 7 either on SVMs with TF-IDF weighted word and character n-gram features, or on the HELI language modeling architecture (ZHAW-InIT (HELI)). The SVM submission to the CH subta"
2020.vardial-1.1,W17-0221,1,0.877702,"Missing"
2020.vardial-1.1,W19-1409,1,0.856366,"Missing"
2020.vardial-1.1,2020.vardial-1.21,1,0.758565,"Missing"
2020.vardial-1.1,W16-4801,1,0.731395,"Missing"
2020.vardial-1.1,2020.vardial-1.27,0,0.363938,"ock downs and restrictive measures in many countries during this period have impacted universities and research centers worldwide causing significant disruption. We believe that this situation is very likely to have discouraged more teams to participate in this year’s evaluation campaign. 2 Team RDI Akanksha Anumit¸i CUBoulder-UBC Phlyers HeLju NRC Piyush Mishra SUKI The Linguistadors T¨ubingen UAIC UnibucKernel UPB ZHAW-InIT Total SMG ULI System Description Paper X X (Popa and S, tef˘anescu, 2020) * (Ceolin and Zhang, 2020) (Scherrer and Ljubeˇsi´c, 2020) (Bernier-Colborne and Goutte, 2020) (Mishra, 2020) (Jauhiainen et al., 2020a) X X X X X X X X X X X (C¸o¨ ltekin, 2020) (Rebeja and Cristea, 2020) (G˘aman and Ionescu, 2020a) (Zaharia et al., 2020) (Benites et al., 2020) X X X 8 7 1 11 Table 1: The teams that participated in the VarDial Evaluation Campaign 2020 along with their system description papers. *The system description paper by team CUBoulder-UBC does not appear in the VarDial workshop proceedings. CUBoulder-UBC reused a system described in Hulden et al. (2015). 4 Romanian Dialect Identification RDI 4.1 Dataset The training data is composed of news articles from the Moldavian and Rom"
2020.vardial-1.1,2020.vardial-1.18,0,0.531349,"Missing"
2020.vardial-1.1,2020.vardial-1.20,0,0.189489,"Missing"
2020.vardial-1.1,L16-1641,1,0.889566,"Missing"
2020.vardial-1.1,2020.vardial-1.19,1,0.750864,"Missing"
2020.vardial-1.1,2020.vardial-1.22,0,0.515168,"Missing"
2020.vardial-1.1,W17-1201,1,0.773425,"Missing"
2020.vardial-1.19,W19-1402,0,0.207892,"Missing"
2020.vardial-1.19,2020.acl-main.747,0,0.0924702,"Missing"
2020.vardial-1.19,N19-1423,0,0.0497397,"d TF-IDF-weighted n-grams of length 3 to 6 occurring at least 5 times in the training corpus. For this and all other experiments presented in the paper, we train and test our systems on lower-cased data, as we found no evidence that casing information would be relevant for geolocation. No further pre-processing was applied to the data. Table 2 (second row) shows that this approach easily beats the centroid baseline for all three subtasks. 4.2 Neural machine learning approaches In recent years, pre-trained language representations have become very successful for various downstream tasks. BERT (Devlin et al., 2019) is currently one of the most popular pre-trained language representation frameworks and is based on the Transformer neural network architecture (Vaswani et al., 2017). Typically, a BERT model is created in two phases. In the pre-training phase, a Transformer is trained from scratch using a masked language modeling task. This task only requires unlabeled data. A 1 We used the MultiOutputRegressor class of the Scikit-Learn toolkit (Pedregosa et al., 2011) to combine the two models. 203 Median distance (km) BCMS DEAT CH Model Centroid baseline SVR with TF-IDF character n-grams Constrained BERT M"
2020.vardial-1.19,C12-1064,0,0.117972,"jana) submission to the SMG task. Our motivation was to investigate how existing classification and regression approaches can be adapted to a double regression task. Most of our work is based on the BERT sentence classification architecture in both constrained and unconstrained settings. We experiment with various pre-trained models, different types of coordinate encoding and other hyperparameters. Finally, we manually analyze the development set predictions made with our best-performing models. 2 Related work One of the first works focusing on predicting geolocation from social media text is Han et al. (2012). The authors investigate feature (token) selection methods for location prediction, showing that traditional predictive algorithms yield significantly better results if feature selection is performed. There has been already a shared task on geolocation prediction at WNUT 2016 (Han et al., 2016). The task focused not only on predicting geolocation from text, but also from various user metadata. The best performing systems were combining the available information via feedforward networks or ensembles. Thomas and Hennig (2018) report significant improvements over the winner of the WNUT-16 shared"
2020.vardial-1.19,W16-3928,0,0.143136,"riment with various pre-trained models, different types of coordinate encoding and other hyperparameters. Finally, we manually analyze the development set predictions made with our best-performing models. 2 Related work One of the first works focusing on predicting geolocation from social media text is Han et al. (2012). The authors investigate feature (token) selection methods for location prediction, showing that traditional predictive algorithms yield significantly better results if feature selection is performed. There has been already a shared task on geolocation prediction at WNUT 2016 (Han et al., 2016). The task focused not only on predicting geolocation from text, but also from various user metadata. The best performing systems were combining the available information via feedforward networks or ensembles. Thomas and Hennig (2018) report significant improvements over the winner of the WNUT-16 shared task by learning separately text and metadata embeddings via different neural network architectures This work is licensed under a Creative Commons Attribution 4.0 International Licence. creativecommons.org/licenses/by/4.0/. Licence details: http:// 202 Proceedings of the 7th VarDial Workshop on"
2020.vardial-1.19,2020.lrec-1.329,0,0.220743,"et al., 2019). 3 For all BERT experiments reported in this section, we use the hyperparameters specified in Sections 4.3 and 4.4, but report numbers from single runs. 4 We pre-trained the models for 50 000 steps with a batch size of 32, for all three models. For each model, a WordPiece tokenizer with vocabulary size 30 000 is trained on the same data. 5 https://huggingface.co/bert-base-multilingual-uncased 6 https://huggingface.co/EMBEDDIA/crosloengual-bert 7 https://huggingface.co/dbmdz/bert-base-german-uncased 204 and continued pre-training for another five epochs on the SwissCrawl corpus (Linder et al., 2020). For all three tasks, we rely on the provided tokenizers without modifications. This third setup further improves geolocation results (see last line of Table 2). Our final submissions are based on the first BERT setup for the constrained setting, and on the third BERT setup for the unconstrained setting. 4.3 Hyperparameter tuning During our initial experiments, we found the BERT models to be quite sensitive to some hyperparameter settings. We found that the optimal batch sizes for the fine-tuning step depended on the amount of available training data. We obtained the best results with batch s"
2020.wmt-1.1,2020.nlpcovid19-2.5,1,0.796341,"tails of the evaluation. 4.1.1 Covid Test Suite TICO-19 The TICO-19 test suite was developed to evaluate how well can MT systems handle the newlyemerged topic of COVID-19. Accurate automatic translation can play an important role in facilitating communication in order to protect at-risk populations and combat the infodemic of misinformation, as described by the World Health Organization. The test suite has no corresponding paper so its authors provided an analysis of the outcomes directly here. The submitted systems were evaluated using the test set from the recently-released TICO-19 dataset (Anastasopoulos et al., 2020). The dataset provides manually created translations of COVID19 related data. The test set consists of PubMed articles (678 sentences from 5 scientific articles), patient-medical professional conversations (104 sentences), as well as related Wikipedia articles (411 sentences), announcements (98 sentences from Wikisource), and news items (67 sentences from Wikinews), for a total of 2100 sentences. Table 15 outlines the BLEU scores by each submitted system in the English-to-X directions, also breaking down the results per domain. The analysis shows that some systems are significantly more prepar"
2020.wmt-1.1,2020.wmt-1.6,0,0.0647231,"AIP-NTT U BIQUS UEDIN UEDIN-CUNI UQAM_TAN L E VOLC T RANS W E C HAT WMTB IOMED BASELINE YOLO ZLABS - NLP Institution Air Force Research Laboratory (Gwinnup and Anderson, 2020) Independent submission (Xv, 2020) Charles University (Popel, 2020, 2018; Kocmi, 2020) Dublin City University (Parthasarathy et al., 2020) DeepMind (Yu et al., 2020) DiDi AI Labs (Chen et al., 2020b) (no associated paper) Indepdendent Submission (Kim et al., 2020) eTranslation (Oravecz et al., 2020) Facebook AI (Chen et al., 2020a) University of Groningen (Roest et al., 2020; Dhar et al., 2020) Global Tone Communication (Bei et al., 2020) University of Helsinki and Aalto University (Scherrer et al., 2020a) Huawei TSC (Wei et al., 2020a) Institute of Information Engineering, Chinese Academy of Sciences (Wei et al., 2020b) Microsoft STC India (Goyal et al., 2020) NICT-Kyoto (Marie et al., 2020) NICT-Rui (Li et al., 2020) NiuTrans (Zhang et al., 2020) National Research Council Canada (Knowles et al., 2020) OPPO (Shi et al., 2020) PROMT (Molchanov, 2020) SJTU-NICT (Li et al., 2020) Samsung Research Poland (Krubi´nski et al., 2020) TALP UPC (Escolano et al., 2020) Tencent Translation (Wu et al., 2020b) NLP Lab at Tsinghua Universit"
2020.wmt-1.1,2020.wmt-1.38,0,0.0746945,"Missing"
2020.wmt-1.1,2020.wmt-1.54,1,0.802974,"Missing"
2020.wmt-1.1,W07-0718,1,0.671054,"Missing"
2020.wmt-1.1,W08-0309,1,0.762341,"Missing"
2020.wmt-1.1,W12-3102,1,0.500805,"Missing"
2020.wmt-1.1,2020.lrec-1.461,0,0.0795779,"Missing"
2020.wmt-1.1,2012.eamt-1.60,0,0.124643,"tted systems, for those where the authors provided such details. The training corpus for Inuktitut↔English is the recently released Nunavut Hansard Inuktitut– English Parallel Corpus 3.0 (Joanis et al., 2020). For the Japanese↔English tasks, we added several freely available parallel corpora to the training data. It includes JParaCrawl v2.0 (Morishita et al., 2020), a large web-based parallel corpus, Japanese-English Subtitle Corpus (JESC) (Pryzant et al., 2017), the Kyoto Free Translation Task (KFTT) corpus (Neubig, 2011), constructed from the Kyoto-related Wikipedia articles, and TED Talks (Cettolo et al., 2012). The monolingual data we provided was similar to last year’s, with a 2019 news crawl added to all the news corpora. In addition, we provided versions of the news corpora for Czech, English and German, with both the document and paragraph structure retained. In other words, we did not apply sentence splitting to these corpora, and we retained the document boundaries and text ordering of the originals. Training, development, and test data for Pashto↔English and Khmer↔English are shared with the Parallel Corpus Filtering Shared Task (Koehn et al., 2020). The training data mostly comes from OPUS"
2020.wmt-1.1,2020.wmt-1.3,0,0.0731913,"on Machine Translation (WMT20)1 was held online with EMNLP 2020 and hosted a number of shared tasks on various aspects of machine translation. This conference built on 14 previous editions of WMT as workshops and conferences (Koehn and Monz, 2006; CallisonBurch et al., 2007, 2008, 2009, 2010, 2011, 2012; Bojar et al., 2013, 2014, 2015, 2016, 2017, 2018; Barrault et al., 2019). This year we conducted several official tasks. We report in this paper on the news and similar translation tasks. Additional shared tasks are described in separate papers in these proceedings: • automatic post-editing (Chatterjee et al., 2020) • biomedical translation (Bawden et al., 2020b) • chat translation (Farajian et al., 2020) • lifelong learning (Barrault et al., 2020) 1 Makoto Morishita NTT Santanu Pal WIPRO AI Abstract 1 Philipp Koehn JHU http://www.statmt.org/wmt20/ 1 Proceedings of the 5th Conference on Machine Translation (WMT), pages 1–55 c Online, November 19–20, 2020. 2020 Association for Computational Linguistics as “direct assessment”) that we explored in the previous years with convincing results in terms of the trade-off between annotation effort and reliable distinctions between systems. The primary objectives o"
2020.wmt-1.1,2020.wmt-1.8,0,0.0898111,"D D I D I -NLP DONG - NMT ENMT E T RANSLATION FACEBOOK AI G RONINGEN GTCOM H ELSINKI NLP H UAWEI TSC IIE M ICROSOFT STC I NDIA NICT-K YOTO NICT-RUI N IU T RANS NRC OPPO PROMT SJTU-NICT SRPOL TALP UPC T ENCENT T RANSLATION THUNLP T ILDE T OHOKU -AIP-NTT U BIQUS UEDIN UEDIN-CUNI UQAM_TAN L E VOLC T RANS W E C HAT WMTB IOMED BASELINE YOLO ZLABS - NLP Institution Air Force Research Laboratory (Gwinnup and Anderson, 2020) Independent submission (Xv, 2020) Charles University (Popel, 2020, 2018; Kocmi, 2020) Dublin City University (Parthasarathy et al., 2020) DeepMind (Yu et al., 2020) DiDi AI Labs (Chen et al., 2020b) (no associated paper) Indepdendent Submission (Kim et al., 2020) eTranslation (Oravecz et al., 2020) Facebook AI (Chen et al., 2020a) University of Groningen (Roest et al., 2020; Dhar et al., 2020) Global Tone Communication (Bei et al., 2020) University of Helsinki and Aalto University (Scherrer et al., 2020a) Huawei TSC (Wei et al., 2020a) Institute of Information Engineering, Chinese Academy of Sciences (Wei et al., 2020b) Microsoft STC India (Goyal et al., 2020) NICT-Kyoto (Marie et al., 2020) NICT-Rui (Li et al., 2020) NiuTrans (Zhang et al., 2020) National Research Council Canada (Know"
2020.wmt-1.1,2009.freeopmt-1.3,0,0.088081,"ve (CONTRASTIVE) or primary (PRIMARY), and the BLEU, RIBES and TER results. The scores are sorted by BLEU. In general, primary systems tend to be better than contrastive systems, as expected, but there are some exceptions. This year we recived major number of participants for the case of Indo-Aryan language group NUST-FJWU NUST-FJWU system is an extension of state-of-the-art Transformer model with hierarchical attention networks to incorporate contextual information. During training the model used back-translation. Prompsit This team is participating with a rulebased system based on Apertium (Forcada et al., 2009-11). Apertium is a free/open-source platform for developing rule-based machine translation systems and language technology that was first released in 2005. Apertium is hosted in Github where both language data and code are licensed under the GNU GPL. It is a research and business platform with a very active community that loves small languages. Language pairs are at a very different level of development and output quality in the platform, depending on two main variables: how much funded or in-kind effort has 32 5.4 i.e. Hindi–Marathi (in both directions). We received 22 submissions from 14 te"
2020.wmt-1.1,2020.wmt-1.80,0,0.0933589,"airs, to be evaluated on test sets consisting mainly of news stories. The task was also opened up to additional test suites to probe specific aspects of translation. In the similar language translation task, participants built machine translation systems for translating between closely related pairs of languages. Chi-kiu Lo NRC Masaaki Nagata NTT Marcos Zampieri Rochester Institute of Technology metrics (Mathur et al., 2020) parallel corpus filtering (Koehn et al., 2020) quality estimation (Specia et al., 2020a) robustness (Specia et al., 2020b) unsupervised and very low-resource translation (Fraser, 2020) In the news translation task (Section 2), participants were asked to translate a shared test set, optionally restricting themselves to the provided training data (“constrained” condition). We included 22 translation directions this year, with translation between English and each of Chinese, Czech, German and Russian, as well as French to and from German being repeated from last year, and English to and from Inuktitut, Japanese, Polish and Tamil being new for this year. Furthermore, English to and from Khmer and Pashto were included, using the same test sets as in the corpus filtering task. Th"
2020.wmt-1.1,W19-5204,0,0.0543621,"Missing"
2020.wmt-1.1,2020.emnlp-main.5,0,0.0410594,"luation of out-ofEnglish translations, HITs were generated using the same method as described for the SR+DC evaluation of into-English translations in Section 3.2.1 with minor modifications. Source-based DA allows to include human references in the evaluation as another system to provide an estimate of human performance. Human references were added to the pull of system outputs prior to sampling documents for tasks generation. If multiple references are available, which is the case for English→German (3 alternative reference translations, including 1 generated using the paraphrasing method of Freitag et al. (2020)) and English→Chinese (2 translations), each reference is assessed individually. Since the annotations are made by researchers and professional translators who ensure a betTable 11: Amount of data collected in the WMT20 manual document- and segment-level evaluation campaigns for bilingual/source-based evaluation out of English and nonEnglish pairs. et al., 2020; Laubli et al., 2020). It differs from SR+DC DA introduced in WMT19 (Bojar et al., 2019), and still used in into-English human evaluation this year, where a single segment from a document is provided on a screen at a time, followed by s"
2020.wmt-1.1,W18-3931,1,0.874637,"ese improvements to increased capacity (which allows increased sensitivity to long-range relations) of the models. 5 Similar Language Translation Most shared tasks at WMT (e.g. News, Biomedical) have historically dealt with translating texts from and to English. In recent years, we observed a growing interest in training systems to translate between languages other than English. This includes a number of papers applying MT to translate between pairs of closely-related languages, national language varieties, and dialects 27 of the same language (Zhang, 1998; Marujo et al., 2011; Hassani, 2017; Costa-jussà et al., 2018; Popovi´c et al., 2020). To address this topic, the first Similar Language Translation (SLT) shared task at WMT 2019 has been organized. It featured data from three pairs of closely-related languages from different language families: Spanish - Portuguese (Romance languages), Czech - Polish (Slavic languages), and Hindi - Nepali (IndoAryan languages). Following the success of the first SLT shared task at WMT 2019 and the interest of the community in this topic, we organize, for the second time at WMT, this shared task to evaluate the performance of state-of-the-art translation systems on trans"
2020.wmt-1.1,2020.wmt-1.18,0,0.0913945,"2020b) Microsoft STC India (Goyal et al., 2020) NICT-Kyoto (Marie et al., 2020) NICT-Rui (Li et al., 2020) NiuTrans (Zhang et al., 2020) National Research Council Canada (Knowles et al., 2020) OPPO (Shi et al., 2020) PROMT (Molchanov, 2020) SJTU-NICT (Li et al., 2020) Samsung Research Poland (Krubi´nski et al., 2020) TALP UPC (Escolano et al., 2020) Tencent Translation (Wu et al., 2020b) NLP Lab at Tsinghua University (no associated paper) Tilde (Krišlauks and Pinnis, 2020) Tohoku-AIP-NTT (Kiyono et al., 2020) Ubiqus (Hernandez and Nguyen, 2020) University of Edinburgh (Bawden et al., 2020a; Germann, 2020) University of Edinburgh and Charles University (Germann et al., 2020) Université du Québec à Montréal (no associated paper) ByteDance AI Lab (Wu et al., 2020a) WeChat (Meng et al., 2020) Baseline System from Biomedical Task (Bawden et al., 2020b) American University of Beirut (no associated paper) Zoho Corporation (no associated paper) Table 6: Participants in the shared translation task. Not all teams participated in all language pairs. The translations from the online systems were not submitted by their respective companies but were obtained by us, and are therefore anonymized in a fashion"
2020.wmt-1.1,2020.wmt-1.43,0,0.0835067,"Missing"
2020.wmt-1.1,2020.wmt-1.9,0,0.0939415,"Missing"
2020.wmt-1.1,2020.wmt-1.19,0,0.0674131,"Missing"
2020.wmt-1.1,2009.mtsummit-btm.6,0,0.103443,"Missing"
2020.wmt-1.1,W13-2305,1,0.929934,"work which can be well applied to different translation. directions. Techniques used in the submitted systems include optional multilingual pre-training (mRASP) for low resource languages, very deep Transformer or dynamic convolution models up to 50 encoder layers, iterative backtranslation, knowledge distillation, model ensemble and development set fine-tuning. The key ingredient of the process seems the strong focus on diversification of the (synthetic) training data, using multiple scalings of the Transformer model 3.1 Direct Assessment Since running a comparison of direct assessments (DA, Graham et al., 2013, 2014, 2016) and relative ranking in 2016 (Bojar et al., 2016) and verifying a high correlation of system rankings for the two methods, as well as the advantages of DA, such as quality controlled crowd-sourcing and linear growth relative to numbers of submissions, we have employed DA as the primary mechanism for evaluating systems. With DA human evaluation, 15 human assessors are asked to rate a given translation by how adequately it expresses the meaning of the corresponding reference translation or source language input on an analogue scale, which corresponds to an underlying absolute 0–100"
2020.wmt-1.1,E14-1047,1,0.888167,"Missing"
2020.wmt-1.1,2020.lrec-1.312,1,0.804196,"A screenshot of OCELoT is shown in Figure 5. For presentation of the results, systems are treated as either constrained or unconstrained, depending on whether their models were trained only on the provided data. Since we do not know how they were built, the online systems are treated as unconstrained during the automatic and human evaluations. In the rest of this section, we provide brief details of the submitted systems, for those where the authors provided such details. The training corpus for Inuktitut↔English is the recently released Nunavut Hansard Inuktitut– English Parallel Corpus 3.0 (Joanis et al., 2020). For the Japanese↔English tasks, we added several freely available parallel corpora to the training data. It includes JParaCrawl v2.0 (Morishita et al., 2020), a large web-based parallel corpus, Japanese-English Subtitle Corpus (JESC) (Pryzant et al., 2017), the Kyoto Free Translation Task (KFTT) corpus (Neubig, 2011), constructed from the Kyoto-related Wikipedia articles, and TED Talks (Cettolo et al., 2012). The monolingual data we provided was similar to last year’s, with a 2019 news crawl added to all the news corpora. In addition, we provided versions of the news corpora for Czech, Engli"
2020.wmt-1.1,2020.emnlp-main.6,1,0.839606,"shown in Table 3, where the first and second are simple merges or splits, whereas the third is a rare case of more complex reordering. We leave a detailed analysis of the translators’ treatment of paragraph-split data for future work. development set is provided, it is a mixture of both “source-original” and “target-original” texts, in order to maximise its size, although the original language is always marked in the sgm file, except for Inuktitut↔English. The consequences of directionality in test sets has been discussed recently in the literature (Freitag et al., 2019; Laubli et al., 2020; Graham et al., 2020), and the conclusion is that it can have an effect on detrimental effect on the accuracy of system evaluation. We use “source-original” parallel sentences wherever possible, on the basis that it is the more realistic scenario for practical MT usage. Exception: the test sets for the two Inuktitut↔English translation directions contain the same data, without regard to original direction. For most news text in the test and development sets, English was the original language and Inuktitut the translation, while the parliamentary data mixes the two directions. The origins of the news test documents"
2020.wmt-1.1,2020.wmt-1.11,0,0.0940191,"N GTCOM H ELSINKI NLP H UAWEI TSC IIE M ICROSOFT STC I NDIA NICT-K YOTO NICT-RUI N IU T RANS NRC OPPO PROMT SJTU-NICT SRPOL TALP UPC T ENCENT T RANSLATION THUNLP T ILDE T OHOKU -AIP-NTT U BIQUS UEDIN UEDIN-CUNI UQAM_TAN L E VOLC T RANS W E C HAT WMTB IOMED BASELINE YOLO ZLABS - NLP Institution Air Force Research Laboratory (Gwinnup and Anderson, 2020) Independent submission (Xv, 2020) Charles University (Popel, 2020, 2018; Kocmi, 2020) Dublin City University (Parthasarathy et al., 2020) DeepMind (Yu et al., 2020) DiDi AI Labs (Chen et al., 2020b) (no associated paper) Indepdendent Submission (Kim et al., 2020) eTranslation (Oravecz et al., 2020) Facebook AI (Chen et al., 2020a) University of Groningen (Roest et al., 2020; Dhar et al., 2020) Global Tone Communication (Bei et al., 2020) University of Helsinki and Aalto University (Scherrer et al., 2020a) Huawei TSC (Wei et al., 2020a) Institute of Information Engineering, Chinese Academy of Sciences (Wei et al., 2020b) Microsoft STC India (Goyal et al., 2020) NICT-Kyoto (Marie et al., 2020) NICT-Rui (Li et al., 2020) NiuTrans (Zhang et al., 2020) National Research Council Canada (Knowles et al., 2020) OPPO (Shi et al., 2020) PROMT (Molchanov, 2020) S"
2020.wmt-1.1,D19-1632,1,0.881933,"ent and paragraph structure retained. In other words, we did not apply sentence splitting to these corpora, and we retained the document boundaries and text ordering of the originals. Training, development, and test data for Pashto↔English and Khmer↔English are shared with the Parallel Corpus Filtering Shared Task (Koehn et al., 2020). The training data mostly comes from OPUS (software localization, Tatoeba, Global Voices), the Bible, and specialprepared corpora from TED Talks and the Jehova Witness web site (JW300). The development and test sets were created as part of the Flores initiative (Guzmán et al., 2019) by professional translation of Wikipedia content with careful vetting of the translations. Please refer the to the Parallel Corpus Filtering Shared Task overview paper for details on these corpora. 2.3.1 AFRL (Gwinnup and Anderson, 2020) AFRL - SYSCOMB 20 is a system combination consisting of two Marian transformer ensembles, one OpenNMT transformer system and a Moses phrase-based system. AFRL - FINETUNE is an OpenNMT transformer system fine-tuned on newstest2014-2017. 2.3.2 (Xv, 2020) ARIEL XV is a Transformer base model trained with the Sockeye sequence modeling toolkit usSome statistics ab"
2020.wmt-1.1,2020.wmt-1.12,1,0.754946,"Missing"
2020.wmt-1.1,2020.wmt-1.13,0,0.0737827,"2020b) (no associated paper) Indepdendent Submission (Kim et al., 2020) eTranslation (Oravecz et al., 2020) Facebook AI (Chen et al., 2020a) University of Groningen (Roest et al., 2020; Dhar et al., 2020) Global Tone Communication (Bei et al., 2020) University of Helsinki and Aalto University (Scherrer et al., 2020a) Huawei TSC (Wei et al., 2020a) Institute of Information Engineering, Chinese Academy of Sciences (Wei et al., 2020b) Microsoft STC India (Goyal et al., 2020) NICT-Kyoto (Marie et al., 2020) NICT-Rui (Li et al., 2020) NiuTrans (Zhang et al., 2020) National Research Council Canada (Knowles et al., 2020) OPPO (Shi et al., 2020) PROMT (Molchanov, 2020) SJTU-NICT (Li et al., 2020) Samsung Research Poland (Krubi´nski et al., 2020) TALP UPC (Escolano et al., 2020) Tencent Translation (Wu et al., 2020b) NLP Lab at Tsinghua University (no associated paper) Tilde (Krišlauks and Pinnis, 2020) Tohoku-AIP-NTT (Kiyono et al., 2020) Ubiqus (Hernandez and Nguyen, 2020) University of Edinburgh (Bawden et al., 2020a; Germann, 2020) University of Edinburgh and Charles University (Germann et al., 2020) Université du Québec à Montréal (no associated paper) ByteDance AI Lab (Wu et al., 2020a) WeChat (Meng et al"
2020.wmt-1.1,2020.wmt-1.20,0,0.057602,"Missing"
2020.wmt-1.1,2020.wmt-1.14,1,0.820019,"set, and the origlang tag indicates the original source language. 9 Team AFRL ARIEL XV CUNI DCU D EEP M IND D I D I -NLP DONG - NMT ENMT E T RANSLATION FACEBOOK AI G RONINGEN GTCOM H ELSINKI NLP H UAWEI TSC IIE M ICROSOFT STC I NDIA NICT-K YOTO NICT-RUI N IU T RANS NRC OPPO PROMT SJTU-NICT SRPOL TALP UPC T ENCENT T RANSLATION THUNLP T ILDE T OHOKU -AIP-NTT U BIQUS UEDIN UEDIN-CUNI UQAM_TAN L E VOLC T RANS W E C HAT WMTB IOMED BASELINE YOLO ZLABS - NLP Institution Air Force Research Laboratory (Gwinnup and Anderson, 2020) Independent submission (Xv, 2020) Charles University (Popel, 2020, 2018; Kocmi, 2020) Dublin City University (Parthasarathy et al., 2020) DeepMind (Yu et al., 2020) DiDi AI Labs (Chen et al., 2020b) (no associated paper) Indepdendent Submission (Kim et al., 2020) eTranslation (Oravecz et al., 2020) Facebook AI (Chen et al., 2020a) University of Groningen (Roest et al., 2020; Dhar et al., 2020) Global Tone Communication (Bei et al., 2020) University of Helsinki and Aalto University (Scherrer et al., 2020a) Huawei TSC (Wei et al., 2020a) Institute of Information Engineering, Chinese Academy of Sciences (Wei et al., 2020b) Microsoft STC India (Goyal et al., 2020) NICT-Kyoto (Mari"
2020.wmt-1.1,2020.wmt-1.39,1,0.812433,"kables was collected in the first phase of the annotation, which amounted to 4k assessments across the systems. The second annotation phase with 6.5k assessments compared markable translations, always checking outputs of all the 13 competing MT systems but still considering the document-level context of each of them. Among other things, the observations indicate that the better the system, the lower the variance in manual scores. Markables annotation then confirms that frequent errors like bad translation of a term need not be the most severe and conversely, 4.1.3 Gender Coreference and Bias (Kocmi et al., 2020) The test suite by Kocmi et al. (2020) focuses on the gender bias in professions (e.g. physician, teacher, secretary) for the translation from English into Czech, German, Polish and Russian. These nouns are ambiguous with respect to gender in English but exhibit gender in the examined target languages. The test suite is based on the fact that a pronoun referring to the ambiguous noun can reveal the gender of the noun in the English source sentence. Once disambiguated, the gender needs to be preserved in translation. To correctly translate the given noun, the translation system thus has to corr"
2020.wmt-1.1,2020.wmt-1.53,0,0.089538,"Missing"
2020.wmt-1.1,2020.wmt-1.78,1,0.815194,"rence on Machine Translation (WMT) 2020. In the news task, participants were asked to build machine translation systems for any of 11 language pairs, to be evaluated on test sets consisting mainly of news stories. The task was also opened up to additional test suites to probe specific aspects of translation. In the similar language translation task, participants built machine translation systems for translating between closely related pairs of languages. Chi-kiu Lo NRC Masaaki Nagata NTT Marcos Zampieri Rochester Institute of Technology metrics (Mathur et al., 2020) parallel corpus filtering (Koehn et al., 2020) quality estimation (Specia et al., 2020a) robustness (Specia et al., 2020b) unsupervised and very low-resource translation (Fraser, 2020) In the news translation task (Section 2), participants were asked to translate a shared test set, optionally restricting themselves to the provided training data (“constrained” condition). We included 22 translation directions this year, with translation between English and each of Chinese, Czech, German and Russian, as well as French to and from German being repeated from last year, and English to and from Inuktitut, Japanese, Polish and Tamil being new fo"
2020.wmt-1.1,W17-1208,0,0.0524248,"ttribute all these improvements to increased capacity (which allows increased sensitivity to long-range relations) of the models. 5 Similar Language Translation Most shared tasks at WMT (e.g. News, Biomedical) have historically dealt with translating texts from and to English. In recent years, we observed a growing interest in training systems to translate between languages other than English. This includes a number of papers applying MT to translate between pairs of closely-related languages, national language varieties, and dialects 27 of the same language (Zhang, 1998; Marujo et al., 2011; Hassani, 2017; Costa-jussà et al., 2018; Popovi´c et al., 2020). To address this topic, the first Similar Language Translation (SLT) shared task at WMT 2019 has been organized. It featured data from three pairs of closely-related languages from different language families: Spanish - Portuguese (Romance languages), Czech - Polish (Slavic languages), and Hindi - Nepali (IndoAryan languages). Following the success of the first SLT shared task at WMT 2019 and the interest of the community in this topic, we organize, for the second time at WMT, this shared task to evaluate the performance of state-of-the-art tr"
2020.wmt-1.1,2020.wmt-1.21,0,0.0791885,"Missing"
2020.wmt-1.1,2020.wmt-1.23,0,0.0607352,"020) Dublin City University (Parthasarathy et al., 2020) DeepMind (Yu et al., 2020) DiDi AI Labs (Chen et al., 2020b) (no associated paper) Indepdendent Submission (Kim et al., 2020) eTranslation (Oravecz et al., 2020) Facebook AI (Chen et al., 2020a) University of Groningen (Roest et al., 2020; Dhar et al., 2020) Global Tone Communication (Bei et al., 2020) University of Helsinki and Aalto University (Scherrer et al., 2020a) Huawei TSC (Wei et al., 2020a) Institute of Information Engineering, Chinese Academy of Sciences (Wei et al., 2020b) Microsoft STC India (Goyal et al., 2020) NICT-Kyoto (Marie et al., 2020) NICT-Rui (Li et al., 2020) NiuTrans (Zhang et al., 2020) National Research Council Canada (Knowles et al., 2020) OPPO (Shi et al., 2020) PROMT (Molchanov, 2020) SJTU-NICT (Li et al., 2020) Samsung Research Poland (Krubi´nski et al., 2020) TALP UPC (Escolano et al., 2020) Tencent Translation (Wu et al., 2020b) NLP Lab at Tsinghua University (no associated paper) Tilde (Krišlauks and Pinnis, 2020) Tohoku-AIP-NTT (Kiyono et al., 2020) Ubiqus (Hernandez and Nguyen, 2020) University of Edinburgh (Bawden et al., 2020a; Germann, 2020) University of Edinburgh and Charles University (Germann et al., 2"
2020.wmt-1.1,2020.wmt-1.77,1,0.84299,"slation task, both organised alongside the Conference on Machine Translation (WMT) 2020. In the news task, participants were asked to build machine translation systems for any of 11 language pairs, to be evaluated on test sets consisting mainly of news stories. The task was also opened up to additional test suites to probe specific aspects of translation. In the similar language translation task, participants built machine translation systems for translating between closely related pairs of languages. Chi-kiu Lo NRC Masaaki Nagata NTT Marcos Zampieri Rochester Institute of Technology metrics (Mathur et al., 2020) parallel corpus filtering (Koehn et al., 2020) quality estimation (Specia et al., 2020a) robustness (Specia et al., 2020b) unsupervised and very low-resource translation (Fraser, 2020) In the news translation task (Section 2), participants were asked to translate a shared test set, optionally restricting themselves to the provided training data (“constrained” condition). We included 22 translation directions this year, with translation between English and each of Chinese, Czech, German and Russian, as well as French to and from German being repeated from last year, and English to and from Inu"
2020.wmt-1.1,2020.wmt-1.24,0,0.0435945,"Missing"
2020.wmt-1.1,D18-1512,0,0.05415,"Missing"
2020.wmt-1.1,2020.wmt-1.47,1,0.740067,"Missing"
2020.wmt-1.1,W18-3601,1,0.891679,", we have employed DA as the primary mechanism for evaluating systems. With DA human evaluation, 15 human assessors are asked to rate a given translation by how adequately it expresses the meaning of the corresponding reference translation or source language input on an analogue scale, which corresponds to an underlying absolute 0–100 rating scale.5 No sentence or document length restriction is applied during manual evaluation. Direct Assessment is also employed for evaluation of video captioning systems at TRECvid (Graham et al., 2018; Awad et al., 2019) and multilingual surface realisation (Mille et al., 2018, 2019). 3.1.1 tion 2, most of our test sets do not include reversecreated sentence pairs, except when there were resource constraints on the creation of the test sets. 3.1.3 Prior to WMT19, the issue of including document context was raised within the community (Läubli et al., 2018; Toral et al., 2018) and at WMT19 a range of DA styles were subsequently tested that included document context. In WMT19, two options were run, firstly, an evaluation that included the document context “+DC” (with document context), and secondly, a variation that omitted document context “−DC” (without document con"
2020.wmt-1.1,2020.wmt-1.27,0,0.247738,"he original source language. 9 Team AFRL ARIEL XV CUNI DCU D EEP M IND D I D I -NLP DONG - NMT ENMT E T RANSLATION FACEBOOK AI G RONINGEN GTCOM H ELSINKI NLP H UAWEI TSC IIE M ICROSOFT STC I NDIA NICT-K YOTO NICT-RUI N IU T RANS NRC OPPO PROMT SJTU-NICT SRPOL TALP UPC T ENCENT T RANSLATION THUNLP T ILDE T OHOKU -AIP-NTT U BIQUS UEDIN UEDIN-CUNI UQAM_TAN L E VOLC T RANS W E C HAT WMTB IOMED BASELINE YOLO ZLABS - NLP Institution Air Force Research Laboratory (Gwinnup and Anderson, 2020) Independent submission (Xv, 2020) Charles University (Popel, 2020, 2018; Kocmi, 2020) Dublin City University (Parthasarathy et al., 2020) DeepMind (Yu et al., 2020) DiDi AI Labs (Chen et al., 2020b) (no associated paper) Indepdendent Submission (Kim et al., 2020) eTranslation (Oravecz et al., 2020) Facebook AI (Chen et al., 2020a) University of Groningen (Roest et al., 2020; Dhar et al., 2020) Global Tone Communication (Bei et al., 2020) University of Helsinki and Aalto University (Scherrer et al., 2020a) Huawei TSC (Wei et al., 2020a) Institute of Information Engineering, Chinese Academy of Sciences (Wei et al., 2020b) Microsoft STC India (Goyal et al., 2020) NICT-Kyoto (Marie et al., 2020) NICT-Rui (Li et al., 2020) NiuTrans"
2020.wmt-1.1,D19-6301,1,0.888512,"Missing"
2020.wmt-1.1,W18-6424,0,0.0431841,"(Kocmi, 2020) combines transfer learning from a high-resource language pair Czech–English into the low-resource Inuktitut-English with an additional backtranslation step. Surprising behaviour is noticed when using synthetic data, which can be possibly attributed to a narrow domain of training and test data. The system is the Transformer model in a constrained submission. 2.3.3 Charles University (CUNI) CUNI-D OC T RANSFORMER (Popel, 2020) is similar to the sentence-level version (CUNI-T2T2018, CUBBITT), but trained on sequences with multiple sentences of up to 3000 characters. CUNI-T2T-2018 (Popel, 2018), also called CUBBITT, is exactly the same system as in WMT2018. It is the Transformer model trained according to Popel and Bojar (2018) plus a novel concat-regime backtranslation with checkpoint averaging (Popel et al., 2020), tuned separately for CZ-domain and non CZ-domain articles, possibly handling also translation-direction (“translationese”) issues. For cs→en also a coreference preprocessing was used adding the female-gender CUNI-T RANSFORMER (Popel, 2020) is similar to the WMT2018 version of CUBBITT, but with 12 encoder layers instead of 6 and trained on CzEng 2.0 instead of CzEng 1.7."
2020.wmt-1.1,2020.wmt-1.25,0,0.094349,"Missing"
2020.wmt-1.1,2020.wmt-1.28,0,0.0792624,"cument in the test set, and the origlang tag indicates the original source language. 9 Team AFRL ARIEL XV CUNI DCU D EEP M IND D I D I -NLP DONG - NMT ENMT E T RANSLATION FACEBOOK AI G RONINGEN GTCOM H ELSINKI NLP H UAWEI TSC IIE M ICROSOFT STC I NDIA NICT-K YOTO NICT-RUI N IU T RANS NRC OPPO PROMT SJTU-NICT SRPOL TALP UPC T ENCENT T RANSLATION THUNLP T ILDE T OHOKU -AIP-NTT U BIQUS UEDIN UEDIN-CUNI UQAM_TAN L E VOLC T RANS W E C HAT WMTB IOMED BASELINE YOLO ZLABS - NLP Institution Air Force Research Laboratory (Gwinnup and Anderson, 2020) Independent submission (Xv, 2020) Charles University (Popel, 2020, 2018; Kocmi, 2020) Dublin City University (Parthasarathy et al., 2020) DeepMind (Yu et al., 2020) DiDi AI Labs (Chen et al., 2020b) (no associated paper) Indepdendent Submission (Kim et al., 2020) eTranslation (Oravecz et al., 2020) Facebook AI (Chen et al., 2020a) University of Groningen (Roest et al., 2020; Dhar et al., 2020) Global Tone Communication (Bei et al., 2020) University of Helsinki and Aalto University (Scherrer et al., 2020a) Huawei TSC (Wei et al., 2020a) Institute of Information Engineering, Chinese Academy of Sciences (Wei et al., 2020b) Microsoft STC India (Goyal et al., 20"
2020.wmt-1.1,2020.lrec-1.443,1,0.79707,"er their models were trained only on the provided data. Since we do not know how they were built, the online systems are treated as unconstrained during the automatic and human evaluations. In the rest of this section, we provide brief details of the submitted systems, for those where the authors provided such details. The training corpus for Inuktitut↔English is the recently released Nunavut Hansard Inuktitut– English Parallel Corpus 3.0 (Joanis et al., 2020). For the Japanese↔English tasks, we added several freely available parallel corpora to the training data. It includes JParaCrawl v2.0 (Morishita et al., 2020), a large web-based parallel corpus, Japanese-English Subtitle Corpus (JESC) (Pryzant et al., 2017), the Kyoto Free Translation Task (KFTT) corpus (Neubig, 2011), constructed from the Kyoto-related Wikipedia articles, and TED Talks (Cettolo et al., 2012). The monolingual data we provided was similar to last year’s, with a 2019 news crawl added to all the news corpora. In addition, we provided versions of the news corpora for Czech, English and German, with both the document and paragraph structure retained. In other words, we did not apply sentence splitting to these corpora, and we retained t"
2020.wmt-1.1,2020.wmt-1.48,0,0.090424,"Missing"
2020.wmt-1.1,2020.wmt-1.49,0,0.0519886,"Missing"
2020.wmt-1.1,W19-6712,0,0.0573476,"tly crawled multilingual parallel corpora from Indian government websites (Haddow and Kirefu, 2020; Siripragada et al., 2020), the Tanzil corpus (Tiedemann, 2009), the Pavlick dicParagraph-split Test Sets For the language pairs English↔Czech, English↔German and English→Chinese, we provided the translators with paragraph-split texts, instead of sentence-split texts. We did this in order to provide the translators with greater freedom and, hopefully, to improve the quality of the translation. Allowing translators to merge and split sentences removes one of the “translation shifts” identified by Popovic (2019), which can make translations create solely for MT evaluation different from translations produced for other purposes. We first show some descriptive statistics of the source texts, for Czech, English and German, in 3 Europarl Parallel Corpus Czech ↔ English German ↔ English Polish↔ English German ↔ French Sentences 645,241 1,825,745 632,435 1,801,076 Words 14,948,900 17,380,340 48,125,573 50,506,059 14,691,199 16,995,232 47,517,102 55,366,136 Distinct words 172,452 63,289 371,748 113,960 170,271 62,694 368,585 134,762 News Commentary Parallel Corpus Czech ↔ English 248,927 5,570,734 6,156,063"
2020.wmt-1.1,2020.wmt-1.26,0,0.0845151,"Missing"
2020.wmt-1.1,2020.vardial-1.10,0,0.0933804,"Missing"
2020.wmt-1.1,W18-6301,0,0.038239,"Missing"
2020.wmt-1.1,2020.wmt-1.51,0,0.0917045,"Missing"
2020.wmt-1.1,2020.wmt-1.50,1,0.78567,"Missing"
2020.wmt-1.1,2020.wmt-1.52,0,0.0485419,"Missing"
2020.wmt-1.1,P19-1164,0,0.0581517,"26 26.37 25.51 24.82 28.33 23.33 21.13 21.96 20.43 22.90 22.58 21.90 22.17 22.17 20.53 19.40 20.01 40.44 32.39 30.39 37.04 32.27 27.54 25.97 26.09 46.38 37.30 36.05 35.96 33.76 33.07 27.20 27.07 Table 15: TICO-19 test suite results on the English-to-X WMT20 translation directions. 26 4.1.5 antecedent (a less common direction of information flow), and then correctly express the noun in the target language. The success of the MT system in this test can be established automatically, whenever the gender of the target word can be automatically identified. Kocmi et al. (2020) build upon the WinoMT (Stanovsky et al., 2019) test set, which provides exactly the necessary type of sentences containing an ambiguous profession noun and a personal pronoun which unambiguously (for the human eye) refers to it based the situation described. When extending WinMT with Czech and Polish, Stanovsky et al. have to disregard some test patterns but the principle remains. The results indicate that all MT systems fail in this test, following gender bias (stereotypical patterns attributing the masculine gender to some professions and feminine gender to others) rather than the coreference link. Word Sense Disambiguation (Scherrer et"
2020.wmt-1.1,2020.wmt-1.31,0,0.0881792,"morphological segmentation of the polysynthetic Inuktitut, testing rule-based, supervised, semi-supervised as well as unsupervised word segmentation methods, (2) whether or not adding data from a related language (Greenlandic) helps, and (3) whether contextual word embeddings (XLM) improve translation. G RONINGEN - ENIU use Transformer implemented in Marian with the default setting, improving the performance also with tagged backtranslation, domain-specific data, ensembling and finetuning. 2.3.7 DONG - NMT (no associated paper) No description provided. 2.3.8 ENMT (Kim et al., 2020) Kim et al. (2020) base their approach on transferring knowledge of domain and linguistic characteristics by pre-training the encoder-decoder model with large amount of in-domain monolingual data through unsupervised and supervised prediction task. The model is then fine-tuned with parallel data and in-domain synthetic data, generated with iterative back-translation. For additional gain, final results are generated with an ensemble model and re-ranked with averaged models and language models. G RONINGEN - ENTAM (Dhar et al., 2020) study the effects of various techniques such as linguistically motivated segmenta"
2020.wmt-1.1,2020.wmt-1.32,0,0.0839317,"- NLP Institution Air Force Research Laboratory (Gwinnup and Anderson, 2020) Independent submission (Xv, 2020) Charles University (Popel, 2020, 2018; Kocmi, 2020) Dublin City University (Parthasarathy et al., 2020) DeepMind (Yu et al., 2020) DiDi AI Labs (Chen et al., 2020b) (no associated paper) Indepdendent Submission (Kim et al., 2020) eTranslation (Oravecz et al., 2020) Facebook AI (Chen et al., 2020a) University of Groningen (Roest et al., 2020; Dhar et al., 2020) Global Tone Communication (Bei et al., 2020) University of Helsinki and Aalto University (Scherrer et al., 2020a) Huawei TSC (Wei et al., 2020a) Institute of Information Engineering, Chinese Academy of Sciences (Wei et al., 2020b) Microsoft STC India (Goyal et al., 2020) NICT-Kyoto (Marie et al., 2020) NICT-Rui (Li et al., 2020) NiuTrans (Zhang et al., 2020) National Research Council Canada (Knowles et al., 2020) OPPO (Shi et al., 2020) PROMT (Molchanov, 2020) SJTU-NICT (Li et al., 2020) Samsung Research Poland (Krubi´nski et al., 2020) TALP UPC (Escolano et al., 2020) Tencent Translation (Wu et al., 2020b) NLP Lab at Tsinghua University (no associated paper) Tilde (Krišlauks and Pinnis, 2020) Tohoku-AIP-NTT (Kiyono et al., 2020) Ub"
2020.wmt-1.1,2020.wmt-1.33,0,0.080166,"Missing"
2020.wmt-1.1,2020.wmt-1.34,0,0.0803867,"Missing"
2020.wmt-1.1,2020.wmt-1.35,0,0.0951745,"ss web site (JW300). The development and test sets were created as part of the Flores initiative (Guzmán et al., 2019) by professional translation of Wikipedia content with careful vetting of the translations. Please refer the to the Parallel Corpus Filtering Shared Task overview paper for details on these corpora. 2.3.1 AFRL (Gwinnup and Anderson, 2020) AFRL - SYSCOMB 20 is a system combination consisting of two Marian transformer ensembles, one OpenNMT transformer system and a Moses phrase-based system. AFRL - FINETUNE is an OpenNMT transformer system fine-tuned on newstest2014-2017. 2.3.2 (Xv, 2020) ARIEL XV is a Transformer base model trained with the Sockeye sequence modeling toolkit usSome statistics about the training and test materials are given in Figures 1, 2, 3 and 4. 4 8 ARIEL XV https://github.com/AppraiseDev/OCELoT English I English II English III Chinese Czech German Inuktitut Japanese Polish Russian Tamil ABC News (2), All Africa (5), Brisbane Times (1), CBS LA (1), CBS News (1), CNBC (3), CNN (2), Daily Express (1), Daily Mail (2), Fox News (1), Gateway (1), Guardian (3), Huffington Post (2), London Evening Standard (2), Metro (2), NDTV (7), RTE (7), Reuters (4), STV (2), S"
2020.wmt-1.1,2020.wmt-1.55,0,0.0877526,"Missing"
2020.wmt-1.1,P17-4012,0,0.0273892,"o SJTU-NICT using large XLM model to improve NMT but the exact relation is unclear. 2.3.14 H UAWEI TSC (Wei et al., 2020a) H UAWEI TSC use Transformer-big with a further increased model size, focussing on standard techniques of careful pre-processing and filtering, back-translation and forward translation, including self-training, i.e. translating one of the sides of the original parallel data. Ensembling of individual training runs is used in the forward as well as backward translation, and single models are created from the ensembles using knowledge distillation. The submission uses THUNMT (Zhang et al., 2017) open-source engine. 2.3.19 N IU T RANS (Zhang et al., 2020) N IU T RANS gain their performance from focussed attention to six areas: (1) careful data preprocessing and filtering, (2) iterative back-translation to generate additional training data, (3) using different model architectures, such as wider and/or deeper models, relative position representation and relative length, to enhance the diversity of translations, (4) iterative knowledge distillation by in-domain monolingual data, (5) iterative finetuning for domain adaptation using small training batches, (6) rule-based post-processing of"
2020.wmt-1.1,P98-2238,0,0.590812,"and punctuation, and we tend to attribute all these improvements to increased capacity (which allows increased sensitivity to long-range relations) of the models. 5 Similar Language Translation Most shared tasks at WMT (e.g. News, Biomedical) have historically dealt with translating texts from and to English. In recent years, we observed a growing interest in training systems to translate between languages other than English. This includes a number of papers applying MT to translate between pairs of closely-related languages, national language varieties, and dialects 27 of the same language (Zhang, 1998; Marujo et al., 2011; Hassani, 2017; Costa-jussà et al., 2018; Popovi´c et al., 2020). To address this topic, the first Similar Language Translation (SLT) shared task at WMT 2019 has been organized. It featured data from three pairs of closely-related languages from different language families: Spanish - Portuguese (Romance languages), Czech - Polish (Slavic languages), and Hindi - Nepali (IndoAryan languages). Following the success of the first SLT shared task at WMT 2019 and the interest of the community in this topic, we organize, for the second time at WMT, this shared task to evaluate th"
2020.wmt-1.1,2020.wmt-1.41,1,0.814291,"Missing"
2021.bsnlp-1.5,P19-4007,0,0.0250319,"Missing"
2021.bsnlp-1.5,2020.vardial-1.1,1,0.852983,"Missing"
2021.bsnlp-1.5,2021.ccl-1.108,0,0.0908418,"Missing"
2021.bsnlp-1.5,W19-3704,1,0.806964,"Missing"
2021.bsnlp-1.5,W14-0405,1,0.873329,"Missing"
2021.bsnlp-1.5,2020.emnlp-main.185,0,0.061221,"Missing"
2021.bsnlp-1.5,2020.acl-demos.14,0,0.0285095,"ts of 8,387,681,518 words. 3 4.1 Model training Morphosyntactic tagging On the task of morphosyntactic tagging (assigning each word one among multiple hundreds of detailed morphosyntactic classes, e.g. Ncmsay referring to a common masculine noun, in accusative case, singular number, animate) we compare the three transformer models, mBERT, cseBERT and BERTi´ c. We additionally report results, when available, for the current production tagger for the two languages - the CLASSLA tool (Ljubeˇsi´c and Dobrovoljc, 2019), based on Stanford’s Stanza, exploiting static embedding and BiLSTM technology (Qi et al., 2020). We perform evaluation of the models on this task on four datasets: the Croatian standard language dataset hr500k (Ljubeˇsi´c et al., 2018), the Croatian non-standard language dataset ReLDI-hr (Ljubeˇsi´c et al., 2019a), the Serbian standard language dataset For training this model we selected the Electra approach to training transformer models (Clark et al., 2020). These models are based on training a smaller generator model and the main, larger, discriminator model whose task is to discriminate whether a specific word is the original word from the text, or a word generated by the generator"
2021.bsnlp-1.5,2020.vardial-1.19,1,0.491873,"Missing"
2021.vardial-1.1,2020.vardial-1.26,0,0.0398438,"Missing"
2021.vardial-1.1,W19-1402,0,0.103765,"Missing"
2021.vardial-1.1,2021.vardial-1.15,0,0.0589664,"Missing"
2021.vardial-1.1,P19-1068,1,0.833848,"ntification (RDI): The 2021 Romanian Dialect Identification shared task is at the third iteration, following the 2019 Moldavian vs. Romanian Cross-Dialect Topic identification (MRC) (Zampieri et al., 2019) and the 2020 Romanian Dialect Identification (RDI) (G˘aman et al., 2020) shared tasks. The 2021 RDI shared task is formulated as a cross-domain binary classification by dialect problem, in which a classification model is required to discriminate between the Moldavian (MD) and the Romanian (RO) subdialects. This year, we provided participants with an augmented version of the MOROCO data set (Butnaru and Ionescu, 2019) for training, which contains Moldavian and Romanian samples of text collected from the news domain. Last year’s test set of tweets (G˘aman and Ionescu, 2020b) is used for validation. A new set of tweets has been collected for the 2021 shared task. The task has two formats, open and closed. In the closed format, participants are not allowed to use external data to train their models. In the open format, participants are allowed to use external resources such as unlabeled corpora, lexicons and pre-trained embeddings (e.g. BERT), but the use of additional labeled data is still not allowed. Urali"
2021.vardial-1.1,2021.vardial-1.12,0,0.478027,"nnada) grammar with English lexicon or English grammar with south Dravidian lexicons (Jose et al., 2020; Priyadharshini et al., 2020). The comments were written in the Latin Script with different types of code-mixing. The language tag of the comment were given. The challenge of the task was to identify the language of the given comment. It was 2 http://urn.fi/urn:nbn:fi: lb-2020102201 2 Team DLI RDI SMG ULI HeLju HWR LAST NAYEL NRC Phlyers SUKI UnibucKernel UPB System Description Papers (Scherrer and Ljubeˇsi´c, 2021) (Jauhiainen et al., 2021b) (Bestgen, 2021) (Bernier-Colborne et al., 2021) (Ceolin, 2021) (Jauhiainen et al., 2021a) (G˘aman et al., 2021) (Zaharia et al., 2021) Table 1: The teams that participated in the VarDial Evaluation Campaign 2021. a challenging task, since Tamil, Malayalam and Kannada are closely related languages, some of the words being common in all these languages. The participants had to train a system to identify the language of each comment. Our dataset size is 16,672 comments for training and 4,588 for testing. There were three language tags such as Tamil, Malayalam and Kannada. A new category Not in intended language was added to include comments written in a lan"
2021.vardial-1.1,2020.vardial-1.25,0,0.247563,"opment data for training and (ii) the idea of adapting the language model to the test set. The team that was ranked in the second place is UPB. Their best submission is an ensemble that comprises several deep models, including a Romanian BERT. Different from their last year’s participation (Zaharia et al., 2020), they carefully split the training set into sentences. This idea was borrowed from top-ranked teams of the 2020 RDI shared task. Phlyers ranked on the third place in the 2021 ranking, without significant differences in terms of performance with respect to their previous participation (Ceolin and Zhang, 2020). Despite having access to significantly more in-domain data compared with the previous RDI shared task, the participants were not able to report significant performance gains. Indeed, the top scoring team (C ¸ o¨ ltekin, 2020) in 2020 reached a macro F1 score of 0.7876, while the top scoring team in 2021 achieved a macro F1 score of 0.7772. Although the test sets are not identical, we 6 Social Media Variety Geolocation (SMG) 6.1 Dataset The SMG task is based on three datasets from two Social Media platforms, Jodel and Twitter. Since its first edition in 2020, the datasets have been expanded."
2021.vardial-1.1,2020.sltu-1.25,1,0.720696,"format and evaluation methodology. 4 3 Participating Teams A total of nine teams submitted runs to one or more shared tasks in this year’s VarDial evaluation campaign. In Table 1, we list the teams that participated in the shared tasks, including references to the 8 system description papers which will be published as parts of the VarDial workshop proceedings. Detailed information about the submissions in each respective task is included in the following sections of this report. 4.1 Dravidian Language Identification (DLI) Dataset The DLI task is based on three datasets from YouTube comments (Chakravarthi et al., 2020b,a; Hande et al., 2020). In the 2021 (DLI) shared task, participants have to train a model on comments written in Roman script. Our corpora contains all the three types of code-mixed sentences: InterSentential switch, Intra-Sentential switch and Tag switching. All comments were written in Roman script (Non-native script) with either one of the south Dravidian (Tamil, Malayalam, and Kannada) grammar with English lexicon or English grammar with south Dravidian lexicons (Jose et al., 2020; Priyadharshini et al., 2020). The comments were written in the Latin Script with different types of code-mi"
2021.vardial-1.1,2020.sltu-1.28,1,0.713401,"format and evaluation methodology. 4 3 Participating Teams A total of nine teams submitted runs to one or more shared tasks in this year’s VarDial evaluation campaign. In Table 1, we list the teams that participated in the shared tasks, including references to the 8 system description papers which will be published as parts of the VarDial workshop proceedings. Detailed information about the submissions in each respective task is included in the following sections of this report. 4.1 Dravidian Language Identification (DLI) Dataset The DLI task is based on three datasets from YouTube comments (Chakravarthi et al., 2020b,a; Hande et al., 2020). In the 2021 (DLI) shared task, participants have to train a model on comments written in Roman script. Our corpora contains all the three types of code-mixed sentences: InterSentential switch, Intra-Sentential switch and Tag switching. All comments were written in Roman script (Non-native script) with either one of the south Dravidian (Tamil, Malayalam, and Kannada) grammar with English lexicon or English grammar with south Dravidian lexicons (Jose et al., 2020; Priyadharshini et al., 2020). The comments were written in the Latin Script with different types of code-mi"
2021.vardial-1.1,W19-1409,1,0.900062,"Missing"
2021.vardial-1.1,W18-3929,1,0.901892,"Missing"
2021.vardial-1.1,W14-5316,0,0.049676,"Missing"
2021.vardial-1.1,2021.vardial-1.10,1,0.840728,"Missing"
2021.vardial-1.1,2020.vardial-1.21,1,0.873523,"Missing"
2021.vardial-1.1,2020.vardial-1.1,1,0.844419,"Missing"
2021.vardial-1.1,2021.vardial-1.9,1,0.849095,"Missing"
2021.vardial-1.1,2020.vardial-1.23,1,0.84553,"Missing"
2021.vardial-1.1,W17-0221,1,0.89467,"Missing"
2021.vardial-1.1,2020.peoples-1.6,1,0.765867,"logy. 4 3 Participating Teams A total of nine teams submitted runs to one or more shared tasks in this year’s VarDial evaluation campaign. In Table 1, we list the teams that participated in the shared tasks, including references to the 8 system description papers which will be published as parts of the VarDial workshop proceedings. Detailed information about the submissions in each respective task is included in the following sections of this report. 4.1 Dravidian Language Identification (DLI) Dataset The DLI task is based on three datasets from YouTube comments (Chakravarthi et al., 2020b,a; Hande et al., 2020). In the 2021 (DLI) shared task, participants have to train a model on comments written in Roman script. Our corpora contains all the three types of code-mixed sentences: InterSentential switch, Intra-Sentential switch and Tag switching. All comments were written in Roman script (Non-native script) with either one of the south Dravidian (Tamil, Malayalam, and Kannada) grammar with English lexicon or English grammar with south Dravidian lexicons (Jose et al., 2020; Priyadharshini et al., 2020). The comments were written in the Latin Script with different types of code-mixing. The language tag o"
2021.vardial-1.1,W19-1419,1,0.865245,"Missing"
2021.vardial-1.1,2021.vardial-1.14,1,0.79889,"poro movie la Enna irukunu baki ellam. 4.2 Results 4 Participants and Approaches Due to the short time between the announcement of the shared task and the submission deadline, the participation was lower than we expected. Four teams submitted results to the shared task. Bestgen (2021) proposed a logistic regression model based on n-grams of characters with maximum length as features to classify the comments. The authors achieved a high score with simple techniques. The authors also analyzed the results in detail. For more information, the reader should look at the working notes of the author. Jauhiainen et al. (2021b) submitted results using two models, a Na¨ıve Bayes (NB) classifier with adaptive language models, which was shown to obtain competitive performance in many language and dialect identification tasks, and a transformerbased model, which is widely regarded as the stateof-the-art in a number of NLP tasks. Their first Table 2: The results of all entries by the four team participating in the DLI shared task in terms of Macro-F1. Given the difficulty of the DLI 2021 task, the level of performance achieved by the systems is appreciable. Identifying the Other-language category was particularly diffi"
2021.vardial-1.1,2021.vardial-1.13,0,0.185158,"h Dravidian lexicons (Jose et al., 2020; Priyadharshini et al., 2020). The comments were written in the Latin Script with different types of code-mixing. The language tag of the comment were given. The challenge of the task was to identify the language of the given comment. It was 2 http://urn.fi/urn:nbn:fi: lb-2020102201 2 Team DLI RDI SMG ULI HeLju HWR LAST NAYEL NRC Phlyers SUKI UnibucKernel UPB System Description Papers (Scherrer and Ljubeˇsi´c, 2021) (Jauhiainen et al., 2021b) (Bestgen, 2021) (Bernier-Colborne et al., 2021) (Ceolin, 2021) (Jauhiainen et al., 2021a) (G˘aman et al., 2021) (Zaharia et al., 2021) Table 1: The teams that participated in the VarDial Evaluation Campaign 2021. a challenging task, since Tamil, Malayalam and Kannada are closely related languages, some of the words being common in all these languages. The participants had to train a system to identify the language of each comment. Our dataset size is 16,672 comments for training and 4,588 for testing. There were three language tags such as Tamil, Malayalam and Kannada. A new category Not in intended language was added to include comments written in a language other than the Dravidian languages. A sample comment from our data"
2021.vardial-1.1,W17-1201,1,0.533591,"Missing"
2021.vardial-1.1,C16-1322,1,0.889269,"Missing"
2021.vardial-1.1,W18-3901,1,0.749269,"Missing"
2021.vardial-1.1,2021.vardial-1.16,1,0.843333,"Missing"
2021.vardial-1.1,W14-5307,1,0.728758,"Missing"
2021.vardial-1.16,2020.vardial-1.24,0,0.0617826,"Missing"
2021.vardial-1.16,N19-1423,0,0.0391256,"ects, pages 135–140 April 20, 2021 ©2021 Association for Computational Linguistics Instances Training Development Test BCMS CH DE-AT Task 353 953 38 013 4 189 25 261 2 416 2 438 318 487 29 122 31 515 Table 1: Data characteristics. the unconstrained one. We will inspect last year’s submissions in more detail in Section 4.2. 3 4 Experiments Due to lack of time (the two evaluation campaigns were held just a few months apart due to the *ACL conference bidding procedure), all our experiments are based on our successful 2020 submissions (Scherrer and Ljubeˇsi´c, 2020): we use the BERT architecture (Devlin et al., 2019) with a fully connected layer on top of the CLS token. This fully connected layer implements double regression with a two-dimensional output vector and Mean Absolute Error loss. We provide both constrained submissions, where Median distance Mean distance Eps. BCMS 3k 30k 92.74 59.93 129.14 109.51 28 23 CH 3k 30k 22.94 21.20 33.01 30.60 11 9 DE-AT 3k 30k 182.76 160.67 205.90 186.35 4 2 Table 2: Effect of different vocabulary sizes on constrained model performance, evaluated on the development set. Eps. refers to the number of fine-tuning epochs to reach minimum median distance. Data The VarDial"
2021.vardial-1.16,2020.vardial-1.1,1,0.922113,"Missing"
2021.vardial-1.16,C12-1064,0,0.0295633,"itter in the case of BCMS (Ljubeˇsi´c et al., 2016) and Jodel in the case of DEAT and CH (Hovy and Purschke, 2018). This paper describes the HeLju (Helsinki– Ljubljana) submission to the SMG task. Following our successful participation in 2020 (Scherrer and Ljubeˇsi´c, 2020), we again propose systems based on the BERT architecture in both constrained and unconstrained settings. We report experiments with different tokenization parameters and with newly available pre-trained models. Furthermore, we Related work One of the first works focusing on predicting geolocation from social media text is Han et al. (2012). The authors investigate feature (token) selection methods for location prediction, showing that traditional predictive algorithms yield significantly better results if feature selection is performed. There has been already a shared task on geolocation prediction at WNUT 2016 (Han et al., 2016). The task focused not only on predicting geolocation from text, but also from various user metadata. The best performing systems combined the available information via feedforward networks or ensembles. Thomas and Hennig (2018) report significant improvements over the winner of the WNUT-16 shared task"
2021.vardial-1.16,W16-3928,0,0.0188952,"stems based on the BERT architecture in both constrained and unconstrained settings. We report experiments with different tokenization parameters and with newly available pre-trained models. Furthermore, we Related work One of the first works focusing on predicting geolocation from social media text is Han et al. (2012). The authors investigate feature (token) selection methods for location prediction, showing that traditional predictive algorithms yield significantly better results if feature selection is performed. There has been already a shared task on geolocation prediction at WNUT 2016 (Han et al., 2016). The task focused not only on predicting geolocation from text, but also from various user metadata. The best performing systems combined the available information via feedforward networks or ensembles. Thomas and Hennig (2018) report significant improvements over the winner of the WNUT-16 shared task by separately learning text and metadata embeddings via different neural network architectures (LSTM, feedforward), merging those embeddings and performing the final classification via a softmax layer. During the last iteration of the VarDial social media geolocation shared task (Gaman et al., 2"
2021.vardial-1.16,D18-1469,0,0.0179281,"-longitude coordinate pairs. This contrasts with most other VarDial tasks, in which the goal is to choose from a finite set of variety labels. The second edition of the SMG task is run at VarDial 2021 (Chakravarthi et al., 2021), with the same three language areas as in the previous year: the BosnianCroatian-Montenegrin-Serbian (BCMS) language area, the German language area comprised of Germany and Austria (DE-AT), and German-speaking Switzerland (CH). All three datasets are based on social media data, Twitter in the case of BCMS (Ljubeˇsi´c et al., 2016) and Jodel in the case of DEAT and CH (Hovy and Purschke, 2018). This paper describes the HeLju (Helsinki– Ljubljana) submission to the SMG task. Following our successful participation in 2020 (Scherrer and Ljubeˇsi´c, 2020), we again propose systems based on the BERT architecture in both constrained and unconstrained settings. We report experiments with different tokenization parameters and with newly available pre-trained models. Furthermore, we Related work One of the first works focusing on predicting geolocation from social media text is Han et al. (2012). The authors investigate feature (token) selection methods for location prediction, showing that"
2021.vardial-1.16,2020.vardial-1.21,0,0.247774,"Missing"
2021.vardial-1.16,2020.lrec-1.329,0,0.0250875,"Missing"
2021.vardial-1.16,2021.bsnlp-1.5,1,0.835998,"Missing"
2021.vardial-1.16,C16-1322,1,0.288624,"Missing"
2021.vardial-1.16,2020.vardial-1.19,1,0.846959,"Missing"
2021.wassa-1.16,W19-3501,0,0.0194155,"ied. We propose the hypothesis that stylometric characteristics of hateful writing are distinctive enough to contribute to the hate speech detection task. In other words, hate speech acts as a specific text type with an associated writing style. On the other hand, we are motivated by psychological and sociological studies, which correlate toxic behaviour online with the emotional profile of the user (Kokkinos and Kipritsi, 2012). However, unlike previous research that used sentiment information for detecting unacceptable content (Davidson et al., 2017; Dani et al., 2017; Van Hee et al., 2018; Brassard-Gourdeau and Khoury, 2019), we test whether we are able to capture some of these phenomena by going beyond the sentiment level (positive / negative / neutral) to a more fine-grained emotion level. We compare the performance of stylometric and emotion-based features with commonly used features for hate speech detection: words, character n-grams, and their combination, and with more recent deep learning models that currently provide the state-of-the-art results for the hate speech detection task (Mandl et al., 2019; Basile et al., 2019): convolutional neural networks (CNN), long shortterm memory networks (LSTM), and bidi"
2021.wassa-1.16,N19-1423,0,0.0584378,"Missing"
2021.wassa-1.16,P82-1020,0,0.702682,"Missing"
2021.wassa-1.16,W14-0908,0,0.349442,"that the style and emotional dimension of hateful textual content may provide useful cues for its detection. We investigate this through a binary hate speech classification task using features that model such information, i.e., function words and emotion-based features. The latter are operationalized in terms of the types of emotions expressed and the frequency of emotionconveying words in the data. Function word usage is one of the most important and revealing aspects of style in written language, as shown by numerous studies in stylometric analysis for authorship attribution (Grieve, 2007; Kestemont, 2014; Markov et al., 2018). While stylometric characteristics have been implicitly included in some hate speech detection studies (e.g., in bag-of-words or character-level models), 149 Proceedings of the 11th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis, pages 149–159 April 19, 2021. ©2021 Association for Computational Linguistics their impact on the task has not been studied. We propose the hypothesis that stylometric characteristics of hateful writing are distinctive enough to contribute to the hate speech detection task. In other words, hate speech a"
2021.wassa-1.16,D14-1181,0,0.00897115,"Missing"
2021.wassa-1.16,2020.peoples-1.15,1,0.377744,"Missing"
2021.wassa-1.16,K18-2016,0,0.0269953,"Missing"
2021.wassa-1.16,P19-1163,0,0.0452067,"onality, religion, or other characteristics (Nockleby, 2000). The exact definition of hate speech, however, remains a disputed topic, as it is a subjective and multi-interpretable concept (Waseem et al., 2017; Poletto et al., 2020). The lack of a consensus on its definition poses a challenge to hate speech annotation. Annotating hateful content remains prone to personal bias and is culture-dependent, which often results in low inter-annotator agreement and therefore scarcity of high quality training data for developing supervised hate speech detection systems (Ross et al., 2016; Waseem, 2016; Sap et al., 2019). Hate speech online presents additional challenges for natural language processing (NLP): offensive vocabulary and keywords evolve fast due to their relatedness with the hate speech triggering events (Florio et al., 2020), moreover, users may adapt their lexical choices as a countermeasure against identification or introduce minor misspellings to bypass filtering systems (Berger and Perez, 2006; Vidgen et al., 2019). Therefore, we intend to investigate more abstract features, less susceptible to specific vocabulary, topic or corpus bias, which we examine in in-domain and crossdomain settings:"
2021.wassa-1.16,N03-1033,0,0.184881,"llness on parade. Table 3 shows an example of the representation of this message through the features described above. From the POS & FW & emotion word representations, n-grams (n = 1– 3) are built.4 The count of emotionally-charged words and the emotion associations were added as additional feature vectors. Part-of-speech (POS) POS features capture the morpho-syntactic patterns in a text, and are indicative of hate speech, especially when used in combination with other types of features (Warner and Hirschberg, 2012; Robinson et al., 2018). POS tags were obtained with the Stanford POS Tagger (Toutanova et al., 2003). We used the same 17 universal POS tags for the three languages and built n-grams from this representation with n = 1–3. Stylometric features Function words (FW) are considered one of the most important stylometric feature types (Kestemont, 2014). They clarify the relationships between the content-carrying elements of a sentence, and introduce syntactic structures like verbal complements, relative clauses, and questions (Smith and Witten, 1993). With respect to emotion features, FW can appear as quantifiers, intensifiers (e.g., very good) or modify the emotion phrase Mental illness on parade"
2021.wassa-1.16,R15-1086,1,0.828623,"Missing"
2021.wassa-1.16,W19-3509,0,0.0395717,"esults in low inter-annotator agreement and therefore scarcity of high quality training data for developing supervised hate speech detection systems (Ross et al., 2016; Waseem, 2016; Sap et al., 2019). Hate speech online presents additional challenges for natural language processing (NLP): offensive vocabulary and keywords evolve fast due to their relatedness with the hate speech triggering events (Florio et al., 2020), moreover, users may adapt their lexical choices as a countermeasure against identification or introduce minor misspellings to bypass filtering systems (Berger and Perez, 2006; Vidgen et al., 2019). Therefore, we intend to investigate more abstract features, less susceptible to specific vocabulary, topic or corpus bias, which we examine in in-domain and crossdomain settings: training and testing on social media datasets belonging to same/different domains, for three languages: English, Slovene, and Dutch. Our hypothesis is that the style and emotional dimension of hateful textual content may provide useful cues for its detection. We investigate this through a binary hate speech classification task using features that model such information, i.e., function words and emotion-based feature"
2021.wassa-1.16,W12-2103,0,0.0286242,"sage is. Consider the following English comment from our data belonging to the hate speech class: Mental illness on parade. Table 3 shows an example of the representation of this message through the features described above. From the POS & FW & emotion word representations, n-grams (n = 1– 3) are built.4 The count of emotionally-charged words and the emotion associations were added as additional feature vectors. Part-of-speech (POS) POS features capture the morpho-syntactic patterns in a text, and are indicative of hate speech, especially when used in combination with other types of features (Warner and Hirschberg, 2012; Robinson et al., 2018). POS tags were obtained with the Stanford POS Tagger (Toutanova et al., 2003). We used the same 17 universal POS tags for the three languages and built n-grams from this representation with n = 1–3. Stylometric features Function words (FW) are considered one of the most important stylometric feature types (Kestemont, 2014). They clarify the relationships between the content-carrying elements of a sentence, and introduce syntactic structures like verbal complements, relative clauses, and questions (Smith and Witten, 1993). With respect to emotion features, FW can appear"
2021.wassa-1.16,W16-5618,0,0.00931299,"entation, nationality, religion, or other characteristics (Nockleby, 2000). The exact definition of hate speech, however, remains a disputed topic, as it is a subjective and multi-interpretable concept (Waseem et al., 2017; Poletto et al., 2020). The lack of a consensus on its definition poses a challenge to hate speech annotation. Annotating hateful content remains prone to personal bias and is culture-dependent, which often results in low inter-annotator agreement and therefore scarcity of high quality training data for developing supervised hate speech detection systems (Ross et al., 2016; Waseem, 2016; Sap et al., 2019). Hate speech online presents additional challenges for natural language processing (NLP): offensive vocabulary and keywords evolve fast due to their relatedness with the hate speech triggering events (Florio et al., 2020), moreover, users may adapt their lexical choices as a countermeasure against identification or introduce minor misspellings to bypass filtering systems (Berger and Perez, 2006; Vidgen et al., 2019). Therefore, we intend to investigate more abstract features, less susceptible to specific vocabulary, topic or corpus bias, which we examine in in-domain and cr"
2021.wassa-1.16,W17-3012,0,0.0138072,"orms words and character n-gram features under cross-domain conditions, and provides a significant boost to deep learning models, which currently obtain the best results, when combined with them in an ensemble. 1 Introduction Hate speech is commonly defined as communication that disparages a person or a group on the basis of some characteristic such as race, color, ethnicity, gender, sexual orientation, nationality, religion, or other characteristics (Nockleby, 2000). The exact definition of hate speech, however, remains a disputed topic, as it is a subjective and multi-interpretable concept (Waseem et al., 2017; Poletto et al., 2020). The lack of a consensus on its definition poses a challenge to hate speech annotation. Annotating hateful content remains prone to personal bias and is culture-dependent, which often results in low inter-annotator agreement and therefore scarcity of high quality training data for developing supervised hate speech detection systems (Ross et al., 2016; Waseem, 2016; Sap et al., 2019). Hate speech online presents additional challenges for natural language processing (NLP): offensive vocabulary and keywords evolve fast due to their relatedness with the hate speech triggeri"
2021.wnut-1.52,2020.coling-main.583,0,0.153727,"Missing"
2021.wnut-1.52,P17-4012,0,0.0504319,"Missing"
2021.wnut-1.52,P16-1009,0,0.0406311,"have a positive effect on all languages but Danish and Indonesian–English.6 In general, the accuracies of the BERT constraints lie about halfway between the unconstrained and the oracle ones. 6 Including synthetic training data from back-translation The results of the language-independent models of Section 4 suggest that the provided training data is of insufficient size to train reliable translation models, especially neural ones. A well-known strategy to augment the training data in MT is backtranslation, where target language data is translated to the source language by an auxiliary model (Sennrich et al., 2016). The resulting parallel data (a standard target side, and a noisy source side) is then included in the training data of the main model. In the normalization setting, this amounts to finding “clean” data and running it through a model that produces a noisy version of it. To this end, we used filtered subsets of the monolingual OpenSubtitles corpora from OPUS7 (Tiedemann, 2012) as input data for producing back-translations. We filtered the OPUS data using the OpusFilter package (Aulamo et al., 2020) and the following filters: • The length of the line lies between 5 and 25 words (this correspond"
2021.wnut-1.52,C18-1112,0,0.0260908,"der Goot et al., human supervision is low (Zupan et al., 2019). 2021). The main motivation behind lexical norWhile neural approaches have almost entirely malization is to minimize the variability of the lin- replaced statistical ones in “standard” translation guistic signal, either for computational usage or settings (translating between distinct languages), human consumption. Accordingly, the shared task recent studies have shown that SMT-based apsubmissions are evaluated both intrinsically and proaches remain competitive for normalization extrinsically (on a dependency parsing task). tasks (Tang et al., 2018; Bollmann, 2019). The need for lexical normalization for computaNormalization systems not based on translation tional usage is diminishing these days, given the architectures have also been proposed. For examend-to-end methodology that is becoming more ple, MoNoise (van der Goot, 2019) generates a list and more popular, where the systems are robust of normalization candidates for each token and then 465 Proceedings of the 2021 EMNLP Workshop W-NUT: The Seventh Workshop on Noisy User-generated Text, pages 465–472 November 11, 2021. ©2021 Association for Computational Linguistics Code Language"
2021.wnut-1.52,2020.lrec-1.769,0,0.15916,"Missing"
agic-etal-2010-towards,W07-1702,1,\N,Missing
agic-etal-2010-towards,P07-1124,0,\N,Missing
agic-etal-2010-towards,P02-1053,0,\N,Missing
agic-etal-2010-towards,devitt-ahmad-2008-sentiment,0,\N,Missing
agic-etal-2010-towards,ahmad-etal-2006-sentiments,0,\N,Missing
agic-etal-2010-towards,esuli-sebastiani-2006-sentiwordnet,0,\N,Missing
agic-ljubesic-2014-setimes,C10-1011,0,\N,Missing
agic-ljubesic-2014-setimes,E03-1009,0,\N,Missing
agic-ljubesic-2014-setimes,W06-2920,0,\N,Missing
agic-ljubesic-2014-setimes,P05-1045,0,\N,Missing
agic-ljubesic-2014-setimes,H05-1066,0,\N,Missing
agic-ljubesic-2014-setimes,W03-0419,0,\N,Missing
agic-ljubesic-2014-setimes,W13-4903,1,\N,Missing
agic-ljubesic-2014-setimes,W13-2408,1,\N,Missing
agic-ljubesic-2014-setimes,berovic-etal-2012-croatian,1,\N,Missing
agic-ljubesic-2014-setimes,D07-1096,0,\N,Missing
agic-ljubesic-2014-setimes,erjavec-etal-2010-jos,0,\N,Missing
C12-1160,I11-1062,0,0.0622133,"Missing"
C12-1160,P12-3005,0,0.107931,"Missing"
C12-1160,C00-2137,0,0.206395,"Missing"
C16-1322,E14-1011,0,0.0270469,"methods such as kernel density estimation (for smoothing data points representation on maps), factor analysis (for reducing the dimensionality) and clustering (for grouping similar areas). This tool requires a previously prepared data set, which can be collected using traditional methods. User-generated content available on the Internet is used in computational linguistics mostly to study demographic characteristics of speakers based on the linguistic variety they use (Eisenstein et al., 2011; Nguyen et al., 2011; Danescu-Niculescu-Mizil et al., 2013), often including a geographic component (Doyle, 2014; Hovy and Johannsen, 2016). We concentrate here on the work where associated software was made available. Doyle (2014), proposes a method based on conditional probability to estimate the geographical distribution of linguistic features using Twitter. This method can be used to overcome the problem of uneven geographic distribution of collection points.3 The software associated with this work is SeeTweet4 , a Python tool that uses the Twitter search API to collect tweets containing terms of interest, as well as base terms used for estimating the prior spatial frequency of tweets. The tool does"
C16-1322,P11-1137,0,0.0188951,"oLing2 , a tool, written in Java, that enables researchers to visualise the spatial distribution of linguistic features using methods such as kernel density estimation (for smoothing data points representation on maps), factor analysis (for reducing the dimensionality) and clustering (for grouping similar areas). This tool requires a previously prepared data set, which can be collected using traditional methods. User-generated content available on the Internet is used in computational linguistics mostly to study demographic characteristics of speakers based on the linguistic variety they use (Eisenstein et al., 2011; Nguyen et al., 2011; Danescu-Niculescu-Mizil et al., 2013), often including a geographic component (Doyle, 2014; Hovy and Johannsen, 2016). We concentrate here on the work where associated software was made available. Doyle (2014), proposes a method based on conditional probability to estimate the geographical distribution of linguistic features using Twitter. This method can be used to overcome the problem of uneven geographic distribution of collection points.3 The software associated with this work is SeeTweet4 , a Python tool that uses the Twitter search API to collect tweets containing"
C16-1322,L16-1477,0,0.0248072,"as kernel density estimation (for smoothing data points representation on maps), factor analysis (for reducing the dimensionality) and clustering (for grouping similar areas). This tool requires a previously prepared data set, which can be collected using traditional methods. User-generated content available on the Internet is used in computational linguistics mostly to study demographic characteristics of speakers based on the linguistic variety they use (Eisenstein et al., 2011; Nguyen et al., 2011; Danescu-Niculescu-Mizil et al., 2013), often including a geographic component (Doyle, 2014; Hovy and Johannsen, 2016). We concentrate here on the work where associated software was made available. Doyle (2014), proposes a method based on conditional probability to estimate the geographical distribution of linguistic features using Twitter. This method can be used to overcome the problem of uneven geographic distribution of collection points.3 The software associated with this work is SeeTweet4 , a Python tool that uses the Twitter search API to collect tweets containing terms of interest, as well as base terms used for estimating the prior spatial frequency of tweets. The tool does not perform any visualisat"
C16-1322,ljubesic-etal-2014-tweetcat,1,0.892704,"Missing"
C16-1322,L16-1676,1,0.841134,"Missing"
C16-1322,P12-3005,0,0.043797,"Missing"
C16-1322,W11-1515,0,0.0207695,"Missing"
espla-gomis-etal-2014-comparing,C10-1054,0,\N,Missing
espla-gomis-etal-2014-comparing,J03-3002,0,\N,Missing
espla-gomis-etal-2014-comparing,P02-1033,0,\N,Missing
espla-gomis-etal-2014-comparing,P07-2045,0,\N,Missing
espla-gomis-etal-2014-comparing,P11-1134,0,\N,Missing
espla-gomis-etal-2014-comparing,P09-1098,0,\N,Missing
espla-gomis-etal-2014-comparing,C12-1013,0,\N,Missing
espla-gomis-etal-2014-comparing,P12-3005,0,\N,Missing
espla-gomis-etal-2014-comparing,2005.mtsummit-papers.11,0,\N,Missing
espla-gomis-etal-2014-comparing,san-vicente-manterola-2012-paco2,0,\N,Missing
espla-gomis-etal-2014-comparing,W11-1218,0,\N,Missing
espla-gomis-etal-2014-comparing,W13-2506,1,\N,Missing
espla-gomis-etal-2014-comparing,W13-2201,0,\N,Missing
espla-gomis-etal-2014-comparing,1999.mtsummit-1.79,0,\N,Missing
espla-gomis-etal-2014-comparing,P06-1062,0,\N,Missing
espla-gomis-etal-2014-comparing,I05-4010,0,\N,Missing
fiser-etal-2012-addressing,N09-5005,0,\N,Missing
fiser-etal-2012-addressing,E09-1005,0,\N,Missing
fiser-etal-2012-addressing,N03-1015,0,\N,Missing
fiser-etal-2012-addressing,P99-1067,0,\N,Missing
fiser-etal-2012-addressing,P06-1014,0,\N,Missing
fiser-etal-2012-addressing,W11-1204,1,\N,Missing
L16-1242,agic-ljubesic-2014-setimes,1,0.905996,"Missing"
L16-1242,W13-2408,1,0.919568,"Missing"
L16-1242,C12-1015,0,0.072104,"Missing"
L16-1242,P07-2053,0,0.366009,"Missing"
L16-1242,kobylinski-2014-polita,0,0.0458155,"Missing"
L16-1242,R15-1050,1,0.882971,"Missing"
L16-1242,W96-0213,0,0.536528,"However, we extend their approach by using a sequential tagger and inspecting many additional features. 3. The Dataset 4. Tagger Features and Evaluation In this section we describe the feature selection process for our tagger. For extracting the features we use our own Python code while we train our models using CRFsuite (Okazaki, 2007). We close this section with an evaluation of the final tagger. During the feature selection process we discriminate between two sets of features. The first, the core feature set, consists of features that were traditionally proven to work well for PoS tagging (Ratnaparkhi, 1996; Toutanova et al., 2003). On the second, the experimental feature set, we ran a large number of experiments in the quest for a (near-to) optimal feature set for the given language (family). 4.1. The Core Feature Set The core feature set consists of the following features: For Slovene a number or resources are available as open datasets in the CLARIN.SI1 repository. For our experiments we used ssj500k 1.3 (Krek et al., 2013), a 500k word corpus manually annotated with context-disambiguated MSDs (and lemmas) and the Sloleks morphological lexicon 1.2 (Dobrovoljc et al., 2015) which contains abou"
L16-1242,E09-1087,0,0.0612501,"Missing"
L16-1242,P14-5003,0,0.0821697,"Missing"
L16-1242,N03-1033,0,0.00954978,"their approach by using a sequential tagger and inspecting many additional features. 3. The Dataset 4. Tagger Features and Evaluation In this section we describe the feature selection process for our tagger. For extracting the features we use our own Python code while we train our models using CRFsuite (Okazaki, 2007). We close this section with an evaluation of the final tagger. During the feature selection process we discriminate between two sets of features. The first, the core feature set, consists of features that were traditionally proven to work well for PoS tagging (Ratnaparkhi, 1996; Toutanova et al., 2003). On the second, the experimental feature set, we ran a large number of experiments in the quest for a (near-to) optimal feature set for the given language (family). 4.1. The Core Feature Set The core feature set consists of the following features: For Slovene a number or resources are available as open datasets in the CLARIN.SI1 repository. For our experiments we used ssj500k 1.3 (Krek et al., 2013), a 500k word corpus manually annotated with context-disambiguated MSDs (and lemmas) and the Sloleks morphological lexicon 1.2 (Dobrovoljc et al., 2015) which contains about 100,000 lemmas with the"
L16-1242,C12-1170,0,0.0511636,"Missing"
L16-1471,W11-1218,0,0.0442929,"Missing"
L16-1471,C12-1013,0,0.0137106,"rbian, and English– Finnish language pairs, while Section 5 describes the evaluation carried out for these new resources and the results obtained. The paper ends with some concluding remarks in Section 6. 2 Related work One of the most usual strategies to crawl parallel data from the Internet is to focus on web sites that make it straightforward to detect parallel documents (Nie et al., 1999; Koehn, 2005; Tiedemann, 2012). Many approaches use content-based metrics (Jiang et al., 2009; Utiyama et al., 2009; Yan et al., 2009; Hong et al., 2010; Sridhar et al., 2011; Antonova and Misyurev, 2011; Barbosa et al., 2012), such as bag-of-words overlapping. Although these metrics have proved to be useful for parallel data detection, their main limitation is that they require some linguistic resources (such as a bilingual lexicon or a basic machine translation system) which may not be available for some language pairs. To avoid this problem, other works use the HTML structure of the web pages, which usually remains stable between different translations of the same document (Ma and Liberman, 1999; Nie et al., 1999; Resnik and Smith, 2003; Zhang et al., 2006; D´esilets et al., 2008; Espl`a-Gomis and Forcada, 2010;"
L16-1471,2008.tc-1.1,0,0.133345,"Missing"
L16-1471,P14-1129,0,0.0151313,"t, the first 1, 000 segments were manually translated into Croatian by a native speaker. Models. The SMT systems are phrase-based and built with the Moses toolkit (Koehn et al., 2007) using default parameters, except for the following. On top of the default word-based reordering model, our system implements two additional ones, phrase-based and hierarchical (Galley and Manning, 2008). On top of this, two additional resources were obtained from the hrenWaC parallel corpus and used in the MT systems built: an operation sequence model (Durrani et al., 2011) and a bilingual neural language model (Devlin et al., 2014). Results. We evaluate the systems trained in both directions and on both data collections (only hrenWaC and all the training corpora) with two widely used automatic evaluation metrics: BLEU (Papineni et al., 2002) and TER (Snover et al., 2006). The results are shown in Table 4. As it can be seen, in both translation directions the performance of the new SMT systems built on the hrenWaC corpus obtain results that are comparable to those obtained by the third-party systems. In the harder translation direction, English→Croatian, the newly built SMT systems outperform two of the reference systems"
L16-1471,P11-1105,0,0.0131717,"se languages. Therefore, in order to build a suitable test set, the first 1, 000 segments were manually translated into Croatian by a native speaker. Models. The SMT systems are phrase-based and built with the Moses toolkit (Koehn et al., 2007) using default parameters, except for the following. On top of the default word-based reordering model, our system implements two additional ones, phrase-based and hierarchical (Galley and Manning, 2008). On top of this, two additional resources were obtained from the hrenWaC parallel corpus and used in the MT systems built: an operation sequence model (Durrani et al., 2011) and a bilingual neural language model (Devlin et al., 2014). Results. We evaluate the systems trained in both directions and on both data collections (only hrenWaC and all the training corpora) with two widely used automatic evaluation metrics: BLEU (Papineni et al., 2002) and TER (Snover et al., 2006). The results are shown in Table 4. As it can be seen, in both translation directions the performance of the new SMT systems built on the hrenWaC corpus obtain results that are comparable to those obtained by the third-party systems. In the harder translation direction, English→Croatian, the new"
L16-1471,espla-gomis-etal-2014-comparing,1,0.894395,"Missing"
L16-1471,D08-1089,0,0.0253675,"f WMT13.22 This corpus consists of a collection of news stories in English which are freely available, translated into other languages. Unfortunately, Croatian was not one of these languages. Therefore, in order to build a suitable test set, the first 1, 000 segments were manually translated into Croatian by a native speaker. Models. The SMT systems are phrase-based and built with the Moses toolkit (Koehn et al., 2007) using default parameters, except for the following. On top of the default word-based reordering model, our system implements two additional ones, phrase-based and hierarchical (Galley and Manning, 2008). On top of this, two additional resources were obtained from the hrenWaC parallel corpus and used in the MT systems built: an operation sequence model (Durrani et al., 2011) and a bilingual neural language model (Devlin et al., 2014). Results. We evaluate the systems trained in both directions and on both data collections (only hrenWaC and all the training corpora) with two widely used automatic evaluation metrics: BLEU (Papineni et al., 2002) and TER (Snover et al., 2006). The results are shown in Table 4. As it can be seen, in both translation directions the performance of the new SMT syste"
L16-1471,C10-1054,0,0.0197424,"new corpora crated for English– Croatian, English–Slovene, English–Serbian, and English– Finnish language pairs, while Section 5 describes the evaluation carried out for these new resources and the results obtained. The paper ends with some concluding remarks in Section 6. 2 Related work One of the most usual strategies to crawl parallel data from the Internet is to focus on web sites that make it straightforward to detect parallel documents (Nie et al., 1999; Koehn, 2005; Tiedemann, 2012). Many approaches use content-based metrics (Jiang et al., 2009; Utiyama et al., 2009; Yan et al., 2009; Hong et al., 2010; Sridhar et al., 2011; Antonova and Misyurev, 2011; Barbosa et al., 2012), such as bag-of-words overlapping. Although these metrics have proved to be useful for parallel data detection, their main limitation is that they require some linguistic resources (such as a bilingual lexicon or a basic machine translation system) which may not be available for some language pairs. To avoid this problem, other works use the HTML structure of the web pages, which usually remains stable between different translations of the same document (Ma and Liberman, 1999; Nie et al., 1999; Resnik and Smith, 2003; Z"
L16-1471,P09-1098,0,0.0210947,"tion 3 describes the tool Spidextor. Section 4 describes the new corpora crated for English– Croatian, English–Slovene, English–Serbian, and English– Finnish language pairs, while Section 5 describes the evaluation carried out for these new resources and the results obtained. The paper ends with some concluding remarks in Section 6. 2 Related work One of the most usual strategies to crawl parallel data from the Internet is to focus on web sites that make it straightforward to detect parallel documents (Nie et al., 1999; Koehn, 2005; Tiedemann, 2012). Many approaches use content-based metrics (Jiang et al., 2009; Utiyama et al., 2009; Yan et al., 2009; Hong et al., 2010; Sridhar et al., 2011; Antonova and Misyurev, 2011; Barbosa et al., 2012), such as bag-of-words overlapping. Although these metrics have proved to be useful for parallel data detection, their main limitation is that they require some linguistic resources (such as a bilingual lexicon or a basic machine translation system) which may not be available for some language pairs. To avoid this problem, other works use the HTML structure of the web pages, which usually remains stable between different translations of the same document (Ma and"
L16-1471,P07-2045,0,0.00393889,"that led to the best results in the development phase. Test set. The test set used for evaluating the SMT systems described in this work was based on the test set used in the evaluation campaign of WMT13.22 This corpus consists of a collection of news stories in English which are freely available, translated into other languages. Unfortunately, Croatian was not one of these languages. Therefore, in order to build a suitable test set, the first 1, 000 segments were manually translated into Croatian by a native speaker. Models. The SMT systems are phrase-based and built with the Moses toolkit (Koehn et al., 2007) using default parameters, except for the following. On top of the default word-based reordering model, our system implements two additional ones, phrase-based and hierarchical (Galley and Manning, 2008). On top of this, two additional resources were obtained from the hrenWaC parallel corpus and used in the MT systems built: an operation sequence model (Durrani et al., 2011) and a bilingual neural language model (Devlin et al., 2014). Results. We evaluate the systems trained in both directions and on both data collections (only hrenWaC and all the training corpora) with two widely used automat"
L16-1471,2005.mtsummit-papers.11,0,0.384697,"sist of collections of texts in different languages which are mutual translations. This resource is specially relevant in the field of statistical machine translation (SMT), where parallel corpora are used to learn translation models automatically. The growing interest in SMT in the last decades has increased the demand of parallel corpora and, as a consequence, new strategies have been proposed to collect such data. Many sources of bitexts have been identified; some examples are: • texts from multilingual institutions, such as the Hansards corpus (Roukos et al., 1995) or the Europarl corpus (Koehn, 2005); • translations of software interfaces and documentation, such as KDE4 and OpenOffice (Tiedemann, 2009); or • news translated into different languages, such as the SETimes corpus (Ljubeˇsi´c, 2009), or the News Commentaries corpus (Bojar et al., 2013). However, one of the most obvious sources for collecting parallel data is the Internet. On the one hand, most of the sources already mentioned are currently available on the Web. In addition to this, it is worth noting that many websites are available in several languages and this translated content is another useful source of parallel data. The"
L16-1471,1999.mtsummit-1.79,0,0.464757,"., 2009; Utiyama et al., 2009; Yan et al., 2009; Hong et al., 2010; Sridhar et al., 2011; Antonova and Misyurev, 2011; Barbosa et al., 2012), such as bag-of-words overlapping. Although these metrics have proved to be useful for parallel data detection, their main limitation is that they require some linguistic resources (such as a bilingual lexicon or a basic machine translation system) which may not be available for some language pairs. To avoid this problem, other works use the HTML structure of the web pages, which usually remains stable between different translations of the same document (Ma and Liberman, 1999; Nie et al., 1999; Resnik and Smith, 2003; Zhang et al., 2006; D´esilets et al., 2008; Espl`a-Gomis and Forcada, 2010; San Vicente and Manterola, 2012; Papavassiliou et al., 2013). Another useful strategy is to identify language markers in the URLs (Ma and Liberman, 1999; Nie et al., 1999; Resnik and Smith, 2003; Zhang et al., 2006; D´esilets et al., 2008; Espl`a-Gomis and Forcada, 2010; San Vicente and Manterola, 2012) that help detecting possible parallel documents. Some authors have used similar approaches to crawl comparable corpora. For instance, Smith et al. (2010) use the links between"
L16-1471,W03-2806,0,0.0298721,"Missing"
L16-1471,J05-4003,0,0.0638318,"rcada, 2010; San Vicente and Manterola, 2012; Papavassiliou et al., 2013). Another useful strategy is to identify language markers in the URLs (Ma and Liberman, 1999; Nie et al., 1999; Resnik and Smith, 2003; Zhang et al., 2006; D´esilets et al., 2008; Espl`a-Gomis and Forcada, 2010; San Vicente and Manterola, 2012) that help detecting possible parallel documents. Some authors have used similar approaches to crawl comparable corpora. For instance, Smith et al. (2010) use the links between translated articles in Wikipedia to crawl parallel sentences or words. A more complex strategy is used by Munteanu and Marcu (2005), who compare news published in versions of news websites written in different languages by using a publication time stamp window. In this way, it is possible to retrieve likely on-topic news on which a cross-lingual information retrieval strategy is applied based on word-to-word machine translation. Even though these methods have proven to be useful for specific web sites, the real challenge is to find strategies that allow to extend them to crawl the Web in an unsupervised fashion, therefore allowing to exploit the real potential of this resource. Resnik (1998) uses language anchors, i.e. ph"
L16-1471,W13-2506,0,0.490072,"allel data. Therefore, a considerable scientific effort has been put during the last years in order to exploit the web as a source to automatically acquire new parallel data (see Section 2). Some examples of corpora built from multilingual web pages are the Tourism English–Croatian Parallel Corpus 2.01 (Toral et al., 2014) or the Panacea project’s parallel corpora for English–French and English–Greek in two different domains: environment2 and labour legislation3 (Pecina et al., 2014). There are several tools that can be used for automatically crawling parallel data from multilingual websites (Papavassiliou et al., 2013; Espl`a-Gomis and Forcada, 2010). However, all of them share the same limitation: they require the user to provide the URLs of the multilingual websites to be crawled. Despite the fact that large amounts of parallel data can be obtained from a single website, this requirement implies that these tools will require a list of web pages to crawl and will not be able to exploit the web as a parallel corpus in a fully automated way. To deal with this limitation, we propose a new method that focuses on crawling top-level domains (TLD) for multilingual data, and then detects parallel data inside the"
L16-1471,P02-1040,0,0.0993055,"cept for the following. On top of the default word-based reordering model, our system implements two additional ones, phrase-based and hierarchical (Galley and Manning, 2008). On top of this, two additional resources were obtained from the hrenWaC parallel corpus and used in the MT systems built: an operation sequence model (Durrani et al., 2011) and a bilingual neural language model (Devlin et al., 2014). Results. We evaluate the systems trained in both directions and on both data collections (only hrenWaC and all the training corpora) with two widely used automatic evaluation metrics: BLEU (Papineni et al., 2002) and TER (Snover et al., 2006). The results are shown in Table 4. As it can be seen, in both translation directions the performance of the new SMT systems built on the hrenWaC corpus obtain results that are comparable to those obtained by the third-party systems. In the harder translation direction, English→Croatian, the newly built SMT systems outperform two of the reference systems, Bing and Yandex, while we do not observe a substantial decrease in the quality of the MT system built solely on the hrenWaC parallel corpus compared to the system built on all the training corpora. direction en→h"
L16-1471,J03-3002,0,0.763102,", 2009; Hong et al., 2010; Sridhar et al., 2011; Antonova and Misyurev, 2011; Barbosa et al., 2012), such as bag-of-words overlapping. Although these metrics have proved to be useful for parallel data detection, their main limitation is that they require some linguistic resources (such as a bilingual lexicon or a basic machine translation system) which may not be available for some language pairs. To avoid this problem, other works use the HTML structure of the web pages, which usually remains stable between different translations of the same document (Ma and Liberman, 1999; Nie et al., 1999; Resnik and Smith, 2003; Zhang et al., 2006; D´esilets et al., 2008; Espl`a-Gomis and Forcada, 2010; San Vicente and Manterola, 2012; Papavassiliou et al., 2013). Another useful strategy is to identify language markers in the URLs (Ma and Liberman, 1999; Nie et al., 1999; Resnik and Smith, 2003; Zhang et al., 2006; D´esilets et al., 2008; Espl`a-Gomis and Forcada, 2010; San Vicente and Manterola, 2012) that help detecting possible parallel documents. Some authors have used similar approaches to crawl comparable corpora. For instance, Smith et al. (2010) use the links between translated articles in Wikipedia to crawl"
L16-1471,resnik-1998-parallel,0,0.221213,"trategy is used by Munteanu and Marcu (2005), who compare news published in versions of news websites written in different languages by using a publication time stamp window. In this way, it is possible to retrieve likely on-topic news on which a cross-lingual information retrieval strategy is applied based on word-to-word machine translation. Even though these methods have proven to be useful for specific web sites, the real challenge is to find strategies that allow to extend them to crawl the Web in an unsupervised fashion, therefore allowing to exploit the real potential of this resource. Resnik (1998) uses language anchors, i.e. phrases in a given language that may be a hint indicating that the translation of a web page is available through a link, such as the link captions “in Chinese” or “Chinese” in a website in English. Resnik (1998) builds queries containing two possible anchors in two languages and queries the Altavista search engine to find potentially parallel websites. Chen and Nie (2000) use a similar approach, but they look for anchors separately, i.e. in two different queries for each language. Once this is done, the URLs in both results are compared in order to obtain the list"
L16-1471,san-vicente-manterola-2012-paco2,0,0.0456805,"Missing"
L16-1471,2010.iwslt-papers.16,0,0.0552492,"Missing"
L16-1471,N10-1063,0,0.0183588,"the same document (Ma and Liberman, 1999; Nie et al., 1999; Resnik and Smith, 2003; Zhang et al., 2006; D´esilets et al., 2008; Espl`a-Gomis and Forcada, 2010; San Vicente and Manterola, 2012; Papavassiliou et al., 2013). Another useful strategy is to identify language markers in the URLs (Ma and Liberman, 1999; Nie et al., 1999; Resnik and Smith, 2003; Zhang et al., 2006; D´esilets et al., 2008; Espl`a-Gomis and Forcada, 2010; San Vicente and Manterola, 2012) that help detecting possible parallel documents. Some authors have used similar approaches to crawl comparable corpora. For instance, Smith et al. (2010) use the links between translated articles in Wikipedia to crawl parallel sentences or words. A more complex strategy is used by Munteanu and Marcu (2005), who compare news published in versions of news websites written in different languages by using a publication time stamp window. In this way, it is possible to retrieve likely on-topic news on which a cross-lingual information retrieval strategy is applied based on word-to-word machine translation. Even though these methods have proven to be useful for specific web sites, the real challenge is to find strategies that allow to extend them to"
L16-1471,P13-1135,0,0.0614331,"results are compared in order to obtain the lists of websites that might contain parallel documents. Ma and Liberman (1999) use a more direct approach; they download the list of websites of a given top level domain (TLD), download each of them, and apply language identification to keep only the documents in the languages desired. Similarly, Resnik and Smith (2003) use the Internet Archive4 to obtain a list of URLs for several specific TLDs. A set of rules are then applied on the URLs of the different TLDs in order to find parallelisms between them and, therefore, candidate parallel documents. Smith et al. (2013) extend this approach to use it on the Common Crawl corpus (Spiegler, 2013). In this paper we propose a novel strategy for building both parallel and monolingual corpora automatically by crawling TLDs. This strategy consists in combining two different existing tools: the SpiderLing monolingual crawler (Suchomel et al., 2012), which is able to automatically harvest documents from a given TLD starting from a collection of seed URLs, and the Bitextor parallel data crawler (Espl`aGomis et al., 2014). The main differences between this approach and other previous works are as follows: (i) this metho"
L16-1471,2006.amta-papers.25,0,0.0146066,"f the default word-based reordering model, our system implements two additional ones, phrase-based and hierarchical (Galley and Manning, 2008). On top of this, two additional resources were obtained from the hrenWaC parallel corpus and used in the MT systems built: an operation sequence model (Durrani et al., 2011) and a bilingual neural language model (Devlin et al., 2014). Results. We evaluate the systems trained in both directions and on both data collections (only hrenWaC and all the training corpora) with two widely used automatic evaluation metrics: BLEU (Papineni et al., 2002) and TER (Snover et al., 2006). The results are shown in Table 4. As it can be seen, in both translation directions the performance of the new SMT systems built on the hrenWaC corpus obtain results that are comparable to those obtained by the third-party systems. In the harder translation direction, English→Croatian, the newly built SMT systems outperform two of the reference systems, Bing and Yandex, while we do not observe a substantial decrease in the quality of the MT system built solely on the hrenWaC parallel corpus compared to the system built on all the training corpora. direction en→hr hr→en system Google Bing Yan"
L16-1471,tiedemann-2012-parallel,0,0.0828156,"main approaches to the problem of parallel data crawling. Section 3 describes the tool Spidextor. Section 4 describes the new corpora crated for English– Croatian, English–Slovene, English–Serbian, and English– Finnish language pairs, while Section 5 describes the evaluation carried out for these new resources and the results obtained. The paper ends with some concluding remarks in Section 6. 2 Related work One of the most usual strategies to crawl parallel data from the Internet is to focus on web sites that make it straightforward to detect parallel documents (Nie et al., 1999; Koehn, 2005; Tiedemann, 2012). Many approaches use content-based metrics (Jiang et al., 2009; Utiyama et al., 2009; Yan et al., 2009; Hong et al., 2010; Sridhar et al., 2011; Antonova and Misyurev, 2011; Barbosa et al., 2012), such as bag-of-words overlapping. Although these metrics have proved to be useful for parallel data detection, their main limitation is that they require some linguistic resources (such as a bilingual lexicon or a basic machine translation system) which may not be available for some language pairs. To avoid this problem, other works use the HTML structure of the web pages, which usually remains stab"
L16-1471,2014.eamt-1.45,1,0.880004,"Missing"
L16-1471,2009.mtsummit-papers.18,0,0.0205938,"tool Spidextor. Section 4 describes the new corpora crated for English– Croatian, English–Slovene, English–Serbian, and English– Finnish language pairs, while Section 5 describes the evaluation carried out for these new resources and the results obtained. The paper ends with some concluding remarks in Section 6. 2 Related work One of the most usual strategies to crawl parallel data from the Internet is to focus on web sites that make it straightforward to detect parallel documents (Nie et al., 1999; Koehn, 2005; Tiedemann, 2012). Many approaches use content-based metrics (Jiang et al., 2009; Utiyama et al., 2009; Yan et al., 2009; Hong et al., 2010; Sridhar et al., 2011; Antonova and Misyurev, 2011; Barbosa et al., 2012), such as bag-of-words overlapping. Although these metrics have proved to be useful for parallel data detection, their main limitation is that they require some linguistic resources (such as a bilingual lexicon or a basic machine translation system) which may not be available for some language pairs. To avoid this problem, other works use the HTML structure of the web pages, which usually remains stable between different translations of the same document (Ma and Liberman, 1999; Nie et"
L16-1513,W14-0405,1,0.888305,"Missing"
L16-1513,L16-1676,1,0.863099,"Missing"
L16-1513,rodrigues-rytting-2012-typing,0,0.0282355,"more of a document collection than a corpus (Pedler, 2007). Although remarkable by its size of 12,000 tokens, it is composed of various available materials collected from various sources without a defined sampling methodology. Among the examples of full-fledged (yet very small by its size of 1k tokens) corpora, we could mention the work of Rello et al. (2012) who approached the problem of collecting the corpus of texts produced by adolescents with dyslexia with a solid methodology. Worth mentioning are also the attempts of automated building of error corpora by Miłkowski (2007), and those by Rodrigues and Rytting (2012), using crowdsourcing principles. Our corpus has already served as the base for different streams of research. Kuvaˇc Kraljevi´c et al. (2016) have tackled the methodological questions concerning collecting and sampling of specialized oral and written adult speakers corpora. In the framework of cognitive model of writing Kuvaˇc Kraljevi´c and Kolograni´c Beli´c (2015) have analyzed the grammatical and orthographic features in written language production of adolescents with specific language impairment (SLI). Recent study based on this corpus was oriented towards text quality measured by variou"
L16-1573,W11-2123,0,0.0839554,"Missing"
L16-1573,W14-0405,1,0.889826,"Missing"
L16-1573,ljubesic-etal-2014-tweetcat,1,0.901883,"Missing"
L16-1573,R15-1049,1,0.901157,"Missing"
L16-1573,W02-2021,0,0.889319,"Missing"
L16-1573,D15-1275,0,0.0399206,"Missing"
L16-1573,W98-1504,0,0.311496,"Missing"
L16-1573,tufis-ceausu-2008-diac,0,0.0661917,"Missing"
L16-1573,P94-1013,0,0.196687,"Missing"
L16-1676,W15-5301,1,0.750211,"Missing"
L16-1676,W13-2408,1,0.927236,"Missing"
L16-1676,W13-4903,1,0.895257,"Missing"
L16-1676,P07-2053,0,0.357836,"Missing"
L16-1676,W14-0405,1,0.88338,"Missing"
L16-1676,R15-1050,1,0.895159,"Missing"
L16-1676,L16-1242,1,0.830604,"Missing"
L16-1676,P14-5003,0,0.0722968,"Missing"
L16-1676,W03-2906,0,0.144031,"Missing"
ljubesic-etal-2008-generating,W07-1702,0,\N,Missing
ljubesic-etal-2008-generating,W03-2906,0,\N,Missing
ljubesic-etal-2008-generating,P97-1008,0,\N,Missing
ljubesic-etal-2008-generating,A00-1031,0,\N,Missing
ljubesic-etal-2008-generating,C96-2151,0,\N,Missing
ljubesic-etal-2008-generating,O98-4002,0,\N,Missing
ljubesic-etal-2014-tweetcat,W10-0513,0,\N,Missing
ljubesic-etal-2014-tweetcat,W14-0405,1,\N,Missing
ljubesic-etal-2014-tweetcat,baroni-bernardini-2004-bootcat,0,\N,Missing
ljubesic-toral-2014-cawac,W06-1704,0,\N,Missing
ljubesic-toral-2014-cawac,P02-1040,0,\N,Missing
ljubesic-toral-2014-cawac,W09-0432,0,\N,Missing
ljubesic-toral-2014-cawac,P07-2045,0,\N,Missing
ljubesic-toral-2014-cawac,W13-2411,1,\N,Missing
ljubesic-toral-2014-cawac,schafer-bildhauer-2012-building,0,\N,Missing
ljubesic-toral-2014-cawac,W14-0405,1,\N,Missing
ljubesic-toral-2014-cawac,padro-stanilovsky-2012-freeling,0,\N,Missing
ljubesic-toral-2014-cawac,P03-1021,0,\N,Missing
P18-2061,D11-1120,0,0.0396552,"ive: bleaching text, i.e., transforming lexical strings into more abstract features. This study provides evidence that such features allow for better transfer across languages. Moreover, we present a first study on the ability of humans to perform cross-lingual gender prediction. We find that human predictive power proves similar to that of our bleached models, and both perform better than lexical models. 1 Introduction Author profiling is the task of discovering latent user attributes disclosed through text, such as gender, age, personality, income, location and occupation (Rao et al., 2010; Burger et al., 2011; Feng et al., 2012; Jurgens, 2013; Bamman et al., 2014; Plank and Hovy, 2015; Flekova et al., 2016). It is of interest to several applications including personalized machine translation, forensics, and marketing (Mirkin et al., 2015; Rangel et al., 2015). Early approaches to gender prediction (Koppel et al., 2002; Schler et al., 2006, e.g.) are inspired by pioneering work on authorship attribution (Mosteller and Wallace, 1964). Such stylometric models typically rely on carefully handselected sets of content-independent features to capture style beyond topic. Recently, open vocabulary approach"
P18-2061,W17-4407,0,0.0934559,"rkin et al., 2015; Rangel et al., 2015). Early approaches to gender prediction (Koppel et al., 2002; Schler et al., 2006, e.g.) are inspired by pioneering work on authorship attribution (Mosteller and Wallace, 1964). Such stylometric models typically rely on carefully handselected sets of content-independent features to capture style beyond topic. Recently, open vocabulary approaches (Schwartz et al., 2013), where the entire linguistic production of an author is used, yielded substantial performance gains in online user-attribute prediction (Nguyen et al., 2014; Preo¸tiuc-Pietro et al., 2015; Emmery et al., 2017). Indeed, the best performing gender prediction models exploit chiefly lexical information (Rangel et al., 2017; Basile et al., 2017). Relying heavily on the lexicon though has its limitations, as it results in models with limited portability. Moreover, performance might be overly optimistic due to topic bias (Sarawgi et al., 2011). Recent work on cross-lingual author profiling has proposed the use of solely language-independent features (Ljubeši´c et al., 2017), e.g., specific textual elements (percentage of emojis, URLs, etc) and users’ meta-data/network (number of followers, etc), but this"
P18-2061,C14-1184,0,0.0612336,"Missing"
P18-2061,D12-1139,0,0.0579617,"Missing"
P18-2061,P16-1080,0,0.100081,"Missing"
P18-2061,N07-1051,0,0.0192212,"ence of additional user-specific metadata. Our approach can be seen as taking advantage of elements from a data-driven open-vocabulary approach, while trying to capture gender-specific style in text beyond topic. To represent utterances in a more language agnostic way, we propose to simply transform the text into alternative textual representations, which deviate from the lexical form to allow for abstraction. We propose the following transformations, exemplified in Table 1. They are mostly motivated by intuition and inspired by prior work, like the use of shape features from NER and parsing (Petrov and Klein, 2007; Schnabel and Schütze, 2014; Plank et al., 2016; Limsopatham and Collier, 2016): • AllAbs A combination (concatenation) of all previously described features. 3 Experiments In order to test whether abstract features are effective and transfer across languages, we set up experiments for gender prediction comparing lexicalized and bleached models for both in- and cross-language experiments. We compare them to a model using multilingual embeddings (Ruder, 2017). Finally, we elicit human judgments both within language and across language. The latter is to check whether a person with no prior knowl"
P18-2061,W16-3920,0,0.0222583,"aking advantage of elements from a data-driven open-vocabulary approach, while trying to capture gender-specific style in text beyond topic. To represent utterances in a more language agnostic way, we propose to simply transform the text into alternative textual representations, which deviate from the lexical form to allow for abstraction. We propose the following transformations, exemplified in Table 1. They are mostly motivated by intuition and inspired by prior work, like the use of shape features from NER and parsing (Petrov and Klein, 2007; Schnabel and Schütze, 2014; Plank et al., 2016; Limsopatham and Collier, 2016): • AllAbs A combination (concatenation) of all previously described features. 3 Experiments In order to test whether abstract features are effective and transfer across languages, we set up experiments for gender prediction comparing lexicalized and bleached models for both in- and cross-language experiments. We compare them to a model using multilingual embeddings (Ruder, 2017). Finally, we elicit human judgments both within language and across language. The latter is to check whether a person with no prior knowledge of (the lexicon of) a given language can predict the gender of a user, and"
P18-2061,W17-2901,1,0.855355,"Missing"
P18-2061,D15-1130,0,0.0306333,"ans to perform cross-lingual gender prediction. We find that human predictive power proves similar to that of our bleached models, and both perform better than lexical models. 1 Introduction Author profiling is the task of discovering latent user attributes disclosed through text, such as gender, age, personality, income, location and occupation (Rao et al., 2010; Burger et al., 2011; Feng et al., 2012; Jurgens, 2013; Bamman et al., 2014; Plank and Hovy, 2015; Flekova et al., 2016). It is of interest to several applications including personalized machine translation, forensics, and marketing (Mirkin et al., 2015; Rangel et al., 2015). Early approaches to gender prediction (Koppel et al., 2002; Schler et al., 2006, e.g.) are inspired by pioneering work on authorship attribution (Mosteller and Wallace, 1964). Such stylometric models typically rely on carefully handselected sets of content-independent features to capture style beyond topic. Recently, open vocabulary approaches (Schwartz et al., 2013), where the entire linguistic production of an author is used, yielded substantial performance gains in online user-attribute prediction (Nguyen et al., 2014; Preo¸tiuc-Pietro et al., 2015; Emmery et al., 20"
P18-2061,I17-4024,1,0.880203,"Missing"
P18-2061,W15-2913,1,0.891191,"eatures. This study provides evidence that such features allow for better transfer across languages. Moreover, we present a first study on the ability of humans to perform cross-lingual gender prediction. We find that human predictive power proves similar to that of our bleached models, and both perform better than lexical models. 1 Introduction Author profiling is the task of discovering latent user attributes disclosed through text, such as gender, age, personality, income, location and occupation (Rao et al., 2010; Burger et al., 2011; Feng et al., 2012; Jurgens, 2013; Bamman et al., 2014; Plank and Hovy, 2015; Flekova et al., 2016). It is of interest to several applications including personalized machine translation, forensics, and marketing (Mirkin et al., 2015; Rangel et al., 2015). Early approaches to gender prediction (Koppel et al., 2002; Schler et al., 2006, e.g.) are inspired by pioneering work on authorship attribution (Mosteller and Wallace, 1964). Such stylometric models typically rely on carefully handselected sets of content-independent features to capture style beyond topic. Recently, open vocabulary approaches (Schwartz et al., 2013), where the entire linguistic production of an auth"
P18-2061,P16-2067,1,0.838314,"ach can be seen as taking advantage of elements from a data-driven open-vocabulary approach, while trying to capture gender-specific style in text beyond topic. To represent utterances in a more language agnostic way, we propose to simply transform the text into alternative textual representations, which deviate from the lexical form to allow for abstraction. We propose the following transformations, exemplified in Table 1. They are mostly motivated by intuition and inspired by prior work, like the use of shape features from NER and parsing (Petrov and Klein, 2007; Schnabel and Schütze, 2014; Plank et al., 2016; Limsopatham and Collier, 2016): • AllAbs A combination (concatenation) of all previously described features. 3 Experiments In order to test whether abstract features are effective and transfer across languages, we set up experiments for gender prediction comparing lexicalized and bleached models for both in- and cross-language experiments. We compare them to a model using multilingual embeddings (Ruder, 2017). Finally, we elicit human judgments both within language and across language. The latter is to check whether a person with no prior knowledge of (the lexicon of) a given language can pr"
P18-2061,P15-1169,0,0.0610355,"Missing"
P18-2061,W11-0310,0,0.018254,"yle beyond topic. Recently, open vocabulary approaches (Schwartz et al., 2013), where the entire linguistic production of an author is used, yielded substantial performance gains in online user-attribute prediction (Nguyen et al., 2014; Preo¸tiuc-Pietro et al., 2015; Emmery et al., 2017). Indeed, the best performing gender prediction models exploit chiefly lexical information (Rangel et al., 2017; Basile et al., 2017). Relying heavily on the lexicon though has its limitations, as it results in models with limited portability. Moreover, performance might be overly optimistic due to topic bias (Sarawgi et al., 2011). Recent work on cross-lingual author profiling has proposed the use of solely language-independent features (Ljubeši´c et al., 2017), e.g., specific textual elements (percentage of emojis, URLs, etc) and users’ meta-data/network (number of followers, etc), but this information is not always available. We propose a novel approach where the actual text is still used, but bleached out and transformed into more abstract, and potentially better transferable features. One could view this as a method in between the open vocabulary strategy and the stylometric approach. It has the advantage of fading"
P18-2061,Q14-1002,0,0.020286,"specific metadata. Our approach can be seen as taking advantage of elements from a data-driven open-vocabulary approach, while trying to capture gender-specific style in text beyond topic. To represent utterances in a more language agnostic way, we propose to simply transform the text into alternative textual representations, which deviate from the lexical form to allow for abstraction. We propose the following transformations, exemplified in Table 1. They are mostly motivated by intuition and inspired by prior work, like the use of shape features from NER and parsing (Petrov and Klein, 2007; Schnabel and Schütze, 2014; Plank et al., 2016; Limsopatham and Collier, 2016): • AllAbs A combination (concatenation) of all previously described features. 3 Experiments In order to test whether abstract features are effective and transfer across languages, we set up experiments for gender prediction comparing lexicalized and bleached models for both in- and cross-language experiments. We compare them to a model using multilingual embeddings (Ruder, 2017). Finally, we elicit human judgments both within language and across language. The latter is to check whether a person with no prior knowledge of (the lexicon of) a g"
P18-2061,L16-1258,1,0.881658,"rediction comparing lexicalized and bleached models for both in- and cross-language experiments. We compare them to a model using multilingual embeddings (Ruder, 2017). Finally, we elicit human judgments both within language and across language. The latter is to check whether a person with no prior knowledge of (the lexicon of) a given language can predict the gender of a user, and how that compares to an in-language setup and the machine. If humans can predict gender cross-lingually, they are likely to rely on aspects beyond lexical information. Data We obtain data from the T WI S TY corpus (Verhoeven et al., 2016), a multi-lingual collection of Twitter users, for the languages with 500+ users, namely Dutch, French, Portuguese, and Spanish. We complement them with English, using data from a predecessor of T WI S TY (Plank and Hovy, 2015). All datasets contain manually annotated gender information. To simplify interpretation for the cross-language experiments, we balance gender in all datasets by downsampling to the minority class. The datasets’ final sizes are given in Table 2. We use 200 tweets per user, as done by previous work (Verhoeven et al., 2016). We leave the data untokenized to exclude any lan"
R11-1018,P02-1051,0,0.11789,"Missing"
R11-1018,W11-1204,1,0.736877,"Missing"
R11-1018,C10-2055,0,0.0310454,", Hissar, Bulgaria, 12-14 September 2011. precision of the extracted translation equivalents that consequently results in a more usable resource in a real-world setting. And finally, we are not limiting our experiments to nouns, but are working with all content words. term and its translation share similar contexts. The method consists of two steps: first, contexts of words are modeled and then similarity between the source-language and target-language contexts are measured with the help of a dictionary. Most approaches represent contexts as weighted collections of words using log-likelihood (Ismail and Manandhar, 2010), TF-IDF (Fung, 1998) or PMI (Shezaf and Rappoport, 2010). After building context vectors for words in both languages, the similarity between a source word’s context vector and all the context vectors in the target language is computed using a similarity measure, such as cosine (Fung, 1998), Jaccard (Otero and Campos, 2005) or Dice (Otero, 2007). 3 Building Resources In this section we present two resources we built for this experiment: the comparable corpus and the seed lexicon. Since our goal in the experiment reported in this paper is the extraction of translation equivalents for the genera"
R11-1018,W02-0902,0,0.827561,"Missing"
R11-1018,P99-1067,0,0.948155,"elated Languages Darja Fiˇser Nikola Ljubeˇsi´c Faculty of Arts, Faculty of Humanities and Social Sciences, Univeristy of Ljubljana University of Zagreb darja.fiser@ff.uni-lj.si nikola.ljubesic@ffzg.hr Abstract This is why an alternative approach has become popular in recent years. It relies on texts in two languages which are not parallel but comparable (Fung, 1998; Rapp, 1999) and therefore easier to compile, especially from the increasingly rich web data (Xiao and McEnery, 2006). The approach relies on the assumption that the term and its translation appear in similar contexts (Fung, 1998; Rapp, 1999). This means that the translation of a source word can be found by identifying a target word which has the most similar context vector in a comparable corpus. However, a direct comparison of vectors in two different languages is not possible, which is why a dictionary is needed to first translate the features of source context vectors into the target language and compute similarity on those. But this step seems paradoxical: the very reason why we are applying the complex comparable corpus technique for extracting translation equivalents is the fact that we do not have a bilingual dictionary at"
R11-1018,C04-1137,0,0.432778,"used as similarity measure. Finally, ten top-ranking translation candidates are kept for automatic and manual evaluation. We try to improve the results by extending the seed lexicon with contextually confirmed cognates as well as with first translations of the most frequent 4.3 Extending the Seed Lexicon with Cognates In order to beat the baseline we first extended the seed lexicon with cognates. We calculated them with BI-SIM, the longest common subsequence of 127 bigrams with a space prefix added to the beginning of each word in order to punish the differences at the beginning of the words (Kondrak and Dorr, 2004). The threshold for cognates has been empirically set to 0.7. In this step, translation equivalents were calculated as explained above for all content words (nouns, adjectives, verbs and adverbs), taking into account 20 top-ranking translations and analyzing them for cognate clues in that order. If we found a translation equivalent that met the cognate threshold of 0.7, we added that pair to the lexicon. If the seed lexicon already contained a translation for a cognate we identified with this procedure, we replaced the existing lexicon entry with the new identified cognate pair. Replacing entr"
R11-1018,C04-1089,0,0.139573,"Missing"
R11-1018,P10-1011,0,0.141428,"e extracted translation equivalents that consequently results in a more usable resource in a real-world setting. And finally, we are not limiting our experiments to nouns, but are working with all content words. term and its translation share similar contexts. The method consists of two steps: first, contexts of words are modeled and then similarity between the source-language and target-language contexts are measured with the help of a dictionary. Most approaches represent contexts as weighted collections of words using log-likelihood (Ismail and Manandhar, 2010), TF-IDF (Fung, 1998) or PMI (Shezaf and Rappoport, 2010). After building context vectors for words in both languages, the similarity between a source word’s context vector and all the context vectors in the target language is computed using a similarity measure, such as cosine (Fung, 1998), Jaccard (Otero and Campos, 2005) or Dice (Otero, 2007). 3 Building Resources In this section we present two resources we built for this experiment: the comparable corpus and the seed lexicon. Since our goal in the experiment reported in this paper is the extraction of translation equivalents for the general vocabulary, we built a Croatian-Slovene comparable news"
R11-1018,P00-1056,0,0.358778,"Missing"
R15-1049,N13-1037,0,0.0510739,"Missing"
R15-1049,I11-1100,0,0.0306528,"Missing"
R15-1049,P11-2008,0,0.320414,"Missing"
R15-1049,D12-1039,0,0.063609,"Missing"
R15-1049,1981.tc-1.7,0,0.844586,"Missing"
R15-1049,ljubesic-etal-2014-tweetcat,1,0.813486,"Missing"
R15-1049,I13-1041,0,0.0267248,"Missing"
R15-1050,agic-ljubesic-2014-setimes,1,0.87184,"Missing"
R15-1050,W13-2408,1,0.787473,"Missing"
R15-1050,E14-1060,0,0.0196582,". The approach by Lind´en (2009) does not rely on corpus evidence only, but uses the existing lexicon as well, showing that by combining corpus and lexicon evidence significant gains can be achieved. The first approach to exploit machine learning over multiple sources of information for extending morphological lexicons is the work of Kaufmann and Pfister (2010) who use the information from a morphological lexicon, a morphological grammar and a corpus, and combine it via a machine-learning approach to guess the stem and morphosyntactic information for unknown words. Using a different approach, Ahlberg et al. (2014) learns paradigms from an initial collection of inflection tables, and new words are assigned to these paradigms by using a confidence score. This approach is later extended by Ahlberg et al. (2015) to use multi-class classification (using support vector machines) for choosing the best paradigm. In this work, all the possible suffixes and prefixes from a given surface form are used as binary features, 379 Proceedings of Recent Advances in Natural Language Processing, pages 379–387, Hissar, Bulgaria, Sep 7–9 2015. after applying feature selection in order to optimise the performance. 3.1 Regard"
R15-1050,N15-1107,0,0.0313098,". The first approach to exploit machine learning over multiple sources of information for extending morphological lexicons is the work of Kaufmann and Pfister (2010) who use the information from a morphological lexicon, a morphological grammar and a corpus, and combine it via a machine-learning approach to guess the stem and morphosyntactic information for unknown words. Using a different approach, Ahlberg et al. (2014) learns paradigms from an initial collection of inflection tables, and new words are assigned to these paradigms by using a confidence score. This approach is later extended by Ahlberg et al. (2015) to use multi-class classification (using support vector machines) for choosing the best paradigm. In this work, all the possible suffixes and prefixes from a given surface form are used as binary features, 379 Proceedings of Recent Advances in Natural Language Processing, pages 379–387, Hissar, Bulgaria, Sep 7–9 2015. after applying feature selection in order to optimise the performance. 3.1 Regarding supervised approaches, it is worth noting the work by Durrett and DeNero (2013), in which patterns are built from morphologically analysed corpora to infer paradigms. For a given new surface for"
R15-1050,clement-etal-2004-morphology,0,0.503837,"Missing"
R15-1050,R11-1047,1,0.897125,"Missing"
R15-1050,W14-0405,1,0.879859,"Missing"
R15-1050,N13-1138,0,0.0602785,"Missing"
R15-1050,oliver-tadic-2004-enlarging,0,0.704197,"Missing"
rubino-etal-2014-quality,quirk-2004-training,0,\N,Missing
rubino-etal-2014-quality,2008.iwslt-papers.1,0,\N,Missing
rubino-etal-2014-quality,W12-3102,0,\N,Missing
rubino-etal-2014-quality,P02-1040,0,\N,Missing
rubino-etal-2014-quality,P07-2045,0,\N,Missing
rubino-etal-2014-quality,C04-1046,0,\N,Missing
rubino-etal-2014-quality,2009.eamt-1.5,0,\N,Missing
rubino-etal-2014-quality,P09-1018,0,\N,Missing
rubino-etal-2014-quality,2005.mtsummit-papers.11,0,\N,Missing
rubino-etal-2014-quality,W04-3250,0,\N,Missing
rubino-etal-2014-quality,2011.eamt-1.15,0,\N,Missing
rubino-etal-2014-quality,I13-1166,1,\N,Missing
rubino-etal-2014-quality,W12-5706,1,\N,Missing
rubino-etal-2014-quality,W03-0413,0,\N,Missing
rubino-etal-2014-quality,W12-3118,0,\N,Missing
W11-1204,W02-0902,0,0.339098,"Missing"
W11-1204,C10-1085,0,0.0143031,"n equivalent of a term is therefore reduced to finding the word in the target language whose context vector is most similar to the source term’s context vector based on their occurrence in a comparable corpus. This is basically a three-step procedure: (1) Building context vectors. When representing a word’s context, some approaches look at a simple co-occurrence window of a certain size while others include some syntactic information as well. For example, Otero (2007) proposes binary dependences previously extracted from a parallel corpus, while Yu and Tsujii (2009) use dependency parsers and Marsi and Krahmer (2010) use syntactic trees. Instead of context windows, Shao and Ng (2004) use language models. Next, words in co-occurrence vectors can be represented as binary features, by term frequency or weighted by different association measures, such as TF-IDF (Fung, 1998), PMI (Shezaf and Rappoport, 2010) or, one of the most popular, the log likelihood score. Approaches also exist that weigh cooccurrence terms differently if they appear closer to or further from the nucleus word in the context (e.g. Saralegi et al., 2008). (2) Translating context vectors. Finding the most similar context vectors in the sour"
W11-1204,P00-1056,0,0.315855,"Missing"
W11-1204,P99-1067,0,0.863106,"quency threshold for context vectors, the drop in precision is much slower than the increase of recall. 1 Introduction Research into using comparable corpora in NLP has gained momentum in the past decade largely due to limited availability of parallel data for many language pairs and domains. As an alternative to already established parallel approaches (e.g. Och 2000, Tiedemann 2005) the comparable corpusbased approach relies on texts in two or more languages which are not parallel but nevertheless share several parameters, such as topic, time of publication and communicative goal (Fung 1998, Rapp 1999). The main advantage of this approach is the simpler, faster and more time efficient compilation of comparable corpora, especially from the rich web data (Xiao & McEnery 2006). In this paper we describe the compilation process of a large comparable corpus of texts on healthrelated topics for Slovene and English that were published on the web. Then we report on a set of experiments we conducted in order to automatically extract translation equivalents for terms from the health domain. The parameters we tested and analysed are: 1- and 2-way translations of context vectors with a seed lexicon, th"
W11-1204,C04-1089,0,0.680665,"get language whose context vector is most similar to the source term’s context vector based on their occurrence in a comparable corpus. This is basically a three-step procedure: (1) Building context vectors. When representing a word’s context, some approaches look at a simple co-occurrence window of a certain size while others include some syntactic information as well. For example, Otero (2007) proposes binary dependences previously extracted from a parallel corpus, while Yu and Tsujii (2009) use dependency parsers and Marsi and Krahmer (2010) use syntactic trees. Instead of context windows, Shao and Ng (2004) use language models. Next, words in co-occurrence vectors can be represented as binary features, by term frequency or weighted by different association measures, such as TF-IDF (Fung, 1998), PMI (Shezaf and Rappoport, 2010) or, one of the most popular, the log likelihood score. Approaches also exist that weigh cooccurrence terms differently if they appear closer to or further from the nucleus word in the context (e.g. Saralegi et al., 2008). (2) Translating context vectors. Finding the most similar context vectors in the source and target language is not straightforward because a direct compa"
W11-1204,steinberger-etal-2006-jrc,0,0.250641,"Missing"
W11-1204,2009.mtsummit-posters.26,0,0.0448074,". The task of finding the appropriate translation equivalent of a term is therefore reduced to finding the word in the target language whose context vector is most similar to the source term’s context vector based on their occurrence in a comparable corpus. This is basically a three-step procedure: (1) Building context vectors. When representing a word’s context, some approaches look at a simple co-occurrence window of a certain size while others include some syntactic information as well. For example, Otero (2007) proposes binary dependences previously extracted from a parallel corpus, while Yu and Tsujii (2009) use dependency parsers and Marsi and Krahmer (2010) use syntactic trees. Instead of context windows, Shao and Ng (2004) use language models. Next, words in co-occurrence vectors can be represented as binary features, by term frequency or weighted by different association measures, such as TF-IDF (Fung, 1998), PMI (Shezaf and Rappoport, 2010) or, one of the most popular, the log likelihood score. Approaches also exist that weigh cooccurrence terms differently if they appear closer to or further from the nucleus word in the context (e.g. Saralegi et al., 2008). (2) Translating context vectors."
W11-1204,P10-1011,0,\N,Missing
W13-2408,A00-1031,0,0.456422,"Missing"
W13-2408,erjavec-2004-multext,0,0.0523663,"Missing"
W13-2408,2012.freeopmt-1.6,0,0.0181909,"ion set.test lemmatization-capable decision-tree-based TreeTagger12 (Schmid, 1995), support vector machine tagger SVMTool13 (Gim´enez and M`arquez, 2004) and CST’s14 data-driven rule-based lemmatizer (Ingason et al., 2008). Keeping in mind the previously mentioned state-of-the-art scores on Serbian 1984 corpus and statistical lemmatization capability, we also tested BTagger (Gesmundo and Samardˇzi´c, 2012a; Gesmundo and Samardˇzi´c, 2012b). Since some lemmatizers and taggers are capable of using an external morphological lexicon, we used a MTE v5r1 version of Apertium’s lexicon of Croatian15 (Peradin and Tyers, 2012) where applicable.16 All tools are well-documented and successfully applied across languages, as indicated in related work. wiki.test Model hr sr hr sr CST + lex 97.78 97.04 95.95 95.52 96.59 96.38 96.30 96.61 Table 5: Overall lemmatization accuracy with and without the inflectional lexicon zation and tagging accuracy as well as processing speed in both training and testing, the top performing tools are CST lemmatizer and HunPos tagger. Thus, we chose these two for further investigation in the following batches of experiments. It should be noted that, even though its performance is comparable"
W13-2408,E12-1050,0,0.107609,"Missing"
W13-2408,P11-2009,0,0.0681194,"Missing"
W13-2408,P12-2072,0,0.0888677,"Missing"
W13-2408,W07-1709,0,0.0136785,"Missing"
W13-2408,gesmundo-samardzic-2012-lemmatising,0,0.149047,"Missing"
W13-2408,W03-2906,0,0.447022,"Missing"
W13-2408,gimenez-marquez-2004-svmtool,0,0.0700347,"Missing"
W13-2408,P07-2053,0,0.192258,"Missing"
W13-2411,C10-1070,0,0.143276,"ng of pairs of words (RANDOM). Since the result of the procedure of identifying false friends in this setting is a single ranked list of lemma pairs where the ranking is performed by contextual or frequency dissimilarity, the same evaluation method can be applied as to evaluating a single query response in information retrieval. That is why we evaluated the output of each setting with average precision (AP), which averages over all precisions calculated on lists of false friend candidates built from each positive example upwards. 3. discounted log-odds (LO) first used in lexicon extraction by Laroche and Langlais (2010), showing consistently better performance than LL ; it is calculated from contingency table information as follows: LO = log X f We took under consideration the following association measures: IDF (t, V ) = log KL(v1 |v2 ) KL(v2 |v1 ) + 2 2 (O11 + 0.5)(O22 + 0.5) (O12 + 0.5)(O21 + 0.5) The following similarity measures were taken into account: 1. the well-known cosine measure (COSINE), 2. the Dice measure (DICE), defined in (Otero, 2008) as DiceMin, which has proven to be very good in various tasks of distributional 72 As three categories were encoded in the GOLD 2 gold standard, we weighted F"
W13-2411,R09-1054,0,0.148747,"etup and Section 6 reports on the results. We conclude the paper with final remarks and ideas for future work. 69 Proceedings of the 4th Biennial International Workshop on Balto-Slavic Natural Language Processing, pages 69–77, c Sofia, Bulgaria, 8-9 August 2013. 2010 Association for Computational Linguistics 2 Related Work most similar but contextually most dissimilar word pairs. The feature weighting used throughout the related work is mostly plain frequency with one case of using TF-IDF (Nakov and Nakov, 2007) whereas cosine is the most widely used similarity measure (Nakov and Nakov, 2007; Nakov and Nakov, 2009; Schulz et al., 2004) while Mitkov et al. (2007) use skew divergence which is very similar to Jensen-Shannon divergence. The main differences between the work we report on in this paper and the related work are: Automatic detection of false friends was initially limited to parallel corpora but has been extended to comparable corpora and web snippets (Nakov et al., 2007). The approaches to automatically identify false friends fall into two categories: those that only look at orthographic features of the source and the target word, and those that combine orthographic features with the semantic"
W13-2411,J93-1003,0,0.241396,"Missing"
W13-2411,C04-1117,0,0.0588284,"Missing"
W13-2411,2007.jeptalnrecital-long.8,0,0.621539,"tify false friends fall into two categories: those that only look at orthographic features of the source and the target word, and those that combine orthographic features with the semantic ones. Orthographic approaches typically rely on combinations of a number of orthographic similarity measures and machine learning techniques to classify source and target word pairs to cognates, false friends or unrelated words and evaluate the different combinations against a manually compiled list of legitimate and illegitimate cognates. This has been attempted for English and French (Inkpen et al., 2005; Frunza and Inkpen, 2007) as well as for Spanish and Portuguese (Torres and Alu´ısio, 2011). Most of the approaches that combine orthographic features with the semantic ones have been performed on parallel corpora where word frequency information and alignments at paragraph, sentence as well as word level play a crucial role at singling out false friends, which has been tested on Bulgarian and Russian (Nakov and Nakov, 2009). Work on non-parallel data, on the other hand, often treats false friend candidates as search queries, and considers the retrieved web snippets for these queries as contexts that are used to estab"
W13-2411,W11-4508,0,0.116182,"Missing"
W13-2501,fiser-etal-2012-addressing,1,0.850739,"(Fung, 1998; Rapp, 1999; Shao and Ng, 2004; Otero, 2007; Yu and Tsujii, 2009; Marsi and Krahmer, 2010) presuppose the availability of a bilingual lexicon for translating source vectors into the target language. A translation candidate is generally considered as correct if it is an appropriate translation for at least one sense of the source word in the dictionary, which often corresponds to its most frequent sense. An alternative consists in considering all translations provided for a word in the dictionary but weighting them by their frequency in the target language (Prochasson et al., 2009; Hazem and Morin, 2012). The high quality of the exploited handcrafted resources, combined to the skewed distribution of the translations corresponding to different word senses, often lead to satisfying results. Nevertheless, the applicability of the methods is limited to languages and domains where bilingual resources are available. Moreover, by promoting the most frequent sense/translation, this approach neglects polysemy. We believe that feature disambiguation can lead to the production of cleaner vectors and, consequently, to higher quality results. The need to bypass pre-existing dictionaries has been addressed"
W13-2501,N03-1015,0,0.0355119,"howed that the results with an automatically created seed lexicon, based on language similarity, can be as good as with a pre-existing dictionary. But all these approaches work on closely-related languages and cannot be used as successfully for language pairs with little lexical overlap, such as English and Slovene, which is the case in this experiment. Regarding the translation of the source vectors, we use contextual information to disambiguate their features and translate them using clusters of semantically similar translations in the target language. A similar idea has been implemented by Kaji (2003) who performed sense-based word 3 3.1 Resources Comparable corpus The comparable corpus from which the bilingual lexicon will be extracted is a collection of English (EN) and Slovene (SL) texts extracted from Wikipedia. The February 2013 dumps of Wikipedia articles were downloaded and cleaned for both languages after which the English corpus was tokenized, part-of-speech (PoS) tagged and lemmatized with the TreeTagger (Schmid, 1994). The same pre-processing was applied to the Slovene corpus with the ToTaLe analyzer (Erjavec et al., 2010) which uses the TnT tagger (Brants, 2000) and was trained"
W13-2501,apidianaki-2008-translation,1,0.80435,"rd We evaluate the quality of the bilingual lexicons extracted from the comparable corpus by comparing them to a gold standard lexicon, which was built from the aligned English (Fellbaum, 1998) and Slovene wordnets (Fiˇser and Sagot, 2008). We extracted all English synsets from the Base Concept sets that belong to the Factotum domain and contain literals with polysemy levels 1-5 and their 4.2 Translation clustering The translations of the English words in the lexicon built as described in 3.2 are clustered according to their semantic proximity using a crosslingual Word Sense Induction method (Apidianaki, 2008). For each translation Ti of a word w, a vector is built from the content word co3 Language POS Nouns Source word sphere address {obravnava, reˇsevanje, obravnavanje} (dealing with) {naslov} (postal address) portion {kos} (piece) {obrok, porcija} (serving) {deleˇz} (share) figure {ˇstevilka, podatek, znesek} (amount) {slika} (image) {osebnost} (person) seal EN–SL weigh Verbs Slovene sense clusters {krogla} (geometrical shape) {sfera, podroˇcje} (area) {tesniti} (to be water-/airtight) {zapreti, zapeˇcatiti} (to close an envelope or some other container) {pretehtati} (consider possibilities) {t"
W13-2501,W02-0902,0,0.0403888,"e high quality of the exploited handcrafted resources, combined to the skewed distribution of the translations corresponding to different word senses, often lead to satisfying results. Nevertheless, the applicability of the methods is limited to languages and domains where bilingual resources are available. Moreover, by promoting the most frequent sense/translation, this approach neglects polysemy. We believe that feature disambiguation can lead to the production of cleaner vectors and, consequently, to higher quality results. The need to bypass pre-existing dictionaries has been addressed by Koehn and Knight (2002) who built the initial seed dictionary automatically, based on identical spelling features between English and German. Cognate detection has also been used by Saralegi et al. (2008) for extracting word translations from English-Basque comparable corpora. The cognate and seed lexicon approaches have been successfully combined by Fiˇser and Ljubeˇsi´c (2011) who showed that the results with an automatically created seed lexicon, based on language similarity, can be as good as with a pre-existing dictionary. But all these approaches work on closely-related languages and cannot be used as successf"
W13-2501,E09-1010,1,0.937001,"r that contains the feature. The translations found in the disambiguation output convey the sense of the features in the source vector, while the use of translation clusters permits to expand their translation with several variants. As a consequence, the translated vectors are less noisy and richer, and allow for the extraction of higher quality lexicons compared to simpler methods. 1 In this paper, we show how source vectors can be translated into the target language by a cross-lingual Word Sense Disambiguation (WSD) method which exploits the output of data-driven Word Sense Induction (WSI) (Apidianaki, 2009), and demonstrate how feature disambiguation enhances the quality of the translations extracted from the comparable corpus. This study extends our previous work on the topic (Apidianaki et al., 2012) by applying the proposed methods to a comparable corpus of general language (built from Wikipedia) and optimizing various parameters that affect the quality of the extracted translations. We expect the disambiguation to have a beneficial impact on the results given that polysemy is a frequent phenomenon in a general, mixed-domain corpus. Our experiments are carried out on the English-Slovene langu"
W13-2501,2005.mtsummit-papers.11,0,0.00864014,"approach to a specialized comparable corpus from the health domain (Apidianaki et al., 2012). The results were encouraging, showing how translation clustering and vector disambiguation help to improve the quality of the translations extracted from the comparable corpus. We believe that the positive impact of this approach will be more significant on lexicon extraction from a general language comparable corpus, in which polysemy is more prominent. 3.2 Parallel corpus The parallel corpus used for clustering and word sense induction consists of the Slovene-English parts of Europarl (release v6) (Koehn, 2005) and of JRC-Acquis (Steinberger et al., 2006) and amounts to approximately 35M words per language. A number of pre-processing steps are applied to the corpus prior to sense induction, such 2 Figure 1: Translation extraction from comparable corpora using cross-lingual WSI and WSD. Slovene equivalents which have been validated by a lexicographer. Of 1,589 such synsets, 200 were randomly selected and used as a gold standard for automatic evaluation of the method proposed in this paper. as elimination of sentence pairs with a great difference in length, lemmatization and PoS tagging with the TreeT"
W13-2501,C10-1085,0,0.0140729,"ranslation and to suggest multiple semantically correct translations. A similar approach has been adopted by D´ejean et al. (2005) who expand vector translation by using a bilingual thesaurus instead of a lexicon. In contrast to their work, the method proposed here does not rely on any external knowledge source to determine word senses or translation equivalents, and is thus fully datadriven and language independent. The traditional approch to translation extraction from comparable corpora and most of its extensions (Fung, 1998; Rapp, 1999; Shao and Ng, 2004; Otero, 2007; Yu and Tsujii, 2009; Marsi and Krahmer, 2010) presuppose the availability of a bilingual lexicon for translating source vectors into the target language. A translation candidate is generally considered as correct if it is an appropriate translation for at least one sense of the source word in the dictionary, which often corresponds to its most frequent sense. An alternative consists in considering all translations provided for a word in the dictionary but weighting them by their frequency in the target language (Prochasson et al., 2009; Hazem and Morin, 2012). The high quality of the exploited handcrafted resources, combined to the skewe"
W13-2501,A00-1031,0,0.0109396,"en implemented by Kaji (2003) who performed sense-based word 3 3.1 Resources Comparable corpus The comparable corpus from which the bilingual lexicon will be extracted is a collection of English (EN) and Slovene (SL) texts extracted from Wikipedia. The February 2013 dumps of Wikipedia articles were downloaded and cleaned for both languages after which the English corpus was tokenized, part-of-speech (PoS) tagged and lemmatized with the TreeTagger (Schmid, 1994). The same pre-processing was applied to the Slovene corpus with the ToTaLe analyzer (Erjavec et al., 2010) which uses the TnT tagger (Brants, 2000) and was trained on MultextEast corpora. The Wikipedia corpus contains about 1.5 billion tokens for English and almost 24 million tokens for Slovene. In previous work, we applied our approach to a specialized comparable corpus from the health domain (Apidianaki et al., 2012). The results were encouraging, showing how translation clustering and vector disambiguation help to improve the quality of the translations extracted from the comparable corpus. We believe that the positive impact of this approach will be more significant on lexicon extraction from a general language comparable corpus, in"
W13-2501,J05-4003,0,0.0420579,"polysemy is a frequent phenomenon in a general, mixed-domain corpus. Our experiments are carried out on the English-Slovene language pair but as the methods are totally data-driven, the approach can be easily applied to other languages. Introduction Large-scale comparable corpora are available in many language pairs and are viewed as a source of valuable information for multilingual applications. Identifying translation correspondences in this type of corpora permits to construct bilingual lexicons for low-resourced languages, and to complement and reduce the sparseness of existing resources (Munteanu and Marcu, 2005; Snover et al., 2008). The main assumption behind translation extraction from comparable corpora is that a source word and its translation appear in similar contexts (Fung, 1998; Rapp, 1999). So, in order to identify a translation correspondence between the two languages, the contexts of the source word and the candidate translation have to be compared. For this comparison to take place, the same vector space has to be produced, which means that the vectors of the one language have to be translated The paper is organized as follows: In the next section, we present some related work on bilingu"
W13-2501,J03-1002,0,0.00375309,"processing steps are applied to the corpus prior to sense induction, such 2 Figure 1: Translation extraction from comparable corpora using cross-lingual WSI and WSD. Slovene equivalents which have been validated by a lexicographer. Of 1,589 such synsets, 200 were randomly selected and used as a gold standard for automatic evaluation of the method proposed in this paper. as elimination of sentence pairs with a great difference in length, lemmatization and PoS tagging with the TreeTagger (for English) and ToTaLe (for Slovene) (Erjavec et al., 2010). Next, the corpus is word-aligned with GIZA++ (Och and Ney, 2003) and two bilingual lexicons are extracted, one for each translation direction (EN–SL/SL– EN). To clean the lexicons from noisy alignments, the translations are filtered on the basis of their alignment score and PoS, keeping only translations that pertain to the same grammatical category as the source word. We retain only intersecting alignments and use for clustering translations that translate a source word more than 10 times in the training corpus. This threshold reduces data sparseness issues that affect the clustering and eliminates erroneous word alignments. The filtered EN-SL lexicon con"
W13-2501,erjavec-etal-2010-jos,1,0.876362,"Missing"
W13-2501,2007.mtsummit-papers.26,0,0.0099246,"usters permits to expand feature translation and to suggest multiple semantically correct translations. A similar approach has been adopted by D´ejean et al. (2005) who expand vector translation by using a bilingual thesaurus instead of a lexicon. In contrast to their work, the method proposed here does not rely on any external knowledge source to determine word senses or translation equivalents, and is thus fully datadriven and language independent. The traditional approch to translation extraction from comparable corpora and most of its extensions (Fung, 1998; Rapp, 1999; Shao and Ng, 2004; Otero, 2007; Yu and Tsujii, 2009; Marsi and Krahmer, 2010) presuppose the availability of a bilingual lexicon for translating source vectors into the target language. A translation candidate is generally considered as correct if it is an appropriate translation for at least one sense of the source word in the dictionary, which often corresponds to its most frequent sense. An alternative consists in considering all translations provided for a word in the dictionary but weighting them by their frequency in the target language (Prochasson et al., 2009; Hazem and Morin, 2012). The high quality of the exploit"
W13-2501,2009.mtsummit-posters.14,0,0.0248424,"d most of its extensions (Fung, 1998; Rapp, 1999; Shao and Ng, 2004; Otero, 2007; Yu and Tsujii, 2009; Marsi and Krahmer, 2010) presuppose the availability of a bilingual lexicon for translating source vectors into the target language. A translation candidate is generally considered as correct if it is an appropriate translation for at least one sense of the source word in the dictionary, which often corresponds to its most frequent sense. An alternative consists in considering all translations provided for a word in the dictionary but weighting them by their frequency in the target language (Prochasson et al., 2009; Hazem and Morin, 2012). The high quality of the exploited handcrafted resources, combined to the skewed distribution of the translations corresponding to different word senses, often lead to satisfying results. Nevertheless, the applicability of the methods is limited to languages and domains where bilingual resources are available. Moreover, by promoting the most frequent sense/translation, this approach neglects polysemy. We believe that feature disambiguation can lead to the production of cleaner vectors and, consequently, to higher quality results. The need to bypass pre-existing diction"
W13-2501,R11-1018,1,0.902304,"Missing"
W13-2501,P99-1067,0,0.343625,"sily applied to other languages. Introduction Large-scale comparable corpora are available in many language pairs and are viewed as a source of valuable information for multilingual applications. Identifying translation correspondences in this type of corpora permits to construct bilingual lexicons for low-resourced languages, and to complement and reduce the sparseness of existing resources (Munteanu and Marcu, 2005; Snover et al., 2008). The main assumption behind translation extraction from comparable corpora is that a source word and its translation appear in similar contexts (Fung, 1998; Rapp, 1999). So, in order to identify a translation correspondence between the two languages, the contexts of the source word and the candidate translation have to be compared. For this comparison to take place, the same vector space has to be produced, which means that the vectors of the one language have to be translated The paper is organized as follows: In the next section, we present some related work on bilingual lexicon extraction from comparable corpora. Section 3 presents the data used in our experiments and Section 4 provides details on the approach and the experimental setup. In Section 5, we"
W13-2501,C04-1089,0,0.0319701,"sing translation clusters permits to expand feature translation and to suggest multiple semantically correct translations. A similar approach has been adopted by D´ejean et al. (2005) who expand vector translation by using a bilingual thesaurus instead of a lexicon. In contrast to their work, the method proposed here does not rely on any external knowledge source to determine word senses or translation equivalents, and is thus fully datadriven and language independent. The traditional approch to translation extraction from comparable corpora and most of its extensions (Fung, 1998; Rapp, 1999; Shao and Ng, 2004; Otero, 2007; Yu and Tsujii, 2009; Marsi and Krahmer, 2010) presuppose the availability of a bilingual lexicon for translating source vectors into the target language. A translation candidate is generally considered as correct if it is an appropriate translation for at least one sense of the source word in the dictionary, which often corresponds to its most frequent sense. An alternative consists in considering all translations provided for a word in the dictionary but weighting them by their frequency in the target language (Prochasson et al., 2009; Hazem and Morin, 2012). The high quality o"
W13-2501,N09-2031,0,0.0211288,"s to expand feature translation and to suggest multiple semantically correct translations. A similar approach has been adopted by D´ejean et al. (2005) who expand vector translation by using a bilingual thesaurus instead of a lexicon. In contrast to their work, the method proposed here does not rely on any external knowledge source to determine word senses or translation equivalents, and is thus fully datadriven and language independent. The traditional approch to translation extraction from comparable corpora and most of its extensions (Fung, 1998; Rapp, 1999; Shao and Ng, 2004; Otero, 2007; Yu and Tsujii, 2009; Marsi and Krahmer, 2010) presuppose the availability of a bilingual lexicon for translating source vectors into the target language. A translation candidate is generally considered as correct if it is an appropriate translation for at least one sense of the source word in the dictionary, which often corresponds to its most frequent sense. An alternative consists in considering all translations provided for a word in the dictionary but weighting them by their frequency in the target language (Prochasson et al., 2009; Hazem and Morin, 2012). The high quality of the exploited handcrafted resour"
W13-2501,D08-1090,0,0.033079,"Missing"
W13-2501,steinberger-etal-2006-jrc,0,0.0167182,"able corpus from the health domain (Apidianaki et al., 2012). The results were encouraging, showing how translation clustering and vector disambiguation help to improve the quality of the translations extracted from the comparable corpus. We believe that the positive impact of this approach will be more significant on lexicon extraction from a general language comparable corpus, in which polysemy is more prominent. 3.2 Parallel corpus The parallel corpus used for clustering and word sense induction consists of the Slovene-English parts of Europarl (release v6) (Koehn, 2005) and of JRC-Acquis (Steinberger et al., 2006) and amounts to approximately 35M words per language. A number of pre-processing steps are applied to the corpus prior to sense induction, such 2 Figure 1: Translation extraction from comparable corpora using cross-lingual WSI and WSD. Slovene equivalents which have been validated by a lexicographer. Of 1,589 such synsets, 200 were randomly selected and used as a gold standard for automatic evaluation of the method proposed in this paper. as elimination of sentence pairs with a great difference in length, lemmatization and PoS tagging with the TreeTagger (for English) and ToTaLe (for Slovene)"
W14-0405,agic-ljubesic-2014-setimes,1,0.898045,"Missing"
W14-0405,W13-2408,1,0.399097,"Missing"
W14-0405,W13-4903,0,0.0480246,"Missing"
W14-0405,baroni-etal-2008-cleaneval,0,0.0897804,"Missing"
W14-0405,P09-1017,0,0.0150078,"Missing"
W14-0405,P12-3005,0,0.0319989,"t of three top-level domains (TLDs) crawled. 1 A more thorough comparison of the three languages is available at http://en.wikipedia.org/ wiki/Comparison_of_standard_Bosnian, _Croatian_and_Serbian 29 Felix Bildhauer & Roland Schäfer (eds.), Proceedings of the 9th Web as Corpus Workshop (WaC-9) @ EACL 2014, pages 29–35, c Gothenburg, Sweden, April 26 2014. 2014 Association for Computational Linguistics 2 Related work solved problem by many, the recently growing interest for it indicates the opposite. Recently, researchers focused on improving off-the-shelf tools for identifying many languages (Lui and Baldwin, 2012), discriminating between similar languages where standard tools fail (Tiedemann and Ljubeˇsi´c, 2012), identifying documents written in multiple languages and identifying the languages in such multilingual documents (Lui et al., 2014). Text quality in automatically constructed web corpora is quite an underresearched topic, with the exception of boilerplate removal / content extraction approaches that deal with this problem implicitly (Baroni et al., 2008; Kohlsch¨utter et al., 2010), but quite drastically, by removing all content that does not conform to the criteria set. A recent approach to"
W14-0405,schafer-bildhauer-2012-building,0,0.0335485,"Missing"
W14-0405,C12-1160,1,0.901762,"Missing"
W14-0405,Q14-1003,0,\N,Missing
W14-4210,espla-gomis-etal-2014-comparing,1,0.887321,"Missing"
W14-4210,W14-0405,1,0.855262,"Missing"
W14-4210,W13-2408,1,0.889943,"Missing"
W14-4210,P07-2045,0,0.00751512,"Missing"
W14-4210,P02-1040,0,0.0913714,"d on the content published on the SETimes.com news portal which publishes “news and views from Southeast Europe” in ten languages: Bulgarian, Bosnian, Greek, English, Croatian, Macedonian, Romanian, Albanian and Serbian. We used the parallel trilingual Croatian-English-Serbian part of the corpus. The detailed corpus statistic is shown in Table 2. The Croatian language is further referred to as hr, Serbian as sr and English as en. The translation system used is the phrase-based Moses system (Koehn et al., 2007). The evaluation metrics used for assessment of the translations are the BLEU score (Papineni et al., 2002) and the F-score, which also takes recall into account and generally better correlates with human rankings which has been shown in (Melamed et al., 2003) and confirmed in (Popovi´c, 2011). For Language adaptation methods The following methods were investigated for adaptation of the test set in the other language: • lexical conversion of the most frequent words (conv); The most frequent6 different words together with simple morphological variations are replaced by the words in the corresponding language. This method is simple and fast, however it is very basic and also requires knowledge of the"
W14-4210,N03-2021,0,0.0269268,"ek, English, Croatian, Macedonian, Romanian, Albanian and Serbian. We used the parallel trilingual Croatian-English-Serbian part of the corpus. The detailed corpus statistic is shown in Table 2. The Croatian language is further referred to as hr, Serbian as sr and English as en. The translation system used is the phrase-based Moses system (Koehn et al., 2007). The evaluation metrics used for assessment of the translations are the BLEU score (Papineni et al., 2002) and the F-score, which also takes recall into account and generally better correlates with human rankings which has been shown in (Melamed et al., 2003) and confirmed in (Popovi´c, 2011). For Language adaptation methods The following methods were investigated for adaptation of the test set in the other language: • lexical conversion of the most frequent words (conv); The most frequent6 different words together with simple morphological variations are replaced by the words in the corresponding language. This method is simple and fast, however it is very basic and also requires knowledge of the involved languages to be set up. It can be seen as a very first step towards the use of a rule-based Croatian-Serbian system. • Croatian-Serbian transla"
W14-4210,W11-2110,1,0.877682,"Missing"
W14-4210,E12-1015,0,0.0780458,"LT4CloseLang), pages 76–84, c October 29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics 2.2 BLEU ) over the Google baseline in the tourism domain (Toral et al., 2014). A rule-based Apertium system (Peradin et al., 2014) has been recently developed for translation from and into Slovenian (also closely related language, but more distant). Techniques simpler than general SMT such as character-level translation have been investigated for translation between various close language pairs, where for the South Slavic group the Bulgarian-Macedonian pair has been explored (Nakov and Tiedemann, 2012). Characterbased translation has also been used for translating between Bosnian and Macedonian in order to build pivot translation systems from and into English (Tiedemann, 2012). Developing POS taggers and lemmatizers for Croatian and Serbian and using Croatian models on Serbian data has been explored in (Agi´c et al., 2013). To the best of our knowledge, a systematic investigation of cross-language translation systems involving Croatian and Serbian, thereby exploiting benefits from the language closeness and analyzing problems induced by language differences has not been carried out yet. 2 2"
W14-4210,P12-2059,0,\N,Missing
W14-5307,W14-5316,0,0.368134,"Missing"
W14-5307,Y08-1042,0,0.178551,"Groups - DSL 2014 Shared Task For this collection, randomly sampled sentences from journalistic corpora (and corpora collections) were selected for each of the 13 classes. Journalistic corpora were preferred because they represent standard language, which is an important factor to be considered when working with language varieties. Other data sources (e.g. Wikipedia) do not make any distinction between language varieties and they are therefore not suitable for the purpose of the shared task. A number of studies mentioned in the related work section use journalistic texts for similar reasons (Huang and Lee, 2008; Grouin et al., 2010; Zampieri and Gebre, 2012) Given what has been said in this section, we consider the collection to be a suitable comparable corpora from this task, which was compiled to avoid bias in classification towards source, register and topics. The 5 6 We considered a token as orthographic units delimited by white spaces. http://www.loc.gov/standards/iso639-2/php/English_list.php 60 DSL corpus collection was distributed in tab delimited format; the first column contains a sentence in the language/variety, the second column states its group and the last column refers to its languag"
W14-5307,W14-5317,0,0.274993,"within each group: binary for groups B-F and one versus all for group A, which contains three classes (Bosnian, Croatian and Serbian). An interesting contribution proposed by the RAE team (Porta and Sancho, 2014) are the so-called ‘white lists’ inspired by the ‘blacklist’ classifier (Tiedemann and Ljubeˇsi´c, 2012). These lists are word lists exclusive to a language or variety, similar to one of the features that Ranaivo-Malanc¸on (2006) proposed to discriminate between Malay and Indonesian. 64 Two groups used Information Gain (IG) to select the best features for classification, namely UMich (King et al., 2014) and UniMelb-NLP (Lui et al., 2014). These teams were also the only ones to submit open submissions. The UniMelb-NLP team tried different classification methods and features (including delexicalized models) in each run. The best results were obtained by their own method, the off-the-shelf general-purpose language identification software langid.py (Lui and Baldwin, 2012). This method has been widely used for general-purpose language identification and its performance is regarded superior to similar general-purpose methods such as TextCat. In the shared task, the system was modelled hierarchical"
W14-5307,P12-3005,0,0.0284087,"or variety, similar to one of the features that Ranaivo-Malanc¸on (2006) proposed to discriminate between Malay and Indonesian. 64 Two groups used Information Gain (IG) to select the best features for classification, namely UMich (King et al., 2014) and UniMelb-NLP (Lui et al., 2014). These teams were also the only ones to submit open submissions. The UniMelb-NLP team tried different classification methods and features (including delexicalized models) in each run. The best results were obtained by their own method, the off-the-shelf general-purpose language identification software langid.py (Lui and Baldwin, 2012). This method has been widely used for general-purpose language identification and its performance is regarded superior to similar general-purpose methods such as TextCat. In the shared task, the system was modelled hierarchically firstly identifying the language group that a sentence belongs to and subsequently the specific language, achieving performance comparable to the state-of-the-art, but still slightly below the other three systems. The QMUL team (Purver, 2014) proposed a linear SVM classifier using words and characters as features. The author investigated the influence of the cost par"
W14-5307,U13-1003,0,0.384905,"between more languages1 , they still struggle to discriminate between similar languages such as Croatian and Serbian or Malay and Indonesian. From an NLP point of view, the difficulty systems face when discriminating between closely related languages is similar to the problem of discriminating between standard national language varieties (e.g. American English and British English or Brazilian Portuguese and European Portuguese), henceforth varieties. Recent studies show that language varieties can be discriminated automatically using words or characters as features (Zampieri and Gebre, 2012; Lui and Cook, 2013) . However, due to performance limitations, state-of-the-art general-purpose language identification systems do not distinguish texts from different national varieties, modelling pluricentric languages as unique classes. To evaluate how state-of-the-art systems perform in identifying similar languages and varieties, we decided to organize the Discriminating between Similar Languages (DSL)2 shared task. This shared task was organized within the scope of the workshop on Applying NLP Tools to Similar Languages, Varieties and Dialects (VarDial) in the 2014 edition of COLING. The motivation behind"
W14-5307,W14-5315,0,0.274321,"B-F and one versus all for group A, which contains three classes (Bosnian, Croatian and Serbian). An interesting contribution proposed by the RAE team (Porta and Sancho, 2014) are the so-called ‘white lists’ inspired by the ‘blacklist’ classifier (Tiedemann and Ljubeˇsi´c, 2012). These lists are word lists exclusive to a language or variety, similar to one of the features that Ranaivo-Malanc¸on (2006) proposed to discriminate between Malay and Indonesian. 64 Two groups used Information Gain (IG) to select the best features for classification, namely UMich (King et al., 2014) and UniMelb-NLP (Lui et al., 2014). These teams were also the only ones to submit open submissions. The UniMelb-NLP team tried different classification methods and features (including delexicalized models) in each run. The best results were obtained by their own method, the off-the-shelf general-purpose language identification software langid.py (Lui and Baldwin, 2012). This method has been widely used for general-purpose language identification and its performance is regarded superior to similar general-purpose methods such as TextCat. In the shared task, the system was modelled hierarchically firstly identifying the language"
W14-5307,W14-5314,0,0.685536,"ly: NRCCNRC, RAE, UMich, UniMelb-NLP and QMUL. The best scores were obtained by the NRC-CNRC (Goutte et al., 2014) team which proposed a twostep approach to predict first the language group than the language of each instance. The language group was predicted in a 6-way classification using a probabilistic model similar to a Naive Bayes classifier, and later the method applied SVM classifiers to discriminate within each group: binary for groups B-F and one versus all for group A, which contains three classes (Bosnian, Croatian and Serbian). An interesting contribution proposed by the RAE team (Porta and Sancho, 2014) are the so-called ‘white lists’ inspired by the ‘blacklist’ classifier (Tiedemann and Ljubeˇsi´c, 2012). These lists are word lists exclusive to a language or variety, similar to one of the features that Ranaivo-Malanc¸on (2006) proposed to discriminate between Malay and Indonesian. 64 Two groups used Information Gain (IG) to select the best features for classification, namely UMich (King et al., 2014) and UniMelb-NLP (Lui et al., 2014). These teams were also the only ones to submit open submissions. The UniMelb-NLP team tried different classification methods and features (including delexical"
W14-5307,W13-1706,0,0.0421907,"own (2013) reports results on a system trained to recognize more than 1,100 languages 2 http://corporavm.uni-koeln.de/vardial/sharedtask.html 58 Proceedings of the First Workshop on Applying NLP Tools to Similar Languages, Varieties and Dialects, pages 58–67, Dublin, Ireland, August 23 2014. decade in which they were published. Other related shared tasks include the ALTW 2010 multilingual language identification shared task, a general-purpose language identification task containing data from 74 languages (Baldwin and Lui, 2010) and finally the Native Language Identification (NLI) shared task (Tetreault et al., 2013) where participants were provided English essays written by foreign students of 11 different mother tongues (Blanchard et al., 2013). Participants had to train their systems to identify the native language of the writer of each text. 2 Related Work Among the first studies to investigate the question of discriminating between similar languages is the study published by Ranaivo-Malanc¸on (2006). The author presents a semi-supervised model to distinguish between Indonesian and Malay, two closely related languages from the Austronesian family represented in the DSL shared task. The study uses the"
W14-5307,C12-1160,1,0.601337,"Missing"
W14-5307,U10-1003,0,\N,Missing
W14-5307,J14-1006,0,\N,Missing
W14-5307,W14-5318,0,\N,Missing
W15-3022,D11-1033,0,0.0118991,"used to train the translation models, after pre-processing. lation models are presented in Table 1. Figure 1 shows how different segmentation methods affect the vocabulary size; given that linguistic segmentation have larger vocabularies as statistical their contribution to translation models may be at least partially complementary. The two unconstrained parallel datasets are split into three subsets: pseudo in-domain, pseudo outof-domain top and pseudo out-of-domain bottom, henceforth in, outt and outb. We rank the sentence pairs according to bilingual cross-entropy difference on the devset (Axelrod et al., 2011) and calculate the perplexity on the devset of LMs trained on different portions of the top ranked sentences (the top 1/64, 1/32 and so on). The subset for which we obtain the lowest perplexities is kept as in (this was 1/4 for fienwac (403.89 and 3610.95 for English and Finnish, respectively), and 1/16 for osubs (702.45 and 7032.2). The remaining part of each dataset is split in two sequential parts in ranking order of same number of lines, which are kept as outt and outb. The out-of-domain part of osubs is further processed with vocabulary saturation (Lewis and Eetemadi, 2013) in order to ha"
W15-3022,P05-1033,0,0.0336499,"ipt omorfi-factorise.py http://opus.lingfil.uu.se/ 1.0 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0.0 words flatcat hfst comp news shuffled 2014 hfst morph Corpora morfessor europarl Figure 1: Effects of segmentation on unique token counts for Finnish. Translation Models We empirically evaluate several types of SMT systems: phrase-based SMT (Och and Ney, 2004) trained on word forms or morphs as described in Section 3, Factored Models (Koehn and Hoang, 2007) including morphological and suffix information as provided by OMORFI,11 in addition to surface forms, and finally hierarchical phrase-based SMT (Chiang, 2005) as an unsupervised tree-based model. All the systems are trained with M OSES, relying on MGIZA (Gao and Vogel, 2008) for word alignment and MIRA (Watanabe et al., 2007) for tuning. This tuning algorithm was shown to be faster and as efficient as MERT for model core features, as well as a better stability with larger numbers of features (Hasler et al., 2011). In order to compare the individually trained SMT systems, we use the same parallel data for each model, as well as the provided development set to tune the systems. The phrase-based SMT system is augmented with additional features: an Ope"
W15-3022,P14-1129,0,0.147789,"Missing"
W15-3022,P11-1105,0,0.0620347,"d model. All the systems are trained with M OSES, relying on MGIZA (Gao and Vogel, 2008) for word alignment and MIRA (Watanabe et al., 2007) for tuning. This tuning algorithm was shown to be faster and as efficient as MERT for model core features, as well as a better stability with larger numbers of features (Hasler et al., 2011). In order to compare the individually trained SMT systems, we use the same parallel data for each model, as well as the provided development set to tune the systems. The phrase-based SMT system is augmented with additional features: an Operation Sequence Model (OSM) (Durrani et al., 2011) and a Bilingual Neural Language Model (BiNLM) (Devlin et al., 2014), both trained on the parallel data used to learn the phrase-table. All the translation systems also benefit from two additional reordering models, namely a phrase-based model with three different orientations (monotone, swap and discontinuous) and a hierarchical model with four orientations (non merged discontinuous left and right orientations), both trained in a bidirectional way (Koehn et al., 2005; Galley and Manning, 2008). Our constrained systems are trained on the data available for the shared task, while unconstrained"
W15-3022,espla-gomis-etal-2014-comparing,1,0.815616,"Missing"
W15-3022,D08-1089,0,0.0261127,"s. The phrase-based SMT system is augmented with additional features: an Operation Sequence Model (OSM) (Durrani et al., 2011) and a Bilingual Neural Language Model (BiNLM) (Devlin et al., 2014), both trained on the parallel data used to learn the phrase-table. All the translation systems also benefit from two additional reordering models, namely a phrase-based model with three different orientations (monotone, swap and discontinuous) and a hierarchical model with four orientations (non merged discontinuous left and right orientations), both trained in a bidirectional way (Koehn et al., 2005; Galley and Manning, 2008). Our constrained systems are trained on the data available for the shared task, while unconstrained systems are trained with two additional sets of parallel data, the F I E N WAC crawled dataset (cf. Section 2.2) and Open Subtitles, henceforth OSUBS.12 The details about the corpora used to train the trans11 Unique Tokens (M) language and translation models. We rely on the scripts included in the M OSES toolkit (Koehn et al., 2007) and perform the following operations: punctuation normalisation, tokenisation, true-casing and escaping of problematic characters. The truecaser is lexicon-based, t"
W15-3022,W08-0509,0,0.135098,"fst comp news shuffled 2014 hfst morph Corpora morfessor europarl Figure 1: Effects of segmentation on unique token counts for Finnish. Translation Models We empirically evaluate several types of SMT systems: phrase-based SMT (Och and Ney, 2004) trained on word forms or morphs as described in Section 3, Factored Models (Koehn and Hoang, 2007) including morphological and suffix information as provided by OMORFI,11 in addition to surface forms, and finally hierarchical phrase-based SMT (Chiang, 2005) as an unsupervised tree-based model. All the systems are trained with M OSES, relying on MGIZA (Gao and Vogel, 2008) for word alignment and MIRA (Watanabe et al., 2007) for tuning. This tuning algorithm was shown to be faster and as efficient as MERT for model core features, as well as a better stability with larger numbers of features (Hasler et al., 2011). In order to compare the individually trained SMT systems, we use the same parallel data for each model, as well as the provided development set to tune the systems. The phrase-based SMT system is augmented with additional features: an Operation Sequence Model (OSM) (Durrani et al., 2011) and a Bilingual Neural Language Model (BiNLM) (Devlin et al., 2014"
W15-3022,D07-1091,0,0.013807,"ters of the word alignment, phrase extraction and decoding algorithms have not been modified to take into account the nature of the segmented data. 4.1 12 using the script omorfi-factorise.py http://opus.lingfil.uu.se/ 1.0 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0.0 words flatcat hfst comp news shuffled 2014 hfst morph Corpora morfessor europarl Figure 1: Effects of segmentation on unique token counts for Finnish. Translation Models We empirically evaluate several types of SMT systems: phrase-based SMT (Och and Ney, 2004) trained on word forms or morphs as described in Section 3, Factored Models (Koehn and Hoang, 2007) including morphological and suffix information as provided by OMORFI,11 in addition to surface forms, and finally hierarchical phrase-based SMT (Chiang, 2005) as an unsupervised tree-based model. All the systems are trained with M OSES, relying on MGIZA (Gao and Vogel, 2008) for word alignment and MIRA (Watanabe et al., 2007) for tuning. This tuning algorithm was shown to be faster and as efficient as MERT for model core features, as well as a better stability with larger numbers of features (Hasler et al., 2011). In order to compare the individually trained SMT systems, we use the same paral"
W15-3022,2005.iwslt-1.8,0,0.121773,"t to tune the systems. The phrase-based SMT system is augmented with additional features: an Operation Sequence Model (OSM) (Durrani et al., 2011) and a Bilingual Neural Language Model (BiNLM) (Devlin et al., 2014), both trained on the parallel data used to learn the phrase-table. All the translation systems also benefit from two additional reordering models, namely a phrase-based model with three different orientations (monotone, swap and discontinuous) and a hierarchical model with four orientations (non merged discontinuous left and right orientations), both trained in a bidirectional way (Koehn et al., 2005; Galley and Manning, 2008). Our constrained systems are trained on the data available for the shared task, while unconstrained systems are trained with two additional sets of parallel data, the F I E N WAC crawled dataset (cf. Section 2.2) and Open Subtitles, henceforth OSUBS.12 The details about the corpora used to train the trans11 Unique Tokens (M) language and translation models. We rely on the scripts included in the M OSES toolkit (Koehn et al., 2007) and perform the following operations: punctuation normalisation, tokenisation, true-casing and escaping of problematic characters. The tr"
W15-3022,P07-2045,0,0.00899697,"d a hierarchical model with four orientations (non merged discontinuous left and right orientations), both trained in a bidirectional way (Koehn et al., 2005; Galley and Manning, 2008). Our constrained systems are trained on the data available for the shared task, while unconstrained systems are trained with two additional sets of parallel data, the F I E N WAC crawled dataset (cf. Section 2.2) and Open Subtitles, henceforth OSUBS.12 The details about the corpora used to train the trans11 Unique Tokens (M) language and translation models. We rely on the scripts included in the M OSES toolkit (Koehn et al., 2007) and perform the following operations: punctuation normalisation, tokenisation, true-casing and escaping of problematic characters. The truecaser is lexicon-based, trained on all the monolingual and parallel data. In addition, we remove sentence pairs from the parallel corpora where either side is longer than 80 tokens. Corpus Europarl v8 fienwac.in fienwac.outt fienwac.outb osubs.in osubs.outt osubs.outb Sentences (k) Words (M) Finnish English Constrained System 1,901.1 36.5 Unconstrained System 640.1 9.2 838.9 12.5 838.9 13.9 492.2 3.6 1,169.6 8.8 1,169.6 7.8 50.9 13.6 18.1 18.1 5.6 14.4 13."
W15-3022,2005.mtsummit-papers.11,0,0.134429,"ne translation (SMT) systems submitted by the Abu-MaTran project for the WMT 2015 translation task. The language pair concerned is Finnish–English with a strong focus on the English-to-Finnish direction. The Finnish language is newly introduced this year as a particular translation challenge due to its rich morphology and to the lack of resources available, compared to e.g. English or French. Morphologically rich languages, and especially Finnish, are known to be difficult to translate using phrase-based SMT systems mainly because of the large diversity of word forms leading to data scarcity (Koehn, 2005). We assume that data acquisition and morphological segmentation should contribute to decrease the out-of-vocabulary rate and thus improve the performance of SMT. To gather additional data, we decide to build on previous work conducted in the Abu-MaTran project and crawl the Web looking for monolingual and parallel corpora (Toral et al., 2014). In addition, morphological segmentation of Finnish is used in our systems as pre- and post-processing steps. Four segmentation methods are proposed in this paper, two unsupervised and two rule-based. Both constrained and unconstrained translation system"
W15-3022,W13-2235,0,0.0133834,"nce on the devset (Axelrod et al., 2011) and calculate the perplexity on the devset of LMs trained on different portions of the top ranked sentences (the top 1/64, 1/32 and so on). The subset for which we obtain the lowest perplexities is kept as in (this was 1/4 for fienwac (403.89 and 3610.95 for English and Finnish, respectively), and 1/16 for osubs (702.45 and 7032.2). The remaining part of each dataset is split in two sequential parts in ranking order of same number of lines, which are kept as outt and outb. The out-of-domain part of osubs is further processed with vocabulary saturation (Lewis and Eetemadi, 2013) in order to have a more efficient and compact system (Rubino et al., 2014). We traverse the sentence pairs in the order they are ranked 187 Corpus Europarl v8 News Commentary v10 News Shuffled 2007 2008 2009 2010 2011 2012 2013 2014 Gigaword 5th Sentences (k) Words (M) 2,218.2 344.9 59.9 8.6 3 782.5 12 954.5 14 680.0 6 797.2 15 437.7 14 869.7 21 688.4 28 221.3 28,178.1 90.2 308.1 347.0 157.8 358.1 345.5 495.2 636.6 4,831.5 Corpus Constrained System News Shuffle 2014 1,378.8 Unconstrained System FiWaC 146,557.4 System and filter out those for which we have seen already each 1-gram at least 10"
W15-3022,W14-0405,1,0.869044,"Missing"
W15-3022,J04-4002,0,0.0271016,"n form of automake scriptlets at http:// github.com/flammie/autostuff-moses-smt/. 10 The parameters of the word alignment, phrase extraction and decoding algorithms have not been modified to take into account the nature of the segmented data. 4.1 12 using the script omorfi-factorise.py http://opus.lingfil.uu.se/ 1.0 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0.0 words flatcat hfst comp news shuffled 2014 hfst morph Corpora morfessor europarl Figure 1: Effects of segmentation on unique token counts for Finnish. Translation Models We empirically evaluate several types of SMT systems: phrase-based SMT (Och and Ney, 2004) trained on word forms or morphs as described in Section 3, Factored Models (Koehn and Hoang, 2007) including morphological and suffix information as provided by OMORFI,11 in addition to surface forms, and finally hierarchical phrase-based SMT (Chiang, 2005) as an unsupervised tree-based model. All the systems are trained with M OSES, relying on MGIZA (Gao and Vogel, 2008) for word alignment and MIRA (Watanabe et al., 2007) for tuning. This tuning algorithm was shown to be faster and as efficient as MERT for model core features, as well as a better stability with larger numbers of features (Ha"
W15-3022,C14-1111,0,0.168889,"Missing"
W15-3022,W13-2506,1,0.831045,"precisionoriented. In this first step, a large amount of potentially parallel data is obtained by post-processing data collected with a TLD crawl, which is not primarily aimed at finding parallel data. To make use of this resource in a more efficient way, we re-crawl some of the most promising web sites (we call them multilingual hotspots) with the ILSP-FC crawler specialised in locating parallel documents during crawling. According to Espl`a-Gomis et al. (2014), B ITEXTOR and ILSP-FC have shown to be complementary, and combining both tools leads to a larger amount of parallel data. ILSP-FC (Papavassiliou et al., 2013) is a modular crawling system allowing to easily acquire domain-specific and generic corpora from the Web.5 This crawler includes a de-duplicator which checks all documents in a pairwise manner to identify near-duplicates. This is achieved by comparing the quantised word frequencies and the paragraphs of each pair of candidate duplicate documents. A document-pair detector also examines each document in the same manner and identifies pairs of documents that could be considered parallel. The main methods used by the pair detector are URL similarity, co-occurrences of images with the same filenam"
W15-3022,P02-1040,0,0.096443,"47 0.830 0.828 0.819 0.864 0.849 Combination 14.61 0.786 13.54 0.801 Table 4: Results obtained on the development and test sets for the constrained English-to-Finnish translation task. Best individual system in bold. 2010) using default settings, except for the beam size (set to 1, 500) and radius (5 for Finnish and 7 for English), following empirical results obtained on the development set. 5.1 Constrained Results Individual systems trained on the provided data are evaluated before being combined. The results obtained for the English-to-Finnish direction are presented in Table 4.13 The BLEU (Papineni et al., 2002) and TER (Snover et al., 2006) scores obtained by the system trained on compoundsegmented data (HFST Comp) show a positive impact of this method on SMT according to the development set, compared to the other individual systems. The unsupervised segmentation methods do not improve over phrase-based SMT, while the hierarchical model shows an interesting reduction of the TER score compared to a classic phrasebased approach. On the test set, the use of inflectional morph segments as well as compounds (HFST Morph) leads to the best results for the individual systems on both evaluation metrics. The"
W15-3022,W15-1844,1,0.795965,"section: morfessor produces 1-best segmentation: and ‘Kun→←ta→←liito→←ksen selvitt¨a→←misess¨a’ and flatcat ‘Kun→←tali→←itoksen selvitt¨amis→←ess¨a’ 3.1 3.3 3.2 Rule-based Segmentation Rule-based morphological segmentation is based on linguistically motivated computational descriptions of the morphology by dividing the word-forms into morphs (minimal segments carrying semantic or syntactic meaning). The rule-based approach to morphological segmentation uses a morphological dictionary of words and an implementation of the morphological grammar to analyse word-forms. In our case, we use OMORFI (Pirinen, 2015), an open-source implementation of the Finnish morphology.8 OMORFI’s segmentation produces named segment boundaries: stem, inflection, derivation, compound-word and other etymological. The two variants of rule-based segmentation we use are based on selection of the boundary points: compound segmentation uses compound segments and discards the rest (referred in tables and figures to as HFST Comp), and morph segmentation uses compound and Unsupervised Segmentation Segments in the SMT Pipeline The segmented data is used exactly as the wordform-based data during training, tuning and testing of the"
W15-3022,P13-2121,0,0.0253168,"Missing"
W15-3022,W14-3319,1,0.888215,"Missing"
W15-3022,E12-1055,0,0.0141743,".2 636.6 4,831.5 Corpus Constrained System News Shuffle 2014 1,378.8 Unconstrained System FiWaC 146,557.4 System and filter out those for which we have seen already each 1-gram at least 10 times. This results in a reduction of 3.2x on the number of sentence pairs (from 7.3M to 2.3M ) and 2.6x on the number of words (from 114M to 44M ). The resulting parallel datasets (7 in total: Europarl and 3 sets for each fienwac and osubs) are used individually to train translation and reordering models before being combined by linear interpolation based on perplexity minimisation on the development set. (Sennrich, 2012) Language Models All the Language Models (LM) used in our experiments are 5-grams modified Kneser-Ney smoothed LMs trained using KenLM (Heafield et al., 2013). For the constrained setup, the Finnish and the English LMs are trained following two different approaches. The English LM is trained on the concatenation of all available corpora while the Finnish LM is obtained by linearly interpolating individually trained LMs based on each corpus. The weights given to each individual LM is calculated by minimising the perplexity obtained on the development set. For the unconstrained setup, the Finnis"
W15-3022,2006.amta-papers.25,0,0.0135668,"Combination 14.61 0.786 13.54 0.801 Table 4: Results obtained on the development and test sets for the constrained English-to-Finnish translation task. Best individual system in bold. 2010) using default settings, except for the beam size (set to 1, 500) and radius (5 for Finnish and 7 for English), following empirical results obtained on the development set. 5.1 Constrained Results Individual systems trained on the provided data are evaluated before being combined. The results obtained for the English-to-Finnish direction are presented in Table 4.13 The BLEU (Papineni et al., 2002) and TER (Snover et al., 2006) scores obtained by the system trained on compoundsegmented data (HFST Comp) show a positive impact of this method on SMT according to the development set, compared to the other individual systems. The unsupervised segmentation methods do not improve over phrase-based SMT, while the hierarchical model shows an interesting reduction of the TER score compared to a classic phrasebased approach. On the test set, the use of inflectional morph segments as well as compounds (HFST Morph) leads to the best results for the individual systems on both evaluation metrics. The combination of these 7 systems"
W15-3022,2014.eamt-1.45,1,0.822092,"Missing"
W15-3022,D07-1080,0,0.0247453,"fessor europarl Figure 1: Effects of segmentation on unique token counts for Finnish. Translation Models We empirically evaluate several types of SMT systems: phrase-based SMT (Och and Ney, 2004) trained on word forms or morphs as described in Section 3, Factored Models (Koehn and Hoang, 2007) including morphological and suffix information as provided by OMORFI,11 in addition to surface forms, and finally hierarchical phrase-based SMT (Chiang, 2005) as an unsupervised tree-based model. All the systems are trained with M OSES, relying on MGIZA (Gao and Vogel, 2008) for word alignment and MIRA (Watanabe et al., 2007) for tuning. This tuning algorithm was shown to be faster and as efficient as MERT for model core features, as well as a better stability with larger numbers of features (Hasler et al., 2011). In order to compare the individually trained SMT systems, we use the same parallel data for each model, as well as the provided development set to tune the systems. The phrase-based SMT system is augmented with additional features: an Operation Sequence Model (OSM) (Durrani et al., 2011) and a Bilingual Neural Language Model (BiNLM) (Devlin et al., 2014), both trained on the parallel data used to learn t"
W15-4944,E06-1032,0,\N,Missing
W15-4944,W10-1751,0,\N,Missing
W15-4944,W14-3301,0,\N,Missing
W15-4944,P02-1040,0,\N,Missing
W15-4944,W14-3319,1,\N,Missing
W15-4944,P11-1105,0,\N,Missing
W15-4944,P10-2041,0,\N,Missing
W15-4944,W05-0909,0,\N,Missing
W15-4944,P07-2045,0,\N,Missing
W15-4944,W07-0718,0,\N,Missing
W15-4944,C14-1111,0,\N,Missing
W15-4944,P12-3005,0,\N,Missing
W15-4944,2012.eamt-1.67,1,\N,Missing
W15-4944,2014.eamt-1.4,1,\N,Missing
W15-4944,W14-3320,0,\N,Missing
W15-4944,2005.mtsummit-papers.11,0,\N,Missing
W15-4944,ljubesic-etal-2014-tweetcat,1,\N,Missing
W15-4944,W15-3036,1,\N,Missing
W15-4944,rubino-etal-2014-quality,1,\N,Missing
W15-4944,W15-3022,1,\N,Missing
W15-4944,W15-4903,1,\N,Missing
W15-4944,2015.eamt-1.4,1,\N,Missing
W15-4944,espla-gomis-etal-2014-comparing,1,\N,Missing
W15-4944,W15-3001,0,\N,Missing
W15-4944,ljubesic-toral-2014-cawac,1,\N,Missing
W15-4944,W14-0405,1,\N,Missing
W15-4944,2005.iwslt-1.8,0,\N,Missing
W15-4944,W16-3421,1,\N,Missing
W15-4944,D07-1078,0,\N,Missing
W15-4944,W08-0509,0,\N,Missing
W15-4944,W11-2123,0,\N,Missing
W15-4944,P14-1129,0,\N,Missing
W15-4944,W16-2347,0,\N,Missing
W15-4944,W16-2375,1,\N,Missing
W15-4944,W16-2367,1,\N,Missing
W15-4944,W16-3423,1,\N,Missing
W15-5301,W13-2408,1,0.900938,"Missing"
W15-5301,agic-etal-2014-croatian,1,0.892533,"Missing"
W15-5301,W14-4203,1,0.874786,"Missing"
W15-5301,C12-2001,0,0.0377519,"Missing"
W15-5301,petrov-etal-2012-universal,0,0.0873193,"Missing"
W15-5301,W06-2920,0,0.0915796,"two sets of experiments. The first one features monolingual parsing of Croatian and the transfer, albeit trivial, of Croatian parsers to Serbian as a target language, while in the second one, we transfer delexicalized parsers from a number of well-resourced languages to Croatian and Serbian as targets in a cross-lingual parsing scenario. 3.1 Data. In the first batch of experiments, we train the parsers on the 3,557 sentences from S E TIMES .H R and Croatian UD, i.e., we omit the development set from all runs. In the second batch, we use the source treebanks from the CoNLL 2006-2007 datasets (Buchholz and Marsi, 2006; Nivre et al., 2007), and the UD version 1.0 release.5 The test sets always remain the same, albeit they do appear in their lexicalized or delexicalized forms: they are the 4 x 100 Croatian and Serbian newswire (NEWS) and Wikipedia (WIKI) samples. Next, we provide a more detailed insight into the experiments as we discuss the results of the two batches. Setup Parser. In all our test runs, we use the graphbased parser of Bohnet (2010).4 It trains and parses very fast, and it records top-level performance across a number of morphologically rich languages (Seddah et al., 2013). Other than that,"
W15-5301,berovic-etal-2012-croatian,1,0.884436,"Missing"
W15-5301,P11-1061,0,0.0311314,"or a similar language—say, Croatian—exist within an uniform representations framework such as UD. This work opened up a cross-lingual parsing research avenue that addresses issues such as multisource transfer, in which multiple source treebanks are combined to improve target language parsing (McDonald et al., 2011), or annotation projection, in which the trees are transferred via parallel corpora and parsers trained on the projections (Tiedemann, 2014). Apart from dependency parsing, this line of work also includes the developments in cross-lingual POS tagging, mainly drawing from the work of Das and Petrov (2011), even if seeded much earlier through the seminal work of Yarowsky et al. (2001). Most of this work, however, does not include the under-resourced SEE languages, and thus we stress that topic in particular in our paper. We introduce a new dependency treebank for Croatian within the Universal Dependencies framework. We construct it on top of the S ETIMES .H R corpus, augmenting the resource by additional part-of-speech and dependency-syntactic annotation layers adherent to the framework guidelines. In this contribution, we outline the treebank design choices, and we use the resource to benchmar"
W15-5301,de-marneffe-etal-2014-universal,0,0.0629764,"Missing"
W15-5301,C14-1175,0,0.0265215,"accuracies. In short, their research indicates that enabling POS tagging and dependency parsing for, e.g., Macedonian would largely benefit should a treebank for a similar language—say, Croatian—exist within an uniform representations framework such as UD. This work opened up a cross-lingual parsing research avenue that addresses issues such as multisource transfer, in which multiple source treebanks are combined to improve target language parsing (McDonald et al., 2011), or annotation projection, in which the trees are transferred via parallel corpora and parsers trained on the projections (Tiedemann, 2014). Apart from dependency parsing, this line of work also includes the developments in cross-lingual POS tagging, mainly drawing from the work of Das and Petrov (2011), even if seeded much earlier through the seminal work of Yarowsky et al. (2001). Most of this work, however, does not include the under-resourced SEE languages, and thus we stress that topic in particular in our paper. We introduce a new dependency treebank for Croatian within the Universal Dependencies framework. We construct it on top of the S ETIMES .H R corpus, augmenting the resource by additional part-of-speech and dependenc"
W15-5301,N13-1070,0,0.0174332,".H R scheme is inherently harder to parse, since it plateaus for both POS feature sets, while UD benefits from the change (back) to UPOS. The first observation is unsurprising given that UPOS differentiates, e.g., between main and auxiliary verbs, or common and proper nouns, while MTE4 POS does not. The second observation is much more interesting, especially given the syntactic tagset differences, as there are only 15 tags in S ETIMES .H R, and 39 in Croatian UD. The result seems to indicate that UD outperforms S ETIMES .H R without sacrificing the expressivity. However, we do note—following Elming et al. (2013)—that our evaluation is intrinsic, and that the two treebanks should be compared on downstream tasks that require parses as input. 5 3.3 ity, as we know from a large body of related work from McDonald et al. (2011) on. In contrast to the CoNLL scores, the UD parsers perform much better, and in much more accordance with our typological intuitions. The best two parsers are trained on Bulgarian and Czech data, the latter one scoring a notable 69.9 and 71.9 points UAS on Croatian and Serbian. The LAS scores are expectedly much lower, and the accuracies are consistent with related work (McDonald et"
W15-5301,H01-1035,0,0.0088822,"mework such as UD. This work opened up a cross-lingual parsing research avenue that addresses issues such as multisource transfer, in which multiple source treebanks are combined to improve target language parsing (McDonald et al., 2011), or annotation projection, in which the trees are transferred via parallel corpora and parsers trained on the projections (Tiedemann, 2014). Apart from dependency parsing, this line of work also includes the developments in cross-lingual POS tagging, mainly drawing from the work of Das and Petrov (2011), even if seeded much earlier through the seminal work of Yarowsky et al. (2001). Most of this work, however, does not include the under-resourced SEE languages, and thus we stress that topic in particular in our paper. We introduce a new dependency treebank for Croatian within the Universal Dependencies framework. We construct it on top of the S ETIMES .H R corpus, augmenting the resource by additional part-of-speech and dependency-syntactic annotation layers adherent to the framework guidelines. In this contribution, we outline the treebank design choices, and we use the resource to benchmark dependency parsing of Croatian and Serbian. We also experiment with cross-ling"
W15-5301,N13-1013,0,0.0236449,"companied by cross-domain test sets for Croatian and Serbian, ii) a set of experiments with parsing the two languages within the UD framework, and iii) cross-lingual parsing experiments targeting Croatian and Serbian by source models from two sets of 10 treebanks. We make our datasets available under free-culture licensing.3 2 text. While the usefulness of this particular approach in contrast to opting for an entirely different text sample could be argued, our decision was motivated by i) facilitating empirical comparability across different annotation schemes, and by ii) the line of work by Johansson (2013) with combining diverse treebanks for improved dependency parsing, which we wish to explore in future work focusing on sharing parsers between closely related languages. Treebank UD requires adherence to POS tagset, dependency attachment, and edge labeling guidelines, as well as to the universal morphological feature specifications, the inclusion of which is at this point not mandatory. We provide an UD treebank for Croatian, implementing all the annotation layers. 2.1 2.2 S ETIMES .H R implements the Multext East version 4 morphosyntactic tagset (MTE4) (Erjavec, 2012). We manually convert it"
W15-5301,D11-1006,0,0.526441,"uation of dependency parsers, and ii) facilitate typologically motivated transfer of dependency parsers to under-resourced languages with improved accuracies. In short, their research indicates that enabling POS tagging and dependency parsing for, e.g., Macedonian would largely benefit should a treebank for a similar language—say, Croatian—exist within an uniform representations framework such as UD. This work opened up a cross-lingual parsing research avenue that addresses issues such as multisource transfer, in which multiple source treebanks are combined to improve target language parsing (McDonald et al., 2011), or annotation projection, in which the trees are transferred via parallel corpora and parsers trained on the projections (Tiedemann, 2014). Apart from dependency parsing, this line of work also includes the developments in cross-lingual POS tagging, mainly drawing from the work of Das and Petrov (2011), even if seeded much earlier through the seminal work of Yarowsky et al. (2001). Most of this work, however, does not include the under-resourced SEE languages, and thus we stress that topic in particular in our paper. We introduce a new dependency treebank for Croatian within the Universal De"
W15-5306,W14-0405,1,0.750949,"Missing"
W15-5401,Y08-1042,0,0.153038,"iscusses related work, Section 3 describes the general setup of the task, Section 4 presents the results of the competition, Section 5 summarizes the approaches used by the participants, and Section 6 offers conclusions. 2 Related Work 2.2 Language identification has attracted a lot of research attention in recent years, covering a number of similar languages and language varieties such as Malay and Indonesian (RanaivoMalanc¸on, 2006), Persian and Dari (Malmasi and Dras, 2015a), Brazilian and European Portuguese (Zampieri and Gebre, 2012), varieties of Mandarin in China, Taiwan and Singapore (Huang and Lee, 2008), and English varieties (Lui and Cook, 2013), among others. This interest has eventually given rise to special shared tasks, which allowed researchers to compare and benchmark various approaches on common standard datasets. Below we will describe some of these shared tasks, including the first edition of the DSL task. 2.1 The First Edition of the DSL Task For the first edition of the task, we compiled the DSL Corpus Collection (Tan et al., 2014), or DSLCC v.1.0, which included excerpts from journalistic texts from sources such as the SETimes Corpus1 (Tyers and Alperen, 2010), HC Corpora2 and t"
W15-5401,W15-5408,0,0.229488,"Missing"
W15-5401,W15-5410,0,0.0986295,"sk track, we further made available DSLCC v2.1, which extended DSLCC v2.0 with Mexican Spanish and Macanese Portuguese data. 6 The script we used to substitute named entities with placeholders is available here: https://github.com/ Simdiva/DSL-Task/blob/master/blindNE.py 4 Date May 20, 2015 June 22, 2015 June 24, 2015 June 26, 2015 July 20, 2015 Table 3: The DSL 2015 Shared Task schedule. 4 Team BOICEV BRUNIBP INRIA MAC MMS* NLEL NRC OSEVAL PRHLT SUKI Total Closed (Normal) X X X X X X X X X 9 Closed (No NEs) X X X X X X X 7 Open (Normal) X X X 3 Open (No NEs) X X X 3 System Description Paper (Bobicev, 2015) ´ et al., 2015) (Acs (Malmasi and Dras, 2015b) (Zampieri et al., 2015) (Fabra-Boluda et al., 2015) (Goutte and L´eger, 2015) (Franco-Salvador et al., 2015) (Jauhiainen et al., 2015a) 8 Table 4: The participating teams in the DSL 2015 Shared Task. 4 Rank 1 2-3 2-3 4 5 6 7 8 9 Results In this section, we present the results of the 2nd edition of the DSL shared task.7 Most of the participating teams used DSLCC v2.0 only, and thus took part in the closed submission track. Yet, three of the teams collected additional data or used DSLCC v1.0, and thereby participated in the open submission. 4.1 Sub"
W15-5401,D14-1069,0,0.183053,"Missing"
W15-5401,W14-5317,0,0.204271,"Missing"
W15-5401,W15-5409,0,0.310823,"Spanish and Macanese Portuguese data. 6 The script we used to substitute named entities with placeholders is available here: https://github.com/ Simdiva/DSL-Task/blob/master/blindNE.py 4 Date May 20, 2015 June 22, 2015 June 24, 2015 June 26, 2015 July 20, 2015 Table 3: The DSL 2015 Shared Task schedule. 4 Team BOICEV BRUNIBP INRIA MAC MMS* NLEL NRC OSEVAL PRHLT SUKI Total Closed (Normal) X X X X X X X X X 9 Closed (No NEs) X X X X X X X 7 Open (Normal) X X X 3 Open (No NEs) X X X 3 System Description Paper (Bobicev, 2015) ´ et al., 2015) (Acs (Malmasi and Dras, 2015b) (Zampieri et al., 2015) (Fabra-Boluda et al., 2015) (Goutte and L´eger, 2015) (Franco-Salvador et al., 2015) (Jauhiainen et al., 2015a) 8 Table 4: The participating teams in the DSL 2015 Shared Task. 4 Rank 1 2-3 2-3 4 5 6 7 8 9 Results In this section, we present the results of the 2nd edition of the DSL shared task.7 Most of the participating teams used DSLCC v2.0 only, and thus took part in the closed submission track. Yet, three of the teams collected additional data or used DSLCC v1.0, and thereby participated in the open submission. 4.1 Submitted Runs Accuracy 95.54 95.24 95.24 94.67 94.14 93.66 92.74 83.91 64.04 Table 5: Closed submissi"
W15-5401,W15-5403,0,0.148019,"used to substitute named entities with placeholders is available here: https://github.com/ Simdiva/DSL-Task/blob/master/blindNE.py 4 Date May 20, 2015 June 22, 2015 June 24, 2015 June 26, 2015 July 20, 2015 Table 3: The DSL 2015 Shared Task schedule. 4 Team BOICEV BRUNIBP INRIA MAC MMS* NLEL NRC OSEVAL PRHLT SUKI Total Closed (Normal) X X X X X X X X X 9 Closed (No NEs) X X X X X X X 7 Open (Normal) X X X 3 Open (No NEs) X X X 3 System Description Paper (Bobicev, 2015) ´ et al., 2015) (Acs (Malmasi and Dras, 2015b) (Zampieri et al., 2015) (Fabra-Boluda et al., 2015) (Goutte and L´eger, 2015) (Franco-Salvador et al., 2015) (Jauhiainen et al., 2015a) 8 Table 4: The participating teams in the DSL 2015 Shared Task. 4 Rank 1 2-3 2-3 4 5 6 7 8 9 Results In this section, we present the results of the 2nd edition of the DSL shared task.7 Most of the participating teams used DSLCC v2.0 only, and thus took part in the closed submission track. Yet, three of the teams collected additional data or used DSLCC v1.0, and thereby participated in the open submission. 4.1 Submitted Runs Accuracy 95.54 95.24 95.24 94.67 94.14 93.66 92.74 83.91 64.04 Table 5: Closed submission results for test set A. A total of 24 teams subscribed"
W15-5401,I11-1062,0,0.0379892,"Missing"
W15-5401,P12-3005,0,0.0283147,"Missing"
W15-5401,W13-1728,1,0.611887,"e group, and then chooses between languages or varieties within this group. The team achieved very strong results this year, ranking second in the closed submission on test set A, third on test set B, and first in the open submission on both test sets A and B. Two other participants used two-stage classification: NLEL (Fabra-Boluda et al., 2015) and ´ et al., 2015). BRUniBP (Acs The MMS team experimented with three approaches (Zampieri et al., 2015), and their best run combined TF.IDF weighting and an SVM classifier, which was previously successfully applied to native language identification (Gebre et al., 2013). The SUKI team (Jauhiainen et al., 2015a) used token-based backoff, which was previously applied to general-purpose language identification (Jauhiainen et al., 2015b). The BOBICEV team applied prediction by partial matching, which had not been used for this task before (Bobicev, 2015). Finally, the PRHLT team (Franco-Salvador et al., 2015) used word and sentence vectors, which is to our knowledge the first attempt to apply them to discriminating between similar languages. Table 7: Open submission results for test set A. This could be related to the availability of DSLCC v1.0 as an obvious add"
W15-5401,U13-1003,0,0.339119,"e general setup of the task, Section 4 presents the results of the competition, Section 5 summarizes the approaches used by the participants, and Section 6 offers conclusions. 2 Related Work 2.2 Language identification has attracted a lot of research attention in recent years, covering a number of similar languages and language varieties such as Malay and Indonesian (RanaivoMalanc¸on, 2006), Persian and Dari (Malmasi and Dras, 2015a), Brazilian and European Portuguese (Zampieri and Gebre, 2012), varieties of Mandarin in China, Taiwan and Singapore (Huang and Lee, 2008), and English varieties (Lui and Cook, 2013), among others. This interest has eventually given rise to special shared tasks, which allowed researchers to compare and benchmark various approaches on common standard datasets. Below we will describe some of these shared tasks, including the first edition of the DSL task. 2.1 The First Edition of the DSL Task For the first edition of the task, we compiled the DSL Corpus Collection (Tan et al., 2014), or DSLCC v.1.0, which included excerpts from journalistic texts from sources such as the SETimes Corpus1 (Tyers and Alperen, 2010), HC Corpora2 and the Leipzig Corpora Collection (Biemann et al"
W15-5401,W15-5413,0,0.538185,"Missing"
W15-5401,W14-5315,0,0.370837,"Missing"
W15-5401,W14-5316,0,0.374689,"Missing"
W15-5401,W15-5407,0,0.1816,"ke the task more challenging and less dependent on the text topic and domain. The remainder of this paper is organized as follows: Section 2 discusses related work, Section 3 describes the general setup of the task, Section 4 presents the results of the competition, Section 5 summarizes the approaches used by the participants, and Section 6 offers conclusions. 2 Related Work 2.2 Language identification has attracted a lot of research attention in recent years, covering a number of similar languages and language varieties such as Malay and Indonesian (RanaivoMalanc¸on, 2006), Persian and Dari (Malmasi and Dras, 2015a), Brazilian and European Portuguese (Zampieri and Gebre, 2012), varieties of Mandarin in China, Taiwan and Singapore (Huang and Lee, 2008), and English varieties (Lui and Cook, 2013), among others. This interest has eventually given rise to special shared tasks, which allowed researchers to compare and benchmark various approaches on common standard datasets. Below we will describe some of these shared tasks, including the first edition of the DSL task. 2.1 The First Edition of the DSL Task For the first edition of the task, we compiled the DSL Corpus Collection (Tan et al., 2014), or DSLCC"
W15-5401,C12-1160,1,0.783877,"Missing"
W15-5401,tiedemann-2012-parallel,1,0.851577,"Missing"
W15-5401,W14-5314,0,0.282427,"Missing"
W15-5401,xia-etal-2010-problems,0,0.0651996,"Missing"
W15-5401,W14-5904,0,0.0750336,"Missing"
W15-5401,W14-5307,1,0.737281,"Missing"
W15-5401,W15-5411,1,0.87912,"Missing"
W15-5401,W14-2505,0,0.0277162,"raining and development subsets, and we further prepared two test sets, as described in Section 3.3 below. As in 2014, teams could make two types of submissions (for each team, we allowed up to three runs per submission type; in the official ranking, we included the run with the highest score only): • Closed submission: Using only the DSLCC v2.0 for training. • Open submission: Using any dataset other than DSLCC v2.0 for training.3 3.2 The Unshared Task Track Along with the Shared Task, this year we proposed an Unshared Task track inspired by the unshared task in PoliInformatics held in 2014 (Smith et al., 2014). For this track, teams were allowed to use any version of DSLCC to investigate differences between similar languages and language varieties using NLP methods. We were interested in studying questions like these: • Are there fundamental grammatical differences in a language group? • What are the most distinctive lexical choices for each language? • Which text representation is most suitable to investigate language variation? • What is the impact of lexical and grammatical variation on NLP applications? Although eleven teams subscribed for the Unshared Task track, none of them ended up submitin"
W15-5401,W14-3907,0,0.0457835,"2015. 2015 Association for Computational Linguistics Another popular research direction has been on language identification on Twitter, which was driven by interest in geolocation prediction for end-user applications (Ljubeˇsi´c and Kranjˇci´c, 2015). This interest has given rise to the TweetLID shared task (Zubiaga et al., 2014), which asked participants to recognize the language of tweet messages, focusing on English and on languages spoken on the Iberian peninsula such as Basque, Catalan, Spanish, and Portuguese. The Shared Task on Language Identification in CodeSwitched Data held in 2014 (Solorio et al., 2014) is another related competition, where the focus was on tweets in which users were mixing two or more languages in the same tweet. First, in order to simulate a real-world language identification scenario, we included in the testing dataset some languages that were not present in the training dataset. Moreover, we included a second test set, where we substituted the named entities with placeholders to make the task more challenging and less dependent on the text topic and domain. The remainder of this paper is organized as follows: Section 2 discusses related work, Section 3 describes the gene"
W15-5401,U10-1003,0,\N,Missing
W15-5401,W14-5318,0,\N,Missing
W16-3421,agic-ljubesic-2014-setimes,1,0.856315,"Missing"
W16-3421,W07-0735,0,0.0313019,"WaC + factored data selection Google Translate data selection + factored 5 BLEU 0.2458 0.2576 0.2673 0.2700 TER METEOR 0.6226 0.2167 0.6060 0.2264 0.5946 0.2321 0.5963 0.2338 Related work Factored models have been deeply studied by Tamchyna and Bojar (2013), who concluded that automatically searching for the best factored model architecture in a given 9 http://translate.google.com Dealing with Data Sparseness in SMT: a Case Study on Croatian 359 language pair is not feasible. Successful application of factored models to different language pairs has been already reported by other authors, like Bojar (2007) and Koehn et al. (2010). Regarding morphological expansion, to the best of our knowledge, the approach by Turchi and Ehrmann (2011) is the only one that addresses the expansion of the TL side of the phrase table. Concerning other ways of adding linguistic information to an SMT system, we refer the reader to the survey by Costa-Juss`a and Farr´us (2014). 6 Conclusions and future work In this paper, we presented a set of strategies on how to leverage existing Croatian linguistic resources to address data sparseness in a general-domain English-to-Croatian SMT system. Applying factored models sho"
W16-3421,W04-3250,0,0.175242,"Missing"
W16-3421,W10-1715,0,0.0212896,"ta selection Google Translate data selection + factored 5 BLEU 0.2458 0.2576 0.2673 0.2700 TER METEOR 0.6226 0.2167 0.6060 0.2264 0.5946 0.2321 0.5963 0.2338 Related work Factored models have been deeply studied by Tamchyna and Bojar (2013), who concluded that automatically searching for the best factored model architecture in a given 9 http://translate.google.com Dealing with Data Sparseness in SMT: a Case Study on Croatian 359 language pair is not feasible. Successful application of factored models to different language pairs has been already reported by other authors, like Bojar (2007) and Koehn et al. (2010). Regarding morphological expansion, to the best of our knowledge, the approach by Turchi and Ehrmann (2011) is the only one that addresses the expansion of the TL side of the phrase table. Concerning other ways of adding linguistic information to an SMT system, we refer the reader to the survey by Costa-Juss`a and Farr´us (2014). 6 Conclusions and future work In this paper, we presented a set of strategies on how to leverage existing Croatian linguistic resources to address data sparseness in a general-domain English-to-Croatian SMT system. Applying factored models showed to be successful. We"
W16-3421,D07-1091,0,0.0564683,"try. Croatian is a highly inflected language and hence it is affected by data sparseness. For instance, adjectives inflect for 3 genders, 2 numbers and 7 cases and the hrLex Croatian inflectional lexicon (Ljubeˇsi´c et al, 2016b) contains 939 unique morphosyntactic description tags. In this paper, we show how we dealt with that problem in a general-domain Englishto-Croatian phrase-based SMT system by leveraging a Croatian inflectional lexicon and adapting existing solutions in the literature, namely factored translation models Dealing with Data Sparseness in SMT: a Case Study on Croatian 355 (Koehn and Hoang, 2007) and morphological expansion (Turchi and Ehrmann, 2011). Sections 2 and 3 respectively describe these solutions, while Section 4 shows that they can be successfully combined with a data selection strategy. The paper ends with a brief description of related approaches and some concluding remarks and future directions. 2 Factored translation models Factored translation models (Koehn and Hoang, 2007) split the translation of words in the translation of different factors (surface forms, lemmas, lexical categories, morphosyntactic information, etc.). Among the different ways these factors can be co"
W16-3421,W07-0733,0,0.0317683,"ed to optimise filtering thresholds. We created new phrase pairs by changing only the TL side of existing phrase pairs, and only for those phrase pairs whose TL side is a single word or a grammatically meaningful phrase. We select, among others, TL phrases that contain a noun, a noun phrase, an adjective, a verb, etc. Then, we generate new phrases with all the possible values of the morphological inflection features not present in English.8 We added the generated phrase pairs to a new phrase table which is combined with the original one at decoding time by means of independent decoding paths (Koehn and Schroeder, 2007). We added morphological expansion to the best factored system described in Section 2.2 and repeated the evaluation. In view of the positive results of reducing translation alternatives by constraining PoS tagging, we also evaluated an alternative morphological expansion strategy in which only noun phrases were expanded. Our expansion rules generate only 3 alternatives for them (for nominative, accusative and instrumental cases), while the number of generated entries is higher for verbal phrases and adjectives. Results displayed in Table 3 show that there is not a clear difference between both"
W16-3421,D12-1088,0,0.0693679,"Missing"
W16-3421,L16-1471,1,0.798463,"Missing"
W16-3421,W14-0405,1,0.820741,"Missing"
W16-3421,D07-1080,0,0.0458578,"or an open-domain system. In particular, we used hrenWaC as parallel corpus (Ljubeˇsi´c et al., 2016a) and hrWaC (Ljubeˇsi´c and Klubiˇcka, 2014) as TL monolingual corpus. The parallel corpus contains 1 166 732 sentences, 32 908 281 English words and 29 199 856 Croatian words. The size of the vocabularies is 605 929 (English) and 888 405 (Croatian): the ratio between them is 1.47, which gives us an idea of the morphological richness of Croatian as compared with English. The monolingual corpus contains 67 403 231 sentences and 1 404 303 868 words. We used Moses5 with the MIRA tuning algorithm (Watanabe et al., 2007). We estimated a 5-gram surface-form LM from the TL monolingual corpus. Our factored system contains an additional MSD LM estimated from the same monolingual corpus. We experimented with orders 3, 5 and 7 and the two tagging alternatives discussed in the previous section. We used KenLM and Knesser-Ney discounting.6 Table 1. Results of the evaluation of factored models. A score in bold means that the system outperforms the plain baseline by a statistically significant margin according to paired bootstrap resampling (Koehn, 2004) (p = 0.05, 1 000 iterations). MSD LM order constrained tagging bas"
W16-3422,cortes-etal-2012-free,0,0.0365767,"Missing"
W16-3422,R11-1018,1,0.895232,"Missing"
W16-3422,P02-1040,0,0.108654,"Missing"
W16-3422,2012.freeopmt-1.6,0,0.0354528,"Translate,6 which reaches a BLEU score of 82.27 in the Serbian-Croatian direction. However, the statistical approach that Google uses, which has also been explored in (Popovi´c et al, 2014) but only using small corpora, is not a feasible option for us, as there are not enough parallel corpora available to train SMT systems that can deal with the minute differences between the two languages without introducing additional noise. Nonetheless, some free linguistic resources were initially available to us: the HBS monolingual dictionary7 built for other Apertium language pairs like HBS-Macedonian (Peradin and Tyers, 2012) and HBS-Slovene (Peradin et al, 2014), the SETimes news corpora of both Croatian and Serbian8 and the hrWaC and srWaC web corpora (Ljubeˇsi´c and Klubiˇcka, 2014). This is always an advantage, as both monolingual and bilingual corpora are extensively used to semiautomatically extract knowledge for Apertium such as frequent non-covered entries, bilingual correspondences, rules, development and test sets, and data needed to train statistical part-of-speech taggers. Considering the amount of available data, coupled with the fact that differences between Croatian and Serbian occur mostly at the l"
W16-3904,R11-1018,1,0.825495,"Missing"
W16-3904,ljubesic-etal-2014-tweetcat,1,0.682135,"Missing"
W16-3904,W13-2411,1,0.897699,"Missing"
W16-3904,C00-2137,0,0.0480892,"Missing"
W16-3904,D11-1120,0,\N,Missing
W16-4801,W16-4821,0,0.0339271,"Missing"
W16-4801,W16-4826,0,0.0502832,"Missing"
W16-4801,W16-4827,0,0.11257,"Missing"
W16-4801,W16-4819,0,0.0808398,"Missing"
W16-4801,W16-4816,0,0.0637155,"Missing"
W16-4801,W15-5410,0,0.238809,"Missing"
W16-4801,W16-4802,0,0.114498,"Missing"
W16-4801,W16-4831,0,0.0913939,"Missing"
W16-4801,W16-4830,0,0.0438028,"Missing"
W16-4801,D14-1154,0,0.0630846,"otivated the organization of shared tasks such as the DSL challenge, which allowed researchers to compare various approaches using the same dataset. Along with the interest in similar languages and language variety identification, we observed substantial interest in applying natural language processing (NLP) methods for the processing of dialectal Arabic with special interest in methods to discriminate between Arabic dialects. Shoufan and Al-Ameri (2015) presented a comprehensive survey on these methods including recent studies on Arabic dialect identification such as (Elfardy and Diab, 2014; Darwish et al., 2014; Zaidan and Callison-Burch, 2014; This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http://creativecommons.org/licenses/by/4.0/ 1 Proceedings of the Third Workshop on NLP for Similar Languages, Varieties and Dialects, pages 1–14, Osaka, Japan, December 12 2016. Tillmann et al., 2014; Malmasi and Dras, 2015a). Methods for Arabic dialect detection present significant overlap with methods proposed for similar language identification. For this reason, in the 2016 edition of the DSL challenge we offered a subtask on Arabic dialect identification"
W16-4801,W16-4828,0,0.0387334,"Missing"
W16-4801,W15-5409,0,0.149587,"Missing"
W16-4801,W16-4829,0,0.0511602,"Missing"
W16-4801,W15-5403,0,0.27462,"Missing"
W16-4801,W16-4822,0,0.0759257,"Missing"
W16-4801,W15-5413,0,0.562658,"Missing"
W16-4801,W16-4823,0,0.0418563,"Missing"
W16-4801,W14-5316,0,0.385305,"Missing"
W16-4801,L16-1284,1,0.849869,"Missing"
W16-4801,W16-4824,0,0.0430542,"Missing"
W16-4801,W16-4817,0,0.0342647,"Missing"
W16-4801,W16-4815,0,0.0384942,"Missing"
W16-4801,Y08-1042,0,0.0574334,"ticipants. Below we present the task setup, the evaluation results, and a brief discussion about the features and learning methods that worked best. More detail about each particular system can be found in the corresponding system description paper, as cited in this report. 2 Related Work Language and dialect identification have attracted a lot of research attention in recent years, covering a number of similar languages and language varieties such as South-Slavic languages (Ljubeˇsi´c et al., 2007), English varieties (Lui and Cook, 2013), varieties of Mandarin in China, Taiwan and Singapore (Huang and Lee, 2008), Malay vs. Indonesian (Ranaivo-Malanc¸on, 2006), Brazilian vs. European Portuguese (Zampieri and Gebre, 2012), and Persian vs. Dari (Malmasi and Dras, 2015a), to mention just a few. The interest in this aspect of language identification has motivated the organization of shared tasks such as the DSL challenge, which allowed researchers to compare various approaches using the same dataset. Along with the interest in similar languages and language variety identification, we observed substantial interest in applying natural language processing (NLP) methods for the processing of dialectal Arabic"
W16-4801,W16-4818,0,0.128184,"Missing"
W16-4801,J16-3005,0,0.112984,"Missing"
W16-4801,W15-5408,0,0.217783,"Missing"
W16-4801,W16-4820,0,0.38437,"Missing"
W16-4801,W14-5317,0,0.0630949,"Missing"
W16-4801,U13-1003,0,0.256503,"size and scope featuring two subtasks and attracting a record number of participants. Below we present the task setup, the evaluation results, and a brief discussion about the features and learning methods that worked best. More detail about each particular system can be found in the corresponding system description paper, as cited in this report. 2 Related Work Language and dialect identification have attracted a lot of research attention in recent years, covering a number of similar languages and language varieties such as South-Slavic languages (Ljubeˇsi´c et al., 2007), English varieties (Lui and Cook, 2013), varieties of Mandarin in China, Taiwan and Singapore (Huang and Lee, 2008), Malay vs. Indonesian (Ranaivo-Malanc¸on, 2006), Brazilian vs. European Portuguese (Zampieri and Gebre, 2012), and Persian vs. Dari (Malmasi and Dras, 2015a), to mention just a few. The interest in this aspect of language identification has motivated the organization of shared tasks such as the DSL challenge, which allowed researchers to compare various approaches using the same dataset. Along with the interest in similar languages and language variety identification, we observed substantial interest in applying natur"
W16-4801,W14-5315,0,0.0614151,"Missing"
W16-4801,W15-5407,1,0.493189,"detail about each particular system can be found in the corresponding system description paper, as cited in this report. 2 Related Work Language and dialect identification have attracted a lot of research attention in recent years, covering a number of similar languages and language varieties such as South-Slavic languages (Ljubeˇsi´c et al., 2007), English varieties (Lui and Cook, 2013), varieties of Mandarin in China, Taiwan and Singapore (Huang and Lee, 2008), Malay vs. Indonesian (Ranaivo-Malanc¸on, 2006), Brazilian vs. European Portuguese (Zampieri and Gebre, 2012), and Persian vs. Dari (Malmasi and Dras, 2015a), to mention just a few. The interest in this aspect of language identification has motivated the organization of shared tasks such as the DSL challenge, which allowed researchers to compare various approaches using the same dataset. Along with the interest in similar languages and language variety identification, we observed substantial interest in applying natural language processing (NLP) methods for the processing of dialectal Arabic with special interest in methods to discriminate between Arabic dialects. Shoufan and Al-Ameri (2015) presented a comprehensive survey on these methods incl"
W16-4801,W16-4814,1,0.85912,"Missing"
W16-4801,W16-4825,0,0.0362109,"Missing"
W16-4801,W14-5314,0,0.0924571,"Missing"
W16-4801,W15-3205,0,0.0361398,"ortuguese (Zampieri and Gebre, 2012), and Persian vs. Dari (Malmasi and Dras, 2015a), to mention just a few. The interest in this aspect of language identification has motivated the organization of shared tasks such as the DSL challenge, which allowed researchers to compare various approaches using the same dataset. Along with the interest in similar languages and language variety identification, we observed substantial interest in applying natural language processing (NLP) methods for the processing of dialectal Arabic with special interest in methods to discriminate between Arabic dialects. Shoufan and Al-Ameri (2015) presented a comprehensive survey on these methods including recent studies on Arabic dialect identification such as (Elfardy and Diab, 2014; Darwish et al., 2014; Zaidan and Callison-Burch, 2014; This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http://creativecommons.org/licenses/by/4.0/ 1 Proceedings of the Third Workshop on NLP for Similar Languages, Varieties and Dialects, pages 1–14, Osaka, Japan, December 12 2016. Tillmann et al., 2014; Malmasi and Dras, 2015a). Methods for Arabic dialect detection present significant overlap with met"
W16-4801,W14-5313,0,0.115089,"of dialectal Arabic with special interest in methods to discriminate between Arabic dialects. Shoufan and Al-Ameri (2015) presented a comprehensive survey on these methods including recent studies on Arabic dialect identification such as (Elfardy and Diab, 2014; Darwish et al., 2014; Zaidan and Callison-Burch, 2014; This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http://creativecommons.org/licenses/by/4.0/ 1 Proceedings of the Third Workshop on NLP for Similar Languages, Varieties and Dialects, pages 1–14, Osaka, Japan, December 12 2016. Tillmann et al., 2014; Malmasi and Dras, 2015a). Methods for Arabic dialect detection present significant overlap with methods proposed for similar language identification. For this reason, in the 2016 edition of the DSL challenge we offered a subtask on Arabic dialect identification. Below, we discuss some related shared tasks including the first two editions of the DSL challenge. 2.1 Related Shared Tasks Several shared tasks related to the DSL task have been organized in recent years. Two examples are the ALTW language identification shared task (Baldwin and Lui, 2010) on general-purpose language identification,"
W16-4801,P11-1122,0,0.0270219,"Missing"
W16-4801,W14-5307,1,0.744553,"Missing"
W16-4801,W15-5411,1,0.882098,"Missing"
W16-4813,2013.iwslt-evaluation.1,0,0.0459683,"Missing"
W16-4813,W16-3422,1,0.61955,"Missing"
W16-4813,J03-1002,0,0.00529792,"12k parallel segments were extracted, and for English-Serbian about 50k. An interesting observation is that although Croatian is generally better supported in terms of publicly available parallel data,6 Serbian is currently better supported for educational parallel texts. As for the out-of-domain corpus, we used the SETimes news corpus (Tyers and Alperen, 2010) since it is relatively large (200k parallel sentences) and clean. Moses set-ups We trained the statistical phrase-based systems using the Moses toolkit (Koehn et al., 2007) with MERT tuning. The word alignments were built with GIZA++ (Och and Ney, 2003) and a 5-gram language model was built with SRILM (Stolcke, 2002). The investigated bilingual training set-ups are: 1. en-hr SEtimes (relatively large clean out-of-domain corpus) 2. en-hr Coursera (small in-domain corpus) 3. en-hr Coursera (small in-domain corpus) + en-sr Coursera (larger in-domain corpus) 4. en-hr Coursera + en-hr’ Coursera 5. en-hr SEtimes + en-hr Coursera + en-hr’ Coursera 6 http://opus.lingfil.uu.se/ 100 sentences Training Dev Test 1) setimes 2) coursera 3) 2+coursera en-sr 4) 2+coursera en-hr’ 5) 1+4 coursera coursera 206k 12k 62k 62k 268k 2935 2091 running words en hr 4."
W16-4813,P02-1040,0,0.0956083,"Missing"
W16-4813,W14-4210,1,0.479533,"Missing"
W16-4813,W15-3049,1,0.871794,"Missing"
W16-4813,W16-3421,1,0.40739,"Missing"
W16-4813,2014.eamt-1.45,0,0.669565,"Missing"
W16-4813,W16-3423,0,0.0385996,"Missing"
W17-1201,W16-4802,0,0.127115,"character ngrams and a Na”ive Bayes classifier. The system followed the work of the system submitted to the DSL 2016 by Barbaresi (2016). • CECL: The system uses a two-step approach as in (Goutte et al., 2014). The first step identifies the language group using an SVM classifier with a linear kernel trained on character n-grams (1-4) that occur at least 100 times in the dataset weighted by Okapi BM25 (Robertson et al., 1995). The second step discriminates between each language within the group using a set of SVM classifiers trained • tubasfs: Following the success of tubasfs at DSL 2016 (C¸o¨ ltekin and Rama, 2016), which was ranked first in the closed training track, this year’s tubasfs submission used a linear SVM classifier. The system used both characters and words as features, and carefully optimized hyperparameters: n-gram size and margin/regularization parameter for SVM. 5 In 2016 ADI and DSL were organized under the name DSL shared task, and ADI was run as a sub-task. 4 • gauge: This team submitted a total of three runs. Run 1 used an SVM classifier with character n-grams (2–6), run 2 (their best run) used logistic regression trained using character n-grams (1–6), and run 3 used hard voting of t"
W17-1201,W17-1221,0,0.532877,"of related languages. 1 Proceedings of the Fourth Workshop on NLP for Similar Languages, Varieties and Dialects, pages 1–15, c Valencia, Spain, April 3, 2017. 2017 Association for Computational Linguistics Team ahaqst bayesline CECL cic ualg Citius Ixa Imaxin CLUZH CUNI deepCybErNet gauge Helsinki-CLP MAZA (ADI) MAZA (GDI) mm lct qcri mit SUKI timeflow tubasfs unibuckernel XAC Bayesline Total DSL X X X X X X ADI X X X X X 11 CLP X X X X X X GDI X X X X X X X X X X 10 6 X X X 3 System Description Paper (Hanani et al., 2017) – (Bestgen, 2017) (G´omez-Adorno et al., 2017) (Gamallo et al., 2017) (Clematide and Makarov, 2017) (Rosa et al., 2017) – – (Tiedemann, 2017) (Malmasi and Zampieri, 2017a) (Malmasi and Zampieri, 2017b) (Medvedeva et al., 2017) – (Jauhiainen et al., 2017) (Criscuolo and Aluisio, 2017) (C ¸ o¨ ltekin and Rama, 2017) (Ionescu and Butnaru, 2017) (Barbaresi, 2017) 15 Table 1: The teams that participated in the VarDial’2017 Evaluation Campaign. 1.2 Participating Teams The fourth edition of the DSL shared task was motivated by the success of the previous editions and by the growing interest of the research community in the identification of dialects and similar languages, as evidenced by recent pu"
W17-1201,W17-1215,0,0.0474398,"Missing"
W17-1201,W17-1213,0,0.0702486,"pairs include a triple of related languages. 1 Proceedings of the Fourth Workshop on NLP for Similar Languages, Varieties and Dialects, pages 1–15, c Valencia, Spain, April 3, 2017. 2017 Association for Computational Linguistics Team ahaqst bayesline CECL cic ualg Citius Ixa Imaxin CLUZH CUNI deepCybErNet gauge Helsinki-CLP MAZA (ADI) MAZA (GDI) mm lct qcri mit SUKI timeflow tubasfs unibuckernel XAC Bayesline Total DSL X X X X X X ADI X X X X X 11 CLP X X X X X X GDI X X X X X X X X X X 10 6 X X X 3 System Description Paper (Hanani et al., 2017) – (Bestgen, 2017) (G´omez-Adorno et al., 2017) (Gamallo et al., 2017) (Clematide and Makarov, 2017) (Rosa et al., 2017) – – (Tiedemann, 2017) (Malmasi and Zampieri, 2017a) (Malmasi and Zampieri, 2017b) (Medvedeva et al., 2017) – (Jauhiainen et al., 2017) (Criscuolo and Aluisio, 2017) (C ¸ o¨ ltekin and Rama, 2017) (Ionescu and Butnaru, 2017) (Barbaresi, 2017) 15 Table 1: The teams that participated in the VarDial’2017 Evaluation Campaign. 1.2 Participating Teams The fourth edition of the DSL shared task was motivated by the success of the previous editions and by the growing interest of the research community in the identification of dialects and similar langua"
W17-1201,W13-1728,1,0.0339111,"rovided lexical features. This year, we added a multi-model aspect to the task by further providing acoustic features. The system description paper of CECL (Bestgen, 2017) provides some interesting insights about the DSL task. First, they found out that BM25 weighting, which was previously applied to native language identification (NLI) (Wang et al., 2016), worked better than using TF.IDF. They further highlighted the similarity between similar language identification and NLI as evidenced by a number of entries in the DSL task that are adaptations of systems used for NLI (Goutte et al., 2013; Gebre et al., 2013; Jarvis et al., 2013). We observe that the variation in performance among the top ten teams is less than four percentage points. The team ranked last (eleventh) approached the task using LSTM and achieved an F1 score of 0.202. Unfortunately, they did not submit a system description paper, and thus we do not have much detail about their system. However, in the DSL 2016 task (Malmasi et al., 2016), neural network-based approaches already proved not to be very competitive for the task. See (Medvedeva et al., 2017) for a comparison between the performance of an SVM and an RNN approach for the DSL"
W17-1201,W17-1217,0,0.0300428,"Missing"
W17-1201,W15-5413,0,0.101469,"Missing"
W17-1201,W13-1712,0,0.199423,"Missing"
W17-1201,W14-5316,0,0.160565,"Missing"
W17-1201,U13-1003,0,0.160089,"est set A: in the closed training setting, where the systems were trained only using the training data provided by the DSL organizers, their accuracy dropped from 95.54 to 94.01, from 95.24 to 92.78, from 95.24 to 93.01, and from 94.67 to 93.02, respectively.3 Finally, inspired by recent work on language identification of user-generated content (Ljubeˇsi´c and Kranjˇci´c, 2015; Abainia et al., 2016), in the DSL 2016 task (Malmasi et al., 2016), we looked at how systems perform on discriminating between similar languages and language varieties across different domains, an aspect highlighted by Lui and Cook (2013) and Lui (2014). For this purpose, we provided an out-of-domain test set containing manually annotated microblog posts written in Bosnian, Croatian, Serbian, Brazilian and European Portuguese. 2.1 2.2 Dataset The DSLCC v4.04 contains 22,000 short excerpts of news texts for each language or language variety divided into 20,000 texts for training (18,000 texts) and development (2,000 texts), and 2,000 texts for testing. It contains a total of 8.6 million tokens for training and over half a million tokens for testing. The fourteen languages included in the v4.0 grouped by similarity are Bosnian,"
W17-1201,L16-1284,1,0.900242,"Missing"
W17-1201,W16-3928,0,0.0176574,"ranging from 3 for CLP to 11 for DSL. Below we describe the individual tasks. 2 Discriminating between Similar Languages (DSL) Discriminating between similar languages is one of the main challenges faced by language identification systems. Since 2014 the DSL shared task has been organized every year providing scholars and developers with an opportunity to evaluate language identification methods using a standard dataset and evaluation methodology. Albeit related to other shared tasks such as the 2014 TweetLID challenge (Zubiaga et al., 2014) and the 2016 shared task on Geolocation Prediction (Han et al., 2016), the DSL shared task continues to be the only shared task focusing on the discrimination between similar languages and language varieties. 1 The MAZA team submitted two separate papers: one for each task they participated in. 2 This number does not include the submissions to the Arabic Dialect Identification subtask of DSL in 2016. 2 At DSL 2015, the four best systems, MAC (Malmasi and Dras, 2015b), MMS (Zampieri et al., 2015a), NRC (Goutte and L´eger, 2015), and SUKI (Jauhiainen et al., 2015) performed similarly on test set B compared to test set A: in the closed training setting, where the"
W17-1201,W15-5407,1,0.88226,"ods using a standard dataset and evaluation methodology. Albeit related to other shared tasks such as the 2014 TweetLID challenge (Zubiaga et al., 2014) and the 2016 shared task on Geolocation Prediction (Han et al., 2016), the DSL shared task continues to be the only shared task focusing on the discrimination between similar languages and language varieties. 1 The MAZA team submitted two separate papers: one for each task they participated in. 2 This number does not include the submissions to the Arabic Dialect Identification subtask of DSL in 2016. 2 At DSL 2015, the four best systems, MAC (Malmasi and Dras, 2015b), MMS (Zampieri et al., 2015a), NRC (Goutte and L´eger, 2015), and SUKI (Jauhiainen et al., 2015) performed similarly on test set B compared to test set A: in the closed training setting, where the systems were trained only using the training data provided by the DSL organizers, their accuracy dropped from 95.54 to 94.01, from 95.24 to 92.78, from 95.24 to 93.01, and from 94.67 to 93.02, respectively.3 Finally, inspired by recent work on language identification of user-generated content (Ljubeˇsi´c and Kranjˇci´c, 2015; Abainia et al., 2016), in the DSL 2016 task (Malmasi et al., 2016), we l"
W17-1201,W17-1222,1,0.800029,"milar Languages, Varieties and Dialects, pages 1–15, c Valencia, Spain, April 3, 2017. 2017 Association for Computational Linguistics Team ahaqst bayesline CECL cic ualg Citius Ixa Imaxin CLUZH CUNI deepCybErNet gauge Helsinki-CLP MAZA (ADI) MAZA (GDI) mm lct qcri mit SUKI timeflow tubasfs unibuckernel XAC Bayesline Total DSL X X X X X X ADI X X X X X 11 CLP X X X X X X GDI X X X X X X X X X X 10 6 X X X 3 System Description Paper (Hanani et al., 2017) – (Bestgen, 2017) (G´omez-Adorno et al., 2017) (Gamallo et al., 2017) (Clematide and Makarov, 2017) (Rosa et al., 2017) – – (Tiedemann, 2017) (Malmasi and Zampieri, 2017a) (Malmasi and Zampieri, 2017b) (Medvedeva et al., 2017) – (Jauhiainen et al., 2017) (Criscuolo and Aluisio, 2017) (C ¸ o¨ ltekin and Rama, 2017) (Ionescu and Butnaru, 2017) (Barbaresi, 2017) 15 Table 1: The teams that participated in the VarDial’2017 Evaluation Campaign. 1.2 Participating Teams The fourth edition of the DSL shared task was motivated by the success of the previous editions and by the growing interest of the research community in the identification of dialects and similar languages, as evidenced by recent publications (Xu et al., 2016; Radford and Gall´e, 2016; Castro et al.,"
W17-1201,W17-1211,0,0.333885,"Danish, and Norwegian (TL) – Swedish (SL). Note that the latter two pairs include a triple of related languages. 1 Proceedings of the Fourth Workshop on NLP for Similar Languages, Varieties and Dialects, pages 1–15, c Valencia, Spain, April 3, 2017. 2017 Association for Computational Linguistics Team ahaqst bayesline CECL cic ualg Citius Ixa Imaxin CLUZH CUNI deepCybErNet gauge Helsinki-CLP MAZA (ADI) MAZA (GDI) mm lct qcri mit SUKI timeflow tubasfs unibuckernel XAC Bayesline Total DSL X X X X X X ADI X X X X X 11 CLP X X X X X X GDI X X X X X X X X X X 10 6 X X X 3 System Description Paper (Hanani et al., 2017) – (Bestgen, 2017) (G´omez-Adorno et al., 2017) (Gamallo et al., 2017) (Clematide and Makarov, 2017) (Rosa et al., 2017) – – (Tiedemann, 2017) (Malmasi and Zampieri, 2017a) (Malmasi and Zampieri, 2017b) (Medvedeva et al., 2017) – (Jauhiainen et al., 2017) (Criscuolo and Aluisio, 2017) (C ¸ o¨ ltekin and Rama, 2017) (Ionescu and Butnaru, 2017) (Barbaresi, 2017) 15 Table 1: The teams that participated in the VarDial’2017 Evaluation Campaign. 1.2 Participating Teams The fourth edition of the DSL shared task was motivated by the success of the previous editions and by the growing interest of the r"
W17-1201,W17-1220,1,0.878577,"milar Languages, Varieties and Dialects, pages 1–15, c Valencia, Spain, April 3, 2017. 2017 Association for Computational Linguistics Team ahaqst bayesline CECL cic ualg Citius Ixa Imaxin CLUZH CUNI deepCybErNet gauge Helsinki-CLP MAZA (ADI) MAZA (GDI) mm lct qcri mit SUKI timeflow tubasfs unibuckernel XAC Bayesline Total DSL X X X X X X ADI X X X X X 11 CLP X X X X X X GDI X X X X X X X X X X 10 6 X X X 3 System Description Paper (Hanani et al., 2017) – (Bestgen, 2017) (G´omez-Adorno et al., 2017) (Gamallo et al., 2017) (Clematide and Makarov, 2017) (Rosa et al., 2017) – – (Tiedemann, 2017) (Malmasi and Zampieri, 2017a) (Malmasi and Zampieri, 2017b) (Medvedeva et al., 2017) – (Jauhiainen et al., 2017) (Criscuolo and Aluisio, 2017) (C ¸ o¨ ltekin and Rama, 2017) (Ionescu and Butnaru, 2017) (Barbaresi, 2017) 15 Table 1: The teams that participated in the VarDial’2017 Evaluation Campaign. 1.2 Participating Teams The fourth edition of the DSL shared task was motivated by the success of the previous editions and by the growing interest of the research community in the identification of dialects and similar languages, as evidenced by recent publications (Xu et al., 2016; Radford and Gall´e, 2016; Castro et al.,"
W17-1201,W16-4801,1,0.679876,"Missing"
W17-1201,W17-1225,0,0.428199,"us Ixa Imaxin CLUZH CUNI deepCybErNet gauge Helsinki-CLP MAZA (ADI) MAZA (GDI) mm lct qcri mit SUKI timeflow tubasfs unibuckernel XAC Bayesline Total DSL X X X X X X ADI X X X X X 11 CLP X X X X X X GDI X X X X X X X X X X 10 6 X X X 3 System Description Paper (Hanani et al., 2017) – (Bestgen, 2017) (G´omez-Adorno et al., 2017) (Gamallo et al., 2017) (Clematide and Makarov, 2017) (Rosa et al., 2017) – – (Tiedemann, 2017) (Malmasi and Zampieri, 2017a) (Malmasi and Zampieri, 2017b) (Medvedeva et al., 2017) – (Jauhiainen et al., 2017) (Criscuolo and Aluisio, 2017) (C ¸ o¨ ltekin and Rama, 2017) (Ionescu and Butnaru, 2017) (Barbaresi, 2017) 15 Table 1: The teams that participated in the VarDial’2017 Evaluation Campaign. 1.2 Participating Teams The fourth edition of the DSL shared task was motivated by the success of the previous editions and by the growing interest of the research community in the identification of dialects and similar languages, as evidenced by recent publications (Xu et al., 2016; Radford and Gall´e, 2016; Castro et al., 2016). We also saw the number of system submissions to the DSL challenge grow from 8 in 2014 to 10 in 2015 and then to 17 in 2016.2 The 2015 and the 2016 editions of the DSL"
W17-1201,W13-1714,0,0.148384,"ures. This year, we added a multi-model aspect to the task by further providing acoustic features. The system description paper of CECL (Bestgen, 2017) provides some interesting insights about the DSL task. First, they found out that BM25 weighting, which was previously applied to native language identification (NLI) (Wang et al., 2016), worked better than using TF.IDF. They further highlighted the similarity between similar language identification and NLI as evidenced by a number of entries in the DSL task that are adaptations of systems used for NLI (Goutte et al., 2013; Gebre et al., 2013; Jarvis et al., 2013). We observe that the variation in performance among the top ten teams is less than four percentage points. The team ranked last (eleventh) approached the task using LSTM and achieved an F1 score of 0.202. Unfortunately, they did not submit a system description paper, and thus we do not have much detail about their system. However, in the DSL 2016 task (Malmasi et al., 2016), neural network-based approaches already proved not to be very competitive for the task. See (Medvedeva et al., 2017) for a comparison between the performance of an SVM and an RNN approach for the DSL task. 2.5 Arabic Dial"
W17-1201,W17-1219,0,0.132166,"cia, Spain, April 3, 2017. 2017 Association for Computational Linguistics Team ahaqst bayesline CECL cic ualg Citius Ixa Imaxin CLUZH CUNI deepCybErNet gauge Helsinki-CLP MAZA (ADI) MAZA (GDI) mm lct qcri mit SUKI timeflow tubasfs unibuckernel XAC Bayesline Total DSL X X X X X X ADI X X X X X 11 CLP X X X X X X GDI X X X X X X X X X X 10 6 X X X 3 System Description Paper (Hanani et al., 2017) – (Bestgen, 2017) (G´omez-Adorno et al., 2017) (Gamallo et al., 2017) (Clematide and Makarov, 2017) (Rosa et al., 2017) – – (Tiedemann, 2017) (Malmasi and Zampieri, 2017a) (Malmasi and Zampieri, 2017b) (Medvedeva et al., 2017) – (Jauhiainen et al., 2017) (Criscuolo and Aluisio, 2017) (C ¸ o¨ ltekin and Rama, 2017) (Ionescu and Butnaru, 2017) (Barbaresi, 2017) 15 Table 1: The teams that participated in the VarDial’2017 Evaluation Campaign. 1.2 Participating Teams The fourth edition of the DSL shared task was motivated by the success of the previous editions and by the growing interest of the research community in the identification of dialects and similar languages, as evidenced by recent publications (Xu et al., 2016; Radford and Gall´e, 2016; Castro et al., 2016). We also saw the number of system submissions to th"
W17-1201,W15-5408,0,0.243101,"Missing"
W17-1201,W16-4820,0,0.425079,"Missing"
W17-1201,W17-1212,0,0.183999,"Missing"
W17-1201,W17-1226,0,0.0868204,"Missing"
W17-1201,L16-1641,1,0.367123,"Missing"
W17-1201,N15-1010,0,0.0146702,"Missing"
W17-1201,W15-3040,0,0.0163982,"ams (1–6), and run 3 used hard voting of three systems: SVM, Logistic Regression, and Na”ive Bayes and character ngrams (2–6) as features. • bayesline: This team participated with a Multinomial Na¨ıve Bayes (MNB) classifier similar to that of Tan et al. (2014), with no special parameter tuning, as this system was initially intended to serve as an intelligent baseline for the task (but now it has matured into a competitive system). In their bestperforming run 1, they relied primarily on character 4-grams as features. The feature sets they used were selected by a search strategy as proposed in (Scarton et al., 2015). • cic ualg: This team submitted three runs. Runs 1 and 2 first predict the language group, and then discriminate between the languages within that group. The first step uses an SVM classifier with a combination of character 3– 5-grams, typed character 3-grams, applying the character n-gram categories introduced by Sapkota et al. (2015), and word unigrams using TF-weighting. The second step uses the same features and different classifiers: SVMs + Multinominal Na¨ıve Bayes (MNB) in run 1, and MNB in run 2 (which works best). Run 3 uses a single MNB classifier to discriminate between all fourte"
W17-1201,D10-1112,1,0.910362,"CN i-vector (as in Run 2) with (ii) an SVM model trained on count bag of characters 2–4-grams, which yielded an F1 of 0.612. This year, we introduced a new dialectal area, which focused on German dialects of Switzerland. Indeed, the German-speaking part of Switzerland is characterized by the widespread use of dialects in everyday communication, and by a large number of different dialects and dialectal areas. There have been two major approaches to Swiss German dialect identification in the literature. The corpus-based approach predicts the dialect of any text fragment extracted from a corpus (Scherrer and Rambow, 2010; Hollenstein and Aepli, 2015). The dialectological approach tries to identify a small set of distinguishing dialectal features, which are then elicited interactively from the user in order to identify his or her dialect (Leemann et al., 2016). In this task, we adopt a corpus-based approach, and we develop a new dataset for this. • deepCybErNet: This team submitted two runs. Run 1 adopted a Bi-LSTM architecture using the lexical features, and achieved an F1 score of 0.208, while run 2 used the i-vector features and achieved an F1 of 0.574. 3.3 Results Table 5 shows the evaluation results for t"
W17-1201,W14-5307,1,0.307051,"Missing"
W17-1201,W15-5411,1,0.900323,"Missing"
W17-1201,L16-1680,0,0.0188911,"Missing"
W17-1201,N12-1052,0,0.0102303,"Missing"
W17-1201,W14-1614,1,0.904663,"Missing"
W17-1201,tiedemann-2012-parallel,1,0.0255189,"LP task: parallel training data. Participants were asked not to use the development data with their gold standard annotation of dependency relations for any training purposes. The purpose of the development datasets is entirely for testing model performance during system development. All the knowledge used for parsing should origin in the provided source language data. Other sources (except for target language sources) could also be used in unconstrained submissions, but none of the participants chose that option. For the constrained setup, we also provided parallel datasets coming from OPUS (Tiedemann, 2012) that could be used for training cross-lingual parsers in any way. The datasets included translated movie subtitles and contained quite a bit of noise in terms of alignment, encoding, and translation quality. They were also from a very different domain, which made the setup quite realistic considering that one would used whatever could be found for the task. The sizes of the parallel datasets are given in Table 8. In the setup of the shared task, we also provided simple baselines and an “upper bound” of a model trained on annotated target language data. The cross-lingual baselines included del"
W17-1201,C14-1175,1,0.927054,"nd without any optimization of the hyper parameters. The size of the source language data is given in Table 5. We can see that for Czech we have by far the largest corpus, which will also be reflected in the results we obtain. VarDial 2017 featured for the first time a crosslingual parsing task for closely related languages.7 Transfer learning and annotation projection are popular approaches in this field and various techniques and models have been proposed in the literature in particular in connection with dependency parsing (Hwa et al., 2005; McDonald et al., 2013; T¨ackstr¨om et al., 2012; Tiedemann, 2014). The motivation for cross-lingual models is the attempt to bootstrap tools for languages that do not have annotated resources, which are typically necessary for supervised data-driven techniques, using data and resources from other languages. This is especially successful for closely related languages with similar syntactic structures and strong lexical overlap (Agi´c et al., 2012). With this background, it is a natural extension for our shared task to consider cross-lingual parsing as well. We do so by simulating the resource-poor situation by selecting language pairs from the Universal Depe"
W17-1201,W15-2137,1,0.514813,"ad of around 0.7. 4.5 Summary This first edition of the GDI task was a success, given the short time between the 2016 and 2017 editions. In the future, we would like to better control transcriber effects, either by a more thorough selection of training and test data, or by adding transcriber-independent features such as acoustic features, as has been done in the ADI task this year. Further dialectal areas could also be added. 10 5 Cross-lingual Dependency Parsing (CLP) Avoiding gold labels is important here in order to avoid exaggerated results that blur the picture of a more realistic setup (Tiedemann, 2015). The tagger models are trained on the original target language treebanks using UDpipe (Straka et al., 2016) with standard settings and without any optimization of the hyper parameters. The size of the source language data is given in Table 5. We can see that for Czech we have by far the largest corpus, which will also be reflected in the results we obtain. VarDial 2017 featured for the first time a crosslingual parsing task for closely related languages.7 Transfer learning and annotation projection are popular approaches in this field and various techniques and models have been proposed in th"
W17-1201,W17-1216,1,0.921592,"shop on NLP for Similar Languages, Varieties and Dialects, pages 1–15, c Valencia, Spain, April 3, 2017. 2017 Association for Computational Linguistics Team ahaqst bayesline CECL cic ualg Citius Ixa Imaxin CLUZH CUNI deepCybErNet gauge Helsinki-CLP MAZA (ADI) MAZA (GDI) mm lct qcri mit SUKI timeflow tubasfs unibuckernel XAC Bayesline Total DSL X X X X X X ADI X X X X X 11 CLP X X X X X X GDI X X X X X X X X X X 10 6 X X X 3 System Description Paper (Hanani et al., 2017) – (Bestgen, 2017) (G´omez-Adorno et al., 2017) (Gamallo et al., 2017) (Clematide and Makarov, 2017) (Rosa et al., 2017) – – (Tiedemann, 2017) (Malmasi and Zampieri, 2017a) (Malmasi and Zampieri, 2017b) (Medvedeva et al., 2017) – (Jauhiainen et al., 2017) (Criscuolo and Aluisio, 2017) (C ¸ o¨ ltekin and Rama, 2017) (Ionescu and Butnaru, 2017) (Barbaresi, 2017) 15 Table 1: The teams that participated in the VarDial’2017 Evaluation Campaign. 1.2 Participating Teams The fourth edition of the DSL shared task was motivated by the success of the previous editions and by the growing interest of the research community in the identification of dialects and similar languages, as evidenced by recent publications (Xu et al., 2016; Radford and G"
W17-1407,W15-5301,1,0.830449,"Missing"
W17-1407,C10-1011,0,0.0267272,"or N if the lemma ends in -nje Table 1: Automatic conversion from UD v1 to UD v2. add morphosyntactic definitions (MSD) following the modified Multext-East version 4 format (Erjavec, 2012) documented in the draft of version 5.2 MSD annotation is first added automatically using the state-of-the-art Croatian tagger described by Ljubeˇsi´c et al. (2016), and then corrected manually by two experts native in Serbian, resulting in gold MSD labels. Once morphologically annotated, the Serbian side of SETimes.HR, coined SETimes.SR, was then parsed using the mate-tools, a graphbased dependency parser (Bohnet, 2010) trained on the Croatian UD v1.2 treebank data. The parser was trained with default parameters. 3 Category Comparison and Adaptation In this step, we perform manual inspection of a sample of parsed sentences in order to decide what categories and relations to use for Serbian. We extract and evaluate a handful of examples of all annotated relations, comparing the annotation to the general guidelines and to the language-specific en2 40 http://nl.ijs.si/ME/V5/msd/html/ In expl reparandum det nummod Out NA NA det:numgov nummod:gov compound amod nmod flat ALL ALL compound det ALL xcomp Context ALL"
W17-1407,W06-2920,0,0.0608056,"annotation guidelines. We describe the automatic and manual annotation procedures, discuss the annotation of Slavicspecific categories (case governing quantifiers, reflexive pronouns, question particles) and propose an approach to handling deverbal nouns in Slavic languages. 1 Introduction The notion Universal Dependencies (UD) refers to an international movement started with the goal to reduce to a minimum cross-linguistic variation in the formalisms used to label syntactic structure (McDonald et al., 2013; Nivre et al., 2016). This goal was defined following multilingual parsing campaigns (Buchholz and Marsi, 2006; Hajiˇc et al., 2009) that revealed substantial cross-linguistic differences in the sets of labels and relations used in different treebanks, making it hard to compare parsers’ performances across languages (McDonald and Nivre, 2007). In this paper, we document the process of building a UD treebank for Serbian underlining the advantages of using the existing general framework, but also data and tools already available for other languages. The availability of shared resources is especially important for languages such as Serbian, which, more than 20 years after the publication of Penn Treebank"
W17-1407,L16-1676,1,0.880613,"Missing"
W17-1407,D07-1013,0,0.040589,"andling deverbal nouns in Slavic languages. 1 Introduction The notion Universal Dependencies (UD) refers to an international movement started with the goal to reduce to a minimum cross-linguistic variation in the formalisms used to label syntactic structure (McDonald et al., 2013; Nivre et al., 2016). This goal was defined following multilingual parsing campaigns (Buchholz and Marsi, 2006; Hajiˇc et al., 2009) that revealed substantial cross-linguistic differences in the sets of labels and relations used in different treebanks, making it hard to compare parsers’ performances across languages (McDonald and Nivre, 2007). In this paper, we document the process of building a UD treebank for Serbian underlining the advantages of using the existing general framework, but also data and tools already available for other languages. The availability of shared resources is especially important for languages such as Serbian, which, more than 20 years after the publication of Penn Treebank (Marcus et al., 1994), still 39 has no resource with annotated syntactic structure, lagging behind its close relatives for which UD annotation is available. Labeled as automatic conversion with manual corrections in the UD documentat"
W17-1407,P13-2017,0,0.063635,"Missing"
W17-1407,C12-1160,1,0.886366,"Missing"
W17-1407,J93-2004,0,\N,Missing
W17-1407,L16-1262,0,\N,Missing
W17-1410,W13-2408,1,0.89407,"Missing"
W17-1410,W16-2606,0,0.030665,"Missing"
W17-1410,J92-4003,0,0.54702,"Missing"
W17-1410,P07-1033,0,0.262882,"Missing"
W17-1410,N13-1037,0,0.0793345,"cribes the tagging experiments we performed, Section 5 reports on the error analysis of the results and Section 6 gives some conclusions and directions for further research. 2 Related Work Early work on PoS tagging social media was, as usual, mostly focused on English (Gimpel et al., 2011; Owoputi et al., 2013). Recently there has been more work on other languages, primarily through the organization of shared tasks, such the EmpiriST on German (Beißwenger et al., 2016) and PoSTWITA on Italian.1 There are two main approaches to processing non-standard data: normalization and domain adaptation (Eisenstein, 2013). Most approaches nowadays follow the domain adaptation path al1 http://corpora.ficlit.unibo.it/ PoSTWITA/ Proceedings of the 6th Workshop on Balto-Slavic Natural Language Processing, pages 60–68, c Valencia, Spain, 4 April 2017. 2017 Association for Computational Linguistics though the literature still lacks a detailed comparison of the two strategies on specific tasks. In domain adaptation there are, again, two main strategies (Horsmann and Zesch, 2015): adding more labeled data (Daum´e III, 2007; Hovy et al., 2015) and incorporating external knowledge (Owoputi et al., 2013). Horsmann and Ze"
W17-1410,P11-2008,0,0.414792,"Missing"
W17-1410,P07-2053,0,0.238028,"Missing"
W17-1410,N15-1135,0,0.0120933,"roaches to processing non-standard data: normalization and domain adaptation (Eisenstein, 2013). Most approaches nowadays follow the domain adaptation path al1 http://corpora.ficlit.unibo.it/ PoSTWITA/ Proceedings of the 6th Workshop on Balto-Slavic Natural Language Processing, pages 60–68, c Valencia, Spain, 4 April 2017. 2017 Association for Computational Linguistics though the literature still lacks a detailed comparison of the two strategies on specific tasks. In domain adaptation there are, again, two main strategies (Horsmann and Zesch, 2015): adding more labeled data (Daum´e III, 2007; Hovy et al., 2015) and incorporating external knowledge (Owoputi et al., 2013). Horsmann and Zesch (2015) show that (1) adding manually annotated in-domain data is highly effective (but costly) and (2) adding out-of-domain training data or machine-tagged data is less effective than adding more external knowledge, especially word clustering information. The contribution of our paper is the following: First, we perform the first experiments in annotating Slavic non-standard texts with part-of-speech and morphosyntactic information, therefore dealing with several hundreds of tags. Next, we investigate the impact o"
W17-1410,R15-1049,1,0.891607,"Missing"
W17-1410,L16-1242,1,0.893411,"Missing"
W17-1410,L16-1676,1,0.915116,"Missing"
W17-1410,N13-1039,0,0.107593,"Missing"
W17-1410,C14-1168,0,0.045415,"Missing"
W17-1410,P10-1040,0,0.189388,"Missing"
W17-2901,W15-2913,0,0.0713611,"Missing"
W17-2901,D11-1120,0,0.101339,"arious languages. Finally we perform a comparative analysis of feature effect sizes across the six languages and show that differences in our features correspond to cultural distances. 1 Introduction Gender prediction is a well-established task in author profiling, useful for a series of downstream analyses (Schler et al., 2006; Schwartz et al., 2013; Bamman et al., 2014) as well as predictive model improvements (Hovy, 2015). Most existing work on predicting gender focuses on exploiting the linguistic production of the users (Koppel et al., 2003; Schler et al., 2006; Kucukyilmaz et al., 2006; Burger et al., 2011; Miller et al., 2012; Rangel et al., 2016), just rarely using nonlinguistic information such as metadata (Plank 1 Proceedings of the Second Workshop on Natural Language Processing and Computational Social Science, pages 1–6, c Vancouver, Canada, August 3, 2017. 2017 Association for Computational Linguistics 2 The Dataset working hours, posting during weekends, truncated tweets, favorited tweets, quotes, retweeted tweets). By following the three types, mean, med and var, we encode the following distributions in our feature space: retweet count, favorite count, posting hour, day of week the twe"
W17-2901,L16-1258,0,0.0841781,"average daily number of tweets, overall number of tweets, number of tweets the user has favorited, number of followers, number of friends, the ratio of follower to friend numbers, number of lists the user is on, whether the user has a background image defined, whether the user has the default profile image, whether the user has a profile description, whether the user has a location defined, and red, green and blue color component intensity (two-digit hexadecimal code from the RGB color definition) of the user’s text and background color. In our experiments we fully rely on the TwiSty corpus (Verhoeven et al., 2016) which was developed for research in author profiling. It contains personality (MBTI) and gender annotations for a total of 18,168 authors posting in German, Italian, Dutch, French, Portuguese or Spanish. The manual gender annotations in the TwiSty corpus are based on the user’s name, handle, description and profile picture and follow the performative view of gender, i.e., that gender is discriminated by performances that respond to societal norms or conventions (Larson, 2017). The corpus is distributed in the form of Twitter user IDs and specific tweet IDs of that user. In this work we use on"
W17-2901,P15-1073,0,0.0203148,"f-words model when training and testing on the same language, it regularly outperforms the bag-of-words model when applied to different languages, showing very stable results across various languages. Finally we perform a comparative analysis of feature effect sizes across the six languages and show that differences in our features correspond to cultural distances. 1 Introduction Gender prediction is a well-established task in author profiling, useful for a series of downstream analyses (Schler et al., 2006; Schwartz et al., 2013; Bamman et al., 2014) as well as predictive model improvements (Hovy, 2015). Most existing work on predicting gender focuses on exploiting the linguistic production of the users (Koppel et al., 2003; Schler et al., 2006; Kucukyilmaz et al., 2006; Burger et al., 2011; Miller et al., 2012; Rangel et al., 2016), just rarely using nonlinguistic information such as metadata (Plank 1 Proceedings of the Second Workshop on Natural Language Processing and Computational Social Science, pages 1–6, c Vancouver, Canada, August 3, 2017. 2017 Association for Computational Linguistics 2 The Dataset working hours, posting during weekends, truncated tweets, favorited tweets, quotes, r"
W17-2901,W17-1601,0,0.0138108,"definition) of the user’s text and background color. In our experiments we fully rely on the TwiSty corpus (Verhoeven et al., 2016) which was developed for research in author profiling. It contains personality (MBTI) and gender annotations for a total of 18,168 authors posting in German, Italian, Dutch, French, Portuguese or Spanish. The manual gender annotations in the TwiSty corpus are based on the user’s name, handle, description and profile picture and follow the performative view of gender, i.e., that gender is discriminated by performances that respond to societal norms or conventions (Larson, 2017). The corpus is distributed in the form of Twitter user IDs and specific tweet IDs of that user. In this work we use only the user IDs and their gender and language annotations to collect timelines of users through the Twitter API. For each user we collect up to 3,200 tweets (API restriction) and discard users with less than 100 tweets. By doing so we collected 45 million tweets for 16,156 users across the six languages. 3 4 The Features Experimental Setup In this section we outline the setup of our gender classification experiments, whose results we report in Section 5.1. We train models base"
W17-3007,N16-2013,0,0.0293422,"ew exceptions for Dutch (van Halteren and Oostdijk, 2013) and German (Ross et al., 2017). State-of-the-art approaches tackle this task through supervised machine learning (Sood et al., 2012; Dadvar et al., 2013). For this, of course, manually annotated datasets are needed. A major limitation of most existing work in this area is that it is based on an ad-hoc treatment of SUD classification in natural language processing and a lack of detailed guidelines that are necessary for reliable annotation (Ross et al., 2017). Annotated datasets have started to emerge only recently (Nobata et al., 2016; Waseem and Hovy, 2016), but nevertheless they lack precise documentation on data annotation and make use of only very basic In this paper we present the legal framework, dataset and annotation schema of socially unacceptable discourse practices on social networking platforms in Slovenia. On this basis we aim to train an automatic identification and classification system with which we wish contribute towards an improved methodology, understanding and treatment of such practices in the contemporary, increasingly multicultural information society. 1 Introduction In Slovenia, Socially Unacceptable Discourse (SUD) pract"
W18-3028,S15-2102,0,0.0458372,"Missing"
W18-3028,N16-1091,0,0.0838501,"Missing"
W18-3028,P16-2083,0,0.152164,"Missing"
W18-3028,C14-1018,0,0.0646429,"Missing"
W18-3028,P14-1024,0,0.13915,"are very important notions in psycholinguistic research, building on the theory of the double, verbal and non-verbal, modality of representation of concrete words in the mental lexicon, contrasted to single verbal representation of abstract words (Paivio, 1975, 2010). Although often correlated with concreteness, imageability is not a redundant property. While most abstract things are hard to visualize, some call up images, e.g., torture calls up an emotional and even visual image. There are concrete things that are hard to visualize too, for example, abbey is harder to visualize than banana (Tsvetkov et al., 2014). 217 Proceedings of the 3rd Workshop on Representation Learning for NLP, pages 217–222 c Melbourne, Australia, July 20, 2018. 2018 Association for Computational Linguistics ing campaign in which each word was labeled by 20 annotators on a 1–5 scale. For Croatian we use the MEGAHR database (MEGA onwards), consisting of 3,000 words, with concreteness and imageability ratings summarized through arithmetic mean and standard deviation. The ratings were collected in an annotation campaign among university students, with each word obtaining 30 annotations per variable on a 1–5 scale. For performing"
W18-3028,D11-1063,0,0.148297,"Missing"
W18-3028,D16-1057,0,0.0475519,"Missing"
W18-3028,E17-1072,0,0.0311061,"ned between languages with a linear transformation learned via SVD (Smith et al., 2017) on a bilingual dictionary of 500 out of the 1000 most frequent English words, obtained via the Google Translate API3 . We also experimented with another crosslingual embedding collection (Conneau et al., 2017), obtaining similar results and backing all our conclusions. This is in line with recent work on comparing cross-lingual embedding models which suggests that the actual choice of monolingual and bilingual signal is more important for the final model performance than the actual underlying architecture (Levy et al., 2017; Ruder et al., 2017). Given that one of our goals is to transfer concreteness and imageability annotations to as many languages as possible, using cross-lingual word embeddings based on Wikipedia dumps and dictionaries obtained through a translation API is the most plausible option. Contributions In this paper we perform a systematic investigation of transfer of two lexical notions, concreteness and imageability, (1) to the remainder of the lexicon not covered in an annotation campaign, and (2) to other languages. While there were already successful transfers within a language based on word e"
W18-3901,W18-3913,0,0.217545,"per. Team ADI Arabic Identification benf BZU CLiPS CEA List DeepLIMA CoAStaL DFSlangid dkosmajac GDI classification ILIdentification JANES JSI LaMa LTL-UDE mmb lct safina STEVENDU2018 SUKI X SYSTRAN Taurus T¨ubingen-Oslo Twist Bytes Meta UH&CU UnibucKernel we are indian XAC Total X DFS GDI ILI MTT System Description Papers X X X X X X X X X X X X X X X (Naser and Hanani, 2018) (Kreutz and Daelemans, 2018) (Meftah and Semmar, 2018) (Ciobanu et al., 2018a) (Ciobanu et al., 2018b) (Ljubeˇsi´c, 2018) (Ljubeˇsi´c, 2018) X X X X X X X X X X X X X X X X X X X 6 X 12 X 8 X X 8 6 (Kroon et al., 2018) (Ali, 2018a; Ali, 2018b; Ali, 2018c) (Du and Wang, 2018) (Jauhiainen et al., 2018a; Jauhiainen et al., 2018b; Jauhiainen et al., 2018c) (Michon et al., 2018) (van Halteren and Oostdijk, 2018) (C ¸ o¨ ltekin et al., 2018) (Benites et al., 2018) (Silfverberg and Drobac, 2018) (Butnaru and Ionescu, 2018) (Gupta et al., 2018) (Barbaresi, 2018) 22 Table 1: The teams that participated in the VarDial’2018 evaluation campaign. 4 Previous Shared Tasks Since the first DSL challenge, the shared tasks organized within the scope of the VarDial workshop have enjoyed substantial increase in the number of participants"
W18-3901,W18-3919,0,0.254393,"per. Team ADI Arabic Identification benf BZU CLiPS CEA List DeepLIMA CoAStaL DFSlangid dkosmajac GDI classification ILIdentification JANES JSI LaMa LTL-UDE mmb lct safina STEVENDU2018 SUKI X SYSTRAN Taurus T¨ubingen-Oslo Twist Bytes Meta UH&CU UnibucKernel we are indian XAC Total X DFS GDI ILI MTT System Description Papers X X X X X X X X X X X X X X X (Naser and Hanani, 2018) (Kreutz and Daelemans, 2018) (Meftah and Semmar, 2018) (Ciobanu et al., 2018a) (Ciobanu et al., 2018b) (Ljubeˇsi´c, 2018) (Ljubeˇsi´c, 2018) X X X X X X X X X X X X X X X X X X X 6 X 12 X 8 X X 8 6 (Kroon et al., 2018) (Ali, 2018a; Ali, 2018b; Ali, 2018c) (Du and Wang, 2018) (Jauhiainen et al., 2018a; Jauhiainen et al., 2018b; Jauhiainen et al., 2018c) (Michon et al., 2018) (van Halteren and Oostdijk, 2018) (C ¸ o¨ ltekin et al., 2018) (Benites et al., 2018) (Silfverberg and Drobac, 2018) (Butnaru and Ionescu, 2018) (Gupta et al., 2018) (Barbaresi, 2018) 22 Table 1: The teams that participated in the VarDial’2018 evaluation campaign. 4 Previous Shared Tasks Since the first DSL challenge, the shared tasks organized within the scope of the VarDial workshop have enjoyed substantial increase in the number of participants"
W18-3901,W18-3932,0,0.129693,"per. Team ADI Arabic Identification benf BZU CLiPS CEA List DeepLIMA CoAStaL DFSlangid dkosmajac GDI classification ILIdentification JANES JSI LaMa LTL-UDE mmb lct safina STEVENDU2018 SUKI X SYSTRAN Taurus T¨ubingen-Oslo Twist Bytes Meta UH&CU UnibucKernel we are indian XAC Total X DFS GDI ILI MTT System Description Papers X X X X X X X X X X X X X X X (Naser and Hanani, 2018) (Kreutz and Daelemans, 2018) (Meftah and Semmar, 2018) (Ciobanu et al., 2018a) (Ciobanu et al., 2018b) (Ljubeˇsi´c, 2018) (Ljubeˇsi´c, 2018) X X X X X X X X X X X X X X X X X X X 6 X 12 X 8 X X 8 6 (Kroon et al., 2018) (Ali, 2018a; Ali, 2018b; Ali, 2018c) (Du and Wang, 2018) (Jauhiainen et al., 2018a; Jauhiainen et al., 2018b; Jauhiainen et al., 2018c) (Michon et al., 2018) (van Halteren and Oostdijk, 2018) (C ¸ o¨ ltekin et al., 2018) (Benites et al., 2018) (Silfverberg and Drobac, 2018) (Butnaru and Ionescu, 2018) (Gupta et al., 2018) (Barbaresi, 2018) 22 Table 1: The teams that participated in the VarDial’2018 evaluation campaign. 4 Previous Shared Tasks Since the first DSL challenge, the shared tasks organized within the scope of the VarDial workshop have enjoyed substantial increase in the number of participants"
W18-3901,W17-1223,0,0.131933,"data to reduce the transcriber effects seen last year. 7 • The LaMa system is a blend (weighted vote) of eight classifiers being stochastic gradient descent (hinge and modified Huber), multinomial Na¨ıve Bayes, both counts and tf-idf, FastText, and modified Kneser-Ney smoothing. The classifiers were trained using word n-grams (1-6) and character n-grams (1-8). The hyperparameters were determined with cross-validation and searching on the development set. • XAC system is a refined version of the n-gram-based Bayesline system described in last year’s XAC submission to the VarDial shared tasks (Barbaresi, 2017), and previously used as a baseline for the DSL shared task (Tan et al., 2014). The XAC team achieved their best results using a Na¨ıve Bayes classifier. • The GDI classification system is based on an ensemble of multiple SVM classifiers. The system was trained on various word- and character-level features. • The dkosmajac system is based on a normalized Euclidean distance measure. The distances are calculated between a sample and each class profile. The class profiles are generated by selecting the most frequent features for each class, which results in profiles that are of the same length fo"
W18-3901,W18-3918,0,0.0590619,"Missing"
W18-3901,W18-3925,0,0.142653,"Missing"
W18-3901,W17-1214,0,0.138486,"was part of the first VarDial evaluation campaign (Zampieri et al., 2017). It provided manual transcriptions of recorded interviews from four dialect areas of the German-speaking Switzerland, namely Bern, Basel, Lucerne, and Zurich. The training and the test data was extracted from the ArchiMob corpus (Samardˇzi´c et al., 2016). The training data consisted in 3,000–4,000 utterances from 3–5 different speakers per dialect; the test data consisted of about 900 utterances by a single speaker per dialect. A total of ten teams participated in the 2017 GDI task and the two best-performing systems (Bestgen, 2017; Malmasi and Zampieri, 2017b) achieved weighted F1-measure of up to 0.66. Transcribers were shown to affect the performance of the systems, e.g., for the Lucerne dialect, whose test set was transcribed by a different person than the training set, recall figures were only around 0.3. 5 Third Arabic Dialect Identification (ADI) This year’s third edition of the ADI task addressed the multi-dialectal challenge in spoken Arabic in the broadcast domain. Last year, in the second edition of the ADI task (Zampieri et al., 2017), we offered the input represented as (i) automatic text transcriptions gen"
W18-3901,W18-3909,0,0.15091,"Total X DFS GDI ILI MTT System Description Papers X X X X X X X X X X X X X X X (Naser and Hanani, 2018) (Kreutz and Daelemans, 2018) (Meftah and Semmar, 2018) (Ciobanu et al., 2018a) (Ciobanu et al., 2018b) (Ljubeˇsi´c, 2018) (Ljubeˇsi´c, 2018) X X X X X X X X X X X X X X X X X X X 6 X 12 X 8 X X 8 6 (Kroon et al., 2018) (Ali, 2018a; Ali, 2018b; Ali, 2018c) (Du and Wang, 2018) (Jauhiainen et al., 2018a; Jauhiainen et al., 2018b; Jauhiainen et al., 2018c) (Michon et al., 2018) (van Halteren and Oostdijk, 2018) (C ¸ o¨ ltekin et al., 2018) (Benites et al., 2018) (Silfverberg and Drobac, 2018) (Butnaru and Ionescu, 2018) (Gupta et al., 2018) (Barbaresi, 2018) 22 Table 1: The teams that participated in the VarDial’2018 evaluation campaign. 4 Previous Shared Tasks Since the first DSL challenge, the shared tasks organized within the scope of the VarDial workshop have enjoyed substantial increase in the number of participants and in the overall interest from the NLP community. This motivated the organizers to turn the shared tasks at VarDial into a more comprehensive evaluation exercise with four shared tasks in 2017. This year, the VarDial workshop featured the second edition of the VarDial evaluation campaign w"
W18-3901,W18-3933,1,0.889149,"Missing"
W18-3901,W18-3920,1,0.880795,"Missing"
W18-3901,W18-3926,0,0.0553879,"Missing"
W18-3901,W18-3921,0,0.054375,"Missing"
W18-3901,W17-1225,0,0.134424,"r of hours. 5.2 Participants and Approaches In this section, we present a short description of the systems that competed in the ADI shared task: • UnibucKernel system (Butnaru and Ionescu, 2018) combines three kernel matrices: one calculated using just the lexical features, another one computed on embeddings, and a combined kernel computed on the phonetic features. The final matrix is the mean of these three matrices. As a classifier, they used Kernel Ridge Regression. The approach is similar to the systems that ranked second and first in the previous two ADI tasks (Ionescu and Popescu, 2016; Ionescu and Butnaru, 2017). • Safina system (Ali, 2018a) accepts a sequence of 256 characters as input in addition to the acoustic embedding vectors. First, the sequence of characters is one-hot encoded, then it is passed to a GRU layer, which is followed by a convolutional layer with different filter sizes ranging from 2 to 7. The convolutional layer is followed by batch normalizations, max-pooling, and dropout layers, and finally a softmax layer. In contrast, the acoustic embedding vectors go directly to another softmax layer. The final output is the average between these two softmax layers, which represents the prob"
W18-3901,W16-4818,0,0.048944,", duration (Dur.), in number of hours. 5.2 Participants and Approaches In this section, we present a short description of the systems that competed in the ADI shared task: • UnibucKernel system (Butnaru and Ionescu, 2018) combines three kernel matrices: one calculated using just the lexical features, another one computed on embeddings, and a combined kernel computed on the phonetic features. The final matrix is the mean of these three matrices. As a classifier, they used Kernel Ridge Regression. The approach is similar to the systems that ranked second and first in the previous two ADI tasks (Ionescu and Popescu, 2016; Ionescu and Butnaru, 2017). • Safina system (Ali, 2018a) accepts a sequence of 256 characters as input in addition to the acoustic embedding vectors. First, the sequence of characters is one-hot encoded, then it is passed to a GRU layer, which is followed by a convolutional layer with different filter sizes ranging from 2 to 7. The convolutional layer is followed by batch normalizations, max-pooling, and dropout layers, and finally a softmax layer. In contrast, the acoustic embedding vectors go directly to another softmax layer. The final output is the average between these two softmax layer"
W18-3901,W18-3915,0,0.0733204,"Missing"
W18-3901,W18-3929,0,0.411409,"Missing"
W18-3901,W18-3907,0,0.178349,"Missing"
W18-3901,W18-3922,0,0.0589219,"Missing"
W18-3901,W18-3928,0,0.0556119,"Missing"
W18-3901,kumar-2012-challenges,1,0.824307,"ed printed stories, novels and essays in books, magazines, and newspapers. We scanned the printed materials, then we performed OCR, and finally we asked native speakers of the respective languages to correct the OCR output. Since there are no specific OCR models available for these languages, we used the Google OCR for Hindi, part of the Drive API. Since all the languages used the Devanagari script, we expected the OCR to work reasonably well, and overall it did. We further managed to get some blogs in Magahi and Bhojpuri. There are several corpora already available for Modern Standard Hindi (Kumar, 2012; Kumar, 2014a; Kumar, 2014b; Choudhary and Jha, 2011). However, in order to keep the domain the same as for the other languages, we collected data from blogs that mainly contain stories and novels. Thus, the Modern Standard Hindi data collected for this study is also from the literature domain.5 9.2 Participants and Approaches • The SUKI team used HeLI with adaptive language models based on character n-grams from 1 to 6, as previously described in Section 5. The also used an iterative version of the language model adaptation technique, with three additional adaptation epochs. ¨ • Tubingen-Osl"
W18-3901,kumar-2014-developing,1,0.832345,"ories, novels and essays in books, magazines, and newspapers. We scanned the printed materials, then we performed OCR, and finally we asked native speakers of the respective languages to correct the OCR output. Since there are no specific OCR models available for these languages, we used the Google OCR for Hindi, part of the Drive API. Since all the languages used the Devanagari script, we expected the OCR to work reasonably well, and overall it did. We further managed to get some blogs in Magahi and Bhojpuri. There are several corpora already available for Modern Standard Hindi (Kumar, 2012; Kumar, 2014a; Kumar, 2014b; Choudhary and Jha, 2011). However, in order to keep the domain the same as for the other languages, we collected data from blogs that mainly contain stories and novels. Thus, the Modern Standard Hindi data collected for this study is also from the literature domain.5 9.2 Participants and Approaches • The SUKI team used HeLI with adaptive language models based on character n-grams from 1 to 6, as previously described in Section 5. The also used an iterative version of the language model adaptation technique, with three additional adaptation epochs. ¨ • Tubingen-Oslo team submit"
W18-3901,W14-0405,1,0.912975,"Missing"
W18-3901,W18-3917,1,0.897664,"Missing"
W18-3901,L16-1242,1,0.902855,"Missing"
W18-3901,L16-1676,1,0.914666,"Missing"
W18-3901,W16-4814,1,0.86288,"-Oslo system (C ¸ o¨ ltekin et al., 2018) is trained on word and character n-grams using a single SVM classifier, which is fine-tuned using cross-validation. It is similar to the submissions by the same authors to previous VarDial shared tasks (C¸o¨ ltekin and Rama, 2017; C¸o¨ ltekin and Rama, 2016). They also tried an approach based on RNN, which worked worse. • Arabic Identification system is based on an ensemble of SVM classifiers trained on character and word n-grams. The approach is similar to the systems ranked second and first in the previous two ADI tasks (Malmasi and Zampieri, 2017a; Malmasi and Zampieri, 2016). 5 5.3 Results Six teams submitted runs for the ADI shared task and the results are shown in Table 3. The best result, an F1 score of 0.589, was achieved by UnibucKernel,1 followed by safina, with an F1 score of 0.575. The following three teams are tied for the third place as they are not statistically different. Rank 1 2 3 3 3 4 Team F1 (Macro) UnibucKernel safina BZU SYSTRAN T¨ubingen-Oslo Arabic Identification 0.589 0.576 0.534 0.529 0.514 0.500 Table 3: ADI results: ranked taking statistical significance into account. 5.4 Summary We introduced multi-phoneme representation for the dialecta"
W18-3901,W17-1222,1,0.833986,"e first VarDial evaluation campaign (Zampieri et al., 2017). It provided manual transcriptions of recorded interviews from four dialect areas of the German-speaking Switzerland, namely Bern, Basel, Lucerne, and Zurich. The training and the test data was extracted from the ArchiMob corpus (Samardˇzi´c et al., 2016). The training data consisted in 3,000–4,000 utterances from 3–5 different speakers per dialect; the test data consisted of about 900 utterances by a single speaker per dialect. A total of ten teams participated in the 2017 GDI task and the two best-performing systems (Bestgen, 2017; Malmasi and Zampieri, 2017b) achieved weighted F1-measure of up to 0.66. Transcribers were shown to affect the performance of the systems, e.g., for the Lucerne dialect, whose test set was transcribed by a different person than the training set, recall figures were only around 0.3. 5 Third Arabic Dialect Identification (ADI) This year’s third edition of the ADI task addressed the multi-dialectal challenge in spoken Arabic in the broadcast domain. Last year, in the second edition of the ADI task (Zampieri et al., 2017), we offered the input represented as (i) automatic text transcriptions generated using large-vocabular"
W18-3901,W17-1220,1,0.80558,"e first VarDial evaluation campaign (Zampieri et al., 2017). It provided manual transcriptions of recorded interviews from four dialect areas of the German-speaking Switzerland, namely Bern, Basel, Lucerne, and Zurich. The training and the test data was extracted from the ArchiMob corpus (Samardˇzi´c et al., 2016). The training data consisted in 3,000–4,000 utterances from 3–5 different speakers per dialect; the test data consisted of about 900 utterances by a single speaker per dialect. A total of ten teams participated in the 2017 GDI task and the two best-performing systems (Bestgen, 2017; Malmasi and Zampieri, 2017b) achieved weighted F1-measure of up to 0.66. Transcribers were shown to affect the performance of the systems, e.g., for the Lucerne dialect, whose test set was transcribed by a different person than the training set, recall figures were only around 0.3. 5 Third Arabic Dialect Identification (ADI) This year’s third edition of the ADI task addressed the multi-dialectal challenge in spoken Arabic in the broadcast domain. Last year, in the second edition of the ADI task (Zampieri et al., 2017), we offered the input represented as (i) automatic text transcriptions generated using large-vocabular"
W18-3901,W16-4801,1,0.733081,"Missing"
W18-3901,W18-3927,0,0.0554913,"Missing"
W18-3901,W18-3914,0,0.175754,"ES JSI LaMa LTL-UDE mmb lct safina STEVENDU2018 SUKI X SYSTRAN Taurus T¨ubingen-Oslo Twist Bytes Meta UH&CU UnibucKernel we are indian XAC Total X DFS GDI ILI MTT System Description Papers X X X X X X X X X X X X X X X (Naser and Hanani, 2018) (Kreutz and Daelemans, 2018) (Meftah and Semmar, 2018) (Ciobanu et al., 2018a) (Ciobanu et al., 2018b) (Ljubeˇsi´c, 2018) (Ljubeˇsi´c, 2018) X X X X X X X X X X X X X X X X X X X 6 X 12 X 8 X X 8 6 (Kroon et al., 2018) (Ali, 2018a; Ali, 2018b; Ali, 2018c) (Du and Wang, 2018) (Jauhiainen et al., 2018a; Jauhiainen et al., 2018b; Jauhiainen et al., 2018c) (Michon et al., 2018) (van Halteren and Oostdijk, 2018) (C ¸ o¨ ltekin et al., 2018) (Benites et al., 2018) (Silfverberg and Drobac, 2018) (Butnaru and Ionescu, 2018) (Gupta et al., 2018) (Barbaresi, 2018) 22 Table 1: The teams that participated in the VarDial’2018 evaluation campaign. 4 Previous Shared Tasks Since the first DSL challenge, the shared tasks organized within the scope of the VarDial workshop have enjoyed substantial increase in the number of participants and in the overall interest from the NLP community. This motivated the organizers to turn the shared tasks at VarDial into a more comprehensive eva"
W18-3901,W18-3924,0,0.0833746,"and the number of submissions varied widely across the tasks, ranging from 6 entries for ADI and MTT to 12 entries for DFS. Table 1 lists the participating teams, the shared tasks they took part in, and a reference to the system description paper. Team ADI Arabic Identification benf BZU CLiPS CEA List DeepLIMA CoAStaL DFSlangid dkosmajac GDI classification ILIdentification JANES JSI LaMa LTL-UDE mmb lct safina STEVENDU2018 SUKI X SYSTRAN Taurus T¨ubingen-Oslo Twist Bytes Meta UH&CU UnibucKernel we are indian XAC Total X DFS GDI ILI MTT System Description Papers X X X X X X X X X X X X X X X (Naser and Hanani, 2018) (Kreutz and Daelemans, 2018) (Meftah and Semmar, 2018) (Ciobanu et al., 2018a) (Ciobanu et al., 2018b) (Ljubeˇsi´c, 2018) (Ljubeˇsi´c, 2018) X X X X X X X X X X X X X X X X X X X 6 X 12 X 8 X X 8 6 (Kroon et al., 2018) (Ali, 2018a; Ali, 2018b; Ali, 2018c) (Du and Wang, 2018) (Jauhiainen et al., 2018a; Jauhiainen et al., 2018b; Jauhiainen et al., 2018c) (Michon et al., 2018) (van Halteren and Oostdijk, 2018) (C ¸ o¨ ltekin et al., 2018) (Benites et al., 2018) (Silfverberg and Drobac, 2018) (Butnaru and Ionescu, 2018) (Gupta et al., 2018) (Barbaresi, 2018) 22 Table 1: The teams that participate"
W18-3901,L16-1641,1,0.509642,"Missing"
W18-3901,W17-1224,1,0.744067,"Missing"
W18-3901,W18-3923,1,0.879634,"Missing"
W18-3901,W14-5307,1,0.857122,"Missing"
W18-3901,W15-5401,1,0.855743,"Missing"
W18-3901,W17-1201,1,0.607978,"Missing"
W18-3917,Q17-1010,0,0.0296448,"shown in Table 2. We performed our experiments on each of the above mentioned issues subsequently, always propagating to the next experiment set the setup achieving best results in the previous one. The setup we start with consists only of the main network, without the character-level subnetwork. 2.3.1 Word Embeddings The first group of results considers different ways of pretraining word embeddings. The word embeddings were always pretrained on the web data available for each language. We considered only two tools for pretraining word embeddings: word2vec (Mikolov et al., 2013) and fasttext (Bojanowski et al., 2017), and two architectures, CBOW and Skipgram. The results (word2vec cbow vs. word2vec skipgram) show for Skipgram to be significantly better suited for this task, which is in line with previous results (Reimers and Gurevych, 2017). Comparing word2vec and fasttext (word2vec skipgram vs. fasttext skipgram), fasttext shows a slightly better performance, but the difference gets more obvious (almost half a point in token accuracy) once fasttext is used to generate representations for the words not present in the pretrained word embeddings (fasttext skipgram generated).3 2 https://github.com/neulab/dy"
W18-3917,J92-4003,0,0.296907,"cased focus token (token for which features are being extracted) • lowercased tokens in a window of {−3, −2, −1, 1, 2, 3} form the focus token • focus token suffixes of length {1, 2, 3, 4} • features encoding whether the focus token starts with http (link), # (hashtag) or @ (mention) • Brown cluster binary paths for the focus token, with the path length of {2, 4, 6, 8} These features were proven to yield optimal results in our previous work on tagging non-standard Slovene (Ljubeˇsi´c et al., 2017a). The Brown clusters, the output of a method for context-dependent hierarchical word clustering (Brown et al., 1992), were calculated from the web data that were made available through the shared task, namely the slWaC web corpus of Slovene (Erjavec et al., 2015) and the hrWaC and srWaC corpora of Croatian and Serbian (Ljubeˇsi´c and Klubiˇcka, 2014). We have used default parameters for calculating Brown clusters, except for the minimum occurrence parameter which was set to 5. The web text was previously lowercased and punctuations and newlines were removed from it. For training the tagger, we exploited (1) the proximity of the Croatian and Serbian language, and (2) the fact that we have much more standard"
W18-3917,P07-2053,0,0.0654006,"Missing"
W18-3917,W14-0405,1,0.905206,"Missing"
W18-3917,W17-1410,1,0.893365,"Missing"
W18-3917,L16-1242,1,0.85532,"Missing"
W18-3917,L16-1676,1,0.910783,"Missing"
W18-3917,W18-3901,1,0.900017,"Missing"
W18-5116,N16-2013,0,0.0486845,"r-generated content, there is increased pressure to manage inappropriate online content with (semi)automated methods. The research community is by now well aware of the multiple faces of inappropriateness in on-line communication, which preclude the use of simple vocabulary-based approaches, and are therefore turning to more robust machine learning methods (Pavlopoulos et al., 2017). These, however, require training data. Currently available datasets of inappropriate on-line communication are primarily datasets of English, such as a Twitter dataset annotated for racist and sexist hate speech (Waseem and Hovy, 2016)1 , the Wikimedia Toxicity Data Set (Wulczyn et al., 2017)2 , the Hate Speech Identifica3 https://data.world/crowdflower/ hate-speech-identification 4 https://github.com/sfu-discourse-lab/ SOCC 5 https://github.com/UCSM-DUE/IWG_ hatespeech_public 6 https://straintek.wediacloud.net/ static/gazzetta-comments-dataset/ gazzetta-comments-dataset.tar.gz 7 https://straintek.wediacloud.net/ static/gazzetta-comments-dataset/README. txt 1 https://github.com/ZeerakW/hatespeech https://figshare.com/projects/ Wikipedia_Talk/16731 2 124 Proceedings of the Second Workshop on Abusive Language Online (ALW2), p"
W18-5116,E17-2068,0,0.0753478,"Missing"
W18-5116,D17-1117,0,0.256497,"ja Fiˇser Faculty of Arts, University of Ljubljana Aˇskerˇceva cesta 2, 1000 Ljubljana, Slovenia darja.fiser@ff.uni-lj.si Abstract tion dataset containing tweets annotated as hate speech, offensive language, or neither (Davidson et al., 2017)3 , and the SFU Opinion and Comment Corpus consisting of online opinion articles and their comments annotated for toxicity4 . Datasets in other languages have recently also started to emerge, with a German Twitter dataset focused on the topic of refugees in Germany (Ross et al., 2017)5 and a Greek Sport News Comment dataset containing moderation metadata (Pavlopoulos et al., 2017)6 . In this paper we present two new and large datasets of news comments, one in Slovene, and one in Croatian. Apart from the texts, they also contain various metadata, the primary being whether the comment was removed by the site administrators. Given the sensitivity of the content, we publish the datasets in full-text form, but with user metadata semi-anonymised and the comment content encrypted via a simple character replacement method using a random, undisclosed bijective mapping, similar to the encryption method applied to the Gazzetta Greek Sport News Comments dataset7 introduced in Pavl"
W19-3704,L16-1242,1,0.877956,"Missing"
W19-3704,L16-1676,1,0.90033,"Missing"
W19-3704,K18-2016,0,0.0673766,"kola.ljubesic@ijs.si Abstract Ljubeˇsi´c et al., 2016), the previous state-of-theart for morphosyntactic tagging and lemmatisation of the three focus languages due to (1) carefully engineered features for the CRF-based tagger, (2) integration of an inflectional lexicon both for the morphosyntactic tagging and the lemmatisation task and (3) lemma guessing for unknown word forms via morphosyntactic-tagspecific Naive Bayes classifiers, predicting the transformation of the surface form. The tool that we use as the representative for the neural approaches is stanfordnlp, the Stanford NLP pipeline (Qi et al., 2018), a state-of-the-art in neural morphosyntactic and dependency syntax text annotation. The system took part in the CoNLL 2018 shared task (Zeman et al., 2018) as one of the best-performing systems, which would have, with ”an unfortunate bug fixed”, placed among the top-three for all evaluation metrics, including lemmatisation and morphology prediction. The tool is, additionally, released as open source and has a vivid development community,1 with a named entity recognition module being in development. We present experiments on Slovenian, Croatian and Serbian morphosyntactic annotation and lemma"
W19-3704,K18-2001,0,0.152537,"of the three focus languages due to (1) carefully engineered features for the CRF-based tagger, (2) integration of an inflectional lexicon both for the morphosyntactic tagging and the lemmatisation task and (3) lemma guessing for unknown word forms via morphosyntactic-tagspecific Naive Bayes classifiers, predicting the transformation of the surface form. The tool that we use as the representative for the neural approaches is stanfordnlp, the Stanford NLP pipeline (Qi et al., 2018), a state-of-the-art in neural morphosyntactic and dependency syntax text annotation. The system took part in the CoNLL 2018 shared task (Zeman et al., 2018) as one of the best-performing systems, which would have, with ”an unfortunate bug fixed”, placed among the top-three for all evaluation metrics, including lemmatisation and morphology prediction. The tool is, additionally, released as open source and has a vivid development community,1 with a named entity recognition module being in development. We present experiments on Slovenian, Croatian and Serbian morphosyntactic annotation and lemmatisation between the former stateof-the-art for these three languages and one of the best performing systems at the CoNLL 20"
W19-8004,W15-5301,1,0.851002,"uage resources, as well. This is especially true for morphological annotation (lemmatization, PoS tagging and morphological feature prediction), as many languages employ much larger morphology-annotated corpora than the costly (sub)corpora annotated for syntax, as well as morphological lexicons, essential for high-quality processing of languages with complex morphology. Examples of such cases are Croatian and Slovenian, two South Slavic languages with rich inflection. Their official UD releases include the conversions of the largest syntactically annotated corpora available for each language (Agić and Ljubešić, 2015; Dobrovoljc et al., 2017a), however, other manually created resources, such as the larger morphologically annotated corpora (Ljubešić et al., 2018b; Krek et al., 2019) and inflectional lexicons (Ljubešić, 2019; Dobrovoljc et al., 2019), have also been developed to support the development of related NLP tools (Ljubešić and Erjavec, 2016; Grčar et al., 2012) in the past. The aim of this paper is to present the conversion of these resources to the UD formalism and explore their potential contribution to the state-of-the-art in UD processing for both languages, from lemmatization to morphology an"
W19-8004,W17-1406,1,0.910535,"This is especially true for morphological annotation (lemmatization, PoS tagging and morphological feature prediction), as many languages employ much larger morphology-annotated corpora than the costly (sub)corpora annotated for syntax, as well as morphological lexicons, essential for high-quality processing of languages with complex morphology. Examples of such cases are Croatian and Slovenian, two South Slavic languages with rich inflection. Their official UD releases include the conversions of the largest syntactically annotated corpora available for each language (Agić and Ljubešić, 2015; Dobrovoljc et al., 2017a), however, other manually created resources, such as the larger morphologically annotated corpora (Ljubešić et al., 2018b; Krek et al., 2019) and inflectional lexicons (Ljubešić, 2019; Dobrovoljc et al., 2019), have also been developed to support the development of related NLP tools (Ljubešić and Erjavec, 2016; Grčar et al., 2012) in the past. The aim of this paper is to present the conversion of these resources to the UD formalism and explore their potential contribution to the state-of-the-art in UD processing for both languages, from lemmatization to morphology and syntax prediction. Usin"
W19-8004,erjavec-etal-2010-jos,1,0.779291,"development, the content and the availability of the extended UD resources for Slovenian and Croatian, namely the larger training sets for UD morphology (the ssj500k and hr500k corpora) and the large-scale UD-compliant lexicons of inflected forms (Sloleks and hrLex). Given the methodological differences in resource development for both languages due to divergent project frameworks and scopes, we present the resources by language rather than type. However, a brief quantitative overview and comparison is given at the end of the section. 2.1 Slovenian resources Both the ssj500k training corpus (Erjavec et al., 2010) and the Sloleks lexicon of inflected forms (Dobrovoljc et al., 2017b) adopt the JOS morphosyntactic annotation scheme (Erjavec and Krek, 2008), compatible with MULTEXT-East morphosyntactic specifications (Erjavec, 2012), which define the part-of-speech categories for Slovene, their morphological features (attributes) and values, and their mapping to morphosyntactic descriptions (MSDs).1 An automatic rule-based mapping from JOS to UD part-of-speech tags and features had already been developed as part of the original Slovenian UD Treebank conversion from the syntactically annotated subset of th"
W19-8004,erjavec-krek-2008-jos,1,0.705466,"D morphology (the ssj500k and hr500k corpora) and the large-scale UD-compliant lexicons of inflected forms (Sloleks and hrLex). Given the methodological differences in resource development for both languages due to divergent project frameworks and scopes, we present the resources by language rather than type. However, a brief quantitative overview and comparison is given at the end of the section. 2.1 Slovenian resources Both the ssj500k training corpus (Erjavec et al., 2010) and the Sloleks lexicon of inflected forms (Dobrovoljc et al., 2017b) adopt the JOS morphosyntactic annotation scheme (Erjavec and Krek, 2008), compatible with MULTEXT-East morphosyntactic specifications (Erjavec, 2012), which define the part-of-speech categories for Slovene, their morphological features (attributes) and values, and their mapping to morphosyntactic descriptions (MSDs).1 An automatic rule-based mapping from JOS to UD part-of-speech tags and features had already been developed as part of the original Slovenian UD Treebank conversion from the syntactically annotated subset of the ssj500k corpus (Dobrovoljc et al., 2017a), with the conversion scripts now publicly available at the CLARIN.SI GitHub repository.2 The large"
W19-8004,L16-1498,0,0.0717467,"Missing"
W19-8004,T87-1035,0,0.338695,"Missing"
W19-8004,W19-3704,1,0.743771,"Missing"
W19-8004,L16-1242,1,0.827209,"of languages with complex morphology. Examples of such cases are Croatian and Slovenian, two South Slavic languages with rich inflection. Their official UD releases include the conversions of the largest syntactically annotated corpora available for each language (Agić and Ljubešić, 2015; Dobrovoljc et al., 2017a), however, other manually created resources, such as the larger morphologically annotated corpora (Ljubešić et al., 2018b; Krek et al., 2019) and inflectional lexicons (Ljubešić, 2019; Dobrovoljc et al., 2019), have also been developed to support the development of related NLP tools (Ljubešić and Erjavec, 2016; Grčar et al., 2012) in the past. The aim of this paper is to present the conversion of these resources to the UD formalism and explore their potential contribution to the state-of-the-art in UD processing for both languages, from lemmatization to morphology and syntax prediction. Using the stanfordnlp tool, we investigate the impact of newly available data on all three tasks by (1) retraining the tagging and lemmatization models on larger training sets and (2) performing a simple lexicon lookup intervention in the lemmatization procedure. This paper is structured as follows. We first briefly"
W19-8004,L16-1676,1,0.863421,"les for pronouns and determiners, adverbs, numbers and the negated auxiliary.5 The only non-automatic part of the mapping was the resolution of the category of abbreviations from MULTEXT-East to the corresponding parts-of-speech. The resulting hr500k corpus was part of the initial release of hr500k (v1.0) and was published under CC BY-SA 4.0 (Ljubešić et al., 2018b). 2.2.2 hrLex inflectional lexicon The hrLex inflectional lexicon (Ljubešić, 2019) is currently the largest inflectional lexicon of Croatian. The process of semi-automatically building the hrLex inflectional lexicon is described in Ljubešić et al. (2016). 3 https://reldi.spur.uzh.ch 4 https://github.com/nljubesi/hr500k/blob/master/mte5-udv2.mapping 5 https://github.com/vukbatanovic/SETimes.SR/blob/master/msd_mapper.py The mapping of the MULTEXT-East tags that were initially present in the lexicon to the UPOS and FEATS layers was performed by applying the mapping that was used to map the hr500k training corpus to these layers, without the need for the manual mapping. The UD information became part of the hrLex lexicon with version 1.3 (Ljubešić, 2019), when the lexicon was published under the CC BY-SA 4.0 license. The lexicon is published as a"
W19-8004,petrov-etal-2012-universal,0,0.134172,"Missing"
W19-8004,K18-2016,0,0.0565309,"o explain the large difference in the ambiguity of the two lexicons, as possessive adjectives and proper nouns have a somewhat higher ambiguity than the remainder of the lexicon: possessive adjectives have an ambiguity of 4.68, and proper nouns of 3.78. 3 Experiment setup 3.1 Tool We perform experiments on morphosyntactic tagging, lemmatization and dependency parsing via the stanfordnlp tool, one of the best-performing systems in the CoNLL shared task in 2018 (Zeman et al., 2018) with code released and a vivid development community.6 The details on the implementation of the tool are given in (Qi et al., 2018). The tool assumes that morphosyntactic tagging is performed first, producing the UPOS and FEATS annotation layer. Next, lemmatization is performed by using the UPOS (but not FEATS) predictions. Finally, parsing is performed by exploiting all previously predicted layers (UPOS, FEATS and LEMMA). We investigate the impact of additional data on all three tasks by (1) retraining morphosyntactic tagging and lemmatization models with more data and (2) performing a simple intervention in the lemmatization procedure so that the lexicon lookup is not performed over the training data only, but the exter"
W19-8004,K18-2001,0,0.0500359,"in Figure 1. In any case, the different principles in the creation of the lexicons account for the larger size of hrLex lexicon and also explain the large difference in the ambiguity of the two lexicons, as possessive adjectives and proper nouns have a somewhat higher ambiguity than the remainder of the lexicon: possessive adjectives have an ambiguity of 4.68, and proper nouns of 3.78. 3 Experiment setup 3.1 Tool We perform experiments on morphosyntactic tagging, lemmatization and dependency parsing via the stanfordnlp tool, one of the best-performing systems in the CoNLL shared task in 2018 (Zeman et al., 2018) with code released and a vivid development community.6 The details on the implementation of the tool are given in (Qi et al., 2018). The tool assumes that morphosyntactic tagging is performed first, producing the UPOS and FEATS annotation layer. Next, lemmatization is performed by using the UPOS (but not FEATS) predictions. Finally, parsing is performed by exploiting all previously predicted layers (UPOS, FEATS and LEMMA). We investigate the impact of additional data on all three tasks by (1) retraining morphosyntactic tagging and lemmatization models with more data and (2) performing a simpl"
