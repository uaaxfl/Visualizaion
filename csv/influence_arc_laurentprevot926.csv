2009.jeptalnrecital-court.5,W04-2322,0,0.0343493,"Missing"
2009.jeptalnrecital-court.5,J05-2005,0,0.110676,"Missing"
2011.jeptalnrecital-court.27,2010.jeptalnrecital-court.32,1,0.836904,"Missing"
2011.jeptalnrecital-court.27,2010.jeptalnrecital-demonstration.11,1,0.875841,"Missing"
2020.lrec-1.69,2020.isa-1.1,1,0.705423,"cs of the SemAF-SR markup language. See https://dit.uvt.nl for details. Quantifiers and Modifiers A plug-in for semantic content is more general and more powerful as it takes more aspects into account of the meanings of phrases, clauses, sentences, and other natural language structures. On top of the identification of events with their time, place, and participants with their respective roles, the interpretation of quantifier and modifier structures forms the most important source of semantic information. The ISO standard under development (ISO-DIS 24617-12 (see Bunt, Pustejovsky & Lee, 2018; Bunt, 2020) can be the basis of a plug-in for this type of information. 6.2.2. Rhetorical Relations Reflecting the lack of theoretical consensus in this area, ISO 24617-2 does not specify any particular set of relations to be used when marking up rhetorical relations between dialogue acts, Users of the ISO scheme have often employed a variant of the relation set defined in ISO standard 24617-8 (‘DR-Core’). This is a set of 18 ‘core’ relations that occur in many annotation schemes. Using ISO 24617-2 (first edition), two problems were noted when annotating rhetorical relations. First, many rhetorical relat"
2020.lrec-1.69,L18-1282,1,0.87873,"Missing"
2020.lrec-1.69,bunt-romary-2004-standardization,1,0.604556,"Missing"
2020.lrec-1.69,L16-1020,0,0.0448338,"Missing"
2020.lrec-1.69,geertzen-etal-2008-evaluating,1,0.685929,"r speaker beliefs and intentions. This enables expert annotators to assign fine-grained characterisations to segments of dialogue behaviour with high accuracy. To support manual annotation, the annotation scheme should therefore include fine-grained concepts with the level of detail that expert annotators can use. On the other hand, manual annotation is often done by annotators who do not have quite the skills of experts, partly because of the high cost, but also because it is interesting to know whether an annotation scheme can successfully be used by less skilled or even ‘naive’ annotators (Geertzen et al., 2008); for such annotators it is useful if the annotation scheme includes less fine-grained concepts. In both DIT++ and ISO 24617-2 these two requirements are met by using a hierarchically structured set of more and less specific communicative functions. UC2: Automatic annotation of human-human dialogue, or of the user’s contributions in a human-computer dialogue, typically lacks the general world knowledge and the skills of expert human annotators, and has limited access to context information - often only as far as represented in the dialogue history. Automatic annotation therefore in general can"
2020.lrec-1.69,L18-1633,1,0.872073,"Missing"
2020.lrec-1.69,W10-1840,1,0.741963,"semantic information. A link structure contains information about the way two or more segments of primary data are semantically related. In the abstract syntax of the Dialogue Act Markup Language (DiAML) of the ISO 24617-2 scheme, an entity structure is a formal specification of a dialogue act, while link structures describe rhetorical relations between dialogue acts. The annotation structures defined by the abstract syntax can be represented (or ‘encoded’) in a many ways; XML is the most popular representation format, but other formats, such as attribute-value matrices or annotation graphs (Ide and Bunt, 2010) are equally possible. Bunt et al. (2019) describe alternative representation formats for DiAML annotation structures and how they are formally related. Every concrete syntax must satisfy two requirements: (1) completeness, which means that every structure defined by the abstract syntax has a representation, and (2) unambiguity, which means that every representation structure encodes exactly one annotation structure. These requirements are formalized in the DiAML specification by the stipulation that the concrete syntax definition includes (1) an encoding function for the structures generated"
2020.lrec-1.69,W06-1306,1,0.599852,"Missing"
2020.lrec-1.69,W19-5945,1,0.915658,"d interfaces to ISO 24617-2 for enriching the description of individual dialogue acts and dialogue act sequences; Dimensions: Due to the orthogonality of the set of dimensions, additional dimensions may be introduced • predefined plug-ins and interfaces for: 1 – rhetorical relations between dialogue acts; – the semantic content of dialogue acts; – fine-grained communicative functions for feedback; See e.g. Fang et al., 2012; Chowdhury et al. (2014); Petukhova et al. (2014), Gilmartin et al., 2018; Ngo et al. (2018); Bunt et al. (2019); Mezza et al. (2019). 2 See e.g. Keizer and Rieser (2018); Keizer et al. (2019), Malchanau (2019), Malchanau et al. (2019) 549 – application-specific communicative functions; – emotional aspects of dialogue acts. ISO 24617-2 has remained fixed since its publication in 2012, as is appropriate for a good standard. The DIT++ annotation scheme, on which the ISO scheme is based, has evolved in order to accommodate inaccuracies and limitations encountered in use, The changes in ISO 24617-2 have first been implemented in DIT++ Release 5.2 (see https://dit.uvt.nl/#Release5.2). This paper describes the changes in the ISO standard. Section 2 documents the changes relating to the a"
2020.lrec-1.69,C18-1300,0,0.0469312,"Missing"
2020.lrec-1.69,L18-1630,0,0.0179685,"functions specific for this dimension; • a mechanism for using ‘layered’ plug-ins with structured interfaces to ISO 24617-2 for enriching the description of individual dialogue acts and dialogue act sequences; Dimensions: Due to the orthogonality of the set of dimensions, additional dimensions may be introduced • predefined plug-ins and interfaces for: 1 – rhetorical relations between dialogue acts; – the semantic content of dialogue acts; – fine-grained communicative functions for feedback; See e.g. Fang et al., 2012; Chowdhury et al. (2014); Petukhova et al. (2014), Gilmartin et al., 2018; Ngo et al. (2018); Bunt et al. (2019); Mezza et al. (2019). 2 See e.g. Keizer and Rieser (2018); Keizer et al. (2019), Malchanau (2019), Malchanau et al. (2019) 549 – application-specific communicative functions; – emotional aspects of dialogue acts. ISO 24617-2 has remained fixed since its publication in 2012, as is appropriate for a good standard. The DIT++ annotation scheme, on which the ISO scheme is based, has evolved in order to accommodate inaccuracies and limitations encountered in use, The changes in ISO 24617-2 have first been implemented in DIT++ Release 5.2 (see https://dit.uvt.nl/#Release5.2). Thi"
2020.lrec-1.69,prasad-etal-2008-penn,0,0.126217,"ents undesirable limitations due to the fact that classes of linguistic or communicative phenomena are hard to define in such a way that there is no overlap or interference with phenomena that fall outside the scope. The annotation of information about time and events using TimeML, for example, runs into problems in the annotation of quantification (as in ”every Monday” or ”most Wednesdays”), and overlaps with semantic role labelling schemes for roles of a temporal character, like the start- and end times and the duration of an event. Likewise, PDTB annotation of rhetorical relations in text (Prasad et al., 2008) struggles with occurrences of negation. Such limitations restrict the applicability of annotation schemes in use case UC1. Generality: Annotation standards are designed to be applicable across domains and applications, and therefore do not include concepts that are specific for certain domains or applications. This is part of their strength, but is sometimes also a weakness, especially for applications where it is deemed essential to include the use of specialised domain-specific concepts. Such a situation brings the need to customise the standard by adding the relevant specialised concepts."
2020.lrec-1.84,W13-4011,1,0.787135,"inished his speech turn. Reaction times have positive values in case of a pause between turn taking, and negative values in case of overlapping speech of interlocutors. Lexical richness was computed considering the amount of spoken tokens and adjectives plus adverbs divided by a total number of extracted tokens (including adjectives, adverbs, auxiliary words, conjunction, determiners, nouns, prepositions, pronouns, verbs) (Ochs, Jain, & Blache, 2018). Furthermore we considered filled breaks (i.e., utterances like “mmh” during pauses of active speech) (Swerts, 1998) and lexical feedback items (Prévot, Bigi, & Bertrand, 2013; Prévot, Gorish, & Mukherjee, 2015), representing expressions to communicate perception and understanding, as well as reactions to what the conversational partner had said (E.g.; “yes”, “no”, “okay”, etc.) (Gravano, Hirschberg, & Beňuš, 2011). Additionally, we extracted discourse markers, used to organize the ongoing discourse, such as “so” or “therefore” (Schiffrin, 1987), particle items, which express the speaker’s mood (Barnes, 1995) and also laughter (Ellis, 1997). We further calculated subjectivity and polarity of speech with the Pattern library (Smedt & Daelemans, 2012). This sentiment"
2020.lrec-1.84,Y15-1034,1,0.734986,"n times have positive values in case of a pause between turn taking, and negative values in case of overlapping speech of interlocutors. Lexical richness was computed considering the amount of spoken tokens and adjectives plus adverbs divided by a total number of extracted tokens (including adjectives, adverbs, auxiliary words, conjunction, determiners, nouns, prepositions, pronouns, verbs) (Ochs, Jain, & Blache, 2018). Furthermore we considered filled breaks (i.e., utterances like “mmh” during pauses of active speech) (Swerts, 1998) and lexical feedback items (Prévot, Bigi, & Bertrand, 2013; Prévot, Gorish, & Mukherjee, 2015), representing expressions to communicate perception and understanding, as well as reactions to what the conversational partner had said (E.g.; “yes”, “no”, “okay”, etc.) (Gravano, Hirschberg, & Beňuš, 2011). Additionally, we extracted discourse markers, used to organize the ongoing discourse, such as “so” or “therefore” (Schiffrin, 1987), particle items, which express the speaker’s mood (Barnes, 1995) and also laughter (Ellis, 1997). We further calculated subjectivity and polarity of speech with the Pattern library (Smedt & Daelemans, 2012). This sentiment analysis is an automated process th"
2020.lrec-1.89,J12-1001,0,0.0838221,"Missing"
2020.lrec-1.89,2020.lrec-1.84,1,0.799095,"Missing"
2020.paclic-1.5,W02-0203,0,0.297614,"Missing"
2020.paclic-1.5,J07-3005,0,0.104251,"Missing"
2020.paclic-1.5,W03-2106,0,0.203337,"Missing"
2020.paclic-1.5,P05-1031,0,0.129814,"Missing"
2020.paclic-1.5,J00-3003,0,0.759666,"Missing"
2020.paclic-1.5,J11-1005,0,0.0157144,"Missing"
2020.sigdial-1.25,N16-1174,0,0.0273755,"Missing"
2021.cmcl-1.7,C16-1066,0,0.0186748,"s Affair at Styles, for a total of 54, 364 tokens, it contains eye-tracking data from 33 subjects, both English native speakers (14) and bilingual speakers of Dutch and English (19), and comes with the Dutch counterpart. The Provo corpus (Luke and Christianson, 2017) contains 55 short English texts about various topics, with 2.5 sentences and 50 words on average, for a total of 2, 689 tokens, and eye-tracking measures collected from 85 subjects. Annotated eye-tracking corpora are also available for other languages, including German (Kliegl et al., 2006), Hindi (Husain et al., 2015), Japanese (Asahara et al., 2016) and Russian (Laurinavichyute et al., 2019), among others. 3 Feature N F IX FFD GPT TRT FIX P ROP min max mean (std) 0.0 0.0 0.0 0.0 0.0 7.25 296.8 2424.9 996.2 1.0 1.1 (0.7) 77.3 (34.4) 154.1 (143.6) 128.8 (88.6) 0.67 (0.26) Table 1: Minimum, maximum, mean and standard deviation of the feature values before scaling in both training and test data, after averaging across readers. Feature min max mean (std) N F IX FFD GPT TRT FIX P ROP 0.0 0.0 0.0 0.0 0.0 100.0 12.2 100.0 41.1 100.0 15.1 (9.5) 3.2 (1.4) 6.4 (5.9) 5.3 (3.7) 67.1 (26.0) Table 2: Minimum, maximum, mean and standard deviation of the"
2021.cmcl-1.7,2021.cmcl-1.17,0,0.0950795,"Missing"
2021.cmcl-1.7,P16-2094,0,0.0756797,"orpus (ZuCo). Eyetracking data were recorded during natural reading of English sentences. In total, we received submissions from 13 registered teams, whose systems include boosting algorithms with handcrafted features, neural models leveraging transformer language models, or hybrid approaches. The winning system used a range of linguistic and psychometric features in a gradient boosting framework. 1 Figure 1: Example sentence from the ZuCo corpus read by a single reader. The blue dots mark fixations on the corresponding words above, a wider diameter represent a longer fixation duration. data (Barrett et al., 2016; Hollenstein et al., 2019; Toneva and Wehbe, 2019). Thanks to the recent introduction of a standardized dataset (Hollenstein et al., 2018, 2020), it is now possible to compare the capabilities of machine learning approaches to model and analyze human patterns of reading. In this shared task, we present the challenge of predicting eye word-level tracking-based metrics recorded during English sentence processing. We encouraged submissions concerning both cognitive modeling and linguistically motivated approaches (e.g., language models). All data files are available on the competition website.1"
2021.cmcl-1.7,2021.cmcl-1.14,0,0.034899,"osting methods using tree-based algorithms with extensive feature extraction (e.g., CatBoost2 or LightGBM3 ), 7 Results In this section, we describe the prediction performance achieved by the participating teams. The official results of this shared task are presented in Table 3. The best results were achieved by a linguistic feature-based approach (Bestgen, 2021). As described above, other teams opted for neural 2 https://catboost.ai/ https://lightgbm.readthedocs.io/en/ latest/ 3 75 References approaches (e.g., Li and Rudzicz, 2021 and Oh, 2021) or hybrid approaches (e.g., Yu et al., 2021 and Choudhary et al., 2021), combining linguistic features and state-of-the-art language representations. Raksha Agarwal and Niladri Chatterjee. 2021. LangResearchLab_NC at CMCL2021 Shared Task: Predicting Gaze Behaviour using Linguistic Features and Tree Regressors. In Proceedings of the NAACL Workshop on Cognitive Modeling and Computational Linguistics. The difficulty of predicting the individual eyetracking features is analogous in all submitted systems. FFD is the most accurately predicted feature. This seems to suggest that the models are more capable to capture early processing stages of lexical access compared to"
2021.cmcl-1.7,N06-1038,0,0.0202231,"e 2: Boxplot showing the feature value distributions of both training and test sets. Below each box is the median value of each feature. contain any information that can be linked to the participants. The eye-tracking data was recorded with an EyeLink 1000 system in a series of naturalistic reading experiments. Full sentences were presented at the same position on the screen one at a time. The participants read each sentence at their own reading speed. The reading material included sentences from movie reviews from the Stanford Sentiment Treebank (Socher et al., 2013) and a Wikipedia dataset (Culotta et al., 2006). For a detailed description of the data acquisition, please refer to the original publications. An example sentence is presented in Figure 1. We use the normal reading paradigms from ZuCo, i.e, Task 1 and Task 2 from ZuCo 1.0, and all tasks from ZuCo 2.0. We extracted the eyetracking data from all 12 subjects from ZuCo 1.0 and all 18 subjects from ZuCo 2.0. The dataset contains 990 sentences. All sentences were shuffled randomly before splitting into training and test sets. The training data contains 800 sentences, and the test data 190 sentences. 4.1 endings are marked with an &lt;EOS&gt; symbol a"
2021.cmcl-1.7,2021.cmcl-1.13,0,0.0888806,"Missing"
2021.cmcl-1.7,N19-1423,0,0.0272424,"res. 5.2 Features The features included for training the systems include surface features (e.g., word length, sentence length, word positions in the sentence), lexical features (e.g., lemmas, named entities) token probability features (word frequency and ngram metrics), syntactic features (e.g., part-ofspeech tags and dependency parsing), text complexity metrics, behavioral measures, (e.g., concreteness, familiarity, age of acquisition), context features (i.e., information about the preceding and following tokens) as well as representations from state-of-the-art language models, such as BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019), and XLNet (Yang et al., 2019). Mean Baseline We use the mean central tendency as a baseline for this regression problem, i.e., we calculate the mean value for each feature from the training data and use it as a prediction for all words in the test data. Table 3 shows the MAE scores achieved by this mean baseline for each eye-tracking feature. 6 Participating Teams & Systems 13 teams and a total of 42 participants registered on the competition website. All 13 teams, including 26 registered participants, submitted their predictions during the evaluation phase. Each"
2021.cmcl-1.7,W12-1706,0,0.027643,"prisal theory (Hale, 2001; Levy, 2008), which claims that the processing difficulty of a word is proportional to its surprisal, i.e., the negative logarithm of the probabil1 https://competitions.codalab.org/ competitions/28176 72 Proceedings of the Workshop on Cognitive Modeling and Computational Linguistics, pages 72–78 Online Event, June 10, 2021. ©2021 Association for Computational Linguistics ity of the word given the context. Surprisal theory was the reference framework for several studies on language models and eye-tracking data prediction (Demberg and Keller, 2008; Frank and Bod, 2011; Fossum and Levy, 2012). These studies use the data from the Dundee Corpus (Kennedy et al., 2003), which consists of sentences from British newspapers with eye-tracking measurements from 10 participants, as one of the earliest and most popular benchmarks. Later work on the topic found that the perplexity of a language model is the primary factor determining the fit to human reading times (Goodkind and Bicknell, 2018), a result that was confirmed also by the recent investigations involving neural language models such as GRU networks (Aurnhammer and Frank, 2019) and Transformers (Merkx and Frank, 2020; Wilcox et al.,"
2021.cmcl-1.7,2021.cmcl-1.9,0,0.0304026,"he training data and use it as a prediction for all words in the test data. Table 3 shows the MAE scores achieved by this mean baseline for each eye-tracking feature. 6 Participating Teams & Systems 13 teams and a total of 42 participants registered on the competition website. All 13 teams, including 26 registered participants, submitted their predictions during the evaluation phase. Each team was allowed three submissions during the evaluation phase. Finally, 10 teams published system description papers outlining their approach (see Table 3 for all references). Additional data Only one team (Li and Rudzicz, 2021) used external eye-tracking data, leveraging the Provo corpus (Luke and Christianson, 2017) for additional word-level eye movement samples. Methods The participating teams submitted predictions generated from various approaches. Mainly two methods were used: (1) Boosting methods using tree-based algorithms with extensive feature extraction (e.g., CatBoost2 or LightGBM3 ), 7 Results In this section, we describe the prediction performance achieved by the participating teams. The official results of this shared task are presented in Table 3. The best results were achieved by a linguistic feature-"
2021.cmcl-1.7,W18-0102,0,0.0165043,"guistics ity of the word given the context. Surprisal theory was the reference framework for several studies on language models and eye-tracking data prediction (Demberg and Keller, 2008; Frank and Bod, 2011; Fossum and Levy, 2012). These studies use the data from the Dundee Corpus (Kennedy et al., 2003), which consists of sentences from British newspapers with eye-tracking measurements from 10 participants, as one of the earliest and most popular benchmarks. Later work on the topic found that the perplexity of a language model is the primary factor determining the fit to human reading times (Goodkind and Bicknell, 2018), a result that was confirmed also by the recent investigations involving neural language models such as GRU networks (Aurnhammer and Frank, 2019) and Transformers (Merkx and Frank, 2020; Wilcox et al., 2020; Hao et al., 2020). Using an alternative approach, Bautista and Naval (2020) obtained good results for the prediction of eye movements with autoencoders. In addition to the ZuCo corpus used for this shared task (see Section 4), there are several other resources of eye-tracking data for English. The Ghent Eye-Tracking Corpus (GECO; Cop et al., 2017) is composed of the entire Agatha Christie"
2021.cmcl-1.7,2021.ccl-1.108,0,0.058169,"Missing"
2021.cmcl-1.7,N01-1021,0,0.498484,"ocessing and computer vision. Not only can it reveal the workings of the underlying cognitive processes of language understanding, but the performance of computational models can also be improved if their inductive bias is adjusted using human cognitive signals such as eye-tracking, fMRI, or EEG 2 Related Work Research on naturalistic reading has shown that fixation patterns are influenced by the predictability of words in their sentence context (Ehrlich and Rayner, 1981). In natural language processing and psycholinguistics, the most influential account of the phenomenon is surprisal theory (Hale, 2001; Levy, 2008), which claims that the processing difficulty of a word is proportional to its surprisal, i.e., the negative logarithm of the probabil1 https://competitions.codalab.org/ competitions/28176 72 Proceedings of the Workshop on Cognitive Modeling and Computational Linguistics, pages 72–78 Online Event, June 10, 2021. ©2021 Association for Computational Linguistics ity of the word given the context. Surprisal theory was the reference framework for several studies on language models and eye-tracking data prediction (Demberg and Keller, 2008; Frank and Bod, 2011; Fossum and Levy, 2012). T"
2021.cmcl-1.7,2020.cmcl-1.10,0,0.0217989,"e studies use the data from the Dundee Corpus (Kennedy et al., 2003), which consists of sentences from British newspapers with eye-tracking measurements from 10 participants, as one of the earliest and most popular benchmarks. Later work on the topic found that the perplexity of a language model is the primary factor determining the fit to human reading times (Goodkind and Bicknell, 2018), a result that was confirmed also by the recent investigations involving neural language models such as GRU networks (Aurnhammer and Frank, 2019) and Transformers (Merkx and Frank, 2020; Wilcox et al., 2020; Hao et al., 2020). Using an alternative approach, Bautista and Naval (2020) obtained good results for the prediction of eye movements with autoencoders. In addition to the ZuCo corpus used for this shared task (see Section 4), there are several other resources of eye-tracking data for English. The Ghent Eye-Tracking Corpus (GECO; Cop et al., 2017) is composed of the entire Agatha Christie’s novel The Mysterious Affair at Styles, for a total of 54, 364 tokens, it contains eye-tracking data from 33 subjects, both English native speakers (14) and bilingual speakers of Dutch and English (19), and comes with the Du"
2021.cmcl-1.7,2021.cmcl-1.11,0,0.0337718,"m various approaches. Mainly two methods were used: (1) Boosting methods using tree-based algorithms with extensive feature extraction (e.g., CatBoost2 or LightGBM3 ), 7 Results In this section, we describe the prediction performance achieved by the participating teams. The official results of this shared task are presented in Table 3. The best results were achieved by a linguistic feature-based approach (Bestgen, 2021). As described above, other teams opted for neural 2 https://catboost.ai/ https://lightgbm.readthedocs.io/en/ latest/ 3 75 References approaches (e.g., Li and Rudzicz, 2021 and Oh, 2021) or hybrid approaches (e.g., Yu et al., 2021 and Choudhary et al., 2021), combining linguistic features and state-of-the-art language representations. Raksha Agarwal and Niladri Chatterjee. 2021. LangResearchLab_NC at CMCL2021 Shared Task: Predicting Gaze Behaviour using Linguistic Features and Tree Regressors. In Proceedings of the NAACL Workshop on Cognitive Modeling and Computational Linguistics. The difficulty of predicting the individual eyetracking features is analogous in all submitted systems. FFD is the most accurately predicted feature. This seems to suggest that the models are more"
2021.cmcl-1.7,2021.cmcl-1.12,0,0.0571858,"Missing"
2021.cmcl-1.7,D13-1170,0,0.00305362,"ixProp (a) Training data. (b) Test data. Figure 2: Boxplot showing the feature value distributions of both training and test sets. Below each box is the median value of each feature. contain any information that can be linked to the participants. The eye-tracking data was recorded with an EyeLink 1000 system in a series of naturalistic reading experiments. Full sentences were presented at the same position on the screen one at a time. The participants read each sentence at their own reading speed. The reading material included sentences from movie reviews from the Stanford Sentiment Treebank (Socher et al., 2013) and a Wikipedia dataset (Culotta et al., 2006). For a detailed description of the data acquisition, please refer to the original publications. An example sentence is presented in Figure 1. We use the normal reading paradigms from ZuCo, i.e, Task 1 and Task 2 from ZuCo 1.0, and all tasks from ZuCo 2.0. We extracted the eyetracking data from all 12 subjects from ZuCo 1.0 and all 18 subjects from ZuCo 2.0. The dataset contains 990 sentences. All sentences were shuffled randomly before splitting into training and test sets. The training data contains 800 sentences, and the test data 190 sentences"
2021.cmcl-1.7,2020.lrec-1.18,1,0.7841,"Missing"
2021.cmcl-1.7,2021.cmcl-1.16,0,0.0867029,"Missing"
2021.cmcl-1.7,2021.cmcl-1.15,0,0.0332678,"ds were used: (1) Boosting methods using tree-based algorithms with extensive feature extraction (e.g., CatBoost2 or LightGBM3 ), 7 Results In this section, we describe the prediction performance achieved by the participating teams. The official results of this shared task are presented in Table 3. The best results were achieved by a linguistic feature-based approach (Bestgen, 2021). As described above, other teams opted for neural 2 https://catboost.ai/ https://lightgbm.readthedocs.io/en/ latest/ 3 75 References approaches (e.g., Li and Rudzicz, 2021 and Oh, 2021) or hybrid approaches (e.g., Yu et al., 2021 and Choudhary et al., 2021), combining linguistic features and state-of-the-art language representations. Raksha Agarwal and Niladri Chatterjee. 2021. LangResearchLab_NC at CMCL2021 Shared Task: Predicting Gaze Behaviour using Linguistic Features and Tree Regressors. In Proceedings of the NAACL Workshop on Cognitive Modeling and Computational Linguistics. The difficulty of predicting the individual eyetracking features is analogous in all submitted systems. FFD is the most accurately predicted feature. This seems to suggest that the models are more capable to capture early processing stages o"
afantenos-etal-2012-empirical,C10-1001,1,\N,Missing
afantenos-etal-2012-empirical,J00-3005,0,\N,Missing
afantenos-etal-2012-empirical,J96-1002,0,\N,Missing
afantenos-etal-2012-empirical,J95-2003,0,\N,Missing
afantenos-etal-2012-empirical,J05-2005,0,\N,Missing
afantenos-etal-2012-empirical,prasad-etal-2008-penn,0,\N,Missing
afantenos-etal-2012-empirical,J86-3001,0,\N,Missing
afantenos-etal-2012-empirical,W04-2322,0,\N,Missing
afantenos-etal-2012-empirical,2009.jeptalnrecital-court.23,0,\N,Missing
bigi-etal-2014-representing,ide-etal-2000-xces,0,\N,Missing
bigi-etal-2014-representing,bird-etal-2000-atlas,0,\N,Missing
bigi-etal-2014-representing,bigi-2012-sppas-tool,1,\N,Missing
bigi-etal-2014-representing,W10-1829,1,\N,Missing
blache-etal-2010-otim,bigi-etal-2010-automatic,1,\N,Missing
C10-2008,W10-1829,1,0.907692,"(Allwood, 2005), both integrating McNeill’s gesture description (McNeill05). The following structure encodes the description of gesture phases, phrases (representing different semiotic types), the hand shape as well as its orientation, the gesture space, and the possible contact with bodies or objects. A last feature also describes the movement itself: trajectory, quality (fast, normal or slow) and amplitude (small, medium and large). M OVEMENT A MPLITUDE Amplitude_Type Q UALITY quality_Type Application We have experimented this modeling in the complete annotation of a multimodal corpus (see (Blache, 2010)). In this project, a complete TFS model has been first designed, covering all the different domains (prosody, syntax, gestures, discourse, etc.). From this model, the annotations have been created, leading to a 3-hours corpus of narrative dialogs, fully transcribed. The corpus is fully annotated for some domains (phonetics, prosody and syntax) and partly for others (gestures, discourse, disfluencies, specific phenomena). The result is one of the first large annotated multimodal corpus. 3 Graphs for Multimodal Annotation Graphs are frequently used in the representation of complex information,"
C10-2008,bird-etal-2000-atlas,0,0.0941573,"Missing"
C10-2008,J00-2002,0,0.0943733,"Missing"
C10-2008,W07-1524,0,0.0556121,"Missing"
C10-2008,W07-1501,0,0.0839278,"Missing"
C10-2008,wittenburg-etal-2006-elan,0,\N,Missing
C10-2008,W09-3004,0,\N,Missing
chung-etal-2008-extracting,P94-1019,0,\N,Missing
chung-etal-2008-extracting,huang-etal-2004-sinica,1,\N,Missing
gorisch-etal-2014-aix,kurtic-etal-2012-corpus,0,\N,Missing
gorisch-etal-2014-aix,bigi-2012-sppas-tool,1,\N,Missing
L16-1148,N09-2066,0,0.0452941,"Missing"
L16-1148,baroni-etal-2004-introducing,0,0.468739,", PLANT, POSSESSION , PROCESS , QUAN TITY, RELATION , SHAPE , STATE , SUBSTANCE , TIME. In this way, the selectional preferences that can be inferred from the lexical sets in sentence (4) can be characterized as: 6. [PERSON]subj lire [COMMUNICATION - ARTIFACT]obj If lexical sets describe the behavior of verbs as observed in a corpus, selectional preferences make an important generalization over the semantic properties of arguments. 3.3. A Resource on Italian Argument Structure The database described by Lenci et al. (2012) has been built by applying the LexIt framework to the ‘La Repubblica’ (Baroni et al., 2004) corpus (ca. 331 millions tokens) and to a dump of the Italian section of Wikipedia (ca. 152 millions of tokens). The resulting dataset in the former setting encodes 3,873 verbs, 12,766 nouns and 5,559 adjectives, while the latter setting resulted in the characterization of 2,831 verbs and 11,056 nouns. The LexIt extracted methodology has been evaluated by comparing the SCF frames available in three gold standard dictionaries for 100 test verbs against those automatically extracted from the ‘La Repubblica’ corpus, filtered by exploiting either a MLE-based threshold or a LMI-based threshold (se"
L16-1148,P13-1133,0,0.0160537,"ng its relative simplicity, this strategy turned to be significantly effective to limit the influence of both annotation errors (e.g., wrong syntactic parses) and marginally relevant dependency patterns, often due to a idiosyncratic sequences of adjuncts in a sentence. 4.3. • categorizing the fillers into WordNet supersenses by following the general methodology described by Resnik (1996). To extract the candidate synsets and general classes we resorted to the Wordnet Libre du Franc¸ais lexicon (Sagot and Fiˇser, 2008) available in the Open Multilingual WordNet repository (Bond and Paik, 2012; Bond and Foster, 2013); • aggregating the single co-occurrence for each information of interest (i.e., slots, SCFs, fillers, semantic classes), thus collecting, for each predicate of interest, its joint frequency with: 1. each SCF; 2. each slot (in isolation or in the context of each SCF) ; 3. each filler for given a slot (in isolation or in the context of each SCF) ; 4. each semantic class (in isolation or in the context of each SCF). • calculating the strength of association to be loaded in the distributional profiles. Various weighting measures can be selected, among which relative frequency and common associati"
L16-1148,J10-4007,0,0.379513,"Missing"
L16-1148,korhonen-etal-2006-large,0,0.0424249,"ense disambiguation, machine translation, knowledge extraction (Schulte im Walde, 2009; Korhonen, 2009), so that the automatic acquisition of argument structure information is a long-standing topic in computational linguistics. By embracing the theoretical assumption that the semantics of a predicate and the morpho-syntactic realization of its arguments are intimately related (Levin, 1993; Bresnan, 1996; Roland and Jurafsky, 2002; Levin and Rappaport Hovav, 2005), the last thirty years have witnessed the development of automatic methods for the identification of verb subcategorization frames (Korhonen et al., 2006; Messiant et al., 2008; Schulte im Walde, 2009), selectional preferences (Resnik, 1996; Light and Greiff, 2002; Erk et al., 2010) and diathesis alternation (McCarthy, 2001). The literature reports few examples of automatically built, wide coverage, lexica encoding this information, a.k.a. combinatorial lexica, among which notable mentions include VALEX for English verbs (Korhonen et al., 2006), LexSchem from French verbs (Messiant et al., 2008) and LexIt for Italian verbs, nouns and adjectives (Lenci et al., 2012). These resources represent a reference point for the work presented in these pa"
L16-1148,Y09-1003,0,0.231356,"and-built resources were counterbalanced by the higher cost-effectiveness and flexibility of the automatic methods, features that come in handy especially for languages, domains of topics for which hand-built resources are not available or are just too limited in their scope. Among the several kinds of information that can be included in a lexicon, the description of linguistic entities at the syntax-semantic interface proved to be useful for many traditional Natural Language Processing tasks such as word-sense disambiguation, machine translation, knowledge extraction (Schulte im Walde, 2009; Korhonen, 2009), so that the automatic acquisition of argument structure information is a long-standing topic in computational linguistics. By embracing the theoretical assumption that the semantics of a predicate and the morpho-syntactic realization of its arguments are intimately related (Levin, 1993; Bresnan, 1996; Roland and Jurafsky, 2002; Levin and Rappaport Hovav, 2005), the last thirty years have witnessed the development of automatic methods for the identification of verb subcategorization frames (Korhonen et al., 2006; Messiant et al., 2008; Schulte im Walde, 2009), selectional preferences (Resnik,"
L16-1148,R09-1038,0,0.0615085,"Missing"
L16-1148,lenci-etal-2012-lexit,1,0.848274,"of automatic methods for the identification of verb subcategorization frames (Korhonen et al., 2006; Messiant et al., 2008; Schulte im Walde, 2009), selectional preferences (Resnik, 1996; Light and Greiff, 2002; Erk et al., 2010) and diathesis alternation (McCarthy, 2001). The literature reports few examples of automatically built, wide coverage, lexica encoding this information, a.k.a. combinatorial lexica, among which notable mentions include VALEX for English verbs (Korhonen et al., 2006), LexSchem from French verbs (Messiant et al., 2008) and LexIt for Italian verbs, nouns and adjectives (Lenci et al., 2012). These resources represent a reference point for the work presented in these pages, where we investigated the possibility to automatically extract distributional information about French predicates by adapting an existing framework, LexIt. This led to the realization of LexFr, an automatically built French lexicon describing the syntactic and semantic properties of the argument structures of 2,493 verbs, 7,939 nouns and 2,628 adjectives. As in the original framework, the behavior of a group of target predicates is characterized by a series of statistical information (a.k.a. distributional pro"
L16-1148,messiant-etal-2008-lexschem,0,0.84581,"chine translation, knowledge extraction (Schulte im Walde, 2009; Korhonen, 2009), so that the automatic acquisition of argument structure information is a long-standing topic in computational linguistics. By embracing the theoretical assumption that the semantics of a predicate and the morpho-syntactic realization of its arguments are intimately related (Levin, 1993; Bresnan, 1996; Roland and Jurafsky, 2002; Levin and Rappaport Hovav, 2005), the last thirty years have witnessed the development of automatic methods for the identification of verb subcategorization frames (Korhonen et al., 2006; Messiant et al., 2008; Schulte im Walde, 2009), selectional preferences (Resnik, 1996; Light and Greiff, 2002; Erk et al., 2010) and diathesis alternation (McCarthy, 2001). The literature reports few examples of automatically built, wide coverage, lexica encoding this information, a.k.a. combinatorial lexica, among which notable mentions include VALEX for English verbs (Korhonen et al., 2006), LexSchem from French verbs (Messiant et al., 2008) and LexIt for Italian verbs, nouns and adjectives (Lenci et al., 2012). These resources represent a reference point for the work presented in these pages, where we investiga"
L16-1148,P08-3010,0,0.0267644,"exical elements, among which 6,825 verbs, 37,530 nouns and 10,483 adjectives. Its lexical framework, Alexina (Architecture pour les LEXiques INformatiques et leur Acquisition), has been successfully exploited to create Lefff -like resource in other languages such as Italian and Dutch. manual effort, namely in the need for manual correction of the output of the automatic module. LexSchem has been the first automatically built lexical resource characterizing the subcategorization behavior of a large set of French verbs (Messiant et al., 2008). This information has been extracted by using ASSCI (Messiant, 2008), a subcategorization frames acquisition system whose main task is to extract all the patterns for each target verb and exploit a MLE-based strategy (see section 5) to identify the more plausible set of subcategorization frames. By applying ASSCI to a newspaper corpus composed by 10 years of the French newspaper ‘Le Monde’, 336 subcategorization frames have been isolated and used to describe the combinatorial behavior of 3,297 French verbs. The goodness of the LexSchem framework has been tested by comparing the entries for 20 test verbs against a gold standard dictionary, thus showing 0.79 pre"
L16-1148,P07-1115,0,0.362513,"Extractor The goal of the first module is to analyze a dependencyparsed corpus in order to identify the occurrences of each predicate and to extract, for each occurrence with: the list of dependencies headed by the target predicate; the lexical elements filling each syntactic position. This process is carried out by an algorithm developed to filter and interpret the linguistic annotation available in the input. As a consequence, the design of this algorithm is strictly dependent on the properties of the linguistic annotation available in the corpus. Furthermore, we agree with those scholars (Preiss et al., 2007, inter alia) suggesting that the calibration of this module on the behavior of the specific parser has the effect of reducing the parser-specific bias in the input data. In the original LexIt framework, data were extracted from the linguistic annotation realized by the Part-Of-Speech tagger described in Dell’Orletta (2009), together with the dependency parser DeSR (Attardi and Dell’Orletta, 2009). For this first release of LexFr, we tailored our extraction algorithm on the annotation provided by Talismane, an opensource suite of NLP tools proving a transition based statistical dependency pars"
L16-1148,sagot-2010-lefff,0,0.0344787,"inguishing between 14 semantic classes. • Dicovalence (van den Eynde and Mertens, 2010) is a valency lexicon containing information for more than 3,700 verbs. It is based on the pronominal approach (van den Eynde and Blanche-Benveniste, 1978), a research method that treats pronouns as semantic primitives due to the purely linguistic nature and finite inventory of this lexical class. Accordingly, in this resource valence slots are characterized by the set of accepted pronouns, which subsume the possible lexicalizations of that slot. • The Lexique des Formes Fl´echies du Franc¸ais, a.k.a Lefff (Sagot, 2010), is a semi-automatically built morphological and syntactic lexicon. The several manual validation, correction and extension steps needed to build this resource led some authors to describe it as “an automatically acquired morphological lexicon [...] which has been manually supplemented with partial syntactic information” (Messiant et al., 2008). Version 3.0.1 of this resource describes more than 110k lexical elements, among which 6,825 verbs, 37,530 nouns and 10,483 adjectives. Its lexical framework, Alexina (Architecture pour les LEXiques INformatiques et leur Acquisition), has been successf"
L16-1148,J06-2001,0,0.115271,"Missing"
L16-1148,J90-1003,0,\N,Missing
L16-1245,C08-1085,0,0.0780084,"Missing"
L16-1245,E12-2021,0,0.0574362,"Missing"
L16-1245,P13-4001,0,0.0319782,"Missing"
L16-1245,W12-4903,1,0.837408,"uncts of different types, as well as enumerations. Other kinds of errors come, classically, from ambiguous attachments. Some of the errors concern the labelling. For example VPinf introduced by a Prep should be encoded as a PP plus a VPinf. 4.1. Text Selector The Text Selector is a tool helping in the selection of the texts, on the form of HTML files each containing 10 texts to evaluate. Each unit presents the book description and the text, segmented into sentences. It also proposes an evaluation form (containing check boxes and drop-down lists), 5 Our previous study on morpho-syntax effects (Blache and Rauzy, 2012) included 10000 tokens). 1548 and the list of unknown words, to be manually tagged. This interface (see figure 6) makes it possible to easily correct different types of errors, including sentence segmentation as well as metadata. Using autonomous HTML files makes easier the distribution of the revision work between several annotators. It does not require any particular environment (files being edited directly in a browser), neither a connection to a server. The revision tool relies on an adaptation of a TiddlyWiki6 enriched with scripts for adding extra information to the texts. 4.3. Tree edit"
L16-1507,bunt-etal-2012-iso,0,0.153517,"d Cue-Ending but also functions such as Literal modifier. Neiberg et al. (2013) also adopt a form-driven approach but it is an approach that combines automatic data selection with lexical and acoustic cues. As for the function annotation, they identify five scalar attributes related to feedback: non-understanding – understanding, disagreement – agreement, uninterested – interested, expectation – surprise, uncertainty – certainty. This scalar approach is appealing because many of these values seem to have indeed a scalar nature. Finally, the work around ISO-TC37 linguistic annotation standard (Bunt et al., 2012) provides a fine grained annotation schema for communicative functions. The framework identifies two dimensions for feedback: Auto-feedback concerns information processing by the feedback producer (I have understood), while Allo-feedback deals with information processing by interlocutor (You have understood.). The standard created distinguishes between positive, negative and elicit values for both Auto- and Allo-feedback. This overview on related work on conversational feedback shows the large variety of approaches towards communicative function annotation. It might seem unnecessary to introdu"
L16-1507,gorisch-etal-2014-aix,1,0.899483,"Missing"
L16-1507,J12-1001,0,0.0370763,"Missing"
L16-1507,W15-4620,1,0.809697,"Missing"
L16-1507,Y15-1034,1,0.810697,"Missing"
L16-1507,wittenburg-etal-2006-elan,0,0.0874965,"scribed by Gorisch and Pr´evot (2015). 3.2. Gesture pre-segmentation As our project aims to describe conversational feedback in general, the visible part of that feedback should receive sufficient attention, too. Three of the four corpora include participants’ visibility and video recordings2 . An entire labeling of all gestures of the corpus was however impossible. Therefore, we employed two students who performed a presegmentation task. Those sections of a video that involve feedback in the domain of gestures or facial expressions were segmented using the ELAN tool in its segmentation mode (Wittenburg et al., 2006). The focus on this pass was on recall rather than precision since all the marked units 2 In the CID-corpus, only 3 out of 8 sessions were video recorded and merely on a single camera, which made the identification of gestures difficult. Therefore, the results (cf. Figure 3) do not include gesture annotations for CID 3181 were annotated later on for precise gestures and potentially discarded if it turned out that they are not feedback. This means that we were able to provide very broad instruction to catch any facial expression or head movement possibly involved in providing feedback. 3.3. Ins"
O13-1025,C02-1049,0,\N,Missing
P06-2106,francopoulo-etal-2006-lexical,1,0.861995,"Missing"
P06-2106,W03-1905,1,0.81081,"ess ordinal, pronoun, for instance. The classifier phrase is syntactically generated according to a specific pattern. Here are some usages of classifiers and their syntactic patterns. gual conditions and perform operations on lexical entries. Originally, in order to meet expectations placed upon lexicons as critical resources for content processing in the Semantic Web, the MILE syntactic and semantic lexical objects have been formalized in RDF(S), thus providing a web-based means to implement the MILE architecture and allowing for encoding individual lexical entries as instances of the model (Ide et al., 2003; Bertagna et al., 2004b). In the framework of our project, by situating our work in the context of W3C standards and relying on standardized technologies underlying this community, the original RDF schema for ISLE lexical entries has been made compliant to OWL. The whole data model has been formalized in OWL by using Prot´eg´e 3.2 beta and has been extended to cover the morphological component as well (see Figure 2). Prot´eg´e 3.2 beta has been also used as a tool to instantiate the lexical entries of our sample monolingual lexicons, thus ensuring adherence to the model, encoding coherence an"
P06-2106,bel-etal-2000-simple,1,0.877753,"s: to increase the competitive edge of Asian countries, to bring Asian countries to closer to their western counterparts, and to bring more cohesion among Asian countries. To achieve this goal, we have launched a two year project to create a common standard for Asian language resources. The project is comprised of the following four research items. There is a long history of creating a standard for western language resources. The human language technology (HLT) society in Europe has been particularly zealous for the standardization, making a series of attempts such as EAGLES1 , PAROLE/SIMPLE (Lenci et al., 2000), ISLE/MILE (Calzolari et al., 2003) and LIRICS2 . These continuous efforts has been crystallized as activities in ISO-TC37/SC4 which aims to make an international standard for language resources. 2 (4) Evaluation through application classification Figure 1: Relations among research items 1 Introduction 1 (2) Sample lexicons (1) building a description framework of lexical entries (2) building sample lexicons (3) building an upper-layer ontology (4) evaluating the proposed framework through an application Figure 1 illustrates the relations among these research items. Our main aim is the researc"
P06-2106,C94-1091,1,0.485541,"ical operations, which are special lexical entities allowing the user to define multilin3 MILE is based on the experience derived from existing computational lexicons (e.g. LE-PAROLE, SIMPLE, EuroWordNet, etc.). 828 “CL” stands for a classifier. They always follow cardinal numbers in Japanese. Note that different classifiers are used for different nouns. In the above examples, classifier “hiki” is used to count noun “inu (dog)”, while “satsu” for “hon (book)”. The classifier is determined based on the semantic type of the noun. In the Thai language, classifiers are used in various situations (Sornlertlamvanich et al., 1994). The classifier plays an important role in construction with noun to express ordinal, pronoun, for instance. The classifier phrase is syntactically generated according to a specific pattern. Here are some usages of classifiers and their syntactic patterns. gual conditions and perform operations on lexical entries. Originally, in order to meet expectations placed upon lexicons as critical resources for content processing in the Semantic Web, the MILE syntactic and semantic lexical objects have been formalized in RDF(S), thus providing a web-based means to implement the MILE architecture and al"
P06-2106,zhang-etal-2004-distributional,1,0.766907,"the set of postpositions as values of FunctionType instead of conventional function types such as “subj” and “obj”. It might be an user defined data category or language dependent data category. Furthermore, it is preferable to prepare the mapping between Japanese postpositions and conventional function types. This is interesting because it seems more a terminological difference, but the model can be applied also to Japanese. 4 Building sample lexicons 4.1 Swadesh list and basic lexicon The issue involved in defining a basic lexicon for a given language is more complicated than one may think (Zhang et al., 2004). The naive approach of simply taking the most frequent words in a language is flawed in many ways. First, all frequency counts are corpus-based and hence inherit the bias of corpus sampling. For instance, since it is easier to sample written formal texts, words used predominantly in informal contexts are usually underrepresented. Second, frequency of content words is topic-dependent and may vary from corpus to corpus. Last, and most crucially, frequency of a word does not correlate to its conceptual necessity, 4.2 Aligning multilingual lexical entries Since our goal is to build a multilingual"
P06-2106,bertagna-etal-2004-content,1,0.887142,"e morphological, syntactic and semantic layers. Moreover, an intermediate module allows to define mechanisms of linkage and mapping between the syntactic and semantic layers. Within each layer, a basic linguistic information unit is identified; basic units are separated but still interlinked each other across the different layers. Within each of the MLM layers, different types of lexical object are distinguished : fits with as many Asian languages as possible, and contributing to the ISO-TC37/SC4 activities. As a starting point, we employ an existing description framework, the MILE framework (Bertagna et al., 2004a), to describe several lexical entries of several Asian languages. Through building sample lexicons (research item (2)), we will find problems of the existing framework, and extend it so as to fit with Asian languages. In this extension, we need to be careful in keeping consistency with the existing framework. We start with Chinese, Japanese and Thai as target Asian languages and plan to expand the coverage of languages. The research items (2) and (3) also comprise the similar feedback loop. Through building sample lexicons, we refine an upper-layer ontology. An application built in the resea"
P06-2106,Y06-1043,1,\N,Missing
P07-2018,P04-1059,0,0.0824876,"Missing"
P07-2018,O99-2001,0,0.0950061,"Missing"
P07-2018,W03-1719,0,0.081678,"Missing"
P07-2018,O03-4002,0,\N,Missing
P07-2018,C92-1019,0,\N,Missing
P07-2018,W03-1726,0,\N,Missing
peshkov-prevot-2014-segmentation,N10-1143,0,\N,Missing
peshkov-prevot-2014-segmentation,W01-0708,0,\N,Missing
peshkov-prevot-2014-segmentation,C12-2079,0,\N,Missing
peshkov-prevot-2014-segmentation,fort-etal-2012-analyzing,0,\N,Missing
peshkov-prevot-2014-segmentation,N12-1016,0,\N,Missing
peshkov-prevot-2014-segmentation,P13-1167,0,\N,Missing
W08-1912,W99-0501,0,0.017548,"Missing"
W09-3303,baroni-etal-2008-cleaneval,0,0.0117928,"stencies from the French EuroWordnet. Later, Sagot and Fišer (2008) explained how they needed to recourse to PWN, BalkaNet (Tufis, 2000) and other resources (notably Wikipedia) to build WOLF, a free French WordNet that is promising but still a very preliminary resource. Some languages are straight-off purely under-resourced. The Web as Corpus initiative arose (Kilgarriff and Grefenstette, 2003) as an attempt to design tools and methodologies to use the web for overcoming data sparseness (Keller and Lapata, 2002). Nevertheless, this initiative raised non-trivial technical problems described in Baroni et al. (2008). Moreover, the web is not structured enough to easily and massively extract semantic relations. In this context, Wiktionary could appear to be a paradisiac playground for creating various lexiNote: The experiments of this paper are based on Wiktionary’s dumps downloaded in year 2008. Differences may be observed with the current versions available online. 1 Kuo Tzu-Yi Graduate Institute of Linguistics NTU, Taiwan tzuyikuo@ntu.edu.tw Introduction Reliable and comprehensive lexical resources constitute a crucial prerequisite for various NLP tasks. However their building cost keeps them rare. In"
W09-3303,zesch-etal-2008-extracting,0,0.0625772,"more languages only makes this observation more acute. In spite of various initiatives, costs make resource development extremely slow or/and result in non freely accessible resources. Collaborative resources might bring an attractive solution 19 Proceedings of the 2009 Workshop on the People’s Web Meets NLP, ACL-IJCNLP 2009, pages 19–27, c Suntec, Singapore, 7 August 2009. 2009 ACL and AFNLP cal resources. We describe below the Wiktionary resource and we explain the restrictions and problems we are facing when trying to exploit it. This description may complete few earlier ones, for example Zesch et al. (2008a). 2.1 2.2.2 Layouts In the following paragraph, we outline wiktionary’s general structure. We only consider words in the wiktionary’s own language. An entry consists of a graphical form and a corresponding article that is divided into the following, possibly embedded, sections: • etymology sections separate homonyms when relevant; • among an etymology section, different parts of speech may occur; • definitions and examples belong to a part of speech section and may be subdivided into subsenses; • translations, synonyms/antonyms and hypernyms/hyponyms are linked to a given part of speech, wit"
W09-3303,W08-1912,1,0.934088,"er, V is 4 http://it.wiktionary.org/w/index.php? title=cardinale&oldid=758205 5 http://en.wiktionary.org/wiki/WT:ELE 6 http://meta.wikimedia.org/wiki/List_ of_Wiktionaries 21 a set of words and E is defined by a relation R R E 7−→ E : (w1 , w2 ) ∈ E if and only if w1 → w2 . Most of lexical networks, as networks extracted from real world, are small worlds (SW) networks. Comparing structural characteristics of wiktionary-based lexical networks to some standard resource should be done according to wellknown properties of SW networks (Watts and Strogatz, 1998; Barabasi et al., 2000; Newman, 2003; Gaume et al., 2008). These properties are: • Edge sparsity: SW are sparse in edges m = O(n) or m = O(n log(n)) • Short paths: in SW, the average path length (L)7 is short. Generally there is at least one short path between any two nodes. • High clustering: in SW, the clustering coefficient (C) that expresses the probability that two distinct nodes adjacent to a given third one are adjacent, is an order of magnitude higher than for Erdos-Renyi (random) graphs: CSW  Crandom ; this indicates that the graph is locally dense, although it is globally sparse. • Heavy-tailed degree distribution: the distribution of the"
W09-3303,W02-1030,0,0.0283747,"ample, we consider French language resources. Jacquin et al. (2002) highlighted the limitations and inconsistencies from the French EuroWordnet. Later, Sagot and Fišer (2008) explained how they needed to recourse to PWN, BalkaNet (Tufis, 2000) and other resources (notably Wikipedia) to build WOLF, a free French WordNet that is promising but still a very preliminary resource. Some languages are straight-off purely under-resourced. The Web as Corpus initiative arose (Kilgarriff and Grefenstette, 2003) as an attempt to design tools and methodologies to use the web for overcoming data sparseness (Keller and Lapata, 2002). Nevertheless, this initiative raised non-trivial technical problems described in Baroni et al. (2008). Moreover, the web is not structured enough to easily and massively extract semantic relations. In this context, Wiktionary could appear to be a paradisiac playground for creating various lexiNote: The experiments of this paper are based on Wiktionary’s dumps downloaded in year 2008. Differences may be observed with the current versions available online. 1 Kuo Tzu-Yi Graduate Institute of Linguistics NTU, Taiwan tzuyikuo@ntu.edu.tw Introduction Reliable and comprehensive lexical resources co"
W09-3303,J03-3001,0,0.0277846,"of lexical resources, be it due to the low-quality or non-existence of such resources, or to copyrightsrelated problems. As an example, we consider French language resources. Jacquin et al. (2002) highlighted the limitations and inconsistencies from the French EuroWordnet. Later, Sagot and Fišer (2008) explained how they needed to recourse to PWN, BalkaNet (Tufis, 2000) and other resources (notably Wikipedia) to build WOLF, a free French WordNet that is promising but still a very preliminary resource. Some languages are straight-off purely under-resourced. The Web as Corpus initiative arose (Kilgarriff and Grefenstette, 2003) as an attempt to design tools and methodologies to use the web for overcoming data sparseness (Keller and Lapata, 2002). Nevertheless, this initiative raised non-trivial technical problems described in Baroni et al. (2008). Moreover, the web is not structured enough to easily and massively extract semantic relations. In this context, Wiktionary could appear to be a paradisiac playground for creating various lexiNote: The experiments of this paper are based on Wiktionary’s dumps downloaded in year 2008. Differences may be observed with the current versions available online. 1 Kuo Tzu-Yi Gradua"
W13-4011,W10-4335,0,0.070502,"Missing"
W13-4011,W11-2028,0,0.0391553,"Missing"
W13-4011,bigi-2012-sppas-tool,1,0.812109,"n from which the phonetic tokens are obtained to be used by the grapheme-phoneme converter. From the phoneme sequence and the audio signal, the aligner outputs for each phoneme its time localization. This corpus has been processed with several aligners. The first and main one (Brun et al., 2004) is HMM-based, it uses a set of 10 macro-classes of vowel (7 oral and 3 nasal), 2 semi-vowels and 15 consonants. Finally, from the time aligned phoneme sequence plus the EOT, the orthographic tokens is time-aligned. The alignment for this paper is another version that has been carried out using SPPAS3 (Bigi, 2012). SPPAS is a tool to produce automatic annotations which include utterance, word, syllabic and phonemic segmentations from a recorded speech sound and its transcription. Alignment of items of the list given in (1) were then manually verified. Largest errors were corrected to obtain reliable alignments. DM prononciations are the standard ones except 3 Descriptive statistics for the lexical markers used in feedback (1) ah (ah), bon (well), ben (well), euh (err, uh), mh (mh), ouais (yeah), oui (yes), non (no), d’accord (agreed), OK (okay), voil`a (that’s it, right) Strictly speaking, the list (1)"
W13-4011,W10-1829,1,0.744047,"Missing"
W13-4011,J00-3003,0,0.153936,"ID). The paper includes the raw figures for feedback lexical item as well as more detailed figures concerning interindividual variability. This effort is a first step before a broader analysis including more discourse situations and featuring communicative function annotation. Index Terms: Feedback, Backchannel, Corpus, French Language 1 Objectives Conversational feedback is mostly performed through short utterances such as yeah, mh, okay not produced by the main speaker but by one of the other participants of a conversation. Such utterances are among the most frequent in conversational data (Stolcke et al., 2000). They also have been described in psycho-linguistic models of communication as a crucial communicative tool for achieving coordination or alignment in dialogue (Clark, 1996). The general objective of the project (ANR CoFee: Conversational Feedback)1 (Pr´evot and Bertrand, 2012) in which this work takes place is to propose a fine grained model of the form/function relationship concerning feedback behaviors in conversation. The present study is first exploration aiming at knowing better the distribution of these items in one of our corpus. More precisely, we would to verify how much interindivi"
W13-4011,J12-1001,0,0.0441453,"Missing"
W15-4620,J00-3003,0,0.0537815,"and datasets of a project scrutinizing this kind of feedback utterances in French. We present the genesis of the corpora (for a total of about 16 hours of transcribed and phone force-aligned speech) involved in the project. We introduce the resulting datasets and discuss how they are being used in on-going work with focus on the form-function relationship of conversational feedback. All the corpora created and the datasets produced in the framework of this project will be made available for research purposes. 1 Introduction Feedback utterances are the most frequent utterance type in dialogue (Stolcke et al., 2000; Misu et al., 2011). They also play a crucial role in managing the common ground of a conversation (Clark, 1996). However, perhaps due to their apparent simplicity, they have been ignored in many linguistic studies on dialogue. The main contribution to the understanding of the feedback utterance type comes from neighboring fields: (i) Conversational Analysis (CA) has shed light on turntaking including a careful description of response tokens, such as “uh-huh” (Schegloff, 1982), formerly also termed back-channels by (ii) computational linguist Victor Yngve (Yngve, 1970)1 ; (iii) Dialogue engin"
W15-4620,bigi-2012-sppas-tool,1,0.760818,"putational Linguistics through the Ortolang platform (http://sldr. org/ortolang-000911). scriptions and large feature files, how they were produced and how they can also be useful for other researchers and their studies. 2 3.1 Feedback items All recordings include headset microphone channels that were transcribed on IPU level and automatically aligned on word and phone level. The recording setups are illustrated in Figure 1. The first two corpora (CID and MTR) already existed before our current project, while the other two (MTX and DVD) were specifically recorded and transcribed (using SPPAS (Bigi, 2012)) for this project and are therefore explained in more detail below. CID, MTX and DVD primary are directly accessible for research purposes; MTR requires agreement from its creators. Concerning the definition of the term feedback utterance, we follow Bunt (1994, p.27): “Feedback is the phenomenon that a dialogue participant provides information about his processing of the partner’s previous utterances. This includes information about perceptual processing (hearing, reading), about interpretation (direct or indirect), about evaluation (agreement, disbelief, surprise,...) and about dispatch (ful"
W15-4620,wittenburg-etal-2006-elan,0,0.0698301,"ok ca. 5 minutes to complete. 3.3 As our project aims to describe conversational feedback in general, the visible part of that feedback should receive sufficient attention, too. Three of the four corpora include participants’ visibility and video recordings. An entire labelling of all gestures of the corpus is however impossible. Therefore, we employed two students (working on gesture for their research) to perform a presegmentation task. Those sections of a video that involve feedback in the domain of gestures or facial expressions were segmented using the ELAN tool in its segmentation mode (Wittenburg et al., 2006). The focus on this pass was on recall rather than precision since all the marked units will be annotated later on for precise gestures and potentially discarded if it turns out that they are not feedback. DVD We recruited 16 participants to take part in the recording of this corpus. The aim was to involve them in a discussion on movies, DVDs, actors, and all other topics that they may come up with during a 30 minute conversation. A set of DVD boxes (with content) were placed on a table in front of them: 4 on each side (see Figure 1d). The instructions included that each participant can take 2"
W15-4620,gorisch-etal-2014-aix,1,0.629675,"Missing"
W15-4620,J12-1001,0,0.227551,"Missing"
W15-4620,W11-2028,0,0.0201212,"ect scrutinizing this kind of feedback utterances in French. We present the genesis of the corpora (for a total of about 16 hours of transcribed and phone force-aligned speech) involved in the project. We introduce the resulting datasets and discuss how they are being used in on-going work with focus on the form-function relationship of conversational feedback. All the corpora created and the datasets produced in the framework of this project will be made available for research purposes. 1 Introduction Feedback utterances are the most frequent utterance type in dialogue (Stolcke et al., 2000; Misu et al., 2011). They also play a crucial role in managing the common ground of a conversation (Clark, 1996). However, perhaps due to their apparent simplicity, they have been ignored in many linguistic studies on dialogue. The main contribution to the understanding of the feedback utterance type comes from neighboring fields: (i) Conversational Analysis (CA) has shed light on turntaking including a careful description of response tokens, such as “uh-huh” (Schegloff, 1982), formerly also termed back-channels by (ii) computational linguist Victor Yngve (Yngve, 1970)1 ; (iii) Dialogue engineers dealt with them"
W18-4703,W11-0101,1,0.893735,"Missing"
W18-4703,W15-0201,1,0.826275,"t; ‘representaIc Ia +  ? tion’ refers to the rendering of annotations in a Semantics particular format. Following the ISO Principles, this distinction is implemented in the DiAML definition by disFigure 1: Abstract and concrete syntax, and semantinguishing an abstract syntax that specifies a tics class of annotation structures as set-theoretical 3 See Bunt (2007); Allen & Core (1997); Dhillon et al. (2004); Anderson et al. (1991); Alexandersson et al. (1998); Jurafsky et al. (1997); and Bunt (1994; 2000), respectively. 4 ISO 24612:2010; see also Ide & Romary (2004). 5 ISO 24617-6; see also Bunt (2015). 22 constructs, like pairs and triples of concepts, and a concrete syntax that specifies a rendering of these annotation structures in a particular format. A representation format is defined called DiAML-XML, which uses abbreviated XML-expressions. The annotations have a semantics which is defined for the abstract syntax (see Fig. 1), thus allowing alternative representation formats to share the same semantics. According to ISO 24617-2, dialogue acts are expressed by ‘functional segments’, defined as minimal stretches of communicative behaviour that have a communicative function and a semanti"
W18-4703,L16-1503,1,0.873526,"Missing"
W18-4703,bunt-etal-2010-towards,1,0.536666,"Missing"
W18-4703,bunt-etal-2012-iso,1,0.908343,"Missing"
W18-4703,W17-7405,1,0.860044,"Missing"
W18-4703,W17-7406,1,0.769023,"t dialogAct=“#da3” content=“#e3”/&gt; <DRLink rel=“cause” reason=“#e3” result=“#da2” /&gt; 30 The <event&gt; element introduced in (13) for specifying information about the semantic content of a dialogue act could be the same as, or a simplified version of, the element with the same name that is used in the ISO standards for time and events (ISO 24617-2, see also Pustejovsky et al., 2010), for annotating semantic roles (ISO 24617-4, see also Bunt & Palmer, 2013), and for spatial information (ISO 246177, see also Pustejovsky et al., 2013), and that has also been proposed for the annotation of modality (Lapina & Petukhova, 2017) and quantification (Bunt et al., 2017). This suggests that the introduction of <semanticContent&gt; and <event&gt; elements, with their underlying abstract syntax and semantics, may open the possibility to specify quite detailed information about the semantic content of dialogue acts. 7 Conclusions and Perspectives In this paper we have considered the requirements for a revision of the ISO standard for dialogue act annotation. One of the requirements is that, where possible, a second edition should be downward compatible with the original (current) version of the standard. The notion of compatibili"
W18-4703,L16-1500,1,0.622665,"Missing"
W18-4703,petukhova-etal-2014-interoperability,1,0.911823,"ended to add more fine-grained concepts to the standard, and to provide use-case dependent guidelines for how to optimally make use of the concepts that the standard makes available. 6 ISO 24617-2 Extensions 6.1 Dimensions Users of ISO 24617-2 have mentioned two dimensions that they missed, namely Task Management, known from DAMSL, and Contact Management, known from DIT++ . Task Management acts discuss or explain a certain task or activity that is pursued through the dialogue (as opposed to performing that task/activity). They occur for example in TV debates and in interactive games (see e.g. Petukhova et al., 2014). Contact Management acts serve to establish and manage contact and attention. Casual conversations are known to contain a rich variety of greetings and leavetaking acts (Gilmartin et al., 2017), which often have such a function (see also the next subsection). Since one of the attractive features of the ISO scheme is that its dimensions are ‘orthogonal’, Task Management and Contact Management can be added as optional additions without interfering with the existing 9-dimensional system, keeping the extended system downward compatible with the existing system, and are available in a given use ca"
W18-4703,petukhova-etal-2014-dbox,1,0.874592,"ended to add more fine-grained concepts to the standard, and to provide use-case dependent guidelines for how to optimally make use of the concepts that the standard makes available. 6 ISO 24617-2 Extensions 6.1 Dimensions Users of ISO 24617-2 have mentioned two dimensions that they missed, namely Task Management, known from DAMSL, and Contact Management, known from DIT++ . Task Management acts discuss or explain a certain task or activity that is pursued through the dialogue (as opposed to performing that task/activity). They occur for example in TV debates and in interactive games (see e.g. Petukhova et al., 2014). Contact Management acts serve to establish and manage contact and attention. Casual conversations are known to contain a rich variety of greetings and leavetaking acts (Gilmartin et al., 2017), which often have such a function (see also the next subsection). Since one of the attractive features of the ISO scheme is that its dimensions are ‘orthogonal’, Task Management and Contact Management can be added as optional additions without interfering with the existing 9-dimensional system, keeping the extended system downward compatible with the existing system, and are available in a given use ca"
W18-4703,pustejovsky-etal-2010-iso,1,0.780656,"task” communicativeFunction=“answer” functionalDependence=“#da1”&gt; <dialogueAct xml:id=“da3” target=“#s3” sender=“#b” addressee=“#a” dimension=“task” communicativeFunction=“inform” &gt; <event xml:id=“e3” target=“#s3” type=“ill” /&gt; <semanticContent dialogAct=“#da3” content=“#e3”/&gt; <DRLink rel=“cause” reason=“#e3” result=“#da2” /&gt; 30 The <event&gt; element introduced in (13) for specifying information about the semantic content of a dialogue act could be the same as, or a simplified version of, the element with the same name that is used in the ISO standards for time and events (ISO 24617-2, see also Pustejovsky et al., 2010), for annotating semantic roles (ISO 24617-4, see also Bunt & Palmer, 2013), and for spatial information (ISO 246177, see also Pustejovsky et al., 2013), and that has also been proposed for the annotation of modality (Lapina & Petukhova, 2017) and quantification (Bunt et al., 2017). This suggests that the introduction of <semanticContent&gt; and <event&gt; elements, with their underlying abstract syntax and semantics, may open the possibility to specify quite detailed information about the semantic content of dialogue acts. 7 Conclusions and Perspectives In this paper we have considered the requirem"
Y06-1043,W93-0231,0,0.223651,"committing choice. In this example we placed all the terms in question under the AstronomicalBody SUMO concept and under SkyObject for our own taxonomy proposal. Moreover, it is clear that it exists different lexical organization for a given domain. See for example, the division studies of bodyparts in [8] or the one of geographical object in [9]. Closer to our experiment, EuroWordNet team noticed that in Dutch there is no concept for a generic container while in other languages this term was available for being included in the core lexical structure. Finally, see [12] for a discussion of the [5] example of wood, tree and forest in French, German and English. As a conclusion on this topic, the nature of the list and structure we are aiming too is essentially linguistic and do not pretend to say anything about deeper cognitive structure. This issue highlights again our need to separate the lexical and the ontological level. The fact that a language selected a term does not mean that concepts that did not received similar label are absent. Moreover the permeability between word senses [7] and the robustness of the semantic system allows for an adaptation of the language. In the ’sun’ ex"
Y06-1043,huang-etal-2004-sinica,1,0.874655,"e in the context of face-to-face interaction. 3 The experiments 3.1 The experiments on Chinese The Chinese Swadesh list was obtained by consulting with the Academia Sinica Chinese Wordnet group. One or more Chinese Wordnet entry for each item of the list were obtained, and non basic readings were eliminated. Subsequently, we obtain automatically the concept distribution of the items in SUMO taxonomy through SINICABOW,3 a resource developed at the Academia Sinica which combines the Chinese wordnet, the Princeton WordNet and SUMO. 2 3 See http://www.rosettaproject.org/ for more information. See [6] and http://bow.sinica.edu.tw/ for more information. 326 3.2 The experiments on English About the English list we studied three different ways for building a taxonomy out of the simple list: A. Really keep the structure as minimal as possible by not adding any further (generalizing) concept in the list. B. Keep the structure as minimal a possible but also try to get a reasonable organization from a knowledge representation viewpoint. C. Simply align the terms to SUMO ontology. The options (B) and (C) were performed in two steps: (i) disambiguate the words of the list by mapping them to WordNet"
Y06-1043,zhang-etal-2004-distributional,1,0.548079,"roach, the terms appearing frequently in definitions gloss are good candidate for being integrated in the core lexicon. The main problem of this approach is that the upper level of existing ontologies are generally fairly abstract concepts that are rarely lexicalized 1stClassEntity in EuroWordNet or SelfConnectedObject in SUMO[10], and that are intuitively far from being member of a core lexicon. 2.2 Frequency criterion The second approach uses more statistical data such as word frequency. However, simple word frequency is not good criterion for selecting the basic terms. A recent elaboration [18] proposed to use the notion of distributional consistency. This measure provides better result than other statistically based approaches but it requires balanced corpus of significant size. Such corpora are only available for few languages and we would like to have a method that could be used with languages deprived from extensive resources. 2.3 Swadesh list or the universality criterion The lack of resources for most of languages led us to consider the Swadesh list [15] (reproduced as an appendix) as a potential core lexicon. The Swadesh list has been developed by Moriss Swadesh in the fiftie"
Y06-1043,P06-2106,1,\N,Missing
Y09-1036,W08-1912,1,0.854182,"Missing"
Y09-1036,C04-1173,1,0.780773,"(See below). Using this method, we are able to project video clips in the same space (defined by video clips as dimensions), while we still take in account the influence of one language or another. Our projected data points are video clips typed according to a language (fr or ch). We therefore have 34 data points into the 17-dimensions video clips space. Then we compare the distance between French and Chinese interpretations of each video clip and perform a cluster analysis. 6.1 Formal definition and computation To conduct the computation of the coordinates, we followed the method stated in (Gaume et al., 2004) with a number of steps i = 2. If G = (V, E) is our graph with |V |= n vertices, we consider the adjacency matrix of G that we note [G]i,j . [G]i,j = 1 iff there is an edge between the vertices i and j, 0 otherwise. The next step is to create the Markovian matrix of G, such that [G]r,s x∈V ([G]r,x ) ˆ r,s = P [G] ˆ 2 will be a matrix that contains all the coordinates we need.See Following this definition, [G] (Gaume et al., 2004) for more details about this method. 6.2 Random walks in experimental data To obtain the coordinates of the video clips in the video clips space, we first built two we"
Y10-1096,D10-1115,0,0.0450835,"Missing"
Y10-1096,Y10-1091,1,0.718857,"form precise comparison of such different data. We will see however in section 6 that a more careful look at the data lead us to revise this observation. 4 Cross-linguistics issues The schematic view of the project Figure 1, propose an validation from a study at different perspective, and this across languages as well. Several issues have been raised however about the cross-linguistic validation. Movies cultural bias First, in Figure 1 there is an hidden ingredient: the movies. The characterization in the Approx protocol relies on the movies themselves. As explained in (Magistry et al., 2009; Cheung et al., 2010) these movies may introduced a cultural bias since they are all very PACLIC 24 Proceedings familiar for French subjects but they may appear strange to Taiwanese subjects. This could lead to peculiar answers triggered by surprise or amusement. This was the main reason of the extralinguistic study carried on the movies and that characterize each movie according to its cultural bias. The study has been carried on through an extra-linguistic questionnaire that has been answered by both French and Taiwanese subjects. This study has the effect to reject a few movies has they appeared culturally bias"
Y10-1096,Y10-1093,1,0.748428,"rement of Meaning). Copyright 2010 by Laurent Prévot, Chun-Han Chang, and Yann Desalle More information about M3 project on these websites: http://erss.irit.fr/flexsem (France) http://140.112.147.149:81/m3 /(Taiwan). Paradigmatic graphs a here a sort of synonymy graph in which the synonymy relation is kept relatively loose. 841 842 Workshop on Model and Measurement of Meaning Figure 1: M3 Square A validation across the methodologies is proposed as the following. Given a set of answers of a population, we can look at their properties in the paradigmatic graph. Then as explained in this volume (Desalle et al., 2010), an hypothesis based on a measure, so-called flexsemy measure, ties the properties of the lexemes nodes in the graph with their psycholinguistic characteristics (which populations produced them) (see section 2). This methodological cross-validation is represented by the vertical arrows in the Figure 1. The paper is structured as follows. We first briefly introduced the whole model build on French and sum-up some previous validation studies and experiments on French language. Then in section 3 we recap the work done to setup and experiment the model on Mandarin language. In section 4 we presen"
Y10-1096,Y10-1094,0,0.12113,"ons by the subjects at various ages. This section briefly introduces and points the relevant works in which these issues have been addressed during the project. 3 4 http://www.crisco.unicaen.fr/Presentation-du-dictionnaire.html In fact, one can start from any state of the graph. 843 844 Workshop on Model and Measurement of Meaning 3.1 Lexical Resources The French lexical graph has been created from a compilation of synonym dictionaries. The same resource was not available for Mandarin language. Moreover the notion of lexical unit is not completely settled among the linguists. In this volume, (Gaillard et al., 2010) describes how a comparable resource to our French paradigmatic graph was built for Mandarin language on the ground on existing resources such as the CiLin (Mei et al., 1984) and the Chinese WordNet (Huang et al., 2004). 3.2 Experiments For the M3 project, we focused on two populations in France and in Taiwan : children without cognitive disorders and adults. We built two databases (one for each language), listing all the verbs produced by the participants for each video clip. Taiwanese analysts faced a crucial issue with Mandarin verbal forms that are difficult to disentangle from more comple"
Y10-1096,huang-etal-2004-sinica,0,0.0591472,".html In fact, one can start from any state of the graph. 843 844 Workshop on Model and Measurement of Meaning 3.1 Lexical Resources The French lexical graph has been created from a compilation of synonym dictionaries. The same resource was not available for Mandarin language. Moreover the notion of lexical unit is not completely settled among the linguists. In this volume, (Gaillard et al., 2010) describes how a comparable resource to our French paradigmatic graph was built for Mandarin language on the ground on existing resources such as the CiLin (Mei et al., 1984) and the Chinese WordNet (Huang et al., 2004). 3.2 Experiments For the M3 project, we focused on two populations in France and in Taiwan : children without cognitive disorders and adults. We built two databases (one for each language), listing all the verbs produced by the participants for each video clip. Taiwanese analysts faced a crucial issue with Mandarin verbal forms that are difficult to disentangle from more complex constructions. Resultative constructions, well known in Mandarin (Thompson, 1973; Lu, 1977; Gu, 1992; Cheng and Huang, 1994; Gao, 1997) are very frequent in our experimental data. In fact most of the action descriptio"
Y10-1096,Y09-1036,1,0.393056,"we would be able to perform precise comparison of such different data. We will see however in section 6 that a more careful look at the data lead us to revise this observation. 4 Cross-linguistics issues The schematic view of the project Figure 1, propose an validation from a study at different perspective, and this across languages as well. Several issues have been raised however about the cross-linguistic validation. Movies cultural bias First, in Figure 1 there is an hidden ingredient: the movies. The characterization in the Approx protocol relies on the movies themselves. As explained in (Magistry et al., 2009; Cheung et al., 2010) these movies may introduced a cultural bias since they are all very PACLIC 24 Proceedings familiar for French subjects but they may appear strange to Taiwanese subjects. This could lead to peculiar answers triggered by surprise or amusement. This was the main reason of the extralinguistic study carried on the movies and that characterize each movie according to its cultural bias. The study has been carried on through an extra-linguistic questionnaire that has been answered by both French and Taiwanese subjects. This study has the effect to reject a few movies has they ap"
Y13-1007,N12-1016,0,0.0149263,"mation of the agreement by chance. The issue here is that it is a segmentation task, therefore we have to decide on what are the decision points. We are using the tokens as decision points rather than a fixed sample (as it is done in some annotation tools) because the French guidelines are using words as the base units for instructing where to put the boundaries. Agreement on no-boudary (0-0) is therefore an agreement for this decision task and there is no satisfying way to evaluate a kappa score if these agreements are left out. Other measures need to be introduced (Pevzner and Hearst, 2002; Fournier and Inkpen, 2012) if one wants to measure a different aspect of the segmentation agreement. However to be perfectly transparent with the annotation results, Figure 1 presents the contingency table for the Orchid’s style prosodic units (See also (Peshkov et al., 2012) for deeper evaluation of the annotation of the whole CID corpus). A/B (0-1) (2-3) (0-1) 12242 581 paralinguistic sounds (disjunction or disruption of utterances such as pauses, inhalation, and laughter). The annotation of prosodic units of the ORCHID.tw Dataset has been accomplished in an earlier project (Liu and Tseng, 2009). Three labelers were"
Y13-1007,W12-3611,0,0.0213317,"anguages: French and Taiwan Mandarin. We believe this combination of linguistic resources and skills for these two languages is a rather unique situation and allows for comparative quantitative experiments on high-level linguistic analysis such as discourse and prosody. Our objective is to understand the commonalities and the differences between discourse prosody interface in these two languages. More precisely, we look at how prosodic units and discourse units are distributed onto each other. In spirit, our work is closely related to the one of (Simon and Degand, 2009; Lacheret et al., 2010; Gerdes et al., 2012), however our focus here are the insights we can get from a comparative study. Moreover our dataset has a more conversational nature than the datasets studied in their work. About the data, (Gerdes et al., 2012) wanted to have an interesting spectrum of discourse genres and speak92 Copyright 2013 by Laurent Prévot, Shu-Chuan Tseng, Alvin Cheng-Hsien Chen, and Klim Peshkov 27th Pacific Asia Conference on Language, Information, and Computation pages 92－101 PACLIC-27 ing styles while we focused on conversations both for making possible the comparative studies and to make sure to have enough coher"
Y13-1007,W10-1842,0,0.0208751,"typologically diverse languages: French and Taiwan Mandarin. We believe this combination of linguistic resources and skills for these two languages is a rather unique situation and allows for comparative quantitative experiments on high-level linguistic analysis such as discourse and prosody. Our objective is to understand the commonalities and the differences between discourse prosody interface in these two languages. More precisely, we look at how prosodic units and discourse units are distributed onto each other. In spirit, our work is closely related to the one of (Simon and Degand, 2009; Lacheret et al., 2010; Gerdes et al., 2012), however our focus here are the insights we can get from a comparative study. Moreover our dataset has a more conversational nature than the datasets studied in their work. About the data, (Gerdes et al., 2012) wanted to have an interesting spectrum of discourse genres and speak92 Copyright 2013 by Laurent Prévot, Shu-Chuan Tseng, Alvin Cheng-Hsien Chen, and Klim Peshkov 27th Pacific Asia Conference on Language, Information, and Computation pages 92－101 PACLIC-27 ing styles while we focused on conversations both for making possible the comparative studies and to make sur"
Y13-1007,afantenos-etal-2012-empirical,1,0.888643,"Missing"
Y13-1007,J08-4004,0,0.0310584,"tation style of the Taiwan Mandarin data but also to improve the reliability of the data produced. Indeed, the interannotator agreement was overall higher when levels 2 and 3 are collapsed. Finally, we added breaks on pauses over 400ms. We computed a κ-score for our data set by taking each token as a decision point and counting the number of matching and non-matching boundaries across annotators. This method of calculation yielded a κ-score of 0.71 for our dataset which is a nice score for naive coders on prosodic phrasing task. Cohen’s kappa (Cohen and others, 1960) (and see (Carletta, 1996; Artstein and Poesio, 2008) for further discussion) is a measure designed to measure criteria for labeling discourse units according to the commonly defined operational guidelines for French and Taiwan Mandarin data processing. 2.2 Creation of the Taiwan Mandarin dataset The ORCHID.tw Dataset is a subset of the Taiwan Mandarin Conversational Corpus (the TMC Corpus), consisting of 3.5 hours of conversational speech produced by 7 male and 9 female speakers (Tseng, 2013). The TMC Corpus is a collection of 42 hours of free, task-oriented and topic-specific conversations in Taiwan Mandarin. All the speaker turn boundaries as"
Y13-1007,W12-4903,0,0.022571,"res built from the tags but do not deal with long dependencies or rich constituence. They are basically units centered on a syntactic head, a content word. As reminded by Abney, chunks can be related to φ-sentences (Gee and Grosjean, 1983) which have a more intonational nature. An idea defended in these early works is that chunks are indeed language processing units from a cognitive viewpoint. The break-up of experimental linguistics as renewed the interest for this hypothesis and is attempting to make it more precise (Blache, 2013) and relate to other empirical evidences such as eyetracking (Blache and Rauzy, 2012). With this idea in mind, we will investigate our prosodic and discourse units in terms of chunk size and constituency. The first basic hypothesis we are testing is if tokens are syntactic units and chunks more processing units, the structure of PUs and DUs in terms of tokens does not have to match across languages while it should in terms of chunks. More precisely, we expect a significant variation of PU/DU size across languages in terms number of tokens but not in terms of chunk size. 7.1 2. Propose a set of rules aggregating tags and chunks into coherent chunks (e.g Prep NC ; PC ; VC Part ;"
Y13-1007,J96-2004,0,0.203718,"o match the annotation style of the Taiwan Mandarin data but also to improve the reliability of the data produced. Indeed, the interannotator agreement was overall higher when levels 2 and 3 are collapsed. Finally, we added breaks on pauses over 400ms. We computed a κ-score for our data set by taking each token as a decision point and counting the number of matching and non-matching boundaries across annotators. This method of calculation yielded a κ-score of 0.71 for our dataset which is a nice score for naive coders on prosodic phrasing task. Cohen’s kappa (Cohen and others, 1960) (and see (Carletta, 1996; Artstein and Poesio, 2008) for further discussion) is a measure designed to measure criteria for labeling discourse units according to the commonly defined operational guidelines for French and Taiwan Mandarin data processing. 2.2 Creation of the Taiwan Mandarin dataset The ORCHID.tw Dataset is a subset of the Taiwan Mandarin Conversational Corpus (the TMC Corpus), consisting of 3.5 hours of conversational speech produced by 7 male and 9 female speakers (Tseng, 2013). The TMC Corpus is a collection of 42 hours of free, task-oriented and topic-specific conversations in Taiwan Mandarin. All th"
Y13-1007,Y96-1018,0,0.126436,"andarin Conversational Corpus (the TMC Corpus), consisting of 3.5 hours of conversational speech produced by 7 male and 9 female speakers (Tseng, 2013). The TMC Corpus is a collection of 42 hours of free, task-oriented and topic-specific conversations in Taiwan Mandarin. All the speaker turn boundaries as well as syllable boundaries were human-labeled in the ORCHID.tw Dataset. Boundaries of words and POS tags were automatically generated based on the syllable boundary information and the output of the automatic word segmentation and POS tagging system developed by the CKIP at Academia Sinica (Chen et al., 1996). Previously, the ORCHID.tw dataset has been annotated with boundaries of prosodic units as defined in (Liu and Tseng, 2009) and with boundaries of discourse units in (Chen, 2011). In the present ORCHID project, we modified the criteria for labeling discourse units according to the commonly defined operational guidelines for French and Taiwan Mandarin data processing. The definition for prosodic units remains unchanged. 3 3.1 Producing prosodic units French data The definition of prosodic units is adopted mainly from prosodic phonology (Selkirk, 1986; Nespor and Vogel, 1986) that proposed a un"
Y13-1007,J02-1002,0,0.0213422,"e raw agreement by an estimation of the agreement by chance. The issue here is that it is a segmentation task, therefore we have to decide on what are the decision points. We are using the tokens as decision points rather than a fixed sample (as it is done in some annotation tools) because the French guidelines are using words as the base units for instructing where to put the boundaries. Agreement on no-boudary (0-0) is therefore an agreement for this decision task and there is no satisfying way to evaluate a kappa score if these agreements are left out. Other measures need to be introduced (Pevzner and Hearst, 2002; Fournier and Inkpen, 2012) if one wants to measure a different aspect of the segmentation agreement. However to be perfectly transparent with the annotation results, Figure 1 presents the contingency table for the Orchid’s style prosodic units (See also (Peshkov et al., 2012) for deeper evaluation of the annotation of the whole CID corpus). A/B (0-1) (2-3) (0-1) 12242 581 paralinguistic sounds (disjunction or disruption of utterances such as pauses, inhalation, and laughter). The annotation of prosodic units of the ORCHID.tw Dataset has been accomplished in an earlier project (Liu and Tseng,"
Y13-1007,J86-2004,0,0.22294,"eloped by the CKIP at Academia Sinica (Chen et al., 1996). Previously, the ORCHID.tw dataset has been annotated with boundaries of prosodic units as defined in (Liu and Tseng, 2009) and with boundaries of discourse units in (Chen, 2011). In the present ORCHID project, we modified the criteria for labeling discourse units according to the commonly defined operational guidelines for French and Taiwan Mandarin data processing. The definition for prosodic units remains unchanged. 3 3.1 Producing prosodic units French data The definition of prosodic units is adopted mainly from prosodic phonology (Selkirk, 1986; Nespor and Vogel, 1986) that proposed a universal hierarchy of prosodic constituents. At least two levels of phrasing above the word have been admitted in French: the lowest level of phonological phrases (Post, 2000) or accentual phrases (AP) (Jun and Fougeron, 2000) and the highest level of Intonational phrases (IPs). The accentual phrase is the domain of primary stress. This latter is realized on the final full syllable of a word with longer duration and higher intensity than non-final syllables, and associated with a melodic movement. The secondary stress, more variable and optional, is g"
Y13-1007,O13-2001,1,0.705931,"r dataset which is a nice score for naive coders on prosodic phrasing task. Cohen’s kappa (Cohen and others, 1960) (and see (Carletta, 1996; Artstein and Poesio, 2008) for further discussion) is a measure designed to measure criteria for labeling discourse units according to the commonly defined operational guidelines for French and Taiwan Mandarin data processing. 2.2 Creation of the Taiwan Mandarin dataset The ORCHID.tw Dataset is a subset of the Taiwan Mandarin Conversational Corpus (the TMC Corpus), consisting of 3.5 hours of conversational speech produced by 7 male and 9 female speakers (Tseng, 2013). The TMC Corpus is a collection of 42 hours of free, task-oriented and topic-specific conversations in Taiwan Mandarin. All the speaker turn boundaries as well as syllable boundaries were human-labeled in the ORCHID.tw Dataset. Boundaries of words and POS tags were automatically generated based on the syllable boundary information and the output of the automatic word segmentation and POS tagging system developed by the CKIP at Academia Sinica (Chen et al., 1996). Previously, the ORCHID.tw dataset has been annotated with boundaries of prosodic units as defined in (Liu and Tseng, 2009) and with"
Y13-1007,J08-2004,0,0.0230037,"ion campaign also involved naive annotators that have segmented the whole corpus (half of it being cross annotated). This annotation was performed without listening to the signal but with timing information. It was performed with Praat (Boersma, 2002) but without including the signal window, only the time-aligned token tiers. The segmentation was performed by adopting a set of discourse segmentation guidelines, inspired from (Muller et al., 2012) and (Chen, 2011). We combined semantic criterion (Vendler’s (Vendler, 1957) style eventualities identification and Xue’s proposition identification (Xue, 2008)), discourse criterion (presence of discourse markers) and pragmatic criterion (recognition of specific speech acts) to perform the segmentation. More practically the task consisted in first identifying a main predicate, and then all its complements and adjuncts as illustrated in (1) and (2). Mandarin spontaneous speech presents an additional challenge in the task of DU annotation for its lack of tensemarking verbal system. Our segmentation proceeds on the basis of the semantic bonding between prediFigure 1: Contingency table for the French prosodic units 3.2 Producing discourse units Taiwan M"
Y13-1007,2008.jeptalnrecital-long.29,0,\N,Missing
Y15-1034,bigi-2012-sppas-tool,0,0.0153636,"e Aix-MapTask (Bard et al., 2013; Gorisch et al., 2014) is a reproduction of the original HCRC MapTask protocol (Anderson et al., 1991) in the French language. It involves 4 pairs of participants with 8 maps per pair and turning roles of giver and follower. The remote condition (MTR) contains audio recordings that sum up to 2h30 with an average of 6 min. 52 sec. per map2 . Data extraction Our objective is to obtain a dataset that covers as completely as possible feedback utterances. We exploited our rather precise transcriptions (aligned with the signal at the phone level with the tool SPPAS (Bigi, 2012)) that include laughter, truncated words, filled pauses and other speech events. We started from the observation that the majority of feedback utterances are IPUs composed of only a few tokens. We first identified the small set of most frequent lexical items composing feedback utterances by building the lexical tokens distribution for IPUs made of three tokens or less. The 10 most frequent lexical forms are : ouais / yeah (2781), mh (2321), d’accord / agree-right (1082), laughter (920), oui / yes (888), euh / uh (669), ok (632), ah (433), voil / that’s it-right (360). The next ones are et / an"
Y15-1034,W10-1829,0,0.0431277,"Missing"
Y15-1034,gorisch-etal-2014-aix,1,0.790479,"Missing"
Y15-1034,J12-1001,0,0.229804,"Missing"
Y15-1034,N09-2050,0,0.0322601,"y 6000 utterances). We present its evaluation not merely in terms of inter-rater agreement but also in terms of usability of the resulting reference dataset both from a linguistic research perspective and from a more applicative viewpoint. 1 Introduction Positive feedback tokens (yeah, yes, mhm ...) are the most frequent tokens in spontaneous speech. They play a crucial role in managing the common ground of a conversation. Several studies have attempted to provide a detailed quantitative analysis of these tokens in particular by looking at the form-function relationship (Allwood et al., 2007; Petukhova and Bunt, 2009; Gravano et al., 2012; Despite the previous attempts to quantify that form-function relationship of feedback, we think that more work needs to be done on the conversational part of it. For example, Gravano et al. (2012) used automatic classification of positive cue words, however the underlying corpus consists of games, that are far off being “conversational” and therefore do not permit to draw any conclusions on how feedback is performed in conversational talk or talkin-interaction. What concerns the selection of the feedback units, i.e. utterances, more work that clarifies what consists of"
Y15-1034,W15-4620,1,0.854205,"Missing"
