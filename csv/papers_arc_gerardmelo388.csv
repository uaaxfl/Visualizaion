2021.naacl-main.245,Faithfully Explainable Recommendation via Neural Logic Reasoning,2021,-1,-1,4,0,3975,yaxin zhu,Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"Knowledge graphs (KG) have become increasingly important to endow modern recommender systems with the ability to generate traceable reasoning paths to explain the recommendation process. However, prior research rarely considers the faithfulness of the derived explanations to justify the decision-making process. To the best of our knowledge, this is the first work that models and evaluates faithfully explainable recommendation under the framework of KG reasoning. Specifically, we propose neural logic reasoning for explainable recommendation (LOGER) by drawing on interpretable logical rules to guide the path-reasoning process for explanation generation. We experiment on three large-scale datasets in the e-commerce domain, demonstrating the effectiveness of our method in delivering high-quality recommendations as well as ascertaining the faithfulness of the derived explanation."
2021.louhi-1.4,Fast and Effective Biomedical Entity Linking Using a Dual Encoder,2021,-1,-1,3,1,5388,rajarshi bhowmik,Proceedings of the 12th International Workshop on Health Text Mining and Information Analysis,0,"Biomedical entity linking is the task of identifying mentions of biomedical concepts in text documents and mapping them to canonical entities in a target thesaurus. Recent advancements in entity linking using BERT-based models follow a \textit{retrieve and rerank} paradigm, where the candidate entities are first selected using a retriever model, and then the retrieved candidates are ranked by a reranker model. While this paradigm produces state-of-the-art results, they are slow both at training and test time as they can process only one mention at a time. To mitigate these issues, we propose a BERT-based dual encoder model that resolves multiple mentions in a document in one shot. We show that our proposed model is multiple times faster than existing BERT-based models while being competitive in accuracy for biomedical entity linking. Additionally, we modify our dual encoder model for end-to-end biomedical entity linking that performs both mention span detection and entity disambiguation and out-performs two recently proposed models."
2021.lantern-1.3,Exploiting Image{--}Text Synergy for Contextual Image Captioning,2021,-1,-1,4,1,5528,sreyasi chowdhury,Proceedings of the Third Workshop on Beyond Vision and LANguage: inTEgrating Real-world kNowledge (LANTERN),0,"Modern web content - news articles, blog posts, educational resources, marketing brochures - is predominantly multimodal. A notable trait is the inclusion of media such as images placed at meaningful locations within a textual narrative. Most often, such images are accompanied by captions - either factual or stylistic (humorous, metaphorical, etc.) - making the narrative more engaging to the reader. While standalone image captioning has been extensively studied, captioning an image based on external knowledge such as its surrounding text remains under-explored. In this paper, we study this new task: given an image and an associated unstructured knowledge snippet, the goal is to generate a contextual caption for the image."
2021.emnlp-main.312,Context-Aware Interaction Network for Question Matching,2021,-1,-1,4,0,9346,zhe hu,Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,0,"Impressive milestones have been achieved in text matching by adopting a cross-attention mechanism to capture pertinent semantic connections between two sentence representations. However, regular cross-attention focuses on word-level links between the two input sequences, neglecting the importance of contextual information. We propose a context-aware interaction network (COIN) to properly align two sequences and infer their semantic relationship. Specifically, each interaction block includes (1) a context-aware cross-attention mechanism to effectively integrate contextual information when aligning two sequences, and (2) a gate fusion layer to flexibly interpolate aligned representations. We apply multiple stacked interaction blocks to produce alignments at different levels and gradually refine the attention results. Experiments on two question matching datasets and detailed analyses demonstrate the effectiveness of our model."
2021.emnlp-main.781,Guilt by Association: Emotion Intensities in Lexical Representations,2021,-1,-1,2,0,10188,shahab raji,Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,0,"What do linguistic models reveal about the emotions associated with words? In this study, we consider the task of estimating word-level emotion intensity scores for specific emotions, exploring unsupervised, supervised, and finally a self-supervised method of extracting emotional associations from pretrained vectors and models. Overall, we find that linguistic models carry substantial potential for inducing fine-grained emotion intensity scores, showing a far higher correlation with human ground truth ratings than state-of-the-art emotion lexicons based on labeled data."
2021.acl-long.110,Assessing Emoji Use in Modern Text Processing Tools,2021,-1,-1,2,1,12861,abu shoeb,Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),1,"Emojis have become ubiquitous in digital communication, due to their visual appeal as well as their ability to vividly convey human emotion, among other factors. This also leads to an increased need for systems and tools to operate on text containing emojis. In this study, we assess this support by considering test sets of tweets with emojis, based on which we perform a series of experiments investigating the ability of prominent NLP and text processing tools to adequately process them. In particular, we consider tokenization, part-of-speech tagging, dependency parsing, as well as sentiment analysis. Our findings show that many systems still have notable shortcomings when operating on text containing emojis."
2021.acl-long.379,{R}2{D}2: Recursive Transformer based on Differentiable Tree for Interpretable Hierarchical Language Modeling,2021,-1,-1,7,0,13256,xiang hu,Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),1,"Human language understanding operates at multiple levels of granularity (e.g., words, phrases, and sentences) with increasing levels of abstraction that can be hierarchically combined. However, existing deep models with stacked layers do not explicitly model any sort of hierarchical process. In this paper, we propose a recursive Transformer model based on differentiable CKY style binary trees to emulate this composition process, and we extend the bidirectional language model pre-training objective to this architecture, attempting to predict each word given its left and right abstraction nodes. To scale up our approach, we also introduce an efficient pruning and growing algorithm to reduce the time complexity and enable encoding in linear time. Experimental results on language modeling and unsupervised parsing show the effectiveness of our approach."
2021.acl-long.401,Data Augmentation with Adversarial Training for Cross-Lingual {NLI},2021,-1,-1,5,0.897436,9257,xin dong,Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),1,"Due to recent pretrained multilingual representation models, it has become feasible to exploit labeled data from one language to train a cross-lingual model that can then be applied to multiple new languages. In practice, however, we still face the problem of scarce labeled data, leading to subpar results. In this paper, we propose a novel data augmentation strategy for better cross-lingual natural language inference by enriching the data to reflect more diversity in a semantically faithful way. To this end, we propose two methods of training a generative model to induce synthesized examples, and then leverage the resulting data using an adversarial training regimen for more robustness. In a series of detailed experiments, we show that this fruitful combination leads to substantial gains in cross-lingual inference."
2020.lrec-1.382,Inducing Universal Semantic Tag Vectors,2020,-1,-1,2,0,17427,da huo,Proceedings of the 12th Language Resources and Evaluation Conference,0,"Given the well-established usefulness of part-of-speech tag annotations in many syntactically oriented downstream NLP tasks, the recently proposed notion of semantic tagging (Bjerva et al. 2016) aims at tagging words with tags informed by semantic distinctions, which are likely to be useful across a range of semantic tasks. To this end, their annotation scheme distinguishes, for instance, privative attributes from subsective ones. While annotated corpora exist, their size is limited and thus many words are out-of-vocabulary words. In this paper, we study to what extent we can automatically predict the tags associated with unseen words. We draw on large-scale word representation data to derive a large new Semantic Tag lexicon. Our experiments show that we can infer semantic tags for words with high accuracy both monolingually and cross-lingually."
2020.lrec-1.856,Correcting the Autocorrect: Context-Aware Typographical Error Correction via Training Data Augmentation,2020,19,0,2,0,18307,kshitij shah,Proceedings of the 12th Language Resources and Evaluation Conference,0,"In this paper, we explore the artificial generation of typographical errors based on real-world statistics. We first draw on a small set of annotated data to compute spelling error statistics. These are then invoked to introduce errors into substantially larger corpora. The generation methodology allows us to generate particularly challenging errors that require context-aware error detection. We use it to create a set of English language error detection and correction datasets. Finally, we examine the effectiveness of machine learning models for detecting and correcting errors based on this data."
2020.emnlp-main.720,{E}mo{T}ag1200: Understanding the Association between Emojis and Emotions,2020,24,0,2,1,12861,abu shoeb,Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),0,"Given the growing ubiquity of emojis in language, there is a need for methods and resources that shed light on their meaning and communicative role. One conspicuous aspect of emojis is their use to convey affect in ways that may otherwise be non-trivial to achieve. In this paper, we seek to explore the connection between emojis and emotions by means of a new dataset consisting of human-solicited association ratings. We additionally conduct experiments to assess to what extent such associations can be inferred from existing data in an unsupervised manner. Our experiments show that this succeeds when high-quality word-level information is available."
2020.coling-main.300,Sentence Analogies: Linguistic Regularities in Sentence Embeddings,2020,36,0,2,0,21398,xunjie zhu,Proceedings of the 28th International Conference on Computational Linguistics,0,"While important properties of word vector representations have been studied extensively, far less is known about the properties of sentence vector representations. Word vectors are often evaluated by assessing to what degree they exhibit regularities with regard to relationships of the sort considered in word analogies. In this paper, we investigate to what extent commonly used sentence vector representation spaces as well reflect certain kinds of regularities. We propose a number of schemes to induce evaluation data, based on lexical analogy data as well as semantic relationships between sentences. Our experiments consider a wide range of sentence embedding methods, including ones based on BERT-style contextual embeddings. We find that different models differ substantially in their ability to reflect such regularities."
2020.coling-main.479,Data Augmentation for Multiclass Utterance Classification {--} A Systematic Study,2020,-1,-1,6,0,21576,binxia xu,Proceedings of the 28th International Conference on Computational Linguistics,0,"Utterance classification is a key component in many conversational systems. However, classifying real-world user utterances is challenging, as people may express their ideas and thoughts in manifold ways, and the amount of training data for some categories may be fairly limited, resulting in imbalanced data distributions. To alleviate these issues, we conduct a comprehensive survey regarding data augmentation approaches for text classification, including simple random resampling, word-level transformations, and neural text generation to cope with imbalanced data. Our experiments focus on multi-class datasets with a large number of data samples, which has not been systematically studied in previous work. The results show that the effectiveness of different data augmentation schemes depends on the nature of the dataset under consideration."
2020.coling-main.517,Cross-Lingual Emotion Lexicon Induction using Representation Alignment in Low-Resource Settings,2020,-1,-1,2,0,21629,arun ramachandran,Proceedings of the 28th International Conference on Computational Linguistics,0,"Emotion lexicons provide information about associations between words and emotions. They have proven useful in analyses of reviews, literary texts, and posts on social media, among other things. We evaluate the feasibility of deriving emotion lexicons cross-lingually, especially for low-resource languages, from existing emotion lexicons in resource-rich languages. For this, we start out from very small corpora to induce cross-lingually aligned vector spaces. Our study empirically analyses the effectiveness of the induced emotion lexicons by measuring translation precision and correlations with existing emotion lexicons, along with measurements on a downstream task of sentence emotion prediction."
2020.coling-main.578,Domain-Specific Sentiment Lexicons Induced from Labeled Documents,2020,-1,-1,3,0,21703,sm islam,Proceedings of the 28th International Conference on Computational Linguistics,0,"Sentiment analysis is an area of substantial relevance both in industry and in academia, including for instance in social studies. Although supervised learning algorithms have advanced considerably in recent years, in many settings it remains more practical to apply an unsupervised technique. The latter are oftentimes based on sentiment lexicons. However, existing sentiment lexicons reflect an abstract notion of polarity and do not do justice to the substantial differences of word polarities between different domains. In this work, we draw on a collection of domain-specific data to induce a set of 24 domain-specific sentiment lexicons. We rely on initial linear models to induce initial word intensity scores, and then train new deep models based on word vector representations to overcome the scarcity of the original seed data. Our analysis shows substantial differences between domains, which make domain-specific sentiment lexicons a promising form of lexical resource in downstream tasks, and the predicted lexicons indeed perform effectively on tasks such as review classification and cross-lingual word sentiment prediction."
2020.coling-industry.4,Query Distillation: {BERT}-based Distillation for Ensemble Ranking,2020,-1,-1,5,0,21732,wangshu zhang,Proceedings of the 28th International Conference on Computational Linguistics: Industry Track,0,"Recent years have witnessed substantial progress in the development of neural ranking networks, but also an increasingly heavy computational burden due to growing numbers of parameters and the adoption of model ensembles. Knowledge Distillation (KD) is a common solution to balance the effectiveness and efficiency. However, it is not straightforward to apply KD to ranking problems. Ranking Distillation (RD) has been proposed to address this issue, but only shows effectiveness on recommendation tasks. We present a novel two-stage distillation method for ranking problems that allows a smaller student model to be trained while benefitting from the better performance of the teacher model, providing better control of the inference latency and computational burden. We design a novel BERT-based ranking model structure for list-wise ranking to serve as our student model. All ranking candidates are fed to the BERT model simultaneously, such that the self-attention mechanism can enable joint inference to rank the document list. Our experiments confirm the advantages of our method, not just with regard to the inference latency but also in terms of higher-quality rankings compared to the original teacher model."
2020.coling-industry.8,Interactive Question Clarification in Dialogue via Reinforcement Learning,2020,-1,-1,5,0,13256,xiang hu,Proceedings of the 28th International Conference on Computational Linguistics: Industry Track,0,"Coping with ambiguous questions has been a perennial problem in real-world dialogue systems. Although clarification by asking questions is a common form of human interaction, it is hard to define appropriate questions to elicit more specific intents from a user. In this work, we propose a reinforcement model to clarify ambiguous questions by suggesting refinements of the original query. We first formulate a collection partitioning problem to select a set of labels enabling us to distinguish potential unambiguous intents. We list the chosen labels as intent phrases to the user for further confirmation. The selected label along with the original user query then serves as a refined query, for which a suitable response can more easily be identified. The model is trained using reinforcement learning with a deep policy network. We evaluate our model based on real-world user clicks and demonstrate significant improvements across several different experiments."
W19-0421,Using Multi-Sense Vector Embeddings for Reverse Dictionaries,2019,29,1,4,0,3874,michael hedderich,Proceedings of the 13th International Conference on Computational Semantics - Long Papers,0,"Popular word embedding methods such as word2vec and GloVe assign a single vector representation to each word, even if a word has multiple distinct meanings. Multi-sense embeddings instead provide different vectors for each sense of a word. However, they typically cannot serve as a drop-in replacement for conventional single-sense embeddings, because the correct sense vector needs to be selected for each word. In this work, we study the effect of multi-sense embeddings on the task of reverse dictionaries. We propose a technique to easily integrate them into an existing neural network architecture using an attention mechanism. Our experiments demonstrate that large improvements can be obtained when employing multi-sense embeddings both in the input sequence as well as for the target representation. An analysis of the sense distributions and of the learned attention is provided as well."
R19-1126,{E}mo{T}ag {--} Towards an Emotion-Based Analysis of Emojis,2019,0,1,3,1,12861,abu shoeb,Proceedings of the International Conference on Recent Advances in Natural Language Processing (RANLP 2019),0,"Despite being a fairly recent phenomenon, emojis have quickly become ubiquitous. Besides their extensive use in social media, they are now also invoked in customer surveys and feedback forms. Hence, there is a need for techniques to understand their sentiment and emotion. In this work, we provide a method to quantify the emotional association of basic emotions such as anger, fear, joy, and sadness for a set of emojis. We collect and process a unique corpus of 20 million emoji-centric tweets, such that we can capture rich emoji semantics using a comparably small dataset. We evaluate the induced emotion profiles of emojis with regard to their ability to predict word affect intensities as well as sentiment scores."
P19-1192,Rhetorically Controlled Encoder-Decoder for {M}odern {C}hinese Poetry Generation,2019,0,2,4,0,21358,zhiqiang liu,Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,1,"Rhetoric is a vital element in modern poetry, and plays an essential role in improving its aesthetics. However, to date, it has not been considered in research on automatic poetry generation. In this paper, we propose a rhetorically controlled encoder-decoder for modern Chinese poetry generation. Our model relies on a continuous latent variable as a rhetoric controller to capture various rhetorical patterns in an encoder, and then incorporates rhetoric-based mixtures while generating modern Chinese poetry. For metaphor and personification, an automated evaluation shows that our model outperforms state-of-the-art baselines by a substantial margin, while human evaluation shows that our model generates better poems than baseline methods in terms of fluency, coherence, meaningfulness, and rhetorical aesthetics."
N19-1056,{CITE}: A Corpus of Image-Text Discourse Relations,2019,17,2,3,0,1539,malihe alikhani,"Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",0,"This paper presents a novel crowd-sourced resource for multimodal discourse: our resource characterizes inferences in image-text contexts in the domain of cooking recipes in the form of coherence relations. Like previous corpora annotating discourse structure between text arguments, such as the Penn Discourse Treebank, our new corpus aids in establishing a better understanding of natural communication and common-sense reasoning, while our findings have implications for a wide range of applications, such as understanding and generation of multimodal documents."
D19-1658,A Robust Self-Learning Framework for Cross-Lingual Text Classification,2019,0,1,2,1,9257,xin dong,Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),0,"Based on massive amounts of data, recent pretrained contextual representation models have made significant strides in advancing a number of different English NLP tasks. However, for other languages, relevant training data may be lacking, while state-of-the-art deep learning methods are known to be data-hungry. In this paper, we present an elegantly simple robust self-learning framework to include unlabeled non-English samples in the fine-tuning process of pretrained multilingual representation models. We leverage a multilingual model{'}s own predictions on unlabeled non-English data in order to obtain additional information that can be used during further fine-tuning. Compared with original multilingual models and other cross-lingual classification models, we observe significant gains in effectiveness on document and sentiment classification for a range of diverse languages."
Q18-1013,Video Captioning with Multi-Faceted Attention,2018,1,16,3,0,28934,xiang long,Transactions of the Association for Computational Linguistics,0,"Video captioning has attracted an increasing amount of interest, due in part to its potential for improved accessibility and information retrieval. While existing methods rely on different kinds of visual features and model architectures, they do not make full use of pertinent semantic cues. We present a unified and extensible framework to jointly leverage multiple sorts of visual features and semantic attributes. Our novel architecture builds on LSTMs with two multi-faceted attention layers. These first learn to automatically select the most salient visual features or semantic attributes, and then yield overall representations for the input and output of the sentence generation component via custom feature scaling operations. Experimental results on the challenging MSVD and MSR-VTT datasets show that our framework outperforms previous work and performs robustly even in the presence of added noise to the features and attributes."
P18-2100,Exploring Semantic Properties of Sentence Embeddings,2018,0,16,3,0,21398,xunjie zhu,Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"Neural vector representations are ubiquitous throughout all subfields of NLP. While word vectors have been studied in much detail, thus far only little light has been shed on the properties of sentence embeddings. In this paper, we assess to what extent prominent sentence embedding methods exhibit select semantic properties. We propose a framework that generate triplets of sentences to explore how changes in the syntactic structure or semantics of a given sentence affect the similarities obtained between their sentence embeddings."
P18-1081,Generating Fine-Grained Open Vocabulary Entity Type Descriptions,2018,35,0,2,1,5388,rajarshi bhowmik,Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"While large-scale knowledge graphs provide vast amounts of structured facts about entities, a short textual description can often be useful to succinctly characterize an entity and its type. Unfortunately, many knowledge graphs entities lack such textual descriptions. In this paper, we introduce a dynamic memory-based network that generates a short open vocabulary description of an entity by jointly leveraging induced fact embeddings as well as the dynamic context of the generated sequence of words. We demonstrate the ability of our architecture to discern relevant information for more accurate generation of type description by pitting the system against several strong baselines."
P18-1235,A Helping Hand: Transfer Learning for Deep Sentiment Analysis,2018,0,9,2,1,9257,xin dong,Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"Deep convolutional neural networks excel at sentiment polarity classification, but tend to require substantial amounts of training data, which moreover differs quite significantly between domains. In this work, we present an approach to feed generic cues into the training process of such networks, leading to better generalization abilities given limited training data. We propose to induce sentiment embeddings via supervision on extrinsic data, which are then fed into the model via a dedicated memory-based component. We observe significant gains in effectiveness on a range of different datasets in seven different languages."
L18-1010,{F}ont{L}ex: A Typographical Lexicon based on Affective Associations,2018,0,1,2,0,29506,tugba kulahcioglu,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,None
L18-1695,Metaphor Suggestions based on a Semantic Metaphor Repository,2018,0,0,1,1,3978,gerard melo,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,None
P17-4020,{W}eb{C}hild 2.0 : Fine-Grained Commonsense Knowledge Distillation,2017,6,15,2,1,6886,niket tandon,"Proceedings of {ACL} 2017, System Demonstrations",0,None
I17-5002,"Multilingual Vector Representations of Words, Sentences, and Documents",2017,9,1,1,1,3978,gerard melo,"Proceedings of the {IJCNLP} 2017, Tutorial Abstracts",0,"Neural vector representations are now ubiquitous in all subfields of natural language processing and text mining. While methods such as word2vec and GloVe are well-known, this tutorial focuses on multilingual and cross-lingual vector representations, of words, but also of sentences and documents as well."
D17-1110,{PACRR}: A Position-Aware Neural {IR} Model for Relevance Matching,2017,0,16,4,0,19950,kai hui,Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing,0,"In order to adopt deep learning for information retrieval, models are needed that can capture all relevant information required to assess the relevance of a document to a given user query. While previous works have successfully captured unigram term matches, how to fully employ position-dependent information such as proximity and term dependencies has been insufficiently explored. In this work, we propose a novel neural IR model named PACRR aiming at better modeling position-dependent interactions between a query and a document. Extensive experiments on six years{'} TREC Web Track data confirm that the proposed model yields better results under multiple benchmarks."
Q16-1004,Detecting Cross-Cultural Differences Using a Multilingual Topic Model,2016,39,13,4,0,34344,ed gutierrez,Transactions of the Association for Computational Linguistics,0,"Understanding cross-cultural differences has important implications for world affairs and many aspects of the life of society. Yet, the majority of text-mining methods to date focus on the analysis of monolingual texts. In contrast, we present a statistical model that simultaneously learns a set of common topics from multilingual, non-parallel data and automatically discovers the differences in perspectives on these topics across linguistic communities. We perform a behavioural evaluation of a subset of the differences identified by our model in English and Spanish to investigate their psychological validity."
P16-4005,Visualizing and Curating Knowledge Graphs over Time and Space,2016,8,7,3,0,34359,tong ge,Proceedings of {ACL}-2016 System Demonstrations,0,"Publicly available knowledge repositories, such as Wikipedia and Freebase, benefit significantly from volunteers, whose contributions ensure that the knowledge keeps expanding and is kept up-to-date and accurate. User interactions are often limited to hypertext, tabular, or graph visualization interfaces. For spatio-temporal information, however, other interaction paradigms may be better-suited. We present an integrated system that combines crowdsourcing, automatic or semi-automatic knowledge harvesting from text, and visual analytics. It enables users to analyze large quantities of structured data and unstructured textual data from a spatio-temporal perspective and gain deep insights that are not easily observed in individual facts."
P16-1123,Relation Classification via Multi-Level Attention {CNN}s,2016,18,111,3,1,1669,linlin wang,Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"Relation classification is a crucial ingredientn in numerous information extraction systemsn seeking to mine structured facts fromn text. We propose a novel convolutionaln neural network architecture for this task,n relying on two levels of attention in ordern to better discern patterns in heterogeneousn contexts. This architecture enables endto-endn learning from task-specific labeledn data, forgoing the need for external knowledgen such as explicit dependency structures.n Experiments show that our model outperformsn previous state-of-the-art methods, includingn those relying on much richer formsn of prior knowledge."
L16-1386,The Open Linguistics Working Group: Developing the Linguistic Linked Open Data Cloud,2016,16,11,6,0,1255,john mccrae,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"The Open Linguistics Working Group (OWLG) brings together researchers from various fields of linguistics, natural language processing, and information technology to present and discuss principles, case studies, and best practices for representing, publishing and linking linguistic data collections. A major outcome of our work is the Linguistic Linked Open Data (LLOD) cloud, an LOD (sub-)cloud of linguistic resources, which covers various linguistic databases, lexicons, corpora, terminologies, and metadata repositories. We present and summarize five years of progress on the development of the cloud and of advancements in open data in linguistics, and we describe recent community activities. The paper aims to serve as a guideline to orient and involve researchers with the community and/or Linguistic Linked Open Data."
L16-1733,Medical Concept Embeddings via Labeled Background Corpora,2016,20,14,2,0,18260,eneldo mencia,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"In recent years, we have seen an increasing amount of interest in low-dimensional vector representations of words. Among other things, these facilitate computing word similarity and relatedness scores. The most well-known example of algorithms to produce representations of this sort are the word2vec approaches. In this paper, we investigate a new model to induce such vector spaces for medical concepts, based on a joint objective that exploits not only word co-occurrences but also manually labeled documents, as available from sources such as PubMed. Our extensive experimental analysis shows that our embeddings lead to significantly higher correlations with human similarity and relatedness assessments than previous work. Due to the simplicity and versatility of vector representations, these findings suggest that our resource can easily be used as a drop-in replacement to improve any systems relying on medical concept similarity measures."
W15-1523,Semantic Information Extraction for Improved Word Embeddings,2015,21,9,2,0,37046,jiaqiang chen,Proceedings of the 1st Workshop on Vector Space Modeling for Natural Language Processing,0,"Word embeddings have recently proven useful in a number of different applications that deal with natural language. Such embeddings succinctly reflect semantic similarities between words based on their sentence-internal contexts in large corpora. In this paper, we show that information extraction techniques provide valuable additional evidence of semantic relationships that can be exploited when producing word embeddings. We propose a joint model to train word embeddings both on regular context information and on more explicit semantic extractions. The word vectors obtained from such an augmented joint training show improved results on word similarity tasks, suggesting that they can be useful in applications that involve word meanings."
P15-1060,Sentiment-Aspect Extraction based on Restricted Boltzmann Machines,2015,26,22,5,1,1669,linlin wang,Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),1,Aspect extraction and sentiment analysis of reviews are both important tasks in opinion mining. We propose a novel sentiment and aspect extraction model based on Restricted Boltzmann Machines to jointly address these two tasks in an unsupervised setting. This model reflects the generation process of reviews by introducing a heterogeneous structure into the hidden layer and incorporating informative priors. Experiments show that our model outperforms previous state-of-the-art methods.
P15-1092,Perceptually Grounded Selectional Preferences,2015,51,7,3,0.459643,4353,ekaterina shutova,Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),1,"Selectional preferences (SPs) are widely used in NLP as a rich source of semantic information. While SPs have been traditionally induced from textual data, human lexical acquisition is known to rely on both linguistic and perceptual experience. We present the first SP learning method that simultaneously draws knowledge from text, images and videos, using image and video descriptions to obtain visual features. Our results show that it outperforms linguistic and visual models in isolation, as well as the existing SP induction approaches."
2015.mtsummit-papers.27,{W}iktionary-based word embeddings,2015,-1,-1,1,1,3978,gerard melo,Proceedings of Machine Translation Summit XV: Papers,0,None
W14-0152,Embedding {N}om{L}ex-{BR} nominalizations into {O}pen{W}ordnet-{PT},2014,13,3,3,1,1261,alexandre rademaker,Proceedings of the Seventh Global {W}ordnet Conference,0,"This paper presents NomLex-BR, a lexical resource describing Brazilian Portuguese nominalizations, and its integration with OpenWordnet-PT. We first describe the original English NOMLEX lexical resource and how we used it to bootstrap a Portuguese version. Subsequently, we describe how this lexicon can be embedded into OpenWordnet-PT, which facilitates its use and helps spot-checking both the bigger integrated resource and the original lexicon. Lastly, we outline some of the other, more substantial work that we plan to engage for the project of using linguistic insights for knowledge representation in Portuguese."
W14-0153,{O}pen{W}ord{N}et-{PT}: A Project Report,2014,22,6,3,1,1261,alexandre rademaker,Proceedings of the Seventh Global {W}ordnet Conference,0,"This paper presents OpenWordNet-PT, a freely available open-source wordnet for Portuguese, with its latest developments and practical uses. We provide a detailed description of the RDF representation developed for OpenWordnet-PT. We highlight our efforts to extend the coverage of our resource and add nominalization relations connecting nouns and verbs. Finally, we present several real-world applications where OpenWordnet-PT was put to use, including a large-scale high-throughput sentiment analysis system."
P14-1098,Structured Learning for Taxonomy Induction with Belief Propagation,2014,42,29,3,0,717,mohit bansal,Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"We present a structured learning approach to inducing hypernym taxonomies using a probabilistic graphical model formulation. Our model incorporates heterogeneous relational evidence about both hypernymy and siblinghood, captured by semantic features based on patterns and statistics from Web n-grams and Wikipedia abstracts. For efficient inference over taxonomy structures, we use loopy belief propagation along with a directed spanning tree algorithm for the core hypernymy factor. To train the system, we extract sub-structures of WordNet and discriminatively learn to reproduce them, using adaptive subgradient stochastic optimization. On the task of reproducing sub-hierarchies of WordNet, our approach achieves a 51% error reduction over a chance baseline, including a 15% error reduction due to the non-hypernym-factored sibling features. On a comparison setup, we find up to 29% relative error reduction over previous work on ancestor F1."
de-paiva-etal-2014-nomlex,{N}om{L}ex-{PT}: A Lexicon of {P}ortuguese Nominalizations,2014,17,9,4,1,21557,valeria paiva,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"This paper presents NomLex-PT, a lexical resource describing Portuguese nominalizations. NomLex-PT connects verbs to their nominalizations, thereby enabling NLP systems to observe the potential semantic relationships between the two words when analysing a text. NomLex-PT is freely available and encoded in RDF for easy integration with other resources. Most notably, we have integrated NomLex-PT with OpenWordNet-PT, an open Portuguese Wordnet."
de-melo-2014-etymological,Etymological {W}ordnet: Tracing The History of Words,2014,11,6,1,1,3978,gerard melo,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"Research on the history of words has led to remarkable insights about language and also about the history of human civilization more generally. This paper presents the Etymological Wordnet, the first database that aims at making word origin information available as a large, machine-readable network of words in many languages. The information in this resource is obtained from Wiktionary. Extracting a network of etymological information from Wiktionary requires significant effort, as much of the etymological information is only given in prose. We rely on custom pattern matching techniques and mine a large network with over 500,000 word origin links as well as over 2 million derivational/compositional links."
borin-etal-2014-bring,Bring vs. {MTR}oget: Evaluating automatic thesaurus translation,2014,18,1,3,0,16835,lars borin,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"Evaluation of automatic language-independent methods for language technology resource creation is difficult, and confounded by a largely unknown quantity, viz. to what extent typological differences among languages are significant for results achieved for one language or language pair to be applicable across languages generally. In the work presented here, as a simplifying assumption, language-independence is taken as axiomatic within certain specified bounds. We evaluate the automatic translation of Roget{'}s {``}Thesaurus{''} from English into Swedish using an independently compiled Roget-style Swedish thesaurus, S.C. Bring{'}s {``}Swedish vocabulary arranged into conceptual classes{''} (1930). Our expectation is that this explicit evaluation of one of the thesaureses created in the MTRoget project will provide a good estimate of the quality of the other thesauruses created using similar methods."
Q13-1023,"Good, Great, Excellent: Global Inference of Semantic Intensities",2013,32,17,1,1,3978,gerard melo,Transactions of the Association for Computational Linguistics,0,"Adjectives like good, great, and excellent are similar in meaning, but differ in intensity. Intensity order information is very useful for language learners as well as in several NLP tasks, but is missing in most lexical resources (dictionaries, WordNet, and thesauri). In this paper, we present a primarily unsupervised approach that uses semantics from Web-scale data (e.g., phrases like good but not excellent) to rank words by assigning them positions on a continuous scale. We rely on Mixed Integer Linear Programming to jointly determine the ranks, such that individual decisions benefit from global information. When ranking English adjectives, our global algorithm achieves substantial improvements over previous work on both pairwise and rank correlation metrics (specifically, 70{\%} pairwise accuracy as compared to only 56{\%} by previous work). Moreover, our approach can incorporate external synonymy information (increasing its pairwise accuracy to 78{\%}) and extends easily to new languages. We also make our code and data freely available."
P12-3026,{UWN}: A Large Multilingual Lexical Knowledge Base,2012,22,10,1,1,3978,gerard melo,Proceedings of the {ACL} 2012 System Demonstrations,0,"We present UWN, a large multilingual lexical knowledge base that describes the meanings and relationships of words in over 200 languages. This paper explains how link prediction, information integration and taxonomy induction methods have been used to build UWN based on WordNet and extend it with millions of named entities from Wikipedia. We additionally introduce extensions to cover lexical relationships, frame-semantic knowledge, and language data. An online interface provides human access to the data, while a software API enables applications to look up over 16 million words and names."
de-melo-etal-2012-empirical,Empirical Comparisons of {MASC} Word Sense Annotations,2012,6,9,1,1,3978,gerard melo,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"We analyze how different conceptions of lexical semantics affect sense annotations and how multiple sense inventories can be compared empirically, based on annotated text. Our study focuses on the MASC project, where data has been annotated using WordNet sense identifiers on the one hand, and FrameNet lexical units on the other. This allows us to compare the sense inventories of these lexical resources empirically rather than just theoretically, based on their glosses, leading to new insights. In particular, we compute contingency matrices and develop a novel measure, the Expected Jaccard Index, that quantifies the agreement between annotations of the same data based on two different resources even when they have different sets of categories."
C12-3044,{O}pen{W}ord{N}et-{PT}: An Open {B}razilian {W}ordnet for Reasoning,2012,12,37,3,1,21557,valeria paiva,Proceedings of {COLING} 2012: Demonstration Papers,0,"Brazilian Portuguese needs a Wordnet that is open access, downloadable and changeable, so that it can be improved by the community interested in using it for knowledge representation and automated deduction. This kind of resource is also very valuable to linguists and computer scientists interested in extracting and representing knowledge obtained from texts. We discuss briefly the reasons for a Brazilian Portuguese Wordnet and the process we used to get a preliminary version of such a resource. Then we discuss possible steps to improving our preliminary version.1"
C12-3055,{M}arkov Chains for Robust Graph-Based Commonsense Information Extraction,2012,7,1,3,1,6886,niket tandon,Proceedings of {COLING} 2012: Demonstration Papers,0,"Commonsense knowledge is useful for making Web search, local search, and mobile assistance behave in a way that the user perceives as xe2x80x9csmartxe2x80x9d. Most machine-readable knowledge bases, however, lack basic commonsense facts about the world, e.g. the property of ice cream being cold. This paper proposes a graph-based Markov chain approach to extract common-sense knowledge from Web-scale language models or other sources. Unlike previous work on information extraction where the graph representation of factual knowledge is rather sparse, our Markov chain approach is geared towards the challenging nature of commonsense knowledge when determining the accuracy of candidate facts. The experiments show that our method results in more accurate and robust extractions. Based on our method, we develop an online system that provides commonsense property lookup for an object in real time."
P10-1087,Untangling the Cross-Lingual Link Structure of {W}ikipedia,2010,22,37,1,1,3978,gerard melo,Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,1,"Wikipedia articles in different languages are connected by interwiki links that are increasingly being recognized as a valuable source of cross-lingual information. Unfortunately, large numbers of links are imprecise or simply wrong. In this paper, techniques to detect such problems are identified. We formalize their removal as an optimization task based on graph repair operations. We then present an algorithm with provable properties that uses linear programming and a region growing technique to tackle this challenge. This allows us to transform Wikipedia into a much more consistent multilingual register of the world's entities and concepts."
de-melo-weikum-2010-providing,"Providing Multilingual, Multimodal Answers to Lexical Database Queries",2010,16,8,1,1,3978,gerard melo,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"Language users are increasingly turning to electronic resources to address their lexical information needs, due to their convenience and their ability to simultaneously capture different facets of lexical knowledge in a single interface. In this paper, we discuss techniques to respond to a user's lexical queries by providing multilingual and multimodal information, and facilitating navigating along different types of links. To this end, structured information from sources like WordNet, Wikipedia, Wiktionary, as well as Web services is linked and integrated to provide a multi-faceted yet consistent response to user queries. The meanings of words in many different languages are characterized by mapping them to appropriate WordNet sense identifiers and adding multilingual gloss descriptions as well as example sentences. Relationships are derived from WordNet and Wiktionary to allow users to discover semantically related words, etymologically related words, alternative spellings, as well as misspellings. Last but not least, images, audio recordings, and geographical maps extracted from Wikipedia and Wiktionary allow for a multimodal experience."
W09-4407,Extracting Sense-Disambiguated Example Sentences From Parallel Corpora,2009,44,7,1,1,3978,gerard melo,Proceedings of the 1st Workshop on Definition Extraction,0,"Example sentences provide an intuitive means of grasping the meaning of a word, and are frequently used to complement conventional word definitions. When a word has multiple meanings, it is useful to have example sentences for specific senses (and hence definitions) of that word rather than indiscriminately lumping all of them together. In this paper, we investigate to what extent such sense-specific example sentences can be extracted from parallel corpora using lexical knowledge bases for multiple languages as a sense index. We use word sense disambiguation heuristics and a cross-lingual measure of semantic similarity to link example sentences to specific word senses. From the sentences found for a given sense, an algorithm then selects a smaller subset that can be presented to end users, taking into account both representativeness and diversity. Preliminary results show that a precision of around 80% can be obtained for a reasonable number of word senses, and that the subset selection yields convincing results."
de-melo-weikum-2008-mapping,Mapping {R}oget{'}s Thesaurus and {W}ord{N}et to {F}rench,2008,14,3,1,1,3978,gerard melo,Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08),0,"RogetÂs Thesaurus and WordNet are very widely used lexical reference works. We describe an automatic mapping procedure that effectively produces French translations of the terms in these two resources. Our approach to the challenging task of disambiguation is based on structural statistics as well as measures of semantic relatedness that are utilized to learn a classification model for associations between entries in the thesaurus and French terms taken from bilingual dictionaries. By building and applying such models, we have produced French versions of RogetÂs Thesaurus and WordNet with a considerable level of accuracy, which can be used for a variety of different purposes, by humans as well as in computational applications."
