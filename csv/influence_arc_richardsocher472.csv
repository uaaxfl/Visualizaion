2020.acl-demos.24,P19-1444,0,0.163026,"Missing"
2020.acl-demos.24,D19-1378,0,0.0980991,"Missing"
2020.acl-demos.24,D18-1547,0,0.0138849,"by (Rajpurkar et al., 2018), we create a synthetic dataset which consists of untranslatable questions generated by applying rule-based transformations and adversarial filtering (Zellers et al., 2018) to examples in existing text-to-SQL datasets. We then train a stagewise model that first classifies if the input is translatable or not, and then predicts confusing spans in an untranslatable input. Dataset Construction. In order to construct the untranslatable questions, we firstly exam the types of untranslatable questions seen on the manually constructed CoSQL (Yu et al., 2019a) and MultiWOZ (Budzianowski et al., 2018) datasets (Table 4 of A.1). We then design our modification strategies to generate the untranslatable questions from the original text-to-SQL dataset automatically. Specifically, for a text-to-SQL example that contains a natural language question, a DB schema and a SQL query, we first identify all non-overlapping question spans that possibly refer to a table field occurred in the SELECT and WHERE clauses of the SQL query using string-matching heuristics. Then we apply Swap and Drop operations on the question and DB schema respectively to generate different types of untranslatable questions. Th"
2020.acl-demos.24,H94-1010,0,0.812291,"Missing"
2020.acl-demos.24,N19-1423,0,0.524929,"plemented the neural semantic parser. † Work done during internship at Salesforce Research. Jagadish, 2014; Setlur et al., 2016, 2019). While they have been shown effective in pilot study and production, rule-based approaches are limited in terms of coverage, scalability and naturalness – they are not robust against the diversity of human language expressions and are difficult to scale across domains. Recent advances in neural natural language processing (Sutskever et al., 2014; Dong and Lapata, 2016; See et al., 2017a; Liang et al., 2017; Lin et al., 2019; Bogin et al., 2019a), pre-training (Devlin et al., 2019; Hwang et al., 2019), and the availability of large-scale supervised datasets (Zhong 204 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 204–214 c July 5 - July 10, 2020. 2020 Association for Computational Linguistics et al., 2017; Finegan-Dollak et al., 2018; Yu et al., 2018, 2019b,a) enabled deep learning based approaches to significantly improve the state-of-theart in nearly all subtasks of building an NLIDB. These include semantic parsing (Dong and Lapata, 2018; Zhang et al., 2019), ambiguity detection and confidence estimation (Dong et al.,"
2020.acl-demos.24,P18-1033,0,0.0408187,"not robust against the diversity of human language expressions and are difficult to scale across domains. Recent advances in neural natural language processing (Sutskever et al., 2014; Dong and Lapata, 2016; See et al., 2017a; Liang et al., 2017; Lin et al., 2019; Bogin et al., 2019a), pre-training (Devlin et al., 2019; Hwang et al., 2019), and the availability of large-scale supervised datasets (Zhong 204 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 204–214 c July 5 - July 10, 2020. 2020 Association for Computational Linguistics et al., 2017; Finegan-Dollak et al., 2018; Yu et al., 2018, 2019b,a) enabled deep learning based approaches to significantly improve the state-of-theart in nearly all subtasks of building an NLIDB. These include semantic parsing (Dong and Lapata, 2018; Zhang et al., 2019), ambiguity detection and confidence estimation (Dong et al., 2018; Yao et al., 2019), natural language response generation (Liu et al., 2019) and so on. Moreover, by jointly modeling the natural language question and database schema in the neural space, latest text-to-SQL semantic parsers can work cross domains (Yu et al., 2018; Zhang et al., 2019). In this work, we"
2020.acl-demos.24,H90-1021,0,0.541793,"Missing"
2020.acl-demos.24,P17-1003,0,0.168557,"teraction flow and the neural question corrector. Victoria designed and implemented the neural semantic parser. † Work done during internship at Salesforce Research. Jagadish, 2014; Setlur et al., 2016, 2019). While they have been shown effective in pilot study and production, rule-based approaches are limited in terms of coverage, scalability and naturalness – they are not robust against the diversity of human language expressions and are difficult to scale across domains. Recent advances in neural natural language processing (Sutskever et al., 2014; Dong and Lapata, 2016; See et al., 2017a; Liang et al., 2017; Lin et al., 2019; Bogin et al., 2019a), pre-training (Devlin et al., 2019; Hwang et al., 2019), and the availability of large-scale supervised datasets (Zhong 204 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 204–214 c July 5 - July 10, 2020. 2020 Association for Computational Linguistics et al., 2017; Finegan-Dollak et al., 2018; Yu et al., 2018, 2019b,a) enabled deep learning based approaches to significantly improve the state-of-theart in nearly all subtasks of building an NLIDB. These include semantic parsing (Dong and Lapata, 2018; Zhang"
2020.acl-demos.24,P19-1600,0,0.0431758,"Missing"
2020.acl-demos.24,D15-1166,0,0.0190622,"ng Untranslatable Questions and Confusing Spans. We utilize the BERT contextualized representations of [CLS] token, followed by a single-layer classifier to tell whether a given user question and table schema can be translated into SQL or not. To identify the questionable token spans of untranslatable question, following Zhang et al. (2019), we employ a hierarchical bi-LSTM structure to encode each column header and use the hidden states as the column header embedding. We then use a bi-LSTM to encode the question’s BERT embedding, and the hidden states are fed into a dot-product co-attention (Luong et al., 2015) layer over the column header embedding. The output of co-attention augmented question embedding is fed into a linear layer follow by softmax operator to predict the start and end tokens indices of the confusing spans in the question. 3.2.2 Database-aware Token Correction Figure 5 illustrates the proposed tokens correction module in P HOTON. We use the masked language model (MLM) of BERT (Devlin et al., 2019) to auto-correct the confusing tokens. Specifically, we replace the confusing tokens with the [MASK] special token. The output distribution of MLM head on the mask token is employed to sco"
2020.acl-demos.24,P17-1099,0,0.0610987,"emented the demo interaction flow and the neural question corrector. Victoria designed and implemented the neural semantic parser. † Work done during internship at Salesforce Research. Jagadish, 2014; Setlur et al., 2016, 2019). While they have been shown effective in pilot study and production, rule-based approaches are limited in terms of coverage, scalability and naturalness – they are not robust against the diversity of human language expressions and are difficult to scale across domains. Recent advances in neural natural language processing (Sutskever et al., 2014; Dong and Lapata, 2016; See et al., 2017a; Liang et al., 2017; Lin et al., 2019; Bogin et al., 2019a), pre-training (Devlin et al., 2019; Hwang et al., 2019), and the availability of large-scale supervised datasets (Zhong 204 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 204–214 c July 5 - July 10, 2020. 2020 Association for Computational Linguistics et al., 2017; Finegan-Dollak et al., 2018; Yu et al., 2018, 2019b,a) enabled deep learning based approaches to significantly improve the state-of-theart in nearly all subtasks of building an NLIDB. These include semantic parsing (Dong and"
2020.acl-demos.24,D19-1284,0,0.0131539,"on the Spider dev set, which validates the effectiveness of our neural semantic parser for translating an input question into a valid SQL query. 5 Related Work Natural Language Interfaces to Databases. NLIDBs has been studied extensively in the past decades. Thanks to the availability of large-scale datasets (Zhong et al., 2017; Finegan-Dollak et al., 2018; Yu et al., 2018), data-driven approaches have dominated the field, in which deep learning based models achieve the best performance in both strongly (Hwang et al., 2019; Zhang et al., 2019; Guo et al., 2019) and weakly (Liang et al., 2017; Min et al., 2019) supervised settings. However, most of existing text-to-SQL datasets include only questions that can be translated into a valid SQL query. Spider (Finegan-Dollak et al., 2018) specifically controlled question clarify during data collection to exclude poorly phrased and ambiguous questions. WikiSQL (Zhong et al., 2017) was constructed on top of manually written synchronous grammars, and the mapping between its questions and SQL queries can be effectively resolved via lexical matching in vector space (Hwang et al., 2019). CoSQL (Yu et al., 2019a) is by far the only existing corpus to our knowled"
2020.acl-demos.24,J84-3009,0,0.225912,"Missing"
2020.acl-demos.24,P18-2124,0,0.07226,"Missing"
2020.acl-demos.24,D19-1547,0,0.0418812,"estion “How many are there?”. Once we have the synthetic untranslatable questions, adversarial filtering is employed to iteratively refine the set of untranslatable examples to be more indiscernible by trivial stylistic classifiers (Zellers et al., 2018). Predicting Untranslatable Questions and Confusing Spans. We utilize the BERT contextualized representations of [CLS] token, followed by a single-layer classifier to tell whether a given user question and table schema can be translated into SQL or not. To identify the questionable token spans of untranslatable question, following Zhang et al. (2019), we employ a hierarchical bi-LSTM structure to encode each column header and use the hidden states as the column header embedding. We then use a bi-LSTM to encode the question’s BERT embedding, and the hidden states are fed into a dot-product co-attention (Luong et al., 2015) layer over the column header embedding. The output of co-attention augmented question embedding is fed into a linear layer follow by softmax operator to predict the start and end tokens indices of the confusing spans in the question. 3.2.2 Database-aware Token Correction Figure 5 illustrates the proposed tokens correctio"
2020.acl-demos.24,D19-1204,1,0.830008,"Missing"
2020.acl-demos.24,D18-1425,0,0.230392,"Missing"
2020.acl-demos.24,P19-1443,1,0.936631,"able Question Detection Inspired by (Rajpurkar et al., 2018), we create a synthetic dataset which consists of untranslatable questions generated by applying rule-based transformations and adversarial filtering (Zellers et al., 2018) to examples in existing text-to-SQL datasets. We then train a stagewise model that first classifies if the input is translatable or not, and then predicts confusing spans in an untranslatable input. Dataset Construction. In order to construct the untranslatable questions, we firstly exam the types of untranslatable questions seen on the manually constructed CoSQL (Yu et al., 2019a) and MultiWOZ (Budzianowski et al., 2018) datasets (Table 4 of A.1). We then design our modification strategies to generate the untranslatable questions from the original text-to-SQL dataset automatically. Specifically, for a text-to-SQL example that contains a natural language question, a DB schema and a SQL query, we first identify all non-overlapping question spans that possibly refer to a table field occurred in the SELECT and WHERE clauses of the SQL query using string-matching heuristics. Then we apply Swap and Drop operations on the question and DB schema respectively to generate diff"
2020.acl-demos.24,D18-1009,0,0.109076,"sifier to detect user input to which a SQL mapping cannot be immediately determined. This covers questions that are incomplete (e.g. What is the total?), ambiguous or vague (e.g. Show me homes with good schools), beyond the representation scope of SQL (e.g. How many tourists visited all of the 10 attractions?), or simply noisy (e.g. Cyrus teaches physics in department). 3.2.1 Untranslatable Question Detection Inspired by (Rajpurkar et al., 2018), we create a synthetic dataset which consists of untranslatable questions generated by applying rule-based transformations and adversarial filtering (Zellers et al., 2018) to examples in existing text-to-SQL datasets. We then train a stagewise model that first classifies if the input is translatable or not, and then predicts confusing spans in an untranslatable input. Dataset Construction. In order to construct the untranslatable questions, we firstly exam the types of untranslatable questions seen on the manually constructed CoSQL (Yu et al., 2019a) and MultiWOZ (Budzianowski et al., 2018) datasets (Table 4 of A.1). We then design our modification strategies to generate the untranslatable questions from the original text-to-SQL dataset automatically. Specifica"
2020.acl-main.263,S13-1004,0,0.0679212,"Missing"
2020.acl-main.263,D18-1316,0,0.182867,"in the correct answer sentence. Belinkov and Bisk (2018) followed by demonstrating the brittleness of neural machine translation systems against character-level perturbations like randomly swapping/replacing characters. However, these attacks are not optimized on the target models, unlike Ebrahimi et al. (2018), which makes use of the target model’s gradient to find the character change that maximizes the model’s error. Since these attacks tend to disrupt the sentence’s semantics, Ribeiro et al. (2018) and Michel et al. (2019) propose searching for adversaries that preserve semantic content. Alzantot et al. (2018) and Jin et al. (2019) explore the use of synonym substitution to create adversarial examples, using word embeddings to find the n nearest words. Eger et al. (2019) take a different approach, arguing that adding visual noise to characters leaves their semantic content undisturbed. Iyyer et al. (2018) propose to create paraphrase adversaries by conditioning their generation on a syntactic template, while Zhang et al. (2019b) swap key entities in the sentences. Zhang et al. (2019a) provide a comprehensive survey of this topic. Adversarial training. In order to ensure our NLP systems are not left"
2020.acl-main.263,N19-3002,0,0.0256124,"bustness, while preserving performance on clean examples (Table 5). To the best of our knowledge, we are the first to investigate the robustness of NLP models to inflectional perturbations and its ethical implications. 2 Related Work Fairness in NLP. It is crucial that NLP systems do not amplify and entrench social biases (Hovy and Spruit, 2016). Recent research on fairness has primarily focused on racial and gender biases within distributed word representations (Bolukbasi et al., 2016), coreference resolution (Rudinger et al., 2018), sentence encoders (May et al., 2019), and language models (Bordia and Bowman, 2019). However, we posit that there exists a significant potential for linguistic bias that has yet to be investigated, which is the motivation for our work. Adversarial attacks in NLP. First discovered in computer vision by Szegedy et al. (2014), adversarial examples are data points crafted with the intent of causing a model to output a wrong prediction. In NLP, this could take place at the character, morphological, lexical, syntactic, or semantic level. Jia and Liang (2017) showed that question answering models could be misled into choosing a distractor sentence in the passage that was created by"
2020.acl-main.263,P16-2096,0,0.0396823,"eness on multiple machine comprehension and translation models, including BERT and Transformer (Tables 1 & 2). • Show that adversarially fine-tuning the model on an adversarial training set generated via weighted random sampling is sufficient for it to acquire significant robustness, while preserving performance on clean examples (Table 5). To the best of our knowledge, we are the first to investigate the robustness of NLP models to inflectional perturbations and its ethical implications. 2 Related Work Fairness in NLP. It is crucial that NLP systems do not amplify and entrench social biases (Hovy and Spruit, 2016). Recent research on fairness has primarily focused on racial and gender biases within distributed word representations (Bolukbasi et al., 2016), coreference resolution (Rudinger et al., 2018), sentence encoders (May et al., 2019), and language models (Bordia and Bowman, 2019). However, we posit that there exists a significant potential for linguistic bias that has yet to be investigated, which is the motivation for our work. Adversarial attacks in NLP. First discovered in computer vision by Szegedy et al. (2014), adversarial examples are data points crafted with the intent of causing a model"
2020.acl-main.263,N18-1170,0,0.0348915,"l. (2018), which makes use of the target model’s gradient to find the character change that maximizes the model’s error. Since these attacks tend to disrupt the sentence’s semantics, Ribeiro et al. (2018) and Michel et al. (2019) propose searching for adversaries that preserve semantic content. Alzantot et al. (2018) and Jin et al. (2019) explore the use of synonym substitution to create adversarial examples, using word embeddings to find the n nearest words. Eger et al. (2019) take a different approach, arguing that adding visual noise to characters leaves their semantic content undisturbed. Iyyer et al. (2018) propose to create paraphrase adversaries by conditioning their generation on a syntactic template, while Zhang et al. (2019b) swap key entities in the sentences. Zhang et al. (2019a) provide a comprehensive survey of this topic. Adversarial training. In order to ensure our NLP systems are not left vulnerable to powerful attacks, most existing work make use of adversarial training to improve the model’s robustness (Goodfellow et al., 2015). This involves augmenting the training data either by adding the adversaries to or replacing the clean examples in the training set. Summary. Existing work"
2020.acl-main.263,D17-1215,0,0.0316738,"t al., 2016), coreference resolution (Rudinger et al., 2018), sentence encoders (May et al., 2019), and language models (Bordia and Bowman, 2019). However, we posit that there exists a significant potential for linguistic bias that has yet to be investigated, which is the motivation for our work. Adversarial attacks in NLP. First discovered in computer vision by Szegedy et al. (2014), adversarial examples are data points crafted with the intent of causing a model to output a wrong prediction. In NLP, this could take place at the character, morphological, lexical, syntactic, or semantic level. Jia and Liang (2017) showed that question answering models could be misled into choosing a distractor sentence in the passage that was created by replacing key entities in the correct answer sentence. Belinkov and Bisk (2018) followed by demonstrating the brittleness of neural machine translation systems against character-level perturbations like randomly swapping/replacing characters. However, these attacks are not optimized on the target models, unlike Ebrahimi et al. (2018), which makes use of the target model’s gradient to find the character change that maximizes the model’s error. Since these attacks tend to"
2020.acl-main.263,N19-1063,0,0.0496558,"g performance on clean data.1 1 Introduction In recent years, Natural Language Processing (NLP) systems have gotten increasingly better at learning complex patterns in language by pretraining large language models like BERT, GPT-2, and CTRL (Devlin et al., 2019; Radford et al., 2019; Keskar et al., 2019), and fine-tuning them on taskspecific data to achieve state of the art results has become a norm. However, deep learning models are only as good as the data they are trained on. Existing work on societal bias in NLP primarily focuses on attributes like race and gender (Bolukbasi et al., 2016; May et al., 2019). In contrast, we investigate a uniquely NLP attribute that has been largely ignored: linguistic background. Current NLP models seem to be trained with the implicit assumption that everyone speaks fluent (often U.S.) Standard English, even though twothirds (&gt;700 million) of the English speakers in the world speak it as a second language (L2) (Eberhard et al., 2019). Even among native speakers, a significant number speak a dialect like African American Vernacular English (AAVE) rather than Standard English (Crystal, 2003). In addition, these 1 Code and adversarially fine-tuned models available"
2020.acl-main.263,N19-1314,0,0.136714,"hoosing a distractor sentence in the passage that was created by replacing key entities in the correct answer sentence. Belinkov and Bisk (2018) followed by demonstrating the brittleness of neural machine translation systems against character-level perturbations like randomly swapping/replacing characters. However, these attacks are not optimized on the target models, unlike Ebrahimi et al. (2018), which makes use of the target model’s gradient to find the character change that maximizes the model’s error. Since these attacks tend to disrupt the sentence’s semantics, Ribeiro et al. (2018) and Michel et al. (2019) propose searching for adversaries that preserve semantic content. Alzantot et al. (2018) and Jin et al. (2019) explore the use of synonym substitution to create adversarial examples, using word embeddings to find the n nearest words. Eger et al. (2019) take a different approach, arguing that adding visual noise to characters leaves their semantic content undisturbed. Iyyer et al. (2018) propose to create paraphrase adversaries by conditioning their generation on a syntactic template, while Zhang et al. (2019b) swap key entities in the sentences. Zhang et al. (2019a) provide a comprehensive su"
2020.acl-main.263,N19-4009,0,0.0397215,"Missing"
2020.acl-main.263,W18-6301,0,0.0191745,"ed solution. To solve this problem, we propose M ORPHEUS (Algorithm 1), an approach that greedily searches for the inflectional form of each noun, verb, or adjective in x that maximally increases f ’s loss (Eq. 1). For each token in x, M ORPHEUS calls M AX I NFLECTED to find the inflected form that caused the greatest increase in f ’s loss.3 Table 1 presents some adversarial examples obtained by running M ORPHEUS on state-of-theart machine reading comprehension and translation models: namely, BERT (Devlin et al., 2019), SpanBERT (Joshi et al., 2019), and Transformer-big (Vaswani et al., 2017; Ott et al., 2018). 3 A task-specific evaluation metric may be used instead of the loss in situations where it is unavailable. However, as we discuss later, the choice of metric is important for optimal performance and should be chosen wisely. Require: Original instance x, Label y, Model f Ensure: Adversarial example x0 T ← TOKENIZE(x) for all i = 1, . . . , |T |do if POS(Ti ) ∈ {NOUN, VERB, ADJ} then I ← G ET I NFLECTIONS(Ti ) Ti ← M AX I NFLECTED(I, T, y, f ) end if end for x0 ← DETOKENIZE(T ) return x0 There are two possible approaches to implementing M AX I NFLECTED: one is to modify each token independentl"
2020.acl-main.263,P02-1040,0,0.108717,"Missing"
2020.acl-main.263,N18-1202,0,0.0583613,"Missing"
2020.acl-main.263,W15-3049,0,0.0418897,"Missing"
2020.acl-main.263,W18-6319,0,0.0326363,"Missing"
2020.acl-main.263,P18-2124,0,0.0425504,"Missing"
2020.acl-main.263,D16-1264,0,0.016695,"cts each eligible word in each original example. Measures. In addition to the raw scores, we also report the relative decrease for easier comparison across models since they perform differently on the clean dataset. Relative decrease (dr ) is calculated using the following formula: dr = 4.1 scoreoriginal − scoreadversarial scoreoriginal (2) Extractive Question Answering Given a question and a passage containing spans corresponding to the correct answer, the model is expected to predict the span corresponding to the answer. Performance for this task is computed using exact match or average F1 (Rajpurkar et al., 2016). We evaluate the effectiveness of our attack using average F1 , which is more forgiving (for the target model). From our experiments, the exact match score is usually between 3-9 points lower than the average F1 score. SQuAD 1.1 and 2.0. The Stanford Question Answering Dataset (SQuAD) comprises over 100,000 question–answer pairs written by crowdworkers 5 https://github.com/bjascob/LemmInflect 2923 https://github.com/alvations/sacremoses Dataset Model Clean Random M ORPHEUS SQuAD 2.0 Answerable Questions (F1 ) GloVe-BiDAF ELMo-BiDAF BERTSQuAD 1.1 SpanBERTSQuAD 1.1 BERTSQuAD 2 SpanBERTSQuAD 2 7"
2020.acl-main.263,P18-1079,0,0.219105,"chances of propagating linguistic discrimination. Hence, in this paper, we: 2 Inflections convey tense, quantity, etc. See Appendix A for dialectal examples. 2920 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 2920–2935 c July 5 - 10, 2020. 2020 Association for Computational Linguistics • Propose M ORPHEUS, a method for generating plausible and semantically similar adversaries by perturbing the inflections in the clean examples (Figure 1). In contrast to recent work on adversarial examples in NLP (Belinkov and Bisk, 2018; Ebrahimi et al., 2018; Ribeiro et al., 2018), we exploit morphology to craft our adversaries. • Demonstrate its effectiveness on multiple machine comprehension and translation models, including BERT and Transformer (Tables 1 & 2). • Show that adversarially fine-tuning the model on an adversarial training set generated via weighted random sampling is sufficient for it to acquire significant robustness, while preserving performance on clean examples (Table 5). To the best of our knowledge, we are the first to investigate the robustness of NLP models to inflectional perturbations and its ethical implications. 2 Related Work Fairness in NLP"
2020.acl-main.263,N18-2002,0,0.0526665,"Missing"
2020.acl-main.263,P16-1162,0,0.110478,"Missing"
2020.acl-main.263,N19-1131,0,0.0227152,"Missing"
2020.acl-main.263,W17-1606,0,0.0225226,"izes the target model’s loss. To maximize semantic preservation, M ORPHEUS only considers inflections belonging to the same universal part of speech as the original word. World Englishes exhibit variation at multiple levels of linguistic analysis (Kachru et al., 2009). Therefore, putting these models directly into production without addressing this inherent bias puts them at risk of committing linguistic discrimination by performing poorly for many speech communities (e.g., AAVE and L2 speakers). This could take the form of either failing to understand these speakers (Rickford and King, 2016; Tatman, 2017), or misinterpreting them. For example, the recent mistranslation of a minority speaker’s social media post resulted in his wrongful arrest (Hern, 2017). Since L2 (and many L1 dialect) speakers often exhibit variability in their production of inflectional morphology2 (Lardiere, 1998; Pr´evost and White, 2000; Haznedar, 2002; White, 2003; Seymour, 2004), we argue that NLP models should be robust to inflectional perturbations in order to minimize their chances of propagating linguistic discrimination. Hence, in this paper, we: 2 Inflections convey tense, quantity, etc. See Appendix A for dialect"
2020.acl-main.263,P18-2006,0,\N,Missing
2020.acl-main.263,N19-1165,0,\N,Missing
2020.acl-main.263,N19-1423,0,\N,Missing
2020.acl-main.263,W19-4406,0,\N,Missing
2020.acl-main.408,2020.acl-main.386,0,0.013867,"make a prediction. We refer to rationales that correspond to the inputs most relied upon to come to a disposition as faithful. Most automatic evaluations of faithfulness measure the impact of perturbing or erasing words or tokens identified as important on model output (Arras et al., 2017; Montavon et al., 2017; Serrano and Smith, 2019; Samek et al., 2016; Jain and Wallace, 2019). We build upon these methods in Section 4. Finally, we note that a recent article urges the community to evaluate faithfulness on a continuous scale of acceptability, rather than viewing this as a binary proposition (Jacovi and Goldberg, 2020). 3 Datasets in ERASER For all datasets in ERASER we distribute both reference labels and rationales marked by humans as supporting these in a standardized format. We 4445 delineate train, validation, and test splits for all corpora (see Appendix A for processing details). We ensure that these splits comprise disjoint sets of source documents to avoid contamination.3 We have made the decision to distribute the test sets publicly,4 in part because we do not view the ‘correct’ metrics to use as settled. We plan to acquire additional human annotations on held-out portions of some of the included"
2020.acl-main.408,N19-1357,1,0.918768,"uring training. However, such direct supervision will not always be available, motivating work on methods that can explain (or “rationalize”) model predictions using only instance-level supervision. In the context of modern neural models for text classification, one might use variants of attention (Bahdanau et al., 2015) to extract rationales. Attention mechanisms learn to assign soft weights to (usually contextualized) token representations, and so one can extract highly weighted tokens as rationales. However, attention weights do not in general provide faithful explanations for predictions (Jain and Wallace, 2019; Serrano and Smith, 2019; Wiegreffe and Pinter, 2019; Zhong et al., 2019; Pruthi et al., 2020; Brunner et al., 2020; Moradi et al., 2019; Vashishth et al., 2019). This likely owes to encoders entangling inputs, complicating the interpretation of attention weights on inputs over contextualized representations of the same.2 By contrast, hard attention mechanisms discretely extract snippets from the input to pass to the classifier, by construction providing faithful explanations. Recent work has proposed hard attention mechanisms as a means of providing explanations. Lei et al. (2016) proposed i"
2020.acl-main.408,2020.acl-main.409,1,0.78141,"g the interpretation of attention weights on inputs over contextualized representations of the same.2 By contrast, hard attention mechanisms discretely extract snippets from the input to pass to the classifier, by construction providing faithful explanations. Recent work has proposed hard attention mechanisms as a means of providing explanations. Lei et al. (2016) proposed instantiating two models with their own parameters; one to extract rationales, and one that consumes these to make a prediction. They trained these models jointly via REINFORCE (Williams, 1992) style optimization. Recently, Jain et al. (2020) proposed a variant of this two-model setup that uses heuristic feature scores to derive pseudo-labels on tokens comprising rationales; one model can then be used to perform hard extraction in this way, while a second (independent) model can make predictions on the basis of these. Elsewhere, Chang et al. (2019) introduced the notion of classwise rationales that explains support for different output classes using a game theoretic framework. Finally, other recent work has proposed using a differentiable binary mask over inputs, which also avoids recourse to REINFORCE (Bastings et al., 2019). Pos"
2020.acl-main.408,N18-1023,0,0.0239798,"for BoolQ, wherein source documents in the original train and validation set were not disjoint and we preserve this structure in our dataset. Questions, of course, are disjoint. 4 Consequently, for datasets that have been part of previous benchmarks with other aims (namely, GLUE/superGLUE) but which we have re-purposed for work on rationales in ERASER, e.g., BoolQ (Clark et al., 2019), we have carved out for release test sets from the original validation sets. 5 Annotation details are in Appendix B. texts. We take a subset of this dataset, including only supported and refuted claims. MultiRC (Khashabi et al., 2018). A reading comprehension dataset composed of questions with multiple correct answers that by construction depend on information from multiple sentences. Here each rationale is associated with a question, while answers are independent of one another. We convert each rationale/question/answer triplet into an instance within our dataset. Each answer candidate then has a label of True or False. Commonsense Explanations (CoS-E) (Rajani et al., 2019). This corpus comprises multiplechoice questions and answers from (Talmor et al., 2019) along with supporting rationales. The rationales in this case c"
2020.acl-main.408,D19-1002,0,0.0200855,"will not always be available, motivating work on methods that can explain (or “rationalize”) model predictions using only instance-level supervision. In the context of modern neural models for text classification, one might use variants of attention (Bahdanau et al., 2015) to extract rationales. Attention mechanisms learn to assign soft weights to (usually contextualized) token representations, and so one can extract highly weighted tokens as rationales. However, attention weights do not in general provide faithful explanations for predictions (Jain and Wallace, 2019; Serrano and Smith, 2019; Wiegreffe and Pinter, 2019; Zhong et al., 2019; Pruthi et al., 2020; Brunner et al., 2020; Moradi et al., 2019; Vashishth et al., 2019). This likely owes to encoders entangling inputs, complicating the interpretation of attention weights on inputs over contextualized representations of the same.2 By contrast, hard attention mechanisms discretely extract snippets from the input to pass to the classifier, by construction providing faithful explanations. Recent work has proposed hard attention mechanisms as a means of providing explanations. Lei et al. (2016) proposed instantiating two models with their own parameters; on"
2020.acl-main.408,D19-1420,0,0.0131407,"in the test set in Table 3. 4.2 Measuring faithfulness As discussed above, a model may provide rationales that are plausible (agreeable to humans) but that it did not rely on for its output. In many settings one may want rationales that actually explain model predictions, i.e., rationales extracted for an instance in this case ought to have meaningfully influenced its prediction for the same. We call these faithful rationales. How best to measure rationale faithfulness is an open question. In this first version of ERASER we propose simple metrics motivated by prior work (Zaidan et al., 2007; Yu et al., 2019). In particular, following Yu et al. (2019) we define metrics intended to measure the comprehensiveness (were all features needed to make a prediction selected?) and sufficiency (do the extracted rationales contain enough signal to come to a disposition?) of rationales, respectively. Comprehensiveness. To calculate rationale comprehensiveness we create contrast examples (Zaidan et al., 2007): We construct a contrast example for xi , x ˜i , which is xi with the predicted rationales ri removed. Assuming a classification setting, let m(xi )j be the original prediction provided by a model m for th"
2020.acl-main.408,N07-1033,0,0.340523,"cy in terms of model performance realized given a fixed amount of annotator effort (Zaidan and Eisner, 2008). In particular, recent work by McDonnell et al. (2017, 2016) has observed that at least for some tasks, asking annotators to provide rationales justifying their categorizations does not impose much additional effort. Combining rationale annotation with active learning (Settles, 2012) is another promising direction (Wallace et al., 2010; Sharma et al., 2015). Learning from rationales. Work on learning from rationales marked by annotators for text classification dates back over a decade (Zaidan et al., 2007). Earlier efforts proposed extending standard discriminative models like Support Vector Machines (SVMs) with regularization terms that penalized parameter estimates which disagreed with provided rationales (Zaidan et al., 2007; Small et al., 2011). Other efforts have attempted to specify generative models of rationales (Zaidan and Eisner, 2008). More recent work has aimed to exploit rationales in training neural text classifiers. Zhang et al. (2016) proposed a rationale-augmented Convolutional Neural Network (CNN) for text classification, explicitly trained to identify sentences supporting cat"
2020.acl-main.408,D08-1004,0,0.182552,"aviors (Feng et al., 2018). Gradients of course assume model differentiability. Other methods do not require any model properties. Examples include LIME (Ribeiro et al., 2016) and Alvarez-Melis and Jaakkola (2017); these methods approximate model behavior locally by having it repeatedly make predictions over perturbed inputs and fitting a simple, explainable model over the outputs. Acquiring rationales. Aside from interpretability considerations, collecting rationales from annotators may afford greater efficiency in terms of model performance realized given a fixed amount of annotator effort (Zaidan and Eisner, 2008). In particular, recent work by McDonnell et al. (2017, 2016) has observed that at least for some tasks, asking annotators to provide rationales justifying their categorizations does not impose much additional effort. Combining rationale annotation with active learning (Settles, 2012) is another promising direction (Wallace et al., 2010; Sharma et al., 2015). Learning from rationales. Work on learning from rationales marked by annotators for text classification dates back over a decade (Zaidan et al., 2007). Earlier efforts proposed extending standard discriminative models like Support Vector"
2020.acl-main.408,D16-1076,1,0.90058,"Missing"
2020.acl-main.706,D19-1052,0,0.0451164,"Missing"
2020.acl-main.706,2020.acl-main.708,0,0.20003,"e bases (ODonnell et al., 2000; Trisedya et al., 2018; Zhu et al., 2019; Yu et al., 2019) and source code (Iyer et al., 2016), and dialog response generation from slot-value pairs (Wen et al., 2015). Recently, neural encoder-decoder models (Sutskever et al., 2014; Cho et al., 2014) based on attention (Bahdanau et al., 2015; Luong et al., 2015) and copy mechanisms (Gu et al., 2016; Gulcehre et al., 2016) have shown promising results on table-to-text tasks (Wiseman et al., 2017; Gehrmann et al., 2018; Puduppully et al., 2019a,b; Iso et al., 2019; Castro Ferreira et al., 2019; Zhao et al., 2020; Chen et al., 2020a). While traditional methods use different modules for each generation stage in a pipeline (Reiter and Dale, 2000), neural table-to-text models are trained on large-scale datasets, relying on representation learning for generating coherent and grammatical texts. Puduppully et al. (2019a) propose a neural network approach that first selects data records to be mentioned and then generates a summary from the selected data, in an end-to-end fashion. Chen et al. (2020b) use pre-trained language models to generate descriptions for tabular data in a few shot setting. 7 Conclusions and Future Directi"
2020.acl-main.706,2020.acl-main.18,0,0.0282211,"e bases (ODonnell et al., 2000; Trisedya et al., 2018; Zhu et al., 2019; Yu et al., 2019) and source code (Iyer et al., 2016), and dialog response generation from slot-value pairs (Wen et al., 2015). Recently, neural encoder-decoder models (Sutskever et al., 2014; Cho et al., 2014) based on attention (Bahdanau et al., 2015; Luong et al., 2015) and copy mechanisms (Gu et al., 2016; Gulcehre et al., 2016) have shown promising results on table-to-text tasks (Wiseman et al., 2017; Gehrmann et al., 2018; Puduppully et al., 2019a,b; Iso et al., 2019; Castro Ferreira et al., 2019; Zhao et al., 2020; Chen et al., 2020a). While traditional methods use different modules for each generation stage in a pipeline (Reiter and Dale, 2000), neural table-to-text models are trained on large-scale datasets, relying on representation learning for generating coherent and grammatical texts. Puduppully et al. (2019a) propose a neural network approach that first selects data records to be mentioned and then generates a summary from the selected data, in an end-to-end fashion. Chen et al. (2020b) use pre-trained language models to generate descriptions for tabular data in a few shot setting. 7 Conclusions and Future Directi"
2020.acl-main.706,P17-1025,0,0.0299591,"for question answering such as NLVR (Suhr et al., 2017), CLEVR (Johnson et al., 2017), and VQA (Antol et al., 2015). In a parallel work, Yi et al. (2020) introduced the CLEVRER dataset for reasoning about collision events from videos with different types of questions. In contrast, we develop the ability to reason and explain the behavior of dynamic physical systems by generating natural language. Natural language explanations and Commonsense reasoning. Several recent papers propose 7913 to use natural language for explanation and commonsense reasoning (Lei et al., 2016; Camburu et al., 2018; Forbes and Choi, 2017; Chai et al., 2018; Forbes et al., 2019; Rajani et al., 2019; DeYoung et al., 2020). Lei et al. (2016), for example, generate textual rationales for sentiment analysis by highlighting phrases in the input. Forbes and Choi (2017) learn the physical knowledge of actions and objects from natural language. Camburu et al. (2018) propose e-SNLI by generating explanations for the natural language inference problem at a cost of performance. Rajani et al. (2019) propose to use LMs to generate explanations that can be used during training and inference in a classifier and significantly improve Commonse"
2020.acl-main.706,W18-6505,0,0.0214463,"Lebret et al., 2016; Sha et al., 2018; Liu et al., 2018; Perez-Beltrachini and Lapata, 2018), descriptions of knowledge bases (ODonnell et al., 2000; Trisedya et al., 2018; Zhu et al., 2019; Yu et al., 2019) and source code (Iyer et al., 2016), and dialog response generation from slot-value pairs (Wen et al., 2015). Recently, neural encoder-decoder models (Sutskever et al., 2014; Cho et al., 2014) based on attention (Bahdanau et al., 2015; Luong et al., 2015) and copy mechanisms (Gu et al., 2016; Gulcehre et al., 2016) have shown promising results on table-to-text tasks (Wiseman et al., 2017; Gehrmann et al., 2018; Puduppully et al., 2019a,b; Iso et al., 2019; Castro Ferreira et al., 2019; Zhao et al., 2020; Chen et al., 2020a). While traditional methods use different modules for each generation stage in a pipeline (Reiter and Dale, 2000), neural table-to-text models are trained on large-scale datasets, relying on representation learning for generating coherent and grammatical texts. Puduppully et al. (2019a) propose a neural network approach that first selects data records to be mentioned and then generates a summary from the selected data, in an end-to-end fashion. Chen et al. (2020b) use pre-trained"
2020.acl-main.706,P16-1154,0,0.0267129,"sts (Liang et al., 2009; Konstas and Lapata, 2012; Mei et al., 2016), biographical texts from Wikipedia infoboxes (Lebret et al., 2016; Sha et al., 2018; Liu et al., 2018; Perez-Beltrachini and Lapata, 2018), descriptions of knowledge bases (ODonnell et al., 2000; Trisedya et al., 2018; Zhu et al., 2019; Yu et al., 2019) and source code (Iyer et al., 2016), and dialog response generation from slot-value pairs (Wen et al., 2015). Recently, neural encoder-decoder models (Sutskever et al., 2014; Cho et al., 2014) based on attention (Bahdanau et al., 2015; Luong et al., 2015) and copy mechanisms (Gu et al., 2016; Gulcehre et al., 2016) have shown promising results on table-to-text tasks (Wiseman et al., 2017; Gehrmann et al., 2018; Puduppully et al., 2019a,b; Iso et al., 2019; Castro Ferreira et al., 2019; Zhao et al., 2020; Chen et al., 2020a). While traditional methods use different modules for each generation stage in a pipeline (Reiter and Dale, 2000), neural table-to-text models are trained on large-scale datasets, relying on representation learning for generating coherent and grammatical texts. Puduppully et al. (2019a) propose a neural network approach that first selects data records to be men"
2020.acl-main.706,P16-1014,0,0.0262209,", 2009; Konstas and Lapata, 2012; Mei et al., 2016), biographical texts from Wikipedia infoboxes (Lebret et al., 2016; Sha et al., 2018; Liu et al., 2018; Perez-Beltrachini and Lapata, 2018), descriptions of knowledge bases (ODonnell et al., 2000; Trisedya et al., 2018; Zhu et al., 2019; Yu et al., 2019) and source code (Iyer et al., 2016), and dialog response generation from slot-value pairs (Wen et al., 2015). Recently, neural encoder-decoder models (Sutskever et al., 2014; Cho et al., 2014) based on attention (Bahdanau et al., 2015; Luong et al., 2015) and copy mechanisms (Gu et al., 2016; Gulcehre et al., 2016) have shown promising results on table-to-text tasks (Wiseman et al., 2017; Gehrmann et al., 2018; Puduppully et al., 2019a,b; Iso et al., 2019; Castro Ferreira et al., 2019; Zhao et al., 2020; Chen et al., 2020a). While traditional methods use different modules for each generation stage in a pipeline (Reiter and Dale, 2000), neural table-to-text models are trained on large-scale datasets, relying on representation learning for generating coherent and grammatical texts. Puduppully et al. (2019a) propose a neural network approach that first selects data records to be mentioned and then generate"
2020.acl-main.706,P16-1195,0,0.0197135,"generation. Table-to-text generation aims to produce natural language output from structured input. Applications include generating sports commentaries from game records (Tanaka-Ishii et al., 1998; Chen and Mooney, 2008; Taniguchi et al., 2019), weather forecasts (Liang et al., 2009; Konstas and Lapata, 2012; Mei et al., 2016), biographical texts from Wikipedia infoboxes (Lebret et al., 2016; Sha et al., 2018; Liu et al., 2018; Perez-Beltrachini and Lapata, 2018), descriptions of knowledge bases (ODonnell et al., 2000; Trisedya et al., 2018; Zhu et al., 2019; Yu et al., 2019) and source code (Iyer et al., 2016), and dialog response generation from slot-value pairs (Wen et al., 2015). Recently, neural encoder-decoder models (Sutskever et al., 2014; Cho et al., 2014) based on attention (Bahdanau et al., 2015; Luong et al., 2015) and copy mechanisms (Gu et al., 2016; Gulcehre et al., 2016) have shown promising results on table-to-text tasks (Wiseman et al., 2017; Gehrmann et al., 2018; Puduppully et al., 2019a,b; Iso et al., 2019; Castro Ferreira et al., 2019; Zhao et al., 2020; Chen et al., 2020a). While traditional methods use different modules for each generation stage in a pipeline (Reiter and Dale"
2020.acl-main.706,P04-1050,0,0.0480877,"Missing"
2020.acl-main.706,N12-1093,0,0.0316404,"e to use a question answering task to test the model’s physical commonsense and reasoning ability. In contrast to the previous work, we focus on identifying pivotal physical events and then generating natural language explanations for them. We find that this two-step approach works more effectively. Table-to-text generation. Table-to-text generation aims to produce natural language output from structured input. Applications include generating sports commentaries from game records (Tanaka-Ishii et al., 1998; Chen and Mooney, 2008; Taniguchi et al., 2019), weather forecasts (Liang et al., 2009; Konstas and Lapata, 2012; Mei et al., 2016), biographical texts from Wikipedia infoboxes (Lebret et al., 2016; Sha et al., 2018; Liu et al., 2018; Perez-Beltrachini and Lapata, 2018), descriptions of knowledge bases (ODonnell et al., 2000; Trisedya et al., 2018; Zhu et al., 2019; Yu et al., 2019) and source code (Iyer et al., 2016), and dialog response generation from slot-value pairs (Wen et al., 2015). Recently, neural encoder-decoder models (Sutskever et al., 2014; Cho et al., 2014) based on attention (Bahdanau et al., 2015; Luong et al., 2015) and copy mechanisms (Gu et al., 2016; Gulcehre et al., 2016) have show"
2020.acl-main.706,D16-1128,0,0.0238773,"ability. In contrast to the previous work, we focus on identifying pivotal physical events and then generating natural language explanations for them. We find that this two-step approach works more effectively. Table-to-text generation. Table-to-text generation aims to produce natural language output from structured input. Applications include generating sports commentaries from game records (Tanaka-Ishii et al., 1998; Chen and Mooney, 2008; Taniguchi et al., 2019), weather forecasts (Liang et al., 2009; Konstas and Lapata, 2012; Mei et al., 2016), biographical texts from Wikipedia infoboxes (Lebret et al., 2016; Sha et al., 2018; Liu et al., 2018; Perez-Beltrachini and Lapata, 2018), descriptions of knowledge bases (ODonnell et al., 2000; Trisedya et al., 2018; Zhu et al., 2019; Yu et al., 2019) and source code (Iyer et al., 2016), and dialog response generation from slot-value pairs (Wen et al., 2015). Recently, neural encoder-decoder models (Sutskever et al., 2014; Cho et al., 2014) based on attention (Bahdanau et al., 2015; Luong et al., 2015) and copy mechanisms (Gu et al., 2016; Gulcehre et al., 2016) have shown promising results on table-to-text tasks (Wiseman et al., 2017; Gehrmann et al., 20"
2020.acl-main.706,D16-1011,0,0.0343869,"which are combined with natural language for question answering such as NLVR (Suhr et al., 2017), CLEVR (Johnson et al., 2017), and VQA (Antol et al., 2015). In a parallel work, Yi et al. (2020) introduced the CLEVRER dataset for reasoning about collision events from videos with different types of questions. In contrast, we develop the ability to reason and explain the behavior of dynamic physical systems by generating natural language. Natural language explanations and Commonsense reasoning. Several recent papers propose 7913 to use natural language for explanation and commonsense reasoning (Lei et al., 2016; Camburu et al., 2018; Forbes and Choi, 2017; Chai et al., 2018; Forbes et al., 2019; Rajani et al., 2019; DeYoung et al., 2020). Lei et al. (2016), for example, generate textual rationales for sentiment analysis by highlighting phrases in the input. Forbes and Choi (2017) learn the physical knowledge of actions and objects from natural language. Camburu et al. (2018) propose e-SNLI by generating explanations for the natural language inference problem at a cost of performance. Rajani et al. (2019) propose to use LMs to generate explanations that can be used during training and inference in a"
2020.acl-main.706,P19-1195,0,0.115119,"f variable diameters) in the simulator to achieve a given goal. Figure 1 shows an example of a task with a specified goal. The input to ESPRIT is a sequence of frames from a physics simulation and the output is a natural language narrative that reflects the locations of the objects in the initial scene and a description of the sequence of physical events that would lead to the desired goal state, as shown in Figure 2. The first phase of the framework uses a neural network classifier to identify salient frames from the simulation. For the second phase we experimented with table-to-text models (Puduppully et al., 2019a,b) as well as pre-trained language models (Radford et al., 2018). We evaluated our framework for natural language generated reasoning using several automated and human evaluations with a focus on the understanding of qualitative physics and the ordering of a natural sequence of physical events. We found that our model achieves very high performance for phase one (identifying frames with salient physical events) and that, for phase two, the table-to-text models outperform pre-trained language models on qualitative physics reasoning. 2 2.1 Dataset PHYRE Benchmark We build our dataset by extend"
2020.acl-main.706,P09-1011,0,0.0603867,"et al. (2020) propose to use a question answering task to test the model’s physical commonsense and reasoning ability. In contrast to the previous work, we focus on identifying pivotal physical events and then generating natural language explanations for them. We find that this two-step approach works more effectively. Table-to-text generation. Table-to-text generation aims to produce natural language output from structured input. Applications include generating sports commentaries from game records (Tanaka-Ishii et al., 1998; Chen and Mooney, 2008; Taniguchi et al., 2019), weather forecasts (Liang et al., 2009; Konstas and Lapata, 2012; Mei et al., 2016), biographical texts from Wikipedia infoboxes (Lebret et al., 2016; Sha et al., 2018; Liu et al., 2018; Perez-Beltrachini and Lapata, 2018), descriptions of knowledge bases (ODonnell et al., 2000; Trisedya et al., 2018; Zhu et al., 2019; Yu et al., 2019) and source code (Iyer et al., 2016), and dialog response generation from slot-value pairs (Wen et al., 2015). Recently, neural encoder-decoder models (Sutskever et al., 2014; Cho et al., 2014) based on attention (Bahdanau et al., 2015; Luong et al., 2015) and copy mechanisms (Gu et al., 2016; Gulceh"
2020.acl-main.706,P19-1487,1,0.734658,"(Johnson et al., 2017), and VQA (Antol et al., 2015). In a parallel work, Yi et al. (2020) introduced the CLEVRER dataset for reasoning about collision events from videos with different types of questions. In contrast, we develop the ability to reason and explain the behavior of dynamic physical systems by generating natural language. Natural language explanations and Commonsense reasoning. Several recent papers propose 7913 to use natural language for explanation and commonsense reasoning (Lei et al., 2016; Camburu et al., 2018; Forbes and Choi, 2017; Chai et al., 2018; Forbes et al., 2019; Rajani et al., 2019; DeYoung et al., 2020). Lei et al. (2016), for example, generate textual rationales for sentiment analysis by highlighting phrases in the input. Forbes and Choi (2017) learn the physical knowledge of actions and objects from natural language. Camburu et al. (2018) propose e-SNLI by generating explanations for the natural language inference problem at a cost of performance. Rajani et al. (2019) propose to use LMs to generate explanations that can be used during training and inference in a classifier and significantly improve CommonsenseQA performance. Bisk et al. (2020) propose to use a questi"
2020.acl-main.706,D15-1166,0,0.0153733,"; Taniguchi et al., 2019), weather forecasts (Liang et al., 2009; Konstas and Lapata, 2012; Mei et al., 2016), biographical texts from Wikipedia infoboxes (Lebret et al., 2016; Sha et al., 2018; Liu et al., 2018; Perez-Beltrachini and Lapata, 2018), descriptions of knowledge bases (ODonnell et al., 2000; Trisedya et al., 2018; Zhu et al., 2019; Yu et al., 2019) and source code (Iyer et al., 2016), and dialog response generation from slot-value pairs (Wen et al., 2015). Recently, neural encoder-decoder models (Sutskever et al., 2014; Cho et al., 2014) based on attention (Bahdanau et al., 2015; Luong et al., 2015) and copy mechanisms (Gu et al., 2016; Gulcehre et al., 2016) have shown promising results on table-to-text tasks (Wiseman et al., 2017; Gehrmann et al., 2018; Puduppully et al., 2019a,b; Iso et al., 2019; Castro Ferreira et al., 2019; Zhao et al., 2020; Chen et al., 2020a). While traditional methods use different modules for each generation stage in a pipeline (Reiter and Dale, 2000), neural table-to-text models are trained on large-scale datasets, relying on representation learning for generating coherent and grammatical texts. Puduppully et al. (2019a) propose a neural network approach that"
2020.acl-main.706,N16-1086,0,0.015485,"ing task to test the model’s physical commonsense and reasoning ability. In contrast to the previous work, we focus on identifying pivotal physical events and then generating natural language explanations for them. We find that this two-step approach works more effectively. Table-to-text generation. Table-to-text generation aims to produce natural language output from structured input. Applications include generating sports commentaries from game records (Tanaka-Ishii et al., 1998; Chen and Mooney, 2008; Taniguchi et al., 2019), weather forecasts (Liang et al., 2009; Konstas and Lapata, 2012; Mei et al., 2016), biographical texts from Wikipedia infoboxes (Lebret et al., 2016; Sha et al., 2018; Liu et al., 2018; Perez-Beltrachini and Lapata, 2018), descriptions of knowledge bases (ODonnell et al., 2000; Trisedya et al., 2018; Zhu et al., 2019; Yu et al., 2019) and source code (Iyer et al., 2016), and dialog response generation from slot-value pairs (Wen et al., 2015). Recently, neural encoder-decoder models (Sutskever et al., 2014; Cho et al., 2014) based on attention (Bahdanau et al., 2015; Luong et al., 2015) and copy mechanisms (Gu et al., 2016; Gulcehre et al., 2016) have shown promising results"
2020.acl-main.706,W00-1418,0,0.219091,"explanations for them. We find that this two-step approach works more effectively. Table-to-text generation. Table-to-text generation aims to produce natural language output from structured input. Applications include generating sports commentaries from game records (Tanaka-Ishii et al., 1998; Chen and Mooney, 2008; Taniguchi et al., 2019), weather forecasts (Liang et al., 2009; Konstas and Lapata, 2012; Mei et al., 2016), biographical texts from Wikipedia infoboxes (Lebret et al., 2016; Sha et al., 2018; Liu et al., 2018; Perez-Beltrachini and Lapata, 2018), descriptions of knowledge bases (ODonnell et al., 2000; Trisedya et al., 2018; Zhu et al., 2019; Yu et al., 2019) and source code (Iyer et al., 2016), and dialog response generation from slot-value pairs (Wen et al., 2015). Recently, neural encoder-decoder models (Sutskever et al., 2014; Cho et al., 2014) based on attention (Bahdanau et al., 2015; Luong et al., 2015) and copy mechanisms (Gu et al., 2016; Gulcehre et al., 2016) have shown promising results on table-to-text tasks (Wiseman et al., 2017; Gehrmann et al., 2018; Puduppully et al., 2019a,b; Iso et al., 2019; Castro Ferreira et al., 2019; Zhao et al., 2020; Chen et al., 2020a). While tra"
2020.acl-main.706,N18-1137,0,0.0145506,"entifying pivotal physical events and then generating natural language explanations for them. We find that this two-step approach works more effectively. Table-to-text generation. Table-to-text generation aims to produce natural language output from structured input. Applications include generating sports commentaries from game records (Tanaka-Ishii et al., 1998; Chen and Mooney, 2008; Taniguchi et al., 2019), weather forecasts (Liang et al., 2009; Konstas and Lapata, 2012; Mei et al., 2016), biographical texts from Wikipedia infoboxes (Lebret et al., 2016; Sha et al., 2018; Liu et al., 2018; Perez-Beltrachini and Lapata, 2018), descriptions of knowledge bases (ODonnell et al., 2000; Trisedya et al., 2018; Zhu et al., 2019; Yu et al., 2019) and source code (Iyer et al., 2016), and dialog response generation from slot-value pairs (Wen et al., 2015). Recently, neural encoder-decoder models (Sutskever et al., 2014; Cho et al., 2014) based on attention (Bahdanau et al., 2015; Luong et al., 2015) and copy mechanisms (Gu et al., 2016; Gulcehre et al., 2016) have shown promising results on table-to-text tasks (Wiseman et al., 2017; Gehrmann et al., 2018; Puduppully et al., 2019a,b; Iso et al., 2019; Castro Ferreira et al.,"
2020.acl-main.706,P17-2034,0,0.0279544,"hang et al., 2016). Recent papers use neural networks over visual inputs to predict future pixels (Finn et al., 2016; Lerer et al., 2016; Mirza et al., 2016; Du and Narasimhan, 2019) or make qualitative predictions (Groth et al., 2018; Li et al., 2016, 2017; Janner et al., 2019; Wu et al., 2015; Mao et al., 2019). Furthermore, several frameworks and benchmarks have been introduced to test visual reasoning such as PHYRE (Bakhtin et al., 2019), Mujoco (Todorov et al., 2012), and Intphys (Riochet et al., 2018), some of which are combined with natural language for question answering such as NLVR (Suhr et al., 2017), CLEVR (Johnson et al., 2017), and VQA (Antol et al., 2015). In a parallel work, Yi et al. (2020) introduced the CLEVRER dataset for reasoning about collision events from videos with different types of questions. In contrast, we develop the ability to reason and explain the behavior of dynamic physical systems by generating natural language. Natural language explanations and Commonsense reasoning. Several recent papers propose 7913 to use natural language for explanation and commonsense reasoning (Lei et al., 2016; Camburu et al., 2018; Forbes and Choi, 2017; Chai et al., 2018; Forbes et al.,"
2020.acl-main.706,P98-2209,0,0.135936,"ining and inference in a classifier and significantly improve CommonsenseQA performance. Bisk et al. (2020) propose to use a question answering task to test the model’s physical commonsense and reasoning ability. In contrast to the previous work, we focus on identifying pivotal physical events and then generating natural language explanations for them. We find that this two-step approach works more effectively. Table-to-text generation. Table-to-text generation aims to produce natural language output from structured input. Applications include generating sports commentaries from game records (Tanaka-Ishii et al., 1998; Chen and Mooney, 2008; Taniguchi et al., 2019), weather forecasts (Liang et al., 2009; Konstas and Lapata, 2012; Mei et al., 2016), biographical texts from Wikipedia infoboxes (Lebret et al., 2016; Sha et al., 2018; Liu et al., 2018; Perez-Beltrachini and Lapata, 2018), descriptions of knowledge bases (ODonnell et al., 2000; Trisedya et al., 2018; Zhu et al., 2019; Yu et al., 2019) and source code (Iyer et al., 2016), and dialog response generation from slot-value pairs (Wen et al., 2015). Recently, neural encoder-decoder models (Sutskever et al., 2014; Cho et al., 2014) based on attention ("
2020.acl-main.706,P18-1151,1,0.861331,"We find that this two-step approach works more effectively. Table-to-text generation. Table-to-text generation aims to produce natural language output from structured input. Applications include generating sports commentaries from game records (Tanaka-Ishii et al., 1998; Chen and Mooney, 2008; Taniguchi et al., 2019), weather forecasts (Liang et al., 2009; Konstas and Lapata, 2012; Mei et al., 2016), biographical texts from Wikipedia infoboxes (Lebret et al., 2016; Sha et al., 2018; Liu et al., 2018; Perez-Beltrachini and Lapata, 2018), descriptions of knowledge bases (ODonnell et al., 2000; Trisedya et al., 2018; Zhu et al., 2019; Yu et al., 2019) and source code (Iyer et al., 2016), and dialog response generation from slot-value pairs (Wen et al., 2015). Recently, neural encoder-decoder models (Sutskever et al., 2014; Cho et al., 2014) based on attention (Bahdanau et al., 2015; Luong et al., 2015) and copy mechanisms (Gu et al., 2016; Gulcehre et al., 2016) have shown promising results on table-to-text tasks (Wiseman et al., 2017; Gehrmann et al., 2018; Puduppully et al., 2019a,b; Iso et al., 2019; Castro Ferreira et al., 2019; Zhao et al., 2020; Chen et al., 2020a). While traditional methods use di"
2020.acl-main.706,D15-1199,0,0.0419796,"Missing"
2020.acl-main.706,D17-1239,0,0.0932855,"alls fall. The red ball lands on the ground and the green ball lands on the red ball and rolls to the right over the black vertical bar. Generation (AVG) The red ball lands in the cubby and the green ball lands on top and a little to the right, sending the green ball right. It rolls over the short black wall of the cage and onto the floor, where it keeps rolling right towards the purple goal... The red ball falls and knocks the green ball off of its curved black platform and to the left. It rolls leftwards and continues falling until it lands on the purple floor... Generation (BiLSTM) maries (Wiseman et al., 2017). Second, the output generated by the BiLSTM model predicts the incorrect direction of motion for the green ball, an error that is occasionally seen across generation descriptions of both models. This indicates that a table-to-text paradigm for generating such solution explanations is not adequate for learning the direction of motion for the physical reasoning required for these explanations. Table 6: Example input records, gold annotation, and generated simulation description from the AVG and BiLSTM models, taken from example 00014:394. We show only a short segment of the actual input records"
2020.acl-main.706,2020.acl-main.224,0,0.0218279,"iptions of knowledge bases (ODonnell et al., 2000; Trisedya et al., 2018; Zhu et al., 2019; Yu et al., 2019) and source code (Iyer et al., 2016), and dialog response generation from slot-value pairs (Wen et al., 2015). Recently, neural encoder-decoder models (Sutskever et al., 2014; Cho et al., 2014) based on attention (Bahdanau et al., 2015; Luong et al., 2015) and copy mechanisms (Gu et al., 2016; Gulcehre et al., 2016) have shown promising results on table-to-text tasks (Wiseman et al., 2017; Gehrmann et al., 2018; Puduppully et al., 2019a,b; Iso et al., 2019; Castro Ferreira et al., 2019; Zhao et al., 2020; Chen et al., 2020a). While traditional methods use different modules for each generation stage in a pipeline (Reiter and Dale, 2000), neural table-to-text models are trained on large-scale datasets, relying on representation learning for generating coherent and grammatical texts. Puduppully et al. (2019a) propose a neural network approach that first selects data records to be mentioned and then generates a summary from the selected data, in an end-to-end fashion. Chen et al. (2020b) use pre-trained language models to generate descriptions for tabular data in a few shot setting. 7 Conclusions"
2020.acl-main.706,D14-1179,0,\N,Missing
2020.acl-main.706,P19-1202,0,\N,Missing
2020.acl-main.706,D19-1204,1,\N,Missing
2020.acl-main.706,C98-2204,0,\N,Missing
2020.acl-main.88,D18-1241,0,0.0381685,"t locate the span within the ground truth rule sentence, while in 9 cases, it finds the correct rule sentence but extracts a different span. Another challenge comes from the one-to-many problem in sequence generation. When there are multiple underspecified rule sentences, the model asks about one of these underspecified rule sentences which is different from the ground truth one. This suggests that new evaluation metrics could be proposed by taking this into consideration. 4 Related Work ShARC Conversational Machine Reading (Saeidi et al., 2018) differs from conversational question answering (Choi et al., 2018; Reddy et al., 2019) and conversational question generation (Gao et al., 2019) in that 1) machines are required to formulate follow-up questions to fill the information gap, and 2) machines have to interpret a set of complex decision rules and make a question-related conclusion, instead of extracting the answer from the text. CMR can be viewed as a special type of task-oriented dialog systems (Wen et al., 2017; Zhong et al., 2018; Wu et al., 2019) to help users achieve their goals. However, it does not rely on predefined slot and ontology information but natural language rules. On the ShARC C"
2020.acl-main.88,N19-1423,0,0.47998,"licitly tracking rules with external memories boosts both the decision accuracy and the quality of generated follow-up questions. In particular, EMT outperforms the previous best model E3 by 1.3 in macro-averaged decision accuracy and 10.8 in BLEU4 for follow-up question generation. In addition to the performance improvement, EMT yields interpretability by explicitly tracking rules, which is visualized to show the entailment-oriented reasoning process of our model. 2 As illustrated in Figure 2, our proposed method consists of the following four main modules. (1) The Encoding module uses BERT (Devlin et al., 2019) to encode the concatenation of the rule text, initial question, scenario and dialog history into contextualized representations. However, there are two main drawbacks to the existing methods. First, with respect to the reasoning of the rule text, existing methods do not explicitly track whether a condition listed in the rule has already been satisfied as the conversation flows so that it can make a better decision. Second, with respect to the extraction of question-related rules, it is difficult in the current approach to extract the most relevant text span to generate the next question. For"
2020.acl-main.88,P18-1177,0,0.0218596,"Missing"
2020.acl-main.88,P19-1480,1,0.83023,"inds the correct rule sentence but extracts a different span. Another challenge comes from the one-to-many problem in sequence generation. When there are multiple underspecified rule sentences, the model asks about one of these underspecified rule sentences which is different from the ground truth one. This suggests that new evaluation metrics could be proposed by taking this into consideration. 4 Related Work ShARC Conversational Machine Reading (Saeidi et al., 2018) differs from conversational question answering (Choi et al., 2018; Reddy et al., 2019) and conversational question generation (Gao et al., 2019) in that 1) machines are required to formulate follow-up questions to fill the information gap, and 2) machines have to interpret a set of complex decision rules and make a question-related conclusion, instead of extracting the answer from the text. CMR can be viewed as a special type of task-oriented dialog systems (Wen et al., 2017; Zhong et al., 2018; Wu et al., 2019) to help users achieve their goals. However, it does not rely on predefined slot and ontology information but natural language rules. On the ShARC CMR challenge (Saeidi et al., 2018), Lawrence et al. (2019) propose an end-toend"
2020.acl-main.88,D19-1001,0,0.306002,"onal question generation (Gao et al., 2019) in that 1) machines are required to formulate follow-up questions to fill the information gap, and 2) machines have to interpret a set of complex decision rules and make a question-related conclusion, instead of extracting the answer from the text. CMR can be viewed as a special type of task-oriented dialog systems (Wen et al., 2017; Zhong et al., 2018; Wu et al., 2019) to help users achieve their goals. However, it does not rely on predefined slot and ontology information but natural language rules. On the ShARC CMR challenge (Saeidi et al., 2018), Lawrence et al. (2019) propose an end-toend bidirectional sequence generation approach with mixed decision making and question generation stages. Saeidi et al. (2018) split it into sub-tasks and combines hand-designed sub-models for decision classification, entailment and question generation. Zhong and Zettlemoyer (2019) propose to extract all possible rule text spans, assign each of them an entailment score, and edit the span with the highest score into a follow-up question. However, they do not use these entailment scores for decision making. Sharma et al. (2019) study patterns of the dataset and include addition"
2020.acl-main.88,P18-1136,1,0.84912,"tion) separately, EMT is a unified approach that exploits its memory states for both decision making and question generation. Memory-Augmented Neural Networks. Our work is also related to memory-augmented neural networks (Graves et al., 2014, 2016), which have been applied in some NLP tasks such as question answering (Henaff et al., 2017) and machine translation (Wang et al., 2016). For dialog applications, Zhang et al. (2019) propose a dialogue management model that employs a memory controller and a slot-value memory, Bordes et al. (2016) learn a restaurant bot by end-to-end memory networks, Madotto et al. (2018) incorporate external memory modules into dialog generation. 5 Conclusions In this paper, we have proposed a new framework for conversational machine reading (CMR) that comprises a novel explicit memory tracker (EMT) to track entailment states of the rule sentences explicitly within its memory module. The updated states are utilized for decision making and coarseto-fine follow-up question generation in a unified manner. EMT achieved a new state-of-the-art result on the ShARC CMR challenge. EMT also gives interpretability by showing the entailment-oriented reasoning process as the conversation"
2020.acl-main.88,P02-1040,0,0.10833,"Missing"
2020.acl-main.88,Q19-1016,0,0.061325,"ithin the ground truth rule sentence, while in 9 cases, it finds the correct rule sentence but extracts a different span. Another challenge comes from the one-to-many problem in sequence generation. When there are multiple underspecified rule sentences, the model asks about one of these underspecified rule sentences which is different from the ground truth one. This suggests that new evaluation metrics could be proposed by taking this into consideration. 4 Related Work ShARC Conversational Machine Reading (Saeidi et al., 2018) differs from conversational question answering (Choi et al., 2018; Reddy et al., 2019) and conversational question generation (Gao et al., 2019) in that 1) machines are required to formulate follow-up questions to fill the information gap, and 2) machines have to interpret a set of complex decision rules and make a question-related conclusion, instead of extracting the answer from the text. CMR can be viewed as a special type of task-oriented dialog systems (Wen et al., 2017; Zhong et al., 2018; Wu et al., 2019) to help users achieve their goals. However, it does not rely on predefined slot and ontology information but natural language rules. On the ShARC CMR challenge (Saeidi"
2020.acl-main.88,D18-1233,0,0.180418,"Missing"
2020.acl-main.88,D16-1027,0,0.0143228,"as two key differences: (1) EMT makes decision via explicitly entailmentoriented reasoning, which, to our knowledge, is the first such approach; (2) Instead of treating decision making and follow-up question generation (or span extraction) separately, EMT is a unified approach that exploits its memory states for both decision making and question generation. Memory-Augmented Neural Networks. Our work is also related to memory-augmented neural networks (Graves et al., 2014, 2016), which have been applied in some NLP tasks such as question answering (Henaff et al., 2017) and machine translation (Wang et al., 2016). For dialog applications, Zhang et al. (2019) propose a dialogue management model that employs a memory controller and a slot-value memory, Bordes et al. (2016) learn a restaurant bot by end-to-end memory networks, Madotto et al. (2018) incorporate external memory modules into dialog generation. 5 Conclusions In this paper, we have proposed a new framework for conversational machine reading (CMR) that comprises a novel explicit memory tracker (EMT) to track entailment states of the rule sentences explicitly within its memory module. The updated states are utilized for decision making and coar"
2020.acl-main.88,E17-1042,0,0.073334,"Missing"
2020.acl-main.88,P19-1078,1,0.832483,"this into consideration. 4 Related Work ShARC Conversational Machine Reading (Saeidi et al., 2018) differs from conversational question answering (Choi et al., 2018; Reddy et al., 2019) and conversational question generation (Gao et al., 2019) in that 1) machines are required to formulate follow-up questions to fill the information gap, and 2) machines have to interpret a set of complex decision rules and make a question-related conclusion, instead of extracting the answer from the text. CMR can be viewed as a special type of task-oriented dialog systems (Wen et al., 2017; Zhong et al., 2018; Wu et al., 2019) to help users achieve their goals. However, it does not rely on predefined slot and ontology information but natural language rules. On the ShARC CMR challenge (Saeidi et al., 2018), Lawrence et al. (2019) propose an end-toend bidirectional sequence generation approach with mixed decision making and question generation stages. Saeidi et al. (2018) split it into sub-tasks and combines hand-designed sub-models for decision classification, entailment and question generation. Zhong and Zettlemoyer (2019) propose to extract all possible rule text spans, assign each of them an entailment score, and"
2020.acl-main.88,P18-1135,1,0.842643,"proposed by taking this into consideration. 4 Related Work ShARC Conversational Machine Reading (Saeidi et al., 2018) differs from conversational question answering (Choi et al., 2018; Reddy et al., 2019) and conversational question generation (Gao et al., 2019) in that 1) machines are required to formulate follow-up questions to fill the information gap, and 2) machines have to interpret a set of complex decision rules and make a question-related conclusion, instead of extracting the answer from the text. CMR can be viewed as a special type of task-oriented dialog systems (Wen et al., 2017; Zhong et al., 2018; Wu et al., 2019) to help users achieve their goals. However, it does not rely on predefined slot and ontology information but natural language rules. On the ShARC CMR challenge (Saeidi et al., 2018), Lawrence et al. (2019) propose an end-toend bidirectional sequence generation approach with mixed decision making and question generation stages. Saeidi et al. (2018) split it into sub-tasks and combines hand-designed sub-models for decision classification, entailment and question generation. Zhong and Zettlemoyer (2019) propose to extract all possible rule text spans, assign each of them an ent"
2020.acl-main.88,P19-1223,0,0.218973,"Missing"
2020.emnlp-main.411,N19-5002,0,0.0187219,"xamples. Table 1 (b) shows a negative example, where the input utterance comes from the intent, “bill due”, and the paired sentence from another intent, “bill balance”. 5066 3.3 Seamless Transfer from NLI A key characteristic of our method is that we seek to model the relations between the utterance pairs, instead of explicitly modeling the intent classes. To mitigate the data scarcity setting in few-shot learning, we consider transferring another intersentence-relation task. This work focuses on NLI; the task is to identify whether a hypothesis sentence can be entailed by a premise sentence (Bowman and Zhu, 2019). We treat the NLI task as a binary classification task: entailment (positive) or non-entailment (negative).2 We first pre-train our model with the NLI task, where the premise sentence corresponds to the u-position, and the hypothesis sentence corresponds to the ej,i -position in Equation (5). Note that it is not necessary to modify the model architecture since the task format is consistent, and we can train the NLI model solely based on existing NLI datasets. Once the NLI model pre-training is completed, we fine-tune the NLI model with the intent classification training examples described in"
2020.emnlp-main.411,D15-1075,0,0.428534,"u-position, and the hypothesis sentence corresponds to the ej,i -position in Equation (5). Note that it is not necessary to modify the model architecture since the task format is consistent, and we can train the NLI model solely based on existing NLI datasets. Once the NLI model pre-training is completed, we fine-tune the NLI model with the intent classification training examples described in Section 3.2. This allows us to transfer the NLI model to any intent detection datasets seamlessly. Why NLI? The NLI task has been actively studied, especially since the emergence of large scale datasets (Bowman et al., 2015; Williams et al., 2018), and we can directly leverage the progress. Moreover, recent work is investigating cross-lingual NLI (Eriguchi et al., 2018; Conneau et al., 2018), and this is encouraging to consider multilinguality in future work. On the other hand, while we can find examples relevant to the intent detection task, as shown in Table 1 ((c), (d), and (e)), we still need the few-shot fine-tuning. This is because a domain mismatch still exists in general, and perhaps more importantly, our intent detection approach is not exactly modeling NLI. Why not other tasks? There are other tasks mo"
2020.emnlp-main.411,2020.nlp4convai-1.5,0,0.056721,"Missing"
2020.emnlp-main.411,P10-1046,0,0.0432628,"Missing"
2020.emnlp-main.411,P17-1171,0,0.0222836,"same pre-trained models. However, our examplebased method has an inference-time bottleneck in Equation (5), where we need to compute the BERT encoding for all N × K (u, ej,i ) pairs. We follow common practice in document retrieval to reduce the inference-time bottleneck (Nie et al., 2019; Asai et al., 2020), by introducing a fast text retrieval model to select a set of top-k examples Ek from the training set E, based on its retrieval scores. We then replace E in Equation (4) with the shrunk set Ek . The cost of the paired BERT encoding is now constant, regardless the size of E. Either TF-IDF (Chen et al., 2017) or embedding-based retrieval (Johnson et al., 2017; Seo et al., 2019; Lee et al., 2019) can be used for the first step. We use the following fast kNN. Faster kNN As a baseline and a way to instantiate our joint approach, we use Sentence-BERT (SBERT) (Reimers and Gurevych, 2019) to separately encode u and ej,i (x ∈ {u, ej,i }) as follows: v(x) = SBERT(x) ∈ Rd , (7) where the input text format is identical to that of BERT in Equation (2). SBERT is a BERT-based text embedding model, fine-tuned by siamese networks with NLI datasets. Thus both our method and SBERT transfer the NLI task in differen"
2020.emnlp-main.411,D18-1269,0,0.0235176,"format is consistent, and we can train the NLI model solely based on existing NLI datasets. Once the NLI model pre-training is completed, we fine-tune the NLI model with the intent classification training examples described in Section 3.2. This allows us to transfer the NLI model to any intent detection datasets seamlessly. Why NLI? The NLI task has been actively studied, especially since the emergence of large scale datasets (Bowman et al., 2015; Williams et al., 2018), and we can directly leverage the progress. Moreover, recent work is investigating cross-lingual NLI (Eriguchi et al., 2018; Conneau et al., 2018), and this is encouraging to consider multilinguality in future work. On the other hand, while we can find examples relevant to the intent detection task, as shown in Table 1 ((c), (d), and (e)), we still need the few-shot fine-tuning. This is because a domain mismatch still exists in general, and perhaps more importantly, our intent detection approach is not exactly modeling NLI. Why not other tasks? There are other tasks modeling relationships between sentences. Paraphrase (Wieting and Gimpel, 2018) and semantic relatedness (Marelli et al., 2014) tasks are such examples. It is possible to au"
2020.emnlp-main.411,L18-1004,0,0.0464104,"Missing"
2020.emnlp-main.411,D19-1258,0,0.109979,"he same model in Equation (2), except that we follow a different input format to accommodate pairs of utterances: [[CLS], u, [SEP], ej,i , [SEP]]. σ is the sigmoid function, and W ∈ R1×d and b ∈ R are the model parameters. We can interpret our method as wrapping both the embedding and matching functions into the paired encoding with the deep self-attention in BERT (Equation (5)) along with the discriminative model (Equation (6)). It has been shown that the paired text encoding is crucial in capturing complex relations between queries and documents in document retrieval (Watanabe et al., 2017; Nie et al., 2019; Asai et al., 2020). 3 3.2 I(u) = class arg max S(u, ej,i ) , (4) ej,i ∈E Proposed Method This section first describes how to directly model inter-utterance relations in our nearest neighbor classification scenario. We then introduce a binary classification strategy by synthesizing pairwise examples, and propose a seamless transfer of NLI. Finally, we describe how to speedup our method’s inference process. 3.1 Deep Pairwise Matching Function The objective of S(u, ej,i ) in Equation (4) is to find the best matched utterance from the training set E, given the input utterance u. The typical meth"
2020.emnlp-main.411,D19-1410,0,0.0228225,"(Nie et al., 2019; Asai et al., 2020), by introducing a fast text retrieval model to select a set of top-k examples Ek from the training set E, based on its retrieval scores. We then replace E in Equation (4) with the shrunk set Ek . The cost of the paired BERT encoding is now constant, regardless the size of E. Either TF-IDF (Chen et al., 2017) or embedding-based retrieval (Johnson et al., 2017; Seo et al., 2019; Lee et al., 2019) can be used for the first step. We use the following fast kNN. Faster kNN As a baseline and a way to instantiate our joint approach, we use Sentence-BERT (SBERT) (Reimers and Gurevych, 2019) to separately encode u and ej,i (x ∈ {u, ej,i }) as follows: v(x) = SBERT(x) ∈ Rd , (7) where the input text format is identical to that of BERT in Equation (2). SBERT is a BERT-based text embedding model, fine-tuned by siamese networks with NLI datasets. Thus both our method and SBERT transfer the NLI task in different ways. Cosine similarity between v(u) and v(ej,i ) then replaces S(u, ej,i ) in Equation (6). To get a fair comparison, instead of using the encoding vectors produced by the original SBERT, we fine-tune SBERT with our intent training examples described in Section 3.2. The cosin"
2020.emnlp-main.411,P19-1436,0,0.0141365,"nce-time bottleneck in Equation (5), where we need to compute the BERT encoding for all N × K (u, ej,i ) pairs. We follow common practice in document retrieval to reduce the inference-time bottleneck (Nie et al., 2019; Asai et al., 2020), by introducing a fast text retrieval model to select a set of top-k examples Ek from the training set E, based on its retrieval scores. We then replace E in Equation (4) with the shrunk set Ek . The cost of the paired BERT encoding is now constant, regardless the size of E. Either TF-IDF (Chen et al., 2017) or embedding-based retrieval (Johnson et al., 2017; Seo et al., 2019; Lee et al., 2019) can be used for the first step. We use the following fast kNN. Faster kNN As a baseline and a way to instantiate our joint approach, we use Sentence-BERT (SBERT) (Reimers and Gurevych, 2019) to separately encode u and ej,i (x ∈ {u, ej,i }) as follows: v(x) = SBERT(x) ∈ Rd , (7) where the input text format is identical to that of BERT in Equation (2). SBERT is a BERT-based text embedding model, fine-tuned by siamese networks with NLI datasets. Thus both our method and SBERT transfer the NLI task in different ways. Cosine similarity between v(u) and v(ej,i ) then replaces S(u"
2020.emnlp-main.411,D19-1045,0,0.0215046,"udied in the first scenario. In our paper, we have focused on the second scenario, assuming that there are only a limited number of training examples for each class. Our work is related to metric-based approaches such as matching networks (Vinyals et al., 2016a), prototypical networks (Snell et al., 2017) and relation networks (Sung et al., 2018), as they model nearest neighbours in an example-embedding or a classembedding space. We showed that a relation network with the RoBERTa embeddings does not perform comparably to our method. We also considered several ideas from prototypical networks (Sun et al., 2019), but those did not outperform our EmbkNN baseline. These results indicate that deep self-attention is the key to the nearest neighbor approach with OOS detection. 7 Conclusion In this paper, we have presented a simple yet efficient nearest-neighbor classification model to detect user intents and OOS intents. It includes paired encoding and discriminative training to model relations between the input and example utterances. Moreover, a seamless transfer from NLI and a joint approach with fast retrieval are designed to improve the performance in terms of the accuracy and inference speed. Experi"
2020.emnlp-main.411,P19-1488,0,0.0169508,"er input. This is illustrated in Figure 4, where the in-domain intent and OOS accuracy metrics (on the development set of the banking domain in the 5shot setting) improve with the increase of k, while the latency increases at the same time. Empirically, k = 20 appears to strike the balance between latency and accuracy, with the accuracy metrics similar to those of the DNNC method, while being much faster than DNNC (dashed lines are the corresponding DNNC references). 6 Discussions and Related Work Interpretability Interpretability is an important line of research recently (Jiang et al., 2019; Sydorova et al., 2019; Asai et al., 2020). The nearest neighbor approach (Simard et al., 1993) is appealing in that we can explicitly know which training example triggers each prediction. Table 11 in Appendix C shows some examples. Call for better embeddings Emb-kNN and RNkNN are not as competitive as DNNC. This encourages future work on the task-oriented evaluation of text embeddings in kNN. Training time Our DNNC method needs longer training time than that of the classifier (e.g., 90 vs. 40 seconds to train a single-domain model), because we synthesize the pairwise examples. As a first step, we used all the trai"
2020.emnlp-main.411,W18-5446,0,0.0863705,"Missing"
2020.emnlp-main.411,D19-1670,0,0.0380088,"ion for each result. Using the development set We would not always have access to a large enough development set in the few-shot learning scenario. However, we still use the development set provided by the dataset to investigate the models’ behaviors when changing hyper-parameters like the threshold. Models compared our experiments: We list the models used in • Classifier baselines: “Classifier” is the RoBERTa-based classification model described in Section 2.2. We further seek solid baselines by data augmentation. “ClassifierEDA” is the classifier trained with data augmentation techniques in Wei and Zou (2019). “Classifier-BT” is the classifier trained with back-translation data augmentation (Yu et al., 2018; Shleifer, 2019) by using a transformer-based English↔German translation system (Vaswani et al., 2017). • Non-BERT classifier: We also test a stateof-the-art fast embedding-based classifier, “USE+ConveRT” (Henderson et al., 2019; Casanueva et al., 2020), in the “all domains” setting. Casanueva et al. (2020) showed that the “USE+ConveRT” outperformed a BERT classifier on the CLINC150 dataset, while it was not evaluated along with the OOS detection task. We modified their original code6 to apply"
2020.emnlp-main.411,P18-1042,0,0.0266018,"ge the progress. Moreover, recent work is investigating cross-lingual NLI (Eriguchi et al., 2018; Conneau et al., 2018), and this is encouraging to consider multilinguality in future work. On the other hand, while we can find examples relevant to the intent detection task, as shown in Table 1 ((c), (d), and (e)), we still need the few-shot fine-tuning. This is because a domain mismatch still exists in general, and perhaps more importantly, our intent detection approach is not exactly modeling NLI. Why not other tasks? There are other tasks modeling relationships between sentences. Paraphrase (Wieting and Gimpel, 2018) and semantic relatedness (Marelli et al., 2014) tasks are such examples. It is possible to automatically create large-scale paraphrase datasets by machine translation (Ganitkevitch et al., 2013). However, our task is not a paraphrasing task, and creating negative examples is crucial and non-trivial (Cham2 A widely-used format is a three-way classification task with entailment, neutral, and contradiction, but we merge the latter two classes into a single non-entailment class. bers and Jurafsky, 2010). In contrast, as described above, the NLI setting comes with negative examples by nature. The"
2020.emnlp-main.411,N18-1101,0,0.5181,"tell me any of the names and addresses? Annie considered. It’s Sunday, what channel is this? I want to go back to Marguerite. Utterance to be compared help me move my money please Label pos. i need to know the amounts due for my utilities and cable bills Are you able to inform me of any name or address? neg. pos. It’s Sunday, can you change the channel? I never want to return to Marguerite. neg. neg. Table 1: Training examples for our model. The first two examples ((a)–(b)) come from the CLINC150 dataset (Larson et al., 2019), and the other three examples ((c)–(e)) come from the MNLI dataset (Williams et al., 2018). with k = 1), a simple and well-established concept for classification (Simard et al., 1993; Cunningham and Delany, 2007). The basic idea is to classify an input into the same class of the most relevant training example based on a certain metric. In our task, we formulate a nearest neighbor classification model as the following: ! we propose to formulate S(u, ej,i ) as follows: h = BERT(u, ej,i ) ∈ Rd , S(u, ej,i ) = σ(W · h + b) ∈ R, (5) (6) where class(ej,i ) is a function returning the intent label (class) of the training example ej,i , and S is a function that estimates some relevance sco"
2020.emnlp-main.411,2020.emnlp-main.66,1,0.85037,"We follow Larson et al. (2019) to report in-domain accuracy, Accin , and OOS recall, Roos . These two metrics are defined as follows: https://github.com/clinc/oos-eval. (9) and select a threshold value to maximize the score on the development set. There is a trade-off to be noted; the larger the value of T is, the higher Roos (and the lower Accin ) we expect, because the models predict OOS more frequently. Notes on Jin oos Our joint score Jin oos in Equation (9) gives the same weight to the two metrics, Accin and Roos , compared to other combined metrics. For example, Larson et al. (2019) and Wu et al. (2020) used a joint accuracy score: Cin + Coos Accin + rRoos , = Nin + Noos 1+r (10) where r = Noos /Nin depends on the balance between Nin and Noos , and thus this combined metric can put much more weight on the in-domain accuracy when Nin  Noos . Table 2 shows r = 100/3000 (= 0.0333) in the development set of the “all domains” setting, which underestimates the importance of Roos . Actually, the OOS recall scores in Larson et al. (2019) and Wu et al. (2020) are much lower than those with our RoBERTa classifier, and the trade-off with respect to the tuning process was not discussed.4 We also report"
2020.emnlp-main.412,2020.acl-main.9,0,0.0425933,"Missing"
2020.emnlp-main.412,D18-1547,0,0.0456783,"Missing"
2020.emnlp-main.412,D19-1459,1,0.843017,"across domains and possibly datasets. To address this issue, Paul et al. (2019) propose a universal schema for DAs by aligning annotations for multiple existing corpora. In this regard, another useful corpora employed as a testbed in this work is Schema-guided dialogues (SGD) (Rastogi et al., 2020), which covers 20 domains under the same DA annotation schema. It is often challenging and costly to obtain a large amount of in-domain dialogues with annotations. However, unlabeled dialogue corpora in target domain can easily be curated from past conversation logs or collected via crowd-sourcing (Byrne et al., 2019; Budzianowski et al., 2018) at a more reasonable cost. The goal of this work is to investigate how to leverage pre-trained masked language models (e.g., BERT) to better adapt DA taggers to unseen domains with available unlabeled dialogues. Pre-trained language models (Devlin et al., 2019; Liu et al., 2019) have been successful for several NLP tasks including dialogue systems (Wolf et al., 5083 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 5083–5089, c November 16–20, 2020. 2020 Association for Computational Linguistics Figure 2: Given a dialogue"
2020.emnlp-main.412,N19-1423,0,0.485933,"t al., 2020), which covers 20 domains under the same DA annotation schema. It is often challenging and costly to obtain a large amount of in-domain dialogues with annotations. However, unlabeled dialogue corpora in target domain can easily be curated from past conversation logs or collected via crowd-sourcing (Byrne et al., 2019; Budzianowski et al., 2018) at a more reasonable cost. The goal of this work is to investigate how to leverage pre-trained masked language models (e.g., BERT) to better adapt DA taggers to unseen domains with available unlabeled dialogues. Pre-trained language models (Devlin et al., 2019; Liu et al., 2019) have been successful for several NLP tasks including dialogue systems (Wolf et al., 5083 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 5083–5089, c November 16–20, 2020. 2020 Association for Computational Linguistics Figure 2: Given a dialogue turn in target domain, we obtain teacher and student representations by applying two different maskings on its flattened original representation. We use the output binary probability distributions (per dialog act) of the teacher as soft targets to train the student. Orange and green colo"
2020.emnlp-main.412,2020.acl-main.740,0,0.0499982,"Missing"
2020.emnlp-main.412,W14-4337,0,0.0624687,"Missing"
2020.emnlp-main.412,2021.ccl-1.108,0,0.0917789,"Missing"
2020.emnlp-main.412,C18-1300,0,0.0346237,"Missing"
2020.emnlp-main.412,N19-1373,0,0.0215908,"ces of the same dialog act (DA) are distinct due to the domain difference, making the crossdomain generalization challenging. Introduction Dialog act (DA) tagging, one of the important NLU components of modern task-oriented dialog systems, aims to capture the speaker’s intention behind the utterances at each dialog turn. Several different schema and taxonomies have been introduced by several different researchers (Core and Allen, 1997; Stolcke et al., 2000; Bunt et al., 2010; Mezza et al., 2018) over the years. However, the main focus of the recent work (Kumar et al., 2018; Chen et al., 2018; Raheja and Tetreault, 2019) on DA tagging was on human-human social conversations (Godfrey et al., 1992; Jurafsky et al., 1997), which is less applicable for task-oriented setting. Recently, several task-oriented dialogue datasets (Shah et al., 2018; Henderson et al., 2014; Budzianowski et al., 2018) have been released. However, the discrepancy in their annotation schema hinders the progress on building DA taggers that can generalize across domains and possibly datasets. To address this issue, Paul et al. (2019) propose a universal schema for DAs by aligning annotations for multiple existing corpora. In this regard, ano"
2020.emnlp-main.412,J00-3003,0,0.7045,"Missing"
2020.emnlp-main.412,D19-1670,0,0.0288883,"This objective is used to update the DA tagger via the supervision coming from labeled source data S. We use binarycross entropy loss JSTL (θ; x, y) defined as: − [y · log pθ (·|x) + (1 − y) · log(1 − pθ (·|x))] (2) 2.3 Learning with M ASK AUGMENT Semi-supervised learning (SSL) (Berthelot et al., 2019, 2020; Sohn et al., 2020; Li et al., 2020) is 5084 an effective approach for improving deep learning models by leveraging in-domain unlabeled data. Unlike traditional SSL setting, our objective is to primarily address the underlying source-to-target domain shift. In prior work (Xie et al., 2019; Wei and Zou, 2019), unsupervised data augmentation methods including word replacement and backtranslation have been shown useful for short written text classification. However, such augmentation methods are shown to be less effective (Shleifer, 2019) when used with pre-trained models. Besides, back-translation is less applicable in our scenario as translation of multi-turn dialogue itself is a rather challenging task compared to short text. Instead, we propose a simple and controllable data augmentation–M ASK AUGMENT–to explore a new unsupervised teacher-student learning scheme for domain adaptation of DA tagge"
2020.emnlp-main.412,2020.emnlp-main.66,1,0.86437,"Missing"
2020.emnlp-main.501,P19-4007,0,0.0319994,"Missing"
2020.emnlp-main.501,D18-1269,0,0.028776,"tudy how multilingual pre-training affects the extractor and show that it allows transfer of task-specific knowledge from the victim model to other pre-trained languages. We consider two instances in our experiments: (i) one where the extractor has access to no real data and only queries gibberish, and (ii) extractor has access to some real data in English. Here, we refer to real data as data which was also used to train the victim model. In our experiments, the victim model is trained on the MNLI dataset (Williams et al., 2018). We perform all cross-lingual experiments on the XNLI benchmark (Conneau et al., 2018). This benchmark contains NLI instances in several languages whose test sets were translated by humans using the MNLI dataset. In order to generate gibberish input data, we follow the approach of Krishna et al. (2020). For the hypothesis, we generate sentences of random length by sampling words uniformly from the wordlevel vocabulary of WikiText-103. The length of the sentence is sampled based on the distribution of lengths in WikiText-103. For the premise, we randomly swap three words of the hypothesis for random words leaving the rest identical. This is to mimic common NLI inputs which have"
2020.emnlp-main.501,N19-1423,0,0.119058,"leaning, and model training and tuning. Recent work by Krishna et al. (2020) has demonstrated that deployed NLP models can be stolen by adversaries by querying victim models with gibberish input data that consists of random sequences of words. In particular, they showed that the following approach is sufficient for stealing text classification and question answering models. First, unlabeled data is created by randomly sampling words from a vocabulary. Second, a deployed API is queried with each random input sequence to obtain a label for each. Third, a pre-trained language model such as BERT (Devlin et al., 2019) is fine-tuned on the victim-labeled gibberish data. The resulting model retains a significant fraction of the victim model’s performance without ever seeing a single well-formed input sentence. This Extracted Multilingual NLI Model Figure 1: Extraction of multilingual models from monolingual APIs. (Extraction phase:) A pre-trained multilingual model is fine-tuned on gibberish data whose labels are queried from a monolingual API. (Inference phase): This model is then used for zero-shot cross-lingual transfer on different languages. process of “stealing” from an API, or “extracting” a local cop"
2020.emnlp-main.501,D18-1407,0,0.0199121,"Missing"
2020.emnlp-main.501,N18-1101,0,0.0128824,"that is trained by extracting task-specific knowledge from the victim model. We aim to study how multilingual pre-training affects the extractor and show that it allows transfer of task-specific knowledge from the victim model to other pre-trained languages. We consider two instances in our experiments: (i) one where the extractor has access to no real data and only queries gibberish, and (ii) extractor has access to some real data in English. Here, we refer to real data as data which was also used to train the victim model. In our experiments, the victim model is trained on the MNLI dataset (Williams et al., 2018). We perform all cross-lingual experiments on the XNLI benchmark (Conneau et al., 2018). This benchmark contains NLI instances in several languages whose test sets were translated by humans using the MNLI dataset. In order to generate gibberish input data, we follow the approach of Krishna et al. (2020). For the hypothesis, we generate sentences of random length by sampling words uniformly from the wordlevel vocabulary of WikiText-103. The length of the sentence is sampled based on the distribution of lengths in WikiText-103. For the premise, we randomly swap three words of the hypothesis for"
2020.emnlp-main.66,W17-5526,0,0.0710966,"Missing"
2020.emnlp-main.66,2020.acl-main.9,0,0.242502,"Missing"
2020.emnlp-main.66,D19-5602,0,0.0703563,"Missing"
2020.emnlp-main.66,D18-1547,0,0.27744,"Missing"
2020.emnlp-main.66,D19-1459,0,0.205233,". Henderson et al. (2019b) pre-trained a response selection model for task-oriented dialogues. They first pre-train on Reddit corpora and then fine-tune on target dialogue domains, but their training and fine-tuning code is not released. Peng et al. (2020) focus on the natural language generation (NLG) task, which assumes dialogue acts and slot-tagging results are given to generate a natural language response. Pre-training on a set of annotated NLG corpora can improve conditional generation quality using a GPT-2 model. Name MetaLWOZ (Lee et al., 2019) Schema (Rastogi et al., 2019) Taskmaster (Byrne et al., 2019) MWOZ (Budzianowski et al., 2018) MSR-E2E (Li et al., 2018) SMD (Eric and Manning, 2017) Frames (Asri et al., 2017) WOZ (Mrkˇsi´c et al., 2016) CamRest676 (Wen et al., 2016) # Dialogue 37,884 22,825 13,215 10,420 10,087 3,031 1,369 1,200 676 # Utterance 432,036 463,284 303,066 71,410 74,686 15,928 19,986 5,012 2,744 Avg. Turn 11.4 20.3 22.9 6.9 7.4 5.3 14.6 4.2 4.1 # Domain 47 17 6 7 3 3 3 1 1 Table 1: Data statistics for task-oriented dialogue datasets. 3 Method This section discusses each dataset used in our taskoriented pre-training and how we process the data. Then we introduce the selecte"
2020.emnlp-main.66,W14-4337,0,0.303403,"Missing"
2020.emnlp-main.66,P19-1536,0,0.0530865,"Missing"
2020.emnlp-main.66,D19-1131,0,0.101159,"espectively. Across seven different domains, in total, it has 30 (domain, slot) pairs that need to be tracked in the test set. We use its revised version MWOZ 2.1, which has the same dialogue transcripts but with cleaner state label annotation. Evaluation Datasets We pick up several datasets, OOS, DSTC2, GSIM, and MWOZ, for downstream evaluation. The first three corpora are not included in the pre-trained task-oriented datasets. For MWOZ, to be fair, we do not include its test set dialogues during the pretraining stage. Details of each evaluation dataset are discussed in the following: • OOS (Larson et al., 2019): The out-of-scope intent dataset is one of the largest annotated intent datasets, including 15,100/3,100/5,500 samples for the train, validation, and test sets, respectively. It covers 151 intent classes over ten domains, including 150 in-scope intent and one outof-scope intent. The out-of-scope intent means that a user utterance that does not fall into any of the predefined intents. Each of the intents has 100 training samples. • DSTC2 (Henderson et al., 2014): DSTC2 is a human-machine task-oriented dataset that may include a certain system response noise. It has 1,612/506/1117 dialogues for"
2020.emnlp-main.66,2021.ccl-1.108,0,0.0909468,"Missing"
2020.emnlp-main.66,2020.findings-emnlp.17,0,0.0434299,"gues, on the other hand, has few related works. Budzianowski and Vuli´c (2019) first apply the GPT-2 model to train on response generation task, which takes system belief, database result, and last dialogue turn as input to predict next system responses. It only uses one dataset to train its model because few public datasets have database information available. Henderson et al. (2019b) pre-trained a response selection model for task-oriented dialogues. They first pre-train on Reddit corpora and then fine-tune on target dialogue domains, but their training and fine-tuning code is not released. Peng et al. (2020) focus on the natural language generation (NLG) task, which assumes dialogue acts and slot-tagging results are given to generate a natural language response. Pre-training on a set of annotated NLG corpora can improve conditional generation quality using a GPT-2 model. Name MetaLWOZ (Lee et al., 2019) Schema (Rastogi et al., 2019) Taskmaster (Byrne et al., 2019) MWOZ (Budzianowski et al., 2018) MSR-E2E (Li et al., 2018) SMD (Eric and Manning, 2017) Frames (Asri et al., 2017) WOZ (Mrkˇsi´c et al., 2016) CamRest676 (Wen et al., 2016) # Dialogue 37,884 22,825 13,215 10,420 10,087 3,031 1,369 1,200"
2020.emnlp-main.66,W18-5446,0,0.0933208,"Missing"
2020.emnlp-main.66,D16-1264,0,0.16871,"Missing"
2020.emnlp-main.66,W17-7301,0,0.0134412,"icted distributions Pint and the true intent labels. Dialogue state tracking can be treated as a multi-class classification problem using a predefined ontology. Unlike intent, we use dialogue history X (a sequence of utterances) as input and a model predicts slot values for each (domain, slot) pair at each dialogue turn. Each corresponding value vij , the i-th value for the j-th (domain, slot) pair, is passed into a pre-trained model and fixed its representation during training. limited by hardware. We also try different negative sampling strategies during pre-training such as local sampling (Saeidi et al., 2017), but do not observe significant change compared to random sampling. Overall pre-training loss function is the weighted-sum of Lmlm and Lrcl , and in our experiments, we simply sum them up. We gradually reduce the learning rate without a warm-up period. We train TOD-BERT with AdamW (Loshchilov and Hutter, 2017) optimizer with a dropout ratio of 0.1 on all layers and attention weights. GELU activation functions (Hendrycks and Gimpel, 2016) is used. Models are early-stopped using perplexity scores of a held-out development set, with mini-batches containing 32 sequences of maximum length 512 toke"
2020.emnlp-main.66,N18-3006,0,0.0531715,"over ten domains, including 150 in-scope intent and one outof-scope intent. The out-of-scope intent means that a user utterance that does not fall into any of the predefined intents. Each of the intents has 100 training samples. • DSTC2 (Henderson et al., 2014): DSTC2 is a human-machine task-oriented dataset that may include a certain system response noise. It has 1,612/506/1117 dialogues for train, validation, and test sets, respectively. We follow Paul et al. (2019) to map the original dialogue act labels to universal dialogue acts, which results in 9 different system dialogue acts. • GSIM (Shah et al., 2018a): GSIM is a humanrewrote machine-machine task-oriented corpus, including 1500/469/1039 dialogues for the train, validation, and test sets, respectively. We combine its two domains, movie and restaurant domains, into one single corpus. It is collected by Machines Talking To Machines (M2M) (Shah et al., 2018b) approach, a functionality-driven process combining a dialogue self-play step and a crowdsourcing step. We map its dialogue act labels to universal dialogue acts (Paul et al., 2019), resulting in 6 different system dialogue acts. • MWOZ (Budzianowski et al., 2018): MWOZ is the most common"
2020.emnlp-main.66,P19-1078,1,0.877039,"Missing"
2020.emnlp-main.66,W18-3022,0,0.0856921,"Missing"
2020.emnlp-main.66,W17-5506,0,\N,Missing
2020.emnlp-main.66,W19-5932,0,\N,Missing
2020.emnlp-main.660,D15-1075,0,0.334807,"way. By a forward-looking perspective, instead, a single machine that can handle diverse (seen and unseen) tasks is desired. The reason is that we cannot always rely on expensive human resources to annotate large-scale task-specific labeled data, especially considering the inestimable number of tasks to be explored. Therefore, a reasonable attempt is to map diverse NLP tasks into a common learning problem—solving this common problem equals to solving any downstream NLP tasks, even some tasks that are new or have insufficient annotations. Textual entailment (aka. natural language inference in Bowman et al. (2015)) is the task of studying the relation of two assertive sentences, Premise (P) and Hypothesis (H): whether H is true given P. Textual entailment (TE) was originally brought up as a unified framework for modeling diverse NLP tasks (Dagan et al., 2005; Poliak et al., 2018). The research on TE dates back more than two decades and has made significant progress. Particularly, with the advances of deep neural networks and the availability of large-scale human annotated datasets, fine-tuned systems often claim surpassing human performance on certain benchmarks. Nevertheless, two open problems remain."
2020.emnlp-main.660,N19-1300,0,0.0152841,"AIL is to not only learn the matching function, but also map the instances in S and T to the same space. 8233 UFO-E NTAIL vs. STILTS. Given the source data S and a couple of labeled examples from the target T , STILTS (Phang et al., 2018) first trains RoBERTa on S, then fine-tune on the labeled examples of T . Both the pretraining and fine-tuning use the same RoBERTa system in Figure 1. It has been widely used as the state of the art technique for making use of related tasks to improve target tasks, especially when the target tasks have limited annotations (Liu et al., 2019; Sap et al., 2019; Clark et al., 2019). By the architecture, STILTS relies on the standard RoBERTa classifier which consists of a RoBERTa encoder and a logistic regression on the top; UFO-E NTAIL instead has a cross-task nearest neighbor block on the top of the RoBERTa encoder. STILTS tries to learn the target-specific parameters by tuning on the k labeled examples. However, this is very challenging if k is over small, like values {1, 3, 5, 10} we will use in our problems. We can also think STILTS learns class prototypical representations implicitly (i.e., the weights in the logistic regression layer), however, the bias term in th"
2020.emnlp-main.660,P07-1033,0,0.367983,"Missing"
2020.emnlp-main.660,N19-1423,0,0.0427111,"w years, the research on textual entailment has been driven by the creation of large-scale datasets, such as SNLI (Bowman et al., 2015), science domain SciTail (Khot et al., 2018), and multi-genre MNLI (Williams et al., 2018). Representative work includes the first attentive recurrent neural network (Rockt¨aschel et al., 2016) and its followers (Wang and Jiang, 2016; Wang et al., 2017), as well as the attentive convolutional networks such as attentive pooling (dos Santos et al., 2016) and attentive convolution (Yin and Sch¨utze, 2018), and self-attentive large-scale language models like BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019). All these studies result in systems that are overly tailored to the datasets. Our work differs in that we care more about fewshot applications of textual entailment, assuming that a new domain or an NLP task is not provided with rich annotated data. Generalization via domain adaptation. Two main types of domain adaptation (DA) problems have been studied in literature: supervised DA and semi-supervised DA. In the supervised case, we have access to a large annotated data in the source domain and a small-scale annotated data in the target domain (Daum´e III, 2007;"
2020.emnlp-main.660,N19-1039,0,0.0214007,"r work differs in that we care more about fewshot applications of textual entailment, assuming that a new domain or an NLP task is not provided with rich annotated data. Generalization via domain adaptation. Two main types of domain adaptation (DA) problems have been studied in literature: supervised DA and semi-supervised DA. In the supervised case, we have access to a large annotated data in the source domain and a small-scale annotated data in the target domain (Daum´e III, 2007; Kang and Feng, 2018). In the semi-supervised case, we have a large but unannotated corpus in the target domain (Miller, 2019). In contrast to semi-supervised DA, our work does not assume the availability of a large unlabeled data from the target domain or task. We also build more ambitious missions than the supervised DA since our work aims to adapt the model to new domains as well as new NLP tasks. Generalization via few-shot learning. Fewshot problems are studied typically in the image domain (Koch et al., 2015; Vinyals et al., 2016; Snell et al., 2017; Ren et al., 2018; Sung et al., 2018). The core idea in metric-based few-shot 8230 learning is similar to nearest neighbors. The predicted probability of a test ins"
2020.emnlp-main.660,W18-5441,0,0.0444137,"Missing"
2020.emnlp-main.660,N18-2017,0,0.0191469,"so think STILTS learns class prototypical representations implicitly (i.e., the weights in the logistic regression layer), however, the bias term in the logistic regression layer reflect mainly the distribution in the source S, which is less optimal for predicting in the target T . 4 Experiments We apply UFO-E NTAIL to entailment tasks of open domain and open NLP tasks. Experimental setup. Our system is implemented with Pytorch on the transformers package released by Huggingface2 . We use “RoBERTalarge” initialized by the pretrained language model. To mitigate the potential bias or artifacts (Gururangan et al., 2018) in sampling, all numbers of k-shot are average of five runs in seeds {42, 16, 32, 64, 128}. Due to GPU memory constraints, we only update the nearest neighbor block, the hidden layer and top-5 layers in RoBERTa. For other training configurations, please refer to our released code. Baselines. The following baselines are shared by experiments on open entailment tasks and open NLP tasks. • 0-shot. We assume zero examples from target domains. We train a RoBERTa classifier3 on 2 https://github.com/huggingface/ transformers 3 Specifically, the “RobertaForSequenceClassification” classifier in the Hu"
2020.emnlp-main.660,D13-1020,0,0.03641,"onverted to be textual entailment. Our work provides a new perspective to tackle these NLP issues, especially given only a couple of labeled examples. Question Answering. We attempt to handle the QA setting in which only a couple of labeled examples are provided. A QA problem can be formulated as a textual entailment problem—the document acts as the premise, and the (question, answer candidate), after converting into a natural sentence, acts as the hypothesis. Then a true (resp. false) hypothesis can be translated into a correct (resp. incorrect) answer. We choose the QA benchmark MCTest-500 (Richardson et al., 2013) which releases an entailment-formatted corpus. MCTest500 is a set of 500 items (split into 300 train, 50 dev and 150 test). Each item consists of a document, four questions followed by one correct answer, and three incorrect answers. Deep learning has not achieved significant success on it because of the limited training data (Trischler et al., 2016)—this is exactly our motivation that applying few-shot textual entailment to handle annotation-scarce NLP problems. For MCTest benchmark, we treat one question as one example. K-shot means we randomly sample k annotated questions (each corresponds"
2020.emnlp-main.660,D18-1514,0,0.0520493,"a method named Matching Networks. Snell et al. (2017) propose Prototypical Networks which first build prototypical representations for each class by summing up representations of supporting examples, then compare classes with test instances by squared Euclidean distances. Unlike fixed metric measures, the Relation Network (Sung et al., 2018) implements the comparison through learning a matching metric in a multi-layer architecture. In the language domain, Yu et al. (2018) combine multiple metrics learned from diverse clusters of training tasks for an unseen few-shot text classification task. Han et al. (2018) release a few-shot relation classification dataset “FewRel” and compare a couple of representative methods on it. These few-shot studies assume that, in the same domain, a part of the classes have limited samples, while other classes have adequate examples. In this work, we make a more challenging assumption that all classes in the target domain have only a couple of examples, and the training classes and testing classes are from different domains. Unified natural language processing. McCann et al. (2018) cast a group of NLP tasks as question answering over context, such as machine translatio"
2020.emnlp-main.660,D19-1454,0,0.0395338,"Missing"
2020.emnlp-main.660,2021.ccl-1.108,0,0.306573,"Missing"
2020.emnlp-main.660,P16-1041,0,0.012281,"emise, and the (question, answer candidate), after converting into a natural sentence, acts as the hypothesis. Then a true (resp. false) hypothesis can be translated into a correct (resp. incorrect) answer. We choose the QA benchmark MCTest-500 (Richardson et al., 2013) which releases an entailment-formatted corpus. MCTest500 is a set of 500 items (split into 300 train, 50 dev and 150 test). Each item consists of a document, four questions followed by one correct answer, and three incorrect answers. Deep learning has not achieved significant success on it because of the limited training data (Trischler et al., 2016)—this is exactly our motivation that applying few-shot textual entailment to handle annotation-scarce NLP problems. For MCTest benchmark, we treat one question as one example. K-shot means we randomly sample k annotated questions (each corresponds to a short article and has four answer candidates). We obtain k entailment pairs for the class “entailment” and 3k pairs for the class “non-entailment”. The official evaluation metrics in MCTest include accuracy and NDCG4 . Here, we report accuracy. Coreference Resolution. Coreference resolution aims to cluster the entities and pronouns that 8235 ref"
2020.emnlp-main.660,N16-1170,0,0.0186353,"not guarantee the accessibility of rich annotations. 2 Related Work Textual Entailment. Textual entailment was first studied in Dagan et al. (2005) and the main focus in the early stages was to study lexical and some syntactic features. In the past few years, the research on textual entailment has been driven by the creation of large-scale datasets, such as SNLI (Bowman et al., 2015), science domain SciTail (Khot et al., 2018), and multi-genre MNLI (Williams et al., 2018). Representative work includes the first attentive recurrent neural network (Rockt¨aschel et al., 2016) and its followers (Wang and Jiang, 2016; Wang et al., 2017), as well as the attentive convolutional networks such as attentive pooling (dos Santos et al., 2016) and attentive convolution (Yin and Sch¨utze, 2018), and self-attentive large-scale language models like BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019). All these studies result in systems that are overly tailored to the datasets. Our work differs in that we care more about fewshot applications of textual entailment, assuming that a new domain or an NLP task is not provided with rich annotated data. Generalization via domain adaptation. Two main types of domain ad"
2020.emnlp-main.660,Q18-1042,0,0.0190498,"we randomly sample k annotated questions (each corresponds to a short article and has four answer candidates). We obtain k entailment pairs for the class “entailment” and 3k pairs for the class “non-entailment”. The official evaluation metrics in MCTest include accuracy and NDCG4 . Here, we report accuracy. Coreference Resolution. Coreference resolution aims to cluster the entities and pronouns that 8235 refer to the same object. This is a challenging task in NLP, and greatly influences the capability of machines in understanding the text. We test on the coreference resolution benchmark GAP (Webster et al., 2018), a human-labeled corpus from Wikipedia for recognizing ambiguous pronoun-name coreference. An example from the GAP dataset is shown here: “McFerran’s horse farm was named Glen View. After his death in 1885, John E. Green acquired the farm.” For a specific pronoun in the sentence, GAP provides two entity candidates for it to link. To correctly understand the meaning of this sentence, a machine must know which person (“McFerran” or “John E. Green”) the pronoun “his” refers to. GAP has such kind of annotated examples of sizes split as 2k/454/2k in train/dev/test. Please note that some examples h"
2020.emnlp-main.660,N18-1101,0,0.652655,", c November 16–20, 2020. 2020 Association for Computational Linguistics entailment. We argue that textual entailment particularly matters when the target NLP task has insufficient annotations; in this way, some NLP tasks that share the same inference pattern and annotations are insufficient to build a task-specific model can be handled by a unified entailment system. Motivated by the two issues, we build UFOE NTAIL—the first ever generalized few-shot textual entailment system with the following setting. We first assume that we can access a largescale generic purpose TE dataset, such as MNLI (Williams et al., 2018); this dataset enables us to build a base entailment system with acceptable performance. To get even better performance in any new domain or new task, we combine the generic purpose TE dataset with a couple of domain/taskspecific examples to learn a better-performing entailment for that new domain/task. This is a reasonable assumption because in the real-world, any new domain or new task does not typically have large annotated data, but obtaining a couple of examples is usually feasible. Technically, our UFO-E NTAIL is inspired by the Prototypical Network (Snell et al., 2017), a popular metric"
2020.emnlp-main.660,D19-1404,1,0.883773,"Missing"
2020.emnlp-main.660,Q18-1047,1,0.890372,"Missing"
2020.emnlp-main.660,N18-1109,0,0.0760185,"ses for those supporting samples. Vinyals et al. (2016) compare each test instance with those supporting examples by the cosine distance in a method named Matching Networks. Snell et al. (2017) propose Prototypical Networks which first build prototypical representations for each class by summing up representations of supporting examples, then compare classes with test instances by squared Euclidean distances. Unlike fixed metric measures, the Relation Network (Sung et al., 2018) implements the comparison through learning a matching metric in a multi-layer architecture. In the language domain, Yu et al. (2018) combine multiple metrics learned from diverse clusters of training tasks for an unseen few-shot text classification task. Han et al. (2018) release a few-shot relation classification dataset “FewRel” and compare a couple of representative methods on it. These few-shot studies assume that, in the same domain, a part of the classes have limited samples, while other classes have adequate examples. In this work, we make a more challenging assumption that all classes in the target domain have only a couple of examples, and the training classes and testing classes are from different domains. Unifie"
2020.emnlp-main.750,P18-1013,0,0.0265814,"Missing"
2020.emnlp-main.750,K16-1028,0,0.17715,"Missing"
2020.emnlp-main.750,D19-1051,1,0.869105,"Missing"
2020.emnlp-main.750,D18-1207,1,0.894773,"Missing"
2020.emnlp-main.750,D18-1441,0,0.0178223,"on Figure 1: Procedure to generate synthetic training data. S is a set of source documents, T + is a set of semantically invariant text transformations, T − is a set of semantically variant text transformations, + is a positive label, − is a negative label. caused by poor generation were not labeled. The validation set consists of 931 examples, the test set contains 503 examples. The model outputs used for annotation were provided by the authors of papers: Hsu et al. (2018); Gehrmann et al. (2018); Jiang and Bansal (2018); Chen and Bansal (2018); See et al. (2017); Kry´sci´nski et al. (2018); Li et al. (2018); Pasunuru and Bansal (2018); Zhang et al. (2018); Guo et al. (2018). Effort was made to collect a larger set of annotations through crowdsourcing platforms, however the inter-annotator agreement and general quality of annotations was too low to be considered reliable. This aligns with the conclusions of (Falke et al., 2019), where the authors showed that for the task of factual consistency the interannotator agreement coefficient κ reached 0.75 only when 12 annotations were collected for each example. This in turn yields high annotations costs that our approach aims to circumvent. 3.3 Models"
2020.emnlp-main.750,D19-1387,0,0.0785939,"Missing"
2020.emnlp-main.750,D15-1044,0,0.0989058,"ul assistance in the process of verifying factual consistency. We also release a manually annotated dataset for factual consistency verification, code for training data generation, and trained model weights at https://github.com/salesforce/factCC. 1 Introduction The goal of text summarization is to transduce long documents into a shorter form that retains the most important aspects from the source document. Common approaches to summarization are extractive (Dorr et al., 2003; Nallapati et al., 2017) where models directly copy salient parts of the source document into the summary, abstractive (Rush et al., 2015; Paulus et al., 2017) where the important parts are paraphrased to form novel sentences, and hybrid (Gehrmann et al., 2018), combining the two methods by employing specialized extractive and abstractive components. Advancements in neural architectures (Cho et al., 2014; Bahdanau et al., 2015; Vaswani et al., 2017), transfer learning (McCann et al., 2017; Devlin et al., 2018), and availability of large-scale supervised datasets (Nallapati et al., 2016; Grusky et al., 2018) allowed deep learning-based approaches to dominate the field. State-of-the-art solutions utilize self-attentive Transforme"
2020.emnlp-main.750,P17-1099,0,0.142467,"w sent, −)} end for end for return D end function Figure 1: Procedure to generate synthetic training data. S is a set of source documents, T + is a set of semantically invariant text transformations, T − is a set of semantically variant text transformations, + is a positive label, − is a negative label. caused by poor generation were not labeled. The validation set consists of 931 examples, the test set contains 503 examples. The model outputs used for annotation were provided by the authors of papers: Hsu et al. (2018); Gehrmann et al. (2018); Jiang and Bansal (2018); Chen and Bansal (2018); See et al. (2017); Kry´sci´nski et al. (2018); Li et al. (2018); Pasunuru and Bansal (2018); Zhang et al. (2018); Guo et al. (2018). Effort was made to collect a larger set of annotations through crowdsourcing platforms, however the inter-annotator agreement and general quality of annotations was too low to be considered reliable. This aligns with the conclusions of (Falke et al., 2019), where the authors showed that for the task of factual consistency the interannotator agreement coefficient κ reached 0.75 only when 12 annotations were collected for each example. This in turn yields high annotations costs tha"
2020.emnlp-main.750,N18-1074,0,0.0784731,"Missing"
2020.emnlp-main.750,D19-1670,0,0.125476,"thors proposed a novel, dual-encoder architecture that in parallel encodes the source documents and all the facts contained in them. During generation, the decoder attends to both the encoded source and facts which, according to the authors, forces the output to be conditioned on the both inputs. Human evaluation showed that the proposed technique substantially lowered the number of errors in generated single-sentence summaries. The synthetic data generation process proposed as part of our approach is based on prior work done in the domains of data augmentation and weakly-supervised learning. Wei and Zou (2019) proposed an augmentation framework aimed at boosting performance of text classification models. The authors used 4 text transformations to synthesize data: synonym replacement, random insertion, random swap, random deletion, 9333 and showed increased performance of classifiers on 5 downstream tasks, both for convolutional and recurrent neural models. In (Sennrich et al., 2015; Edunov et al., 2018) the authors introduced and analyzed the effects of using backtranslation based data augmentation on the performance of machine translation models, while Iyyer et al. (2018) used the mentioned transf"
2020.emnlp-main.750,N18-1101,0,0.290348,"n’s corridors of power. Luckily, Japanese can sleep soundly in their beds tonight as the government’s top military official earnestly revealed that (...) Model generated claims Quadriplegic man Nyia Parler, 41, left in woods for days can not be extradited. Video game ”Space Invaders” was developed in Japan back in 1970. Table 1: Examples of factually incorrect claims output by summarization models. Green text highlights the support in the source documents for the generated claims, red text highlights the errors made by summarization models. checking. Current NLI datasets (Bowman et al., 2015; Williams et al., 2018) focus on classifying logical entailment between short, single sentence pairs, but verifying factual consistency requires the entire source document. Fact checking focuses on verifying facts against the whole of available knowledge, whereas factual consistency checking focuses on adherence of facts to information provided by a source document without guarantee that the information is true. We propose a novel, weakly-supervised BERTbased (Devlin et al., 2018) model for verifying factual consistency, and we add specialized modules that explain which portions of both the source document and gener"
2020.emnlp-main.750,D18-1089,0,0.16615,"ntence of the summary is verified against the entire body of the source document. 3.1 Training data Currently, there are no training datasets for factual consistency checking. Creating a large-scale, highquality dataset with supervision collected from human annotators is expensive and time consuming. We consider an alternative approach to acquiring training data that is highly scalable. Considering the state of summarization, in which the level of abstraction of generated summaries is low and models mostly paraphrase single sentences and short spans from the source (Kry´sci´nski et al., 2018; Zhang et al., 2018), we propose using a synthetic, weaklysupervised dataset for the task at hand. Our data creation method requires an unannotated collection of source documents in the same domain as the summarization models that are to be checked. Examples are created by first sampling single sentences, later referred to as claims, from the source documents. Claims then pass through a set of textual transformations that output novel sentences with both positive and negative labels. Though transformations are applied to single sentences, we found that, in keeping with our aforementioned observations of model-gen"
2020.emnlp-main.750,N19-1131,0,0.35857,"model outputs. Through human evaluation we show that the explanatory modules that augment our factual consistency model provide useful assistance to humans as they verify the factual consistency between a source document and generated summaries. Together with this manuscript we release a manually annotated dataset for factual consistency verification, code for training data generation, and trained model weights at https://github.com/salesforce/factCC. 2 Related Work This work builds on prior research in factual consistency in text summarization and natural language generation. Goodrich et al. (2019) proposed an automatic, model-dependent metric for evaluating the factual accuracy of generated text. Facts are represented as subject-relation-object triplets and factual accuracy is defined as the precision between facts extracted from the summary and source document. Despite positive results, the authors highlighted remaining challenges, such as its inability to adapt to negated relations or relation names expressed by synonyms. A parallel line of research focused on improving factual consistency of summarization models by exploring different architectural choices and strategies for both tr"
2020.findings-emnlp.303,P17-1061,0,0.0308137,"next token. To simulate the left-toright generation process, another attention mask is utilized for the decoder. In the attention mask for the decoder, tokens in the intent can only attend to intent tokens, while tokens in the utterance can attend to both the intent and all the left tokens in the utterance. For the first token z which holds composed latent information, it is only allowed to attend to itself due to the vanishing latent variable problem. The latent information can be overwhelmed by the information of other tokens when adapting VAE to natural language generators either for LSTM (Zhao et al., 2017) or transformers (Xia et al., 2020). To further increase the impact of the composed latent information z and alleviate the vanishing latent variable problem, we concatenate the token representations of z to all the other token embeddings output from the last transformer layer in the decoder. The hidden dimension increases to 2 × dh after the concatenation. To reduce the hidden dimension 2.4 Learning with contrastive loss Although the model can generate utterances for a given intent, such as “are there any alarms set for seven am” for “Alarm Query”, there are some negative utterances generated."
2020.findings-emnlp.303,D19-1670,0,0.0855476,"Missing"
2020.findings-emnlp.303,D18-1348,1,0.710184,"ts show that our proposed model achieves state-of-the-art performances on two real-world intent detection datasets. 1 Introduction Intelligent assistants have gained great popularity in recent years since they provide a new way for people to interact with the Internet conversationally (Hoy, 2018). However, it is still challenging to answer people’s diverse questions effectively. Among all the challenges, identifying user intentions from their spoken language is important and essential for all the downstream tasks. Most existing works (Hu et al., 2009; Xu and Sarikaya, 2013; Chen et al., 2016; Xia et al., 2018) formulate intent detection as a classification task and achieve high performance on pre-defined intents with sufficient labeled examples. With this ∗ Work was done when Congying was a research intern at Salesforce Research. ever-changing world, a realistic scenario is that we have imbalanced training data with existing manyshot intents and insufficient few-shot intents. Previous intent detection models (Yin, 2020; Yin et al., 2019) deteriorate drastically in discriminating the few-shot intents. To alleviate this scarce annotation problem, several methods (Wei and Zou, 2019; Malandrakis et al."
2020.findings-emnlp.303,D19-1404,0,0.0325446,"from their spoken language is important and essential for all the downstream tasks. Most existing works (Hu et al., 2009; Xu and Sarikaya, 2013; Chen et al., 2016; Xia et al., 2018) formulate intent detection as a classification task and achieve high performance on pre-defined intents with sufficient labeled examples. With this ∗ Work was done when Congying was a research intern at Salesforce Research. ever-changing world, a realistic scenario is that we have imbalanced training data with existing manyshot intents and insufficient few-shot intents. Previous intent detection models (Yin, 2020; Yin et al., 2019) deteriorate drastically in discriminating the few-shot intents. To alleviate this scarce annotation problem, several methods (Wei and Zou, 2019; Malandrakis et al., 2019; Yoo et al., 2019) have been proposed to augment the training data for low-resource spoken language understanding (SLU). Wei and Zou (2019) introduce simple data augmentation rules for language transformation like insert, delete and swap. Malandrakis et al. (2019) and Yoo et al. (2019) utilize variational autoencoders (Kingma and Welling, 2013) with simple LSTMs (Hochreiter and Schmidhuber, 1997) that have limited model capac"
2020.findings-emnlp.438,H94-1010,0,0.608493,"Missing"
2020.findings-emnlp.438,N19-1423,0,0.182714,"estion, the target DB structure, and the contextualization of both. State-of-the-art cross-DB text-to-SQL semantic parsers adopt the following design principles to 4870 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 4870–4888 c November 16 - 20, 2020. 2020 Association for Computational Linguistics address the aforementioned challenges. First, the question and schema representation should be contextualized with each other (Hwang et al., 2019; Guo et al., 2019; Wang et al., 2019; Yin et al., 2020). Second, large-scale pre-trained language models (LMs) such as BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019c) can significantly boost parsing accuracy by providing better representations of text and capturing long-term dependencies. Third, under data privacy constraints, leveraging available DB content can resolve ambiguities in the DB schema (Bogin et al., 2019b; Wang et al., 2019; Yin et al., 2020). Consider the second example in Figure 1, knowing “PLVDB” is a value of the field Journal.Name helps the model to generate the WHERE condition. We present BRIDGE, a powerful sequential textDB encoding framework assembling the three design principles mentioned above. BRIDGE"
2020.findings-emnlp.438,P16-1154,0,0.0204889,"” with it. To resolve this problem, we make use of anchor text to link value mentions in the question with the corresponding DB fields. We perform fuzzy string match between Q and the picklist of each field in the DB. The matched field values (anchor texts) where PV (yt ) is the softmax LSTM output distribution and X˜ is the length-(|Q |+ |S|) sequence that consists of only the question words and special tokens [T] and [C] from X. We use the attention weights of the last head to compute the pointing distribution2 . We extend the input state to the LSTM decoder using selective read proposed by Gu et al. (2016). 1 This approach may over-match anchor texts from fields other than those in the correct SQL query. Yet keeping the additional matches in X may provide useful information rather than noise. We plan to verify this in future work. 2 In practice we find this approach better than using just one head or using the average of multiple head weights. 4872 hS hQ … g … … fprimary … fforegin … ftype hX Bi-LSTM Bi-LSTM BERT CLS Show names… SEP T Properties … C Property Type Code V House V Apartment C … T Reference Property Types … Show names of properties that are either houses or apartments C Property Ty"
2020.findings-emnlp.438,P19-1444,0,0.279783,"area focus on training and testing the semantic parser on a single DB (Hemphill et al., 1990; Dahl et al., 1994; Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Dong and Lapata, 2016). However, DBs are widely used in many domains and developing a semantic parser for each individual DB is unlikely to scale in practice. More recently, large-scale datasets consisting of hundreds of DBs and the corresponding questionSQL pairs have been released (Yu et al., 2018; Zhong et al., 2017; Yu et al., 2019b,a) to encourage the development of semantic parsers that can work well across different DBs (Guo et al., 2019; Bogin et al., 2019b; Zhang et al., 2019; Wang et al., 2019; Suhr et al., 2020; Choi et al., 2020). The setup is challenging as it requires the model to interpret a question conditioned on a relational DB unseen during training and accurately express the question intent via SQL logic. Consider the two examples shown in Figure 1, both questions have the intent to count, but the corresponding SQL queries are drastically different due to differences in the target DB schema. As a result, cross-DB text-to-SQL semantic parsers cannot trivially memorize seen SQL patterns, but instead has to accurate"
2020.findings-emnlp.438,H90-1021,0,0.170618,"Missing"
2020.findings-emnlp.438,2020.acl-main.398,0,0.0570484,"lier version of this model is implemented within the Photon NLIDB model (Zeng et al., 2020), with up to one anchor text per field and an inferior anchor text matching algorithm. Joint Text-Table Representation and Pretraining BRIDGE is a general framework for jointly representing question, relational DB schema and DB values, and has the potential to be applied to a wide range of problems that requires joint textual-tabular data understanding. Recently, Yin et al. (2020) proposes TaBERT, an LM for jointly representing textual and tabular data pre-trained over millions of web tables. Similarly, Herzig et al. (2020) proposes TaPas, a pretrained text-table LM that supports arithmetic operations for weakly supervised table QA. Both TaBERT and TaPaSand supports arit focus on representing text with a single table. TaBERT was applied to Spider by encoding each table individually and modeling crosstable correlation through hierarchical attention. In comparison, BRIDGE serialized the relational DB schema and uses BERT to model cross-table dependencies. TaBERT adopts the “content snapshot” Train Dev Test #Q # SQL #DB 8,695 1,034 2,147 4,730 564 – 140 20 40 Table 2: Spider Dataset Statistics mechanism which retri"
2020.findings-emnlp.438,P82-1020,0,0.707721,"Missing"
2020.findings-emnlp.438,2021.ccl-1.108,0,0.161608,"Missing"
2020.findings-emnlp.438,N18-2074,0,0.053925,"e condition in Lemma 1 with little overhead in decoding speed. 3 Written: SELECT FROM WHERE GROUPBY HAVING ORDERBY LIMIT Exec: FROM WHERE GROUPBY HAVING SELECT ORDERBY LIMIT Table 1: The written order vs. execution order of all SQL clauses appeared in Spider. 3 Related Work Text-to-SQL Semantic Parsing Recently the field has witnessed a re-surge of interest for textto-SQL semantic parsing (Androutsopoulos et al., 1995), by virtue of the newly released large-scale datasets (Zhong et al., 2017; Yu et al., 2018; Zhang et al., 2019) and matured neural network modeling tools (Vaswani et al., 2017; Shaw et al., 2018; Devlin et al., 2019). While existing models have surpassed human performance on benchmarks consisting of single-table and simple SQL queries (Hwang et al., 2019; Lyu et al., 2020; He et al., 2019a), ample space of improvement still remains for the Spider benchmark which consists of relational DBs and complex SQL queries4 . Recent architectures proposed for this problem show increasing complexity in both the encoder and the decoder (Guo et al., 2019; Wang et al., 2019; Choi et al., 2020). Bogin et al. (2019a,b) proposed to encode relational DB schema as a graph and also use the graph structur"
2020.findings-emnlp.438,D18-1548,0,0.064112,"Missing"
2020.findings-emnlp.438,2020.acl-main.742,0,0.458229,"l et al., 1990; Dahl et al., 1994; Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Dong and Lapata, 2016). However, DBs are widely used in many domains and developing a semantic parser for each individual DB is unlikely to scale in practice. More recently, large-scale datasets consisting of hundreds of DBs and the corresponding questionSQL pairs have been released (Yu et al., 2018; Zhong et al., 2017; Yu et al., 2019b,a) to encourage the development of semantic parsers that can work well across different DBs (Guo et al., 2019; Bogin et al., 2019b; Zhang et al., 2019; Wang et al., 2019; Suhr et al., 2020; Choi et al., 2020). The setup is challenging as it requires the model to interpret a question conditioned on a relational DB unseen during training and accurately express the question intent via SQL logic. Consider the two examples shown in Figure 1, both questions have the intent to count, but the corresponding SQL queries are drastically different due to differences in the target DB schema. As a result, cross-DB text-to-SQL semantic parsers cannot trivially memorize seen SQL patterns, but instead has to accurately model the natural language question, the target DB structure, and the contex"
2020.findings-emnlp.438,P19-3007,0,0.0410546,"Missing"
2020.findings-emnlp.438,J84-3009,0,0.268833,"Missing"
2020.findings-emnlp.438,P17-1099,0,0.015613,"N | hS = [h , . . . , h , h , . . . , h are inserted into the question-schema representation X, succeeding the corresponding field names and separated by the special token [V]. If multiple values were matched for one field, we concatenate all of them in matching order (Figure 2). If a question mention is matched with values in multiple fields. We add all matches and let the model learn to resolve ambiguity1 . The anchor texts provide additional lexical clues for BERT to identify the corresponding mention in Q. And we name this mechanism “bridging”. 2.4 We use an LSTM-based pointer-generator (See et al., 2017) with multi-head attention (Vaswani et al., 2017) as the decoder. The decoder starts from the final state of the question encoder. At each step, the decoder performs one of the following actions: generating a token from the vocabulary V, copying a token from the question Q or copying a schema component from S. Mathematically, at each step t, given the decoder state st and the encoder representation [hQ ; hS ] ∈ R(|Q|+|S|)×n , we compute the multi-head attention as defined in Vaswani et al. (2017): + bg ) ]∈R |S|×n Decoder n o s t WU(h) (h j WV(h) )> ; αt(h)j = softmax e(h) √ tj j n/H |Q|+|S| X"
2020.findings-emnlp.438,P19-1010,0,0.0178259,"tecture of RAT-SQL is deep, consisting of 8 relational self-attention layers on top of BERT-large. In comparison, BRIDGE uses BERT combined with minimal subsequent layers. It uses a simple sequence decoder with search space-pruning heuristics and applies little abstraction to the SQL surface form. Its encoding architecture took inspiration from the table-aware BERT encoder proposed by Hwang et al. (2019), which is very effective for WikiSQL but has not been successful adapted to Spider. Yavuz et al. (2018) uses question-value matches to achieve high-precision condition predictions on WikiSQL. Shaw et al. (2019) also shows that value information is critical to the cross-DB semantic parsing tasks, yet the paper reported negative results augmenting an GNN encoder with BERT and the overall model performance is much below state-of-the-art. While previous work such as (Guo et al., 2019; Wang et al., 2019; Yin et al., 2020) use feature embeddings or relational attention layers to explicitly model schema linking, BRIDGE models the linking implicitly with BERT and lexical anchors. An earlier version of this model is implemented within the Photon NLIDB model (Zeng et al., 2020), with up to one anchor text per"
2020.findings-emnlp.438,D18-1197,0,0.0222403,"which effectively covers relations in the schema graph and its linking with the question. The overall architecture of RAT-SQL is deep, consisting of 8 relational self-attention layers on top of BERT-large. In comparison, BRIDGE uses BERT combined with minimal subsequent layers. It uses a simple sequence decoder with search space-pruning heuristics and applies little abstraction to the SQL surface form. Its encoding architecture took inspiration from the table-aware BERT encoder proposed by Hwang et al. (2019), which is very effective for WikiSQL but has not been successful adapted to Spider. Yavuz et al. (2018) uses question-value matches to achieve high-precision condition predictions on WikiSQL. Shaw et al. (2019) also shows that value information is critical to the cross-DB semantic parsing tasks, yet the paper reported negative results augmenting an GNN encoder with BERT and the overall model performance is much below state-of-the-art. While previous work such as (Guo et al., 2019; Wang et al., 2019; Yin et al., 2020) use feature embeddings or relational attention layers to explicitly model schema linking, BRIDGE models the linking implicitly with BERT and lexical anchors. An earlier version of"
2020.findings-emnlp.438,2020.acl-main.745,0,0.432253,"memorize seen SQL patterns, but instead has to accurately model the natural language question, the target DB structure, and the contextualization of both. State-of-the-art cross-DB text-to-SQL semantic parsers adopt the following design principles to 4870 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 4870–4888 c November 16 - 20, 2020. 2020 Association for Computational Linguistics address the aforementioned challenges. First, the question and schema representation should be contextualized with each other (Hwang et al., 2019; Guo et al., 2019; Wang et al., 2019; Yin et al., 2020). Second, large-scale pre-trained language models (LMs) such as BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019c) can significantly boost parsing accuracy by providing better representations of text and capturing long-term dependencies. Third, under data privacy constraints, leveraging available DB content can resolve ambiguities in the DB schema (Bogin et al., 2019b; Wang et al., 2019; Yin et al., 2020). Consider the second example in Figure 1, knowing “PLVDB” is a value of the field Journal.Name helps the model to generate the WHERE condition. We present BRIDGE, a powerful sequentia"
2020.findings-emnlp.438,D18-1193,0,0.423657,"Text-to-SQL semantic parsing addresses the problem of mapping natural language utterances to executable relational DB queries. Early work in this area focus on training and testing the semantic parser on a single DB (Hemphill et al., 1990; Dahl et al., 1994; Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Dong and Lapata, 2016). However, DBs are widely used in many domains and developing a semantic parser for each individual DB is unlikely to scale in practice. More recently, large-scale datasets consisting of hundreds of DBs and the corresponding questionSQL pairs have been released (Yu et al., 2018; Zhong et al., 2017; Yu et al., 2019b,a) to encourage the development of semantic parsers that can work well across different DBs (Guo et al., 2019; Bogin et al., 2019b; Zhang et al., 2019; Wang et al., 2019; Suhr et al., 2020; Choi et al., 2020). The setup is challenging as it requires the model to interpret a question conditioned on a relational DB unseen during training and accurately express the question intent via SQL logic. Consider the two examples shown in Figure 1, both questions have the intent to count, but the corresponding SQL queries are drastically different due to differences"
2020.findings-emnlp.438,D19-1204,1,0.840283,"Missing"
2020.findings-emnlp.438,P19-1443,1,0.8214,"es the problem of mapping natural language utterances to executable relational DB queries. Early work in this area focus on training and testing the semantic parser on a single DB (Hemphill et al., 1990; Dahl et al., 1994; Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Dong and Lapata, 2016). However, DBs are widely used in many domains and developing a semantic parser for each individual DB is unlikely to scale in practice. More recently, large-scale datasets consisting of hundreds of DBs and the corresponding questionSQL pairs have been released (Yu et al., 2018; Zhong et al., 2017; Yu et al., 2019b,a) to encourage the development of semantic parsers that can work well across different DBs (Guo et al., 2019; Bogin et al., 2019b; Zhang et al., 2019; Wang et al., 2019; Suhr et al., 2020; Choi et al., 2020). The setup is challenging as it requires the model to interpret a question conditioned on a relational DB unseen during training and accurately express the question intent via SQL logic. Consider the two examples shown in Figure 1, both questions have the intent to count, but the corresponding SQL queries are drastically different due to differences in the target DB schema. As a result,"
2020.findings-emnlp.438,2020.acl-demos.24,1,0.850485,"ndition predictions on WikiSQL. Shaw et al. (2019) also shows that value information is critical to the cross-DB semantic parsing tasks, yet the paper reported negative results augmenting an GNN encoder with BERT and the overall model performance is much below state-of-the-art. While previous work such as (Guo et al., 2019; Wang et al., 2019; Yin et al., 2020) use feature embeddings or relational attention layers to explicitly model schema linking, BRIDGE models the linking implicitly with BERT and lexical anchors. An earlier version of this model is implemented within the Photon NLIDB model (Zeng et al., 2020), with up to one anchor text per field and an inferior anchor text matching algorithm. Joint Text-Table Representation and Pretraining BRIDGE is a general framework for jointly representing question, relational DB schema and DB values, and has the potential to be applied to a wide range of problems that requires joint textual-tabular data understanding. Recently, Yin et al. (2020) proposes TaBERT, an LM for jointly representing textual and tabular data pre-trained over millions of web tables. Similarly, Herzig et al. (2020) proposes TaPas, a pretrained text-table LM that supports arithmetic op"
2020.nlp4convai-1.14,D14-1162,1,0.113795,"Missing"
2020.nlp4convai-1.14,N19-1170,0,0.0144113,"conversation increased. Figure 4 visualizes this metric both for our model and KVMemNet. In the case of both models, the consistency decreases as the chat history get longer, indicating that models have problems keeping track of their previous statements. When analyzing the linear trend we noticed that the decrease in performance is slower for the Sketch-Fill-A-R model. We hypothesize that this effect can be partially caused by the high diversity of sequences generated by the KVMemNet, which in turn affects the models ability to generate consistent conversation. Effect of question responses (See et al., 2019) note that for a conversation to be engaging, responses in chit-chat dialogue should be a mix of statements and questions, where the model inquires about certain traits and information of the other agent. We expand on this by evaluating the effect of a question’s presence in the response has on the ratings coming from the judges. The results are presented in Figure 4c. The study showed that there is a strong correlation between the model asking a question and the users rating the response as (a) KVMemNet (b) Sketch-Fill-A-R (c) Sketch-Fill-A-R: Human ratings vs question/no-question responses F"
2020.starsem-1.17,D18-1547,0,0.0637825,"Missing"
2020.starsem-1.17,D19-5817,0,0.0477496,"Missing"
2020.starsem-1.17,P19-1546,0,0.489185,"rs. To mitigate the above issues, recently, (Zhou and Small, 2019) introduced a question asking model to generate questions asking for values of eachdomain slot pair and a dynamic knowledge graph to learn relationships between the (domain, slot) pairs. (Rastogi et al., 2020) introduced a BERT-based model (Devlin et al., 2019) to strike a balance between the two methods by pre-defining categorical and non-categorical slots. However, more studies are needed to know which slots are better handled by either of the two slot types, and the way to use the pre-trained models is not well investigated (Lee et al., 2019; Gao et al., 2019b; Rastogi et al., 2020). Inspired by the task-oriented dialog schema design in (Rastogi et al., 2020) and the recent successful experience in locating text spans in machine reading comprehensions (Gao et al., 2019b; Asai et al., 2019). we design a simple yet effective DualStrategy Dialog State Tracking model (DS-DST), which adapts a single BERT question answering model to jointly handle both the categorical and non-categorical slots, and different with previous approaches on multi-domain DST, we enable the model with direct interactions between dialog context and the slot. W"
2020.starsem-1.17,N19-1423,0,0.580859,"ctly from the input source using a copy mechanism without requiring an ontology, e.g., learning span matching with start and end positions in the dialog context. However, it is nontrivial to handle situations where values do not appear in the dialog context or have various descriptions by users. To mitigate the above issues, recently, (Zhou and Small, 2019) introduced a question asking model to generate questions asking for values of eachdomain slot pair and a dynamic knowledge graph to learn relationships between the (domain, slot) pairs. (Rastogi et al., 2020) introduced a BERT-based model (Devlin et al., 2019) to strike a balance between the two methods by pre-defining categorical and non-categorical slots. However, more studies are needed to know which slots are better handled by either of the two slot types, and the way to use the pre-trained models is not well investigated (Lee et al., 2019; Gao et al., 2019b; Rastogi et al., 2020). Inspired by the task-oriented dialog schema design in (Rastogi et al., 2020) and the recent successful experience in locating text spans in machine reading comprehensions (Gao et al., 2019b; Asai et al., 2019). we design a simple yet effective DualStrategy Dialog Sta"
2020.starsem-1.17,P18-1133,0,0.0548056,"dialog contexts, SUMBT (Lee et al., 2019) employs BERT to extract representations of candidate values, and BERT-DST (Rastogi et al., 2020) adopts BERT to encode the inputs of the user turn as well as the previous system turn. Different from these approaches where the dialog context and domain-slot pairs are usually separately encoded, we employ strong interactions to encode them. 1 . Moreover, We investigate and provide insights to decide slot types and conduct a comprehensive analysis of the popular MultiWOZ datasets. Another direction for multi-domain DST is based on generative approaches (Lei et al., 2018; Wu et al., 2019; Le et al., 2020) which generate slot values without relying on fixed vocabularies and spans. However, such generative methods suffer from generating ill-formatted strings (e.g., repeated words) upon long strings, which is common in DST. For example, the hotel address may be long and a small difference makes the whole dialog state tracking incorrect. By contrast, both the categorical (picklist-based) and non-categorical (span-based) methods can rely on existing strings rather than generating them. 3 pose a dual strategy model with direct interactions between dialog context an"
2020.starsem-1.17,P19-1617,0,0.0221163,"turn and Xt has {x1 , . . . , xm } tokens. Our goal is to predict the values for all the domain-slot pairs in S. Here we assume that M domain-slot pairs in S are treated as non-categorical slots, and the remaining N − M pairs as categorical slots. Each categorical slot has L possible candidate values (picklist), i.e., {V1 , . . . , VL }, where L is the size of the picklist, and each value has {v1 , . . . , vc } tokens. Bearing these notations in mind, we then pro1 Recent work on question answering has shown that the joint encoding of query-context pairs is crucial to achieving high accuracy (Qiu et al., 2019; Asai et al., 2019) Slot-Context Encoder 3.2 Slot-Gate Classification As there are many domain-slot pairs in multidomain dialogues, it is nontrivial to correctly predict whether a domain-slot pair appears at each turn of the dialogue. Here we follow (Wu et al., 2019; Xu and Hu, 2018) and design a slot gate classification module for our neural network. Specifically, at the tth turn, the classifier makes a decision among {none, dontcare, prediction}, where none denotes that a domain-slot pair is not mentioned or the value is ‘none’ at this turn, dontcare implies that the user can accept any val"
2020.starsem-1.17,W19-5932,0,0.141048,"Missing"
2020.starsem-1.17,W14-4337,0,0.149343,"ultiWOZ 2.0 (Budzianowski et al., 2018) and competitive performance on MultiWOZ 2.1 (Eric et al., 2019). Our model also performs robustly across the two different settings. • We conducted a comprehensive error analysis on the dataset, including the effects of the dual strategy for each slot, to facilitate future research. 2 Related Work Multi-domain DST, which tracks dialog states in complicated conversations across multiple domains with many slots, has been a hot research topic during the past few years, along with the development of Dialogue State Tracking Challenges (Williams et al., 2013; Henderson et al., 2014a,b; Kim et al., 2016, 2017, 2019). Traditional approaches usually rely on hand-crafted features or domain-specific lexicon (Henderson et al., 2014c; Wen et al., 2016), making them difficult to be adapted to new domains. In addition, these approaches require a pre-defined full ontology, in which the values of a slot are constrained by a set of candidate values (Ramadan et al., 2018; Liu and Lane, 2017; Zhong et al., 2018; Lee et al., 2019; Chen et al., 2020). To tackle these issues, several methods have been proposed to extract slot values through span matching with start and end positions in"
2020.starsem-1.17,P18-2069,0,0.104889,"Missing"
2020.starsem-1.17,W14-4340,0,0.0770855,"ultiWOZ 2.0 (Budzianowski et al., 2018) and competitive performance on MultiWOZ 2.1 (Eric et al., 2019). Our model also performs robustly across the two different settings. • We conducted a comprehensive error analysis on the dataset, including the effects of the dual strategy for each slot, to facilitate future research. 2 Related Work Multi-domain DST, which tracks dialog states in complicated conversations across multiple domains with many slots, has been a hot research topic during the past few years, along with the development of Dialogue State Tracking Challenges (Williams et al., 2013; Henderson et al., 2014a,b; Kim et al., 2016, 2017, 2019). Traditional approaches usually rely on hand-crafted features or domain-specific lexicon (Henderson et al., 2014c; Wen et al., 2016), making them difficult to be adapted to new domains. In addition, these approaches require a pre-defined full ontology, in which the values of a slot are constrained by a set of candidate values (Ramadan et al., 2018; Liu and Lane, 2017; Zhong et al., 2018; Lee et al., 2019; Chen et al., 2020). To tackle these issues, several methods have been proposed to extract slot values through span matching with start and end positions in"
2020.starsem-1.17,D19-1196,0,0.160931,"Missing"
2020.starsem-1.17,2020.acl-main.563,0,0.573832,"Missing"
2020.starsem-1.17,W13-4065,0,0.0277001,"s state of the art on MultiWOZ 2.0 (Budzianowski et al., 2018) and competitive performance on MultiWOZ 2.1 (Eric et al., 2019). Our model also performs robustly across the two different settings. • We conducted a comprehensive error analysis on the dataset, including the effects of the dual strategy for each slot, to facilitate future research. 2 Related Work Multi-domain DST, which tracks dialog states in complicated conversations across multiple domains with many slots, has been a hot research topic during the past few years, along with the development of Dialogue State Tracking Challenges (Williams et al., 2013; Henderson et al., 2014a,b; Kim et al., 2016, 2017, 2019). Traditional approaches usually rely on hand-crafted features or domain-specific lexicon (Henderson et al., 2014c; Wen et al., 2016), making them difficult to be adapted to new domains. In addition, these approaches require a pre-defined full ontology, in which the values of a slot are constrained by a set of candidate values (Ramadan et al., 2018; Liu and Lane, 2017; Zhong et al., 2018; Lee et al., 2019; Chen et al., 2020). To tackle these issues, several methods have been proposed to extract slot values through span matching with sta"
2020.starsem-1.17,2020.emnlp-main.66,1,0.894296,"Missing"
2020.starsem-1.17,P19-1078,1,0.940578,"sed approaches (Ramadan et al., 2018; Zhong et al., 2018; Chen et al., 2020) require full access to the pre-defined ontol154 Proceedings of the Ninth Joint Conference on Lexical and Computational Semantics (*SEM), pages 154–167 Barcelona, Spain (Online), December 12–13, 2020 ogy to perform classification over the candidatevalue list. However, in practice, we may not have access to an ontology or only have partial ontology in the industry. Even if a full ontology exists, it is computationally expensive to enumerate all the values when the full ontology for some slots is very large and diverse (Wu et al., 2019; Xu and Hu, 2018). The ontology-free approaches (Gao et al., 2019b; Xu and Hu, 2018) find slot values directly from the input source using a copy mechanism without requiring an ontology, e.g., learning span matching with start and end positions in the dialog context. However, it is nontrivial to handle situations where values do not appear in the dialog context or have various descriptions by users. To mitigate the above issues, recently, (Zhou and Small, 2019) introduced a question asking model to generate questions asking for values of eachdomain slot pair and a dynamic knowledge graph to l"
2020.starsem-1.17,P18-1134,0,0.241203,"onsists of a set of < domain, slot, value &gt; triplets, and DST aims to track all the states accumulated across the conversational turns. Fig. 1 shows a dialogue with corresponding annotated turn states. Introduction ∗ Work done while the first author was an intern at Salesforce Research. † Corresponding author. This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http: //creativecommons.org/licenses/by/4.0/. Traditional approaches for DST usually rely on hand-crafted features and domain-specific lexicon, and can be categorized into two classes (Xu and Hu, 2018; Gao et al., 2019b; Ramadan et al., 2018; Zhong et al., 2018): i.e., ontology-based and ontology-free. The ontology-based approaches (Ramadan et al., 2018; Zhong et al., 2018; Chen et al., 2020) require full access to the pre-defined ontol154 Proceedings of the Ninth Joint Conference on Lexical and Computational Semantics (*SEM), pages 154–167 Barcelona, Spain (Online), December 12–13, 2020 ogy to perform classification over the candidatevalue list. However, in practice, we may not have access to an ontology or only have partial ontology in the industry. Even if a full ontology exists, it is"
2020.starsem-1.17,2020.nlp4convai-1.13,1,0.866872,"Missing"
2020.starsem-1.17,P18-1135,1,0.945962,"DST aims to track all the states accumulated across the conversational turns. Fig. 1 shows a dialogue with corresponding annotated turn states. Introduction ∗ Work done while the first author was an intern at Salesforce Research. † Corresponding author. This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http: //creativecommons.org/licenses/by/4.0/. Traditional approaches for DST usually rely on hand-crafted features and domain-specific lexicon, and can be categorized into two classes (Xu and Hu, 2018; Gao et al., 2019b; Ramadan et al., 2018; Zhong et al., 2018): i.e., ontology-based and ontology-free. The ontology-based approaches (Ramadan et al., 2018; Zhong et al., 2018; Chen et al., 2020) require full access to the pre-defined ontol154 Proceedings of the Ninth Joint Conference on Lexical and Computational Semantics (*SEM), pages 154–167 Barcelona, Spain (Online), December 12–13, 2020 ogy to perform classification over the candidatevalue list. However, in practice, we may not have access to an ontology or only have partial ontology in the industry. Even if a full ontology exists, it is computationally expensive to enumerate all the values when the"
2021.findings-emnlp.424,N19-3002,0,0.0216482,"2020) gives an automatic evaluation of toxicity using generations from different language models using a set of webtext prompts. (Gehman et al., 2020) also tests methods for mitigating toxicity, and finds that applying PPLM was more effective than simpler decoding-based detoxification methods such as swear word filters. Xu et al. (2020) develop a human in the loop method for adversarially probing toxic responses in conversational agents, and train a model to give preset responses when encountering potentially unsafe probes. Other work has focused on removing gender bias from language models (Bordia and Bowman, 2019; Dinan et al., 2020; Bolukbasi et al., 2016). Related to the problem of addressing toxicity in generation is toxicity detection, which can be performed using the Perspective API or using a classifier trained on a labelled toxicity dataset such as the Jigsaw Toxic Comment Classification Dataset (Borkan et al., 2019). Toxicity detection is difficult as toxicity labelling is subjective and often has poor annotator agreement (Waseem, 2016; Ross et al., 2017). Additionally, existing toxicity classifiers are often biased in that they overestimate the toxicity of text that mentions sexual orientatio"
2021.findings-emnlp.424,P19-1102,0,0.0611326,"Missing"
2021.findings-emnlp.424,W18-2706,0,0.0240402,"lace of x1 . This works well when GeDi is initialized as a pretrained language model, as the model will have learned embeddings for many topics during its pretraining that can be used as zero-shot control codes. 4 Related Work Methods for controlling text generation can be categorized broadly into two categories: training or finetuning a model directly for controllable gen&lt;false&gt; &lt;science&gt; T-rex achieved its massive eration (Chan et al., 2021; Madotto et al., 2020; size due to an enormous growth spurt during its Keskar et al., 2019; Ziegler et al., 2019; Rajani adolescent years. et al., 2019; Fan et al., 2018; Ficler and Goldberg, 4932 2017; Yu et al., 2017; Hu et al., 2017) or using a discriminator to guide decoding (Ghazvininejad et al., 2017; Holtzman et al., 2018; Dathathri et al., 2020). Keskar et al. (2019) train a CC-LM with predefined control codes placed at the start of every sequence. GeDi also uses CC-LMs, but instead of generating from them directly, GeDi uses them as discriminators to guide decoding from another language model. This is much more computationally efficient than previous methods for discriminator guided decoding. Holtzman et al. (2018) apply discriminators to re-weight a"
2021.findings-emnlp.424,W17-4912,0,0.0489744,"Missing"
2021.findings-emnlp.424,N18-2070,0,0.0250397,"ases, since PPLM updates the previously stored keys and values. GeDi in comparison only adds constant overhead that is independent of the size of the base LM, and this constant will be minimal if the GeDi is significantly smaller than the base LM. GeDi also relates to the rational speech acts framework for computational pragmatics (Frank and Goodman, 2012; Goodman and Stuhlmüller, 2013) where a “listener” model and a “speaker” model interactively generate a sequence such that the listener can recover the input. GeDi most closely relates to distractor based pragmatics (Andreas and Klein, 2016; Cohn-Gordon et al., 2018; Shen et al., 2019), where a single model processes a true input and a distractor input, and uses Bayes rule to produce text that fits the true input but not the distractor input. GeDi differs from previous pragmatics based approaches in that it trains a separate class-conditional language model (which acts as the listener) on a single attribute, allowing that attribute to be isolated, and uses it to guide generation from a separate language model (which acts as the speaker). ation. RealToxictyPrompts (Gehman et al., 2020) gives an automatic evaluation of toxicity using generations from diffe"
2021.findings-emnlp.424,2020.findings-emnlp.301,0,0.451059,"y relates to distractor based pragmatics (Andreas and Klein, 2016; Cohn-Gordon et al., 2018; Shen et al., 2019), where a single model processes a true input and a distractor input, and uses Bayes rule to produce text that fits the true input but not the distractor input. GeDi differs from previous pragmatics based approaches in that it trains a separate class-conditional language model (which acts as the listener) on a single attribute, allowing that attribute to be isolated, and uses it to guide generation from a separate language model (which acts as the speaker). ation. RealToxictyPrompts (Gehman et al., 2020) gives an automatic evaluation of toxicity using generations from different language models using a set of webtext prompts. (Gehman et al., 2020) also tests methods for mitigating toxicity, and finds that applying PPLM was more effective than simpler decoding-based detoxification methods such as swear word filters. Xu et al. (2020) develop a human in the loop method for adversarially probing toxic responses in conversational agents, and train a model to give preset responses when encountering potentially unsafe probes. Other work has focused on removing gender bias from language models (Bordia"
2021.findings-emnlp.424,P18-1152,0,0.146388,"downstream applications. Existing approaches to controlling LMs have limitations. Class-conditional LMs (CC-LMs) such as CTRL (Keskar et al., 2019) attempt to control text generation by conditioning on a control code, which is an attribute variable representing a data source. However, using a specific control code can reduce sample diversity across prompts, as samples will generally resemble the data source of the control code. Another approach for controlling LMs is to use discriminators to guide decoding, but existing methods to do this are very computationally intensive. Weighted decoding (Holtzman et al., 2018) requires feeding candidate next tokens into a discriminator, and thus scales linearly in computation with the number of tokens to be re-weighted. Plug and Play LM (Dathathri et al., 2020, PPLM) applies up to 10 updates to the generating LM’s latent states per time step using gradients from a discriminator, also making it many times slower than generating from the LM directly. We present GeDi1,2 as a significantly more efficient algorithm for discriminator guided decoding. Our proposed method uses class-conditional LMs as generative discriminators (GeDis) to steer language generation towards d"
2021.findings-emnlp.424,2020.acl-main.487,0,0.0197636,"roblem of addressing toxicity in generation is toxicity detection, which can be performed using the Perspective API or using a classifier trained on a labelled toxicity dataset such as the Jigsaw Toxic Comment Classification Dataset (Borkan et al., 2019). Toxicity detection is difficult as toxicity labelling is subjective and often has poor annotator agreement (Waseem, 2016; Ross et al., 2017). Additionally, existing toxicity classifiers are often biased in that they overestimate the toxicity of text that mentions sexual orientations or racial minorities (Dixon et al., 2018; Sap et al., 2019; Hutchinson et al., 2020). 5 Experiments We experiment with GeDi-guided decoding for sentiment, detoxification, and topic control. We finetune GPT2-medium (345M parameter) (Radford et al., 2019) using the loss in Equation (3) with control codes specific to each task to form a classconditional language model. We use these CC-LMs as GeDis to guide generation from GPT2-XL (1.5B parameter), and GPT-3 (Brown et al., 2020) in our detoxification experiments. All experiments were performed using adaptations of Huggingface Transformers (Wolf et al., 2020). We include experiments with greedy decoding with a repetition penalty ("
2021.findings-emnlp.424,2020.findings-emnlp.219,0,0.0308097,"he sequence. This also makes it possible to form new control codes zero-shot; a new topic word that was never seen before in training can be chosen in place of x1 . This works well when GeDi is initialized as a pretrained language model, as the model will have learned embeddings for many topics during its pretraining that can be used as zero-shot control codes. 4 Related Work Methods for controlling text generation can be categorized broadly into two categories: training or finetuning a model directly for controllable gen&lt;false&gt; &lt;science&gt; T-rex achieved its massive eration (Chan et al., 2021; Madotto et al., 2020; size due to an enormous growth spurt during its Keskar et al., 2019; Ziegler et al., 2019; Rajani adolescent years. et al., 2019; Fan et al., 2018; Ficler and Goldberg, 4932 2017; Yu et al., 2017; Hu et al., 2017) or using a discriminator to guide decoding (Ghazvininejad et al., 2017; Holtzman et al., 2018; Dathathri et al., 2020). Keskar et al. (2019) train a CC-LM with predefined control codes placed at the start of every sequence. GeDi also uses CC-LMs, but instead of generating from them directly, GeDi uses them as discriminators to guide decoding from another language model. This is muc"
2021.findings-emnlp.424,P19-1487,1,0.87302,"Missing"
2021.findings-emnlp.424,P19-1163,0,0.0186535,". Related to the problem of addressing toxicity in generation is toxicity detection, which can be performed using the Perspective API or using a classifier trained on a labelled toxicity dataset such as the Jigsaw Toxic Comment Classification Dataset (Borkan et al., 2019). Toxicity detection is difficult as toxicity labelling is subjective and often has poor annotator agreement (Waseem, 2016; Ross et al., 2017). Additionally, existing toxicity classifiers are often biased in that they overestimate the toxicity of text that mentions sexual orientations or racial minorities (Dixon et al., 2018; Sap et al., 2019; Hutchinson et al., 2020). 5 Experiments We experiment with GeDi-guided decoding for sentiment, detoxification, and topic control. We finetune GPT2-medium (345M parameter) (Radford et al., 2019) using the loss in Equation (3) with control codes specific to each task to form a classconditional language model. We use these CC-LMs as GeDis to guide generation from GPT2-XL (1.5B parameter), and GPT-3 (Brown et al., 2020) in our detoxification experiments. All experiments were performed using adaptations of Huggingface Transformers (Wolf et al., 2020). We include experiments with greedy decoding w"
2021.findings-emnlp.424,N19-1410,0,0.0232002,"he previously stored keys and values. GeDi in comparison only adds constant overhead that is independent of the size of the base LM, and this constant will be minimal if the GeDi is significantly smaller than the base LM. GeDi also relates to the rational speech acts framework for computational pragmatics (Frank and Goodman, 2012; Goodman and Stuhlmüller, 2013) where a “listener” model and a “speaker” model interactively generate a sequence such that the listener can recover the input. GeDi most closely relates to distractor based pragmatics (Andreas and Klein, 2016; Cohn-Gordon et al., 2018; Shen et al., 2019), where a single model processes a true input and a distractor input, and uses Bayes rule to produce text that fits the true input but not the distractor input. GeDi differs from previous pragmatics based approaches in that it trains a separate class-conditional language model (which acts as the listener) on a single attribute, allowing that attribute to be isolated, and uses it to guide generation from a separate language model (which acts as the speaker). ation. RealToxictyPrompts (Gehman et al., 2020) gives an automatic evaluation of toxicity using generations from different language models"
2021.findings-emnlp.424,D13-1170,1,0.01707,"Missing"
2021.findings-emnlp.424,W16-5618,0,0.0288337,"in a model to give preset responses when encountering potentially unsafe probes. Other work has focused on removing gender bias from language models (Bordia and Bowman, 2019; Dinan et al., 2020; Bolukbasi et al., 2016). Related to the problem of addressing toxicity in generation is toxicity detection, which can be performed using the Perspective API or using a classifier trained on a labelled toxicity dataset such as the Jigsaw Toxic Comment Classification Dataset (Borkan et al., 2019). Toxicity detection is difficult as toxicity labelling is subjective and often has poor annotator agreement (Waseem, 2016; Ross et al., 2017). Additionally, existing toxicity classifiers are often biased in that they overestimate the toxicity of text that mentions sexual orientations or racial minorities (Dixon et al., 2018; Sap et al., 2019; Hutchinson et al., 2020). 5 Experiments We experiment with GeDi-guided decoding for sentiment, detoxification, and topic control. We finetune GPT2-medium (345M parameter) (Radford et al., 2019) using the loss in Equation (3) with control codes specific to each task to form a classconditional language model. We use these CC-LMs as GeDis to guide generation from GPT2-XL (1.5B"
2021.naacl-main.37,D19-1052,0,0.0377301,"Missing"
2021.naacl-main.37,2020.acl-main.708,0,0.0149657,"construction framework effectively merged heterogeneous sources from open domain semantic parsing and spoken dialogue systems by utilizing techniques including tree ontology annotation, question-answer pair to declarative sentence conversion and predicate unification, all with minimum post-editing. We present systematic evaluation on DART as well as new state-of-the-art results on WebNLG 2017 to show that DART (1) poses new challenges to existing data-to-text datasets and (2) facilitates out-of-domain generalization. Our data and code can be found at https://github. com/Yale-LILY/dart. 2017; Chen et al., 2020a; Parikh et al., 2020). This flat structure is not powerful enough to encode rich semantic relationships in the ontology of the structured data, especially tables, whose representation can be further improved with these semantic knowledge. Second, some of the datasets only focus on a small number of domains or knowledge graphs, therefore providing limited number of predicates and data ontologies. For example, E2E (Novikova et al., 2017b) on restaurants and WebNLG (Gardent et al., 2017) on 15 categories from DBPedia. Furthermore, some of them only have loose alignments between data input and s"
2021.naacl-main.37,2020.findings-emnlp.190,0,0.0356274,"Missing"
2021.naacl-main.37,2020.acl-main.18,0,0.0285505,"construction framework effectively merged heterogeneous sources from open domain semantic parsing and spoken dialogue systems by utilizing techniques including tree ontology annotation, question-answer pair to declarative sentence conversion and predicate unification, all with minimum post-editing. We present systematic evaluation on DART as well as new state-of-the-art results on WebNLG 2017 to show that DART (1) poses new challenges to existing data-to-text datasets and (2) facilitates out-of-domain generalization. Our data and code can be found at https://github. com/Yale-LILY/dart. 2017; Chen et al., 2020a; Parikh et al., 2020). This flat structure is not powerful enough to encode rich semantic relationships in the ontology of the structured data, especially tables, whose representation can be further improved with these semantic knowledge. Second, some of the datasets only focus on a small number of domains or knowledge graphs, therefore providing limited number of predicates and data ontologies. For example, E2E (Novikova et al., 2017b) on restaurants and WebNLG (Gardent et al., 2017) on 15 categories from DBPedia. Furthermore, some of them only have loose alignments between data input and s"
2021.naacl-main.37,P19-1483,0,0.0424379,"Missing"
2021.naacl-main.37,W19-8652,0,0.0305433,"Missing"
2021.naacl-main.37,L18-1544,0,0.0274201,"e structured data, especially tables, whose representation can be further improved with these semantic knowledge. Second, some of the datasets only focus on a small number of domains or knowledge graphs, therefore providing limited number of predicates and data ontologies. For example, E2E (Novikova et al., 2017b) on restaurants and WebNLG (Gardent et al., 2017) on 15 categories from DBPedia. Furthermore, some of them only have loose alignments between data input and sentence due to the nature of the task (Wiseman et al., 2017) and the automatic generation procedure (Vougiouklis et al., 2018; Elsahar et al., 2018). To address some of these issues and to encourage further research in natural language generation from structured data, we introduce DART, a large and open-domain structured DAta-Record-to-Text generation corpus. The goal of DART is to harvest the diverse predicates occurred in Wikipedia tables, which is significantly richer than those defined in the domain specific ontologies E2E and WebNLG were built on (Table 2). We also intro1 Introduction duce a novel tree ontology annotation approach on Automatically generating textual descriptions from tables, which converts a flat table schema into a"
2021.naacl-main.37,D19-1428,0,0.0202842,"generating textual descriptions from tables, which converts a flat table schema into a structured data improves the accessibility of knowl- tree structured semantic frame. The tree ontology edge bases to lay users. Such applications include reflects the core and auxiliary relations in the table explaining data records to non-experts (Cawsey schema, and naturally occurs across many domains. et al., 1997), writing sports news (Chen and As a result, DART provides high-quality sentence Mooney, 2008), summarizing information in mul- annotations to tree structured semantic frames extiple documents (Fan et al., 2019), and generating tracted from various data sources, including Wikdialogue responses (Wen et al., 2015). iSQL (Zhong et al., 2017) and WikiTableQuestions While significant progress has been made in this (Pasupat and Liang, 2015), two open-domain quesfield, there are still several issues with existing tion answering datasets, as well as E2E (Novikova Data-to-Text datasets. First, they adopt a flat ontol- et al., 2017b) and WebNLG (Gardent et al., 2017) ogy structure of the data, such as slot-value pairs (Figure 1). We evaluated several state-of-the-art for data records (Lebret et al., 2016; Novi"
2021.naacl-main.37,W17-3518,0,0.177339,"itates out-of-domain generalization. Our data and code can be found at https://github. com/Yale-LILY/dart. 2017; Chen et al., 2020a; Parikh et al., 2020). This flat structure is not powerful enough to encode rich semantic relationships in the ontology of the structured data, especially tables, whose representation can be further improved with these semantic knowledge. Second, some of the datasets only focus on a small number of domains or knowledge graphs, therefore providing limited number of predicates and data ontologies. For example, E2E (Novikova et al., 2017b) on restaurants and WebNLG (Gardent et al., 2017) on 15 categories from DBPedia. Furthermore, some of them only have loose alignments between data input and sentence due to the nature of the task (Wiseman et al., 2017) and the automatic generation procedure (Vougiouklis et al., 2018; Elsahar et al., 2018). To address some of these issues and to encourage further research in natural language generation from structured data, we introduce DART, a large and open-domain structured DAta-Record-to-Text generation corpus. The goal of DART is to harvest the diverse predicates occurred in Wikipedia tables, which is significantly richer than those defi"
2021.naacl-main.37,2020.acl-main.703,0,0.0345274,"Missing"
2021.naacl-main.37,P09-1011,0,0.106929,"Missing"
2021.naacl-main.37,2020.findings-emnlp.165,0,0.0746934,"Missing"
2021.naacl-main.37,N19-1236,0,0.0269971,"Missing"
2021.naacl-main.37,D17-1238,0,0.0117771,"ges to existing data-to-text datasets and (2) facilitates out-of-domain generalization. Our data and code can be found at https://github. com/Yale-LILY/dart. 2017; Chen et al., 2020a; Parikh et al., 2020). This flat structure is not powerful enough to encode rich semantic relationships in the ontology of the structured data, especially tables, whose representation can be further improved with these semantic knowledge. Second, some of the datasets only focus on a small number of domains or knowledge graphs, therefore providing limited number of predicates and data ontologies. For example, E2E (Novikova et al., 2017b) on restaurants and WebNLG (Gardent et al., 2017) on 15 categories from DBPedia. Furthermore, some of them only have loose alignments between data input and sentence due to the nature of the task (Wiseman et al., 2017) and the automatic generation procedure (Vougiouklis et al., 2018; Elsahar et al., 2018). To address some of these issues and to encourage further research in natural language generation from structured data, we introduce DART, a large and open-domain structured DAta-Record-to-Text generation corpus. The goal of DART is to harvest the diverse predicates occurred in Wikipedia ta"
2021.naacl-main.37,P19-1195,0,0.0491104,"Missing"
2021.naacl-main.37,W07-2315,0,0.135635,"Missing"
2021.naacl-main.37,D19-1314,0,0.0450819,"Missing"
2021.naacl-main.37,2020.acl-main.704,0,0.020086,"Missing"
2021.naacl-main.37,W17-5525,0,0.0286406,"ges to existing data-to-text datasets and (2) facilitates out-of-domain generalization. Our data and code can be found at https://github. com/Yale-LILY/dart. 2017; Chen et al., 2020a; Parikh et al., 2020). This flat structure is not powerful enough to encode rich semantic relationships in the ontology of the structured data, especially tables, whose representation can be further improved with these semantic knowledge. Second, some of the datasets only focus on a small number of domains or knowledge graphs, therefore providing limited number of predicates and data ontologies. For example, E2E (Novikova et al., 2017b) on restaurants and WebNLG (Gardent et al., 2017) on 15 categories from DBPedia. Furthermore, some of them only have loose alignments between data input and sentence due to the nature of the task (Wiseman et al., 2017) and the automatic generation procedure (Vougiouklis et al., 2018; Elsahar et al., 2018). To address some of these issues and to encourage further research in natural language generation from structured data, we introduce DART, a large and open-domain structured DAta-Record-to-Text generation corpus. The goal of DART is to harvest the diverse predicates occurred in Wikipedia ta"
2021.naacl-main.37,P17-2002,0,0.0678008,"Missing"
2021.naacl-main.37,2020.emnlp-main.89,0,0.0169662,"ork effectively merged heterogeneous sources from open domain semantic parsing and spoken dialogue systems by utilizing techniques including tree ontology annotation, question-answer pair to declarative sentence conversion and predicate unification, all with minimum post-editing. We present systematic evaluation on DART as well as new state-of-the-art results on WebNLG 2017 to show that DART (1) poses new challenges to existing data-to-text datasets and (2) facilitates out-of-domain generalization. Our data and code can be found at https://github. com/Yale-LILY/dart. 2017; Chen et al., 2020a; Parikh et al., 2020). This flat structure is not powerful enough to encode rich semantic relationships in the ontology of the structured data, especially tables, whose representation can be further improved with these semantic knowledge. Second, some of the datasets only focus on a small number of domains or knowledge graphs, therefore providing limited number of predicates and data ontologies. For example, E2E (Novikova et al., 2017b) on restaurants and WebNLG (Gardent et al., 2017) on 15 categories from DBPedia. Furthermore, some of them only have loose alignments between data input and sentence due to the natu"
2021.naacl-main.37,T87-1019,0,0.795136,"Missing"
D11-1014,D08-1083,0,0.0210461,"ing one via heuristics (Kim and Hovy, 2007; Esuli and Sebastiani, 2007), manual annotation (Das and Chen, 2001) or machine learning techniques (Turney, 2002). In contrast, we do not require an initial or constructed sentiment lexicon of positive and negative words. In fact, when training our approach on documents or sentences, it jointly learns such lexica for both single words and n-grams (see Table 5). (Mao and Lebanon, 2007) propose isotonic conditional random fields and differentiate between local, sentence-level and global, document-level sentiment. The work of (Polanyi and Zaenen, 2006; Choi and Cardie, 2008) focuses on manually constructing several lexica and rules for both polar words and related content-word negators, such as “prevent cancer”, where prevent reverses the negative polarity of cancer. Like our approach they capture compositional semantics. However, our model does so without manually constructing any rules or lexica. Recently, (Velikovich et al., 2010) showed how to use a seed lexicon and a graph propagation framework to learn a larger sentiment lexicon that also includes polar multi-word phrases such as “once in a life time”. While our method can also learn multiword phrases it do"
D11-1014,P07-1054,0,0.0495924,"features including dependency trees. They also show improvements by first distinguishing between neutral and polar sentences. Our model naturally incorporates the recursive interaction between context and polarity words in sentences in a unified framework while simultaneously learning the necessary features to make accurate predictions. Other approaches for sentence-level sentiment detection include (Yu and Hatzivassiloglou, 2003; Grefenstette et al., 2004; Ikeda et al., 2008). Most previous work is centered around a given sentiment lexicon or building one via heuristics (Kim and Hovy, 2007; Esuli and Sebastiani, 2007), manual annotation (Das and Chen, 2001) or machine learning techniques (Turney, 2002). In contrast, we do not require an initial or constructed sentiment lexicon of positive and negative words. In fact, when training our approach on documents or sentences, it jointly learns such lexica for both single words and n-grams (see Table 5). (Mao and Lebanon, 2007) propose isotonic conditional random fields and differentiate between local, sentence-level and global, document-level sentiment. The work of (Polanyi and Zaenen, 2006; Choi and Cardie, 2008) focuses on manually constructing several lexica"
D11-1014,I08-1039,0,0.0131334,"ument level sentiment classification, (Wilson et al., 2005) analyze the contextual polarity of phrases and incorporate many well designed features including dependency trees. They also show improvements by first distinguishing between neutral and polar sentences. Our model naturally incorporates the recursive interaction between context and polarity words in sentences in a unified framework while simultaneously learning the necessary features to make accurate predictions. Other approaches for sentence-level sentiment detection include (Yu and Hatzivassiloglou, 2003; Grefenstette et al., 2004; Ikeda et al., 2008). Most previous work is centered around a given sentiment lexicon or building one via heuristics (Kim and Hovy, 2007; Esuli and Sebastiani, 2007), manual annotation (Das and Chen, 2001) or machine learning techniques (Turney, 2002). In contrast, we do not require an initial or constructed sentiment lexicon of positive and negative words. In fact, when training our approach on documents or sentences, it jointly learns such lexica for both single words and n-grams (see Table 5). (Mao and Lebanon, 2007) propose isotonic conditional random fields and differentiate between local, sentence-level and"
D11-1014,D07-1113,0,0.0138366,"e many well designed features including dependency trees. They also show improvements by first distinguishing between neutral and polar sentences. Our model naturally incorporates the recursive interaction between context and polarity words in sentences in a unified framework while simultaneously learning the necessary features to make accurate predictions. Other approaches for sentence-level sentiment detection include (Yu and Hatzivassiloglou, 2003; Grefenstette et al., 2004; Ikeda et al., 2008). Most previous work is centered around a given sentiment lexicon or building one via heuristics (Kim and Hovy, 2007; Esuli and Sebastiani, 2007), manual annotation (Das and Chen, 2001) or machine learning techniques (Turney, 2002). In contrast, we do not require an initial or constructed sentiment lexicon of positive and negative words. In fact, when training our approach on documents or sentences, it jointly learns such lexica for both single words and n-grams (see Table 5). (Mao and Lebanon, 2007) propose isotonic conditional random fields and differentiate between local, sentence-level and global, document-level sentiment. The work of (Polanyi and Zaenen, 2006; Choi and Cardie, 2008) focuses on manually"
D11-1014,N10-1120,0,0.768432,"vergence, predicting simply the average distribution in the training data give 0.83. Fig. 4 shows that our RAE-based model outperforms the other baselines. Table 2 shows EP example entries with predicted and gold distributions, as well as numbers of votes. 4.4 Binary Polarity Classification In order to compare our approach to other methods we also show results on commonly used sentiment datasets: movie reviews4 (MR) (Pang and Lee, 2005) and opinions5 (MPQA) (Wiebe et al., 2005).We give statistical information on these and the EP corpus in Table 1. We compare to the state-of-the-art system of (Nakagawa et al., 2010), a dependency tree based classification method that uses CRFs with hidden variables. We use the same training and testing regimen (10-fold cross validation) as well as their baselines: majority phrase voting using sentiment and reversal lexica; rule-based reversal using a dependency tree; Bag-of-Features and their full Tree-CRF model. As shown in Table 4, our algorithm outperforms their approach on both datasets. For the movie review (MR) data set, we do not use any handdesigned lexica. An error analysis on the MPQA dataset showed several cases of single words which never occurred in the trai"
D11-1014,P04-1035,0,0.140477,"ysis include (Maas et al., 2011). 5.2 Sentiment Analysis Pang et al. (2002) were one of the first to experiment with sentiment classification. They show that simple bag-of-words approaches based on Naive Bayes, MaxEnt models or SVMs are often insufficient for predicting sentiment of documents even though they work well for general topic-based document classification. Even adding specific negation words, bigrams or part-of-speech information to these models did not add significant improvements. Other document-level sentiment work includes (Turney, 2002; Dave et al., 2003; Beineke et al., 2004; Pang and Lee, 2004). For further references, see (Pang and Lee, 2008). Instead of document level sentiment classification, (Wilson et al., 2005) analyze the contextual polarity of phrases and incorporate many well designed features including dependency trees. They also show improvements by first distinguishing between neutral and polar sentences. Our model naturally incorporates the recursive interaction between context and polarity words in sentences in a unified framework while simultaneously learning the necessary features to make accurate predictions. Other approaches for sentence-level sentiment detection i"
D11-1014,P05-1015,0,0.817704,"semantic vector space (blue). Then they are recursively merged by the same autoencoder network into a fixed length sentence representation. The vectors at each node are used as features to predict a distribution over sentiment labels. 2010) that can capture such phenomena use many manually constructed resources (sentiment lexica, parsers, polarity-shifting rules). This limits the applicability of these methods to a broader range of tasks and languages. Lastly, almost all previous work is based on single, positive/negative categories or scales such as star ratings. Examples are movie reviews (Pang and Lee, 2005), opinions (Wiebe et al., 2005), customer reviews (Ding et al., 2008) or multiple aspects of restaurants (Snyder and Barzilay, 2007). Such a one-dimensional scale does not accurately reflect the complexity of human emotions and sentiments. In this work, we seek to address three issues. (i) Instead of using a bag-of-words representation, our model exploits hierarchical structure and uses compositional semantics to understand sentiment. (ii) Our system can be trained both on unlabeled domain data and on supervised sentiment data and does not require any language-specific sentiment lexica, Procee"
D11-1014,W02-1011,0,0.0414298,"onal reactions. Our algorithm can more accurately predict distributions over such labels compared to several competitive baselines. 1 Introduction The ability to identify sentiments about personal experiences, products, movies etc. is crucial to understand user generated content in social networks, blogs or product reviews. Detecting sentiment in these data is a challenging task which has recently spawned a lot of interest (Pang and Lee, 2008). Current baseline methods often use bag-of-words representations which cannot properly capture more complex linguistic phenomena in sentiment analysis (Pang et al., 2002). For instance, while the two phrases “white blood cells destroying an infection” and “an infection destroying white blood cells” have the same bag-of-words representation, the former is a positive reaction while the later is very negative. More advanced methods such as (Nakagawa et al., 151 ang@cs.stanford.edu Recursive Autoencoder Sorry, Hugs You Rock Teehee I Understand Wow, Just Wow Predicted Sentiment Distribution Semantic Representations i walked into a parked car Indices Words Figure 1: Illustration of our recursive autoencoder architecture which learns semantic vector representations o"
D11-1014,N07-1038,0,0.0263354,"representation. The vectors at each node are used as features to predict a distribution over sentiment labels. 2010) that can capture such phenomena use many manually constructed resources (sentiment lexica, parsers, polarity-shifting rules). This limits the applicability of these methods to a broader range of tasks and languages. Lastly, almost all previous work is based on single, positive/negative categories or scales such as star ratings. Examples are movie reviews (Pang and Lee, 2005), opinions (Wiebe et al., 2005), customer reviews (Ding et al., 2008) or multiple aspects of restaurants (Snyder and Barzilay, 2007). Such a one-dimensional scale does not accurately reflect the complexity of human emotions and sentiments. In this work, we seek to address three issues. (i) Instead of using a bag-of-words representation, our model exploits hierarchical structure and uses compositional semantics to understand sentiment. (ii) Our system can be trained both on unlabeled domain data and on supervised sentiment data and does not require any language-specific sentiment lexica, Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 151–161, c Edinburgh, Scotland, UK, July 27–"
D11-1014,P10-1040,0,0.528937,"ce project (EP) dataset, results of standard classification tasks on this dataset and how to predict its sentiment label distributions. We then show results on other commonly used datasets and conclude with an analysis of the important parameters of the model. In all experiments involving our model, we represent words using 100-dimensional word vectors. We explore the two settings mentioned in Sec. 2.1. We compare performance on standard datasets when using randomly initialized word vectors (random word init.) or word vectors trained by the model of Collobert and Weston (2008) and provided by Turian et al. (2010).2 These vectors were trained on an unlabeled corpus of the English Wikipedia. Note that alternatives such as Brown clusters are not suitable since they do not capture sentiment information (good and bad are usually in the same cluster) and cannot be modified via backpropagation. 2 http://metaoptimize.com/projects/ wordreprs/ Corpus MPQA MR EP EP≥ 4 K 2 2 5 5 Instances 10,624 10,662 31,675 6,129 Distr.(+/-) 0.31/0.69 0.5/0.5 .2/.2/.1/.4/.1 .2/.2/.1/.4/.1 Avg|W | 3 22 113 129 Table 1: Statistics on the different datasets. K is the number of classes. Distr. is the distribution of the different c"
D11-1014,P02-1053,0,0.0452827,". Other recent deep learning methods for sentiment analysis include (Maas et al., 2011). 5.2 Sentiment Analysis Pang et al. (2002) were one of the first to experiment with sentiment classification. They show that simple bag-of-words approaches based on Naive Bayes, MaxEnt models or SVMs are often insufficient for predicting sentiment of documents even though they work well for general topic-based document classification. Even adding specific negation words, bigrams or part-of-speech information to these models did not add significant improvements. Other document-level sentiment work includes (Turney, 2002; Dave et al., 2003; Beineke et al., 2004; Pang and Lee, 2004). For further references, see (Pang and Lee, 2008). Instead of document level sentiment classification, (Wilson et al., 2005) analyze the contextual polarity of phrases and incorporate many well designed features including dependency trees. They also show improvements by first distinguishing between neutral and polar sentences. Our model naturally incorporates the recursive interaction between context and polarity words in sentences in a unified framework while simultaneously learning the necessary features to make accurate predicti"
D11-1014,N10-1119,0,0.0168405,"oth single words and n-grams (see Table 5). (Mao and Lebanon, 2007) propose isotonic conditional random fields and differentiate between local, sentence-level and global, document-level sentiment. The work of (Polanyi and Zaenen, 2006; Choi and Cardie, 2008) focuses on manually constructing several lexica and rules for both polar words and related content-word negators, such as “prevent cancer”, where prevent reverses the negative polarity of cancer. Like our approach they capture compositional semantics. However, our model does so without manually constructing any rules or lexica. Recently, (Velikovich et al., 2010) showed how to use a seed lexicon and a graph propagation framework to learn a larger sentiment lexicon that also includes polar multi-word phrases such as “once in a life time”. While our method can also learn multiword phrases it does not require a seed set or a large web graph. (Nakagawa et al., 2010) introduced an approach based on CRFs with hidden variables with very good performance. We compare to their stateof-the-art system. We outperform them on the standard corpora that we tested on without requiring external systems such as POS taggers, dependency parsers and sentiment lexica. Our a"
D11-1014,H05-1044,0,0.215154,"nt classification. They show that simple bag-of-words approaches based on Naive Bayes, MaxEnt models or SVMs are often insufficient for predicting sentiment of documents even though they work well for general topic-based document classification. Even adding specific negation words, bigrams or part-of-speech information to these models did not add significant improvements. Other document-level sentiment work includes (Turney, 2002; Dave et al., 2003; Beineke et al., 2004; Pang and Lee, 2004). For further references, see (Pang and Lee, 2008). Instead of document level sentiment classification, (Wilson et al., 2005) analyze the contextual polarity of phrases and incorporate many well designed features including dependency trees. They also show improvements by first distinguishing between neutral and polar sentences. Our model naturally incorporates the recursive interaction between context and polarity words in sentences in a unified framework while simultaneously learning the necessary features to make accurate predictions. Other approaches for sentence-level sentiment detection include (Yu and Hatzivassiloglou, 2003; Grefenstette et al., 2004; Ikeda et al., 2008). Most previous work is centered around"
D11-1014,W03-1017,0,0.0245035,"rther references, see (Pang and Lee, 2008). Instead of document level sentiment classification, (Wilson et al., 2005) analyze the contextual polarity of phrases and incorporate many well designed features including dependency trees. They also show improvements by first distinguishing between neutral and polar sentences. Our model naturally incorporates the recursive interaction between context and polarity words in sentences in a unified framework while simultaneously learning the necessary features to make accurate predictions. Other approaches for sentence-level sentiment detection include (Yu and Hatzivassiloglou, 2003; Grefenstette et al., 2004; Ikeda et al., 2008). Most previous work is centered around a given sentiment lexicon or building one via heuristics (Kim and Hovy, 2007; Esuli and Sebastiani, 2007), manual annotation (Das and Chen, 2001) or machine learning techniques (Turney, 2002). In contrast, we do not require an initial or constructed sentiment lexicon of positive and negative words. In fact, when training our approach on documents or sentences, it jointly learns such lexica for both single words and n-grams (see Table 5). (Mao and Lebanon, 2007) propose isotonic conditional random fields and"
D11-1014,P06-1055,0,\N,Missing
D12-1110,D10-1115,0,0.787239,"ith human judgments of word similarity (Griffiths et al., 2007). Despite their success, single word vector models are severely limited since they do not capture compositionality, the important quality of natural language that allows speakers to determine the meaning of a longer expression based on the meanings of its words and the rules used to combine them (Frege, 1892). This prevents them from gaining a deeper understanding of the semantics of longer phrases or sentences. Recently, there has been much progress in capturing compositionality in vector spaces, e.g., (Mitchell and Lapata, 2010; Baroni and Zamparelli, 2010; Zanzotto et al., 2010; Yessenalina and Cardie, 2011; Socher et al., 2011c) (see related work). We extend these approaches with a more general and powerful model of semantic composition. We present a novel recursive neural network model for semantic compositionality. In our context, compositionality is the ability to learn compositional vector representations for various types of phrases and sentences of arbitrary length. Fig. 1 shows an illustration of the model in which each constituent (a word or longer phrase) has a matrix-vector (MV) 1201 Proceedings of the 2012 Joint Conference on Empir"
D12-1110,W06-1670,0,0.0811849,"Missing"
D12-1110,W11-0112,0,0.0537427,"th no relation to each other. There have been many attempts at automatically parsing natural language to a logical form using recursive compositional rules. Conversely, vector space models have the attractive property that they can automatically extract knowledge from large corpora without supervision. Unlike logic-based approaches, these models allow us to make fine-grained statements about the semantic similarity of words which correlate well with human judgments (Griffiths et al., 2007). Logic-based approaches are often seen as orthogonal to distributional vector-based approaches. However, Garrette et al. (2011) recently introduced a combination of a vector space model inside a Markov Logic Network. One open question is whether vector-based models can learn some of the simple logic encountered in language such as negation or conjunctives. To this end, we illustrate in a simple example that our MV-RNN model and its learned word matrices (operators) have the ability to learn propositional logic operators such as ∧, ∨, ¬ (and, or, not). This is a necessary (though not sufficient) condition for the ability to pick up these phenomena in real datasets 1207 and tasks such as sentiment detection which we foc"
D12-1110,D11-1129,0,0.0583693,"Missing"
D12-1110,S10-1006,0,0.0179044,"Missing"
D12-1110,P03-1054,1,0.0608372,"truct any propositional logic function of any number of inputs (including xor). Hence, this MV-RNN is complete in terms of propositional logic. 4 Predicting Movie Review Ratings In this section, we analyze the model’s performance on full length sentences. We compare to previous state of the art methods on a standard benchmark dataset of movie reviews (Pang and Lee, 2005; Nakagawa et al., 2010; Socher et al., 2011c). This dataset consists of 10,000 positive and negative single sentences describing movie sentiment. In this and the next experiment we use binarized trees from the Stanford Parser (Klein and Manning, 2003). We use the exact same setup and parameters (regularization, word vector size, etc.) as the published code of Socher et al. (2011c).2 2 www.socher.org Method Tree-CRF (Nakagawa et al., 2010) RAE (Socher et al., 2011c) Linear MVR MV-RNN Acc. 77.3 77.7 77.1 79.0 MV-RNN for Relationship Classification … … Table 1: Accuracy of classification on full length movie review polarity (MR). S. C. √ 1 √ 0 0 √ 0 x 1 x Review sentence The film is bright and flashy in all the right ways. Not always too whimsical for its own good this strange hybrid of crime thriller, quirky character study, third-rate roman"
D12-1110,P98-2127,0,0.058322,"Missing"
D12-1110,N10-1120,0,0.357826,"s a training error of 0. Hence, the output of these 6 examples has exactly one of the truth representations, making it recursively compatible with further combinations of operators. Thus, we can combine these operators to construct any propositional logic function of any number of inputs (including xor). Hence, this MV-RNN is complete in terms of propositional logic. 4 Predicting Movie Review Ratings In this section, we analyze the model’s performance on full length sentences. We compare to previous state of the art methods on a standard benchmark dataset of movie reviews (Pang and Lee, 2005; Nakagawa et al., 2010; Socher et al., 2011c). This dataset consists of 10,000 positive and negative single sentences describing movie sentiment. In this and the next experiment we use binarized trees from the Stanford Parser (Klein and Manning, 2003). We use the exact same setup and parameters (regularization, word vector size, etc.) as the published code of Socher et al. (2011c).2 2 www.socher.org Method Tree-CRF (Nakagawa et al., 2010) RAE (Socher et al., 2011c) Linear MVR MV-RNN Acc. 77.3 77.7 77.1 79.0 MV-RNN for Relationship Classification … … Table 1: Accuracy of classification on full length movie review po"
D12-1110,P06-1102,0,0.0491241,"Missing"
D12-1110,J07-2002,0,0.0182724,"Missing"
D12-1110,P05-1015,0,0.329341,"imizer quickly yields a training error of 0. Hence, the output of these 6 examples has exactly one of the truth representations, making it recursively compatible with further combinations of operators. Thus, we can combine these operators to construct any propositional logic function of any number of inputs (including xor). Hence, this MV-RNN is complete in terms of propositional logic. 4 Predicting Movie Review Ratings In this section, we analyze the model’s performance on full length sentences. We compare to previous state of the art methods on a standard benchmark dataset of movie reviews (Pang and Lee, 2005; Nakagawa et al., 2010; Socher et al., 2011c). This dataset consists of 10,000 positive and negative single sentences describing movie sentiment. In this and the next experiment we use binarized trees from the Stanford Parser (Klein and Manning, 2003). We use the exact same setup and parameters (regularization, word vector size, etc.) as the published code of Socher et al. (2011c).2 2 www.socher.org Method Tree-CRF (Nakagawa et al., 2010) RAE (Socher et al., 2011c) Linear MVR MV-RNN Acc. 77.3 77.7 77.1 79.0 MV-RNN for Relationship Classification … … Table 1: Accuracy of classification on full"
D12-1110,P11-1138,0,0.246925,"1: A recursive neural network which learns semantic vector representations of phrases in a tree structure. Each word and phrase is represented by a vector and a matrix, e.g., very = (a, A). The matrix is applied to neighboring vectors. The same function is repeated to combine the phrase very good with movie. Introduction Semantic word vector spaces are at the core of many useful natural language applications such as search query expansions (Jones et al., 2006), fact extraction for information retrieval (Pas¸ca et al., 2006) and automatic annotation of text with disambiguated Wikipedia links (Ratinov et al., 2011), among many others (Turney and Pantel, 2010). In these models the meaning of a word is encoded as a vector computed from co-occurrence statistics of a word and its neighboring words. Such vectors have been shown to correlate well with human judgments of word similarity (Griffiths et al., 2007). Despite their success, single word vector models are severely limited since they do not capture compositionality, the important quality of natural language that allows speakers to determine the meaning of a longer expression based on the meanings of its words and the rules used to combine them (Frege,"
D12-1110,S10-1057,0,0.412789,"Missing"
D12-1110,P10-1093,0,0.0142925,"Missing"
D12-1110,J98-1004,0,0.0537195,"Missing"
D12-1110,D11-1014,1,0.651216,"cess, single word vector models are severely limited since they do not capture compositionality, the important quality of natural language that allows speakers to determine the meaning of a longer expression based on the meanings of its words and the rules used to combine them (Frege, 1892). This prevents them from gaining a deeper understanding of the semantics of longer phrases or sentences. Recently, there has been much progress in capturing compositionality in vector spaces, e.g., (Mitchell and Lapata, 2010; Baroni and Zamparelli, 2010; Zanzotto et al., 2010; Yessenalina and Cardie, 2011; Socher et al., 2011c) (see related work). We extend these approaches with a more general and powerful model of semantic composition. We present a novel recursive neural network model for semantic compositionality. In our context, compositionality is the ability to learn compositional vector representations for various types of phrases and sentences of arbitrary length. Fig. 1 shows an illustration of the model in which each constituent (a word or longer phrase) has a matrix-vector (MV) 1201 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural c La"
D12-1110,D11-1016,0,0.741154,"al., 2007). Despite their success, single word vector models are severely limited since they do not capture compositionality, the important quality of natural language that allows speakers to determine the meaning of a longer expression based on the meanings of its words and the rules used to combine them (Frege, 1892). This prevents them from gaining a deeper understanding of the semantics of longer phrases or sentences. Recently, there has been much progress in capturing compositionality in vector spaces, e.g., (Mitchell and Lapata, 2010; Baroni and Zamparelli, 2010; Zanzotto et al., 2010; Yessenalina and Cardie, 2011; Socher et al., 2011c) (see related work). We extend these approaches with a more general and powerful model of semantic composition. We present a novel recursive neural network model for semantic compositionality. In our context, compositionality is the ability to learn compositional vector representations for various types of phrases and sentences of arbitrary length. Fig. 1 shows an illustration of the model in which each constituent (a word or longer phrase) has a matrix-vector (MV) 1201 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Compu"
D12-1110,C10-1142,0,0.639488,"imilarity (Griffiths et al., 2007). Despite their success, single word vector models are severely limited since they do not capture compositionality, the important quality of natural language that allows speakers to determine the meaning of a longer expression based on the meanings of its words and the rules used to combine them (Frege, 1892). This prevents them from gaining a deeper understanding of the semantics of longer phrases or sentences. Recently, there has been much progress in capturing compositionality in vector spaces, e.g., (Mitchell and Lapata, 2010; Baroni and Zamparelli, 2010; Zanzotto et al., 2010; Yessenalina and Cardie, 2011; Socher et al., 2011c) (see related work). We extend these approaches with a more general and powerful model of semantic composition. We present a novel recursive neural network model for semantic compositionality. In our context, compositionality is the ability to learn compositional vector representations for various types of phrases and sentences of arbitrary length. Fig. 1 shows an illustration of the model in which each constituent (a word or longer phrase) has a matrix-vector (MV) 1201 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural"
D12-1110,J10-4006,0,\N,Missing
D12-1110,C98-2122,0,\N,Missing
D12-1110,D13-1170,1,\N,Missing
D13-1141,D10-1005,0,0.03299,"nd Bengio ( 2005) proposed efficient hierarchical continuous-space models. To systematically compare embeddings, Turian et al. (2010) evaluated improvements they bring to state-of-theart NLP benchmarks. Huang et al. (2012) introduced global document context and multiple word prototypes. Recently, morphology is explored to learn better word representations through Recursive Neural Networks (Luong et al., 2013). Bilingual word representations have been explored with hand-designed vector space models (Peirsman and Pad´o , 2010; Sumita, 2000), and with unsupervised algorithms such as LDA and LSA (Boyd-Graber and Resnik, 2010; Tam et al., 2007; Zhao and Xing, 2006). Only recently have continuous space models been applied to machine translation (Le et al., 2012). Despite growing interest in these models, little work has been done along the same lines to train bilingual distributioned word represenations to improve machine translation. In this paper, we learn bilingual word embeddings which achieve competitive performance on semantic word similarity, and apply them in a practical phrase-based MT system. 3 3.1 Unsupervised training with global context Our method starts with embedding learning formulations in Collober"
D13-1141,N10-1080,1,0.345132,"onolingual, but significantly better than Align-Init (as in Section3.2.1) on the NER task. 4.3 Vector matching alignment Translation equivalence of the bilingual embeddings is evaluated by naive word alignment to match word embeddings by cosine distance.5 The Alignment Error Rates (AER) reported in Table 3 suggest that bilingual training using Equation 5 produces embeddings with better translation equivalence compared to those produced by monolingual training. 4.4 Phrase-based machine translation Our experiments are performed using the Stanford Phrasal phrase-based machine translation system (Cer et al., 2010). In addition to NIST08 training data, we perform phrase extraction, filtering and phrase table learning with additional data from GALE MT evaluations in the past 5 years. In turn, our baseline is established at 30.01 BLEU and reasonably competitive relative to NIST08 results. We use Minimum Error Rate Training (MERT) (Och, 2003) to tune the decoder. In the phrase-based MT system, we add one feature to bilingual phrase-pairs. For each phrase, the word embeddings are averaged to obtain a feature vector. If a word is not found in the vocabulary, we disregard and assume it is not in the phrase; i"
D13-1141,W09-0439,0,0.00837207,"Missing"
D13-1141,P13-1031,1,0.225345,"Missing"
D13-1141,N06-2015,0,0.261157,"dings capture not only semantic information of monolingual words, but also semantic relationships across different languages. This propConsequently, we produce for the research community a first set of Mandarin Chinese word embeddings with 100,000 words trained on the Chinese Gigaword corpus. We evaluate these embedding on Chinese word semantic similarity from SemEval2012 (Jin and Wu, 2012). The embeddings significantly out-perform prior work and pruned tf-idf base-lines. In addition, the learned embeddings give rise to 0.11 F1 improvement in Named Entity Recognition on the OntoNotes dataset (Hovy et al., 2006) with a neural network model. We apply the bilingual embeddings in an end-toend phrase-based MT system by computing semantic similarities between phrase pairs. On NIST08 Chinese-English translation task, we obtain an improvement of 0.48 BLEU from a competitive baseline (30.01 BLEU to 30.49 BLEU) with the Stanford Phrasal MT system. 1393 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1393–1398, c Seattle, Washington, USA, 18-21 October 2013. 2013 Association for Computational Linguistics 2 Review of prior work Distributed word representations are u"
D13-1141,P12-1092,1,0.809739,"s across languages. The Fr-En phrase-pair {‘un cas de force majeure’, ‘case of absolute necessity’}, Zh-En phrase pair {‘依然故我’,‘persist in a stubborn manner’} are similar in semantics. If cooccurrences of exact word combinations are rare in the training parallel text, it can be difficult for classical statistical MT methods to identify this similarity, or produce a reasonable translation given the source phrase. We introduce an unsupervised neural model to learn bilingual semantic embedding for words across two languages. As an extension to their monolingual counter-part (Turian et al., 2010; Huang et al., 2012; Bengio et al., 2003), bilingual embeddings capture not only semantic information of monolingual words, but also semantic relationships across different languages. This propConsequently, we produce for the research community a first set of Mandarin Chinese word embeddings with 100,000 words trained on the Chinese Gigaword corpus. We evaluate these embedding on Chinese word semantic similarity from SemEval2012 (Jin and Wu, 2012). The embeddings significantly out-perform prior work and pruned tf-idf base-lines. In addition, the learned embeddings give rise to 0.11 F1 improvement in Named Entity"
D13-1141,S12-1049,0,0.294238,"nsupervised neural model to learn bilingual semantic embedding for words across two languages. As an extension to their monolingual counter-part (Turian et al., 2010; Huang et al., 2012; Bengio et al., 2003), bilingual embeddings capture not only semantic information of monolingual words, but also semantic relationships across different languages. This propConsequently, we produce for the research community a first set of Mandarin Chinese word embeddings with 100,000 words trained on the Chinese Gigaword corpus. We evaluate these embedding on Chinese word semantic similarity from SemEval2012 (Jin and Wu, 2012). The embeddings significantly out-perform prior work and pruned tf-idf base-lines. In addition, the learned embeddings give rise to 0.11 F1 improvement in Named Entity Recognition on the OntoNotes dataset (Hovy et al., 2006) with a neural network model. We apply the bilingual embeddings in an end-toend phrase-based MT system by computing semantic similarities between phrase pairs. On NIST08 Chinese-English translation task, we obtain an improvement of 0.48 BLEU from a competitive baseline (30.01 BLEU to 30.49 BLEU) with the Stanford Phrasal MT system. 1393 Proceedings of the 2013 Conference o"
D13-1141,N03-1017,0,0.0720823,"Missing"
D13-1141,N12-1005,0,0.00587547,"ements they bring to state-of-theart NLP benchmarks. Huang et al. (2012) introduced global document context and multiple word prototypes. Recently, morphology is explored to learn better word representations through Recursive Neural Networks (Luong et al., 2013). Bilingual word representations have been explored with hand-designed vector space models (Peirsman and Pad´o , 2010; Sumita, 2000), and with unsupervised algorithms such as LDA and LSA (Boyd-Graber and Resnik, 2010; Tam et al., 2007; Zhao and Xing, 2006). Only recently have continuous space models been applied to machine translation (Le et al., 2012). Despite growing interest in these models, little work has been done along the same lines to train bilingual distributioned word represenations to improve machine translation. In this paper, we learn bilingual word embeddings which achieve competitive performance on semantic word similarity, and apply them in a practical phrase-based MT system. 3 3.1 Unsupervised training with global context Our method starts with embedding learning formulations in Collobert et al. (2008). Given a context window c in a document d, the optimization minimizes the following Context Objective for a word w in the"
D13-1141,N06-1014,0,0.0184131,"tions, e.g. ‘lake’ and the Chinese word ‘潭’ (deep pond), their semantic proximity could be correctly quantified. We describe in the next sub-sections the methods to intialize and train bilingual embeddings. These methods ensure that bilingual embeddings retain their translational equivalence while their distributional semantics are improved during online training with a monolingual corpus. 3.2.1 Initialization by MT alignments First, we use MT Alignment counts as weighting to initialize Chinese word embeddings. In our experiments, we use MT word alignments extracted with the Berkeley Aligner (Liang et al., 2006) 1 . Specifically, we use the following equation to compute starting word embeddings: Wt-init = S X Cts + 1 s=1 Algorithm and methods JCO = Here f is a function defined by a neural network. wr is a word chosen in a random subset VR of the r vocabulary, and cw is the context window containing word wr . This unsupervised objective function contrasts the score between when the correct word is placed in context with when a random word is placed in the same context. We incorporate the global context information as in Huang et al. (2012), shown to improve performance of word embeddings. r max(0, 1 −"
D13-1141,W13-3512,1,0.24197,"and apply word embeddings using continuous models for language. Collobert et al. (2008) learn embeddings in an unsupervised manner through a contrastive estimation technique. Mnih and Hinton ( 2008), Morin and Bengio ( 2005) proposed efficient hierarchical continuous-space models. To systematically compare embeddings, Turian et al. (2010) evaluated improvements they bring to state-of-theart NLP benchmarks. Huang et al. (2012) introduced global document context and multiple word prototypes. Recently, morphology is explored to learn better word representations through Recursive Neural Networks (Luong et al., 2013). Bilingual word representations have been explored with hand-designed vector space models (Peirsman and Pad´o , 2010; Sumita, 2000), and with unsupervised algorithms such as LDA and LSA (Boyd-Graber and Resnik, 2010; Tam et al., 2007; Zhao and Xing, 2006). Only recently have continuous space models been applied to machine translation (Le et al., 2012). Despite growing interest in these models, little work has been done along the same lines to train bilingual distributioned word represenations to improve machine translation. In this paper, we learn bilingual word embeddings which achieve compe"
D13-1141,P11-1015,0,0.289194,"Missing"
D13-1141,P03-1021,0,0.0270902,"g using Equation 5 produces embeddings with better translation equivalence compared to those produced by monolingual training. 4.4 Phrase-based machine translation Our experiments are performed using the Stanford Phrasal phrase-based machine translation system (Cer et al., 2010). In addition to NIST08 training data, we perform phrase extraction, filtering and phrase table learning with additional data from GALE MT evaluations in the past 5 years. In turn, our baseline is established at 30.01 BLEU and reasonably competitive relative to NIST08 results. We use Minimum Error Rate Training (MERT) (Och, 2003) to tune the decoder. In the phrase-based MT system, we add one feature to bilingual phrase-pairs. For each phrase, the word embeddings are averaged to obtain a feature vector. If a word is not found in the vocabulary, we disregard and assume it is not in the phrase; if no word is found in a phrase, a zero vector is assigned 4 Due to variations caused by online minibatch L-BFGS, we take embeddings from five random points out of last 105 minibatch iterations, and average their semantic similarity results. 1396 5 This is evaluated on 10,000 randomly selected sentence pairs from the MT training s"
D13-1141,P06-1102,0,0.021416,"Missing"
D13-1141,N10-1135,0,0.126436,"Missing"
D13-1141,N10-1013,0,0.320906,"Missing"
D13-1141,D11-1014,1,0.274729,"Missing"
D13-1141,P00-1054,0,0.281177,"a contrastive estimation technique. Mnih and Hinton ( 2008), Morin and Bengio ( 2005) proposed efficient hierarchical continuous-space models. To systematically compare embeddings, Turian et al. (2010) evaluated improvements they bring to state-of-theart NLP benchmarks. Huang et al. (2012) introduced global document context and multiple word prototypes. Recently, morphology is explored to learn better word representations through Recursive Neural Networks (Luong et al., 2013). Bilingual word representations have been explored with hand-designed vector space models (Peirsman and Pad´o , 2010; Sumita, 2000), and with unsupervised algorithms such as LDA and LSA (Boyd-Graber and Resnik, 2010; Tam et al., 2007; Zhao and Xing, 2006). Only recently have continuous space models been applied to machine translation (Le et al., 2012). Despite growing interest in these models, little work has been done along the same lines to train bilingual distributioned word represenations to improve machine translation. In this paper, we learn bilingual word embeddings which achieve competitive performance on semantic word similarity, and apply them in a practical phrase-based MT system. 3 3.1 Unsupervised training wi"
D13-1141,P07-1066,0,0.0208964,"icient hierarchical continuous-space models. To systematically compare embeddings, Turian et al. (2010) evaluated improvements they bring to state-of-theart NLP benchmarks. Huang et al. (2012) introduced global document context and multiple word prototypes. Recently, morphology is explored to learn better word representations through Recursive Neural Networks (Luong et al., 2013). Bilingual word representations have been explored with hand-designed vector space models (Peirsman and Pad´o , 2010; Sumita, 2000), and with unsupervised algorithms such as LDA and LSA (Boyd-Graber and Resnik, 2010; Tam et al., 2007; Zhao and Xing, 2006). Only recently have continuous space models been applied to machine translation (Le et al., 2012). Despite growing interest in these models, little work has been done along the same lines to train bilingual distributioned word represenations to improve machine translation. In this paper, we learn bilingual word embeddings which achieve competitive performance on semantic word similarity, and apply them in a practical phrase-based MT system. 3 3.1 Unsupervised training with global context Our method starts with embedding learning formulations in Collobert et al. (2008). G"
D13-1141,P10-1040,0,0.914221,"semantic similarities across languages. The Fr-En phrase-pair {‘un cas de force majeure’, ‘case of absolute necessity’}, Zh-En phrase pair {‘依然故我’,‘persist in a stubborn manner’} are similar in semantics. If cooccurrences of exact word combinations are rare in the training parallel text, it can be difficult for classical statistical MT methods to identify this similarity, or produce a reasonable translation given the source phrase. We introduce an unsupervised neural model to learn bilingual semantic embedding for words across two languages. As an extension to their monolingual counter-part (Turian et al., 2010; Huang et al., 2012; Bengio et al., 2003), bilingual embeddings capture not only semantic information of monolingual words, but also semantic relationships across different languages. This propConsequently, we produce for the research community a first set of Mandarin Chinese word embeddings with 100,000 words trained on the Chinese Gigaword corpus. We evaluate these embedding on Chinese word semantic similarity from SemEval2012 (Jin and Wu, 2012). The embeddings significantly out-perform prior work and pruned tf-idf base-lines. In addition, the learned embeddings give rise to 0.11 F1 improve"
D13-1141,P13-1106,1,0.321038,"trained embeddings4 out-perform pruned tf-idf by 14.1 and 12.6 Spearman Correlation (×100), respectively. Further, they out-perform embeddings initialized from alignment by 7.9 and 6.4. Both our tf-idf implementation and the word embeddings have significantly higher Kendall’s Tau value compared to Prior work (Jin and Wu, 2012). We verified Tau calculations with original submissions provided by the authors. 4.2 Named Entity Recognition We perform NER experiments on OntoNotes (v4.0) (Hovy et al., 2006) to validate the quality of the Chinese word embeddings. Our experimental setup is the same as Wang et al. (2013). With embeddings, we build a naive feed-forward neural network (Collobert et al., 2008) with 2000 hidden neurons and a sliding window of five words. This naive setting, without sequence modeling or sophisticated Embeddings Align-Init Mono-trained Biling-trained Prec. 0.34 0.54 0.48 Rec. 0.52 0.62 0.55 F1 0.41 0.58 0.52 Improve 0.17 0.11 Table 3: Vector Matching Alignment AER (lower is better) Embeddings Mono-trained Biling-trained Prec. 0.27 0.37 Rec. 0.32 0.45 AER 0.71 0.59 join optimization, is not competitive with state-ofthe-art (Wang et al., 2013). Table 2 shows that the bilingual embedd"
D13-1141,P01-1067,0,0.0161864,"Missing"
D13-1141,P06-2124,0,0.00531532,"l continuous-space models. To systematically compare embeddings, Turian et al. (2010) evaluated improvements they bring to state-of-theart NLP benchmarks. Huang et al. (2012) introduced global document context and multiple word prototypes. Recently, morphology is explored to learn better word representations through Recursive Neural Networks (Luong et al., 2013). Bilingual word representations have been explored with hand-designed vector space models (Peirsman and Pad´o , 2010; Sumita, 2000), and with unsupervised algorithms such as LDA and LSA (Boyd-Graber and Resnik, 2010; Tam et al., 2007; Zhao and Xing, 2006). Only recently have continuous space models been applied to machine translation (Le et al., 2012). Despite growing interest in these models, little work has been done along the same lines to train bilingual distributioned word represenations to improve machine translation. In this paper, we learn bilingual word embeddings which achieve competitive performance on semantic word similarity, and apply them in a practical phrase-based MT system. 3 3.1 Unsupervised training with global context Our method starts with embedding learning formulations in Collobert et al. (2008). Given a context window"
D13-1141,C12-1089,0,\N,Missing
D13-1170,J10-4006,0,0.113389,"junction ‘but’ dominates. The complete training and testing code, a live demo and the Stanford Sentiment Treebank dataset are available at http://nlp.stanford.edu/ sentiment. 2 Related Work This work is connected to five different areas of NLP research, each with their own large amount of related work to which we cannot do full justice given space constraints. Semantic Vector Spaces. The dominant approach in semantic vector spaces uses distributional similarities of single words. Often, co-occurrence statistics of a word and its context are used to describe each word (Turney and Pantel, 2010; Baroni and Lenci, 2010), such as tf-idf. Variants of this idea use more complex frequencies such as how often a 1632 word appears in a certain syntactic context (Pado and Lapata, 2007; Erk and Pad´o, 2008). However, distributional vectors often do not properly capture the differences in antonyms since those often have similar contexts. One possibility to remedy this is to use neural word vectors (Bengio et al., 2003). These vectors can be trained in an unsupervised fashion to capture distributional similarities (Collobert and Weston, 2008; Huang et al., 2012) but then also be fine-tuned and trained to specific tasks"
D13-1170,D08-1094,0,0.140304,"Missing"
D13-1170,D11-1129,0,0.143203,"ions. Mitchell and Lapata (2010) use e.g. two-word phrases and analyze similarities computed by vector addition, multiplication and others. Some related models such as holographic reduced representations (Plate, 1995), quantum logic (Widdows, 2008), discrete-continuous models (Clark and Pulman, 2007) and the recent compositional matrix space model (Rudolph and Giesbrecht, 2010) have not been experimentally validated on larger corpora. Yessenalina and Cardie (2011) compute matrix representations for longer phrases and define composition as matrix multiplication, and also evaluate on sentiment. Grefenstette and Sadrzadeh (2011) analyze subject-verbobject triplets and find a matrix-based categorical model to correlate well with human judgments. We compare to the recent line of work on supervised compositional models. In particular we will describe and experimentally compare our new RNTN model to recursive neural networks (RNN) (Socher et al., 2011b) and matrix-vector RNNs (Socher et al., 2012) both of which have been applied to bag of words sentiment corpora. Logical Form. A related field that tackles compositionality from a very different angle is that of trying to map sentences to logical form (Zettlemoyer and Coll"
D13-1170,W13-0112,0,0.126107,"data. To address this need, we introduce the Stanford Sentiment Treebank and a powerful Recursive Neural Tensor Network that can accurately predict the compositional semantic effects present in this new corpus. Introduction Semantic vector spaces for single words have been widely used as features (Turney and Pantel, 2010). Because they cannot capture the meaning of longer phrases properly, compositionality in semantic vector spaces has recently received a lot of attention (Mitchell and Lapata, 2010; Socher et al., 2010; Zanzotto et al., 2010; Yessenalina and Cardie, 2011; Socher et al., 2012; Grefenstette et al., 2013). However, progress is held back by the current lack of large and labeled compositionality resources and The Stanford Sentiment Treebank is the first corpus with fully labeled parse trees that allows for a complete analysis of the compositional effects of sentiment in language. The corpus is based on the dataset introduced by Pang and Lee (2005) and consists of 11,855 single sentences extracted from movie reviews. It was parsed with the Stanford parser (Klein and Manning, 2003) and includes a total of 215,154 unique phrases from those parse trees, each annotated by 3 human judges. This new dat"
D13-1170,P12-1092,1,0.167577,"used to describe each word (Turney and Pantel, 2010; Baroni and Lenci, 2010), such as tf-idf. Variants of this idea use more complex frequencies such as how often a 1632 word appears in a certain syntactic context (Pado and Lapata, 2007; Erk and Pad´o, 2008). However, distributional vectors often do not properly capture the differences in antonyms since those often have similar contexts. One possibility to remedy this is to use neural word vectors (Bengio et al., 2003). These vectors can be trained in an unsupervised fashion to capture distributional similarities (Collobert and Weston, 2008; Huang et al., 2012) but then also be fine-tuned and trained to specific tasks such as sentiment detection (Socher et al., 2011b). The models in this paper can use purely supervised word representations learned entirely on the new corpus. Compositionality in Vector Spaces. Most of the compositionality algorithms and related datasets capture two word compositions. Mitchell and Lapata (2010) use e.g. two-word phrases and analyze similarities computed by vector addition, multiplication and others. Some related models such as holographic reduced representations (Plate, 1995), quantum logic (Widdows, 2008), discrete-c"
D13-1170,P03-1054,1,0.146647,"ll and Lapata, 2010; Socher et al., 2010; Zanzotto et al., 2010; Yessenalina and Cardie, 2011; Socher et al., 2012; Grefenstette et al., 2013). However, progress is held back by the current lack of large and labeled compositionality resources and The Stanford Sentiment Treebank is the first corpus with fully labeled parse trees that allows for a complete analysis of the compositional effects of sentiment in language. The corpus is based on the dataset introduced by Pang and Lee (2005) and consists of 11,855 single sentences extracted from movie reviews. It was parsed with the Stanford parser (Klein and Manning, 2003) and includes a total of 215,154 unique phrases from those parse trees, each annotated by 3 human judges. This new dataset allows us to analyze the intricacies of sentiment and to capture complex linguistic phenomena. Fig. 1 shows one of the many examples with clear compositional structure. The granularity and size of 1631 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1631–1642, c Seattle, Washington, USA, 18-21 October 2013. 2013 Association for Computational Linguistics this dataset will enable the community to train compositional models that a"
D13-1170,N10-1120,0,0.13649,"Missing"
D13-1170,J07-2002,0,0.140631,"/ sentiment. 2 Related Work This work is connected to five different areas of NLP research, each with their own large amount of related work to which we cannot do full justice given space constraints. Semantic Vector Spaces. The dominant approach in semantic vector spaces uses distributional similarities of single words. Often, co-occurrence statistics of a word and its context are used to describe each word (Turney and Pantel, 2010; Baroni and Lenci, 2010), such as tf-idf. Variants of this idea use more complex frequencies such as how often a 1632 word appears in a certain syntactic context (Pado and Lapata, 2007; Erk and Pad´o, 2008). However, distributional vectors often do not properly capture the differences in antonyms since those often have similar contexts. One possibility to remedy this is to use neural word vectors (Bengio et al., 2003). These vectors can be trained in an unsupervised fashion to capture distributional similarities (Collobert and Weston, 2008; Huang et al., 2012) but then also be fine-tuned and trained to specific tasks such as sentiment detection (Socher et al., 2011b). The models in this paper can use purely supervised word representations learned entirely on the new corpus."
D13-1170,P05-1015,0,0.37586,"he meaning of longer phrases properly, compositionality in semantic vector spaces has recently received a lot of attention (Mitchell and Lapata, 2010; Socher et al., 2010; Zanzotto et al., 2010; Yessenalina and Cardie, 2011; Socher et al., 2012; Grefenstette et al., 2013). However, progress is held back by the current lack of large and labeled compositionality resources and The Stanford Sentiment Treebank is the first corpus with fully labeled parse trees that allows for a complete analysis of the compositional effects of sentiment in language. The corpus is based on the dataset introduced by Pang and Lee (2005) and consists of 11,855 single sentences extracted from movie reviews. It was parsed with the Stanford parser (Klein and Manning, 2003) and includes a total of 215,154 unique phrases from those parse trees, each annotated by 3 human judges. This new dataset allows us to analyze the intricacies of sentiment and to capture complex linguistic phenomena. Fig. 1 shows one of the many examples with clear compositional structure. The granularity and size of 1631 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1631–1642, c Seattle, Washington, USA, 18-21 O"
D13-1170,rentoumi-etal-2010-united,0,0.131845,"Missing"
D13-1170,P10-1093,0,0.139526,"her et al., 2011b). The models in this paper can use purely supervised word representations learned entirely on the new corpus. Compositionality in Vector Spaces. Most of the compositionality algorithms and related datasets capture two word compositions. Mitchell and Lapata (2010) use e.g. two-word phrases and analyze similarities computed by vector addition, multiplication and others. Some related models such as holographic reduced representations (Plate, 1995), quantum logic (Widdows, 2008), discrete-continuous models (Clark and Pulman, 2007) and the recent compositional matrix space model (Rudolph and Giesbrecht, 2010) have not been experimentally validated on larger corpora. Yessenalina and Cardie (2011) compute matrix representations for longer phrases and define composition as matrix multiplication, and also evaluate on sentiment. Grefenstette and Sadrzadeh (2011) analyze subject-verbobject triplets and find a matrix-based categorical model to correlate well with human judgments. We compare to the recent line of work on supervised compositional models. In particular we will describe and experimentally compare our new RNTN model to recursive neural networks (RNN) (Socher et al., 2011b) and matrix-vector R"
D13-1170,N07-1038,0,0.134233,"by Bottou (2011) and Hinton (1990) and first models such as Recursive Auto-associative memories been experimented with by Pollack (1990). The idea to relate inputs through three way interactions, parameterized by a tensor have been proposed for relation classification (Sutskever et al., 2009; Jenatton et al., 2012), extending Restricted Boltzmann machines (Ranzato and Hinton, 2010) and as a special layer for speech recognition (Yu et al., 2012). Sentiment Analysis. Apart from the abovementioned work, most approaches in sentiment analysis use bag of words representations (Pang and Lee, 2008). Snyder and Barzilay (2007) analyzed larger reviews in more detail by analyzing the sentiment of multiple aspects of restaurants, such as food or atmosphere. Several works have explored sentiment compositionality through careful engineering of features or polarity shifting rules on syntactic structures (Polanyi and Zaenen, 2006; Moilanen and Pulman, 2007; Rentoumi et al., 2010; Nakagawa et al., 2010). 3 Stanford Sentiment Treebank Bag of words classifiers can work well in longer documents by relying on a few words with strong sentiment like ‘awesome’ or ‘exhilarating.’ However, sentiment accuracies even for binary posit"
D13-1170,D11-1014,1,0.293141,"this idea use more complex frequencies such as how often a 1632 word appears in a certain syntactic context (Pado and Lapata, 2007; Erk and Pad´o, 2008). However, distributional vectors often do not properly capture the differences in antonyms since those often have similar contexts. One possibility to remedy this is to use neural word vectors (Bengio et al., 2003). These vectors can be trained in an unsupervised fashion to capture distributional similarities (Collobert and Weston, 2008; Huang et al., 2012) but then also be fine-tuned and trained to specific tasks such as sentiment detection (Socher et al., 2011b). The models in this paper can use purely supervised word representations learned entirely on the new corpus. Compositionality in Vector Spaces. Most of the compositionality algorithms and related datasets capture two word compositions. Mitchell and Lapata (2010) use e.g. two-word phrases and analyze similarities computed by vector addition, multiplication and others. Some related models such as holographic reduced representations (Plate, 1995), quantum logic (Widdows, 2008), discrete-continuous models (Clark and Pulman, 2007) and the recent compositional matrix space model (Rudolph and Gies"
D13-1170,D12-1110,1,0.209782,"na presented in such data. To address this need, we introduce the Stanford Sentiment Treebank and a powerful Recursive Neural Tensor Network that can accurately predict the compositional semantic effects present in this new corpus. Introduction Semantic vector spaces for single words have been widely used as features (Turney and Pantel, 2010). Because they cannot capture the meaning of longer phrases properly, compositionality in semantic vector spaces has recently received a lot of attention (Mitchell and Lapata, 2010; Socher et al., 2010; Zanzotto et al., 2010; Yessenalina and Cardie, 2011; Socher et al., 2012; Grefenstette et al., 2013). However, progress is held back by the current lack of large and labeled compositionality resources and The Stanford Sentiment Treebank is the first corpus with fully labeled parse trees that allows for a complete analysis of the compositional effects of sentiment in language. The corpus is based on the dataset introduced by Pang and Lee (2005) and consists of 11,855 single sentences extracted from movie reviews. It was parsed with the Stanford parser (Klein and Manning, 2003) and includes a total of 215,154 unique phrases from those parse trees, each annotated by"
D13-1170,P12-3020,0,0.239464,"Missing"
D13-1170,D11-1016,0,0.210149,"capture the underlying phenomena presented in such data. To address this need, we introduce the Stanford Sentiment Treebank and a powerful Recursive Neural Tensor Network that can accurately predict the compositional semantic effects present in this new corpus. Introduction Semantic vector spaces for single words have been widely used as features (Turney and Pantel, 2010). Because they cannot capture the meaning of longer phrases properly, compositionality in semantic vector spaces has recently received a lot of attention (Mitchell and Lapata, 2010; Socher et al., 2010; Zanzotto et al., 2010; Yessenalina and Cardie, 2011; Socher et al., 2012; Grefenstette et al., 2013). However, progress is held back by the current lack of large and labeled compositionality resources and The Stanford Sentiment Treebank is the first corpus with fully labeled parse trees that allows for a complete analysis of the compositional effects of sentiment in language. The corpus is based on the dataset introduced by Pang and Lee (2005) and consists of 11,855 single sentences extracted from movie reviews. It was parsed with the Stanford parser (Klein and Manning, 2003) and includes a total of 215,154 unique phrases from those parse tree"
D13-1170,C10-1142,0,0.126738,". models to accurately capture the underlying phenomena presented in such data. To address this need, we introduce the Stanford Sentiment Treebank and a powerful Recursive Neural Tensor Network that can accurately predict the compositional semantic effects present in this new corpus. Introduction Semantic vector spaces for single words have been widely used as features (Turney and Pantel, 2010). Because they cannot capture the meaning of longer phrases properly, compositionality in semantic vector spaces has recently received a lot of attention (Mitchell and Lapata, 2010; Socher et al., 2010; Zanzotto et al., 2010; Yessenalina and Cardie, 2011; Socher et al., 2012; Grefenstette et al., 2013). However, progress is held back by the current lack of large and labeled compositionality resources and The Stanford Sentiment Treebank is the first corpus with fully labeled parse trees that allows for a complete analysis of the compositional effects of sentiment in language. The corpus is based on the dataset introduced by Pang and Lee (2005) and consists of 11,855 single sentences extracted from movie reviews. It was parsed with the Stanford parser (Klein and Manning, 2003) and includes a total of 215,154 unique"
D13-1170,R09-1048,0,\N,Missing
D14-1070,W13-3214,0,0.0242311,"vector spaces with neural networks have been successful, although simpler functions have been proposed for short phrases (Mitchell and Lapata, 2008). Recursive neural networks have achieved state-of-the-art performance in sentiment analysis and parsing (Socher et al., 2013c; Hermann and Blunsom, 2013; Socher et al., 2013a). rnns have not been previously used for learning attribute spaces as we do here, although recursive tensor networks were unsuccessfully applied to a knowledge base completion task (Socher et al., 2013b). More relevant to this work are the dialogue analysis model proposed by Kalchbrenner & Blunsom (2013) and the paragraph vector model described in Le and Mikolov (2014), both of which are able to generate distributed representations of paragraphs. Here we present a simpler approach where a single model is able to learn complex sentence representations and average them across paragraphs. 6.2 Factoid Question-Answering Factoid question answering is often functionally equivalent to information retrieval. Given a knowledge base and a query, the goal is to Thomas Mann Henrik Ibsen Joseph Conrad Henry James Franz Kafka Figure 6: A question on the German novelist Thomas Mann that contains no named en"
D14-1070,D12-1118,1,0.767192,"s have a property called pyramidality, which means that sentences early in a question contain harder, more obscure clues, while later sentences are “giveaways”. This design rewards players with deep knowledge of a particular subject and thwarts bag of words methods. Sometimes the first sentence contains no named entities—answering the question correctly requires an actual understanding of the sentence (Figure 1). Later sentences, however, progressively reveal more well-known and uniquely identifying terms. Previous work answers quiz bowl questions using a bag of words (na¨ıve Bayes) approach (Boyd-Graber et al., 2012). These models fail on sentences like the first one in Figure 1, a typical hard, initial clue. Recursive neural networks (rnns), in contrast to simpler models, can capture the compositional aspect of such sentences (Hermann et al., 2013). rnns require many redundant training examples to learn meaningful representations, which in the quiz bowl setting means we need multiple questions about the same answer. Fortunately, hundreds of questions are produced during the school year for quiz bowl competitions, yielding many different examples of questions asking about any entity of note (see Section 4"
D14-1070,de-marneffe-etal-2006-generating,0,0.0211844,"Missing"
D14-1070,W13-0112,0,0.0150837,"ronald_reagan woodrow_wilson Wars, rebellions, and battles U.S. presidents Prime ministers Explorers & emperors Policies Other Figure 5: t-SNE 2-D projections of 451 answer vectors divided into six major clusters. The blue cluster is predominantly populated by U.S. presidents. The zoomed plot reveals temporal clustering among the presidents based on the years they spent in office. from the meaning of the words that it contains as well as the syntax that glues those words together. Many computational models of compositionality focus on learning vector spaces (Zanzotto et al., 2010; Erk, 2012; Grefenstette et al., 2013; Yessenalina and Cardie, 2011). Recent approaches towards modeling compositional vector spaces with neural networks have been successful, although simpler functions have been proposed for short phrases (Mitchell and Lapata, 2008). Recursive neural networks have achieved state-of-the-art performance in sentiment analysis and parsing (Socher et al., 2013c; Hermann and Blunsom, 2013; Socher et al., 2013a). rnns have not been previously used for learning attribute spaces as we do here, although recursive tensor networks were unsuccessfully applied to a knowledge base completion task (Socher et al"
D14-1070,P13-1088,0,0.00846746,". from the meaning of the words that it contains as well as the syntax that glues those words together. Many computational models of compositionality focus on learning vector spaces (Zanzotto et al., 2010; Erk, 2012; Grefenstette et al., 2013; Yessenalina and Cardie, 2011). Recent approaches towards modeling compositional vector spaces with neural networks have been successful, although simpler functions have been proposed for short phrases (Mitchell and Lapata, 2008). Recursive neural networks have achieved state-of-the-art performance in sentiment analysis and parsing (Socher et al., 2013c; Hermann and Blunsom, 2013; Socher et al., 2013a). rnns have not been previously used for learning attribute spaces as we do here, although recursive tensor networks were unsuccessfully applied to a knowledge base completion task (Socher et al., 2013b). More relevant to this work are the dialogue analysis model proposed by Kalchbrenner & Blunsom (2013) and the paragraph vector model described in Le and Mikolov (2014), both of which are able to generate distributed representations of paragraphs. Here we present a simpler approach where a single model is able to learn complex sentence representations and average them acr"
D14-1070,W13-3209,0,0.0158745,"rts bag of words methods. Sometimes the first sentence contains no named entities—answering the question correctly requires an actual understanding of the sentence (Figure 1). Later sentences, however, progressively reveal more well-known and uniquely identifying terms. Previous work answers quiz bowl questions using a bag of words (na¨ıve Bayes) approach (Boyd-Graber et al., 2012). These models fail on sentences like the first one in Figure 1, a typical hard, initial clue. Recursive neural networks (rnns), in contrast to simpler models, can capture the compositional aspect of such sentences (Hermann et al., 2013). rnns require many redundant training examples to learn meaningful representations, which in the quiz bowl setting means we need multiple questions about the same answer. Fortunately, hundreds of questions are produced during the school year for quiz bowl competitions, yielding many different examples of questions asking about any entity of note (see Section 4.1 for more details). Thus, we have built-in redundancy (the number of “askable” entities is limited), but also built-in diversity, as difficult clues cannot appear in every question without becoming well-known. Dependency-Tree Recursive"
D14-1070,P14-1136,0,0.00701547,"ector hs . The error for the sentence is C(S, θ) = XX L(rank(c, s, Z))max(0, s∈S z∈Z 1 − xc · hs + xz · hs ), (5) where the function rank(c, s, Z) provides the rank of correct answer c with respect to the incorrect answers Z. We transform this rank into a loss function4 shown by Usunier et al. (2009) to optimize the top of the ranked list, r P L(r) = 1/i. i=1 Since rank(c, s, Z) is expensive to compute, we approximate it by randomly sampling K incorrect answers until a violation is observed (xc · hs < 1 + xz · hs ) and set rank(c, s, Z) = (|Z |− 1)/K, as in previous work (Weston et al., 2011; Hermann et al., 2014). The model minimizes the sum of the error over all sentences T normalized by the number of nodes N in the training set, J(θ) = 1 X C(t, θ). N (6) t∈T The parameters θ = (Wr∈R , Wv , We , b), where R represents all dependency relations in the data, are optimized using AdaGrad(Duchi et al., 2011).5 In Section 4 we compare performance to an identical model (fixed-qanta) that excludes answer vectors from We and show that training them as part of θ produces significantly better results. The gradient of the objective function, ∂C 1 X ∂J(t) = , ∂θ N ∂θ (7) t∈T is computed using backpropagation throu"
D14-1070,P14-1105,1,0.253879,"ion described below. While we are not interested in obtaining a ranked list of answers,3 we observe better performance by adding the weighted approximaterank pairwise (warp) loss proposed in Weston et al. (2011) to our objective function. where R(n, k) is the dependency relation between node n and child node k. 3.2 Training Our goal is to map questions to their corresponding answer entities. Because there are a limited number of possible answers, we can view this as a multi-class classification task. While a softmax layer over every node in the tree could predict answers (Socher et al., 2011; Iyyer et al., 2014), this method overlooks that most answers are themselves words (features) in other questions (e.g., a question on World 635 Given a sentence paired with its correct answer c, we randomly select j incorrect answers from the set of all incorrect answers and denote this subset as Z. Since c is part of the vocabulary, it has a vector xc ∈ We . An incorrect answer z ∈ Z is also associated with a vector xz ∈ We . We define S to be the set of all nodes in the sentence’s dependency tree, where an individual node s ∈ S is associated with the 2 Of course, questions never contain their own answer as part"
D14-1070,P08-1028,0,0.0784578,"er is predominantly populated by U.S. presidents. The zoomed plot reveals temporal clustering among the presidents based on the years they spent in office. from the meaning of the words that it contains as well as the syntax that glues those words together. Many computational models of compositionality focus on learning vector spaces (Zanzotto et al., 2010; Erk, 2012; Grefenstette et al., 2013; Yessenalina and Cardie, 2011). Recent approaches towards modeling compositional vector spaces with neural networks have been successful, although simpler functions have been proposed for short phrases (Mitchell and Lapata, 2008). Recursive neural networks have achieved state-of-the-art performance in sentiment analysis and parsing (Socher et al., 2013c; Hermann and Blunsom, 2013; Socher et al., 2013a). rnns have not been previously used for learning attribute spaces as we do here, although recursive tensor networks were unsuccessfully applied to a knowledge base completion task (Socher et al., 2013b). More relevant to this work are the dialogue analysis model proposed by Kalchbrenner & Blunsom (2013) and the paragraph vector model described in Le and Mikolov (2014), both of which are able to generate distributed repr"
D14-1070,P14-1037,0,0.028984,"Missing"
D14-1070,N12-1085,1,0.502253,"641 Q A Q A Q A Q A Q Akbar Muhammad Shah Jahan Ghana A Babur Figure 7: An extremely misleading question about John Cabot, at least to computer models. The words muslim and mecca lead to three Mughal emperors in the top five guesses from qanta; other models are similarly led awry. with ir-wiki. A promising avenue for future work would be to incorporate Wikipedia data into qanta by transforming sentences to look like quiz bowl questions (Wang et al., 2007) and to select relevant sentences, as not every sentence in a Wikipedia article directly describes its subject. Syntax-specific annotation (Sayeed et al., 2012) may help in this regard. Finally, we could adapt the attribute space learned by the dt-rnn to use information from knowledge bases and to aid in knowledge base completion. Having learned many facts about entities that occur in question text, a dt-rnn could add new facts to a knowledge base or check existing relationships. 8 Conclusion We present qanta, a dependency-tree recursive neural network for factoid question answering that outperforms bag of words and information retrieval baselines. Our model improves upon a contrastive max-margin objective function from previous work to dynamically u"
D14-1070,D07-1002,0,0.141453,"z Kafka Figure 6: A question on the German novelist Thomas Mann that contains no named entities, along with the five top answers as scored by qanta. Each cell in the heatmap corresponds to the score (inner product) between a node in the parse tree and the given answer, and the dependency parse of the sentence is shown on the left. All of our baselines, including irwiki, are wrong, while qanta uses the plot description to make a correct guess. return the answer. Many approaches to this problem rely on hand-crafted pattern matching and answer-type classification to narrow down the search space (Shen, 2007; Bilotti et al., 2010; Wang, 2006). More recent factoid qa systems incorporate the web and social media into their retrieval systems (Bian et al., 2008). In contrast to these approaches, we place the burden of learning answer types and patterns on the model. 7 Future Work While we have shown that dt-rnns are effective models for quiz bowl question answering, other factoid qa tasks are more challenging. Questions like what does the aarp stand for? from trec qa data require additional infrastructure. A more apt comparison would be to IBM’s proprietary Watson system (Lally et al., 2012) for Jeop"
D14-1070,D11-1014,1,0.167867,"argin objective function described below. While we are not interested in obtaining a ranked list of answers,3 we observe better performance by adding the weighted approximaterank pairwise (warp) loss proposed in Weston et al. (2011) to our objective function. where R(n, k) is the dependency relation between node n and child node k. 3.2 Training Our goal is to map questions to their corresponding answer entities. Because there are a limited number of possible answers, we can view this as a multi-class classification task. While a softmax layer over every node in the tree could predict answers (Socher et al., 2011; Iyyer et al., 2014), this method overlooks that most answers are themselves words (features) in other questions (e.g., a question on World 635 Given a sentence paired with its correct answer c, we randomly select j incorrect answers from the set of all incorrect answers and denote this subset as Z. Since c is part of the vocabulary, it has a vector xc ∈ We . An incorrect answer z ∈ Z is also associated with a vector xz ∈ We . We define S to be the set of all nodes in the sentence’s dependency tree, where an individual node s ∈ S is associated with the 2 Of course, questions never contain the"
D14-1070,P13-1045,1,0.470518,"the Holy Roman Empire. The first sentence contains no words or named entities that by themselves are indicative of the answer, while subsequent sentences contain more and more obvious clues. where inputs are typically a single sentence and outputs are either continuous or a limited discrete set. Neural networks have not yet shown to be useful for tasks that require mapping paragraph-length inputs to rich output spaces. Introduction Deep neural networks have seen widespread use in natural language processing tasks such as parsing, language modeling, and sentiment analysis (Bengio et al., 2003; Socher et al., 2013a; Socher et al., 2013c). The vector spaces learned by these models cluster words and phrases together based on similarity. For example, a neural network trained for a sentiment analysis task such as restaurant review classification might learn that “tasty” and “delicious” should have similar representations since they are synonymous adjectives. These models have so far only seen success in a limited range of text-based prediction tasks, Consider factoid question answering: given a description of an entity, identify the person, place, or thing discussed. We describe a task with high-quality ma"
D14-1070,D13-1170,1,0.042951,"the Holy Roman Empire. The first sentence contains no words or named entities that by themselves are indicative of the answer, while subsequent sentences contain more and more obvious clues. where inputs are typically a single sentence and outputs are either continuous or a limited discrete set. Neural networks have not yet shown to be useful for tasks that require mapping paragraph-length inputs to rich output spaces. Introduction Deep neural networks have seen widespread use in natural language processing tasks such as parsing, language modeling, and sentiment analysis (Bengio et al., 2003; Socher et al., 2013a; Socher et al., 2013c). The vector spaces learned by these models cluster words and phrases together based on similarity. For example, a neural network trained for a sentiment analysis task such as restaurant review classification might learn that “tasty” and “delicious” should have similar representations since they are synonymous adjectives. These models have so far only seen success in a limited range of text-based prediction tasks, Consider factoid question answering: given a description of an entity, identify the person, place, or thing discussed. We describe a task with high-quality ma"
D14-1070,Q14-1017,1,0.765477,"te (see Section 4.1 for more details). Thus, we have built-in redundancy (the number of “askable” entities is limited), but also built-in diversity, as difficult clues cannot appear in every question without becoming well-known. Dependency-Tree Recursive Neural Networks To compute distributed representations for the individual sentences within quiz bowl questions, we use a dependency-tree rnn (dt-rnn). These representations are then aggregated and fed into a multinomial logistic regression classifier, where class labels are the answers associated with each question instance. In previous work, Socher et al. (2014) use dt-rnns to map text descriptions to images. dt-rnns are robust to similar sentences with slightly different syntax, which is ideal for our problem since answers are often described by many sentences that are similar in meaning but different in structure. Our model improves upon the existing dt-rnn model by jointly learning answer and question representations in the same vector space rather than learning them separately. 3.1 Model Description As in other rnn models, we begin by associating each word w in our vocabulary with a vector representation xw ∈ Rd . These vectors are stored as the"
D14-1070,D07-1003,0,0.0464695,"al., 2012) for Jeopardy, which is limited to single sentences, or to models trained on Yago (Hoffart et al., 2013). We would also like to fairly compare qanta 641 Q A Q A Q A Q A Q Akbar Muhammad Shah Jahan Ghana A Babur Figure 7: An extremely misleading question about John Cabot, at least to computer models. The words muslim and mecca lead to three Mughal emperors in the top five guesses from qanta; other models are similarly led awry. with ir-wiki. A promising avenue for future work would be to incorporate Wikipedia data into qanta by transforming sentences to look like quiz bowl questions (Wang et al., 2007) and to select relevant sentences, as not every sentence in a Wikipedia article directly describes its subject. Syntax-specific annotation (Sayeed et al., 2012) may help in this regard. Finally, we could adapt the attribute space learned by the dt-rnn to use information from knowledge bases and to aid in knowledge base completion. Having learned many facts about entities that occur in question text, a dt-rnn could add new facts to a knowledge base or check existing relationships. 8 Conclusion We present qanta, a dependency-tree recursive neural network for factoid question answering that outpe"
D14-1070,D11-1016,0,0.0348678,"on Wars, rebellions, and battles U.S. presidents Prime ministers Explorers & emperors Policies Other Figure 5: t-SNE 2-D projections of 451 answer vectors divided into six major clusters. The blue cluster is predominantly populated by U.S. presidents. The zoomed plot reveals temporal clustering among the presidents based on the years they spent in office. from the meaning of the words that it contains as well as the syntax that glues those words together. Many computational models of compositionality focus on learning vector spaces (Zanzotto et al., 2010; Erk, 2012; Grefenstette et al., 2013; Yessenalina and Cardie, 2011). Recent approaches towards modeling compositional vector spaces with neural networks have been successful, although simpler functions have been proposed for short phrases (Mitchell and Lapata, 2008). Recursive neural networks have achieved state-of-the-art performance in sentiment analysis and parsing (Socher et al., 2013c; Hermann and Blunsom, 2013; Socher et al., 2013a). rnns have not been previously used for learning attribute spaces as we do here, although recursive tensor networks were unsuccessfully applied to a knowledge base completion task (Socher et al., 2013b). More relevant to thi"
D14-1070,C10-1142,0,0.00402061,"ane_addams hull_house jimmy_carter ronald_reagan woodrow_wilson Wars, rebellions, and battles U.S. presidents Prime ministers Explorers & emperors Policies Other Figure 5: t-SNE 2-D projections of 451 answer vectors divided into six major clusters. The blue cluster is predominantly populated by U.S. presidents. The zoomed plot reveals temporal clustering among the presidents based on the years they spent in office. from the meaning of the words that it contains as well as the syntax that glues those words together. Many computational models of compositionality focus on learning vector spaces (Zanzotto et al., 2010; Erk, 2012; Grefenstette et al., 2013; Yessenalina and Cardie, 2011). Recent approaches towards modeling compositional vector spaces with neural networks have been successful, although simpler functions have been proposed for short phrases (Mitchell and Lapata, 2008). Recursive neural networks have achieved state-of-the-art performance in sentiment analysis and parsing (Socher et al., 2013c; Hermann and Blunsom, 2013; Socher et al., 2013a). rnns have not been previously used for learning attribute spaces as we do here, although recursive tensor networks were unsuccessfully applied to a knowle"
D14-1162,P14-1023,0,0.148116,"hieves better results faster, and also obtains the best results irrespective of speed. 5 Conclusion Recently, considerable attention has been focused on the question of whether distributional word representations are best learned from count-based 9 In contrast, noise-contrastive estimation is an approximation which improves with more negative samples. In Table 1 of (Mnih et al., 2013), accuracy on the analogy task is a non-decreasing function of the number of negative samples. methods or from prediction-based methods. Currently, prediction-based models garner substantial support; for example, Baroni et al. (2014) argue that these models perform better across a range of tasks. In this work we argue that the two classes of methods are not dramatically different at a fundamental level since they both probe the underlying co-occurrence statistics of the corpus, but the efficiency with which the count-based methods capture global statistics can be advantageous. We construct a model that utilizes this main benefit of count data while simultaneously capturing the meaningful linear substructures prevalent in recent log-bilinear prediction-based methods like word2vec. The result, GloVe, is a new global log-bil"
D14-1162,E14-1051,0,0.148116,"co-occurrence matrix is first transformed by an entropy- or correlation-based normalization. An advantage of this type of transformation is that the raw co-occurrence counts, which for a reasonably sized corpus might span 8 or 9 orders of magnitude, are compressed so as to be distributed more evenly in a smaller interval. A variety of newer models also pursue this approach, including a study (Bullinaria and Levy, 2007) that indicates that positive pointwise mutual information (PPMI) is a good transformation. More recently, a square root type transformation in the form of Hellinger PCA (HPCA) (Lebret and Collobert, 2014) has been suggested as an effective way of learning word representations. Shallow Window-Based Methods. Another approach is to learn word representations that aid in making predictions within local context windows. For example, Bengio et al. (2003) introduced a model that learns word vector representations as part of a simple neural network architecture for language modeling. Collobert and Weston (2008) decoupled the word vector training from the downstream training objectives, which paved the way for Collobert et al. (2011) to use the full context of a word for learning the word representatio"
D14-1162,W14-1618,0,0.148116,"way for Collobert et al. (2011) to use the full context of a word for learning the word representations, rather than just the preceding context as is the case with language models. Recently, the importance of the full neural network structure for learning useful word representations has been called into question. The skip-gram and continuous bag-of-words (CBOW) models of Mikolov et al. (2013a) propose a simple single-layer architecture based on the inner product between two word vectors. Mnih and Kavukcuoglu (2013) also proposed closely-related vector log-bilinear models, vLBL and ivLBL, and Levy et al. (2014) proposed explicit word embeddings based on a PPMI metric. In the skip-gram and ivLBL models, the objective is to predict a word’s context given the word itself, whereas the objective in the CBOW and vLBL models is to predict a word given its context. Through evaluation on a word analogy task, these models demonstrated the capacity to learn linguistic patterns as linear relationships between the word vectors. Unlike the matrix factorization methods, the shallow window-based methods suffer from the disadvantage that they do not operate directly on the co-occurrence statistics of the corpus. Ins"
D14-1162,W13-3512,1,0.148116,"hat is depends on whether α > 1, ( O(|C|) if α < 1, |X |= (22) 1/α O(|C |) if α > 1. For the corpora studied in this article, we observe that X i j is well-modeled by Eqn. (17) with α = 1.25. In this case we have that |X |= O(|C |0.8 ). Therefore we conclude that the complexity of the model is much better than the worst case O(V 2 ), and in fact it does somewhat better than the on-line window-based methods which scale like O(|C|). 4 4.1 Experiments Evaluation methods We conduct experiments on the word analogy task of Mikolov et al. (2013a), a variety of word similarity tasks, as described in (Luong et al., 2013), and on the CoNLL-2003 shared benchmark Dim. 100 100 100 300 300 300 300 300 300 300 300 300 300 300 1000 1000 300 300 Size 1.5B 1.6B 1.6B 1B 1.6B 1.5B 1.5B 1.6B 6B 6B 6B 6B 6B 6B 6B 6B 42B 42B Sem. 55.9 4.2 67.5 61 16.1 54.2 65.2 80.8 6.3 36.7 56.6 63.6 73.0 77.4 57.3 66.1 38.4 81.9 Syn. 50.1 16.4 54.3 61 52.6 64.8 63.0 61.5 8.1 46.6 63.0 67.4 66.0 67.0 68.9 65.1 58.2 69.3 Tot. 53.2 10.8 60.3 61 36.1 60.0 64.0 70.3 7.3 42.1 60.1 65.7 69.1 71.7 63.7 65.6 49.2 75.0 dataset for NER (Tjong Kim Sang and De Meulder, 2003). Word analogies. The word analogy task consists of questions like, “a is to"
D14-1162,N13-1090,0,0.148116,"recognition. 1 Introduction Semantic vector space models of language represent each word with a real-valued vector. These vectors can be used as features in a variety of applications, such as information retrieval (Manning et al., 2008), document classification (Sebastiani, 2002), question answering (Tellex et al., 2003), named entity recognition (Turian et al., 2010), and parsing (Socher et al., 2013). Most word vector methods rely on the distance or angle between pairs of word vectors as the primary method for evaluating the intrinsic quality of such a set of word representations. Recently, Mikolov et al. (2013c) introduced a new evaluation scheme based on word analogies that probes the finer structure of the word vector space by examining not the scalar distance between word vectors, but rather their various dimensions of difference. For example, the analogy “king is to queen as man is to woman” should be encoded in the vector space by the vector equation king − queen = man − woman. This evaluation scheme favors models that produce dimensions of meaning, thereby capturing the multi-clustering idea of distributed representations (Bengio, 2009). The two main model families for learning word vectors a"
D14-1162,P13-1045,1,0.148116,"Missing"
D14-1162,P10-1040,0,0.148116,"n addition to the HPCA and SVD models discussed previously, we also compare to the models of Huang et al. (2012) (HSMN) and Collobert and Weston (2008) (CW). We trained the CBOW model using the word2vec tool8 . The GloVe model outperforms all other methods on all evaluation metrics, except for the CoNLL test set, on which the HPCA method does slightly better. We conclude that the GloVe vectors are useful in downstream NLP tasks, as was first 8 We use the same parameters as above, except in this case we found 5 negative samples to work slightly better than 10. 1539 shown for neural vectors in (Turian et al., 2010). 4.4 Overall 75 70 65 60 55 50 Wiki2010 1B tokens Wiki2014 1.6B tokens Gigaword5 4.3B tokens Gigaword5 + Wiki2014 6B tokens Common Crawl 42B tokens Figure 3: Accuracy on the analogy task for 300dimensional vectors trained on different corpora. entries are updated to assimilate new knowledge, whereas Gigaword is a fixed news repository with outdated and possibly incorrect information. Model Analysis: Run-time The total run-time is split between populating X and training the model. The former depends on many factors, including window size, vocabulary size, and corpus size. Though we did not do"
D14-1162,I13-1183,1,0.148116,"and Charles, 1991), RG (Rubenstein and Goodenough, 1965), SCWS (Huang et al., 2012), and RW (Luong et al., 2013). Named entity recognition. The CoNLL-2003 English benchmark dataset for NER is a collection of documents from Reuters newswire articles, annotated with four entity types: person, location, organization, and miscellaneous. We train models on CoNLL-03 training data on test on three datasets: 1) ConLL-03 testing data, 2) ACE Phase 2 (2001-02) and ACE-2003 data, and 3) MUC7 Formal Run test set. We adopt the BIO2 annotation standard, as well as all the preprocessing steps described in (Wang and Manning, 2013). We use a comprehensive set of discrete features that comes with the standard distribution of the Stanford NER model (Finkel et al., 2005). A total of 437,905 discrete features were generated for the CoNLL2003 training dataset. In addition, 50-dimensional vectors for each word of a five-word context are added and used as continuous features. With these features as input, we trained a conditional random field (CRF) with exactly the same setup as the CRFjoin model of (Wang and Manning, 2013). 4.2 Corpora and training details We trained our model on five corpora of varying sizes: a 2010 Wikipedi"
D14-1162,W03-0419,0,\N,Missing
D14-1162,P12-1092,1,\N,Missing
D17-1206,D15-1159,0,0.00674333,"= [ht−1 ; ht ; xt ; yt ], where ht is the hidden state of the first (POS) layer. We define (pos) the weighted label embedding yt as follows: (pos) yt = C X j=1 (1) p(yt (1) = j|ht )`(j), (2) (1) where C is the number of the POS tags, p(yt = (1) j|ht ) is the probability value that the j-th POS tag is assigned to wt , and `(j) is the corresponding label embedding. The probability values are predicted by the POS layer, and thus no gold POS tags are needed. This output embedding is similar to the K-best POS tag feature which has been shown to be effective in syntactic tasks (Andor et al., 2016; Alberti et al., 2015). For predicting the chunking tags, we employ the same strategy as POS tagging by using the concatenated bi→ − (2) ← −(2) (2) directional hidden states ht = [ h t ; h t ] in the chunking layer. We also use a single ReLU hidden layer before the softmax classifier. 1924 2.4 Syntactic Task: Dependency Parsing Dependency parsing identifies syntactic relations (such as an adjective modifying a noun) between word pairs in a sentence. We use the third biLSTM layer to classify relations between all pairs of words. The input vector for the LSTM includes hidden states, word representations, and the labe"
D17-1206,P16-1231,0,0.284093,"(2) (1) (pos) (1) gt = [ht−1 ; ht ; xt ; yt ], where ht is the hidden state of the first (POS) layer. We define (pos) the weighted label embedding yt as follows: (pos) yt = C X j=1 (1) p(yt (1) = j|ht )`(j), (2) (1) where C is the number of the POS tags, p(yt = (1) j|ht ) is the probability value that the j-th POS tag is assigned to wt , and `(j) is the corresponding label embedding. The probability values are predicted by the POS layer, and thus no gold POS tags are needed. This output embedding is similar to the K-best POS tag feature which has been shown to be effective in syntactic tasks (Andor et al., 2016; Alberti et al., 2015). For predicting the chunking tags, we employ the same strategy as POS tagging by using the concatenated bi→ − (2) ← −(2) (2) directional hidden states ht = [ h t ; h t ] in the chunking layer. We also use a single ReLU hidden layer before the softmax classifier. 1924 2.4 Syntactic Task: Dependency Parsing Dependency parsing identifies syntactic relations (such as an adjective modifying a noun) between word pairs in a sentence. We use the third biLSTM layer to classify relations between all pairs of words. The input vector for the LSTM includes hidden states, word repres"
D17-1206,C10-1011,0,0.0644419,"Missing"
D17-1206,Q17-1010,0,0.0304414,"encies improve the relatedness task. The relatedness and entailment tasks are closely related to each other. If the semantic relatedness between two sentences is very low, they are unlikely to entail each other. Based on this observation, we make use of the information from the relatedness task for improving the entailment task. 2.1 it = σ (Wi gt + bi ) , ft = σ (Wf gt + bf ) , ut = tanh (Wu gt + bu ) , ct = it ut + ft ct−1 , Word-Level Task: POS Tagging The first layer of the model is a bi-directional LSTM (Graves and Schmidhuber, 2005; Hochreiter and Schmidhuber, 1997) whose hidden states 1 Bojanowski et al. (2017) previously proposed to train the character n-gram embeddings by the Skip-gram objective. (1) ot = σ (Wo gt + bo ) , ht = ot tanh (ct ) , → − where we define the input gt as gt = [ h t−1 ; xt ], i.e. the concatenation of the previous hidden state and the word representation of wt . The backward pass is expanded in the same way, but a different set of weights are used. For predicting the POS tag of wt , we use the concatenation of the forward and backward states in a one-layer bi-LSTM layer corresponding to the → − ← − t-th word: ht = [ h t ; h t ]. Then each ht (1 ≤ t ≤ L) is fed into a standa"
D17-1206,D16-1257,0,0.0160407,"histicated attention mechanism called biaffine attention. It should be promising to incorporate their attention mechanism into our parsing component. Semantic relatedness Table 5 shows the results of the semantic relatedness task, and our JMT model achieves the state-of-the-art result. The result of “JMTDE ” is already better than the previous state-of-the-art results. Both of Zhou et al. (2016) and Tai et al. (2015) explicitly used syntactic trees, and Zhou et al. (2016) relied on attention mechanisms. However, our method uses the simple maxpooling strategy, which suggests that it is worth 4 Choe and Charniak (2016) employed a tri-training method to expand the training data with 400,000 trees in addition to the WSJ data, and they reported 95.9 UAS and 94.1 LAS by converting their constituency trees into dependency trees. Kuncoro et al. (2017) also reported high accuracy (95.8 UAS and 94.6 LAS) by using a converter. 1928 A↑ B↑ Single 97.45 95.02 93.35 91.42 0.247 81.8 POS Chunking Dependency UAS Dependency LAS Relatedness Entailment C↑ D↓ E↑ JMTall 97.55 n/a 94.67 92.90 0.233 86.2 JMTAB 97.52 95.77 n/a n/a n/a n/a JMTABC 97.54 n/a 94.71 92.92 n/a n/a JMTDE n/a n/a n/a n/a 0.238 86.8 JMTCD n/a n/a 93.53 91"
D17-1206,P15-1033,0,0.0112343,"Missing"
D17-1206,P96-1011,0,0.0606322,"didates of the par(3) (3) ent node as m (t, j) = ht · (Wd hj ), where Wd is a parameter matrix. For the root, we define (3) hL+1 = r as a parameterized vector. To compute the probability that wj (or the root node) is the parent of wt , the scores are normalized: exp (m (t, j)) (3) p(j|ht ) = PL+1 . k=1,k6=t exp (m (t, k)) (3) The dependency labels are predicted using as input to a softmax classifier with a single ReLU layer. We greedily select the parent node and the dependency label for each word. When the parsing result is not a well-formed tree, we apply the first-order Eisner’s algorithm (Eisner, 1996) to obtain a well-formed tree from it. (3) (3) [ht ; hj ] 2.5 Semantic Task: Semantic relatedness The next two tasks model the semantic relationships between two input sentences. The first task measures the semantic relatedness between two sentences. The output is a real-valued relatedness score for the input sentence pair. The second task is textual entailment, which requires one to determine whether a premise sentence entails a hypothesis sentence. There are typically three classes: entailment, contradiction, and neutral. We use the fourth and fifth bi-LSTM layer for the relatedness and enta"
D17-1206,P16-1078,1,0.760695,"on Sentence1 Sentence2 Figure 1: Overview of the joint many-task model predicting different linguistic outputs at successively deeper layers. different tasks either entirely separately or at the same depth (Collobert et al., 2011). Introduction The potential for leveraging multiple levels of representation has been demonstrated in various ways in the field of Natural Language Processing (NLP). For example, Part-Of-Speech (POS) tags are used for syntactic parsers. The parsers are used to improve higher-level tasks, such as natural language inference (Chen et al., 2016) and machine translation (Eriguchi et al., 2016). These systems are often pipelines and not trained end-to-end. Deep NLP models have yet shown benefits from predicting many increasingly complex tasks each at a successively deeper layer. Existing models often ignore linguistic hierarchies by predicting ∗ Entailment encoder word level semantic level Transfer and multi-task learning have traditionally focused on either a single source-target pair or very few, similar tasks. Ideally, the linguistic levels of morphology, syntax and semantics would benefit each other by being trained in a single model. We introduce a joint many-task model togethe"
D17-1206,D17-1012,1,0.826872,"Missing"
D17-1206,P82-1020,0,0.819591,"Missing"
D17-1206,N01-1025,0,0.0296584,"Missing"
D17-1206,J81-4005,0,0.687478,"Missing"
D17-1206,E17-1117,0,0.0281293,"Missing"
D17-1206,S14-2055,0,0.0131081,"Missing"
D17-1206,D16-1076,0,0.0204543,"Missing"
D17-1206,D15-1176,0,0.0300327,"heir constituency trees into dependency trees. Kuncoro et al. (2017) also reported high accuracy (95.8 UAS and 94.6 LAS) by using a converter. 1928 A↑ B↑ Single 97.45 95.02 93.35 91.42 0.247 81.8 POS Chunking Dependency UAS Dependency LAS Relatedness Entailment C↑ D↓ E↑ JMTall 97.55 n/a 94.67 92.90 0.233 86.2 JMTAB 97.52 95.77 n/a n/a n/a n/a JMTABC 97.54 n/a 94.71 92.92 n/a n/a JMTDE n/a n/a n/a n/a 0.238 86.8 JMTCD n/a n/a 93.53 91.62 0.251 n/a JMTCE n/a n/a 93.57 91.69 n/a 82.4 Table 1: Test set results for the five tasks. In the relatedness task, the lower scores are better. Method JMTall Ling et al. (2015) Kumar et al. (2016) Ma and Hovy (2016) Søgaard (2011) Collobert et al. (2011) Tsuruoka et al. (2011) Toutanova et al. (2003) Acc. ↑ 97.55 97.78 97.56 97.55 97.50 97.29 97.28 97.27 Method JMTAB Single Søgaard and Goldberg (2016) Suzuki and Isozaki (2008) Collobert et al. (2011) Kudo and Matsumoto (2001) Tsuruoka et al. (2011) Table 3: Chunking results. Table 2: POS tagging results. Method JMTall JMTDE Zhou et al. (2016) Tai et al. (2015) MSE ↓ 0.233 0.238 0.243 0.253 JMTall 97.88 97.59 94.51 92.60 0.236 84.6 w/o SC 97.79 97.08 94.52 92.62 0.698 75.0 w/o LE 97.85 97.40 94.09 92.14 0.261 81.6 LA"
D17-1206,P16-1101,0,0.181132,"Missing"
D17-1206,P16-1105,0,0.122728,"closely-related tasks, such as POS tagging and chunking. However, the number of tasks was limited or they have very similar task settings like word-level tagging, and it was not clear how lower-level tasks could be also improved by combining higher-level tasks. More related to our work, Godwin et al. (2016) also followed Søgaard and Goldberg (2016) to jointly learn POS tagging, chunking, and language modeling, and Zhang and Weiss (2016) have shown that it is effective to jointly learn POS tagging and dependency parsing by sharing internal representations. In the field of relation extraction, Miwa and Bansal (2016) proposed a joint learning model for entity detection and relation extraction. All of them suggest the importance of multi-task learning, and we investigate the potential of handling different types of NLP tasks rather than closely-related ones in a single hierarchical deep model. In the field of computer vision, some transfer and multi-task learning approaches have also been proposed (Li and Hoiem, 2016; Misra et al., 2016). For example, Misra et al. (2016) proposed a multi-task learning model to handle different tasks. However, they assume that each data sample has annotations for the differ"
D17-1206,P11-2009,0,0.0126651,"Missing"
D17-1206,P16-2038,0,0.691838,"ce Research. † Corresponding author. We introduce a Joint Many-Task (JMT) model, outlined in Figure 1, which predicts increasingly complex NLP tasks at successively deeper layers. Unlike traditional pipeline systems, our single JMT model can be trained end-to-end for POS tagging, chunking, dependency parsing, semantic relatedness, and textual entailment, by considering linguistic hierarchies. We propose an adaptive training and regularization strategy to grow this model in its depth. With the help of this strategy we avoid catastrophic interference between the tasks. Our model is motivated by Søgaard and Goldberg (2016) who showed that predicting two different tasks is more accurate when performed in different layers than in the same layer (Collobert et al., 2011). Experimental results show that our single model achieves competitive results for all of the five different tasks, demonstrating that us1923 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1923–1933 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics ing linguistic hierarchies is more important than handling different tasks in the same layer. 2 The Joint Many-Task"
D17-1206,N03-1033,0,0.0461524,"Missing"
D17-1206,W11-0328,1,0.301353,"Missing"
D17-1206,P15-1032,0,0.0320417,"Missing"
D17-1206,Q16-1019,0,0.0649491,"04 92.03 0.765 71.2 POS Chunking Dependency UAS Dependency LAS Table 7: Effectiveness of the Shortcut Connections (SC) and the Label Embeddings (LE). investigating such simple methods before developing complex methods for simple tasks. Currently, our JMT model does not explicitly use the learned dependency structures, and thus the explicit use of the output from the dependency layer should be an interesting direction of future work. Textual entailment Table 6 shows the results of textual entailment, and our JMT model achieves the state-of-the-art result. The previous state-ofthe-art result in Yin et al. (2016) relied on attention mechanisms and dataset-specific data preprocessing and features. Again, our simple maxpooling strategy achieves the state-of-the-art result boosted by the joint training. These results show the importance of jointly handling related tasks. 6.2 UAS ↑ 94.67 93.35 95.74 94.61 94.23 94.10 93.99 93.10 92.88 Table 4: Dependency results. Method JMTall JMTDE Yin et al. (2016) Lai and Hockenmaier (2014) Table 5: Semantic relatedness results. POS Chunking Dependency UAS Dependency LAS Relatedness Entailment Method JMTall Single Dozat and Manning (2017) Andor et al. (2016) Alberti et"
D17-1206,E17-1063,0,0.0843233,"ch as an adjective modifying a noun) between word pairs in a sentence. We use the third biLSTM layer to classify relations between all pairs of words. The input vector for the LSTM includes hidden states, word representations, and the label embeddings for the two previous tasks: (3) (3) (2) (pos) (chk) gt = [ht−1 ; ht ; xt ; (yt + yt )], where we computed the chunking vector in a similar fashion as the POS vector in Eq. (2). We predict the parent node (head) for each word. Then a dependency label is predicted for each child-parent pair. This approach is related to Dozat and Manning (2017) and Zhang et al. (2017), where the main difference is that our model works on a multi-task framework. To predict the parent node of wt , we define a matching function between wt and the candidates of the par(3) (3) ent node as m (t, j) = ht · (Wd hj ), where Wd is a parameter matrix. For the root, we define (3) hL+1 = r as a parameterized vector. To compute the probability that wj (or the root node) is the parent of wt , the scores are normalized: exp (m (t, j)) (3) p(j|ht ) = PL+1 . k=1,k6=t exp (m (t, k)) (3) The dependency labels are predicted using as input to a softmax classifier with a single ReLU layer. We gr"
D17-1206,P16-1147,0,0.107798,"Missing"
D17-1206,C16-1274,0,0.0956201,"uracy without the POS and chunking information. The best result to date has been achieved by the model propsoed in Dozat and Manning (2017), which uses higher dimensional representations than ours and proposes a more sophisticated attention mechanism called biaffine attention. It should be promising to incorporate their attention mechanism into our parsing component. Semantic relatedness Table 5 shows the results of the semantic relatedness task, and our JMT model achieves the state-of-the-art result. The result of “JMTDE ” is already better than the previous state-of-the-art results. Both of Zhou et al. (2016) and Tai et al. (2015) explicitly used syntactic trees, and Zhou et al. (2016) relied on attention mechanisms. However, our method uses the simple maxpooling strategy, which suggests that it is worth 4 Choe and Charniak (2016) employed a tri-training method to expand the training data with 400,000 trees in addition to the WSJ data, and they reported 95.9 UAS and 94.1 LAS by converting their constituency trees into dependency trees. Kuncoro et al. (2017) also reported high accuracy (95.8 UAS and 94.6 LAS) by using a converter. 1928 A↑ B↑ Single 97.45 95.02 93.35 91.42 0.247 81.8 POS Chunking De"
D17-1206,P08-1076,0,0.0292929,"Missing"
D17-1206,P15-1150,1,0.333124,"Missing"
D18-1207,N15-1014,0,0.0325782,"each evaluation criterion are reported in Table 4. These results show that our model matches the relevance score of See et al. (2017) and Liu et al. (2018), but is slightly inferior to them in terms of readability. 5 Related work Text summarization. Existing summarization approaches are usually either extractive or abstractive. In extractive summarization, the model selects passages from the input document and combines them to form a shorter summary, sometimes with a post-processing step to ensure final coherence of the output (Neto et al., 2002; Dorr et al., 2003; Filippova and Altun, 2013; Colmenares et al., 2015; Nallapati et al., 2017). While extractive models are usually robust and produce coherent summaries, they cannot create concise summaries that paraphrase the source document using new phrases. Abstractive summarization allows the model to paraphrase the source document and create concise summaries with phrases not in the source document. The state-of-the-art abstractive summarization models are based on sequence-tosequence models with attention (Bahdanau et al., 2015). Extensions to this model include a selfattention mechanism (Paulus et al., 2017) and an article coverage vector (See et al.,"
D18-1207,W03-0501,0,0.172857,"porates prior knowledge about language generation. Second, we propose a novelty metric that is optimized directly through policy learning to encourage the generation of novel phrases. Our model achieves results comparable to state-of-the-art models, as determined by ROUGE scores and human evaluations, while achieving a significantly higher level of abstraction as measured by n-gram overlap with the source document. 1 Introduction Text summarization concerns the task of compressing a long sequence of text into a more concise form. The two most common approaches to summarization are extractive (Dorr et al., 2003; Nallapati et al., 2017), where the model extracts salient parts of the source document, and abstractive (Paulus et al., 2017; See et al., 2017), where the model not only extracts but also concisely paraphrases the important parts of the document via generation. We focus on developing a summarization model that produces an increased level of abstraction. That is, the model produces concise summaries without only copying long passages from the source document. ∗ Work performed while at Salesforce Research. A high quality summary is shorter than the original document, conveys only the most impo"
D18-1207,K16-1028,0,0.192942,"Missing"
D18-1207,P16-1188,0,0.0660358,"and Cieri, 2003) and some DUC datasets (Over et al., 2007) have been used for headline generation models (Rush et al., 2015; Nallapati et al., 2016), where the generated summary is shorter than 75 characters. However, generating longer summaries is a more challenging task, especially for abstractive models. Nallapati et al. (2016) have proposed using the CNN/Daily Mail dataset (Hermann et al., 2015) to train models for generating longer, multi-sentence summaries up to 100 words. The New York Times dataset (Sandhaus, 2008) has also been used as a benchmark for the generation of long summaries (Durrett et al., 2016; Paulus et al., 2017). Training strategies for sequential models. The common approach to training models for sequence generation is maximum likelihood estimation with teacher forcing. At each time step, the model is given the previous ground-truth output and predicts the current output. The sequence objective is the accumulation of cross entropy losses from each time step. Despite its popularity, this approach for sequence generation is suboptimal due to exposure bias (Huszar, 2015) and loss-evaluation mismatch (Wiseman and Rush, 2016). Goyal et al. (2016) propose one way to reduce exposure b"
D18-1207,D13-1155,0,0.104206,"Missing"
D18-1207,P82-1020,0,0.814979,"Missing"
D18-1207,W04-1013,0,0.116623,". We focus on developing a summarization model that produces an increased level of abstraction. That is, the model produces concise summaries without only copying long passages from the source document. ∗ Work performed while at Salesforce Research. A high quality summary is shorter than the original document, conveys only the most important and no extraneous information, and is semantically and syntactically correct. Because it is difficult to gauge the correctness of the summary, evaluation metrics for summarization models use word overlap with the ground-truth summary in the form of ROUGE (Lin, 2004) scores. However, word overlap metrics do not capture the abstractive nature of high quality human-written summaries: the use of paraphrases with words that do not necessarily appear in the source document. The state-of-the-art abstractive text summarization models have high word overlap performance, however they tend to copy long passages of the source document directly into the summary, thereby producing summaries that are not abstractive (See et al., 2017). We propose two general extensions to summarization models that improve the level of abstraction of the summary while preserving word ov"
D18-1207,N18-2102,0,0.0978431,"ts are omitted in cases where they have not been made available by previous authors. We also include the novel n-gram scores for the ground-truth summaries as a comparison to indicate the level of abstraction of human written summaries. 1812 Model R-1 R-2 R-L NN-1 NN-2 NN-3 NN-4 anonymized Ground-truth summaries ML+RL, intra-attn (Paulus et al., 2017) 39.87 15.82 36.9 14.40 1.04 52.07 10.86 71.63 21.53 80.84 29.27 ML+RL ROUGE+Novel, with LM (ours) 40.02 15.53 37.44 3.54 21.91 37.48 47.13 full-text Ground-truth summaries Pointer-gen + coverage (See et al., 2017) SumGAN (Liu et al., 2018) RSal (Pasunuru and Bansal, 2018) RSal+Ent RL (Pasunuru and Bansal, 2018) 39.53 39.92 40.36 40.43 17.28 17.65 17.97 18.00 36.38 36.71 37.00 37.10 13.55 0.07 0.22 - 49.97 2.24 3.15 2.37 - 70.32 6.03 7.68 6.00 - 80.02 9.72 11.84 9.50 - ML+RL ROUGE+Novel, with LM (ours) 40.19 17.38 37.52 3.25 17.21 30.46 39.47 Table 2: Comparison of ROUGE (R-) and novel n-gram (NN-) test results for our model and other abstractive summarization models on the CNN/Daily Mail dataset. Even though our model outputs significantly fewer novel n-grams than human written summaries, it has a much higher percentage of novel n-grams than all the previous a"
D18-1207,D15-1044,0,0.386242,"Missing"
D18-1207,P17-1099,0,0.815631,"urage the generation of novel phrases. Our model achieves results comparable to state-of-the-art models, as determined by ROUGE scores and human evaluations, while achieving a significantly higher level of abstraction as measured by n-gram overlap with the source document. 1 Introduction Text summarization concerns the task of compressing a long sequence of text into a more concise form. The two most common approaches to summarization are extractive (Dorr et al., 2003; Nallapati et al., 2017), where the model extracts salient parts of the source document, and abstractive (Paulus et al., 2017; See et al., 2017), where the model not only extracts but also concisely paraphrases the important parts of the document via generation. We focus on developing a summarization model that produces an increased level of abstraction. That is, the model produces concise summaries without only copying long passages from the source document. ∗ Work performed while at Salesforce Research. A high quality summary is shorter than the original document, conveys only the most important and no extraneous information, and is semantically and syntactically correct. Because it is difficult to gauge the correctness of the summa"
D18-1207,D16-1137,0,0.0329756,"en used as a benchmark for the generation of long summaries (Durrett et al., 2016; Paulus et al., 2017). Training strategies for sequential models. The common approach to training models for sequence generation is maximum likelihood estimation with teacher forcing. At each time step, the model is given the previous ground-truth output and predicts the current output. The sequence objective is the accumulation of cross entropy losses from each time step. Despite its popularity, this approach for sequence generation is suboptimal due to exposure bias (Huszar, 2015) and loss-evaluation mismatch (Wiseman and Rush, 2016). Goyal et al. (2016) propose one way to reduce exposure bias by explicitly forcing the hidden representations of the model to be similar during training and inference. Bengio et al. (2015) and Wiseman and Rush (2016) propose an alternate method that exposes the network to the test dynamics during training. Reinforcement learning methods (Sutton and Barto, 1998), such as policy learning (Sutton et al., 1999), mitigate the mismatch between the optimization objective and the evaluation metrics by directly optimizing evaluation metrics. This approach has led to consistent improvements in domains"
D18-1362,D13-1160,0,0.0812083,"existing path-based KGQA models on several benchmark datasets and is comparable or better than embedding-based models. 1 belong_to? U.S. Government collaborate _with collaborate_with Barack_Obama John_McCain endorsed_by born_in Hawaii collaborate_with? locate_in U.S. live_in live_in collaborate _with Hillary_Clinton Figure 1: Example of an incomplete knowledge graph which contains missing links (dashed lines) that can possibly be inferred from existing facts (solid lines). Introduction Large-scale knowledge graphs (KGs) support a variety of downstream NLP applications such as semantic search (Berant et al., 2013) and dialogue generation (He et al., 2017). Whether curated automatically or manually, practical KGs often fail to include many relevant facts. A popular approach for modeling incomplete KGs is knowledge graph embeddings, which map both entities and relations in the KG to a vector space and learn a truth value function for any potential KG triple parameterized by the entity and relation vectors (Yang et al., 2014; Dettmers et al., 2018). Embedding based approaches ignore the symbolic compositionality of KG relations, which limit their application in more complex reasoning tasks. An alternative"
D18-1362,N18-1165,0,0.486888,"asoning offers logical insights of the underlying KG and are more directly interpretable. Early work treats it as a link prediction problem and perform maximum-likelihood classification over either discrete path features (Lao et al., 2011, 2012; Gardner et al., 2013) or their hidden representations in a vector space (Guu et al., 2015; Toutanova et al., 2016; McCallum et al., 2017). More recent work formulates multi-hop reasoning as a sequential decision problem, and leverages reinforcement learning (RL) to perform effective path search (Xiong et al., 2017; Das et al., 2018; Shen et al., 2018; Chen et al., 2018). In particular, MINERVA (Das et al., 2018) uses the REINFORCE algorithm (Williams, 1992) to train an end-to-end model for multi-hop KG query answering: given a query relation and a source entity, the trained agent searches over the KG starting from the source and arrives at the candidate answers without access to any pre-computed paths. 3243 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 3243–3253 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics Figure 2: Percentage of false negatives hit (where t"
D18-1362,D13-1080,0,0.103185,"d approaches ignore the symbolic compositionality of KG relations, which limit their application in more complex reasoning tasks. An alternative solution for KG reasoning is to infer missing facts by synthesizing information from multi-hop paths, e.g. bornIn(Obama, Hawaii) ∧ locatedIn(Hawaii, US) ⇒ bornIn(Obama, US), as shown in Figure 1. Path-based reasoning offers logical insights of the underlying KG and are more directly interpretable. Early work treats it as a link prediction problem and perform maximum-likelihood classification over either discrete path features (Lao et al., 2011, 2012; Gardner et al., 2013) or their hidden representations in a vector space (Guu et al., 2015; Toutanova et al., 2016; McCallum et al., 2017). More recent work formulates multi-hop reasoning as a sequential decision problem, and leverages reinforcement learning (RL) to perform effective path search (Xiong et al., 2017; Das et al., 2018; Shen et al., 2018; Chen et al., 2018). In particular, MINERVA (Das et al., 2018) uses the REINFORCE algorithm (Williams, 1992) to train an end-to-end model for multi-hop KG query answering: given a query relation and a source entity, the trained agent searches over the KG starting from"
D18-1362,D15-1038,0,0.0725629,"Missing"
D18-1362,P17-1097,0,0.405676,"use practical KGs are intrinsically incomplete, the agent may arrive at a correct answer whose link to the source entity is missing from the training graph without receiving any reward (false negative targets, Figure 2). Second, since no ground truth path is available for training, the agent may traverse spurious paths that lead to a correct answer only incidentally (false positive paths). Because REINFORCE (Williams, 1992) is an on-policy RL algorithm (Sutton and Barto, 1998) which encourages past actions with high reward, it can bias the policy toward spurious paths found early in training (Guu et al., 2017). We propose two modeling advances for RL approaches in the walk-based QA framework to address the aforementioned problems. First, instead of using a binary reward based on whether the agent has reached a correct answer or not, we adopt pre-trained state-of-the-art embeddingbased models (Dettmers et al., 2018; Trouillon et al., 2016) to estimate a soft reward for target entities whose correctness cannot be determined. As embedding-based models capture link semantics well, unobserved but correct answers would receive a higher reward score compared to a true negative entity using a well-trained"
D18-1362,P17-1162,0,0.0361183,"chmark datasets and is comparable or better than embedding-based models. 1 belong_to? U.S. Government collaborate _with collaborate_with Barack_Obama John_McCain endorsed_by born_in Hawaii collaborate_with? locate_in U.S. live_in live_in collaborate _with Hillary_Clinton Figure 1: Example of an incomplete knowledge graph which contains missing links (dashed lines) that can possibly be inferred from existing facts (solid lines). Introduction Large-scale knowledge graphs (KGs) support a variety of downstream NLP applications such as semantic search (Berant et al., 2013) and dialogue generation (He et al., 2017). Whether curated automatically or manually, practical KGs often fail to include many relevant facts. A popular approach for modeling incomplete KGs is knowledge graph embeddings, which map both entities and relations in the KG to a vector space and learn a truth value function for any potential KG triple parameterized by the entity and relation vectors (Yang et al., 2014; Dettmers et al., 2018). Embedding based approaches ignore the symbolic compositionality of KG relations, which limit their application in more complex reasoning tasks. An alternative solution for KG reasoning is to infer mis"
D18-1362,D11-1049,0,0.34106,"., 2018). Embedding based approaches ignore the symbolic compositionality of KG relations, which limit their application in more complex reasoning tasks. An alternative solution for KG reasoning is to infer missing facts by synthesizing information from multi-hop paths, e.g. bornIn(Obama, Hawaii) ∧ locatedIn(Hawaii, US) ⇒ bornIn(Obama, US), as shown in Figure 1. Path-based reasoning offers logical insights of the underlying KG and are more directly interpretable. Early work treats it as a link prediction problem and perform maximum-likelihood classification over either discrete path features (Lao et al., 2011, 2012; Gardner et al., 2013) or their hidden representations in a vector space (Guu et al., 2015; Toutanova et al., 2016; McCallum et al., 2017). More recent work formulates multi-hop reasoning as a sequential decision problem, and leverages reinforcement learning (RL) to perform effective path search (Xiong et al., 2017; Das et al., 2018; Shen et al., 2018; Chen et al., 2018). In particular, MINERVA (Das et al., 2018) uses the REINFORCE algorithm (Williams, 1992) to train an end-to-end model for multi-hop KG query answering: given a query relation and a source entity, the trained agent searc"
D18-1362,D12-1093,0,0.0602325,"Missing"
D18-1362,E17-1013,0,0.0633859,"asoning tasks. An alternative solution for KG reasoning is to infer missing facts by synthesizing information from multi-hop paths, e.g. bornIn(Obama, Hawaii) ∧ locatedIn(Hawaii, US) ⇒ bornIn(Obama, US), as shown in Figure 1. Path-based reasoning offers logical insights of the underlying KG and are more directly interpretable. Early work treats it as a link prediction problem and perform maximum-likelihood classification over either discrete path features (Lao et al., 2011, 2012; Gardner et al., 2013) or their hidden representations in a vector space (Guu et al., 2015; Toutanova et al., 2016; McCallum et al., 2017). More recent work formulates multi-hop reasoning as a sequential decision problem, and leverages reinforcement learning (RL) to perform effective path search (Xiong et al., 2017; Das et al., 2018; Shen et al., 2018; Chen et al., 2018). In particular, MINERVA (Das et al., 2018) uses the REINFORCE algorithm (Williams, 1992) to train an end-to-end model for multi-hop KG query answering: given a query relation and a source entity, the trained agent searches over the KG starting from the source and arrives at the candidate answers without access to any pre-computed paths. 3243 Proceedings of the 2"
D18-1362,D15-1174,0,0.530458,"Missing"
D18-1362,P16-1136,0,0.0519733,"ation in more complex reasoning tasks. An alternative solution for KG reasoning is to infer missing facts by synthesizing information from multi-hop paths, e.g. bornIn(Obama, Hawaii) ∧ locatedIn(Hawaii, US) ⇒ bornIn(Obama, US), as shown in Figure 1. Path-based reasoning offers logical insights of the underlying KG and are more directly interpretable. Early work treats it as a link prediction problem and perform maximum-likelihood classification over either discrete path features (Lao et al., 2011, 2012; Gardner et al., 2013) or their hidden representations in a vector space (Guu et al., 2015; Toutanova et al., 2016; McCallum et al., 2017). More recent work formulates multi-hop reasoning as a sequential decision problem, and leverages reinforcement learning (RL) to perform effective path search (Xiong et al., 2017; Das et al., 2018; Shen et al., 2018; Chen et al., 2018). In particular, MINERVA (Das et al., 2018) uses the REINFORCE algorithm (Williams, 1992) to train an end-to-end model for multi-hop KG query answering: given a query relation and a source entity, the trained agent searches over the KG starting from the source and arrives at the candidate answers without access to any pre-computed paths. 3"
D18-1362,D17-1060,0,\N,Missing
D18-1362,D17-1260,0,\N,Missing
D18-1362,W16-0106,0,\N,Missing
D19-1051,D13-1155,0,0.0408225,"cal and semantic matching by applying graph analysis algorithms to the WordNet semantic network. Despite being a step in the direction of a more comprehensive evaluation protocol, none of these metrics gained sufficient traction in the research community, leaving ROUGE as the default automatic evaluation toolkit for text summarization. 2.3 Models Existing summarization models fall into three categories: abstractive, extractive, and hybrid. Extractive models select spans of text from the input and copy them directly into the summary. Non-neural approaches (Neto et al., 2002; Dorr et al., 2003; Filippova and Altun, 2013; Colmenares et al., 2015) utilized domain expertise to develop heuristics for summary content selection, whereas more recent, neural techniques allow for end-to-end training. In the most common case, models are trained as word- or sentencelevel classifiers that predict whether a fragment should be included in the summary (Nallapati et al., 2016b, 2017; Narayan et al., 2017; Liu et al., 2019; Xu and Durrett, 2019). Other approaches apply reinforcement learning training strategies to directly optimize the model on task-specific, nondifferentiable reward functions (Narayan et al., 2018b; Dong et"
D19-1051,W07-0718,0,0.117626,"Missing"
D19-1051,D18-1443,0,0.235404,"Missing"
D19-1051,E06-1032,0,0.112419,"ROUGE variants for evaluating summarization outputs. The study reveals superior variants of ROUGE that are different from the commonly used recommendations and shows that the BLEU metric achieves strong correlations with human assessments of generated summaries. Schulman et al. (2015) study the problems related to using ROUGE as an evaluation metric with respect to finding optimal solutions and provide proof of NP-hardness of global optimization with respect to ROUGE. Similar lines of research, where the authors put under scrutiny existing methodologies, datasets, or models were conducted by Callison-Burch et al. (2006, 2007); Tan et al. (2015); Post (2018) in machine translation, (Gkatzia and Mahamood, 2015; Reiter and Belz, 2009; Reiter, 2018) in natural language generation, Lee et al. (2016); Chen et al. (2016); Kaushik and Lipton (2018) in reading comprehension, Gururangan et al. (2018); Poliak et al. (2018); Glockner et al. (2018) in natural language inference, Goyal et al. (2017) in visual question answering, and Xian et al. (2017) in zero-shot image classification. Comments on the general state of scholarship in the field of machine learning were presented by Sculley et al. (2018); Lipton and Steinha"
D19-1051,P16-1223,0,0.0245637,"elations with human assessments of generated summaries. Schulman et al. (2015) study the problems related to using ROUGE as an evaluation metric with respect to finding optimal solutions and provide proof of NP-hardness of global optimization with respect to ROUGE. Similar lines of research, where the authors put under scrutiny existing methodologies, datasets, or models were conducted by Callison-Burch et al. (2006, 2007); Tan et al. (2015); Post (2018) in machine translation, (Gkatzia and Mahamood, 2015; Reiter and Belz, 2009; Reiter, 2018) in natural language generation, Lee et al. (2016); Chen et al. (2016); Kaushik and Lipton (2018) in reading comprehension, Gururangan et al. (2018); Poliak et al. (2018); Glockner et al. (2018) in natural language inference, Goyal et al. (2017) in visual question answering, and Xian et al. (2017) in zero-shot image classification. Comments on the general state of scholarship in the field of machine learning were presented by Sculley et al. (2018); Lipton and Steinhardt (2019) and references therein. Analysis and Critique Most summarization research revolves around new architectures and training strategies that improve the state of the art on benchmark problems."
D19-1051,P18-2103,0,0.0165628,"as an evaluation metric with respect to finding optimal solutions and provide proof of NP-hardness of global optimization with respect to ROUGE. Similar lines of research, where the authors put under scrutiny existing methodologies, datasets, or models were conducted by Callison-Burch et al. (2006, 2007); Tan et al. (2015); Post (2018) in machine translation, (Gkatzia and Mahamood, 2015; Reiter and Belz, 2009; Reiter, 2018) in natural language generation, Lee et al. (2016); Chen et al. (2016); Kaushik and Lipton (2018) in reading comprehension, Gururangan et al. (2018); Poliak et al. (2018); Glockner et al. (2018) in natural language inference, Goyal et al. (2017) in visual question answering, and Xian et al. (2017) in zero-shot image classification. Comments on the general state of scholarship in the field of machine learning were presented by Sculley et al. (2018); Lipton and Steinhardt (2019) and references therein. Analysis and Critique Most summarization research revolves around new architectures and training strategies that improve the state of the art on benchmark problems. However, it is also important to analyze and question the current methods and research settings. Zhang et al. (2018) conduc"
D19-1051,P18-1063,0,0.309801,"Richard Socher Salesforce Research {kryscinski,nkeskar,bmccann,cxiong,rsocher}@salesforce.com Abstract models. Current approaches to text summarization utilize advanced attention and copying mechanisms (See et al., 2017; Tan et al., 2017; Cohan et al., 2018), multi-task and multi-reward training techniques (Guo et al., 2018; Pasunuru and Bansal, 2018; Kry´sci´nski et al., 2018), reinforcement learning strategies (Paulus et al., 2017; Narayan et al., 2018b; Dong et al., 2018; Wu and Hu, 2018), and hybrid extractive-abstractive models (Liu et al., 2018; Hsu et al., 2018; Gehrmann et al., 2018; Chen and Bansal, 2018). Many of the introduced models are trained on the CNN/DailyMail (Nallapati et al., 2016a) news corpus, a popular benchmark for the field, and are evaluated based on n-gram overlap between the generated and target summaries with the ROUGE package (Lin, 2004). Despite substantial research effort, the progress on these benchmarks has stagnated. State-of-theart models only slightly outperform the Lead-3 baseline, which generates summaries by extracting the first three sentences of the source document. We argue that this stagnation can be partially attributed to the current research setup, which i"
D19-1051,N16-1012,0,0.035441,"ge-scale datasets have been proposed. The majority of 540 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 540–551, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics available corpora come from the news domain. Gigaword (Graff and Cieri, 2003) is a set of articles and corresponding titles that was originally used for headline generation (Takase et al., 2016), but it has also been adapted to single-sentence summarization (Rush et al., 2015; Chopra et al., 2016). NYT (Sandhaus, 2008) is a collection of articles from the New York Times magazine with abstracts written by library scientists. It has been primarily used for extractive summarization (Hong and Nenkova, 2014; Li et al., 2016) and phraseimportance prediction (Yang and Nenkova, 2014; Nye and Nenkova, 2015). The CNN/DailyMail (Nallapati et al., 2016a) dataset consists of articles with summaries composed of highlights from the article written by the authors themselves. It is commonly used for both abstractive (See et al., 2017; Paulus et al., 2017; Kry´sci´nski et al., 2018) and extractive (Dong"
D19-1051,D15-1013,0,0.267066,"Bansal, 2018; Kry´sci´nski et al., 2018), and unsupervised training strategies (Chu and Liu, 2018; Schumann, 2018). Hybrid models (Hsu et al., 2018; Liu et al., 2018; Gehrmann et al., 2018; Chen and Bansal, 2018) include both extractive and abstractive modules and allow to separate the summarization process into two phases – content selection and paraphrasing. For the sake of brevity we do not describe details of different models, we refer interested readers to the original papers. 2.4 ever, summary-level rankings and automatic metric correlations benefit from improving annotator consistency. Graham (2015) compare the fitness of the BLEU metric (Papineni et al., 2002) and a number of different ROUGE variants for evaluating summarization outputs. The study reveals superior variants of ROUGE that are different from the commonly used recommendations and shows that the BLEU metric achieves strong correlations with human assessments of generated summaries. Schulman et al. (2015) study the problems related to using ROUGE as an evaluation metric with respect to finding optimal solutions and provide proof of NP-hardness of global optimization with respect to ROUGE. Similar lines of research, where the"
D19-1051,N18-2097,0,0.0279557,"source docEvaluation Metrics Manual and semi-automatic (Nenkova and Passonneau, 2004; Passonneau et al., 2013) evaluation of large-scale summarization models is costly and cumbersome. Much effort has been made to develop automatic metrics that would allow for fast and cheap evaluation of models. The ROUGE package (Lin, 2004) offers a set of automatic metrics based on the lexical over541 uments and create summaries with novel phrases not present in the source document. A common approach in abstractive summarization is to use attention and copying mechanisms (See et al., 2017; Tan et al., 2017; Cohan et al., 2018). Other approaches include using multi-task and multireward training (Paulus et al., 2017; Jiang and Bansal, 2018; Guo et al., 2018; Pasunuru and Bansal, 2018; Kry´sci´nski et al., 2018), and unsupervised training strategies (Chu and Liu, 2018; Schumann, 2018). Hybrid models (Hsu et al., 2018; Liu et al., 2018; Gehrmann et al., 2018; Chen and Bansal, 2018) include both extractive and abstractive modules and allow to separate the summarization process into two phases – content selection and paraphrasing. For the sake of brevity we do not describe details of different models, we refer interested"
D19-1051,N18-1065,0,0.395823,"udy reveals superior variants of ROUGE that are different from the commonly used recommendations and shows that the BLEU metric achieves strong correlations with human assessments of generated summaries. Schulman et al. (2015) study the problems related to using ROUGE as an evaluation metric with respect to finding optimal solutions and provide proof of NP-hardness of global optimization with respect to ROUGE. Similar lines of research, where the authors put under scrutiny existing methodologies, datasets, or models were conducted by Callison-Burch et al. (2006, 2007); Tan et al. (2015); Post (2018) in machine translation, (Gkatzia and Mahamood, 2015; Reiter and Belz, 2009; Reiter, 2018) in natural language generation, Lee et al. (2016); Chen et al. (2016); Kaushik and Lipton (2018) in reading comprehension, Gururangan et al. (2018); Poliak et al. (2018); Glockner et al. (2018) in natural language inference, Goyal et al. (2017) in visual question answering, and Xian et al. (2017) in zero-shot image classification. Comments on the general state of scholarship in the field of machine learning were presented by Sculley et al. (2018); Lipton and Steinhardt (2019) and references therein. Anal"
D19-1051,N15-1014,0,0.0496827,"y applying graph analysis algorithms to the WordNet semantic network. Despite being a step in the direction of a more comprehensive evaluation protocol, none of these metrics gained sufficient traction in the research community, leaving ROUGE as the default automatic evaluation toolkit for text summarization. 2.3 Models Existing summarization models fall into three categories: abstractive, extractive, and hybrid. Extractive models select spans of text from the input and copy them directly into the summary. Non-neural approaches (Neto et al., 2002; Dorr et al., 2003; Filippova and Altun, 2013; Colmenares et al., 2015) utilized domain expertise to develop heuristics for summary content selection, whereas more recent, neural techniques allow for end-to-end training. In the most common case, models are trained as word- or sentencelevel classifiers that predict whether a fragment should be included in the summary (Nallapati et al., 2016b, 2017; Narayan et al., 2017; Liu et al., 2019; Xu and Durrett, 2019). Other approaches apply reinforcement learning training strategies to directly optimize the model on task-specific, nondifferentiable reward functions (Narayan et al., 2018b; Dong et al., 2018; Wu and Hu, 201"
D19-1051,P18-1064,0,0.0405743,"le summarization models is costly and cumbersome. Much effort has been made to develop automatic metrics that would allow for fast and cheap evaluation of models. The ROUGE package (Lin, 2004) offers a set of automatic metrics based on the lexical over541 uments and create summaries with novel phrases not present in the source document. A common approach in abstractive summarization is to use attention and copying mechanisms (See et al., 2017; Tan et al., 2017; Cohan et al., 2018). Other approaches include using multi-task and multireward training (Paulus et al., 2017; Jiang and Bansal, 2018; Guo et al., 2018; Pasunuru and Bansal, 2018; Kry´sci´nski et al., 2018), and unsupervised training strategies (Chu and Liu, 2018; Schumann, 2018). Hybrid models (Hsu et al., 2018; Liu et al., 2018; Gehrmann et al., 2018; Chen and Bansal, 2018) include both extractive and abstractive modules and allow to separate the summarization process into two phases – content selection and paraphrasing. For the sake of brevity we do not describe details of different models, we refer interested readers to the original papers. 2.4 ever, summary-level rankings and automatic metric correlations benefit from improving annotato"
D19-1051,D18-1409,0,0.060945,"Missing"
D19-1051,N18-2017,0,0.0195151,"(2015) study the problems related to using ROUGE as an evaluation metric with respect to finding optimal solutions and provide proof of NP-hardness of global optimization with respect to ROUGE. Similar lines of research, where the authors put under scrutiny existing methodologies, datasets, or models were conducted by Callison-Burch et al. (2006, 2007); Tan et al. (2015); Post (2018) in machine translation, (Gkatzia and Mahamood, 2015; Reiter and Belz, 2009; Reiter, 2018) in natural language generation, Lee et al. (2016); Chen et al. (2016); Kaushik and Lipton (2018) in reading comprehension, Gururangan et al. (2018); Poliak et al. (2018); Glockner et al. (2018) in natural language inference, Goyal et al. (2017) in visual question answering, and Xian et al. (2017) in zero-shot image classification. Comments on the general state of scholarship in the field of machine learning were presented by Sculley et al. (2018); Lipton and Steinhardt (2019) and references therein. Analysis and Critique Most summarization research revolves around new architectures and training strategies that improve the state of the art on benchmark problems. However, it is also important to analyze and question the current methods and"
D19-1051,W03-0501,0,0.343353,"cally collected datasets leave the task underconstrained and may contain noise detrimental to training and evaluation, 2) current evaluation protocol is weakly correlated with human judgment and does not account for important characteristics such as factual correctness, 3) models overfit to layout biases of current datasets and offer limited diversity in their outputs. 1 Introduction Text summarization aims at compressing long textual documents into a short, human readable form that contains the most important information from the source. Two strategies of generating summaries are extractive (Dorr et al., 2003; Nallapati et al., 2017), where salient fragments of the source document are identified and directly copied into the summary, and abstractive (Rush et al., 2015; See et al., 2017), where the salient parts are detected and paraphrased to form the final output. The number of summarization models introduced every year has been increasing rapidly. Advancements in neural network architectures (Sutskever et al., 2014; Bahdanau et al., 2015; Vinyals et al., 2015; Vaswani et al., 2017) and the availability of large scale data (Sandhaus, 2008; Nallapati et al., 2016a; Grusky et al., 2018) enabled the"
D19-1051,W04-1013,0,0.736525,"k and multi-reward training techniques (Guo et al., 2018; Pasunuru and Bansal, 2018; Kry´sci´nski et al., 2018), reinforcement learning strategies (Paulus et al., 2017; Narayan et al., 2018b; Dong et al., 2018; Wu and Hu, 2018), and hybrid extractive-abstractive models (Liu et al., 2018; Hsu et al., 2018; Gehrmann et al., 2018; Chen and Bansal, 2018). Many of the introduced models are trained on the CNN/DailyMail (Nallapati et al., 2016a) news corpus, a popular benchmark for the field, and are evaluated based on n-gram overlap between the generated and target summaries with the ROUGE package (Lin, 2004). Despite substantial research effort, the progress on these benchmarks has stagnated. State-of-theart models only slightly outperform the Lead-3 baseline, which generates summaries by extracting the first three sentences of the source document. We argue that this stagnation can be partially attributed to the current research setup, which involves uncurated, automatically collected datasets and non-informative evaluations protocols. We critically evaluate our hypothesis, and support our claims by analyzing three key components of the experimental setting: datasets, evaluation metrics, and mode"
D19-1051,E14-1075,0,0.0395032,"Processing, pages 540–551, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics available corpora come from the news domain. Gigaword (Graff and Cieri, 2003) is a set of articles and corresponding titles that was originally used for headline generation (Takase et al., 2016), but it has also been adapted to single-sentence summarization (Rush et al., 2015; Chopra et al., 2016). NYT (Sandhaus, 2008) is a collection of articles from the New York Times magazine with abstracts written by library scientists. It has been primarily used for extractive summarization (Hong and Nenkova, 2014; Li et al., 2016) and phraseimportance prediction (Yang and Nenkova, 2014; Nye and Nenkova, 2015). The CNN/DailyMail (Nallapati et al., 2016a) dataset consists of articles with summaries composed of highlights from the article written by the authors themselves. It is commonly used for both abstractive (See et al., 2017; Paulus et al., 2017; Kry´sci´nski et al., 2018) and extractive (Dong et al., 2018; Wu and Hu, 2018; Zhou et al., 2018) neural summarization. The collection was originally introduced as a Cloze-style QA dataset by Hermann et al. (2015). XSum (Narayan et al., 2018a) is a collect"
D19-1051,P18-1013,0,0.235234,"rish Keskar, Bryan McCann, Caiming Xiong, Richard Socher Salesforce Research {kryscinski,nkeskar,bmccann,cxiong,rsocher}@salesforce.com Abstract models. Current approaches to text summarization utilize advanced attention and copying mechanisms (See et al., 2017; Tan et al., 2017; Cohan et al., 2018), multi-task and multi-reward training techniques (Guo et al., 2018; Pasunuru and Bansal, 2018; Kry´sci´nski et al., 2018), reinforcement learning strategies (Paulus et al., 2017; Narayan et al., 2018b; Dong et al., 2018; Wu and Hu, 2018), and hybrid extractive-abstractive models (Liu et al., 2018; Hsu et al., 2018; Gehrmann et al., 2018; Chen and Bansal, 2018). Many of the introduced models are trained on the CNN/DailyMail (Nallapati et al., 2016a) news corpus, a popular benchmark for the field, and are evaluated based on n-gram overlap between the generated and target summaries with the ROUGE package (Lin, 2004). Despite substantial research effort, the progress on these benchmarks has stagnated. State-of-theart models only slightly outperform the Lead-3 baseline, which generates summaries by extracting the first three sentences of the source document. We argue that this stagnation can be partially at"
D19-1051,D18-1208,0,0.135518,"Missing"
D19-1051,K16-1028,0,0.212514,"Missing"
D19-1051,D18-1206,0,0.480626,"mmarization (Hong and Nenkova, 2014; Li et al., 2016) and phraseimportance prediction (Yang and Nenkova, 2014; Nye and Nenkova, 2015). The CNN/DailyMail (Nallapati et al., 2016a) dataset consists of articles with summaries composed of highlights from the article written by the authors themselves. It is commonly used for both abstractive (See et al., 2017; Paulus et al., 2017; Kry´sci´nski et al., 2018) and extractive (Dong et al., 2018; Wu and Hu, 2018; Zhou et al., 2018) neural summarization. The collection was originally introduced as a Cloze-style QA dataset by Hermann et al. (2015). XSum (Narayan et al., 2018a) is a collection of articles associated with one, singlesentence summary targeted at abstractive models. Newsroom (Grusky et al., 2018) is a diverse collection of articles sourced from 38 major online news outlets. This dataset was released together with a leaderboard and held-out testing split. Outside of the news domain, several datasets were collected from open discussion boards and other portals offering structure information. Reddit TIFU (Kim et al., 2018) is a collection of posts scraped from Reddit where users post their daily stories and each post is required to contain a Too Long; D"
D19-1051,D18-1207,1,0.910544,"Missing"
D19-1051,N18-1158,0,0.149252,"mmarization (Hong and Nenkova, 2014; Li et al., 2016) and phraseimportance prediction (Yang and Nenkova, 2014; Nye and Nenkova, 2015). The CNN/DailyMail (Nallapati et al., 2016a) dataset consists of articles with summaries composed of highlights from the article written by the authors themselves. It is commonly used for both abstractive (See et al., 2017; Paulus et al., 2017; Kry´sci´nski et al., 2018) and extractive (Dong et al., 2018; Wu and Hu, 2018; Zhou et al., 2018) neural summarization. The collection was originally introduced as a Cloze-style QA dataset by Hermann et al. (2015). XSum (Narayan et al., 2018a) is a collection of articles associated with one, singlesentence summary targeted at abstractive models. Newsroom (Grusky et al., 2018) is a diverse collection of articles sourced from 38 major online news outlets. This dataset was released together with a leaderboard and held-out testing split. Outside of the news domain, several datasets were collected from open discussion boards and other portals offering structure information. Reddit TIFU (Kim et al., 2018) is a collection of posts scraped from Reddit where users post their daily stories and each post is required to contain a Too Long; D"
D19-1051,W16-3617,0,0.0129595,"51, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics available corpora come from the news domain. Gigaword (Graff and Cieri, 2003) is a set of articles and corresponding titles that was originally used for headline generation (Takase et al., 2016), but it has also been adapted to single-sentence summarization (Rush et al., 2015; Chopra et al., 2016). NYT (Sandhaus, 2008) is a collection of articles from the New York Times magazine with abstracts written by library scientists. It has been primarily used for extractive summarization (Hong and Nenkova, 2014; Li et al., 2016) and phraseimportance prediction (Yang and Nenkova, 2014; Nye and Nenkova, 2015). The CNN/DailyMail (Nallapati et al., 2016a) dataset consists of articles with summaries composed of highlights from the article written by the authors themselves. It is commonly used for both abstractive (See et al., 2017; Paulus et al., 2017; Kry´sci´nski et al., 2018) and extractive (Dong et al., 2018; Wu and Hu, 2018; Zhou et al., 2018) neural summarization. The collection was originally introduced as a Cloze-style QA dataset by Hermann et al. (2015). XSum (Narayan et al., 2018a) is a collection of articles as"
D19-1051,N04-1019,0,0.693315,"cent, neural techniques allow for end-to-end training. In the most common case, models are trained as word- or sentencelevel classifiers that predict whether a fragment should be included in the summary (Nallapati et al., 2016b, 2017; Narayan et al., 2017; Liu et al., 2019; Xu and Durrett, 2019). Other approaches apply reinforcement learning training strategies to directly optimize the model on task-specific, nondifferentiable reward functions (Narayan et al., 2018b; Dong et al., 2018; Wu and Hu, 2018) . Abstractive models paraphrase the source docEvaluation Metrics Manual and semi-automatic (Nenkova and Passonneau, 2004; Passonneau et al., 2013) evaluation of large-scale summarization models is costly and cumbersome. Much effort has been made to develop automatic metrics that would allow for fast and cheap evaluation of models. The ROUGE package (Lin, 2004) offers a set of automatic metrics based on the lexical over541 uments and create summaries with novel phrases not present in the source document. A common approach in abstractive summarization is to use attention and copying mechanisms (See et al., 2017; Tan et al., 2017; Cohan et al., 2018). Other approaches include using multi-task and multireward train"
D19-1051,D18-1441,0,0.0736567,"Missing"
D19-1051,J18-3002,0,0.0166853,"commendations and shows that the BLEU metric achieves strong correlations with human assessments of generated summaries. Schulman et al. (2015) study the problems related to using ROUGE as an evaluation metric with respect to finding optimal solutions and provide proof of NP-hardness of global optimization with respect to ROUGE. Similar lines of research, where the authors put under scrutiny existing methodologies, datasets, or models were conducted by Callison-Burch et al. (2006, 2007); Tan et al. (2015); Post (2018) in machine translation, (Gkatzia and Mahamood, 2015; Reiter and Belz, 2009; Reiter, 2018) in natural language generation, Lee et al. (2016); Chen et al. (2016); Kaushik and Lipton (2018) in reading comprehension, Gururangan et al. (2018); Poliak et al. (2018); Glockner et al. (2018) in natural language inference, Goyal et al. (2017) in visual question answering, and Xian et al. (2017) in zero-shot image classification. Comments on the general state of scholarship in the field of machine learning were presented by Sculley et al. (2018); Lipton and Steinhardt (2019) and references therein. Analysis and Critique Most summarization research revolves around new architectures and traini"
D19-1051,D15-1222,0,0.0820131,"el, and health. 2.2 lap between candidate and reference summaries. Overlap can be computed between consecutive (n-grams) and non-consecutive (skip-grams) subsequences of tokens. ROUGE scores are based on exact token matches, meaning that computing overlap between synonymous phrases is not supported. Many approaches have extended ROUGE with support for synonyms and paraphrasing. ParaEval (Zhou et al., 2006) uses a three-step comparison strategy, where the first two steps perform optimal and greedy paraphrase matching based on paraphrase tables before reverting to exact token overlap. ROUGE-WE (Ng and Abrecht, 2015) replaces exact lexical matches with a soft semantic similarity measure approximated with the cosine distances between distributed representations of tokens. ROUGE 2.0 (Ganesan, 2018) leverages synonym dictionaries, such as WordNet, and considers all synonyms of matched words when computing token overlap. ROUGE-G (ShafieiBavani et al., 2018) combines lexical and semantic matching by applying graph analysis algorithms to the WordNet semantic network. Despite being a step in the direction of a more comprehensive evaluation protocol, none of these metrics gained sufficient traction in the researc"
D19-1051,J09-4008,0,0.0403482,"om the commonly used recommendations and shows that the BLEU metric achieves strong correlations with human assessments of generated summaries. Schulman et al. (2015) study the problems related to using ROUGE as an evaluation metric with respect to finding optimal solutions and provide proof of NP-hardness of global optimization with respect to ROUGE. Similar lines of research, where the authors put under scrutiny existing methodologies, datasets, or models were conducted by Callison-Burch et al. (2006, 2007); Tan et al. (2015); Post (2018) in machine translation, (Gkatzia and Mahamood, 2015; Reiter and Belz, 2009; Reiter, 2018) in natural language generation, Lee et al. (2016); Chen et al. (2016); Kaushik and Lipton (2018) in reading comprehension, Gururangan et al. (2018); Poliak et al. (2018); Glockner et al. (2018) in natural language inference, Goyal et al. (2017) in visual question answering, and Xian et al. (2017) in zero-shot image classification. Comments on the general state of scholarship in the field of machine learning were presented by Sculley et al. (2018); Lipton and Steinhardt (2019) and references therein. Analysis and Critique Most summarization research revolves around new architect"
D19-1051,N15-1166,0,0.0150113,"onal Linguistics available corpora come from the news domain. Gigaword (Graff and Cieri, 2003) is a set of articles and corresponding titles that was originally used for headline generation (Takase et al., 2016), but it has also been adapted to single-sentence summarization (Rush et al., 2015; Chopra et al., 2016). NYT (Sandhaus, 2008) is a collection of articles from the New York Times magazine with abstracts written by library scientists. It has been primarily used for extractive summarization (Hong and Nenkova, 2014; Li et al., 2016) and phraseimportance prediction (Yang and Nenkova, 2014; Nye and Nenkova, 2015). The CNN/DailyMail (Nallapati et al., 2016a) dataset consists of articles with summaries composed of highlights from the article written by the authors themselves. It is commonly used for both abstractive (See et al., 2017; Paulus et al., 2017; Kry´sci´nski et al., 2018) and extractive (Dong et al., 2018; Wu and Hu, 2018; Zhou et al., 2018) neural summarization. The collection was originally introduced as a Cloze-style QA dataset by Hermann et al. (2015). XSum (Narayan et al., 2018a) is a collection of articles associated with one, singlesentence summary targeted at abstractive models. Newsro"
D19-1051,D15-1044,0,0.461502,"orrelated with human judgment and does not account for important characteristics such as factual correctness, 3) models overfit to layout biases of current datasets and offer limited diversity in their outputs. 1 Introduction Text summarization aims at compressing long textual documents into a short, human readable form that contains the most important information from the source. Two strategies of generating summaries are extractive (Dorr et al., 2003; Nallapati et al., 2017), where salient fragments of the source document are identified and directly copied into the summary, and abstractive (Rush et al., 2015; See et al., 2017), where the salient parts are detected and paraphrased to form the final output. The number of summarization models introduced every year has been increasing rapidly. Advancements in neural network architectures (Sutskever et al., 2014; Bahdanau et al., 2015; Vinyals et al., 2015; Vaswani et al., 2017) and the availability of large scale data (Sandhaus, 2008; Nallapati et al., 2016a; Grusky et al., 2018) enabled the transition from systems based on expert knowledge and heuristics to data-driven approaches powered by end-to-end deep neural 2 2.1 Related Work Datasets To accom"
D19-1051,P12-2070,0,0.0927104,"ed dimension. Kedzie et al. (2018) offered a thorough analysis of how neural models perform content selection across different data domains, and exposed data biases that dominate the learning signal in the news domain and architectural limitations of current approaches in learning robust sentence-level representations. Liu and Liu (2010) examine the correlation between ROUGE scores and human judgments when evaluating meeting summarization data and show that the correlation strength is low, but can be improved by leveraging unique meeting characteristics, such as available speaker information. Owczarzak et al. (2012) inspect how inconsistencies in human annotator judgments affect the ranking of summaries and correlations with automatic evaluation metrics. The results showed that systemlevel rankings, considering all summaries, were stable despite inconsistencies in judgments, how3 3.1 Datasets Underconstrained task The task of summarization is to compress long documents by identifying and extracting the most important information from the source documents. However, assessing the importance of information is a difficult task in itself, that highly depends on the expectations and prior knowledge of the targ"
D19-1051,P17-1099,0,0.897715,"n judgment and does not account for important characteristics such as factual correctness, 3) models overfit to layout biases of current datasets and offer limited diversity in their outputs. 1 Introduction Text summarization aims at compressing long textual documents into a short, human readable form that contains the most important information from the source. Two strategies of generating summaries are extractive (Dorr et al., 2003; Nallapati et al., 2017), where salient fragments of the source document are identified and directly copied into the summary, and abstractive (Rush et al., 2015; See et al., 2017), where the salient parts are detected and paraphrased to form the final output. The number of summarization models introduced every year has been increasing rapidly. Advancements in neural network architectures (Sutskever et al., 2014; Bahdanau et al., 2015; Vinyals et al., 2015; Vaswani et al., 2017) and the availability of large scale data (Sandhaus, 2008; Nallapati et al., 2016a; Grusky et al., 2018) enabled the transition from systems based on expert knowledge and heuristics to data-driven approaches powered by end-to-end deep neural 2 2.1 Related Work Datasets To accommodate the requirem"
D19-1051,P02-1040,0,0.106359,"ised training strategies (Chu and Liu, 2018; Schumann, 2018). Hybrid models (Hsu et al., 2018; Liu et al., 2018; Gehrmann et al., 2018; Chen and Bansal, 2018) include both extractive and abstractive modules and allow to separate the summarization process into two phases – content selection and paraphrasing. For the sake of brevity we do not describe details of different models, we refer interested readers to the original papers. 2.4 ever, summary-level rankings and automatic metric correlations benefit from improving annotator consistency. Graham (2015) compare the fitness of the BLEU metric (Papineni et al., 2002) and a number of different ROUGE variants for evaluating summarization outputs. The study reveals superior variants of ROUGE that are different from the commonly used recommendations and shows that the BLEU metric achieves strong correlations with human assessments of generated summaries. Schulman et al. (2015) study the problems related to using ROUGE as an evaluation metric with respect to finding optimal solutions and provide proof of NP-hardness of global optimization with respect to ROUGE. Similar lines of research, where the authors put under scrutiny existing methodologies, datasets, or"
D19-1051,D18-1085,0,0.0891176,"with support for synonyms and paraphrasing. ParaEval (Zhou et al., 2006) uses a three-step comparison strategy, where the first two steps perform optimal and greedy paraphrase matching based on paraphrase tables before reverting to exact token overlap. ROUGE-WE (Ng and Abrecht, 2015) replaces exact lexical matches with a soft semantic similarity measure approximated with the cosine distances between distributed representations of tokens. ROUGE 2.0 (Ganesan, 2018) leverages synonym dictionaries, such as WordNet, and considers all synonyms of matched words when computing token overlap. ROUGE-G (ShafieiBavani et al., 2018) combines lexical and semantic matching by applying graph analysis algorithms to the WordNet semantic network. Despite being a step in the direction of a more comprehensive evaluation protocol, none of these metrics gained sufficient traction in the research community, leaving ROUGE as the default automatic evaluation toolkit for text summarization. 2.3 Models Existing summarization models fall into three categories: abstractive, extractive, and hybrid. Extractive models select spans of text from the input and copy them directly into the summary. Non-neural approaches (Neto et al., 2002; Dorr"
D19-1051,P13-2026,0,0.0314186,"for end-to-end training. In the most common case, models are trained as word- or sentencelevel classifiers that predict whether a fragment should be included in the summary (Nallapati et al., 2016b, 2017; Narayan et al., 2017; Liu et al., 2019; Xu and Durrett, 2019). Other approaches apply reinforcement learning training strategies to directly optimize the model on task-specific, nondifferentiable reward functions (Narayan et al., 2018b; Dong et al., 2018; Wu and Hu, 2018) . Abstractive models paraphrase the source docEvaluation Metrics Manual and semi-automatic (Nenkova and Passonneau, 2004; Passonneau et al., 2013) evaluation of large-scale summarization models is costly and cumbersome. Much effort has been made to develop automatic metrics that would allow for fast and cheap evaluation of models. The ROUGE package (Lin, 2004) offers a set of automatic metrics based on the lexical over541 uments and create summaries with novel phrases not present in the source document. A common approach in abstractive summarization is to use attention and copying mechanisms (See et al., 2017; Tan et al., 2017; Cohan et al., 2018). Other approaches include using multi-task and multireward training (Paulus et al., 2017;"
D19-1051,D16-1112,0,0.0218343,"2.1 Related Work Datasets To accommodate the requirements of modern data-driven approaches, several large-scale datasets have been proposed. The majority of 540 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 540–551, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics available corpora come from the news domain. Gigaword (Graff and Cieri, 2003) is a set of articles and corresponding titles that was originally used for headline generation (Takase et al., 2016), but it has also been adapted to single-sentence summarization (Rush et al., 2015; Chopra et al., 2016). NYT (Sandhaus, 2008) is a collection of articles from the New York Times magazine with abstracts written by library scientists. It has been primarily used for extractive summarization (Hong and Nenkova, 2014; Li et al., 2016) and phraseimportance prediction (Yang and Nenkova, 2014; Nye and Nenkova, 2015). The CNN/DailyMail (Nallapati et al., 2016a) dataset consists of articles with summaries composed of highlights from the article written by the authors themselves. It is commonly used for"
D19-1051,N18-2102,0,0.0260122,"odels is costly and cumbersome. Much effort has been made to develop automatic metrics that would allow for fast and cheap evaluation of models. The ROUGE package (Lin, 2004) offers a set of automatic metrics based on the lexical over541 uments and create summaries with novel phrases not present in the source document. A common approach in abstractive summarization is to use attention and copying mechanisms (See et al., 2017; Tan et al., 2017; Cohan et al., 2018). Other approaches include using multi-task and multireward training (Paulus et al., 2017; Jiang and Bansal, 2018; Guo et al., 2018; Pasunuru and Bansal, 2018; Kry´sci´nski et al., 2018), and unsupervised training strategies (Chu and Liu, 2018; Schumann, 2018). Hybrid models (Hsu et al., 2018; Liu et al., 2018; Gehrmann et al., 2018; Chen and Bansal, 2018) include both extractive and abstractive modules and allow to separate the summarization process into two phases – content selection and paraphrasing. For the sake of brevity we do not describe details of different models, we refer interested readers to the original papers. 2.4 ever, summary-level rankings and automatic metric correlations benefit from improving annotator consistency. Graham (2015"
D19-1051,P17-1108,0,0.043788,"Missing"
D19-1051,S18-2023,0,0.0542373,"Missing"
D19-1051,W15-5009,0,0.0587322,"Missing"
D19-1051,W18-6319,0,0.0134174,"he study reveals superior variants of ROUGE that are different from the commonly used recommendations and shows that the BLEU metric achieves strong correlations with human assessments of generated summaries. Schulman et al. (2015) study the problems related to using ROUGE as an evaluation metric with respect to finding optimal solutions and provide proof of NP-hardness of global optimization with respect to ROUGE. Similar lines of research, where the authors put under scrutiny existing methodologies, datasets, or models were conducted by Callison-Burch et al. (2006, 2007); Tan et al. (2015); Post (2018) in machine translation, (Gkatzia and Mahamood, 2015; Reiter and Belz, 2009; Reiter, 2018) in natural language generation, Lee et al. (2016); Chen et al. (2016); Kaushik and Lipton (2018) in reading comprehension, Gururangan et al. (2018); Poliak et al. (2018); Glockner et al. (2018) in natural language inference, Goyal et al. (2017) in visual question answering, and Xian et al. (2017) in zero-shot image classification. Comments on the general state of scholarship in the field of machine learning were presented by Sculley et al. (2018); Lipton and Steinhardt (2019) and references therein. Anal"
D19-1051,D19-1324,0,0.0183643,"xtractive, and hybrid. Extractive models select spans of text from the input and copy them directly into the summary. Non-neural approaches (Neto et al., 2002; Dorr et al., 2003; Filippova and Altun, 2013; Colmenares et al., 2015) utilized domain expertise to develop heuristics for summary content selection, whereas more recent, neural techniques allow for end-to-end training. In the most common case, models are trained as word- or sentencelevel classifiers that predict whether a fragment should be included in the summary (Nallapati et al., 2016b, 2017; Narayan et al., 2017; Liu et al., 2019; Xu and Durrett, 2019). Other approaches apply reinforcement learning training strategies to directly optimize the model on task-specific, nondifferentiable reward functions (Narayan et al., 2018b; Dong et al., 2018; Wu and Hu, 2018) . Abstractive models paraphrase the source docEvaluation Metrics Manual and semi-automatic (Nenkova and Passonneau, 2004; Passonneau et al., 2013) evaluation of large-scale summarization models is costly and cumbersome. Much effort has been made to develop automatic metrics that would allow for fast and cheap evaluation of models. The ROUGE package (Lin, 2004) offers a set of automatic"
D19-1051,D18-1089,0,0.262614,"8); Glockner et al. (2018) in natural language inference, Goyal et al. (2017) in visual question answering, and Xian et al. (2017) in zero-shot image classification. Comments on the general state of scholarship in the field of machine learning were presented by Sculley et al. (2018); Lipton and Steinhardt (2019) and references therein. Analysis and Critique Most summarization research revolves around new architectures and training strategies that improve the state of the art on benchmark problems. However, it is also important to analyze and question the current methods and research settings. Zhang et al. (2018) conducted a quantitative study of the level of abstraction in abstractive summarization models and showed that wordlevel, copy-only extractive models achieve comparable results to fully abstractive models in the measured dimension. Kedzie et al. (2018) offered a thorough analysis of how neural models perform content selection across different data domains, and exposed data biases that dominate the learning signal in the news domain and architectural limitations of current approaches in learning robust sentence-level representations. Liu and Liu (2010) examine the correlation between ROUGE sco"
D19-1051,N06-1057,0,0.060132,"g, 2018) is a collection of articles from the WikiHow knowledge base, where each article contains instructions for performing procedural, multi-step tasks covering various areas, including: arts, finance, travel, and health. 2.2 lap between candidate and reference summaries. Overlap can be computed between consecutive (n-grams) and non-consecutive (skip-grams) subsequences of tokens. ROUGE scores are based on exact token matches, meaning that computing overlap between synonymous phrases is not supported. Many approaches have extended ROUGE with support for synonyms and paraphrasing. ParaEval (Zhou et al., 2006) uses a three-step comparison strategy, where the first two steps perform optimal and greedy paraphrase matching based on paraphrase tables before reverting to exact token overlap. ROUGE-WE (Ng and Abrecht, 2015) replaces exact lexical matches with a soft semantic similarity measure approximated with the cosine distances between distributed representations of tokens. ROUGE 2.0 (Ganesan, 2018) leverages synonym dictionaries, such as WordNet, and considers all synonyms of matched words when computing token overlap. ROUGE-G (ShafieiBavani et al., 2018) combines lexical and semantic matching by ap"
D19-1051,P18-1061,0,0.0247346,"ection of articles from the New York Times magazine with abstracts written by library scientists. It has been primarily used for extractive summarization (Hong and Nenkova, 2014; Li et al., 2016) and phraseimportance prediction (Yang and Nenkova, 2014; Nye and Nenkova, 2015). The CNN/DailyMail (Nallapati et al., 2016a) dataset consists of articles with summaries composed of highlights from the article written by the authors themselves. It is commonly used for both abstractive (See et al., 2017; Paulus et al., 2017; Kry´sci´nski et al., 2018) and extractive (Dong et al., 2018; Wu and Hu, 2018; Zhou et al., 2018) neural summarization. The collection was originally introduced as a Cloze-style QA dataset by Hermann et al. (2015). XSum (Narayan et al., 2018a) is a collection of articles associated with one, singlesentence summary targeted at abstractive models. Newsroom (Grusky et al., 2018) is a diverse collection of articles sourced from 38 major online news outlets. This dataset was released together with a leaderboard and held-out testing split. Outside of the news domain, several datasets were collected from open discussion boards and other portals offering structure information. Reddit TIFU (Kim et"
D19-1051,W15-4708,0,\N,Missing
D19-1051,D18-1440,0,\N,Missing
D19-1051,N19-1260,0,\N,Missing
D19-1157,D18-1168,0,0.120669,"erest is segmented from long, untrimmed videos. These methods only identify actions from a pre-defined set of categories, which limits their application to situations where only unconstrained language descriptions are available. This more general problem is referred to as natural language localization (NLL) (Hendricks et al., 2017; Gao et al., 2017a). The goal is to retrieve a temporal segment from an untrimmed video based on an arbitrary text query. Recent work focuses on learning the mapping from visual segments to the input text (Hendricks et al., 2017; Gao et al., 2017a; Liu et al., 2018; Hendricks et al., 2018; Zhang et al., ∗ Work done when the author was at Salesforce Research. † Corresponding author. 2018) and retrieving segments based on the alignment scores. However, in order to successfully train a NLL model, a large number of diverse language descriptions are needed to describe different temporal segments of videos which incurs high human labeling cost. We propose Weakly Supervised Language Localization Networks (WSLLN) which requires only video-sentence pairs during training with no information of where the activities temporally occur. Intuitively, it is much easier to annotate videolevel d"
D19-1537,W10-2903,0,0.0605657,"lumn) (6) While the copy mechanism has been introduced by Gu et al. (2016) and See et al. (2017), they focus on summarization or response generation applications by copying from the source sentences. By contrast, our focus is on editing the previously generated query while incorporating the context of user utterances and table schemas. 4 Related Work Semantic parsing is the task of mapping natural language sentences into formal representations. It has been studied for decades including using linguistically-motivated compositional representations, such as logical forms (Zelle and Mooney, 1996; Clarke et al., 2010) and lambda calculus (Zettlemoyer and Collins, 2005; Artzi and Zettlemoyer, 2011), and using executable programs, such as SQL queries (Miller et al., 1996; Zhong et al., 2017) and other general-purpose programming languages (Yin and Neubig, 2017; Iyer et al., 2018). Most of the early studies worked on a few domains and small datasets such as GeoQuery (Zelle and Mooney, 1996) and Overnight (Wang et al., 2015). Recently, large and cross-domain text-to-SQL datasets such as WikiSQL (Zhong et al., 2017) and Spider (Yu et al., 2018c) have received an increasing amount of attention as many data-drive"
D19-1537,H94-1010,0,0.712423,"xample. match accuracy over the previous state-of-the-art. Further analysis shows that our editing approach is more robust to error propagation than copying segments, and the improvement becomes more significant if the basic text-to-SQL generation accuracy (without editing) improves. 2 Cross-Domain Context-Depencent Semantic Parsing 2.1 Datasets We use SParC 1 (Yu et al., 2019b), a large-scale cross-domain context-dependent semantic parsing dataset with SQL labels, as our main evaluation benchmark. A SParC example is shown in Table 3. We also report performance on ATIS (Hemphill et al., 1990; Dahl et al., 1994a) for direct comparison to Suhr et al. (2018). In addition, we evaluate the cross-domain context-independent text-toSQL ability of our model on Spider2 (Yu et al., 1 2 https://yale-lily.github.io/sparc https://yale-lily.github.io/spider 2018c), which SParC is built on. We summarize and compare the data statistics in Table 1 and Table 2. While the ATIS dataset has been extensively studied, it is limited to a particular domain. By contrast, SParC is both context-dependent and cross-domain. Each interaction in SParC is constructed using a question in Spider as the interaction goal, where the ann"
D19-1537,N19-1423,0,0.0255756,"of user utterance and column headers. dorms have a TV louge (b) Utterance Encoder. Bi LSTM Concatennation Attention over Utterance Tokens Self-Attention Among Table Columns Bi LSTM Bi LSTM Bi LSTM Bi LSTM Bi LSTM dorm . id dorm . name has . dorm id has . amenity id amenity Bi LSTM . id amenity . name (c) Table Encoder. Figure 2: Utterance-Table Encoder for the example in (a). Utterance-Table BERT Embedding. We consider two options as the input to the first layer biLSTM. The first choice is the pretrained word embedding. Second, we also consider the contextualized word embedding based on BERT (Devlin et al., 2019). To be specific, we follow Hwang et al. (2019) to concatenate the user utterance and all the column headers in a single sequence separated by the [SEP] token: [CLS], Xi , [SEP], c1 , [SEP], . . . , cm , [SEP] This sequence is fed into the pretrained BERT model whose hidden states at the last layer is used as the input embedding. 3.2 The hidden state of this interaction encoder hI encodes the history as the interaction proceeds. Turn Attention When issuing the current utterance, the user may omit or explicitly refer to the previously mentioned information. To this end, we adopt the turn attent"
D19-1537,P16-1004,0,0.0241913,"2005; Artzi and Zettlemoyer, 2011), and using executable programs, such as SQL queries (Miller et al., 1996; Zhong et al., 2017) and other general-purpose programming languages (Yin and Neubig, 2017; Iyer et al., 2018). Most of the early studies worked on a few domains and small datasets such as GeoQuery (Zelle and Mooney, 1996) and Overnight (Wang et al., 2015). Recently, large and cross-domain text-to-SQL datasets such as WikiSQL (Zhong et al., 2017) and Spider (Yu et al., 2018c) have received an increasing amount of attention as many data-driven neural approaches achieve promising results (Dong and Lapata, 2016; Su and Yan, 2017; Iyer et al., 2017; Xu et al., 2017; Finegan-Dollak et al., 2018; Yu et al., 2018a; Huang et al., 2018; Dong and Lapata, 2018; Sun et al., 2018; Gur et al., 2018; Guo et al., 2018; Yavuz et al., 2018; Shi et al., 2018). Most of them still focus on context-independent semantic parsing by converting single-turn questions into executable queries. Relatively less effort has been devoted to context-dependent semantic parsing on datasets including ATIS (Hemphill et al., 1990; Dahl et al., 1994b), SpaceBook (Vlachos and Clark, 2014), SCONE (Long et al., 2016; Guu et al., 2017; Frie"
D19-1537,P18-1068,0,0.1126,"meaning of user utterances, the structure of table schema, and the relationship between the two. To this end, we build an utterance-table encoder with co-attention between the two as illustrated in Figure 2. Figure 2b shows the utterance encoder. For the user utterance at each turn, we first use a bi-LSTM to encode utterance tokens. The bi-LSTM hidden state is fed into a dot-product attention layer (Luong et al., 2015) over the column header embeddings. For each utterance token embedding, we get an attention weighted average of the column header embeddings to obtain the most relevant columns (Dong and Lapata, 2018). We then concatenate the bi-LSTM hidden state and the column attention vector, and use a second layer biLSTM to generate the utterance token embedding hE . Figure 2c shows the table encoder. For each column header, we concatenate its table name and its column name separated by a special dot token (i.e., table name . column name). Each column header is processed by a bi-LSTM layer. To better capture the internal structure of the table schemas (e.g., foreign key), we then employ a selfattention (Vaswani et al., 2017) among all column headers. We then use an attention layer to capture the relati"
D19-1537,P18-1033,1,0.853357,"L queries (Miller et al., 1996; Zhong et al., 2017) and other general-purpose programming languages (Yin and Neubig, 2017; Iyer et al., 2018). Most of the early studies worked on a few domains and small datasets such as GeoQuery (Zelle and Mooney, 1996) and Overnight (Wang et al., 2015). Recently, large and cross-domain text-to-SQL datasets such as WikiSQL (Zhong et al., 2017) and Spider (Yu et al., 2018c) have received an increasing amount of attention as many data-driven neural approaches achieve promising results (Dong and Lapata, 2016; Su and Yan, 2017; Iyer et al., 2017; Xu et al., 2017; Finegan-Dollak et al., 2018; Yu et al., 2018a; Huang et al., 2018; Dong and Lapata, 2018; Sun et al., 2018; Gur et al., 2018; Guo et al., 2018; Yavuz et al., 2018; Shi et al., 2018). Most of them still focus on context-independent semantic parsing by converting single-turn questions into executable queries. Relatively less effort has been devoted to context-dependent semantic parsing on datasets including ATIS (Hemphill et al., 1990; Dahl et al., 1994b), SpaceBook (Vlachos and Clark, 2014), SCONE (Long et al., 2016; Guu et al., 2017; Fried et al., 2018; Suhr and Artzi, 2018; Huang et al., 2019), SequentialQA (Iyyer et a"
D19-1537,N18-1177,0,0.0515028,"Missing"
D19-1537,P16-1154,0,0.0390467,", we predict a switch pcopy to decide if we need copy from the previous query or insert a new token. pcopy = σ(ck Wcopy + bcopy ) pinsert = 1 − pcopy (5) Then, we use a separate layer to score the query tokens at turn t − 1, and the output distribution is modified as the following to take into account the editing probability: Pprev SQL = softmax(ok Wprev SQL hQ t−1 ) mSQL = ok WSQL + bSQL mcolumn = ok Wcolumn hC PSQL S column = softmax([mSQL ; mcolumn ]) P (yk ) = pcopy · Pprev SQL (yk ∈ prev SQL) [ +pinsert · PSQL S column (yk ∈ SQL column) (6) While the copy mechanism has been introduced by Gu et al. (2016) and See et al. (2017), they focus on summarization or response generation applications by copying from the source sentences. By contrast, our focus is on editing the previously generated query while incorporating the context of user utterances and table schemas. 4 Related Work Semantic parsing is the task of mapping natural language sentences into formal representations. It has been studied for decades including using linguistically-motivated compositional representations, such as logical forms (Zelle and Mooney, 1996; Clarke et al., 2010) and lambda calculus (Zettlemoyer and Collins, 2005; A"
D19-1537,D18-1188,0,0.0130342,"r et al., 2018). Most of the early studies worked on a few domains and small datasets such as GeoQuery (Zelle and Mooney, 1996) and Overnight (Wang et al., 2015). Recently, large and cross-domain text-to-SQL datasets such as WikiSQL (Zhong et al., 2017) and Spider (Yu et al., 2018c) have received an increasing amount of attention as many data-driven neural approaches achieve promising results (Dong and Lapata, 2016; Su and Yan, 2017; Iyer et al., 2017; Xu et al., 2017; Finegan-Dollak et al., 2018; Yu et al., 2018a; Huang et al., 2018; Dong and Lapata, 2018; Sun et al., 2018; Gur et al., 2018; Guo et al., 2018; Yavuz et al., 2018; Shi et al., 2018). Most of them still focus on context-independent semantic parsing by converting single-turn questions into executable queries. Relatively less effort has been devoted to context-dependent semantic parsing on datasets including ATIS (Hemphill et al., 1990; Dahl et al., 1994b), SpaceBook (Vlachos and Clark, 2014), SCONE (Long et al., 2016; Guu et al., 2017; Fried et al., 2018; Suhr and Artzi, 2018; Huang et al., 2019), SequentialQA (Iyyer et al., 2017), SParC (Yu et al., 2019b) and CoSQL (Yu et al., 2019a). On ATIS, Miller et al. (1996) maps utterances to"
D19-1537,P19-1444,0,0.46871,"Missing"
D19-1537,P18-1124,0,0.0623869,"Neubig, 2017; Iyer et al., 2018). Most of the early studies worked on a few domains and small datasets such as GeoQuery (Zelle and Mooney, 1996) and Overnight (Wang et al., 2015). Recently, large and cross-domain text-to-SQL datasets such as WikiSQL (Zhong et al., 2017) and Spider (Yu et al., 2018c) have received an increasing amount of attention as many data-driven neural approaches achieve promising results (Dong and Lapata, 2016; Su and Yan, 2017; Iyer et al., 2017; Xu et al., 2017; Finegan-Dollak et al., 2018; Yu et al., 2018a; Huang et al., 2018; Dong and Lapata, 2018; Sun et al., 2018; Gur et al., 2018; Guo et al., 2018; Yavuz et al., 2018; Shi et al., 2018). Most of them still focus on context-independent semantic parsing by converting single-turn questions into executable queries. Relatively less effort has been devoted to context-dependent semantic parsing on datasets including ATIS (Hemphill et al., 1990; Dahl et al., 1994b), SpaceBook (Vlachos and Clark, 2014), SCONE (Long et al., 2016; Guu et al., 2017; Fried et al., 2018; Suhr and Artzi, 2018; Huang et al., 2019), SequentialQA (Iyyer et al., 2017), SParC (Yu et al., 2019b) and CoSQL (Yu et al., 2019a). On ATIS, Miller et al. (1996) m"
D19-1537,P17-1097,0,0.0224668,"(Dong and Lapata, 2016; Su and Yan, 2017; Iyer et al., 2017; Xu et al., 2017; Finegan-Dollak et al., 2018; Yu et al., 2018a; Huang et al., 2018; Dong and Lapata, 2018; Sun et al., 2018; Gur et al., 2018; Guo et al., 2018; Yavuz et al., 2018; Shi et al., 2018). Most of them still focus on context-independent semantic parsing by converting single-turn questions into executable queries. Relatively less effort has been devoted to context-dependent semantic parsing on datasets including ATIS (Hemphill et al., 1990; Dahl et al., 1994b), SpaceBook (Vlachos and Clark, 2014), SCONE (Long et al., 2016; Guu et al., 2017; Fried et al., 2018; Suhr and Artzi, 2018; Huang et al., 2019), SequentialQA (Iyyer et al., 2017), SParC (Yu et al., 2019b) and CoSQL (Yu et al., 2019a). On ATIS, Miller et al. (1996) maps utterances to semantic frames which are then mapped to SQL queries; Zettlemoyer and Collins (2009) starts with context-independent Combinatory Categorial Grammar (CCG) parsing and then resolves references to generate lambda-calculus logical forms for sequences of sentences. The most relevant to our work is Suhr et al. (2018), who generate ATIS SQL queries from interactions by incorporating history with an i"
D19-1537,H90-1021,0,0.51482,"unge’) Table 3: SParC example. match accuracy over the previous state-of-the-art. Further analysis shows that our editing approach is more robust to error propagation than copying segments, and the improvement becomes more significant if the basic text-to-SQL generation accuracy (without editing) improves. 2 Cross-Domain Context-Depencent Semantic Parsing 2.1 Datasets We use SParC 1 (Yu et al., 2019b), a large-scale cross-domain context-dependent semantic parsing dataset with SQL labels, as our main evaluation benchmark. A SParC example is shown in Table 3. We also report performance on ATIS (Hemphill et al., 1990; Dahl et al., 1994a) for direct comparison to Suhr et al. (2018). In addition, we evaluate the cross-domain context-independent text-toSQL ability of our model on Spider2 (Yu et al., 1 2 https://yale-lily.github.io/sparc https://yale-lily.github.io/spider 2018c), which SParC is built on. We summarize and compare the data statistics in Table 1 and Table 2. While the ATIS dataset has been extensively studied, it is limited to a particular domain. By contrast, SParC is both context-dependent and cross-domain. Each interaction in SParC is constructed using a question in Spider as the interaction"
D19-1537,N18-2115,0,0.0394036,"017) and other general-purpose programming languages (Yin and Neubig, 2017; Iyer et al., 2018). Most of the early studies worked on a few domains and small datasets such as GeoQuery (Zelle and Mooney, 1996) and Overnight (Wang et al., 2015). Recently, large and cross-domain text-to-SQL datasets such as WikiSQL (Zhong et al., 2017) and Spider (Yu et al., 2018c) have received an increasing amount of attention as many data-driven neural approaches achieve promising results (Dong and Lapata, 2016; Su and Yan, 2017; Iyer et al., 2017; Xu et al., 2017; Finegan-Dollak et al., 2018; Yu et al., 2018a; Huang et al., 2018; Dong and Lapata, 2018; Sun et al., 2018; Gur et al., 2018; Guo et al., 2018; Yavuz et al., 2018; Shi et al., 2018). Most of them still focus on context-independent semantic parsing by converting single-turn questions into executable queries. Relatively less effort has been devoted to context-dependent semantic parsing on datasets including ATIS (Hemphill et al., 1990; Dahl et al., 1994b), SpaceBook (Vlachos and Clark, 2014), SCONE (Long et al., 2016; Guu et al., 2017; Fried et al., 2018; Suhr and Artzi, 2018; Huang et al., 2019), SequentialQA (Iyyer et al., 2017), SParC (Yu et al., 2019b) an"
D19-1537,P17-1089,0,0.0436623,"sing executable programs, such as SQL queries (Miller et al., 1996; Zhong et al., 2017) and other general-purpose programming languages (Yin and Neubig, 2017; Iyer et al., 2018). Most of the early studies worked on a few domains and small datasets such as GeoQuery (Zelle and Mooney, 1996) and Overnight (Wang et al., 2015). Recently, large and cross-domain text-to-SQL datasets such as WikiSQL (Zhong et al., 2017) and Spider (Yu et al., 2018c) have received an increasing amount of attention as many data-driven neural approaches achieve promising results (Dong and Lapata, 2016; Su and Yan, 2017; Iyer et al., 2017; Xu et al., 2017; Finegan-Dollak et al., 2018; Yu et al., 2018a; Huang et al., 2018; Dong and Lapata, 2018; Sun et al., 2018; Gur et al., 2018; Guo et al., 2018; Yavuz et al., 2018; Shi et al., 2018). Most of them still focus on context-independent semantic parsing by converting single-turn questions into executable queries. Relatively less effort has been devoted to context-dependent semantic parsing on datasets including ATIS (Hemphill et al., 1990; Dahl et al., 1994b), SpaceBook (Vlachos and Clark, 2014), SCONE (Long et al., 2016; Guu et al., 2017; Fried et al., 2018; Suhr and Artzi, 2018;"
D19-1537,D18-1192,0,0.0123607,"while incorporating the context of user utterances and table schemas. 4 Related Work Semantic parsing is the task of mapping natural language sentences into formal representations. It has been studied for decades including using linguistically-motivated compositional representations, such as logical forms (Zelle and Mooney, 1996; Clarke et al., 2010) and lambda calculus (Zettlemoyer and Collins, 2005; Artzi and Zettlemoyer, 2011), and using executable programs, such as SQL queries (Miller et al., 1996; Zhong et al., 2017) and other general-purpose programming languages (Yin and Neubig, 2017; Iyer et al., 2018). Most of the early studies worked on a few domains and small datasets such as GeoQuery (Zelle and Mooney, 1996) and Overnight (Wang et al., 2015). Recently, large and cross-domain text-to-SQL datasets such as WikiSQL (Zhong et al., 2017) and Spider (Yu et al., 2018c) have received an increasing amount of attention as many data-driven neural approaches achieve promising results (Dong and Lapata, 2016; Su and Yan, 2017; Iyer et al., 2017; Xu et al., 2017; Finegan-Dollak et al., 2018; Yu et al., 2018a; Huang et al., 2018; Dong and Lapata, 2018; Sun et al., 2018; Gur et al., 2018; Guo et al., 201"
D19-1537,P17-1167,0,0.0294541,"al., 2018; Yu et al., 2018a; Huang et al., 2018; Dong and Lapata, 2018; Sun et al., 2018; Gur et al., 2018; Guo et al., 2018; Yavuz et al., 2018; Shi et al., 2018). Most of them still focus on context-independent semantic parsing by converting single-turn questions into executable queries. Relatively less effort has been devoted to context-dependent semantic parsing on datasets including ATIS (Hemphill et al., 1990; Dahl et al., 1994b), SpaceBook (Vlachos and Clark, 2014), SCONE (Long et al., 2016; Guu et al., 2017; Fried et al., 2018; Suhr and Artzi, 2018; Huang et al., 2019), SequentialQA (Iyyer et al., 2017), SParC (Yu et al., 2019b) and CoSQL (Yu et al., 2019a). On ATIS, Miller et al. (1996) maps utterances to semantic frames which are then mapped to SQL queries; Zettlemoyer and Collins (2009) starts with context-independent Combinatory Categorial Grammar (CCG) parsing and then resolves references to generate lambda-calculus logical forms for sequences of sentences. The most relevant to our work is Suhr et al. (2018), who generate ATIS SQL queries from interactions by incorporating history with an interaction-level encoder and copying segments of previously generated queries. Furthermore, SCONE"
D19-1537,D19-1624,0,0.0439311,"r which takes bag-of-words representations of column headers as input. They also modify the decoder to select between a SQL keyword or a column header. (2) SyntaxSQL-con: This is adapted from the original context-agnostic SyntaxSQLNet (Yu et al., 2018b) by using bi-LSTMs to encode the interaction history including the utterance and the associated SQL query response. It also employs a column attention mechanism to compute representations of the previous question and SQL query. Spider. We compare with the results as reported in Yu et al. (2018b). Furthermore, we also include recent results from Lee (2019) who propose to use recursive decoding procedure, Bogin SQLNet (Xu et al., 2017) SyntaxSQLNet (Yu et al., 2018b) +data augmentation (Yu et al., 2018b) Lee (2019) GNN (Bogin et al., 2019) IRNet (Guo et al., 2019) IRNet (BERT) (Guo et al., 2019) Ours + utterance-table BERT Embedding Dev Set 10.9 18.9 24.8 28.5 40.7 53.2 61.9 36.4 57.6 Test Set 12.4 19.7 27.2 24.3 39.4 46.7 54.7 32.9 53.4 Table 4: Spider results on dev set and test set. et al. (2019) introducing graph neural networks for encoding schemas, and Guo et al. (2019) who achieve state-of-the-art performance by using an intermediate repr"
D19-1537,P16-1138,0,0.0348913,"promising results (Dong and Lapata, 2016; Su and Yan, 2017; Iyer et al., 2017; Xu et al., 2017; Finegan-Dollak et al., 2018; Yu et al., 2018a; Huang et al., 2018; Dong and Lapata, 2018; Sun et al., 2018; Gur et al., 2018; Guo et al., 2018; Yavuz et al., 2018; Shi et al., 2018). Most of them still focus on context-independent semantic parsing by converting single-turn questions into executable queries. Relatively less effort has been devoted to context-dependent semantic parsing on datasets including ATIS (Hemphill et al., 1990; Dahl et al., 1994b), SpaceBook (Vlachos and Clark, 2014), SCONE (Long et al., 2016; Guu et al., 2017; Fried et al., 2018; Suhr and Artzi, 2018; Huang et al., 2019), SequentialQA (Iyyer et al., 2017), SParC (Yu et al., 2019b) and CoSQL (Yu et al., 2019a). On ATIS, Miller et al. (1996) maps utterances to semantic frames which are then mapped to SQL queries; Zettlemoyer and Collins (2009) starts with context-independent Combinatory Categorial Grammar (CCG) parsing and then resolves references to generate lambda-calculus logical forms for sequences of sentences. The most relevant to our work is Suhr et al. (2018), who generate ATIS SQL queries from interactions by incorporating"
D19-1537,D15-1166,0,0.0400847,"), (X2 , Y2 ), . . . , (Xt−1 , Yt−1 )] Furthermore, in the cross-domain setting, each interaction is grounded to a different database. Therefore, the model is also given the schema of the current database as an input. We consider relational databases with multiple tables, and each table contains multiple column headers: T = [c1 , c2 , . . . , cl , . . . , cm ] where m is the number of column headers, and each cl consists of multiple words including its table name and column name (§ 3.1). 3 Methodology We employ an encoder-decoder architecture with attention mechanisms (Sutskever et al., 2014; Luong et al., 2015) as illustrated in Figure 1. The framework consists of (1) an utterance-table encoder to explicitly encode the user utterance and table schema at each turn, (2) A turn attention incorporating the recent history for decoding, (3) a table-aware decoder taking into account the context of the utterance, the table schema, and the previously generated query to make editing decisions. 3.1 Utterance-Table Encoder An effective encoder captures the meaning of user utterances, the structure of table schema, and the relationship between the two. To this end, we build an utterance-table encoder with co-att"
D19-1537,P96-1008,0,0.628325,"ations by copying from the source sentences. By contrast, our focus is on editing the previously generated query while incorporating the context of user utterances and table schemas. 4 Related Work Semantic parsing is the task of mapping natural language sentences into formal representations. It has been studied for decades including using linguistically-motivated compositional representations, such as logical forms (Zelle and Mooney, 1996; Clarke et al., 2010) and lambda calculus (Zettlemoyer and Collins, 2005; Artzi and Zettlemoyer, 2011), and using executable programs, such as SQL queries (Miller et al., 1996; Zhong et al., 2017) and other general-purpose programming languages (Yin and Neubig, 2017; Iyer et al., 2018). Most of the early studies worked on a few domains and small datasets such as GeoQuery (Zelle and Mooney, 1996) and Overnight (Wang et al., 2015). Recently, large and cross-domain text-to-SQL datasets such as WikiSQL (Zhong et al., 2017) and Spider (Yu et al., 2018c) have received an increasing amount of attention as many data-driven neural approaches achieve promising results (Dong and Lapata, 2016; Su and Yan, 2017; Iyer et al., 2017; Xu et al., 2017; Finegan-Dollak et al., 2018; Y"
D19-1537,P15-1142,0,0.0267292,"ith context-independent Combinatory Categorial Grammar (CCG) parsing and then resolves references to generate lambda-calculus logical forms for sequences of sentences. The most relevant to our work is Suhr et al. (2018), who generate ATIS SQL queries from interactions by incorporating history with an interaction-level encoder and copying segments of previously generated queries. Furthermore, SCONE contains three domains using stack- or list-like elements and most queries include a single binary predicate. SequentialQA is created by decomposing some complicated questions in WikiTableQuestions (Pasupat and Liang, 2015). Since both SCONE and SequentialQA are annotated with only denotations but not query labels, they don’t include many questions with rich semantic and contextual types. For example, SequentialQA (Iyyer et al., 2017) requires that the answer to follow-up questions must be a subset of previous answers, and most of the questions can be answered by simple SQL queries with SELECT and WHERE clauses. Concurrent with our work, Yu et al. (2019a) introduced CoSQL, a large-scale cross-domain conversational text-to-SQL corpus collected under the Wizard-of-Oz setting. Each dialogue in CoSQL simulates a DB"
D19-1537,D17-1127,0,0.0360468,"oyer, 2011), and using executable programs, such as SQL queries (Miller et al., 1996; Zhong et al., 2017) and other general-purpose programming languages (Yin and Neubig, 2017; Iyer et al., 2018). Most of the early studies worked on a few domains and small datasets such as GeoQuery (Zelle and Mooney, 1996) and Overnight (Wang et al., 2015). Recently, large and cross-domain text-to-SQL datasets such as WikiSQL (Zhong et al., 2017) and Spider (Yu et al., 2018c) have received an increasing amount of attention as many data-driven neural approaches achieve promising results (Dong and Lapata, 2016; Su and Yan, 2017; Iyer et al., 2017; Xu et al., 2017; Finegan-Dollak et al., 2018; Yu et al., 2018a; Huang et al., 2018; Dong and Lapata, 2018; Sun et al., 2018; Gur et al., 2018; Guo et al., 2018; Yavuz et al., 2018; Shi et al., 2018). Most of them still focus on context-independent semantic parsing by converting single-turn questions into executable queries. Relatively less effort has been devoted to context-dependent semantic parsing on datasets including ATIS (Hemphill et al., 1990; Dahl et al., 1994b), SpaceBook (Vlachos and Clark, 2014), SCONE (Long et al., 2016; Guu et al., 2017; Fried et al., 2018; Su"
D19-1537,P18-1193,0,0.0113637,"17; Iyer et al., 2017; Xu et al., 2017; Finegan-Dollak et al., 2018; Yu et al., 2018a; Huang et al., 2018; Dong and Lapata, 2018; Sun et al., 2018; Gur et al., 2018; Guo et al., 2018; Yavuz et al., 2018; Shi et al., 2018). Most of them still focus on context-independent semantic parsing by converting single-turn questions into executable queries. Relatively less effort has been devoted to context-dependent semantic parsing on datasets including ATIS (Hemphill et al., 1990; Dahl et al., 1994b), SpaceBook (Vlachos and Clark, 2014), SCONE (Long et al., 2016; Guu et al., 2017; Fried et al., 2018; Suhr and Artzi, 2018; Huang et al., 2019), SequentialQA (Iyyer et al., 2017), SParC (Yu et al., 2019b) and CoSQL (Yu et al., 2019a). On ATIS, Miller et al. (1996) maps utterances to semantic frames which are then mapped to SQL queries; Zettlemoyer and Collins (2009) starts with context-independent Combinatory Categorial Grammar (CCG) parsing and then resolves references to generate lambda-calculus logical forms for sequences of sentences. The most relevant to our work is Suhr et al. (2018), who generate ATIS SQL queries from interactions by incorporating history with an interaction-level encoder and copying segme"
D19-1537,N18-1203,0,0.0305507,"Missing"
D19-1537,P18-1034,0,0.0408402,"languages (Yin and Neubig, 2017; Iyer et al., 2018). Most of the early studies worked on a few domains and small datasets such as GeoQuery (Zelle and Mooney, 1996) and Overnight (Wang et al., 2015). Recently, large and cross-domain text-to-SQL datasets such as WikiSQL (Zhong et al., 2017) and Spider (Yu et al., 2018c) have received an increasing amount of attention as many data-driven neural approaches achieve promising results (Dong and Lapata, 2016; Su and Yan, 2017; Iyer et al., 2017; Xu et al., 2017; Finegan-Dollak et al., 2018; Yu et al., 2018a; Huang et al., 2018; Dong and Lapata, 2018; Sun et al., 2018; Gur et al., 2018; Guo et al., 2018; Yavuz et al., 2018; Shi et al., 2018). Most of them still focus on context-independent semantic parsing by converting single-turn questions into executable queries. Relatively less effort has been devoted to context-dependent semantic parsing on datasets including ATIS (Hemphill et al., 1990; Dahl et al., 1994b), SpaceBook (Vlachos and Clark, 2014), SCONE (Long et al., 2016; Guu et al., 2017; Fried et al., 2018; Suhr and Artzi, 2018; Huang et al., 2019), SequentialQA (Iyyer et al., 2017), SParC (Yu et al., 2019b) and CoSQL (Yu et al., 2019a). On ATIS, Mill"
D19-1537,P15-1129,0,0.0350818,"tences into formal representations. It has been studied for decades including using linguistically-motivated compositional representations, such as logical forms (Zelle and Mooney, 1996; Clarke et al., 2010) and lambda calculus (Zettlemoyer and Collins, 2005; Artzi and Zettlemoyer, 2011), and using executable programs, such as SQL queries (Miller et al., 1996; Zhong et al., 2017) and other general-purpose programming languages (Yin and Neubig, 2017; Iyer et al., 2018). Most of the early studies worked on a few domains and small datasets such as GeoQuery (Zelle and Mooney, 1996) and Overnight (Wang et al., 2015). Recently, large and cross-domain text-to-SQL datasets such as WikiSQL (Zhong et al., 2017) and Spider (Yu et al., 2018c) have received an increasing amount of attention as many data-driven neural approaches achieve promising results (Dong and Lapata, 2016; Su and Yan, 2017; Iyer et al., 2017; Xu et al., 2017; Finegan-Dollak et al., 2018; Yu et al., 2018a; Huang et al., 2018; Dong and Lapata, 2018; Sun et al., 2018; Gur et al., 2018; Guo et al., 2018; Yavuz et al., 2018; Shi et al., 2018). Most of them still focus on context-independent semantic parsing by converting single-turn questions int"
D19-1537,D14-1162,1,0.106791,"(Guo et al., 2019) IRNet (BERT) (Guo et al., 2019) Ours + utterance-table BERT Embedding Dev Set 10.9 18.9 24.8 28.5 40.7 53.2 61.9 36.4 57.6 Test Set 12.4 19.7 27.2 24.3 39.4 46.7 54.7 32.9 53.4 Table 4: Spider results on dev set and test set. et al. (2019) introducing graph neural networks for encoding schemas, and Guo et al. (2019) who achieve state-of-the-art performance by using an intermediate representation to bridge natural language questions and SQL queries. 5.3 Implementation Details Our model is implemented in PyTorch (Paszke et al., 2017). We use pretrained 300-dimensional GloVe (Pennington et al., 2014) word embedding. All LSTM layers have 300 hidden size, and we use 1 layer for encoder LSTMs, and 2 layers for decoder LSTMs. We use the ADAM optimizer (Kingma and Ba, 2015) to minimize the tokenlevel cross-entropy loss with a batch size of 16. Model parameters are randomly initialized from a uniform distribution U [−0.1, 0.1]. The main model has an initial learning rate of 0.001 and it will be multiplied by 0.8 if the validation loss increases compared with the previous epoch. When using BERT instead of GloVe, we use the pretrained small uncased BERT model with 768 hidden size5 , and we fine t"
D19-1537,P17-1099,0,0.0417505,"pcopy to decide if we need copy from the previous query or insert a new token. pcopy = σ(ck Wcopy + bcopy ) pinsert = 1 − pcopy (5) Then, we use a separate layer to score the query tokens at turn t − 1, and the output distribution is modified as the following to take into account the editing probability: Pprev SQL = softmax(ok Wprev SQL hQ t−1 ) mSQL = ok WSQL + bSQL mcolumn = ok Wcolumn hC PSQL S column = softmax([mSQL ; mcolumn ]) P (yk ) = pcopy · Pprev SQL (yk ∈ prev SQL) [ +pinsert · PSQL S column (yk ∈ SQL column) (6) While the copy mechanism has been introduced by Gu et al. (2016) and See et al. (2017), they focus on summarization or response generation applications by copying from the source sentences. By contrast, our focus is on editing the previously generated query while incorporating the context of user utterances and table schemas. 4 Related Work Semantic parsing is the task of mapping natural language sentences into formal representations. It has been studied for decades including using linguistically-motivated compositional representations, such as logical forms (Zelle and Mooney, 1996; Clarke et al., 2010) and lambda calculus (Zettlemoyer and Collins, 2005; Artzi and Zettlemoyer,"
D19-1537,D18-1197,0,0.0120536,"ost of the early studies worked on a few domains and small datasets such as GeoQuery (Zelle and Mooney, 1996) and Overnight (Wang et al., 2015). Recently, large and cross-domain text-to-SQL datasets such as WikiSQL (Zhong et al., 2017) and Spider (Yu et al., 2018c) have received an increasing amount of attention as many data-driven neural approaches achieve promising results (Dong and Lapata, 2016; Su and Yan, 2017; Iyer et al., 2017; Xu et al., 2017; Finegan-Dollak et al., 2018; Yu et al., 2018a; Huang et al., 2018; Dong and Lapata, 2018; Sun et al., 2018; Gur et al., 2018; Guo et al., 2018; Yavuz et al., 2018; Shi et al., 2018). Most of them still focus on context-independent semantic parsing by converting single-turn questions into executable queries. Relatively less effort has been devoted to context-dependent semantic parsing on datasets including ATIS (Hemphill et al., 1990; Dahl et al., 1994b), SpaceBook (Vlachos and Clark, 2014), SCONE (Long et al., 2016; Guu et al., 2017; Fried et al., 2018; Suhr and Artzi, 2018; Huang et al., 2019), SequentialQA (Iyyer et al., 2017), SParC (Yu et al., 2019b) and CoSQL (Yu et al., 2019a). On ATIS, Miller et al. (1996) maps utterances to semantic frames whic"
D19-1537,P17-1041,0,0.060978,"iously generated query while incorporating the context of user utterances and table schemas. 4 Related Work Semantic parsing is the task of mapping natural language sentences into formal representations. It has been studied for decades including using linguistically-motivated compositional representations, such as logical forms (Zelle and Mooney, 1996; Clarke et al., 2010) and lambda calculus (Zettlemoyer and Collins, 2005; Artzi and Zettlemoyer, 2011), and using executable programs, such as SQL queries (Miller et al., 1996; Zhong et al., 2017) and other general-purpose programming languages (Yin and Neubig, 2017; Iyer et al., 2018). Most of the early studies worked on a few domains and small datasets such as GeoQuery (Zelle and Mooney, 1996) and Overnight (Wang et al., 2015). Recently, large and cross-domain text-to-SQL datasets such as WikiSQL (Zhong et al., 2017) and Spider (Yu et al., 2018c) have received an increasing amount of attention as many data-driven neural approaches achieve promising results (Dong and Lapata, 2016; Su and Yan, 2017; Iyer et al., 2017; Xu et al., 2017; Finegan-Dollak et al., 2018; Yu et al., 2018a; Huang et al., 2018; Dong and Lapata, 2018; Sun et al., 2018; Gur et al., 2"
D19-1537,N18-2093,1,0.932754,"representations, such as logical forms (Zelle and Mooney, 1996; Clarke et al., 2010) and lambda calculus (Zettlemoyer and Collins, 2005; Artzi and Zettlemoyer, 2011), and using executable programs, such as SQL queries (Miller et al., 1996; Zhong et al., 2017) and other general-purpose programming languages (Yin and Neubig, 2017; Iyer et al., 2018). Most of the early studies worked on a few domains and small datasets such as GeoQuery (Zelle and Mooney, 1996) and Overnight (Wang et al., 2015). Recently, large and cross-domain text-to-SQL datasets such as WikiSQL (Zhong et al., 2017) and Spider (Yu et al., 2018c) have received an increasing amount of attention as many data-driven neural approaches achieve promising results (Dong and Lapata, 2016; Su and Yan, 2017; Iyer et al., 2017; Xu et al., 2017; Finegan-Dollak et al., 2018; Yu et al., 2018a; Huang et al., 2018; Dong and Lapata, 2018; Sun et al., 2018; Gur et al., 2018; Guo et al., 2018; Yavuz et al., 2018; Shi et al., 2018). Most of them still focus on context-independent semantic parsing by converting single-turn questions into executable queries. Relatively less effort has been devoted to context-dependent semantic parsing on datasets includin"
D19-1537,P09-1110,0,0.0185736,"still focus on context-independent semantic parsing by converting single-turn questions into executable queries. Relatively less effort has been devoted to context-dependent semantic parsing on datasets including ATIS (Hemphill et al., 1990; Dahl et al., 1994b), SpaceBook (Vlachos and Clark, 2014), SCONE (Long et al., 2016; Guu et al., 2017; Fried et al., 2018; Suhr and Artzi, 2018; Huang et al., 2019), SequentialQA (Iyyer et al., 2017), SParC (Yu et al., 2019b) and CoSQL (Yu et al., 2019a). On ATIS, Miller et al. (1996) maps utterances to semantic frames which are then mapped to SQL queries; Zettlemoyer and Collins (2009) starts with context-independent Combinatory Categorial Grammar (CCG) parsing and then resolves references to generate lambda-calculus logical forms for sequences of sentences. The most relevant to our work is Suhr et al. (2018), who generate ATIS SQL queries from interactions by incorporating history with an interaction-level encoder and copying segments of previously generated queries. Furthermore, SCONE contains three domains using stack- or list-like elements and most queries include a single binary predicate. SequentialQA is created by decomposing some complicated questions in WikiTableQu"
D19-1537,D18-1193,1,0.950706,"representations, such as logical forms (Zelle and Mooney, 1996; Clarke et al., 2010) and lambda calculus (Zettlemoyer and Collins, 2005; Artzi and Zettlemoyer, 2011), and using executable programs, such as SQL queries (Miller et al., 1996; Zhong et al., 2017) and other general-purpose programming languages (Yin and Neubig, 2017; Iyer et al., 2018). Most of the early studies worked on a few domains and small datasets such as GeoQuery (Zelle and Mooney, 1996) and Overnight (Wang et al., 2015). Recently, large and cross-domain text-to-SQL datasets such as WikiSQL (Zhong et al., 2017) and Spider (Yu et al., 2018c) have received an increasing amount of attention as many data-driven neural approaches achieve promising results (Dong and Lapata, 2016; Su and Yan, 2017; Iyer et al., 2017; Xu et al., 2017; Finegan-Dollak et al., 2018; Yu et al., 2018a; Huang et al., 2018; Dong and Lapata, 2018; Sun et al., 2018; Gur et al., 2018; Guo et al., 2018; Yavuz et al., 2018; Shi et al., 2018). Most of them still focus on context-independent semantic parsing by converting single-turn questions into executable queries. Relatively less effort has been devoted to context-dependent semantic parsing on datasets includin"
D19-1537,D18-1425,1,0.89856,"Missing"
D19-1537,P19-1443,1,0.875363,"Missing"
D19-6106,P14-1006,0,0.0249403,"relies on transfer learning between high- and low-resource languages. Word embeddings trained with the Word2Vec (Mikolov et al., 2013b) or GloVe (Pennington et al., 2014) algorithms are trained with large amounts of unsupervised data and transferred to downstream tasksspecific architectures in order to improve performance. Multilingual word embeddings have been trained with varying levels of supervision. Parallel corpora can be leveraged when data is available (Gouws et al., 2015; Luong et al., 2015), monolingual embeddings can be learned separately (Klementiev et al., 2012; Zou et al., 2013; Hermann and Blunsom, 2014) and then aligned 47 Proceedings of the 2nd Workshop on Deep Learning Approaches for Low-Resource NLP (DeepLo), pages 47–55 c Hong Kong, China, November 3, 2019. 2019 Association for Computational Linguistics https://doi.org/10.18653/v1/P17 Figure 1: Agglomerative clustering of Languages based on the PWCCA similarity between their represenations, generated from layer 6 of a pretrained multilingual uncased BERT. data from ten distinct genres of English language for the the task of natural language inference (prediction of whether the relationship between two sentences represents entailment, con"
D19-6106,C12-1089,0,0.0312437,"ssing (NLP) in multilingual settings often relies on transfer learning between high- and low-resource languages. Word embeddings trained with the Word2Vec (Mikolov et al., 2013b) or GloVe (Pennington et al., 2014) algorithms are trained with large amounts of unsupervised data and transferred to downstream tasksspecific architectures in order to improve performance. Multilingual word embeddings have been trained with varying levels of supervision. Parallel corpora can be leveraged when data is available (Gouws et al., 2015; Luong et al., 2015), monolingual embeddings can be learned separately (Klementiev et al., 2012; Zou et al., 2013; Hermann and Blunsom, 2014) and then aligned 47 Proceedings of the 2nd Workshop on Deep Learning Approaches for Low-Resource NLP (DeepLo), pages 47–55 c Hong Kong, China, November 3, 2019. 2019 Association for Computational Linguistics https://doi.org/10.18653/v1/P17 Figure 1: Agglomerative clustering of Languages based on the PWCCA similarity between their represenations, generated from layer 6 of a pretrained multilingual uncased BERT. data from ten distinct genres of English language for the the task of natural language inference (prediction of whether the relationship be"
D19-6106,D19-1167,0,0.373251,"t al., 2013; Conneau et al., 2018). A multilingual version of BERT trained on over 100 languages achieved state-of-the-art performance across a wide range of languages as well. Performance for lowresource languages has been further improved by additionally leveraging parallel data (Lample and Conneau, 2019) and leveraging machine translation systems for cross-lingual regularization (Singh et al., 2019). Prior work in zero-shot machine translation has investigated the extent to which multilingual neural machine translation systems trained with a shared subword vocabulary Johnson et al. (2017); Kudugunta et al. (2019) learn a form of interlingua, a common representational space for semantically similar text across languages. We aim to extend this study to language models pretrained with multilingual data in order to investigate the extent to which the resulting contextualized word embeddings represent an interlingua. Canonical correlation analysis (CCA) is a classical tool from multivariate statistics (Hotelling, 1992) that investigates the relationships between two sets of random variables. Singular value and projection weighted variants of CCA allow for analysis of representations of the same data points"
D19-6106,N19-1112,0,0.036883,"all language combinations tested, the summary representation (associated with the [CLS] token) for semantically similar inputs translated into multiple languages is most similar at the shallower layers of BERT, close to the initial embeddings. The representations steadily become more dissimilar in deeper layers until the final layer. The jump in similarity in the final layer can be explained by the common classification layer that contains only three classes. In order to finally choose an output class, the network must project towards one of the three embeddings associated with those classes (Liu et al., 2019). The trend towards dissimilarity in deeper layers suggests that contextualization in BERT is not a process of semantic abstraction as would be expected of an interlingua. Though semantic features common to the multiple translations of the input might also be extracted, the similarity between representations is dominated by features that differentiate them. BERT appears to preserve and refine features that separate the inputs, which we speculate are more closely related to syntactic and grammatical features of the input. Representations at the shallower layers, closer to the subword embeddings"
D19-6106,P18-1073,0,0.044593,"Missing"
D19-6106,W15-1521,0,0.0156455,"nto multilingual representations. 1 Introduction Natural language processing (NLP) in multilingual settings often relies on transfer learning between high- and low-resource languages. Word embeddings trained with the Word2Vec (Mikolov et al., 2013b) or GloVe (Pennington et al., 2014) algorithms are trained with large amounts of unsupervised data and transferred to downstream tasksspecific architectures in order to improve performance. Multilingual word embeddings have been trained with varying levels of supervision. Parallel corpora can be leveraged when data is available (Gouws et al., 2015; Luong et al., 2015), monolingual embeddings can be learned separately (Klementiev et al., 2012; Zou et al., 2013; Hermann and Blunsom, 2014) and then aligned 47 Proceedings of the 2nd Workshop on Deep Learning Approaches for Low-Resource NLP (DeepLo), pages 47–55 c Hong Kong, China, November 3, 2019. 2019 Association for Computational Linguistics https://doi.org/10.18653/v1/P17 Figure 1: Agglomerative clustering of Languages based on the PWCCA similarity between their represenations, generated from layer 6 of a pretrained multilingual uncased BERT. data from ten distinct genres of English language for the the ta"
D19-6106,D18-1269,0,0.0823751,"g Xiong2 , Richard Socher2 Stanford University1 , Salesforce Research2 jasdeep@cs.stanford.edu {bmccann,cxiong,rsocher}@salesforce.com Abstract using dictionaries between languages (Mikolov et al., 2013a; Faruqui and Dyer, 2014), and crosslingual embeddings can be learned jointly through entirely unsupervised methods (Conneau et al., 2017; Artetxe et al., 2018). Contextualized word embeddings like CoVe, ElMo, and BERT (McCann et al., 2017; Peters et al., 2018; Devlin et al., 2018) improve a wide variety of natural language tasks (Wang et al., 2018; Rajpurkar et al., 2016; Socher et al., 2013; Conneau et al., 2018). A multilingual version of BERT trained on over 100 languages achieved state-of-the-art performance across a wide range of languages as well. Performance for lowresource languages has been further improved by additionally leveraging parallel data (Lample and Conneau, 2019) and leveraging machine translation systems for cross-lingual regularization (Singh et al., 2019). Prior work in zero-shot machine translation has investigated the extent to which multilingual neural machine translation systems trained with a shared subword vocabulary Johnson et al. (2017); Kudugunta et al. (2019) learn a fo"
D19-6106,E14-1049,0,0.0607179,"Missing"
D19-6106,D13-1141,1,0.788308,"al settings often relies on transfer learning between high- and low-resource languages. Word embeddings trained with the Word2Vec (Mikolov et al., 2013b) or GloVe (Pennington et al., 2014) algorithms are trained with large amounts of unsupervised data and transferred to downstream tasksspecific architectures in order to improve performance. Multilingual word embeddings have been trained with varying levels of supervision. Parallel corpora can be leveraged when data is available (Gouws et al., 2015; Luong et al., 2015), monolingual embeddings can be learned separately (Klementiev et al., 2012; Zou et al., 2013; Hermann and Blunsom, 2014) and then aligned 47 Proceedings of the 2nd Workshop on Deep Learning Approaches for Low-Resource NLP (DeepLo), pages 47–55 c Hong Kong, China, November 3, 2019. 2019 Association for Computational Linguistics https://doi.org/10.18653/v1/P17 Figure 1: Agglomerative clustering of Languages based on the PWCCA similarity between their represenations, generated from layer 6 of a pretrained multilingual uncased BERT. data from ten distinct genres of English language for the the task of natural language inference (prediction of whether the relationship between two sentence"
D19-6106,D14-1162,1,0.106837,"enetic trees hand-designed by linguists. The subword tokenization employed by BERT provides a stronger bias towards such structure than character- and wordlevel tokenizations. We release a subset of the XNLI dataset translated into an additional 14 languages at https://www.github. com/salesforce/xnli_extension to assist further research into multilingual representations. 1 Introduction Natural language processing (NLP) in multilingual settings often relies on transfer learning between high- and low-resource languages. Word embeddings trained with the Word2Vec (Mikolov et al., 2013b) or GloVe (Pennington et al., 2014) algorithms are trained with large amounts of unsupervised data and transferred to downstream tasksspecific architectures in order to improve performance. Multilingual word embeddings have been trained with varying levels of supervision. Parallel corpora can be leveraged when data is available (Gouws et al., 2015; Luong et al., 2015), monolingual embeddings can be learned separately (Klementiev et al., 2012; Zou et al., 2013; Hermann and Blunsom, 2014) and then aligned 47 Proceedings of the 2nd Workshop on Deep Learning Approaches for Low-Resource NLP (DeepLo), pages 47–55 c Hong Kong, China,"
D19-6106,N18-1202,0,0.0530808,"Missing"
D19-6106,D16-1264,0,0.0117057,"ation Jasdeep Singh1 , Bryan McCann2 , Caiming Xiong2 , Richard Socher2 Stanford University1 , Salesforce Research2 jasdeep@cs.stanford.edu {bmccann,cxiong,rsocher}@salesforce.com Abstract using dictionaries between languages (Mikolov et al., 2013a; Faruqui and Dyer, 2014), and crosslingual embeddings can be learned jointly through entirely unsupervised methods (Conneau et al., 2017; Artetxe et al., 2018). Contextualized word embeddings like CoVe, ElMo, and BERT (McCann et al., 2017; Peters et al., 2018; Devlin et al., 2018) improve a wide variety of natural language tasks (Wang et al., 2018; Rajpurkar et al., 2016; Socher et al., 2013; Conneau et al., 2018). A multilingual version of BERT trained on over 100 languages achieved state-of-the-art performance across a wide range of languages as well. Performance for lowresource languages has been further improved by additionally leveraging parallel data (Lample and Conneau, 2019) and leveraging machine translation systems for cross-lingual regularization (Singh et al., 2019). Prior work in zero-shot machine translation has investigated the extent to which multilingual neural machine translation systems trained with a shared subword vocabulary Johnson et al"
D19-6106,D13-1170,1,0.016616,"ryan McCann2 , Caiming Xiong2 , Richard Socher2 Stanford University1 , Salesforce Research2 jasdeep@cs.stanford.edu {bmccann,cxiong,rsocher}@salesforce.com Abstract using dictionaries between languages (Mikolov et al., 2013a; Faruqui and Dyer, 2014), and crosslingual embeddings can be learned jointly through entirely unsupervised methods (Conneau et al., 2017; Artetxe et al., 2018). Contextualized word embeddings like CoVe, ElMo, and BERT (McCann et al., 2017; Peters et al., 2018; Devlin et al., 2018) improve a wide variety of natural language tasks (Wang et al., 2018; Rajpurkar et al., 2016; Socher et al., 2013; Conneau et al., 2018). A multilingual version of BERT trained on over 100 languages achieved state-of-the-art performance across a wide range of languages as well. Performance for lowresource languages has been further improved by additionally leveraging parallel data (Lample and Conneau, 2019) and leveraging machine translation systems for cross-lingual regularization (Singh et al., 2019). Prior work in zero-shot machine translation has investigated the extent to which multilingual neural machine translation systems trained with a shared subword vocabulary Johnson et al. (2017); Kudugunta e"
D19-6106,W18-5446,0,0.0484003,"Missing"
P12-1092,D10-1113,0,0.0328734,"th human similarity ratings on pairs of words, such as WordSim-353 (Finkelstein et al., 2001), MC (Miller and Charles, 1991) and RG (Rubenstein and Goodenough, 1965), have been widely used to evaluate vector-space models. Motivated to evaluate composition models, Mitchell and Lapata (2008) introduced a dataset where an intransitive verb, presented with a subject noun, is com880 pared to another verb chosen to be either similar or dissimilar to the intransitive verb in context. The context is short, with only one word, and only verbs are compared. Erk and Pad´o (2008), Thater et al. (2011) and Dinu and Lapata (2010) evaluated word similarity in context with a modified task where systems are to rerank gold-standard paraphrase candidates given the SemEval 2007 Lexical Substitution Task dataset. This task only indirectly evaluates similarity as only reranking of already similar words are evaluated. 6 Conclusion We presented a new neural network architecture that learns more semantic word representations by using both local and global context in learning. These learned word embeddings can be used to represent word contexts as low-dimensional weighted average vectors, which are then clustered to form differen"
P12-1092,D08-1094,0,0.0132498,"Missing"
P12-1092,P08-1028,0,0.238889,"aches and applied them to vector-space models, which was further improved in Reisinger and Mooney (2010a). Two other recent papers (Dhillon et al., 2011; Reddy et al., 2011) present models for constructing word representations that deal with context. It would be interesting to evaluate those models on our new dataset. Many datasets with human similarity ratings on pairs of words, such as WordSim-353 (Finkelstein et al., 2001), MC (Miller and Charles, 1991) and RG (Rubenstein and Goodenough, 1965), have been widely used to evaluate vector-space models. Motivated to evaluate composition models, Mitchell and Lapata (2008) introduced a dataset where an intransitive verb, presented with a subject noun, is com880 pared to another verb chosen to be either similar or dissimilar to the intransitive verb in context. The context is short, with only one word, and only verbs are compared. Erk and Pad´o (2008), Thater et al. (2011) and Dinu and Lapata (2010) evaluated word similarity in context with a modified task where systems are to rerank gold-standard paraphrase candidates given the SemEval 2007 Lexical Substitution Task dataset. This task only indirectly evaluates similarity as only reranking of already similar wor"
P12-1092,I11-1079,0,0.0263083,"are competitive in word similarity tasks. Most of the previous vector-space models use a single vector to represent a word even though many words have multiple meanings. The multi-prototype approach has been widely studied in models of categorization in psychology (Rosseel, 2002; Griffiths et al., 2009), while Sch¨utze (1998) used clustering of contexts to perform word sense discrimination. Reisinger and Mooney (2010b) combined the two approaches and applied them to vector-space models, which was further improved in Reisinger and Mooney (2010a). Two other recent papers (Dhillon et al., 2011; Reddy et al., 2011) present models for constructing word representations that deal with context. It would be interesting to evaluate those models on our new dataset. Many datasets with human similarity ratings on pairs of words, such as WordSim-353 (Finkelstein et al., 2001), MC (Miller and Charles, 1991) and RG (Rubenstein and Goodenough, 1965), have been widely used to evaluate vector-space models. Motivated to evaluate composition models, Mitchell and Lapata (2008) introduced a dataset where an intransitive verb, presented with a subject noun, is com880 pared to another verb chosen to be either similar or dis"
P12-1092,D10-1114,0,0.732839,"rious NLP tasks, one major limitation common to most of these models is that they assume only one representation for each word. This single-prototype representation is problematic because many words have multiple meanings, which can be wildly different. Using one representation simply cannot capture the different meanings. Moreover, using all contexts of a homonymous or polysemous word to build a single prototype could hurt the representation, which cannot represent any one of the meanings well as it is influenced by all meanings of the word. Instead of using only one representation per word, Reisinger and Mooney (2010b) proposed the multiprototype approach for vector-space models, which uses multiple representations to capture different senses and usages of a word. We show how our model can readily adopt the multi-prototype approach. We present a way to use our learned single-prototype embeddings to represent each context window, which can then be used by clustering to perform word sense discrimination (Sch¨utze, 1998). In order to learn multiple prototypes, we first gather the fixed-sized context windows of all occurrences of a word (we use 5 words before and after the word occurrence). Each context is re"
P12-1092,N10-1013,0,0.84753,"e WordSim-353 dataset (Finkelstein et al., 2001), which consists of 353 pairs of nouns. Each pair is presented without context and associated with 13 to 16 human judgments on similarity and relatedness on a scale from 0 to 10. For example, (cup,drink) received an average score of 7.25, while (cup,substance) received an average score of 1.92. Table 3 shows our results compared to previous methods, including C&W’s language model and the hierarchical log-bilinear (HLBL) model (Mnih and Hinton, 2008), which is a probabilistic, linear neural model. We downloaded these embeddings from Turian et al. (2010). These embeddings were trained on the smaller corpus RCV1 that contains one year of Reuters English newswire, and show similar correlations on the dataset. We report the result of 877 our re-implementation of C&W’s model trained on Wikipedia, showing the large effect of using a different corpus. Our model is able to learn more semantic word embeddings and noticeably improves upon C&W’s model. Note that our model achieves higher correlation (64.2) than either using local context alone (C&W: 55.3) or using global context alone (Our Model-g: 22.8). We also found that correlation can be further i"
P12-1092,J98-1004,0,0.0619562,"Missing"
P12-1092,D08-1027,1,0.0618006,"Missing"
P12-1092,D11-1014,1,0.460564,"Missing"
P12-1092,I11-1127,0,0.0697611,"Missing"
P12-1092,P10-1040,0,0.853332,"e models is the WordSim-353 dataset (Finkelstein et al., 2001), which consists of 353 pairs of nouns. Each pair is presented without context and associated with 13 to 16 human judgments on similarity and relatedness on a scale from 0 to 10. For example, (cup,drink) received an average score of 7.25, while (cup,substance) received an average score of 1.92. Table 3 shows our results compared to previous methods, including C&W’s language model and the hierarchical log-bilinear (HLBL) model (Mnih and Hinton, 2008), which is a probabilistic, linear neural model. We downloaded these embeddings from Turian et al. (2010). These embeddings were trained on the smaller corpus RCV1 that contains one year of Reuters English newswire, and show similar correlations on the dataset. We report the result of 877 our re-implementation of C&W’s model trained on Wikipedia, showing the large effect of using a different corpus. Our model is able to learn more semantic word embeddings and noticeably improves upon C&W’s model. Note that our model achieves higher correlation (64.2) than either using local context alone (C&W: 55.3) or using global context alone (Our Model-g: 22.8). We also found that correlation can be further i"
P12-1092,S10-1011,0,\N,Missing
P13-1045,P04-1013,0,0.544519,"erson (2003) was the first to show that neural networks can be successfully used for large scale parsing. He introduced a left-corner parser to estimate the probabilities of parsing decisions conditioned on the parsing history. The input to Henderson’s model consists of pairs of frequent words and their part-of-speech (POS) tags. Both the original parsing system and its probabilistic interpretation (Titov and Henderson, 2007) learn features that represent the parsing history and do not provide a principled linguistic representation like our phrase representations. Other related work includes (Henderson, 2004), who discriminatively trains a parser based on synchrony networks and (Titov and Henderson, 2006), who use an SVM to adapt a generative parser to different domains. Costa et al. (2003) apply recursive neural networks to re-rank possible phrase attachments in an incremental parser. Their work is the first to show that RNNs can capture enough information to make correct parsing decisions, but they only test on a subset of 2000 sentences. Menchetti et al. (2005) use RNNs to re-rank different parses. For their results on full sentence parsing, they rerank candidate trees created by the Collins pa"
P13-1045,W05-1506,0,0.0157004,"nuous nature of the word vectors, the probability of such a CVG rule application is not comparable to probabilities provided by a PCFG since the latter sum to 1 for all children. Assuming that node p1 has syntactic category P1 , we compute the second parent vector via:    a (2) (A,P1 ) p =f W . p(1) We use this knowledge to speed up inference via two bottom-up passes through the parsing chart. During the first one, we use only the base PCFG to run CKY dynamic programming through the tree. The k = 200-best parses at the top cell of the chart are calculated using the efficient algorithm of (Huang and Chiang, 2005). Then, the second pass is a beam search with the full CVG model (including the more expensive matrix multiplications of the SU-RNN). This beam search only considers phrases that appear in the top 200 parses. This is similar to a re-ranking setup but with one main difference: the SU-RNN rule score computation at each node still only has access to its child vectors, not the whole tree or other global features. This allows the second pass to be very fast. We use this setup in our experiments below. The score of the last parent in this trigram is computed via:   T s p(2) = v (A,P1 ) p(2) + log"
P13-1045,J92-4003,0,0.182319,"s in such parsers can be seen as defining effective dimensions of similarity between categories. Second, lexicalized parsers (Collins, 2003; Charniak, 2000) associate each category with a lexical item. This gives a fine-grained notion of semantic similarity, which is useful for tackling problems like ambiguous attachment decisions. However, this approach necessitates complex shrinkage estimation schemes to deal with the sparsity of observations of the lexicalized categories. In many natural language systems, single words and n-grams are usefully described by their distributional similarities (Brown et al., 1992), among many others. But, even with large corpora, many Introduction Syntactic parsing is a central task in natural language processing because of its importance in mediating between linguistic expression and meaning. For example, much work has shown the usefulness of syntactic representations for subsequent tasks such as relation extraction, semantic role labeling (Gildea and Palmer, 2002) and paraphrase detection (Callison-Burch, 2008). Syntactic descriptions standardly use coarse discrete categories such as NP for noun phrases or PP for prepositional phrases. However, recent work has shown"
P13-1045,P12-1092,1,0.205939,"n ≈ queen (Mikolov et al., 2013). Collobert and Weston (2008) introduced a new model to compute such an embedding. The idea is to construct a neural network that outputs high scores for windows that occur in a large unlabeled corpus and low scores for windows where one word is replaced by a random word. When such a network is optimized via gradient ascent the derivatives backpropagate into the word embedding matrix X. In order to predict correct scores the vectors in the matrix capture co-occurrence statistics. For further details and evaluations of these embeddings, see (Turian et al., 2010; Huang et al., 2012). The resulting X matrix is used as follows. Assume we are given a sentence as an ordered list of m words. Each word w has an index [w] = i into the columns of the embedding matrix. This index is used to retrieve the word’s vector representation aw using a simple multiplication with a binary vector e, which is zero everywhere, except Compositional Vector Grammars This section introduces Compositional Vector Grammars (CVGs), a model to jointly find syntactic structure and capture compositional semantic information. CVGs build on two observations. Firstly, that a lot of the structure and regular"
P13-1045,D08-1021,0,0.0313765,"ervations of the lexicalized categories. In many natural language systems, single words and n-grams are usefully described by their distributional similarities (Brown et al., 1992), among many others. But, even with large corpora, many Introduction Syntactic parsing is a central task in natural language processing because of its importance in mediating between linguistic expression and meaning. For example, much work has shown the usefulness of syntactic representations for subsequent tasks such as relation extraction, semantic role labeling (Gildea and Palmer, 2002) and paraphrase detection (Callison-Burch, 2008). Syntactic descriptions standardly use coarse discrete categories such as NP for noun phrases or PP for prepositional phrases. However, recent work has shown that parsing results can be greatly improved by defining more fine-grained syntactic 455 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 455–465, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics sets of discrete states and recursive deep learning models that jointly learn classifiers and continuous feature representations for variable-sized inputs. n-grams w"
P13-1045,C12-2054,0,0.0497241,"every node. Our syntactically untied RNNs outperform them by a significant margin. The idea of untying has also been successfully used in deep learning applied to vision (Le et al., 2010). This paper uses several ideas of (Socher et al., 2011b). The main differences are (i) the dual representation of nodes as discrete categories and vectors, (ii) the combination with a PCFG, and (iii) the syntactic untying of weights based on child categories. We directly compare models with fully tied and untied weights. Another work that represents phrases with a dual discrete-continuous representation is (Kartsaklis et al., 2012). 3 3.1 Word Vector Representations In most systems that use a vector representation for words, such vectors are based on cooccurrence statistics of each word and its context (Turney and Pantel, 2010). Another line of research to learn distributional word vectors is based on neural language models (Bengio et al., 2003) which jointly learn an embedding of words into an n-dimensional feature space and use these embeddings to predict how suitable a word is in its context. These vector representations capture interesting linear relationships (up to some accuracy), such as king−man+woman ≈ queen (M"
P13-1045,P05-1022,0,0.0600912,"Missing"
P13-1045,P03-1054,1,0.0562259,"the current Stanford factored parser. The CVG learns a soft notion of head words and improves performance on the types of ambiguities that require semantic information such as PP attachments. 1 (riding a bike,VP, ) (a bike,NP, (riding,V, ) (a,Det, ) ) (bike,NN, ) Figure 1: Example of a CVG tree with (category,vector) representations at each node. The vectors for nonterminals are computed via a new type of recursive neural network which is conditioned on syntactic categories from a PCFG. categories, which better capture phrases with similar behavior, whether through manual feature engineering (Klein and Manning, 2003a) or automatic learning (Petrov et al., 2006). However, subdividing a category like NP into 30 or 60 subcategories can only provide a very limited representation of phrase meaning and semantic similarity. Two strands of work therefore attempt to go further. First, recent work in discriminative parsing has shown gains from careful engineering of features (Taskar et al., 2004; Finkel et al., 2008). Features in such parsers can be seen as defining effective dimensions of similarity between categories. Second, lexicalized parsers (Collins, 2003; Charniak, 2000) associate each category with a lexi"
P13-1045,A00-2018,0,0.477832,"manual feature engineering (Klein and Manning, 2003a) or automatic learning (Petrov et al., 2006). However, subdividing a category like NP into 30 or 60 subcategories can only provide a very limited representation of phrase meaning and semantic similarity. Two strands of work therefore attempt to go further. First, recent work in discriminative parsing has shown gains from careful engineering of features (Taskar et al., 2004; Finkel et al., 2008). Features in such parsers can be seen as defining effective dimensions of similarity between categories. Second, lexicalized parsers (Collins, 2003; Charniak, 2000) associate each category with a lexical item. This gives a fine-grained notion of semantic similarity, which is useful for tackling problems like ambiguous attachment decisions. However, this approach necessitates complex shrinkage estimation schemes to deal with the sparsity of observations of the lexicalized categories. In many natural language systems, single words and n-grams are usefully described by their distributional similarities (Brown et al., 1992), among many others. But, even with large corpora, many Introduction Syntactic parsing is a central task in natural language processing b"
P13-1045,P97-1003,0,0.167285,"Missing"
P13-1045,J03-4003,0,0.195632,"hether through manual feature engineering (Klein and Manning, 2003a) or automatic learning (Petrov et al., 2006). However, subdividing a category like NP into 30 or 60 subcategories can only provide a very limited representation of phrase meaning and semantic similarity. Two strands of work therefore attempt to go further. First, recent work in discriminative parsing has shown gains from careful engineering of features (Taskar et al., 2004; Finkel et al., 2008). Features in such parsers can be seen as defining effective dimensions of similarity between categories. Second, lexicalized parsers (Collins, 2003; Charniak, 2000) associate each category with a lexical item. This gives a fine-grained notion of semantic similarity, which is useful for tackling problems like ambiguous attachment decisions. However, this approach necessitates complex shrinkage estimation schemes to deal with the sparsity of observations of the lexicalized categories. In many natural language systems, single words and n-grams are usefully described by their distributional similarities (Brown et al., 1992), among many others. But, even with large corpora, many Introduction Syntactic parsing is a central task in natural lang"
P13-1045,D12-1096,0,0.214919,"red parser (Klein and Manning, 2003b)) and other parsers that use richer state representations: the Berkeley parser (Petrov and Klein, 2007), Collins parser (Collins, 1997), SSN: a statistical neural network parser (Henderson, 2004), Factored PCFGs (Hall and Klein, 2012), CharniakSelfTrain: the self-training approach of McClosky et al. (2006), which bootstraps and parses additional large corpora multiple times, Charniak-RS: the state of the art self-trained and discriminatively re-ranked Charniak-Johnson parser combining (Charniak, 2000; McClosky et al., 2006; Charniak and Johnson, 2005). See Kummerfeld et al. (2012) for more comparisons. We compare also to a standard RNN ‘CVG (RNN)’ and to the proposed CVG with SU-RNNs. 4.3 CVG 0.79 0.43 0.29 0.27 0.31 0.32 0.31 0.22 0.19 0.41 Table 2: Detailed comparison of different parsers. performance and were faster than 50-,100- or 200dimensional ones. We hypothesize that the larger word vector sizes, while capturing more semantic knowledge, result in too many SU-RNN matrix parameters to train and hence perform worse. 4.2 Stanford 1.02 0.64 0.40 0.37 0.44 0.39 0.48 0.35 0.28 0.62 Model Analysis Analysis of Error Types. Table 2 shows a detailed comparison of differe"
P13-1045,P05-1010,0,0.252159,"Missing"
P13-1045,N06-1020,0,0.319562,"Missing"
P13-1045,P08-1109,1,0.432055,"ype of recursive neural network which is conditioned on syntactic categories from a PCFG. categories, which better capture phrases with similar behavior, whether through manual feature engineering (Klein and Manning, 2003a) or automatic learning (Petrov et al., 2006). However, subdividing a category like NP into 30 or 60 subcategories can only provide a very limited representation of phrase meaning and semantic similarity. Two strands of work therefore attempt to go further. First, recent work in discriminative parsing has shown gains from careful engineering of features (Taskar et al., 2004; Finkel et al., 2008). Features in such parsers can be seen as defining effective dimensions of similarity between categories. Second, lexicalized parsers (Collins, 2003; Charniak, 2000) associate each category with a lexical item. This gives a fine-grained notion of semantic similarity, which is useful for tackling problems like ambiguous attachment decisions. However, this approach necessitates complex shrinkage estimation schemes to deal with the sparsity of observations of the lexicalized categories. In many natural language systems, single words and n-grams are usefully described by their distributional simil"
P13-1045,P02-1031,0,0.020181,"estimation schemes to deal with the sparsity of observations of the lexicalized categories. In many natural language systems, single words and n-grams are usefully described by their distributional similarities (Brown et al., 1992), among many others. But, even with large corpora, many Introduction Syntactic parsing is a central task in natural language processing because of its importance in mediating between linguistic expression and meaning. For example, much work has shown the usefulness of syntactic representations for subsequent tasks such as relation extraction, semantic role labeling (Gildea and Palmer, 2002) and paraphrase detection (Callison-Burch, 2008). Syntactic descriptions standardly use coarse discrete categories such as NP for noun phrases or PP for prepositional phrases. However, recent work has shown that parsing results can be greatly improved by defining more fine-grained syntactic 455 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 455–465, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics sets of discrete states and recursive deep learning models that jointly learn classifiers and continuous feature repr"
P13-1045,N07-1051,0,0.181023,"Missing"
P13-1045,P06-1055,0,0.178353,"rns a soft notion of head words and improves performance on the types of ambiguities that require semantic information such as PP attachments. 1 (riding a bike,VP, ) (a bike,NP, (riding,V, ) (a,Det, ) ) (bike,NN, ) Figure 1: Example of a CVG tree with (category,vector) representations at each node. The vectors for nonterminals are computed via a new type of recursive neural network which is conditioned on syntactic categories from a PCFG. categories, which better capture phrases with similar behavior, whether through manual feature engineering (Klein and Manning, 2003a) or automatic learning (Petrov et al., 2006). However, subdividing a category like NP into 30 or 60 subcategories can only provide a very limited representation of phrase meaning and semantic similarity. Two strands of work therefore attempt to go further. First, recent work in discriminative parsing has shown gains from careful engineering of features (Taskar et al., 2004; Finkel et al., 2008). Features in such parsers can be seen as defining effective dimensions of similarity between categories. Second, lexicalized parsers (Collins, 2003; Charniak, 2000) associate each category with a lexical item. This gives a fine-grained notion of"
P13-1045,D12-1105,0,0.0155869,".edu. 2 Improving Discrete Syntactic Representations As mentioned in the introduction, there are several approaches to improving discrete representations for parsing. Klein and Manning (2003a) use manual feature engineering, while Petrov et al. (2006) use a learning algorithm that splits and merges the syntactic categories in order to maximize likelihood on the treebank. Their approach splits categories into several dozen subcategories. Another approach is lexicalized parsers (Collins, 2003; Charniak, 2000) that describe each category with a lexical item, usually the head word. More recently, Hall and Klein (2012) combine several such annotation schemes in a factored parser. We extend the above ideas from discrete representations to richer continuous ones. The CVG can be seen as factoring discrete and continuous parsing in one model. Another different approach to the above generative models is to learn discriminative parsers using many well designed features (Taskar et al., 2004; Finkel et al., 2008). We also borrow ideas from this line of research in that our parser combines the generative PCFG model with discriminatively learned RNNs. Deep Learning and Recursive Deep Learning Early attempts at using"
P13-1045,E03-1002,0,0.0419821,"ne syntactic and semantic information by giving the parser access to rich syntacticosemantic information in the form of distributional word vectors and compute compositional semantic vector representations for longer phrases (Costa et al., 2003; Menchetti et al., 2005; Socher et al., 2011b). The CVG model merges ideas from both generative models that assume discrete syntactic categories and discriminative models that are trained using continuous vectors. We will first briefly introduce single word vector representations and then describe the CVG objective function, tree scoring and inference. Henderson (2003) was the first to show that neural networks can be successfully used for large scale parsing. He introduced a left-corner parser to estimate the probabilities of parsing decisions conditioned on the parsing history. The input to Henderson’s model consists of pairs of frequent words and their part-of-speech (POS) tags. Both the original parsing system and its probabilistic interpretation (Titov and Henderson, 2007) learn features that represent the parsing history and do not provide a principled linguistic representation like our phrase representations. Other related work includes (Henderson, 2"
P13-1045,D12-1110,1,0.410492,"standard RNNs can be used for parsing, see Socher et al. (2011b). The standard RNN requires a single composition function to capture all types of compositions: adjectives and nouns, verbs and nouns, adverbs and adjectives, etc. Even though this function is a powerful one, we find a single neural network weight matrix cannot fully capture the richness of compositionality. Several extensions are possible: A two-layered RNN would provide more expressive power, however, it is much harder to train because the resulting neural network becomes very deep and suffers from vanishing gradient problems. Socher et al. (2012) proposed to give every single word a matrix and a vector. The matrix is then applied to the sibling node’s vector during the composition. While this results in a powerful composition function that essentially depends on the words being combined, the number of model parameters explodes and the composition functions do not capture the syntactic commonalities between similar POS tags or syntactic categories. Based on the above considerations, we propose the Compositional Vector Grammar (CVG) that conditions the composition function at each node on discrete syntactic categories extracted from a w"
P13-1045,W04-3201,1,0.382076,"computed via a new type of recursive neural network which is conditioned on syntactic categories from a PCFG. categories, which better capture phrases with similar behavior, whether through manual feature engineering (Klein and Manning, 2003a) or automatic learning (Petrov et al., 2006). However, subdividing a category like NP into 30 or 60 subcategories can only provide a very limited representation of phrase meaning and semantic similarity. Two strands of work therefore attempt to go further. First, recent work in discriminative parsing has shown gains from careful engineering of features (Taskar et al., 2004; Finkel et al., 2008). Features in such parsers can be seen as defining effective dimensions of similarity between categories. Second, lexicalized parsers (Collins, 2003; Charniak, 2000) associate each category with a lexical item. This gives a fine-grained notion of semantic similarity, which is useful for tackling problems like ambiguous attachment decisions. However, this approach necessitates complex shrinkage estimation schemes to deal with the sparsity of observations of the lexicalized categories. In many natural language systems, single words and n-grams are usefully described by thei"
P13-1045,W06-2902,0,0.0177603,"ge scale parsing. He introduced a left-corner parser to estimate the probabilities of parsing decisions conditioned on the parsing history. The input to Henderson’s model consists of pairs of frequent words and their part-of-speech (POS) tags. Both the original parsing system and its probabilistic interpretation (Titov and Henderson, 2007) learn features that represent the parsing history and do not provide a principled linguistic representation like our phrase representations. Other related work includes (Henderson, 2004), who discriminatively trains a parser based on synchrony networks and (Titov and Henderson, 2006), who use an SVM to adapt a generative parser to different domains. Costa et al. (2003) apply recursive neural networks to re-rank possible phrase attachments in an incremental parser. Their work is the first to show that RNNs can capture enough information to make correct parsing decisions, but they only test on a subset of 2000 sentences. Menchetti et al. (2005) use RNNs to re-rank different parses. For their results on full sentence parsing, they rerank candidate trees created by the Collins parser (Collins, 2003). Similar to their work, we use the idea of letting discrete categories reduce"
P13-1045,P07-1080,0,0.592471,"e models that are trained using continuous vectors. We will first briefly introduce single word vector representations and then describe the CVG objective function, tree scoring and inference. Henderson (2003) was the first to show that neural networks can be successfully used for large scale parsing. He introduced a left-corner parser to estimate the probabilities of parsing decisions conditioned on the parsing history. The input to Henderson’s model consists of pairs of frequent words and their part-of-speech (POS) tags. Both the original parsing system and its probabilistic interpretation (Titov and Henderson, 2007) learn features that represent the parsing history and do not provide a principled linguistic representation like our phrase representations. Other related work includes (Henderson, 2004), who discriminatively trains a parser based on synchrony networks and (Titov and Henderson, 2006), who use an SVM to adapt a generative parser to different domains. Costa et al. (2003) apply recursive neural networks to re-rank possible phrase attachments in an incremental parser. Their work is the first to show that RNNs can capture enough information to make correct parsing decisions, but they only test on"
P13-1045,P10-1040,0,0.436676,"such as king−man+woman ≈ queen (Mikolov et al., 2013). Collobert and Weston (2008) introduced a new model to compute such an embedding. The idea is to construct a neural network that outputs high scores for windows that occur in a large unlabeled corpus and low scores for windows where one word is replaced by a random word. When such a network is optimized via gradient ascent the derivatives backpropagate into the word embedding matrix X. In order to predict correct scores the vectors in the matrix capture co-occurrence statistics. For further details and evaluations of these embeddings, see (Turian et al., 2010; Huang et al., 2012). The resulting X matrix is used as follows. Assume we are given a sentence as an ordered list of m words. Each word w has an index [w] = i into the columns of the embedding matrix. This index is used to retrieve the word’s vector representation aw using a simple multiplication with a binary vector e, which is zero everywhere, except Compositional Vector Grammars This section introduces Compositional Vector Grammars (CVGs), a model to jointly find syntactic structure and capture compositional semantic information. CVGs build on two observations. Firstly, that a lot of the"
P13-1045,N13-1090,0,\N,Missing
P15-1150,S14-2114,0,0.0300663,"Missing"
P15-1150,P14-1062,0,0.88283,"Missing"
P15-1150,D14-1082,1,0.187036,"4927 train/dev/test split. The sentences are derived from existing image and video description datasets. Each sentence pair is annotated with a relatedness score y ∈ [1, 5], with 1 indicating that the two sentences are completely unrelated, and 5 indicating that the two sentences are very related. Each label is the average of 10 ratings assigned by different human annotators. Here, we use the similarity model described in Sec. 4.2. For the similarity prediction network (Eqs. 15) we use a hidden layer of size 50. We 3 Dependency parses produced by the Stanford Neural Network Dependency Parser (Chen and Manning, 2014). Fine-grained Binary 43.2 44.4 45.7 48.5 48.7 48.0 47.4 49.8 82.4 82.9 85.4 86.8 87.8 87.2 88.1 86.6 LSTM Bidirectional LSTM 2-layer LSTM 2-layer Bidirectional LSTM 46.4 49.1 46.0 48.5 (1.1) (1.0) (1.3) (1.0) 84.9 87.5 86.3 87.2 (0.6) (0.5) (0.6) (1.0) Dependency Tree-LSTM Constituency Tree-LSTM – randomly initialized vectors – Glove vectors, fixed – Glove vectors, tuned 48.4 (0.4) 85.7 (0.4) 43.9 (0.6) 49.7 (0.4) 51.0 (0.5) 82.0 (0.5) 87.5 (0.8) 88.0 (0.3) Table 2: Test set accuracies on the Stanford Sentiment Treebank. For our experiments, we report mean accuracies over 5 runs (standard dev"
P15-1150,N13-1092,0,0.0261622,"Missing"
P15-1150,W13-0112,0,0.0158871,"ions and plays with the case two men are dancing and singing in front of a crowd 3.37 3.19 4.08 4.01 4.00 Table 4: Most similar sentences from a 1000-sentence sample drawn from the SICK test set. The TreeLSTM model is able to pick up on more subtle relationships, such as that between “beach” and “ocean” in the second example. Pennington et al., 2014) have found wide applicability in a variety of NLP tasks. Following this success, there has been substantial interest in the area of learning distributed phrase and sentence representations (Mitchell and Lapata, 2010; Yessenalina and Cardie, 2011; Grefenstette et al., 2013; Mikolov et al., 2013), as well as distributed representations of longer bodies of text such as paragraphs and documents (Srivastava et al., 2013; Le and Mikolov, 2014). Our approach builds on recursive neural networks (Goller and Kuchler, 1996; Socher et al., 2011), which we abbreviate as Tree-RNNs in order to avoid confusion with recurrent neural networks. Under the Tree-RNN framework, the vector representation associated with each node of a tree is composed as a function of the vectors corresponding to the children of the node. The choice of composition function gives rise to numerous vari"
P15-1150,P12-1092,1,0.0457691,"e over 5 runs, and error bars have been omitted for clarity. We observe that while the Dependency TreeLSTM does significantly outperform its sequential counterparts on the relatedness task for longer sentences of length 13 to 15 (Fig. 4), it also achieves consistently strong performance on shorter sentences. This suggests that unlike sequential LSTMs, Tree-LSTMs are able to encode semantically-useful structural information in the sentence representations that they compose. 8 Related Work Distributed representations of words (Rumelhart et al., 1988; Collobert et al., 2011; Turian et al., 2010; Huang et al., 2012; Mikolov et al., 2013; 1563 Ranking by mean word vector cosine similarity a woman is slicing potatoes a woman is cutting potatoes a woman is slicing herbs a woman is slicing tofu a boy is waving at some young runners from the ocean a man and a boy are standing at the bottom of some stairs , which are outdoors a group of children in uniforms is standing at a gate and one is kissing the mother a group of children in uniforms is standing at a gate and there is no one kissing the mother Score Ranking by Dependency Tree-LSTM model Score 0.96 0.92 0.92 a woman is slicing potatoes a woman is cutting"
P15-1150,S14-2131,0,0.0718846,"Missing"
P15-1150,D14-1181,0,0.164747,"Missing"
P15-1150,P03-1054,1,0.0821096,"Missing"
P15-1150,S14-2055,0,0.196572,"p.stanford.edu/projects/glove/. 1561 Method Pearson’s r Spearman’s ρ MSE 0.7993 0.8070 0.8268 0.8414 0.7538 0.7489 0.7721 – 0.3692 0.3550 0.3224 – Mean vectors DT-RNN (Socher et al., 2014) SDT-RNN (Socher et al., 2014) 0.7577 (0.0013) 0.7923 (0.0070) 0.7900 (0.0042) 0.6738 (0.0027) 0.7319 (0.0071) 0.7304 (0.0076) 0.4557 (0.0090) 0.3822 (0.0137) 0.3848 (0.0074) LSTM Bidirectional LSTM 2-layer LSTM 2-layer Bidirectional LSTM 0.8528 0.8567 0.8515 0.8558 0.7911 0.7966 0.7896 0.7965 0.2831 0.2736 0.2838 0.2762 Constituency Tree-LSTM Dependency Tree-LSTM 0.8582 (0.0038) 0.8676 (0.0030) Illinois-LH (Lai and Hockenmaier, 2014) UNAL-NLP (Jimenez et al., 2014) Meaning Factory (Bjerva et al., 2014) ECNU (Zhao et al., 2014) (0.0031) (0.0028) (0.0066) (0.0014) (0.0059) (0.0053) (0.0088) (0.0018) 0.7966 (0.0053) 0.8083 (0.0042) (0.0092) (0.0063) (0.0150) (0.0020) 0.2734 (0.0108) 0.2532 (0.0052) Table 3: Test set results on the SICK semantic relatedness subtask. For our experiments, we report mean scores over 5 runs (standard deviations in parentheses). Results are grouped as follows: (1) SemEval 2014 submissions; (2) Our own baselines; (3) Sequential LSTMs; (4) Tree-structured LSTMs. 6 6.1 Results Sentiment Classificatio"
P15-1150,S14-2001,0,0.108355,". 4.1 with both Dependency Tree-LSTMs (Sec. 3.1) and Constituency Tree-LSTMs (Sec. 3.2). The Constituency Tree-LSTMs are structured according to the provided parse trees. For the Dependency Tree-LSTMs, we produce dependency parses3 of each sentence; each node in a tree is given a sentiment label if its span matches a labeled span in the training set. 5.2 Semantic Relatedness For a given pair of sentences, the semantic relatedness task is to predict a human-generated rating of the similarity of the two sentences in meaning. We use the Sentences Involving Compositional Knowledge (SICK) dataset (Marelli et al., 2014), consisting of 9927 sentence pairs in a 4500/500/4927 train/dev/test split. The sentences are derived from existing image and video description datasets. Each sentence pair is annotated with a relatedness score y ∈ [1, 5], with 1 indicating that the two sentences are completely unrelated, and 5 indicating that the two sentences are very related. Each label is the average of 10 ratings assigned by different human annotators. Here, we use the similarity model described in Sec. 4.2. For the similarity prediction network (Eqs. 15) we use a hidden layer of size 50. We 3 Dependency parses produced"
P15-1150,D14-1162,1,0.136302,"ford Sentiment Treebank. For our experiments, we report mean accuracies over 5 runs (standard deviations in parentheses). Fine-grained: 5-class sentiment classification. Binary: positive/negative sentiment classification. produce binarized constituency parses4 and dependency parses of the sentences in the dataset for our Constituency Tree-LSTM and Dependency TreeLSTM models. 5.3 Hyperparameters and Training Details The hyperparameters for our models were tuned on the development set for each task. We initialized our word representations using publicly available 300-dimensional Glove vectors5 (Pennington et al., 2014). For the sentiment classification task, word representations were updated during training with a learning rate of 0.1. For the semantic relatedness task, word representations were held fixed as we did not observe any significant improvement when the representations were tuned. Our models were trained using AdaGrad (Duchi et al., 2011) with a learning rate of 0.05 and a minibatch size of 25. The model parameters were regularized with a per-minibatch L2 regularization strength of 10−4 . The sentiment classifier was additionally regularized using dropout (Srivastava et al., 2014) with a dropout"
P15-1150,D12-1110,1,0.330741,"v, 2014). Our approach builds on recursive neural networks (Goller and Kuchler, 1996; Socher et al., 2011), which we abbreviate as Tree-RNNs in order to avoid confusion with recurrent neural networks. Under the Tree-RNN framework, the vector representation associated with each node of a tree is composed as a function of the vectors corresponding to the children of the node. The choice of composition function gives rise to numerous variants of this basic framework. TreeRNNs have been used to parse images of natural scenes (Socher et al., 2011), compose phrase representations from word vectors (Socher et al., 2012), and classify the sentiment polarity of sentences (Socher et al., 2013). 9 Conclusion In this paper, we introduced a generalization of LSTMs to tree-structured network topologies. The Tree-LSTM architecture can be applied to trees with arbitrary branching factor. We demonstrated the effectiveness of the Tree-LSTM by applying the architecture in two tasks: semantic relatedness and sentiment classification, outperforming existing systems on both. Controlling for model dimensionality, we demonstrated that Tree-LSTM models are able to outperform their sequential counterparts. Our results suggest"
P15-1150,Q14-1017,1,0.421964,"were regularized with a per-minibatch L2 regularization strength of 10−4 . The sentiment classifier was additionally regularized using dropout (Srivastava et al., 2014) with a dropout rate of 0.5. We did not observe performance gains using dropout on the semantic relatedness task. 4 Constituency parses produced by the Stanford PCFG Parser (Klein and Manning, 2003). 5 Trained on 840 billion tokens of Common Crawl data, http://nlp.stanford.edu/projects/glove/. 1561 Method Pearson’s r Spearman’s ρ MSE 0.7993 0.8070 0.8268 0.8414 0.7538 0.7489 0.7721 – 0.3692 0.3550 0.3224 – Mean vectors DT-RNN (Socher et al., 2014) SDT-RNN (Socher et al., 2014) 0.7577 (0.0013) 0.7923 (0.0070) 0.7900 (0.0042) 0.6738 (0.0027) 0.7319 (0.0071) 0.7304 (0.0076) 0.4557 (0.0090) 0.3822 (0.0137) 0.3848 (0.0074) LSTM Bidirectional LSTM 2-layer LSTM 2-layer Bidirectional LSTM 0.8528 0.8567 0.8515 0.8558 0.7911 0.7966 0.7896 0.7965 0.2831 0.2736 0.2838 0.2762 Constituency Tree-LSTM Dependency Tree-LSTM 0.8582 (0.0038) 0.8676 (0.0030) Illinois-LH (Lai and Hockenmaier, 2014) UNAL-NLP (Jimenez et al., 2014) Meaning Factory (Bjerva et al., 2014) ECNU (Zhao et al., 2014) (0.0031) (0.0028) (0.0066) (0.0014) (0.0059) (0.0053) (0.0088) (0."
P15-1150,D13-1170,1,0.456665,"+ b(h) ,   pˆθ = softmax W (p) hs + b(p) , yˆ = rT pˆθ , rT k=1 Experiments We evaluate our Tree-LSTM architectures on two tasks: (1) sentiment classification of sentences sampled from movie reviews and (2) predicting the semantic relatedness of sentence pairs. In comparing our Tree-LSTMs against sequential LSTMs, we control for the number of LSTM parameters by varying the dimensionality of the hidden states2 . Details for each model variant are summarized in Table 1. 5.1 In this task, we predict the sentiment of sentences sampled from movie reviews. We use the Stanford Sentiment Treebank (Socher et al., 2013). There are two subtasks: binary classification of sentences, and fine-grained classification over five classes: very negative, negative, neutral, positive, and very positive. We use the standard train/dev/test splits of 6920/872/1821 for the binary classification subtask and 8544/1101/2210 for the fine-grained classification subtask (there are fewer examples for the binary subtask since 1 where = [1 2 . . . K] and the absolute value function is applied elementwise. The use of both distance measures h× and h+ is empirically motivated: we find that the combination outperforms the use of either"
P15-1150,P10-1040,0,0.0214585,"point is a mean score over 5 runs, and error bars have been omitted for clarity. We observe that while the Dependency TreeLSTM does significantly outperform its sequential counterparts on the relatedness task for longer sentences of length 13 to 15 (Fig. 4), it also achieves consistently strong performance on shorter sentences. This suggests that unlike sequential LSTMs, Tree-LSTMs are able to encode semantically-useful structural information in the sentence representations that they compose. 8 Related Work Distributed representations of words (Rumelhart et al., 1988; Collobert et al., 2011; Turian et al., 2010; Huang et al., 2012; Mikolov et al., 2013; 1563 Ranking by mean word vector cosine similarity a woman is slicing potatoes a woman is cutting potatoes a woman is slicing herbs a woman is slicing tofu a boy is waving at some young runners from the ocean a man and a boy are standing at the bottom of some stairs , which are outdoors a group of children in uniforms is standing at a gate and one is kissing the mother a group of children in uniforms is standing at a gate and there is no one kissing the mother Score Ranking by Dependency Tree-LSTM model Score 0.96 0.92 0.92 a woman is slicing potatoe"
P15-1150,D11-1016,0,0.0144923,"s opening the guitar for donations and plays with the case two men are dancing and singing in front of a crowd 3.37 3.19 4.08 4.01 4.00 Table 4: Most similar sentences from a 1000-sentence sample drawn from the SICK test set. The TreeLSTM model is able to pick up on more subtle relationships, such as that between “beach” and “ocean” in the second example. Pennington et al., 2014) have found wide applicability in a variety of NLP tasks. Following this success, there has been substantial interest in the area of learning distributed phrase and sentence representations (Mitchell and Lapata, 2010; Yessenalina and Cardie, 2011; Grefenstette et al., 2013; Mikolov et al., 2013), as well as distributed representations of longer bodies of text such as paragraphs and documents (Srivastava et al., 2013; Le and Mikolov, 2014). Our approach builds on recursive neural networks (Goller and Kuchler, 1996; Socher et al., 2011), which we abbreviate as Tree-RNNs in order to avoid confusion with recurrent neural networks. Under the Tree-RNN framework, the vector representation associated with each node of a tree is composed as a function of the vectors corresponding to the children of the node. The choice of composition function"
P15-1150,S14-2044,0,0.132395,"that during training, components of the gradient vector can grow or decay exponentially over long sequences (Hochreiter, 1998; Bengio et al., 1994). This problem with exploding or vanishing gradients makes it difficult for the RNN model to learn long-distance correlations in a sequence. The LSTM architecture (Hochreiter and Schmidhuber, 1997) addresses this problem of learning long-term dependencies by introducing a memory cell that is able to preserve state over long periods of time. While numerous LSTM variants have been described, here we describe the version used by Zaremba and Sutskever (2014). We define the LSTM unit at each time step t to be a collection of vectors in Rd : an input gate it , a forget gate ft , an output gate ot , a memory cell ct and a hidden state ht . The entries of the gating vectors it , ft and ot are in [0, 1]. We refer to d as the memory dimension of the LSTM. The LSTM transition equations are the following:   (1) it = σ W (i) xt + U (i) ht−1 + b(i) ,   ft = σ W (f ) xt + U (f ) ht−1 + b(f ) ,   ot = σ W (o) xt + U (o) ht−1 + b(o) ,   ut = tanh W (u) xt + U (u) ht−1 + b(u) , ct = it ut + ft ct−1 , Long Short-Term Memory Networks ht = ot tanh(ct ), O"
P18-1135,D16-1053,0,0.127889,"s in the sequence, demb the dimension of the embeddings, and X ∈ Rn×demb the word embeddings corresponding to words in the sequence. We produce a global encoding H g of X using a global bidirectional LSTM. H g = biLSTMg (X) ∈ Rn×drnn (2) H = β s H s + (1 − β s ) H g ∈ Rn×drnn (3) Here, the scalar β s is a learned parameter between 0 and 1 that is specific to the slot s. Next, we compute a global-local self-attention context c over H. Self-attention, or intra-attention, is a very effective method of computing summary context over variable-length sequences for natural language processing tasks (Cheng et al., 2016; Vaswani et al., 2017; He et al., 2017; Lee et al., 2017). In our case, we use a global self-attention module to compute attention context useful for general-purpose state tracking, as well as a local self-attention module to compute slot-specific attention context. For each ith element Hi , we compute a scalar global self-attention score agi which is subsequently normalized across all elements using a softmax function. (1) where drnn is the dimension of the LSTM state. We similarly produce a local encoding H s of X, 1460 agi = W g Hi + bg ∈ R p g = softmax (a ) ∈ R g (4) n (5) The global sel"
P18-1135,N13-1092,0,0.0163688,"Missing"
P18-1135,D17-1206,1,0.897826,", we simply accumulate turn goals. In the event that the current turn goal specifies a slot that has been specified before, the new specification takes precedence. For example, suppose the user specifies a food=french restaurant during the current turn. If the joint goal has no existing food specifications, then we simply add food=french to the joint goal. Alternatively, if food=thai had been specified in a previous turn, we simply replace it with food=french. 3.3 Implementation Details We use fixed, pretrained GloVe embeddings (Pennington et al., 2014) as well as character n-gram embeddings (Hashimoto et al., 2017). Each model is trained using ADAM (Kingma and Ba, 2015). For regularization, we apply dropout with 0.2 drop rate (Srivastava et al., 2014) to the output of each local module and each global module. We use the development split for hyperparameter tuning and apply early stopping using the joint goal accuracy. For the DSTC2 task, we train using transcripts of user utterances and evaluate using the noisy ASR transcriptions. During evaluation, we take the sum of the scores resulting from each ASR output as the output score of a particular slot-value. We then normalize this sum using a sigmoid func"
P18-1135,P17-1044,0,0.137171,"he embeddings, and X ∈ Rn×demb the word embeddings corresponding to words in the sequence. We produce a global encoding H g of X using a global bidirectional LSTM. H g = biLSTMg (X) ∈ Rn×drnn (2) H = β s H s + (1 − β s ) H g ∈ Rn×drnn (3) Here, the scalar β s is a learned parameter between 0 and 1 that is specific to the slot s. Next, we compute a global-local self-attention context c over H. Self-attention, or intra-attention, is a very effective method of computing summary context over variable-length sequences for natural language processing tasks (Cheng et al., 2016; Vaswani et al., 2017; He et al., 2017; Lee et al., 2017). In our case, we use a global self-attention module to compute attention context useful for general-purpose state tracking, as well as a local self-attention module to compute slot-specific attention context. For each ith element Hi , we compute a scalar global self-attention score agi which is subsequently normalized across all elements using a softmax function. (1) where drnn is the dimension of the LSTM state. We similarly produce a local encoding H s of X, 1460 agi = W g Hi + bg ∈ R p g = softmax (a ) ∈ R g (4) n (5) The global self-attention context cg is then the sum"
P18-1135,W14-4337,0,0.776336,"est. The joint goal is the set of accumulated turn goals up to the current turn. Figure 1 shows an example dialogue with annotated turn states, in which the user reserves a restaurant. Traditional dialogue state trackers rely on Spoken Language Understanding (SLU) systems (Henderson et al., 2012) in order to understand user utterances. These trackers accumulate errors from the SLU, which sometimes do not have the necessary dialogue context to interpret the user utterances. Subsequent DST research forgo the SLU and directly infer the state using the conversation history and the user utterance (Henderson et al., 2014b; Zilka and Jurcicek, 2015; Mrkˇsi´c et al., 2015). These trackers rely on handcrafted semantic dictionaries and delexicalization — the anonymization of slots and values using generic tags — to achieve generalization. Recent work by Mrkˇsi´c et al. (2017) apply representation learning using convolutional neural networks to learn features relevant for each state as opposed to hand-crafting features. A key problem in DST that is not addressed by existing methods is the extraction of rare slotvalue pairs that compose the state during each turn. Because task oriented dialogues cover large 1458 Pr"
P18-1135,W14-4340,0,0.538945,"est. The joint goal is the set of accumulated turn goals up to the current turn. Figure 1 shows an example dialogue with annotated turn states, in which the user reserves a restaurant. Traditional dialogue state trackers rely on Spoken Language Understanding (SLU) systems (Henderson et al., 2012) in order to understand user utterances. These trackers accumulate errors from the SLU, which sometimes do not have the necessary dialogue context to interpret the user utterances. Subsequent DST research forgo the SLU and directly infer the state using the conversation history and the user utterance (Henderson et al., 2014b; Zilka and Jurcicek, 2015; Mrkˇsi´c et al., 2015). These trackers rely on handcrafted semantic dictionaries and delexicalization — the anonymization of slots and values using generic tags — to achieve generalization. Recent work by Mrkˇsi´c et al. (2017) apply representation learning using convolutional neural networks to learn features relevant for each state as opposed to hand-crafting features. A key problem in DST that is not addressed by existing methods is the extraction of rare slotvalue pairs that compose the state during each turn. Because task oriented dialogues cover large 1458 Pr"
P18-1135,D17-1018,0,0.103158,"d X ∈ Rn×demb the word embeddings corresponding to words in the sequence. We produce a global encoding H g of X using a global bidirectional LSTM. H g = biLSTMg (X) ∈ Rn×drnn (2) H = β s H s + (1 − β s ) H g ∈ Rn×drnn (3) Here, the scalar β s is a learned parameter between 0 and 1 that is specific to the slot s. Next, we compute a global-local self-attention context c over H. Self-attention, or intra-attention, is a very effective method of computing summary context over variable-length sequences for natural language processing tasks (Cheng et al., 2016; Vaswani et al., 2017; He et al., 2017; Lee et al., 2017). In our case, we use a global self-attention module to compute attention context useful for general-purpose state tracking, as well as a local self-attention module to compute slot-specific attention context. For each ith element Hi , we compute a scalar global self-attention score agi which is subsequently normalized across all elements using a softmax function. (1) where drnn is the dimension of the LSTM state. We similarly produce a local encoding H s of X, 1460 agi = W g Hi + bg ∈ R p g = softmax (a ) ∈ R g (4) n (5) The global self-attention context cg is then the sum of each element Hi"
P18-1135,D15-1166,0,0.0256073,"ships by injecting semantic similarity constraints from the Paraphrase Database (Wieting et al., 2015; Ganitkevitch et al., 2013). On the one hand, these specialized embeddings are more difficult to obtain than word embeddings from language modeling. On the other hand, these embeddings are not specific to any dialogue domain and are directly usable for new domains. Neural attention models in NLP. Attention mechanisms have led to improvements on a variety of natural language processing tasks. Bahdanau et al. (2015) propose attentional sequence to sequence models for neural machine translation. Luong et al. (2015) analyze various attention techniques and highlight the effectiveness of the simple, parameterless dot product attention. Similar models have also proven successful in tasks such as summarization (See et al., 2017; Paulus et al., 2018). Self-attention, or intra-attention, has led improvements in language modeling, sentiment 1464 System actions in previous turn User utterance Predicted turn belief state N/A I would like Polynesian food in the South part of town. Please send me phone number and address. request(phone) request(address) inform(food=polynesian) inform(area=south) request(address) r"
P18-1135,P15-2130,0,0.226293,"Missing"
P18-1135,P17-1163,0,0.753228,"Missing"
P18-1135,D14-1162,1,0.112142,"how to obtain turn goals and requests. To compute the joint goal, we simply accumulate turn goals. In the event that the current turn goal specifies a slot that has been specified before, the new specification takes precedence. For example, suppose the user specifies a food=french restaurant during the current turn. If the joint goal has no existing food specifications, then we simply add food=french to the joint goal. Alternatively, if food=thai had been specified in a previous turn, we simply replace it with food=french. 3.3 Implementation Details We use fixed, pretrained GloVe embeddings (Pennington et al., 2014) as well as character n-gram embeddings (Hashimoto et al., 2017). Each model is trained using ADAM (Kingma and Ba, 2015). For regularization, we apply dropout with 0.2 drop rate (Srivastava et al., 2014) to the output of each local module and each global module. We use the development split for hyperparameter tuning and apply early stopping using the joint goal accuracy. For the DSTC2 task, we train using transcripts of user utterances and evaluate using the noisy ASR transcriptions. During evaluation, we take the sum of the scores resulting from each ASR output as the output score of a partic"
P18-1135,E17-1029,0,0.0509186,"er does not answer the system’s previous request for the choice of food and instead asks for what food is available. The model misinterprets the lack of response as the user not caring about the choice of food. 4 Related Work Dialogue State Tracking. Traditional dialogue state trackers rely on a separate SLU component that serves as the initial stage in the dialogue agent pipeline. The downstream tracker then combines the semantics extracted by the SLU with previous dialogue context in order to estimate the current dialogue state (Thomson and Young, 2010; Wang and Lemon, 2013; Williams, 2014; Perez and Liu, 2017). Recent results in dialogue state tracking show that it is beneficial to jointly learn speech understanding and dialogue tracking (Henderson et al., 2014b; Zilka and Jurcicek, 2015; Wen et al., 2017). These approaches directly take as input the N-best list produced by the ASR system. By avoiding the accumulation of errors from the initial SLU component, these joint approaches typically achieved stronger performance on tasks such as DSTC2. One drawback to these approaches is that they rely on hand-crafted features and complex domain-specific lexicon (in addition to the ontology), and consequen"
P18-1135,P17-1099,0,0.029697,"embeddings from language modeling. On the other hand, these embeddings are not specific to any dialogue domain and are directly usable for new domains. Neural attention models in NLP. Attention mechanisms have led to improvements on a variety of natural language processing tasks. Bahdanau et al. (2015) propose attentional sequence to sequence models for neural machine translation. Luong et al. (2015) analyze various attention techniques and highlight the effectiveness of the simple, parameterless dot product attention. Similar models have also proven successful in tasks such as summarization (See et al., 2017; Paulus et al., 2018). Self-attention, or intra-attention, has led improvements in language modeling, sentiment 1464 System actions in previous turn User utterance Predicted turn belief state N/A I would like Polynesian food in the South part of town. Please send me phone number and address. request(phone) request(address) inform(food=polynesian) inform(area=south) request(address) request(phone) Yes please. request(phone) request(address) I just want to eat at a cheap restaurant in the south part of town. What food types are available, can you also provide some phone numbers? request(phone)"
P18-1135,W13-4067,0,0.346022,"ing instances. the model. Here, the user does not answer the system’s previous request for the choice of food and instead asks for what food is available. The model misinterprets the lack of response as the user not caring about the choice of food. 4 Related Work Dialogue State Tracking. Traditional dialogue state trackers rely on a separate SLU component that serves as the initial stage in the dialogue agent pipeline. The downstream tracker then combines the semantics extracted by the SLU with previous dialogue context in order to estimate the current dialogue state (Thomson and Young, 2010; Wang and Lemon, 2013; Williams, 2014; Perez and Liu, 2017). Recent results in dialogue state tracking show that it is beneficial to jointly learn speech understanding and dialogue tracking (Henderson et al., 2014b; Zilka and Jurcicek, 2015; Wen et al., 2017). These approaches directly take as input the N-best list produced by the ASR system. By avoiding the accumulation of errors from the initial SLU component, these joint approaches typically achieved stronger performance on tasks such as DSTC2. One drawback to these approaches is that they rely on hand-crafted features and complex domain-specific lexicon (in ad"
P18-1135,E17-1042,0,0.236563,"Missing"
P18-1135,Q15-1025,0,0.0289056,"eatures and complex domain-specific lexicon (in addition to the ontology), and consequently are difficult to extend and scale to new domains. The recent Neural Belief Tracker (NBT) by Mrkˇsi´c et al. (2017) avoids reliance on hand-crafted features and lexicon by using representation learning. The NBT employs convolutional filters over word embeddings in lieu of previously-used hand-engineered features. Moreover, to outperform previous methods, the NBT uses pretrained embeddings tailored to retain semantic relationships by injecting semantic similarity constraints from the Paraphrase Database (Wieting et al., 2015; Ganitkevitch et al., 2013). On the one hand, these specialized embeddings are more difficult to obtain than word embeddings from language modeling. On the other hand, these embeddings are not specific to any dialogue domain and are directly usable for new domains. Neural attention models in NLP. Attention mechanisms have led to improvements on a variety of natural language processing tasks. Bahdanau et al. (2015) propose attentional sequence to sequence models for neural machine translation. Luong et al. (2015) analyze various attention techniques and highlight the effectiveness of the simpl"
P18-1135,W14-4339,0,0.19631,"el. Here, the user does not answer the system’s previous request for the choice of food and instead asks for what food is available. The model misinterprets the lack of response as the user not caring about the choice of food. 4 Related Work Dialogue State Tracking. Traditional dialogue state trackers rely on a separate SLU component that serves as the initial stage in the dialogue agent pipeline. The downstream tracker then combines the semantics extracted by the SLU with previous dialogue context in order to estimate the current dialogue state (Thomson and Young, 2010; Wang and Lemon, 2013; Williams, 2014; Perez and Liu, 2017). Recent results in dialogue state tracking show that it is beneficial to jointly learn speech understanding and dialogue tracking (Henderson et al., 2014b; Zilka and Jurcicek, 2015; Wen et al., 2017). These approaches directly take as input the N-best list produced by the ASR system. By avoiding the accumulation of errors from the initial SLU component, these joint approaches typically achieved stronger performance on tasks such as DSTC2. One drawback to these approaches is that they rely on hand-crafted features and complex domain-specific lexicon (in addition to the on"
P18-1135,W13-4065,0,0.183891,"tions, we introduce a sentinel action to each turn which allows the attention mechanism to ignore previous system actions. The score y act indicates the degree to which the value was expressed by the previous actions. 1461 The final score y is then a weighted sum between the two scores y utt and y act , normalized by the sigmoid function σ. ( ) y = σ y utt + wy act ∈ R (23) Here, the weight w is a learned parameter. 3 Experiments 3.1 Dataset The Dialogue Systems Technology Challenges (DSTC) provides a common framework for developing and evaluating dialogue systems and dialogue state trackers (Williams et al., 2013; Henderson et al., 2014a). Under this framework, dialogue semantics such as states and actions are based on a task ontology such as restaurant reservation. During each turn, the user may inform the system of particular goals (e.g. inform(food=french)), or request for more information from the system (e.g. request(address)). For instance, food and area are examples of slots in the DSTC2 task, and french and chinese are example values within the food slot. We train and evaluate our model using DSTC2 as well as the Wizard of Oz (WoZ) restaurant reservation task (Wen et al., 2017), which also adh"
P18-1160,D13-1160,0,0.0870238,"Koˇcisk`y et al., 2017), and textbooks (Lai et al., 2017; Xie et al., 2017). Many neural QA models have successfully addressed these tasks by leveraging coattention or bidirectional attention mechanisms (Xiong et al., 2018; Seo et al., 2017) to model the codependent context over the document and the question. However, Jia and Liang (2017) find that many QA models are sensitive to adversarial inputs. Recently, researchers have developed largescale QA datasets, which requires answering the question over a large set of documents in a closed (Joshi et al., 2017) or open-domain (Dunn et al., 2017; Berant et al., 2013; Chen et al., 2017; Dhingra et al., 2017). Many models for these datasets either retrieve documents/paragraphs relevant to the question (Chen et al., 2017; Clark and Gardner, 2017; Wang et al., 2018), or leverage simple non-recurrent architectures to make training and inference tractable over large corpora (Swayamdipta et al., 2018; Yu et al., 2018). Sentence selection The task of selecting sentences that can answer to the question has been studied across several QA datasets (Yang et al., 2015), by modeling relevance between a sentence and the question (Yin et al., 2016; Miller et al., 2016;"
P18-1160,P17-1171,0,0.314814,"gram of the questions answered correctly (on exact match (EM)) by the model given a full document (F ULL) and the model given an oracle sentence (O RACLE) on SQuAD (left) and NewsQA (right). of-the-art QA models, achieving 83.1 F1 on the SQuAD development set. It features a deep residual coattention encoder, a dynamic pointing decoder, and a mixed objective that combines cross entropy loss with self-critical policy learning. SReader is another competitive QA model that is simpler and faster than DCN+, with 79.9 F1 on the SQuAD development set. It is a simplified version of the reader in DrQA (Chen et al., 2017), which obtains 78.8 F1 on the SQuAD development set. Model details and training procedures are shown in Appendix A. 3.2 Sentence Selector Our sentence selector scores each sentence with respect to the question in parallel. The score indicates whether the question is answerable with this sentence. The model architecture is divided into the encoder module and the decoder module. The encoder is a shared module with S-Reader, which computes sentence encodings and question encodings from the sentence and the question as inputs. First, the encoder computes sentence embeddings D ∈ Rhd ×Ld , question"
P18-1160,P17-1020,0,0.107273,"/paragraphs relevant to the question (Chen et al., 2017; Clark and Gardner, 2017; Wang et al., 2018), or leverage simple non-recurrent architectures to make training and inference tractable over large corpora (Swayamdipta et al., 2018; Yu et al., 2018). Sentence selection The task of selecting sentences that can answer to the question has been studied across several QA datasets (Yang et al., 2015), by modeling relevance between a sentence and the question (Yin et al., 2016; Miller et al., 2016; Min et al., 2017). Several recent works also study joint sentence selection and question answering. Choi et al. (2017) propose a framework that identifies the sentences relevant to the question (property) using simple bag-ofwords representation, then generates the answer from those sentences using recurrent neural networks. Raiman and Miller (2017) cast the task of extractive question answering as a search problem by iteratively selecting the sentences, start position and end position. They are different from our work in that (i) we study of the minimal context required to answer the question, (ii) we choose the minimal context by selecting variable number of sentences for each question, while they use a fixe"
P18-1160,D17-1206,1,0.801512,"Missing"
P18-1160,D17-1215,0,0.270305,"at also provides a paragraph for each question, but the paragraphs are longer than those in SQuAD. TriviaQA (Joshi et al., 2017) is a dataset on a large set of documents from the Wikipedia domain and Web domain. Here, we only use the Wikipedia domain. Each question is given a much longer context in the form of multiple documents. SQuAD-Open (Chen et al., 2017) is an opendomain question answering dataset based on SQuAD. In SQuAD-Open, only the question and the answer are given. The model is responsible for identifying the relevant context from all English Wikipedia articles. SQuAD-Adversarial (Jia and Liang, 2017) is a variant of SQuAD. It shares the same training set as SQuAD, but an adversarial sentence is added to each paragraph in a subset of the development set. We use accuracy (Acc) and mean average precision (MAP) to evaluate sentence selection. We also measure the average number of selected sentences (N sent) to compare the efficiency of our Dyn method and the Top k method. To evaluate the performance in the task of question answering, we measure F1 and EM (Exact Match), both being standard metrics for evaluating span-based QA. In addition, we measure training speed (Train Sp) and inference spe"
P18-1160,P17-1147,0,0.536648,"y comparable to or better than the state-of-the-art on SQuAD, NewsQA, TriviaQA and SQuAD-Open. Furthermore, our experimental results and analyses show that our approach is more robust to adversarial inputs. 1 Introduction The task of textual question answering (QA), in which a machine reads a document and answers a question, is an important and challenging problem in natural language processing. Recent progress in performance of QA models has been largely due to the variety of available QA datasets (Richardson et al., 2013; Hermann et al., 2015; Rajpurkar et al., 2016; Trischler et al., 2016; Joshi et al., 2017; Koˇcisk`y et al., 2017). ∗ All work was done while the author was an intern at Salesforce Research. Many neural QA models have been proposed for these datasets, the most successful of which tend to leverage coattention or bidirectional attention mechanisms that build codependent representations of the document and the question (Xiong et al., 2018; Seo et al., 2017). Yet, learning the full context over the document is challenging and inefficient. In particular, when the model is given a long document, or multiple documents, learning the full context is intractably slow and hence difficult to"
P18-1160,P14-5010,0,0.00471157,"Missing"
P18-1160,D16-1147,0,0.0329925,"; Berant et al., 2013; Chen et al., 2017; Dhingra et al., 2017). Many models for these datasets either retrieve documents/paragraphs relevant to the question (Chen et al., 2017; Clark and Gardner, 2017; Wang et al., 2018), or leverage simple non-recurrent architectures to make training and inference tractable over large corpora (Swayamdipta et al., 2018; Yu et al., 2018). Sentence selection The task of selecting sentences that can answer to the question has been studied across several QA datasets (Yang et al., 2015), by modeling relevance between a sentence and the question (Yin et al., 2016; Miller et al., 2016; Min et al., 2017). Several recent works also study joint sentence selection and question answering. Choi et al. (2017) propose a framework that identifies the sentences relevant to the question (property) using simple bag-ofwords representation, then generates the answer from those sentences using recurrent neural networks. Raiman and Miller (2017) cast the task of extractive question answering as a search problem by iteratively selecting the sentences, start position and end position. They are different from our work in that (i) we study of the minimal context required to answer the questio"
P18-1160,P17-2081,1,0.851718,"; Chen et al., 2017; Dhingra et al., 2017). Many models for these datasets either retrieve documents/paragraphs relevant to the question (Chen et al., 2017; Clark and Gardner, 2017; Wang et al., 2018), or leverage simple non-recurrent architectures to make training and inference tractable over large corpora (Swayamdipta et al., 2018; Yu et al., 2018). Sentence selection The task of selecting sentences that can answer to the question has been studied across several QA datasets (Yang et al., 2015), by modeling relevance between a sentence and the question (Yin et al., 2016; Miller et al., 2016; Min et al., 2017). Several recent works also study joint sentence selection and question answering. Choi et al. (2017) propose a framework that identifies the sentences relevant to the question (property) using simple bag-ofwords representation, then generates the answer from those sentences using recurrent neural networks. Raiman and Miller (2017) cast the task of extractive question answering as a search problem by iteratively selecting the sentences, start position and end position. They are different from our work in that (i) we study of the minimal context required to answer the question, (ii) we choose t"
P18-1160,D14-1162,1,0.109605,"Missing"
P18-1160,D17-1111,0,0.0307052,"l., 2018; Yu et al., 2018). Sentence selection The task of selecting sentences that can answer to the question has been studied across several QA datasets (Yang et al., 2015), by modeling relevance between a sentence and the question (Yin et al., 2016; Miller et al., 2016; Min et al., 2017). Several recent works also study joint sentence selection and question answering. Choi et al. (2017) propose a framework that identifies the sentences relevant to the question (property) using simple bag-ofwords representation, then generates the answer from those sentences using recurrent neural networks. Raiman and Miller (2017) cast the task of extractive question answering as a search problem by iteratively selecting the sentences, start position and end position. They are different from our work in that (i) we study of the minimal context required to answer the question, (ii) we choose the minimal context by selecting variable number of sentences for each question, while they use a fixed size of number as a hyperparameter, (iii) our framework is flexible in that it does not require end-to-end training and can be combined with existing QA models, and (iv) they do not show robustness to adversarial inputs. 6 Conclus"
P18-1160,D16-1264,0,0.788517,"d inference times (up to 13 times), with accuracy comparable to or better than the state-of-the-art on SQuAD, NewsQA, TriviaQA and SQuAD-Open. Furthermore, our experimental results and analyses show that our approach is more robust to adversarial inputs. 1 Introduction The task of textual question answering (QA), in which a machine reads a document and answers a question, is an important and challenging problem in natural language processing. Recent progress in performance of QA models has been largely due to the variety of available QA datasets (Richardson et al., 2013; Hermann et al., 2015; Rajpurkar et al., 2016; Trischler et al., 2016; Joshi et al., 2017; Koˇcisk`y et al., 2017). ∗ All work was done while the author was an intern at Salesforce Research. Many neural QA models have been proposed for these datasets, the most successful of which tend to leverage coattention or bidirectional attention mechanisms that build codependent representations of the document and the question (Xiong et al., 2018; Seo et al., 2017). Yet, learning the full context over the document is challenging and inefficient. In particular, when the model is given a long document, or multiple documents, learning the full context"
P18-1160,D13-1020,0,0.302388,"cant reductions in training (up to 15 times) and inference times (up to 13 times), with accuracy comparable to or better than the state-of-the-art on SQuAD, NewsQA, TriviaQA and SQuAD-Open. Furthermore, our experimental results and analyses show that our approach is more robust to adversarial inputs. 1 Introduction The task of textual question answering (QA), in which a machine reads a document and answers a question, is an important and challenging problem in natural language processing. Recent progress in performance of QA models has been largely due to the variety of available QA datasets (Richardson et al., 2013; Hermann et al., 2015; Rajpurkar et al., 2016; Trischler et al., 2016; Joshi et al., 2017; Koˇcisk`y et al., 2017). ∗ All work was done while the author was an intern at Salesforce Research. Many neural QA models have been proposed for these datasets, the most successful of which tend to leverage coattention or bidirectional attention mechanisms that build codependent representations of the document and the question (Xiong et al., 2018; Seo et al., 2017). Yet, learning the full context over the document is challenging and inefficient. In particular, when the model is given a long document, or"
P18-1160,K17-1028,0,0.0463791,"Missing"
P18-1160,D15-1237,0,0.0517367,"over a large set of documents in a closed (Joshi et al., 2017) or open-domain (Dunn et al., 2017; Berant et al., 2013; Chen et al., 2017; Dhingra et al., 2017). Many models for these datasets either retrieve documents/paragraphs relevant to the question (Chen et al., 2017; Clark and Gardner, 2017; Wang et al., 2018), or leverage simple non-recurrent architectures to make training and inference tractable over large corpora (Swayamdipta et al., 2018; Yu et al., 2018). Sentence selection The task of selecting sentences that can answer to the question has been studied across several QA datasets (Yang et al., 2015), by modeling relevance between a sentence and the question (Yin et al., 2016; Miller et al., 2016; Min et al., 2017). Several recent works also study joint sentence selection and question answering. Choi et al. (2017) propose a framework that identifies the sentences relevant to the question (property) using simple bag-ofwords representation, then generates the answer from those sentences using recurrent neural networks. Raiman and Miller (2017) cast the task of extractive question answering as a search problem by iteratively selecting the sentences, start position and end position. They are"
P18-1160,Q16-1019,0,0.0357082,"(Dunn et al., 2017; Berant et al., 2013; Chen et al., 2017; Dhingra et al., 2017). Many models for these datasets either retrieve documents/paragraphs relevant to the question (Chen et al., 2017; Clark and Gardner, 2017; Wang et al., 2018), or leverage simple non-recurrent architectures to make training and inference tractable over large corpora (Swayamdipta et al., 2018; Yu et al., 2018). Sentence selection The task of selecting sentences that can answer to the question has been studied across several QA datasets (Yang et al., 2015), by modeling relevance between a sentence and the question (Yin et al., 2016; Miller et al., 2016; Min et al., 2017). Several recent works also study joint sentence selection and question answering. Choi et al. (2017) propose a framework that identifies the sentences relevant to the question (property) using simple bag-ofwords representation, then generates the answer from those sentences using recurrent neural networks. Raiman and Miller (2017) cast the task of extractive question answering as a search problem by iteratively selecting the sentences, start position and end position. They are different from our work in that (i) we study of the minimal context required"
P18-1160,D17-1082,0,\N,Missing
P19-1078,D18-1398,0,0.0366563,"y. The reason is that labels of the (hotel, type) pair are sometimes missing in the dataset, 815 6 Related Work intention classifiers (Chen et al., 2016), slotfilling (Bapna et al., 2017), and dialogue policy (Gaˇsi´c and Young, 2014). For language generation, Johnson et al. (2017) propose single encoder-decoder models for zero-shot machine translation, and Zhao and Eskenazi (2018) propose cross-domain zero-shot dialogue generation using action matching. Moreover, few-shot learning in natural language applications has been applied in semantic parsing (Huang et al., 2018), machine translation (Gu et al., 2018), and text classification (Yu et al., 2018) with meta-learning approaches (Schmidhuber, 1987; Finn et al., 2017). These tasks usually have multiple tasks to perform fast adaptation, instead in our case the number of existing domains are limited. Lastly, several approaches have been proposed for continual learning in the machine learning community (Kirkpatrick et al., 2017; Lopez-Paz et al., 2017; Rusu et al., 2016; Fernando et al., 2017; Lee et al., 2017), especially in image recognition tasks (Aljundi et al., 2017; Rannen et al., 2017). The applications within NLP has been comparatively limit"
P19-1078,P16-1014,0,0.0299372,"ded dialogue history is represented as Ht = enc |Xt |×dhdd , where d [henc hdd is the 1 , . . . , h|Xt |] ∈ R hidden size. As mentioned in Section 1, due to the multi-turn mapping problem, the model should infer the states across a sequence of turns. Therefore, we use the recent dialogue history of length l as the utterance encoder input, rather than the current utterance only. 2.2 State Generator To generate slot values using text from the input source, a copy mechanism is required. There are three common ways to perform copying, i.e., index-based copy (Vinyals et al., 2015), hardgated copy (Gulcehre et al., 2016; Madotto et al., 2018; Wu et al., 2019) and soft-gated copy (See et al., 2017; McCann et al., 2018). The indexbased mechanism is not suitable for DST task because the exact word(s) of the true slot value are not always found in the utterance. The hard-gate copy mechanism usually needs additional supervihistory pute the history attention Pjk dialogue history Ht : over the encoded vocab = Softmax(E · (hdec )&gt; ) ∈ R|V |, Pjk jk history &gt; |Xt |. = Softmax(Ht · (hdec Pjk jk ) ) ∈ R (1) final is the weightedThe final output distribution Pjk 810 For the latter, another cross-entropy loss Lv befinal"
P19-1078,K16-1002,0,0.113926,"Missing"
P19-1078,W14-4337,0,0.761555,"lid arrows on the left are the single-turn mapping, and the dot arrows on the right are multi-turn mapping. The state tracker needs to track slot values mentioned by the user for all the slots in all the domains. appropriate dialogue management, where user intention determines the next system action and/or the content to query from the databases. Traditionally, state tracking approaches are based on the assumption that ontology is defined in advance, where all slots and their values are known. Having a predefined ontology can simplify DST into a classification problem and improve performance (Henderson et al., 2014b; Mrkˇsi´c et al., 2017; Zhong et al., 2018). However, there are two major drawbacks to this approach: 1) A full ontology is hard to obtain in advance (Xu and Hu, 2018). In the industry, databases are usually exposed through an external API only, which is owned and maintained by others. It is not feasible to gain access to enumerate all the possible values for each slot. 2) Even if a full ontology exists, the number of possible slot values could be large and variable. For example, a restaurant name or a train departure time can contain a large number Introduction Dialogue state tracking (DST)"
P19-1078,D18-1547,0,0.333872,"For example, as shown in Fig. 1, (slot, value) pairs such as (price, cheap) and (area, centre) are extracted from the conversation. Accurate DST performance is crucial for ∗ Work partially done while the first author was an intern at Salesforce Research. 808 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 808–819 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics of possible values. Therefore, many of the previous works that are based on neural classification models may not be applicable in real scenario. Budzianowski et al. (2018) recently introduced a multi-domain dialogue dataset (MultiWOZ), which adds new challenges in DST due to its mixed-domain conversations. As shown in Fig. 1, a user can start a conversation by asking to reserve a restaurant, then requests information regarding an attraction nearby, and finally asks to book a taxi. In this case, the DST model has to determine the corresponding domain, slot and value at each turn of dialogue, which contains a large number of combinations in the ontology, i.e., 30 (domain, slot) pairs and over 4,500 possible slot values in total. Another challenge in the multidoma"
P19-1078,W14-4340,0,0.678325,"lid arrows on the left are the single-turn mapping, and the dot arrows on the right are multi-turn mapping. The state tracker needs to track slot values mentioned by the user for all the slots in all the domains. appropriate dialogue management, where user intention determines the next system action and/or the content to query from the databases. Traditionally, state tracking approaches are based on the assumption that ontology is defined in advance, where all slots and their values are known. Having a predefined ontology can simplify DST into a classification problem and improve performance (Henderson et al., 2014b; Mrkˇsi´c et al., 2017; Zhong et al., 2018). However, there are two major drawbacks to this approach: 1) A full ontology is hard to obtain in advance (Xu and Hu, 2018). In the industry, databases are usually exposed through an external API only, which is owned and maintained by others. It is not feasible to gain access to enumerate all the possible values for each slot. 2) Even if a full ontology exists, the number of possible slot values could be large and variable. For example, a restaurant name or a train departure time can contain a large number Introduction Dialogue state tracking (DST)"
P19-1078,N18-2115,0,0.0286875,"th only two possible values in the ontology. The reason is that labels of the (hotel, type) pair are sometimes missing in the dataset, 815 6 Related Work intention classifiers (Chen et al., 2016), slotfilling (Bapna et al., 2017), and dialogue policy (Gaˇsi´c and Young, 2014). For language generation, Johnson et al. (2017) propose single encoder-decoder models for zero-shot machine translation, and Zhao and Eskenazi (2018) propose cross-domain zero-shot dialogue generation using action matching. Moreover, few-shot learning in natural language applications has been applied in semantic parsing (Huang et al., 2018), machine translation (Gu et al., 2018), and text classification (Yu et al., 2018) with meta-learning approaches (Schmidhuber, 1987; Finn et al., 2017). These tasks usually have multiple tasks to perform fast adaptation, instead in our case the number of existing domains are limited. Lastly, several approaches have been proposed for continual learning in the machine learning community (Kirkpatrick et al., 2017; Lopez-Paz et al., 2017; Rusu et al., 2016; Fernando et al., 2017; Lee et al., 2017), especially in image recognition tasks (Aljundi et al., 2017; Rannen et al., 2017). The applications"
P19-1078,P18-5002,0,0.0582567,"Missing"
P19-1078,D18-1299,0,0.504676,"Missing"
P19-1078,P18-1133,0,0.202571,"eatures and complex domain-specific lexicons (besides the ontology), and are difficult to extend and scale to new domains. Mrkˇsi´c et al. (2017) use distributional representation learning to leverage semantic information from word embeddings to and resolve lexical/morphological ambiguity. However, parameters are not shared across slots. On the other hand, Nouri and Hosseini-Asl (2018) utilizes global modules to share parameters between slots, and Zhong et al. (2018) uses slot-specific local modules to learn slot features, which has proved to successfully improve tracking of rare slot values. Lei et al. (2018) use a Seq2Seq model to generate belief spans and the delexicalized response at the same time. Ren et al. (2018) propose StateNet that generates a dialogue history representation and compares the distances between this representation and value vectors in the candidate set. Xu and Hu (2018) use the index-based pointer network for different slots, and show the ability to point to unknown values. However, many of them require a predefined domain ontology, and the models were only evaluated on single-domain setting (DSTC2). For multi-domain DST, Rastogi et al. (2017) propose a multi-domain approac"
P19-1078,P18-1136,1,0.873258,"represented as Ht = enc |Xt |×dhdd , where d [henc hdd is the 1 , . . . , h|Xt |] ∈ R hidden size. As mentioned in Section 1, due to the multi-turn mapping problem, the model should infer the states across a sequence of turns. Therefore, we use the recent dialogue history of length l as the utterance encoder input, rather than the current utterance only. 2.2 State Generator To generate slot values using text from the input source, a copy mechanism is required. There are three common ways to perform copying, i.e., index-based copy (Vinyals et al., 2015), hardgated copy (Gulcehre et al., 2016; Madotto et al., 2018; Wu et al., 2019) and soft-gated copy (See et al., 2017; McCann et al., 2018). The indexbased mechanism is not suitable for DST task because the exact word(s) of the true slot value are not always found in the utterance. The hard-gate copy mechanism usually needs additional supervihistory pute the history attention Pjk dialogue history Ht : over the encoded vocab = Softmax(E · (hdec )&gt; ) ∈ R|V |, Pjk jk history &gt; |Xt |. = Softmax(Ht · (hdec Pjk jk ) ) ∈ R (1) final is the weightedThe final output distribution Pjk 810 For the latter, another cross-entropy loss Lv befinal and the true words Y l"
P19-1078,P17-1099,0,0.030768,"he 1 , . . . , h|Xt |] ∈ R hidden size. As mentioned in Section 1, due to the multi-turn mapping problem, the model should infer the states across a sequence of turns. Therefore, we use the recent dialogue history of length l as the utterance encoder input, rather than the current utterance only. 2.2 State Generator To generate slot values using text from the input source, a copy mechanism is required. There are three common ways to perform copying, i.e., index-based copy (Vinyals et al., 2015), hardgated copy (Gulcehre et al., 2016; Madotto et al., 2018; Wu et al., 2019) and soft-gated copy (See et al., 2017; McCann et al., 2018). The indexbased mechanism is not suitable for DST task because the exact word(s) of the true slot value are not always found in the utterance. The hard-gate copy mechanism usually needs additional supervihistory pute the history attention Pjk dialogue history Ht : over the encoded vocab = Softmax(E · (hdec )&gt; ) ∈ R|V |, Pjk jk history &gt; |Xt |. = Softmax(Ht · (hdec Pjk jk ) ) ∈ R (1) final is the weightedThe final output distribution Pjk 810 For the latter, another cross-entropy loss Lv befinal and the true words Y label is used. We tween Pjk j define Lv as sum of two dis"
P19-1078,D16-1022,0,0.0865702,"Missing"
P19-1078,P17-1163,0,0.574186,"Missing"
P19-1078,D17-1314,0,0.0174056,"earning approaches (Schmidhuber, 1987; Finn et al., 2017). These tasks usually have multiple tasks to perform fast adaptation, instead in our case the number of existing domains are limited. Lastly, several approaches have been proposed for continual learning in the machine learning community (Kirkpatrick et al., 2017; Lopez-Paz et al., 2017; Rusu et al., 2016; Fernando et al., 2017; Lee et al., 2017), especially in image recognition tasks (Aljundi et al., 2017; Rannen et al., 2017). The applications within NLP has been comparatively limited, e.g., Shu et al. (2016, 2017b) for opinion mining, Shu et al. (2017a) for document classification, and Lee (2017) for hybrid code networks (Williams et al., 2017). Dialogue State Tracking Traditional dialogue state tracking models combine semantics extracted by language understanding modules to estimate the current dialogue states (Williams and Young, 2007; Thomson and Young, 2010; Wang and Lemon, 2013; Williams, 2014), or to jointly learn speech understanding (Henderson et al., 2014b; Zilka and Jurcicek, 2015; Wen et al., 2017). One drawback is that they rely on hand-crafted features and complex domain-specific lexicons (besides the ontology), and are diffic"
P19-1078,P17-2023,0,0.0220702,"earning approaches (Schmidhuber, 1987; Finn et al., 2017). These tasks usually have multiple tasks to perform fast adaptation, instead in our case the number of existing domains are limited. Lastly, several approaches have been proposed for continual learning in the machine learning community (Kirkpatrick et al., 2017; Lopez-Paz et al., 2017; Rusu et al., 2016; Fernando et al., 2017; Lee et al., 2017), especially in image recognition tasks (Aljundi et al., 2017; Rannen et al., 2017). The applications within NLP has been comparatively limited, e.g., Shu et al. (2016, 2017b) for opinion mining, Shu et al. (2017a) for document classification, and Lee (2017) for hybrid code networks (Williams et al., 2017). Dialogue State Tracking Traditional dialogue state tracking models combine semantics extracted by language understanding modules to estimate the current dialogue states (Williams and Young, 2007; Thomson and Young, 2010; Wang and Lemon, 2013; Williams, 2014), or to jointly learn speech understanding (Henderson et al., 2014b; Zilka and Jurcicek, 2015; Wen et al., 2017). One drawback is that they rely on hand-crafted features and complex domain-specific lexicons (besides the ontology), and are diffic"
P19-1078,D14-1162,1,0.106752,"five domains. The numbers in the last three rows indicate the number of dialogues for train, validation and test sets. Fi (Θi − ΘS,i )2 , (8) MinimizeΘ L(Θ) Hotel price, type, parking, stay, day, people, area, stars, internet, name 3381 416 394 4.2 Training Details Multi-domain Joint Training The model is trained end-to-end using the Adam optimizer (Kingma and Ba, 2015) with a batch size of 32. The learning rate annealing is in the range of [0.001, 0.0001] with a dropout ratio of 0.2. Both α and β in Eq (7) are set to one. All the embeddings are initialized by concatenating Glove embeddings (Pennington et al., 2014) and character embeddings (Hashimoto et al., 2016), where the dimension is 400 for each vocabulary word. A greedy search decoding strategy is used for our state generator since the generated slot values are usually short in length. In addition, to in(9) where L(Θ, K) is the loss value of the K stored samples. Lopez-Paz et al. (2017) show how to solve the optimization problem in Eq (9) with quadratic programming if the loss of the stored samples increases. 812 crease model generalization and simulate an outof-vocabulary setting, a word dropout is utilized with the utterance encoder by randomly"
P19-1078,P18-2069,0,0.230289,"tes a dialogue history representation and compares the distances between this representation and value vectors in the candidate set. Xu and Hu (2018) use the index-based pointer network for different slots, and show the ability to point to unknown values. However, many of them require a predefined domain ontology, and the models were only evaluated on single-domain setting (DSTC2). For multi-domain DST, Rastogi et al. (2017) propose a multi-domain approach using two-layer bi-GRU. Although it does not need an ad-hoc state update mechanism, it relies on delexicalization to extract the features. Ramadan et al. (2018) propose a model to jointly track domain and the dialogue states using multiple bi-LSTM. They utilize semantic similarity between utterances and the ontology terms and allow the information to be shared across domains. For a more general overview, readers may refer to the neural dialogue review paper from Gao et al. (2018). 7 Conclusion We introduce a transferable dialogue state generator for multi-domain dialogue state tracking, which learns to track states without any predefined domain ontology. TRADE shares all of its parameters across multiple domains and achieves stateof-the-art joint goa"
P19-1078,W13-4067,0,0.279858,"al., 2017; Rusu et al., 2016; Fernando et al., 2017; Lee et al., 2017), especially in image recognition tasks (Aljundi et al., 2017; Rannen et al., 2017). The applications within NLP has been comparatively limited, e.g., Shu et al. (2016, 2017b) for opinion mining, Shu et al. (2017a) for document classification, and Lee (2017) for hybrid code networks (Williams et al., 2017). Dialogue State Tracking Traditional dialogue state tracking models combine semantics extracted by language understanding modules to estimate the current dialogue states (Williams and Young, 2007; Thomson and Young, 2010; Wang and Lemon, 2013; Williams, 2014), or to jointly learn speech understanding (Henderson et al., 2014b; Zilka and Jurcicek, 2015; Wen et al., 2017). One drawback is that they rely on hand-crafted features and complex domain-specific lexicons (besides the ontology), and are difficult to extend and scale to new domains. Mrkˇsi´c et al. (2017) use distributional representation learning to leverage semantic information from word embeddings to and resolve lexical/morphological ambiguity. However, parameters are not shared across slots. On the other hand, Nouri and Hosseini-Asl (2018) utilizes global modules to share"
P19-1078,E17-1042,0,0.323621,"Missing"
P19-1078,W14-4339,0,0.101165,", 2016; Fernando et al., 2017; Lee et al., 2017), especially in image recognition tasks (Aljundi et al., 2017; Rannen et al., 2017). The applications within NLP has been comparatively limited, e.g., Shu et al. (2016, 2017b) for opinion mining, Shu et al. (2017a) for document classification, and Lee (2017) for hybrid code networks (Williams et al., 2017). Dialogue State Tracking Traditional dialogue state tracking models combine semantics extracted by language understanding modules to estimate the current dialogue states (Williams and Young, 2007; Thomson and Young, 2010; Wang and Lemon, 2013; Williams, 2014), or to jointly learn speech understanding (Henderson et al., 2014b; Zilka and Jurcicek, 2015; Wen et al., 2017). One drawback is that they rely on hand-crafted features and complex domain-specific lexicons (besides the ontology), and are difficult to extend and scale to new domains. Mrkˇsi´c et al. (2017) use distributional representation learning to leverage semantic information from word embeddings to and resolve lexical/morphological ambiguity. However, parameters are not shared across slots. On the other hand, Nouri and Hosseini-Asl (2018) utilizes global modules to share parameters betwe"
P19-1078,P17-1062,0,0.0575854,"ple tasks to perform fast adaptation, instead in our case the number of existing domains are limited. Lastly, several approaches have been proposed for continual learning in the machine learning community (Kirkpatrick et al., 2017; Lopez-Paz et al., 2017; Rusu et al., 2016; Fernando et al., 2017; Lee et al., 2017), especially in image recognition tasks (Aljundi et al., 2017; Rannen et al., 2017). The applications within NLP has been comparatively limited, e.g., Shu et al. (2016, 2017b) for opinion mining, Shu et al. (2017a) for document classification, and Lee (2017) for hybrid code networks (Williams et al., 2017). Dialogue State Tracking Traditional dialogue state tracking models combine semantics extracted by language understanding modules to estimate the current dialogue states (Williams and Young, 2007; Thomson and Young, 2010; Wang and Lemon, 2013; Williams, 2014), or to jointly learn speech understanding (Henderson et al., 2014b; Zilka and Jurcicek, 2015; Wen et al., 2017). One drawback is that they rely on hand-crafted features and complex domain-specific lexicons (besides the ontology), and are difficult to extend and scale to new domains. Mrkˇsi´c et al. (2017) use distributional representatio"
P19-1078,P18-1134,0,0.500512,"for all the slots in all the domains. appropriate dialogue management, where user intention determines the next system action and/or the content to query from the databases. Traditionally, state tracking approaches are based on the assumption that ontology is defined in advance, where all slots and their values are known. Having a predefined ontology can simplify DST into a classification problem and improve performance (Henderson et al., 2014b; Mrkˇsi´c et al., 2017; Zhong et al., 2018). However, there are two major drawbacks to this approach: 1) A full ontology is hard to obtain in advance (Xu and Hu, 2018). In the industry, databases are usually exposed through an external API only, which is owned and maintained by others. It is not feasible to gain access to enumerate all the possible values for each slot. 2) Even if a full ontology exists, the number of possible slot values could be large and variable. For example, a restaurant name or a train departure time can contain a large number Introduction Dialogue state tracking (DST) is a core component in task-oriented dialogue systems, such as restaurant reservation or ticket booking. The goal of DST is to extract user goals/intentions expressed d"
P19-1078,N18-1109,0,0.0241526,"type) pair are sometimes missing in the dataset, 815 6 Related Work intention classifiers (Chen et al., 2016), slotfilling (Bapna et al., 2017), and dialogue policy (Gaˇsi´c and Young, 2014). For language generation, Johnson et al. (2017) propose single encoder-decoder models for zero-shot machine translation, and Zhao and Eskenazi (2018) propose cross-domain zero-shot dialogue generation using action matching. Moreover, few-shot learning in natural language applications has been applied in semantic parsing (Huang et al., 2018), machine translation (Gu et al., 2018), and text classification (Yu et al., 2018) with meta-learning approaches (Schmidhuber, 1987; Finn et al., 2017). These tasks usually have multiple tasks to perform fast adaptation, instead in our case the number of existing domains are limited. Lastly, several approaches have been proposed for continual learning in the machine learning community (Kirkpatrick et al., 2017; Lopez-Paz et al., 2017; Rusu et al., 2016; Fernando et al., 2017; Lee et al., 2017), especially in image recognition tasks (Aljundi et al., 2017; Rannen et al., 2017). The applications within NLP has been comparatively limited, e.g., Shu et al. (2016, 2017b) for opin"
P19-1078,W18-5001,0,0.0369032,"nd, number-related slots such as arrive by, people, and stay usually have the lowest error rates. We also find that the type slot of hotel domain has a high error rate, even if it is an easy task with only two possible values in the ontology. The reason is that labels of the (hotel, type) pair are sometimes missing in the dataset, 815 6 Related Work intention classifiers (Chen et al., 2016), slotfilling (Bapna et al., 2017), and dialogue policy (Gaˇsi´c and Young, 2014). For language generation, Johnson et al. (2017) propose single encoder-decoder models for zero-shot machine translation, and Zhao and Eskenazi (2018) propose cross-domain zero-shot dialogue generation using action matching. Moreover, few-shot learning in natural language applications has been applied in semantic parsing (Huang et al., 2018), machine translation (Gu et al., 2018), and text classification (Yu et al., 2018) with meta-learning approaches (Schmidhuber, 1987; Finn et al., 2017). These tasks usually have multiple tasks to perform fast adaptation, instead in our case the number of existing domains are limited. Lastly, several approaches have been proposed for continual learning in the machine learning community (Kirkpatrick et al."
P19-1078,P18-1135,1,0.917453,"ng, and the dot arrows on the right are multi-turn mapping. The state tracker needs to track slot values mentioned by the user for all the slots in all the domains. appropriate dialogue management, where user intention determines the next system action and/or the content to query from the databases. Traditionally, state tracking approaches are based on the assumption that ontology is defined in advance, where all slots and their values are known. Having a predefined ontology can simplify DST into a classification problem and improve performance (Henderson et al., 2014b; Mrkˇsi´c et al., 2017; Zhong et al., 2018). However, there are two major drawbacks to this approach: 1) A full ontology is hard to obtain in advance (Xu and Hu, 2018). In the industry, databases are usually exposed through an external API only, which is owned and maintained by others. It is not feasible to gain access to enumerate all the possible values for each slot. 2) Even if a full ontology exists, the number of possible slot values could be large and variable. For example, a restaurant name or a train departure time can contain a large number Introduction Dialogue state tracking (DST) is a core component in task-oriented dialogu"
P19-1443,W06-3000,0,0.242624,"he development in dialogue (Henderson et al., 2014; Mrkˇsi´c et al., 2017; Zhong et al., 2018) uses predefined ontology and slot-value pairs with limited natural language meaning representation, whereas we focus on general SQL queries that enable more powerful semantic meaning representation. Recently, some conversational question answering datasets have been introduced, such as QuAC (Choi et al., 2018) and CoQA (Reddy et al., 2018). They differ from SParC in that the answers are free-form text instead of SQL queries. On the other hand, Kato et al. (2004); Chai and Jin (2004); Bertomeu et al. (2006) conduct early studies of the contextual phenomena and thematic relations in database dialogue/QA systems, which we use as references when constructing SParC. 3 Data Collection We create the SParC dataset in four stages: selecting interaction goals, creating questions, annotating SQL representations, and reviewing. Interaction goal selection To ensure thematic relevance within each question sequence, we use questions in the original Spider dataset as the thematic guidance for constructing meaningful query interactions, i.e. the interaction goal. Each sequence is based on a question in Spider a"
P19-1443,W04-2504,0,0.426463,"atabase (Hemphill et al., 1990; Dahl et al., 1994). In a real-world setting, users tend to ask a sequence of thematically related questions to learn about a particular topic or to achieve a complex goal. Previous studies have shown that by allowing questions to be constructed sequentially, users can explore the data in a more flexible manner, which reduces their cognitive burden (Hale, 2006; Levy, 2008; Frank, 2013; Iyyer et al., 2017) and increases their involvement when interacting with the system. The phrasing of such questions depends heavily on the interaction history (Kato et al., 2004; Chai and Jin, 2004; Bertomeu et al., 2006). The users may explicitly refer to or omit previously mentioned entities and constraints, and may introduce refinements, additions or substitutions to what has already been said (Figure 1). This requires a practical text-to-SQL system to effectively process context information to synthesize the correct SQL logic. To enable modeling advances in contextdependent semantic parsing, we introduce SParC (cross-domain Semantic Parsing in Context), an expert-labeled dataset which contains 4,298 coherent question sequences (12k+ questions paired with SQL queries) querying 200 co"
P19-1443,H94-1010,0,0.882449,"ese work focus on precisely mapping stand-alone utterances to SQL queries, generating SQL queries in a context-dependent scenario (Miller et al., 1996; Zettlemoyer and 4511 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4511–4523 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics Collins, 2009; Suhr et al., 2018) has been studied less often. The most prominent context-dependent text-to-SQL benchmark is ATIS1 , which is set in the flight-booking domain and contains only one database (Hemphill et al., 1990; Dahl et al., 1994). In a real-world setting, users tend to ask a sequence of thematically related questions to learn about a particular topic or to achieve a complex goal. Previous studies have shown that by allowing questions to be constructed sequentially, users can explore the data in a more flexible manner, which reduces their cognitive burden (Hale, 2006; Levy, 2008; Frank, 2013; Iyyer et al., 2017) and increases their involvement when interacting with the system. The phrasing of such questions depends heavily on the interaction history (Kato et al., 2004; Chai and Jin, 2004; Bertomeu et al., 2006). The us"
P19-1443,P16-1004,0,0.0694107,"atabases have to address. In addition, it enables us to test the generalization of the trained systems to unseen databases and domains. We asked 15 college students with SQL experience to come up with question sequences over the Spider databases (§ 3). Questions in the original Spider dataset were used as guidance to the students for constructing meaningful interactions: each sequence is based on a question in Spider and the student has to ask inter-related questions to ob1 A subset of ATIS is also frequently used in contextindependent semantic parsing research (Zettlemoyer and Collins, 2007; Dong and Lapata, 2016). 2 The data is available at https://yale-lily. github.io/spider. tain information that answers the Spider question. At the same time, the students are encouraged to come up with related questions which do not directly contribute to the Spider question so as to increase data diversity. The questions were subsequently translated to complex SQL queries by the same student. Similar to Spider, the SQL Queries in SParC cover complex syntactic structures and most common SQL keywords. We split the dataset such that a database appears in only one of the train, development and test sets. We provide det"
P19-1443,P18-1068,0,0.0403454,"at there is plenty of room for advancement in modeling and learning on the SParC dataset. 2 Related Work Context-independent semantic parsing Early studies in semantic parsing (Zettlemoyer and Collins, 2005; Artzi and Zettlemoyer, 2013; Berant and Liang, 2014; Li and Jagadish, 2014; Pasupat and Liang, 2015; Dong and Lapata, 2016; Iyer et al., 2017) were based on small and singledomain datasets such as ATIS (Hemphill et al., 1990; Dahl et al., 1994) and GeoQuery (Zelle and Mooney, 1996). Recently, an increasing number of neural approaches (Zhong et al., 2017; Xu et al., 2017; Yu et al., 2018a; Dong and Lapata, 2018; Yu et al., 2018b) have started to use large and crossdomain text-to-SQL datasets such as WikiSQL (Zhong et al., 2017) and Spider (Yu et al., 2018c). Most of them focus on converting stand-alone natural language questions to executable queries. Table 1 compares SParC with other semantic parsing datasets. Context-dependent semantic parsing with SQL labels Only a few datasets have been constructed for the purpose of mapping contextdependent questions to structured queries. 3 Exact string match ignores ordering discrepancies of SQL components whose order does not matter. Exact set matching is ab"
P19-1443,H90-1021,0,0.896741,"Missing"
P19-1443,W14-4337,0,0.0264241,"Missing"
P19-1443,P17-1089,0,0.361004,"Missing"
P19-1443,P17-1167,0,0.14015,"Missing"
P19-1443,W04-2509,0,0.345013,"contains only one database (Hemphill et al., 1990; Dahl et al., 1994). In a real-world setting, users tend to ask a sequence of thematically related questions to learn about a particular topic or to achieve a complex goal. Previous studies have shown that by allowing questions to be constructed sequentially, users can explore the data in a more flexible manner, which reduces their cognitive burden (Hale, 2006; Levy, 2008; Frank, 2013; Iyyer et al., 2017) and increases their involvement when interacting with the system. The phrasing of such questions depends heavily on the interaction history (Kato et al., 2004; Chai and Jin, 2004; Bertomeu et al., 2006). The users may explicitly refer to or omit previously mentioned entities and constraints, and may introduce refinements, additions or substitutions to what has already been said (Figure 1). This requires a practical text-to-SQL system to effectively process context information to synthesize the correct SQL logic. To enable modeling advances in contextdependent semantic parsing, we introduce SParC (cross-domain Semantic Parsing in Context), an expert-labeled dataset which contains 4,298 coherent question sequences (12k+ questions paired with SQL quer"
P19-1443,P16-1138,0,0.462801,"rsing datasets. Context-dependent semantic parsing with SQL labels Only a few datasets have been constructed for the purpose of mapping contextdependent questions to structured queries. 3 Exact string match ignores ordering discrepancies of SQL components whose order does not matter. Exact set matching is able to consider ordering issues in SQL evaluation. See more evaluation details in section 6.1. 4512 Dataset SParC ATIS (Hemphill et al., 1990; Dahl et al., 1994) Spider (Yu et al., 2018c) WikiSQL (Zhong et al., 2017) GeoQuery (Zelle and Mooney, 1996) SequentialQA (Iyyer et al., 2017) SCONE (Long et al., 2016) Context X X 7 7 7 X X Resource database database database table database table environment Annotation SQL SQL SQL SQL SQL denotation denotation Cross-domain X 7 X X 7 X 7 Table 1: Comparison of SParC with existing semantic parsing datasets. Hemphill et al. (1990); Dahl et al. (1994) collected the contextualized version of ATIS that includes series of questions from users interacting with a flight database. Adopted by several works later on (Miller et al., 1996; Zettlemoyer and Collins, 2009; Suhr et al., 2018), ATIS has only a single domain for flight planning which limits the possible SQL lo"
P19-1443,P96-1008,0,0.654289,"Q3 : How about for the first 5 customers? S 3 : SELECT customer_name FROM customers ORDER BY date_became_customer LIMIT 5 Figure 1: Two question sequences from the SParC dataset. Questions (Qi ) in each sequence query a database (Di ), obtaining information sufficient to complete the interaction goal (Ci ). Each question is annotated with a corresponding SQL query (Si ). SQL token sequences from the interaction context are underlined. While most of these work focus on precisely mapping stand-alone utterances to SQL queries, generating SQL queries in a context-dependent scenario (Miller et al., 1996; Zettlemoyer and 4511 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4511–4523 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics Collins, 2009; Suhr et al., 2018) has been studied less often. The most prominent context-dependent text-to-SQL benchmark is ATIS1 , which is set in the flight-booking domain and contains only one database (Hemphill et al., 1990; Dahl et al., 1994). In a real-world setting, users tend to ask a sequence of thematically related questions to learn about a particular topic or to ac"
P19-1443,P17-1163,0,0.0488151,"Missing"
P19-1443,P15-1142,0,0.0459989,"asets used in recovering context-dependent meaning (including SCONE (Long et al., 2016) and SequentialQA (Iyyer et al., 2017)) contain no logical form annotations but only denotation (Berant and Liang, 2014) instead. SCONE (Long et al., 2016) contains some instructions in limited domains such as chemistry experiments. The formal representations in the dataset are world states representing state changes after each instruction instead of programs or logical forms. SequentialQA (Iyyer et al., 2017) was created by asking crowd workers to decompose some complicated questions in WikiTableQuestions (Pasupat and Liang, 2015) into sequences of inner-related simple questions. As shown in Table 1, neither of the two datasets were annotated with query labels. Thus, to make the tasks feasible, SCONE (Long et al., 2016) and SequentialQA (Iyyer et al., 2017) exclude many questions with rich semantic and contextual types. For example, (Iyyer et al., 2017) requires that the answers to the questions in SequentialQA must appear in the table, and most of them can be solved by simple SQL queries with SELECT and WHERE clauses. Such direct mapping without formal query labels becomes unfeasible for complex questions. Furthermore"
P19-1443,D14-1162,1,0.110555,"ts corresponding table name and column name separated by a special dot token (i.e., table name.column name), and use the average word embedding10 of tokens in this sequence as the column header embedding hC . Decoder The decoder is implemented with another LSTM (LSTMD ) with attention to the LSTME representations of the questions in η previous turns. At each decoding step, the decoder chooses to generate either a SQL keyword (e.g., select, where, group by) or a column header. To achieve this, we use separate layers to score SQL keywords and column headers, 10 We use the 300-dimensional GloVe (Pennington et al., 2014) pretrained word embeddings. 4517 Model and finally use the softmax operation to generate the output probability distribution over both categories. 5.2 SyntaxSQLNet with history input (SyntaxSQL-con) SyntaxSQLNet is a syntax tree based neural model for the complex and cross-domain contextindependent text-to-SQL task introduced by Yu et al. (2018b). The model consists of a table-aware column attention encoder and a SQL-specific syntax tree-based decoder. The decoder adopts a set of inter-connected neural modules to generate different SQL syntax components. We extend this model by providing the"
P19-1443,N18-1203,0,0.332817,"Missing"
P19-1443,D07-1071,0,0.146156,"atural language interfaces to databases have to address. In addition, it enables us to test the generalization of the trained systems to unseen databases and domains. We asked 15 college students with SQL experience to come up with question sequences over the Spider databases (§ 3). Questions in the original Spider dataset were used as guidance to the students for constructing meaningful interactions: each sequence is based on a question in Spider and the student has to ask inter-related questions to ob1 A subset of ATIS is also frequently used in contextindependent semantic parsing research (Zettlemoyer and Collins, 2007; Dong and Lapata, 2016). 2 The data is available at https://yale-lily. github.io/spider. tain information that answers the Spider question. At the same time, the students are encouraged to come up with related questions which do not directly contribute to the Spider question so as to increase data diversity. The questions were subsequently translated to complex SQL queries by the same student. Similar to Spider, the SQL Queries in SParC cover complex syntactic structures and most common SQL keywords. We split the dataset such that a database appears in only one of the train, development and t"
P19-1443,P09-1110,0,0.281655,"l., 2018c) WikiSQL (Zhong et al., 2017) GeoQuery (Zelle and Mooney, 1996) SequentialQA (Iyyer et al., 2017) SCONE (Long et al., 2016) Context X X 7 7 7 X X Resource database database database table database table environment Annotation SQL SQL SQL SQL SQL denotation denotation Cross-domain X 7 X X 7 X 7 Table 1: Comparison of SParC with existing semantic parsing datasets. Hemphill et al. (1990); Dahl et al. (1994) collected the contextualized version of ATIS that includes series of questions from users interacting with a flight database. Adopted by several works later on (Miller et al., 1996; Zettlemoyer and Collins, 2009; Suhr et al., 2018), ATIS has only a single domain for flight planning which limits the possible SQL logic it contains. In contrast to ATIS, SParC consists of a large number of complex SQL queries (with most SQL syntax components) inquiring 200 databases in 138 different domains, which contributes to its diversity in query semantics and contextual dependencies. Similar to Spider, the databases in the train, development and test sets of SParC do not overlap. Context-dependent semantic parsing with denotations Some datasets used in recovering context-dependent meaning (including SCONE (Long et"
P19-1443,P18-1135,1,0.81917,"uery labels becomes unfeasible for complex questions. Furthermore, SequentialQA contains questions based only on a single Wikipedia tables at a time. In contrast, SParC contains 200 significantly larger databases, and complex query labels with all common SQL key components. This requires a system developed for SParC to handle information needed over larger databases in different domains. Conversational QA and dialogue system Language understanding in context is also studied for dialogue and question answering systems. The development in dialogue (Henderson et al., 2014; Mrkˇsi´c et al., 2017; Zhong et al., 2018) uses predefined ontology and slot-value pairs with limited natural language meaning representation, whereas we focus on general SQL queries that enable more powerful semantic meaning representation. Recently, some conversational question answering datasets have been introduced, such as QuAC (Choi et al., 2018) and CoQA (Reddy et al., 2018). They differ from SParC in that the answers are free-form text instead of SQL queries. On the other hand, Kato et al. (2004); Chai and Jin (2004); Bertomeu et al. (2006) conduct early studies of the contextual phenomena and thematic relations in database di"
P19-1443,N18-2093,1,0.869956,"ORDER BY Introduction date_became_customer DESC LIMIT 1 Querying a relational database is often challenging and a natural language interface has long been regarded by many as the most powerful database interface (Popescu et al., 2003; Bertomeu et al., 2006; Li and Jagadish, 2014). The problem of mapping a natural language utterance into executable SQL queries (text-to-SQL) has attracted increasing attention from the semantic parsing community by virtue of a continuous effort of dataset creation (Zelle and Mooney, 1996; Iyyer et al., 2017; Zhong et al., 2017; Finegan-Dollak et al., 2018; Yu et al., 2018a) and the modeling innovation that follows it (Xu et al., 2017; Wang et al., 2018; Yu et al., 2018b; Shi et al., 2018). Q3 : How about for the first 5 customers? S 3 : SELECT customer_name FROM customers ORDER BY date_became_customer LIMIT 5 Figure 1: Two question sequences from the SParC dataset. Questions (Qi ) in each sequence query a database (Di ), obtaining information sufficient to complete the interaction goal (Ci ). Each question is annotated with a corresponding SQL query (Si ). SQL token sequences from the interaction context are underlined. While most of these work f"
P19-1443,D18-1193,1,0.919435,"ORDER BY Introduction date_became_customer DESC LIMIT 1 Querying a relational database is often challenging and a natural language interface has long been regarded by many as the most powerful database interface (Popescu et al., 2003; Bertomeu et al., 2006; Li and Jagadish, 2014). The problem of mapping a natural language utterance into executable SQL queries (text-to-SQL) has attracted increasing attention from the semantic parsing community by virtue of a continuous effort of dataset creation (Zelle and Mooney, 1996; Iyyer et al., 2017; Zhong et al., 2017; Finegan-Dollak et al., 2018; Yu et al., 2018a) and the modeling innovation that follows it (Xu et al., 2017; Wang et al., 2018; Yu et al., 2018b; Shi et al., 2018). Q3 : How about for the first 5 customers? S 3 : SELECT customer_name FROM customers ORDER BY date_became_customer LIMIT 5 Figure 1: Two question sequences from the SParC dataset. Questions (Qi ) in each sequence query a database (Di ), obtaining information sufficient to complete the interaction goal (Ci ). Each question is annotated with a corresponding SQL query (Si ). SQL token sequences from the interaction context are underlined. While most of these work f"
P19-1443,W06-3001,0,\N,Missing
P19-1443,Q13-1005,0,\N,Missing
P19-1443,P14-1133,0,\N,Missing
P19-1443,P18-1033,1,\N,Missing
P19-1443,D18-1241,0,\N,Missing
P19-1487,D15-1075,0,0.055723,"omplete phrases in the input text that by itself is sufficient to predict the desired output. Humangenerated natural language explanations for classification data have been used in the past to train a semantic parser that in turn generates more noisy 4933 labeled data which can used to train a classifier (Hancock et al., 2018). Camburu et al. (2018) generate explanations and predictions for the natural language inference problem (Camburu et al., 2018). However, the authors report that interpretability comes at the cost of loss in performance on the popular Stanford Natural Language Inference (Bowman et al., 2015) dataset. We find that, unlike for e-SNLI, explanations for CQA lead to improved performance in what Camburu et al. (2018) would call the explain-predict setting. In the multi-modal setting, Rajani and Mooney (2018) showed that visual explanations can be leveraged to improve performance of VQA (Antol et al., 2015) and that an ensemble explanation is significantly better than individual explanations using both automated and human evaluations (Rajani and Mooney, 2017). Knowledge Transfer in NLP Natural language processing has often relied on the transfer of world-knowledge through pretrained wor"
P19-1487,D17-1070,0,0.0529786,"Missing"
P19-1487,N19-1423,0,0.641334,"onsense reasoning Datasets that require models to learn to predict relations between situations or events in natural language have been introduced in the recent past. The Story Cloze (also referred to as ROC Stories) involves predicting the correct story ending from a set of plausible endings (Mostafazadeh et al., 2016) while the Situations with Adversarial Generations (SWAG) involves predicting the next scene based on an initial event (Zellers et al., 2018). Language Modeling based techniques such as the GPT and BERT models get human-level performance on these datasets (Radford et al., 2018; Devlin et al., 2019). They have been less successful on tasks that require clear understanding of how pronouns resolve between sentences and how that interacts with world knowledge. For example, the Winograd Schemas (Winograd, 1972) and challenges derived from that format (Levesque et al., 2012; McCann et al., 2018; Wang et al., 2018) have proven difficult for even the most modern machine learning methods (Trinh and Le, 2018) to achieve near-human performance, but the emphasis on pronoun resolution in those challenges leaves room for exploration of other kinds of commonsense reasoning. CQA is a new dataset that c"
P19-1487,P18-1175,0,0.132063,"Missing"
P19-1487,marelli-etal-2014-sick,0,0.0275802,"Missing"
P19-1487,N16-1098,0,0.0255083,". We demonstrate explanation transfer on two out-of-domain datasets. Note that before our final submission, the organizers released a more challenging v1.11 of CQA with 5 answer choices instead of 3 and so we also included the new version in our results and discussions. 2 Background and Related Work Commonsense reasoning Datasets that require models to learn to predict relations between situations or events in natural language have been introduced in the recent past. The Story Cloze (also referred to as ROC Stories) involves predicting the correct story ending from a set of plausible endings (Mostafazadeh et al., 2016) while the Situations with Adversarial Generations (SWAG) involves predicting the next scene based on an initial event (Zellers et al., 2018). Language Modeling based techniques such as the GPT and BERT models get human-level performance on these datasets (Radford et al., 2018; Devlin et al., 2019). They have been less successful on tasks that require clear understanding of how pronouns resolve between sentences and how that interacts with world knowledge. For example, the Winograd Schemas (Winograd, 1972) and challenges derived from that format (Levesque et al., 2012; McCann et al., 2018; Wan"
P19-1487,P02-1040,0,0.102975,"Missing"
P19-1487,D14-1162,1,0.106837,"r CQA lead to improved performance in what Camburu et al. (2018) would call the explain-predict setting. In the multi-modal setting, Rajani and Mooney (2018) showed that visual explanations can be leveraged to improve performance of VQA (Antol et al., 2015) and that an ensemble explanation is significantly better than individual explanations using both automated and human evaluations (Rajani and Mooney, 2017). Knowledge Transfer in NLP Natural language processing has often relied on the transfer of world-knowledge through pretrained word vectors like Word2Vec (Mikolov et al., 2013) and GloVe (Pennington et al., 2014). Contextualized word vectors (McCann et al., 2017; Peters et al., 2018) refined these representations for particular inputs by using different forms of general encoding. Language models trained from scratch on large amounts of data have made groundbreaking success in this direction by carefully finetuning for specific tasks (Dai and Le, 2015; Radford et al., 2018; Howard and Ruder, 2018; Devlin et al., 2019). These models have the advantage that only a few parameters need to be learned from scratch and thus perform surprisingly well even on small amounts of supervised data. Fine-tuned languag"
P19-1487,P18-1031,0,0.020347,"ni and Mooney, 2017). Knowledge Transfer in NLP Natural language processing has often relied on the transfer of world-knowledge through pretrained word vectors like Word2Vec (Mikolov et al., 2013) and GloVe (Pennington et al., 2014). Contextualized word vectors (McCann et al., 2017; Peters et al., 2018) refined these representations for particular inputs by using different forms of general encoding. Language models trained from scratch on large amounts of data have made groundbreaking success in this direction by carefully finetuning for specific tasks (Dai and Le, 2015; Radford et al., 2018; Howard and Ruder, 2018; Devlin et al., 2019). These models have the advantage that only a few parameters need to be learned from scratch and thus perform surprisingly well even on small amounts of supervised data. Fine-tuned language models do not however work as well for directly predicting answers for CQA (Talmor et al., 2019). In our work, we show how these finetuned language models are more effective when leveraged to generate explanations and empirically prove that they also linguistically capture common sense. 3 Common Sense Explanations (CoS-E) We used Amazon Mechanical Turk (MTurk) to collect explanations f"
P19-1487,N18-1202,0,0.362017,"oS-E)1 . CoS-E contains human explanations in 1 the form of both open-ended natural language explanations as well as highlighted span annotations that represent words selected by humans as important for predicting the right answer (see Table 1). Talmor et al. (2019) show that using Google search to extract context from top 100 result snippets for each of the question and answer choices does not help much in improving the accuracy on CQA trained using even the state-of-the-art reading comprehension model BiDAF++ (Seo et al., 2017) augmented with a self-attention layer and ELMo representations (Peters et al., 2018). In contrast, we leverage a pretrained language model to generate explanations that are useful for commonsense reasoning. We propose Commonsense Auto-Generated Explanations (CAGE) as a framework for generating explanations for CQA. We break down the task of commonsense reasoning into two phases. In the first phase, we provide a CQA example alongside the corresponding CoS-E explanation to a language model. The language model conditions on the question and answer choices from the example and is trained to generate the CoS-E explanation. In the second phase, we use the language model https://git"
P19-1487,D16-1011,0,0.183881,"Missing"
P19-1487,N18-1201,1,0.886933,"Missing"
P19-1487,N19-1421,0,0.589924,"dels for commonsense reasoning. 1 Question: After getting drunk people couldn’t understand him,it was because of his what? Choices: lower standards,slurred speech, or falling down CoS-E: People who are drunk have difficulty speaking. Question: People do what during their time off from work? Choices: take trips, brow shorter, or become hysterical CoS-E: People usually do something relaxing, such as taking trips,when they don’t need to work. Table 1: Examples from our CoS-E dataset. Introduction Commonsense reasoning is a challenging task for modern machine learning methods (Zhong et al., 2018; Talmor et al., 2019). Explanations are a way to verbalize the reasoning that the models learn during training. Common sense Question Answering (CQA) is a multiple-choice question answering dataset proposed for developing natural language processing (NLP) models with commonssense reasoning capabilities (Talmor et al., 2019). Although these efforts have led to progress, it is still unclear how these models perform reasoning and to what extent that reasoning is based on world knowledge. We collect human explanations for commonsense reasoning built on top of CQA and introduce them as Common Sense Explanations (CoS-E)"
P19-1487,W18-5446,0,0.0554676,"Missing"
P19-1487,H89-1033,0,0.788183,"involves predicting the correct story ending from a set of plausible endings (Mostafazadeh et al., 2016) while the Situations with Adversarial Generations (SWAG) involves predicting the next scene based on an initial event (Zellers et al., 2018). Language Modeling based techniques such as the GPT and BERT models get human-level performance on these datasets (Radford et al., 2018; Devlin et al., 2019). They have been less successful on tasks that require clear understanding of how pronouns resolve between sentences and how that interacts with world knowledge. For example, the Winograd Schemas (Winograd, 1972) and challenges derived from that format (Levesque et al., 2012; McCann et al., 2018; Wang et al., 2018) have proven difficult for even the most modern machine learning methods (Trinh and Le, 2018) to achieve near-human performance, but the emphasis on pronoun resolution in those challenges leaves room for exploration of other kinds of commonsense reasoning. CQA is a new dataset that consists of 9500 questions with one correct answer and two distractor answers (Talmor et al., 2019). The authors claim that because all the answer choices are drawn from the same source concept, the dataset requir"
P19-1487,D18-1009,0,0.0407981,"ging v1.11 of CQA with 5 answer choices instead of 3 and so we also included the new version in our results and discussions. 2 Background and Related Work Commonsense reasoning Datasets that require models to learn to predict relations between situations or events in natural language have been introduced in the recent past. The Story Cloze (also referred to as ROC Stories) involves predicting the correct story ending from a set of plausible endings (Mostafazadeh et al., 2016) while the Situations with Adversarial Generations (SWAG) involves predicting the next scene based on an initial event (Zellers et al., 2018). Language Modeling based techniques such as the GPT and BERT models get human-level performance on these datasets (Radford et al., 2018; Devlin et al., 2019). They have been less successful on tasks that require clear understanding of how pronouns resolve between sentences and how that interacts with world knowledge. For example, the Winograd Schemas (Winograd, 1972) and challenges derived from that format (Levesque et al., 2012; McCann et al., 2018; Wang et al., 2018) have proven difficult for even the most modern machine learning methods (Trinh and Le, 2018) to achieve near-human performanc"
Q14-1017,J10-4006,0,0.0265994,"an et al., 2010) which consists of 1000 images, each with 5 descriptions. On all tasks, our model outperforms baselines and related models. 2 Related Work The presented model is connected to several areas of NLP and vision research, each with a large amount of related work to which we can only do some justice given space constraints. Semantic Vector Spaces and Their Compositionality. The dominant approach in semantic vector spaces uses distributional similarities of single words. Often, co-occurrence statistics of a word and its context are used to describe each word (Turney and Pantel, 2010; Baroni and Lenci, 2010), such as tf-idf. Most of the compositionality algorithms and related datasets capture two-word compositions. For instance, (Mitchell and Lapata, 2010) use twoword phrases and analyze similarities computed by vector addition, multiplication and others. Compositionality is an active field of research with many different models and representations being explored (Grefenstette et al., 2013), among many others. We compare to supervised compositional models that 208 can learn task-specific vector representations such as constituency tree recursive neural networks (Socher et al., 2011b; Socher et al"
Q14-1017,de-marneffe-etal-2006-generating,1,0.0740447,"Missing"
Q14-1017,W13-0112,0,0.382139,"an image that fits a sentence description and vice versa. They also give more similar representations to sentences that describe the same image. 1 Introduction Single word vector spaces are widely used (Turney and Pantel, 2010) and successful at classifying single words and capturing their meaning (Collobert and Weston, 2008; Huang et al., 2012; Mikolov et al., 2013). Since words rarely appear in isolation, the task of learning compositional meaning representations for longer phrases has recently received a lot of attention (Mitchell and Lapata, 2010; Socher et al., 2010; Socher et al., 2012; Grefenstette et al., 2013). Similarly, classifying whole images into a fixed set of classes also achieves very high performance (Le et al., 2012; Krizhevsky et al., 2012). However, similar to words, objects in images are often seen in relationships with other objects which are not adequately described by a single label. In this work, we introduce a model, illustrated in Fig. 1, which learns to map sentences and images into a common embedding space in order to be able to retrieve one from the other. We assume word and image representations are first learned in their respective single modalities but finally mapped into a"
Q14-1017,P12-1092,1,0.570804,"ly focus on the action and agents in a sentence. They are better able to abstract from the details of word order and syntactic expression. DT-RNNs outperform other recursive and recurrent neural networks, kernelized CCA and a bag-of-words baseline on the tasks of finding an image that fits a sentence description and vice versa. They also give more similar representations to sentences that describe the same image. 1 Introduction Single word vector spaces are widely used (Turney and Pantel, 2010) and successful at classifying single words and capturing their meaning (Collobert and Weston, 2008; Huang et al., 2012; Mikolov et al., 2013). Since words rarely appear in isolation, the task of learning compositional meaning representations for longer phrases has recently received a lot of attention (Mitchell and Lapata, 2010; Socher et al., 2010; Socher et al., 2012; Grefenstette et al., 2013). Similarly, classifying whole images into a fixed set of classes also achieves very high performance (Le et al., 2012; Krizhevsky et al., 2012). However, similar to words, objects in images are often seen in relationships with other objects which are not adequately described by a single label. In this work, we introdu"
Q14-1017,P13-1088,0,0.0382909,"phrases and analyze similarities computed by vector addition, multiplication and others. Compositionality is an active field of research with many different models and representations being explored (Grefenstette et al., 2013), among many others. We compare to supervised compositional models that 208 can learn task-specific vector representations such as constituency tree recursive neural networks (Socher et al., 2011b; Socher et al., 2011a), chain structured recurrent neural networks and other baselines. Another alternative would be to use CCG trees as a backbone for vector composition (K.M. Hermann, 2013). Multimodal Embeddings. Multimodal embedding methods project data from multiple sources such as sound and video (Ngiam et al., 2011) or images and text. Socher et al. (Socher and Fei-Fei, 2010) project words and image regions into a common space using kernelized canonical correlation analysis to obtain state of the art performance in annotation and segmentation. Similar to our work, they use unsupervised large text corpora to learn semantic word representations. Among other recent work is that by Srivastava and Salakhutdinov (2012) who developed multimodal Deep Boltzmann Machines. Similar to"
Q14-1017,P12-1038,0,0.898854,"ees. We learn to map the outputs of convolutional neural networks applied to images into the same space and can then compare both sentences and images. This allows us to query images with a sentence and give sentence descriptions to images. the visual scene described and to find appropriate images in the learned, multi-modal sentence-image space. Conversely, when given a query image, we would like to find a description that goes beyond a single label by providing a correct sentence describing it, a task that has recently garnered a lot of attention (Farhadi et al., 2010; Ordonez et al., 2011; Kuznetsova et al., 2012). We use the dataset introduced by (Rashtchian et al., 2010) which consists of 1000 images, each with 5 descriptions. On all tasks, our model outperforms baselines and related models. 2 Related Work The presented model is connected to several areas of NLP and vision research, each with a large amount of related work to which we can only do some justice given space constraints. Semantic Vector Spaces and Their Compositionality. The dominant approach in semantic vector spaces uses distributional similarities of single words. Often, co-occurrence statistics of a word and its context are used to d"
Q14-1017,W10-0721,0,0.88407,"works applied to images into the same space and can then compare both sentences and images. This allows us to query images with a sentence and give sentence descriptions to images. the visual scene described and to find appropriate images in the learned, multi-modal sentence-image space. Conversely, when given a query image, we would like to find a description that goes beyond a single label by providing a correct sentence describing it, a task that has recently garnered a lot of attention (Farhadi et al., 2010; Ordonez et al., 2011; Kuznetsova et al., 2012). We use the dataset introduced by (Rashtchian et al., 2010) which consists of 1000 images, each with 5 descriptions. On all tasks, our model outperforms baselines and related models. 2 Related Work The presented model is connected to several areas of NLP and vision research, each with a large amount of related work to which we can only do some justice given space constraints. Semantic Vector Spaces and Their Compositionality. The dominant approach in semantic vector spaces uses distributional similarities of single words. Often, co-occurrence statistics of a word and its context are used to describe each word (Turney and Pantel, 2010; Baroni and Lenci"
Q14-1017,D11-1014,1,0.629328,"mages are often seen in relationships with other objects which are not adequately described by a single label. In this work, we introduce a model, illustrated in Fig. 1, which learns to map sentences and images into a common embedding space in order to be able to retrieve one from the other. We assume word and image representations are first learned in their respective single modalities but finally mapped into a jointly learned multimodal embedding space. Our model for mapping sentences into this space is based on ideas from Recursive Neural Networks (RNNs) (Pollack, 1990; Costa et al., 2003; Socher et al., 2011b). However, unlike all previous RNN models which are based on constituency trees (CTRNNs), our model computes compositional vector representations inside dependency trees. The compositional vectors computed by this new dependency tree RNN (DT-RNN) capture more of the meaning of sentences, where we define meaning in terms of similarity to a “visual representation” of the textual description. DT-RNN induced vector representations of sentences are more robust to changes in the syntactic structure or word order than related models such as CT-RNNs or Recurrent Neural Networks since they naturally"
Q14-1017,D12-1110,1,0.361369,"the tasks of finding an image that fits a sentence description and vice versa. They also give more similar representations to sentences that describe the same image. 1 Introduction Single word vector spaces are widely used (Turney and Pantel, 2010) and successful at classifying single words and capturing their meaning (Collobert and Weston, 2008; Huang et al., 2012; Mikolov et al., 2013). Since words rarely appear in isolation, the task of learning compositional meaning representations for longer phrases has recently received a lot of attention (Mitchell and Lapata, 2010; Socher et al., 2010; Socher et al., 2012; Grefenstette et al., 2013). Similarly, classifying whole images into a fixed set of classes also achieves very high performance (Le et al., 2012; Krizhevsky et al., 2012). However, similar to words, objects in images are often seen in relationships with other objects which are not adequately described by a single label. In this work, we introduce a model, illustrated in Fig. 1, which learns to map sentences and images into a common embedding space in order to be able to retrieve one from the other. We assume word and image representations are first learned in their respective single modaliti"
Q14-1017,P13-1045,1,0.489913,"d Fei-Fei, 2010) project words and image regions into a common space using kernelized canonical correlation analysis to obtain state of the art performance in annotation and segmentation. Similar to our work, they use unsupervised large text corpora to learn semantic word representations. Among other recent work is that by Srivastava and Salakhutdinov (2012) who developed multimodal Deep Boltzmann Machines. Similar to their work, we use techniques from the broad field of deep learning to represent images and words. Recently, single word vector embeddings have been used for zero shot learning (Socher et al., 2013c). Mapping images to word vectors enabled their system to classify images as depicting objects such as ”cat” without seeing any examples of this class. Related work has also been presented at NIPS (Socher et al., 2013b; Frome et al., 2013). This work moves zero-shot learning beyond single categories per image and extends it to unseen phrases and full length sentences, making use of similar ideas of semantic spaces grounded in visual knowledge. Detailed Image Annotation. Interactions between images and texts is a growing research field. Early work in this area includes generating single words"
Q14-1017,W10-0707,0,\N,Missing
Q14-1017,N13-1090,0,\N,Missing
W07-2425,P98-1013,0,0.0260972,"language texts. While syntactic parsers have been intensively studied for decades, broad coverage semantic parsing is a relatively recent research topic. Semantic parsing aims at constructing a semantic representation of a sentence, abstracting from the syntactic form and allowing queries for meaning rather than syntax. That is, semantic relations between concepts are in the focus of interest rather than syntactic relations between words and phrases. A major current line of research for extracting semantic structures from texts is concerned with semantic role labeling. The FrameNet database (Baker et al., 1998) provides an inventory of semantic frames together with a list of lexical units associated with these frames. Semantic parsing then means to choose appropriate semantic frames from the frame inventory depending on the lexical concepts present in the given sentence and to assign frame-specific roles to concepts. A related task has been defined as part of CoNLL 2004 (Carreras and M`arques, 2004). Here, machine learning methods are used to learn a semantic role labeler from an annotated text to extract a fixed set of semantic relations. If one aims at deep semantic parsing, a lexicon containing s"
W07-2425,W04-2412,0,0.0768556,"Missing"
W07-2425,P06-1045,0,0.0130491,"eira et al., 1993; Lin, 1998; Rooth et al., 1999). Here, semantic classes are created by the clustering method rather than assigned to predefined classes in the lexicon; these works also employ the distributional hypothesis, i.e. that similar semantic properties are reflected in similar (syntactic) contexts. Our setup differs from these approaches in that we use a conceptual framework that covers all sorts of nouns rather than concentrating on a small set of domain-specific classes or leaving the definition of the classes to the method. Moreover, we combine three different context types; see (Hagiwara et al., 2006) for a discussion on context combination for synonym acquisition. 2 Semantic Lexicon and Parser This section gives a brief outline of the semantic parsing framework with respect to which our learning task is set up. The task is to automatically extend a semantic lexicon used for semantic parsing by exploiting parses that have been generated on the basis of the already existing lexicon. From these parses we extract three types of syntactic-semantic noun contexts which are then employed to classify unknown nouns on the basis of classified nouns, as explained in more detail in Section 3. 2.1 The"
W07-2425,P98-2127,0,0.0407064,"IDA 2007 Conference Proceedings, pp. 175–182 Richard Socher, Chris Biemann and Rainer Osswald and Charniak, 1998): Conjunctions, lists and appositives being one and noun compounds being the other. Since bootstrapping single categories often leads to category shifts in later steps, (Thelen and Riloff, 2002) use an un-annotated corpus, seed words and a large body of extraction patterns to discover multiple semantic categories like event or human simultaneously. Another related research line is distributional clustering to obtain semantic classes via similar contexts, e.g. (Pereira et al., 1993; Lin, 1998; Rooth et al., 1999). Here, semantic classes are created by the clustering method rather than assigned to predefined classes in the lexicon; these works also employ the distributional hypothesis, i.e. that similar semantic properties are reflected in similar (syntactic) contexts. Our setup differs from these approaches in that we use a conceptual framework that covers all sorts of nouns rather than concentrating on a small set of domain-specific classes or leaving the definition of the classes to the method. Moreover, we combine three different context types; see (Hagiwara et al., 2006) for a"
W07-2425,P93-1024,0,0.171559,"Mare Koit (Eds.) NODALIDA 2007 Conference Proceedings, pp. 175–182 Richard Socher, Chris Biemann and Rainer Osswald and Charniak, 1998): Conjunctions, lists and appositives being one and noun compounds being the other. Since bootstrapping single categories often leads to category shifts in later steps, (Thelen and Riloff, 2002) use an un-annotated corpus, seed words and a large body of extraction patterns to discover multiple semantic categories like event or human simultaneously. Another related research line is distributional clustering to obtain semantic classes via similar contexts, e.g. (Pereira et al., 1993; Lin, 1998; Rooth et al., 1999). Here, semantic classes are created by the clustering method rather than assigned to predefined classes in the lexicon; these works also employ the distributional hypothesis, i.e. that similar semantic properties are reflected in similar (syntactic) contexts. Our setup differs from these approaches in that we use a conceptual framework that covers all sorts of nouns rather than concentrating on a small set of domain-specific classes or leaving the definition of the classes to the method. Moreover, we combine three different context types; see (Hagiwara et al.,"
W07-2425,W97-0313,0,0.0424834,"or semi-automatically extending semantic lexicons are highly needed to overcome the bottleneck and to scale the lexicon to a size where satisfactory coverage can be reached. In this paper, we present a method that enlarges the number of noun entries in the lexicon of a semantic parser for German. 1.1 Related Work Extending a given lexicon with the help of a parser relying on this lexicon can be viewed as a step of a bootstrapping cycle: Lexicon entries of known words are used to obtain entries for previously unknown words by exploiting a parsed corpus. Early bootstrapping approaches such as (Riloff and Shepherd, 1997) were based on few seed words of a semantic category and their nearest neighbor contexts. Higher precision was achieved by separating extraction patterns into two groups by (Roark Joakim Nivre, Heiki-Jaan Kaalep, Kadri Muischnek and Mare Koit (Eds.) NODALIDA 2007 Conference Proceedings, pp. 175–182 Richard Socher, Chris Biemann and Rainer Osswald and Charniak, 1998): Conjunctions, lists and appositives being one and noun compounds being the other. Since bootstrapping single categories often leads to category shifts in later steps, (Thelen and Riloff, 2002) use an un-annotated corpus, seed word"
W07-2425,P98-2182,0,0.0718828,"Missing"
W07-2425,P99-1014,0,0.0218252,"nference Proceedings, pp. 175–182 Richard Socher, Chris Biemann and Rainer Osswald and Charniak, 1998): Conjunctions, lists and appositives being one and noun compounds being the other. Since bootstrapping single categories often leads to category shifts in later steps, (Thelen and Riloff, 2002) use an un-annotated corpus, seed words and a large body of extraction patterns to discover multiple semantic categories like event or human simultaneously. Another related research line is distributional clustering to obtain semantic classes via similar contexts, e.g. (Pereira et al., 1993; Lin, 1998; Rooth et al., 1999). Here, semantic classes are created by the clustering method rather than assigned to predefined classes in the lexicon; these works also employ the distributional hypothesis, i.e. that similar semantic properties are reflected in similar (syntactic) contexts. Our setup differs from these approaches in that we use a conceptual framework that covers all sorts of nouns rather than concentrating on a small set of domain-specific classes or leaving the definition of the classes to the method. Moreover, we combine three different context types; see (Hagiwara et al., 2006) for a discussion on contex"
W07-2425,W02-1028,0,0.0285347,"y bootstrapping approaches such as (Riloff and Shepherd, 1997) were based on few seed words of a semantic category and their nearest neighbor contexts. Higher precision was achieved by separating extraction patterns into two groups by (Roark Joakim Nivre, Heiki-Jaan Kaalep, Kadri Muischnek and Mare Koit (Eds.) NODALIDA 2007 Conference Proceedings, pp. 175–182 Richard Socher, Chris Biemann and Rainer Osswald and Charniak, 1998): Conjunctions, lists and appositives being one and noun compounds being the other. Since bootstrapping single categories often leads to category shifts in later steps, (Thelen and Riloff, 2002) use an un-annotated corpus, seed words and a large body of extraction patterns to discover multiple semantic categories like event or human simultaneously. Another related research line is distributional clustering to obtain semantic classes via similar contexts, e.g. (Pereira et al., 1993; Lin, 1998; Rooth et al., 1999). Here, semantic classes are created by the clustering method rather than assigned to predefined classes in the lexicon; these works also employ the distributional hypothesis, i.e. that similar semantic properties are reflected in similar (syntactic) contexts. Our setup differ"
W07-2425,C98-1013,0,\N,Missing
W07-2425,C98-2122,0,\N,Missing
W13-3512,N06-2001,0,0.651215,"e word structure, and the semantics in grouping related words.2 2 rect morphological inflections was a very central problem in early work in the parallel distributed processing paradigm and criticisms of it (Rumelhart and McClelland, 1986; Plunkett and Marchman, 1991), and later work developed more sophisticated models of morphological structure and meaning (Gasser and Lee, 1990; Gasser, 1994), while not providing a compositional semantics nor working at the scale of what we present. To the best of our knowledge, the work closest to ours in terms of handing unseen words are the factored NLMs (Alexandrescu and Kirchhoff, 2006) and the compositional distributional semantic models (DSMs) (Lazaridou et al., 2013). In the former work, each word is viewed as a vector of features such as stems, morphological tags, and cases, in which a single embedding matrix is used to look up all of these features.3 Though this is a principled way of handling new words in NLMs, the by-product word representations, i.e. the concatenations of factor vectors, do not encode in them the compositional information (they are stored in the NN parameters). Our work does not simply concatenate vectors of morphemes, but rather combines them using"
W13-3512,E03-1009,0,0.0121095,"ds to complement existing ones in an interesting way. 1 Introduction The use of word representations or word clusters pretrained in an unsupervised fashion from lots of text has become a key “secret sauce” for the success of many NLP systems in recent years, across tasks including named entity recognition, part-ofspeech tagging, parsing, and semantic role labeling. This is particularly true in deep neural network models (Collobert et al., 2011), but it is also true in conventional feature-based models (Koo et al., 2008; Ratinov and Roth, 2009). 1 An almost exception is the word clustering of (Clark, 2003), which does have a model of morphology to encourage words ending with the same suffix to appear in the same class, but it still does not capture the relationship between a word and its morphologically derived forms. 104 Proceedings of the Seventeenth Conference on Computational Natural Language Learning, pages 104–113, c Sofia, Bulgaria, August 8-9 2013. 2013 Association for Computational Linguistics an interesting way to learn morphemic semantics and their compositional properties. Our model has the capability of building representations for any new unseen word comprised of known morphemes,"
W13-3512,P12-1092,1,0.666387,"recently such as sentiment analysis at the sentence (Socher et al., 2011c) and document level (Glorot et al., 2011), language modeling (Mnih and Hinton, 2007; Mikolov and Zweig, 2012), paraphrase detection (Socher et al., 2011a), discriminative parsing (Collobert, 2011), and tasks involving semantic relations and compositional meaning of phrases (Socher et al., 2012). Common to many of these works is use of a distributed word representation as the basic input unit. These representations usually capture local cooccurrence statistics but have also been extended to include document-wide context (Huang et al., 2012). Their main advantage is that they can both be learned unsupervisedly as well as be tuned for supervised tasks. In the former training regiment, they are evaluated by how well they can capture human similarity judgments. They have also been shown to perform well as features for supervised tasks, e.g., NER (Turian et al., 2010). While much work has focused on different objective functions for training single and multiword vector representations, very little work has been done to tackle sub-word units and how they can be used to compute syntactic-semantic word vectors. Collobert et al. (2011) e"
W13-3512,P08-1068,0,0.00420093,"ilarity tasks across many datasets, including a new dataset we introduce focused on rare words to complement existing ones in an interesting way. 1 Introduction The use of word representations or word clusters pretrained in an unsupervised fashion from lots of text has become a key “secret sauce” for the success of many NLP systems in recent years, across tasks including named entity recognition, part-ofspeech tagging, parsing, and semantic role labeling. This is particularly true in deep neural network models (Collobert et al., 2011), but it is also true in conventional feature-based models (Koo et al., 2008; Ratinov and Roth, 2009). 1 An almost exception is the word clustering of (Clark, 2003), which does have a model of morphology to encourage words ending with the same suffix to appear in the same class, but it still does not capture the relationship between a word and its morphologically derived forms. 104 Proceedings of the Seventeenth Conference on Computational Natural Language Learning, pages 104–113, c Sofia, Bulgaria, August 8-9 2013. 2013 Association for Computational Linguistics an interesting way to learn morphemic semantics and their compositional properties. Our model has the capab"
W13-3512,P13-1149,0,0.609856,"s was a very central problem in early work in the parallel distributed processing paradigm and criticisms of it (Rumelhart and McClelland, 1986; Plunkett and Marchman, 1991), and later work developed more sophisticated models of morphological structure and meaning (Gasser and Lee, 1990; Gasser, 1994), while not providing a compositional semantics nor working at the scale of what we present. To the best of our knowledge, the work closest to ours in terms of handing unseen words are the factored NLMs (Alexandrescu and Kirchhoff, 2006) and the compositional distributional semantic models (DSMs) (Lazaridou et al., 2013). In the former work, each word is viewed as a vector of features such as stems, morphological tags, and cases, in which a single embedding matrix is used to look up all of these features.3 Though this is a principled way of handling new words in NLMs, the by-product word representations, i.e. the concatenations of factor vectors, do not encode in them the compositional information (they are stored in the NN parameters). Our work does not simply concatenate vectors of morphemes, but rather combines them using RNNs, which captures morphological compositionality. The latter work experimented wit"
W13-3512,P05-1044,0,0.00619239,"been built, the NLM assigns a score for each ngram ni consisting of words x1 , . . . , xn as follows: N ∂J(θ) ∑ ∂s (xi ) = + λθ ∂θ ∂θ i=1 In the case of csmRNN, since the objective function in Eq. (3) is not differentiable, we use the subgradient method (Ratliff et al., 2007) to estimate the objective gradient as: ∂J(θ) = ∂θ s (ni ) = υ ⊤ f (W [x1 ; . . . ; xn ] + b) N ∑ i=1 max{0, 1 − s (ni ) + s (ni )} (3) Here, N is the number of all available ngrams in the training corpus, whereas ni is a “corrupted” ngram created from ni by replacing its last word with a random word similar in spirit to (Smith and Eisner, 2005). Our model parameters are θ = {We , Wm , bm , W , b, υ}. Such a ranking criterion influences the model to assign higher scores to valid ngrams than to 4 i:1−s(ni )+s(ni )&gt;0 − ∂s (ni ) ∂s (ni ) + ∂θ ∂θ Back-propagation through structures (Goller and K¨uchler, 1996) is employed to compute the gradient for each individual cost with similar formulae as in (Socher et al., 2010). Unlike their RNN structures over sentences, where each sentence could have an exponential number of derivations, our morphoRNN structure per word is, in general, deterministic. Each word has a single morphological tree str"
W13-3512,D08-1027,0,0.0216219,"Missing"
W13-3512,N13-1090,0,0.380499,"aptures morphological compositionality. The latter work experimented with different compositional DSMs, originally designed to learn meanings of phrases, to derive representations for complex words, in which the base unit is the morpheme similar to ours. However, their models can only combine a stem with an affix and does not support recursive morpheme composition. It is, however, interesting to compare our neural-based representations with their DSM-derived ones and cross test these models on both our rare word similarity dataset and their nearest neighbor one, which we leave as future work. Mikolov et al. (2013) examined existing word embeddings and showed that these representations already captured meaningful syntactic and semantic regularities such as the singular/plural relation that xapple - xapples ≈ xcar - xcars . However, we believe that these nice relationships will not hold for rare and complex words when their vectors are poorly estimated as we analyze in Section 6. Our model, on the other hand, explicitly represents these regularities through morphological structures of words. Related Work Neural network techniques have found success in several NLP tasks recently such as sentiment analysis"
W13-3512,D11-1014,1,0.596159,"t al., 2003) to hierarchical models (Morin, 2005; Mnih and Hinton, 2009) and recently recurrent neural networks (Mikolov et al., 2010; Mikolov et al., 2011), these approaches treat each full-form word as an independent entity and fail to capture the explicit relationship among morphological variants of a word.1 The fact that morphologically complex words are often rare exacerbates the problem. Though existing clusterings and embeddings represent well frequent words, such as “distinct”, they often badly model rare ones, such as “distinctiveness”. In this work, we use recursive neural networks (Socher et al., 2011b), in a novel way to model morphology and its compositionality. Essentially, we treat each morpheme as a basic unit in the RNNs and construct representations for morphologically complex words on the fly from their morphemes. By training a neural language model (NLM) and integrating RNN structures for complex words, we utilize contextual information in Vector-space word representations have been very successful in recent years at improving performance across a variety of NLP tasks. However, common to most existing work, words are regarded as independent entities without any explicit relationsh"
W13-3512,D12-1110,1,0.144032,"vectors are poorly estimated as we analyze in Section 6. Our model, on the other hand, explicitly represents these regularities through morphological structures of words. Related Work Neural network techniques have found success in several NLP tasks recently such as sentiment analysis at the sentence (Socher et al., 2011c) and document level (Glorot et al., 2011), language modeling (Mnih and Hinton, 2007; Mikolov and Zweig, 2012), paraphrase detection (Socher et al., 2011a), discriminative parsing (Collobert, 2011), and tasks involving semantic relations and compositional meaning of phrases (Socher et al., 2012). Common to many of these works is use of a distributed word representation as the basic input unit. These representations usually capture local cooccurrence statistics but have also been extended to include document-wide context (Huang et al., 2012). Their main advantage is that they can both be learned unsupervisedly as well as be tuned for supervised tasks. In the former training regiment, they are evaluated by how well they can capture human similarity judgments. They have also been shown to perform well as features for supervised tasks, e.g., NER (Turian et al., 2010). While much work has"
W13-3512,P10-1040,0,0.632288,"meaning of phrases (Socher et al., 2012). Common to many of these works is use of a distributed word representation as the basic input unit. These representations usually capture local cooccurrence statistics but have also been extended to include document-wide context (Huang et al., 2012). Their main advantage is that they can both be learned unsupervisedly as well as be tuned for supervised tasks. In the former training regiment, they are evaluated by how well they can capture human similarity judgments. They have also been shown to perform well as features for supervised tasks, e.g., NER (Turian et al., 2010). While much work has focused on different objective functions for training single and multiword vector representations, very little work has been done to tackle sub-word units and how they can be used to compute syntactic-semantic word vectors. Collobert et al. (2011) enhanced word vectors with additional character-level features such as capitalization but still can not recover more detailed semantics for very rare or unseen words, which is the focus of this work. This is somewhat ironic, since working out cor2 The rare word dataset and trained word vectors can be found at http://nlp.stanford"
W13-3512,W09-1119,0,0.0407573,"ss many datasets, including a new dataset we introduce focused on rare words to complement existing ones in an interesting way. 1 Introduction The use of word representations or word clusters pretrained in an unsupervised fashion from lots of text has become a key “secret sauce” for the success of many NLP systems in recent years, across tasks including named entity recognition, part-ofspeech tagging, parsing, and semantic role labeling. This is particularly true in deep neural network models (Collobert et al., 2011), but it is also true in conventional feature-based models (Koo et al., 2008; Ratinov and Roth, 2009). 1 An almost exception is the word clustering of (Clark, 2003), which does have a model of morphology to encourage words ending with the same suffix to appear in the same class, but it still does not capture the relationship between a word and its morphologically derived forms. 104 Proceedings of the Seventeenth Conference on Computational Natural Language Learning, pages 104–113, c Sofia, Bulgaria, August 8-9 2013. 2013 Association for Computational Linguistics an interesting way to learn morphemic semantics and their compositional properties. Our model has the capability of building represe"
W13-3512,N10-1013,0,0.196558,"h makes it a challenging dataset. WS353 MC RG SCWS∗ RW All words 0 |0 / 9 / 87 / 341 0 |0 / 1 / 17 / 21 0 |0 / 4 / 22 / 22 26 |2 / 140 / 472 / 1063 801 |41 / 676 / 719 / 714 Complex words 0 |0 / 1 / 6 / 10 0|0/0/0/0 0|0/0/0/0 8 |2 / 22 / 44 / 45 621 |34 / 311 / 238 / 103 Table 4: Word distribution by frequencies – distinct words in each dataset are grouped based on frequencies and counts are reported for the following bins : unknown |[1, 100] / [101, 1000] / [1001, 10000] / [10001, ∞). We report counts for all words in each dataset as well as complex ones. 5.1 Word Similarity Task Similar to (Reisinger and Mooney, 2010) and (Huang et al., 2012), we evaluate the quality of our morphologically-aware embeddings on the popular WordSim-353 dataset (Finkelstein et al., 2002), WS353 for short. In this task, we compare correlations between the similarity scores given by our models and those rated by human. To avoid overfitting our models to a single dataset, we benchmark our models on a variety of others including MC (Miller and Charles, 1991), RG (Rubenstein and Goodenough, 1965), SCWS∗11 (Huang et al., 2012), and our new rare word (RW) dataset (details in §5.1.1). Information about these datasets are summarized in"
W13-3512,P94-1039,0,\N,Missing
W16-2308,P15-1001,0,0.0329023,"ort-term memory (LSTM) RNNs. The attention mechanism in our four-layer model is what Luong (2015) describes as “Global attention (dot)”; the mechanism in our five-layer Y-LSTM model is described in Section 2.1. Every NMT system must contend with the problem of unbounded output vocabulary: systems that restrict possible output words to the most common 50,000 or 100,000 that can fit comfortably in a softmax classifier will perform poorly due to large numbers of “out-of-vocabulary” or “unknown” outputs. Even models that can produce every word found in the training corpus for the target language (Jean et al., 2015) may be unable to output words found only in the test corpus. There are three main techniques for achieving fully open-ended decoder output. Models may use computed alignments between source and target sentences to directly copy or transform a word from the input sentence whose corresponding translation is not present in the vocabulary (Luong et al., 2015) or they may conduct sentence tokenization at the level of individual characters (Ling et al., 2015) or subword units such as morphemes (Sennrich et al., 2015b). The latter techniques allow the decoder to construct words it has not previously"
W16-2308,D13-1176,0,0.0278882,"aMind’s submissions to WMT ’16 seek to push the state of the art in one such task, English→German newsdomain translation. We integrate promising recent developments in NMT, including subword splitting and back-translation for monolingual data augmentation, and introduce the Y-LSTM, a novel neural translation architecture. 1 Introduction The field of Neural Machine Translation (NMT), which seeks to use end-to-end neural networks to translate natural language text, has existed for only three years. In that time, researchers have explored architectures ranging from convolutional neural networks (Kalchbrenner and Blunsom, 2013) to recurrent neural networks (Chung et al., 2014) to attentional models (Bahdanau et al., 2015; Luong et al., 2015) and achieved better performance than traditional statistical or syntax-based MT techniques on many language pairs. NMT models first achieved state-of-the-art performance on the WMT English→German news-domain task in 2015 (Luong et al., 2015) and subsequent improvements have been reported since then (Sennrich et al., 2015a; Li and Jurafsky, 2016). The problem of machine translation is fundamentally a sequence-to-sequence transduction task, and most approaches have been based on a"
W16-2308,P07-2045,0,0.00828092,"Missing"
W16-2308,D15-1166,0,0.260907,"00 or 100,000 that can fit comfortably in a softmax classifier will perform poorly due to large numbers of “out-of-vocabulary” or “unknown” outputs. Even models that can produce every word found in the training corpus for the target language (Jean et al., 2015) may be unable to output words found only in the test corpus. There are three main techniques for achieving fully open-ended decoder output. Models may use computed alignments between source and target sentences to directly copy or transform a word from the input sentence whose corresponding translation is not present in the vocabulary (Luong et al., 2015) or they may conduct sentence tokenization at the level of individual characters (Ling et al., 2015) or subword units such as morphemes (Sennrich et al., 2015b). The latter techniques allow the decoder to construct words it has not previously encountered out of known characters or morphemes; we apply the subword splitting strategy using Morfessor 2.0, an unsupervised morpheme segmentation model (Virpioja et Neural Machine Translation (NMT) systems, introduced only in 2013, have achieved state of the art results in many MT tasks. MetaMind’s submissions to WMT ’16 seek to push the state of the a"
W16-2308,W14-4012,0,0.157838,"Missing"
W17-2631,D13-1170,1,0.0117617,"reshold method for reducing computational cost and one that uses a secondary decision network. 1 1 True positive False positive negative description (more figurative language) Simple negative description (lots of adjectives) Complex linguistics (negations, contrastive conjunctions) Simple positive description (lots of adjectives) positive description (more figurative language) Figure 1: Illustration with t-SNE (van der Maaten and Hinton, 2008) of the activations of the last hidden layer in a computationally cheap bag-ofwords (BoW) model on the binary Stanford Sentiment Treebank (SST) dataset (Socher et al., 2013). Each data point is one sentence, while the plot has been annotated with qualitative descriptions. Introduction Deep learning models are getting bigger, better and more computationally expensive in the quest to match or exceed human performance (Wu et al., 2016; He et al., 2015; Amodei et al., 2015; Silver et al., 2016). With advances like the sparselygated mixture of experts (Shazeer et al., 2017), pointer sentinel (Merity et al., 2016), or attention mechanisms (Bahdanau et al., 2015), models for natural language processing are growing more complex in order to solve harder linguistic problem"
W17-2631,D14-1162,1,\N,Missing
W17-4303,P16-1139,0,0.0195463,"pproach aims to jointly learn the task at hand and relevant aspects of linguistic hierarchy, inducing from an unannotated training dataset parse trees that may or may not correspond to treebank annotation practices (Wu, 1997; Chiang, 2005). Most deep learning models for NLP that aim to make use of linguistic hierarchy integrate an external parser, either to prescribe the recursive structure of the neural network (Pollack, 1990; Goller and K¨uchler, 1996; Socher et al., 2013) or to provide a supervision signal or training data for a network that predicts its own structure (Socher et al., 2010; Bowman et al., 2016; Dyer et al., 2016b). But some recently described neural network models take the second approach and treat hierarchical structure as a latent variable, applying infer12 Proceedings of the 2nd Workshop on Structured Prediction for Natural Language Processing, pages 12–16 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics level baseline. Analysis of the latent trees produced by the encoder and decoder shows that the model learns a reasonable segmentation and shallow parse, and most phrase-level constituents constructed while ingesting the German input se"
W17-4303,P05-1033,0,0.600624,"s Bradbury and Richard Socher james.bradbury@salesforce.com rsocher@salesforce.com Abstract ence over graph-based conditional random fields (Kim et al., 2017), the straight-through estimator (Chung et al., 2017), or policy gradient reinforcement learning (Yogatama et al., 2017) to work around the inapplicability of gradient-based learning to problems with discrete latent states. For the task of machine translation, syntactically-informed models have shown promise both inside and outside the deep learning context, with hierarchical phrase-based models frequently outperforming traditional ones (Chiang, 2005) and neural MT models augmented with morphosyntactic input features (Sennrich and Haddow, 2016; Nadejde et al., 2017), a tree-structured encoder (Eriguchi et al., 2016; Hashimoto and Tsuruoka, 2017), and a jointly trained parser (Eriguchi et al., 2017) each outperforming purely-sequential baselines. Drawing on many of these precedents, we introduce an attentional neural machine translation model whose encoder and decoder components are both tree-structured neural networks that predict their own constituency structure as they consume or emit text. The encoder and decoder networks are variants o"
W17-4303,W16-2209,0,0.0169344,"Abstract ence over graph-based conditional random fields (Kim et al., 2017), the straight-through estimator (Chung et al., 2017), or policy gradient reinforcement learning (Yogatama et al., 2017) to work around the inapplicability of gradient-based learning to problems with discrete latent states. For the task of machine translation, syntactically-informed models have shown promise both inside and outside the deep learning context, with hierarchical phrase-based models frequently outperforming traditional ones (Chiang, 2005) and neural MT models augmented with morphosyntactic input features (Sennrich and Haddow, 2016; Nadejde et al., 2017), a tree-structured encoder (Eriguchi et al., 2016; Hashimoto and Tsuruoka, 2017), and a jointly trained parser (Eriguchi et al., 2017) each outperforming purely-sequential baselines. Drawing on many of these precedents, we introduce an attentional neural machine translation model whose encoder and decoder components are both tree-structured neural networks that predict their own constituency structure as they consume or emit text. The encoder and decoder networks are variants of the RNNG model introduced by Dyer et al. (2016b), allowing tree structures of unconstrained"
W17-4303,N16-1024,0,0.493392,"with morphosyntactic input features (Sennrich and Haddow, 2016; Nadejde et al., 2017), a tree-structured encoder (Eriguchi et al., 2016; Hashimoto and Tsuruoka, 2017), and a jointly trained parser (Eriguchi et al., 2017) each outperforming purely-sequential baselines. Drawing on many of these precedents, we introduce an attentional neural machine translation model whose encoder and decoder components are both tree-structured neural networks that predict their own constituency structure as they consume or emit text. The encoder and decoder networks are variants of the RNNG model introduced by Dyer et al. (2016b), allowing tree structures of unconstrained arity, while text is ingested at the character level, allowing the model to discover and make use of structure within words. The parsing decisions of the encoder and decoder RNNGs are parameterized by a stochastic policy trained using a weighted sum of two objectives: a language model loss term that rewards predicting the next character with high likelihood, and a tree attention term that rewards one-to-one attentional correspondence between constituents in the encoder and decoder. We evaluate this model on the German-English language pair of the f"
W17-4303,P16-1078,0,0.035277,"the straight-through estimator (Chung et al., 2017), or policy gradient reinforcement learning (Yogatama et al., 2017) to work around the inapplicability of gradient-based learning to problems with discrete latent states. For the task of machine translation, syntactically-informed models have shown promise both inside and outside the deep learning context, with hierarchical phrase-based models frequently outperforming traditional ones (Chiang, 2005) and neural MT models augmented with morphosyntactic input features (Sennrich and Haddow, 2016; Nadejde et al., 2017), a tree-structured encoder (Eriguchi et al., 2016; Hashimoto and Tsuruoka, 2017), and a jointly trained parser (Eriguchi et al., 2017) each outperforming purely-sequential baselines. Drawing on many of these precedents, we introduce an attentional neural machine translation model whose encoder and decoder components are both tree-structured neural networks that predict their own constituency structure as they consume or emit text. The encoder and decoder networks are variants of the RNNG model introduced by Dyer et al. (2016b), allowing tree structures of unconstrained arity, while text is ingested at the character level, allowing the model"
W17-4303,D13-1170,1,0.0134119,"tasks make use of the output of a self-contained parser system trained from a human-annotated treebank (Huang et al., 2006). An alternative approach aims to jointly learn the task at hand and relevant aspects of linguistic hierarchy, inducing from an unannotated training dataset parse trees that may or may not correspond to treebank annotation practices (Wu, 1997; Chiang, 2005). Most deep learning models for NLP that aim to make use of linguistic hierarchy integrate an external parser, either to prescribe the recursive structure of the neural network (Pollack, 1990; Goller and K¨uchler, 1996; Socher et al., 2013) or to provide a supervision signal or training data for a network that predicts its own structure (Socher et al., 2010; Bowman et al., 2016; Dyer et al., 2016b). But some recently described neural network models take the second approach and treat hierarchical structure as a latent variable, applying infer12 Proceedings of the 2nd Workshop on Structured Prediction for Natural Language Processing, pages 12–16 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics level baseline. Analysis of the latent trees produced by the encoder and decoder shows that the"
W17-4303,P17-2012,0,0.114331,"nt learning (Yogatama et al., 2017) to work around the inapplicability of gradient-based learning to problems with discrete latent states. For the task of machine translation, syntactically-informed models have shown promise both inside and outside the deep learning context, with hierarchical phrase-based models frequently outperforming traditional ones (Chiang, 2005) and neural MT models augmented with morphosyntactic input features (Sennrich and Haddow, 2016; Nadejde et al., 2017), a tree-structured encoder (Eriguchi et al., 2016; Hashimoto and Tsuruoka, 2017), and a jointly trained parser (Eriguchi et al., 2017) each outperforming purely-sequential baselines. Drawing on many of these precedents, we introduce an attentional neural machine translation model whose encoder and decoder components are both tree-structured neural networks that predict their own constituency structure as they consume or emit text. The encoder and decoder networks are variants of the RNNG model introduced by Dyer et al. (2016b), allowing tree structures of unconstrained arity, while text is ingested at the character level, allowing the model to discover and make use of structure within words. The parsing decisions of the enco"
W17-4303,W16-2346,0,0.0146785,"lines can replace exponential moving averages, pure reinforcement learning can replace teacher forcing, and beam search can be used in place of greedy inference. Solutions to the translation pathologies presented in Section 3 are likely more complex, although one possible approach would leverage variational inference using a teacher model that can see the buffer and helps train a stack-only student model. represented by a shaded rectangle whose projections on the x and y axes cover the encoder and decoder constituents respectively. tual component of the WMT Multimodal Translation shared task (Specia et al., 2016). An attentional sequence-to-sequence model with two layers and 384 hidden units from the OpenNMT project (Klein et al., 2017) was run at the character level as a baseline, obtaining 32.0 test BLEU with greedy inference. Our model with the same hidden size and greedy inference achieves test BLEU of 28.5 after removing repeated bigrams. We implemented the model in PyTorch, benefiting from its strong support for dynamic and stochastic computation graphs, and trained with batch size 10 and the Adam optimizer (Kingma and Ba, 2015) with early stopping after 12 epochs. Character embeddings and the e"
W17-4303,J97-3002,0,0.0761118,"atasets with no explicit segmentation or parse annotation, the model learns a plausible segmentation and shallow parse, obtaining performance close to an attentional baseline. 1 Introduction Many efforts to exploit linguistic hierarchy in NLP tasks make use of the output of a self-contained parser system trained from a human-annotated treebank (Huang et al., 2006). An alternative approach aims to jointly learn the task at hand and relevant aspects of linguistic hierarchy, inducing from an unannotated training dataset parse trees that may or may not correspond to treebank annotation practices (Wu, 1997; Chiang, 2005). Most deep learning models for NLP that aim to make use of linguistic hierarchy integrate an external parser, either to prescribe the recursive structure of the neural network (Pollack, 1990; Goller and K¨uchler, 1996; Socher et al., 2013) or to provide a supervision signal or training data for a network that predicts its own structure (Socher et al., 2010; Bowman et al., 2016; Dyer et al., 2016b). But some recently described neural network models take the second approach and treat hierarchical structure as a latent variable, applying infer12 Proceedings of the 2nd Workshop on"
W17-4303,D17-1012,0,0.0955153,"Missing"
W17-4303,W06-3601,0,0.0149952,"slation, pairing a recurrent neural network grammar encoder with a novel attentional RNNG decoder and applying policy gradient reinforcement learning to induce unsupervised tree structures on both the source and target. When trained on character-level datasets with no explicit segmentation or parse annotation, the model learns a plausible segmentation and shallow parse, obtaining performance close to an attentional baseline. 1 Introduction Many efforts to exploit linguistic hierarchy in NLP tasks make use of the output of a self-contained parser system trained from a human-annotated treebank (Huang et al., 2006). An alternative approach aims to jointly learn the task at hand and relevant aspects of linguistic hierarchy, inducing from an unannotated training dataset parse trees that may or may not correspond to treebank annotation practices (Wu, 1997; Chiang, 2005). Most deep learning models for NLP that aim to make use of linguistic hierarchy integrate an external parser, either to prescribe the recursive structure of the neural network (Pollack, 1990; Goller and K¨uchler, 1996; Socher et al., 2013) or to provide a supervision signal or training data for a network that predicts its own structure (Soc"
W17-4303,P17-4012,0,0.0611044,"Missing"
W17-4303,E17-1117,0,0.140449,"ation for Computational Linguistics level baseline. Analysis of the latent trees produced by the encoder and decoder shows that the model learns a reasonable segmentation and shallow parse, and most phrase-level constituents constructed while ingesting the German input sentence correspond meaningfully to constituents built while generating the English output. 2 2.1 x0 h1 x 2 E x0 Model h1 x 2 E x0 h3 x 4 i S e T n h i m _ r t x5 ... NT S Encoder/Decoder Architecture The model consists of a coupled encoder and decoder, where the encoder is a modified stackonly recurrent neural network grammar (Kuncoro et al., 2017) and the decoder is a stack-only RNNG augmented with constituent-level attention. An RNNG is a top-down transition-based model that jointly builds a sentence representation and parse tree, representing the parser state with a StackLSTM and using a bidirectional LSTM as a constituent composition function. Our implementation is detailed in Figure 1, and differs from Dyer et al. (2016b) in that it lacks separate newnonterminal tokens for different phrase types, and thus does not include the phrase type as an input to the composition function. Instead, the values of xi for the encoder are fixed to"
W17-4303,D15-1166,0,0.0862597,"on plots are shown in Figure 2. Figure 2b demonstrates a common pathology of the model, where a phrasal encoder constituent would be attended to during decoding of the head word of the corresponding decoder constituent, while the head word of the encoder constituent would be attended to during decoding of the decoder constituent corresponding to the whole phrase. Another common pathology is repeated sentence fragments in the translation, which are likely generated because the model cannot condition future attention directly on past attention weights (the “input feeding” approach introduced by Luong et al. (2015)). Translation quality also suffers because of our use of a stack-only RNNG, which we chose because an RNNG with both stack and buffer inputs is incompatible with a language model loss. During encoding, the model must decide at the very beginning of the sentence how deeply to embed the first character. But with a stack-only RNNG, it must make this decision randomly, since it isn’t able to use the buffer representation—which contains the entire sentence. A boy in a group of seated children smile for the camera. Zwei Inderinnen sehen Stoff durch. Ein Junge in einer Gruppe von sitzenden Kindern l"
W19-5212,P07-2045,0,0.00670993,"Missing"
W19-5212,P17-2021,0,0.0361063,"ext translation without XML. To compute the BLEU scores, we use language-specific tokenizers; for example, we use Kytea (Neubig et al., 2011) for Simplified Chinese and Japanese, and the Moses (Koehn et al., 2007) tokenizer for English, Dutch, Finnish, French, German, and Russian. 3.1 The task in our dataset is to translate text with structured information, and therefore we consider using syntax-based NMT models. A possible approach is incorporating parse trees or parsing algorithms into NMT models (Eriguchi et al., 2016, 2017), and another is using sequential models on linearized structures (Aharoni and Goldberg, 2017). We employ the latter approach to incorporate source-side and target-side XML structures, and note that this allows using standard sequenceto-sequence models without modification. We have a set of parallel text segments for a language pair (X , Y), and the task is translating a text segment x ∈ X to another y ∈ Y. Each x in the dataset is represented with a sequence of tokens including some XML tags: x = [x1 , x2 , . . . , xN ], where N is the length of the sequence. Its corresponding reference y is also represented with a sequence of tokens: y = [y1 , y2 , . . . , yM ], where M is the sequen"
W19-5212,D18-2012,0,0.0151095,"recover the NE&NUM scores, especially for the recall. We have also observed that using beam search, which improves BLEU scores, degrades the NE&NUM scores. A lesson learned from these results is that work to improve BLEU scores can sometimes lead to degradation of other important metrics. This section describes our experimental settings. We will release the preprocessing scripts and the training code (implemented with PyTorch) upon publication. More details are described in the supplementary material. 4.1 Model Configurations Tokenization and Detokenization We used the SentencePiece toolkit (Kudo and Richardson, 2018) for sub-word tokenization and detokenization for the NMT outputs. Without XML tags If we remove all the XML tags from our dataset, the task becomes a plain MT task. We carried out our baseline experiments for the plain text translation task, and for each language pair we trained a joint SentencePiece model to obtain its shared sub-word vocabulary. For training each NMT model, we used training examples whose maximum token length is 100. With XML tags For our XML-based experiments, we also trained a joint SentencePiece model for each language pair, where one important note is that all the XML t"
W19-5212,N18-1118,0,0.017515,"heir tail4 has text, to avoid missing phrases within sentences. Next, we remove the root tag from each translation pair, because the correspondence is obvious. We also remove fine-grained information such as attributes in the XML tags for the dataset; from the viewpoint of real-world usage, we can recover (or copy) the missing information as a post-processing step. As a result of this process, a translation pair can consist of multiple sentences as shown in Example (c) of Figure 1. We do not split them into single sentences, considering a recent trend of context-sensitive machine translation (Bawden et al., 2018; M¨uller et al., 2018; Zhang et al., 2018; Miculicich et al., 2018). One can use split sentences for training a model, but an important note is that there is no guarantee that all the internal sentences are perfectly aligned. We note that this structure-based alignment process means we do not rely on statistical alignment models to construct our parallel datasets. Text alignment Figure 3 shows how to extract parallel text segments based on the tag categorization. There are three aligned translatable tags, and they result in three separate translation pairs. The note tag is translatable, so th"
W19-5212,P18-2059,0,0.0357865,"Missing"
W19-5212,P14-5010,0,0.00428096,"Missing"
W19-5212,P16-1078,1,0.845706,"then evaluate BLEU. The metric is compatible with the case where we use the dataset for plain text translation without XML. To compute the BLEU scores, we use language-specific tokenizers; for example, we use Kytea (Neubig et al., 2011) for Simplified Chinese and Japanese, and the Moses (Koehn et al., 2007) tokenizer for English, Dutch, Finnish, French, German, and Russian. 3.1 The task in our dataset is to translate text with structured information, and therefore we consider using syntax-based NMT models. A possible approach is incorporating parse trees or parsing algorithms into NMT models (Eriguchi et al., 2016, 2017), and another is using sequential models on linearized structures (Aharoni and Goldberg, 2017). We employ the latter approach to incorporate source-side and target-side XML structures, and note that this allows using standard sequenceto-sequence models without modification. We have a set of parallel text segments for a language pair (X , Y), and the task is translating a text segment x ∈ X to another y ∈ Y. Each x in the dataset is represented with a sequence of tokens including some XML tags: x = [x1 , x2 , . . . , xN ], where N is the length of the sequence. Its corresponding referenc"
W19-5212,D18-1050,0,0.0617295,"r services for new markets, and human professionals typically perform the translation with the help of a translation memory (Silvestre Baquero and Mitkov, 2017) to increase efficiency and maintain consistent termiIntroduction Machine translation is a fundamental research area in the field of natural language processing (NLP). To build a machine learning-based translation system, we usually need a large amount of bilingually-aligned text segments. Examples of widely-used datasets are those included in WMT (Bojar et al., 2018) and LDC1 , while new evaluation datasets are being actively created (Michel and Neubig, 2018; Bawden et al., ∗ 1 Now at Google Brain. https://www.ldc.upenn.edu/ 116 Proceedings of the Fourth Conference on Machine Translation (WMT), Volume 1: Research Papers, pages 116–127 c Florence, Italy, August 1-2, 2019. 2019 Association for Computational Linguistics use of structured formatting (using XML) to convey information to readers, so the translators have aimed to ensure consistency of formatting and markup structure, not just text content, between languages. nology. Explicitly handling such structured text can help bring the benefits of state-of-the-art machine translation models to add"
W19-5212,P17-2012,0,0.0273173,"Missing"
W19-5212,D18-1325,0,0.0286765,"Next, we remove the root tag from each translation pair, because the correspondence is obvious. We also remove fine-grained information such as attributes in the XML tags for the dataset; from the viewpoint of real-world usage, we can recover (or copy) the missing information as a post-processing step. As a result of this process, a translation pair can consist of multiple sentences as shown in Example (c) of Figure 1. We do not split them into single sentences, considering a recent trend of context-sensitive machine translation (Bawden et al., 2018; M¨uller et al., 2018; Zhang et al., 2018; Miculicich et al., 2018). One can use split sentences for training a model, but an important note is that there is no guarantee that all the internal sentences are perfectly aligned. We note that this structure-based alignment process means we do not rely on statistical alignment models to construct our parallel datasets. Text alignment Figure 3 shows how to extract parallel text segments based on the tag categorization. There are three aligned translatable tags, and they result in three separate translation pairs. The note tag is translatable, so the entire element is 4 For example, the tail of the xref tag in the E"
W19-5212,W18-6307,0,0.0431427,"Missing"
W19-5212,P11-2093,0,0.0190521,"nclude the most widely-used metric, BLEU, without XML tags. 119 how to handle our dataset with a sequential NMT model. We then propose a simple constrained beam search for accurately generating XML structures conditioned by source information. We further incorporate multiple copy mechanisms to strengthen the baselines. That is, we remove all the XML tags covered by our dataset and then evaluate BLEU. The metric is compatible with the case where we use the dataset for plain text translation without XML. To compute the BLEU scores, we use language-specific tokenizers; for example, we use Kytea (Neubig et al., 2011) for Simplified Chinese and Japanese, and the Moses (Koehn et al., 2007) tokenizer for English, Dutch, Finnish, French, German, and Russian. 3.1 The task in our dataset is to translate text with structured information, and therefore we consider using syntax-based NMT models. A possible approach is incorporating parse trees or parsing algorithms into NMT models (Eriguchi et al., 2016, 2017), and another is using sequential models on linearized structures (Aharoni and Goldberg, 2017). We employ the latter approach to incorporate source-side and target-side XML structures, and note that this allo"
W19-5212,Q17-1024,0,0.0919077,"Missing"
W19-5212,D16-1244,0,0.0477508,"Missing"
W19-5212,E17-2025,0,0.0197621,"y adding a “null” token in natural language inference (Parikh et al., 2016). We then define attention scores between y hK (yj ) and the expanded c: a(j, i) = score(hyK (yj ), ci , c), where the normalized scoring function score is implemented as a singlehead attention model proposed in Vaswani et al. (2017). If the next reference token yj+1 is not included in the copy target sequence, the loss function is defined as follows: (3) j=1 where we assume that y1 is a special token BOS to indicate the beginning of the sequence, and yM is an end-of-sequence token EOS. Following Inan et al. (2017) and Press and Wolf (2017), we use W as an embedding matrix, and we share the single vocabulary V for both X and Y. That is, each of v(xi ) or v(yj ) is equivalent to a row vector in W . 3.2 XML-Constrained Beam Search At test time, standard sequence-to-sequence generation methods do not always output valid XML structures, and even if an output is a valid XML structure, it does not always match the tag set of its source-side XML structure. To generate source-conditioned XML structures as accurately as possible, we propose a simple constrained beam search method. We add three constrains to a standard beam search method."
W19-5212,P17-1099,0,0.397148,"d as hy0 (yj ) = We use NMT models to provide competitive baselines for our dataset. This section first describes 120 √ d · v(yj ) + e(j). For more details about the parameterized functions f and g, and the positional embeddings, please refer to Vaswani et al. (2017). Then hyK (yj ) is used to predict the next token w by a softmax layer: pg (w|x, y≤j ) = softmax(W hyK (yj ) + b), where W ∈ R|V|×d is a weight matrix, b ∈ R|V |is a bias vector, and V is the vocabulary. The loss function is defined as follows: L(x, y) = − M −1 X log pg (w = yj+1 |x, y≤j ), the general idea of the pointer used in See et al. (2017). For the sake of discrete decisions, we reformulate the pointer method. Following the previous work, we have a sequence of tokens which are targets of our pointer method: c = [c(z1 ), c(z2 ), . . . , c(zU )], where c(zi ) ∈ Rd is a vector representation of the i-th token zi , and U is the sequence length. As in Section 3.1, we have hyK (yj ) to predict the (j + 1)-th token. Before defining an attention mechanism between hyK (yj ) and c, we append a parameterized vector c(z0 ) = c0 to c. We expect c0 to be responsible for decisions of “not copying” tokens, and the idea is inspired by adding a"
W19-5212,silvestre-baquero-mitkov-2017-translation,0,0.140085,"n the Web, is not always stored as plain text, but often wrapped with markup languages to incorporate document structure and metadata such as formatting information. Many companies and software platforms provide online help as Web documents, often translated into different languages to deliver useful information to people in different countries. Translating such Web-structured text is a major component of the process by which companies localize their software or services for new markets, and human professionals typically perform the translation with the help of a translation memory (Silvestre Baquero and Mitkov, 2017) to increase efficiency and maintain consistent termiIntroduction Machine translation is a fundamental research area in the field of natural language processing (NLP). To build a machine learning-based translation system, we usually need a large amount of bilingually-aligned text segments. Examples of widely-used datasets are those included in WMT (Bojar et al., 2018) and LDC1 , while new evaluation datasets are being actively created (Michel and Neubig, 2018; Bawden et al., ∗ 1 Now at Google Brain. https://www.ldc.upenn.edu/ 116 Proceedings of the Fourth Conference on Machine Translation (WMT"
W19-5212,I17-1038,0,0.0142075,"nolingual corpora. Can MT help the localization process? In general, it is encouraging to observe many “4” scores in Figure 8. However, one important note is that it takes significant amount of time for the translators to verify the NMT outputs are good enough. That is, having better scored NMT outputs does not necessarily lead to improving the productivity of the translators; in other words, we need to take into account the time for the quality verification when we consider using our NMT system for that purpose. Previous work has investigated the effectiveness of NMT models for post-editing (Skadina and Pinnis, 2017), but it has not yet been investigated whether using NMT models can improve the translators’ productivity alongside the use of a well-constructed translation memory (Silvestre Baquero and Mitkov, 2017). Therefore, our future work is investigating the effectiveness of using the NMT models in the real-world localization process where a translation memory is available. 6 7 Conclusion We have presented our new dataset for XMLstructured text translation. Our dataset covers 17 languages each of which can be either source or target of machine translation. The dataset is of high quality because it con"
W19-5212,D18-1049,0,0.0488915,"Missing"
