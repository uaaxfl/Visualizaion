2008.amta-papers.16,2003.mtsummit-papers.6,0,0.920264,"achine translation. Most of this effort has been directed at the translation model rather than the language model; however, the level of syntactic divergence in parallel text makes it difficult to reliably learn syntax-based translation rules. Focusing on syntax-based approaches within the language model might be a more direct way to address the poor grammatically of most machine translation output. In previous work on syntax-based language modeling for machine translation, Wu and Wong (1998) included the score of an unlexicalized probabilistic context-free grammar (PCFG) in an ITG framework. Charniak et al. (2003) rescored a tree-tostring translation forest with a lexicalized parser and found more grammatical output despite a lower BLEU score (compared to an ngram version). More recently, Shen et al. (2008) used a trigram dependency model, with quite good results. Common to 2 Motivation From a grammatical standpoint, the output of most machine translation systems is very poor. Part of the reason may be that most systems do not incorporate syntax-based language models. The nominal task of the language model is to guide the search (decoding) procedure towards grammatical output, yet most systems use ngra"
2008.amta-papers.16,A00-2018,0,0.113697,"ith the unioned alignment and fewer than eight words on either side, along with the counts. Probabilities were set to relative frequency. We then combined the two tables by linearly interpolating them, experimenting with different weights until we found one that produced the best results on the development set. The ngram language model was trained on the English side of this parallel corpus using SRILM (Stolcke, 2002). The parsers were trained the 49,208 trees from the Penn Treebank plus parses of most of the English side of our parallel data, which was automatically parsed with the parser of Charniak (2000). With both parsers, we treated commas, colons, and periods as if they were just regular words in the vocabulary. Quotation marks were treated as an unknown word, and we used the same set of fifty bins (computed based on word surface features) for unknown words used in Petrov et al. (2006). Our development data consisted of all Chinese sentences with twenty or fewer words from the NIST 2002 evaluation (371 sentences). The parameters of our log-linear model (which includes weights for the ngram and parser models, along with a length bonus) were set with a hill-climbing procedure on the developm"
2008.amta-papers.16,J07-2003,0,0.0475281,"y different component models with overlapping responsibilities. Most recent syntax-based systems put much of what might be considered the language model’s responsibility into the translation model, by learning translation rules that produce structured target-language output from (flat) input phrases. An example is Galley et al. (2006), in which target language structures are linguistically motivated treebank parse fragments with an extended domain of locality, allowing, for example, a phrasal translation pair to specify for (optionally lexicalized) argument structure. In a different approach, Chiang (2007) used a generic (linguistically uninformed) grammar whose translation rules permitted lexicalized reorderings between the source and target languages. More recently, Shen et al. (2008) extended Chiang’s system to learn pieces of dependency structure along with the rules. These approaches (summarized in Figure 1) have shown significant progress, but there are reasons why we might want to construct target-side syntax independent of the translation channel. One problem faced by tree-to-string systems is syntactic divergence. Translation and its evaluation are not welldefined tasks; at a high leve"
2008.amta-papers.16,P97-1003,0,0.0932663,"lation channel. This paper is distinct from previous work in that we decouple the structure of the target language tree from that of the synchronous grammar tree. Instead of extracting fragments of parse tree along with the phrase table and learning syntax-motivated reorderings, we start with simple phrase pairs and build the target-language structure at decoding time. We compare two syntax-based language models in the context of a fairly simple syntactic translation model, as a means of isolating the contributions and potential of these language models. The first is the statistical parser of Collins (1997) (Model 1). The second is based on the dependency parser of Klein and Manning (2004). Our translation model is a generic binary bracketing transduction grammar (Wu, 1997), whose main purpose is to restrict the alignment space to something that can be explored in polynomial time. Most work in syntax-based machine translation has been in translation modeling, but there are many reasons why we may instead want to focus on the language model. We experiment with parsers as language models for machine translation in a simple translation model. This approach demands much more of the language models,"
2008.amta-papers.16,P06-1121,0,0.223698,"ii, 21-25 October 2008] TM xRS Hiero BTG nition community. Furthermore, the distinction between the duties of the language and translation models is somewhat dubious; the noisy-channel approach to MT permits a decomposition into these two models, but modern log-linear systems use many different component models with overlapping responsibilities. Most recent syntax-based systems put much of what might be considered the language model’s responsibility into the translation model, by learning translation rules that produce structured target-language output from (flat) input phrases. An example is Galley et al. (2006), in which target language structures are linguistically motivated treebank parse fragments with an extended domain of locality, allowing, for example, a phrasal translation pair to specify for (optionally lexicalized) argument structure. In a different approach, Chiang (2007) used a generic (linguistically uninformed) grammar whose translation rules permitted lexicalized reorderings between the source and target languages. More recently, Shen et al. (2008) extended Chiang’s system to learn pieces of dependency structure along with the rules. These approaches (summarized in Figure 1) have show"
2008.amta-papers.16,P03-1054,0,0.0272966,"ions. This model comprises three distributions, one each for assigning probabilities to (a) head labels, (b) sibling labels and head tags, and (c) sibling head words. 3.1.2 One potential problem with the use of the Collins parser is that it posits a great deal of hidden structure. The Treebank grammar, or the model parameterization used in the Collins parser, is not necessarily the most useful for machine translation; in fact, many researchers have shown that the treebank grammar isn’t even the best choice for building parsers that are evaluated on their ability to recover treebank structure (Klein and Manning, 2003; Matsuzaki et al., 2005; Petrov and Klein, 2007). It stands to reason that a different grammar might do better as a language model for MT. To explore this, we evaluated the dependency model of Klein and Manning (2004). This model imagines the generative process as adding left and right arguments to a lexical head word until a decision to stop is made, and then recursing on each of those arguments. We changed the model to use word-based (instead of tag-based) parameters, and backed off to unigram probabilities for the sibling distributions (we also ignored the constituent context model describ"
2008.amta-papers.16,P04-1061,0,0.0705305,"ple the structure of the target language tree from that of the synchronous grammar tree. Instead of extracting fragments of parse tree along with the phrase table and learning syntax-motivated reorderings, we start with simple phrase pairs and build the target-language structure at decoding time. We compare two syntax-based language models in the context of a fairly simple syntactic translation model, as a means of isolating the contributions and potential of these language models. The first is the statistical parser of Collins (1997) (Model 1). The second is based on the dependency parser of Klein and Manning (2004). Our translation model is a generic binary bracketing transduction grammar (Wu, 1997), whose main purpose is to restrict the alignment space to something that can be explored in polynomial time. Most work in syntax-based machine translation has been in translation modeling, but there are many reasons why we may instead want to focus on the language model. We experiment with parsers as language models for machine translation in a simple translation model. This approach demands much more of the language models, allowing us to isolate their strengths and weaknesses. We find that unmodified parse"
2008.amta-papers.16,N03-1017,0,0.0497998,"the well-known French/English translation pair {ne X pas / do not X}. BTG cannot handle this alignment, but it can compensate by learning instantiations of the phrase pair for different values of X. It is worth pointing out explicitly that BTG in this form provides very little information to the translation process. In contrast to channel-rich models discussed earlier, BTG has only one parameter governing reordering, which expresses a preference for the straight binary rule over the inverted binary rule. This distortion model is even less informative than that of phrase-based systems such as Koehn et al. (2003)2 . An uninformative translation model allows us to isolate the influence of the language model in assembling the translation hypotheses. 3.1 not too difficult to implement and train, and its basic features – lexicalization and rule markovization – are at the core of the best generative parsers. The exact model we used is the three-level backoff model given in Table 7.1 of Collins (1999), together with special handling of punctuation and conjuctions. This model comprises three distributions, one each for assigning probabilities to (a) head labels, (b) sibling labels and head tags, and (c) sibl"
2008.amta-papers.16,P06-1096,0,0.0593212,"Missing"
2008.amta-papers.16,P05-1010,0,0.0175614,"s three distributions, one each for assigning probabilities to (a) head labels, (b) sibling labels and head tags, and (c) sibling head words. 3.1.2 One potential problem with the use of the Collins parser is that it posits a great deal of hidden structure. The Treebank grammar, or the model parameterization used in the Collins parser, is not necessarily the most useful for machine translation; in fact, many researchers have shown that the treebank grammar isn’t even the best choice for building parsers that are evaluated on their ability to recover treebank structure (Klein and Manning, 2003; Matsuzaki et al., 2005; Petrov and Klein, 2007). It stands to reason that a different grammar might do better as a language model for MT. To explore this, we evaluated the dependency model of Klein and Manning (2004). This model imagines the generative process as adding left and right arguments to a lexical head word until a decision to stop is made, and then recursing on each of those arguments. We changed the model to use word-based (instead of tag-based) parameters, and backed off to unigram probabilities for the sibling distributions (we also ignored the constituent context model described in that paper). Parsi"
2008.amta-papers.16,P02-1040,0,0.0766231,"ures) for unknown words used in Petrov et al. (2006). Our development data consisted of all Chinese sentences with twenty or fewer words from the NIST 2002 evaluation (371 sentences). The parameters of our log-linear model (which includes weights for the ngram and parser models, along with a length bonus) were set with a hill-climbing procedure on the development data. These parameters were then used to produce results from our test data, which was the portion of the NIST 2003 evaluation dataset with no more than twenty words (347 sentences). Our evaluation metric was case-insensitive BLEU-4 (Papineni et al., 2002). Table 1 contains the results of runs on the development and test sets. 5 RUN D EV /10 D EV /4 T EST /4 No LM Bigram Trigram Collins + bigram Dep + bigram 17.35 25.09 26.18 25.00 24.49 13.12 19.18 20.08 18.92 18.73 15.41 18.62 21.55 18.13 18.52 Table 1: Results (BLEU-4 scores). The number following the slash indicates the number of human references used in computing the BLEU score. No post-processing was applied to the MT output. The weight for the parser score was only allowed to go as low at 0.1, which is why the parser + bigram models are able to score slightly below the bigram model alone"
2008.amta-papers.16,N07-1051,0,0.0194814,"ne each for assigning probabilities to (a) head labels, (b) sibling labels and head tags, and (c) sibling head words. 3.1.2 One potential problem with the use of the Collins parser is that it posits a great deal of hidden structure. The Treebank grammar, or the model parameterization used in the Collins parser, is not necessarily the most useful for machine translation; in fact, many researchers have shown that the treebank grammar isn’t even the best choice for building parsers that are evaluated on their ability to recover treebank structure (Klein and Manning, 2003; Matsuzaki et al., 2005; Petrov and Klein, 2007). It stands to reason that a different grammar might do better as a language model for MT. To explore this, we evaluated the dependency model of Klein and Manning (2004). This model imagines the generative process as adding left and right arguments to a lexical head word until a decision to stop is made, and then recursing on each of those arguments. We changed the model to use word-based (instead of tag-based) parameters, and backed off to unigram probabilities for the sibling distributions (we also ignored the constituent context model described in that paper). Parsing models The task of a s"
2008.amta-papers.16,P06-1055,0,0.0146268,"lts on the development set. The ngram language model was trained on the English side of this parallel corpus using SRILM (Stolcke, 2002). The parsers were trained the 49,208 trees from the Penn Treebank plus parses of most of the English side of our parallel data, which was automatically parsed with the parser of Charniak (2000). With both parsers, we treated commas, colons, and periods as if they were just regular words in the vocabulary. Quotation marks were treated as an unknown word, and we used the same set of fifty bins (computed based on word surface features) for unknown words used in Petrov et al. (2006). Our development data consisted of all Chinese sentences with twenty or fewer words from the NIST 2002 evaluation (371 sentences). The parameters of our log-linear model (which includes weights for the ngram and parser models, along with a length bonus) were set with a hill-climbing procedure on the development data. These parameters were then used to produce results from our test data, which was the portion of the NIST 2003 evaluation dataset with no more than twenty words (347 sentences). Our evaluation metric was case-insensitive BLEU-4 (Papineni et al., 2002). Table 1 contains the results"
2008.amta-papers.16,P08-1066,0,0.319863,"ably learn syntax-based translation rules. Focusing on syntax-based approaches within the language model might be a more direct way to address the poor grammatically of most machine translation output. In previous work on syntax-based language modeling for machine translation, Wu and Wong (1998) included the score of an unlexicalized probabilistic context-free grammar (PCFG) in an ITG framework. Charniak et al. (2003) rescored a tree-tostring translation forest with a lexicalized parser and found more grammatical output despite a lower BLEU score (compared to an ngram version). More recently, Shen et al. (2008) used a trigram dependency model, with quite good results. Common to 2 Motivation From a grammatical standpoint, the output of most machine translation systems is very poor. Part of the reason may be that most systems do not incorporate syntax-based language models. The nominal task of the language model is to guide the search (decoding) procedure towards grammatical output, yet most systems use ngrams, which do not model sentence structure and cannot handle long-distance dependencies. There are a number of reasons that researchers have stuck with ngrams. They work well, are easy to train, req"
2008.amta-papers.16,P06-1123,0,0.0388551,"Missing"
2008.amta-papers.16,P98-2230,0,0.0376621,"es. 1 Introduction In the past few years there has been a burgeoning interest in syntax-based approaches to statistical machine translation. Most of this effort has been directed at the translation model rather than the language model; however, the level of syntactic divergence in parallel text makes it difficult to reliably learn syntax-based translation rules. Focusing on syntax-based approaches within the language model might be a more direct way to address the poor grammatically of most machine translation output. In previous work on syntax-based language modeling for machine translation, Wu and Wong (1998) included the score of an unlexicalized probabilistic context-free grammar (PCFG) in an ITG framework. Charniak et al. (2003) rescored a tree-tostring translation forest with a lexicalized parser and found more grammatical output despite a lower BLEU score (compared to an ngram version). More recently, Shen et al. (2008) used a trigram dependency model, with quite good results. Common to 2 Motivation From a grammatical standpoint, the output of most machine translation systems is very poor. Part of the reason may be that most systems do not incorporate syntax-based language models. The nominal"
2008.amta-papers.16,P96-1021,0,0.259346,"Missing"
2008.amta-papers.16,J97-3002,0,0.610291,"xtracting fragments of parse tree along with the phrase table and learning syntax-motivated reorderings, we start with simple phrase pairs and build the target-language structure at decoding time. We compare two syntax-based language models in the context of a fairly simple syntactic translation model, as a means of isolating the contributions and potential of these language models. The first is the statistical parser of Collins (1997) (Model 1). The second is based on the dependency parser of Klein and Manning (2004). Our translation model is a generic binary bracketing transduction grammar (Wu, 1997), whose main purpose is to restrict the alignment space to something that can be explored in polynomial time. Most work in syntax-based machine translation has been in translation modeling, but there are many reasons why we may instead want to focus on the language model. We experiment with parsers as language models for machine translation in a simple translation model. This approach demands much more of the language models, allowing us to isolate their strengths and weaknesses. We find that unmodified parsers do not improve BLEU scores over ngram language models, and provide an analysis of t"
2008.amta-papers.16,J03-4003,0,\N,Missing
2008.amta-papers.16,J05-4003,0,\N,Missing
2008.amta-papers.16,C98-2225,0,\N,Missing
2013.iwslt-papers.14,2005.iwslt-1.20,0,0.0595346,"er, and a host of factors that alter how an individual speaks (such as heartrate, stress, emotional state). Machine translation accuracy is affected by different factors, such as domain (e.g., newswire, medical, SMS, speech), register, and the typological differences between the languages. Because these technologies are imperfect themselves, their inaccuracies tend to multiply when they are chained together in the task of speech translation. Cross-lingual speech applications are typically built by combining speech recognition and machine translation systems, each trained on disparate datasets [1, 2]. The recognizer makes mistakes, passing text to the MT system with vastly different statistical properties from the parallel datasets (usually newswire or government texts) used to train large-scale translation systems, which are then further corrupted with the MT system’s own mistakes. Errors compound, and the results are often very poor. There are many approaches to improving this speech-totext pipeline. One is to gather training data that is closer to the test data, perhaps by paying professionals or using crowdsourcing techniques. The latter has been repeatedly demonstrated to be useful f"
2013.iwslt-papers.14,D08-1027,0,0.0480385,"home Spanish corpus4 comprises 120 transcripts of spontaneous conversations primarily between friends and family members, resulting in approximately 20 hours of speech aligned at the utterance level, with just over 200,000 words (tokens) of transcribed text. The combined dataset features a large variety of dialects, topics, and familiarity level between participants. 2.1. Crowdsourced Translations We obtained translations using the popular crowdsourcing platform Amazon Mechanical Turk (MTurk), following a widespread trend in scientific data collection and annotation across a variety of fields [11, 12, 13, 14, 15, 3], and in particular the translation crowdsourcing work of [16]. We began by lightly preprocessing the transcripts, first to remove all non-linguistic markup in the transcriptions (such as annotations for laughter or background noise), and second to concatenate sequential utterances of a speaker during a single turn. Many utterances in the original transcript consisted only of single words or in some cases only markup, so this second step produced longer sentences for translation, enabling us to provide more context to translators and reduce cost. When the length of a combined utterance exceede"
2013.iwslt-papers.14,D09-1030,1,0.309927,"home Spanish corpus4 comprises 120 transcripts of spontaneous conversations primarily between friends and family members, resulting in approximately 20 hours of speech aligned at the utterance level, with just over 200,000 words (tokens) of transcribed text. The combined dataset features a large variety of dialects, topics, and familiarity level between participants. 2.1. Crowdsourced Translations We obtained translations using the popular crowdsourcing platform Amazon Mechanical Turk (MTurk), following a widespread trend in scientific data collection and annotation across a variety of fields [11, 12, 13, 14, 15, 3], and in particular the translation crowdsourcing work of [16]. We began by lightly preprocessing the transcripts, first to remove all non-linguistic markup in the transcriptions (such as annotations for laughter or background noise), and second to concatenate sequential utterances of a speaker during a single turn. Many utterances in the original transcript consisted only of single words or in some cases only markup, so this second step produced longer sentences for translation, enabling us to provide more context to translators and reduce cost. When the length of a combined utterance exceede"
2013.iwslt-papers.14,N10-1024,1,0.431154,"astly different statistical properties from the parallel datasets (usually newswire or government texts) used to train large-scale translation systems, which are then further corrupted with the MT system’s own mistakes. Errors compound, and the results are often very poor. There are many approaches to improving this speech-totext pipeline. One is to gather training data that is closer to the test data, perhaps by paying professionals or using crowdsourcing techniques. The latter has been repeatedly demonstrated to be useful for collecting relevant training data for both speech and translation [3, 4, 5, 6], and in this paper we do the same for speech-to-text translation, assembling a fourway parallel dataset of audio, transcriptions, ASR output, and translations. The translations were produced inexpensively by non-professional translators using Amazon’s popular crowdsourcing platform, Mechanical Turk (§2). A second approach is to configure the ASR system to expose a portion of its search space by outputting more than just the single best output. Previous in speech-to-text translation have demonstrated success in translating ASR n-best lists [7] and confusion networks1 [8], and lattices [9, 10]."
2013.iwslt-papers.14,N12-1006,1,0.287152,"astly different statistical properties from the parallel datasets (usually newswire or government texts) used to train large-scale translation systems, which are then further corrupted with the MT system’s own mistakes. Errors compound, and the results are often very poor. There are many approaches to improving this speech-totext pipeline. One is to gather training data that is closer to the test data, perhaps by paying professionals or using crowdsourcing techniques. The latter has been repeatedly demonstrated to be useful for collecting relevant training data for both speech and translation [3, 4, 5, 6], and in this paper we do the same for speech-to-text translation, assembling a fourway parallel dataset of audio, transcriptions, ASR output, and translations. The translations were produced inexpensively by non-professional translators using Amazon’s popular crowdsourcing platform, Mechanical Turk (§2). A second approach is to configure the ASR system to expose a portion of its search space by outputting more than just the single best output. Previous in speech-to-text translation have demonstrated success in translating ASR n-best lists [7] and confusion networks1 [8], and lattices [9, 10]."
2013.iwslt-papers.14,P11-1122,1,0.487136,"rily between friends and family members, resulting in approximately 20 hours of speech aligned at the utterance level, with just over 200,000 words (tokens) of transcribed text. The combined dataset features a large variety of dialects, topics, and familiarity level between participants. 2.1. Crowdsourced Translations We obtained translations using the popular crowdsourcing platform Amazon Mechanical Turk (MTurk), following a widespread trend in scientific data collection and annotation across a variety of fields [11, 12, 13, 14, 15, 3], and in particular the translation crowdsourcing work of [16]. We began by lightly preprocessing the transcripts, first to remove all non-linguistic markup in the transcriptions (such as annotations for laughter or background noise), and second to concatenate sequential utterances of a speaker during a single turn. Many utterances in the original transcript consisted only of single words or in some cases only markup, so this second step produced longer sentences for translation, enabling us to provide more context to translators and reduce cost. When the length of a combined utterance exceeded 25 words, it was split on the next utterance boundary. We pr"
2013.iwslt-papers.14,W12-3152,1,0.743004,"astly different statistical properties from the parallel datasets (usually newswire or government texts) used to train large-scale translation systems, which are then further corrupted with the MT system’s own mistakes. Errors compound, and the results are often very poor. There are many approaches to improving this speech-totext pipeline. One is to gather training data that is closer to the test data, perhaps by paying professionals or using crowdsourcing techniques. The latter has been repeatedly demonstrated to be useful for collecting relevant training data for both speech and translation [3, 4, 5, 6], and in this paper we do the same for speech-to-text translation, assembling a fourway parallel dataset of audio, transcriptions, ASR output, and translations. The translations were produced inexpensively by non-professional translators using Amazon’s popular crowdsourcing platform, Mechanical Turk (§2). A second approach is to configure the ASR system to expose a portion of its search space by outputting more than just the single best output. Previous in speech-to-text translation have demonstrated success in translating ASR n-best lists [7] and confusion networks1 [8], and lattices [9, 10]."
2013.iwslt-papers.14,2005.mtsummit-papers.11,0,0.0360278,"d practice, we took steps to deter wholesale use of automated translation services by our translators. • Utterances were presented as images rather than text; this prevented cutting and pasting into online translation services.5 • We obtained translations from Google Translate for the utterances before presenting them to workers. HITs which had a small edit distance from these translations were manually reviewed and rejected if they were too similar (in particular, if they contained many of the same errors). • We also included four consecutive short sentences from the Europarl parallel corpus [17] in each HIT. HITs which had low overlap with the reference translations of these sentences were manually reviewed and rejected if they were of low quality. We obtained four redundant translations of sixty randomly chosen conversations from the Fisher corpus. In total, 115 workers completed 2463 HITs, producing 46,324 utterance-level translations and a little less than half a million words. 2.3. Selection of Preferred Translators We then extended a strategy devised by [16] to select highquality translators from the first round of translations. We designed a second-pass HIT which was used to ra"
2013.iwslt-papers.14,W13-2226,1,0.868991,"performance of the MT system, and we report experiments varying different components of the ASR–MT pipeline to examine their effect on this goal. For Fisher, we use Dev for tuning the parameters of the MT system and present results on Dev2 (reserving Test for future use); for Callhome, we tune on Devtest and present results on Evltest. Because of our focus on speech translation, for all models, we strip all punctuation (except for contractions) from both sides of the parallel data. For machine translation, we used Joshua, an open-source hierarchical machine translation toolkit written in Java [23]. Our grammars are hierarchical synchronous grammars [24]. Decoding proceeds by parsing the input with the source-side projection of the synchronous grammar using the CKY+ algorithm and combining target-side hypotheses with cubepruning [24]. This algorithm can easily be extended to lattice decoding in a way that permits hierarchical decomposition and reordering of words on the input lattice [25]. The decoder’s linear model comprises these features: • Phrasal probabilities (p(e|f ) and p(f |e)) • Lexical probabilities (w(e|f ) and w(f |e)) • Rarity penalty, exp(1 − count(rule)) • Word penalty •"
2013.iwslt-papers.14,J07-2003,0,0.0593645,"arying different components of the ASR–MT pipeline to examine their effect on this goal. For Fisher, we use Dev for tuning the parameters of the MT system and present results on Dev2 (reserving Test for future use); for Callhome, we tune on Devtest and present results on Evltest. Because of our focus on speech translation, for all models, we strip all punctuation (except for contractions) from both sides of the parallel data. For machine translation, we used Joshua, an open-source hierarchical machine translation toolkit written in Java [23]. Our grammars are hierarchical synchronous grammars [24]. Decoding proceeds by parsing the input with the source-side projection of the synchronous grammar using the CKY+ algorithm and combining target-side hypotheses with cubepruning [24]. This algorithm can easily be extended to lattice decoding in a way that permits hierarchical decomposition and reordering of words on the input lattice [25]. The decoder’s linear model comprises these features: • Phrasal probabilities (p(e|f ) and p(f |e)) • Lexical probabilities (w(e|f ) and w(f |e)) • Rarity penalty, exp(1 − count(rule)) • Word penalty • Glue rule penalty • Out-of-vocabulary word penalty Inter"
2013.iwslt-papers.14,P08-1115,0,0.0451524,"we strip all punctuation (except for contractions) from both sides of the parallel data. For machine translation, we used Joshua, an open-source hierarchical machine translation toolkit written in Java [23]. Our grammars are hierarchical synchronous grammars [24]. Decoding proceeds by parsing the input with the source-side projection of the synchronous grammar using the CKY+ algorithm and combining target-side hypotheses with cubepruning [24]. This algorithm can easily be extended to lattice decoding in a way that permits hierarchical decomposition and reordering of words on the input lattice [25]. The decoder’s linear model comprises these features: • Phrasal probabilities (p(e|f ) and p(f |e)) • Lexical probabilities (w(e|f ) and w(f |e)) • Rarity penalty, exp(1 − count(rule)) • Word penalty • Glue rule penalty • Out-of-vocabulary word penalty Interface Transcript 1-best Lattice Oracle Path Euro 41.8 24.3 32.1 LDC 58.7 35.4 37.1 46.2 ASR 54.6 34.7 35.9 44.3 LDC +ASR 58.7 35.5 36.8 46.3 Table 4: BLEU scores (four references) on Fisher/Dev2. The columns vary the data used to train the MT system, and the rows alter the interface between the ASR and MT systems. Training set Interface Tra"
2013.iwslt-papers.14,N12-1047,0,0.0637002,"isher Train, as described above. • ASR. An in-domain model trained on pairs of Spanish ASR outputs and English translations. • LDC+ASR. A model trained by concatenating the training data for LDC and ASR. For (b), we vary the interface in four ways: • Transcript. We translate the LDC transcripts. This serves as an upper bound on the possible performance. • 5-gram language model score • Lattice weight (the input path’s posterior log probability; where appropriate) The language model is always constructed over the target side of the training data. These features are tuned using k-best batch MIRA [26], and results are reported on the average of three runs. Our metric is case-insensitive BLEU-4 [27] with four references (for Fisher) and one reference (for Callhome). • 1-best. We translate the 1-best output as presented by the speech recognizer. • Lattices. We pass a pruned lattice from the recognizer to the MT system. • Oracle Path. The oracle path from the lattice, representing the best transcription found in the ASR system’s hypothesis space (subject to pruning). INCORPORA CORTAR ... DE 4 CUÁL ESCOGER 5 Y 9 POR 6 COSAS TENEMOS 7 CORTAN PROGRAMAS 8 10 QUE 11 UTILIZAR ... INCORPORAMOS Trans"
2013.iwslt-papers.14,P02-1040,0,0.106207,"and English translations. • LDC+ASR. A model trained by concatenating the training data for LDC and ASR. For (b), we vary the interface in four ways: • Transcript. We translate the LDC transcripts. This serves as an upper bound on the possible performance. • 5-gram language model score • Lattice weight (the input path’s posterior log probability; where appropriate) The language model is always constructed over the target side of the training data. These features are tuned using k-best batch MIRA [26], and results are reported on the average of three runs. Our metric is case-insensitive BLEU-4 [27] with four references (for Fisher) and one reference (for Callhome). • 1-best. We translate the 1-best output as presented by the speech recognizer. • Lattices. We pass a pruned lattice from the recognizer to the MT system. • Oracle Path. The oracle path from the lattice, representing the best transcription found in the ASR system’s hypothesis space (subject to pruning). INCORPORA CORTAR ... DE 4 CUÁL ESCOGER 5 Y 9 POR 6 COSAS TENEMOS 7 CORTAN PROGRAMAS 8 10 QUE 11 UTILIZAR ... INCORPORAMOS Transcript 1-best Lattice Reference 1-best → MT Lattice → MT 1-best → Google sí hablar de cuáles y cosas"
2016.amta-researchers.14,J07-2003,0,0.235974,"Missing"
2016.amta-researchers.14,P08-2015,0,0.0273342,"ically (§4). 3. We then produce phrasal translation pairs by extracting English phrases from these sentences and pairing them with the foreign language through the UniMorph tag (§5). We investigate different methods for extracting and scoring phrase pairs. 4. Finally, we evaluate the utility of these phrase pairs to improve machine translation in simulated low-resource settings (§6). A depiction of the full pipeline is in Figure 1. 2 Prior Work Maximizing the utility of a baseline phrase table has been the focus of a large body of prior work in translating from morphologically rich languages. Habash (2008) work on the OOV problem in Arabic, mapping OOV types to in-vocabulary (INV) types by orthographic and morphological smoothing methods. Mirkin et al. (2009) take inspiration from the Textual Entailment (TE) problem, using WordNet to determine a set of entailed alternatives for English OOV tokens. However, since this OOV-resolution scheme is dependent on the existence of a semantic resource like WordNet in the source language, it is unsuitable in general low-resource settings. Yang and Kirchhoff (2006) implement a backoff model for Finnish and German, stemming and splitting OOV tokens at test t"
2016.amta-researchers.14,hajic-etal-2012-announcing,0,0.027935,"Missing"
2016.amta-researchers.14,P13-2121,0,0.0606978,"Missing"
2016.amta-researchers.14,W04-3250,0,0.304107,"Missing"
2016.amta-researchers.14,2005.mtsummit-papers.11,0,0.0678909,"lation systems are typically trained on large bilingual parallel corpora (bitext). Low-resource machine translation focuses on translation of languages for which there exists little bitext, and where translation quality is subsequently often poor. Highly inﬂected languages—those that exhibit large inﬂectional paradigms of words with a common dictionary entry—excacerbate the problems of a low-resource setting. Many inﬂections of words in an inﬂectional paradigm are complex and rare, and their translations are unlikely to be wellestimated even in a moderately large parallel corpus. For example, Koehn (2005) point to the highly inﬂected nature of Finnish as a reason for poor translation performance into English even in high-resource settings. However, even where bitext may be lacking or scarce, there are often many other resources available. One source of rich morphological information is Wiktionary.1 This paper describes a method for using resources extracted from Wiktionary to automatically map inﬂections in paradigms of morphologically rich languages to ranked sets of English phrasal translations. This is done by the following procedure: 1 https://www.wiktionary.org/ 3URFHHGLQJVRI$07$"
2016.amta-researchers.14,2012.amta-papers.9,0,0.0347861,"et to determine a set of entailed alternatives for English OOV tokens. However, since this OOV-resolution scheme is dependent on the existence of a semantic resource like WordNet in the source language, it is unsuitable in general low-resource settings. Yang and Kirchhoff (2006) implement a backoff model for Finnish and German, stemming and splitting OOV tokens at test time and searching a baseline phrase table for the resulting simpliﬁed forms. Many systems attempt to address the incorrect independence assumptions traditional phrase-based MT systems impose on inﬂections in the same paradigm. Koehn and Haddow (2012) train a baseline phrase-based translation model, and back off to a factored model that 3URFHHGLQJVRI$07$YRO075HVHDUFKHUV 7UDFN $XVWLQ2FW1RY_S Inﬂection bude absolvovat absolvuj absolvujete absolvoval jste Lemma absolvovat absolvovat absolvovat absolvovat Mood IND IMP IND IND POS VB VB VB VB Tense FUT Gender PST PST MASC MASC Number 2 2 2 2 Animacy ANIM INAN Person SG SG SG SG Table 1: Czech verb inﬂections and partial annotations from Wiktionary. Empty cells indicate that the inﬂection is not marked in that dimension. decomposes OOV tokens into lemma+"
2016.amta-researchers.14,P09-1089,0,0.0231176,"ge through the UniMorph tag (§5). We investigate different methods for extracting and scoring phrase pairs. 4. Finally, we evaluate the utility of these phrase pairs to improve machine translation in simulated low-resource settings (§6). A depiction of the full pipeline is in Figure 1. 2 Prior Work Maximizing the utility of a baseline phrase table has been the focus of a large body of prior work in translating from morphologically rich languages. Habash (2008) work on the OOV problem in Arabic, mapping OOV types to in-vocabulary (INV) types by orthographic and morphological smoothing methods. Mirkin et al. (2009) take inspiration from the Textual Entailment (TE) problem, using WordNet to determine a set of entailed alternatives for English OOV tokens. However, since this OOV-resolution scheme is dependent on the existence of a semantic resource like WordNet in the source language, it is unsuitable in general low-resource settings. Yang and Kirchhoff (2006) implement a backoff model for Finnish and German, stemming and splitting OOV tokens at test time and searching a baseline phrase table for the resulting simpliﬁed forms. Many systems attempt to address the incorrect independence assumptions traditio"
2016.amta-researchers.14,P15-2111,1,0.803087,"ibes a method for using resources extracted from Wiktionary to automatically map inﬂections in paradigms of morphologically rich languages to ranked sets of English phrasal translations. This is done by the following procedure: 1 https://www.wiktionary.org/ 3URFHHGLQJVRI$07$YRO075HVHDUFKHUV 7UDFN $XVWLQ2FW1RY_S Figure 1: The translation model (TM) construction pipeline. This depicts the process by which we map each morphologically annotated inﬂection to a ranked set of English phrasal translations. 1. We begin with resources from the UniMorph project (Sylak-Glassman et al., 2015), which produced millions of tuples pairing inﬂected words forms with their lemmas and a rich morphological tag (which we refer to as a UniMorph tag or vector) that was designed to be a universal representation of morphological features (§3). 2. We next take a small set of pairs of UniMorph vectors and short English sentences that were produced in an Elicitation Corpus, designed to collect inﬂections that in English are expressed phrasally instead of morphologically (§4). 3. We then produce phrasal translation pairs by extracting English phrases from these sentences and pairing them with the f"
2016.amta-researchers.14,E06-1006,0,0.0198916,"able has been the focus of a large body of prior work in translating from morphologically rich languages. Habash (2008) work on the OOV problem in Arabic, mapping OOV types to in-vocabulary (INV) types by orthographic and morphological smoothing methods. Mirkin et al. (2009) take inspiration from the Textual Entailment (TE) problem, using WordNet to determine a set of entailed alternatives for English OOV tokens. However, since this OOV-resolution scheme is dependent on the existence of a semantic resource like WordNet in the source language, it is unsuitable in general low-resource settings. Yang and Kirchhoff (2006) implement a backoff model for Finnish and German, stemming and splitting OOV tokens at test time and searching a baseline phrase table for the resulting simpliﬁed forms. Many systems attempt to address the incorrect independence assumptions traditional phrase-based MT systems impose on inﬂections in the same paradigm. Koehn and Haddow (2012) train a baseline phrase-based translation model, and back off to a factored model that 3URFHHGLQJVRI$07$YRO075HVHDUFKHUV 7UDFN $XVWLQ2FW1RY_S Inﬂection bude absolvovat absolvuj absolvujete absolvoval jste Lemma ab"
2020.emnlp-main.7,D19-1632,1,0.901696,"Missing"
2020.emnlp-main.7,N06-1003,1,0.659051,"Missing"
2020.emnlp-main.7,K19-1005,1,0.83875,"Missing"
2020.emnlp-main.7,W18-2705,1,0.913357,"Missing"
2020.emnlp-main.7,N12-1017,0,0.0337366,"was Figure 1: Some possible . . . . . . . . . .paraphrases . . . . . . . . . . . of the original reference, ‘The tortoise beat the hare,’ for the Dutch source sentence, ‘De schildpad versloeg de haas.’ A sampled path and some of the other tokens also considered in the training objective are highlighted. We demonstrate the effectiveness of our method on two corpora from the low-resource MATERIAL program, and on bitext from GlobalVoices. We release data & code: data.statmt.org/smrt Introduction Variability and expressiveness are core features of language, and they extend to translation as well. Dreyer and Marcu (2012) showed that naturally occurring sentences have billions of valid translations. Despite this variety, machine translation (MT) models are optimized toward a single translation of each sentence in the training corpus. Training a high resource MT model on millions of sentence pairs likely exposes it to similar sentences translated different ways, but training a low-resource MT model with a single translation for each sentence (out of potentially billions) exacerbates data sparsity. We hypothesize that the discrepancy between linguistic diversity and standard single-reference training hinders MT"
2020.emnlp-main.7,D16-1139,0,0.0503468,"ing (top100 sampling). Greedy search tends to work best: see Table 4. It improves over the baseline for the 10 Global Voices datasets, but not for the two MATERIAL ones. Overall, our proposed method is more effective than this contrastive method. We hypothesize this is due to the wider variety of paraprhases SMRT introduces by sampling and training toward the full distribution from the paraphraser. 5.4 6 Sequence-Level Paraphrastic Data Augmentation Related Work Knowledge Distillation Our proposed objective is similarly structured to word-level knowledge distillation (KD; Hinton et al., 2015; Kim and Rush, 2016), where a student model is trained to match the output distribution of a teacher model. Paraphrasing as preprocessed data augmentation, as discussed in § 5.4, is similarly analogous to sequencelevel knowledge distillation (Kim and Rush, 2016). As a contrastive experiment, we use the paraphraser to generate additional target-side data for use in data augmentation. For each target sentence (y) in 8 All use settings from § 3.2: we use the original reference with LNLL with 1 − p = 0.5 probability, and when sampling we sample from the top w = 100 tokens. 9 This is equivalent to LNLL using a paraphr"
2020.emnlp-main.7,P17-2090,0,0.0597272,"Missing"
2020.emnlp-main.7,P18-1082,0,0.0253537,"beddings, and 2 encoder and decoder attention heads. We regularize with 0.2 label smoothing and 0.4 dropout. We optimize using Adam with a learning rate of 10−3 . We train for 200 epochs, and select the best checkpoint based on validation set perplexity. We translate with a beam size of 5. For our method we use the proposed objective LSMRT with probability p = 0.5 and standard LNLL on the original reference with probability 1 − p. We sample from only the 100 highest probability vocabulary items at a given time step when sampling from the paraphraser distribution to avoid very unlikely tokens (Fan et al., 2018). Using our English paraphraser, we aim to demonstrate improvements in low-resource settings, since these remain a challenge in NMT (Koehn and Knowles, 2017; Sennrich and Zhang, 2019). We use Tagalog (tl) to English and Swahili (sw) to English bitext from the MATERIAL low-resource program (Rubino, 2018). We also report results on MT bitext from GlobalVoices, a non-profit news site that publishes in 53 languages.3 We evaluate on the 10 lowest-resource settings that have at least 10,000 lines of parallel text with English: Hungarian (hu), Indonesian (id), Czech (cs), Serbian (sr), Catalan (ca),"
2020.emnlp-main.7,W04-3250,1,0.636066,"Missing"
2020.emnlp-main.7,W17-3204,1,0.816531,"te of 10−3 . We train for 200 epochs, and select the best checkpoint based on validation set perplexity. We translate with a beam size of 5. For our method we use the proposed objective LSMRT with probability p = 0.5 and standard LNLL on the original reference with probability 1 − p. We sample from only the 100 highest probability vocabulary items at a given time step when sampling from the paraphraser distribution to avoid very unlikely tokens (Fan et al., 2018). Using our English paraphraser, we aim to demonstrate improvements in low-resource settings, since these remain a challenge in NMT (Koehn and Knowles, 2017; Sennrich and Zhang, 2019). We use Tagalog (tl) to English and Swahili (sw) to English bitext from the MATERIAL low-resource program (Rubino, 2018). We also report results on MT bitext from GlobalVoices, a non-profit news site that publishes in 53 languages.3 We evaluate on the 10 lowest-resource settings that have at least 10,000 lines of parallel text with English: Hungarian (hu), Indonesian (id), Czech (cs), Serbian (sr), Catalan (ca), Swahili (sw),4 Dutch (nl), Polish (pl), Macedonian (mk), Arabic (ar). We use 2,000 lines each for a validation set for model selection from checkpoints and"
2020.emnlp-main.7,W18-1902,0,0.0261518,"use the proposed objective LSMRT with probability p = 0.5 and standard LNLL on the original reference with probability 1 − p. We sample from only the 100 highest probability vocabulary items at a given time step when sampling from the paraphraser distribution to avoid very unlikely tokens (Fan et al., 2018). Using our English paraphraser, we aim to demonstrate improvements in low-resource settings, since these remain a challenge in NMT (Koehn and Knowles, 2017; Sennrich and Zhang, 2019). We use Tagalog (tl) to English and Swahili (sw) to English bitext from the MATERIAL low-resource program (Rubino, 2018). We also report results on MT bitext from GlobalVoices, a non-profit news site that publishes in 53 languages.3 We evaluate on the 10 lowest-resource settings that have at least 10,000 lines of parallel text with English: Hungarian (hu), Indonesian (id), Czech (cs), Serbian (sr), Catalan (ca), Swahili (sw),4 Dutch (nl), Polish (pl), Macedonian (mk), Arabic (ar). We use 2,000 lines each for a validation set for model selection from checkpoints and a test set for reporting results. The approximate number of lines of training data is in the top of Table 1. We train an English SentencePiece model"
2020.emnlp-main.7,D18-2012,0,0.125663,"Missing"
2020.emnlp-main.7,P16-1009,0,0.102396,"Missing"
2020.emnlp-main.7,D18-1421,0,0.0608042,"6; Marton et al., 2009), and generation of additional references for tuning (Madnani et al., 2007, 2008). Label Smoothing Label smoothing (which we use when training with LNLL ) spreads probability mass over all non-reference tokens equally (Szegedy et al., 2016); LSMRT places higher probability on semantically plausible tokens. 7 Conclusion We present Simulated Multiple Reference Training (SMRT), which significantly improves performance in low-resource settings—by 1.2 to 7.0 BLEU—and is complementary to back-translation. Neural paraphrasers are rapidly improving (Wieting et al., 2017, 2019b; Li et al., 2018; Wieting and Gimpel, 2018; Hu et al., 2019a,b,c), and the concurrently released P RISM multi-lingual paraphraser Thompson and Post (2020a,b) has coverage of 39 languages and outperforms prior work in English paraphrasing. As paraphrasing continues to improve and cover more languages, we are optimistic SMRT will provide larger improvements across the board—including for higher-resource MT and for additional target languages beyond English. Data Augmentation in NMT Back-translation (BT) translates target-language monolingual text to create synthetic source sentences (Sennrich et al., 2016). BT"
2020.emnlp-main.7,P19-1021,0,0.0255847,"200 epochs, and select the best checkpoint based on validation set perplexity. We translate with a beam size of 5. For our method we use the proposed objective LSMRT with probability p = 0.5 and standard LNLL on the original reference with probability 1 − p. We sample from only the 100 highest probability vocabulary items at a given time step when sampling from the paraphraser distribution to avoid very unlikely tokens (Fan et al., 2018). Using our English paraphraser, we aim to demonstrate improvements in low-resource settings, since these remain a challenge in NMT (Koehn and Knowles, 2017; Sennrich and Zhang, 2019). We use Tagalog (tl) to English and Swahili (sw) to English bitext from the MATERIAL low-resource program (Rubino, 2018). We also report results on MT bitext from GlobalVoices, a non-profit news site that publishes in 53 languages.3 We evaluate on the 10 lowest-resource settings that have at least 10,000 lines of parallel text with English: Hungarian (hu), Indonesian (id), Czech (cs), Serbian (sr), Catalan (ca), Swahili (sw),4 Dutch (nl), Polish (pl), Macedonian (mk), Arabic (ar). We use 2,000 lines each for a validation set for model selection from checkpoints and a test set for reporting re"
2020.emnlp-main.7,W07-0716,0,0.141273,"Missing"
2020.emnlp-main.7,P16-1159,0,0.0874042,"Missing"
2020.emnlp-main.7,2008.amta-papers.13,0,0.201312,"Missing"
2020.emnlp-main.7,D09-1040,0,0.030861,"Missing"
2020.emnlp-main.7,2020.emnlp-main.8,1,0.856459,"Missing"
2020.emnlp-main.7,2020.wmt-1.67,1,0.852344,"Missing"
2020.emnlp-main.7,N19-4009,0,0.0735833,"Missing"
2020.emnlp-main.7,tiedemann-2012-parallel,0,0.0364539,"get token for both the MT model and paraphraser. For a visualization see Figure 1, which shows possible . . . . . . . . .paraphrases . . . . . . . . . . . . of the reference, ‘The tortoise beat the hare.’ The paraphraser and MT model condition on the paraphrase (y 0 ) as the previous output. The paraphrase (y 0 ) and the rest of the tokens in the paraphraser’s distribution make up pPARA , which is used to compute LSMRT . 3 3.1 NMT models Experimental Setup Paraphraser 2 Hu et al. released a trained S OCKEYE paraphraser but we implement our method in FAIRSEQ. 3 We use v2017q3 released on Opus (Tiedemann, 2012, opus.nlpl.eu/GlobalVoices.php). 4 Swahili is in both. MATERIAL data is not widely available, so we separate them to keep GlobalVoices reproducible. For use as an English paraphraser, we train a Transformer model (Vaswani et al., 2017) in FAIRSEQ (Ott et al., 2019) with an 8-layer encoder and decoder, 1024 dimensional embeddings, 16 encoder 83 dataset GlobalVoices MATERIAL * → en train lines hu 8k id 8k cs 11k sr 14k ca 15k sw 24k nl 32k pl 40k mk 44k ar 47k sw 19k tl 46k baseline this work 2.3 5.4 5.3 12.3 3.4 6.6 11.8 16.1 16.0 20.0 17.9 20.5 22.2 24.8 16.0 18.0 27.0 28.2 12.7 14.9 37.8 39."
2020.emnlp-main.7,D18-1100,0,0.0460032,"Missing"
2020.emnlp-main.7,P19-1427,0,0.0478073,"Missing"
2020.emnlp-main.7,P18-1042,0,0.0580104,"2009), and generation of additional references for tuning (Madnani et al., 2007, 2008). Label Smoothing Label smoothing (which we use when training with LNLL ) spreads probability mass over all non-reference tokens equally (Szegedy et al., 2016); LSMRT places higher probability on semantically plausible tokens. 7 Conclusion We present Simulated Multiple Reference Training (SMRT), which significantly improves performance in low-resource settings—by 1.2 to 7.0 BLEU—and is complementary to back-translation. Neural paraphrasers are rapidly improving (Wieting et al., 2017, 2019b; Li et al., 2018; Wieting and Gimpel, 2018; Hu et al., 2019a,b,c), and the concurrently released P RISM multi-lingual paraphraser Thompson and Post (2020a,b) has coverage of 39 languages and outperforms prior work in English paraphrasing. As paraphrasing continues to improve and cover more languages, we are optimistic SMRT will provide larger improvements across the board—including for higher-resource MT and for additional target languages beyond English. Data Augmentation in NMT Back-translation (BT) translates target-language monolingual text to create synthetic source sentences (Sennrich et al., 2016). BT needs a reverse translatio"
2020.emnlp-main.7,P19-1453,0,0.177904,"Missing"
2020.emnlp-main.7,D17-1026,0,0.0879872,"n (Callison-Burch et al., 2006; Marton et al., 2009), and generation of additional references for tuning (Madnani et al., 2007, 2008). Label Smoothing Label smoothing (which we use when training with LNLL ) spreads probability mass over all non-reference tokens equally (Szegedy et al., 2016); LSMRT places higher probability on semantically plausible tokens. 7 Conclusion We present Simulated Multiple Reference Training (SMRT), which significantly improves performance in low-resource settings—by 1.2 to 7.0 BLEU—and is complementary to back-translation. Neural paraphrasers are rapidly improving (Wieting et al., 2017, 2019b; Li et al., 2018; Wieting and Gimpel, 2018; Hu et al., 2019a,b,c), and the concurrently released P RISM multi-lingual paraphraser Thompson and Post (2020a,b) has coverage of 39 languages and outperforms prior work in English paraphrasing. As paraphrasing continues to improve and cover more languages, we are optimistic SMRT will provide larger improvements across the board—including for higher-resource MT and for additional target languages beyond English. Data Augmentation in NMT Back-translation (BT) translates target-language monolingual text to create synthetic source sentences (Sen"
2020.emnlp-main.7,D19-1143,0,0.0478443,"Missing"
2020.emnlp-main.7,N19-1090,1,\N,Missing
2020.emnlp-main.8,P15-1166,0,0.046192,"2019 QE shared task Finally, we contrast the effectiveness of our model when scoring MT output using the source vs the human reference. We observe that human references substantially improve performance, and, crucially, allow our model to rank systems that are substantially better than our model at the task of translation. This is important because it establishes that our method does not require building a state-of-theart multilingual NMT model in order to produce a state-of-the-art MT metric capable of evaluating state-of-the-art MT systems. 1 Related Work Multilingual NMT Multilingual NMT (Dong et al., 2015) has been shown to rival performance of single language pair models in high-resource languages (Aharoni et al., 2019; Arivazhagan et al., 2019) while also improving low-resource translation via transfer learning from higher-resource languages (Zoph et al., 2016; Nguyen and Chiang, 2017; Neubig and Hu, 2018). An extreme low-resource setting is where the system translates between languages seen during training, but in a language pair where it did not see any training 2 Except for Gujarati, where we had no training data. 91 https://github.com/thompsonb/prism Word-level paraphraser log probabiliti"
2020.emnlp-main.8,W19-5435,0,0.0458778,"Missing"
2020.emnlp-main.8,N12-1017,0,0.104749,"udgements. Paraphrase Databases Prior work explored using parallel bitext to identify phrase level paraphrases (Bannard and Callison-Burch, 2005; Ganitkevitch et al., 2013) including bitext in multiple language pairs (Ganitkevitch and Callison-Burch, 2014). Paraphrase tables were, in turn, used in MT metrics to reward systems for paraphrasing words (Banerjee and Lavie, 2005) or phrases (Zhou et al., 2006; Denkowski and Lavie, 2010) from the human reference. Our work can be viewed as extending this idea to the sentence level, without having to enumerate the millions or billions of paraphrases (Dreyer and Marcu, 2012) for each sentence. • Outperforms or ties with prior metrics and several contrastive neural methods on the segment-level WMT 2019 MT metrics task in every language pair;1 • Is able to discriminate between very strong neural systems at the system level, addressing a problem raised at WMT 2019; and • Significantly outperforms all QE metrics submitted to the WMT 2019 QE shared task Finally, we contrast the effectiveness of our model when scoring MT output using the source vs the human reference. We observe that human references substantially improve performance, and, crucially, allow our model to"
2020.emnlp-main.8,P17-1152,0,0.0426271,"Missing"
2020.emnlp-main.8,W17-4766,0,0.0585441,"Missing"
2020.emnlp-main.8,2020.acl-main.253,0,0.0735869,"Missing"
2020.emnlp-main.8,W19-5356,0,0.0136245,"ch we denote Prism (Probability is the metric). We train a single model in 39 languages and show that it: We release our model, metrics toolkit, and preprocessed training data.2 2 MT Metrics Early MT metrics like BLEU (Papineni et al., 2002) and NIST (Doddington, 2002) use token-level n-gram overlap between the MT output and the human reference. Overlap can also be measured at the character level (Popovi´c, 2015, 2017) or using edit distance (Snover et al., 2006). Many metrics use word- and/or sentencelevel embeddings, including ReVal (Gupta et al., 2015), RUSE (Shimanaka et al., 2018), WMDO (Chow et al., 2019), and ESIM (Mathur et al., 2019). MEANT (Lo and Wu, 2011) and MEANT 2.0 (Lo, 2017) measure similarity between semantic frames and role fillers. State-of-the-art methods including YiSi (Lo, 2019) and BERTscore (Zhang et al., 2019, 2020) rely on contextualized embeddings (Devlin et al., 2019) trained on large (non-parallel) corpora. BLEURT (Sellam et al., 2020) applies fine tuning of BERT, including training on prior human judgements. In contrast, our work exploits parallel bitext and doesn’t require training on human judgements. Paraphrase Databases Prior work explored using parallel bitext to"
2020.emnlp-main.8,eisele-chen-2010-multiun,0,0.0411337,"Missing"
2020.emnlp-main.8,P11-2117,0,0.0378965,"ero-shot translation has been shown successful, especially between related languages (Johnson et al., 2017; Gu et al., 2018; Pham et al., 2019). Schwenk, 2018), for example, trains a variant of NMT with a fixed-size intermediate representation in 93 languages. Embeddings produced by the encoder can be compared to measure intra- or interlingual semantic similarity. 3 Generative Paraphrasing Sentential paraphrasing can be accomplished by training an MT system on paraphrase examples instead of translation pairs (Quirk et al., 2004). While natural paraphrase datasets do exist (Quirk et al., 2004; Coster and Kauchak, 2011; Fader et al., 2013; Lin et al., 2014; Federmann et al., 2019), they are somewhat limited. An alternative is to start with much more plentiful bitext and back-translate one side into the language of the other to create synthetic paraphrases on which to train (Prakash et al., 2016; Wieting and Gimpel, 2018; Hu et al., 2019a,b,c). Tiedemann and Scherrer (2019) propose using paraphrasing as a way to measure the semantic abstraction of multilingual NMT. They also propose using a multilingual NMT model as a generative paraphraser.3 Method We propose using a paraphraser to force-decode and estimate"
2020.emnlp-main.8,ganitkevitch-callison-burch-2014-multilingual,0,0.0253474,"s. State-of-the-art methods including YiSi (Lo, 2019) and BERTscore (Zhang et al., 2019, 2020) rely on contextualized embeddings (Devlin et al., 2019) trained on large (non-parallel) corpora. BLEURT (Sellam et al., 2020) applies fine tuning of BERT, including training on prior human judgements. In contrast, our work exploits parallel bitext and doesn’t require training on human judgements. Paraphrase Databases Prior work explored using parallel bitext to identify phrase level paraphrases (Bannard and Callison-Burch, 2005; Ganitkevitch et al., 2013) including bitext in multiple language pairs (Ganitkevitch and Callison-Burch, 2014). Paraphrase tables were, in turn, used in MT metrics to reward systems for paraphrasing words (Banerjee and Lavie, 2005) or phrases (Zhou et al., 2006; Denkowski and Lavie, 2010) from the human reference. Our work can be viewed as extending this idea to the sentence level, without having to enumerate the millions or billions of paraphrases (Dreyer and Marcu, 2012) for each sentence. • Outperforms or ties with prior metrics and several contrastive neural methods on the segment-level WMT 2019 MT metrics task in every language pair;1 • Is able to discriminate between very strong neural systems a"
2020.emnlp-main.8,N13-1092,0,0.142381,"Missing"
2020.emnlp-main.8,W14-3333,0,0.131232,"Missing"
2020.emnlp-main.8,N18-1032,0,0.0330069,"aning of the sentence or introduce a disfluency more harshly than those which are fluent and adequate. Sentence-level BLEU with smoothing=1 (“sBLEU”) and LASER embedding cosine similarity (“LASER”) are shown for comparison. We note that LASER appears fairly insensitive to disfluencies, and sentenceBLEU struggles to reward valid paraphrases. data, denoted ‘zero-shot’ translation. Despite evidence that intermediate representations are not truly language-agnostic (Kudugunta et al., 2019), zero-shot translation has been shown successful, especially between related languages (Johnson et al., 2017; Gu et al., 2018; Pham et al., 2019). Schwenk, 2018), for example, trains a variant of NMT with a fixed-size intermediate representation in 93 languages. Embeddings produced by the encoder can be compared to measure intra- or interlingual semantic similarity. 3 Generative Paraphrasing Sentential paraphrasing can be accomplished by training an MT system on paraphrase examples instead of translation pairs (Quirk et al., 2004). While natural paraphrase datasets do exist (Quirk et al., 2004; Coster and Kauchak, 2011; Fader et al., 2013; Lin et al., 2014; Federmann et al., 2019), they are somewhat limited. An alte"
2020.emnlp-main.8,W19-5357,0,0.0768623,"Missing"
2020.emnlp-main.8,K19-1005,1,0.897458,"Missing"
2020.emnlp-main.8,C18-1266,0,0.0306982,"Missing"
2020.emnlp-main.8,W18-6478,0,0.025107,"oni et al. (2019) found that performance of zero-shot translation in a related language pair increased substantially when increasing the number of languages from 5 languages and 25, with a performance plateau somewhere between 25 and 50 languages. We view paraphrasing as zero-shot translation between sentences in the same language, so we expect to need a similar number of languages. To obtain system-level scores, we average segmentlevel scores over all segments in the test set. 4 Conditional probabilities of MT systems in each direction have been shown effective at filtering MT training data (Junczys-Dowmunt, 2018). 93 Copies We filter sentence pairs with excessive copies and partial copies, as multiple studies (Ott et al., 2018; Khayrallah and Koehn, 2018) have noted that MT performance degrades substantially when systems are exposed to copies in training. 4.2 LASER We explore using the cosine distance between LASER embeddings of the MT output and human reference, using the pretrained 93-language model provided by the authors.8 We are particularly interested in LASER as it, like our model, is trained on parallel bitext in many languages. Model Training We train a Transformer (Vaswani et al., 2017) mode"
2020.emnlp-main.8,W18-2709,0,0.0200204,"sible, we choose datasets with as much language pair diversity as possible (i.e., not just en–* and *–en), as Kudugunta et al. (2019) has shown that encoder representation is affected by both the source language and target language. While it is common to append the target language token to the source sentence, we instead prepend it to the target sentence so that the encoder cannot do anything target-language specific with this tag. At test time, we force-decode the desired language tag prior to scoring. Noise NMT systems are known to be sensitive to noise, including sentence alignment errors (Khayrallah and Koehn, 2018), so we perform filtering with LASER (Schwenk, 2018; Chaudhary et al., 2019). We also perform language ID filtering using FastText (Joulin et al., 2016) to avoid training the decoder with incorrect language tags. 1 1 Prism-ref = H(sys|ref) + H(ref|sys) 2 2 Prism-src = H(sys|src) Number of Languages Aharoni et al. (2019) found that performance of zero-shot translation in a related language pair increased substantially when increasing the number of languages from 5 languages and 25, with a performance plateau somewhere between 25 and 50 languages. We view paraphrasing as zero-shot translation be"
2020.emnlp-main.8,W04-3250,0,0.551967,"Missing"
2020.emnlp-main.8,2005.mtsummit-papers.11,0,0.311644,"Missing"
2020.emnlp-main.8,D18-2012,0,0.066392,"Missing"
2020.emnlp-main.8,D19-1167,0,0.0839227,"at our model generally penalizes any deviations (bolded) from the input sentence, but tends to penalize deviations which change the meaning of the sentence or introduce a disfluency more harshly than those which are fluent and adequate. Sentence-level BLEU with smoothing=1 (“sBLEU”) and LASER embedding cosine similarity (“LASER”) are shown for comparison. We note that LASER appears fairly insensitive to disfluencies, and sentenceBLEU struggles to reward valid paraphrases. data, denoted ‘zero-shot’ translation. Despite evidence that intermediate representations are not truly language-agnostic (Kudugunta et al., 2019), zero-shot translation has been shown successful, especially between related languages (Johnson et al., 2017; Gu et al., 2018; Pham et al., 2019). Schwenk, 2018), for example, trains a variant of NMT with a fixed-size intermediate representation in 93 languages. Embeddings produced by the encoder can be compared to measure intra- or interlingual semantic similarity. 3 Generative Paraphrasing Sentential paraphrasing can be accomplished by training an MT system on paraphrase examples instead of translation pairs (Quirk et al., 2004). While natural paraphrase datasets do exist (Quirk et al., 200"
2020.emnlp-main.8,2020.acl-main.448,0,0.493746,"the model with a particular system outIntroduction Machine Translation (MT) systems have improved dramatically in the past several years. This is largely due to advances in neural MT (NMT) methods, but the pace of improvement would not have been possible without automatic MT metrics, which provide immediate feedback on MT quality without the time and expense associated with obtaining human judgments of MT output. However, the improvements that existing automatic metrics helped enable are now causing the correlation between human judgments and automatic metrics to break down (Ma et al., 2019; Mathur et al., 2020) especially for BLEU (Papineni et al., 2002), which has been the de facto standard 90 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 90–121, c November 16–20, 2020. 2020 Association for Computational Linguistics put, we can use the model score to measure how well the system output paraphrases the human reference translation. Our model is not trained on any human quality judgements, which are not available in many domains and/or language pairs. The best possible MT output is one which perfectly matches a human reference; therefore, for evaluation,"
2020.emnlp-main.8,D18-1103,0,0.0235808,"ask of translation. This is important because it establishes that our method does not require building a state-of-theart multilingual NMT model in order to produce a state-of-the-art MT metric capable of evaluating state-of-the-art MT systems. 1 Related Work Multilingual NMT Multilingual NMT (Dong et al., 2015) has been shown to rival performance of single language pair models in high-resource languages (Aharoni et al., 2019; Arivazhagan et al., 2019) while also improving low-resource translation via transfer learning from higher-resource languages (Zoph et al., 2016; Nguyen and Chiang, 2017; Neubig and Hu, 2018). An extreme low-resource setting is where the system translates between languages seen during training, but in a language pair where it did not see any training 2 Except for Gujarati, where we had no training data. 91 https://github.com/thompsonb/prism Word-level paraphraser log probabilities H(out|in) sBLEU LASER Copy Jason went to school at the University of Madrid . <EOS> -0.08 -0.26 -0.16 -0.16 -0.12 -0.11 -0.14 -0.10 -0.10 -0.11 -0.10 -0.13 100.0 1.000 Disfluent Jason went school at University of Madrid . <EOS> -0.08 -0.26 -7.21 -0.12 -4.81 -0.10 -0.11 -0.11 -0.10 -1.43 35.5 0.989 -0.99"
2020.emnlp-main.8,I17-2050,0,0.0288001,"r than our model at the task of translation. This is important because it establishes that our method does not require building a state-of-theart multilingual NMT model in order to produce a state-of-the-art MT metric capable of evaluating state-of-the-art MT systems. 1 Related Work Multilingual NMT Multilingual NMT (Dong et al., 2015) has been shown to rival performance of single language pair models in high-resource languages (Aharoni et al., 2019; Arivazhagan et al., 2019) while also improving low-resource translation via transfer learning from higher-resource languages (Zoph et al., 2016; Nguyen and Chiang, 2017; Neubig and Hu, 2018). An extreme low-resource setting is where the system translates between languages seen during training, but in a language pair where it did not see any training 2 Except for Gujarati, where we had no training data. 91 https://github.com/thompsonb/prism Word-level paraphraser log probabilities H(out|in) sBLEU LASER Copy Jason went to school at the University of Madrid . <EOS> -0.08 -0.26 -0.16 -0.16 -0.12 -0.11 -0.14 -0.10 -0.10 -0.11 -0.10 -0.13 100.0 1.000 Disfluent Jason went school at University of Madrid . <EOS> -0.08 -0.26 -7.21 -0.12 -4.81 -0.10 -0.11 -0.11 -0.10 -"
2020.emnlp-main.8,N19-4009,0,0.0740201,"Missing"
2020.emnlp-main.8,P09-1034,0,0.102576,"Missing"
2020.emnlp-main.8,W18-6455,0,0.0565335,"Missing"
2020.emnlp-main.8,P02-1040,0,0.127998,"roduction Machine Translation (MT) systems have improved dramatically in the past several years. This is largely due to advances in neural MT (NMT) methods, but the pace of improvement would not have been possible without automatic MT metrics, which provide immediate feedback on MT quality without the time and expense associated with obtaining human judgments of MT output. However, the improvements that existing automatic metrics helped enable are now causing the correlation between human judgments and automatic metrics to break down (Ma et al., 2019; Mathur et al., 2020) especially for BLEU (Papineni et al., 2002), which has been the de facto standard 90 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 90–121, c November 16–20, 2020. 2020 Association for Computational Linguistics put, we can use the model score to measure how well the system output paraphrases the human reference translation. Our model is not trained on any human quality judgements, which are not available in many domains and/or language pairs. The best possible MT output is one which perfectly matches a human reference; therefore, for evaluation, an ideal paraphraser would be one with an ou"
2020.emnlp-main.8,W19-5202,0,0.0950239,"Missing"
2020.emnlp-main.8,W15-3049,0,0.1965,"Missing"
2020.emnlp-main.8,W17-4770,0,0.0471085,"Missing"
2020.emnlp-main.8,W18-6319,1,0.898862,"Missing"
2020.emnlp-main.8,C16-1275,0,0.0332859,"he encoder can be compared to measure intra- or interlingual semantic similarity. 3 Generative Paraphrasing Sentential paraphrasing can be accomplished by training an MT system on paraphrase examples instead of translation pairs (Quirk et al., 2004). While natural paraphrase datasets do exist (Quirk et al., 2004; Coster and Kauchak, 2011; Fader et al., 2013; Lin et al., 2014; Federmann et al., 2019), they are somewhat limited. An alternative is to start with much more plentiful bitext and back-translate one side into the language of the other to create synthetic paraphrases on which to train (Prakash et al., 2016; Wieting and Gimpel, 2018; Hu et al., 2019a,b,c). Tiedemann and Scherrer (2019) propose using paraphrasing as a way to measure the semantic abstraction of multilingual NMT. They also propose using a multilingual NMT model as a generative paraphraser.3 Method We propose using a paraphraser to force-decode and estimate probabilities of MT system outputs, conditioned on their corresponding human references. Let p(yt |yi<t , x) be the probability our paraphraser assigns to the tth token in output sequence y, given the previous output tokens yi<t and the input sequence x. Table 1 shows an example"
2020.emnlp-main.8,W04-3219,0,0.383036,"Missing"
2020.emnlp-main.8,2020.acl-main.704,0,0.395322,"also be measured at the character level (Popovi´c, 2015, 2017) or using edit distance (Snover et al., 2006). Many metrics use word- and/or sentencelevel embeddings, including ReVal (Gupta et al., 2015), RUSE (Shimanaka et al., 2018), WMDO (Chow et al., 2019), and ESIM (Mathur et al., 2019). MEANT (Lo and Wu, 2011) and MEANT 2.0 (Lo, 2017) measure similarity between semantic frames and role fillers. State-of-the-art methods including YiSi (Lo, 2019) and BERTscore (Zhang et al., 2019, 2020) rely on contextualized embeddings (Devlin et al., 2019) trained on large (non-parallel) corpora. BLEURT (Sellam et al., 2020) applies fine tuning of BERT, including training on prior human judgements. In contrast, our work exploits parallel bitext and doesn’t require training on human judgements. Paraphrase Databases Prior work explored using parallel bitext to identify phrase level paraphrases (Bannard and Callison-Burch, 2005; Ganitkevitch et al., 2013) including bitext in multiple language pairs (Ganitkevitch and Callison-Burch, 2014). Paraphrase tables were, in turn, used in MT metrics to reward systems for paraphrasing words (Banerjee and Lavie, 2005) or phrases (Zhou et al., 2006; Denkowski and Lavie, 2010) fr"
2020.emnlp-main.8,W18-6456,0,0.0300158,"e 1 illustrates our method, which we denote Prism (Probability is the metric). We train a single model in 39 languages and show that it: We release our model, metrics toolkit, and preprocessed training data.2 2 MT Metrics Early MT metrics like BLEU (Papineni et al., 2002) and NIST (Doddington, 2002) use token-level n-gram overlap between the MT output and the human reference. Overlap can also be measured at the character level (Popovi´c, 2015, 2017) or using edit distance (Snover et al., 2006). Many metrics use word- and/or sentencelevel embeddings, including ReVal (Gupta et al., 2015), RUSE (Shimanaka et al., 2018), WMDO (Chow et al., 2019), and ESIM (Mathur et al., 2019). MEANT (Lo and Wu, 2011) and MEANT 2.0 (Lo, 2017) measure similarity between semantic frames and role fillers. State-of-the-art methods including YiSi (Lo, 2019) and BERTscore (Zhang et al., 2019, 2020) rely on contextualized embeddings (Devlin et al., 2019) trained on large (non-parallel) corpora. BLEURT (Sellam et al., 2020) applies fine tuning of BERT, including training on prior human judgements. In contrast, our work exploits parallel bitext and doesn’t require training on human judgements. Paraphrase Databases Prior work explored"
2020.emnlp-main.8,2006.amta-papers.25,0,0.294709,"“Quality estimation (QE) as a metric” (Fonseca et al., 2019) by conditioning on the source instead of the reference. Figure 1 illustrates our method, which we denote Prism (Probability is the metric). We train a single model in 39 languages and show that it: We release our model, metrics toolkit, and preprocessed training data.2 2 MT Metrics Early MT metrics like BLEU (Papineni et al., 2002) and NIST (Doddington, 2002) use token-level n-gram overlap between the MT output and the human reference. Overlap can also be measured at the character level (Popovi´c, 2015, 2017) or using edit distance (Snover et al., 2006). Many metrics use word- and/or sentencelevel embeddings, including ReVal (Gupta et al., 2015), RUSE (Shimanaka et al., 2018), WMDO (Chow et al., 2019), and ESIM (Mathur et al., 2019). MEANT (Lo and Wu, 2011) and MEANT 2.0 (Lo, 2017) measure similarity between semantic frames and role fillers. State-of-the-art methods including YiSi (Lo, 2019) and BERTscore (Zhang et al., 2019, 2020) rely on contextualized embeddings (Devlin et al., 2019) trained on large (non-parallel) corpora. BLEURT (Sellam et al., 2020) applies fine tuning of BERT, including training on prior human judgements. In contrast,"
2020.emnlp-main.8,W19-5359,0,0.167649,"Missing"
2020.emnlp-main.8,D17-1026,0,0.0773584,"ns to the tth token in output sequence y, given the previous output tokens yi<t and the input sequence x. Table 1 shows an example of how token-level probabilities from our model (described in §4) penalize both fluency and adequacy errors given a human reference. We consider two ways of combining token-level probabilities from the model—sequence-level log probability (G) and average token-level log probability (H): G(y|x) = |y| X t=1 Semantic Similarity Parallel corpora in many language pairs have been used to produce fixed-size, multilingual sentence representations (Schwenk and Douze, 2017; Wieting et al., 2017; Artetxe and Schwenk, 2018; Wieting et al., 2019; Raganato et al., 2019). LASER (Artetxe and H(y|x) = log p(yt |yi<t , x) 1 G(y|x) |y| Let sys denote an MT system output, ref denote a human reference, and src denote the source. We expect scoring sys conditioned on ref to be most indicative of the quality of sys. However, we also explore scoring ref conditioned on sys as we find qualitatively that output sentences which drop some 3 We find that generating from a well trained multilingual NMT system tends to produce copies of the input, as opposed to interesting paraphrases (see Appendix A). 92"
2020.emnlp-main.8,W19-5410,0,0.0751778,"Missing"
2020.emnlp-main.8,W19-5360,0,0.0448948,"Missing"
2020.emnlp-main.8,N06-1057,0,0.129595,"-parallel) corpora. BLEURT (Sellam et al., 2020) applies fine tuning of BERT, including training on prior human judgements. In contrast, our work exploits parallel bitext and doesn’t require training on human judgements. Paraphrase Databases Prior work explored using parallel bitext to identify phrase level paraphrases (Bannard and Callison-Burch, 2005; Ganitkevitch et al., 2013) including bitext in multiple language pairs (Ganitkevitch and Callison-Burch, 2014). Paraphrase tables were, in turn, used in MT metrics to reward systems for paraphrasing words (Banerjee and Lavie, 2005) or phrases (Zhou et al., 2006; Denkowski and Lavie, 2010) from the human reference. Our work can be viewed as extending this idea to the sentence level, without having to enumerate the millions or billions of paraphrases (Dreyer and Marcu, 2012) for each sentence. • Outperforms or ties with prior metrics and several contrastive neural methods on the segment-level WMT 2019 MT metrics task in every language pair;1 • Is able to discriminate between very strong neural systems at the system level, addressing a problem raised at WMT 2019; and • Significantly outperforms all QE metrics submitted to the WMT 2019 QE shared task Fi"
2020.emnlp-main.8,D16-1163,0,0.0266614,"substantially better than our model at the task of translation. This is important because it establishes that our method does not require building a state-of-theart multilingual NMT model in order to produce a state-of-the-art MT metric capable of evaluating state-of-the-art MT systems. 1 Related Work Multilingual NMT Multilingual NMT (Dong et al., 2015) has been shown to rival performance of single language pair models in high-resource languages (Aharoni et al., 2019; Arivazhagan et al., 2019) while also improving low-resource translation via transfer learning from higher-resource languages (Zoph et al., 2016; Nguyen and Chiang, 2017; Neubig and Hu, 2018). An extreme low-resource setting is where the system translates between languages seen during training, but in a language pair where it did not see any training 2 Except for Gujarati, where we had no training data. 91 https://github.com/thompsonb/prism Word-level paraphraser log probabilities H(out|in) sBLEU LASER Copy Jason went to school at the University of Madrid . <EOS> -0.08 -0.26 -0.16 -0.16 -0.12 -0.11 -0.14 -0.10 -0.10 -0.11 -0.10 -0.13 100.0 1.000 Disfluent Jason went school at University of Madrid . <EOS> -0.08 -0.26 -7.21 -0.12 -4.81"
2020.emnlp-main.8,E06-1031,0,\N,Missing
2020.emnlp-main.8,W05-0909,0,\N,Missing
2020.emnlp-main.8,P11-1023,0,\N,Missing
2020.emnlp-main.8,P13-1158,0,\N,Missing
2020.emnlp-main.8,P05-1074,0,\N,Missing
2020.emnlp-main.8,N10-1031,0,\N,Missing
2020.emnlp-main.8,C12-2044,0,\N,Missing
2020.emnlp-main.8,W15-3050,0,\N,Missing
2020.emnlp-main.8,P16-2013,0,\N,Missing
2020.emnlp-main.8,W16-2342,0,\N,Missing
2020.emnlp-main.8,D17-1262,0,\N,Missing
2020.emnlp-main.8,W17-4767,0,\N,Missing
2020.emnlp-main.8,P18-2037,0,\N,Missing
2020.emnlp-main.8,W18-6401,0,\N,Missing
2020.emnlp-main.8,N19-1388,0,\N,Missing
2020.emnlp-main.8,W19-2005,0,\N,Missing
2020.emnlp-main.8,N19-1090,1,\N,Missing
2020.emnlp-main.8,P19-1453,0,\N,Missing
2020.emnlp-main.8,P19-1269,0,\N,Missing
2020.emnlp-main.8,N19-1423,0,\N,Missing
2020.emnlp-main.8,P18-1042,0,\N,Missing
2020.emnlp-main.8,W19-4304,0,\N,Missing
2020.emnlp-main.8,W19-5358,0,\N,Missing
2020.emnlp-main.8,W19-5401,0,\N,Missing
2020.emnlp-main.8,D19-5503,0,\N,Missing
2020.emnlp-main.8,J13-3001,0,\N,Missing
2020.emnlp-main.8,D15-1124,0,\N,Missing
2020.emnlp-main.8,W17-4768,0,\N,Missing
2020.emnlp-main.8,W18-6450,0,\N,Missing
2020.emnlp-main.8,W19-5302,0,\N,Missing
2020.eval4nlp-1.7,abdelali-etal-2014-amara,0,0.042145,"Missing"
2020.eval4nlp-1.7,P19-1310,0,0.0259153,"Missing"
2020.eval4nlp-1.7,P02-1040,0,0.112512,"se: r2 for Spearman’s = -0.001, slope = -0.010; r2 for Pearson’s = -0.002, slope = -0.083. Best viewed in color. tem would serve as an upstream component of the pipeline. It is less likely for translations to need to be fully valid to be useful as compared to some other MT tasks. CLIR could benefit from combining the terms from several translation outputs regardless of if each entire sentence is perfectly valid. In this way, a measure that can assign partial credit to translations by matching n-grams as well as weighting all translations equally may be appropriate. For this, we turn to B LEU (Papineni et al., 2002), which computes n-gram overlap between a system’s translations and the available references. This raises the question of how many references we should use when we have very many available, and which of the system translations we should be using in this computation. The STAPLE dataset provides an opportunity to explore this question. In this section, we compute B LEU measures with different numbers of references, to different depths in the n-best list. We find that at deep depths with many references B LEU ranks systems similarly to MAP, but that with fewer references its behavior is quite dif"
2020.eval4nlp-1.7,2021.eacl-main.115,0,0.0417359,"Missing"
2020.eval4nlp-1.7,N12-1017,0,0.0231632,"conditions are a representative set of sentences to be translated (in the STAPLE task, the prompts), the items are system-produced translations, and validity is whether a translation is proper (i.e., present in the STAPLE gold translations). 3.2 MAP’s reliance on binary validity rather than the preference order among valid translations simplified the generation of a gold standard, but the implicit assumption that the reference set of valid translations is complete is a potential concern. Due to the richness of human language, most sentences would admit an immense number of valid translations (Dreyer and Marcu, 2012). Even the STAPLE dataset used in this paper, which contains hundreds of valid reference translations for many sentences, is surely still not complete. This effect results in systems being penalized for false negatives, receiving lower MAP scores than they should. However, when our goal is to compare systems, we are most interested in relative, not absolute, scores. So the question to be answered is whether missing data in the ground truth adversely affects comparisons between systems. Zobel (1998) introduced a clever way to characterize such an effect. The key idea is to ablate the ground tru"
2020.eval4nlp-1.7,2020.ngt-1.22,1,0.770259,"Missing"
2020.eval4nlp-1.7,2005.mtsummit-papers.11,0,0.0371673,"Missing"
2020.eval4nlp-1.7,L16-1147,0,0.0568413,"Missing"
2020.eval4nlp-1.7,2020.ngt-1.28,0,0.012118,"fully consider what questions to ask when evaluating systems. The measures we propose are illustrative as answers to our research questions, but are not the only solutions; many others might work. We aim to provide groundwork and encourage future work on the topic. Our investigation is made possible by the recent availability of annotations created for the Duolingo Simultaneous Translation and Paraphrase for Language Education (STAPLE) shared task, which contains an extensive (although not necessarily exhaustive) set of valid translations for each of several thousand “input prompt” sentences (Mayhew et al., 2020). 2 JA some bad systems, many good ones, and many incremental variations in between, especially at the top end. These systems ranked among the best for these languages on the STAPLE leaderboard. All were variations of the following standard training procedure. We used Transformer architectures (Vaswani et al., 2017) trained with fairseq (Ott et al., 2019). Models included 6 encoder and decoder layers, a model size of 512, a feed forward layer size of 2048, and 8 attention heads. Models were trained with the A DAM optimizer (Kingma and Ba, 2015) with a dropout size of 0.1 and an effective batch"
2020.findings-emnlp.82,D18-1113,0,0.0306496,"Missing"
2020.findings-emnlp.82,W08-0330,0,0.0335983,"rjee and Lavie, 2005; Kauchak and Barzilay, 2006; Denkowski and Lavie, 2014). More automatic methods of identifying paraphrases have also been developed. An early example is ParaEval (Zhou et al., 2006), which provides local paraphrase support using paraphrase sets automatically extracted from MT phrase tables. More recently, Apidianaki et al. (2018) exploit contextual word embeddings to build automatic HyTER networks. However they achieve mixed results, particularly when evaluating high performing (neural) models. The use of MT systems to produce paraphrases has also been studied previously. Albrecht and Hwa (2008) create pseudo-references by using out-of-the-box MT systems and see improved correlations with human judgments, helped by the systems being of better quality than those evaluated. This method was extended by Yoshimura et al. (2019), who filter the pseudo-references for quality. An alternative strategy is to use MT-style systems as paraphrasers, applied to the references. Madnani et al. (2007) show that additional (paraphrased) references, even noisy ones, reduce the number of human references needed to tune an SMT system, without significantly affecting MT quality. However their aim for cover"
2020.findings-emnlp.82,W11-2107,0,0.107688,"Missing"
2020.findings-emnlp.82,N18-2077,0,0.0233404,"e clustering method. Related Work Paraphrasing for MT evaluation There is a long history of using paraphrasing to overcome the limitations of B LEU-style metrics. Some early approaches rely on external resources (e.g. WordNet) to provide support for synonym matching (Banerjee and Lavie, 2005; Kauchak and Barzilay, 2006; Denkowski and Lavie, 2014). More automatic methods of identifying paraphrases have also been developed. An early example is ParaEval (Zhou et al., 2006), which provides local paraphrase support using paraphrase sets automatically extracted from MT phrase tables. More recently, Apidianaki et al. (2018) exploit contextual word embeddings to build automatic HyTER networks. However they achieve mixed results, particularly when evaluating high performing (neural) models. The use of MT systems to produce paraphrases has also been studied previously. Albrecht and Hwa (2008) create pseudo-references by using out-of-the-box MT systems and see improved correlations with human judgments, helped by the systems being of better quality than those evaluated. This method was extended by Yoshimura et al. (2019), who filter the pseudo-references for quality. An alternative strategy is to use MT-style system"
2020.findings-emnlp.82,W14-3348,0,0.0330786,"h and for each one a different hypothesis is selected. We adopt Shu et al.’s approach here, due to the automatic nature of constraint selection and the flexibility afforded by constraint definition, allowing us to test different types of diversity by varying the type of sentence clustering method. Related Work Paraphrasing for MT evaluation There is a long history of using paraphrasing to overcome the limitations of B LEU-style metrics. Some early approaches rely on external resources (e.g. WordNet) to provide support for synonym matching (Banerjee and Lavie, 2005; Kauchak and Barzilay, 2006; Denkowski and Lavie, 2014). More automatic methods of identifying paraphrases have also been developed. An early example is ParaEval (Zhou et al., 2006), which provides local paraphrase support using paraphrase sets automatically extracted from MT phrase tables. More recently, Apidianaki et al. (2018) exploit contextual word embeddings to build automatic HyTER networks. However they achieve mixed results, particularly when evaluating high performing (neural) models. The use of MT systems to produce paraphrases has also been studied previously. Albrecht and Hwa (2008) create pseudo-references by using out-of-the-box MT"
2020.findings-emnlp.82,D18-1045,0,0.0544253,"Missing"
2020.findings-emnlp.82,W18-2716,0,0.0166249,"those longer than 100 words and then segment into subwords using SentencePiece (Kudo and Richardson, 2018) (unigram model (Kudo, 2018) of size 16k). The data splits are created by randomly shuffling the data and reserving 3k pairs each for dev and test. For syntactic sentence encoding methods, we use the Berkeley Parser (Petrov et al., 2006) (internal tokenisation and prioritizing accuracy) and prune trees to a depth of 4 for ≈6M distinct trees.4 Paraphrase models are Transformer base models (Vaswani et al., 2017) (Cf. App. B for details). All models are trained using the Marian NMT toolkit (Junczys-Dowmunt et al., 2018), except for S AMPLED and the constraint approach, for which we use the Sockeye toolkit (Hieber et al., 2018), since Marian does not support these features. For baseline models, we produce n additional references by taking the n-best in the beam (using a beam size of 20, which is the maximum number of additional references we test). For models using cluster codes, paraphrases are produced by selecting the n-best cluster codes at the first decoding step and then decoding each of these hypotheses using separate beam searches (of size 6). n=1 ( 1 if c &gt; r BP = 1−r/c e if c ≤ r P P #clip (ngram) h"
2020.findings-emnlp.82,D19-5503,0,0.0188953,"it is a surface metric that operates on explicit n-gram overlap (see. (1) showing two ade1 Our code and outputs are available at https:// github.com/rbawden/paraphrasing-bleu. 2 See Sec. 4.2 and (Papineni et al., 2002, §1.1) for details. 918 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 918–932 c November 16 - 20, 2020. 2020 Association for Computational Linguistics good enough for use in a final evaluation metric. Despite the attention afforded to the task, success has been limited by the fact that until recently, there were no good sentence-level paraphrasers (Federmann et al. (2019) showed that neural paraphrasers can now outperform humans for adequacy and cost). Attempts (e.g. Napoles et al., 2016) using earlier MT paradigms were not able to produce fluent output, and publicly available paraphrase datasets have only been recently released (Wieting and Gimpel, 2018; Hu et al., 2019a). Moreover, most works focus on synonym substitution rather than more radical changes in sentence structure, limiting the coverage achieved. puts. This allows us to compare the effects of diversity against an upper bound that has good coverage. We anchor our study by comparing automatically p"
2020.findings-emnlp.82,N06-1058,0,0.182246,"e dominant metric for MT evaluation has been challenged (e.g., Callison-Burch et al. (2006), Mathur et al. (2020)). Such work typically uses only a single reference, however, which is a deficient form of the metric, since one of B LEU’s raisons d’ˆetre was to permit the use of multiple references, in a bid to represent “legitimate differences in word choice and word order.” Unfortunately, multiple references are rarely available due to the high cost and effort of producing them. One way to inexpensively create them is with automatic paraphrasing. This has been tried before (Zhou et al., 2006; Kauchak and Barzilay, 2006), but only recently have paraphrase systems become good enough to generate fluent, high quality sentential paraphrases (with neural MT-style systems). Moreover, it is currently unclear (i) whether adding automatically paraphrased references can provide the diversity needed to better cover the translation space, and (ii) whether this increased coverage overlaps with observed and valid MT outputs, in turn improving B LEU’s correlation with human judgments. We explore these questions, testing on all intoEnglish directions of the WMT19 metrics shared task (Ma et al., 2019) at the system and segmen"
2020.findings-emnlp.82,P18-1007,0,0.0192751,"s computed by averaging modified n-gram precisions (pn , n = 1..4) and multiplying this product by a brevity penalty (BP), which penalizes overly short translations and thereby works to balance precision with recall: ! N X wn log pn (1) B LEU = BP · exp We train our paraphrasers using data from Parabank 2 (Hu et al., 2019b), containing ≈20M sentences with up to 5 paraphrases each, of which we use the first paraphrase only. We preprocess by removing duplicate sentences and those longer than 100 words and then segment into subwords using SentencePiece (Kudo and Richardson, 2018) (unigram model (Kudo, 2018) of size 16k). The data splits are created by randomly shuffling the data and reserving 3k pairs each for dev and test. For syntactic sentence encoding methods, we use the Berkeley Parser (Petrov et al., 2006) (internal tokenisation and prioritizing accuracy) and prune trees to a depth of 4 for ≈6M distinct trees.4 Paraphrase models are Transformer base models (Vaswani et al., 2017) (Cf. App. B for details). All models are trained using the Marian NMT toolkit (Junczys-Dowmunt et al., 2018), except for S AMPLED and the constraint approach, for which we use the Sockeye toolkit (Hieber et al., 20"
2020.findings-emnlp.82,W13-2305,0,0.0246229,"uency trees into hidden vectors using a TreeLSTM-based recursive autoencoder, with the difference that we use k-means clustering to make the method more comparable to the above, and we encode syntactic information only. Metric evaluation For each set of extra references, we produce multireference B LEU and SENT B LEU metrics, which we use to score all into-English system outputs from the WMT19 news task.3 We evaluate the 3 920 http://statmt.org/wmt19/results.html. scores as in the metrics task (Ma et al., 2019), by calculating the correlation with manual direct assessments (DA) of MT quality (Graham et al., 2013). System-level scores are evaluated using Pearson’s r and statistical significance of improvements (against single-reference B LEU) using the Williams test (Williams, 1959). Segment-level correlations are calculated using Kendall’s τ (and significance against single-reference SENT B LEU with bootstrap resampling) on the DA assessments transformed into relative rankings. 4.2 where randomly selected cluster codes are used at training and test time. As a topline, we compare against manually paraphrased references (H UMAN), which we produce for a subset of 500 sentences from the de–en test set. Tw"
2020.findings-emnlp.82,D18-2012,0,0.0121667,"r to the reader, we review it here. B LEU is computed by averaging modified n-gram precisions (pn , n = 1..4) and multiplying this product by a brevity penalty (BP), which penalizes overly short translations and thereby works to balance precision with recall: ! N X wn log pn (1) B LEU = BP · exp We train our paraphrasers using data from Parabank 2 (Hu et al., 2019b), containing ≈20M sentences with up to 5 paraphrases each, of which we use the first paraphrase only. We preprocess by removing duplicate sentences and those longer than 100 words and then segment into subwords using SentencePiece (Kudo and Richardson, 2018) (unigram model (Kudo, 2018) of size 16k). The data splits are created by randomly shuffling the data and reserving 3k pairs each for dev and test. For syntactic sentence encoding methods, we use the Berkeley Parser (Petrov et al., 2006) (internal tokenisation and prioritizing accuracy) and prune trees to a depth of 4 for ≈6M distinct trees.4 Paraphrase models are Transformer base models (Vaswani et al., 2017) (Cf. App. B for details). All models are trained using the Marian NMT toolkit (Junczys-Dowmunt et al., 2018), except for S AMPLED and the constraint approach, for which we use the Sockey"
2020.findings-emnlp.82,W18-1820,1,0.834435,"odel (Kudo, 2018) of size 16k). The data splits are created by randomly shuffling the data and reserving 3k pairs each for dev and test. For syntactic sentence encoding methods, we use the Berkeley Parser (Petrov et al., 2006) (internal tokenisation and prioritizing accuracy) and prune trees to a depth of 4 for ≈6M distinct trees.4 Paraphrase models are Transformer base models (Vaswani et al., 2017) (Cf. App. B for details). All models are trained using the Marian NMT toolkit (Junczys-Dowmunt et al., 2018), except for S AMPLED and the constraint approach, for which we use the Sockeye toolkit (Hieber et al., 2018), since Marian does not support these features. For baseline models, we produce n additional references by taking the n-best in the beam (using a beam size of 20, which is the maximum number of additional references we test). For models using cluster codes, paraphrases are produced by selecting the n-best cluster codes at the first decoding step and then decoding each of these hypotheses using separate beam searches (of size 6). n=1 ( 1 if c &gt; r BP = 1−r/c e if c ≤ r P P #clip (ngram) h∈H Pngram∈h pn = P , h0 ∈H ngram’∈h0 # (ngram’) Paraphrase model training (2) (3) with c and r the lengths of"
2020.findings-emnlp.82,W19-5302,0,0.0903,"et al., 2006; Kauchak and Barzilay, 2006), but only recently have paraphrase systems become good enough to generate fluent, high quality sentential paraphrases (with neural MT-style systems). Moreover, it is currently unclear (i) whether adding automatically paraphrased references can provide the diversity needed to better cover the translation space, and (ii) whether this increased coverage overlaps with observed and valid MT outputs, in turn improving B LEU’s correlation with human judgments. We explore these questions, testing on all intoEnglish directions of the WMT19 metrics shared task (Ma et al., 2019) at the system and segment level. We compare two approaches: (i) generating diverse references with the hope of covering as much of the valid translation space as possible, and (ii) more directly targeting the relevant areas of the translation space by generating paraphrases that contain n-grams selected from the system outIntroduction There is rarely a single correct way to translate a sentence; work attempting to encode the entire translation space of a sentence suggests there may be billions of valid translations (Dreyer and Marcu, 2012). Despite this, in machine translation (MT), system ou"
2020.findings-emnlp.82,N19-1090,1,0.884803,"Missing"
2020.findings-emnlp.82,W07-0716,0,0.0475714,"utomatic HyTER networks. However they achieve mixed results, particularly when evaluating high performing (neural) models. The use of MT systems to produce paraphrases has also been studied previously. Albrecht and Hwa (2008) create pseudo-references by using out-of-the-box MT systems and see improved correlations with human judgments, helped by the systems being of better quality than those evaluated. This method was extended by Yoshimura et al. (2019), who filter the pseudo-references for quality. An alternative strategy is to use MT-style systems as paraphrasers, applied to the references. Madnani et al. (2007) show that additional (paraphrased) references, even noisy ones, reduce the number of human references needed to tune an SMT system, without significantly affecting MT quality. However their aim for coverage over quality means that their paraphrases are unlikely to be 3 Generating paraphrased references We look at two ways to produce paraphrases of English references using English–English NMT architectures. The first (Sec. 3.1) aims for maximal lexical and syntactic diversity, in a bid to 919 3.2 better cover the space of valid translations. In contrast, the second (Sec. 3.2) aims to produce p"
2020.findings-emnlp.82,K19-1005,1,0.873661,"Missing"
2020.findings-emnlp.82,E17-1083,0,0.0268856,"crease coverage. Manual paraphrasing does give the best systemlevel B LEU results, but even these gains are relatively limited, suggesting that diversity alone has its limits in addressing weaknesses of surfacebased evaluation metrics like B LEU. 2 Structurally diverse outputs Diverse generation is important to ensure a wide coverage of possible translations. Diversity, both lexical and structural, has been a major concern of text generation tasks (Colin and Gardent, 2018; Iyyer et al., 2018). State-of-the-art neural MT-style text generation models used for paraphrasing (Prakash et al., 2016; Mallinson et al., 2017) typically suffer from limited diversity in the beam. Techniques such as sampling from the model distribution or from noisy outputs have been proposed to tackle this (Edunov et al., 2018) but can harm output quality. An effective strategy to encourage structural diversity is to add syntactic information (which can be varied) to the generated text. The constraints can be specified manually, for example by adding a parse tree (Colin and Gardent, 2018; Iyyer et al., 2018) or by specifying more abstract constraints such as rewriting embeddings (Xu et al., 2018). A similar but more flexible approac"
2020.findings-emnlp.82,N18-1170,0,0.0203096,"diversity (and less so adequacy), but see diminishing returns, and fall short of the nondiverse method designed just to increase coverage. Manual paraphrasing does give the best systemlevel B LEU results, but even these gains are relatively limited, suggesting that diversity alone has its limits in addressing weaknesses of surfacebased evaluation metrics like B LEU. 2 Structurally diverse outputs Diverse generation is important to ensure a wide coverage of possible translations. Diversity, both lexical and structural, has been a major concern of text generation tasks (Colin and Gardent, 2018; Iyyer et al., 2018). State-of-the-art neural MT-style text generation models used for paraphrasing (Prakash et al., 2016; Mallinson et al., 2017) typically suffer from limited diversity in the beam. Techniques such as sampling from the model distribution or from noisy outputs have been proposed to tackle this (Edunov et al., 2018) but can harm output quality. An effective strategy to encourage structural diversity is to add syntactic information (which can be varied) to the generated text. The constraints can be specified manually, for example by adding a parse tree (Colin and Gardent, 2018; Iyyer et al., 2018)"
2020.findings-emnlp.82,2020.acl-main.448,0,0.0427189,"hrases elicited to maximize diversity, suggesting inherent limitations to B LEU’s capacity to correctly exploit multiple references. Surprisingly, we also find that adequacy appears to be less important, as shown by the high results of a strong sampling approach, which even beats human paraphrases when used with sentencelevel B LEU.1 1 Matt Post3 (1) Ref: This did not bother anybody . MT1 : This didn ’t bother anybody . MT2 : Nobody was bothered by this . Almost since its creation, B LEU’s status as the dominant metric for MT evaluation has been challenged (e.g., Callison-Burch et al. (2006), Mathur et al. (2020)). Such work typically uses only a single reference, however, which is a deficient form of the metric, since one of B LEU’s raisons d’ˆetre was to permit the use of multiple references, in a bid to represent “legitimate differences in word choice and word order.” Unfortunately, multiple references are rarely available due to the high cost and effort of producing them. One way to inexpensively create them is with automatic paraphrasing. This has been tried before (Zhou et al., 2006; Kauchak and Barzilay, 2006), but only recently have paraphrase systems become good enough to generate fluent, hig"
2020.findings-emnlp.82,P15-1150,0,0.0442482,"Missing"
2020.findings-emnlp.82,E06-1015,0,0.134293,"Missing"
2020.findings-emnlp.82,N16-3013,1,0.835983,"able at https:// github.com/rbawden/paraphrasing-bleu. 2 See Sec. 4.2 and (Papineni et al., 2002, §1.1) for details. 918 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 918–932 c November 16 - 20, 2020. 2020 Association for Computational Linguistics good enough for use in a final evaluation metric. Despite the attention afforded to the task, success has been limited by the fact that until recently, there were no good sentence-level paraphrasers (Federmann et al. (2019) showed that neural paraphrasers can now outperform humans for adequacy and cost). Attempts (e.g. Napoles et al., 2016) using earlier MT paradigms were not able to produce fluent output, and publicly available paraphrase datasets have only been recently released (Wieting and Gimpel, 2018; Hu et al., 2019a). Moreover, most works focus on synonym substitution rather than more radical changes in sentence structure, limiting the coverage achieved. puts. This allows us to compare the effects of diversity against an upper bound that has good coverage. We anchor our study by comparing automatically produced references against humanproduced ones on a subset of our data. Our experiments show that adding paraphrased ref"
2020.findings-emnlp.82,P02-1040,0,0.109349,"of covering as much of the valid translation space as possible, and (ii) more directly targeting the relevant areas of the translation space by generating paraphrases that contain n-grams selected from the system outIntroduction There is rarely a single correct way to translate a sentence; work attempting to encode the entire translation space of a sentence suggests there may be billions of valid translations (Dreyer and Marcu, 2012). Despite this, in machine translation (MT), system outputs are usually evaluated against a single reference. This especially affects MT’s dominant metric, B LEU (Papineni et al., 2002), since it is a surface metric that operates on explicit n-gram overlap (see. (1) showing two ade1 Our code and outputs are available at https:// github.com/rbawden/paraphrasing-bleu. 2 See Sec. 4.2 and (Papineni et al., 2002, §1.1) for details. 918 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 918–932 c November 16 - 20, 2020. 2020 Association for Computational Linguistics good enough for use in a final evaluation metric. Despite the attention afforded to the task, success has been limited by the fact that until recently, there were no good sentence-level paraph"
2020.findings-emnlp.82,P18-1042,0,0.0207225,"Linguistics: EMNLP 2020, pages 918–932 c November 16 - 20, 2020. 2020 Association for Computational Linguistics good enough for use in a final evaluation metric. Despite the attention afforded to the task, success has been limited by the fact that until recently, there were no good sentence-level paraphrasers (Federmann et al. (2019) showed that neural paraphrasers can now outperform humans for adequacy and cost). Attempts (e.g. Napoles et al., 2016) using earlier MT paradigms were not able to produce fluent output, and publicly available paraphrase datasets have only been recently released (Wieting and Gimpel, 2018; Hu et al., 2019a). Moreover, most works focus on synonym substitution rather than more radical changes in sentence structure, limiting the coverage achieved. puts. This allows us to compare the effects of diversity against an upper bound that has good coverage. We anchor our study by comparing automatically produced references against humanproduced ones on a subset of our data. Our experiments show that adding paraphrased references rarely hurts B LEU and can provide moderate gains in its correlation with human judgments. Where it does help, the gains are correlated with diversity (and less"
2020.findings-emnlp.82,P06-1055,0,0.00878742,"ion with recall: ! N X wn log pn (1) B LEU = BP · exp We train our paraphrasers using data from Parabank 2 (Hu et al., 2019b), containing ≈20M sentences with up to 5 paraphrases each, of which we use the first paraphrase only. We preprocess by removing duplicate sentences and those longer than 100 words and then segment into subwords using SentencePiece (Kudo and Richardson, 2018) (unigram model (Kudo, 2018) of size 16k). The data splits are created by randomly shuffling the data and reserving 3k pairs each for dev and test. For syntactic sentence encoding methods, we use the Berkeley Parser (Petrov et al., 2006) (internal tokenisation and prioritizing accuracy) and prune trees to a depth of 4 for ≈6M distinct trees.4 Paraphrase models are Transformer base models (Vaswani et al., 2017) (Cf. App. B for details). All models are trained using the Marian NMT toolkit (Junczys-Dowmunt et al., 2018), except for S AMPLED and the constraint approach, for which we use the Sockeye toolkit (Hieber et al., 2018), since Marian does not support these features. For baseline models, we produce n additional references by taking the n-best in the beam (using a beam size of 20, which is the maximum number of additional r"
2020.findings-emnlp.82,W19-5360,0,0.020043,"ase support using paraphrase sets automatically extracted from MT phrase tables. More recently, Apidianaki et al. (2018) exploit contextual word embeddings to build automatic HyTER networks. However they achieve mixed results, particularly when evaluating high performing (neural) models. The use of MT systems to produce paraphrases has also been studied previously. Albrecht and Hwa (2008) create pseudo-references by using out-of-the-box MT systems and see improved correlations with human judgments, helped by the systems being of better quality than those evaluated. This method was extended by Yoshimura et al. (2019), who filter the pseudo-references for quality. An alternative strategy is to use MT-style systems as paraphrasers, applied to the references. Madnani et al. (2007) show that additional (paraphrased) references, even noisy ones, reduce the number of human references needed to tune an SMT system, without significantly affecting MT quality. However their aim for coverage over quality means that their paraphrases are unlikely to be 3 Generating paraphrased references We look at two ways to produce paraphrases of English references using English–English NMT architectures. The first (Sec. 3.1) aims"
2020.findings-emnlp.82,W18-6319,1,0.838428,"are against manually paraphrased references (H UMAN), which we produce for a subset of 500 sentences from the de–en test set. Two native English speakers together produced five paraphrases per reference (alternately two or three paraphrases). They were instructed to craft paraphrases that were maximally different (lexically and syntactically) from both the reference and the other paraphrases (to which they had access), without altering the original meaning. Baseline and contrastive systems 4.3 Our true baselines are case-sensitive corpus B LEU and SENT B LEU, both calculated using sacreB LEU (Post, 2018) using the standard B LEU formula. Though likely familiar to the reader, we review it here. B LEU is computed by averaging modified n-gram precisions (pn , n = 1..4) and multiplying this product by a brevity penalty (BP), which penalizes overly short translations and thereby works to balance precision with recall: ! N X wn log pn (1) B LEU = BP · exp We train our paraphrasers using data from Parabank 2 (Hu et al., 2019b), containing ≈20M sentences with up to 5 paraphrases each, of which we use the first paraphrase only. We preprocess by removing duplicate sentences and those longer than 100 wo"
2020.findings-emnlp.82,N18-1119,1,0.891311,"Missing"
2020.findings-emnlp.82,W06-1610,0,0.180339,"Missing"
2020.findings-emnlp.82,C16-1275,0,0.0268339,"od designed just to increase coverage. Manual paraphrasing does give the best systemlevel B LEU results, but even these gains are relatively limited, suggesting that diversity alone has its limits in addressing weaknesses of surfacebased evaluation metrics like B LEU. 2 Structurally diverse outputs Diverse generation is important to ensure a wide coverage of possible translations. Diversity, both lexical and structural, has been a major concern of text generation tasks (Colin and Gardent, 2018; Iyyer et al., 2018). State-of-the-art neural MT-style text generation models used for paraphrasing (Prakash et al., 2016; Mallinson et al., 2017) typically suffer from limited diversity in the beam. Techniques such as sampling from the model distribution or from noisy outputs have been proposed to tackle this (Edunov et al., 2018) but can harm output quality. An effective strategy to encourage structural diversity is to add syntactic information (which can be varied) to the generated text. The constraints can be specified manually, for example by adding a parse tree (Colin and Gardent, 2018; Iyyer et al., 2018) or by specifying more abstract constraints such as rewriting embeddings (Xu et al., 2018). A similar"
2020.findings-emnlp.82,P19-1177,0,0.0881342,"diversity in the beam. Techniques such as sampling from the model distribution or from noisy outputs have been proposed to tackle this (Edunov et al., 2018) but can harm output quality. An effective strategy to encourage structural diversity is to add syntactic information (which can be varied) to the generated text. The constraints can be specified manually, for example by adding a parse tree (Colin and Gardent, 2018; Iyyer et al., 2018) or by specifying more abstract constraints such as rewriting embeddings (Xu et al., 2018). A similar but more flexible approach was adopted more recently by Shu et al. (2019), who augment target training sentences with cluster pseudotokens representing the structural signature of the output sentence. When decoding, the top cluster codes are selected automatically using beam search and for each one a different hypothesis is selected. We adopt Shu et al.’s approach here, due to the automatic nature of constraint selection and the flexibility afforded by constraint definition, allowing us to test different types of diversity by varying the type of sentence clustering method. Related Work Paraphrasing for MT evaluation There is a long history of using paraphrasing to"
2020.lrec-1.325,P19-1310,0,0.0559617,"Missing"
2020.lrec-1.325,1983.tc-1.13,0,0.560974,"Missing"
2020.lrec-1.325,P19-1019,0,0.0163,"neural model (Arivazhagan et al., 2019; Johnson et al., 2017; Gu et low-resource languages; for future work, it will be promising to include this in our analysis of found bitext in Table 3. 2671 al., 2018; Wang et al., 2019b) or design stage-wise transfer learning mechanisms (Zoph et al., 2016; Dabre et al., 2019; Kocmi and Bojar, 2018). MT performance may also be improved by using domain adaptation techniques (Thompson et al., 2019a) to deal with the problem of training on heterogeneous resource types and generalizing to new domains. Recent interest in unsupervised machine translation (c.f. (Artetxe et al., 2019)) promises to reduce the requirement for bitext, training only on monolingual data. Finally, general modeling improvements in NMT architectures can also help (Zaremoodi and Haffari, 2019; Nguyen and Chiang, 2018; Neubig and Hu, 2018). For example, results in other settings suggest that paraphrasing the English side of a bitext is a promising approach when translating into English in low-resource settings (Hu et al., 2019). MT for African Languages We focused on Somali and Swahili in this paper. Other African languages have been explored in the context of both SMT and NMT. Hausa, a Chadic (Afro"
2020.lrec-1.325,N19-1006,0,0.0114413,"ave been explored in the context of both SMT and NMT. Hausa, a Chadic (Afroasiatic) language spoken mainly in Nigeria and Niger, was investigated in (Nguyen and Chiang, 2018; Zoph et al., 2016; Beloucif et al., 2016); (Murray et al., 2019) additionally perform experiments on Tigrinya, a Semitic (Afroasiatic) language spoken mainly in Eritrea and Ethiopia. The SMT work by (Tsvetkov and Dyer, 2015) focuses on out-of-vocabulary wods, with experiments in Swahili (but a different dataset from ours). Finally, there is a growing body of results in speech translation (Anastasopoulos and Chiang, 2018; Bansal et al., 2019; Inaguma et al., 2019), utilizing the Mboshi-French dataset of (Godard et al., 2018). 7. translations can make a big difference with state of the art neural architectures. For this reason, we believe it is worth continuing to push the frontier of discovering and curating exploitable bitext for low-resource languages. Acknowledgments We thank Dr. Carl Rubino and his team at the IARPA MATERIAL for providing this data. We also thank Philipp Koehn for help with the Paracrawl data. Appendix: Example Translations Example outputs of our SMT and NMT systems (under the baseline + dictionary + found-bi"
2020.lrec-1.325,W16-4507,0,0.01355,"nts in NMT architectures can also help (Zaremoodi and Haffari, 2019; Nguyen and Chiang, 2018; Neubig and Hu, 2018). For example, results in other settings suggest that paraphrasing the English side of a bitext is a promising approach when translating into English in low-resource settings (Hu et al., 2019). MT for African Languages We focused on Somali and Swahili in this paper. Other African languages have been explored in the context of both SMT and NMT. Hausa, a Chadic (Afroasiatic) language spoken mainly in Nigeria and Niger, was investigated in (Nguyen and Chiang, 2018; Zoph et al., 2016; Beloucif et al., 2016); (Murray et al., 2019) additionally perform experiments on Tigrinya, a Semitic (Afroasiatic) language spoken mainly in Eritrea and Ethiopia. The SMT work by (Tsvetkov and Dyer, 2015) focuses on out-of-vocabulary wods, with experiments in Swahili (but a different dataset from ours). Finally, there is a growing body of results in speech translation (Anastasopoulos and Chiang, 2018; Bansal et al., 2019; Inaguma et al., 2019), utilizing the Mboshi-French dataset of (Godard et al., 2018). 7. translations can make a big difference with state of the art neural architectures. For this reason, we beli"
2020.lrec-1.325,D19-1146,0,0.0816199,"arios, but NMT importantly requires careful hyperparameter tuning to match SMT performance. 2. We find that both SMT and NMT can exploit additional data such as noisy parallel text harvested from the web, but NMT benefits significantly more from it. Our goal is an empirical evaluation comparing standard models in SMT and NMT. In this respect, it is orthogonal to other work that propose novel methods to improve results under low-resource, for example by exploiting monolingual/synthetic corpora (Wang et al., 2019a; Fadaee et al., 2017), multilingual transfer (Zoph et al., 2016; Gu et al., 2018; Dabre et al., 2019; Kocmi and Bojar, 2018), or alternative modeling/training strategies (Zaremoodi and Haffari, 2019; Nguyen and Chiang, 2018; Neubig and Hu, 2018). 2667 Train Validation: Text Test1: Text Test2: Transcripts Somali-English sentences words 24.8k 758k 2.6k 68k 4.0k 122k 7.1k 102k Swahili-English sentences words 24.9k 809k 3.3k 87k 6.7k 181k 5.8k 83k Table 1: Data set sizes in sentences and words. Validation and Test1:Text sets consist of news, topical, and blog text. Test2:Transcripts consists of news broadcast, topical broadcast, and conversational telephony. The training set contains a mix of ge"
2020.lrec-1.325,P17-2090,0,0.0854973,"with example values. 1. We find that SMT and NMT perform similarly in these scenarios, but NMT importantly requires careful hyperparameter tuning to match SMT performance. 2. We find that both SMT and NMT can exploit additional data such as noisy parallel text harvested from the web, but NMT benefits significantly more from it. Our goal is an empirical evaluation comparing standard models in SMT and NMT. In this respect, it is orthogonal to other work that propose novel methods to improve results under low-resource, for example by exploiting monolingual/synthetic corpora (Wang et al., 2019a; Fadaee et al., 2017), multilingual transfer (Zoph et al., 2016; Gu et al., 2018; Dabre et al., 2019; Kocmi and Bojar, 2018), or alternative modeling/training strategies (Zaremoodi and Haffari, 2019; Nguyen and Chiang, 2018; Neubig and Hu, 2018). 2667 Train Validation: Text Test1: Text Test2: Transcripts Somali-English sentences words 24.8k 758k 2.6k 68k 4.0k 122k 7.1k 102k Swahili-English sentences words 24.9k 809k 3.3k 87k 6.7k 181k 5.8k 83k Table 1: Data set sizes in sentences and words. Validation and Test1:Text sets consist of news, topical, and blog text. Test2:Transcripts consists of news broadcast, topical"
2020.lrec-1.325,L18-1531,0,0.0228742,"language spoken mainly in Nigeria and Niger, was investigated in (Nguyen and Chiang, 2018; Zoph et al., 2016; Beloucif et al., 2016); (Murray et al., 2019) additionally perform experiments on Tigrinya, a Semitic (Afroasiatic) language spoken mainly in Eritrea and Ethiopia. The SMT work by (Tsvetkov and Dyer, 2015) focuses on out-of-vocabulary wods, with experiments in Swahili (but a different dataset from ours). Finally, there is a growing body of results in speech translation (Anastasopoulos and Chiang, 2018; Bansal et al., 2019; Inaguma et al., 2019), utilizing the Mboshi-French dataset of (Godard et al., 2018). 7. translations can make a big difference with state of the art neural architectures. For this reason, we believe it is worth continuing to push the frontier of discovering and curating exploitable bitext for low-resource languages. Acknowledgments We thank Dr. Carl Rubino and his team at the IARPA MATERIAL for providing this data. We also thank Philipp Koehn for help with the Paracrawl data. Appendix: Example Translations Example outputs of our SMT and NMT systems (under the baseline + dictionary + found-bitext + paracrawl condition) are shown here for the Text and Transcripts test sets. We"
2020.lrec-1.325,N18-1032,0,0.0204356,"rly in these scenarios, but NMT importantly requires careful hyperparameter tuning to match SMT performance. 2. We find that both SMT and NMT can exploit additional data such as noisy parallel text harvested from the web, but NMT benefits significantly more from it. Our goal is an empirical evaluation comparing standard models in SMT and NMT. In this respect, it is orthogonal to other work that propose novel methods to improve results under low-resource, for example by exploiting monolingual/synthetic corpora (Wang et al., 2019a; Fadaee et al., 2017), multilingual transfer (Zoph et al., 2016; Gu et al., 2018; Dabre et al., 2019; Kocmi and Bojar, 2018), or alternative modeling/training strategies (Zaremoodi and Haffari, 2019; Nguyen and Chiang, 2018; Neubig and Hu, 2018). 2667 Train Validation: Text Test1: Text Test2: Transcripts Somali-English sentences words 24.8k 758k 2.6k 68k 4.0k 122k 7.1k 102k Swahili-English sentences words 24.9k 809k 3.3k 87k 6.7k 181k 5.8k 83k Table 1: Data set sizes in sentences and words. Validation and Test1:Text sets consist of news, topical, and blog text. Test2:Transcripts consists of news broadcast, topical broadcast, and conversational telephony. The training set"
2020.lrec-1.325,E17-3017,0,0.0233731,"the raw untokenized forms, then evaluated via SacreBleu (Post, 1 For the purpose of this benchmark, we use the Build Pack for training and the Analysis Packs for tuning and testing; we do not use other annotations such as domain or query relevance. For more details about the program and data, refer to (Rubino, 2018). 2018)2 to ensure that BLEU (Papineni et al., 2002) is computed using the same tokenization. 3. Comparing Standard SMT and NMT Using the available training data, we built SMT systems using the Apache Joshua toolkit (Post et al., 2013) and NMT systems using the AWS Sockeye toolkit (Hieber et al., 2017).3 Our Joshua system is a phrase-based model that represents the state of the art in SMT, with 4-gram KenLM language model and MIRA-based tuning. Our Sockeye system is a transformer model (Vaswani et al., 2017), which is among the strongest performers in the field of NMT. We vary the following hyperparameters: • Transformer Architecture: number of layers (1, 2, 4, 6); embedding size (256, 512, 1024), number of hidden units in each layer (1024, 2048), number of heads in self-attention (8, 16). • Preprocessing: number of Byte Pair Encoding (BPE) operations (1k, 2k, 4k, 8k, 16k, 32k) • Training c"
2020.lrec-1.325,N19-1090,1,0.893793,"Missing"
2020.lrec-1.325,Q17-1024,0,0.0773216,"Missing"
2020.lrec-1.325,W18-6325,0,0.100594,"tantly requires careful hyperparameter tuning to match SMT performance. 2. We find that both SMT and NMT can exploit additional data such as noisy parallel text harvested from the web, but NMT benefits significantly more from it. Our goal is an empirical evaluation comparing standard models in SMT and NMT. In this respect, it is orthogonal to other work that propose novel methods to improve results under low-resource, for example by exploiting monolingual/synthetic corpora (Wang et al., 2019a; Fadaee et al., 2017), multilingual transfer (Zoph et al., 2016; Gu et al., 2018; Dabre et al., 2019; Kocmi and Bojar, 2018), or alternative modeling/training strategies (Zaremoodi and Haffari, 2019; Nguyen and Chiang, 2018; Neubig and Hu, 2018). 2667 Train Validation: Text Test1: Text Test2: Transcripts Somali-English sentences words 24.8k 758k 2.6k 68k 4.0k 122k 7.1k 102k Swahili-English sentences words 24.9k 809k 3.3k 87k 6.7k 181k 5.8k 83k Table 1: Data set sizes in sentences and words. Validation and Test1:Text sets consist of news, topical, and blog text. Test2:Transcripts consists of news broadcast, topical broadcast, and conversational telephony. The training set contains a mix of genres, but is most simila"
2020.lrec-1.325,W17-3204,0,0.201334,". Introduction Cross-over point Commercial SMT and NMT systems are often trained on millions to tens of millions of sentence pairs, if not more (Wu et al., 2016). It is unclear how these systems perform when the training data contains significantly fewer sentence pairs. For many languages in the world, and in particular for languages in the African continent, at present we cannot reasonably expect such a large amount of training data. While there is no established convention, we might consider systems that are trained on less than 100 thousand sentence pairs to be low-resource. Previous work (Koehn and Knowles, 2017; Sennrich and Zhang, 2019) has established the idea that there is a crossover point between NMT and SMT performance depending on the amount of training data. See Figure 1. The intuition is that NMT is data-hungry, so may perform worse than SMT in low-resource settings, but begins to excel when there is sufficient training data. With recent advances in NMT, the cross-over point has gradually decreased. Nevertheless, in general it is difficult to predict a priori whether we are on the left or right side of the cross-over point until we actually build the systems. In this work, we perform a deta"
2020.lrec-1.325,W19-5404,0,0.0432609,"Missing"
2020.lrec-1.325,D19-5625,0,0.0708507,"can also help (Zaremoodi and Haffari, 2019; Nguyen and Chiang, 2018; Neubig and Hu, 2018). For example, results in other settings suggest that paraphrasing the English side of a bitext is a promising approach when translating into English in low-resource settings (Hu et al., 2019). MT for African Languages We focused on Somali and Swahili in this paper. Other African languages have been explored in the context of both SMT and NMT. Hausa, a Chadic (Afroasiatic) language spoken mainly in Nigeria and Niger, was investigated in (Nguyen and Chiang, 2018; Zoph et al., 2016; Beloucif et al., 2016); (Murray et al., 2019) additionally perform experiments on Tigrinya, a Semitic (Afroasiatic) language spoken mainly in Eritrea and Ethiopia. The SMT work by (Tsvetkov and Dyer, 2015) focuses on out-of-vocabulary wods, with experiments in Swahili (but a different dataset from ours). Finally, there is a growing body of results in speech translation (Anastasopoulos and Chiang, 2018; Bansal et al., 2019; Inaguma et al., 2019), utilizing the Mboshi-French dataset of (Godard et al., 2018). 7. translations can make a big difference with state of the art neural architectures. For this reason, we believe it is worth continu"
2020.lrec-1.325,D18-1103,0,0.0953219,"onal data such as noisy parallel text harvested from the web, but NMT benefits significantly more from it. Our goal is an empirical evaluation comparing standard models in SMT and NMT. In this respect, it is orthogonal to other work that propose novel methods to improve results under low-resource, for example by exploiting monolingual/synthetic corpora (Wang et al., 2019a; Fadaee et al., 2017), multilingual transfer (Zoph et al., 2016; Gu et al., 2018; Dabre et al., 2019; Kocmi and Bojar, 2018), or alternative modeling/training strategies (Zaremoodi and Haffari, 2019; Nguyen and Chiang, 2018; Neubig and Hu, 2018). 2667 Train Validation: Text Test1: Text Test2: Transcripts Somali-English sentences words 24.8k 758k 2.6k 68k 4.0k 122k 7.1k 102k Swahili-English sentences words 24.9k 809k 3.3k 87k 6.7k 181k 5.8k 83k Table 1: Data set sizes in sentences and words. Validation and Test1:Text sets consist of news, topical, and blog text. Test2:Transcripts consists of news broadcast, topical broadcast, and conversational telephony. The training set contains a mix of genres, but is most similar to Validation and Test1. In the following, we first describe our low-resource condition for Swahili and Somali (Section"
2020.lrec-1.325,N18-1031,0,0.0739693,"nd NMT can exploit additional data such as noisy parallel text harvested from the web, but NMT benefits significantly more from it. Our goal is an empirical evaluation comparing standard models in SMT and NMT. In this respect, it is orthogonal to other work that propose novel methods to improve results under low-resource, for example by exploiting monolingual/synthetic corpora (Wang et al., 2019a; Fadaee et al., 2017), multilingual transfer (Zoph et al., 2016; Gu et al., 2018; Dabre et al., 2019; Kocmi and Bojar, 2018), or alternative modeling/training strategies (Zaremoodi and Haffari, 2019; Nguyen and Chiang, 2018; Neubig and Hu, 2018). 2667 Train Validation: Text Test1: Text Test2: Transcripts Somali-English sentences words 24.8k 758k 2.6k 68k 4.0k 122k 7.1k 102k Swahili-English sentences words 24.9k 809k 3.3k 87k 6.7k 181k 5.8k 83k Table 1: Data set sizes in sentences and words. Validation and Test1:Text sets consist of news, topical, and blog text. Test2:Transcripts consists of news broadcast, topical broadcast, and conversational telephony. The training set contains a mix of genres, but is most similar to Validation and Test1. In the following, we first describe our low-resource condition for Swahi"
2020.lrec-1.325,P02-1040,0,0.10688,"data is uniformly preprocessed with the same Joshua tokenizer and then lower-cased. For NMT, we additionally segment words into subwords via Byte Pair Encoding (Sennrich et al., 2016). For a fair evaluation, all translation outputs are mapped backed to the raw untokenized forms, then evaluated via SacreBleu (Post, 1 For the purpose of this benchmark, we use the Build Pack for training and the Analysis Packs for tuning and testing; we do not use other annotations such as domain or query relevance. For more details about the program and data, refer to (Rubino, 2018). 2018)2 to ensure that BLEU (Papineni et al., 2002) is computed using the same tokenization. 3. Comparing Standard SMT and NMT Using the available training data, we built SMT systems using the Apache Joshua toolkit (Post et al., 2013) and NMT systems using the AWS Sockeye toolkit (Hieber et al., 2017).3 Our Joshua system is a phrase-based model that represents the state of the art in SMT, with 4-gram KenLM language model and MIRA-based tuning. Our Sockeye system is a transformer model (Vaswani et al., 2017), which is among the strongest performers in the field of NMT. We vary the following hyperparameters: • Transformer Architecture: number of"
2020.lrec-1.325,W13-2226,1,0.713881,"r a fair evaluation, all translation outputs are mapped backed to the raw untokenized forms, then evaluated via SacreBleu (Post, 1 For the purpose of this benchmark, we use the Build Pack for training and the Analysis Packs for tuning and testing; we do not use other annotations such as domain or query relevance. For more details about the program and data, refer to (Rubino, 2018). 2018)2 to ensure that BLEU (Papineni et al., 2002) is computed using the same tokenization. 3. Comparing Standard SMT and NMT Using the available training data, we built SMT systems using the Apache Joshua toolkit (Post et al., 2013) and NMT systems using the AWS Sockeye toolkit (Hieber et al., 2017).3 Our Joshua system is a phrase-based model that represents the state of the art in SMT, with 4-gram KenLM language model and MIRA-based tuning. Our Sockeye system is a transformer model (Vaswani et al., 2017), which is among the strongest performers in the field of NMT. We vary the following hyperparameters: • Transformer Architecture: number of layers (1, 2, 4, 6); embedding size (256, 512, 1024), number of hidden units in each layer (1024, 2048), number of heads in self-attention (8, 16). • Preprocessing: number of Byte Pa"
2020.lrec-1.325,W18-6319,1,0.854147,"Missing"
2020.lrec-1.325,N18-4016,0,0.0160834,"ters, since neural models are sensitive in low-resource conditions. While hyperparameter tuning can be expensive, it can be feasible when training data is scarce, which is exactly our low-resource scenario. 4. Exploiting Additional Data There are two main research directions for solving lowresource problems: (a) develop new modeling techniques that require less data, and (b) devise ways to exploit additional opportunistic data sources. In this work we first focus on the latter. We explore three types of resources: 1. Dictionary: Pre-existing dictionaries may be available from various sources (Ramesh and Sankaranarayanan, 2018; Thompson et al., 2019b). We define dictionaries as word-by-word or phrase-by-phrase translations, which are different in format from the sentence-bysentence parallel data in that there may be less contex2. Found Bitext: Pre-existing parallel sentences may be found via various sources (Tiedemann, 2012; Christodouloupoulos and Steedman, 2015), such as the Bible. These are relatively clean datasets that contain useful sentence-by-sentence translations, but may be in a different domain/genre from our baseline training set and testset.5 3. Mined Bitext: Parallel sentences can be mined by crawling"
2020.lrec-1.325,W18-1902,0,0.0249852,"st overfitting). For both SMT and NMT, the data is uniformly preprocessed with the same Joshua tokenizer and then lower-cased. For NMT, we additionally segment words into subwords via Byte Pair Encoding (Sennrich et al., 2016). For a fair evaluation, all translation outputs are mapped backed to the raw untokenized forms, then evaluated via SacreBleu (Post, 1 For the purpose of this benchmark, we use the Build Pack for training and the Analysis Packs for tuning and testing; we do not use other annotations such as domain or query relevance. For more details about the program and data, refer to (Rubino, 2018). 2018)2 to ensure that BLEU (Papineni et al., 2002) is computed using the same tokenization. 3. Comparing Standard SMT and NMT Using the available training data, we built SMT systems using the Apache Joshua toolkit (Post et al., 2013) and NMT systems using the AWS Sockeye toolkit (Hieber et al., 2017).3 Our Joshua system is a phrase-based model that represents the state of the art in SMT, with 4-gram KenLM language model and MIRA-based tuning. Our Sockeye system is a transformer model (Vaswani et al., 2017), which is among the strongest performers in the field of NMT. We vary the following hy"
2020.lrec-1.325,P19-1021,0,0.291128,"point Commercial SMT and NMT systems are often trained on millions to tens of millions of sentence pairs, if not more (Wu et al., 2016). It is unclear how these systems perform when the training data contains significantly fewer sentence pairs. For many languages in the world, and in particular for languages in the African continent, at present we cannot reasonably expect such a large amount of training data. While there is no established convention, we might consider systems that are trained on less than 100 thousand sentence pairs to be low-resource. Previous work (Koehn and Knowles, 2017; Sennrich and Zhang, 2019) has established the idea that there is a crossover point between NMT and SMT performance depending on the amount of training data. See Figure 1. The intuition is that NMT is data-hungry, so may perform worse than SMT in low-resource settings, but begins to excel when there is sufficient training data. With recent advances in NMT, the cross-over point has gradually decreased. Nevertheless, in general it is difficult to predict a priori whether we are on the left or right side of the cross-over point until we actually build the systems. In this work, we perform a detailed evaluation of lowresou"
2020.lrec-1.325,P16-1162,0,0.0714187,"ripts, and evaluates how our systems tuned on text might perform with speech data. The validation set is used for MIRA tuning in SMT (for finding weights that tradeoff e.g., language model, translation model, and length penalty), and for early-stopping in NMT (for stopping the training run when perplexity fails to improve after a several consecutive checkpoint updates, which is effective against overfitting). For both SMT and NMT, the data is uniformly preprocessed with the same Joshua tokenizer and then lower-cased. For NMT, we additionally segment words into subwords via Byte Pair Encoding (Sennrich et al., 2016). For a fair evaluation, all translation outputs are mapped backed to the raw untokenized forms, then evaluated via SacreBleu (Post, 1 For the purpose of this benchmark, we use the Build Pack for training and the Analysis Packs for tuning and testing; we do not use other annotations such as domain or query relevance. For more details about the program and data, refer to (Rubino, 2018). 2018)2 to ensure that BLEU (Papineni et al., 2002) is computed using the same tokenization. 3. Comparing Standard SMT and NMT Using the available training data, we built SMT systems using the Apache Joshua toolk"
2020.lrec-1.325,N19-1209,1,0.89183,"tive in low-resource conditions. While hyperparameter tuning can be expensive, it can be feasible when training data is scarce, which is exactly our low-resource scenario. 4. Exploiting Additional Data There are two main research directions for solving lowresource problems: (a) develop new modeling techniques that require less data, and (b) devise ways to exploit additional opportunistic data sources. In this work we first focus on the latter. We explore three types of resources: 1. Dictionary: Pre-existing dictionaries may be available from various sources (Ramesh and Sankaranarayanan, 2018; Thompson et al., 2019b). We define dictionaries as word-by-word or phrase-by-phrase translations, which are different in format from the sentence-bysentence parallel data in that there may be less contex2. Found Bitext: Pre-existing parallel sentences may be found via various sources (Tiedemann, 2012; Christodouloupoulos and Steedman, 2015), such as the Bible. These are relatively clean datasets that contain useful sentence-by-sentence translations, but may be in a different domain/genre from our baseline training set and testset.5 3. Mined Bitext: Parallel sentences can be mined by crawling the web, for example v"
2020.lrec-1.325,D19-1142,1,0.914182,"tive in low-resource conditions. While hyperparameter tuning can be expensive, it can be feasible when training data is scarce, which is exactly our low-resource scenario. 4. Exploiting Additional Data There are two main research directions for solving lowresource problems: (a) develop new modeling techniques that require less data, and (b) devise ways to exploit additional opportunistic data sources. In this work we first focus on the latter. We explore three types of resources: 1. Dictionary: Pre-existing dictionaries may be available from various sources (Ramesh and Sankaranarayanan, 2018; Thompson et al., 2019b). We define dictionaries as word-by-word or phrase-by-phrase translations, which are different in format from the sentence-bysentence parallel data in that there may be less contex2. Found Bitext: Pre-existing parallel sentences may be found via various sources (Tiedemann, 2012; Christodouloupoulos and Steedman, 2015), such as the Bible. These are relatively clean datasets that contain useful sentence-by-sentence translations, but may be in a different domain/genre from our baseline training set and testset.5 3. Mined Bitext: Parallel sentences can be mined by crawling the web, for example v"
2020.lrec-1.325,tiedemann-2012-parallel,0,0.353587,"new modeling techniques that require less data, and (b) devise ways to exploit additional opportunistic data sources. In this work we first focus on the latter. We explore three types of resources: 1. Dictionary: Pre-existing dictionaries may be available from various sources (Ramesh and Sankaranarayanan, 2018; Thompson et al., 2019b). We define dictionaries as word-by-word or phrase-by-phrase translations, which are different in format from the sentence-bysentence parallel data in that there may be less contex2. Found Bitext: Pre-existing parallel sentences may be found via various sources (Tiedemann, 2012; Christodouloupoulos and Steedman, 2015), such as the Bible. These are relatively clean datasets that contain useful sentence-by-sentence translations, but may be in a different domain/genre from our baseline training set and testset.5 3. Mined Bitext: Parallel sentences can be mined by crawling the web, for example via Paracrawl6 . We exploit the fact that various websites exist in multiple languages and devise methods to discover and extract these parallel sentences. Depending on the languagepair, large paracrawl corpora may be possible. The challenge with using this crawled data is that it"
2020.lrec-1.325,P15-2021,0,0.0146504,"the English side of a bitext is a promising approach when translating into English in low-resource settings (Hu et al., 2019). MT for African Languages We focused on Somali and Swahili in this paper. Other African languages have been explored in the context of both SMT and NMT. Hausa, a Chadic (Afroasiatic) language spoken mainly in Nigeria and Niger, was investigated in (Nguyen and Chiang, 2018; Zoph et al., 2016; Beloucif et al., 2016); (Murray et al., 2019) additionally perform experiments on Tigrinya, a Semitic (Afroasiatic) language spoken mainly in Eritrea and Ethiopia. The SMT work by (Tsvetkov and Dyer, 2015) focuses on out-of-vocabulary wods, with experiments in Swahili (but a different dataset from ours). Finally, there is a growing body of results in speech translation (Anastasopoulos and Chiang, 2018; Bansal et al., 2019; Inaguma et al., 2019), utilizing the Mboshi-French dataset of (Godard et al., 2018). 7. translations can make a big difference with state of the art neural architectures. For this reason, we believe it is worth continuing to push the frontier of discovering and curating exploitable bitext for low-resource languages. Acknowledgments We thank Dr. Carl Rubino and his team at the"
2020.lrec-1.325,D19-1073,0,0.0938521,"g, 2019) for figures with example values. 1. We find that SMT and NMT perform similarly in these scenarios, but NMT importantly requires careful hyperparameter tuning to match SMT performance. 2. We find that both SMT and NMT can exploit additional data such as noisy parallel text harvested from the web, but NMT benefits significantly more from it. Our goal is an empirical evaluation comparing standard models in SMT and NMT. In this respect, it is orthogonal to other work that propose novel methods to improve results under low-resource, for example by exploiting monolingual/synthetic corpora (Wang et al., 2019a; Fadaee et al., 2017), multilingual transfer (Zoph et al., 2016; Gu et al., 2018; Dabre et al., 2019; Kocmi and Bojar, 2018), or alternative modeling/training strategies (Zaremoodi and Haffari, 2019; Nguyen and Chiang, 2018; Neubig and Hu, 2018). 2667 Train Validation: Text Test1: Text Test2: Transcripts Somali-English sentences words 24.8k 758k 2.6k 68k 4.0k 122k 7.1k 102k Swahili-English sentences words 24.9k 809k 3.3k 87k 6.7k 181k 5.8k 83k Table 1: Data set sizes in sentences and words. Validation and Test1:Text sets consist of news, topical, and blog text. Test2:Transcripts consists of"
2020.lrec-1.325,D19-5618,0,0.109279,"e. 2. We find that both SMT and NMT can exploit additional data such as noisy parallel text harvested from the web, but NMT benefits significantly more from it. Our goal is an empirical evaluation comparing standard models in SMT and NMT. In this respect, it is orthogonal to other work that propose novel methods to improve results under low-resource, for example by exploiting monolingual/synthetic corpora (Wang et al., 2019a; Fadaee et al., 2017), multilingual transfer (Zoph et al., 2016; Gu et al., 2018; Dabre et al., 2019; Kocmi and Bojar, 2018), or alternative modeling/training strategies (Zaremoodi and Haffari, 2019; Nguyen and Chiang, 2018; Neubig and Hu, 2018). 2667 Train Validation: Text Test1: Text Test2: Transcripts Somali-English sentences words 24.8k 758k 2.6k 68k 4.0k 122k 7.1k 102k Swahili-English sentences words 24.9k 809k 3.3k 87k 6.7k 181k 5.8k 83k Table 1: Data set sizes in sentences and words. Validation and Test1:Text sets consist of news, topical, and blog text. Test2:Transcripts consists of news broadcast, topical broadcast, and conversational telephony. The training set contains a mix of genres, but is most similar to Validation and Test1. In the following, we first describe our low-res"
2020.lrec-1.325,D16-1163,0,0.103973,"NMT perform similarly in these scenarios, but NMT importantly requires careful hyperparameter tuning to match SMT performance. 2. We find that both SMT and NMT can exploit additional data such as noisy parallel text harvested from the web, but NMT benefits significantly more from it. Our goal is an empirical evaluation comparing standard models in SMT and NMT. In this respect, it is orthogonal to other work that propose novel methods to improve results under low-resource, for example by exploiting monolingual/synthetic corpora (Wang et al., 2019a; Fadaee et al., 2017), multilingual transfer (Zoph et al., 2016; Gu et al., 2018; Dabre et al., 2019; Kocmi and Bojar, 2018), or alternative modeling/training strategies (Zaremoodi and Haffari, 2019; Nguyen and Chiang, 2018; Neubig and Hu, 2018). 2667 Train Validation: Text Test1: Text Test2: Transcripts Somali-English sentences words 24.8k 758k 2.6k 68k 4.0k 122k 7.1k 102k Swahili-English sentences words 24.9k 809k 3.3k 87k 6.7k 181k 5.8k 83k Table 1: Data set sizes in sentences and words. Validation and Test1:Text sets consist of news, topical, and blog text. Test2:Transcripts consists of news broadcast, topical broadcast, and conversational telephony."
2020.lrec-1.352,P19-1310,0,0.0676769,"Missing"
2020.lrec-1.352,P15-2044,0,0.0825655,"Missing"
2020.lrec-1.352,D17-1011,0,0.0843599,"Missing"
2020.lrec-1.352,W08-0336,0,0.0231984,"Missing"
2020.lrec-1.352,D19-1632,0,0.0692809,"Missing"
2020.lrec-1.352,L18-1293,1,0.886792,"Missing"
2020.lrec-1.352,2005.mtsummit-papers.11,0,0.16262,"Missing"
2020.lrec-1.352,E17-1072,0,0.0251337,"eat map of the 66 Bible books’ presence by language. (Twenty non-canon books which appear in only a handful of languages are omitted.) Nearly all languages have a complete New Testament, and several also have a complete Old Testament. 5. Bibles as a Low-Resource Asset employ the fact that multiple interpretations can be used in tandem. Exploiting the Bible, Agi´c et al. (2015) learn POS taggers for 100 languages and evaluate on 25 languages with test sets. Parallel Bibles also aid a variety of cross-lingual tasks, e.g., dependency parsing (Schlichtkrull and Søgaard, 2017), sentence embedding (Levy et al., 2017), verbal morphology induction (Yarowsky et al., 2001), and multilingual optical character recognition (Kanungo et al., 2005). None of these By contrast, Xia and Yarowsky (2017) leverage 27 English translations of the Bible. They use alignment and consensus to transfer dependency parses across languages. Nicolai and Yarowsky (2019) build on this, using projection with the same 27 English translations to develop morphosyntactic analyzers for low-resource languages. Nicolai et al. (2020) 2886 Family Tai-Kadai Arai (Left May) Khoe-Kwadi South-Central Papuan Kiowa-Tanoan Eskimo-Aleut Aymaran Tungus"
2020.lrec-1.352,E17-2002,0,0.02835,"not because we did not segment the text in languages that do not indicate word boundaries with spaces.)) take this a step further, releasing fine-grained morphosyntactic analysis and generation tools for more than 1,000 languages. Additional practical resources can be derived from the Bible: translation matrices of named entities (Wu et al., 2018), lowresource transliteration tools (Wu and Yarowsky, 2018), and bitext for multilingual translation (Mueller et al., 2020). Feature Compilation In order to compile a typological description of this corpus, we utilize the URIEL typological database (Littell et al., 2017) and supplement this with a surface-level parser of Ethnologue (Eberhard et al., 2019) typology descriptions in order to leverage the most recent entries. This parser functions similarly to the original parser described by Littell et al. (2017)—using simple Boolean logic on contents of Ethnologue descriptions. As most Ethnologue entries have similar phrasing or terminology, a language can be classified with a particular typological feature if the description contains one of a handful of phrases used on Ethnologue to describe that feature, but does not contain any of the handful of phrases used"
2020.lrec-1.352,mayer-cysouw-2014-creating,0,0.0764406,"treat the verse alignment problem within a chapter as an instance of the longest common subsequence problem. Here, in a chapter with a pre-known number of verses m, the sequence of verse numbers A = [1, 2, . . . , m] is matched to the sequence B of n numbers extracted from the chapter text. The longest common subsequence is going to give the best explanation of the numbers B seen in text, “explaining” them either as a verse ID or a number seen in text. It can be solved efficiently using dynamic programming in O(m × n) time (Wagner and Fischer, 1974). 3.2. Verse alignment Our re-release of the Mayer and Cysouw (2014) Bibles and the web-scraped Bibles (Asgari and Sch¨utze, 2017) is already verse-aligned. The CMU Wilderness Bibles (Black, 2019) are verse-aligned using the dynamic programming approach explained in §3.1. Normalization We normalize all characters to their canonical (Unicode NFKC) form.3 Cross-references, footnote markers, and explanatory parentheticals are stripped. We replace archaisms in the King James Version of the English Bible (‘thou’ forms; ‘-est’ and ‘-eth’ verb forms) with their modern equivalents. Tokenization We preserve the tokenization of the Mayer and Cysouw (2014)–derived Bibles"
2020.lrec-1.352,2020.lrec-1.483,1,0.701077,"individual Bibles in a language that is a member of each family. The percent denotes the percent of the JHUBC Bibles that come from that language family. and clusivity. We leverage the parallelism of the Bible to annotate pronouns in the English Bibles for these features which are otherwise unmarked in English. We first collect a pronoun list for a small set of languages that mark pronouns for a number of different phenomena such as number, gender, plurality, and clusivity, and annotated these lists with UniMorph-style inflectional information (Sylak-Glassman et al., 2015; Kirov et al., 2018; McCarthy et al., 2020). Next, we word-align the English Bibles with the Bibles in those languages. Projecting from the source onto English, we obtain source–English pronoun hypotheses for each English pronoun. For each feature, we vote among the languages to arrive at a final annotation for English. To mitigate the ubiquity of certain feature values over others (i.e., the nominative case is much more prevalent in our languages than the essive case), we normalize each feature by the number of languages in which it is present. We can then use these annotations to identify pronouns in other languages via alignment. Bi"
2020.lrec-1.352,2020.lrec-1.458,1,0.792992,"here are several translations within a single language, we average these first. (Tai-Kadai would be expected to have a low type–token ratio. It does not because we did not segment the text in languages that do not indicate word boundaries with spaces.)) take this a step further, releasing fine-grained morphosyntactic analysis and generation tools for more than 1,000 languages. Additional practical resources can be derived from the Bible: translation matrices of named entities (Wu et al., 2018), lowresource transliteration tools (Wu and Yarowsky, 2018), and bitext for multilingual translation (Mueller et al., 2020). Feature Compilation In order to compile a typological description of this corpus, we utilize the URIEL typological database (Littell et al., 2017) and supplement this with a surface-level parser of Ethnologue (Eberhard et al., 2019) typology descriptions in order to leverage the most recent entries. This parser functions similarly to the original parser described by Littell et al. (2017)—using simple Boolean logic on contents of Ethnologue descriptions. As most Ethnologue entries have similar phrasing or terminology, a language can be classified with a particular typological feature if the d"
2020.lrec-1.352,P19-1172,1,0.803869,"sed in tandem. Exploiting the Bible, Agi´c et al. (2015) learn POS taggers for 100 languages and evaluate on 25 languages with test sets. Parallel Bibles also aid a variety of cross-lingual tasks, e.g., dependency parsing (Schlichtkrull and Søgaard, 2017), sentence embedding (Levy et al., 2017), verbal morphology induction (Yarowsky et al., 2001), and multilingual optical character recognition (Kanungo et al., 2005). None of these By contrast, Xia and Yarowsky (2017) leverage 27 English translations of the Bible. They use alignment and consensus to transfer dependency parses across languages. Nicolai and Yarowsky (2019) build on this, using projection with the same 27 English translations to develop morphosyntactic analyzers for low-resource languages. Nicolai et al. (2020) 2886 Family Tai-Kadai Arai (Left May) Khoe-Kwadi South-Central Papuan Kiowa-Tanoan Eskimo-Aleut Aymaran Tungusic Dravidian Eyak-Athabaskan Algic Mixed language Piawi Iroquoian Har´akmbut Pauwasi Quechuan Unclassified Maipurean East Geelvink Bay Yuat Senagi Tequistlatecan Pidgin Turkic Chibchan Chipaya-Uru Mapudungu Tacanan Mixe-Zoquean Uralic Constructed language Witotoan Muskogean Panoan Huavean North Bougainville Nakh-Daghestanian Jivar"
2020.lrec-1.352,2020.lrec-1.488,1,0.718874,"a variety of cross-lingual tasks, e.g., dependency parsing (Schlichtkrull and Søgaard, 2017), sentence embedding (Levy et al., 2017), verbal morphology induction (Yarowsky et al., 2001), and multilingual optical character recognition (Kanungo et al., 2005). None of these By contrast, Xia and Yarowsky (2017) leverage 27 English translations of the Bible. They use alignment and consensus to transfer dependency parses across languages. Nicolai and Yarowsky (2019) build on this, using projection with the same 27 English translations to develop morphosyntactic analyzers for low-resource languages. Nicolai et al. (2020) 2886 Family Tai-Kadai Arai (Left May) Khoe-Kwadi South-Central Papuan Kiowa-Tanoan Eskimo-Aleut Aymaran Tungusic Dravidian Eyak-Athabaskan Algic Mixed language Piawi Iroquoian Har´akmbut Pauwasi Quechuan Unclassified Maipurean East Geelvink Bay Yuat Senagi Tequistlatecan Pidgin Turkic Chibchan Chipaya-Uru Mapudungu Tacanan Mixe-Zoquean Uralic Constructed language Witotoan Muskogean Panoan Huavean North Bougainville Nakh-Daghestanian Jivaroan Indo-European Arauan Guaykuruan Austro-Asiatic Misumalpan Karaj´a Tarascan South Bougainville Matacoan TTR 0.621 0.590 0.535 0.439 0.427 0.426 0.370 0.35"
2020.lrec-1.352,N18-2084,0,0.0326875,"Missing"
2020.lrec-1.352,E17-1021,0,0.0207306,"r 1 John 2 John 3 John Jude Revelation 0.0 Figure 1: Heat map of the 66 Bible books’ presence by language. (Twenty non-canon books which appear in only a handful of languages are omitted.) Nearly all languages have a complete New Testament, and several also have a complete Old Testament. 5. Bibles as a Low-Resource Asset employ the fact that multiple interpretations can be used in tandem. Exploiting the Bible, Agi´c et al. (2015) learn POS taggers for 100 languages and evaluate on 25 languages with test sets. Parallel Bibles also aid a variety of cross-lingual tasks, e.g., dependency parsing (Schlichtkrull and Søgaard, 2017), sentence embedding (Levy et al., 2017), verbal morphology induction (Yarowsky et al., 2001), and multilingual optical character recognition (Kanungo et al., 2005). None of these By contrast, Xia and Yarowsky (2017) leverage 27 English translations of the Bible. They use alignment and consensus to transfer dependency parses across languages. Nicolai and Yarowsky (2019) build on this, using projection with the same 27 English translations to develop morphosyntactic analyzers for low-resource languages. Nicolai et al. (2020) 2886 Family Tai-Kadai Arai (Left May) Khoe-Kwadi South-Central Papuan"
2020.lrec-1.352,P15-2111,1,0.772899,"contacting the authors. 9. Table 3: The count of individual Bibles in a language that is a member of each family. The percent denotes the percent of the JHUBC Bibles that come from that language family. and clusivity. We leverage the parallelism of the Bible to annotate pronouns in the English Bibles for these features which are otherwise unmarked in English. We first collect a pronoun list for a small set of languages that mark pronouns for a number of different phenomena such as number, gender, plurality, and clusivity, and annotated these lists with UniMorph-style inflectional information (Sylak-Glassman et al., 2015; Kirov et al., 2018; McCarthy et al., 2020). Next, we word-align the English Bibles with the Bibles in those languages. Projecting from the source onto English, we obtain source–English pronoun hypotheses for each English pronoun. For each feature, we vote among the languages to arrive at a final annotation for English. To mitigate the ubiquity of certain feature values over others (i.e., the nominative case is much more prevalent in our languages than the essive case), we normalize each feature by the number of languages in which it is present. We can then use these annotations to identify p"
2020.lrec-1.352,tiedemann-2012-parallel,0,0.219573,"Missing"
2020.lrec-1.352,L18-1150,1,0.836665,"averaged over each language family, sorted by this ratio. When there are several translations within a single language, we average these first. (Tai-Kadai would be expected to have a low type–token ratio. It does not because we did not segment the text in languages that do not indicate word boundaries with spaces.)) take this a step further, releasing fine-grained morphosyntactic analysis and generation tools for more than 1,000 languages. Additional practical resources can be derived from the Bible: translation matrices of named entities (Wu et al., 2018), lowresource transliteration tools (Wu and Yarowsky, 2018), and bitext for multilingual translation (Mueller et al., 2020). Feature Compilation In order to compile a typological description of this corpus, we utilize the URIEL typological database (Littell et al., 2017) and supplement this with a surface-level parser of Ethnologue (Eberhard et al., 2019) typology descriptions in order to leverage the most recent entries. This parser functions similarly to the original parser described by Littell et al. (2017)—using simple Boolean logic on contents of Ethnologue descriptions. As most Ethnologue entries have similar phrasing or terminology, a language"
2020.lrec-1.352,L18-1263,1,0.837513,"morphological richness. We report type–token ratios, averaged over each language family, sorted by this ratio. When there are several translations within a single language, we average these first. (Tai-Kadai would be expected to have a low type–token ratio. It does not because we did not segment the text in languages that do not indicate word boundaries with spaces.)) take this a step further, releasing fine-grained morphosyntactic analysis and generation tools for more than 1,000 languages. Additional practical resources can be derived from the Bible: translation matrices of named entities (Wu et al., 2018), lowresource transliteration tools (Wu and Yarowsky, 2018), and bitext for multilingual translation (Mueller et al., 2020). Feature Compilation In order to compile a typological description of this corpus, we utilize the URIEL typological database (Littell et al., 2017) and supplement this with a surface-level parser of Ethnologue (Eberhard et al., 2019) typology descriptions in order to leverage the most recent entries. This parser functions similarly to the original parser described by Littell et al. (2017)—using simple Boolean logic on contents of Ethnologue descriptions. As most Ethnologu"
2020.lrec-1.352,2020.lrec-1.519,1,0.719738,"Missing"
2020.lrec-1.352,I17-2076,1,0.841713,"New Testament, and several also have a complete Old Testament. 5. Bibles as a Low-Resource Asset employ the fact that multiple interpretations can be used in tandem. Exploiting the Bible, Agi´c et al. (2015) learn POS taggers for 100 languages and evaluate on 25 languages with test sets. Parallel Bibles also aid a variety of cross-lingual tasks, e.g., dependency parsing (Schlichtkrull and Søgaard, 2017), sentence embedding (Levy et al., 2017), verbal morphology induction (Yarowsky et al., 2001), and multilingual optical character recognition (Kanungo et al., 2005). None of these By contrast, Xia and Yarowsky (2017) leverage 27 English translations of the Bible. They use alignment and consensus to transfer dependency parses across languages. Nicolai and Yarowsky (2019) build on this, using projection with the same 27 English translations to develop morphosyntactic analyzers for low-resource languages. Nicolai et al. (2020) 2886 Family Tai-Kadai Arai (Left May) Khoe-Kwadi South-Central Papuan Kiowa-Tanoan Eskimo-Aleut Aymaran Tungusic Dravidian Eyak-Athabaskan Algic Mixed language Piawi Iroquoian Har´akmbut Pauwasi Quechuan Unclassified Maipurean East Geelvink Bay Yuat Senagi Tequistlatecan Pidgin Turkic"
2020.lrec-1.352,H01-1035,1,0.451858,"ge. (Twenty non-canon books which appear in only a handful of languages are omitted.) Nearly all languages have a complete New Testament, and several also have a complete Old Testament. 5. Bibles as a Low-Resource Asset employ the fact that multiple interpretations can be used in tandem. Exploiting the Bible, Agi´c et al. (2015) learn POS taggers for 100 languages and evaluate on 25 languages with test sets. Parallel Bibles also aid a variety of cross-lingual tasks, e.g., dependency parsing (Schlichtkrull and Søgaard, 2017), sentence embedding (Levy et al., 2017), verbal morphology induction (Yarowsky et al., 2001), and multilingual optical character recognition (Kanungo et al., 2005). None of these By contrast, Xia and Yarowsky (2017) leverage 27 English translations of the Bible. They use alignment and consensus to transfer dependency parses across languages. Nicolai and Yarowsky (2019) build on this, using projection with the same 27 English translations to develop morphosyntactic analyzers for low-resource languages. Nicolai et al. (2020) 2886 Family Tai-Kadai Arai (Left May) Khoe-Kwadi South-Central Papuan Kiowa-Tanoan Eskimo-Aleut Aymaran Tungusic Dravidian Eyak-Athabaskan Algic Mixed language Pia"
2020.ngt-1.22,D11-1033,0,0.0375497,"theses that are marked incorrect. 8 JHU-TEST DUO-TEST Baseline Baseline + Moore-Lewis 55.16 53.83 Table 7: Moore–Lewis filtering for Pt (macro F1). score(t) = pID (t) − pOD (t). We swept thresholds and minimum prompt lengths on our JHU-TEST data, and found with a threshold of −1.50 on 7word prompts and longer performed the best. Moore–Lewis filtering was originally designed for more coarse-grained selection of training data. We suspect (but did not have time to test) that a better idea is therefore to apply this upstream, using it to help select data used to train the generaldomain MT system (Axelrod et al., 2011). What Didn’t Work We explored additional methods both for selecting candidates from an n-best lists and for generating additional candidates based on an n-best list. While they did not improve performance and were not included in our final submission, we discuss the methods and the analyses learned from them. 8.1 53.30 53.70 8.2 Dual conditional thresholding Extending the probability score thresholding (§5.3), we consider incorporating a score from a reverse model that represents the probability that the original prompt was generated by the candidate. The reverse model score is also used in D"
2020.ngt-1.22,2020.emnlp-main.21,0,0.0144398,"ded sentences like ‘toda noite a pequena ovelha sonha com surfe’ whereas our system output sentences like ‘toda noite as ovelhas pequenas sonham com surfe’. The error was that our output included ‘ovelhas’ (plural sheep), but the gold translations all used ‘ovelha’ (single sheep). While this phenomenon may be an artifact of these particular metrics, we suspect this is indicative of an interesting topic for further research. MT models trained with NLL are trained to match a 1hot prediction, which may make their output distributions poorly calibrated (Ott et al., 2018; Kumar and Sarawagi, 2019; Desai and Durrett, 2020). More research is needed for strong conclusions, but our initial analysis suggests that training on the more diverse data improves quality of a deep nbest list of translations at the expense of the top beam search output. This may be important in cases where an n-best list of translations is being generated for a downstream NLP task. The data for this task was unique in that it provided diverse translations for a given prompt. In most cases where this type of data is not available, training towards a distribution (rather than a single target word), as is done in word-level knowledge distillat"
2020.ngt-1.22,W11-2123,0,0.00816182,"pret this as domain mismatch between the STAPLE data and our MT training data. To filter out such bad candidates, we experimented with cross-entropy language model filtering (Moore and Lewis, 2010). This takes two language models: a (generally large) out-of-domain language model (OD), and a (typically small) indomain language model (ID), and uses the difference in normalized cross-entropy from these two models to score sentences. Sentences with good OD scores and poor ID scores are likely out-ofdomain and can be discarded based on a score threshold. Experimenting on Portuguese, we used KenLM (Heafield, 2011) to train a Kneser–Ney-smoothed 5-gram model on the Portuguese side of the MT training data (Table 2) as the OD model and a 3-gram model on the Duolingo Portuguese data (ID). These were used to score all candidates t as 8.3 N-gram filtering The Duolingo data generally consists of simple language, which means we did not expect to see novel phrases in the references that were not in our training corpora. We used this idea to filter hypotheses that had any n-grams that didn’t appear in our training data. Our hope was that this would catch rare formulations or ungrammatical sentences, e.g. cachorr"
2020.ngt-1.22,W18-6478,0,0.0385152,"Missing"
2020.ngt-1.22,P18-4020,0,0.0197226,"Missing"
2020.ngt-1.22,W18-2705,1,0.832218,"f translations is being generated for a downstream NLP task. The data for this task was unique in that it provided diverse translations for a given prompt. In most cases where this type of data is not available, training towards a distribution (rather than a single target word), as is done in word-level knowledge distillation (Buciluundefined et al., 2006; Hinton et al., 2015; Kim and Rush, 2016) may prove useful to introduce the diversity needed for a strong n-best list of translations. This can be done either towards a distribution of the base model when fine-tuning (Dakwale and Monz, 2017; Khayrallah et al., 2018) or towards the distribution of an auxiliary model, such as a paraphraser (Khayrallah et al., 2020). 7.2 Qualitative error analysis 7.3 In each language, we performed a qualitative error analysis by manually inspecting the difference between the gold and system translations for prompts with lowest weighted recall on JHU-TEST. Our systems were often incapable of expressing target language nuance absent from the source language. For example, for the prompt “we have asked many times.”, a gold translation was ‘私た ちは何度も尋ねてしまった’ whereas our system output ‘私たちは何度も尋ねました’. The gold translations often i"
2020.ngt-1.22,abdelali-etal-2014-amara,0,0.357647,"Missing"
2020.ngt-1.22,2020.emnlp-main.7,1,0.832132,"hat it provided diverse translations for a given prompt. In most cases where this type of data is not available, training towards a distribution (rather than a single target word), as is done in word-level knowledge distillation (Buciluundefined et al., 2006; Hinton et al., 2015; Kim and Rush, 2016) may prove useful to introduce the diversity needed for a strong n-best list of translations. This can be done either towards a distribution of the base model when fine-tuning (Dakwale and Monz, 2017; Khayrallah et al., 2018) or towards the distribution of an auxiliary model, such as a paraphraser (Khayrallah et al., 2020). 7.2 Qualitative error analysis 7.3 In each language, we performed a qualitative error analysis by manually inspecting the difference between the gold and system translations for prompts with lowest weighted recall on JHU-TEST. Our systems were often incapable of expressing target language nuance absent from the source language. For example, for the prompt “we have asked many times.”, a gold translation was ‘私た ちは何度も尋ねてしまった’ whereas our system output ‘私たちは何度も尋ねました’. The gold translations often included the てしまった verb ending, which conveys a nuance similar to perfect aspect. The prompt’s scena"
2020.ngt-1.22,P19-1310,0,0.0763196,"Missing"
2020.ngt-1.22,D16-1139,0,0.0222681,"r initial analysis suggests that training on the more diverse data improves quality of a deep nbest list of translations at the expense of the top beam search output. This may be important in cases where an n-best list of translations is being generated for a downstream NLP task. The data for this task was unique in that it provided diverse translations for a given prompt. In most cases where this type of data is not available, training towards a distribution (rather than a single target word), as is done in word-level knowledge distillation (Buciluundefined et al., 2006; Hinton et al., 2015; Kim and Rush, 2016) may prove useful to introduce the diversity needed for a strong n-best list of translations. This can be done either towards a distribution of the base model when fine-tuning (Dakwale and Monz, 2017; Khayrallah et al., 2018) or towards the distribution of an auxiliary model, such as a paraphraser (Khayrallah et al., 2020). 7.2 Qualitative error analysis 7.3 In each language, we performed a qualitative error analysis by manually inspecting the difference between the gold and system translations for prompts with lowest weighted recall on JHU-TEST. Our systems were often incapable of expressing"
2020.ngt-1.22,2005.mtsummit-papers.11,0,0.108503,"Missing"
2020.ngt-1.22,P18-1007,0,0.0269186,". Since our approach is to overgenerate candidates and filter, we want to avoid glutting the decoder beam with spurious cased variants. For this reason, we lowercase all text on both the source and (where relevant) target sides prior to training. However, it is worth noting that this has a drawback, as source case can provide a signal towards meaning and word-sense disambiguation (e.g., apple versus Apple). After lowercasing, we train separate SentencePiece models (Kudo and Richardson, 2018) on the source and target sides of the bitext, for each language. We train a regularized unigram model (Kudo, 2018) with a vocabulary size of 5,000 and a character coverage of 0.995. When applying the model, we set α = 0.5. No other preprocessing was applied. Evaluation metric The official metric is weighted macro F1 . This is defined as: Weighted Macro F1 = X Weighted F1 (s) s∈S |S| , where S is all prompts in the test corpus. The weighted F1 is computed with a weighted recall, where TP s are the true positives for a prompt s, and FN s are the false negatives for a prompt s: X WTPs = weight(t) Translation models We used fairseq (Ott et al., 2019) to train standard Transformer (Vaswani et al., 2017) models"
2020.ngt-1.22,K18-2016,0,0.0171174,"34.27 33.42 33.51 31.89 34.38 34.37 34.47 34.45 Table 9: Effect of pronoun-based augmentation on metrics in Vietnamese, computed on JHU-TEST. All strategies improve recall and weighted recall, but they cause precision and F1 to decrease. several acceptable translations were not present in the ground truth dataset (see §7.3). Open class words and morphology In between the extremes of large number of types using raw lexical forms and few types using POS tags is to leverage open class words or additional morphological information. We morphologically tag the dataset with the Stanford NLP toolkit (Qi et al., 2018), then represent each sentence either by its words, its POS tags, its morphological tags, or words for closed-class items and tags for openclass items, as shown in Table 8. This too resulted in few hypotheses being filtered and did not impact F1 performance. Morphological expansions English is morphologically poorer than 4 target languages. As an example, the English word ‘you’ may be translated into Portuguese as ‘tu’, ‘você’, ‘vocês’, or ‘vós’, to consider only nominative forms. We can thus generate three additional candidates by altering the morphosyntax (and maintaining grammatical concord"
2020.ngt-1.22,D18-2012,0,0.0121212,"emaining data (JHU-TRAIN) was used for training the MT models. The Duolingo data (including the evaluation data) is all lowercased. Since our approach is to overgenerate candidates and filter, we want to avoid glutting the decoder beam with spurious cased variants. For this reason, we lowercase all text on both the source and (where relevant) target sides prior to training. However, it is worth noting that this has a drawback, as source case can provide a signal towards meaning and word-sense disambiguation (e.g., apple versus Apple). After lowercasing, we train separate SentencePiece models (Kudo and Richardson, 2018) on the source and target sides of the bitext, for each language. We train a regularized unigram model (Kudo, 2018) with a vocabulary size of 5,000 and a character coverage of 0.995. When applying the model, we set α = 0.5. No other preprocessing was applied. Evaluation metric The official metric is weighted macro F1 . This is defined as: Weighted Macro F1 = X Weighted F1 (s) s∈S |S| , where S is all prompts in the test corpus. The weighted F1 is computed with a weighted recall, where TP s are the true positives for a prompt s, and FN s are the false negatives for a prompt s: X WTPs = weight(t"
2020.ngt-1.22,2021.eacl-main.115,0,0.693301,"Missing"
2020.ngt-1.22,tiedemann-2012-parallel,0,0.0303904,"tiple translations in the target lan188 Proceedings of the 4th Workshop on Neural Generation and Translation (WNGT 2020), pages 188–197 c Online, July 10, 2020. 2020 Association for Computational Linguistics www.aclweb.org/anthology/D19-56%2d hu total prompts ja ko pt 3 vi Machine Translation Systems 4,000 2,500 2,500 4,000 3,500 mean translations median translations STD. translations 63 36 66 342 192 362 280 154 311 132 68 150 We began by building high-quality state-of-the-art machine translation systems. 56 30 62 Data and preprocessing Additional data for our systems was obtained from Opus (Tiedemann, 2012).2 We removed duplicate bitext pairs, then reserved 3k random pairs from each dataset to create a validation, development, and test sets of 1k sentence each. The validation dataset is used as held-out data to determine when to stop training the MT system.3 Table 2 shows the amount of training data used from each source. Table 1: Statistics over the Duolingo-provided data. guage. These translations come from courses intended to teach English to speakers of other languages; the references are initially generated by trained translators, and augmented by verified user translations. Each translatio"
2020.ngt-1.22,N16-1014,0,0.0395696,"st list. While they did not improve performance and were not included in our final submission, we discuss the methods and the analyses learned from them. 8.1 53.30 53.70 8.2 Dual conditional thresholding Extending the probability score thresholding (§5.3), we consider incorporating a score from a reverse model that represents the probability that the original prompt was generated by the candidate. The reverse model score is also used in Dual Conditional Cross-Entropy Filtering when selecting clean data from noisy corpora (JunczysDowmunt, 2018), and for re-scoring n-best lists in MMI decoding (Li et al., 2016) We train base and fine-tuned reverse systems for the five language pairs and use them to score the output translations. We compute the combined score of a hypothesis given a prompt as the arithmetic mean of the forward and backward log probability scores and use them in the probability score thresholding algorithm from §5.3. We find that after sweeping across threshold values, incorporating the reverse score performs slightly worse overall than the standard thresholding method for every language. Moore–Lewis filtering Our error analysis revealed that our systems often output sentences that we"
2020.ngt-1.22,L16-1147,0,0.129712,"Missing"
2020.ngt-1.22,2020.lrec-1.519,1,0.721797,"he overt semantics of the English prompt are preserved. All swapping transformations in Table 9 give poorer performance. Filtering by difficulty level As the Duolingo data was generated by language learners, we also considered filtering sentences by the difficulty of the words within. Experimenting with Japanese, we examined the grade level of kanji15 in each sentence. Ignoring non-kanji characters, the average grade level per sentence on the STAPLE training data was 3.77, indicating a 3rd –4th grade level. Future work could consider filtering by other measures such as the coreness of a word (Wu et al., 2020). 8.4 P Hiragana replacement Japanese has three different writing systems—hiragana, katakana, and kanji—and sometimes a word written in kanji is considered an acceptable translation when written in hiragana. For example, the Japanese word for “child” is 子供 when written with kanji, but an acceptable alternative is the hiragana こども . We experiment with expanding translation candidates by replacing Japanese kanji with pronunciations from a furigana (hiragana pronunciation) dictionary but this method did not improve performance. Generation via post-editing Inspired by query expansion in informatio"
2020.ngt-1.22,2020.ngt-1.28,0,0.063962,"We find that stronger BLEU performance of the beam-search generated translation is not indicative of improvements on the task metric—weighted macro F1 of a set of hypotheses—and suggest this should encourage further research on how to train NMT models when n-best lists are needed (§7.1). We perform detailed analysis on our output (§7.2), which led to additional development on English– Portuguese (§8.1). We also present additional linguistically-informed methods which we experimented with but which ultimately did not improve performance (§8). Introduction The Duolingo 2020 STAPLE Shared Task (Mayhew et al., 2020) focuses on generating a comprehensive set of translations for a given sentence, translating from English into Hungarian, Japanese, Korean, Portuguese, and Vietnamese. The formulation of this task (§2) differs from the conventional machine translation setup: instead of the n-gram match (BLEU) against a single reference, sentence-level exact match is computed between a list of proposed candidates and a weighted list of references (as in Figure 1). The set of references is drawn from Duolingo’s language-teaching app. Any auxiliary data is allowed for building systems, including existing very-lar"
2020.ngt-1.22,P10-2041,0,0.0503663,"output sentences that were not incorrect, but not optimized for the Duolingo task. For example, many of our top candidates for translations of “please” in Portuguese used por obséquio, which is a very formal version, instead of the more common por favor. While both versions were valid for the prompts, the gold translations with por favor were weighted higher, so we would desire models to prefer this translation. We interpret this as domain mismatch between the STAPLE data and our MT training data. To filter out such bad candidates, we experimented with cross-entropy language model filtering (Moore and Lewis, 2010). This takes two language models: a (generally large) out-of-domain language model (OD), and a (typically small) indomain language model (ID), and uses the difference in normalized cross-entropy from these two models to score sentences. Sentences with good OD scores and poor ID scores are likely out-ofdomain and can be discarded based on a score threshold. Experimenting on Portuguese, we used KenLM (Heafield, 2011) to train a Kneser–Ney-smoothed 5-gram model on the Portuguese side of the MT training data (Table 2) as the OD model and a 3-gram model on the Duolingo Portuguese data (ID). These w"
2020.tacl-1.4,W16-2301,1,0.841313,"Missing"
2020.tacl-1.4,N19-1423,0,0.0160164,"in Section 6.1. The results are shown in Table 7. We can see that this extra feature did not provide any significant influence to the accuracy. In a more detailed analysis, we find that the reason is that our in and out probes both contain a range of translations from low to high quality translations, and our QE model may not be sufficiently finegrained to tease apart any potential differences. In fact, this may be difficult even for a human estimator. Another approach to exploit external resources is to use a language model pre-trained on a large amount of text. In particular, we used BERT (Devlin et al., 2019), which has shown competitive results in many NLP tasks. We used BERT directly as a classifier, and followed a fine-tuning setup similar to paraphrase detection: For our case the inputs are the English translation and reference 60 ings of the 2017 conference on machine translation (WMT17). In Proceedings of the Second Conference on Machine Translation, pages 169–214, Copenhagen, Denmark. Association for Computational Linguistics. Our attack approach was a simple one, using shadow models to mimic the target model. Bob can attempt more complex strategies, for example, by using the translation AP"
2020.tacl-1.4,E17-3017,0,0.0210062,"e, and combines in-probe and the rest (Ball minus 10 sets) as a training sets. We use notations 4.2 Alice MT Architecture Alice uses her dataset Atrain (consisting of four subcorpora and ParaCrawl) to train her own MT model. Because Paracrawl is noisy, Alice first applies dual conditional cross-entropy filtering (Junczys-Dowmunt, 2018), retaining the top 4.5 million lines. Alice then trains a joint BPE subword model (Sennrich et al., 2016) using 32,000 merge operations. No recasing is applied. Alice’s model is a six-layer Transformer (Vaswani et al., 2017) using default parameters in Sockeye (Hieber et al., 2017).7 The model was 6 We prepared two different pairs of Ain probe and Aout probe . Thus Ball has 10k fewer samples than Atrain , and not 5k fewer. For the experiment we used only one pair, and kept the other for future use. 7 Three-way tied embeddings, model and embedding size 512, eight attention heads, 2,048 hidden states in the feed forward layers, layer normalization applied before each selfattention layer, and dropout and residual connections applied afterward, word-based batch size of 4,096. 8 Version 1.2.12, case-sensitive, ‘‘13a’’ tokenization for comparability with WMT. 54 Aout probe Ai"
2020.tacl-1.4,W18-6478,0,0.022156,"label to train a binary classifier g(f, e, eˆ). If Bob’s shadow models are sufficiently similar to Alice’s in behavior, this attack can work. Bob first selects 10 sets of 5,000 sentences per subcorpus in Ball . He then chooses two sets and uses one as in-probe and the other as outprobe, and combines in-probe and the rest (Ball minus 10 sets) as a training sets. We use notations 4.2 Alice MT Architecture Alice uses her dataset Atrain (consisting of four subcorpora and ParaCrawl) to train her own MT model. Because Paracrawl is noisy, Alice first applies dual conditional cross-entropy filtering (Junczys-Dowmunt, 2018), retaining the top 4.5 million lines. Alice then trains a joint BPE subword model (Sennrich et al., 2016) using 32,000 merge operations. No recasing is applied. Alice’s model is a six-layer Transformer (Vaswani et al., 2017) using default parameters in Sockeye (Hieber et al., 2017).7 The model was 6 We prepared two different pairs of Ain probe and Aout probe . Thus Ball has 10k fewer samples than Atrain , and not 5k fewer. For the experiment we used only one pair, and kept the other for future use. 7 Three-way tied embeddings, model and embedding size 512, eight attention heads, 2,048 hidden"
2020.tacl-1.4,P19-3020,0,0.0532227,"Missing"
2020.tacl-1.4,P02-1040,0,0.108594,"n decisions that balance between simple experimentation vs. realistic condition. Carol doing a common tokenization removes some of the MT-specific complexity for researchers who want to focus on the Alice or Bob models. However, in a real-world public API, Alice’s tokenization is likely to be unknown to Bob. We decided on a middle ground to have Carol perform a common tokenization, but Alice and Bob do their own subword segmentation. 53 trained until perplexity on newstest2017 (Bojar et al., 2017) had not improved for five consecutive checkpoints, computed every 5,000 batches. The BLEU score (Papineni et al., 2002) on newstest2018 was 42.6, computed using sacreBLEU (Post, 2018) with the default settings.8 4.3 Evaluation Protocol To evaluate membership inference attacks on Alice’s MT models, we use the following procedure: First, Bob asks Alice to translate f . Alice returns her result eˆ to Bob. Bob also has access to the reference e and use his classifier g(f, e, eˆ) to infer whether (e, f ) was in Alice’s training data. The classification is reported to Carol, who computes ‘‘attack accuracy’’. Given a probe set P containing a list of (f, e, eˆ, l), where l is the label (in or out), this accuracy is de"
2020.tacl-1.4,W17-4763,0,0.0142409,"We believe this attests to the inherent difficulty of the sentence-level membership inference problem. Alice Bob:train Bob:valid Bob:test P DT NB NN MLP BERT 50.0 50.3 50.4 49.8 50.4 50.0 49.9 51.4 51.2 66.1 51.0 50.0 50.0 51.1 51.1 50.0 51.0 50.0 50.0 51.1 51.0 50.1 50.8 50.0 Table 7: Membership inference accuracies for classifiers with Quality Estimation sentence score as an extra feature, and a BERT classifier. system. Our models are NMT, so the estimation quality may not be optimally matched, but we believe this is the best data available at this time. We applied the Predictor-Estimator (Kim et al., 2017) implemented in an open source QE framework OpenKiwi (Kepler et al., 2019). It consists of a predictor that predicts each token of the target sentence given the target context and the source, and estimator that takes features produced by the predictor to estimate the labels; both are made of LSTMs. We used this model as this is one of the best models seen in the shared tasks, and it does not require alignment information. The model metrics on the WMT18 dev set, namely, Pearson’s correlation, Mean Average Error, and Root Mean Squared Error for sentence-level scores, are 0.6238, 0.1276, and 0.17"
2020.tacl-1.4,W18-6319,1,0.852908,"tion. Carol doing a common tokenization removes some of the MT-specific complexity for researchers who want to focus on the Alice or Bob models. However, in a real-world public API, Alice’s tokenization is likely to be unknown to Bob. We decided on a middle ground to have Carol perform a common tokenization, but Alice and Bob do their own subword segmentation. 53 trained until perplexity on newstest2017 (Bojar et al., 2017) had not improved for five consecutive checkpoints, computed every 5,000 batches. The BLEU score (Papineni et al., 2002) on newstest2018 was 42.6, computed using sacreBLEU (Post, 2018) with the default settings.8 4.3 Evaluation Protocol To evaluate membership inference attacks on Alice’s MT models, we use the following procedure: First, Bob asks Alice to translate f . Alice returns her result eˆ to Bob. Bob also has access to the reference e and use his classifier g(f, e, eˆ) to infer whether (e, f ) was in Alice’s training data. The classification is reported to Carol, who computes ‘‘attack accuracy’’. Given a probe set P containing a list of (f, e, eˆ, l), where l is the label (in or out), this accuracy is defined as: Figure 3: Illustration of actual MT data splits. Atrai"
2020.tacl-1.4,W17-3204,0,0.0171121,"n MT models, in order to mimic Alice and design his attacks. This data could either be disjoint from Atrain , or contain parts of Atrain . We choose the latter, which assumes that there might be some public data that is accessible to both Alice and Bob. This scenario slightly favors Bob. In the case of MT, parallel data can be hard to come by, and datasets Both Aout probe and Aood should be classified as out by Bob’s classifier. However, it has been known that sequence-to-sequence models behave very differently on data from domains/genre that is significantly different from the training data (Koehn and Knowles, 2017). The goal of having two out probes is to quantify the difficulty or ease of membership inference in different situations. 2.2 Summary and Alternative Definitions Figure 2 summarizes the problem definition. The probes Aout probe and Aood are by construction outside of Alice’s training data Atrain , while the probe Ain probe is included. Bob’s goal is to produce a classifier that can make this distinction. 51 providers may support customized engines if users upload their own bitext training data. The provider promises that the user-supplied data will not be used in the customized engines of oth"
2020.tacl-1.4,P04-1077,0,0.0355638,"notate this as Bin probe , Bout probe , and 1− Btrain . With 10 sets of 5,000 sentences, Bob can create 10 different groups of in-probe, out-probe, and training sets. Figure 4 illustrates the data splits. For each group of data, Bob first trains a shadow MT model using the training set. He then uses this model to translate sentences in the in-probe and out-probe sets. Bob has now a list of (f, e, eˆ) from different shadow models, and he knows for each Bob extracts features from (f, e, eˆ) for a binary classifier. He uses modified 1- to 4-gram precisions and smoothed sentence-level BLEU score (Lin and Och, 2004) as features. Bob’s intuition is that if an unusually large number of n-grams in eˆ matches e, then it could be a sign that this was in the training data and Alice memorized it. Bob calculates n-gram precision by counting the number of n-grams in translation that appear in the reference sentence. In the later investigation Bob also considers the MT model score as an extra feature. 55 Algorithm 1: Construction of A Membership Inference Classifier Data: Ball Result: g(·) i Split Ball into multiples groups of (Bin probe , i i Bout probe , Btrain ) ; foreach i in 1+, 1−, 2+, 2−, 3+, 3− do i Train"
2020.tacl-1.4,tiedemann-2012-parallel,0,0.086097,"Missing"
2020.tacl-1.4,P16-1162,0,0.00956918,"in behavior, this attack can work. Bob first selects 10 sets of 5,000 sentences per subcorpus in Ball . He then chooses two sets and uses one as in-probe and the other as outprobe, and combines in-probe and the rest (Ball minus 10 sets) as a training sets. We use notations 4.2 Alice MT Architecture Alice uses her dataset Atrain (consisting of four subcorpora and ParaCrawl) to train her own MT model. Because Paracrawl is noisy, Alice first applies dual conditional cross-entropy filtering (Junczys-Dowmunt, 2018), retaining the top 4.5 million lines. Alice then trains a joint BPE subword model (Sennrich et al., 2016) using 32,000 merge operations. No recasing is applied. Alice’s model is a six-layer Transformer (Vaswani et al., 2017) using default parameters in Sockeye (Hieber et al., 2017).7 The model was 6 We prepared two different pairs of Ain probe and Aout probe . Thus Ball has 10k fewer samples than Atrain , and not 5k fewer. For the experiment we used only one pair, and kept the other for future use. 7 Three-way tied embeddings, model and embedding size 512, eight attention heads, 2,048 hidden states in the feed forward layers, layer normalization applied before each selfattention layer, and dropou"
2020.tacl-1.4,W17-4717,1,\N,Missing
2020.wmt-1.1,2020.nlpcovid19-2.5,1,0.796341,"tails of the evaluation. 4.1.1 Covid Test Suite TICO-19 The TICO-19 test suite was developed to evaluate how well can MT systems handle the newlyemerged topic of COVID-19. Accurate automatic translation can play an important role in facilitating communication in order to protect at-risk populations and combat the infodemic of misinformation, as described by the World Health Organization. The test suite has no corresponding paper so its authors provided an analysis of the outcomes directly here. The submitted systems were evaluated using the test set from the recently-released TICO-19 dataset (Anastasopoulos et al., 2020). The dataset provides manually created translations of COVID19 related data. The test set consists of PubMed articles (678 sentences from 5 scientific articles), patient-medical professional conversations (104 sentences), as well as related Wikipedia articles (411 sentences), announcements (98 sentences from Wikisource), and news items (67 sentences from Wikinews), for a total of 2100 sentences. Table 15 outlines the BLEU scores by each submitted system in the English-to-X directions, also breaking down the results per domain. The analysis shows that some systems are significantly more prepar"
2020.wmt-1.1,2020.wmt-1.6,0,0.0647231,"AIP-NTT U BIQUS UEDIN UEDIN-CUNI UQAM_TAN L E VOLC T RANS W E C HAT WMTB IOMED BASELINE YOLO ZLABS - NLP Institution Air Force Research Laboratory (Gwinnup and Anderson, 2020) Independent submission (Xv, 2020) Charles University (Popel, 2020, 2018; Kocmi, 2020) Dublin City University (Parthasarathy et al., 2020) DeepMind (Yu et al., 2020) DiDi AI Labs (Chen et al., 2020b) (no associated paper) Indepdendent Submission (Kim et al., 2020) eTranslation (Oravecz et al., 2020) Facebook AI (Chen et al., 2020a) University of Groningen (Roest et al., 2020; Dhar et al., 2020) Global Tone Communication (Bei et al., 2020) University of Helsinki and Aalto University (Scherrer et al., 2020a) Huawei TSC (Wei et al., 2020a) Institute of Information Engineering, Chinese Academy of Sciences (Wei et al., 2020b) Microsoft STC India (Goyal et al., 2020) NICT-Kyoto (Marie et al., 2020) NICT-Rui (Li et al., 2020) NiuTrans (Zhang et al., 2020) National Research Council Canada (Knowles et al., 2020) OPPO (Shi et al., 2020) PROMT (Molchanov, 2020) SJTU-NICT (Li et al., 2020) Samsung Research Poland (Krubi´nski et al., 2020) TALP UPC (Escolano et al., 2020) Tencent Translation (Wu et al., 2020b) NLP Lab at Tsinghua Universit"
2020.wmt-1.1,2020.wmt-1.38,0,0.0746945,"Missing"
2020.wmt-1.1,2020.wmt-1.54,1,0.802974,"Missing"
2020.wmt-1.1,W07-0718,1,0.671054,"Missing"
2020.wmt-1.1,W08-0309,1,0.762341,"Missing"
2020.wmt-1.1,W12-3102,1,0.500805,"Missing"
2020.wmt-1.1,2020.lrec-1.461,0,0.0795779,"Missing"
2020.wmt-1.1,2012.eamt-1.60,0,0.124643,"tted systems, for those where the authors provided such details. The training corpus for Inuktitut↔English is the recently released Nunavut Hansard Inuktitut– English Parallel Corpus 3.0 (Joanis et al., 2020). For the Japanese↔English tasks, we added several freely available parallel corpora to the training data. It includes JParaCrawl v2.0 (Morishita et al., 2020), a large web-based parallel corpus, Japanese-English Subtitle Corpus (JESC) (Pryzant et al., 2017), the Kyoto Free Translation Task (KFTT) corpus (Neubig, 2011), constructed from the Kyoto-related Wikipedia articles, and TED Talks (Cettolo et al., 2012). The monolingual data we provided was similar to last year’s, with a 2019 news crawl added to all the news corpora. In addition, we provided versions of the news corpora for Czech, English and German, with both the document and paragraph structure retained. In other words, we did not apply sentence splitting to these corpora, and we retained the document boundaries and text ordering of the originals. Training, development, and test data for Pashto↔English and Khmer↔English are shared with the Parallel Corpus Filtering Shared Task (Koehn et al., 2020). The training data mostly comes from OPUS"
2020.wmt-1.1,2020.wmt-1.3,0,0.0731913,"on Machine Translation (WMT20)1 was held online with EMNLP 2020 and hosted a number of shared tasks on various aspects of machine translation. This conference built on 14 previous editions of WMT as workshops and conferences (Koehn and Monz, 2006; CallisonBurch et al., 2007, 2008, 2009, 2010, 2011, 2012; Bojar et al., 2013, 2014, 2015, 2016, 2017, 2018; Barrault et al., 2019). This year we conducted several official tasks. We report in this paper on the news and similar translation tasks. Additional shared tasks are described in separate papers in these proceedings: • automatic post-editing (Chatterjee et al., 2020) • biomedical translation (Bawden et al., 2020b) • chat translation (Farajian et al., 2020) • lifelong learning (Barrault et al., 2020) 1 Makoto Morishita NTT Santanu Pal WIPRO AI Abstract 1 Philipp Koehn JHU http://www.statmt.org/wmt20/ 1 Proceedings of the 5th Conference on Machine Translation (WMT), pages 1–55 c Online, November 19–20, 2020. 2020 Association for Computational Linguistics as “direct assessment”) that we explored in the previous years with convincing results in terms of the trade-off between annotation effort and reliable distinctions between systems. The primary objectives o"
2020.wmt-1.1,2020.wmt-1.8,0,0.0898111,"D D I D I -NLP DONG - NMT ENMT E T RANSLATION FACEBOOK AI G RONINGEN GTCOM H ELSINKI NLP H UAWEI TSC IIE M ICROSOFT STC I NDIA NICT-K YOTO NICT-RUI N IU T RANS NRC OPPO PROMT SJTU-NICT SRPOL TALP UPC T ENCENT T RANSLATION THUNLP T ILDE T OHOKU -AIP-NTT U BIQUS UEDIN UEDIN-CUNI UQAM_TAN L E VOLC T RANS W E C HAT WMTB IOMED BASELINE YOLO ZLABS - NLP Institution Air Force Research Laboratory (Gwinnup and Anderson, 2020) Independent submission (Xv, 2020) Charles University (Popel, 2020, 2018; Kocmi, 2020) Dublin City University (Parthasarathy et al., 2020) DeepMind (Yu et al., 2020) DiDi AI Labs (Chen et al., 2020b) (no associated paper) Indepdendent Submission (Kim et al., 2020) eTranslation (Oravecz et al., 2020) Facebook AI (Chen et al., 2020a) University of Groningen (Roest et al., 2020; Dhar et al., 2020) Global Tone Communication (Bei et al., 2020) University of Helsinki and Aalto University (Scherrer et al., 2020a) Huawei TSC (Wei et al., 2020a) Institute of Information Engineering, Chinese Academy of Sciences (Wei et al., 2020b) Microsoft STC India (Goyal et al., 2020) NICT-Kyoto (Marie et al., 2020) NICT-Rui (Li et al., 2020) NiuTrans (Zhang et al., 2020) National Research Council Canada (Know"
2020.wmt-1.1,2009.freeopmt-1.3,0,0.088081,"ve (CONTRASTIVE) or primary (PRIMARY), and the BLEU, RIBES and TER results. The scores are sorted by BLEU. In general, primary systems tend to be better than contrastive systems, as expected, but there are some exceptions. This year we recived major number of participants for the case of Indo-Aryan language group NUST-FJWU NUST-FJWU system is an extension of state-of-the-art Transformer model with hierarchical attention networks to incorporate contextual information. During training the model used back-translation. Prompsit This team is participating with a rulebased system based on Apertium (Forcada et al., 2009-11). Apertium is a free/open-source platform for developing rule-based machine translation systems and language technology that was first released in 2005. Apertium is hosted in Github where both language data and code are licensed under the GNU GPL. It is a research and business platform with a very active community that loves small languages. Language pairs are at a very different level of development and output quality in the platform, depending on two main variables: how much funded or in-kind effort has 32 5.4 i.e. Hindi–Marathi (in both directions). We received 22 submissions from 14 te"
2020.wmt-1.1,2020.wmt-1.80,0,0.0933589,"airs, to be evaluated on test sets consisting mainly of news stories. The task was also opened up to additional test suites to probe specific aspects of translation. In the similar language translation task, participants built machine translation systems for translating between closely related pairs of languages. Chi-kiu Lo NRC Masaaki Nagata NTT Marcos Zampieri Rochester Institute of Technology metrics (Mathur et al., 2020) parallel corpus filtering (Koehn et al., 2020) quality estimation (Specia et al., 2020a) robustness (Specia et al., 2020b) unsupervised and very low-resource translation (Fraser, 2020) In the news translation task (Section 2), participants were asked to translate a shared test set, optionally restricting themselves to the provided training data (“constrained” condition). We included 22 translation directions this year, with translation between English and each of Chinese, Czech, German and Russian, as well as French to and from German being repeated from last year, and English to and from Inuktitut, Japanese, Polish and Tamil being new for this year. Furthermore, English to and from Khmer and Pashto were included, using the same test sets as in the corpus filtering task. Th"
2020.wmt-1.1,W19-5204,0,0.0543621,"Missing"
2020.wmt-1.1,2020.emnlp-main.5,0,0.0410594,"luation of out-ofEnglish translations, HITs were generated using the same method as described for the SR+DC evaluation of into-English translations in Section 3.2.1 with minor modifications. Source-based DA allows to include human references in the evaluation as another system to provide an estimate of human performance. Human references were added to the pull of system outputs prior to sampling documents for tasks generation. If multiple references are available, which is the case for English→German (3 alternative reference translations, including 1 generated using the paraphrasing method of Freitag et al. (2020)) and English→Chinese (2 translations), each reference is assessed individually. Since the annotations are made by researchers and professional translators who ensure a betTable 11: Amount of data collected in the WMT20 manual document- and segment-level evaluation campaigns for bilingual/source-based evaluation out of English and nonEnglish pairs. et al., 2020; Laubli et al., 2020). It differs from SR+DC DA introduced in WMT19 (Bojar et al., 2019), and still used in into-English human evaluation this year, where a single segment from a document is provided on a screen at a time, followed by s"
2020.wmt-1.1,W18-3931,1,0.874637,"ese improvements to increased capacity (which allows increased sensitivity to long-range relations) of the models. 5 Similar Language Translation Most shared tasks at WMT (e.g. News, Biomedical) have historically dealt with translating texts from and to English. In recent years, we observed a growing interest in training systems to translate between languages other than English. This includes a number of papers applying MT to translate between pairs of closely-related languages, national language varieties, and dialects 27 of the same language (Zhang, 1998; Marujo et al., 2011; Hassani, 2017; Costa-jussà et al., 2018; Popovi´c et al., 2020). To address this topic, the first Similar Language Translation (SLT) shared task at WMT 2019 has been organized. It featured data from three pairs of closely-related languages from different language families: Spanish - Portuguese (Romance languages), Czech - Polish (Slavic languages), and Hindi - Nepali (IndoAryan languages). Following the success of the first SLT shared task at WMT 2019 and the interest of the community in this topic, we organize, for the second time at WMT, this shared task to evaluate the performance of state-of-the-art translation systems on trans"
2020.wmt-1.1,2020.wmt-1.18,0,0.0913945,"2020b) Microsoft STC India (Goyal et al., 2020) NICT-Kyoto (Marie et al., 2020) NICT-Rui (Li et al., 2020) NiuTrans (Zhang et al., 2020) National Research Council Canada (Knowles et al., 2020) OPPO (Shi et al., 2020) PROMT (Molchanov, 2020) SJTU-NICT (Li et al., 2020) Samsung Research Poland (Krubi´nski et al., 2020) TALP UPC (Escolano et al., 2020) Tencent Translation (Wu et al., 2020b) NLP Lab at Tsinghua University (no associated paper) Tilde (Krišlauks and Pinnis, 2020) Tohoku-AIP-NTT (Kiyono et al., 2020) Ubiqus (Hernandez and Nguyen, 2020) University of Edinburgh (Bawden et al., 2020a; Germann, 2020) University of Edinburgh and Charles University (Germann et al., 2020) Université du Québec à Montréal (no associated paper) ByteDance AI Lab (Wu et al., 2020a) WeChat (Meng et al., 2020) Baseline System from Biomedical Task (Bawden et al., 2020b) American University of Beirut (no associated paper) Zoho Corporation (no associated paper) Table 6: Participants in the shared translation task. Not all teams participated in all language pairs. The translations from the online systems were not submitted by their respective companies but were obtained by us, and are therefore anonymized in a fashion"
2020.wmt-1.1,2020.wmt-1.43,0,0.0835067,"Missing"
2020.wmt-1.1,2020.wmt-1.9,0,0.0939415,"Missing"
2020.wmt-1.1,2020.wmt-1.19,0,0.0674131,"Missing"
2020.wmt-1.1,2009.mtsummit-btm.6,0,0.103443,"Missing"
2020.wmt-1.1,W13-2305,1,0.929934,"work which can be well applied to different translation. directions. Techniques used in the submitted systems include optional multilingual pre-training (mRASP) for low resource languages, very deep Transformer or dynamic convolution models up to 50 encoder layers, iterative backtranslation, knowledge distillation, model ensemble and development set fine-tuning. The key ingredient of the process seems the strong focus on diversification of the (synthetic) training data, using multiple scalings of the Transformer model 3.1 Direct Assessment Since running a comparison of direct assessments (DA, Graham et al., 2013, 2014, 2016) and relative ranking in 2016 (Bojar et al., 2016) and verifying a high correlation of system rankings for the two methods, as well as the advantages of DA, such as quality controlled crowd-sourcing and linear growth relative to numbers of submissions, we have employed DA as the primary mechanism for evaluating systems. With DA human evaluation, 15 human assessors are asked to rate a given translation by how adequately it expresses the meaning of the corresponding reference translation or source language input on an analogue scale, which corresponds to an underlying absolute 0–100"
2020.wmt-1.1,E14-1047,1,0.888167,"Missing"
2020.wmt-1.1,2020.lrec-1.312,1,0.804196,"A screenshot of OCELoT is shown in Figure 5. For presentation of the results, systems are treated as either constrained or unconstrained, depending on whether their models were trained only on the provided data. Since we do not know how they were built, the online systems are treated as unconstrained during the automatic and human evaluations. In the rest of this section, we provide brief details of the submitted systems, for those where the authors provided such details. The training corpus for Inuktitut↔English is the recently released Nunavut Hansard Inuktitut– English Parallel Corpus 3.0 (Joanis et al., 2020). For the Japanese↔English tasks, we added several freely available parallel corpora to the training data. It includes JParaCrawl v2.0 (Morishita et al., 2020), a large web-based parallel corpus, Japanese-English Subtitle Corpus (JESC) (Pryzant et al., 2017), the Kyoto Free Translation Task (KFTT) corpus (Neubig, 2011), constructed from the Kyoto-related Wikipedia articles, and TED Talks (Cettolo et al., 2012). The monolingual data we provided was similar to last year’s, with a 2019 news crawl added to all the news corpora. In addition, we provided versions of the news corpora for Czech, Engli"
2020.wmt-1.1,2020.emnlp-main.6,1,0.839606,"shown in Table 3, where the first and second are simple merges or splits, whereas the third is a rare case of more complex reordering. We leave a detailed analysis of the translators’ treatment of paragraph-split data for future work. development set is provided, it is a mixture of both “source-original” and “target-original” texts, in order to maximise its size, although the original language is always marked in the sgm file, except for Inuktitut↔English. The consequences of directionality in test sets has been discussed recently in the literature (Freitag et al., 2019; Laubli et al., 2020; Graham et al., 2020), and the conclusion is that it can have an effect on detrimental effect on the accuracy of system evaluation. We use “source-original” parallel sentences wherever possible, on the basis that it is the more realistic scenario for practical MT usage. Exception: the test sets for the two Inuktitut↔English translation directions contain the same data, without regard to original direction. For most news text in the test and development sets, English was the original language and Inuktitut the translation, while the parliamentary data mixes the two directions. The origins of the news test documents"
2020.wmt-1.1,2020.wmt-1.11,0,0.0940191,"N GTCOM H ELSINKI NLP H UAWEI TSC IIE M ICROSOFT STC I NDIA NICT-K YOTO NICT-RUI N IU T RANS NRC OPPO PROMT SJTU-NICT SRPOL TALP UPC T ENCENT T RANSLATION THUNLP T ILDE T OHOKU -AIP-NTT U BIQUS UEDIN UEDIN-CUNI UQAM_TAN L E VOLC T RANS W E C HAT WMTB IOMED BASELINE YOLO ZLABS - NLP Institution Air Force Research Laboratory (Gwinnup and Anderson, 2020) Independent submission (Xv, 2020) Charles University (Popel, 2020, 2018; Kocmi, 2020) Dublin City University (Parthasarathy et al., 2020) DeepMind (Yu et al., 2020) DiDi AI Labs (Chen et al., 2020b) (no associated paper) Indepdendent Submission (Kim et al., 2020) eTranslation (Oravecz et al., 2020) Facebook AI (Chen et al., 2020a) University of Groningen (Roest et al., 2020; Dhar et al., 2020) Global Tone Communication (Bei et al., 2020) University of Helsinki and Aalto University (Scherrer et al., 2020a) Huawei TSC (Wei et al., 2020a) Institute of Information Engineering, Chinese Academy of Sciences (Wei et al., 2020b) Microsoft STC India (Goyal et al., 2020) NICT-Kyoto (Marie et al., 2020) NICT-Rui (Li et al., 2020) NiuTrans (Zhang et al., 2020) National Research Council Canada (Knowles et al., 2020) OPPO (Shi et al., 2020) PROMT (Molchanov, 2020) S"
2020.wmt-1.1,D19-1632,1,0.881933,"ent and paragraph structure retained. In other words, we did not apply sentence splitting to these corpora, and we retained the document boundaries and text ordering of the originals. Training, development, and test data for Pashto↔English and Khmer↔English are shared with the Parallel Corpus Filtering Shared Task (Koehn et al., 2020). The training data mostly comes from OPUS (software localization, Tatoeba, Global Voices), the Bible, and specialprepared corpora from TED Talks and the Jehova Witness web site (JW300). The development and test sets were created as part of the Flores initiative (Guzmán et al., 2019) by professional translation of Wikipedia content with careful vetting of the translations. Please refer the to the Parallel Corpus Filtering Shared Task overview paper for details on these corpora. 2.3.1 AFRL (Gwinnup and Anderson, 2020) AFRL - SYSCOMB 20 is a system combination consisting of two Marian transformer ensembles, one OpenNMT transformer system and a Moses phrase-based system. AFRL - FINETUNE is an OpenNMT transformer system fine-tuned on newstest2014-2017. 2.3.2 (Xv, 2020) ARIEL XV is a Transformer base model trained with the Sockeye sequence modeling toolkit usSome statistics ab"
2020.wmt-1.1,2020.wmt-1.12,1,0.754946,"Missing"
2020.wmt-1.1,2020.wmt-1.13,0,0.0737827,"2020b) (no associated paper) Indepdendent Submission (Kim et al., 2020) eTranslation (Oravecz et al., 2020) Facebook AI (Chen et al., 2020a) University of Groningen (Roest et al., 2020; Dhar et al., 2020) Global Tone Communication (Bei et al., 2020) University of Helsinki and Aalto University (Scherrer et al., 2020a) Huawei TSC (Wei et al., 2020a) Institute of Information Engineering, Chinese Academy of Sciences (Wei et al., 2020b) Microsoft STC India (Goyal et al., 2020) NICT-Kyoto (Marie et al., 2020) NICT-Rui (Li et al., 2020) NiuTrans (Zhang et al., 2020) National Research Council Canada (Knowles et al., 2020) OPPO (Shi et al., 2020) PROMT (Molchanov, 2020) SJTU-NICT (Li et al., 2020) Samsung Research Poland (Krubi´nski et al., 2020) TALP UPC (Escolano et al., 2020) Tencent Translation (Wu et al., 2020b) NLP Lab at Tsinghua University (no associated paper) Tilde (Krišlauks and Pinnis, 2020) Tohoku-AIP-NTT (Kiyono et al., 2020) Ubiqus (Hernandez and Nguyen, 2020) University of Edinburgh (Bawden et al., 2020a; Germann, 2020) University of Edinburgh and Charles University (Germann et al., 2020) Université du Québec à Montréal (no associated paper) ByteDance AI Lab (Wu et al., 2020a) WeChat (Meng et al"
2020.wmt-1.1,2020.wmt-1.20,0,0.057602,"Missing"
2020.wmt-1.1,2020.wmt-1.14,1,0.820019,"set, and the origlang tag indicates the original source language. 9 Team AFRL ARIEL XV CUNI DCU D EEP M IND D I D I -NLP DONG - NMT ENMT E T RANSLATION FACEBOOK AI G RONINGEN GTCOM H ELSINKI NLP H UAWEI TSC IIE M ICROSOFT STC I NDIA NICT-K YOTO NICT-RUI N IU T RANS NRC OPPO PROMT SJTU-NICT SRPOL TALP UPC T ENCENT T RANSLATION THUNLP T ILDE T OHOKU -AIP-NTT U BIQUS UEDIN UEDIN-CUNI UQAM_TAN L E VOLC T RANS W E C HAT WMTB IOMED BASELINE YOLO ZLABS - NLP Institution Air Force Research Laboratory (Gwinnup and Anderson, 2020) Independent submission (Xv, 2020) Charles University (Popel, 2020, 2018; Kocmi, 2020) Dublin City University (Parthasarathy et al., 2020) DeepMind (Yu et al., 2020) DiDi AI Labs (Chen et al., 2020b) (no associated paper) Indepdendent Submission (Kim et al., 2020) eTranslation (Oravecz et al., 2020) Facebook AI (Chen et al., 2020a) University of Groningen (Roest et al., 2020; Dhar et al., 2020) Global Tone Communication (Bei et al., 2020) University of Helsinki and Aalto University (Scherrer et al., 2020a) Huawei TSC (Wei et al., 2020a) Institute of Information Engineering, Chinese Academy of Sciences (Wei et al., 2020b) Microsoft STC India (Goyal et al., 2020) NICT-Kyoto (Mari"
2020.wmt-1.1,2020.wmt-1.39,1,0.812433,"kables was collected in the first phase of the annotation, which amounted to 4k assessments across the systems. The second annotation phase with 6.5k assessments compared markable translations, always checking outputs of all the 13 competing MT systems but still considering the document-level context of each of them. Among other things, the observations indicate that the better the system, the lower the variance in manual scores. Markables annotation then confirms that frequent errors like bad translation of a term need not be the most severe and conversely, 4.1.3 Gender Coreference and Bias (Kocmi et al., 2020) The test suite by Kocmi et al. (2020) focuses on the gender bias in professions (e.g. physician, teacher, secretary) for the translation from English into Czech, German, Polish and Russian. These nouns are ambiguous with respect to gender in English but exhibit gender in the examined target languages. The test suite is based on the fact that a pronoun referring to the ambiguous noun can reveal the gender of the noun in the English source sentence. Once disambiguated, the gender needs to be preserved in translation. To correctly translate the given noun, the translation system thus has to corr"
2020.wmt-1.1,2020.wmt-1.53,0,0.089538,"Missing"
2020.wmt-1.1,2020.wmt-1.78,1,0.815194,"rence on Machine Translation (WMT) 2020. In the news task, participants were asked to build machine translation systems for any of 11 language pairs, to be evaluated on test sets consisting mainly of news stories. The task was also opened up to additional test suites to probe specific aspects of translation. In the similar language translation task, participants built machine translation systems for translating between closely related pairs of languages. Chi-kiu Lo NRC Masaaki Nagata NTT Marcos Zampieri Rochester Institute of Technology metrics (Mathur et al., 2020) parallel corpus filtering (Koehn et al., 2020) quality estimation (Specia et al., 2020a) robustness (Specia et al., 2020b) unsupervised and very low-resource translation (Fraser, 2020) In the news translation task (Section 2), participants were asked to translate a shared test set, optionally restricting themselves to the provided training data (“constrained” condition). We included 22 translation directions this year, with translation between English and each of Chinese, Czech, German and Russian, as well as French to and from German being repeated from last year, and English to and from Inuktitut, Japanese, Polish and Tamil being new fo"
2020.wmt-1.1,W17-1208,0,0.0524248,"ttribute all these improvements to increased capacity (which allows increased sensitivity to long-range relations) of the models. 5 Similar Language Translation Most shared tasks at WMT (e.g. News, Biomedical) have historically dealt with translating texts from and to English. In recent years, we observed a growing interest in training systems to translate between languages other than English. This includes a number of papers applying MT to translate between pairs of closely-related languages, national language varieties, and dialects 27 of the same language (Zhang, 1998; Marujo et al., 2011; Hassani, 2017; Costa-jussà et al., 2018; Popovi´c et al., 2020). To address this topic, the first Similar Language Translation (SLT) shared task at WMT 2019 has been organized. It featured data from three pairs of closely-related languages from different language families: Spanish - Portuguese (Romance languages), Czech - Polish (Slavic languages), and Hindi - Nepali (IndoAryan languages). Following the success of the first SLT shared task at WMT 2019 and the interest of the community in this topic, we organize, for the second time at WMT, this shared task to evaluate the performance of state-of-the-art tr"
2020.wmt-1.1,2020.wmt-1.21,0,0.0791885,"Missing"
2020.wmt-1.1,2020.wmt-1.23,0,0.0607352,"020) Dublin City University (Parthasarathy et al., 2020) DeepMind (Yu et al., 2020) DiDi AI Labs (Chen et al., 2020b) (no associated paper) Indepdendent Submission (Kim et al., 2020) eTranslation (Oravecz et al., 2020) Facebook AI (Chen et al., 2020a) University of Groningen (Roest et al., 2020; Dhar et al., 2020) Global Tone Communication (Bei et al., 2020) University of Helsinki and Aalto University (Scherrer et al., 2020a) Huawei TSC (Wei et al., 2020a) Institute of Information Engineering, Chinese Academy of Sciences (Wei et al., 2020b) Microsoft STC India (Goyal et al., 2020) NICT-Kyoto (Marie et al., 2020) NICT-Rui (Li et al., 2020) NiuTrans (Zhang et al., 2020) National Research Council Canada (Knowles et al., 2020) OPPO (Shi et al., 2020) PROMT (Molchanov, 2020) SJTU-NICT (Li et al., 2020) Samsung Research Poland (Krubi´nski et al., 2020) TALP UPC (Escolano et al., 2020) Tencent Translation (Wu et al., 2020b) NLP Lab at Tsinghua University (no associated paper) Tilde (Krišlauks and Pinnis, 2020) Tohoku-AIP-NTT (Kiyono et al., 2020) Ubiqus (Hernandez and Nguyen, 2020) University of Edinburgh (Bawden et al., 2020a; Germann, 2020) University of Edinburgh and Charles University (Germann et al., 2"
2020.wmt-1.1,2020.wmt-1.77,1,0.84299,"slation task, both organised alongside the Conference on Machine Translation (WMT) 2020. In the news task, participants were asked to build machine translation systems for any of 11 language pairs, to be evaluated on test sets consisting mainly of news stories. The task was also opened up to additional test suites to probe specific aspects of translation. In the similar language translation task, participants built machine translation systems for translating between closely related pairs of languages. Chi-kiu Lo NRC Masaaki Nagata NTT Marcos Zampieri Rochester Institute of Technology metrics (Mathur et al., 2020) parallel corpus filtering (Koehn et al., 2020) quality estimation (Specia et al., 2020a) robustness (Specia et al., 2020b) unsupervised and very low-resource translation (Fraser, 2020) In the news translation task (Section 2), participants were asked to translate a shared test set, optionally restricting themselves to the provided training data (“constrained” condition). We included 22 translation directions this year, with translation between English and each of Chinese, Czech, German and Russian, as well as French to and from German being repeated from last year, and English to and from Inu"
2020.wmt-1.1,2020.wmt-1.24,0,0.0435945,"Missing"
2020.wmt-1.1,D18-1512,0,0.05415,"Missing"
2020.wmt-1.1,2020.wmt-1.47,1,0.740067,"Missing"
2020.wmt-1.1,W18-3601,1,0.891679,", we have employed DA as the primary mechanism for evaluating systems. With DA human evaluation, 15 human assessors are asked to rate a given translation by how adequately it expresses the meaning of the corresponding reference translation or source language input on an analogue scale, which corresponds to an underlying absolute 0–100 rating scale.5 No sentence or document length restriction is applied during manual evaluation. Direct Assessment is also employed for evaluation of video captioning systems at TRECvid (Graham et al., 2018; Awad et al., 2019) and multilingual surface realisation (Mille et al., 2018, 2019). 3.1.1 tion 2, most of our test sets do not include reversecreated sentence pairs, except when there were resource constraints on the creation of the test sets. 3.1.3 Prior to WMT19, the issue of including document context was raised within the community (Läubli et al., 2018; Toral et al., 2018) and at WMT19 a range of DA styles were subsequently tested that included document context. In WMT19, two options were run, firstly, an evaluation that included the document context “+DC” (with document context), and secondly, a variation that omitted document context “−DC” (without document con"
2020.wmt-1.1,2020.wmt-1.27,0,0.247738,"he original source language. 9 Team AFRL ARIEL XV CUNI DCU D EEP M IND D I D I -NLP DONG - NMT ENMT E T RANSLATION FACEBOOK AI G RONINGEN GTCOM H ELSINKI NLP H UAWEI TSC IIE M ICROSOFT STC I NDIA NICT-K YOTO NICT-RUI N IU T RANS NRC OPPO PROMT SJTU-NICT SRPOL TALP UPC T ENCENT T RANSLATION THUNLP T ILDE T OHOKU -AIP-NTT U BIQUS UEDIN UEDIN-CUNI UQAM_TAN L E VOLC T RANS W E C HAT WMTB IOMED BASELINE YOLO ZLABS - NLP Institution Air Force Research Laboratory (Gwinnup and Anderson, 2020) Independent submission (Xv, 2020) Charles University (Popel, 2020, 2018; Kocmi, 2020) Dublin City University (Parthasarathy et al., 2020) DeepMind (Yu et al., 2020) DiDi AI Labs (Chen et al., 2020b) (no associated paper) Indepdendent Submission (Kim et al., 2020) eTranslation (Oravecz et al., 2020) Facebook AI (Chen et al., 2020a) University of Groningen (Roest et al., 2020; Dhar et al., 2020) Global Tone Communication (Bei et al., 2020) University of Helsinki and Aalto University (Scherrer et al., 2020a) Huawei TSC (Wei et al., 2020a) Institute of Information Engineering, Chinese Academy of Sciences (Wei et al., 2020b) Microsoft STC India (Goyal et al., 2020) NICT-Kyoto (Marie et al., 2020) NICT-Rui (Li et al., 2020) NiuTrans"
2020.wmt-1.1,D19-6301,1,0.888512,"Missing"
2020.wmt-1.1,W18-6424,0,0.0431841,"(Kocmi, 2020) combines transfer learning from a high-resource language pair Czech–English into the low-resource Inuktitut-English with an additional backtranslation step. Surprising behaviour is noticed when using synthetic data, which can be possibly attributed to a narrow domain of training and test data. The system is the Transformer model in a constrained submission. 2.3.3 Charles University (CUNI) CUNI-D OC T RANSFORMER (Popel, 2020) is similar to the sentence-level version (CUNI-T2T2018, CUBBITT), but trained on sequences with multiple sentences of up to 3000 characters. CUNI-T2T-2018 (Popel, 2018), also called CUBBITT, is exactly the same system as in WMT2018. It is the Transformer model trained according to Popel and Bojar (2018) plus a novel concat-regime backtranslation with checkpoint averaging (Popel et al., 2020), tuned separately for CZ-domain and non CZ-domain articles, possibly handling also translation-direction (“translationese”) issues. For cs→en also a coreference preprocessing was used adding the female-gender CUNI-T RANSFORMER (Popel, 2020) is similar to the WMT2018 version of CUBBITT, but with 12 encoder layers instead of 6 and trained on CzEng 2.0 instead of CzEng 1.7."
2020.wmt-1.1,2020.wmt-1.25,0,0.094349,"Missing"
2020.wmt-1.1,2020.wmt-1.28,0,0.0792624,"cument in the test set, and the origlang tag indicates the original source language. 9 Team AFRL ARIEL XV CUNI DCU D EEP M IND D I D I -NLP DONG - NMT ENMT E T RANSLATION FACEBOOK AI G RONINGEN GTCOM H ELSINKI NLP H UAWEI TSC IIE M ICROSOFT STC I NDIA NICT-K YOTO NICT-RUI N IU T RANS NRC OPPO PROMT SJTU-NICT SRPOL TALP UPC T ENCENT T RANSLATION THUNLP T ILDE T OHOKU -AIP-NTT U BIQUS UEDIN UEDIN-CUNI UQAM_TAN L E VOLC T RANS W E C HAT WMTB IOMED BASELINE YOLO ZLABS - NLP Institution Air Force Research Laboratory (Gwinnup and Anderson, 2020) Independent submission (Xv, 2020) Charles University (Popel, 2020, 2018; Kocmi, 2020) Dublin City University (Parthasarathy et al., 2020) DeepMind (Yu et al., 2020) DiDi AI Labs (Chen et al., 2020b) (no associated paper) Indepdendent Submission (Kim et al., 2020) eTranslation (Oravecz et al., 2020) Facebook AI (Chen et al., 2020a) University of Groningen (Roest et al., 2020; Dhar et al., 2020) Global Tone Communication (Bei et al., 2020) University of Helsinki and Aalto University (Scherrer et al., 2020a) Huawei TSC (Wei et al., 2020a) Institute of Information Engineering, Chinese Academy of Sciences (Wei et al., 2020b) Microsoft STC India (Goyal et al., 20"
2020.wmt-1.1,2020.lrec-1.443,1,0.79707,"er their models were trained only on the provided data. Since we do not know how they were built, the online systems are treated as unconstrained during the automatic and human evaluations. In the rest of this section, we provide brief details of the submitted systems, for those where the authors provided such details. The training corpus for Inuktitut↔English is the recently released Nunavut Hansard Inuktitut– English Parallel Corpus 3.0 (Joanis et al., 2020). For the Japanese↔English tasks, we added several freely available parallel corpora to the training data. It includes JParaCrawl v2.0 (Morishita et al., 2020), a large web-based parallel corpus, Japanese-English Subtitle Corpus (JESC) (Pryzant et al., 2017), the Kyoto Free Translation Task (KFTT) corpus (Neubig, 2011), constructed from the Kyoto-related Wikipedia articles, and TED Talks (Cettolo et al., 2012). The monolingual data we provided was similar to last year’s, with a 2019 news crawl added to all the news corpora. In addition, we provided versions of the news corpora for Czech, English and German, with both the document and paragraph structure retained. In other words, we did not apply sentence splitting to these corpora, and we retained t"
2020.wmt-1.1,2020.wmt-1.48,0,0.090424,"Missing"
2020.wmt-1.1,2020.wmt-1.49,0,0.0519886,"Missing"
2020.wmt-1.1,W19-6712,0,0.0573476,"tly crawled multilingual parallel corpora from Indian government websites (Haddow and Kirefu, 2020; Siripragada et al., 2020), the Tanzil corpus (Tiedemann, 2009), the Pavlick dicParagraph-split Test Sets For the language pairs English↔Czech, English↔German and English→Chinese, we provided the translators with paragraph-split texts, instead of sentence-split texts. We did this in order to provide the translators with greater freedom and, hopefully, to improve the quality of the translation. Allowing translators to merge and split sentences removes one of the “translation shifts” identified by Popovic (2019), which can make translations create solely for MT evaluation different from translations produced for other purposes. We first show some descriptive statistics of the source texts, for Czech, English and German, in 3 Europarl Parallel Corpus Czech ↔ English German ↔ English Polish↔ English German ↔ French Sentences 645,241 1,825,745 632,435 1,801,076 Words 14,948,900 17,380,340 48,125,573 50,506,059 14,691,199 16,995,232 47,517,102 55,366,136 Distinct words 172,452 63,289 371,748 113,960 170,271 62,694 368,585 134,762 News Commentary Parallel Corpus Czech ↔ English 248,927 5,570,734 6,156,063"
2020.wmt-1.1,2020.wmt-1.26,0,0.0845151,"Missing"
2020.wmt-1.1,2020.vardial-1.10,0,0.0933804,"Missing"
2020.wmt-1.1,W18-6301,0,0.038239,"Missing"
2020.wmt-1.1,2020.wmt-1.51,0,0.0917045,"Missing"
2020.wmt-1.1,2020.wmt-1.50,1,0.78567,"Missing"
2020.wmt-1.1,2020.wmt-1.52,0,0.0485419,"Missing"
2020.wmt-1.1,P19-1164,0,0.0581517,"26 26.37 25.51 24.82 28.33 23.33 21.13 21.96 20.43 22.90 22.58 21.90 22.17 22.17 20.53 19.40 20.01 40.44 32.39 30.39 37.04 32.27 27.54 25.97 26.09 46.38 37.30 36.05 35.96 33.76 33.07 27.20 27.07 Table 15: TICO-19 test suite results on the English-to-X WMT20 translation directions. 26 4.1.5 antecedent (a less common direction of information flow), and then correctly express the noun in the target language. The success of the MT system in this test can be established automatically, whenever the gender of the target word can be automatically identified. Kocmi et al. (2020) build upon the WinoMT (Stanovsky et al., 2019) test set, which provides exactly the necessary type of sentences containing an ambiguous profession noun and a personal pronoun which unambiguously (for the human eye) refers to it based the situation described. When extending WinMT with Czech and Polish, Stanovsky et al. have to disregard some test patterns but the principle remains. The results indicate that all MT systems fail in this test, following gender bias (stereotypical patterns attributing the masculine gender to some professions and feminine gender to others) rather than the coreference link. Word Sense Disambiguation (Scherrer et"
2020.wmt-1.1,2020.wmt-1.31,0,0.0881792,"morphological segmentation of the polysynthetic Inuktitut, testing rule-based, supervised, semi-supervised as well as unsupervised word segmentation methods, (2) whether or not adding data from a related language (Greenlandic) helps, and (3) whether contextual word embeddings (XLM) improve translation. G RONINGEN - ENIU use Transformer implemented in Marian with the default setting, improving the performance also with tagged backtranslation, domain-specific data, ensembling and finetuning. 2.3.7 DONG - NMT (no associated paper) No description provided. 2.3.8 ENMT (Kim et al., 2020) Kim et al. (2020) base their approach on transferring knowledge of domain and linguistic characteristics by pre-training the encoder-decoder model with large amount of in-domain monolingual data through unsupervised and supervised prediction task. The model is then fine-tuned with parallel data and in-domain synthetic data, generated with iterative back-translation. For additional gain, final results are generated with an ensemble model and re-ranked with averaged models and language models. G RONINGEN - ENTAM (Dhar et al., 2020) study the effects of various techniques such as linguistically motivated segmenta"
2020.wmt-1.1,2020.wmt-1.32,0,0.0839317,"- NLP Institution Air Force Research Laboratory (Gwinnup and Anderson, 2020) Independent submission (Xv, 2020) Charles University (Popel, 2020, 2018; Kocmi, 2020) Dublin City University (Parthasarathy et al., 2020) DeepMind (Yu et al., 2020) DiDi AI Labs (Chen et al., 2020b) (no associated paper) Indepdendent Submission (Kim et al., 2020) eTranslation (Oravecz et al., 2020) Facebook AI (Chen et al., 2020a) University of Groningen (Roest et al., 2020; Dhar et al., 2020) Global Tone Communication (Bei et al., 2020) University of Helsinki and Aalto University (Scherrer et al., 2020a) Huawei TSC (Wei et al., 2020a) Institute of Information Engineering, Chinese Academy of Sciences (Wei et al., 2020b) Microsoft STC India (Goyal et al., 2020) NICT-Kyoto (Marie et al., 2020) NICT-Rui (Li et al., 2020) NiuTrans (Zhang et al., 2020) National Research Council Canada (Knowles et al., 2020) OPPO (Shi et al., 2020) PROMT (Molchanov, 2020) SJTU-NICT (Li et al., 2020) Samsung Research Poland (Krubi´nski et al., 2020) TALP UPC (Escolano et al., 2020) Tencent Translation (Wu et al., 2020b) NLP Lab at Tsinghua University (no associated paper) Tilde (Krišlauks and Pinnis, 2020) Tohoku-AIP-NTT (Kiyono et al., 2020) Ub"
2020.wmt-1.1,2020.wmt-1.33,0,0.080166,"Missing"
2020.wmt-1.1,2020.wmt-1.34,0,0.0803867,"Missing"
2020.wmt-1.1,2020.wmt-1.35,0,0.0951745,"ss web site (JW300). The development and test sets were created as part of the Flores initiative (Guzmán et al., 2019) by professional translation of Wikipedia content with careful vetting of the translations. Please refer the to the Parallel Corpus Filtering Shared Task overview paper for details on these corpora. 2.3.1 AFRL (Gwinnup and Anderson, 2020) AFRL - SYSCOMB 20 is a system combination consisting of two Marian transformer ensembles, one OpenNMT transformer system and a Moses phrase-based system. AFRL - FINETUNE is an OpenNMT transformer system fine-tuned on newstest2014-2017. 2.3.2 (Xv, 2020) ARIEL XV is a Transformer base model trained with the Sockeye sequence modeling toolkit usSome statistics about the training and test materials are given in Figures 1, 2, 3 and 4. 4 8 ARIEL XV https://github.com/AppraiseDev/OCELoT English I English II English III Chinese Czech German Inuktitut Japanese Polish Russian Tamil ABC News (2), All Africa (5), Brisbane Times (1), CBS LA (1), CBS News (1), CNBC (3), CNN (2), Daily Express (1), Daily Mail (2), Fox News (1), Gateway (1), Guardian (3), Huffington Post (2), London Evening Standard (2), Metro (2), NDTV (7), RTE (7), Reuters (4), STV (2), S"
2020.wmt-1.1,2020.wmt-1.55,0,0.0877526,"Missing"
2020.wmt-1.1,P17-4012,0,0.0273892,"o SJTU-NICT using large XLM model to improve NMT but the exact relation is unclear. 2.3.14 H UAWEI TSC (Wei et al., 2020a) H UAWEI TSC use Transformer-big with a further increased model size, focussing on standard techniques of careful pre-processing and filtering, back-translation and forward translation, including self-training, i.e. translating one of the sides of the original parallel data. Ensembling of individual training runs is used in the forward as well as backward translation, and single models are created from the ensembles using knowledge distillation. The submission uses THUNMT (Zhang et al., 2017) open-source engine. 2.3.19 N IU T RANS (Zhang et al., 2020) N IU T RANS gain their performance from focussed attention to six areas: (1) careful data preprocessing and filtering, (2) iterative back-translation to generate additional training data, (3) using different model architectures, such as wider and/or deeper models, relative position representation and relative length, to enhance the diversity of translations, (4) iterative knowledge distillation by in-domain monolingual data, (5) iterative finetuning for domain adaptation using small training batches, (6) rule-based post-processing of"
2020.wmt-1.1,P98-2238,0,0.590812,"and punctuation, and we tend to attribute all these improvements to increased capacity (which allows increased sensitivity to long-range relations) of the models. 5 Similar Language Translation Most shared tasks at WMT (e.g. News, Biomedical) have historically dealt with translating texts from and to English. In recent years, we observed a growing interest in training systems to translate between languages other than English. This includes a number of papers applying MT to translate between pairs of closely-related languages, national language varieties, and dialects 27 of the same language (Zhang, 1998; Marujo et al., 2011; Hassani, 2017; Costa-jussà et al., 2018; Popovi´c et al., 2020). To address this topic, the first Similar Language Translation (SLT) shared task at WMT 2019 has been organized. It featured data from three pairs of closely-related languages from different language families: Spanish - Portuguese (Romance languages), Czech - Polish (Slavic languages), and Hindi - Nepali (IndoAryan languages). Following the success of the first SLT shared task at WMT 2019 and the interest of the community in this topic, we organize, for the second time at WMT, this shared task to evaluate th"
2020.wmt-1.1,2020.wmt-1.41,1,0.814291,"Missing"
2020.wmt-1.67,P15-1166,0,0.0333463,"(2019c) used constrained decoding (Hokamp and Liu, 2017) in conjunction with a set of constraints (e.g., avoiding certain words which are present in the input) when creating synthetic paraphrases from bitext. Kajiwara (2019) also used hard constraints, but at decoding time. Our work is similar but uses “soft” constraints (i.e., down-weighting tokens which complete n-grams in the input, but not disallowing them all together). Another approach is to control generation with syntactic examples (Iyyer et al., 2018; Chen et al., 2019a) or codes (Shu et al., 2019). Multilingual NMT Multilingual NMT (Dong et al., 2015) has been shown to enable zero-shot translation—that is, translation between languages pairs not included in training (e.g., translating from Spanish→Arabic at test time when the model was trained on Spanish→English and English→Arabic, but not Spanish→Arabic) (Johnson et al., 2017; Gu et al., 2018; Pham et al., 2019). Zhou et al. (2019) also explored incorporating paraphrase data into training to improve multilingual NMT performance. Tiedemann and Scherrer (2019) explored using paraphrase recognition to test the semantic abstraction of a fairly small multilingual NMT system trained on Bibles a"
2020.wmt-1.67,W19-5301,1,0.882738,"Missing"
2020.wmt-1.67,D17-1091,0,0.0385855,"r method produces paraphrases that better preserve meaning and are more gramatical, for the same level of lexical diversity. Additional smaller human assessments demonstrate our approach also works in two non-English languages. 1 Introduction Paraphrase generation is the task of producing a fluent output sentence which is semantically similar to the input sentence while being syntactically and/or lexically different from it (Bhagat and Hovy, 2013). Paraphrasing has been of longstanding interest in the NLP community (McKeown, 1983) and has been used for data augmentation in question answering (Dong et al., 2017; Gan and Ng, 2019), machine translation (MT) (Hu et al., 2019a; Khayrallah et al., 2020), task oriented dialog (Niu and Bansal, 2018, 2019), and MT metrics (Banerjee and Lavie, 2005; Zhou et al., 2006; Denkowski and Lavie, 2010; Thompson and Post, 2020). Thompson and Post (2020) recently released the Prism MT metric, which uses a multilingual neural MT (NMT) model as a paraphraser to score paraphrastic pairs; they treat paraphrasing as a zero-shot translation task (e.g., “translation” from English to English) and force-decode and score MT system outputs conditioned on their respective human t"
2020.wmt-1.67,J13-3001,0,0.216505,"conduct a human evaluation to compare our method to a paraphraser trained on the large English synthetic paraphrase database ParaBank 2 (Hu et al., 2019c) and find that our method produces paraphrases that better preserve meaning and are more gramatical, for the same level of lexical diversity. Additional smaller human assessments demonstrate our approach also works in two non-English languages. 1 Introduction Paraphrase generation is the task of producing a fluent output sentence which is semantically similar to the input sentence while being syntactically and/or lexically different from it (Bhagat and Hovy, 2013). Paraphrasing has been of longstanding interest in the NLP community (McKeown, 1983) and has been used for data augmentation in question answering (Dong et al., 2017; Gan and Ng, 2019), machine translation (MT) (Hu et al., 2019a; Khayrallah et al., 2020), task oriented dialog (Niu and Bansal, 2018, 2019), and MT metrics (Banerjee and Lavie, 2005; Zhou et al., 2006; Denkowski and Lavie, 2010; Thompson and Post, 2020). Thompson and Post (2020) recently released the Prism MT metric, which uses a multilingual neural MT (NMT) model as a paraphraser to score paraphrastic pairs; they treat paraphras"
2020.wmt-1.67,W16-2302,0,0.0529486,"Missing"
2020.wmt-1.67,P19-1599,0,0.0334054,"ck again (Mallinson et al., 2017). Multiple pivot languages can be used to lessen the effect of inherent ambiguities (Aziz and Specia, 2013), at the expense of complication. Several works have focused on training on paraphrase data, including synthetic data created by starting with bitext and translating one side into the language of the other side to create synthetic paraphrases (Wieting et al., 2017; Wieting and Gimpel, 2018; Hu et al., 2019c). Ideas such as adversarial training (Iyyer et al., 2018), reinforcement learning (Li et al., 2018), and variational autoencoders (Gupta et al., 2018; Chen et al., 2019b) have also been explored in the context of paraphrase generation. Diversity in Generation Creating paraphrases which differ from their input in non-trivial ways is a challenging problem. Hu et al. (2019c) used constrained decoding (Hokamp and Liu, 2017) in conjunction with a set of constraints (e.g., avoiding certain words which are present in the input) when creating synthetic paraphrases from bitext. Kajiwara (2019) also used hard constraints, but at decoding time. Our work is similar but uses “soft” constraints (i.e., down-weighting tokens which complete n-grams in the input, but not disa"
2020.wmt-1.67,N19-1254,0,0.0232615,"ck again (Mallinson et al., 2017). Multiple pivot languages can be used to lessen the effect of inherent ambiguities (Aziz and Specia, 2013), at the expense of complication. Several works have focused on training on paraphrase data, including synthetic data created by starting with bitext and translating one side into the language of the other side to create synthetic paraphrases (Wieting et al., 2017; Wieting and Gimpel, 2018; Hu et al., 2019c). Ideas such as adversarial training (Iyyer et al., 2018), reinforcement learning (Li et al., 2018), and variational autoencoders (Gupta et al., 2018; Chen et al., 2019b) have also been explored in the context of paraphrase generation. Diversity in Generation Creating paraphrases which differ from their input in non-trivial ways is a challenging problem. Hu et al. (2019c) used constrained decoding (Hokamp and Liu, 2017) in conjunction with a set of constraints (e.g., avoiding certain words which are present in the input) when creating synthetic paraphrases from bitext. Kajiwara (2019) also used hard constraints, but at decoding time. Our work is similar but uses “soft” constraints (i.e., down-weighting tokens which complete n-grams in the input, but not disa"
2020.wmt-1.67,P19-1610,0,0.019075,"araphrases that better preserve meaning and are more gramatical, for the same level of lexical diversity. Additional smaller human assessments demonstrate our approach also works in two non-English languages. 1 Introduction Paraphrase generation is the task of producing a fluent output sentence which is semantically similar to the input sentence while being syntactically and/or lexically different from it (Bhagat and Hovy, 2013). Paraphrasing has been of longstanding interest in the NLP community (McKeown, 1983) and has been used for data augmentation in question answering (Dong et al., 2017; Gan and Ng, 2019), machine translation (MT) (Hu et al., 2019a; Khayrallah et al., 2020), task oriented dialog (Niu and Bansal, 2018, 2019), and MT metrics (Banerjee and Lavie, 2005; Zhou et al., 2006; Denkowski and Lavie, 2010; Thompson and Post, 2020). Thompson and Post (2020) recently released the Prism MT metric, which uses a multilingual neural MT (NMT) model as a paraphraser to score paraphrastic pairs; they treat paraphrasing as a zero-shot translation task (e.g., “translation” from English to English) and force-decode and score MT system outputs conditioned on their respective human translations. They d"
2020.wmt-1.67,N18-1032,0,0.024517,"es “soft” constraints (i.e., down-weighting tokens which complete n-grams in the input, but not disallowing them all together). Another approach is to control generation with syntactic examples (Iyyer et al., 2018; Chen et al., 2019a) or codes (Shu et al., 2019). Multilingual NMT Multilingual NMT (Dong et al., 2015) has been shown to enable zero-shot translation—that is, translation between languages pairs not included in training (e.g., translating from Spanish→Arabic at test time when the model was trained on Spanish→English and English→Arabic, but not Spanish→Arabic) (Johnson et al., 2017; Gu et al., 2018; Pham et al., 2019). Zhou et al. (2019) also explored incorporating paraphrase data into training to improve multilingual NMT performance. Tiedemann and Scherrer (2019) explored using paraphrase recognition to test the semantic abstraction of a fairly small multilingual NMT system trained on Bibles and also demonstrate the model’s ability to paraphrase in English. However, they did not perform a human evaluation of paraphrase quality, and Thompson and Post (2020) found that simply generating via beam search from a multilingual NMT model trained on a large general domain corpus results in triv"
2020.wmt-1.67,P17-1141,0,0.160265,"c data created by starting with bitext and translating one side into the language of the other side to create synthetic paraphrases (Wieting et al., 2017; Wieting and Gimpel, 2018; Hu et al., 2019c). Ideas such as adversarial training (Iyyer et al., 2018), reinforcement learning (Li et al., 2018), and variational autoencoders (Gupta et al., 2018; Chen et al., 2019b) have also been explored in the context of paraphrase generation. Diversity in Generation Creating paraphrases which differ from their input in non-trivial ways is a challenging problem. Hu et al. (2019c) used constrained decoding (Hokamp and Liu, 2017) in conjunction with a set of constraints (e.g., avoiding certain words which are present in the input) when creating synthetic paraphrases from bitext. Kajiwara (2019) also used hard constraints, but at decoding time. Our work is similar but uses “soft” constraints (i.e., down-weighting tokens which complete n-grams in the input, but not disallowing them all together). Another approach is to control generation with syntactic examples (Iyyer et al., 2018; Chen et al., 2019a) or codes (Shu et al., 2019). Multilingual NMT Multilingual NMT (Dong et al., 2015) has been shown to enable zero-shot tr"
2020.wmt-1.67,N19-1090,1,0.911193,"Missing"
2020.wmt-1.67,K19-1005,1,0.893102,"Missing"
2020.wmt-1.67,N18-1170,0,0.026196,"et al., 2004). Another method to generate a paraphrase is to translate a text to a different language and then back again (Mallinson et al., 2017). Multiple pivot languages can be used to lessen the effect of inherent ambiguities (Aziz and Specia, 2013), at the expense of complication. Several works have focused on training on paraphrase data, including synthetic data created by starting with bitext and translating one side into the language of the other side to create synthetic paraphrases (Wieting et al., 2017; Wieting and Gimpel, 2018; Hu et al., 2019c). Ideas such as adversarial training (Iyyer et al., 2018), reinforcement learning (Li et al., 2018), and variational autoencoders (Gupta et al., 2018; Chen et al., 2019b) have also been explored in the context of paraphrase generation. Diversity in Generation Creating paraphrases which differ from their input in non-trivial ways is a challenging problem. Hu et al. (2019c) used constrained decoding (Hokamp and Liu, 2017) in conjunction with a set of constraints (e.g., avoiding certain words which are present in the input) when creating synthetic paraphrases from bitext. Kajiwara (2019) also used hard constraints, but at decoding time. Our work is sim"
2020.wmt-1.67,P19-1607,0,0.0183289,"2018; Hu et al., 2019c). Ideas such as adversarial training (Iyyer et al., 2018), reinforcement learning (Li et al., 2018), and variational autoencoders (Gupta et al., 2018; Chen et al., 2019b) have also been explored in the context of paraphrase generation. Diversity in Generation Creating paraphrases which differ from their input in non-trivial ways is a challenging problem. Hu et al. (2019c) used constrained decoding (Hokamp and Liu, 2017) in conjunction with a set of constraints (e.g., avoiding certain words which are present in the input) when creating synthetic paraphrases from bitext. Kajiwara (2019) also used hard constraints, but at decoding time. Our work is similar but uses “soft” constraints (i.e., down-weighting tokens which complete n-grams in the input, but not disallowing them all together). Another approach is to control generation with syntactic examples (Iyyer et al., 2018; Chen et al., 2019a) or codes (Shu et al., 2019). Multilingual NMT Multilingual NMT (Dong et al., 2015) has been shown to enable zero-shot translation—that is, translation between languages pairs not included in training (e.g., translating from Spanish→Arabic at test time when the model was trained on Spanis"
2020.wmt-1.67,2020.emnlp-main.7,1,0.833445,", for the same level of lexical diversity. Additional smaller human assessments demonstrate our approach also works in two non-English languages. 1 Introduction Paraphrase generation is the task of producing a fluent output sentence which is semantically similar to the input sentence while being syntactically and/or lexically different from it (Bhagat and Hovy, 2013). Paraphrasing has been of longstanding interest in the NLP community (McKeown, 1983) and has been used for data augmentation in question answering (Dong et al., 2017; Gan and Ng, 2019), machine translation (MT) (Hu et al., 2019a; Khayrallah et al., 2020), task oriented dialog (Niu and Bansal, 2018, 2019), and MT metrics (Banerjee and Lavie, 2005; Zhou et al., 2006; Denkowski and Lavie, 2010; Thompson and Post, 2020). Thompson and Post (2020) recently released the Prism MT metric, which uses a multilingual neural MT (NMT) model as a paraphraser to score paraphrastic pairs; they treat paraphrasing as a zero-shot translation task (e.g., “translation” from English to English) and force-decode and score MT system outputs conditioned on their respective human translations. They denote their paraphraser as lexically/syntactically unbiased as it does"
2020.wmt-1.67,2005.mtsummit-papers.11,0,0.0606572,"20), which uses a Transformer (Vaswani et al., 2017) architecture with approximately 750 million parameters. The model was trained in fairseq (Ott et al., 2019). The authors take several steps to encourage the encoder and decoder to be language agnostic, including specifying the target language as the first token in the target, so that the encoder does not know the target language, and training on several datasets that include a large number of different language pairs. The model was trained on several open source datasets including WikiMatrix (Schwenk et al., 2019), Global Voices,6 EuroParl (Koehn, 2005) SETimes,7 and United Nations. After filtering, this resulted in approximately 100 million translation pairs and covering 39 languages. The model uses a shared, multilingual vocabulary of 64k SentencePiece tokens (Kudo and Richardson, 2018). 4.2 Baseline Model As a baseline, we train an English-only paraphraser in fairseq on the ParaBank 2 dataset (Hu et al., 2019c) with approximately 253M parameters and a SentencePiece vocabulary of 16k tokens. We train a Transformer with an 8-layer encoder, 8-layer decoder, 1024 dimensional embeddings, embedding sizes of 1024, feed-forward size of 4096, and"
2020.wmt-1.67,D18-2012,0,0.0291503,"der to be language agnostic, including specifying the target language as the first token in the target, so that the encoder does not know the target language, and training on several datasets that include a large number of different language pairs. The model was trained on several open source datasets including WikiMatrix (Schwenk et al., 2019), Global Voices,6 EuroParl (Koehn, 2005) SETimes,7 and United Nations. After filtering, this resulted in approximately 100 million translation pairs and covering 39 languages. The model uses a shared, multilingual vocabulary of 64k SentencePiece tokens (Kudo and Richardson, 2018). 4.2 Baseline Model As a baseline, we train an English-only paraphraser in fairseq on the ParaBank 2 dataset (Hu et al., 2019c) with approximately 253M parameters and a SentencePiece vocabulary of 16k tokens. We train a Transformer with an 8-layer encoder, 8-layer decoder, 1024 dimensional embeddings, embedding sizes of 1024, feed-forward size of 4096, and 16 attention heads. Dropout is set to 0.3, label smooth564 6 http://casmacat.eu/corpus/ global-voices.html 7 http://nlp.ffzg.hr/resources/corpora/ setimes/ Reference Among other things, the developments in terms of turnover, employment, war"
2020.wmt-1.67,D18-1421,0,0.0617633,"araphrase is to translate a text to a different language and then back again (Mallinson et al., 2017). Multiple pivot languages can be used to lessen the effect of inherent ambiguities (Aziz and Specia, 2013), at the expense of complication. Several works have focused on training on paraphrase data, including synthetic data created by starting with bitext and translating one side into the language of the other side to create synthetic paraphrases (Wieting et al., 2017; Wieting and Gimpel, 2018; Hu et al., 2019c). Ideas such as adversarial training (Iyyer et al., 2018), reinforcement learning (Li et al., 2018), and variational autoencoders (Gupta et al., 2018; Chen et al., 2019b) have also been explored in the context of paraphrase generation. Diversity in Generation Creating paraphrases which differ from their input in non-trivial ways is a challenging problem. Hu et al. (2019c) used constrained decoding (Hokamp and Liu, 2017) in conjunction with a set of constraints (e.g., avoiding certain words which are present in the input) when creating synthetic paraphrases from bitext. Kajiwara (2019) also used hard constraints, but at decoding time. Our work is similar but uses “soft” constraints (i.e., do"
2020.wmt-1.67,E17-1083,0,0.130974,"side of Czech–English bitext into English and pairing it with the original English. We find that our method outperforms this baseline—both in terms of semantic similarity and grammaticality—when our system is adjusted to match the lexical diversity of the baseline. We also present small scale evaluations that suggest our method is effective in other languages. 2 Related Work Paraphrase Generation Machine translation techniques can be used to train paraphrase models (Quirk et al., 2004). Another method to generate a paraphrase is to translate a text to a different language and then back again (Mallinson et al., 2017). Multiple pivot languages can be used to lessen the effect of inherent ambiguities (Aziz and Specia, 2013), at the expense of complication. Several works have focused on training on paraphrase data, including synthetic data created by starting with bitext and translating one side into the language of the other side to create synthetic paraphrases (Wieting et al., 2017; Wieting and Gimpel, 2018; Hu et al., 2019c). Ideas such as adversarial training (Iyyer et al., 2018), reinforcement learning (Li et al., 2018), and variational autoencoders (Gupta et al., 2018; Chen et al., 2019b) have also bee"
2020.wmt-1.67,J83-1001,0,0.758888,"h synthetic paraphrase database ParaBank 2 (Hu et al., 2019c) and find that our method produces paraphrases that better preserve meaning and are more gramatical, for the same level of lexical diversity. Additional smaller human assessments demonstrate our approach also works in two non-English languages. 1 Introduction Paraphrase generation is the task of producing a fluent output sentence which is semantically similar to the input sentence while being syntactically and/or lexically different from it (Bhagat and Hovy, 2013). Paraphrasing has been of longstanding interest in the NLP community (McKeown, 1983) and has been used for data augmentation in question answering (Dong et al., 2017; Gan and Ng, 2019), machine translation (MT) (Hu et al., 2019a; Khayrallah et al., 2020), task oriented dialog (Niu and Bansal, 2018, 2019), and MT metrics (Banerjee and Lavie, 2005; Zhou et al., 2006; Denkowski and Lavie, 2010; Thompson and Post, 2020). Thompson and Post (2020) recently released the Prism MT metric, which uses a multilingual neural MT (NMT) model as a paraphraser to score paraphrastic pairs; they treat paraphrasing as a zero-shot translation task (e.g., “translation” from English to English) and"
2020.wmt-1.67,K18-1047,0,0.0370598,"ional smaller human assessments demonstrate our approach also works in two non-English languages. 1 Introduction Paraphrase generation is the task of producing a fluent output sentence which is semantically similar to the input sentence while being syntactically and/or lexically different from it (Bhagat and Hovy, 2013). Paraphrasing has been of longstanding interest in the NLP community (McKeown, 1983) and has been used for data augmentation in question answering (Dong et al., 2017; Gan and Ng, 2019), machine translation (MT) (Hu et al., 2019a; Khayrallah et al., 2020), task oriented dialog (Niu and Bansal, 2018, 2019), and MT metrics (Banerjee and Lavie, 2005; Zhou et al., 2006; Denkowski and Lavie, 2010; Thompson and Post, 2020). Thompson and Post (2020) recently released the Prism MT metric, which uses a multilingual neural MT (NMT) model as a paraphraser to score paraphrastic pairs; they treat paraphrasing as a zero-shot translation task (e.g., “translation” from English to English) and force-decode and score MT system outputs conditioned on their respective human translations. They denote their paraphraser as lexically/syntactically unbiased as it does not prefer output that differs lexically or"
2020.wmt-1.67,D19-1132,0,0.11365,"Missing"
2020.wmt-1.67,N19-4009,0,0.0208789,"times words or phrases (e.g., proper nouns) that should not be paraphrased, as doing so would change the meaning of the sentence. Thus heuristics are often used to determine which words/phrases should be constrained. 5 In particular, the relative ranking judgements collected through 2016 (Bojar et al., 2016) are probably the most relevant. 4 4.1 Experimental Setup Primary Model We use the multilingual NMT model released with Prism (Thompson and Post, 2020), which uses a Transformer (Vaswani et al., 2017) architecture with approximately 750 million parameters. The model was trained in fairseq (Ott et al., 2019). The authors take several steps to encourage the encoder and decoder to be language agnostic, including specifying the target language as the first token in the target, so that the encoder does not know the target language, and training on several datasets that include a large number of different language pairs. The model was trained on several open source datasets including WikiMatrix (Schwenk et al., 2019), Global Voices,6 EuroParl (Koehn, 2005) SETimes,7 and United Nations. After filtering, this resulted in approximately 100 million translation pairs and covering 39 languages. The model us"
2020.wmt-1.67,P02-1040,0,0.111249,"ith the help of colleagues who are native speakers. We perform human evaluations following (Hu et al., 2019b), described in more detail below. 4.3.1 English Evaluation In this work, we focus on evaluation of semantic similarity, grammatical correctness, and lexical diversity. For the model trained on ParaBank 2, the trade-off between these dimensions is fixed and built into the model. To make a fair comparison, we adjust our overlap penalty (α) such that the output of our method matches the lexical diversity of the model trained on ParaBank 2. Following Hu et al. (2019c), we use uncased BLEU (Papineni et al., 2002), computed between input and output, to estimate the lexical diversity of the paraphraser. We evaluate in English using Mechanical Turk workers who were selected from a curated list of previously vetted workers. Annotators were presented with a reference sentence and four paraphrases: three paraphrases from our proposed method (at three different operating points) and one from the model trained on ParaBank 2, presented in random order. For each paraphrase, the annotators were asked to (1) rate the paraphrase as (i) grammatical, (ii) having one or two small grammatical errors, or (iii) ungramma"
2020.wmt-1.67,W19-5202,0,0.0401611,"Missing"
2020.wmt-1.67,N18-1119,1,0.779573,". 3.3 Diversity Control 2. By applying the lexical/syntactic bias in generation, development of the generation algorithm can be conducted without the time/cost of re-training a model, and multiple generation schemes can be directly compared using the same p(y |M(x)) model, such as the freely available Prism model (Thompson and Post, 2020). The α parameter in Equation 1 provides the user with a knob to control how strongly the output is “pushed” away from the input, in lexical space, during generation. In contrast to positive and negative hard lexical lexical constraints (Hokamp and Liu, 2017; Post and Vilar, 2018; Hu et al., 2019c), our method requires no user-defined constraints, making it simpler and perhaps more language agnostic.4 3.4 3. Being able to control the amount of lexical and/or syntactic diversity at inference time allows for easier comparison with prior paraphrasing work, as the diversity can be adjusted to match that of a prior method. (We employ this approach in §4.3.1.) Development and Evaluation Paraphrase evaluation is complicated by the fact that many different aspects of paraphrases can be evaluated including semantic similarity between input and output, fluency, grammatical corr"
2020.wmt-1.67,W04-3219,0,0.231691,"Missing"
2020.wmt-1.67,W19-4304,0,0.0393407,"Missing"
2020.wmt-1.67,2021.eacl-main.115,0,0.0727277,"Missing"
2020.wmt-1.67,W17-2619,0,0.019697,"f paraphrase quality, and Thompson and Post (2020) found that simply generating via beam search from a multilingual NMT model trained on a large general domain corpus results in trivial copies most of the time. We build upon Tiedemann and Scherrer (2019) by using a larger, general domain model, introducing a novel generation algorithm to produce output with lexical diversity, and performing human evaluations. Paraphrastic similarity Similarity between intermediate representations produced by multilingual NMT encoders has been used to measure semantic similarity and/or paraphrastic similarity (Schwenk and Douze, 2017; Wieting et al., 2019; Raganato et al., 2019). Similarly, Prism (Thompson and Post, 2020) use a multilingual NMT model as a lexically/syntactically unbiased paraphraser for scoring MT system outputs conditioned on their associated human reference translations. We build on this by introducing a lexical bias away from the input at generation time, enabling the use of a multilingual NMT model as a generative paraphraser. 562 3 Method Let x and y be sentences, let M(x) represent the meaning of x, and let S(x, y) measure the lexical and/or syntactic similarity between the two sentences. Formally,"
2020.wmt-1.67,P19-1177,0,0.116026,"non-trivial ways is a challenging problem. Hu et al. (2019c) used constrained decoding (Hokamp and Liu, 2017) in conjunction with a set of constraints (e.g., avoiding certain words which are present in the input) when creating synthetic paraphrases from bitext. Kajiwara (2019) also used hard constraints, but at decoding time. Our work is similar but uses “soft” constraints (i.e., down-weighting tokens which complete n-grams in the input, but not disallowing them all together). Another approach is to control generation with syntactic examples (Iyyer et al., 2018; Chen et al., 2019a) or codes (Shu et al., 2019). Multilingual NMT Multilingual NMT (Dong et al., 2015) has been shown to enable zero-shot translation—that is, translation between languages pairs not included in training (e.g., translating from Spanish→Arabic at test time when the model was trained on Spanish→English and English→Arabic, but not Spanish→Arabic) (Johnson et al., 2017; Gu et al., 2018; Pham et al., 2019). Zhou et al. (2019) also explored incorporating paraphrase data into training to improve multilingual NMT performance. Tiedemann and Scherrer (2019) explored using paraphrase recognition to test the semantic abstraction of a f"
2020.wmt-1.67,2020.emnlp-main.8,1,0.867442,"phrase generation is the task of producing a fluent output sentence which is semantically similar to the input sentence while being syntactically and/or lexically different from it (Bhagat and Hovy, 2013). Paraphrasing has been of longstanding interest in the NLP community (McKeown, 1983) and has been used for data augmentation in question answering (Dong et al., 2017; Gan and Ng, 2019), machine translation (MT) (Hu et al., 2019a; Khayrallah et al., 2020), task oriented dialog (Niu and Bansal, 2018, 2019), and MT metrics (Banerjee and Lavie, 2005; Zhou et al., 2006; Denkowski and Lavie, 2010; Thompson and Post, 2020). Thompson and Post (2020) recently released the Prism MT metric, which uses a multilingual neural MT (NMT) model as a paraphraser to score paraphrastic pairs; they treat paraphrasing as a zero-shot translation task (e.g., “translation” from English to English) and force-decode and score MT system outputs conditioned on their respective human translations. They denote their paraphraser as lexically/syntactically unbiased as it does not prefer output that differs lexically or syntactically from the input; this is advantageous for an MT metric as it assigns the highest score to an MT output whic"
2020.wmt-1.67,W19-2005,0,0.0186757,"rol generation with syntactic examples (Iyyer et al., 2018; Chen et al., 2019a) or codes (Shu et al., 2019). Multilingual NMT Multilingual NMT (Dong et al., 2015) has been shown to enable zero-shot translation—that is, translation between languages pairs not included in training (e.g., translating from Spanish→Arabic at test time when the model was trained on Spanish→English and English→Arabic, but not Spanish→Arabic) (Johnson et al., 2017; Gu et al., 2018; Pham et al., 2019). Zhou et al. (2019) also explored incorporating paraphrase data into training to improve multilingual NMT performance. Tiedemann and Scherrer (2019) explored using paraphrase recognition to test the semantic abstraction of a fairly small multilingual NMT system trained on Bibles and also demonstrate the model’s ability to paraphrase in English. However, they did not perform a human evaluation of paraphrase quality, and Thompson and Post (2020) found that simply generating via beam search from a multilingual NMT model trained on a large general domain corpus results in trivial copies most of the time. We build upon Tiedemann and Scherrer (2019) by using a larger, general domain model, introducing a novel generation algorithm to produce out"
2020.wmt-1.67,P18-1042,0,0.243435,"od to enable paraphrase generation from a multilingual NMT model.1 Our method discourages the model from producing n-grams that match n-grams in the input sentence. This serves to lexically bias the output away from the input sentence, resulting in nontrivial paraphrases. When considered together with Prism model of Thompson and Post (2020), our paraphrase generation approach offers several potential advantages over the common technique of training a paraphrase model on synthetic paraphrases generated by translating one side of bitext into the language of the other side (Wieting et al., 2017; Wieting and Gimpel, 2018; Hu et al., 2019c): • The fluency/semantic similarity vs lexical diversity trade-off can be controlled at generation time. • The approach works in many languages, with a single model. • The approach addresses an inherent shortcoming in creating synthetic paraphrases from bi1 We release our code at https://github.com/ thompsonb/prism 561 Proceedings of the 5th Conference on Machine Translation (WMT), pages 561–570 c Online, November 19–20, 2020. 2020 Association for Computational Linguistics text in which ambiguities in one language can create errorful synthetic paraphrases in the other (see §"
2020.wmt-1.67,P19-1453,0,0.063475,"Thompson and Post (2020) found that simply generating via beam search from a multilingual NMT model trained on a large general domain corpus results in trivial copies most of the time. We build upon Tiedemann and Scherrer (2019) by using a larger, general domain model, introducing a novel generation algorithm to produce output with lexical diversity, and performing human evaluations. Paraphrastic similarity Similarity between intermediate representations produced by multilingual NMT encoders has been used to measure semantic similarity and/or paraphrastic similarity (Schwenk and Douze, 2017; Wieting et al., 2019; Raganato et al., 2019). Similarly, Prism (Thompson and Post, 2020) use a multilingual NMT model as a lexically/syntactically unbiased paraphraser for scoring MT system outputs conditioned on their associated human reference translations. We build on this by introducing a lexical bias away from the input at generation time, enabling the use of a multilingual NMT model as a generative paraphraser. 562 3 Method Let x and y be sentences, let M(x) represent the meaning of x, and let S(x, y) measure the lexical and/or syntactic similarity between the two sentences. Formally, we can state the probl"
2020.wmt-1.67,D17-1026,0,0.280149,"ntroduce a simple method to enable paraphrase generation from a multilingual NMT model.1 Our method discourages the model from producing n-grams that match n-grams in the input sentence. This serves to lexically bias the output away from the input sentence, resulting in nontrivial paraphrases. When considered together with Prism model of Thompson and Post (2020), our paraphrase generation approach offers several potential advantages over the common technique of training a paraphrase model on synthetic paraphrases generated by translating one side of bitext into the language of the other side (Wieting et al., 2017; Wieting and Gimpel, 2018; Hu et al., 2019c): • The fluency/semantic similarity vs lexical diversity trade-off can be controlled at generation time. • The approach works in many languages, with a single model. • The approach addresses an inherent shortcoming in creating synthetic paraphrases from bi1 We release our code at https://github.com/ thompsonb/prism 561 Proceedings of the 5th Conference on Machine Translation (WMT), pages 561–570 c Online, November 19–20, 2020. 2020 Association for Computational Linguistics text in which ambiguities in one language can create errorful synthetic parap"
2020.wmt-1.67,N06-1057,0,0.0634559,"two non-English languages. 1 Introduction Paraphrase generation is the task of producing a fluent output sentence which is semantically similar to the input sentence while being syntactically and/or lexically different from it (Bhagat and Hovy, 2013). Paraphrasing has been of longstanding interest in the NLP community (McKeown, 1983) and has been used for data augmentation in question answering (Dong et al., 2017; Gan and Ng, 2019), machine translation (MT) (Hu et al., 2019a; Khayrallah et al., 2020), task oriented dialog (Niu and Bansal, 2018, 2019), and MT metrics (Banerjee and Lavie, 2005; Zhou et al., 2006; Denkowski and Lavie, 2010; Thompson and Post, 2020). Thompson and Post (2020) recently released the Prism MT metric, which uses a multilingual neural MT (NMT) model as a paraphraser to score paraphrastic pairs; they treat paraphrasing as a zero-shot translation task (e.g., “translation” from English to English) and force-decode and score MT system outputs conditioned on their respective human translations. They denote their paraphraser as lexically/syntactically unbiased as it does not prefer output that differs lexically or syntactically from the input; this is advantageous for an MT metric"
2020.wmt-1.67,P19-2015,0,0.0182955,"ghting tokens which complete n-grams in the input, but not disallowing them all together). Another approach is to control generation with syntactic examples (Iyyer et al., 2018; Chen et al., 2019a) or codes (Shu et al., 2019). Multilingual NMT Multilingual NMT (Dong et al., 2015) has been shown to enable zero-shot translation—that is, translation between languages pairs not included in training (e.g., translating from Spanish→Arabic at test time when the model was trained on Spanish→English and English→Arabic, but not Spanish→Arabic) (Johnson et al., 2017; Gu et al., 2018; Pham et al., 2019). Zhou et al. (2019) also explored incorporating paraphrase data into training to improve multilingual NMT performance. Tiedemann and Scherrer (2019) explored using paraphrase recognition to test the semantic abstraction of a fairly small multilingual NMT system trained on Bibles and also demonstrate the model’s ability to paraphrase in English. However, they did not perform a human evaluation of paraphrase quality, and Thompson and Post (2020) found that simply generating via beam search from a multilingual NMT model trained on a large general domain corpus results in trivial copies most of the time. We build up"
2020.wmt-1.98,W13-2305,0,0.173331,"ntegrating multiple references We augment each of the base metrics described in Section 2 to produce three new metrics: parB LEU, parCHR F++ and parE SIM. Both B LEU and CHR F++ have inbuilt support for multiple references. For E SIM, we calculate the score for each reference separately and then average them to get the final score. Metrics Task Setup Awaiting the gold judgments for WMT’20, we test and report the results of each method on the WMT19 metrics task.5 We follow the metrics task setup (Ma et al., 2019) by calculating the correlation with manual direct assessments (DA) of MT quality (Graham et al., 2013). System-level scores are evaluated using Pearson’s r and segment-level correlations using Kendall’s τ on the DA assessments converted into relative rankings. Statistically significant improvements (over the single-reference base metric) are marked in bold (with p ≤ 0.05). Significance is calculated using the Williams test (Williams, 1959) at the system level and bootstrap resampling at the segment level. 4 We pass the following arguments: fairseq-interactive ... --beam 100 --nbest 100 --diverse-beam-groups 10 --diverse-beam-strength 1 5 http://statmt.org/wmt19/results.html 889 • There is ofte"
2020.wmt-1.98,W19-5302,0,0.0809111,"aphrases are divere and generally of high quality. However, the later paraphrases may be noisier. Integrating multiple references We augment each of the base metrics described in Section 2 to produce three new metrics: parB LEU, parCHR F++ and parE SIM. Both B LEU and CHR F++ have inbuilt support for multiple references. For E SIM, we calculate the score for each reference separately and then average them to get the final score. Metrics Task Setup Awaiting the gold judgments for WMT’20, we test and report the results of each method on the WMT19 metrics task.5 We follow the metrics task setup (Ma et al., 2019) by calculating the correlation with manual direct assessments (DA) of MT quality (Graham et al., 2013). System-level scores are evaluated using Pearson’s r and segment-level correlations using Kendall’s τ on the DA assessments converted into relative rankings. Statistically significant improvements (over the single-reference base metric) are marked in bold (with p ≤ 0.05). Significance is calculated using the Williams test (Williams, 1959) at the system level and bootstrap resampling at the segment level. 4 We pass the following arguments: fairseq-interactive ... --beam 100 --nbest 100 --dive"
2020.wmt-1.98,W05-0909,0,0.374535,"of reference translations have been developed. One example is E SIM (Chen et al., 2017; Mathur et al., 2019), which uses contextual embeddings with the aim of creating an abstract meaning representation of the reference, with the potential of covering all translations with the correct meaning. We explore an alternative way of increasing the capacity of MT metrics to reward multiple valid translations: create additional references by automatically paraphrasing the original reference. There have been previous efforts to provide some sort of paraphrase support, mostly concentrating on synonyms (Banerjee and Lavie, 2005; Kauchak and Barzilay, 2006; Denkowski and Lavie, 2014). However, we base our work on a more recent attempt to improve B LEU using diverse automatic parahrasing with high quality MT-style sentential paraphrasing (Bawden et al., 2020). We put this to the test in the WMT’20 metrics shared task by applying Bawden et al.’s (2020) approach to three different metrics: B LEU, CHR F++ and E SIM. We compare the different metrics’ capacity to exploit automatically generated multiple references. We choose to use diverse paraphrases produced using P RISM (Thompson and Post, 2020a), since they are availab"
2020.wmt-1.98,P17-1152,0,0.262831,"and an automatic metric should be able to reward them all. Some of the most widely used MT metrics, including B LEU (Papineni et al., 2002) and CHR F++ (Popovi´c, 2015, 2017), rely on a surface-form comparison of MT outputs to a human-produced reference translation. Both metrics support the use of multiple references. However, even for metrics that support multiple references, human-produced references are expensive to produce and so are rarely available. To overcome this problem, metrics that do not rely on the surface form of reference translations have been developed. One example is E SIM (Chen et al., 2017; Mathur et al., 2019), which uses contextual embeddings with the aim of creating an abstract meaning representation of the reference, with the potential of covering all translations with the correct meaning. We explore an alternative way of increasing the capacity of MT metrics to reward multiple valid translations: create additional references by automatically paraphrasing the original reference. There have been previous efforts to provide some sort of paraphrase support, mostly concentrating on synonyms (Banerjee and Lavie, 2005; Kauchak and Barzilay, 2006; Denkowski and Lavie, 2014). Howev"
2020.wmt-1.98,W14-3348,0,0.04964,"ample is E SIM (Chen et al., 2017; Mathur et al., 2019), which uses contextual embeddings with the aim of creating an abstract meaning representation of the reference, with the potential of covering all translations with the correct meaning. We explore an alternative way of increasing the capacity of MT metrics to reward multiple valid translations: create additional references by automatically paraphrasing the original reference. There have been previous efforts to provide some sort of paraphrase support, mostly concentrating on synonyms (Banerjee and Lavie, 2005; Kauchak and Barzilay, 2006; Denkowski and Lavie, 2014). However, we base our work on a more recent attempt to improve B LEU using diverse automatic parahrasing with high quality MT-style sentential paraphrasing (Bawden et al., 2020). We put this to the test in the WMT’20 metrics shared task by applying Bawden et al.’s (2020) approach to three different metrics: B LEU, CHR F++ and E SIM. We compare the different metrics’ capacity to exploit automatically generated multiple references. We choose to use diverse paraphrases produced using P RISM (Thompson and Post, 2020a), since they are available in multiple languages, including most languages of th"
2020.wmt-1.98,N06-1058,0,0.0983504,"have been developed. One example is E SIM (Chen et al., 2017; Mathur et al., 2019), which uses contextual embeddings with the aim of creating an abstract meaning representation of the reference, with the potential of covering all translations with the correct meaning. We explore an alternative way of increasing the capacity of MT metrics to reward multiple valid translations: create additional references by automatically paraphrasing the original reference. There have been previous efforts to provide some sort of paraphrase support, mostly concentrating on synonyms (Banerjee and Lavie, 2005; Kauchak and Barzilay, 2006; Denkowski and Lavie, 2014). However, we base our work on a more recent attempt to improve B LEU using diverse automatic parahrasing with high quality MT-style sentential paraphrasing (Bawden et al., 2020). We put this to the test in the WMT’20 metrics shared task by applying Bawden et al.’s (2020) approach to three different metrics: B LEU, CHR F++ and E SIM. We compare the different metrics’ capacity to exploit automatically generated multiple references. We choose to use diverse paraphrases produced using P RISM (Thompson and Post, 2020a), since they are available in multiple languages, in"
2020.wmt-1.98,P19-1269,0,0.336405,"tric should be able to reward them all. Some of the most widely used MT metrics, including B LEU (Papineni et al., 2002) and CHR F++ (Popovi´c, 2015, 2017), rely on a surface-form comparison of MT outputs to a human-produced reference translation. Both metrics support the use of multiple references. However, even for metrics that support multiple references, human-produced references are expensive to produce and so are rarely available. To overcome this problem, metrics that do not rely on the surface form of reference translations have been developed. One example is E SIM (Chen et al., 2017; Mathur et al., 2019), which uses contextual embeddings with the aim of creating an abstract meaning representation of the reference, with the potential of covering all translations with the correct meaning. We explore an alternative way of increasing the capacity of MT metrics to reward multiple valid translations: create additional references by automatically paraphrasing the original reference. There have been previous efforts to provide some sort of paraphrase support, mostly concentrating on synonyms (Banerjee and Lavie, 2005; Kauchak and Barzilay, 2006; Denkowski and Lavie, 2014). However, we base our work o"
2020.wmt-1.98,2020.acl-main.448,0,0.152757,"results are given in Table 9. Somewhat surprisingly, even though they are not designed to be diverse, the n-best paraphrases give good correlations, at least up to 20 paraphrases, which was the maximum number tested with the sampled paraphraser. The sampled paraphrases also often perform better than the diverse approach produced by the P RISM paraphraser. One reason for this could be that the sampled paraphraser is trained specifically as an English paraphraser, whereas the P RISM paraphraser is multilingual (therefore providing greater support for automatic evaluation). Exclusion of outliers Mathur et al. (2020) suggested that system-level correlations computed with Pearson’s are artificially inflated due to the presence of outliers, which are typically very poorly performing systems with low human scores. They propose a method based on mean average deviation (MAD) to exclude those outliers. We applied this method to the WMT19 system-level data to exclude systems, and then recomputed the system-level correlations. The complete results are in Table 10. Comparing this to Table 7, we see an absolute drop in values, but little to nothing in the way of reversals between the B LEU (single-reference, zero-p"
2020.wmt-1.98,P02-1040,0,0.108693,"references. We find that gains are possible when using additional, automatically paraphrased references, although they are not systematic. However, segment-level correlations, particularly into English, are improved for all three metrics and even with higher numbers of paraphrased references. 1 Introduction One of the major challenges faced when automatically evaluating machine translation (MT) outputs is that there are almost always multiple correct translations of a sentence, and an automatic metric should be able to reward them all. Some of the most widely used MT metrics, including B LEU (Papineni et al., 2002) and CHR F++ (Popovi´c, 2015, 2017), rely on a surface-form comparison of MT outputs to a human-produced reference translation. Both metrics support the use of multiple references. However, even for metrics that support multiple references, human-produced references are expensive to produce and so are rarely available. To overcome this problem, metrics that do not rely on the surface form of reference translations have been developed. One example is E SIM (Chen et al., 2017; Mathur et al., 2019), which uses contextual embeddings with the aim of creating an abstract meaning representation of th"
2020.wmt-1.98,W15-3049,0,0.154816,"Missing"
2020.wmt-1.98,W17-4770,0,0.0751346,"Missing"
2020.wmt-1.98,W18-6319,1,0.819809,"pectively, H is the set of hypothesis translations, # (ngram) the number of times ngram appears in the hypothesis, and #clip (ngram) is the same but clipped to the maximum number of times it appears in any one reference (if several references are available). B LEU is typically used in its corpus-based variant, where a single score is produced for a test set. However, a segment-level variant also exists, where each sentence is scored individually. Smoothing is necessary in this segment-level variant to counteract the effect of 0 n-gram precision. We use the sacreB LEU implementation1 of B LEU (Post, 2018), with default tokenisation (and -tok zh tokenisation for Chinese) and exponential smoothing for the sentence-level variant. 2.2 E SIM (Chen et al., 2017; Mathur et al., 2019), is an embedding-based metric, which relies on neural models to handle inter-sentence semantic relatedness, going beyond surface-level matching (as in B LEU and CHR F++). E SIM was originally proposed to compare and match sentence pairs for natural language inference (Chen et al., 2017). Mathur et al. (2019) adapted it to evaluate MT performance by pairing the human reference and the MT output as E SIM input. Following ("
2020.wmt-1.98,2020.emnlp-main.8,1,0.881637,"ncentrating on synonyms (Banerjee and Lavie, 2005; Kauchak and Barzilay, 2006; Denkowski and Lavie, 2014). However, we base our work on a more recent attempt to improve B LEU using diverse automatic parahrasing with high quality MT-style sentential paraphrasing (Bawden et al., 2020). We put this to the test in the WMT’20 metrics shared task by applying Bawden et al.’s (2020) approach to three different metrics: B LEU, CHR F++ and E SIM. We compare the different metrics’ capacity to exploit automatically generated multiple references. We choose to use diverse paraphrases produced using P RISM (Thompson and Post, 2020a), since they are available in multiple languages, including most languages of the WMT shared task. We find that gains in correlation are possible, but this depends largely on the language direction and on whether the metric is system- or segment-level. The most positive gains are seen at the segment level, especially for into-English and even at higher numbers of additional paraphrases. This holds for all three metrics, despite E SIM relying on more abstract semantic representations. 2 Overview of Base Metrics In an extension of (Bawden et al., 2020), we augment three base metrics with autom"
2020.wmt-1.98,2020.wmt-1.67,1,0.87616,"ncentrating on synonyms (Banerjee and Lavie, 2005; Kauchak and Barzilay, 2006; Denkowski and Lavie, 2014). However, we base our work on a more recent attempt to improve B LEU using diverse automatic parahrasing with high quality MT-style sentential paraphrasing (Bawden et al., 2020). We put this to the test in the WMT’20 metrics shared task by applying Bawden et al.’s (2020) approach to three different metrics: B LEU, CHR F++ and E SIM. We compare the different metrics’ capacity to exploit automatically generated multiple references. We choose to use diverse paraphrases produced using P RISM (Thompson and Post, 2020a), since they are available in multiple languages, including most languages of the WMT shared task. We find that gains in correlation are possible, but this depends largely on the language direction and on whether the metric is system- or segment-level. The most positive gains are seen at the segment level, especially for into-English and even at higher numbers of additional paraphrases. This holds for all three metrics, despite E SIM relying on more abstract semantic representations. 2 Overview of Base Metrics In an extension of (Bawden et al., 2020), we augment three base metrics with autom"
2021.acl-long.309,J97-2002,0,0.783413,"mented sentences are required, they must be split using a sentence segmentation technique that can resolve these ambiguities. Despite its importance and early position in the NLP pipeline, sentence segmentation is the subject of relatively little research. Widely-used tools such as that in Moses (Koehn et al., 2007) are implemented with ad-hoc, manually-designed, language-specific rules, leaving them vulnerable to the long tail of languages and language phenomena. The little comparative work that does exist generally focuses on techniques that work in English or other Indo-European languages (Palmer and Hearst, 1997; Gillick, 2009). Secondly, there is not a well-understood methodology for training segmenters that do not make narrow assumptions about the features or characteristics of the languages they support. At the heart of this is the lack of labeled training data. Manually-split datasets that accompany annotation projects tend to be small, and larger datasets are typically (imperfectly) segmented by the very tools whose performance is under question. Tools such 3995 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on"
2021.acl-long.309,P04-3031,0,0.683053,"ssumptions about the features or characteristics of the languages they support. At the heart of this is the lack of labeled training data. Manually-split datasets that accompany annotation projects tend to be small, and larger datasets are typically (imperfectly) segmented by the very tools whose performance is under question. Tools such 3995 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 3995–4007 August 1–6, 2021. ©2021 Association for Computational Linguistics as NLTK (Bird and Loper, 2004), which packages Punkt (Kiss and Strunk, 2006), provide an unsupervised method to train a model, but it is unclear what the effect is when switching to non-Latin-script languages, or how a more supervised approach would handle such noisy data. Finally, and perhaps most importantly, there are no standard test sets or even metrics for evaluating segmenter performance, leaving researchers and engineers with no objective way to determine which one is best. The work described in this paper is aimed at these problems. We propose a simple window-based model and semi-supervised training paradigm for t"
2021.acl-long.309,2020.lrec-1.878,0,0.032989,"e a sentence boundary. This work expanded the most multilingually, considering 10 Indo-European languages as well as Estonian and Turkish. Other work has focused on specific non-English languages. Xue and Yang (2011) study Chinese and dissect the theoretical reasons behind segmenting Chinese sentences to match their English equivalents. To segment Thai, which lacks punctuation, Zhou et al. (2016) use POS-taggers. Some work has tackled the problem of domains. Sanchez (2019) approaches the problem of legal text, which has a set structure without punctuation; other approaches (Wang et al., 2019; Rehbein et al., 2020) have investigated speech, which lacks both punctuation and written textual structure. A popular splitter is packaged in the Moses toolkit (Koehn et al., 2007),3 which works by splitting on all sentence-final punctuation unless the preceding context is a “non-breaking prefix”—a hand-built, language-specific list of acronyms and abbreviations. This approach cannot resolve the ambiguity where punctuation legitimately exists at the end of a sentence and is indifferent to novel 2 1 https://github.com/rewicks/ersatz pip install ersatz. or Alternately called sentence boundary detection. We use the r"
2021.emnlp-main.539,2020.acl-main.747,0,0.030344,"975). The state-of-the-art approach to this task is based on the Predictor-Estimator (PredEst) architecture (Kim et al., 2017; Kepler et al., 2019). At a very high level, the predictor training uses a crosslingual masked language model (MLM) objective, which trains the model to predict a word in the target sentence given the source and both the left and right context on the target side. An estimator is then finetuned from the predictor model to predict word-level QE tags. In recent years, the top-ranking systems also incorporate large-scale pre-trained crosslingual encoder such as XLMRoBERTa (Conneau et al., 2020), glass-box features (Moura et al., 2020) and pseudo post-editing data augmentation (Wei et al., 2020; Lee, 2020). The Levenshtein Transformer (LevT, Gu et al., 2019) is a neural network architecture that can iteratively generate sequences in a non-autoregressive manner. Unlike normal autoregressive sequence models that have only one prediction head Aw to predict the next output words, LevT has two extra prediction heads Adel and Ains that predicts deletion and insertion operations based on the output sequence from the previous iteration. For translation generation, at the k-th iteration durin"
2021.emnlp-main.539,2020.wmt-1.117,0,0.0681255,"Missing"
2021.emnlp-main.576,P18-1007,0,0.0665835,"Missing"
2021.emnlp-main.576,N19-4009,0,0.0287458,"kin to the Vision Transformer (Dosovitskiy et al., 2021) from image classification where attentional layers are applied directly to image slices1 after a flattening linear transformation. With c = 7, we compare the depth of VistaOCR (Rawls et al., 2017), a competitive OCR model, but without its additional color channels and subsequent maxpooling.2 After replacing text embeddings with visual rep resentations, the standard MT architecture remains the same. The full model is trained endtoend with the typical crossentropy objective. All models are trained using a modified version of fairseq (Ott et al., 2019), which we release with the paper.3 The slices output from the rendering stage are analogous to subword text tokens. The next step produces “embeddings” from these slices. Em beddings typically refer to entries in a fixed size weight matrix, with the vocabulary ID as an index. Our image slices are not drawn from a predeter mined set, so we cannot work with normal embed dings. Instead, we use the outputs of 2D convolu tional blocks run over the image slices, projected to the model hidden size, as a continuous vocabulary. While OCR models for tasks such as handwrit 3 Experimental Setup ing"
2021.emnlp-main.576,2020.acl-main.170,0,0.250254,"Missing"
2021.emnlp-main.576,2020.emnlp-main.213,0,0.0267648,"o б _э т o й _т e x н o л o ги и . I’m going to put my mouth in the dam of ecsta chhallogi. 1.236 Table 6: Examples of data with induced noise, and the resulting inputs and outputs for text and visual text models. One example is shown for each type of tested noise: unicode, diacritics, l33tspeak, swap, and cambridge. For each example, we show the original source sentence (src); noise induced with probability p; the reference translation (ref); rendered text with sliding windows (invis ); visual text model output (outvis ); BPE’d text input (intext ); text model output (outtext ); and COMET (Rei et al., 2020) scores computed using the default model (wmt largedaestimator1719). 7240 French German 7 Figure 3: Different unicode codepoints may appear vi sually similar. In this English example from WIPO (JunczysDowmunt et al., 2016), all characters in red are not from the Roman alphabet but Cyrillic. We induce noise in the form of Latin characters which are visually similar to Cyrillic characters for Russian (unicode), diacritization for Arabic (diacritics), and l33tspeak for French and Ger man (l33tspeak). We use CAMeL Tools (Obeid et al., 2020) for Arabic diacritization. Figure 4 shows that the"
2021.emnlp-main.576,P11-2093,0,0.00887456,"subword vocabulary for each lan guage pair and dataset. We saw no difference be tween joint/disjoint vocabularies, so use separate vocabularies to create a direct comparison with the visual text models: the same target vocabulary is used for both and only the source representations are varied. We tuned ∼5k BPE intervals from 2.5k–35k5 to optimize source language BPE gran ularity with the target vocabulary held constant at 10k BPE. We additionally compare characterlevel and wordlevel models; to produce wordlevel seg mentations for Chinese, we use jieba,6 and for Japanese, we use kytea (Neubig et al., 2011). The character vocabulary for Chinese is greater than 2.5k so we do not have a BPE model of this size. Our best performing BPE models used source vo cabularies of approximately 5k (see Figure 2). We jointly tuned batch size and subword vo 5 4 https://www.ted.com 18.3 17.7 17.2 17.4 17.2 17.5 17.2 0.5 6 For the MTTT datasets, ∼40k BPE recovers words. https://github.com/fxsjy/jieba 7237 Text Lang BPE ar de fr ja ko ru zh Visual text Text char s = 5 s = 10 s = 15 s = 20 24.4 78.9 97.1 32.3 104.3 130.5 28.8 107.6 130.2 22.5 36.9 95.5 24.7 50.8 97.0 27.1 94.7 132.7 23.0 29.8 75.6 Time 1.0× 2.3×"
2021.emnlp-main.576,2020.acl-main.737,0,0.024741,"(2019) used hybrid con many kinds of induced noise, including the sub volutional networks to improve several NLP tasks stitution of visually similar characters and charac for Chinese, using historical scripts for additional ter permutations. An important benefit of our ap pictographic information. Sun et al. (2018, 2019) proach is that it operates on raw text, doing away created dense fixedsize square text renderings in with the standard preprocessing routines that in Chinese and English for convolutions for down clude normalization, tokenization, and subword stream sentiment analysis. Ryskina et al. (2020) segmentation. used visual similarity for Russian romanization. We believe our approach opens many avenues In machine translation, visual information was for future work. Standard data techniques from also first used for Chinese. Initial work improved OCR (such as varied font and font size) and train translation models by initializing character em ing on noise would likely further improve robust beddings with linearized bitmaps of each character ness. There are many possible visual architec (Aldón Mínguez et al., 2016; Costajussà et al., tures, and visual pretraining has benefited vision"
2021.emnlp-main.576,W18-6302,0,0.0616016,"Missing"
2021.emnlp-main.576,W18-6245,0,0.0273724,"seven language pairs and in two data settings, approach or match the per convolutions over characterlevel images for Chi formance of traditional text models. Further, we nese for downstream language modeling and word showed that visual text models are more robust to segmentation. Meng et al. (2019) used hybrid con many kinds of induced noise, including the sub volutional networks to improve several NLP tasks stitution of visually similar characters and charac for Chinese, using historical scripts for additional ter permutations. An important benefit of our ap pictographic information. Sun et al. (2018, 2019) proach is that it operates on raw text, doing away created dense fixedsize square text renderings in with the standard preprocessing routines that in Chinese and English for convolutions for down clude normalization, tokenization, and subword stream sentiment analysis. Ryskina et al. (2020) segmentation. used visual similarity for Russian romanization. We believe our approach opens many avenues In machine translation, visual information was for future work. Standard data techniques from also first used for Chinese. Initial work improved OCR (such as varied font and font size) and tr"
D19-1084,Q16-1022,0,0.0583752,"Missing"
D19-1084,W18-6318,0,0.118821,"attention weights towards alignments. For example, Chen et al. (2016) introduce a loss over the attention weight that penalizes the model when the attention deviates from alignments; this yields only minor BLEU score improvements.1 Neural Alignment Models Legrand et al. (2016) develop a neural alignment model that uses a convolutional encoder for the source and target sequences and a negative-sampling-based objective. Tamura et al. (2014) introduce a supervised RNNbased aligner which conditions each alignment decision on the source and target sequences as well as previous alignment decisions. Alkhouli et al. (2018) extend the Transformer architecture for alignment by adding an additional alignment head to the multi-head encoder-decoder attention, biasing the attention weights to correspond with alignments. Similarly, Zenkel et al. (2019) introduce a single-layer attention module (akin to the standard Transformer decoder) which predicts the target word conditioned on the encoder and decoder states, following the intuition that a source word should be aligned to a target word if it is highly predictive of the target word. Unlike our model, all of these models are asymmetrical and are trained without expli"
D19-1084,W16-2206,0,0.0816841,"wever, despite their intuitive analogy to word alignments, attention-based alignments typically do not correspond very closely to gold-standard human annotations and suffer from systematic errors (Koehn and Knowles, 2017; Ghader and Monz, 2017), falling short of the promise to jointly learn to align and translate. This problem has only been exacerbated by the introduction of deeper models containing multiple attention layers and heads (e.g. Vaswani et al. (2017)). There have been several attempts to mitigate this discrepancy, mostly with a view to improving the quality of output translations (Alkhouli et al., 2016, 2018; Chen et al., 2016; Liu et al., 2016) – though some work has focused specifically on alignment (Legrand et al., 2016; Zenkel et al., 2019). We confirm that the poor performance of encoder-decoder attention for Recurrent Neural Networks (RNNs) found by Koehn and Knowles (2017) and Ghader and Monz (2017) can also be observed in the self-attentional Transformer model (Vaswani et al., 2017), further motivating an 910 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 910–920,"
D19-1084,W17-4711,0,0.0634044,"n to the standard Transformer decoder) which predicts the target word conditioned on the encoder and decoder states, following the intuition that a source word should be aligned to a target word if it is highly predictive of the target word. Unlike our model, all of these models are asymmetrical and are trained without explicit alignment data. Following the generative formulation of the alignment problem, Alkhouli et al. (2016) present neural lexical translation and alignment models, which they train using silver-standard alignments obtained from GIZA++. A similar training strategy is used in Alkhouli and Ney (2017), who bias an RNN attention module with silver-standard alignments for use as a lexical model. In the same vein, Peter et al. (2017) improve the attention module by allowing it to peek at the next target word. Note that unlike our framework, none of the models mentioned make use of existing gold-standard labeled data. Following Legrand et al. (2016) and Zenkel et al. (2019) we focus exclusively on alignment quality. A similar approach is taken 911 1 This may also be a result of noisy training data, as they use unsupervised alignments as training data for their models. by Ouyang and McKeown (20"
D19-1084,D16-1162,0,0.0358275,"and attention-based alignments. FastAlign is a fast loglinear reparameterization of IBM Model 2 (Brown et al., 1993). Unlike in the RNN case, where a single attention layer intervenes between the encoder and decoder, Transformers make use of multi-head and multi-layer attention. Our attentional alignments are obtained by averaging across all heads of the final multi-head encoder-decoder attention layer. Both of these methods allow for fast online decoding, making them comparable to our model, and both have been used for word alignment; moreover, FastAlign is a standard choice for projection (Arthur et al., 2016; Fu et al., 2014; Eger et al., 2018; Agi´c et al., 2016; Sharoff, 2018). 912 Our model extends the Sockeye implementation of the Transformer model (Hieber et al., 2017).3 Following Dyer et al. (2013), FastAlign hyperparameters were tuned by training on 10K training examples, evaluating on a 100-sentence subset of the validation split. The grow-diag-final-and heuristic was used for symmetrization. Thresholds for both attention alignment and our model were tuned on the same 100-sentence validation subset. 4.2 Data Chinese character-tokenized bitext and alignments were sourced from the GALE Chin"
D19-1084,J93-2003,0,0.104894,"an be observed for Chinese, and that the amount of annotated alignment data has a far greater impact on alignment score than the amount of unlabelled bitext used for pre-training. Furthermore, our NER projection trials demonstrate a major downstream improvement when using our alignments over FastAlign. Taken together, these results motivate the further annotation of small amounts of quality alignment data in a variety of languages. We demonstrate that such annotation can be performed rapidly by untrained L2 (second language) speakers. 2 Related work Statistical Alignment Generative alignment (Brown et al., 1993) models the posterior over a target sequence t given a source sequence s as: XY p(t|s) = p(ti |ai , t<i , s) p(ai |a<i , t<i , s) {z }| {z } | a i lexical model alignment model Such models are asymmetrical: they are learned in both directions and then heuristically combined. Discriminative alignment models, on the other hand, directly model p(a|s, t), usually by extracting features from the source and target sequences and training a supervised classifier using labelled data. A comprehensive account of methods and features for discriminative alignment can be found in Tomeh (2012). Neural Machin"
D19-1084,2016.amta-researchers.10,0,0.263412,"e analogy to word alignments, attention-based alignments typically do not correspond very closely to gold-standard human annotations and suffer from systematic errors (Koehn and Knowles, 2017; Ghader and Monz, 2017), falling short of the promise to jointly learn to align and translate. This problem has only been exacerbated by the introduction of deeper models containing multiple attention layers and heads (e.g. Vaswani et al. (2017)). There have been several attempts to mitigate this discrepancy, mostly with a view to improving the quality of output translations (Alkhouli et al., 2016, 2018; Chen et al., 2016; Liu et al., 2016) – though some work has focused specifically on alignment (Legrand et al., 2016; Zenkel et al., 2019). We confirm that the poor performance of encoder-decoder attention for Recurrent Neural Networks (RNNs) found by Koehn and Knowles (2017) and Ghader and Monz (2017) can also be observed in the self-attentional Transformer model (Vaswani et al., 2017), further motivating an 910 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 910–920, c Hong Kong, China, Nove"
D19-1084,N13-1073,0,0.177245,", t) where a ˆij = 1 if source word i is aligned to target word j in the gold-standard data, and 0 otherwise. Note that the convolution step ensures these are not independent alignment decisions. When training our model, we begin by pretraining an MT model on unlabelled bitext; the weights of this model are used to initialize the encoder and decoder in the alignment model. To obtain an alignment at test time, the source and target word sequences are encoded and presented to the aligner. 4 4.1 Alignment Experiments Baselines We compare against two baselines: alignments obtained from FastAlign (Dyer et al., 2013), and attention-based alignments. FastAlign is a fast loglinear reparameterization of IBM Model 2 (Brown et al., 1993). Unlike in the RNN case, where a single attention layer intervenes between the encoder and decoder, Transformers make use of multi-head and multi-layer attention. Our attentional alignments are obtained by averaging across all heads of the final multi-head encoder-decoder attention layer. Both of these methods allow for fast online decoding, making them comparable to our model, and both have been used for word alignment; moreover, FastAlign is a standard choice for projection"
D19-1084,C18-1071,0,0.012975,"lign is a fast loglinear reparameterization of IBM Model 2 (Brown et al., 1993). Unlike in the RNN case, where a single attention layer intervenes between the encoder and decoder, Transformers make use of multi-head and multi-layer attention. Our attentional alignments are obtained by averaging across all heads of the final multi-head encoder-decoder attention layer. Both of these methods allow for fast online decoding, making them comparable to our model, and both have been used for word alignment; moreover, FastAlign is a standard choice for projection (Arthur et al., 2016; Fu et al., 2014; Eger et al., 2018; Agi´c et al., 2016; Sharoff, 2018). 912 Our model extends the Sockeye implementation of the Transformer model (Hieber et al., 2017).3 Following Dyer et al. (2013), FastAlign hyperparameters were tuned by training on 10K training examples, evaluating on a 100-sentence subset of the validation split. The grow-diag-final-and heuristic was used for symmetrization. Thresholds for both attention alignment and our model were tuned on the same 100-sentence validation subset. 4.2 Data Chinese character-tokenized bitext and alignments were sourced from the GALE Chinese-English Parallel Aligned Treeban"
D19-1084,J07-3002,0,0.14783,"nkel et al. (2019), to reduce BPE alignments to regular alignments, we considered a source and target word to be aligned if any of their subwords were aligned.4 This heuristic was used to evaluate all BPE models. With the task of projection for NER data in mind (Yarowsky et al., 2001), we evaluate all models on Chinese NER spans. These spans were obtained from the OntoNotes corpus, which subsumes the GALE Chinese corpus. Due to formatting differences, only a subset of the GALE sentences (287 validation, 189 test) were recoverable from OntoNotes. We evaluate our models using macro-F1 score, as Fraser and Marcu (2007) showed that alignment error rate does not match human judgments. F1 scores were obtained using the macro-F1 scorer provided by FastAlign. 3 We pre-train a 6-layer model with the Adam optimizer (Kingma and Ba, 2014) using a learning rate of 0.0002, 8headed multi-head attention, 2048-dimensional feed-forward layers, and 512-dimensional encoder/decoder output layers. We include weight-tying (Press and Wolf, 2017) between the source and target embedding layers. All other MT hyperparameters were set to the Sockeye defaults. 913 4.3 Results Our model outperforms both baselines for all languages and"
D19-1084,I17-1004,0,0.18185,"allows researchers to address a lack of annotated multilingual resources and changing ontologies with minimal annotation effort. The introduction of attention (Bahdanau et al., 2014) has allowed NMT to advance past decoding from a single sentence-level vector representation and has been crucial in supporting fluent, meaningful translations, especially of longer sentences. However, despite their intuitive analogy to word alignments, attention-based alignments typically do not correspond very closely to gold-standard human annotations and suffer from systematic errors (Koehn and Knowles, 2017; Ghader and Monz, 2017), falling short of the promise to jointly learn to align and translate. This problem has only been exacerbated by the introduction of deeper models containing multiple attention layers and heads (e.g. Vaswani et al. (2017)). There have been several attempts to mitigate this discrepancy, mostly with a view to improving the quality of output translations (Alkhouli et al., 2016, 2018; Chen et al., 2016; Liu et al., 2016) – though some work has focused specifically on alignment (Legrand et al., 2016; Zenkel et al., 2019). We confirm that the poor performance of encoder-decoder attention for Recurr"
D19-1084,E17-3017,0,0.0241198,"layer intervenes between the encoder and decoder, Transformers make use of multi-head and multi-layer attention. Our attentional alignments are obtained by averaging across all heads of the final multi-head encoder-decoder attention layer. Both of these methods allow for fast online decoding, making them comparable to our model, and both have been used for word alignment; moreover, FastAlign is a standard choice for projection (Arthur et al., 2016; Fu et al., 2014; Eger et al., 2018; Agi´c et al., 2016; Sharoff, 2018). 912 Our model extends the Sockeye implementation of the Transformer model (Hieber et al., 2017).3 Following Dyer et al. (2013), FastAlign hyperparameters were tuned by training on 10K training examples, evaluating on a 100-sentence subset of the validation split. The grow-diag-final-and heuristic was used for symmetrization. Thresholds for both attention alignment and our model were tuned on the same 100-sentence validation subset. 4.2 Data Chinese character-tokenized bitext and alignments were sourced from the GALE Chinese-English Parallel Aligned Treebank (Li et al., 2015); Arabic data was obtained from the GALE Arabic-English Parallel Aligned Treebank (Li et al., 2013). Both GALE dat"
D19-1084,D13-1176,0,0.0175993,"national Joint Conference on Natural Language Processing, pages 910–920, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics approach focusing explicitly on alignment quality. We introduce a novel alignment module that learns to produce high-quality alignments after training on 1.7K to 4.9K human-annotated alignments for Arabic and Chinese, respectively. While our module is integrated into the state-of-the-art Transformer model (Vaswani et al., 2017), the implementation is architecture-neutral, and can therefore be applied to RNN-based models (Schwenk, 2012; Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014) or fully convolutional models (Gehring et al., 2017). Our experiments on English-Chinese and English-Arabic bitext show that our model yields major improvements of 11 and 27 alignment F1 points (respectively) over baseline models; ablation experiments indicate with only half the data minor improvements can be observed for Chinese, and that the amount of annotated alignment data has a far greater impact on alignment score than the amount of unlabelled bitext used for pre-training. Furthermore, our NER projection trials demonstrate a major downstream improvement when us"
D19-1084,P07-2045,0,0.00724692,"Missing"
D19-1084,W17-3204,0,0.109247,"Hwa, 2005). This paradigm allows researchers to address a lack of annotated multilingual resources and changing ontologies with minimal annotation effort. The introduction of attention (Bahdanau et al., 2014) has allowed NMT to advance past decoding from a single sentence-level vector representation and has been crucial in supporting fluent, meaningful translations, especially of longer sentences. However, despite their intuitive analogy to word alignments, attention-based alignments typically do not correspond very closely to gold-standard human annotations and suffer from systematic errors (Koehn and Knowles, 2017; Ghader and Monz, 2017), falling short of the promise to jointly learn to align and translate. This problem has only been exacerbated by the introduction of deeper models containing multiple attention layers and heads (e.g. Vaswani et al. (2017)). There have been several attempts to mitigate this discrepancy, mostly with a view to improving the quality of output translations (Alkhouli et al., 2016, 2018; Chen et al., 2016; Liu et al., 2016) – though some work has focused specifically on alignment (Legrand et al., 2016; Zenkel et al., 2019). We confirm that the poor performance of encoder-deco"
D19-1084,E17-2025,0,0.0146817,"corpus. Due to formatting differences, only a subset of the GALE sentences (287 validation, 189 test) were recoverable from OntoNotes. We evaluate our models using macro-F1 score, as Fraser and Marcu (2007) showed that alignment error rate does not match human judgments. F1 scores were obtained using the macro-F1 scorer provided by FastAlign. 3 We pre-train a 6-layer model with the Adam optimizer (Kingma and Ba, 2014) using a learning rate of 0.0002, 8headed multi-head attention, 2048-dimensional feed-forward layers, and 512-dimensional encoder/decoder output layers. We include weight-tying (Press and Wolf, 2017) between the source and target embedding layers. All other MT hyperparameters were set to the Sockeye defaults. 913 4.3 Results Our model outperforms both baselines for all languages and experimental conditions. Table 2 shows that our model performs especially well in the NER setting, where we observe the largest improve4 Note that this step is necessary in order to make the produced and reference alignments comparable; to evaluate alignments, the sequences must be identical, or the evaluation is ill-posed. Figure 2: (a) Dev. F1 as function of training corpus size (log scale), contrast against"
D19-1084,C02-1070,0,0.44148,"nstream tasks, such as transferring input formatting, incorporating lexica, and humanin-the-loop translation. Crucially, they are central to cross-lingual dataset creation via projection (Yarowsky et al., 2001), where token-level annotations in a high-resource language are projected across alignments to a low-resource language; using projection, datasets have been created for a variety of natural language processing tasks, including named-entity recognition (NER), part-of-speech tagging, parsing, information extraction (IE), and semantic role labeling (Yarowsky et al., 2001; Hwa et al., 2005; Riloff et al., 2002; Pad´o and Lapata, 2009; Xi and Hwa, 2005). This paradigm allows researchers to address a lack of annotated multilingual resources and changing ontologies with minimal annotation effort. The introduction of attention (Bahdanau et al., 2014) has allowed NMT to advance past decoding from a single sentence-level vector representation and has been crucial in supporting fluent, meaningful translations, especially of longer sentences. However, despite their intuitive analogy to word alignments, attention-based alignments typically do not correspond very closely to gold-standard human annotations an"
D19-1084,C12-2104,0,0.0124335,"d the 9th International Joint Conference on Natural Language Processing, pages 910–920, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics approach focusing explicitly on alignment quality. We introduce a novel alignment module that learns to produce high-quality alignments after training on 1.7K to 4.9K human-annotated alignments for Arabic and Chinese, respectively. While our module is integrated into the state-of-the-art Transformer model (Vaswani et al., 2017), the implementation is architecture-neutral, and can therefore be applied to RNN-based models (Schwenk, 2012; Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014) or fully convolutional models (Gehring et al., 2017). Our experiments on English-Chinese and English-Arabic bitext show that our model yields major improvements of 11 and 27 alignment F1 points (respectively) over baseline models; ablation experiments indicate with only half the data minor improvements can be observed for Chinese, and that the amount of annotated alignment data has a far greater impact on alignment score than the amount of unlabelled bitext used for pre-training. Furthermore, our NER projection trials demonstrate a majo"
D19-1084,P16-1162,0,0.162944,"otated alignment sentence pairs. et al., 2007). Chinese data was drawn from the WMT 2017 En-Zh bitext, while the Arabic data was sourced from local resources; both Chinese and Arabic models were trained with a 60K-word vocabulary. For FastAlign, the GALE train split was concatenated to the pretraining bitext, while for both the discriminative and attentional models the GALE data was used to finetune a model pre-trained on bitext alone. Contemporary MT systems use subword units to tackle the problem of out-of-vocabulary lowfrequency words. This is often implemented by byte-pair encoding (BPE) (Sennrich et al., 2016). We applied a BPE model with 30K merge operations to all of our data for the BPE experimental condition. Training alignments were expanded by aligning all subwords of a given source word to all the subwords of the target word to which the source word aligns. At test time, following Zenkel et al. (2019), to reduce BPE alignments to regular alignments, we considered a source and target word to be aligned if any of their subwords were aligned.4 This heuristic was used to evaluate all BPE models. With the task of projection for NER data in mind (Yarowsky et al., 2001), we evaluate all models on C"
D19-1084,W16-2207,0,0.183026,"Missing"
D19-1084,L18-1135,0,0.0134943,"tion of IBM Model 2 (Brown et al., 1993). Unlike in the RNN case, where a single attention layer intervenes between the encoder and decoder, Transformers make use of multi-head and multi-layer attention. Our attentional alignments are obtained by averaging across all heads of the final multi-head encoder-decoder attention layer. Both of these methods allow for fast online decoding, making them comparable to our model, and both have been used for word alignment; moreover, FastAlign is a standard choice for projection (Arthur et al., 2016; Fu et al., 2014; Eger et al., 2018; Agi´c et al., 2016; Sharoff, 2018). 912 Our model extends the Sockeye implementation of the Transformer model (Hieber et al., 2017).3 Following Dyer et al. (2013), FastAlign hyperparameters were tuned by training on 10K training examples, evaluating on a 100-sentence subset of the validation split. The grow-diag-final-and heuristic was used for symmetrization. Thresholds for both attention alignment and our model were tuned on the same 100-sentence validation subset. 4.2 Data Chinese character-tokenized bitext and alignments were sourced from the GALE Chinese-English Parallel Aligned Treebank (Li et al., 2015); Arabic data was"
D19-1084,P14-1138,0,0.311652,"and Monz (2017), who provide a detailed analysis of where attention and alignment diverge, offering possible explanations for the modest results obtained when attempting to bias attention weights towards alignments. For example, Chen et al. (2016) introduce a loss over the attention weight that penalizes the model when the attention deviates from alignments; this yields only minor BLEU score improvements.1 Neural Alignment Models Legrand et al. (2016) develop a neural alignment model that uses a convolutional encoder for the source and target sequences and a negative-sampling-based objective. Tamura et al. (2014) introduce a supervised RNNbased aligner which conditions each alignment decision on the source and target sequences as well as previous alignment decisions. Alkhouli et al. (2018) extend the Transformer architecture for alignment by adding an additional alignment head to the multi-head encoder-decoder attention, biasing the attention weights to correspond with alignments. Similarly, Zenkel et al. (2019) introduce a single-layer attention module (akin to the standard Transformer decoder) which predicts the target word conditioned on the encoder and decoder states, following the intuition that"
D19-1084,C16-1291,0,0.0356299,"lignments, attention-based alignments typically do not correspond very closely to gold-standard human annotations and suffer from systematic errors (Koehn and Knowles, 2017; Ghader and Monz, 2017), falling short of the promise to jointly learn to align and translate. This problem has only been exacerbated by the introduction of deeper models containing multiple attention layers and heads (e.g. Vaswani et al. (2017)). There have been several attempts to mitigate this discrepancy, mostly with a view to improving the quality of output translations (Alkhouli et al., 2016, 2018; Chen et al., 2016; Liu et al., 2016) – though some work has focused specifically on alignment (Legrand et al., 2016; Zenkel et al., 2019). We confirm that the poor performance of encoder-decoder attention for Recurrent Neural Networks (RNNs) found by Koehn and Knowles (2017) and Ghader and Monz (2017) can also be observed in the self-attentional Transformer model (Vaswani et al., 2017), further motivating an 910 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 910–920, c Hong Kong, China, November 3–7, 2019. 201"
D19-1084,P19-1467,0,0.0985843,"lkhouli and Ney (2017), who bias an RNN attention module with silver-standard alignments for use as a lexical model. In the same vein, Peter et al. (2017) improve the attention module by allowing it to peek at the next target word. Note that unlike our framework, none of the models mentioned make use of existing gold-standard labeled data. Following Legrand et al. (2016) and Zenkel et al. (2019) we focus exclusively on alignment quality. A similar approach is taken 911 1 This may also be a result of noisy training data, as they use unsupervised alignments as training data for their models. by Ouyang and McKeown (2019), who introduce a pointer network-based model for monolingual phrase alignment which allows for the alignment of variable-length phrases. 3 Model NMT systems typically use an encoder-decoder architecture, where one neural network (the encoder) learns to provide another network (the decoder) with a continuous representation of the input sequence of words in the source language, s1 , . . . , sN , from which the decoder can accurately predict the target sequence t1 , . . . , tM . The intermediate representations of the encoder and decoder are known as hidden states. After training, the hidden sta"
D19-1084,H05-1107,0,0.0475175,"matting, incorporating lexica, and humanin-the-loop translation. Crucially, they are central to cross-lingual dataset creation via projection (Yarowsky et al., 2001), where token-level annotations in a high-resource language are projected across alignments to a low-resource language; using projection, datasets have been created for a variety of natural language processing tasks, including named-entity recognition (NER), part-of-speech tagging, parsing, information extraction (IE), and semantic role labeling (Yarowsky et al., 2001; Hwa et al., 2005; Riloff et al., 2002; Pad´o and Lapata, 2009; Xi and Hwa, 2005). This paradigm allows researchers to address a lack of annotated multilingual resources and changing ontologies with minimal annotation effort. The introduction of attention (Bahdanau et al., 2014) has allowed NMT to advance past decoding from a single sentence-level vector representation and has been crucial in supporting fluent, meaningful translations, especially of longer sentences. However, despite their intuitive analogy to word alignments, attention-based alignments typically do not correspond very closely to gold-standard human annotations and suffer from systematic errors (Koehn and"
D19-1084,H01-1035,0,0.863526,"slation (MT) have set new standards for performance, especially when large amounts of parallel text (bitext) are available. However, explicit word-to-word alignments, which were foundational to pre-neural statistical MT (SMT) (Brown et al., 1993), have largely been lost in neural MT (NMT) models. This is unfortunate: while alignments are not necessary for NMT systems, they have a wealth of applications in downstream tasks, such as transferring input formatting, incorporating lexica, and humanin-the-loop translation. Crucially, they are central to cross-lingual dataset creation via projection (Yarowsky et al., 2001), where token-level annotations in a high-resource language are projected across alignments to a low-resource language; using projection, datasets have been created for a variety of natural language processing tasks, including named-entity recognition (NER), part-of-speech tagging, parsing, information extraction (IE), and semantic role labeling (Yarowsky et al., 2001; Hwa et al., 2005; Riloff et al., 2002; Pad´o and Lapata, 2009; Xi and Hwa, 2005). This paradigm allows researchers to address a lack of annotated multilingual resources and changing ontologies with minimal annotation effort. The"
drexler-etal-2014-wikipedia,W00-0507,0,\N,Missing
drexler-etal-2014-wikipedia,P02-1040,0,\N,Missing
drexler-etal-2014-wikipedia,P05-1033,0,\N,Missing
drexler-etal-2014-wikipedia,P12-2023,0,\N,Missing
drexler-etal-2014-wikipedia,2005.mtsummit-papers.11,0,\N,Missing
drexler-etal-2014-wikipedia,J07-2003,0,\N,Missing
drexler-etal-2014-wikipedia,W11-2123,0,\N,Missing
drexler-etal-2014-wikipedia,P03-1021,0,\N,Missing
drexler-etal-2014-wikipedia,W13-2226,1,\N,Missing
E17-2018,J93-2004,0,0.0579074,"st construct one. Past work (focused on translating out of English into MRLs) assigned a handful of morphological annotations using manually-developed heuristics (Dr´abek and Yarowsky, 2005; Avramidis and Koehn, 2008), but this is hard to scale. We therefore instead look to obtain rich morphological tags by projecting them (Yarowsky et al., 2001) from a language (such as Czech) where such rich tags have already been annotated. We use the Prague Czech–English Dependency Treebank (PCEDT) (Hajiˇc et al., 2012), a complete translation of the Wall Street Journal portion of the Penn Treebank (PTB) (Marcus et al., 1993). Each word on the Czech side of the PCEDT was originally hand-annotated with complex 15-dimensional morphological tags containing positional subtag values for morphological categories specific to Czech.1 We manually mapped these tags to the UniMorph Schema tagset (SylakGlassman et al., 2015), which provides a universal, typologically-informed annotation framework for representing morphological features of inflected words in the world’s languages. UniMorph tags are in principle up to 23-dimensional, but tags are not positionally dependent, and not every dimension needs to be specified. Table 1"
E17-2018,D15-1272,1,0.906538,"Missing"
E17-2018,P08-1087,0,0.038713,"en the feature projected from a UniMorph ‘translation’ of the original PCEDT annotation of Czech matches the feature that would be expected subtag. Note that the core part-of-speech must agree as a precondition for further evaluation. Training a system to tag English text with multidimensional morphological tags requires a corpus of English text annotated with those tags. Since no such corpora exist, we must construct one. Past work (focused on translating out of English into MRLs) assigned a handful of morphological annotations using manually-developed heuristics (Dr´abek and Yarowsky, 2005; Avramidis and Koehn, 2008), but this is hard to scale. We therefore instead look to obtain rich morphological tags by projecting them (Yarowsky et al., 2001) from a language (such as Czech) where such rich tags have already been annotated. We use the Prague Czech–English Dependency Treebank (PCEDT) (Hajiˇc et al., 2012), a complete translation of the Wall Street Journal portion of the Penn Treebank (PTB) (Marcus et al., 1993). Each word on the Czech side of the PCEDT was originally hand-annotated with complex 15-dimensional morphological tags containing positional subtag values for morphological categories specific to"
E17-2018,J03-1002,0,0.00945149,"sal, typologically-informed annotation framework for representing morphological features of inflected words in the world’s languages. UniMorph tags are in principle up to 23-dimensional, but tags are not positionally dependent, and not every dimension needs to be specified. Table 1 shows the subset of UniMorph subtags used here. PTB tags have no formal internal subtag structure. See Figure 1 for a comparison of the PCEDT, UniMorph, and PTB tag systems for a Czech word and its aligned English translation. The PCEDT also contains automatically generated word alignments produced by using GIZA++ (Och and Ney, 2003) to align the Czech and English sides of the treebank. We use these alignments to project morphological tags from the Czech words to their English counterparts through the following process. For every English word, if the word is aligned to a single Czech word, take its tag. If the word is mapped to multiple Czech words, take the annotation from the alignment point belonging to the intersection of the two underlying GIZA++ models used to produce the many-many alignment.2 If no such alignment point is found, take the leftmost aligned word. Unaligned English words get no annotation. 3 Validating"
E17-2018,D14-1162,0,0.0865909,"t or case, under the Universal Dependency features.4 Finally, we use features from CFG parsing: • POS features. A word’s part-of-speech (POS) tag, its parent’s, and its grandparent’s. • Chain features. We compute chains of the tree nodes, starting with its POS tag and moving upward (NN NP S). • The distance to the root. Non-lexical features are treated as real-valued when appropriate (such as in the case of the distance to the root), while all others are treated as binary. For lexical features, we use pretrained GLoVe embeddings, specifically 200-dimensional 400K-vocab uncased embeddings from Pennington et al. (2014). This is an approach similar to Tran et al. (2015), but we additionally augment the pretrained embeddings with randomly initialized embeddings for vocabulary items outside of the 400K lexicon. Neural Morphological Tag Prediction Features With our projections validated, we turn to the prediction model itself. Based on the idea that languages with rich morphology use that morphology to convey similar distinctions in meaning to 4.2 Neural Model In order to take advantage of correlated information between subtags, we present a neural model 3 English also uses morphology to mark the 3rd person sin"
E17-2018,P16-1184,0,0.0647408,"ture and vice versa. We explore the veracity of this claim computationally by asking the following: Can we develop a tagger for English that uses the signal available in English-only syntactic structure to recover the rich semantic distinctions conveyed by morphology in Czech? Can we, for example, accurately detect which English contexts would have a Czech translation that employs the accusative case marker? Traditionally, morphological analysis and tagging is a task that has been limited to morphologically rich languages (MRLs) (Hajiˇc, 2000; Dr´abek and Yarowsky, 2005; M¨uller et al., 2015; Buys and Botha, 2016). In order to build a rich morphological tagger for a morphologically poor language (MPL) like English, we need some way to build a gold standard set of richly tagged English data for training and testing. Our approach is to project the complex morphological tags of Czech words directly onto the English words they align to in a large parallel corpus. After evaluating the validity of these projections, we develop a neural network tagging architecture that takes as input a number of English features derived from off-theshelf dependency parsing and attempts to recover the projected Czech tags. A"
E17-2018,W16-2209,0,0.0413943,"Missing"
E17-2018,W05-0807,0,0.752671,"Missing"
E17-2018,P15-2111,1,0.845556,"Missing"
E17-2018,2015.mtsummit-papers.12,0,0.0185026,"features from CFG parsing: • POS features. A word’s part-of-speech (POS) tag, its parent’s, and its grandparent’s. • Chain features. We compute chains of the tree nodes, starting with its POS tag and moving upward (NN NP S). • The distance to the root. Non-lexical features are treated as real-valued when appropriate (such as in the case of the distance to the root), while all others are treated as binary. For lexical features, we use pretrained GLoVe embeddings, specifically 200-dimensional 400K-vocab uncased embeddings from Pennington et al. (2014). This is an approach similar to Tran et al. (2015), but we additionally augment the pretrained embeddings with randomly initialized embeddings for vocabulary items outside of the 400K lexicon. Neural Morphological Tag Prediction Features With our projections validated, we turn to the prediction model itself. Based on the idea that languages with rich morphology use that morphology to convey similar distinctions in meaning to 4.2 Neural Model In order to take advantage of correlated information between subtags, we present a neural model 3 English also uses morphology to mark the 3rd person singular verb form. 4 114 universaldependencies.org Ot"
E17-2018,hajic-etal-2012-announcing,0,0.0623432,"Missing"
E17-2018,H01-1035,0,0.328922,"cted subtag. Note that the core part-of-speech must agree as a precondition for further evaluation. Training a system to tag English text with multidimensional morphological tags requires a corpus of English text annotated with those tags. Since no such corpora exist, we must construct one. Past work (focused on translating out of English into MRLs) assigned a handful of morphological annotations using manually-developed heuristics (Dr´abek and Yarowsky, 2005; Avramidis and Koehn, 2008), but this is hard to scale. We therefore instead look to obtain rich morphological tags by projecting them (Yarowsky et al., 2001) from a language (such as Czech) where such rich tags have already been annotated. We use the Prague Czech–English Dependency Treebank (PCEDT) (Hajiˇc et al., 2012), a complete translation of the Wall Street Journal portion of the Penn Treebank (PTB) (Marcus et al., 1993). Each word on the Czech side of the PCEDT was originally hand-annotated with complex 15-dimensional morphological tags containing positional subtag values for morphological categories specific to Czech.1 We manually mapped these tags to the UniMorph Schema tagset (SylakGlassman et al., 2015), which provides a universal, typol"
E17-2018,A00-2013,0,0.732801,"Missing"
I17-2004,D16-1162,0,0.0293866,"sults compared to NMT decoding with standard beam search. Introduction Domain adaptation is a major challenge for neural machine translation (NMT). Although impressive improvements have been achieved in recent years (c.f. Bojar et al. (2016)), NMT systems require a large amount of training data and thus perform poorly relative to phrase-based machine translation (PBMT) systems in low resource and domain adaptation scenarios (Koehn and Knowles, 2017). In such situations, neural systems often produce fluent output that unfortunately contains words not licensed by the unfamiliar source sentence (Arthur et al., 2016; Tu et al., 2016). Phrase-based systems, in contrast, explicitly model the translation of all source words via coverage vectors, and tend to produce translations that are adequate but less fluent. This situation is depicted in Table 1, which contains examples of PBMT and NMT systems trained on WMT training sets which are then applied to IT texts. We present an approach that combines the best of both worlds by using the lattice output of PBMT to constrain the search space available to an NMT decoder, thereby bringing together the adequacy Search Graph Source sentence PBMT NMT Lattice Search Ta"
I17-2004,D13-1106,0,0.0627969,"Missing"
I17-2004,P17-2061,0,0.0264912,"on? While the answer depends on the amount of in-domain and out-of-domain data, we find that PBMTin × NMTin and PBMTin × NMTout perform the best. This supports previous findings (Koehn and Knowles, 2017) that PBMTin is robust when training data is insufficient. In conclusion, we recommend using lattice search with search graphs from PBMTin , and NMT models can be trained on either in-domain or out-of-domain corpora. Related Work Previous work on domain adaptation in NMT focuses on training methods such as transfer learning or fine-tuning (Luong and Manning, 2015; Freitag and Al-Onaizan, 2016; Chu et al., 2017). This strategy begins with a strong model trained on a large out-of-domain corpus and then continuesx training on an in-domain corpus. Our approach is orthogonal in that we focus on search. Conceivably, advances in training methods might be incorporated to improve our individual NMTin models. Our lattice search algorithm is related to previous work in hybrid NMT/PBMT systems, which can be visualized on a spectrum depending on how tightly integrated the two systems are. On one end, NMT can easily be used to rerank N best lists output by PBMT; on the other, NMT can be incorporated as features i"
I17-2004,D11-1103,0,0.0778349,"Missing"
I17-2004,P14-1129,0,0.0327377,"combine in (b). This figure does not demonstrate pruning, descendants of items that fall off the beam would not be explored. 2 This is similar to PBMT stack decoding. However, in PBMT stack decoding, stacks are grouped by the number of translated source words, which is not possible in NMT, since the translation of individual source words is not tracked. 21 Corpus Medical IT Koran Subtitles WMT Words 14,301,472 3,041,677 9,848,539 114,371,754 113,165,079 Sentences 1,104,752 337,817 480,421 13,873,398 4,562,102 W/S 13 9 21 8 25 include German specific processing and Neural Network Joint Models (Devlin et al., 2014), replicating Ding et al. (2016). The PBMTin models are Moses models with standard settings, replicating Koehn and Knowles (2017). The NMT models are trained with Nematus (Sennrich et al., 2017). The NMTout models replicate Sennrich et al. (2016);4 the NMTin models replicate Koehn and Knowles (2017). We use Marian (Junczys-Dowmunt et al., 2016a) to rescore N -best lists. The search graphs are pre-processed by converting them to the OpenFST format (Allauzen et al., 2007) and applying operations to remove epsilon arcs, determinize, minimize and topsort. Since the search graphs may be prohibitive"
I17-2004,E17-3017,0,0.0437165,"Missing"
I17-2004,W16-2323,0,0.0301409,"ed source words, which is not possible in NMT, since the translation of individual source words is not tracked. 21 Corpus Medical IT Koran Subtitles WMT Words 14,301,472 3,041,677 9,848,539 114,371,754 113,165,079 Sentences 1,104,752 337,817 480,421 13,873,398 4,562,102 W/S 13 9 21 8 25 include German specific processing and Neural Network Joint Models (Devlin et al., 2014), replicating Ding et al. (2016). The PBMTin models are Moses models with standard settings, replicating Koehn and Knowles (2017). The NMT models are trained with Nematus (Sennrich et al., 2017). The NMTout models replicate Sennrich et al. (2016);4 the NMTin models replicate Koehn and Knowles (2017). We use Marian (Junczys-Dowmunt et al., 2016a) to rescore N -best lists. The search graphs are pre-processed by converting them to the OpenFST format (Allauzen et al., 2007) and applying operations to remove epsilon arcs, determinize, minimize and topsort. Since the search graphs may be prohibitively large in size, we prune them with a threshold.5 We perform 5fold cross-validation over pruning thresholds (.1, .25, .5) and lattice search beamsizes (1, 10, 100). Very aggressive pruning with a small beam limits the search to be very similar t"
I17-2004,W16-2316,0,0.0613373,"ds is not tracked. 21 Corpus Medical IT Koran Subtitles WMT Words 14,301,472 3,041,677 9,848,539 114,371,754 113,165,079 Sentences 1,104,752 337,817 480,421 13,873,398 4,562,102 W/S 13 9 21 8 25 include German specific processing and Neural Network Joint Models (Devlin et al., 2014), replicating Ding et al. (2016). The PBMTin models are Moses models with standard settings, replicating Koehn and Knowles (2017). The NMT models are trained with Nematus (Sennrich et al., 2017). The NMTout models replicate Sennrich et al. (2016);4 the NMTin models replicate Koehn and Knowles (2017). We use Marian (Junczys-Dowmunt et al., 2016a) to rescore N -best lists. The search graphs are pre-processed by converting them to the OpenFST format (Allauzen et al., 2007) and applying operations to remove epsilon arcs, determinize, minimize and topsort. Since the search graphs may be prohibitively large in size, we prune them with a threshold.5 We perform 5fold cross-validation over pruning thresholds (.1, .25, .5) and lattice search beamsizes (1, 10, 100). Very aggressive pruning with a small beam limits the search to be very similar to the PBMT output. In contrast, a very deep lattice with a large beam begins to approach the uncons"
I17-2004,E17-2058,0,0.0705903,"Missing"
I17-2004,P07-2045,1,0.0125337,"in terms of domain, but the training data is not large (relative to WMT). 3. PBMTin × NMTout : PBMT is trained on small in-domain data while NMT is trained on larger out-of-domain data. 4. PBMTout × NMTin : NMT is trained on small in-domain data while PBMT is trained on larger out-of-domain data. For each training configuration, we are interested in seeing how our proposed NMT lattice search compares to standard NMT beam search. Additionally, we compare the results of PBMT 1best decoding and PBMT N -best lists rescoring (N=500) using the same NMT model. The PBMT models are trained with Moses (Koehn et al., 2007). The PBMTout models 3 Results 4 github.com/rsennrich/wmt16-scripts Pruning removes arcs that do not appear on a lattice path whose score is within than t ⊗ w, where w is the weight of the FST’s shortest path, and t is the pruning threshold. 5 opensubtitles.org 22 Test Domain IT Medical Koran Subtitle Training Configuration PBMTout × NMTout PBMTin × NMTin PBMTin × NMTout PBMTout × NMTin PBMTout × NMTout PBMTin × NMTin PBMTin × NMTout PBMTout × NMTin PBMTout × NMTout PBMTin × NMTin PBMTin × NMTout PBMTout × NMTin PBMTout × NMTout PBMTin × NMTin PBMTin × NMTout PBMTout × NMTin PBMT 1-best 25.1 ("
I17-2004,P16-2049,0,0.037377,"us on search. Conceivably, advances in training methods might be incorporated to improve our individual NMTin models. Our lattice search algorithm is related to previous work in hybrid NMT/PBMT systems, which can be visualized on a spectrum depending on how tightly integrated the two systems are. On one end, NMT can easily be used to rerank N best lists output by PBMT; on the other, NMT can be incorporated as features in PBMT (JunczysDowmunt et al., 2016b). In the middle of the spectrum is NMT search (or re-scoring) based on constraints from PBMT. Our algorithm is conceptually very similar to Stahlberg et al. (2016), who rescore a WFSA reformulation of the Hiero formalism. Their 23 References algorithm is a breadth-first search over all the nodes of the lattice, capped by a beam. Other hybrid methods include: constraining the output vocabulary of NMT on a per-sentence basis, using bilingual information provided by PBMT (Mi et al., 2016), Minimum Bayes Risk decoding with PBMT n-gram posteriors (Stahlberg et al., 2017), and incorporating PBMT hypotheses as additional input in a modified NMT architecture (Wang et al., 2017). Related works in lattice search/re-scoring with RNNs (without NMT encoder-decoders)"
I17-2004,W17-3204,1,0.933362,"le stack-based lattice search algorithm for NMT,1 and (2) a set of domain adaptation experiments showing that PBMT lattice constraints are effective in achieving robust results compared to NMT decoding with standard beam search. Introduction Domain adaptation is a major challenge for neural machine translation (NMT). Although impressive improvements have been achieved in recent years (c.f. Bojar et al. (2016)), NMT systems require a large amount of training data and thus perform poorly relative to phrase-based machine translation (PBMT) systems in low resource and domain adaptation scenarios (Koehn and Knowles, 2017). In such situations, neural systems often produce fluent output that unfortunately contains words not licensed by the unfamiliar source sentence (Arthur et al., 2016; Tu et al., 2016). Phrase-based systems, in contrast, explicitly model the translation of all source words via coverage vectors, and tend to produce translations that are adequate but less fluent. This situation is depicted in Table 1, which contains examples of PBMT and NMT systems trained on WMT training sets which are then applied to IT texts. We present an approach that combines the best of both worlds by using the lattice ou"
I17-2004,tiedemann-2012-parallel,0,0.123673,"Missing"
I17-2004,P16-1008,0,0.025807,"decoding with standard beam search. Introduction Domain adaptation is a major challenge for neural machine translation (NMT). Although impressive improvements have been achieved in recent years (c.f. Bojar et al. (2016)), NMT systems require a large amount of training data and thus perform poorly relative to phrase-based machine translation (PBMT) systems in low resource and domain adaptation scenarios (Koehn and Knowles, 2017). In such situations, neural systems often produce fluent output that unfortunately contains words not licensed by the unfamiliar source sentence (Arthur et al., 2016; Tu et al., 2016). Phrase-based systems, in contrast, explicitly model the translation of all source words via coverage vectors, and tend to produce translations that are adequate but less fluent. This situation is depicted in Table 1, which contains examples of PBMT and NMT systems trained on WMT training sets which are then applied to IT texts. We present an approach that combines the best of both worlds by using the lattice output of PBMT to constrain the search space available to an NMT decoder, thereby bringing together the adequacy Search Graph Source sentence PBMT NMT Lattice Search Target translation F"
I17-2004,2015.iwslt-evaluation.11,0,0.0598293,"hich training configuration is best for domain adaptation? While the answer depends on the amount of in-domain and out-of-domain data, we find that PBMTin × NMTin and PBMTin × NMTout perform the best. This supports previous findings (Koehn and Knowles, 2017) that PBMTin is robust when training data is insufficient. In conclusion, we recommend using lattice search with search graphs from PBMTin , and NMT models can be trained on either in-domain or out-of-domain corpora. Related Work Previous work on domain adaptation in NMT focuses on training methods such as transfer learning or fine-tuning (Luong and Manning, 2015; Freitag and Al-Onaizan, 2016; Chu et al., 2017). This strategy begins with a strong model trained on a large out-of-domain corpus and then continuesx training on an in-domain corpus. Our approach is orthogonal in that we focus on search. Conceivably, advances in training methods might be incorporated to improve our individual NMTin models. Our lattice search algorithm is related to previous work in hybrid NMT/PBMT systems, which can be visualized on a spectrum depending on how tightly integrated the two systems are. On one end, NMT can easily be used to rerank N best lists output by PBMT; on"
I17-2004,P16-2021,0,0.0231845,"o rerank N best lists output by PBMT; on the other, NMT can be incorporated as features in PBMT (JunczysDowmunt et al., 2016b). In the middle of the spectrum is NMT search (or re-scoring) based on constraints from PBMT. Our algorithm is conceptually very similar to Stahlberg et al. (2016), who rescore a WFSA reformulation of the Hiero formalism. Their 23 References algorithm is a breadth-first search over all the nodes of the lattice, capped by a beam. Other hybrid methods include: constraining the output vocabulary of NMT on a per-sentence basis, using bilingual information provided by PBMT (Mi et al., 2016), Minimum Bayes Risk decoding with PBMT n-gram posteriors (Stahlberg et al., 2017), and incorporating PBMT hypotheses as additional input in a modified NMT architecture (Wang et al., 2017). Related works in lattice search/re-scoring with RNNs (without NMT encoder-decoders) (Ladhak et al., 2016; Deoras et al., 2011; Hori et al., 2014) may serve as other interesting comparisons. Specifically, Auli et al. (2013) and Liu et al. (2016) provide alternatives to our approach to the problem of recombination. The former work allows the splitting of previously recombined decoder states (thresholded) whil"
I17-2062,W14-3301,1,0.894246,"Missing"
I17-2062,D16-1161,0,0.256696,"C corpus (§3), demonstrating that NRL outperforms the MLE baseline both in human and automated evaluation metrics. Research in automated Grammatical Error Correction (GEC) has expanded from token-level, closed class corrections (e.g., determiners, prepositions, verb forms) to phrase-level, open class issues that consider fluency (e.g., content word choice, idiomatic collocation, word order, etc.). The expanded goals of GEC have led to new proposed models deriving from techniques in data-driven machine translation, including phrasebased MT (PBMT) (Felice et al., 2014; Chollampatt et al., 2016; Junczys-Dowmunt and Grundkiewicz, 2016) and neural encoder-decoder models (Yuan and Briscoe, 2016). Napoles et al. (2017) recently showed that a neural encoderdecoder can outperform PBMT on a fluencyoriented GEC data and metric. We investigate training methodologies in the neural encoder-decoder for GEC. To train the neural encoder-decoder models, maximum likelihood estimation (MLE) has been used, where the objective is to maximize the (log) likelihood of the parameters for a given training data. As Ranzato et al. (2015) indicates, however, MLE has drawbacks. The MLE objective is based 366 Proceedings of the The 8th International J"
I17-2062,E17-3017,0,0.0610603,"Missing"
I17-2062,W04-1013,0,0.0304549,"Missing"
I17-2062,P15-2097,1,0.863842,"Missing"
I17-2062,P16-1159,0,0.0419389,"ty and metric score for each sample yˆi (line 7). In the encoder-decoder model, the parameters θ are updated through back-propagation and the number of parameter updates is determined by the partial derivative of J(θ), called the policy gradient (Williams, 1992; Sutton et al., 1999) in reinforcement learning: ∂J(θ) = αE [∇ log p(ˆ y ){r(ˆ y , y) − b}] (4) ∂θ where α is a learning rate and b is an arbitrary baseline reward to reduce the variance. The sample mean reward is often used for b (Williams, 1992), and we follow it in NRL. It is reasonable to compare NRL to minimum risk training (MRT) (Shen et al., 2016). In fact, The gradient of L(θ) is as follows: T X X ∂L(θ) ∇p(yt |x, y1t−1 ; θ) = ∂θ p(yt |x, y1t−1 ; θ) hX,Y i t=1 Neural Reinforcement Learning (2) One drawback of MLE is the exposure bias (Ranzato et al., 2015). The decoder predicts a word conditioned on the correct word sequence (y1t−1 ) during training, whereas it does with the predicted word sequence (ˆ y1t−1 ) at test time. Namely, the model is not exposed to the predicted words in training time. This is problematic, because once the model fails to predict a correct word at test time, it falls off the right track and does not come back"
I17-2062,E17-2037,1,0.792118,"ation metrics. Research in automated Grammatical Error Correction (GEC) has expanded from token-level, closed class corrections (e.g., determiners, prepositions, verb forms) to phrase-level, open class issues that consider fluency (e.g., content word choice, idiomatic collocation, word order, etc.). The expanded goals of GEC have led to new proposed models deriving from techniques in data-driven machine translation, including phrasebased MT (PBMT) (Felice et al., 2014; Chollampatt et al., 2016; Junczys-Dowmunt and Grundkiewicz, 2016) and neural encoder-decoder models (Yuan and Briscoe, 2016). Napoles et al. (2017) recently showed that a neural encoderdecoder can outperform PBMT on a fluencyoriented GEC data and metric. We investigate training methodologies in the neural encoder-decoder for GEC. To train the neural encoder-decoder models, maximum likelihood estimation (MLE) has been used, where the objective is to maximize the (log) likelihood of the parameters for a given training data. As Ranzato et al. (2015) indicates, however, MLE has drawbacks. The MLE objective is based 366 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 366–372, c Taipei, Taiwan, N"
I17-2062,P03-1021,0,0.0568408,"Missing"
I17-2062,P12-2039,0,0.270885,"r/h) if h ≤ r where N (A, B, C, ...) is the number of overlapped n-grams among the sets, and BP brevity penalty is compute based on token length in the system hypothesis (h) and the reference (r). 3 Methods Hybrid (rule + PBMT) PBMT + GEC-feat. PBMT + Neural feat. enc-dec (MLE) + unk alignment enc-dec (MLE/NRL) Experiments Data For training the models (MLE and NRL), we use the following corpora: the NUS Corpus of Learner English (NUCLE) (Dahlmeier et al., 2013), the Cambridge Learner Corpus First Certificate English (FCE) (Yannakoudakis et al., 2011), and the Lang-8 Corpus of learner English (Tajiri et al., 2012). The basic statistics are shown in Table 1.3 We exclude some unreasonable edits (comments by editors, incomplete sentences such 4 3 https://github.com/AbiWord/enchant NRL code is available at https://github.com/ keisks/nematus/tree/nrl-gleu All the datasets are publicly available, for purposes of reproducibility. For more details about each dataset, refer to Sakaguchi et al. (2017). 5 368 Models Original AMU CAMB14 NUS CAMB16 MLE NRL Reference dev set Human GLEU -1.072 38.21 -0.405 41.74 -0.160 42.81 -0.131 46.27 -0.117 47.20 -0.052 48.24 0.169 49.82 1.769 55.26 test set Human GLEU -0.760 40."
I17-2062,P02-1040,0,0.09751,"ck of MLE is the exposure bias (Ranzato et al., 2015). The decoder predicts a word conditioned on the correct word sequence (y1t−1 ) during training, whereas it does with the predicted word sequence (ˆ y1t−1 ) at test time. Namely, the model is not exposed to the predicted words in training time. This is problematic, because once the model fails to predict a correct word at test time, it falls off the right track and does not come back to it easily. Furthermore, in most sentence generation tasks, the MLE objective does not necessarily correlate with our final evaluation metrics, such as BLEU (Papineni et al., 2002) in machine translation and ROUGE (Lin, 2004) in summarization. This is because MLE optimizes word level predictions at each time step instead of evaluating sentences as a whole. GEC is no exception. It depends on sentencelevel evaluation that considers grammaticality and fluency. For this purpose, it is natural to use GLEU (Napoles et al., 2015), which has been used as a 1 The reward is given at the end of the decoder output (i.e., delayed reward). 2 We sampled sentences from softmax distribution. 367 Corpus NUCLE FCE Lang-8 # sents. 57k 34k 1M mean chars per sent. 115 74 56 # sents. edited 3"
I17-2062,P11-1019,0,0.259539,"H, R) − [N (H, S) − N (H, S, R)] p0n = N (H) ( 1 if h &gt; r BP = exp(1 − r/h) if h ≤ r where N (A, B, C, ...) is the number of overlapped n-grams among the sets, and BP brevity penalty is compute based on token length in the system hypothesis (h) and the reference (r). 3 Methods Hybrid (rule + PBMT) PBMT + GEC-feat. PBMT + Neural feat. enc-dec (MLE) + unk alignment enc-dec (MLE/NRL) Experiments Data For training the models (MLE and NRL), we use the following corpora: the NUS Corpus of Learner English (NUCLE) (Dahlmeier et al., 2013), the Cambridge Learner Corpus First Certificate English (FCE) (Yannakoudakis et al., 2011), and the Lang-8 Corpus of learner English (Tajiri et al., 2012). The basic statistics are shown in Table 1.3 We exclude some unreasonable edits (comments by editors, incomplete sentences such 4 3 https://github.com/AbiWord/enchant NRL code is available at https://github.com/ keisks/nematus/tree/nrl-gleu All the datasets are publicly available, for purposes of reproducibility. For more details about each dataset, refer to Sakaguchi et al. (2017). 5 368 Models Original AMU CAMB14 NUS CAMB16 MLE NRL Reference dev set Human GLEU -1.072 38.21 -0.405 41.74 -0.160 42.81 -0.131 46.27 -0.117 47.20 -0."
K19-1005,P09-1053,0,0.0492448,"g(s), g(s0 )] + cos[g(s), g(t)])+ max(0, δ − cos[g(s), g(s0 )] + cos[g(s0 ), g(t0 )]) where g is one of (WORD, TRIGRAM, LSTM) and (t, t0 ) is a negative sample selected from a megabatch, an aggregation of m minibatches (Wieting and Gimpel, 2018).3 We evaluate the WORD model trained4 on PARA NMT, PARA BANK and PARA BANK 2 (our work). We retrieved the paraphrases from PARA - 3.8 Improving contextualized encoders with paraphrastic data Paraphrastic data can be used to fine-tune contextualized encoders such as BERT (Devlin et al., 2018). We frame the fine-tuning task as paraphrase identification (Das and Smith, 2009), where given a pair of sentences, the task is to classify them as paraphrases or non-paraphrases. To generate the training data, we extract, for each 3 We confirmed this loss with Wieting and Gimpel, that it captures their open implementation, which we employ. Wieting and Gimpel (2018) described their loss as: max(0, δ − cos(g(s), g(s0 )) + cos(g(s), g(t))), which is equivalent under their assumption the paraphrases are equivalent. 4 https://github.com/jwieting/ para-nmt-50m 49 BERT pBERT QQP MNLI STS-B MRPC 87.90 88.14 83.86 82.64 88.40 88.59 84.00 86.55 downstream tasks, in particular when"
K19-1005,C04-1051,0,0.12119,"a paraphrase pair, and (s, n) is a non-paraphrase pair. We use these to train a binary classifier with cross-entropy loss. We then use this BERT fine-tuned on paraphrases (henceforth pBERT) for fine-tuning on SQuAD 2.0 (Rajpurkar et al., 2018) and 4 NLP tasks present in the General Language Understanding Evaluation (GLUE) benchmark (Wang et al., 2019): Quora Question Pairs (QQP) (Chen et al., 2017), Multi-Genre Natural Language Inference (MNLI) (Williams et al., 2018), the Semantic Textual Similarity Benchmark (STS-B) (Agirre et al., 2016), and the Microsoft Research Paraphrase Corpus (MRPC) (Dolan et al., 2004). Following the model formulation, hyper-parameter selection and training procedure specified in Devlin et al. (2018), we add a single task-specific, randomly initialized output layer for the classifier. We present our results in Tab. 4 and Tab. 5. We observe gains for STS-B, MRPC and QQP, tasks strongly related to paraphrase identification. Fine-tuning on our paraphrase corpus also improves performance on SQuAD, a questionanswering task, while slightly degrading performance on MNLI. Overall, simple fine-tuning of BERT on our corpus leads to improvements on Sentential resources There exists mu"
K19-1005,S16-1081,0,0.166659,"PARA BANK17 /PARA BANK34 Our work1 /Our work3 Our work3 /Our work5 Our work1 /Our work5 20.58 64.16±.21 71.05±.22 69.46±.27 80.93 52.77±.48 45.00±.51 46.79±.12 2.26 5.51±.01 6.40±.19 6.25±.18 Our workmax /Our workmin 66.03±.86 49.10±.16 5.84±.33 Table 2: Collective diversity within our work compared to PARA BANK, as measured by (1-BLEU)×100, intersection/union score×100, and parse tree edit-distance. of paraphrase corpora as training data for the Semantic Textual Similarity (STS) task. STS aims to measure the degree of equivalence in meaning or semantics between a pair of sentences. Notably, Agirre et al. (2016) having been a part of the SemEval workshop (2012 -2017). The evaluation consists of human annotated English sentence pairs, scored on a scale of 0 to 5 to quantify similarity of meaning, with 0 being the least, and 5 the most similar. Wieting and Gimpel (Wieting and Gimpel, 2018) compared three encoding mechanisms: WORD, TRIGRAM and LSTM. The WORD model (Wieting et al., 2016) averages the embedding for each word in the sentence into a fixed length vector embedding for the sentence; the TRIGRAM model (Huang et al., 2013) averages over character trigrams; and the LSTM (Hochreiter and Schmidhube"
K19-1005,D17-1091,0,0.0293981,"ability of Neural Machine Translation (NMT) to recreate the translation target by conditioning on the source side of the bitext. Leveraging paraphrases in NLP In the context of semantic parsing, Berant and Liang (2014) use a paraphrase classification module to determine the match between a canonical utterance and a logical form, both using a phrase table and distributed representations. To improve question answering (QA), Duboue and ChuCarroll (2006) generate paraphrases of a given question using back-translation, and optionally replace the original question with the most relevant paraphrase. Dong et al. (2017) tackle QA by marginalizing the probability of an answer over a set of paraphrases, generated using rule-based and NMT-based methods. Fader et al. (2013) use a corpus of questions with paraphrases, to construct a corpus of semantically equivalent queries. PARA BANK took a similar approach but with the inclusion of lexical constraints from the target side of the bitext. This step allows for multiple translations from one bilingual sentence pair and promotes lexical diversity. Their work, despite being larger and shown to be less noisy than PARA NMT, relies on heuristics to produce hard constrai"
K19-1005,P98-1013,0,0.0242805,"accuracy scores are reported for MNLI. Numbers reported on Dev set F1 Exact Match Type BERT pBERT HasAns NoAns Total HasAns NoAns Total 76.81 71.44 74.12 70.34 71.44 70.89 74.21 74.95 74.58 68.00 74.95 71.48 4.1 Related works Paraphrastic resources Paraphrastic resources exist across different scopes (i.e., lexical, phrasal, sentential) and different creation strategies (i.e., manually curated, automatically generated). For a more comprehensive survey on data-driven approaches to paraphrasing, please refer to Madnani and Dorr (2010). Sub-sentential resources WordNet (Miller, 1995), FrameNet (Baker et al., 1998), and VerbNet (Schuler, 2006) can be used to extract paraphrastic expressions at lexical levels. They contain the grouping of words or phrases that share similar semantics and sometimes entailment relations. While FrameNet and VerbNet do have example sentences or frames where lexical units are put into contexts, there is no explicit paraphrastic relations among these examples. Also, these datasets tend to be small, as they were curated manually. There have been efforts to augment such resources with automatic methods (Snow et al., 2006; Pavlick et al., 2015b), but they are still confined to le"
K19-1005,N12-1017,0,0.119116,"anguage sentence which, when paired with the target-language reference, constitute a set of paraphrases. Working from the very large CzEng parallel corpus, Wieting and Gimpel (2018) produced a single paraphrase for each English sentence by translating from the Czech source. Hu et al. (2019) expanded on this by translating the Czech sentence several times, using positive or negative constraints obtained from the English reference. In terms of producing diverse paraphrases, both approaches are limited because they rely on beam search. There are potentially billions of paraphrases of a sentence (Dreyer and Marcu, 2012), yet beam search with recurrent models can only search a constant subset of them (in the beam size). There are techniques for producing more diverse paraphrases, such as the use of positive and negative constraints (Hu et al., 2019) or syntactic 1 3. Clustering. The samples are then clustered. The best item from each cluster (according to the summed score) is then returned (§2.3). 2.1 Constrained sampling Sampling is a more effective way to explore model search space than beam search, particularly in auto-regressive models that do not permit dynamic programming. We introduce two means by whic"
K19-1005,P01-1008,0,0.360989,"procedure specified in Devlin et al. (2018), we add a single task-specific, randomly initialized output layer for the classifier. We present our results in Tab. 4 and Tab. 5. We observe gains for STS-B, MRPC and QQP, tasks strongly related to paraphrase identification. Fine-tuning on our paraphrase corpus also improves performance on SQuAD, a questionanswering task, while slightly degrading performance on MNLI. Overall, simple fine-tuning of BERT on our corpus leads to improvements on Sentential resources There exists multiple human translations in the same language for some classic readings. Barzilay and McKeown (2001) sought to extract lexical paraphrastic expression from such sources. Unfortunately such resources – along with those manually constructed for text generation research (Robin, 1995; Pang et al., 2003) – are small and limited in domain. PARA NMT and PARA BANK are two much larger sentential paraphrastic resources created through back-translation. 50 Reference: Real life is sometimes thoughtless and mean. Hey, stop right there! real life is sometimes reckless and cruel . hey , stop . The real life is occasionally ruthless and cruel. The real world is occasionally ruthless and cruel. The real life"
K19-1005,P14-1133,0,0.0336483,"ANK: Our work: Table 6: Selected examples from our work, compared to paraphrastic resources with prior approaches. Our work has paraphrases that are not only different from the reference, but also diverse among themselves. 4.2 4.3 Translation-based Approaches PARA NMT is an automatically generated sentential paraphrastic resource through back-translating bilingual resources. It leveraged the imperfect ability of Neural Machine Translation (NMT) to recreate the translation target by conditioning on the source side of the bitext. Leveraging paraphrases in NLP In the context of semantic parsing, Berant and Liang (2014) use a paraphrase classification module to determine the match between a canonical utterance and a logical form, both using a phrase table and distributed representations. To improve question answering (QA), Duboue and ChuCarroll (2006) generate paraphrases of a given question using back-translation, and optionally replace the original question with the most relevant paraphrase. Dong et al. (2017) tackle QA by marginalizing the probability of an answer over a set of paraphrases, generated using rule-based and NMT-based methods. Fader et al. (2013) use a corpus of questions with paraphrases, to"
K19-1005,N06-2009,0,0.216059,"Missing"
K19-1005,D18-1045,0,0.0371487,"han beam search, particularly in auto-regressive models that do not permit dynamic programming. We introduce two means by which we can expand the hypothesis space, and produce a more diverse set of paraphrases, relative to straightforward beam search. Top-k sampling In auto-regressive neural MT, the standard sampling approach would be to choose a word wt at each decoder timestep t by sampling from the distribution P (wt |w1...t−1 ). This approach has been found effective over 1best beam search in generating source sentences in Available at http://nlp.jhu.edu/parabank2 45 2.3 back-translation (Edunov et al., 2018). However, for paraphrasing, this is not ideal, since words that are not semantically licensed by the source may be selected. Instead, we propose top-k sampling, in which we choose wt from the top k most-probable tokens at each time step. This way, we allow the model to sample flexibly, vastly opening up the hypothesis space, without creating a large risk of producing nonsensical translations. The above process produces a large set of translations of the source sentence. Many of them will be minor variants of one another, but we expect that there will be a lot of variety in the large pool. The"
K19-1005,P13-1158,0,0.0672992,"in NLP In the context of semantic parsing, Berant and Liang (2014) use a paraphrase classification module to determine the match between a canonical utterance and a logical form, both using a phrase table and distributed representations. To improve question answering (QA), Duboue and ChuCarroll (2006) generate paraphrases of a given question using back-translation, and optionally replace the original question with the most relevant paraphrase. Dong et al. (2017) tackle QA by marginalizing the probability of an answer over a set of paraphrases, generated using rule-based and NMT-based methods. Fader et al. (2013) use a corpus of questions with paraphrases, to construct a corpus of semantically equivalent queries. PARA BANK took a similar approach but with the inclusion of lexical constraints from the target side of the bitext. This step allows for multiple translations from one bilingual sentence pair and promotes lexical diversity. Their work, despite being larger and shown to be less noisy than PARA NMT, relies on heuristics to produce hard constraints on the decoder, which often causes unintended changes in semantics or grammar. Both works largely follow standard approaches in NMT, generating 1-bes"
K19-1005,P18-1007,0,0.0746948,"raints can be provided as tokens or phrases; the decoder tracks the progress of generation through each constraint and adds an infinite cost to the final word of any constraints, precluding its selection in both sampling and beam search. In order to further increase sample diversity when generating the hypotheses (§2.1), we obtain negative constraints from the source by randomly choosing a subset of tokens. We do this independently multiple times for each input sentence. This provides new sets of constraints for the inputs, independent of the decoding. Note that we use subword regularization (Kudo, 2018) during training, causing different subword segmentations to be applied to training data types each time they are encountered and helping to build more robust models. We only constrain on the Viterbi segmentation, effectively discouraging negatively constrained words from appearing in the output, instead of prohibiting them, since there are often ways for the model to produce a word by generating a different decomposition. 2.2 Edit-distance-based clustering 3 3.1 Evaluations Data All of our experiments are based on the CzEng 1.7 corpus, a subset of CzEng 1.6 (Bojar et al., 2016b) that has been"
K19-1005,J10-3003,0,0.0377699,"res are reported for QQP and MRPC, Spearman correlations are reported for STS-B, and accuracy scores are reported for MNLI. Numbers reported on Dev set F1 Exact Match Type BERT pBERT HasAns NoAns Total HasAns NoAns Total 76.81 71.44 74.12 70.34 71.44 70.89 74.21 74.95 74.58 68.00 74.95 71.48 4.1 Related works Paraphrastic resources Paraphrastic resources exist across different scopes (i.e., lexical, phrasal, sentential) and different creation strategies (i.e., manually curated, automatically generated). For a more comprehensive survey on data-driven approaches to paraphrasing, please refer to Madnani and Dorr (2010). Sub-sentential resources WordNet (Miller, 1995), FrameNet (Baker et al., 1998), and VerbNet (Schuler, 2006) can be used to extract paraphrastic expressions at lexical levels. They contain the grouping of words or phrases that share similar semantics and sometimes entailment relations. While FrameNet and VerbNet do have example sentences or frames where lexical units are put into contexts, there is no explicit paraphrastic relations among these examples. Also, these datasets tend to be small, as they were curated manually. There have been efforts to augment such resources with automatic metho"
K19-1005,N12-1019,0,0.0612033,"Missing"
K19-1005,N13-1092,1,0.878282,"Missing"
K19-1005,W17-3206,0,0.0268669,"Missing"
K19-1005,P14-5010,0,0.003484,"paraphrases for each reference sentence using the approach outlined in this work. To account for randomness, we average over two independent runs in the result, shown in Tab. 1. We consider two sources of paraphrastic diversity: 1) lexical diversity, the use of different words; and 2) syntactic diversity, the change of sentence or phrasal structure. We separately measure them using bag-of-word Intersection/Union scores and parse-tree edit-distances, respectively. will be. We consider only the top 3 levels of the parse trees, excluding any terminals. Sentences are parsed with Stanford CoreNLP (Manning et al., 2014); the tree edit-distance is calculated with the APTED (Pawlik and Augsten, 2015a,b) algorithm. The average tree edit-distance for each system is shown in Tab. 1. Diversity among paraphrases Hu et al. (2019) produced multiple paraphrases for each reference. While shown to be diverse compared to the reference, the authors did not investigate whether these paraphrases are trivial rewrites of one another, as it is likely the case with beam search under a few lexical constraints. Our clustering step is specifically designed to retrieve collectively diverse paraphrases. We use the same metrics to ev"
K19-1005,E17-3017,0,0.0132251,"omposition. 2.2 Edit-distance-based clustering 3 3.1 Evaluations Data All of our experiments are based on the CzEng 1.7 corpus, a subset of CzEng 1.6 (Bojar et al., 2016b) that has been chosen for higher quality. Based on experience with data quality issues in neural MT (Ott et al., 2018; Junczys-Dowmunt, 2018), we decided to further clean the corpus. First, we normalize Unicode punctuation, and keep only bilingual pairs whose English side can be encoded with latin-1 and Czech side with latin-2. We then filter the data with dual cross-entropy filtering (Junczys-Dowmunt, 2018). We use Sockeye (Hieber et al., 2017) to train two NMT models, CS–EN and EN-CS, on a relatively clean subset of the data provided for WMT 2018 (Bojar et al., 2016a): Europarl, Wiki titles, and news commentary. We use 4 layer Transformer models (Vaswani et al., 2017) trained to convergence, with held-out likelihood evaluated on a random 500sentence subset of the WMT16 and WMT17 news test data. These models are then used to score all the remaining CzEng data after deduplication. We kept all sentences with a model score (negative log-likelihood) of less than 3.5. After applying the above two filters, we keep 19, 723, 003 out of the"
K19-1005,N16-3013,1,0.893873,"k-oriented trial-and-error. The ability to understand and produce paraphrases is a basic competency task, one that is often used as a teaching aid to validate if a student understands a statement or a concept. Current deep learning systems struggle with this task, exhibiting brittleness to both understanding and producing paraphrastic expressions (Iyyer et al., 2018). One crucial factor behind this incompetence is the dearth of sentential paraphrastic data. Many works have sought to leverage the relative abundance of sub-sentential paraphrastic resources in paraphrase detection or generation (Napoles et al., 2016). Yet, they fail to capture contextualized word choices or syntactical variations, as wordor phrase-level resources cannot incorporate information from the whole input sentence. Recent works have focused on leveraging bilingual resources to create large sentence-level paraphrastic collections using translation-based methods (Wieting and Gimpel, 2018; Hu et al., 2019). We present a novel resource with accurate and collectively diverse paraphrases, generated using stochastic decoding and clustering. By collectively diverse, we mean that the paraphrases of a given sentence cover a wide lexical an"
K19-1005,N18-1170,0,0.136454,"idates. One approach to force diverse translations is the use of hard lexical constraints at inference time (Hu et al., 2019). While effective in some cases, current approaches to automatic selection of such constraints is based on heuristics and task-oriented trial-and-error. The ability to understand and produce paraphrases is a basic competency task, one that is often used as a teaching aid to validate if a student understands a statement or a concept. Current deep learning systems struggle with this task, exhibiting brittleness to both understanding and producing paraphrastic expressions (Iyyer et al., 2018). One crucial factor behind this incompetence is the dearth of sentential paraphrastic data. Many works have sought to leverage the relative abundance of sub-sentential paraphrastic resources in paraphrase detection or generation (Napoles et al., 2016). Yet, they fail to capture contextualized word choices or syntactical variations, as wordor phrase-level resources cannot incorporate information from the whole input sentence. Recent works have focused on leveraging bilingual resources to create large sentence-level paraphrastic collections using translation-based methods (Wieting and Gimpel, 2"
K19-1005,W18-6478,0,0.0446558,"they are encountered and helping to build more robust models. We only constrain on the Viterbi segmentation, effectively discouraging negatively constrained words from appearing in the output, instead of prohibiting them, since there are often ways for the model to produce a word by generating a different decomposition. 2.2 Edit-distance-based clustering 3 3.1 Evaluations Data All of our experiments are based on the CzEng 1.7 corpus, a subset of CzEng 1.6 (Bojar et al., 2016b) that has been chosen for higher quality. Based on experience with data quality issues in neural MT (Ott et al., 2018; Junczys-Dowmunt, 2018), we decided to further clean the corpus. First, we normalize Unicode punctuation, and keep only bilingual pairs whose English side can be encoded with latin-1 and Czech side with latin-2. We then filter the data with dual cross-entropy filtering (Junczys-Dowmunt, 2018). We use Sockeye (Hieber et al., 2017) to train two NMT models, CS–EN and EN-CS, on a relatively clean subset of the data provided for WMT 2018 (Bojar et al., 2016a): Europarl, Wiki titles, and news commentary. We use 4 layer Transformer models (Vaswani et al., 2017) trained to convergence, with held-out likelihood evaluated on"
K19-1005,N03-1024,0,0.117882,"nd QQP, tasks strongly related to paraphrase identification. Fine-tuning on our paraphrase corpus also improves performance on SQuAD, a questionanswering task, while slightly degrading performance on MNLI. Overall, simple fine-tuning of BERT on our corpus leads to improvements on Sentential resources There exists multiple human translations in the same language for some classic readings. Barzilay and McKeown (2001) sought to extract lexical paraphrastic expression from such sources. Unfortunately such resources – along with those manually constructed for text generation research (Robin, 1995; Pang et al., 2003) – are small and limited in domain. PARA NMT and PARA BANK are two much larger sentential paraphrastic resources created through back-translation. 50 Reference: Real life is sometimes thoughtless and mean. Hey, stop right there! real life is sometimes reckless and cruel . hey , stop . The real life is occasionally ruthless and cruel. The real world is occasionally ruthless and cruel. The real life is sometimes reckless and cruel. Stay where you are! True life is sometimes ruthless and cruel. Actual life is sometimes ruthless and cruel. Sometimes real life is ruthless and cruel. Real life can b"
K19-1005,P15-2070,1,0.904204,"Missing"
K19-1005,P15-2067,1,0.88505,"Missing"
K19-1005,P18-1042,0,0.256353,"(Iyyer et al., 2018). One crucial factor behind this incompetence is the dearth of sentential paraphrastic data. Many works have sought to leverage the relative abundance of sub-sentential paraphrastic resources in paraphrase detection or generation (Napoles et al., 2016). Yet, they fail to capture contextualized word choices or syntactical variations, as wordor phrase-level resources cannot incorporate information from the whole input sentence. Recent works have focused on leveraging bilingual resources to create large sentence-level paraphrastic collections using translation-based methods (Wieting and Gimpel, 2018; Hu et al., 2019). We present a novel resource with accurate and collectively diverse paraphrases, generated using stochastic decoding and clustering. By collectively diverse, we mean that the paraphrases of a given sentence cover a wide lexical and syntactic spectrum. Given a bilingual input pair, our core idea is to sample a large space of outputs from a translation system, cluster the results according to a notion of token-sequence similarity, score them with two translation models (one in each direction), and then select the best item from each cluster. We believe that sampling from the w"
K19-1005,N18-1101,0,0.0412191,"tence s, we then find the (approximate) nearest neighbour n which is not s0 , among all of the sentences. We thus obtain two pairs, where (s, s0 ) is a paraphrase pair, and (s, n) is a non-paraphrase pair. We use these to train a binary classifier with cross-entropy loss. We then use this BERT fine-tuned on paraphrases (henceforth pBERT) for fine-tuning on SQuAD 2.0 (Rajpurkar et al., 2018) and 4 NLP tasks present in the General Language Understanding Evaluation (GLUE) benchmark (Wang et al., 2019): Quora Question Pairs (QQP) (Chen et al., 2017), Multi-Genre Natural Language Inference (MNLI) (Williams et al., 2018), the Semantic Textual Similarity Benchmark (STS-B) (Agirre et al., 2016), and the Microsoft Research Paraphrase Corpus (MRPC) (Dolan et al., 2004). Following the model formulation, hyper-parameter selection and training procedure specified in Devlin et al. (2018), we add a single task-specific, randomly initialized output layer for the classifier. We present our results in Tab. 4 and Tab. 5. We observe gains for STS-B, MRPC and QQP, tasks strongly related to paraphrase identification. Fine-tuning on our paraphrase corpus also improves performance on SQuAD, a questionanswering task, while slig"
K19-1005,N18-1119,1,0.671347,"skip the re-centering for that cluster. This improves the chance of the k clusters congregating candidates different from the reference in different ways. When the clustering has converged, we take the candidate with the best score from each cluster (except for the one with the reference sentence), rank them by score, and take the best n as the final output. Randomized negative constraints Negative constraints are tokens that are not permitted in the decoder output. They are not formally described in the literature, but an implementation was provided with the associated positive constraints (Post and Vilar, 2018). Negative constraints can be provided as tokens or phrases; the decoder tracks the progress of generation through each constraint and adds an infinite cost to the final word of any constraints, precluding its selection in both sampling and beam search. In order to further increase sample diversity when generating the hypotheses (§2.1), we obtain negative constraints from the source by randomly choosing a subset of tokens. We do this independently multiple times for each input sentence. This provides new sets of constraints for the inputs, independent of the decoding. Note that we use subword"
K19-1005,N13-1106,1,0.652759,"Missing"
K19-1005,P18-2124,0,0.0676573,"Missing"
K19-1005,N15-1091,0,0.0358836,"Missing"
K19-1005,P18-1020,1,0.844276,"Missing"
K19-1005,P06-1101,0,0.0617858,"b-sentential resources WordNet (Miller, 1995), FrameNet (Baker et al., 1998), and VerbNet (Schuler, 2006) can be used to extract paraphrastic expressions at lexical levels. They contain the grouping of words or phrases that share similar semantics and sometimes entailment relations. While FrameNet and VerbNet do have example sentences or frames where lexical units are put into contexts, there is no explicit paraphrastic relations among these examples. Also, these datasets tend to be small, as they were curated manually. There have been efforts to augment such resources with automatic methods (Snow et al., 2006; Pavlick et al., 2015b), but they are still confined to lexical level and sometimes require the use of other paraphrastic resources (Pavlick et al., 2015b). PPDB (Ganitkevitch et al., 2013; Pavlick et al., 2015a) automated the generation of lexical paraphrases via bilingual pivoting, taking advantage of the relative abundance of bilingual corpora. While significantly larger and more informative (e.g., ranking, entailment relations, etc.) than the above manually curated resources, PPDB suffers from ambiguity as words or phrases are removed from their sentential contexts. Table 5: SQuAD 2.0 res"
N12-1033,P06-1005,1,0.825122,"Missing"
N12-1033,P05-1022,0,0.0203883,"pful; this-VBG also occurs in both cases. We need the deeper knowledge that a specific determiner is used as a complete NP. We evaluate three feature types that aim to capture such knowledge. In each case, we aggregate the feature counts over all the parse trees constituting a document. The feature value is the log-count of how often each feature occurs. To remove content information from the features, we preprocess the parse tree terminals: all non-style-word terminals are replaced with their spelling signature (see §6.2). C & J Reranking Features: We also extracted the reranking features of Charniak and Johnson (2005). These features were hand-crafted for reranking the output of a parser, but have recently been used for other NLP tasks (Post, 2011; Wong and Dras, 2011). They include lexicalized features for sub-trees and head-to-head dependencies, and aggregate features for conjunct parallelism and the degree of rightbranching. We get the features using another script from Post.9 While TSG fragments tile a parse tree into a few useful fragments, C & J features can produce thousands of features per sentence, and are thus much more computationally-demanding. CFG Rules: 7 We include a feature for every unique"
N12-1033,2008.amta-papers.4,0,0.0441715,"ART-system stopwords (following Tomokiyo and Jones (2001)). Latin abbreviations are i.e., e.g., etc., c.f., et or al. 7 E.g., signature ‘LC-ing’ means lower-case, ending in ing. These are created via a script included with the Berkeley parser. VP ... NP NP PRP VBD DT VBG this using we did ... Figure 2: Motivating deeper syntactic features: The shaded TSG fragment indicates native English, but is not directly encoded in Bow, Style, nor standard CFG-rules. lometry, Post (2011) uses them to predict sentence grammaticality (i.e. detecting pseudo-sentences following Okanohara and Tsujii (2007) and Cherry and Quirk (2008)). We use Post’s TSG training settings and his public code.8 We parse with the TSG grammar and extract the fragments as features. We also follow Post by having features for aggregate TSG statistics, e.g., how many fragments are of a given size, tree-depth, etc. These syntactic meta-features are somewhat similar to the manually-defined stylometric features of Stamatatos et al. (2001). Style features are likewise unhelpful; this-VBG also occurs in both cases. We need the deeper knowledge that a specific determiner is used as a complete NP. We evaluate three feature types that aim to capture such"
N12-1033,W07-1604,0,0.0280403,"ative vs. Non-Native English We introduce the task of predicting whether a scientific paper is written by a native English speaker (NES) or non-native speaker (NNS). Prior work has mostly made this prediction in learner corpora (Koppel et al., 2005; Tsur and Rappoport, 2007; Wong and Dras, 2011), although there have been attempts in elicited speech transcripts (Tomokiyo and Jones, 2001) and e-mail (Estival et al., 2007). There has also been a large body of work on correcting errors in non-native writing, with a specific focus on difficulties in preposition and article usage (Han et al., 2006; Chodorow et al., 2007; Felice and Pulman, 2007; Tetreault and Chodorow, 2008; Gamon, 2010). We annotate papers using two pieces of associated meta-data: (1) author first names and (2) countries of affiliation. We manually marked each country for whether English is predominantly spoken there. We 329 then built a list of common first names of English speakers via the top 150 male and female names from the U.S. census.3 If the first author of a paper has an English first name and English-speakingcountry affiliation, we mark as NES.4 If none of the authors have an English first name nor an Englishspeaking-country affi"
N12-1033,W10-4236,0,0.0100164,"ence of individual articles by modeling their impact on the content of future papers. Yogatama et al. (2011) predict whether a paper will be cited based on both its content and its meta-data such as author names and publication venues. Johri et al. (2011) use per-author topic models to assess the nature of collaboration in a particular article (e.g., apprenticeship or synergy). One of the tasks in Sarawgi et al. (2011) concerned predicting gender in scientific writing, but they use a corpus of only ten “highly established” authors and make the prediction using twenty papers for each. Finally, Dale and Kilgarriff (2010) initiated a shared task on automatic editing of scientific papers written by non-native speakers, with the objective of developing “tools which can help non-native speakers of English (NNSs) (and maybe some native ones) write academic English prose of the kind that helps a paper get accepted.” Lexical and pragmatic choices in academic writing have also been analyzed within the applied linguistics community (Myers, 1989; Vassileva, 1998). 3 ACL Dataset and Preprocessing We use papers from the ACL Anthology Network (Radev et al., 2009b, Release 2011) and exploit its manually-curated meta-data s"
N12-1033,P11-1137,0,0.0241915,"ttributes of documents from the style of the writing. In some domains, statistical techniques have successfully deduced author identity (Mosteller and Wallace, 1984), gender (Koppel et al., 2003), native language (Koppel et al., 2005), and even whether an author has dementia (Le et al., 2011). Stylometric analysis is important to marketers, analysts and social scientists because it provides demographic data directly from raw text. There has been growing interest in applying stylometry to the content generated by users of Internet applications, e.g., detecting author ethnicity in social media (Eisenstein et al., 2011; Rao et al., 2011), or whether someone is writing deceptive online reviews (Ott et al., 2011). We evaluate stylometric techniques in the novel domain of scientific writing. Science is a difficult domain; authors are encouraged, often explicitly by reviewers/submission-guidelines, to comply with normative practices in style, spelling and grammar. Moreover, topical clues are less salient than in domains like social media. Success in this challenging domain can bring us closer to correctly analyzing the huge volumes of online text that are currently unmarked for useful author attributes such as"
N12-1033,W07-1607,0,0.0665596,"Missing"
N12-1033,C04-1088,0,0.0676934,"r other NLP tasks (Post, 2011; Wong and Dras, 2011). They include lexicalized features for sub-trees and head-to-head dependencies, and aggregate features for conjunct parallelism and the degree of rightbranching. We get the features using another script from Post.9 While TSG fragments tile a parse tree into a few useful fragments, C & J features can produce thousands of features per sentence, and are thus much more computationally-demanding. CFG Rules: 7 We include a feature for every unique, single-level context-free-grammar (CFG) rule application in a paper (following Baayen et al. (1996), Gamon (2004), Hirst and Feiguina (2007), Wong and Dras (2011)). The Figure 2 tree would have features: NP¢PRP, NP¢DT, DT¢this, etc. Such features do capture that a determiner was used as an NP, but they do not jointly encode which determiner was used. This is an important omission; we’ll see that other determiners acting as stand-alone NPs indicate non-native writing (e.g., the word that, see §7.2). TSG Fragments: A tree-substitution grammar is a generalization of CFGs that allow rewriting to tree fragments rather than sequences of non-terminals (Joshi and Schabes, 1997). Figure 2 gives the example NP¢(DT"
N12-1033,N10-1019,0,0.0171364,"entific paper is written by a native English speaker (NES) or non-native speaker (NNS). Prior work has mostly made this prediction in learner corpora (Koppel et al., 2005; Tsur and Rappoport, 2007; Wong and Dras, 2011), although there have been attempts in elicited speech transcripts (Tomokiyo and Jones, 2001) and e-mail (Estival et al., 2007). There has also been a large body of work on correcting errors in non-native writing, with a specific focus on difficulties in preposition and article usage (Han et al., 2006; Chodorow et al., 2007; Felice and Pulman, 2007; Tetreault and Chodorow, 2008; Gamon, 2010). We annotate papers using two pieces of associated meta-data: (1) author first names and (2) countries of affiliation. We manually marked each country for whether English is predominantly spoken there. We 329 then built a list of common first names of English speakers via the top 150 male and female names from the U.S. census.3 If the first author of a paper has an English first name and English-speakingcountry affiliation, we mark as NES.4 If none of the authors have an English first name nor an Englishspeaking-country affiliation, we mark as NNS. We use this rule to label our development an"
N12-1033,D08-1038,0,0.0300691,"Hill and Provost (2003) predict author identities. They ignore the article body and instead consider (a) potential self-citations and (b) similarity between the article’s citation list and the citation lists of known papers. Radev et al. (2009a) perform a bibliometric analysis of computational linguistics. Teufel and Moens (2002) and Qazvinian and Radev (2008) summarize scientific articles, the latter by automatically finding and filtering sentences in other papers that cite the target article. Our system does not consider citations; it is most 328 similar to work that uses raw article text. Hall et al. (2008) build per-year topic models over scientific literature to track the evolution of scientific ideas. Gerrish and Blei (2010) assess the influence of individual articles by modeling their impact on the content of future papers. Yogatama et al. (2011) predict whether a paper will be cited based on both its content and its meta-data such as author names and publication venues. Johri et al. (2011) use per-author topic models to assess the nature of collaboration in a particular article (e.g., apprenticeship or synergy). One of the tasks in Sarawgi et al. (2011) concerned predicting gender in scient"
N12-1033,W11-1516,0,0.0839927,"ractices in style, spelling and grammar. Moreover, topical clues are less salient than in domains like social media. Success in this challenging domain can bring us closer to correctly analyzing the huge volumes of online text that are currently unmarked for useful author attributes such as gender and native-language. Yet science is more than just a good steppingstone for stylometry; it is an important area in itself. Systems for scientific stylometry would give sociologists new tools for analyzing academic communities, and new ways to resolve the nature of collaboration in specific articles (Johri et al., 2011). Authors might also use these tools, e.g., to help ensure a consistent style in multi-authored papers (Glover and Hirst, 1995), or to determine sections of a paper needing revision. 327 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 327–337, c Montr´eal, Canada, June 3-8, 2012. 2012 Association for Computational Linguistics The contributions of our paper include: New Stylometric Tasks: We predict whether a paper is written: (1) by a native or non-native speaker, (2) by a male or female, and (3) in the style of"
N12-1033,P07-1010,0,0.0443822,"st is the standard set of 524 SMART-system stopwords (following Tomokiyo and Jones (2001)). Latin abbreviations are i.e., e.g., etc., c.f., et or al. 7 E.g., signature ‘LC-ing’ means lower-case, ending in ing. These are created via a script included with the Berkeley parser. VP ... NP NP PRP VBD DT VBG this using we did ... Figure 2: Motivating deeper syntactic features: The shaded TSG fragment indicates native English, but is not directly encoded in Bow, Style, nor standard CFG-rules. lometry, Post (2011) uses them to predict sentence grammaticality (i.e. detecting pseudo-sentences following Okanohara and Tsujii (2007) and Cherry and Quirk (2008)). We use Post’s TSG training settings and his public code.8 We parse with the TSG grammar and extract the fragments as features. We also follow Post by having features for aggregate TSG statistics, e.g., how many fragments are of a given size, tree-depth, etc. These syntactic meta-features are somewhat similar to the manually-defined stylometric features of Stamatatos et al. (2001). Style features are likewise unhelpful; this-VBG also occurs in both cases. We need the deeper knowledge that a specific determiner is used as a complete NP. We evaluate three feature ty"
N12-1033,P11-1032,0,0.0408499,"uccessfully deduced author identity (Mosteller and Wallace, 1984), gender (Koppel et al., 2003), native language (Koppel et al., 2005), and even whether an author has dementia (Le et al., 2011). Stylometric analysis is important to marketers, analysts and social scientists because it provides demographic data directly from raw text. There has been growing interest in applying stylometry to the content generated by users of Internet applications, e.g., detecting author ethnicity in social media (Eisenstein et al., 2011; Rao et al., 2011), or whether someone is writing deceptive online reviews (Ott et al., 2011). We evaluate stylometric techniques in the novel domain of scientific writing. Science is a difficult domain; authors are encouraged, often explicitly by reviewers/submission-guidelines, to comply with normative practices in style, spelling and grammar. Moreover, topical clues are less salient than in domains like social media. Success in this challenging domain can bring us closer to correctly analyzing the huge volumes of online text that are currently unmarked for useful author attributes such as gender and native-language. Yet science is more than just a good steppingstone for stylometry;"
N12-1033,W02-1011,0,0.0129865,"atures that have been used in stylometry, ranging from early manual selection of potentially discriminative words, to approaches based on automated text categorization (Sebastiani, 2002). We use the following three feature classes; the particular features were chosen based on development experiments. 330 6.1 Bow Features A variety of “discouraging results” in the text categorization literature have shown that simple bag-ofwords (Bow) representations usually perform better than “more sophisticated” ones (e.g. using syntax) (Sebastiani, 2002). This was also observed in sentiment classification (Pang et al., 2002). One key aim of our research is to see whether this is true of scientific stylometry. Our Bow representation uses a feature for each unique lower-case word-type in an article. We also preprocess papers by making all digits ’0’. Normalizing digits and filtering capitalized words helps ensure citations and named-entities are excluded from our features. The feature value is the log-count of how often the corresponding word occurs in the document. 6.2 Style Features While text categorization relies on keywords, stylometry focuses on topic-independent measures like function word frequency (Mostell"
N12-1033,P06-1055,0,0.02213,"mation). In case the text was garbled, we then filtered the first 3 lines from every file and any line with an ’@’ symbol (which might be part of an affiliation). We remove footers like Proceedings of ..., table/figure captions, and any lines with non-ASCII characters (e.g. math equations). Papers are then parsed via the Berke1 2 Via the open-source utility pdftotext Splitter from cogcomp.cs.illinois.edu/page/tools Task NativeL Venue Gender Training Set: Strict Lenient 2127 3963 2484 3991 2125 3497 Dev Set 450 400 400 Test Set 477 421 409 Table 1: Number of documents for each task ley parser (Petrov et al., 2006), and part-of-speech (PoS) tagged using CRFTagger (Phan, 2006). Training sets always comprise papers from 20012007, while test sets are created by randomly shuffling the 2008-2009 portion and then dividing it into development/test sets. We also use papers from 1990-2000 for experiments in §7.3 and §7.4. 4 Stylometric Tasks Each task has both a Strict training set, using only the data for which we are most confident in the labels (as described below), and a Lenient set, which forcibly assigns every paper in the training period to some class (Table 1). All test papers are annotated using a Stric"
N12-1033,P09-2012,1,0.84112,"s is an important omission; we’ll see that other determiners acting as stand-alone NPs indicate non-native writing (e.g., the word that, see §7.2). TSG Fragments: A tree-substitution grammar is a generalization of CFGs that allow rewriting to tree fragments rather than sequences of non-terminals (Joshi and Schabes, 1997). Figure 2 gives the example NP¢(DT this). This fragment captures both the identity of the determiner and its syntactic function as an NP, as desired. Efficient Bayesian procedures have recently been developed that enable the training of large-scale probabilistic TSG grammars (Post and Gildea, 2009; Cohn et al., 2010). While TSGs have not been used previously in sty331 Experiments and Results We take the minority class as the positive class: NES for NativeL, top-tier for Venue and female for Gender, and calculate the precision/recall of these classes. We tune three hyperparameters for F1score on development data: (1) the SVM regularization parameter, (2) the threshold for classifying an instance as positive (using the signed hyperplanedistance as the score), and (3) for transductive training (§5), the fraction of unlabeled data to label as positive. Statistical significance on held-out"
N12-1033,P11-2038,1,0.803867,"ies a noun. The Bow features are clearly unhelpful: this occurs in both cases. The 6 The stopword list is the standard set of 524 SMART-system stopwords (following Tomokiyo and Jones (2001)). Latin abbreviations are i.e., e.g., etc., c.f., et or al. 7 E.g., signature ‘LC-ing’ means lower-case, ending in ing. These are created via a script included with the Berkeley parser. VP ... NP NP PRP VBD DT VBG this using we did ... Figure 2: Motivating deeper syntactic features: The shaded TSG fragment indicates native English, but is not directly encoded in Bow, Style, nor standard CFG-rules. lometry, Post (2011) uses them to predict sentence grammaticality (i.e. detecting pseudo-sentences following Okanohara and Tsujii (2007) and Cherry and Quirk (2008)). We use Post’s TSG training settings and his public code.8 We parse with the TSG grammar and extract the fragments as features. We also follow Post by having features for aggregate TSG statistics, e.g., how many fragments are of a given size, tree-depth, etc. These syntactic meta-features are somewhat similar to the manually-defined stylometric features of Stamatatos et al. (2001). Style features are likewise unhelpful; this-VBG also occurs in both c"
N12-1033,C08-1087,0,0.0181928,"re; citation analysis is a well-known bibliometric approach for ranking authors and papers (Borgman and Furner, 2001). Bibliometry and stylometry can share goals but differ in techniques. For example, in a work questioning the blindness of double-blind reviewing, Hill and Provost (2003) predict author identities. They ignore the article body and instead consider (a) potential self-citations and (b) similarity between the article’s citation list and the citation lists of known papers. Radev et al. (2009a) perform a bibliometric analysis of computational linguistics. Teufel and Moens (2002) and Qazvinian and Radev (2008) summarize scientific articles, the latter by automatically finding and filtering sentences in other papers that cite the target article. Our system does not consider citations; it is most 328 similar to work that uses raw article text. Hall et al. (2008) build per-year topic models over scientific literature to track the evolution of scientific ideas. Gerrish and Blei (2010) assess the influence of individual articles by modeling their impact on the content of future papers. Yogatama et al. (2011) predict whether a paper will be cited based on both its content and its meta-data such as author"
N12-1033,W09-3607,0,0.0268762,"olling for paper venue and origin. 2 Related Work Bibliometrics is the empirical analysis of scholarly literature; citation analysis is a well-known bibliometric approach for ranking authors and papers (Borgman and Furner, 2001). Bibliometry and stylometry can share goals but differ in techniques. For example, in a work questioning the blindness of double-blind reviewing, Hill and Provost (2003) predict author identities. They ignore the article body and instead consider (a) potential self-citations and (b) similarity between the article’s citation list and the citation lists of known papers. Radev et al. (2009a) perform a bibliometric analysis of computational linguistics. Teufel and Moens (2002) and Qazvinian and Radev (2008) summarize scientific articles, the latter by automatically finding and filtering sentences in other papers that cite the target article. Our system does not consider citations; it is most 328 similar to work that uses raw article text. Hall et al. (2008) build per-year topic models over scientific literature to track the evolution of scientific ideas. Gerrish and Blei (2010) assess the influence of individual articles by modeling their impact on the content of future papers."
N12-1033,P10-2008,0,0.0633266,"pendent measures like function word frequency (Mosteller and Wallace, 1984), sentence length (Yule, 1939), and PoS (Hirst and Feiguina, 2007). We define a style-word to be: (1) punctuation, (2) a stopword, or (3) a Latin abbreviation.6 We create Style features for all unigrams and bigrams, replacing non-style-words separately with both PoS-tags and spelling signatures.7 Each feature is an N-gram, the value is its log-count in the article. We also include stylistic meta-features such as mean-words-per-sentence and mean-word-length. 6.3 Syntax Features Unlike recent work using generative PCFGs (Raghavan et al., 2010; Sarawgi et al., 2011), we use syntax directly as features in discriminative models, which can easily incorporate arbitrary and overlapping syntactic clues. For example, we will see that one indicator of native text is the use of certain determiners as stand-alone noun phrases (NPs), like this in Figure 2. This contrasts with a proposed non-native phrase, “this/DT growing/VBG area/NN,” where this instead modifies a noun. The Bow features are clearly unhelpful: this occurs in both cases. The 6 The stopword list is the standard set of 524 SMART-system stopwords (following Tomokiyo and Jones (20"
N12-1033,W11-0310,0,0.510526,"ilar to work that uses raw article text. Hall et al. (2008) build per-year topic models over scientific literature to track the evolution of scientific ideas. Gerrish and Blei (2010) assess the influence of individual articles by modeling their impact on the content of future papers. Yogatama et al. (2011) predict whether a paper will be cited based on both its content and its meta-data such as author names and publication venues. Johri et al. (2011) use per-author topic models to assess the nature of collaboration in a particular article (e.g., apprenticeship or synergy). One of the tasks in Sarawgi et al. (2011) concerned predicting gender in scientific writing, but they use a corpus of only ten “highly established” authors and make the prediction using twenty papers for each. Finally, Dale and Kilgarriff (2010) initiated a shared task on automatic editing of scientific papers written by non-native speakers, with the objective of developing “tools which can help non-native speakers of English (NNSs) (and maybe some native ones) write academic English prose of the kind that helps a paper get accepted.” Lexical and pragmatic choices in academic writing have also been analyzed within the applied linguis"
N12-1033,C08-1109,0,0.0119864,"sk of predicting whether a scientific paper is written by a native English speaker (NES) or non-native speaker (NNS). Prior work has mostly made this prediction in learner corpora (Koppel et al., 2005; Tsur and Rappoport, 2007; Wong and Dras, 2011), although there have been attempts in elicited speech transcripts (Tomokiyo and Jones, 2001) and e-mail (Estival et al., 2007). There has also been a large body of work on correcting errors in non-native writing, with a specific focus on difficulties in preposition and article usage (Han et al., 2006; Chodorow et al., 2007; Felice and Pulman, 2007; Tetreault and Chodorow, 2008; Gamon, 2010). We annotate papers using two pieces of associated meta-data: (1) author first names and (2) countries of affiliation. We manually marked each country for whether English is predominantly spoken there. We 329 then built a list of common first names of English speakers via the top 150 male and female names from the U.S. census.3 If the first author of a paper has an English first name and English-speakingcountry affiliation, we mark as NES.4 If none of the authors have an English first name nor an Englishspeaking-country affiliation, we mark as NNS. We use this rule to label our"
N12-1033,J02-4002,0,0.0214971,"alysis of scholarly literature; citation analysis is a well-known bibliometric approach for ranking authors and papers (Borgman and Furner, 2001). Bibliometry and stylometry can share goals but differ in techniques. For example, in a work questioning the blindness of double-blind reviewing, Hill and Provost (2003) predict author identities. They ignore the article body and instead consider (a) potential self-citations and (b) similarity between the article’s citation list and the citation lists of known papers. Radev et al. (2009a) perform a bibliometric analysis of computational linguistics. Teufel and Moens (2002) and Qazvinian and Radev (2008) summarize scientific articles, the latter by automatically finding and filtering sentences in other papers that cite the target article. Our system does not consider citations; it is most 328 similar to work that uses raw article text. Hall et al. (2008) build per-year topic models over scientific literature to track the evolution of scientific ideas. Gerrish and Blei (2010) assess the influence of individual articles by modeling their impact on the content of future papers. Yogatama et al. (2011) predict whether a paper will be cited based on both its content a"
N12-1033,N01-1031,0,0.794482,"using a Strict rule. While our approaches for automatically-assigning labels can be coarse, they allow us to scale our analysis to a realistic crosssection of academic papers, letting us discover some interesting trends. 4.1 NativeL: Native vs. Non-Native English We introduce the task of predicting whether a scientific paper is written by a native English speaker (NES) or non-native speaker (NNS). Prior work has mostly made this prediction in learner corpora (Koppel et al., 2005; Tsur and Rappoport, 2007; Wong and Dras, 2011), although there have been attempts in elicited speech transcripts (Tomokiyo and Jones, 2001) and e-mail (Estival et al., 2007). There has also been a large body of work on correcting errors in non-native writing, with a specific focus on difficulties in preposition and article usage (Han et al., 2006; Chodorow et al., 2007; Felice and Pulman, 2007; Tetreault and Chodorow, 2008; Gamon, 2010). We annotate papers using two pieces of associated meta-data: (1) author first names and (2) countries of affiliation. We manually marked each country for whether English is predominantly spoken there. We 329 then built a list of common first names of English speakers via the top 150 male and fema"
N12-1033,W07-0602,0,0.12042,", which forcibly assigns every paper in the training period to some class (Table 1). All test papers are annotated using a Strict rule. While our approaches for automatically-assigning labels can be coarse, they allow us to scale our analysis to a realistic crosssection of academic papers, letting us discover some interesting trends. 4.1 NativeL: Native vs. Non-Native English We introduce the task of predicting whether a scientific paper is written by a native English speaker (NES) or non-native speaker (NNS). Prior work has mostly made this prediction in learner corpora (Koppel et al., 2005; Tsur and Rappoport, 2007; Wong and Dras, 2011), although there have been attempts in elicited speech transcripts (Tomokiyo and Jones, 2001) and e-mail (Estival et al., 2007). There has also been a large body of work on correcting errors in non-native writing, with a specific focus on difficulties in preposition and article usage (Han et al., 2006; Chodorow et al., 2007; Felice and Pulman, 2007; Tetreault and Chodorow, 2008; Gamon, 2010). We annotate papers using two pieces of associated meta-data: (1) author first names and (2) countries of affiliation. We manually marked each country for whether English is predomina"
N12-1033,D11-1148,0,0.513421,"very paper in the training period to some class (Table 1). All test papers are annotated using a Strict rule. While our approaches for automatically-assigning labels can be coarse, they allow us to scale our analysis to a realistic crosssection of academic papers, letting us discover some interesting trends. 4.1 NativeL: Native vs. Non-Native English We introduce the task of predicting whether a scientific paper is written by a native English speaker (NES) or non-native speaker (NNS). Prior work has mostly made this prediction in learner corpora (Koppel et al., 2005; Tsur and Rappoport, 2007; Wong and Dras, 2011), although there have been attempts in elicited speech transcripts (Tomokiyo and Jones, 2001) and e-mail (Estival et al., 2007). There has also been a large body of work on correcting errors in non-native writing, with a specific focus on difficulties in preposition and article usage (Han et al., 2006; Chodorow et al., 2007; Felice and Pulman, 2007; Tetreault and Chodorow, 2008; Gamon, 2010). We annotate papers using two pieces of associated meta-data: (1) author first names and (2) countries of affiliation. We manually marked each country for whether English is predominantly spoken there. We"
N12-1033,D11-1055,0,0.0516188,"Missing"
N12-1033,J00-4001,0,\N,Missing
N16-3013,W03-1004,0,0.0785871,"tuned using the parallel data described therein as well as the Newsela corpus (Xu et al., 2015). There is no specialized grammar for these models; instead, the parameters were tuned to choose appropriate paraphrases from the PPDB. Sample output generated with these models is shown in Table 1. 3.2 Hand-derived weights To configure the general-purpose model, which generates paraphrases for no specific task, we examined the output of 100 sentences randomly selected from each of three different domains: newswire (WSJ 0– 1 (Marcus et al., 1993)), “simple” English (the Britannica Elementary corpus (Barzilay and Elhadad, 2003)), and general text (the WaCky corpus (Baroni et al., 2009)). We systematically varied the weights of the Gigaword LM and the PPDB 2.0 score features and selected values that yielded the best output as judged by the authors. The parameters selected for the generic language packs are weightlm = 10 and weightppdb2 = 15, with all other weights are set to zero. Example output is shown in Table 1. 4 User customization The language packs include configuration files with pre-determined weights that can be used on their own or as a jumping-off point for custom configurations. There are weights for eac"
N16-3013,ganitkevitch-callison-burch-2014-multilingual,1,0.907711,"Missing"
N16-3013,D11-1108,1,0.889367,"Missing"
N16-3013,W12-3134,1,0.836518,"1 The components of the language packs are described below. 1 http://joshua-decoder.com/ language-packs/paraphrase/ 62 Proceedings of NAACL-HLT 2016 (Demonstrations), pages 62–66, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics Grammar Our approach to sentential paraphrasing is analogous to machine translation. As a translation grammar, we use PPDB 2.0, which contains 170-million lexical, phrasal, and syntactic paraphrases (Pavlick et al., 2015). Each language pack contains a PPDB grammar that has been packed into a binary form for faster computation (Ganitkevitch et al., 2012), and users can select which size grammar to use. The rules present in each grammar are determined by the PPDB 2.0 score, which indicates the paraphrase quality (as given by a supervised regression model) and correlates strongly with human judgments of paraphrase appropriateness (Pavlick et al., 2015). Grammars of different sizes are created by changing the paraphrase score thresholds; larger grammars therefore contain a wider diversity of paraphrases, but with lower confidences. Features Each paraphrase in PPDB 2.0 contains 44 features, described in Ganitkevitch and CallisonBurch (2014) and P"
N16-3013,N13-1092,1,0.877691,"Missing"
N16-3013,J10-3003,0,0.0799258,"e available at the language pack home page: http://joshua-decoder.com/ language-packs/paraphrase/ phrase grammar for sentence compression (Ganitkevitch et al., 2011) and sentence simplification (Xu et al., 2016), both of which developed custom metrics and task-specific features. Various other MT approaches have been used for generating sentence simplifications, however none of these used a general-purpose paraphrase grammar (Narayan and Gardent, 2014; Wubben et al., 2012, among others). Another application of sentential paraphrases is to expand multiple reference sets for machine translation (Madnani and Dorr, 2010). PPDB has been used for many tasks, including recognizing textual entailment, question generation, and measuring semantic similarity. These language packs were inspired by the foreign language packs released with Joshua 6 (Post et al., 2015). 6 We have presented a black box for generating sentential paraphrases: PPDB language packs. The language packs include everything necessary for generation, so that they can be downloaded and invoked with a single command. This toolkit can be used for Related work Previous work has applied machine translation techniques to monolingual sentence rewriting t"
N16-3013,J93-2004,0,0.053127,"d in Xu et al. (2016), and is optimized to the SARI metric. The system was tuned using the parallel data described therein as well as the Newsela corpus (Xu et al., 2015). There is no specialized grammar for these models; instead, the parameters were tuned to choose appropriate paraphrases from the PPDB. Sample output generated with these models is shown in Table 1. 3.2 Hand-derived weights To configure the general-purpose model, which generates paraphrases for no specific task, we examined the output of 100 sentences randomly selected from each of three different domains: newswire (WSJ 0– 1 (Marcus et al., 1993)), “simple” English (the Britannica Elementary corpus (Barzilay and Elhadad, 2003)), and general text (the WaCky corpus (Baroni et al., 2009)). We systematically varied the weights of the Gigaword LM and the PPDB 2.0 score features and selected values that yielded the best output as judged by the authors. The parameters selected for the generic language packs are weightlm = 10 and weightppdb2 = 15, with all other weights are set to zero. Example output is shown in Table 1. 4 User customization The language packs include configuration files with pre-determined weights that can be used on their"
N16-3013,P14-1041,0,0.0213724,"file to paraphrase a large batch of sentences. Detailed instructions for using the tool and shell scripts, as well as a detailed description of the configuration file, are available at the language pack home page: http://joshua-decoder.com/ language-packs/paraphrase/ phrase grammar for sentence compression (Ganitkevitch et al., 2011) and sentence simplification (Xu et al., 2016), both of which developed custom metrics and task-specific features. Various other MT approaches have been used for generating sentence simplifications, however none of these used a general-purpose paraphrase grammar (Narayan and Gardent, 2014; Wubben et al., 2012, among others). Another application of sentential paraphrases is to expand multiple reference sets for machine translation (Madnani and Dorr, 2010). PPDB has been used for many tasks, including recognizing textual entailment, question generation, and measuring semantic similarity. These language packs were inspired by the foreign language packs released with Joshua 6 (Post et al., 2015). 6 We have presented a black box for generating sentential paraphrases: PPDB language packs. The language packs include everything necessary for generation, so that they can be downloaded"
N16-3013,P15-2070,1,0.915574,"Missing"
N16-3013,2006.amta-papers.25,0,0.0549992,"udes an input text box (one sentence Figure 1: A screen shot of the web tool. The number to the right of each output sentence is the TER. at a time), and slider bars to change the weights of any of the features used for decoding. Since this model has not been manually evaluated, we favor precision over recall and maintain a relatively conservative level of paraphrasing. The user is shown the top 10 outputs, as ranked by the sentence score. For each output sentence, we report the Translation Edit Rate (TER), which is the number of changes needed to transform the output sentence into the input (Snover et al., 2006). This tool can be used to demonstrate and test a model or to hand-tune the model in order to determine the parameters for a configuration file to paraphrase a large batch of sentences. Detailed instructions for using the tool and shell scripts, as well as a detailed description of the configuration file, are available at the language pack home page: http://joshua-decoder.com/ language-packs/paraphrase/ phrase grammar for sentence compression (Ganitkevitch et al., 2011) and sentence simplification (Xu et al., 2016), both of which developed custom metrics and task-specific features. Various oth"
N16-3013,P12-1107,0,0.0647131,"Missing"
N16-3013,Q15-1021,1,0.83388,"distinguished only 63 by the different weight vectors, and are selected by point the Joshua invocation script to the corresponding configuration file. 3.1 Tuned models We include two models that were tuned for (1) sentence compression and (2) simplification. The compression model is based on the work of Ganitkevitch et al. (2011), and uses the same features, tuning data, and objective function, P R E´ CIS. The simplification model is described in Xu et al. (2016), and is optimized to the SARI metric. The system was tuned using the parallel data described therein as well as the Newsela corpus (Xu et al., 2015). There is no specialized grammar for these models; instead, the parameters were tuned to choose appropriate paraphrases from the PPDB. Sample output generated with these models is shown in Table 1. 3.2 Hand-derived weights To configure the general-purpose model, which generates paraphrases for no specific task, we examined the output of 100 sentences randomly selected from each of three different domains: newswire (WSJ 0– 1 (Marcus et al., 1993)), “simple” English (the Britannica Elementary corpus (Barzilay and Elhadad, 2003)), and general text (the WaCky corpus (Baroni et al., 2009)). We sys"
N16-3013,Q16-1029,1,0.937774,"ere are tuned models for (1) sentence compression, (2) text simplification, and (3) a general-purpose model with handtuned weights. These models are distinguished only 63 by the different weight vectors, and are selected by point the Joshua invocation script to the corresponding configuration file. 3.1 Tuned models We include two models that were tuned for (1) sentence compression and (2) simplification. The compression model is based on the work of Ganitkevitch et al. (2011), and uses the same features, tuning data, and objective function, P R E´ CIS. The simplification model is described in Xu et al. (2016), and is optimized to the SARI metric. The system was tuned using the parallel data described therein as well as the Newsela corpus (Xu et al., 2015). There is no specialized grammar for these models; instead, the parameters were tuned to choose appropriate paraphrases from the PPDB. Sample output generated with these models is shown in Table 1. 3.2 Hand-derived weights To configure the general-purpose model, which generates paraphrases for no specific task, we examined the output of 100 sentences randomly selected from each of three different domains: newswire (WSJ 0– 1 (Marcus et al., 1993))"
N18-1119,D17-1098,0,0.373576,"and hat die Absicht, eine Mauer zu bauen. “No one has the intention, a wall to build.” The end-to-end nature of neural machine translation (NMT) removes many ways of manually guiding the translation process that were available in older paradigms. Recent work, however, has introduced a new capability: lexically constrained or guided decoding, a modification to beam search that forces the inclusion of pre-specified words and phrases in the output. However, while theoretically sound, existing approaches have computational complexities that are either linear (Hokamp and Liu, 2017) or exponential (Anderson et al., 2017) in the number of constraints. We present an algorithm for lexically constrained decoding with a complexity of O(1) in the number of constraints. We demonstrate the algorithm’s remarkable ability to properly place these constraints, and use it to explore the shaky relationship between model and BLEU scores. Our implementation is available as part of S OCKEYE. 1 “errichten” “Keiner” “Keiner” ""errichten” Niemand hat die Absicht, eine Mauer zu errichten. “No one has the intention, a wall to construct.” Keiner hat die Absicht, eine Mauer zu bauen. “No one has the intention, a wall to build.” Keine"
N18-1119,D16-1162,0,0.355416,"on One appeal of the phrase-based statistical approach to machine translation (Koehn et al., 2003) was that it provided control over system output. For example, it was relatively easy to incorporate domain-specific dictionaries, or to force a translation choice for certain words. These kinds of interventions were useful in a range of settings, including interactive machine translation or domain adaptation. In the new paradigm of neural machine translation (NMT), these kinds of manual interventions are much more difficult, and a lot of time has been spent investigating how to restore them (cf. Arthur et al. (2016)). At the same time, NMT has also provided new capabilities. One interesting recent innovation is lexically constrained decoding, a modification to beam search that allows the user to specify words and phrases that must appear in the system output (Figure 1). Two algorithms have been proposed for this: grid beam search (Hokamp and Liu, 2017, GBS) and constrained beam search (Anderson et al., 2017, CBS). These papers showed that these algorithms do a good job automatically placing constraints and improving results in tasks such as simulated post-editing, domain adaptation, and caption generatio"
N18-1119,P17-1141,0,0.334689,"the intention of building a wall. Niemand hat die Absicht, eine Mauer zu bauen. “No one has the intention, a wall to build.” The end-to-end nature of neural machine translation (NMT) removes many ways of manually guiding the translation process that were available in older paradigms. Recent work, however, has introduced a new capability: lexically constrained or guided decoding, a modification to beam search that forces the inclusion of pre-specified words and phrases in the output. However, while theoretically sound, existing approaches have computational complexities that are either linear (Hokamp and Liu, 2017) or exponential (Anderson et al., 2017) in the number of constraints. We present an algorithm for lexically constrained decoding with a complexity of O(1) in the number of constraints. We demonstrate the algorithm’s remarkable ability to properly place these constraints, and use it to explore the shaky relationship between model and BLEU scores. Our implementation is available as part of S OCKEYE. 1 “errichten” “Keiner” “Keiner” ""errichten” Niemand hat die Absicht, eine Mauer zu errichten. “No one has the intention, a wall to construct.” Keiner hat die Absicht, eine Mauer zu bauen. “No one has"
N18-1119,D07-1103,0,0.0934441,"Missing"
N18-1119,2016.amta-researchers.9,0,0.041654,"imum timestep, but then fills the lower beam with garbage until forced to stop. 7 Related Work Hokamp and Liu (2017) was novel in that it allowed the specification of arbitrary target-side words as hard constraints, implemented entirely as a restructuring of beam search, and without reference to the source. A related approach was that of Anderson et al. (2017), who extended beam search with a finite state machine whose states marked completed subsets of the set of constraints, at an exponential cost in the number of constraints. Lexically-constrained decoding also generalizes prefix decoding (Knowles and Koehn, 2016; Wuebker et al., 2016), since the hsi symbol can easily be included as the first word of a constraint. Our work here has not explored where to get lexical constraints, but considering that question naturally brings to mind attempts to improve NMT by using lexicons and phrase tables (Arthur et al., 2016; Tang et al., 2016). Finally, another approach which shares the hard-decision made by lexically constrained decoding is the placeholder approach (Crego et al., 2016), wherein identifiable elements in the input are transformed to masks during preprocessing, and then replaced with their original"
N18-1119,W17-3204,0,0.147066,"ide confidence that DBA is intelligently placing the constraints. Reference Aversion The inference procedure in S OCKEYE maximizes the length-normalized version of the sentence’s log probability. While there is no explicit training towards the metric, BLEU, modeling in machine translation assumes that better model scores correlate with better BLEU scores. However, a general repeated observation from the NMT literature is the disconnect between model score and BLEU score. For example, work has shown that opening up the beam to let the decoder find better hypotheses results in lower BLEU score (Koehn and Knowles, 2017), even as the model score rises. The phenomenon is not well understood, but it seems that NMT models have learned to travel a path straight towards their goal; as soon as they get off this path, they get lost, and can no longer function (Ott et al., 2018). Another way to look at this problem is to ask what the neural model thinks of the references. Scoring against complete references is easy with NMT (Sennrich, 2017), but lexically-constrained decoding allows us to investigate this in finergrained detail by including just portions of the references. We observe that forcing the decoder to inclu"
N18-1119,N03-1017,0,0.0216164,"Missing"
N18-1119,L16-1147,0,0.0358489,"Missing"
N18-1119,P14-5010,0,0.00574844,"Missing"
N18-1119,E17-2060,0,0.0163954,"disconnect between model score and BLEU score. For example, work has shown that opening up the beam to let the decoder find better hypotheses results in lower BLEU score (Koehn and Knowles, 2017), even as the model score rises. The phenomenon is not well understood, but it seems that NMT models have learned to travel a path straight towards their goal; as soon as they get off this path, they get lost, and can no longer function (Ott et al., 2018). Another way to look at this problem is to ask what the neural model thinks of the references. Scoring against complete references is easy with NMT (Sennrich, 2017), but lexically-constrained decoding allows us to investigate this in finergrained detail by including just portions of the references. We observe that forcing the decoder to include even a single word from the reference imposes a cost in model score that is inversely 1 3 24.5 25.1 25.3 24.7 23.7 33.5 5 24.5 25.2 25.6 24.9 23.9 33.5 10 24.4 25.6 26.1 25.7 24.6 34.0 20 24.5 25.5 26.7 26.9 26.0 35.0 30 24.4 25.3 26.4 27.2 26.9 35.9 Table 3: BLEU scores decoding with a beam size of 10. Runtimes for unpruned systems (column 0) are nearly twice those of the other columns. But it is only at large th"
N18-1119,P16-1162,0,0.681699,". Inputs: max output length N , beam size k. Output: highest-scoring hypothesis. 1: 2: Table 1: Complexity of decoding (sentence length N , beam size k, and constraint count C) with target-side constraints under various approaches. banks (similar in spirit to the grouping of hypotheses into stacks for phrase-based decoding (Koehn et al., 2003)) and dynamically dividing a fixed-size beam across these banks at each time step. As a result, the algorithm scales easily to large constraint sets that can be created when words and phrases are expanded, for example, by sub-word processing such as BPE (Sennrich et al., 2016). We compare it to GBS and demonstrate empirically that it is significantly faster, making constrained decoding with an arbitrary number of constraints feasible with GPU-based inference. We also use the algorithm to study beam search interactions between model and metric scores, beam size, and pruning. 2 Beam Search and Grid Beam Search Inference in statistical machine translation seeks to find the output sequence, yˆ, that maximizes the probability of a function parameterized by a model, θ, and an input sequence, x: yˆ = argmaxy∈Y pθ (y |x) The space of possible translations, Y, is the set of"
N18-1119,P16-1007,0,0.0556527,"ills the lower beam with garbage until forced to stop. 7 Related Work Hokamp and Liu (2017) was novel in that it allowed the specification of arbitrary target-side words as hard constraints, implemented entirely as a restructuring of beam search, and without reference to the source. A related approach was that of Anderson et al. (2017), who extended beam search with a finite state machine whose states marked completed subsets of the set of constraints, at an exponential cost in the number of constraints. Lexically-constrained decoding also generalizes prefix decoding (Knowles and Koehn, 2016; Wuebker et al., 2016), since the hsi symbol can easily be included as the first word of a constraint. Our work here has not explored where to get lexical constraints, but considering that question naturally brings to mind attempts to improve NMT by using lexicons and phrase tables (Arthur et al., 2016; Tang et al., 2016). Finally, another approach which shares the hard-decision made by lexically constrained decoding is the placeholder approach (Crego et al., 2016), wherein identifiable elements in the input are transformed to masks during preprocessing, and then replaced with their original sourcelanguage strings"
N19-1090,I17-2075,0,0.0203472,"ntation has been used to improve performance and robustness in deep neural models. In NMT, the most common approach is back-translation, where monolingual text in the target language is translated to create synthetic source sentences (Sennrich et al., 2016). Variants of back-translation target specific words with high prediction loss (Fadaee and Monz, 2018), employed sampling to increase diversity (Edunov et al., 2018), replace rare words (Fadaee et al., 2017), or replace at random (Wang et al., 2018). Automatic data generation has also been successfully used for community question answering (Chen and Bunescu, 2017), semantic parsing (Jia and Liang, 2016), and task-oriented dialogue (Hou et al., 2018) by generating new data from the training dataset. In contrast, our model is trained on a much larger external corpus and is fixed, independent of the task. Kobayashi (2018) utilized a pretrained language model for automated data augmentation, though they only consider word-level rewrites and encourage label-preservation, while we paraphrase whole sentences with lexical constraints, independent of a gold label. Most similar to our experiments, Iyyer et al. (2018) explored syntactic paraphrasing for augmentat"
N19-1090,D18-1316,0,0.0513884,"Missing"
N19-1090,D17-1070,0,0.0136465,"ns. Given both the numerical boost seen by aggregation and the above examples, we hypothesize that the rewriter does not frequently change entailment semantics. Because the semantics remain similar, and because the paraphrases were gener5.2 Question Answering We apply our paraphrastic rewriter to the task of question answer sentence selection to see if augmenting with paraphrases leads to improvements. The task is defined as follows: Given a question q and a set of candidate sentences {ci }, select the candidates which answer q. Model We adapt a popular neural architecture for NLI, InferSent (Conneau et al., 2017), to our QA sentence selection task. In InferSent, the questions and answers (originally the premises and hypotheses) are embedded using an uncontextualized word embedding (e.g. GloVe), which we also experiment with ELMo (Peters et al., 2018) to incorporate recent advancements in large-scale contextualized pre-training. Bidirectional LSTMs (Graves and Schmidhuber, 2005) are run atop of these contextualized embeddings and a maxpooling layer is used to generate a feature vector for both the question and the answer. Following various matching methods (Mou et al., 2016) and a multi-layer feed-forw"
N19-1090,D17-1098,0,0.276218,"ster exploration of constraint strategies: we illustrate this via data augmentation experiments with a monolingual rewriter applied to the tasks of natural language inference, question answering and machine translation, showing improvements in all three. 1 Introduction For many natural language generation tasks, we often know word(s) that should (or should not) be in the output sentence. Examples include terminology databases in Machine Translation (MT) (Hokamp and Liu, 2017), names (and generic responses) in dialogue generation (Li et al., 2016; Gu et al., 2016), objects in image captioning (Anderson et al., 2017), and facts in abstractive summarization (See et al., 2017). One approach to enforce hard lexical constraints in the output is to modify the inference procedure to enforce their presence directly (Hokamp and Liu, 2017). These constraints could be either positive (a word must appear in the output) or negative (a word must be avoided). While negative constraints could be easily enforced by preventing hypotheses with prohibited tokens from entering the beam, placing positive constraints in natural and meaningful ways is less straightforward. We improve upon previous work by vectorizing the dynami"
N19-1090,D18-1045,0,0.0242484,"iter; nor demonstrate its utility on NLP tasks. • Monolingual rewriting constraint heuristics for automatic data augmentation leading to improvements on NLI / QA / MT. 2 Data augmentation Data augmentation has been used to improve performance and robustness in deep neural models. In NMT, the most common approach is back-translation, where monolingual text in the target language is translated to create synthetic source sentences (Sennrich et al., 2016). Variants of back-translation target specific words with high prediction loss (Fadaee and Monz, 2018), employed sampling to increase diversity (Edunov et al., 2018), replace rare words (Fadaee et al., 2017), or replace at random (Wang et al., 2018). Automatic data generation has also been successfully used for community question answering (Chen and Bunescu, 2017), semantic parsing (Jia and Liang, 2016), and task-oriented dialogue (Hou et al., 2018) by generating new data from the training dataset. In contrast, our model is trained on a much larger external corpus and is fixed, independent of the task. Kobayashi (2018) utilized a pretrained language model for automated data augmentation, though they only consider word-level rewrites and encourage label-pr"
N19-1090,P17-2090,0,0.0246626,"asks. • Monolingual rewriting constraint heuristics for automatic data augmentation leading to improvements on NLI / QA / MT. 2 Data augmentation Data augmentation has been used to improve performance and robustness in deep neural models. In NMT, the most common approach is back-translation, where monolingual text in the target language is translated to create synthetic source sentences (Sennrich et al., 2016). Variants of back-translation target specific words with high prediction loss (Fadaee and Monz, 2018), employed sampling to increase diversity (Edunov et al., 2018), replace rare words (Fadaee et al., 2017), or replace at random (Wang et al., 2018). Automatic data generation has also been successfully used for community question answering (Chen and Bunescu, 2017), semantic parsing (Jia and Liang, 2016), and task-oriented dialogue (Hou et al., 2018) by generating new data from the training dataset. In contrast, our model is trained on a much larger external corpus and is fixed, independent of the task. Kobayashi (2018) utilized a pretrained language model for automated data augmentation, though they only consider word-level rewrites and encourage label-preservation, while we paraphrase whole sent"
N19-1090,D18-1040,0,0.0196406,"e-wild sentences; explore more sophisticated versions of the rewriter; nor demonstrate its utility on NLP tasks. • Monolingual rewriting constraint heuristics for automatic data augmentation leading to improvements on NLI / QA / MT. 2 Data augmentation Data augmentation has been used to improve performance and robustness in deep neural models. In NMT, the most common approach is back-translation, where monolingual text in the target language is translated to create synthetic source sentences (Sennrich et al., 2016). Variants of back-translation target specific words with high prediction loss (Fadaee and Monz, 2018), employed sampling to increase diversity (Edunov et al., 2018), replace rare words (Fadaee et al., 2017), or replace at random (Wang et al., 2018). Automatic data generation has also been successfully used for community question answering (Chen and Bunescu, 2017), semantic parsing (Jia and Liang, 2016), and task-oriented dialogue (Hou et al., 2018) by generating new data from the training dataset. In contrast, our model is trained on a much larger external corpus and is fixed, independent of the task. Kobayashi (2018) utilized a pretrained language model for automated data augmentation, thoug"
N19-1090,N13-1092,1,0.879148,"Missing"
N19-1090,P16-1154,0,0.0178036,"ositive constraints. Faster decoding enables faster exploration of constraint strategies: we illustrate this via data augmentation experiments with a monolingual rewriter applied to the tasks of natural language inference, question answering and machine translation, showing improvements in all three. 1 Introduction For many natural language generation tasks, we often know word(s) that should (or should not) be in the output sentence. Examples include terminology databases in Machine Translation (MT) (Hokamp and Liu, 2017), names (and generic responses) in dialogue generation (Li et al., 2016; Gu et al., 2016), objects in image captioning (Anderson et al., 2017), and facts in abstractive summarization (See et al., 2017). One approach to enforce hard lexical constraints in the output is to modify the inference procedure to enforce their presence directly (Hokamp and Liu, 2017). These constraints could be either positive (a word must appear in the output) or negative (a word must be avoided). While negative constraints could be easily enforced by preventing hypotheses with prohibited tokens from entering the beam, placing positive constraints in natural and meaningful ways is less straightforward. We"
N19-1090,E17-3017,0,0.025193,"tic pairs in PARA BANK originated from CzEng5 that: (1) have a regression score over 0.50; (2) only consist of ASCII characters after punctuation normalization; and (3) have a reference/paraphrase token Jaccard index between 0.25 and 0.65. We retain 141,381,887 paraphrastic pairs, out of over 220 million, as training data after applying these filters. To ensure output quality, we only use back-translated paraphrases as source. PARA BANK is a real-cased resource. We mark all words that have first-character capitalization and convert them to lowercase. The marking is Evaluation We use S OCKEYE (Hieber et al., 2017)4 for our evaluations. We trained a 6-layer German–English Transformer using the default settings on the WMT’18 training data and the newstest2018 test set for evaluation (Bojar et al., 2018). Following Post and Vilar (2018), we compare decoding results in an unconstrained setting and with two sets of positive constraints: “rand3”, which selects 3 random words from the reference, and “phr4”, which selects a single 4-word phrase. We report decoding speed (in sentences per second) and BLEU score (Papineni et al., 2002), as measured by SacreBLEU (Post, 2018). The results are 4 Improved Monolingua"
N19-1090,P17-1141,0,0.345145,"ive translation and domain adaptation. Translation applications handling large amounts of data will clearly benefit from improvements in speed: the same is true for large-scale data augmentation via rewriting. In this case, a practitioner will ideally explore various task-specific rewriting strategies that may lead to improvements as observed during development, and then incorporate the best strategy into a test-final model. Recently, sentential paraphrasing gained the ability to enforce lexical constraints (Hu et al., 2019), but constrained decoding was still too inefficient to be practical (Hokamp and Liu, 2017) at a large scale. Even with the approach described by Post and Vilar, exploring the space of possible rewriting strategies on a taskspecific basis may be overly time consuming: our performance improvements to their algorithm lowers the barrier of entry, where one may more practically experiment with various strategies during development. To illustrate our point, we build an improved monolingual sentential rewriter that can be conditioned on arbitrary positive and negative lexical constraints and use this to augment data for three external NLP tasks with different strategies: Natural Language"
N19-1090,C18-1105,0,0.0135787,"e most common approach is back-translation, where monolingual text in the target language is translated to create synthetic source sentences (Sennrich et al., 2016). Variants of back-translation target specific words with high prediction loss (Fadaee and Monz, 2018), employed sampling to increase diversity (Edunov et al., 2018), replace rare words (Fadaee et al., 2017), or replace at random (Wang et al., 2018). Automatic data generation has also been successfully used for community question answering (Chen and Bunescu, 2017), semantic parsing (Jia and Liang, 2016), and task-oriented dialogue (Hou et al., 2018) by generating new data from the training dataset. In contrast, our model is trained on a much larger external corpus and is fixed, independent of the task. Kobayashi (2018) utilized a pretrained language model for automated data augmentation, though they only consider word-level rewrites and encourage label-preservation, while we paraphrase whole sentences with lexical constraints, independent of a gold label. Most similar to our experiments, Iyyer et al. (2018) explored syntactic paraphrasing for augmentation in sentiment and NLI tasks, extending prior work on PARA NMT. Background Constraine"
N19-1090,2015.iwslt-evaluation.11,0,0.0433639,"Missing"
N19-1090,E17-1083,0,0.0375255,"hs in dependency trees. Weisman et al. (2012) explored learning inference relations between verbs in broader scopes (document or corpus level). PPDB (Ganitkevitch et al., 2013) constructs paraphrase pairs by linking words or phrases that share the same translation in another language. PARA NMT (Wieting and Gimpel, 2018) and PARA BANK (Hu et al., 2019) used back-translation to build a large paraphrase collection from bilingual corpora. For arbitrary sentence rewriting, Napoles et al. (2016) used statistical machine translation in tandem with PPDB as a black box monolingual sentential rewriter. Mallinson et al. (2017) used a series of NMT model pairs to perform backtranslations for monolingual paraphrasing. A similar approach was adopted by PARA NMT to create a large paraphrase collection, which is used to train a monolingual sentence rewriter for canonicalization. PARA BANK (Hu et al., 2019) extends 1 3 Improved Constrained Decoding Lexically-constrained decoding is a modification to beam search that yields decoder outputs honoring user-supplied constraints. These constraints can be provided in the form of: positive constraints, which specify that certain tokens or token sequences must be present in the o"
N19-1090,N18-1170,0,0.0440982,"ly used for community question answering (Chen and Bunescu, 2017), semantic parsing (Jia and Liang, 2016), and task-oriented dialogue (Hou et al., 2018) by generating new data from the training dataset. In contrast, our model is trained on a much larger external corpus and is fixed, independent of the task. Kobayashi (2018) utilized a pretrained language model for automated data augmentation, though they only consider word-level rewrites and encourage label-preservation, while we paraphrase whole sentences with lexical constraints, independent of a gold label. Most similar to our experiments, Iyyer et al. (2018) explored syntactic paraphrasing for augmentation in sentiment and NLI tasks, extending prior work on PARA NMT. Background Constrained decoding Prior work explored methods to apply lexical constraints to a Neural Machine Translation (NMT) decoder (Hokamp and Liu, 2017; Anderson et al., 2017). However, most of these methods are slow and impractical as they change beam sizes at different time steps, which breaks the optimized computation graph. Post and Vilar (2018) proposed a means of dynamically allocating the slots in a fixed-size beam to ensure that even progress was made in meeting an arbit"
N19-1090,P16-2022,0,0.0214771,"ure for NLI, InferSent (Conneau et al., 2017), to our QA sentence selection task. In InferSent, the questions and answers (originally the premises and hypotheses) are embedded using an uncontextualized word embedding (e.g. GloVe), which we also experiment with ELMo (Peters et al., 2018) to incorporate recent advancements in large-scale contextualized pre-training. Bidirectional LSTMs (Graves and Schmidhuber, 2005) are run atop of these contextualized embeddings and a maxpooling layer is used to generate a feature vector for both the question and the answer. Following various matching methods (Mou et al., 2016) and a multi-layer feed-forward neural network, the model produces a final score. We train the system following the method proposed by Rao et al. (2016), utilizing a ranking loss (Weston and Watkins, 1999) that contrasts positive answers against negative ones. 846 Paraphrase Generation We augment each answer candidate sentence with exactly 1 paraphrase in the dataset using the following heuristics: (1) named entities shared between a specific answer and its corresponding question are retained as positive constraints; (2) correct answer spans are retained as positive constraints; (3) words with"
N19-1090,P16-1002,0,0.0135507,"e and robustness in deep neural models. In NMT, the most common approach is back-translation, where monolingual text in the target language is translated to create synthetic source sentences (Sennrich et al., 2016). Variants of back-translation target specific words with high prediction loss (Fadaee and Monz, 2018), employed sampling to increase diversity (Edunov et al., 2018), replace rare words (Fadaee et al., 2017), or replace at random (Wang et al., 2018). Automatic data generation has also been successfully used for community question answering (Chen and Bunescu, 2017), semantic parsing (Jia and Liang, 2016), and task-oriented dialogue (Hou et al., 2018) by generating new data from the training dataset. In contrast, our model is trained on a much larger external corpus and is fixed, independent of the task. Kobayashi (2018) utilized a pretrained language model for automated data augmentation, though they only consider word-level rewrites and encourage label-preservation, while we paraphrase whole sentences with lexical constraints, independent of a gold label. Most similar to our experiments, Iyyer et al. (2018) explored syntactic paraphrasing for augmentation in sentiment and NLI tasks, extendin"
N19-1090,N16-3013,1,0.86673,"astic expressions through existing corpora. For example, DIRT (Lin and Pantel, 2001) extracts paraphrastic expressions from paths in dependency trees. Weisman et al. (2012) explored learning inference relations between verbs in broader scopes (document or corpus level). PPDB (Ganitkevitch et al., 2013) constructs paraphrase pairs by linking words or phrases that share the same translation in another language. PARA NMT (Wieting and Gimpel, 2018) and PARA BANK (Hu et al., 2019) used back-translation to build a large paraphrase collection from bilingual corpora. For arbitrary sentence rewriting, Napoles et al. (2016) used statistical machine translation in tandem with PPDB as a black box monolingual sentential rewriter. Mallinson et al. (2017) used a series of NMT model pairs to perform backtranslations for monolingual paraphrasing. A similar approach was adopted by PARA NMT to create a large paraphrase collection, which is used to train a monolingual sentence rewriter for canonicalization. PARA BANK (Hu et al., 2019) extends 1 3 Improved Constrained Decoding Lexically-constrained decoding is a modification to beam search that yields decoder outputs honoring user-supplied constraints. These constraints ca"
N19-1090,P02-1040,0,0.104681,"ation and convert them to lowercase. The marking is Evaluation We use S OCKEYE (Hieber et al., 2017)4 for our evaluations. We trained a 6-layer German–English Transformer using the default settings on the WMT’18 training data and the newstest2018 test set for evaluation (Bojar et al., 2018). Following Post and Vilar (2018), we compare decoding results in an unconstrained setting and with two sets of positive constraints: “rand3”, which selects 3 random words from the reference, and “phr4”, which selects a single 4-word phrase. We report decoding speed (in sentences per second) and BLEU score (Papineni et al., 2002), as measured by SacreBLEU (Post, 2018). The results are 4 Improved Monolingual Rewriter 5 PARA BANK generated paraphrases from two large bilingual corpora, CzEng (Bojar et al., 2016a) and GigaFrEn (Callison-Burch et al., 2009). We picked paraphrases from only CzEng, the larger one of the two. https://github.com/awslabs/sockeye/ 843 used as a source factor (Sennrich and Haddow, 2016) to the encoder. This helps us to decrease the vocabulary size of the the training data. We learn a shared byte-pair encoding (BPE) over the entire training data with 30,000 BPE operations (Sennrich et al., 2016),"
N19-1090,N18-2072,0,0.0130767,"of back-translation target specific words with high prediction loss (Fadaee and Monz, 2018), employed sampling to increase diversity (Edunov et al., 2018), replace rare words (Fadaee et al., 2017), or replace at random (Wang et al., 2018). Automatic data generation has also been successfully used for community question answering (Chen and Bunescu, 2017), semantic parsing (Jia and Liang, 2016), and task-oriented dialogue (Hou et al., 2018) by generating new data from the training dataset. In contrast, our model is trained on a much larger external corpus and is fixed, independent of the task. Kobayashi (2018) utilized a pretrained language model for automated data augmentation, though they only consider word-level rewrites and encourage label-preservation, while we paraphrase whole sentences with lexical constraints, independent of a gold label. Most similar to our experiments, Iyyer et al. (2018) explored syntactic paraphrasing for augmentation in sentiment and NLI tasks, extending prior work on PARA NMT. Background Constrained decoding Prior work explored methods to apply lexical constraints to a Neural Machine Translation (NMT) decoder (Hokamp and Liu, 2017; Anderson et al., 2017). However, mos"
N19-1090,N18-1202,0,0.0123599,"estion Answering We apply our paraphrastic rewriter to the task of question answer sentence selection to see if augmenting with paraphrases leads to improvements. The task is defined as follows: Given a question q and a set of candidate sentences {ci }, select the candidates which answer q. Model We adapt a popular neural architecture for NLI, InferSent (Conneau et al., 2017), to our QA sentence selection task. In InferSent, the questions and answers (originally the premises and hypotheses) are embedded using an uncontextualized word embedding (e.g. GloVe), which we also experiment with ELMo (Peters et al., 2018) to incorporate recent advancements in large-scale contextualized pre-training. Bidirectional LSTMs (Graves and Schmidhuber, 2005) are run atop of these contextualized embeddings and a maxpooling layer is used to generate a feature vector for both the question and the answer. Following various matching methods (Mou et al., 2016) and a multi-layer feed-forward neural network, the model produces a final score. We train the system following the method proposed by Rao et al. (2016), utilizing a ranking loss (Weston and Watkins, 1999) that contrasts positive answers against negative ones. 846 Parap"
N19-1090,P07-2045,0,0.00818714,"Missing"
N19-1090,W18-6319,1,0.814891,"s Evaluation We use S OCKEYE (Hieber et al., 2017)4 for our evaluations. We trained a 6-layer German–English Transformer using the default settings on the WMT’18 training data and the newstest2018 test set for evaluation (Bojar et al., 2018). Following Post and Vilar (2018), we compare decoding results in an unconstrained setting and with two sets of positive constraints: “rand3”, which selects 3 random words from the reference, and “phr4”, which selects a single 4-word phrase. We report decoding speed (in sentences per second) and BLEU score (Papineni et al., 2002), as measured by SacreBLEU (Post, 2018). The results are 4 Improved Monolingual Rewriter 5 PARA BANK generated paraphrases from two large bilingual corpora, CzEng (Bojar et al., 2016a) and GigaFrEn (Callison-Burch et al., 2009). We picked paraphrases from only CzEng, the larger one of the two. https://github.com/awslabs/sockeye/ 843 used as a source factor (Sennrich and Haddow, 2016) to the encoder. This helps us to decrease the vocabulary size of the the training data. We learn a shared byte-pair encoding (BPE) over the entire training data with 30,000 BPE operations (Sennrich et al., 2016), keeping all vocabulary items with a fre"
N19-1090,N18-1119,1,0.356301,"n (See et al., 2017). One approach to enforce hard lexical constraints in the output is to modify the inference procedure to enforce their presence directly (Hokamp and Liu, 2017). These constraints could be either positive (a word must appear in the output) or negative (a word must be avoided). While negative constraints could be easily enforced by preventing hypotheses with prohibited tokens from entering the beam, placing positive constraints in natural and meaningful ways is less straightforward. We improve upon previous work by vectorizing the dynamic beam allocation (DBA) algorithm from Post and Vilar (2018) and by incorporating multi-state tries, • A more efficient and robust approach to lexically-constrained decoding with vectorized DBA and trie representations; • A trained and freely available lexically839 Proceedings of NAACL-HLT 2019, pages 839–850 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics constrained monolingual rewriter1 with improvements in both human-judged semantic similarity and fluency over the initial PARA BANK rewriter (Hu et al., 2019); PARA NMT’s approach and produced a NMTbased rewriter with the ability to apply lexical constr"
N19-1090,N16-1014,0,0.0613793,"en working with positive constraints. Faster decoding enables faster exploration of constraint strategies: we illustrate this via data augmentation experiments with a monolingual rewriter applied to the tasks of natural language inference, question answering and machine translation, showing improvements in all three. 1 Introduction For many natural language generation tasks, we often know word(s) that should (or should not) be in the output sentence. Examples include terminology databases in Machine Translation (MT) (Hokamp and Liu, 2017), names (and generic responses) in dialogue generation (Li et al., 2016; Gu et al., 2016), objects in image captioning (Anderson et al., 2017), and facts in abstractive summarization (See et al., 2017). One approach to enforce hard lexical constraints in the output is to modify the inference procedure to enforce their presence directly (Hokamp and Liu, 2017). These constraints could be either positive (a word must appear in the output) or negative (a word must be avoided). While negative constraints could be easily enforced by preventing hypotheses with prohibited tokens from entering the beam, placing positive constraints in natural and meaningful ways is less s"
N19-1090,P18-1020,1,0.800323,"Missing"
N19-1090,P17-1099,0,0.0185148,"a data augmentation experiments with a monolingual rewriter applied to the tasks of natural language inference, question answering and machine translation, showing improvements in all three. 1 Introduction For many natural language generation tasks, we often know word(s) that should (or should not) be in the output sentence. Examples include terminology databases in Machine Translation (MT) (Hokamp and Liu, 2017), names (and generic responses) in dialogue generation (Li et al., 2016; Gu et al., 2016), objects in image captioning (Anderson et al., 2017), and facts in abstractive summarization (See et al., 2017). One approach to enforce hard lexical constraints in the output is to modify the inference procedure to enforce their presence directly (Hokamp and Liu, 2017). These constraints could be either positive (a word must appear in the output) or negative (a word must be avoided). While negative constraints could be easily enforced by preventing hypotheses with prohibited tokens from entering the beam, placing positive constraints in natural and meaningful ways is less straightforward. We improve upon previous work by vectorizing the dynamic beam allocation (DBA) algorithm from Post and Vilar (2018"
N19-1090,P18-1042,0,0.0752775,"t scale to batching, instead they sequentially processed constraints for sentences within the batch. Paraphrases and Rewriting Many works sought to create paraphrases or paraphrastic expressions through existing corpora. For example, DIRT (Lin and Pantel, 2001) extracts paraphrastic expressions from paths in dependency trees. Weisman et al. (2012) explored learning inference relations between verbs in broader scopes (document or corpus level). PPDB (Ganitkevitch et al., 2013) constructs paraphrase pairs by linking words or phrases that share the same translation in another language. PARA NMT (Wieting and Gimpel, 2018) and PARA BANK (Hu et al., 2019) used back-translation to build a large paraphrase collection from bilingual corpora. For arbitrary sentence rewriting, Napoles et al. (2016) used statistical machine translation in tandem with PPDB as a black box monolingual sentential rewriter. Mallinson et al. (2017) used a series of NMT model pairs to perform backtranslations for monolingual paraphrasing. A similar approach was adopted by PARA NMT to create a large paraphrase collection, which is used to train a monolingual sentence rewriter for canonicalization. PARA BANK (Hu et al., 2019) extends 1 3 Impro"
N19-1090,N18-1101,0,0.0129018,"merely indicative of the potential in data augmentation via constrained paraphrasing, and are by no means a thorough investigation of strategies that yield the best improvements. Such an investigation, however, could be enabled by our algorithmic improvements and practitioners’ domain expertise. 5.1 Natural Language Inference Natural language inference is the task of determining entailment. Two sentences, a premise p and a hypothesis h, are labelled with ENTAIL MENT, CONTRADICTION , or NEUTRAL depending on whether p logically entails, contradicts, or does not interact with h. MultiNLI (MNLI) (Williams et al., 2018) is a large, multi-genre dataset for natural language inference. The dataset is also divided into matched and mismatched portions based on whether the source of the evaluation data matches the source of the training set. Recent models rely on contextual sentence encoders pretrained on vast amounts of English monolingual text (Peters et al., 2018; Devlin et al., 2018). Paraphrastic Data Augmentation We demonstrate the utility of our improved lexically-constrained decoding via data augmentation with some simple rewriting heuristics and two augmentation strategies. First, the model could be We tr"
N19-1090,W16-2209,0,0.0270133,"and with two sets of positive constraints: “rand3”, which selects 3 random words from the reference, and “phr4”, which selects a single 4-word phrase. We report decoding speed (in sentences per second) and BLEU score (Papineni et al., 2002), as measured by SacreBLEU (Post, 2018). The results are 4 Improved Monolingual Rewriter 5 PARA BANK generated paraphrases from two large bilingual corpora, CzEng (Bojar et al., 2016a) and GigaFrEn (Callison-Burch et al., 2009). We picked paraphrases from only CzEng, the larger one of the two. https://github.com/awslabs/sockeye/ 843 used as a source factor (Sennrich and Haddow, 2016) to the encoder. This helps us to decrease the vocabulary size of the the training data. We learn a shared byte-pair encoding (BPE) over the entire training data with 30,000 BPE operations (Sennrich et al., 2016), keeping all vocabulary items with a frequency over 50 in the post-BPE data. We follow Sennrich and Haddow (2016) and use “BIOE&quot; tagging to annotate BPE segmentation and broadcast the casing factor accordingly. The encoder uses both source factors. The model is trained on 2 NVIDIA GTX 1080Ti’s until convergence (5 days). LSTM alpha Transf. alpha Transf. Full STD Fluency 74.5 78.3 81.7"
N19-1090,P16-1009,0,0.0391384,"multiple paraphrases. However, Hu et al. (2019) did not: evaluate the rewriter’s performance on in-the-wild sentences; explore more sophisticated versions of the rewriter; nor demonstrate its utility on NLP tasks. • Monolingual rewriting constraint heuristics for automatic data augmentation leading to improvements on NLI / QA / MT. 2 Data augmentation Data augmentation has been used to improve performance and robustness in deep neural models. In NMT, the most common approach is back-translation, where monolingual text in the target language is translated to create synthetic source sentences (Sennrich et al., 2016). Variants of back-translation target specific words with high prediction loss (Fadaee and Monz, 2018), employed sampling to increase diversity (Edunov et al., 2018), replace rare words (Fadaee et al., 2017), or replace at random (Wang et al., 2018). Automatic data generation has also been successfully used for community question answering (Chen and Bunescu, 2017), semantic parsing (Jia and Liang, 2016), and task-oriented dialogue (Hou et al., 2018) by generating new data from the training dataset. In contrast, our model is trained on a much larger external corpus and is fixed, independent of"
N19-1090,D07-1003,0,0.0643621,"the training data. 5.3 We apply our paraphrastic rewriter to the WMT 2016 Turkish-English translation task (Bojar et al., 2016b). We see no improvement in English to Turkish translation, but see a 1.1 BLEU improvement when training an initial NMT system on half paraphrased and half original data, and continued training on the original data. Full details of the experiments are in Appendix C. This was the highest concentration of standard data we experimented with, and future work will explore additional ways of data augmentation using paraphrases. Data Setup We augment the raw TREC-QA dataset (Wang et al., 2007) under the following orthogonal strategies: (1) augmenting the training set with the paraphrases generated via the approach described above; (2) augmenting the answer candidates at evaluation time, and choosing the max score among the paraphrases as the score (aggregation by voting). 6 Experimental Results We evaluate our models using average precision (MAP) and mean reciprocal rank (MRR). Model selection is done with early stopping to choose the epoch with the maximum MAP score. Note that the “Baseline (+ELMo)” settings below falls back to the standard QA selection task, and our score under E"
N19-1090,D18-1100,0,0.0224374,"ristics for automatic data augmentation leading to improvements on NLI / QA / MT. 2 Data augmentation Data augmentation has been used to improve performance and robustness in deep neural models. In NMT, the most common approach is back-translation, where monolingual text in the target language is translated to create synthetic source sentences (Sennrich et al., 2016). Variants of back-translation target specific words with high prediction loss (Fadaee and Monz, 2018), employed sampling to increase diversity (Edunov et al., 2018), replace rare words (Fadaee et al., 2017), or replace at random (Wang et al., 2018). Automatic data generation has also been successfully used for community question answering (Chen and Bunescu, 2017), semantic parsing (Jia and Liang, 2016), and task-oriented dialogue (Hou et al., 2018) by generating new data from the training dataset. In contrast, our model is trained on a much larger external corpus and is fixed, independent of the task. Kobayashi (2018) utilized a pretrained language model for automated data augmentation, though they only consider word-level rewrites and encourage label-preservation, while we paraphrase whole sentences with lexical constraints, independen"
N19-1090,D12-1018,0,0.0122348,"d Vilar (2018) proposed a means of dynamically allocating the slots in a fixed-size beam to ensure that even progress was made in meeting an arbitrary number of constraints provided with the input sentence. However, despite it being their motivation, their approach did not scale to batching, instead they sequentially processed constraints for sentences within the batch. Paraphrases and Rewriting Many works sought to create paraphrases or paraphrastic expressions through existing corpora. For example, DIRT (Lin and Pantel, 2001) extracts paraphrastic expressions from paths in dependency trees. Weisman et al. (2012) explored learning inference relations between verbs in broader scopes (document or corpus level). PPDB (Ganitkevitch et al., 2013) constructs paraphrase pairs by linking words or phrases that share the same translation in another language. PARA NMT (Wieting and Gimpel, 2018) and PARA BANK (Hu et al., 2019) used back-translation to build a large paraphrase collection from bilingual corpora. For arbitrary sentence rewriting, Napoles et al. (2016) used statistical machine translation in tandem with PPDB as a black box monolingual sentential rewriter. Mallinson et al. (2017) used a series of NMT"
P09-2012,P01-1010,0,0.879883,"0 150 100 50 0 0 2 4 6 8 subtree height 10 12 2.2 Collapsed Gibbs sampling with a DP prior2 14 For an excellent introduction to collapsed Gibbs sampling with a DP prior, we refer the reader to Appendix A of Goldwater et al. (2009), which we follow closely here. Our training data is a set of parse trees T that we assume was produced by an unknown TSG g with probability Pr(T |g). Using Bayes’ rule, we can compute the probability of a particular hypothesized grammar as Figure 1: Subtree count (thousands) across heights for the “all subtrees” grammar () and the superior “minimal subset” () from Bod (2001). events can simply be counted in the training data. In contrast, there are no treebanks annotated with TSG derivations, and a treebank parse tree of n nodes is ambiguous among 2n possible derivations. One solution would be to manually annotate a treebank with TSG derivations, but in addition to being expensive, this task requires one to know what the grammar actually is. Part of the thinking motivating TSGs is to let the data determine the best set of subtrees. One approach to grammar-learning is DataOriented Parsing (DOP), whose strategy is to simply take all subtrees in the training data as"
P09-2012,C02-1126,0,0.0180372,"nd CFGs (and their probabilistic counterparts, which concern us here) by allowing nonterminals to be rewritten as subtrees of arbitrary size. Although nonterminal rewrites are still context-free, in practice TSGs can loosen the independence assumptions of CFGs because larger rules capture more context. This is simpler than the complex independence and backoff decisions of Markovized grammars. Furthermore, subtrees with terminal symbols can be viewed as learning dependencies among the words in the subtree, obviating the need for the manual specification (Magerman, 1995) or automatic inference (Chiang and Bikel, 2002) of lexical dependencies. Following standard notation for PCFGs, the probability of a derivation d in the grammar is given as Y Pr(r) Pr(d) = Introduction Tree substition grammars (TSGs) have potential advantages over regular context-free grammars (CFGs), but there is no obvious way to learn these grammars. In particular, learning procedures are not able to take direct advantage of manually annotated corpora like the Penn Treebank, which are not marked for derivations and thus assume a standard CFG. Since different TSG derivations can produce the same parse tree, learning procedures must guess"
P09-2012,N09-1062,0,0.470075,"oduct of the PCFG rules r ∈ t that constitute it and a geometric distribution Pr$ over the number of those rules, thus encoding a preference for smaller subtrees.3 The parameter α contributes to the probability that previously unseen subtrees will be sampled. All DPs share parameters p$ and α. An entire grammar is then given as g = {gX : X ∈ N }. We emphasize that no head information is used by the sampler. Rather than explicitly consider each segmentation of the parse trees (which would define a TSG and its associated parameters), we use a collapsed Gibbs sampler to integrate over all possi2 Cohn et al. (2009) and O’Donnell et al. (2009) independently developed similar models. 3 GX (t) = 0 unless root(t) = X. 2P R . P +R 46 3 Experiments S1 NP ADVP VP NN RB VBZ Someone always makes 3.1 Setup We used the standard split for the Wall Street Journal portion of the Treebank, training on sections 2 to 21, and reporting results on sentences with no more than forty words from section 23. We compare with three other grammars. S2 NP VP PRP VB you quit • A standard Treebank PCFG. • A “spinal” TSG, produced by extracting n lexicalized subtrees from each length n sentence in the training data. Each subtree is d"
P09-2012,D08-1033,0,0.0676377,"Missing"
P09-2012,P95-1037,0,0.0792182,"ed ones on parsing accuracy. 1 TSGs extend CFGs (and their probabilistic counterparts, which concern us here) by allowing nonterminals to be rewritten as subtrees of arbitrary size. Although nonterminal rewrites are still context-free, in practice TSGs can loosen the independence assumptions of CFGs because larger rules capture more context. This is simpler than the complex independence and backoff decisions of Markovized grammars. Furthermore, subtrees with terminal symbols can be viewed as learning dependencies among the words in the subtree, obviating the need for the manual specification (Magerman, 1995) or automatic inference (Chiang and Bikel, 2002) of lexical dependencies. Following standard notation for PCFGs, the probability of a derivation d in the grammar is given as Y Pr(r) Pr(d) = Introduction Tree substition grammars (TSGs) have potential advantages over regular context-free grammars (CFGs), but there is no obvious way to learn these grammars. In particular, learning procedures are not able to take direct advantage of manually annotated corpora like the Penn Treebank, which are not marked for derivations and thus assume a standard CFG. Since different TSG derivations can produce the"
P09-2012,E93-1006,0,\N,Missing
P09-2012,C96-2215,0,\N,Missing
P11-2038,E93-1006,0,0.41433,"most anywhere as a sort of ungrammatical glue. Thus, on average, grammatical sentences will license deriva218 tions with larger fragments, whereas ungrammatical sentences will be forced to resort to small fragments. This is the central idea explored in this paper. This raises the question of what exactly the larger fragments are. A fundamental problem with TSGs is that they are hard to learn, since there is no annotated corpus of TSG derivations and the number of possible derivations is exponential in the size of a tree. The most popular TSG approach has been DataOriented Parsing (Scha, 1990; Bod, 1993), which takes all fragments in the training data. The large size of such grammars (exponential in the size of the training data) forces either implicit representations (Goodman, 1996; Bansal and Klein, 2010) — which do not permit arbitrary probability distributions over the grammar fragments — or explicit approximations to all fragments (Bod, 2001). A number of researchers have presented ways to address the learning problems for explicitly represented TSGs (Zollmann and Sima’an, 2005; Zuidema, 2007; Cohn et al., 2009; Post and Gildea, 2009a). Of these approaches, work in Bayesian learning of T"
P11-2038,P05-1022,0,0.0684647,"ver the past two decades in the field of parsing. This paper demonstrates the utility of local features derived from the fragments of tree substitution grammar derivations. Following Cherry and Quirk (2008), we conduct experiments in a classification setting, where the task is to distinguish between real text and “pseudo-negative” text obtained by sampling from a trigram language model (Okanohara and Tsujii, 2007). Our primary points of comparison are the latent SVM training of Cherry and Quirk (2008), mentioned above, and the extensive set of local and nonlocal feature templates developed by Charniak and Johnson (2005) for parse tree reranking. In contrast to this latter set of features, the feature sets from TSG derivations require no engineering; instead, they are obtained directly from the identity of the fragments used in the derivation, plus simple statistics computed over them. Since these fragments are in turn learned automatically from a Treebank with a Bayesian model, their usefulness here suggests a greater potential for adapting to other languages and datasets. 2 Tree substitution grammars Tree substitution grammars (Joshi and Schabes, 1997) generalize context-free grammars by allowing nontermina"
P11-2038,A00-2018,0,0.147223,"Missing"
P11-2038,2008.amta-papers.4,0,0.402611,"ng-studied and well understood, and can be efficiently incorporated into search procedures, such 217 as for machine translation. As a result, the output of such text generation systems is often very poor grammatically, even if it is understandable. Since grammaticality judgments are a matter of the syntax of a language, the obvious approach for modeling grammaticality is to start with the extensive work produced over the past two decades in the field of parsing. This paper demonstrates the utility of local features derived from the fragments of tree substitution grammar derivations. Following Cherry and Quirk (2008), we conduct experiments in a classification setting, where the task is to distinguish between real text and “pseudo-negative” text obtained by sampling from a trigram language model (Okanohara and Tsujii, 2007). Our primary points of comparison are the latent SVM training of Cherry and Quirk (2008), mentioned above, and the extensive set of local and nonlocal feature templates developed by Charniak and Johnson (2005) for parse tree reranking. In contrast to this latter set of features, the feature sets from TSG derivations require no engineering; instead, they are obtained directly from the i"
P11-2038,N09-1062,0,0.0958465,"Missing"
P11-2038,W09-2112,0,0.151596,"Missing"
P11-2038,W96-0214,0,0.125765,"forced to resort to small fragments. This is the central idea explored in this paper. This raises the question of what exactly the larger fragments are. A fundamental problem with TSGs is that they are hard to learn, since there is no annotated corpus of TSG derivations and the number of possible derivations is exponential in the size of a tree. The most popular TSG approach has been DataOriented Parsing (Scha, 1990; Bod, 1993), which takes all fragments in the training data. The large size of such grammars (exponential in the size of the training data) forces either implicit representations (Goodman, 1996; Bansal and Klein, 2010) — which do not permit arbitrary probability distributions over the grammar fragments — or explicit approximations to all fragments (Bod, 2001). A number of researchers have presented ways to address the learning problems for explicitly represented TSGs (Zollmann and Sima’an, 2005; Zuidema, 2007; Cohn et al., 2009; Post and Gildea, 2009a). Of these approaches, work in Bayesian learning of TSGs produces intuitive grammars in a principled way, and has demonstrated potential in language modeling tasks (Post and Gildea, 2009b; Post, 2010). Our experiments make use of Bayes"
P11-2038,P08-1067,0,0.0235079,"comes at a cost of using ten times as many features. The classifiers with TSG features outperform all the other models. The (near)-perfect performance of the TSG models on the Treebank is a result of the large number of features relative to the size of the training data: 2 The sampler was run with the default settings for 1,000 iterations, and a grammar of 192,667 fragments was then extracted from counts taken from every 10th iteration between iterations 500 and 1,000, inclusive. Code was obtained from http://github.com/mjpost/dptsg. 3 Local features can be computed in a bottom-up manner. See Huang (2008, §3.2) for more detail. 4 A fragment’s frontier is the number of terminals and nonterminals among its leaves, also known its rank. For example, the fragment in Figure 1 has a frontier size of 5. 4.1 Base models and features Our experiments compare a number of different feature sets. Central to these feature sets are features computed from the output of four language models. 1. Bigram and trigram language models (the same ones used to generate the negative data) 2. A Treebank grammar (Charniak, 1996) 219 feature set length (l) 3-gram score (S3 ) PCFG score (SP ) TSG score (ST ) Charniak score"
P11-2038,J98-4004,0,0.12537,"occurrences are in subject settings over articles that aren’t required to modify a noun, such as that, some, this, and all. However, in the BLLIP n-gram data, this rule is used over the definite article the 465 times – the second-most common use. Yet this rule occurs only nine times in the Treebank where the grammar was learned. The small fragment size, together with the coarseness of the nonterminal, permit the fragment to be used in distributional settings where it should not be licensed. This suggests some complementarity between fragment learning and work in using nonterminal refinements (Johnson, 1998; Petrov et al., 2006). 6 Related work Past approaches using parsers as language models in discriminative settings have seen varying degrees of success. Och et al. (2004) found that the score of a bilexicalized parser was not useful in distinguishing machine translation (MT) output from human reference translations. Cherry and Quirk (2008) addressed this problem by using a latent SVM to adjust the CFG rule weights such that the parser score was a much more useful discriminator between grammatical text and n-gram samples. Mutton et al. (2007) also addressed this problem by combining scores from"
P11-2038,J93-2004,0,0.0384484,"for explicitly represented TSGs (Zollmann and Sima’an, 2005; Zuidema, 2007; Cohn et al., 2009; Post and Gildea, 2009a). Of these approaches, work in Bayesian learning of TSGs produces intuitive grammars in a principled way, and has demonstrated potential in language modeling tasks (Post and Gildea, 2009b; Post, 2010). Our experiments make use of Bayesian-learned TSGs. 4 Experiments We experiment with a binary classification task, defined as follows: given a sequence of words, determine whether it is grammatical or not. We use two datasets: the Wall Street Journal portion of the Penn Treebank (Marcus et al., 1993), and the BLLIP ’99 dataset,1 a collection of automatically-parsed sentences from three years of articles from the Wall Street Journal. For both datasets, positive examples are obtained from the leaves of the parse trees, retaining their tokenization. Negative examples were produced from a trigram language model by randomly generating sentences of length no more than 100 so as to match the size of the positive data. The language model was built with SRILM (Stolcke, 2002) using interpolated Kneser-Ney smoothing. The average sentence lengths for the positive and negative data were 23.9 and 24.7,"
P11-2038,P07-1044,0,0.0175714,"ragment learning and work in using nonterminal refinements (Johnson, 1998; Petrov et al., 2006). 6 Related work Past approaches using parsers as language models in discriminative settings have seen varying degrees of success. Och et al. (2004) found that the score of a bilexicalized parser was not useful in distinguishing machine translation (MT) output from human reference translations. Cherry and Quirk (2008) addressed this problem by using a latent SVM to adjust the CFG rule weights such that the parser score was a much more useful discriminator between grammatical text and n-gram samples. Mutton et al. (2007) also addressed this problem by combining scores from different parsers using an SVM and showed an improved metric of fluency. grammatical (VP VBD (NP CD) PP) (S (NP PRP) VP) (S NP (VP TO VP)) F2l (NP NP (VP VBG NP)) (SBAR (S (NP PRP) VP)) (SBAR (IN that) S) (TOP (S NP (VP (VBD said) NP SBAR) .)) (NP (NP DT JJ NN) PP) (NP (NP NNP NNP) , NP ,) (TOP (S NP (ADVP (RB also)) VP .)) (VP (VB be) VP) (NP (NP NNS) PP) (NP NP , (SBAR WHNP (S VP)) ,) (TOP (S SBAR , NP VP .)) (ADJP (QP $ CD (CD million))) (SBAR (IN that) (S NP VP)) F8 ungrammatical F0l (NP (NP CD) PP) (TOP (NP NP NP .)) F5 (S (NP (NNP UNK"
P11-2038,P07-1010,0,0.221802,"mmatically, even if it is understandable. Since grammaticality judgments are a matter of the syntax of a language, the obvious approach for modeling grammaticality is to start with the extensive work produced over the past two decades in the field of parsing. This paper demonstrates the utility of local features derived from the fragments of tree substitution grammar derivations. Following Cherry and Quirk (2008), we conduct experiments in a classification setting, where the task is to distinguish between real text and “pseudo-negative” text obtained by sampling from a trigram language model (Okanohara and Tsujii, 2007). Our primary points of comparison are the latent SVM training of Cherry and Quirk (2008), mentioned above, and the extensive set of local and nonlocal feature templates developed by Charniak and Johnson (2005) for parse tree reranking. In contrast to this latter set of features, the feature sets from TSG derivations require no engineering; instead, they are obtained directly from the identity of the fragments used in the derivation, plus simple statistics computed over them. Since these fragments are in turn learned automatically from a Treebank with a Bayesian model, their usefulness here su"
P11-2038,P06-1055,0,0.142053,"in subject settings over articles that aren’t required to modify a noun, such as that, some, this, and all. However, in the BLLIP n-gram data, this rule is used over the definite article the 465 times – the second-most common use. Yet this rule occurs only nine times in the Treebank where the grammar was learned. The small fragment size, together with the coarseness of the nonterminal, permit the fragment to be used in distributional settings where it should not be licensed. This suggests some complementarity between fragment learning and work in using nonterminal refinements (Johnson, 1998; Petrov et al., 2006). 6 Related work Past approaches using parsers as language models in discriminative settings have seen varying degrees of success. Och et al. (2004) found that the score of a bilexicalized parser was not useful in distinguishing machine translation (MT) output from human reference translations. Cherry and Quirk (2008) addressed this problem by using a latent SVM to adjust the CFG rule weights such that the parser score was a much more useful discriminator between grammatical text and n-gram samples. Mutton et al. (2007) also addressed this problem by combining scores from different parsers usi"
P11-2038,P09-2012,1,0.829651,"t popular TSG approach has been DataOriented Parsing (Scha, 1990; Bod, 1993), which takes all fragments in the training data. The large size of such grammars (exponential in the size of the training data) forces either implicit representations (Goodman, 1996; Bansal and Klein, 2010) — which do not permit arbitrary probability distributions over the grammar fragments — or explicit approximations to all fragments (Bod, 2001). A number of researchers have presented ways to address the learning problems for explicitly represented TSGs (Zollmann and Sima’an, 2005; Zuidema, 2007; Cohn et al., 2009; Post and Gildea, 2009a). Of these approaches, work in Bayesian learning of TSGs produces intuitive grammars in a principled way, and has demonstrated potential in language modeling tasks (Post and Gildea, 2009b; Post, 2010). Our experiments make use of Bayesian-learned TSGs. 4 Experiments We experiment with a binary classification task, defined as follows: given a sequence of words, determine whether it is grammatical or not. We use two datasets: the Wall Street Journal portion of the Penn Treebank (Marcus et al., 1993), and the BLLIP ’99 dataset,1 a collection of automatically-parsed sentences from three years of"
P11-2038,W10-3815,0,0.0408319,"either implicit representations (Goodman, 1996; Bansal and Klein, 2010) — which do not permit arbitrary probability distributions over the grammar fragments — or explicit approximations to all fragments (Bod, 2001). A number of researchers have presented ways to address the learning problems for explicitly represented TSGs (Zollmann and Sima’an, 2005; Zuidema, 2007; Cohn et al., 2009; Post and Gildea, 2009a). Of these approaches, work in Bayesian learning of TSGs produces intuitive grammars in a principled way, and has demonstrated potential in language modeling tasks (Post and Gildea, 2009b; Post, 2010). Our experiments make use of Bayesian-learned TSGs. 4 Experiments We experiment with a binary classification task, defined as follows: given a sequence of words, determine whether it is grammatical or not. We use two datasets: the Wall Street Journal portion of the Penn Treebank (Marcus et al., 1993), and the BLLIP ’99 dataset,1 a collection of automatically-parsed sentences from three years of articles from the Wall Street Journal. For both datasets, positive examples are obtained from the leaves of the parse trees, retaining their tokenization. Negative examples were produced from a trigram"
P11-2038,P07-1011,0,0.0282631,"discriminative tasks, for example, in discriminating between finer grained and more realistic grammatical errors (Foster and Vogel, 2004; Wagner et al., 2009), or in discriminating among translation candidates in a machine translation framework. In another line of potential work, it could prove useful to incorporate into the grammar learning procedure some knowledge of the sorts of fragments and features shown here to be helpful for discriminating grammatical and ungrammatical text. Outside of MT, Foster and Vogel (2004) argued for parsers that do not assume the grammaticality of their input. Sun et al. (2007) used a set of templates to extract labeled sequential part-of-speech patterns together with some other linguistic features) which were then used in an SVM setting to classify sentences in Japanese and Chinese learners’ English corpora. Wagner et al. (2009) and Foster and Andersen (2009) attempt finer-grained, more realistic (and thus more difficult) classifications against ungrammatical text modeled on the sorts of mistakes References made by language learners using parser probabili- Mohit Bansal and Dan Klein. 2010. Simple, accurate ties. More recently, some researchers have shown parsing wi"
P11-2038,U10-1011,0,0.152548,"NP VP)) F8 ungrammatical F0l (NP (NP CD) PP) (TOP (NP NP NP .)) F5 (S (NP (NNP UNKCAPS-NUM))) (TOP (S NP VP (. .))) (TOP (PP IN NP .)) (TOP (S “ NP VP (. .))) (TOP (S PP NP VP .)) (TOP (NP NP PP .)) F4 (NP (DT that) NN) (TOP (S NP VP . ”)) (TOP (NP NP , NP .)) (QP CD (CD million)) (NP NP (CC and) NP) (PP (IN In) NP) (QP $ CD (CD million)) Table 4: Highest-weighted TSG features. grammatical (WHNP CD) (NP JJ NNS) (PRT RP) (WHNP WP NN) (SBAR WHNP S) (WHNP WDT NN) ungrammatical (NN UNK-CAPS) (S VP) (S NP) (TOP FRAG) (NP DT JJ) (NP DT) Table 5: Highest-weighted depth-one rules. used) is fruitful (Wong and Dras, 2010; Post, 2010). 7 Summary Parsers were designed to discriminate among structures, whereas language models discriminate among strings. Small fragments, abstract rules, independence assumptions, and errors or peculiarities in the training corpus allow probable structures to be produced over ungrammatical text when using models that were optimized for parser accuracy. The experiments in this paper demonstrate the utility of tree-substitution grammars in discriminating between grammatical and ungrammatical sentences. Features are derived from the identities of the fragments used in the derivations"
P11-2038,D07-1058,0,0.0576003,"ial in the size of a tree. The most popular TSG approach has been DataOriented Parsing (Scha, 1990; Bod, 1993), which takes all fragments in the training data. The large size of such grammars (exponential in the size of the training data) forces either implicit representations (Goodman, 1996; Bansal and Klein, 2010) — which do not permit arbitrary probability distributions over the grammar fragments — or explicit approximations to all fragments (Bod, 2001). A number of researchers have presented ways to address the learning problems for explicitly represented TSGs (Zollmann and Sima’an, 2005; Zuidema, 2007; Cohn et al., 2009; Post and Gildea, 2009a). Of these approaches, work in Bayesian learning of TSGs produces intuitive grammars in a principled way, and has demonstrated potential in language modeling tasks (Post and Gildea, 2009b; Post, 2010). Our experiments make use of Bayesian-learned TSGs. 4 Experiments We experiment with a binary classification task, defined as follows: given a sequence of words, determine whether it is grammatical or not. We use two datasets: the Wall Street Journal portion of the Penn Treebank (Marcus et al., 1993), and the BLLIP ’99 dataset,1 a collection of automati"
P11-2038,P01-1010,0,\N,Missing
P11-2038,P10-1112,0,\N,Missing
P11-2038,N04-1021,0,\N,Missing
P13-2150,W05-0904,0,0.0143063,"t was multiple orders of magnitude increase in both training and testing time. These results were consistent across a range of task types, dataset sizes, and classification arities (binary and multiclass). There are a number of important caveats. We explored a range of data settings, but there are many others where tree kernels have been proven useful, such as parse tree reranking (Collins and Duffy, 2002; Shen and Joshi, 2003), sentence subjectivity (Suzuki et al., 2004), pronoun resolution (Yang et al., 2006), relation extraction (Culotta and Sorensen, 2004), machine translation evaluation (Liu and Gildea, 2005), predicate-argument recognition, and semantic role labeling (Pighin and Moschitti, 2009). There are also tree kernel variations such as dependency tree kernels (Culotta and Sorensen, 2004) and shallow semantic tree kernels (Moschitti et al., 2007). These variables provide a rich environment for future work; in the meantime, we take these results as compelling motivation for the continued development of explicit syntactic features (both manual and automatically induced), and suggest that such features should be part of the baseline systems on applicable discriminative NLP tasks. Which fragment"
P13-2150,J93-2004,0,0.0613322,".1 3,840 Coarse grammatical classification Our first comparison is coarse grammatical classification, where the goal is to distinguish between human-written sentences and “pseudo-negative” sentences sampled from a trigram language model constructed from in3.2 Fine grammatical classification Real-world grammaticality judgments require much finer-grained distinctions than the coarse ones of the previous section (for example, marking dropped determiners or wrong verb inflections). For this task, we too positive examples from all sentences of sections 2–21 of the WSJ portion of the Penn Treebank (Marcus et al., 1993). Negative examples were created by inserting one or two errors provement. 2 github.com/mjpost/dptsg 3 www.csie.ntu.edu.tw/~cjlin/liblinear/ 4 disi.unitn.it/moschitti/Tree-Kernel.htm 5 Optimizing SVM-TK’s decay parameter (-L) did not improve test-set accuracy, but did increase training time (squaring the number of hyperparameter combinations to evaluate), so we stuck with the default. 6 Increased from the default of 40 MB, which halves the running time. 7 867 LDC Catalog No. LDC2000T43 system Wong & Dras Chance N-gram CFG TSG C&J SVM-TK accuracy 60.6 50.0 61.4 64.5 67.0 71.9 67.8 system Wong &"
P13-2150,N12-1033,1,0.667795,"Missing"
P13-2150,P07-1098,0,0.0421394,"We explored a range of data settings, but there are many others where tree kernels have been proven useful, such as parse tree reranking (Collins and Duffy, 2002; Shen and Joshi, 2003), sentence subjectivity (Suzuki et al., 2004), pronoun resolution (Yang et al., 2006), relation extraction (Culotta and Sorensen, 2004), machine translation evaluation (Liu and Gildea, 2005), predicate-argument recognition, and semantic role labeling (Pighin and Moschitti, 2009). There are also tree kernel variations such as dependency tree kernels (Culotta and Sorensen, 2004) and shallow semantic tree kernels (Moschitti et al., 2007). These variables provide a rich environment for future work; in the meantime, we take these results as compelling motivation for the continued development of explicit syntactic features (both manual and automatically induced), and suggest that such features should be part of the baseline systems on applicable discriminative NLP tasks. Which fragments? Our findings support the observations of Cumby and Roth (2003), who point out that kernels introduce a large number of irrelevant features that may be especially harmful in small-data settings, and that, when possible, it is often better to have"
P13-2150,E93-1006,0,0.30816,"Missing"
P13-2150,P04-1043,0,0.0503447,"substitution grammar (Post and Gildea, 2009, TSG)2 and extracted fragment counts from Viterbi derivations. We build classifiers with Liblinear3 (Fan et al., 2008). We divided each dataset into training, dev, and test sets. We then trained an L2-regularized L1-loss support vector machine (-s 3) with a bias parameter of 1 (-B 1), optimizing the regularization parameter (-c) on the dev set over the range {0.0001 . . . 100} by multiples of 10. The best model was then used to classify the test set. A sentence length feature was included for every sentence. For tree kernels, we used SVM-light-TK4 (Moschitti, 2004; Moschitti, 2006) with the default settings (-t 5 -D 1 -L 0.4),5 which also solves an L2-regularized L1-loss SVM optimization problem. We tuned the regularization parameter (-c) on the dev set in the same manner as described above, providing 4 GB of memory to the kernel cache (-m 4000).6 We used subset tree kernels, which compute the similarity between two trees by implicitly enumerating all possible fragments of the trees (in contrast with subtree kernels, where all fragments fully extend to the leaves). 3 Fine grammaticality (PTB) sentences 79,664 3,978 Question classification (TREC-10) sen"
P13-2150,P05-1022,0,0.0512634,"ernel approaches 2 Experimental setup We used the following feature sets: N-grams All unigrams and bigrams.1 1 Experiments with trigrams did not show any im866 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 866–872, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics train dev test Coarse grammaticality (BLLIP) sentences 100,000 6,000 6,000 CFG rules Counts of depth-one contextfree grammar (CFG) productions obtained from the Berkeley parser (Petrov et al., 2006). C&J features The parse-tree reranking feature set of Charniak and Johnson (2005), extracted from the Berkeley parse trees. TSG features We also parsed with a Bayesian tree substitution grammar (Post and Gildea, 2009, TSG)2 and extracted fragment counts from Viterbi derivations. We build classifiers with Liblinear3 (Fan et al., 2008). We divided each dataset into training, dev, and test sets. We then trained an L2-regularized L1-loss support vector machine (-s 3) with a bias parameter of 1 (-B 1), optimizing the regularization parameter (-c) on the dev set over the range {0.0001 . . . 100} by multiples of 10. The best model was then used to classify the test set. A sentenc"
P13-2150,E06-1015,0,0.0650657,"mmar (Post and Gildea, 2009, TSG)2 and extracted fragment counts from Viterbi derivations. We build classifiers with Liblinear3 (Fan et al., 2008). We divided each dataset into training, dev, and test sets. We then trained an L2-regularized L1-loss support vector machine (-s 3) with a bias parameter of 1 (-B 1), optimizing the regularization parameter (-c) on the dev set over the range {0.0001 . . . 100} by multiples of 10. The best model was then used to classify the test set. A sentence length feature was included for every sentence. For tree kernels, we used SVM-light-TK4 (Moschitti, 2004; Moschitti, 2006) with the default settings (-t 5 -D 1 -L 0.4),5 which also solves an L2-regularized L1-loss SVM optimization problem. We tuned the regularization parameter (-c) on the dev set in the same manner as described above, providing 4 GB of memory to the kernel cache (-m 4000).6 We used subset tree kernels, which compute the similarity between two trees by implicitly enumerating all possible fragments of the trees (in contrast with subtree kernels, where all fragments fully extend to the leaves). 3 Fine grammaticality (PTB) sentences 79,664 3,978 Question classification (TREC-10) sentences 4,907 545 5"
P13-2150,2008.amta-papers.4,0,0.150322,"sets of tree features on five diverse tasks, and find that explicit features often perform as well as tree kernels on accuracy and always in orders of magnitude less time, and with smaller models. Since explicit features are easy to generate and use (with publicly available tools), we suggest they should always be included as baseline comparisons in tree kernel method evaluations. 1 Introduction Features computed over parse trees are useful for a range of discriminative tasks, including authorship attribution (Baayen et al., 1996), parse reranking (Collins and Duffy, 2002), language modeling (Cherry and Quirk, 2008), and native-language detection (Wong and Dras, 2011). A major distinction among these uses of syntax is how the features are represented. The implicit approach uses tree kernels (Collins and Duffy, 2001), which make predictions with inner products between tree pairs. These products can be computed efficiently with a dynamic program that produces weighted counts of all the shared tree fragments between a pair of trees, essentially incorporating all fragments without representing any of them explicitly. Tree kernel approaches 2 Experimental setup We used the following feature sets: N-grams All"
P13-2150,P07-1010,0,0.0156293,"subtree kernels, where all fragments fully extend to the leaves). 3 Fine grammaticality (PTB) sentences 79,664 3,978 Question classification (TREC-10) sentences 4,907 545 500 Native language (ICLE; 7 languages) documents 490 105 175 sentences 17,715 3,968 6,777 Native language (ACL; 5 languages) documents 987 195 185 sentences 146,257 28,139 28,403 Table 1: Datasets. system Chance N-gram CFG TSG C&J SVM-TK accuracy 50.0 68.4 86.3 89.8 92.9 91.0 CPU time minutes minutes minutes an hour a week Table 2: Coarse grammaticality. CPU time is for classifier setup, training, and testing. domain data (Okanohara and Tsujii, 2007). Cherry and Quirk (2008) first applied syntax to this task, learning weighted parameters for a CFG with a latent SVM. Post (2011) found further improvements with fragmentbased representations (TSGs and C&J) with a regular SVM. Here, we compare their results to kernel methods. We repeat Post’s experiments on the BLLIP dataset,7 using his exact data splits (Table 2). To our knowledge, tree kernels have not been applied to this task. Tasks Table 1 summarizes our datasets. 3.1 3,840 Coarse grammatical classification Our first comparison is coarse grammatical classification, where the goal is to d"
P13-2150,P02-1034,0,0.0593887,"e compare tree kernels to different explicit sets of tree features on five diverse tasks, and find that explicit features often perform as well as tree kernels on accuracy and always in orders of magnitude less time, and with smaller models. Since explicit features are easy to generate and use (with publicly available tools), we suggest they should always be included as baseline comparisons in tree kernel method evaluations. 1 Introduction Features computed over parse trees are useful for a range of discriminative tasks, including authorship attribution (Baayen et al., 1996), parse reranking (Collins and Duffy, 2002), language modeling (Cherry and Quirk, 2008), and native-language detection (Wong and Dras, 2011). A major distinction among these uses of syntax is how the features are represented. The implicit approach uses tree kernels (Collins and Duffy, 2001), which make predictions with inner products between tree pairs. These products can be computed efficiently with a dynamic program that produces weighted counts of all the shared tree fragments between a pair of trees, essentially incorporating all fragments without representing any of them explicitly. Tree kernel approaches 2 Experimental setup We u"
P13-2150,P06-1055,0,0.0441571,"rporating all fragments without representing any of them explicitly. Tree kernel approaches 2 Experimental setup We used the following feature sets: N-grams All unigrams and bigrams.1 1 Experiments with trigrams did not show any im866 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 866–872, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics train dev test Coarse grammaticality (BLLIP) sentences 100,000 6,000 6,000 CFG rules Counts of depth-one contextfree grammar (CFG) productions obtained from the Berkeley parser (Petrov et al., 2006). C&J features The parse-tree reranking feature set of Charniak and Johnson (2005), extracted from the Berkeley parse trees. TSG features We also parsed with a Bayesian tree substitution grammar (Post and Gildea, 2009, TSG)2 and extracted fragment counts from Viterbi derivations. We build classifiers with Liblinear3 (Fan et al., 2008). We divided each dataset into training, dev, and test sets. We then trained an L2-regularized L1-loss support vector machine (-s 3) with a bias parameter of 1 (-B 1), optimizing the regularization parameter (-c) on the dev set over the range {0.0001 . . . 100} by"
P13-2150,P04-1054,0,0.0102604,"ree kernels in accuracy, and even where they did not, the cost was multiple orders of magnitude increase in both training and testing time. These results were consistent across a range of task types, dataset sizes, and classification arities (binary and multiclass). There are a number of important caveats. We explored a range of data settings, but there are many others where tree kernels have been proven useful, such as parse tree reranking (Collins and Duffy, 2002; Shen and Joshi, 2003), sentence subjectivity (Suzuki et al., 2004), pronoun resolution (Yang et al., 2006), relation extraction (Culotta and Sorensen, 2004), machine translation evaluation (Liu and Gildea, 2005), predicate-argument recognition, and semantic role labeling (Pighin and Moschitti, 2009). There are also tree kernel variations such as dependency tree kernels (Culotta and Sorensen, 2004) and shallow semantic tree kernels (Moschitti et al., 2007). These variables provide a rich environment for future work; in the meantime, we take these results as compelling motivation for the continued development of explicit syntactic features (both manual and automatically induced), and suggest that such features should be part of the baseline systems"
P13-2150,D09-1012,0,0.112058,"eriments on two datasets. In order to mitigate topic bias10 and other problems that have been reported with Question Classification We look next at question classification (QC). Li and Roth (2002) introduced the TREC-10 dataset,8 a set of questions paired with labels that categorize the question by the type of answer it seeks. The labels are organized hierarchically into six (coarse) top-level labels and fifty (fine) refinements. An example question from the ENTY/animal category is What was the first domesticated bird?. Table 4 contains results predicting just the coarse labels. We compare to Pighin and Moschitti (2009), and also repeat their experiments, finding a slightly better result for them. 8 sent. 42.0 39.5 38.7 42.9 40.7 42.5 39.2 40.4 49.2 42.1 9 Pighin and Moschitti (2009) did not report results on this version of the task. 10 E.g., when we train with all words, the keyword ’Japanese’ is a strong indicator for Japanese authors, while ’Arabic’ is a strong indicator for English ones. cogcomp.cs.illinois.edu/Data/QA/QC/ 868 3.4.1 100 CFG TSG C&J SVM-TK uSVM-TK 80 70 60 0 0.01 0.1 1 10 100 1,000 timetime (thousands of seconds) seconds)vs. Figuretraining 1: Training (1000s of test accuracy for coarse g"
P13-2150,P09-2012,1,0.30231,"not show any im866 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 866–872, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics train dev test Coarse grammaticality (BLLIP) sentences 100,000 6,000 6,000 CFG rules Counts of depth-one contextfree grammar (CFG) productions obtained from the Berkeley parser (Petrov et al., 2006). C&J features The parse-tree reranking feature set of Charniak and Johnson (2005), extracted from the Berkeley parse trees. TSG features We also parsed with a Bayesian tree substitution grammar (Post and Gildea, 2009, TSG)2 and extracted fragment counts from Viterbi derivations. We build classifiers with Liblinear3 (Fan et al., 2008). We divided each dataset into training, dev, and test sets. We then trained an L2-regularized L1-loss support vector machine (-s 3) with a bias parameter of 1 (-B 1), optimizing the regularization parameter (-c) on the dev set over the range {0.0001 . . . 100} by multiples of 10. The best model was then used to classify the test set. A sentence length feature was included for every sentence. For tree kernels, we used SVM-light-TK4 (Moschitti, 2004; Moschitti, 2006) with the d"
P13-2150,W09-2112,0,0.00943606,"stem Wong & Dras Chance N-gram CFG TSG C&J SVM-TK accuracy 60.6 50.0 61.4 64.5 67.0 71.9 67.8 system Wong & Dras Style CFG TSG C&J SVM-TK Style CFG TSG C&J SVM-TK CPU time minutes minutes minutes an hour weeks Table 3: Fine-grained classification accuracy (the Wong and Dras (2010) score is the highest score from the last column of their Table 3). system Pighin & Moschitti Bigram CFG TSG C&J SVM-TK accuracy 86.6 73.2 90.0 85.6 89.6 87.7 CPU time seconds seconds seconds minutes twenty min. Table 4: Question classification (6 classes). into the parse trees from the positive data using GenERRate (Foster and Andersen, 2009). An example sentence pair is But the ballplayers disagree[ing], where the negative example incorrectly inflects the verb. Wong and Dras (2010) reported good results with parsers trained separately on the positive and negative sides of the training data and classifiers built from comparisons between the CFG productions of those parsers. We obtained their data splits (described as NoisyWSJ in their paper) and repeat their experiments here (Table 3). 3.3 voting 75.3 73.2 72.1 76.3 69.5 65.3 52.6 56.8 66.3 52.6 whole 80.0 86.8 83.7 83.2 86.3 83.7 86.3 84.7 81.1 - Table 5: Accuracy on ICLE (7 lang"
P13-2150,P11-2038,1,0.899528,"h Processing Johns Hopkins University Baltimore, MD Abstract have been applied successfully in many areas of NLP (Collins and Duffy, 2002; Moschitti, 2004; Pighin and Moschitti, 2009). Tree kernels were inspired in part by ideas from Data-Oriented Parsing (Scha, 1990; Bod, 1993), which was in turn motivated by uncertainty about which fragments to include in a grammar. However, manual and automatic approaches to inducing tree fragments have recently been found to be useful in an explicit approach to text classification, which employs specific tree fragments as features in standard classifiers (Post, 2011; Wong and Dras, 2011; Swanson and Charniak, 2012). These feature sets necessarily represent only a small subset of all possible tree patterns, leaving open the question of what further gains might be had from the unusued fragments. Somewhat surprisingly, explicit and implicit syntactic features have been explored largely independently. Here, we compare them on a range of classification tasks: (1,2) grammatical classification (is a sentence written by a human?), (3) question classification (what type of answer is sought by this question?), and (4,5) native language prediction (what is the nati"
P13-2150,W09-3607,0,0.0619558,"Missing"
P13-2150,C02-1150,0,0.028096,"ough cues to identify native languages at smaller granularities. As such, this task presents a challenge to tree kernels, which are defined at the level of a single parse tree and have no obvious document-level extension. Table 5 therefore presents three evaluations: (a) sentencelevel accuracy, and document-level accuracy from (b) sentence-level voting and (c) direct, whole-document classification. We perform these experiments on two datasets. In order to mitigate topic bias10 and other problems that have been reported with Question Classification We look next at question classification (QC). Li and Roth (2002) introduced the TREC-10 dataset,8 a set of questions paired with labels that categorize the question by the type of answer it seeks. The labels are organized hierarchically into six (coarse) top-level labels and fifty (fine) refinements. An example question from the ENTY/animal category is What was the first domesticated bird?. Table 4 contains results predicting just the coarse labels. We compare to Pighin and Moschitti (2009), and also repeat their experiments, finding a slightly better result for them. 8 sent. 42.0 39.5 38.7 42.9 40.7 42.5 39.2 40.4 49.2 42.1 9 Pighin and Moschitti (2009) d"
P13-2150,W03-0402,0,0.0305958,"epresented subset of them. In addition to their flexibility and interpetability, explicit syntactic features often outperformed tree kernels in accuracy, and even where they did not, the cost was multiple orders of magnitude increase in both training and testing time. These results were consistent across a range of task types, dataset sizes, and classification arities (binary and multiclass). There are a number of important caveats. We explored a range of data settings, but there are many others where tree kernels have been proven useful, such as parse tree reranking (Collins and Duffy, 2002; Shen and Joshi, 2003), sentence subjectivity (Suzuki et al., 2004), pronoun resolution (Yang et al., 2006), relation extraction (Culotta and Sorensen, 2004), machine translation evaluation (Liu and Gildea, 2005), predicate-argument recognition, and semantic role labeling (Pighin and Moschitti, 2009). There are also tree kernel variations such as dependency tree kernels (Culotta and Sorensen, 2004) and shallow semantic tree kernels (Moschitti et al., 2007). These variables provide a rich environment for future work; in the meantime, we take these results as compelling motivation for the continued development of exp"
P13-2150,P04-1016,0,0.032236,"r flexibility and interpetability, explicit syntactic features often outperformed tree kernels in accuracy, and even where they did not, the cost was multiple orders of magnitude increase in both training and testing time. These results were consistent across a range of task types, dataset sizes, and classification arities (binary and multiclass). There are a number of important caveats. We explored a range of data settings, but there are many others where tree kernels have been proven useful, such as parse tree reranking (Collins and Duffy, 2002; Shen and Joshi, 2003), sentence subjectivity (Suzuki et al., 2004), pronoun resolution (Yang et al., 2006), relation extraction (Culotta and Sorensen, 2004), machine translation evaluation (Liu and Gildea, 2005), predicate-argument recognition, and semantic role labeling (Pighin and Moschitti, 2009). There are also tree kernel variations such as dependency tree kernels (Culotta and Sorensen, 2004) and shallow semantic tree kernels (Moschitti et al., 2007). These variables provide a rich environment for future work; in the meantime, we take these results as compelling motivation for the continued development of explicit syntactic features (both manual and aut"
P13-2150,P12-2038,0,0.0688052,"sity Baltimore, MD Abstract have been applied successfully in many areas of NLP (Collins and Duffy, 2002; Moschitti, 2004; Pighin and Moschitti, 2009). Tree kernels were inspired in part by ideas from Data-Oriented Parsing (Scha, 1990; Bod, 1993), which was in turn motivated by uncertainty about which fragments to include in a grammar. However, manual and automatic approaches to inducing tree fragments have recently been found to be useful in an explicit approach to text classification, which employs specific tree fragments as features in standard classifiers (Post, 2011; Wong and Dras, 2011; Swanson and Charniak, 2012). These feature sets necessarily represent only a small subset of all possible tree patterns, leaving open the question of what further gains might be had from the unusued fragments. Somewhat surprisingly, explicit and implicit syntactic features have been explored largely independently. Here, we compare them on a range of classification tasks: (1,2) grammatical classification (is a sentence written by a human?), (3) question classification (what type of answer is sought by this question?), and (4,5) native language prediction (what is the native language of a text’s author?). Our main contrib"
P13-2150,C12-1158,0,0.014303,"raining 1: Training (1000s of test accuracy for coarse grammaticality, plotting test scores from models trained on 100, 300, 1k, 3k, 10k, 30k, and 100k instances. ICLE v.2 4 The first dataset is a seven-language subset of the International Corpus of Learner English, Version 2 (ICLE) (Granger et al., 2009), which contains 3.7 million words of English documents written by people with sixteen different native languages. Table 1 contains scores, including one reported by Wong and Dras (2011), who used the CFG and C&J features, and whose data splits we mirror.15 3.4.2 90 accuracy the ICLE dataset (Tetreault et al., 2012),11 we preprocessed each dataset into two signaturestylized versions by replacing all words not in a stopword list.12 The first version replaces nonstopwords with word classes computed from surface-form signatures,13 and the second with POS tags.14 N-gram features are then taken from both stylized versions of the corpus. Restricting the feature representation to be topic-independent is standard-practice in stylometric tasks like authorship attribution, gender identification, and native-language identification (Mosteller and Wallace, 1984; Koppel et al., 2003; Tomokiyo and Jones, 2001). Discuss"
P13-2150,N01-1031,0,0.0210095,"E dataset (Tetreault et al., 2012),11 we preprocessed each dataset into two signaturestylized versions by replacing all words not in a stopword list.12 The first version replaces nonstopwords with word classes computed from surface-form signatures,13 and the second with POS tags.14 N-gram features are then taken from both stylized versions of the corpus. Restricting the feature representation to be topic-independent is standard-practice in stylometric tasks like authorship attribution, gender identification, and native-language identification (Mosteller and Wallace, 1984; Koppel et al., 2003; Tomokiyo and Jones, 2001). Discussion Syntactic features improve upon the n-gram baseline for all tasks except whole-document classification for ICLE. Tree kernels are often among the best, but always trail (by orders of magnitude) when runtime is considered. Constructing the multi-class SVM-TK models for the NLI tasks in particular was computationally burdensome, requiring cpu-months of time. The C&J features are similarly often the best, but incur a runtime cost due to the large models. CFG and TSG features balance performance, model size, and runtime. We now compare these approaches in more depth. ACL Anthology Net"
P13-2150,U10-1011,0,0.110642,"it/moschitti/Tree-Kernel.htm 5 Optimizing SVM-TK’s decay parameter (-L) did not improve test-set accuracy, but did increase training time (squaring the number of hyperparameter combinations to evaluate), so we stuck with the default. 6 Increased from the default of 40 MB, which halves the running time. 7 867 LDC Catalog No. LDC2000T43 system Wong & Dras Chance N-gram CFG TSG C&J SVM-TK accuracy 60.6 50.0 61.4 64.5 67.0 71.9 67.8 system Wong & Dras Style CFG TSG C&J SVM-TK Style CFG TSG C&J SVM-TK CPU time minutes minutes minutes an hour weeks Table 3: Fine-grained classification accuracy (the Wong and Dras (2010) score is the highest score from the last column of their Table 3). system Pighin & Moschitti Bigram CFG TSG C&J SVM-TK accuracy 86.6 73.2 90.0 85.6 89.6 87.7 CPU time seconds seconds seconds minutes twenty min. Table 4: Question classification (6 classes). into the parse trees from the positive data using GenERRate (Foster and Andersen, 2009). An example sentence pair is But the ballplayers disagree[ing], where the negative example incorrectly inflects the verb. Wong and Dras (2010) reported good results with parsers trained separately on the positive and negative sides of the training data a"
P13-2150,D11-1148,0,0.134143,"Johns Hopkins University Baltimore, MD Abstract have been applied successfully in many areas of NLP (Collins and Duffy, 2002; Moschitti, 2004; Pighin and Moschitti, 2009). Tree kernels were inspired in part by ideas from Data-Oriented Parsing (Scha, 1990; Bod, 1993), which was in turn motivated by uncertainty about which fragments to include in a grammar. However, manual and automatic approaches to inducing tree fragments have recently been found to be useful in an explicit approach to text classification, which employs specific tree fragments as features in standard classifiers (Post, 2011; Wong and Dras, 2011; Swanson and Charniak, 2012). These feature sets necessarily represent only a small subset of all possible tree patterns, leaving open the question of what further gains might be had from the unusued fragments. Somewhat surprisingly, explicit and implicit syntactic features have been explored largely independently. Here, we compare them on a range of classification tasks: (1,2) grammatical classification (is a sentence written by a human?), (3) question classification (what type of answer is sought by this question?), and (4,5) native language prediction (what is the native language of a text"
P13-2150,P06-1006,0,0.0175355,"t syntactic features often outperformed tree kernels in accuracy, and even where they did not, the cost was multiple orders of magnitude increase in both training and testing time. These results were consistent across a range of task types, dataset sizes, and classification arities (binary and multiclass). There are a number of important caveats. We explored a range of data settings, but there are many others where tree kernels have been proven useful, such as parse tree reranking (Collins and Duffy, 2002; Shen and Joshi, 2003), sentence subjectivity (Suzuki et al., 2004), pronoun resolution (Yang et al., 2006), relation extraction (Culotta and Sorensen, 2004), machine translation evaluation (Liu and Gildea, 2005), predicate-argument recognition, and semantic role labeling (Pighin and Moschitti, 2009). There are also tree kernel variations such as dependency tree kernels (Culotta and Sorensen, 2004) and shallow semantic tree kernels (Moschitti et al., 2007). These variables provide a rich environment for future work; in the meantime, we take these results as compelling motivation for the continued development of explicit syntactic features (both manual and automatically induced), and suggest that su"
P15-2097,W13-3601,1,0.745346,"l shared tasks (Dale et al., 2012; Dale and Kilgarriff, 2011), where participants were asked to detect and correct closedclass errors (i.e., determiners and prepositions). One of the issues with F-score is that it fails to capture phrase-level edits. Thus Dahlmeier and Ng (2012) proposed the MaxMatch (M2 ) scorer, which calculates the F-score over an edit lattice that captures phrase-level edits. For GEC, M2 is the standard, having been used to rank error correction systems in the 2013 and 2014 CoNLL shared tasks, where the error types to be corrected were not limited to closed-class errors. (Ng et al., 2013; Ng et al., 2014). M2 was assessed by comparing its output against that of the official Helping Our Own (HOO) scorer (Dale and Kilgarriff, 2011), itself based on the GNU wdiff utility.2 In other words, it was evaluated under the assumption that evaluating GEC can be reduced to checking whether a set of predefined errors have been changed into a set of associated corrections. M2 is not without its own issues. First, phraselevel edits can be gamed because the lattice treats a long phrase deletion as one edit.3 Second, the F-score does not capture the difference between “no change” and “wrong ed"
P15-2097,P15-1068,0,0.296006,"tion between the M2 and I-measure scores (Pearson’s r = −0.694). A difficulty with all these metrics is that they require detailed annotations of the location and erBLEU I-Measure 87 2 1 86 0 85 -1 84 -2 -3 83 -4 82 -5 81 -6 0 10 20 30 40 50 M2 Score Figure 1: Correlation among M2 , I-measure, and BLEU scores: M2 score shows negative correlations to other metrics. ror type of each correction in response to an explicit error annotation scheme. Due to the inherent subjectivity and poor definition of the task, mentioned above, it is difficult for annotators to reliably produce these annotations (Bryant and Ng, 2015). However, this requirement can be relinquished by treating GEC as a text-to-text rewriting task and borrowing metrics from machine translation, as Park and Levy (2011) did with BLEU (Papineni et al., 2002) and METEOR (Lavie and Agarwal, 2007). As we will show in more detail in Section 5, taking the twelve publicly released system outputs from the CoNLL-2014 Shared Task,4 we actually find a negative correlation between the M2 and BLEU scores (r = −0.772) and positive correlation between I-measure and BLEU scores (r = 0.949) (Figure 1). With the earlier-reported negative correlation between I-m"
P15-2097,W14-1701,0,0.457886,"ale et al., 2012; Dale and Kilgarriff, 2011), where participants were asked to detect and correct closedclass errors (i.e., determiners and prepositions). One of the issues with F-score is that it fails to capture phrase-level edits. Thus Dahlmeier and Ng (2012) proposed the MaxMatch (M2 ) scorer, which calculates the F-score over an edit lattice that captures phrase-level edits. For GEC, M2 is the standard, having been used to rank error correction systems in the 2013 and 2014 CoNLL shared tasks, where the error types to be corrected were not limited to closed-class errors. (Ng et al., 2013; Ng et al., 2014). M2 was assessed by comparing its output against that of the official Helping Our Own (HOO) scorer (Dale and Kilgarriff, 2011), itself based on the GNU wdiff utility.2 In other words, it was evaluated under the assumption that evaluating GEC can be reduced to checking whether a set of predefined errors have been changed into a set of associated corrections. M2 is not without its own issues. First, phraselevel edits can be gamed because the lattice treats a long phrase deletion as one edit.3 Second, the F-score does not capture the difference between “no change” and “wrong edits” made by syste"
P15-2097,C12-1038,1,0.763083,"trics should be compared. That is, what is the metric metric? The answer is that it should be rooted in the end-use case for the task under consideration. This could be some other metric further downstream of the task, or something simpler like direct human evaluation. This latter approach is the one often taken in machine translation; for example, the organizers of the Workshop on Statistical Machine 2 Grammatical error correction metrics GEC is often viewed as a matter of correcting isolated grammatical errors, but is much more complicated, nuanced, and subjective than that. As discussed in Chodorow et al. (2012), there is often no single correction for an error (e.g., whether to correct a subject-verb agreement error by changing the number of the subject or the verb), and errors cover a range of factors including style, register, venue, audience, and usage questions, about 1 Our code and rankings of the CoNLL-2014 Shared Task system outputs can be downloaded from github.com/ cnap/gec-ranking/. 588 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 588–593, c Beijing, Ch"
P15-2097,P02-1040,0,0.124111,"-3 83 -4 82 -5 81 -6 0 10 20 30 40 50 M2 Score Figure 1: Correlation among M2 , I-measure, and BLEU scores: M2 score shows negative correlations to other metrics. ror type of each correction in response to an explicit error annotation scheme. Due to the inherent subjectivity and poor definition of the task, mentioned above, it is difficult for annotators to reliably produce these annotations (Bryant and Ng, 2015). However, this requirement can be relinquished by treating GEC as a text-to-text rewriting task and borrowing metrics from machine translation, as Park and Levy (2011) did with BLEU (Papineni et al., 2002) and METEOR (Lavie and Agarwal, 2007). As we will show in more detail in Section 5, taking the twelve publicly released system outputs from the CoNLL-2014 Shared Task,4 we actually find a negative correlation between the M2 and BLEU scores (r = −0.772) and positive correlation between I-measure and BLEU scores (r = 0.949) (Figure 1). With the earlier-reported negative correlation between I-measure and M2 , we have a troubling picture: which of these metrics is best? Which one actually captures and rewards the behaviors we would like our systems to report? Despite these many proposed metrics, n"
P15-2097,N12-1067,0,0.459535,"existence of different style manuals at newspapers, and questions about the legitimacy of prescriptivist grammar conventions. Several automatic metrics have been used for evaluating GEC systems. F-score, the harmonic mean of precision and recall, is one of the most commonly used metrics. It was used as an official evaluation metric for several shared tasks (Dale et al., 2012; Dale and Kilgarriff, 2011), where participants were asked to detect and correct closedclass errors (i.e., determiners and prepositions). One of the issues with F-score is that it fails to capture phrase-level edits. Thus Dahlmeier and Ng (2012) proposed the MaxMatch (M2 ) scorer, which calculates the F-score over an edit lattice that captures phrase-level edits. For GEC, M2 is the standard, having been used to rank error correction systems in the 2013 and 2014 CoNLL shared tasks, where the error types to be corrected were not limited to closed-class errors. (Ng et al., 2013; Ng et al., 2014). M2 was assessed by comparing its output against that of the official Helping Our Own (HOO) scorer (Dale and Kilgarriff, 2011), itself based on the GNU wdiff utility.2 In other words, it was evaluated under the assumption that evaluating GEC can"
P15-2097,P11-1094,0,0.0260818,"U I-Measure 87 2 1 86 0 85 -1 84 -2 -3 83 -4 82 -5 81 -6 0 10 20 30 40 50 M2 Score Figure 1: Correlation among M2 , I-measure, and BLEU scores: M2 score shows negative correlations to other metrics. ror type of each correction in response to an explicit error annotation scheme. Due to the inherent subjectivity and poor definition of the task, mentioned above, it is difficult for annotators to reliably produce these annotations (Bryant and Ng, 2015). However, this requirement can be relinquished by treating GEC as a text-to-text rewriting task and borrowing metrics from machine translation, as Park and Levy (2011) did with BLEU (Papineni et al., 2002) and METEOR (Lavie and Agarwal, 2007). As we will show in more detail in Section 5, taking the twelve publicly released system outputs from the CoNLL-2014 Shared Task,4 we actually find a negative correlation between the M2 and BLEU scores (r = −0.772) and positive correlation between I-measure and BLEU scores (r = 0.949) (Figure 1). With the earlier-reported negative correlation between I-measure and M2 , we have a troubling picture: which of these metrics is best? Which one actually captures and rewards the behaviors we would like our systems to report?"
P15-2097,W11-2838,0,0.028626,"g (Short Papers), pages 588–593, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics which there can be much disagreement. In addition, errors are not always errors, as can be seen from the existence of different style manuals at newspapers, and questions about the legitimacy of prescriptivist grammar conventions. Several automatic metrics have been used for evaluating GEC systems. F-score, the harmonic mean of precision and recall, is one of the most commonly used metrics. It was used as an official evaluation metric for several shared tasks (Dale et al., 2012; Dale and Kilgarriff, 2011), where participants were asked to detect and correct closedclass errors (i.e., determiners and prepositions). One of the issues with F-score is that it fails to capture phrase-level edits. Thus Dahlmeier and Ng (2012) proposed the MaxMatch (M2 ) scorer, which calculates the F-score over an edit lattice that captures phrase-level edits. For GEC, M2 is the standard, having been used to rank error correction systems in the 2013 and 2014 CoNLL shared tasks, where the error types to be corrected were not limited to closed-class errors. (Ng et al., 2013; Ng et al., 2014). M2 was assessed by compari"
P15-2097,W14-3301,1,0.884876,"Missing"
P15-2097,W12-2006,0,0.0374068,"Language Processing (Short Papers), pages 588–593, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics which there can be much disagreement. In addition, errors are not always errors, as can be seen from the existence of different style manuals at newspapers, and questions about the legitimacy of prescriptivist grammar conventions. Several automatic metrics have been used for evaluating GEC systems. F-score, the harmonic mean of precision and recall, is one of the most commonly used metrics. It was used as an official evaluation metric for several shared tasks (Dale et al., 2012; Dale and Kilgarriff, 2011), where participants were asked to detect and correct closedclass errors (i.e., determiners and prepositions). One of the issues with F-score is that it fails to capture phrase-level edits. Thus Dahlmeier and Ng (2012) proposed the MaxMatch (M2 ) scorer, which calculates the F-score over an edit lattice that captures phrase-level edits. For GEC, M2 is the standard, having been used to rank error correction systems in the 2013 and 2014 CoNLL shared tasks, where the error types to be corrected were not limited to closed-class errors. (Ng et al., 2013; Ng et al., 2014)"
P15-2097,N15-1060,0,0.534138,"f utility.2 In other words, it was evaluated under the assumption that evaluating GEC can be reduced to checking whether a set of predefined errors have been changed into a set of associated corrections. M2 is not without its own issues. First, phraselevel edits can be gamed because the lattice treats a long phrase deletion as one edit.3 Second, the F-score does not capture the difference between “no change” and “wrong edits” made by systems. Chodorow et al. (2012) also list other complications arising from using F-score or M2 , depending on the application of GEC. Considering these problems, Felice and Briscoe (2015) proposed a new metric, I-measure, which is based on accuracy computed by edit distance between the source, reference, and system output. Their results are striking: there is a negative correlation between the M2 and I-measure scores (Pearson’s r = −0.694). A difficulty with all these metrics is that they require detailed annotations of the location and erBLEU I-Measure 87 2 1 86 0 85 -1 84 -2 -3 83 -4 82 -5 81 -6 0 10 20 30 40 50 M2 Score Figure 1: Correlation among M2 , I-measure, and BLEU scores: M2 score shows negative correlations to other metrics. ror type of each correction in response"
P15-2097,W07-0734,0,\N,Missing
P17-2030,P16-1070,0,0.0764261,"sy-first strategy. The decoder iteratively corrects the most probable ungrammatical token by applying different classifiers for each error type. The EREF parser also depends on the easy-first strategy to find ungrammatical index to be deleted, inserted, or substituted, but it parses and corrects errors jointly whereas the decoder is designed as a grammatical error correction framework rather than a parser. There is a line of work for parsing ungrammatical sentences (e.g., web forum) by adapting an existing parsing scheme on domain specific annotations (Petrov and McDonald, 2012; Cahill, 2015; Berzak et al., 2016; Nagata and Sakaguchi, 2016). Although we share an interest with respect to dealing with ungrammatical sentences, EREF focuses on the parsing scheme for repairing grammatical errors instead of adapting a parser with a domain specific annotation scheme. More broadly, our work can also be regarded as one of the joint parsing and text normalization tasks such as joint spelling correction and POS tagging (Sakaguchi et al., 2012), word segmentation and POS tagging (Kaji and Kitsuregawa, 2014; Qian et al., 2015). 5 Acknowledgments This work was supported in part by the JHU Human Language Technology"
P17-2030,W15-1616,0,0.0125666,"er with the easy-first strategy. The decoder iteratively corrects the most probable ungrammatical token by applying different classifiers for each error type. The EREF parser also depends on the easy-first strategy to find ungrammatical index to be deleted, inserted, or substituted, but it parses and corrects errors jointly whereas the decoder is designed as a grammatical error correction framework rather than a parser. There is a line of work for parsing ungrammatical sentences (e.g., web forum) by adapting an existing parsing scheme on domain specific annotations (Petrov and McDonald, 2012; Cahill, 2015; Berzak et al., 2016; Nagata and Sakaguchi, 2016). Although we share an interest with respect to dealing with ungrammatical sentences, EREF focuses on the parsing scheme for repairing grammatical errors instead of adapting a parser with a domain specific annotation scheme. More broadly, our work can also be regarded as one of the joint parsing and text normalization tasks such as joint spelling correction and POS tagging (Sakaguchi et al., 2012), word segmentation and POS tagging (Kaji and Kitsuregawa, 2014; Qian et al., 2015). 5 Acknowledgments This work was supported in part by the JHU Huma"
P17-2030,W02-1001,0,0.0923888,"tokens w1 , ..., wn , and it keeps updating pending through derivations. Unlike left-to-right (e.g., shift-reduce) parsing algorithms (Yamada and Matsumoto, 2003; Nivre, 2004), EF iteratively selects the best pair of adjoining tokens and chooses the direction of attachment: ATTACHLEFT or ATTACHRIGHT. Once the action is committed, the corresponding dependency arc is added and the child token is removed from pending. The first two derivations in Figure 1 depict ATTACHRIGHT and ATTACHLEFT. Pseudocode is shown in Algorithm 1 (lines 1, 3-12). The parser is trained using the structured perceptron (Collins, 2002) to choose actions to take given a set of features expanded from templates. The cost of actions is computed at every step by checking the validity: whether a new arc is included in the gold parse and whether the child already has all its children. See GE for further description of feature templates and structured perceptron training. Since it is possible that there are multiple valid sequence of actions and it is important to examine a large search space, the parser is allowed to explore (possibly incorrect) actions with a certain probability, termed learning with exploration by Goldberg and N"
P17-2030,D12-1052,0,0.0308823,"d interregnum) for a given speech utterance. Our work could be considered an extension via adding SUBSTITUTE and INSERT actions, although we depend on easy-first non-directional parsing framework instead of a left-to-right strategy. Importantly, the DELETE action is easier to handle than the SUBSTITUTE and INSERT actions, because they bring us challenging issues about a process of candidate word generation and avoiding an infinite loop in derivation. We have addressed these issues as explained in §2. In terms of the literature from grammatical error correction, this work is closely related to Dahlmeier and Ng (2012), where they show an error correction decoder with the easy-first strategy. The decoder iteratively corrects the most probable ungrammatical token by applying different classifiers for each error type. The EREF parser also depends on the easy-first strategy to find ungrammatical index to be deleted, inserted, or substituted, but it parses and corrects errors jointly whereas the decoder is designed as a grammatical error correction framework rather than a parser. There is a line of work for parsing ungrammatical sentences (e.g., web forum) by adapting an existing parsing scheme on domain specif"
P17-2030,J93-2004,0,0.0613868,". We do not deal with a swapping action (Nivre, 2009) to deal with word reordering errors, since the errors are even less frequent than other error types (Leacock et al., 2014). SUBSTITUTE replaces a token to a grammatically more probable token, DELETE removes an unnecessary token, and INSERT inserts a new token at a designated index. These actions are shown in Figure 1 and Algorithm 1 (lines 13-25). Because the length of pending decreases as an attachment occurs, the parser 190 In the first experiment, as in GE, we train and evaluate our parser on the English dataset from the Penn Treebank (Marcus et al., 1993) with the Penn2Malt conversion program (Sections 2-21 for training, 22 for tuning, and 23 for test). We use the PTB for the dependency experiment, since there are no ungrammatical text corpora that has dependency annotation on the corrected texts by human. We choose the following most frequent error types that are used in CoNLL 2013 shared task (Ng et al., 2013): Algorithm 2: Check validity during training 1 2 3 4 5 6 7 Function isValid(act, repaired, Gold) d before = editDistance(repaired, Gold) repaired + = repaired.apply(act) d after = editDistance(repaired + , Gold) if d before &gt; d after t"
P17-2030,P16-1173,1,0.797184,"e decoder iteratively corrects the most probable ungrammatical token by applying different classifiers for each error type. The EREF parser also depends on the easy-first strategy to find ungrammatical index to be deleted, inserted, or substituted, but it parses and corrects errors jointly whereas the decoder is designed as a grammatical error correction framework rather than a parser. There is a line of work for parsing ungrammatical sentences (e.g., web forum) by adapting an existing parsing scheme on domain specific annotations (Petrov and McDonald, 2012; Cahill, 2015; Berzak et al., 2016; Nagata and Sakaguchi, 2016). Although we share an interest with respect to dealing with ungrammatical sentences, EREF focuses on the parsing scheme for repairing grammatical errors instead of adapting a parser with a domain specific annotation scheme. More broadly, our work can also be regarded as one of the joint parsing and text normalization tasks such as joint spelling correction and POS tagging (Sakaguchi et al., 2012), word segmentation and POS tagging (Kaji and Kitsuregawa, 2014; Qian et al., 2015). 5 Acknowledgments This work was supported in part by the JHU Human Language Technology Center of Excellence (HLTCOE"
P17-2030,W09-2112,0,0.0856712,"Missing"
P17-2030,W12-3018,1,0.794323,"Missing"
P17-2030,N10-1115,0,0.0306583,"riety of noisy outputs, such as ungrammatical webpages, speech disfluencies, and the text in language learner’s essays. Such non-canonical text contains grammatical errors such as substitutions, insertions, and deletions. For example, a nonnative speaker of English might write “*I look in forward hear from you”, where in is inserted, to is deleted, and hearing is substituted incorrectly. We propose a novel dependency parsing scheme that jointly parses and repairs ungrammatical sentences with these sorts of errors. The parser is based on the non-directional easy-first (EF) parser introduced by Goldberg and Elhadad (2010) (GE herein), which iteratively adds the most probable arc until the parse tree is completed. These actions are called ATTACHLEFT and ATTACHRIGHT depending on the direction of the arc. We extend the EF parsing scheme to be robust for ungrammatical inputs by correcting grammatical erforward to hearing from you Figure 1: Illustrative example of partial derivation under error-repair easy-first non-directional dependency parsing. Solid arrows represent ATTACHRIGHT and ATTACHLEFT in Goldberg and Elhadad (2010). Dotted arcs correspond to actions for each step. Following the notation by GE: arcs are"
P17-2030,W13-3601,0,0.0221208,"ure 1 and Algorithm 1 (lines 13-25). Because the length of pending decreases as an attachment occurs, the parser 190 In the first experiment, as in GE, we train and evaluate our parser on the English dataset from the Penn Treebank (Marcus et al., 1993) with the Penn2Malt conversion program (Sections 2-21 for training, 22 for tuning, and 23 for test). We use the PTB for the dependency experiment, since there are no ungrammatical text corpora that has dependency annotation on the corrected texts by human. We choose the following most frequent error types that are used in CoNLL 2013 shared task (Ng et al., 2013): Algorithm 2: Check validity during training 1 2 3 4 5 6 7 Function isValid(act, repaired, Gold) d before = editDistance(repaired, Gold) repaired + = repaired.apply(act) d after = editDistance(repaired + , Gold) if d before &gt; d after then return true; else return false; end for ActsER is based on validity. The validity of the new actions is computed by taking the edit distance (d) between the Gold tokens (w1∗ ... wr∗ ) and the sentence state that the parser stores in repaired (w ˆ1 ... w ˆm ). When the edit distance after taking an action (d after ) is smaller than before (d before ), we rega"
P17-2030,Q13-1033,0,0.0179366,"Collins, 2002) to choose actions to take given a set of features expanded from templates. The cost of actions is computed at every step by checking the validity: whether a new arc is included in the gold parse and whether the child already has all its children. See GE for further description of feature templates and structured perceptron training. Since it is possible that there are multiple valid sequence of actions and it is important to examine a large search space, the parser is allowed to explore (possibly incorrect) actions with a certain probability, termed learning with exploration by Goldberg and Nivre (2013). act∈Acts∪Acts ER 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 s.t. 1 ≤ i ≤ len(pending) ∩ isLegal(act, pending) if best ∈ Acts then (parent, child) ← edgeFor(best) Arcs.add((parent, child)) pending.remove(child) else if best = SUBSTITUTE then c = bestCandidate(best, repaired) pending.replace(pi , c) repaired.replace(w ˆpi .idx , c) else if best = DELETE then pending.remove(pi ) repaired.remove(w ˆpi .idx ) Arcs.updateIndex() else if best = INSERT then c = bestCandidate(best, repaired) pending.insert(i, c) repaired.insert(pi .idx, c) Arcs.updateIndex() end return Arcs, repaired a"
P17-2030,W04-0308,0,0.0803614,"← w1 ...wn 5 repaired = w ˆ1 ...w ˆn ← w1 ...wn 6 while len (pending) &gt; 1 do 7 best ← argmax score (act (i)) Model Non-directional Easy-first Parsing Let us begin with a brief review of a non-directional easyfirst (EF) parsing scheme proposed by GE, which is the foundation of our proposed scheme described in the following sections. The EF parser has a list of partial structures p1 , ..., pk (called pending) initialized with sentence tokens w1 , ..., wn , and it keeps updating pending through derivations. Unlike left-to-right (e.g., shift-reduce) parsing algorithms (Yamada and Matsumoto, 2003; Nivre, 2004), EF iteratively selects the best pair of adjoining tokens and chooses the direction of attachment: ATTACHLEFT or ATTACHRIGHT. Once the action is committed, the corresponding dependency arc is added and the child token is removed from pending. The first two derivations in Figure 1 depict ATTACHRIGHT and ATTACHLEFT. Pseudocode is shown in Algorithm 1 (lines 1, 3-12). The parser is trained using the structured perceptron (Collins, 2002) to choose actions to take given a set of features expanded from templates. The cost of actions is computed at every step by checking the validity: whether a new"
P17-2030,W11-2123,0,0.0477824,"Missing"
P17-2030,P09-1040,0,0.028119,"ding, repaired, and parents and children in a (partial) dependency tree (Arcs). To find the best candidate for SUBSTITUTE and INSERT efficiently, we restrict candidates to the same part-of-speech or pre-defined candidate list. We select the best candidate by comparing each n-gram language model score with the same surrounding context. Similar to EF, while training the parser, the cost Error-repair variant of EF Error-repair nondirectional easy-first parsing scheme (EREF) is a variant of EF. We add three new actions: SUBSTITUTE, DELETE, INSERT as ActsER . We do not deal with a swapping action (Nivre, 2009) to deal with word reordering errors, since the errors are even less frequent than other error types (Leacock et al., 2014). SUBSTITUTE replaces a token to a grammatically more probable token, DELETE removes an unnecessary token, and INSERT inserts a new token at a designated index. These actions are shown in Figure 1 and Algorithm 1 (lines 13-25). Because the length of pending decreases as an attachment occurs, the parser 190 In the first experiment, as in GE, we train and evaluate our parser on the English dataset from the Penn Treebank (Marcus et al., 1993) with the Penn2Malt conversion pro"
P17-2030,P14-2029,0,0.0728258,"Missing"
P17-2030,D15-1211,0,0.0393197,"Missing"
P17-2030,Q14-1011,0,0.0174173,"d the total number of edits during derivation. The experimental result has demonstrated robustness of EREF parsers against EF and grammaticality improvement. Our work is positioned at the intersection of noisy text parsing and grammatical error correction. The EREF is a flexible formalism not only for grammatical error correction but other tasks with jointly editing and parsing a given sentence. Our work lies at the intersection of parsing noncanonical texts and grammatical error correction. Joint dependency parsing and disfluency detection has been pursued (Rasooli and Tetreault, 2013, 2014; Honnibal and Johnson, 2014; Wu et al., 2015; Yoshikawa et al., 2016), where a parser jointly parses and detects disfluency (e.g., reparandum and interregnum) for a given speech utterance. Our work could be considered an extension via adding SUBSTITUTE and INSERT actions, although we depend on easy-first non-directional parsing framework instead of a left-to-right strategy. Importantly, the DELETE action is easier to handle than the SUBSTITUTE and INSERT actions, because they bring us challenging issues about a process of candidate word generation and avoiding an infinite loop in derivation. We have addressed these issu"
P17-2030,D14-1011,0,0.027595,"Missing"
P17-2030,D13-1013,0,0.0197434,"f editing history for each token and the total number of edits during derivation. The experimental result has demonstrated robustness of EREF parsers against EF and grammaticality improvement. Our work is positioned at the intersection of noisy text parsing and grammatical error correction. The EREF is a flexible formalism not only for grammatical error correction but other tasks with jointly editing and parsing a given sentence. Our work lies at the intersection of parsing noncanonical texts and grammatical error correction. Joint dependency parsing and disfluency detection has been pursued (Rasooli and Tetreault, 2013, 2014; Honnibal and Johnson, 2014; Wu et al., 2015; Yoshikawa et al., 2016), where a parser jointly parses and detects disfluency (e.g., reparandum and interregnum) for a given speech utterance. Our work could be considered an extension via adding SUBSTITUTE and INSERT actions, although we depend on easy-first non-directional parsing framework instead of a left-to-right strategy. Importantly, the DELETE action is easier to handle than the SUBSTITUTE and INSERT actions, because they bring us challenging issues about a process of candidate word generation and avoiding an infinite loop in deriva"
P17-2030,E14-4010,0,0.0333871,"Missing"
P17-2030,roark-etal-2006-sparseval,0,0.0914786,"Missing"
P17-2030,Q14-1033,0,0.023988,"Missing"
P17-2030,C12-1144,1,0.836526,"Missing"
P17-2030,P15-1048,0,0.0129816,"during derivation. The experimental result has demonstrated robustness of EREF parsers against EF and grammaticality improvement. Our work is positioned at the intersection of noisy text parsing and grammatical error correction. The EREF is a flexible formalism not only for grammatical error correction but other tasks with jointly editing and parsing a given sentence. Our work lies at the intersection of parsing noncanonical texts and grammatical error correction. Joint dependency parsing and disfluency detection has been pursued (Rasooli and Tetreault, 2013, 2014; Honnibal and Johnson, 2014; Wu et al., 2015; Yoshikawa et al., 2016), where a parser jointly parses and detects disfluency (e.g., reparandum and interregnum) for a given speech utterance. Our work could be considered an extension via adding SUBSTITUTE and INSERT actions, although we depend on easy-first non-directional parsing framework instead of a left-to-right strategy. Importantly, the DELETE action is easier to handle than the SUBSTITUTE and INSERT actions, because they bring us challenging issues about a process of candidate word generation and avoiding an infinite loop in derivation. We have addressed these issues as explained i"
P17-2030,W03-3023,0,0.162372,"= { } 4 pending = p1 ...pn ← w1 ...wn 5 repaired = w ˆ1 ...w ˆn ← w1 ...wn 6 while len (pending) &gt; 1 do 7 best ← argmax score (act (i)) Model Non-directional Easy-first Parsing Let us begin with a brief review of a non-directional easyfirst (EF) parsing scheme proposed by GE, which is the foundation of our proposed scheme described in the following sections. The EF parser has a list of partial structures p1 , ..., pk (called pending) initialized with sentence tokens w1 , ..., wn , and it keeps updating pending through derivations. Unlike left-to-right (e.g., shift-reduce) parsing algorithms (Yamada and Matsumoto, 2003; Nivre, 2004), EF iteratively selects the best pair of adjoining tokens and chooses the direction of attachment: ATTACHLEFT or ATTACHRIGHT. Once the action is committed, the corresponding dependency arc is added and the child token is removed from pending. The first two derivations in Figure 1 depict ATTACHRIGHT and ATTACHLEFT. Pseudocode is shown in Algorithm 1 (lines 1, 3-12). The parser is trained using the structured perceptron (Collins, 2002) to choose actions to take given a set of features expanded from templates. The cost of actions is computed at every step by checking the validity:"
P17-2030,D16-1109,0,0.0201967,". The experimental result has demonstrated robustness of EREF parsers against EF and grammaticality improvement. Our work is positioned at the intersection of noisy text parsing and grammatical error correction. The EREF is a flexible formalism not only for grammatical error correction but other tasks with jointly editing and parsing a given sentence. Our work lies at the intersection of parsing noncanonical texts and grammatical error correction. Joint dependency parsing and disfluency detection has been pursued (Rasooli and Tetreault, 2013, 2014; Honnibal and Johnson, 2014; Wu et al., 2015; Yoshikawa et al., 2016), where a parser jointly parses and detects disfluency (e.g., reparandum and interregnum) for a given speech utterance. Our work could be considered an extension via adding SUBSTITUTE and INSERT actions, although we depend on easy-first non-directional parsing framework instead of a left-to-right strategy. Importantly, the DELETE action is easier to handle than the SUBSTITUTE and INSERT actions, because they bring us challenging issues about a process of candidate word generation and avoiding an infinite loop in derivation. We have addressed these issues as explained in §2. In terms of the lit"
Q13-1014,P05-1074,1,0.728369,"Missing"
Q13-1014,W08-0208,0,0.0265935,", and supervised learning, respectively. In real MT systems, these problems are highly interdependent, a point we emphasized in class and at the end of each assignment—for example, that alignment is an exercise in parameter estimation for translation models, that model choice is a tradeoff between expressivity and efficient inference, and that optimal search does not guarantee optimal accuracy. However, presenting each problem independently and holding all else constant enables more focused exploration. For each problem we provided data, a na¨ıve solution, and an evaluation program. Following Bird et al. (2008) and Madnani and Dorr (2008), we implemented the challenges in Python, a high-level pro1 http://alopez.github.io/dreamt 166 gramming language that can be used to write very concise programs resembling pseudocode.2,3 By default, each baseline system reads the test data and generates output in the evaluation format, so setup required zero configuration, and students could begin experimenting immediately. For example, on receipt of the alignment code, aligning data and evaluating results required only typing: > align |grade Students could then run experiments within minutes of beginning the assig"
Q13-1014,W11-2101,0,0.0267628,"Missing"
Q13-1014,J93-2003,0,0.0664021,"nload/hansard/ 9 This invited the possibility of cheating, since alignments of the test data are publicly available on the web. We did not advertise this, but as an added safeguard we obfuscated the data by distributing the test sentences randomly throughout the file. 20 AER × 100 Listing 1 The default aligner in DREAMT: thresholding Dice’s coefficient. for (f, e) in bitext: for f_i in set(f): f_count[f_i] += 1 for e_j in set(e): fe_count[(f_i,e_j)] += 1 for e_j in set(e): e_count[e_j] += 1 30 40 50 60 due ays ays ays -2 d -4 d -6 d ays days -8 d 168 days We privately implemented IBM Model 1 (Brown et al., 1993) as the target algorithm for a passing grade. We ran it for five iterations with English as the target language and French as the source. Our implementation did not use null alignment or symmetrization—leaving out these common improvements offered students the possibility of discovering them independently, and thereby rewarded. -10 > align -n 1000 |grade By varying the number of input sentences and the threshold for an alignment, students could immediately see the effect of various parameters on alignment quality. days The default implementation enabled immediate experimentation. On receipt of"
Q13-1014,W09-0401,1,0.880881,"Missing"
Q13-1014,W11-2103,1,0.932739,"between the human ranking and an output ranking. The check program simply ensures that a submission contains a valid ranking. We were concerned about hill-climbing on the test data, so we modified the leaderboard to report new results only twice a day. This encouraged students to experiment on the development data before posting new submissions, while still providing intermittent feedback. We privately implemented a version of BLEU, which obtained a correlation of 38.6 with the human rankings, a modest improvement over the baseline of 34.0. Our implementation underperforms the one reported in Callison-Burch et al. (2011) since it performs no tokenization or normalization of the data. This also left room for improvement. 0.8 Spearman’s ρ tems, computed from pairwise human judgements according to a formula recommended by Bojar et al. (2011).12 Figure 3: Submission history for the evaluation challenge. • Implementation of several techniques used in AMBER (Chen and Kuhn, 2005). The best submission, obtaining a correlation of 83.5, relied on the idea that the reference and machine translation should be good paraphrases of each other (Owczarzak et al., 2006; Kauchak and Barzilay, 2006). It employed a simple paraphr"
Q13-1014,W12-3102,1,0.884682,"Missing"
Q13-1014,D11-1003,0,0.016178,"4). Our implementation increased the codebase by 44 lines of code and included parameters for beam size, distortion limit, and the maximum number of translations considered for each input phrase. We posted a baseline to the leaderboard using values of 50, 3, and 20 for these, respectively. • Implementation of a private oracle. Some students observed that the ideal beam setting was not uniform across the corpus. They ran their decoder under different settings, and then selected the most probable translation of each sentence. 11 We implemented a version of the Lagrangian relaxation algorithm of Chang and Collins (2011), but found it difficult to obtain tight (optimal) solutions without iteratively reintroducing all of the original constraints. We suspect this is due to the lack of a distortion penalty, which enforces a strong preference towards translations with little reordering. However, the solution found by this algorithm is only approximates the objective implied by Equation 2, which sums over alignments. 170 Many teams who implemented the standard stack decoding algorithm experimented heavily with its pruning parameters. The best submission used extremely wide beam settings in conjunction with a reimp"
Q13-1014,W00-0601,0,0.0575064,"5 (strongly agree). Several questions pertained to assignments (Table 1), and allay two possible concerns about competition: most students felt that the assignments enhanced their collaborative skills, and that their open-endedness did not result in an overload of work. For all survey questions, student satisfaction was higher than average for courses in our department. 8 Discussion DREAMT is inspired by several different approaches to teaching NLP, AI, and computer science. Eisner and Smith (2008) teach NLP using a competitive game in which students aim to write fragments of English grammar. Charniak et al. (2000) improve the state-of-the-art in a reading comprehension task as part of a group project. Christopher et al. (1993) use NACHOS, a classic tool for teaching operating systems by providing a rudimentary system that students then augment. DeNero and Klein (2010) devise a series of assignments based on Pac-Man, for which students implement several classic AI techniques. A crucial element in such approaches is a highly functional but simple scaffolding. The DREAMT codebase, including grading and 174 validation scripts, consists of only 656 lines of code (LOC) over four assignments: 141 LOC for alig"
Q13-1014,J07-2003,0,0.0183732,"improvement, and thus competition. We told students the exact algorithm that produced the target accuracy (though we expected them to derive it themselves based on lectures, notes, or literature). We did not specifically require them to implement it, but the guarantee of a passing grade provided a powerful incentive for this to be the first step of each assignment. Submissions that beat this target received additional credit. The top five submissions received full credit, while the top three received extra credit. 2 http://python.org Some well-known MT systems have been implemented in Python (Chiang, 2007; Huang and Chiang, 2007). 4 Thanks to an anonymous reviewer for this turn of phrase. 3 This scheme provided strong incentive to continue experimentation beyond the target algorithm.5 For each assignment, students could form teams of any size, under three rules: each team had to publicize its formation to the class, all team members agreed to receive the same grade, and teams could not drop members. Our hope was that these requirements would balance the perceived competitive advantage of collaboration against a reluctance to take (and thus support) teammates who did not contribute to the compe"
Q13-1014,P08-2007,0,0.0256888,"lement consists of a span in both e and f such that every word in both e and f is aligned exactly once, the conditional probability of e and a given f is as follows.10 p(e, a|f ) = Y hi,i0 ,j,j 0 i∈a 0 0 p(fii |ejj ) J+1 Y j=1 p(ej |ej−1 , ej−2 ) (1) To evaluate output, we compute the conditional probability of e as follows. p(e|f ) = X p(e, a|f ) (2) a Note that this formulation is different from the typical Viterbi objective of standard beam search decoders, which do not sum over all alignments, but approximate p(e|f ) by maxa p(e, a|f ). Though the computation in Equation 2 is intractable (DeNero and Klein, 2008), it can be computed in a few minutes via dynamic programming on reasonably short sentences. We ensured that our data met this criterion. The corpus-level probability is then the product of all sentence-level probabilities in the data. The model includes no distortion limit or distortion model, for two reasons. First, leaving out the distortion model slightly simplifies the implementation, since it is not necessary to keep track of the last word translated in a beam decoder; we felt that this detail was secondary to understanding the difficulty of search over phrase permutations. Second, it ac"
Q13-1014,P10-4002,1,0.928176,"now offer such classes. There are excellent introductory texts—depending on the level of detail required, instructors can choose from a comprehensive MT textbook (Koehn, 2010), a chapter of a popular NLP textbook (Jurafsky and Martin, 2009), a tutorial survey (Lopez, 2008), or an intuitive tutorial on the IBM Models (Knight, 1999b), among many others. But MT is not just an object of academic study. It’s a real application that isn’t fully perfected, and the best way to learn about it is to build an MT system. This can be done with open-source toolkits such as Moses (Koehn et al., 2007), cdec (Dyer et al., 2010), or Joshua (Ganitkevitch et al., 2012), but these systems are not designed for pedagogy. They are mature codebases featuring tens of thousands of source code lines, making it difficult to focus on their core algorithms. Most tutorials present them as black boxes. But our goal is for students to learn the key techniques in MT, and ideally to learn by doing. Black boxes are incompatible with this goal. We solve this dilemma by presenting students with concise, fully-functioning, self-contained components of a statistical MT system: word alignment, decoding, evaluation, and reranking. Each imple"
Q13-1014,W08-0212,0,0.0278916,"ting final grades. Each student rated aspects of the course on a five point Likert scale, from 1 (strongly disagree) to 5 (strongly agree). Several questions pertained to assignments (Table 1), and allay two possible concerns about competition: most students felt that the assignments enhanced their collaborative skills, and that their open-endedness did not result in an overload of work. For all survey questions, student satisfaction was higher than average for courses in our department. 8 Discussion DREAMT is inspired by several different approaches to teaching NLP, AI, and computer science. Eisner and Smith (2008) teach NLP using a competitive game in which students aim to write fragments of English grammar. Charniak et al. (2000) improve the state-of-the-art in a reading comprehension task as part of a group project. Christopher et al. (1993) use NACHOS, a classic tool for teaching operating systems by providing a rudimentary system that students then augment. DeNero and Klein (2010) devise a series of assignments based on Pac-Man, for which students implement several classic AI techniques. A crucial element in such approaches is a highly functional but simple scaffolding. The DREAMT codebase, includi"
Q13-1014,J07-3002,0,0.0344561,"Missing"
Q13-1014,W12-3134,1,0.927189,"re excellent introductory texts—depending on the level of detail required, instructors can choose from a comprehensive MT textbook (Koehn, 2010), a chapter of a popular NLP textbook (Jurafsky and Martin, 2009), a tutorial survey (Lopez, 2008), or an intuitive tutorial on the IBM Models (Knight, 1999b), among many others. But MT is not just an object of academic study. It’s a real application that isn’t fully perfected, and the best way to learn about it is to build an MT system. This can be done with open-source toolkits such as Moses (Koehn et al., 2007), cdec (Dyer et al., 2010), or Joshua (Ganitkevitch et al., 2012), but these systems are not designed for pedagogy. They are mature codebases featuring tens of thousands of source code lines, making it difficult to focus on their core algorithms. Most tutorials present them as black boxes. But our goal is for students to learn the key techniques in MT, and ideally to learn by doing. Black boxes are incompatible with this goal. We solve this dilemma by presenting students with concise, fully-functioning, self-contained components of a statistical MT system: word alignment, decoding, evaluation, and reranking. Each implementation consists of a na¨ıve baseline"
Q13-1014,P01-1030,0,0.330619,"Canadian Hansards to serve as test data. To create a simple translation model, we used the Berkeley aligner to align the parallel text from the first assignment, and extracted a phrase table using the method of Lopez (2007), as implemented in cdec (Dyer et al., 2010). To create a simple language model, we used SRILM (Stolcke, 2002). We received 71 submissions from 10 teams (Figure 2), again exhibiting variety of solutions. Challenge Results • Implementation of greedy decoder which at each step chooses the most probable translation from among those reachable by a single swap or retranslation (Germann et al., 2001; Langlais et al., 2007). 4.2 Implementation • Inclusion of heuristic estimates of future cost. We distributed two Python programs. The first, decode, decodes the test data monotonically— using both the language model and translation model, but without permuting phrases. The implementation is completely self-contained with no external dependencies: it implements both models and a simple stack decoding algorithm for monotonic translation. It contains only 122 lines of Python— orders of magnitude fewer than most full-featured decoders. To see its similarity to pseudocode, compare the decoding al"
Q13-1014,P07-1019,0,0.0264172,"nd thus competition. We told students the exact algorithm that produced the target accuracy (though we expected them to derive it themselves based on lectures, notes, or literature). We did not specifically require them to implement it, but the guarantee of a passing grade provided a powerful incentive for this to be the first step of each assignment. Submissions that beat this target received additional credit. The top five submissions received full credit, while the top three received extra credit. 2 http://python.org Some well-known MT systems have been implemented in Python (Chiang, 2007; Huang and Chiang, 2007). 4 Thanks to an anonymous reviewer for this turn of phrase. 3 This scheme provided strong incentive to continue experimentation beyond the target algorithm.5 For each assignment, students could form teams of any size, under three rules: each team had to publicize its formation to the class, all team members agreed to receive the same grade, and teams could not drop members. Our hope was that these requirements would balance the perceived competitive advantage of collaboration against a reluctance to take (and thus support) teammates who did not contribute to the competitive effort.6 This stra"
Q13-1014,D07-1031,0,0.0154999,"saw many other solutions, indicating that many truly experimented with the problem: • Implementing heuristic constraints to require alignment of proper names and punctuation. • Running the algorithm on stems rather than surface words. • Initializing the first iteration of Model 1 with parameters estimated on the observed alignments in the development data. • Running Model 1 for many iterations. Most researchers typically run Model 1 for five iterations or fewer, and there are few experiments in the literature on its behavior over many iterations, as there are for hidden Markov model taggers (Johnson, 2007). Our students carried out these experiments, reporting runs of 5, 20, 100, and even 2000 iterations. No improvement was observed after 20 iterations. • Implementing various alternative approaches from the literature, including IBM Model 2 (Brown et al., 1993), competitive linking (Melamed, 2000), and smoothing (Moore, 2004). One of the best solutions was competitive linking with Dice’s coefficient, modified to incorporate the observation that alignments tend to be monotonic by restricting possible alignment points to a window of eight words around the diagonal. Although simple, it acheived an"
Q13-1014,N06-1058,0,0.0115716,"performs the one reported in Callison-Burch et al. (2011) since it performs no tokenization or normalization of the data. This also left room for improvement. 0.8 Spearman’s ρ tems, computed from pairwise human judgements according to a formula recommended by Bojar et al. (2011).12 Figure 3: Submission history for the evaluation challenge. • Implementation of several techniques used in AMBER (Chen and Kuhn, 2005). The best submission, obtaining a correlation of 83.5, relied on the idea that the reference and machine translation should be good paraphrases of each other (Owczarzak et al., 2006; Kauchak and Barzilay, 2006). It employed a simple paraphrase system trained on the alignment challenge data, using the pivot technique of Bannard and CallisonBurch (2005), and computing the optimal alignment between machine translation and reference under a simple model in which words could align if they were paraphrases. When compared with the 20 systems submitted to the original task from which the data was obtained (Callison-Burch et al., 2011), this system would have ranked fifth, quite near the top-scoring competitors, whose correlations ranged from 88 to 94. 6 The Reranking Challenge The fourth challenge was reran"
Q13-1014,W05-0104,0,0.0345053,"eria. Everyone who completed all four assignments placed in the top five at least once. 6 The equilibrium point is a single team, though this team would still need to decide on a division of labor. One student contemplated organizing this team, but decided against it. 167 Some did so after the assignment deadline. 3 The Alignment Challenge The first challenge was word alignment: given a parallel text, students were challenged to produce wordto-word alignments with low alignment error rate (AER; Och and Ney, 2000). This is a variant of a classic assignment not just in MT, but in NLP generally. Klein (2005) describes a version of it, and we know several other instructors who use it.7 In most of these, the object is to implement IBM Model 1 or 2, or a hidden Markov model. Our version makes it open-ended by asking students to match or beat an IBM Model 1 baseline. 3.1 Data We provided 100,000 sentences of parallel data from the Canadian Hansards, totaling around two million words.8 This dataset is small enough to align in a few minutes with our implementation—enabling rapid experimentation—yet large enough to obtain reasonable results. In fact, Liang et al. (2006) report alignment accuracy on data"
Q13-1014,J99-4005,0,0.395409,"lass on natural language processing (NLP), machine learning (ML), or artificial intelligence (AI). A course that promises to show students how Google Translate works and teach them how to build something like it is especially appealing, and several universities and summer schools now offer such classes. There are excellent introductory texts—depending on the level of detail required, instructors can choose from a comprehensive MT textbook (Koehn, 2010), a chapter of a popular NLP textbook (Jurafsky and Martin, 2009), a tutorial survey (Lopez, 2008), or an intuitive tutorial on the IBM Models (Knight, 1999b), among many others. But MT is not just an object of academic study. It’s a real application that isn’t fully perfected, and the best way to learn about it is to build an MT system. This can be done with open-source toolkits such as Moses (Koehn et al., 2007), cdec (Dyer et al., 2010), or Joshua (Ganitkevitch et al., 2012), but these systems are not designed for pedagogy. They are mature codebases featuring tens of thousands of source code lines, making it difficult to focus on their core algorithms. Most tutorials present them as black boxes. But our goal is for students to learn the key te"
Q13-1014,N03-1017,0,0.0330225,"to note that AER does not necessarily correlate with downstream performance, particularly on the Hansards dataset (Fraser and Marcu, 2007). We used the conclusion of the assignment as an opportunity to emphasize this point. 4 The Decoding Challenge The second challenge was decoding: given a fixed translation model and a set of input sentences, students were challenged to produce translations with the highest model score. This challenge introduced the difficulties of combinatorial optimization under a deceptively simple setup: the model we provided was a simple phrase-based translation model (Koehn et al., 2003) consisting only of a phrase table and tri169 gram language model. Under this simple model, for a French sentence f of length I, English sentence e of length J, and alignment a where each element consists of a span in both e and f such that every word in both e and f is aligned exactly once, the conditional probability of e and a given f is as follows.10 p(e, a|f ) = Y hi,i0 ,j,j 0 i∈a 0 0 p(fii |ejj ) J+1 Y j=1 p(ej |ej−1 , ej−2 ) (1) To evaluate output, we compute the conditional probability of e as follows. p(e|f ) = X p(e, a|f ) (2) a Note that this formulation is different from the typica"
Q13-1014,P07-2045,1,0.0187571,"rsities and summer schools now offer such classes. There are excellent introductory texts—depending on the level of detail required, instructors can choose from a comprehensive MT textbook (Koehn, 2010), a chapter of a popular NLP textbook (Jurafsky and Martin, 2009), a tutorial survey (Lopez, 2008), or an intuitive tutorial on the IBM Models (Knight, 1999b), among many others. But MT is not just an object of academic study. It’s a real application that isn’t fully perfected, and the best way to learn about it is to build an MT system. This can be done with open-source toolkits such as Moses (Koehn et al., 2007), cdec (Dyer et al., 2010), or Joshua (Ganitkevitch et al., 2012), but these systems are not designed for pedagogy. They are mature codebases featuring tens of thousands of source code lines, making it difficult to focus on their core algorithms. Most tutorials present them as black boxes. But our goal is for students to learn the key techniques in MT, and ideally to learn by doing. Black boxes are incompatible with this goal. We solve this dilemma by presenting students with concise, fully-functioning, self-contained components of a statistical MT system: word alignment, decoding, evaluation,"
Q13-1014,koen-2004-pharaoh,0,0.0336528,"ntained with no external dependencies: it implements both models and a simple stack decoding algorithm for monotonic translation. It contains only 122 lines of Python— orders of magnitude fewer than most full-featured decoders. To see its similarity to pseudocode, compare the decoding algorithm (Listing 2) with the pseudocode in Koehn’s (2010) popular textbook (reproduced here as Algorithm 1). The second program, grade, computes the log-probability of a set of translations, as outline above. We privately implemented a simple stack decoder that searched over permutations of phrases, similar to Koehn (2004). Our implementation increased the codebase by 44 lines of code and included parameters for beam size, distortion limit, and the maximum number of translations considered for each input phrase. We posted a baseline to the leaderboard using values of 50, 3, and 20 for these, respectively. • Implementation of a private oracle. Some students observed that the ideal beam setting was not uniform across the corpus. They ran their decoder under different settings, and then selected the most probable translation of each sentence. 11 We implemented a version of the Lagrangian relaxation algorithm of Ch"
Q13-1014,J10-4005,0,0.199806,"remaining authors were students in the worked described here. This research was conducted while Chris Callison-Burch was at Johns Hopkins University. in a class on natural language processing (NLP), machine learning (ML), or artificial intelligence (AI). A course that promises to show students how Google Translate works and teach them how to build something like it is especially appealing, and several universities and summer schools now offer such classes. There are excellent introductory texts—depending on the level of detail required, instructors can choose from a comprehensive MT textbook (Koehn, 2010), a chapter of a popular NLP textbook (Jurafsky and Martin, 2009), a tutorial survey (Lopez, 2008), or an intuitive tutorial on the IBM Models (Knight, 1999b), among many others. But MT is not just an object of academic study. It’s a real application that isn’t fully perfected, and the best way to learn about it is to build an MT system. This can be done with open-source toolkits such as Moses (Koehn et al., 2007), cdec (Dyer et al., 2010), or Joshua (Ganitkevitch et al., 2012), but these systems are not designed for pedagogy. They are mature codebases featuring tens of thousands of source cod"
Q13-1014,2012.iwslt-papers.5,0,0.0214715,"Missing"
Q13-1014,N04-1022,0,0.0453486,"rank order of the underlying translation system. Students discovered that simply returning the first can173 didate earned a very high score, and most of them quickly converged to this solution. Unfortunately, the high accuracy of this baseline left little room for additional competition. Nevertheless, we were encouraged that most students discovered this by accident while attempting other strategies to rerank the translations. • Experimentation with parameters of the PRO algorithm. • Substitution of alternative learning algorithms. • Implementation of a simplified minimum Bayes risk reranker (Kumar and Byrne, 2004). Over a baseline of 24.02, the latter approach obtained a BLEU of 27.08, nearly matching the score of 27.39 from the underlying system despite an impoverished feature set. 7 Pedagogical Outcomes Could our students have obtained similar results by running standard toolkits? Undoubtedly. However, our goal was for students to learn by doing: they obtained these results by implementing key MT algorithms, observing their behavior on real data, and improving them. This left them with much more insight into how MT systems actually work, and in this sense, DREAMT was a success. At the end of class, w"
Q13-1014,2007.tmi-papers.13,0,0.0443585,"serve as test data. To create a simple translation model, we used the Berkeley aligner to align the parallel text from the first assignment, and extracted a phrase table using the method of Lopez (2007), as implemented in cdec (Dyer et al., 2010). To create a simple language model, we used SRILM (Stolcke, 2002). We received 71 submissions from 10 teams (Figure 2), again exhibiting variety of solutions. Challenge Results • Implementation of greedy decoder which at each step chooses the most probable translation from among those reachable by a single swap or retranslation (Germann et al., 2001; Langlais et al., 2007). 4.2 Implementation • Inclusion of heuristic estimates of future cost. We distributed two Python programs. The first, decode, decodes the test data monotonically— using both the language model and translation model, but without permuting phrases. The implementation is completely self-contained with no external dependencies: it implements both models and a simple stack decoding algorithm for monotonic translation. It contains only 122 lines of Python— orders of magnitude fewer than most full-featured decoders. To see its similarity to pseudocode, compare the decoding algorithm (Listing 2) with"
Q13-1014,N06-1014,0,0.0123873,"nt not just in MT, but in NLP generally. Klein (2005) describes a version of it, and we know several other instructors who use it.7 In most of these, the object is to implement IBM Model 1 or 2, or a hidden Markov model. Our version makes it open-ended by asking students to match or beat an IBM Model 1 baseline. 3.1 Data We provided 100,000 sentences of parallel data from the Canadian Hansards, totaling around two million words.8 This dataset is small enough to align in a few minutes with our implementation—enabling rapid experimentation—yet large enough to obtain reasonable results. In fact, Liang et al. (2006) report alignment accuracy on data of this size that is within a fraction of a point of their accuracy on the complete Hansards data. To evaluate, we used manual alignments of a small fraction of sentences, developed by Och and Ney (2000), which we obtained from the shared task resources organized by Mihalcea and Pedersen (2003). The first 37 sentences of the corpus were development data, with manual alignments provided in a separate file. Test data consisted of an additional 447 sentences, for which we did not provide alignments.9 3.2 Implementation We distributed three Python programs with t"
Q13-1014,C04-1072,0,0.0405183,"Missing"
Q13-1014,J10-3002,0,0.0284528,"Missing"
Q13-1014,D07-1104,1,0.803017,"t know the true solution.11 We also posted an oracle containing the most probable output for each sentence, selected from among all submissions received so far. The intent of this oracle was to provide a lower bound on the best possible output, giving students additional incentive to continue improving their systems. 4.1 Data 4.3 We chose 48 French sentences totaling 716 words from the Canadian Hansards to serve as test data. To create a simple translation model, we used the Berkeley aligner to align the parallel text from the first assignment, and extracted a phrase table using the method of Lopez (2007), as implemented in cdec (Dyer et al., 2010). To create a simple language model, we used SRILM (Stolcke, 2002). We received 71 submissions from 10 teams (Figure 2), again exhibiting variety of solutions. Challenge Results • Implementation of greedy decoder which at each step chooses the most probable translation from among those reachable by a single swap or retranslation (Germann et al., 2001; Langlais et al., 2007). 4.2 Implementation • Inclusion of heuristic estimates of future cost. We distributed two Python programs. The first, decode, decodes the test data monotonically— using both the l"
Q13-1014,W12-3101,1,0.89068,"Missing"
Q13-1014,W08-0209,0,0.0250327,"ng, respectively. In real MT systems, these problems are highly interdependent, a point we emphasized in class and at the end of each assignment—for example, that alignment is an exercise in parameter estimation for translation models, that model choice is a tradeoff between expressivity and efficient inference, and that optimal search does not guarantee optimal accuracy. However, presenting each problem independently and holding all else constant enables more focused exploration. For each problem we provided data, a na¨ıve solution, and an evaluation program. Following Bird et al. (2008) and Madnani and Dorr (2008), we implemented the challenges in Python, a high-level pro1 http://alopez.github.io/dreamt 166 gramming language that can be used to write very concise programs resembling pseudocode.2,3 By default, each baseline system reads the test data and generates output in the evaluation format, so setup required zero configuration, and students could begin experimenting immediately. For example, on receipt of the alignment code, aligning data and evaluating results required only typing: > align |grade Students could then run experiments within minutes of beginning the assignment. Three of the four cha"
Q13-1014,J00-2004,0,0.0633455,"Missing"
Q13-1014,W03-0301,0,0.0297199,"a We provided 100,000 sentences of parallel data from the Canadian Hansards, totaling around two million words.8 This dataset is small enough to align in a few minutes with our implementation—enabling rapid experimentation—yet large enough to obtain reasonable results. In fact, Liang et al. (2006) report alignment accuracy on data of this size that is within a fraction of a point of their accuracy on the complete Hansards data. To evaluate, we used manual alignments of a small fraction of sentences, developed by Och and Ney (2000), which we obtained from the shared task resources organized by Mihalcea and Pedersen (2003). The first 37 sentences of the corpus were development data, with manual alignments provided in a separate file. Test data consisted of an additional 447 sentences, for which we did not provide alignments.9 3.2 Implementation We distributed three Python programs with the data. The first, align, computes Dice’s coefficient (1945) for every pair of French and English words, then aligns every pair for which its value is above an adjustable threshold. Our implementation (most of 7 Among them, Jordan Boyd-Graber, John DeNero, Philipp Koehn, and Slav Petrov (personal communication). 8 http://www.is"
Q13-1014,P04-1066,0,0.0562929,"Missing"
Q13-1014,P00-1056,0,0.0804879,"signment would earn an A; and top three placement compensated for weaker grades in other course criteria. Everyone who completed all four assignments placed in the top five at least once. 6 The equilibrium point is a single team, though this team would still need to decide on a division of labor. One student contemplated organizing this team, but decided against it. 167 Some did so after the assignment deadline. 3 The Alignment Challenge The first challenge was word alignment: given a parallel text, students were challenged to produce wordto-word alignments with low alignment error rate (AER; Och and Ney, 2000). This is a variant of a classic assignment not just in MT, but in NLP generally. Klein (2005) describes a version of it, and we know several other instructors who use it.7 In most of these, the object is to implement IBM Model 1 or 2, or a hidden Markov model. Our version makes it open-ended by asking students to match or beat an IBM Model 1 baseline. 3.1 Data We provided 100,000 sentences of parallel data from the Canadian Hansards, totaling around two million words.8 This dataset is small enough to align in a few minutes with our implementation—enabling rapid experimentation—yet large enoug"
Q13-1014,J03-1002,0,0.00871586,"Missing"
Q13-1014,N04-1021,0,0.039354,"as a simple program that produced a vector of feature weights using pairwise ranking optimization (PRO; Hopkins and May, 2011), with a perceptron as the underlying learning algorithm. A second, rerank, takes a weight vector as input and reranks the sentences; both programs were designed to work with arbitrary numbers of features. The grade program computed the BLEU score on development data, while check ensured that a test submission is valid. Finally, we provided an oracle program, which computed a lower bound on the achievable BLEU score on the development data using a greedy approximation (Och et al., 2004). The leaderboard likewise displayed an oracle on test data. We did not assign a target algorithm, but left the assignment fully open-ended. 6.3 Reranking Challenge Outcome For each assignment, we made an effort to create room for competition above the target algorithm. However, we did not accomplish this in the reranking challenge: we had removed most of the features from the candidate translations, in hopes that students might reinvent some of them, but we left one highly predictive implicit feature in the data: the rank order of the underlying translation system. Students discovered that si"
Q13-1014,W06-3112,0,0.013171,"Our implementation underperforms the one reported in Callison-Burch et al. (2011) since it performs no tokenization or normalization of the data. This also left room for improvement. 0.8 Spearman’s ρ tems, computed from pairwise human judgements according to a formula recommended by Bojar et al. (2011).12 Figure 3: Submission history for the evaluation challenge. • Implementation of several techniques used in AMBER (Chen and Kuhn, 2005). The best submission, obtaining a correlation of 83.5, relied on the idea that the reference and machine translation should be good paraphrases of each other (Owczarzak et al., 2006; Kauchak and Barzilay, 2006). It employed a simple paraphrase system trained on the alignment challenge data, using the pivot technique of Bannard and CallisonBurch (2005), and computing the optimal alignment between machine translation and reference under a simple model in which words could align if they were paraphrases. When compared with the 20 systems submitted to the original task from which the data was obtained (Callison-Burch et al., 2011), this system would have ranked fifth, quite near the top-scoring competitors, whose correlations ranged from 88 to 94. 6 The Reranking Challenge T"
Q13-1014,P02-1040,0,0.0866501,"Missing"
Q13-1014,P09-1038,0,0.0198477,"or two reasons. First, leaving out the distortion model slightly simplifies the implementation, since it is not necessary to keep track of the last word translated in a beam decoder; we felt that this detail was secondary to understanding the difficulty of search over phrase permutations. Second, it actually makes the problem more difficult, since a simple distance-based distortion model prefers translations with fewer permutations; without it, the model may easily prefer any permutation of the target phrases, making even the Viterbi search problem exhibit its true NP-hardness (Knight, 1999a; Zaslavskiy et al., 2009). Since the goal was to find the translation with the highest probability, we did not provide a held-out test set; with access to both the input sentences and 10 For simplicity, this formula assumes that e is padded with two sentence-initial symbols and one sentence-final symbol, and ignores the probability of sentence segmentation, which we take to be uniform. the model, students had enough information to compute the evaluation score on any dataset themselves. The difficulty of the challenge lies simply in finding the translation that maximizes the evaluation. Indeed, since the problem is int"
Q13-1014,W11-2105,0,\N,Missing
Q14-1007,W10-0710,0,0.0986571,"cedonian Bulgarian Catalan Punjabi 253 131 85 72 61 48 41 37 31 25 22 21 Malayalam Telugu Portuguese German Urdu Russian Gujarati Turkish Cebuano Swahili Thai Others 219 87 82 66 56 44 39 35 29 23 22 ≤ 20 Table 1: Self-reported native language of 3,216 bilingual Turkers. Not shown are 49 languages with ≤20 speakers. We omit 1,801 Turkers who did not report their native language, 243 who reported 2 native languages, and 83 with ≥3 native languages. Several researchers have examined cost optimization using active learning techniques to select the most useful sentences or fragments to translate (Ambati and Vogel, 2010; Bloodgood and CallisonBurch, 2010; Ambati, 2012). To contrast our research with previous work, the main contributions of this paper are: (1) a robust methodology for assessing the bilingual skills of anonymous workers, (2) the largest-scale census to date of language skills of workers on MTurk, and (3) a detailed analysis of the data gathered in our study. 3 Experimental Design The central task in this study was to investigate Mechanical Turk’s bilingual population. We accomplished this through self-reported surveys combined with a HIT to translate individual words for 100 languages. We eval"
Q14-1007,ambati-etal-2010-active,0,0.134405,"ual texts. They examined translations of 100 words in 42 low-resource languages, and reported geolocated countries for their workers (India, the US, Romania, Pakistan, Macedonia, Latvia, Bangladesh and the Philippines). Irvine and Klementiev discussed the difficulty of quality control and assessing the plausibility of workers’ language skills for rare languages, which we address in this paper. Several researchers have investigated using MTurk to build bilingual parallel corpora for machine translation, a task which stands to benefit low cost, high volume translation on demand (Germann, 2001). Ambati et al. (2010) conducted a pilot study by posting 25 sentences to MTurk for Spanish, Chinese, Hindi, Telugu, Urdu, and Haitian Creole. In a study of 2000 Urdu sentences, Zaidan and Callison-Burch (2011) presented methods for achieving professional-level translation quality from Turkers by soliciting multiple English translations of each foreign sentence. Zbib et al. (2012) used crowdsourcing to construct a 1.5 million word parallel corpus of dialect Arabic and English, training a statistical machine translation system that produced higher quality translations of dialect Arabic than a system a trained on 100"
Q14-1007,P10-1088,1,0.849791,"Missing"
Q14-1007,W10-0701,1,0.73723,"Missing"
Q14-1007,W01-1409,0,0.50491,"ed from monolingual texts. They examined translations of 100 words in 42 low-resource languages, and reported geolocated countries for their workers (India, the US, Romania, Pakistan, Macedonia, Latvia, Bangladesh and the Philippines). Irvine and Klementiev discussed the difficulty of quality control and assessing the plausibility of workers’ language skills for rare languages, which we address in this paper. Several researchers have investigated using MTurk to build bilingual parallel corpora for machine translation, a task which stands to benefit low cost, high volume translation on demand (Germann, 2001). Ambati et al. (2010) conducted a pilot study by posting 25 sentences to MTurk for Spanish, Chinese, Hindi, Telugu, Urdu, and Haitian Creole. In a study of 2000 Urdu sentences, Zaidan and Callison-Burch (2011) presented methods for achieving professional-level translation quality from Turkers by soliciting multiple English translations of each foreign sentence. Zbib et al. (2012) used crowdsourcing to construct a 1.5 million word parallel corpus of dialect Arabic and English, training a statistical machine translation system that produced higher quality translations of dialect Arabic than a s"
Q14-1007,W11-2148,0,0.0270578,"ting multiple English translations of each foreign sentence. Zbib et al. (2012) used crowdsourcing to construct a 1.5 million word parallel corpus of dialect Arabic and English, training a statistical machine translation system that produced higher quality translations of dialect Arabic than a system a trained on 100 times more Modern Standard Arabic-English parallel data. Zbib et al. (2013) conducted a systematic study that showed that training an MT system on crowdsourced translations resulted in the same performance as training on professional translations, at 15 the cost. Hu et al. (2010; Hu et al. (2011) performed crowdsourced translation by having monolingual speakers collaborate and iteratively improve MT output. 81 English Hindi Chinese Arabic French Tagalog Italian Hebrew Vietnamese Swedish Hungarian Lithuanian 689 149 86 74 63 54 43 38 34 26 23 21 Tamil Spanish Romanian Kannada Polish Marathi Bengali Dutch Macedonian Bulgarian Catalan Punjabi 253 131 85 72 61 48 41 37 31 25 22 21 Malayalam Telugu Portuguese German Urdu Russian Gujarati Turkish Cebuano Swahili Thai Others 219 87 82 66 56 44 39 35 29 23 22 ≤ 20 Table 1: Self-reported native language of 3,216 bilingual Turkers. Not shown ar"
Q14-1007,W10-0717,1,0.775416,"ographic study by Ipeirotis (2010b) focused on age, gender, martial status, income levels, motivation for working on MTurk, and whether workers used it as a primary or supplemental form of income. The study contrasted Indian and US workers. Ross et al. (2010) completed a longitudinal follow-on study. A number of other studies have informally investigated Turkers’ language abilities. Munro and Tily (2011) compiled survey responses of 2,000 Turkers, revealing that four of the six most represented languages come from India (the top six being Hindi, Malayalam, Tamil, Spanish, French, and Telugu). Irvine and Klementiev (2010) had Turkers evaluate the accuracy of translations that had been automatically inducted from monolingual texts. They examined translations of 100 words in 42 low-resource languages, and reported geolocated countries for their workers (India, the US, Romania, Pakistan, Macedonia, Latvia, Bangladesh and the Philippines). Irvine and Klementiev discussed the difficulty of quality control and assessing the plausibility of workers’ language skills for rare languages, which we address in this paper. Several researchers have investigated using MTurk to build bilingual parallel corpora for machine tran"
Q14-1007,W10-0729,0,0.0260829,"Little et it first offered payment only in Amazon credits, and al., 2009; Quinn and Bederson, 2011). It has appli- later offered direct payment in US dollars. More recation to research areas like human-computer inter- cently, it has expanded to include one foreign curaction (Bigham et al., 2010; Bernstein et al., 2010), rency, the Indian rupee. Despite its payments becomputer vision (Sorokin and Forsyth, 2008; Deng ing limited to two currencies or Amazon credits, et al., 2010; Rashtchian et al., 2010), speech pro- MTurk claims over half a million workers from 190 cessing (Marge et al., 2010; Lane et al., 2010; Parent countries (Amazon, 2013). This suggests that its file:///Users/ellie/Documents/Research/turker-demographics/code/src/20130905/paper-rewrite/turkermap.html and Eskenazi, 2011; Eskenazi et al., 2013), and natu- worker population should represent a diverse set of ral language processing (Snow et al., 2008; Callison- languages. 80 1/1 A demographic study by Ipeirotis (2010b) focused on age, gender, martial status, income levels, motivation for working on MTurk, and whether workers used it as a primary or supplemental form of income. The study contrasted Indian and US workers. Ross et al."
Q14-1007,D11-1143,0,0.0205911,"Missing"
Q14-1007,W10-0716,0,0.012553,"call (von Ahn, 2005; Little et it first offered payment only in Amazon credits, and al., 2009; Quinn and Bederson, 2011). It has appli- later offered direct payment in US dollars. More recation to research areas like human-computer inter- cently, it has expanded to include one foreign curaction (Bigham et al., 2010; Bernstein et al., 2010), rency, the Indian rupee. Despite its payments becomputer vision (Sorokin and Forsyth, 2008; Deng ing limited to two currencies or Amazon credits, et al., 2010; Rashtchian et al., 2010), speech pro- MTurk claims over half a million workers from 190 cessing (Marge et al., 2010; Lane et al., 2010; Parent countries (Amazon, 2013). This suggests that its file:///Users/ellie/Documents/Research/turker-demographics/code/src/20130905/paper-rewrite/turkermap.html and Eskenazi, 2011; Eskenazi et al., 2013), and natu- worker population should represent a diverse set of ral language processing (Snow et al., 2008; Callison- languages. 80 1/1 A demographic study by Ipeirotis (2010b) focused on age, gender, martial status, income levels, motivation for working on MTurk, and whether workers used it as a primary or supplemental form of income. The study contrasted Indian and US wo"
Q14-1007,N10-1024,1,0.220696,"Missing"
Q14-1007,W12-3152,1,0.790157,"Some languages follow the pattern of having a smattering of assignments completed early, with the rate picking up later. Figure 6 gives the throughput of the full-sentence translation task for the six Indian languages. The fastest language was Malayalam, for which we collected half a million words of translations in just under a week. Table 4 gives the size of the data set that we created for each of these languages. Training SMT systems We trained statistical translation models from the parallel corpora that we created for the six Indian languages using the Joshua machine translation system (Post et al., 2012). Table 5 shows the translation performance when trained on the bitexts alone, and when incorporating the bilingual dictionaries created in our earlier HIT. The scores reflect the performance when tested on held out sentences from the training data. Adding the dic87 trained on bitexts alone 12.03 16.19 6.65 8.08 11.94 19.22 bitext + dictionaries 17.29 18.10 9.72 9.66 13.70 21.98 BLEU ∆ 5.26 1.91 3.07 1.58 1.76 2.76 Table 5: BLEU scores for translating into English using bilingual parallel corpora by themselves, and with the addition of single-word dictionaries. Scores are calculated using four"
Q14-1007,W10-0721,0,0.0153205,"tation, where people can Ernkvist, 2011). When Amazon introduced MTurk, be treated as a function call (von Ahn, 2005; Little et it first offered payment only in Amazon credits, and al., 2009; Quinn and Bederson, 2011). It has appli- later offered direct payment in US dollars. More recation to research areas like human-computer inter- cently, it has expanded to include one foreign curaction (Bigham et al., 2010; Bernstein et al., 2010), rency, the Indian rupee. Despite its payments becomputer vision (Sorokin and Forsyth, 2008; Deng ing limited to two currencies or Amazon credits, et al., 2010; Rashtchian et al., 2010), speech pro- MTurk claims over half a million workers from 190 cessing (Marge et al., 2010; Lane et al., 2010; Parent countries (Amazon, 2013). This suggests that its file:///Users/ellie/Documents/Research/turker-demographics/code/src/20130905/paper-rewrite/turkermap.html and Eskenazi, 2011; Eskenazi et al., 2013), and natu- worker population should represent a diverse set of ral language processing (Snow et al., 2008; Callison- languages. 80 1/1 A demographic study by Ipeirotis (2010b) focused on age, gender, martial status, income levels, motivation for working on MTurk, and whether workers"
Q14-1007,D08-1027,0,0.151761,"Missing"
Q14-1007,P11-1122,1,0.467909,"a, Latvia, Bangladesh and the Philippines). Irvine and Klementiev discussed the difficulty of quality control and assessing the plausibility of workers’ language skills for rare languages, which we address in this paper. Several researchers have investigated using MTurk to build bilingual parallel corpora for machine translation, a task which stands to benefit low cost, high volume translation on demand (Germann, 2001). Ambati et al. (2010) conducted a pilot study by posting 25 sentences to MTurk for Spanish, Chinese, Hindi, Telugu, Urdu, and Haitian Creole. In a study of 2000 Urdu sentences, Zaidan and Callison-Burch (2011) presented methods for achieving professional-level translation quality from Turkers by soliciting multiple English translations of each foreign sentence. Zbib et al. (2012) used crowdsourcing to construct a 1.5 million word parallel corpus of dialect Arabic and English, training a statistical machine translation system that produced higher quality translations of dialect Arabic than a system a trained on 100 times more Modern Standard Arabic-English parallel data. Zbib et al. (2013) conducted a systematic study that showed that training an MT system on crowdsourced translations resulted in th"
Q14-1007,N12-1006,1,0.854548,", which we address in this paper. Several researchers have investigated using MTurk to build bilingual parallel corpora for machine translation, a task which stands to benefit low cost, high volume translation on demand (Germann, 2001). Ambati et al. (2010) conducted a pilot study by posting 25 sentences to MTurk for Spanish, Chinese, Hindi, Telugu, Urdu, and Haitian Creole. In a study of 2000 Urdu sentences, Zaidan and Callison-Burch (2011) presented methods for achieving professional-level translation quality from Turkers by soliciting multiple English translations of each foreign sentence. Zbib et al. (2012) used crowdsourcing to construct a 1.5 million word parallel corpus of dialect Arabic and English, training a statistical machine translation system that produced higher quality translations of dialect Arabic than a system a trained on 100 times more Modern Standard Arabic-English parallel data. Zbib et al. (2013) conducted a systematic study that showed that training an MT system on crowdsourced translations resulted in the same performance as training on professional translations, at 15 the cost. Hu et al. (2010; Hu et al. (2011) performed crowdsourced translation by having monolingual speak"
Q14-1007,N13-1069,0,0.0609685,"for Spanish, Chinese, Hindi, Telugu, Urdu, and Haitian Creole. In a study of 2000 Urdu sentences, Zaidan and Callison-Burch (2011) presented methods for achieving professional-level translation quality from Turkers by soliciting multiple English translations of each foreign sentence. Zbib et al. (2012) used crowdsourcing to construct a 1.5 million word parallel corpus of dialect Arabic and English, training a statistical machine translation system that produced higher quality translations of dialect Arabic than a system a trained on 100 times more Modern Standard Arabic-English parallel data. Zbib et al. (2013) conducted a systematic study that showed that training an MT system on crowdsourced translations resulted in the same performance as training on professional translations, at 15 the cost. Hu et al. (2010; Hu et al. (2011) performed crowdsourced translation by having monolingual speakers collaborate and iteratively improve MT output. 81 English Hindi Chinese Arabic French Tagalog Italian Hebrew Vietnamese Swedish Hungarian Lithuanian 689 149 86 74 63 54 43 38 34 26 23 21 Tamil Spanish Romanian Kannada Polish Marathi Bengali Dutch Macedonian Bulgarian Catalan Punjabi 253 131 85 72 61 48 41 37 3"
Q14-1007,W10-0707,0,\N,Missing
Q16-1013,P15-1068,0,0.817081,"ion between them, if possible, would be very complex. Additionally, it is difficult for annotators to agree on error annotations, which complicates the annotation validity as a gold standard (Leacock et al., 2014). This is due to the nature of grammatical error correction, where there can be diverse correct edits for a sentence (Figure 1). In other words, there is no single gold-standard correction. The variety of error types and potential correct edits result in very low inter-annotator agreement (IAA), as reported in previous studies (Tetreault and Chodorow, 2008; Rozovskaya and Roth, 2010; Bryant and Ng, 2015). This leads to a more fundamental question: why do we depend so much on fine-grained, lowconsensus error-type annotations as a gold standard for evaluating GEC systems? One answer is that error tags can be informative and useful to provide feedback to language learners, especially for specific closed-class error types fluency evaluation. As the development of the technology , social media becomes more and more significant role in the whole world . With the development of technology As the technology develops As technology develops plays a more and more significant role becomes more and more s"
Q16-1013,A00-2019,0,0.131062,"with our new annotation scheme has very strong correlation with expert rankings (ρ = 0.82). As a result, we advocate for a fundamental and necessary shift in the goal of GEC, from correcting small, labeled error types, to producing text that has native fluency. 1 Introduction What is the purpose of grammatical error correction (GEC)? One response is that GEC aims to help people become better writers by correcting grammatical mistakes in their writing. In the NLP community, the original scope of GEC was correcting targeted error types with the goal of providing feedback to non-native writers (Chodorow and Leacock, 2000; Dale and Kilgarriff, 2011; Leacock et al., 2014). As systems improved and more advanced methods were applied to the task, the definition evolved to wholesentence correction, or correcting all errors of every error type (Ng et al., 2014). With this pivot, we urge the community to revisit the original question. It is often the case that writing exhibits problems that are difficult to ascribe to specific grammatical categories. Consider the following example: Original: From this scope , social media has shorten our distance . Corrected: From this scope , social media has shortened our distance"
Q16-1013,C12-1038,1,0.764396,"mmatical errors. The ability to do this with the non-error-coded, fluent annotations we advocate here is no longer direct, but is not lost entirely. For this purpose, some recent studies have proposed post hoc automated errortype classification methods (Swanson and Yamangil, 2012; Xue and Hwa, 2014), which compare the original sentence to its correction and deduce the error types. We speculate that, by removing the error-coding restraint, we can obtain edits that sound more fluent to native speakers while also reducing the expense of annotation, with diminished time and training requirements. Chodorow et al. (2012) and Tetreault et al. (2014) suggested that it is better to have a large number of annotators to reduce bias in automatic evaluation. Following this recommendation, we collected additional annotations without error codes, written by both experts and non-experts. 5 It is important to note that both grammaticality and fluency are determined with respect to a particular speaker population and setting. In this paper, we focus on Standard Written English, which is the standard used in education, business, and journalism. While judgments of individual sentences would differ for other populations and"
Q16-1013,N12-1067,0,0.315246,"pora— and more of them—could be created. There has been some work exploring this, namely Tetreault and Chodorow (2008), which used a sampling approach that would only work for errors involving closed-class words. Pavlick et al. (2014) also describe preliminary work into designing an improved crowdsourcing interface to expedite data collection of coded errors. Section 3 outlines our annotation approach, which is faster and cheaper than previous approaches because it does not make use of error coding. 2.2 Evaluation practices Three evaluation metrics3 have been proposed for GEC: MaxMatch (M2 ) (Dahlmeier and Ng, 2012), I-measure (Felice and Briscoe, 2015), and GLEU (Napoles et al., 2015). The first two compare the changes made in the output to error-coded spans of the reference corrections. M2 was the metric used 3 Not including the metrics of the HOO shared tasks, which were precision, recall, and F-score. for the 2013 and 2014 CoNLL GEC shared tasks (Ng et al., 2013; Ng et al., 2014). It captures wordand phrase-level edits by building an edit lattice and calculating an F-score over the lattice. Felice and Briscoe (2015) note problems with M2 : specifically, it does not distinguish between a “do-nothing b"
Q16-1013,W13-1703,0,0.887528,"provide corrections to those spans for each error in the sentence. One of the main issues with coded annotation schemes is the difficulty of defining the granularity of error types. These sets of error tags are not easily interchangeable between different corpora. Specifically, two major GEC corpora have different taxonomies: the Cambridge Learner Corpus (CLC) (Nicholls, 2003) has 80 tags, which generally represent the word class of the error and the type of error (such as replace preposition, unnecessary pronoun, or missing determiner). In contrast, the NUS Corpus of Learner English (NUCLE) (Dahlmeier et al., 2013) has only 27 error types. A direct conversion between them, if possible, would be very complex. Additionally, it is difficult for annotators to agree on error annotations, which complicates the annotation validity as a gold standard (Leacock et al., 2014). This is due to the nature of grammatical error correction, where there can be diverse correct edits for a sentence (Figure 1). In other words, there is no single gold-standard correction. The variety of error types and potential correct edits result in very low inter-annotator agreement (IAA), as reported in previous studies (Tetreault and C"
Q16-1013,W11-2838,0,0.199732,"eme has very strong correlation with expert rankings (ρ = 0.82). As a result, we advocate for a fundamental and necessary shift in the goal of GEC, from correcting small, labeled error types, to producing text that has native fluency. 1 Introduction What is the purpose of grammatical error correction (GEC)? One response is that GEC aims to help people become better writers by correcting grammatical mistakes in their writing. In the NLP community, the original scope of GEC was correcting targeted error types with the goal of providing feedback to non-native writers (Chodorow and Leacock, 2000; Dale and Kilgarriff, 2011; Leacock et al., 2014). As systems improved and more advanced methods were applied to the task, the definition evolved to wholesentence correction, or correcting all errors of every error type (Ng et al., 2014). With this pivot, we urge the community to revisit the original question. It is often the case that writing exhibits problems that are difficult to ascribe to specific grammatical categories. Consider the following example: Original: From this scope , social media has shorten our distance . Corrected: From this scope , social media has shortened our distance . If the goal is to correct"
Q16-1013,W12-2006,0,0.0226628,"terminers and prepositions). Indeed, the CLC, the first large-scale corpus of annotated grammatical errors, was coded specifically with the intent of gathering statistics about errors to inform the development of tools to help English language learners (Nicholls, 2003). Later GEC corpora adhered to the same error-coding template, if not the same error types (Rozovskaya and Roth, 2010; Yannakoudakis et al., 2011; Dahlmeier et al., 2013). The first shared task in GEC aspired to the CLC’s same objective: to develop tools for language learners (Dale and Kilgarriff, 2011). Subsequent shared tasks (Dale et al., 2012; Ng et al., 2013) followed suit, targeting specific error types. Error-coded corpora are effective training and evaluation data for targeted error correction, and statistical classifiers have been developed to handle errors involving closed-class words (Rozovskaya and Roth, 2014). However, the 2014 CoNLL shared task engendered a sea change in GEC: in this shared task, systems needed to correct all errors in a sentence, of all error types, including ones more stylistic in nature (Ng et al., 2014). The evaluation metrics and annotated data from the previous shared task were used; however we arg"
Q16-1013,N15-1060,0,0.590262,"antly, as we will show in this paper, annotating for explicit error codes places a downward pressure on annotators to find and fix concrete, easily-identifiable grammatical errors (such as wrong verb tense) in lieu of addressing the native fluency of the text. A related problem is the presence of multiple evaluation metrics computed over error-annotated corpora. Recent work has shown that metrics like M2 and I-measure, both of which require errorcoded corpora, produce dramatically different results when used to score system output and produce a ranking of systems in conventional competitions (Felice and Briscoe, 2015). In light of all of this, we suggest that the GEC task has overlooked a fundamental question: What are the best practices for corpus annotation and system evaluation? This work attempts to answer this question. We show that native speakers prefer text that exhibits fluent sentences over ones that have only minimal grammatical corrections. We explore different methods for corpus annotation (with and without error codes, written by experts and non-experts) and different evaluation metrics to determine which configuration of annotated corpus and metric has the strongest correlation with the huma"
Q16-1013,D15-1052,0,0.201953,"14). The evaluation metrics and annotated data from the previous shared task were used; however we argue that they do not align with the use case of this reframed task. What is the use case of whole-sentence correction? It should not be to provide specific targeted feedback on error types, but rather to rewrite sentences as a proofreader would. The community has already begun to view wholesentence correction as a task, with the yet unstated goal of improving the overall fluency of sentences. Independent papers published human evaluations of the shared task system output (Napoles et al., 2015; Grundkiewicz et al., 2015), asking judges to rank systems based on their grammaticality. As GEC moves toward correcting an entire sentence instead of targeted error types, the myriad acceptable edits will result in much lower IAA, compromising evaluation metrics based on the precision and recall of 171 coded errors. At this juncture, it is crucial that we examine whether error-coded corpora and evaluation are necessary for this new direction of GEC. Finally, it would be remiss not to address the cost and time of corpus annotation. Tetreault and Chodorow (2008) noted that it would take 80 hours to correct 1,000 preposit"
Q16-1013,2012.iwslt-papers.5,0,0.0181853,"r knowledge, this is the first time that the interplay of annotation scheme and evaluation metric, as well as the rater expertise, has been evaluated jointly for GEC. 4.1 Table 5: Human ranking of the new annotations by grammaticality. Lines between systems indicate clusters according to bootstrap resampling at p ≤ 0.05. Systems in the same cluster are considered to be tied. tems. By running TrueSkill 1,000 times using bootstrap resampling and producing a system ranking each time, we collect a range of ranks for each system. We can then cluster systems according to nonoverlapping rank ranges (Koehn, 2012) to produce the final ranking, allowing ties. Table 5 shows the ranking of “grammatical” judgments for the additional annotations and the original NUCLE annotations. While the score of the expert fluency edits is higher than the non-expert fluency, they are within the same cluster, suggesting that the judges perceived them to be just as good. The fluency rewrites by both experts and nonexperts are clearly preferable over the minimal edit corrections, although the error-coded NUCLE corrections are perceived as more grammatical than the minimal corrections. 4 Automatic metrics We have demonstrat"
Q16-1013,P11-2089,1,0.931933,"Missing"
Q16-1013,P07-1044,0,0.0326389,"annotation and evaluation. As we will show, the two areas are intimately related. Fundamentally, this work reframes grammatical error correction as a fluency task. Our proposed evaluation framework produces system rankings with strong to very strong correlations with human judgments (Spearman’s ρ = 0.82, Pearson’s r = 0.73), using a variation of the GLEU metric (Napoles et al., 2015)2 and two sets of “fluent” sen1 All the scripts and new data we collected are available at https://github.com/keisks/reassess-gec. 2 This metric should not be confused with the method of the same name presented in Mutton et al. (2007) for sentence-level 170 tence rewrites as a gold standard, which are simpler and cheaper to collect than previous annotations. 2 Current issues in GEC In this section, we will address issues of the GEC task, reviewing previous work with respect to error annotation and evaluation metrics. 2.1 Annotation methodologies Existing corpora for GEC are annotated for errors using fine-grained coding schemes. To create error-coded corpora, trained annotators must identify spans of text containing an error, assign codes corresponding to the error type, and provide corrections to those spans for each erro"
Q16-1013,P15-2097,1,0.127014,"rrelation with the human ranking. In so doing, we establish a reliable and replicable evaluation procedure to help further the advancement of GEC methods.1 To date, this is the only work to undertake a comprehensive empirical study of annotation and evaluation. As we will show, the two areas are intimately related. Fundamentally, this work reframes grammatical error correction as a fluency task. Our proposed evaluation framework produces system rankings with strong to very strong correlations with human judgments (Spearman’s ρ = 0.82, Pearson’s r = 0.73), using a variation of the GLEU metric (Napoles et al., 2015)2 and two sets of “fluent” sen1 All the scripts and new data we collected are available at https://github.com/keisks/reassess-gec. 2 This metric should not be confused with the method of the same name presented in Mutton et al. (2007) for sentence-level 170 tence rewrites as a gold standard, which are simpler and cheaper to collect than previous annotations. 2 Current issues in GEC In this section, we will address issues of the GEC task, reviewing previous work with respect to error annotation and evaluation metrics. 2.1 Annotation methodologies Existing corpora for GEC are annotated for error"
Q16-1013,W13-3601,1,0.930945,"sitions). Indeed, the CLC, the first large-scale corpus of annotated grammatical errors, was coded specifically with the intent of gathering statistics about errors to inform the development of tools to help English language learners (Nicholls, 2003). Later GEC corpora adhered to the same error-coding template, if not the same error types (Rozovskaya and Roth, 2010; Yannakoudakis et al., 2011; Dahlmeier et al., 2013). The first shared task in GEC aspired to the CLC’s same objective: to develop tools for language learners (Dale and Kilgarriff, 2011). Subsequent shared tasks (Dale et al., 2012; Ng et al., 2013) followed suit, targeting specific error types. Error-coded corpora are effective training and evaluation data for targeted error correction, and statistical classifiers have been developed to handle errors involving closed-class words (Rozovskaya and Roth, 2014). However, the 2014 CoNLL shared task engendered a sea change in GEC: in this shared task, systems needed to correct all errors in a sentence, of all error types, including ones more stylistic in nature (Ng et al., 2014). The evaluation metrics and annotated data from the previous shared task were used; however we argue that they do no"
Q16-1013,W14-1701,0,0.739859,"ative fluency. 1 Introduction What is the purpose of grammatical error correction (GEC)? One response is that GEC aims to help people become better writers by correcting grammatical mistakes in their writing. In the NLP community, the original scope of GEC was correcting targeted error types with the goal of providing feedback to non-native writers (Chodorow and Leacock, 2000; Dale and Kilgarriff, 2011; Leacock et al., 2014). As systems improved and more advanced methods were applied to the task, the definition evolved to wholesentence correction, or correcting all errors of every error type (Ng et al., 2014). With this pivot, we urge the community to revisit the original question. It is often the case that writing exhibits problems that are difficult to ascribe to specific grammatical categories. Consider the following example: Original: From this scope , social media has shorten our distance . Corrected: From this scope , social media has shortened our distance . If the goal is to correct verb errors, the grammatical mistake in the original sentence has been addressed and we can move on. However, when we aim to correct the sentence as a whole, a more vexing problem remains. The more prominent er"
Q16-1013,P02-1040,0,0.105581,"nd Briscoe (2015) note problems with M2 : specifically, it does not distinguish between a “do-nothing baseline” and systems that only propose wrong corrections; also, phrase-level edits can be easily gamed because the lattice treats the deletion of a long phrase as a single edit. To address these issues, they propose I-measure, which generates a token-level alignment between the source sentence, system output, and gold-standard sentences, and then computes accuracy based on the alignment. Unlike these approaches, GLEU does not use error-coded references4 (Napoles et al., 2015). Based on BLEU (Papineni et al., 2002), it computes n-gram precision of the system output against reference sentences. GLEU additionally penalizes text in the output that was unchanged from the source but changed in the reference sentences. Recent work by Napoles et al. (2015) and Grundkiewicz et al. (2015) evaluated these metrics against human evaluations obtained using methods borrowed from the Workshop on Statistical Machine Translation (Bojar et al., 2014). Both papers found a moderate to strong correlation with human judgments for GLEU and M2 , and a slightly negative correlation for I-measure. Importantly, however, none of t"
Q16-1013,W10-1004,0,0.0643602,"ror types. A direct conversion between them, if possible, would be very complex. Additionally, it is difficult for annotators to agree on error annotations, which complicates the annotation validity as a gold standard (Leacock et al., 2014). This is due to the nature of grammatical error correction, where there can be diverse correct edits for a sentence (Figure 1). In other words, there is no single gold-standard correction. The variety of error types and potential correct edits result in very low inter-annotator agreement (IAA), as reported in previous studies (Tetreault and Chodorow, 2008; Rozovskaya and Roth, 2010; Bryant and Ng, 2015). This leads to a more fundamental question: why do we depend so much on fine-grained, lowconsensus error-type annotations as a gold standard for evaluating GEC systems? One answer is that error tags can be informative and useful to provide feedback to language learners, especially for specific closed-class error types fluency evaluation. As the development of the technology , social media becomes more and more significant role in the whole world . With the development of technology As the technology develops As technology develops plays a more and more significant role b"
Q16-1013,Q14-1033,0,0.0593669,"ater GEC corpora adhered to the same error-coding template, if not the same error types (Rozovskaya and Roth, 2010; Yannakoudakis et al., 2011; Dahlmeier et al., 2013). The first shared task in GEC aspired to the CLC’s same objective: to develop tools for language learners (Dale and Kilgarriff, 2011). Subsequent shared tasks (Dale et al., 2012; Ng et al., 2013) followed suit, targeting specific error types. Error-coded corpora are effective training and evaluation data for targeted error correction, and statistical classifiers have been developed to handle errors involving closed-class words (Rozovskaya and Roth, 2014). However, the 2014 CoNLL shared task engendered a sea change in GEC: in this shared task, systems needed to correct all errors in a sentence, of all error types, including ones more stylistic in nature (Ng et al., 2014). The evaluation metrics and annotated data from the previous shared task were used; however we argue that they do not align with the use case of this reframed task. What is the use case of whole-sentence correction? It should not be to provide specific targeted feedback on error types, but rather to rewrite sentences as a proofreader would. The community has already begun to v"
Q16-1013,W14-3301,1,0.231647,"Missing"
Q16-1013,2006.amta-papers.25,0,0.0758344,"-experts taking more liberties than experts with both the number of sentences changed and the degree of change within each sentence (see Table 2 for an extreme example of this phenomenon). In order to quantify the extent of changes made in the different annotations, we look at the percent of sentences that were left unchanged as well as the number of changes needed to transform the original sentence into the corrected annotation. To calculate the number of changes, we used a modified Translation Edit Rate (TER), which measures the number of edits needed to transform one sentence into another (Snover et al., 2006). An edit can be an insertion, deletion, substitution, or shift. We chose this metric because it counts the movement of a phrase (a shift) as one change, which the Levenshtein distance would heavily penalize. TER is calculated as the number of changes per token, but instead we report the number of changes per sentence for ease of interpretation, which we call the sTER. We compare the original set of sentences to the new annotations and the existing NUCLE and BN15 reference sets to determine the relative extent of changes made by the fluency and minimal edits (Figure 2). Compared to the origina"
Q16-1013,N12-1037,0,0.0690572,"address overlapping and interacting errors. For example, the annotators of the NUCLE corpus, which was used for the recent shared tasks, were explicitly instructed to select the minimal text span of possible alternatives (Dahlmeier et al., 2013). There are situations where error-coded annotations are useful to help students correct specific grammatical errors. The ability to do this with the non-error-coded, fluent annotations we advocate here is no longer direct, but is not lost entirely. For this purpose, some recent studies have proposed post hoc automated errortype classification methods (Swanson and Yamangil, 2012; Xue and Hwa, 2014), which compare the original sentence to its correction and deduce the error types. We speculate that, by removing the error-coding restraint, we can obtain edits that sound more fluent to native speakers while also reducing the expense of annotation, with diminished time and training requirements. Chodorow et al. (2012) and Tetreault et al. (2014) suggested that it is better to have a large number of annotators to reduce bias in automatic evaluation. Following this recommendation, we collected additional annotations without error codes, written by both experts and non-expe"
Q16-1013,W08-1205,1,0.952379,"r et al., 2013) has only 27 error types. A direct conversion between them, if possible, would be very complex. Additionally, it is difficult for annotators to agree on error annotations, which complicates the annotation validity as a gold standard (Leacock et al., 2014). This is due to the nature of grammatical error correction, where there can be diverse correct edits for a sentence (Figure 1). In other words, there is no single gold-standard correction. The variety of error types and potential correct edits result in very low inter-annotator agreement (IAA), as reported in previous studies (Tetreault and Chodorow, 2008; Rozovskaya and Roth, 2010; Bryant and Ng, 2015). This leads to a more fundamental question: why do we depend so much on fine-grained, lowconsensus error-type annotations as a gold standard for evaluating GEC systems? One answer is that error tags can be informative and useful to provide feedback to language learners, especially for specific closed-class error types fluency evaluation. As the development of the technology , social media becomes more and more significant role in the whole world . With the development of technology As the technology develops As technology develops plays a more"
Q16-1013,W10-1006,1,0.882862,"Missing"
Q16-1013,P14-2098,0,0.0349226,"racting errors. For example, the annotators of the NUCLE corpus, which was used for the recent shared tasks, were explicitly instructed to select the minimal text span of possible alternatives (Dahlmeier et al., 2013). There are situations where error-coded annotations are useful to help students correct specific grammatical errors. The ability to do this with the non-error-coded, fluent annotations we advocate here is no longer direct, but is not lost entirely. For this purpose, some recent studies have proposed post hoc automated errortype classification methods (Swanson and Yamangil, 2012; Xue and Hwa, 2014), which compare the original sentence to its correction and deduce the error types. We speculate that, by removing the error-coding restraint, we can obtain edits that sound more fluent to native speakers while also reducing the expense of annotation, with diminished time and training requirements. Chodorow et al. (2012) and Tetreault et al. (2014) suggested that it is better to have a large number of annotators to reduce bias in automatic evaluation. Following this recommendation, we collected additional annotations without error codes, written by both experts and non-experts. 5 It is importa"
Q16-1013,P11-1019,0,0.0837216,"e technology develops As technology develops plays a more and more significant role becomes more and more significant world Figure 1: An ungrammatical sentence that can be corrected in different ways. (such as determiners and prepositions). Indeed, the CLC, the first large-scale corpus of annotated grammatical errors, was coded specifically with the intent of gathering statistics about errors to inform the development of tools to help English language learners (Nicholls, 2003). Later GEC corpora adhered to the same error-coding template, if not the same error types (Rozovskaya and Roth, 2010; Yannakoudakis et al., 2011; Dahlmeier et al., 2013). The first shared task in GEC aspired to the CLC’s same objective: to develop tools for language learners (Dale and Kilgarriff, 2011). Subsequent shared tasks (Dale et al., 2012; Ng et al., 2013) followed suit, targeting specific error types. Error-coded corpora are effective training and evaluation data for targeted error correction, and statistical classifiers have been developed to handle errors involving closed-class words (Rozovskaya and Roth, 2014). However, the 2014 CoNLL shared task engendered a sea change in GEC: in this shared task, systems needed to correct"
W09-3815,N03-1016,0,0.0262451,"· xr ∈ P do for i ∈ (2 · · · r) do κ[xi−1 , xi ]++ return κ NP 0.6/1.0 0.4/0.67 〈a:〈〈JJ:NN〉:NN〉〉 PP the a 1.0/1.0 〈〈JJ:NN〉:NN〉 1.0/1.0 〈JJ:NN〉 NN 1.0/0.6 JJ Figure 2: A greedy binarization algorithm. The rank of a grammar is the rank of its largest rule. Our implementation updates the counts in κ more efficiently, but we present it this way for clarity. NN Figure 3: The binarized rules of Figure 1 arranged in a shared hypergraph forest. Each hyperedge is labeled with its weight before/after pushing. q, shifting a constant amount of weight d(q)−1 from q’s outgoing edges to its incoming edges. Klein and Manning (2003) describe an encoding of context-free grammar rule binarization that permits weight pushing to be applied. Their approach, however, works only with left or right binarizations whose rules can be encoded as an FST. We propose a form of weight pushing that works for arbitrary binarizations. Weight pushing across a grammar can be viewed as generalizing pushing from weighted transducers to a certain kind of weighted hypergraph. To begin, we use the following definition of a hypergraph: nonfinal hyperedges have a probability of 1, and final hyperedges have a probability equal to the that of the ori"
W09-3815,J97-2003,0,0.243152,"ecause of the way the probability of a rule is distributed among its binarized pieces: The standard approach is to place all of that probability on the top-level binarized rule, and to set the probabilities of lower binarized pieces to 1.0. Because these rules are reconstructed from the bottom up, pruning procedures do not have a good estimate of the complete cost of a rule until the entire original rule has been reconstructed. It is preferable to have this information earlier on, especially for larger rules. In this paper we adapt the technique of weight pushing for finite state transducers (Mohri, 1997) to arbitrary binarizations of context-free grammar rules. Weight pushing takes the probability (or, more generally, the weight) of a rule in the original grammar and pushes it down across the rule’s binarized pieces. This helps the parser make betWe apply the idea of weight pushing (Mohri, 1997) to CKY parsing with fixed context-free grammars. Applied after rule binarization, weight pushing takes the weight from the original grammar rule and pushes it down across its binarized pieces, allowing the parser to make better pruning decisions earlier in the parsing process. This process can be view"
W09-3815,P09-2012,1,0.900638,"was produced by inducing a TSG derivation on each of the trees in the training data, from which subtree counts were read directly. These derivations were induced using a collapsed Gibbs sampler, which sampled from the posterior of a Dirichlet process (DP) defined over the subtree rewrites of each nonterminal. The DP describes a generative process that prefers small subtrees but occasionally produces larger ones; when used for inference, it essentially discovers TSG derivations that contain larger subtrees only if they are frequent in the training data, which discourages model overfitting. See Post and Gildea (2009) for more detail. We ran the sampler for 100 iterations with a stop probability of 0.7 and the DP parameter α = 100, accumulating subtree counts from the derivation state at the end of all the iterations, which corresponds to the (100, 0.7, ≤ 100) grammar from that paper. All four grammar were learned from all sentences in sections 2 to 21 of the Wall Street Journal portion of the Penn Treebank. All trees were preprocessed to remove empty nodes and nontermi• level of binarization (0,1,2+) The level of binarization refers to the height of a nonterminal in the subtree created by binarizing a CFG"
W09-3815,C04-1024,0,0.024843,"inarization (depicted) produces only 5 rules due to the fact that two of them are shared. Since the complexity of parsing with CKY is a function of the grammar size as well as the input sentence length, and since in practice parsing requires significant pruning, having a smaller grammar with maximal shared substructure among the rules is desirable. We investigate two kinds of binarization in this paper. The first is right binarization, in which nonterminal pairs are collapsed beginning from the two rightmost children and moving leftward. The second is a greedy binarization, similar to that of Schmid (2004), in which the most frequently occurring (grammar-wide) nonterminal pair is collapsed in turn, according to the algorithm given in Figure 2. Binarization must ensure that the product of the probabilities of the binarized pieces is the same as that of the original rule. The easiest way to do this is to assign each newly-created binarized rule a probability of 1.0, and give the top-level rule the complete probability of the original rule. In the following subsection, we describe a better way. 2.2 Weight pushing Spreading the weight of an original rule across its binarized pieces is complicated b"
W09-3815,P01-1010,0,0.382362,"rs to hypergraphs. We examine its effect on parsing efficiency with various binarization schemes applied to tree substitution grammars from previous work. We find that weight pushing produces dramatic improvements in efficiency, especially with small amounts of time and with large grammars. 1 Introduction Fixed grammar-parsing refers to parsing that employs grammars comprising a finite set of rules that is fixed before inference time. This is in contrast to markovized grammars (Collins, 1999; Charniak, 2000), variants of tree-adjoining grammars (Chiang, 2000), or grammars with wildcard rules (Bod, 2001), all of which allow the construction and use of rules not seen in the training data. Fixed grammars must be binarized (either explicitly or implicitly) in order to maintain the O(n3 |G|) (n the sentence length, |G |the grammar size) complexity of algorithms such as the CKY algorithm. Recently, Song et al. (2008) explored different methods of binarization of a PCFG read directly from the Penn Treebank (the Treebank PCFG), 89 Proceedings of the 11th International Conference on Parsing Technologies (IWPT), pages 89–98, c Paris, October 2009. 2009 Association for Computational Linguistics ter pru"
W09-3815,D08-1018,0,0.025794,"Missing"
W09-3815,N06-1033,1,0.884574,"oth the number of rules and new nonterminals introduced, and subsequently on parsing time. This variation occurs because different binarization schemes produce different amounts of shared rules, which are rules produced during the binarization process from more than one rule in the original grammar. Increasing sharing reduces the amount of state that the parser must explore. Binarization has also been investigated in the context of parsing-based approaches to machine translation, where it has been shown that paying careful attention to the binarization scheme can produce much faster decoders (Zhang et al., 2006; Huang, 2007; DeNero et al., 2009). The choice of binarization scheme will not affect parsing results if the parser is permitted to explore the whole search space. In practice, however, this space is too large, so parsers use pruning to discard unlikely hypotheses. This presents a problem for bottom-up parsing algorithms because of the way the probability of a rule is distributed among its binarized pieces: The standard approach is to place all of that probability on the top-level binarized rule, and to set the probabilities of lower binarized pieces to 1.0. Because these rules are reconstruc"
W09-3815,A00-2018,0,0.219823,"Missing"
W09-3815,P00-1058,0,0.264512,"ed as generalizing weight pushing from transducers to hypergraphs. We examine its effect on parsing efficiency with various binarization schemes applied to tree substitution grammars from previous work. We find that weight pushing produces dramatic improvements in efficiency, especially with small amounts of time and with large grammars. 1 Introduction Fixed grammar-parsing refers to parsing that employs grammars comprising a finite set of rules that is fixed before inference time. This is in contrast to markovized grammars (Collins, 1999; Charniak, 2000), variants of tree-adjoining grammars (Chiang, 2000), or grammars with wildcard rules (Bod, 2001), all of which allow the construction and use of rules not seen in the training data. Fixed grammars must be binarized (either explicitly or implicitly) in order to maintain the O(n3 |G|) (n the sentence length, |G |the grammar size) complexity of algorithms such as the CKY algorithm. Recently, Song et al. (2008) explored different methods of binarization of a PCFG read directly from the Penn Treebank (the Treebank PCFG), 89 Proceedings of the 11th International Conference on Parsing Technologies (IWPT), pages 89–98, c Paris, October 2009. 2009 Asso"
W09-3815,N09-1026,0,0.0210377,"onterminals introduced, and subsequently on parsing time. This variation occurs because different binarization schemes produce different amounts of shared rules, which are rules produced during the binarization process from more than one rule in the original grammar. Increasing sharing reduces the amount of state that the parser must explore. Binarization has also been investigated in the context of parsing-based approaches to machine translation, where it has been shown that paying careful attention to the binarization scheme can produce much faster decoders (Zhang et al., 2006; Huang, 2007; DeNero et al., 2009). The choice of binarization scheme will not affect parsing results if the parser is permitted to explore the whole search space. In practice, however, this space is too large, so parsers use pruning to discard unlikely hypotheses. This presents a problem for bottom-up parsing algorithms because of the way the probability of a rule is distributed among its binarized pieces: The standard approach is to place all of that probability on the top-level binarized rule, and to set the probabilities of lower binarized pieces to 1.0. Because these rules are reconstructed from the bottom up, pruning pro"
W09-3815,W07-0405,0,0.0195718,"les and new nonterminals introduced, and subsequently on parsing time. This variation occurs because different binarization schemes produce different amounts of shared rules, which are rules produced during the binarization process from more than one rule in the original grammar. Increasing sharing reduces the amount of state that the parser must explore. Binarization has also been investigated in the context of parsing-based approaches to machine translation, where it has been shown that paying careful attention to the binarization scheme can produce much faster decoders (Zhang et al., 2006; Huang, 2007; DeNero et al., 2009). The choice of binarization scheme will not affect parsing results if the parser is permitted to explore the whole search space. In practice, however, this space is too large, so parsers use pruning to discard unlikely hypotheses. This presents a problem for bottom-up parsing algorithms because of the way the probability of a rule is distributed among its binarized pieces: The standard approach is to place all of that probability on the top-level binarized rule, and to set the probabilities of lower binarized pieces to 1.0. Because these rules are reconstructed from the"
W09-3815,J03-4003,0,\N,Missing
W10-1406,P01-1010,0,0.0873773,"iteration (after which performance began to fall). With the empty elements, we have achieved accuracy scores that are on par with the best accuracy scores obtained parsing the English Treebank. 6 Tree substitution grammars We have shown that coarse labels and the prevalence of N ULL elements in Korean both contribute to parsing difficulty. We now turn to grammar formalisms that allow us to work with larger fragments of parse trees than the height-one rules of standard contextfree grammars. Tree substitution grammars (TSGs) have been shown to improve upon the standard English Treebank grammar (Bod, 2001) in parser accuracy, and more recently, techniques for inferring TSG subtrees in a Bayesian framework have enabled learning more efficiently representable grammars, permitting some interesting analysis (O’Donnell et al., 2009; Cohn et al., 2009; Post and Gildea, 2009). In this section, we try parsing the KTB with TSGs. We experiment with different methods of learning TSGs to see whether they can reveal any insights into the difficulties parsing Korean. 6.1 Head rules TSGs present some difficulties in learning and representation, but a simple extraction heuristic called a spinal grammar has bee"
W10-1406,P00-1058,0,0.057207,"re recently, techniques for inferring TSG subtrees in a Bayesian framework have enabled learning more efficiently representable grammars, permitting some interesting analysis (O’Donnell et al., 2009; Cohn et al., 2009; Post and Gildea, 2009). In this section, we try parsing the KTB with TSGs. We experiment with different methods of learning TSGs to see whether they can reveal any insights into the difficulties parsing Korean. 6.1 Head rules TSGs present some difficulties in learning and representation, but a simple extraction heuristic called a spinal grammar has been shown to be very useful (Chiang, 2000; Sangati and Zuidema, 2009; Post and Gildea, 2009). Spinal subtrees are extracted from a parse tree by using a set of head rules to maximally project each lexical item (a word or morpheme). Each node in the parse tree having a different head from its parent becomes the root of a new subtree, which induces a spinal TSG derivation in the parse tree (see Figure 1). A probabilistic grammar is derived by taking counts from these trees, smoothing them with counts of all depth-one rules from the same training set, and setting rule probabilities to relative frequency. This heuristic requires a set of"
W10-1406,N09-1062,0,0.0933868,"that coarse labels and the prevalence of N ULL elements in Korean both contribute to parsing difficulty. We now turn to grammar formalisms that allow us to work with larger fragments of parse trees than the height-one rules of standard contextfree grammars. Tree substitution grammars (TSGs) have been shown to improve upon the standard English Treebank grammar (Bod, 2001) in parser accuracy, and more recently, techniques for inferring TSG subtrees in a Bayesian framework have enabled learning more efficiently representable grammars, permitting some interesting analysis (O’Donnell et al., 2009; Cohn et al., 2009; Post and Gildea, 2009). In this section, we try parsing the KTB with TSGs. We experiment with different methods of learning TSGs to see whether they can reveal any insights into the difficulties parsing Korean. 6.1 Head rules TSGs present some difficulties in learning and representation, but a simple extraction heuristic called a spinal grammar has been shown to be very useful (Chiang, 2000; Sangati and Zuidema, 2009; Post and Gildea, 2009). Spinal subtrees are extracted from a parse tree by using a set of head rules to maximally project each lexical item (a word or morpheme). Each node in t"
W10-1406,P97-1003,0,0.2432,"y taking counts from these trees, smoothing them with counts of all depth-one rules from the same training set, and setting rule probabilities to relative frequency. This heuristic requires a set of head rules, which we present in Table 6. As an evaluation of our rules, we list in Table 7 the accuracy results for parsing with spinal grammars extracted using the head rules we developed as well as with two head rule heuristics (head-left and head-right). As a point of comparison, we provide the same results for English, using the standard Magerman/Collins head rules for English (Magerman, 1995; Collins, 1997). Function tags were retained for Korean but not for English. We observe a number of things from Table 7. First, the relative performance of the head-left and 54 NT S VV VX ADJP CV LV NP VJ VP ⋆ RC SFN EFN EFN EFN EFN EFN EFN EFN EFN ⋆ rule second rightmost child rightmost XSV rightmost VJ or CO rightmost VJ rightmost VV rightmost VV rightmost CO rightmost XSV or XSJ rightmost VX, XSV, or VV rightmost child Table 6: Head rules for the Korean Treebank. NT is the nonterminal whose head is being determined, RC identifies the label of its rightmost child. The default is to take the rightmost child"
W10-1406,A00-2016,0,0.0309149,"orphological and syntactic information. The corpus contains roughly 5K sentences, 132K words, and 14K unique morphemes. The syntactic bracketing rules are mostly the same as the previous version of the treebank (Han et al., 2001) and the phrase structure annotation schemes used are very similar to the ones used in Penn English treebank. The Korean Treebank is constructed over text that has been morphologically analyzed; not only is the text tokenized into morphemes, but all allomorphs are neutralized. To our knowledge, there have been only a few papers focusing on syntactic parsing of Korean. Hermjakob (2000) implemented a shift-reduce parser for Korean trained on very limited (1K sentences) data, and Sarkar and Han (2002) used an earlier version of the Treebank to train a lexicalized tree adjoining grammar. In this paper, we conduct a range of experiments using the Korean Treebank 2.0 (hereafter, KTB) as our training data and provide analyses that reveal insights into parsing morphologically rich languages like Korean. We try to provide comparisons with English parsing using parsers trained on a similar amount of data wherever applicable. 2 Difficulties parsing Korean There are several challenges"
W10-1406,J98-4004,0,0.189834,"side nonterminal is present three or more times on its righthand side. There are only three instances of such “triple+recursive” NPs among the ∼40K trees in the training portion of the PTB, each occurring only once. NP → NP NP NP , CC NP NP → NP NP NP CC NP NP → NP NP NP NP . The KTB is an eighth of the size of this, but has fifteen instances of such NPs (listed here with their frequencies): 1 We thank one of our anonymous reviewers for bringing this to our attention. 51 Nonterminal granularity There are many ways to refine the set of nonterminals in a Treebank. A simple approach suggested by Johnson (1998) is to simply annotate each node with its parent’s label. The effect of this is to refine the distribution of each nonterminal over sequences of children according to its position in the sentence; for example, a VP beneath an SBAR node will have a different distribution over children than a VP beneath an S node. This simple technique alone produces a large improvement in English Treebank parsing. Klein and Manning (2003) expanded this idea with a series of experiments wherein they manually refined nonterminals to different degrees, which resulted in parsing accuracy rivaling that of bilexicali"
W10-1406,W04-3306,0,0.0293379,"a even greater problem than in the English Treebank. We also find that Korean’s relatively free word order does not impact parsing results as much as one might expect, but in fact the prevalence of zero pronouns accounts for a large portion of the difference between Korean and English parsing scores. 1 Introduction Korean is a head-final, agglutinative, and morphologically productive language. The language presents multiple challenges for syntactic parsing. Like some other head-final languages such as German, Japanese, and Hindi, Korean exhibits long-distance scrambling (Rambow and Lee, 1994; Kallmeyer and Yoon, 2004). Compound nouns are formed freely (Park et al., 2004), and verbs have well over 400 paradigmatic endings (Martin, 1992). Korean Treebank 2.0 (LDC2006T09) (Han and Ryu, 2005) is a subset of a Korean newswire corpus (LDC2000T45) annotated with morphological and syntactic information. The corpus contains roughly 5K sentences, 132K words, and 14K unique morphemes. The syntactic bracketing rules are mostly the same as the previous version of the treebank (Han et al., 2001) and the phrase structure annotation schemes used are very similar to the ones used in Penn English treebank. The Korean Treeba"
W10-1406,P03-1054,0,0.0364175,"ur anonymous reviewers for bringing this to our attention. 51 Nonterminal granularity There are many ways to refine the set of nonterminals in a Treebank. A simple approach suggested by Johnson (1998) is to simply annotate each node with its parent’s label. The effect of this is to refine the distribution of each nonterminal over sequences of children according to its position in the sentence; for example, a VP beneath an SBAR node will have a different distribution over children than a VP beneath an S node. This simple technique alone produces a large improvement in English Treebank parsing. Klein and Manning (2003) expanded this idea with a series of experiments wherein they manually refined nonterminals to different degrees, which resulted in parsing accuracy rivaling that of bilexicalized parsing models of the time. More recently, Petrov et al. (2006) refined techniques originally proposed by Matsuzaki et al. (2005) and Prescher SBJ OBJ COMP ADV VOC LV subject with nominative case marker complement with accusative case marker complement with adverbial postposition NP that function as adverbial phrase noun with vocative case maker NP coupled with “light” verb construction Table 2: Function tags in the"
W10-1406,I05-2034,0,0.0266992,"Missing"
W10-1406,P00-1048,0,0.0259375,"Missing"
W10-1406,P95-1037,0,0.251065,"mar is derived by taking counts from these trees, smoothing them with counts of all depth-one rules from the same training set, and setting rule probabilities to relative frequency. This heuristic requires a set of head rules, which we present in Table 6. As an evaluation of our rules, we list in Table 7 the accuracy results for parsing with spinal grammars extracted using the head rules we developed as well as with two head rule heuristics (head-left and head-right). As a point of comparison, we provide the same results for English, using the standard Magerman/Collins head rules for English (Magerman, 1995; Collins, 1997). Function tags were retained for Korean but not for English. We observe a number of things from Table 7. First, the relative performance of the head-left and 54 NT S VV VX ADJP CV LV NP VJ VP ⋆ RC SFN EFN EFN EFN EFN EFN EFN EFN EFN ⋆ rule second rightmost child rightmost XSV rightmost VJ or CO rightmost VJ rightmost VV rightmost VV rightmost CO rightmost XSV or XSJ rightmost VX, XSV, or VV rightmost child Table 6: Head rules for the Korean Treebank. NT is the nonterminal whose head is being determined, RC identifies the label of its rightmost child. The default is to take the"
W10-1406,P05-1010,0,0.0263646,"each nonterminal over sequences of children according to its position in the sentence; for example, a VP beneath an SBAR node will have a different distribution over children than a VP beneath an S node. This simple technique alone produces a large improvement in English Treebank parsing. Klein and Manning (2003) expanded this idea with a series of experiments wherein they manually refined nonterminals to different degrees, which resulted in parsing accuracy rivaling that of bilexicalized parsing models of the time. More recently, Petrov et al. (2006) refined techniques originally proposed by Matsuzaki et al. (2005) and Prescher SBJ OBJ COMP ADV VOC LV subject with nominative case marker complement with accusative case marker complement with adverbial postposition NP that function as adverbial phrase noun with vocative case maker NP coupled with “light” verb construction Table 2: Function tags in the Korean treebank model F1 Korean 52.78 coarse w/ function tags 56.18 English (small) 72.20 coarse w/ function tags 70.50 English (standard) 71.61 coarse w/ function tags 72.82 F1≤40 56.55 60.21 73.29 71.78 72.74 74.05 Table 3: Parser scores for Treebank PCFGs in Korean and English with and without function ta"
W10-1406,P06-1055,0,0.755068,"26 73.29 72.74 types 6.6K 5.5K 7.5K 23K tokens 194K 96K 147K 950K Table 1: Parser scores for Treebank PCFGs in Korean and English. For English, we vary the size of the training data to provide a better point of comparison against Korean. Types and tokens denote vocabulary sizes (which for Korean is the mean over the folds). was set to all words occurring more than once in its training data, with a handful of count one tokens replacing unknown words based on properties of the word’s surface form (all Korean words were placed in a single bin, and English words were binned following the rules of Petrov et al. (2006)). We report scores on the development set. We report parser accuracy scores using the standard F1 metric, which balances precision and recall of the labeled constituents recovered by the parser: 2P R/(P + R). Throughout the paper, all evaluation occurs against gold standard trees that contain no N ULL elements or nonterminal function tags or annotations, which in some cases requires the removal of those elements from parse trees output by the parser. 3.2 Treebank grammars We begin by presenting in Table 1 scores for the standard Treebank grammar, obtained by reading a standard context-free gr"
W10-1406,P09-2012,1,0.934853,"and the prevalence of N ULL elements in Korean both contribute to parsing difficulty. We now turn to grammar formalisms that allow us to work with larger fragments of parse trees than the height-one rules of standard contextfree grammars. Tree substitution grammars (TSGs) have been shown to improve upon the standard English Treebank grammar (Bod, 2001) in parser accuracy, and more recently, techniques for inferring TSG subtrees in a Bayesian framework have enabled learning more efficiently representable grammars, permitting some interesting analysis (O’Donnell et al., 2009; Cohn et al., 2009; Post and Gildea, 2009). In this section, we try parsing the KTB with TSGs. We experiment with different methods of learning TSGs to see whether they can reveal any insights into the difficulties parsing Korean. 6.1 Head rules TSGs present some difficulties in learning and representation, but a simple extraction heuristic called a spinal grammar has been shown to be very useful (Chiang, 2000; Sangati and Zuidema, 2009; Post and Gildea, 2009). Spinal subtrees are extracted from a parse tree by using a set of head rules to maximally project each lexical item (a word or morpheme). Each node in the parse tree having a d"
W10-1406,W07-2460,0,0.122936,"Missing"
W10-1406,E09-1080,0,0.0126512,"echniques for inferring TSG subtrees in a Bayesian framework have enabled learning more efficiently representable grammars, permitting some interesting analysis (O’Donnell et al., 2009; Cohn et al., 2009; Post and Gildea, 2009). In this section, we try parsing the KTB with TSGs. We experiment with different methods of learning TSGs to see whether they can reveal any insights into the difficulties parsing Korean. 6.1 Head rules TSGs present some difficulties in learning and representation, but a simple extraction heuristic called a spinal grammar has been shown to be very useful (Chiang, 2000; Sangati and Zuidema, 2009; Post and Gildea, 2009). Spinal subtrees are extracted from a parse tree by using a set of head rules to maximally project each lexical item (a word or morpheme). Each node in the parse tree having a different head from its parent becomes the root of a new subtree, which induces a spinal TSG derivation in the parse tree (see Figure 1). A probabilistic grammar is derived by taking counts from these trees, smoothing them with counts of all depth-one rules from the same training set, and setting rule probabilities to relative frequency. This heuristic requires a set of head rules, which we prese"
W10-1406,W02-2207,0,0.526071,"rphemes. The syntactic bracketing rules are mostly the same as the previous version of the treebank (Han et al., 2001) and the phrase structure annotation schemes used are very similar to the ones used in Penn English treebank. The Korean Treebank is constructed over text that has been morphologically analyzed; not only is the text tokenized into morphemes, but all allomorphs are neutralized. To our knowledge, there have been only a few papers focusing on syntactic parsing of Korean. Hermjakob (2000) implemented a shift-reduce parser for Korean trained on very limited (1K sentences) data, and Sarkar and Han (2002) used an earlier version of the Treebank to train a lexicalized tree adjoining grammar. In this paper, we conduct a range of experiments using the Korean Treebank 2.0 (hereafter, KTB) as our training data and provide analyses that reveal insights into parsing morphologically rich languages like Korean. We try to provide comparisons with English parsing using parsers trained on a similar amount of data wherever applicable. 2 Difficulties parsing Korean There are several challenges in parsing Korean compared to languages like English. At the root of many of these challenges is the fact that it i"
W10-1406,2004.jeptalnrecital-long.24,0,\N,Missing
W11-2160,J07-2003,0,0.622091,"ntire MT pipeline, from data preparation to evaluation. This script is built on top of a module called CachePipe. CachePipe is a simple wrapper around shell commands that uses SHA-1 hashes and explicitlyprovided lists of dependencies to determine whether a command needs to be run, saving time both in running and debugging machine translation pipelines. We present progress on Joshua, an opensource decoder for hierarchical and syntaxbased machine translation. The main focus is describing Thrax, a flexible, open source synchronous context-free grammar extractor. Thrax extracts both hierarchical (Chiang, 2007) and syntax-augmented machine translation (Zollmann and Venugopal, 2006) grammars. It is built on Apache Hadoop for efficient distributed performance, and can easily be extended with support for new grammars, feature functions, and output formats. 1 2 Introduction Joshua is an open-source1 toolkit for hierarchical machine translation of human languages. The original version of Joshua (Li et al., 2009) was a reimplementation of the Python-based Hiero machinetranslation system (Chiang, 2007); it was later extended (Li et al., 2010) to support richer formalisms, such as SAMT (Zollmann and Venugop"
W11-2160,P10-1146,0,0.0355065,"Missing"
W11-2160,clark-lavie-2010-loonybin,0,0.0141231,"have scores for the final version of the paper. 5 LDC2009T13 4 482 4 CachePipe: Cached pipeline runs Machine translation pipelines involve the specification and execution of many different datasets, training procedures, and pre- and post-processing techniques that can have large effects on translation outcome, and which make direct comparisons between systems difficult. The complexity of managing these pipelines and experimental environments has led to a number of different experimental management systems, such as Experiment.perl,6 Joshua 2.0’s Makefile system (Li et al., 2010), and LoonyBin (Clark and Lavie, 2010). In addition to managing the pipeline, these scripts employ different techniques to avoid expensive recomputation by caching steps. 6 http://www.statmt.org/moses/?n= FactoredTraining.EMS S GLUE VP GLUE COMMA+SBAR+. PP DT+NP VBN NP the reactor type der reaktortyp ADJP will be wird zwar NN operated mit NP with betrieben uran , das JJ , which is nicht angereichert ist . JJ PP GLUE not enriched . VBN NN DT+NP uranium ADJP VP COMMA+SBAR+. GLUE S Figure 2: An SAMT derivation. The shaded terminal symbols are the lexicalized part of a rule with terminals and non-terminals. The unshaded terminals are"
W11-2160,P10-4002,1,0.121611,"s to describe this past year’s work in developing Thrax (Weese, 2011), an open-source grammar extractor for Hiero and SAMT grammars. Grammar extraction has shown itself to be something of a black art, with decoding performance depending crucially on a variety of features and options that are not always clearly described in papers. This hindered direct comparison both between and within grammatical formalisms. Thrax standardizes Joshua’s grammar ex1 http://github.com/joshua-decoder/joshua Thrax: grammar extraction In modern machine translation systems such as Joshua (Li et al., 2009) and cdec (Dyer et al., 2010), a translation model is represented as a synchronous context-free grammar (SCFG). Formally, an SCFG may be considered as a tuple (N, S, Tσ , Tτ , G) where N is a set of nonterminal symbols of the grammar, S ∈ N is the goal symbol, Tσ and Tτ are the source- and target-side terminal symbol vocabularies, respectively, and G is a set of production rules of the grammar. Each rule in G is of the form X → hα, γ, ∼i where X ∈ N is a nonterminal symbol, α is a sequence of symbols from N ∪ Tσ , γ is a sequence of 478 Proceedings of the 6th Workshop on Statistical Machine Translation, pages 478–484, c E"
W11-2160,N04-1035,0,0.158565,"Missing"
W11-2160,N03-1017,0,0.00301836,"a constant phrase penalty In addition to simple features, Thrax also implements map-reduce features. These are features that require comparing rules in a certain order. Thrax uses Hadoop to sort the rules efficiently and calculate these feature functions. Thrax implements the following map-reduce features: • Phrasal translation probabilities p(α|γ) and p(γ|α), calculated with relative frequency: p(α|γ) = C(α, γ) C(γ) (2) (and vice versa), where C(·) is the number of times a given event was extracted. • Lexical weighting plex (α|γ, A) and plex (γ|α, A). We calculate these weights as given in (Koehn et al., 2003): let A be the alignment between α and γ, so (i, j) ∈ A if and only if the ith word of α is aligned to the jth word of γ. Then we can define plex (γ|α) as n Y i=1 X 1 w(γj |αi ) |{j : (i, j) ∈ A}| (3) (i,j)∈A where αi is the ith word of α, γj is the jth word of γ, and w(y|x) is the relative frequency of seeing word y given x. • Rarity penalty, given by exp(1 − C(X → hα, γi)) (4) where again C(·) is a count of the number of times the rule was extracted. 481 The above features are all implemented and can be turned on or off with a keyword in the Thrax configuration file. It is easy to extend Thr"
W11-2160,N04-1022,0,0.0316682,"to build and test new models. the data. We also removed any sentences longer than 50 tokens (after tokenization). For SAMT grammar extraction, we parsed the English training data using the Berkeley Parser (Petrov et al., 2006) with the provided Treebank-trained grammar. We tuned the model weights against the WMT08 test set (news-test2008) using ZMERT (Zaidan, 2009), an implementation of minimum error-rate training included with Joshua. We decoded the test set to produce a 300-best list of unique translations, then chose the best candidate for each sentence using Minimum Bayes Risk reranking (Kumar and Byrne, 2004). Figure 2 shows an example derivation with an SAMT grammar. To re-case the 1-best test set output, we trained a true-case 5gram language model using the same LM training data as before, and used an SCFG translation model to translate from the lowercased to true-case output. The translation model used rules limited to five tokens in length, and contained no hierarchical rules. 3 Experiments We built systems for six language pairs for the WMT 2011 shared task: cz-en, en-cz, de-en, en-de, fr-en, and en-fr.3 For each language pair, we built both SAMT and hiero grammars.4 Table 3 contains the resu"
W11-2160,W09-0424,1,0.922719,"Missing"
W11-2160,W10-1718,1,0.929398,"s context-free grammar extractor. Thrax extracts both hierarchical (Chiang, 2007) and syntax-augmented machine translation (Zollmann and Venugopal, 2006) grammars. It is built on Apache Hadoop for efficient distributed performance, and can easily be extended with support for new grammars, feature functions, and output formats. 1 2 Introduction Joshua is an open-source1 toolkit for hierarchical machine translation of human languages. The original version of Joshua (Li et al., 2009) was a reimplementation of the Python-based Hiero machinetranslation system (Chiang, 2007); it was later extended (Li et al., 2010) to support richer formalisms, such as SAMT (Zollmann and Venugopal, 2006). The main focus of this paper is to describe this past year’s work in developing Thrax (Weese, 2011), an open-source grammar extractor for Hiero and SAMT grammars. Grammar extraction has shown itself to be something of a black art, with decoding performance depending crucially on a variety of features and options that are not always clearly described in papers. This hindered direct comparison both between and within grammatical formalisms. Thrax standardizes Joshua’s grammar ex1 http://github.com/joshua-decoder/joshua T"
W11-2160,P09-1063,0,0.0262969,"Missing"
W11-2160,C08-1064,1,0.294204,"pus with word-level alignments. SAMT additionally requires that the target side of the corpus be parsed. There are several parameters that can make a significant difference in a grammar’s overall translation performance. Each of these parameters is easily adjustable in Thrax by changing its value in a configuration file. • maximum rule span • maximum span of consistent phrase pairs • maximum number of nonterminals • whether to allow unaligned words at the edges of consistent phrase pairs Chiang (2007) gives reasonable heuristic choices for these parameters when extracting a Hiero grammar, and Lopez (2008) confirms some of them (maximum rule span of 10, maximum number of sourceside symbols at 5, and maximum number of nonterminals at 2 per rule). ?) provided comparisons among phrase-based, hierarchical, and syntax-based models, but did not report extensive experimentation with the model parameterizations. When extracting Hiero- or SAMT-style grammars, the first Hadoop job in the Thrax workflow takes in a parallel corpus and produces a set of rules. But in fact Thrax’s extraction mechanism is more general than that; all it requires is a function that maps a string to a set of rules. This makes it"
W11-2160,P06-1055,0,0.0220837,"n run on Hadoop, as Thrax does. The Joshua and cdec extractors only extract Hiero grammars, and Zollmann and Venugopal’s extractor can only extract SAMT-style grammars. They are not designed to score arbitrary feature sets, either. Since variation in translation models and feature sets can have a significant effect on translation performance, we have developed Thrax in order to make it easy to build and test new models. the data. We also removed any sentences longer than 50 tokens (after tokenization). For SAMT grammar extraction, we parsed the English training data using the Berkeley Parser (Petrov et al., 2006) with the provided Treebank-trained grammar. We tuned the model weights against the WMT08 test set (news-test2008) using ZMERT (Zaidan, 2009), an implementation of minimum error-rate training included with Joshua. We decoded the test set to produce a 300-best list of unique translations, then chose the best candidate for each sentence using Minimum Bayes Risk reranking (Kumar and Byrne, 2004). Figure 2 shows an example derivation with an SAMT grammar. To re-case the 1-best test set output, we trained a true-case 5gram language model using the same LM training data as before, and used an SCFG t"
W11-2160,P99-1039,0,0.0362819,"Missing"
W11-2160,P08-1064,0,\N,Missing
W11-2160,C08-1144,0,\N,Missing
W11-2160,W06-3119,0,\N,Missing
W12-1904,P10-1112,0,0.0148274,", and though Monte Carlo techniques can provide an approximation, the samplers can be complex, difficult to code, and slow to converge. This history suggests two approaches to state-split TSGs: (1) a Bayesian non-parametric sampling approach (incorporate state-splitting into existing TSG work), or (2) EM (incorporate TSG induction into existing state-splitting work). We choose the latter path, and in the next section will describe our approach which combines the simplicity of DOP, the intuitions motivating the Bayesian approach, and the efficiency of EM-based state-splitting. In related work, Bansal and Klein (2010) combine (1996)’s implicit DOP representation with a number of the manual refinements described in Klein and Manning (2003). They achieve some of the best reported parsing scores for TSG work and demonstrate the complementarity of the tasks, but their approach is not able to learn arbitrary distributions over fragments, and the state splits are determined in a fixed pre-processing step. Our approach addresses both of these limitations. 3 State-Split TSG Induction In this section we describe how we combine the ideas of dop, Bayesian-induced TSGs and Petrov et al. (2006)’s state-splitting framew"
W12-1904,E93-1006,0,0.272999,"contextfree grammars by generalizing the atomic units of the grammar from depth-one productions to fragments of arbitrary size. An example TSG fragment along with equivalent CFG rules are depicted in Figure 1. The two formalisms are weakly equivalent, and computing the most probable derivation of a sentence with a TSG can be done in cubic time. Unfortunately, learning TSGs is not straightforward, in large part because TSG-specific resources (e.g., large scale TSG-annotated treebanks) do not exist. One class of existing approaches, known as Data-Oriented Parsing, simply uses all the fragments (Bod, 1993, DOP). This does not scale well to large treebanks, forcing the use of implicit representations (Goodman, 1996) or heuristic subsets (Bod, 2001). It has also been generally observed that the use of all fragments results in poor, overfit grammars, though this can be addressed with held-out data (Zollmann and Sima’an, 2005) or statistical estimators to rule out fragments that are unlikely to generalize (Zuidema, 2007). More recently, a number of groups have found success employing Bayesian non-parametric priors (Post and Gildea, 2009; Cohn et al., 2010), which put a downward pressure on fragmen"
W12-1904,P01-1010,0,0.0368287,"ragment along with equivalent CFG rules are depicted in Figure 1. The two formalisms are weakly equivalent, and computing the most probable derivation of a sentence with a TSG can be done in cubic time. Unfortunately, learning TSGs is not straightforward, in large part because TSG-specific resources (e.g., large scale TSG-annotated treebanks) do not exist. One class of existing approaches, known as Data-Oriented Parsing, simply uses all the fragments (Bod, 1993, DOP). This does not scale well to large treebanks, forcing the use of implicit representations (Goodman, 1996) or heuristic subsets (Bod, 2001). It has also been generally observed that the use of all fragments results in poor, overfit grammars, though this can be addressed with held-out data (Zollmann and Sima’an, 2005) or statistical estimators to rule out fragments that are unlikely to generalize (Zuidema, 2007). More recently, a number of groups have found success employing Bayesian non-parametric priors (Post and Gildea, 2009; Cohn et al., 2010), which put a downward pressure on fragment size except where the data warrant the inclusion of larger fragments. Unfortunately, proper inference under these models is intractable, and th"
W12-1904,W10-1406,1,0.815854,"y by thresholding at every iteration. (a) Modal construction. (b) Modifiable NP. NP2 S2 NP0 MD VP0 NN VP0 president will (c) Nominal-modification. 4 Datasets (d) PP construction. NP0 We perform a qualitative analysis of fragments learned on datasets for two languages: the Korean Treebank v2.0 (Han and Ryu, 2005) and a comparably-sized portion of the WSJ portion of the Penn Treebank (Marcus et al., 1993). The Korean Treebank (KTB) has predefined splits; to be comparable for our analysis, from the PTB we used §2-3 for training and §22 for validation (we refer to this as wsj2-3). As described in Chung et al. (2010), although Korean presents its own challenges to grammar induction, the KTB yields additional difficulties by including a high occurrence of very flat rules (in 5K sentences, there are 13 NP rules with at least four righthand side NPs) and a coarser nonterminal set than that of the Penn Treebank. On both sets, we run for two iterations. Recall that our algorithm is designed to induce a state-split TSG on a binarized tree; as neither dataset is binarized in native form we apply a left-branching binarization across all trees in both collections as a preprocessing step. Petrov et al. (2006) found"
W12-1904,N09-1062,0,0.0423612,"Missing"
W12-1904,W06-1638,0,0.0185372,"aging the complementary natures of these two approaches. 1 Introduction Context-free grammars (CFGs) are a useful tool for describing the structure of language, modeling a variety of linguistic phenomena while still permitting efficient inference. However, it is widely acknowledged that CFGs employed in practice make unrealistic independence and structural assumptions, resulting in grammars that are overly permissive. One successful approach has been to refine the nonterminals of grammars, first manually (Johnson, 1998; Klein and Manning, 2003) and later automatically (Matsuzaki et al., 2005; Dreyer and Eisner, 2006; Petrov et al., 2006). In addition to improving parsing accuracy, the automatically learned latent annotations of these latter approaches yield results that accord well with human intuitions, especially at the lexical or preterminal level (for example, separating demonstrative adjectives from definite articles under the DT tag). It is more difficult, though, to extend this analysis to higher-level nonterminals, where the long-distance interactions among latent annotations of internal nodes are subtle and difficult to trace. In another line of work, many researchers have examined the use of fo"
W12-1904,W12-2013,1,0.826852,"Missing"
W12-1904,W96-0214,0,0.0867703,"s of arbitrary size. An example TSG fragment along with equivalent CFG rules are depicted in Figure 1. The two formalisms are weakly equivalent, and computing the most probable derivation of a sentence with a TSG can be done in cubic time. Unfortunately, learning TSGs is not straightforward, in large part because TSG-specific resources (e.g., large scale TSG-annotated treebanks) do not exist. One class of existing approaches, known as Data-Oriented Parsing, simply uses all the fragments (Bod, 1993, DOP). This does not scale well to large treebanks, forcing the use of implicit representations (Goodman, 1996) or heuristic subsets (Bod, 2001). It has also been generally observed that the use of all fragments results in poor, overfit grammars, though this can be addressed with held-out data (Zollmann and Sima’an, 2005) or statistical estimators to rule out fragments that are unlikely to generalize (Zuidema, 2007). More recently, a number of groups have found success employing Bayesian non-parametric priors (Post and Gildea, 2009; Cohn et al., 2010), which put a downward pressure on fragment size except where the data warrant the inclusion of larger fragments. Unfortunately, proper inference under th"
W12-1904,J98-4004,0,0.322991,"ility in grammar induction within the structured guidance provided by the treebank, leveraging the complementary natures of these two approaches. 1 Introduction Context-free grammars (CFGs) are a useful tool for describing the structure of language, modeling a variety of linguistic phenomena while still permitting efficient inference. However, it is widely acknowledged that CFGs employed in practice make unrealistic independence and structural assumptions, resulting in grammars that are overly permissive. One successful approach has been to refine the nonterminals of grammars, first manually (Johnson, 1998; Klein and Manning, 2003) and later automatically (Matsuzaki et al., 2005; Dreyer and Eisner, 2006; Petrov et al., 2006). In addition to improving parsing accuracy, the automatically learned latent annotations of these latter approaches yield results that accord well with human intuitions, especially at the lexical or preterminal level (for example, separating demonstrative adjectives from definite articles under the DT tag). It is more difficult, though, to extend this analysis to higher-level nonterminals, where the long-distance interactions among latent annotations of internal nodes are s"
W12-1904,P03-1054,0,0.588832,"r induction within the structured guidance provided by the treebank, leveraging the complementary natures of these two approaches. 1 Introduction Context-free grammars (CFGs) are a useful tool for describing the structure of language, modeling a variety of linguistic phenomena while still permitting efficient inference. However, it is widely acknowledged that CFGs employed in practice make unrealistic independence and structural assumptions, resulting in grammars that are overly permissive. One successful approach has been to refine the nonterminals of grammars, first manually (Johnson, 1998; Klein and Manning, 2003) and later automatically (Matsuzaki et al., 2005; Dreyer and Eisner, 2006; Petrov et al., 2006). In addition to improving parsing accuracy, the automatically learned latent annotations of these latter approaches yield results that accord well with human intuitions, especially at the lexical or preterminal level (for example, separating demonstrative adjectives from definite articles under the DT tag). It is more difficult, though, to extend this analysis to higher-level nonterminals, where the long-distance interactions among latent annotations of internal nodes are subtle and difficult to tra"
W12-1904,J93-2004,0,0.0451158,"inference) while at the same time allowing for the modeling of long distance dependencies. Fragments from such grammars are intuitive, capturing exactly the sorts of phrasal-level properties (such as predicate-argument structure) that are not present in Treebank CFGs and which are difficult to model with latent annotations. This paper is motivated by the complementarity of these approaches. We present our progress in learning latent-variable TSGs in a joint approach that extends the split-merge framework of Petrov et al. (2006). We present our current results on the Penn and Korean treebanks (Marcus et al., 1993; Han et al., 2001), demonstrating that we are able to learn fragments that draw on the strengths of both approaches. Table 1 situates this work among other contributions. In addition to experimenting directly with the Penn and Korean Treebanks, we also conducted two experiments in this framework with the Universal 23 NAACL-HLT Workshop on the Induction of Linguistic Structure, pages 23–30, c Montr´eal, Canada, June 3-8, 2012. 2012 Association for Computational Linguistics CFG none Charniak ’97 manual Klein & Manning ’03 automatic Matsuzaki et al. ’05 Petrov et al. ’06 Dreyer & Eisner ’06 TSG"
W12-1904,P05-1010,0,0.608017,"d by the treebank, leveraging the complementary natures of these two approaches. 1 Introduction Context-free grammars (CFGs) are a useful tool for describing the structure of language, modeling a variety of linguistic phenomena while still permitting efficient inference. However, it is widely acknowledged that CFGs employed in practice make unrealistic independence and structural assumptions, resulting in grammars that are overly permissive. One successful approach has been to refine the nonterminals of grammars, first manually (Johnson, 1998; Klein and Manning, 2003) and later automatically (Matsuzaki et al., 2005; Dreyer and Eisner, 2006; Petrov et al., 2006). In addition to improving parsing accuracy, the automatically learned latent annotations of these latter approaches yield results that accord well with human intuitions, especially at the lexical or preterminal level (for example, separating demonstrative adjectives from definite articles under the DT tag). It is more difficult, though, to extend this analysis to higher-level nonterminals, where the long-distance interactions among latent annotations of internal nodes are subtle and difficult to trace. In another line of work, many researchers ha"
W12-1904,P06-1055,0,0.825777,"atures of these two approaches. 1 Introduction Context-free grammars (CFGs) are a useful tool for describing the structure of language, modeling a variety of linguistic phenomena while still permitting efficient inference. However, it is widely acknowledged that CFGs employed in practice make unrealistic independence and structural assumptions, resulting in grammars that are overly permissive. One successful approach has been to refine the nonterminals of grammars, first manually (Johnson, 1998; Klein and Manning, 2003) and later automatically (Matsuzaki et al., 2005; Dreyer and Eisner, 2006; Petrov et al., 2006). In addition to improving parsing accuracy, the automatically learned latent annotations of these latter approaches yield results that accord well with human intuitions, especially at the lexical or preterminal level (for example, separating demonstrative adjectives from definite articles under the DT tag). It is more difficult, though, to extend this analysis to higher-level nonterminals, where the long-distance interactions among latent annotations of internal nodes are subtle and difficult to trace. In another line of work, many researchers have examined the use of formalisms with an exten"
W12-1904,P09-2012,1,0.927289,"pproaches, known as Data-Oriented Parsing, simply uses all the fragments (Bod, 1993, DOP). This does not scale well to large treebanks, forcing the use of implicit representations (Goodman, 1996) or heuristic subsets (Bod, 2001). It has also been generally observed that the use of all fragments results in poor, overfit grammars, though this can be addressed with held-out data (Zollmann and Sima’an, 2005) or statistical estimators to rule out fragments that are unlikely to generalize (Zuidema, 2007). More recently, a number of groups have found success employing Bayesian non-parametric priors (Post and Gildea, 2009; Cohn et al., 2010), which put a downward pressure on fragment size except where the data warrant the inclusion of larger fragments. Unfortunately, proper inference under these models is intractable, and though Monte Carlo techniques can provide an approximation, the samplers can be complex, difficult to code, and slow to converge. This history suggests two approaches to state-split TSGs: (1) a Bayesian non-parametric sampling approach (incorporate state-splitting into existing TSG work), or (2) EM (incorporate TSG induction into existing state-splitting work). We choose the latter path, and"
W12-1904,D07-1058,0,0.0186452,"ecause TSG-specific resources (e.g., large scale TSG-annotated treebanks) do not exist. One class of existing approaches, known as Data-Oriented Parsing, simply uses all the fragments (Bod, 1993, DOP). This does not scale well to large treebanks, forcing the use of implicit representations (Goodman, 1996) or heuristic subsets (Bod, 2001). It has also been generally observed that the use of all fragments results in poor, overfit grammars, though this can be addressed with held-out data (Zollmann and Sima’an, 2005) or statistical estimators to rule out fragments that are unlikely to generalize (Zuidema, 2007). More recently, a number of groups have found success employing Bayesian non-parametric priors (Post and Gildea, 2009; Cohn et al., 2010), which put a downward pressure on fragment size except where the data warrant the inclusion of larger fragments. Unfortunately, proper inference under these models is intractable, and though Monte Carlo techniques can provide an approximation, the samplers can be complex, difficult to code, and slow to converge. This history suggests two approaches to state-split TSGs: (1) a Bayesian non-parametric sampling approach (incorporate state-splitting into existin"
W12-1904,petrov-etal-2012-universal,0,\N,Missing
W12-2013,E93-1006,0,0.813376,"titution Grammars (TSGs) Though CFGs and TSGs are weakly equivalent, TSGs permit nonterminals to rewrite as tree fragments of arbitrary size, whereas CFG rewrites are limited to depth-one productions. Figure 1 depicts an example TSG fragment and equivalent CFG rules; note that the entire internal structure of 1a is described within a single rewrite. Unfortunately, learning probabilistic TSGs is not straight-forward, in large part because TSG-specific resources (e.g., large scale TSG-annotated treebanks) do not exist. Approaches to this problem began by taking all fragments Fall in a treebank (Bod, 1993; Goodman, 1996), which resulted in very large grammars composed mostly of fragments very unlikely to generalize.1 A range of heuristic solutions reduced these grammar sizes to a much smaller, more compact subset of all fragments (Zollmann and Sima’an, 2005; Zuidema, 2007). More recently, more principled models have been proposed, taking the form of inference in Bayesian non-parametric models (Post and Gildea, 2009; Cohn et al., 2009). In addition to providing a formal model for TSGs, these techniques address the overfitting problem of 1 The n-gram analog would be something like storing all 30"
W12-2013,2008.amta-papers.4,0,0.0977812,"f languages, since they do not model language structure or correspondences beyond the narrow Markov context. Context-free grammars (CFGs) address many of the problems inherent in n-grams, and are therefore intuitively much better suited for grammaticality judgments. Unfortunately, CFGs used in practice are permissive (Och et al., 2004) and make unrealistic independence and structural assumptions, resulting in “leaky” grammars that overgenerate and thus serve poorly as models of language. However, approaches that make use of the CFG productions as discriminative features have performed better. Cherry and Quirk (2008) improved upon an ngram baseline in grammatical classification by adjusting CFG production weights with a latent SVM, while others have found it useful to use comparisons between scores of different parsers (Wagner et al., 2009) or the use of CFG productions in linear classification settings (Wong and Dras, 2010) in classifying sentences in different grammaticality settings. Another successful approach in grammaticality tasks has been the use of grammars with an extended domain of locality. Post (2011) demonstrated that larger syntactic patterns obtained from Tree Substitution Grammars (Joshi,"
W12-2013,P10-2042,0,0.0187858,"dea, 2009; Cohn et al., 2009). In addition to providing a formal model for TSGs, these techniques address the overfitting problem of 1 The n-gram analog would be something like storing all 30grams seen in a corpus. 117 all fragments grammars with priors that discourage large fragments unless there is enough evidence to warrant their inclusion in the grammar. The problem with such approaches, however, is that the sampling procedures used to infer them can be complex, difficult to code, and slow to converge. Although more general techniques have been proposed to better explore the search space (Cohn and Blunsom, 2010; Cohn et al., 2010; Liang et al., 2010), the complexity and non-determinism of these samplers remain, and there are no publicly available implementations. The underlying premise behind these grammar learning approaches was the need for a probabilistic grammar for parsing. Post (2011) showed that the fragments extracted from derivations obtained by parsing with probabilistic TSGs were useful as features in two coarse-grained grammaticality tasks. In such a setting, fragments are needed for classification, but it is not clear that they need to be obtained from derivations produced by parsing wi"
W12-2013,N09-1062,0,0.0776293,"part because TSG-specific resources (e.g., large scale TSG-annotated treebanks) do not exist. Approaches to this problem began by taking all fragments Fall in a treebank (Bod, 1993; Goodman, 1996), which resulted in very large grammars composed mostly of fragments very unlikely to generalize.1 A range of heuristic solutions reduced these grammar sizes to a much smaller, more compact subset of all fragments (Zollmann and Sima’an, 2005; Zuidema, 2007). More recently, more principled models have been proposed, taking the form of inference in Bayesian non-parametric models (Post and Gildea, 2009; Cohn et al., 2009). In addition to providing a formal model for TSGs, these techniques address the overfitting problem of 1 The n-gram analog would be something like storing all 30grams seen in a corpus. 117 all fragments grammars with priors that discourage large fragments unless there is enough evidence to warrant their inclusion in the grammar. The problem with such approaches, however, is that the sampling procedures used to infer them can be complex, difficult to code, and slow to converge. Although more general techniques have been proposed to better explore the search space (Cohn and Blunsom, 2010; Cohn"
W12-2013,W09-2112,0,0.0950589,"he growth of intermediate rankings Fhr,Ki . Secondly, we have two tunable parameters R and K, which can be thought of as weakly being related to the base measure and concentration parameter of (Post and Gildea, 2009; Cohn et al., 2010). Note that by thresholding at every iteration, we enforce sparsity. 4 Experiments We view grammaticality judgment as a binary classification task: is a sequence of words grammatical or not? We evaluate on two tasks of differing granularity: the first, a coarse-grain classification, follows Cherry and Quirk (2008); the other, a fine-grain analogue, is built upon Foster and Andersen (2009). 4.1 Datasets For the coarse-grained task, we use the BLLIP5 inspired dataset, as in Post (2011), which discriminates between BLLIP sentences and KneyserNey trigram generated sentences (of equal length). Grammatical and ungrammatical examples are given in 1 and 2 below, respectively: (1) The most troublesome report may be the August merchandise trade deficit due out tomorrow . (2) To and , would come Hughey Co. may be crash victims , three billion . 5 (3) The league ’s promoters hope retirees and tourists will join die-hard fans like Mr. de Castro and pack then stands to see the seniors . Bot"
W12-2013,W96-0214,0,0.717446,"ammars (TSGs) Though CFGs and TSGs are weakly equivalent, TSGs permit nonterminals to rewrite as tree fragments of arbitrary size, whereas CFG rewrites are limited to depth-one productions. Figure 1 depicts an example TSG fragment and equivalent CFG rules; note that the entire internal structure of 1a is described within a single rewrite. Unfortunately, learning probabilistic TSGs is not straight-forward, in large part because TSG-specific resources (e.g., large scale TSG-annotated treebanks) do not exist. Approaches to this problem began by taking all fragments Fall in a treebank (Bod, 1993; Goodman, 1996), which resulted in very large grammars composed mostly of fragments very unlikely to generalize.1 A range of heuristic solutions reduced these grammar sizes to a much smaller, more compact subset of all fragments (Zollmann and Sima’an, 2005; Zuidema, 2007). More recently, more principled models have been proposed, taking the form of inference in Bayesian non-parametric models (Post and Gildea, 2009; Cohn et al., 2009). In addition to providing a formal model for TSGs, these techniques address the overfitting problem of 1 The n-gram analog would be something like storing all 30grams seen in a"
W12-2013,P06-1055,0,0.0770331,"r model. We optimized the models on dev data, letting the smoothing parameter be 10m , for integral m ∈ [−4, 2]: 0.1 was optimal for all models. 6 For the fine-grained task we use a version of the BNC that has been automatically modified to be 4 ungrammatical, via insertions, deletions or substitutions of grammatically important words. As has been argued in previous work, these automatically generated errors, simulate more realistic errors (Foster and Andersen, 2009). Example 3 gives an original sentence, with an italicized substitution error: We parsed all sentences with the Berkeley parser (Petrov et al., 2006). 7 We used the Berkeley grammar/parser (Petrov et al., 2006) in accurate mode; all other options were their default values. 8 csie.ntu.edu.tw/˜cjlin/liblinear/ Task coarse fine COUNT COUNT + LEX COUNT + CFG 86.3 62.9 86.8 64.3 88.3 67.0 Method COUNT + CFG , R = 3 COUNT + CFG , R = 15 bigram CFG TSG (a) Our count-based models, with R = 15, K = 50k. Task coarse fine 3 89.2 67.9 (b) Performance of 50k and varying R. 5 89.1 67.2 10 88.6 67.2 COUNT + CFG , 15 88.3 67.0 coarse 89.1 88.2 68.4 86.3 89.1 fine 67.2 66.6 61.4 64.5 67.0 Table 2: Classification accuracy on test portions for both coarse an"
W12-2013,P09-2012,1,0.954487,"ight-forward, in large part because TSG-specific resources (e.g., large scale TSG-annotated treebanks) do not exist. Approaches to this problem began by taking all fragments Fall in a treebank (Bod, 1993; Goodman, 1996), which resulted in very large grammars composed mostly of fragments very unlikely to generalize.1 A range of heuristic solutions reduced these grammar sizes to a much smaller, more compact subset of all fragments (Zollmann and Sima’an, 2005; Zuidema, 2007). More recently, more principled models have been proposed, taking the form of inference in Bayesian non-parametric models (Post and Gildea, 2009; Cohn et al., 2009). In addition to providing a formal model for TSGs, these techniques address the overfitting problem of 1 The n-gram analog would be something like storing all 30grams seen in a corpus. 117 all fragments grammars with priors that discourage large fragments unless there is enough evidence to warrant their inclusion in the grammar. The problem with such approaches, however, is that the sampling procedures used to infer them can be complex, difficult to code, and slow to converge. Although more general techniques have been proposed to better explore the search space (Cohn and"
W12-2013,P11-2038,1,0.30613,"rraro, Matt Post and Benjamin Van Durme Department of Computer Science, and HLTCOE Johns Hopkins University {ferraro,post,vandurme}@cs.jhu.edu Abstract Prior work has shown the utility of syntactic tree fragments as features in judging the grammaticality of text. To date such fragments have been extracted from derivations of Bayesianinduced Tree Substitution Grammars (TSGs). Evaluating on discriminative coarse and fine grammaticality classification tasks, we show that a simple, deterministic, count-based approach to fragment identification performs on par with the more complicated grammars of Post (2011). This represents a significant reduction in complexity for those interested in the use of such fragments in the development of systems for the educational domain. 1 Introduction Automatically judging grammaticality is an important component in computer-assisted education, with potential applications including large-scale essay grading and helping to interactively improve the writing of both native and L2 speakers. While ngram models have been productive throughout natural language processing (NLP), they are obviously insufficient as models of languages, since they do not model language struct"
W12-2013,U10-1011,0,0.0194587,"e (Och et al., 2004) and make unrealistic independence and structural assumptions, resulting in “leaky” grammars that overgenerate and thus serve poorly as models of language. However, approaches that make use of the CFG productions as discriminative features have performed better. Cherry and Quirk (2008) improved upon an ngram baseline in grammatical classification by adjusting CFG production weights with a latent SVM, while others have found it useful to use comparisons between scores of different parsers (Wagner et al., 2009) or the use of CFG productions in linear classification settings (Wong and Dras, 2010) in classifying sentences in different grammaticality settings. Another successful approach in grammaticality tasks has been the use of grammars with an extended domain of locality. Post (2011) demonstrated that larger syntactic patterns obtained from Tree Substitution Grammars (Joshi, 1985) outperformed the Cherry and Quirk models. The intuitions underlying their approach were that larger fragments are more natural atomic units in modeling grammatical text, and that larger fragments reduce the independence assumptions of context-free generative models since there are fewer substitution points"
W12-2013,D07-1058,0,0.386078,"note that the entire internal structure of 1a is described within a single rewrite. Unfortunately, learning probabilistic TSGs is not straight-forward, in large part because TSG-specific resources (e.g., large scale TSG-annotated treebanks) do not exist. Approaches to this problem began by taking all fragments Fall in a treebank (Bod, 1993; Goodman, 1996), which resulted in very large grammars composed mostly of fragments very unlikely to generalize.1 A range of heuristic solutions reduced these grammar sizes to a much smaller, more compact subset of all fragments (Zollmann and Sima’an, 2005; Zuidema, 2007). More recently, more principled models have been proposed, taking the form of inference in Bayesian non-parametric models (Post and Gildea, 2009; Cohn et al., 2009). In addition to providing a formal model for TSGs, these techniques address the overfitting problem of 1 The n-gram analog would be something like storing all 30grams seen in a corpus. 117 all fragments grammars with priors that discourage large fragments unless there is enough evidence to warrant their inclusion in the grammar. The problem with such approaches, however, is that the sampling procedures used to infer them can be co"
W12-2013,N10-1082,0,\N,Missing
W12-2013,N04-1021,0,\N,Missing
W12-2013,P07-1010,0,\N,Missing
W12-3102,W10-1703,1,0.557844,"Missing"
W12-3102,W11-2103,1,0.709065,"- ANNOTATOR AGREEMENT P (A) 0.567 0.576 0.595 0.598 0.540 0.504 0.568 0.519 0.568 0.601 P (A) 0.660 0.566 0.733 0.732 0.792 0.566 0.719 0.634 0.671 0.722 P (E) 0.405 0.383 0.401 0.394 0.408 0.398 0.406 0.388 0.396 0.362 κ 0.272 0.312 0.323 0.336 0.222 0.176 0.272 0.214 0.284 0.375 P (E) 0.405 0.383 0.401 0.394 0.408 0.398 0.406 0.388 0.396 0.362 κ 0.428 0.296 0.554 0.557 0.648 0.279 0.526 0.401 0.455 0.564 Table 3: Inter- and intra-annotator agreement rates for the WMT12 manual evaluation. For comparison, the WMT11 rows contain the results from the European languages individual systems task (Callison-Burch et al. (2011), Table 7). Agreement rates vary widely across languages. For inter-annotator agreements, the range is 0.176 to 0.336, while intra-annotator agreement ranges from 0.279 to 0.648. We note in particular the low agreement rates among judgments in the English-Spanish task, which is reflected in the relative lack of statistical significance Table 4. The agreement rates for this year were somewhat lower than last year. 3.3 Results of the Translation Task We used the results of the manual evaluation to analyze the translation quality of the different systems that were submitted to the workshop. In ou"
W12-3102,D09-1030,1,0.149941,"Missing"
W12-3102,W12-3103,0,0.0434403,"Missing"
W12-3102,W11-2107,0,0.311039,"Missing"
W12-3102,W12-3131,0,0.0259292,"Missing"
W12-3102,W12-3133,0,0.0166865,"Missing"
W12-3102,W12-3134,1,0.0973948,"Missing"
W12-3102,W12-3135,0,0.0202653,"Missing"
W12-3102,W12-3111,0,0.0311487,"Missing"
W12-3102,W12-3136,0,0.0403021,"Missing"
W12-3102,W12-3112,0,0.176358,"Missing"
W12-3102,2011.eamt-1.32,0,0.0209858,"with only two linear equations. System “SDLLW SVM” uses a 20-feature set and an SVM epsilon regression model with radial basis function kernel with parameters C, gamma, and epsilon tuned on a development set (305 training instances). The model was trained with 10-fold cross validation and the tuning process was restarted several times using different starting points and step sizes to avoid overfitting. The final model was selected based on its performance on the development set and the number of support vectors. UU (R, S): System “UU best” uses the 17 baseline features, plus 82 features from Hardmeier (2011) (with some redundancy and some overlap with baseline features), and constituency trees over input sentences generated by the Stanford parser and dependency trees over both input and output sentences generated by the MaltParser. System “UU bltk” uses only the 17 baseline features plus constituency and dependency trees as above. The machine learning component in both cases is SVM regression (SVMlight software). For the ranking task, 29 the ranking induced by the regression output is used. The system uses polynomial kernels of degree 2 (UU best) and 3 (UU bltk) as well as two different types of"
W12-3102,W12-3137,0,0.0533752,"Missing"
W12-3102,W12-3106,0,0.0413119,"Missing"
W12-3102,W12-3145,0,0.0522389,"Missing"
W12-3102,W12-3146,0,0.0998404,"Missing"
W12-3102,W09-0415,0,0.0172168,"Missing"
W12-3102,W12-3150,1,0.188192,"Missing"
W12-3102,W12-3119,0,0.030516,"Missing"
W12-3102,W12-3151,0,0.046244,"Missing"
W12-3102,W12-3117,0,\N,Missing
W12-3102,W12-3138,0,\N,Missing
W12-3102,W12-3130,0,\N,Missing
W12-3102,W12-3148,0,\N,Missing
W12-3102,W12-3114,0,\N,Missing
W12-3102,W12-3107,0,\N,Missing
W12-3102,W12-3139,1,\N,Missing
W12-3102,W11-2108,0,\N,Missing
W12-3102,W09-0401,1,\N,Missing
W12-3102,W12-3140,0,\N,Missing
W12-3102,W12-3132,0,\N,Missing
W12-3102,W12-3109,0,\N,Missing
W12-3102,W10-1711,0,\N,Missing
W12-3102,2010.iwslt-evaluation.22,0,\N,Missing
W12-3102,W07-0718,1,\N,Missing
W12-3102,W06-3114,1,\N,Missing
W12-3102,2009.eamt-1.5,1,\N,Missing
W12-3102,W12-3144,0,\N,Missing
W12-3102,W11-2158,0,\N,Missing
W12-3102,W12-3110,1,\N,Missing
W12-3102,W08-0309,1,\N,Missing
W12-3102,W12-3105,0,\N,Missing
W12-3102,W12-3149,0,\N,Missing
W12-3102,2011.eamt-1.12,1,\N,Missing
W12-3102,W04-3250,1,\N,Missing
W12-3102,W12-3142,0,\N,Missing
W12-3102,W11-2113,0,\N,Missing
W12-3102,W12-3143,0,\N,Missing
W12-3102,W11-2145,0,\N,Missing
W12-3102,W12-3147,0,\N,Missing
W12-3102,W12-3101,0,\N,Missing
W12-3102,W12-3115,0,\N,Missing
W12-3102,W11-2101,0,\N,Missing
W12-3102,W12-3113,0,\N,Missing
W12-3102,W12-3104,0,\N,Missing
W12-3102,W12-3108,0,\N,Missing
W12-3102,W12-3118,1,\N,Missing
W12-3134,P05-1074,1,0.106708,"9h18m 25h46m 28h10m Rules 223M 328M 654M 716M Table 4: Extraction times and grammar sizes for the SAMT grammars using the Europarl and News Commentary training data for each listed language pair. 4.2 Translation Grammars Time 4h41m 5h20m 16h47m 16h22m Paraphrase Extraction Recently English-to-English text generation tasks have seen renewed interest in the NLP community. Paraphrases are a key component in largescale state-of-the-art text-to-text generation systems. We present an extended version of Thrax that implements distributed, Hadoop-based paraphrase extraction via the pivoting approach (Bannard and Callison-Burch, 2005). Our toolkit is capable of extracting syntactically informed paraphrase grammars at scale. The paraphrase grammars obtained with Thrax have been shown to achieve state-of-theart results on text-to-text generation tasks (Ganitkevitch et al., 2011). For every supported translation feature, Thrax implements a corresponding pivoted feature for paraphrases. The pivoted features are set up to be aware of the prerequisite translation features they are derived from. This allows Thrax to automatically detect the needed translation features and spawn the corresponding map-reduce passes before the pivot"
W12-3134,N10-1033,0,0.0375622,"a. With appropriate pruning settings, we are able to obtain paraphrase grammars estimated over bitexts with more than 100 million words. 5 Additional New Features • With the help of the respective original authors, the language model implementations by Heafield (2011) and Pauls and Klein (2011) have been integrated with Joshua, dropping support for the slower and more difficult to compile SRILM toolkit (Stolcke, 2002). • We modified Joshua so that it can be used as a parser to analyze pairs of sentences using a synchronous context-free grammar. We implemented the two-pass parsing algorithm of Dyer (2010). 6 Conclusion We present a new iteration of the Joshua machine translation toolkit. Our system has been extended towards efficiently supporting large-scale experiments 290 in parsing-based machine translation and text-to-text generation: Joshua 4.0 supports compactly represented large grammars with its packed grammars, as well as large language models via KenLM and BerkeleyLM.We include an implementation of PRO, allowing for stable and fast tuning of large feature sets, and extend our toolkit beyond pure translation applications by extending Thrax with a large-scale paraphrase extraction modu"
W12-3134,W06-3113,0,0.0178341,"pes, as well as an 8-bit quantizer. We chose 8 bit as a compromise between compression, value decoding speed and translaHiero (43M rules) Syntax (200M rules) Format Baseline Packed Baseline Packed Packed 8-bit Memory 13.6G 1.8G 99.5G 9.8G 5.8G Table 1: Decoding-time memory use for the packed grammar versus the standard grammar format. Even without lossy quantization the packed grammar representation yields significant savings in memory consumption. Adding 8-bit quantization for the realvalued features in the grammar reduces even large syntactic grammars to a manageable size. tion performance (Federico and Bertoldi, 2006). Our quantization approach follows Federico and Bertoldi (2006) and Heafield (2011) in partitioning the value histogram into 256 equal-sized buckets. We quantize by mapping each feature value onto the weighted average of its bucket. Joshua allows for an easily per-feature specification of type. Quantizers can be share statistics across multiple features with similar value distributions. 2.2 Experiments We assess the packed grammar representation’s memory efficiency and impact on the decoding speed on the WMT12 French-English task. Table 1 shows a comparison of the memory needed to store our W"
W12-3134,D11-1108,1,0.323188,"Missing"
W12-3134,W11-2123,0,0.182249,"Using more diverse sets of nonterminal labels – which can significantly improve translation performance – further aggravates this problem. As a consequence, the space requirements for storing the grammar in memory during decoding quickly grow impractical. In some cases grammars may become too large to fit into the memory on a single machine. As an alternative to the commonly used trie structures based on hash maps, we propose a packed trie representation for SCFGs. The approach we take is similar to work on efficiently storing large phrase tables by Zens and Ney (2007) and language models by Heafield (2011) and Pauls and Klein (2011) – both language model implementations are now integrated with Joshua. 2.1 Packed Synchronous Tries For our grammar representation, we break the SCFG up into three distinct structures. As Figure 1 indicates, we store the grammar rules’ source sides {αi }, target sides {γi }, and feature data {~ ϕi } in separate formats of their own. Each of the structures is packed into a flat array, and can thus be quickly read into memory. All terminal and nonterminal symbols in the grammar are mapped to integer symbol id’s using a globally accessible vocabulary map. We will now de"
W12-3134,D11-1125,0,0.25698,"paper we describe a set of recent extensions to the Joshua system. We present a new compact grammar representation format that leverages sparse features, quantization, and data redundancies to store grammars in a dense binary format. This allows for both near-instantaneous start-up times and decoding with extremely large grammars. In Section 2 we outline our packed grammar format and 1 joshua-decoder.org present experimental results regarding its impact on decoding speed, memory use and translation quality. Additionally, we present Joshua’s implementation of the pairwise ranking optimization (Hopkins and May, 2011) approach to translation model tuning. J-PRO, like Z-MERT, makes it easy to implement new metrics and comes with both a built-in perceptron classifier and out-of-the-box support for widely used binary classifiers such as MegaM and MaxEnt (Daum´e III and Marcu, 2006; Manning and Klein, 2003). We describe our implementation in Section 3, presenting experimental results on performance, classifier convergence, and tuning speed. Finally, we introduce the inclusion of bilingual pivoting-based paraphrase extraction into Thrax, Joshua’s grammar extractor. Thrax’s paraphrase extraction mode is simple t"
W12-3134,W09-0424,1,0.601865,"4.0, the newest version of our open-source decoder for parsing-based statistical machine translation. The main contributions in this release are the introduction of a compact grammar representation based on packed tries, and the integration of our implementation of pairwise ranking optimization, J-PRO. We further present the extension of the Thrax SCFG grammar extractor to pivot-based extraction of syntactically informed sentential paraphrases. 1 Introduction Joshua is an open-source toolkit1 for parsing-based statistical machine translation of human languages. The original version of Joshua (Li et al., 2009) was a reimplementation of the Python-based Hiero machine translation system (Chiang, 2007). It was later extended to support grammars with rich syntactic labels (Li et al., 2010a). More recent efforts introduced the Thrax module, an extensible Hadoopbased extraction toolkit for synchronous contextfree grammars (Weese et al., 2011). In this paper we describe a set of recent extensions to the Joshua system. We present a new compact grammar representation format that leverages sparse features, quantization, and data redundancies to store grammars in a dense binary format. This allows for both ne"
W12-3134,W10-1718,1,0.942532,"mar representation based on packed tries, and the integration of our implementation of pairwise ranking optimization, J-PRO. We further present the extension of the Thrax SCFG grammar extractor to pivot-based extraction of syntactically informed sentential paraphrases. 1 Introduction Joshua is an open-source toolkit1 for parsing-based statistical machine translation of human languages. The original version of Joshua (Li et al., 2009) was a reimplementation of the Python-based Hiero machine translation system (Chiang, 2007). It was later extended to support grammars with rich syntactic labels (Li et al., 2010a). More recent efforts introduced the Thrax module, an extensible Hadoopbased extraction toolkit for synchronous contextfree grammars (Weese et al., 2011). In this paper we describe a set of recent extensions to the Joshua system. We present a new compact grammar representation format that leverages sparse features, quantization, and data redundancies to store grammars in a dense binary format. This allows for both near-instantaneous start-up times and decoding with extremely large grammars. In Section 2 we outline our packed grammar format and 1 joshua-decoder.org present experimental result"
W12-3134,C10-2075,0,0.0606954,"mar representation based on packed tries, and the integration of our implementation of pairwise ranking optimization, J-PRO. We further present the extension of the Thrax SCFG grammar extractor to pivot-based extraction of syntactically informed sentential paraphrases. 1 Introduction Joshua is an open-source toolkit1 for parsing-based statistical machine translation of human languages. The original version of Joshua (Li et al., 2009) was a reimplementation of the Python-based Hiero machine translation system (Chiang, 2007). It was later extended to support grammars with rich syntactic labels (Li et al., 2010a). More recent efforts introduced the Thrax module, an extensible Hadoopbased extraction toolkit for synchronous contextfree grammars (Weese et al., 2011). In this paper we describe a set of recent extensions to the Joshua system. We present a new compact grammar representation format that leverages sparse features, quantization, and data redundancies to store grammars in a dense binary format. This allows for both near-instantaneous start-up times and decoding with extremely large grammars. In Section 2 we outline our packed grammar format and 1 joshua-decoder.org present experimental result"
W12-3134,N03-5008,0,0.0335893,"and decoding with extremely large grammars. In Section 2 we outline our packed grammar format and 1 joshua-decoder.org present experimental results regarding its impact on decoding speed, memory use and translation quality. Additionally, we present Joshua’s implementation of the pairwise ranking optimization (Hopkins and May, 2011) approach to translation model tuning. J-PRO, like Z-MERT, makes it easy to implement new metrics and comes with both a built-in perceptron classifier and out-of-the-box support for widely used binary classifiers such as MegaM and MaxEnt (Daum´e III and Marcu, 2006; Manning and Klein, 2003). We describe our implementation in Section 3, presenting experimental results on performance, classifier convergence, and tuning speed. Finally, we introduce the inclusion of bilingual pivoting-based paraphrase extraction into Thrax, Joshua’s grammar extractor. Thrax’s paraphrase extraction mode is simple to use, and yields state-ofthe-art syntactically informed sentential paraphrases (Ganitkevitch et al., 2011). The full feature set of Thrax (Weese et al., 2011) is supported for paraphrase grammars. An easily configured feature-level pruning mechanism allows to keep the paraphrase grammar si"
W12-3134,P03-1021,0,0.00352708,"d format). Even though decoding speed is slightly slower with the packed grammars (an average of 5.3 seconds per sentence versus 4.2 for the baseline), the effective translation speed is more than twice that of the baseline (1004 seconds to complete decoding the 2489 sentences, versus 2551 seconds with the standard setup). 3 J-PRO: Pairwise Ranking Optimization in Joshua Pairwise ranking optimization (PRO) proposed by (Hopkins and May, 2011) is a new method for discriminative parameter tuning in statistical machine translation. It is reported to be more stable than the popular MERT algorithm (Och, 2003) and is more scalable with regard to the number of features. PRO treats parameter tuning as an n-best list reranking problem, and the idea is similar to other pairwise ranking techniques like ranking SVM and IR SVMs (Li, 2011). The algorithm can be described thusly: Let h(c) = hw, Φ(c)i be the linear model score of a candidate translation c, in which Φ(c) is the feature vector of c and w is the parameter vector. Also let g(c) be the metric score of c (without loss of generality, we assume a higher score indicates a better translation). We aim to find a parameter vector w such that for a pair o"
W12-3134,P11-1027,0,0.0329495,"sets of nonterminal labels – which can significantly improve translation performance – further aggravates this problem. As a consequence, the space requirements for storing the grammar in memory during decoding quickly grow impractical. In some cases grammars may become too large to fit into the memory on a single machine. As an alternative to the commonly used trie structures based on hash maps, we propose a packed trie representation for SCFGs. The approach we take is similar to work on efficiently storing large phrase tables by Zens and Ney (2007) and language models by Heafield (2011) and Pauls and Klein (2011) – both language model implementations are now integrated with Joshua. 2.1 Packed Synchronous Tries For our grammar representation, we break the SCFG up into three distinct structures. As Figure 1 indicates, we store the grammar rules’ source sides {αi }, target sides {γi }, and feature data {~ ϕi } in separate formats of their own. Each of the structures is packed into a flat array, and can thus be quickly read into memory. All terminal and nonterminal symbols in the grammar are mapped to integer symbol id’s using a globally accessible vocabulary map. We will now describe the implementation d"
W12-3134,W11-2160,1,0.362052,"ension of the Thrax SCFG grammar extractor to pivot-based extraction of syntactically informed sentential paraphrases. 1 Introduction Joshua is an open-source toolkit1 for parsing-based statistical machine translation of human languages. The original version of Joshua (Li et al., 2009) was a reimplementation of the Python-based Hiero machine translation system (Chiang, 2007). It was later extended to support grammars with rich syntactic labels (Li et al., 2010a). More recent efforts introduced the Thrax module, an extensible Hadoopbased extraction toolkit for synchronous contextfree grammars (Weese et al., 2011). In this paper we describe a set of recent extensions to the Joshua system. We present a new compact grammar representation format that leverages sparse features, quantization, and data redundancies to store grammars in a dense binary format. This allows for both near-instantaneous start-up times and decoding with extremely large grammars. In Section 2 we outline our packed grammar format and 1 joshua-decoder.org present experimental results regarding its impact on decoding speed, memory use and translation quality. Additionally, we present Joshua’s implementation of the pairwise ranking opti"
W12-3134,N07-1062,0,0.0137962,"so does the resulting translation grammar. Using more diverse sets of nonterminal labels – which can significantly improve translation performance – further aggravates this problem. As a consequence, the space requirements for storing the grammar in memory during decoding quickly grow impractical. In some cases grammars may become too large to fit into the memory on a single machine. As an alternative to the commonly used trie structures based on hash maps, we propose a packed trie representation for SCFGs. The approach we take is similar to work on efficiently storing large phrase tables by Zens and Ney (2007) and language models by Heafield (2011) and Pauls and Klein (2011) – both language model implementations are now integrated with Joshua. 2.1 Packed Synchronous Tries For our grammar representation, we break the SCFG up into three distinct structures. As Figure 1 indicates, we store the grammar rules’ source sides {αi }, target sides {γi }, and feature data {~ ϕi } in separate formats of their own. Each of the structures is packed into a flat array, and can thus be quickly read into memory. All terminal and nonterminal symbols in the grammar are mapped to integer symbol id’s using a globally ac"
W12-3134,J07-2003,0,\N,Missing
W12-3152,W10-0710,0,0.068614,"t guess, an X marks a false positive, and a • denotes a false negative. Hiero’s extraction heuristics yield 4 rules for the top alignment and 16 for the bottom. reasonable translation accuracy. Closely related to our work here is that of Novotney and Callison-Burch (2010), who showed that transcriptions for training speech recognition systems could be obtained from Mechanical Turk with near baseline recognition performance and at a significantly lower cost. They also showed that redundant annotation was not worthwhile, and suggested that money was better spent obtaining more data. Separately, Ambati and Vogel (2010) probed the MTurk worker pool for workers capable of translating a number of low-resource languages, including Hindi, Telugu, and Urdu, demonstrating that such workers could be found and quantifying acceptable 408 We have described the collection of six parallel corpora containing four-way redundant translations of the source-language text. The Indian languages of these corpora are low-resource and understudied, and exhibit markedly different linguistic properties compared to English. We performed baseline experiments quantifying the translation performance of a number of systems, investigated"
W12-3152,J07-2003,0,0.227477,"present in these languages is a high degree of morphological complexity relative to English (Figure 2). Indian languages can be highly agglutinative, which means that words are formed by concatenating morphological affixes that convey information such as tense, person, number, gender, mood, and voice. Morphological complexity is a considerable hindrance at all stages of the MT pipeline, but particularly alignment, where inflectional variations mask patterns from alignment tools that treat words as atoms. 3 We use hierarchical to denote translation grammars that use only a single nonterminal (Chiang, 2007), in contrast to syntactic systems, which make use of linguistic annotations (Zollmann and Venugopal, 2006; Galley et al., 2006). 402 The source of the documents for our translation task for each of the languages in Table 1 was the set of the top-100 most-viewed documents from each language’s Wikipedia. These lists were obtained using page view statistics compiled from dammit.lt/ wikistats over a one year period. We did not apply any filtering for topic or content. Table 2 contains a manually categorized list of documents for Hindi, with some minimal annotations indicating how the documents re"
W12-3152,P11-2031,0,0.0160195,"on results? 3. What is the effect of data quality on model quality? 4.1 Setup A principal point of comparison in this paper is between Hiero grammars (Chiang, 2007) and SAMT grammars (Zollmann and Venugopal, 2006), the latter of which make use of linguistic annotations to 405 improve nonterminal reordering. These grammars were trained with the Thrax grammar extractor using its default settings, and translated using Joshua (Weese et al., 2011). We tuned with minimum errorrate training (Och, 2003) using Z-MERT (Zaidan, 2009) and present the mean BLEU score on test data over three separate runs (Clark et al., 2011). MBR reranking (Kumar and Byrne, 2004) was applied to Joshua’s 300-best (unique) output, and evaluation was conducted with case-insensitive BLEU with four references. The training data was produced by pairing a source sentence with each of its four translations. We also added the dictionaries to the training data. We built five-gram language models from the target side of the training data using interpolated KneserNey smoothing. We also experimented with a largerscale language model built from English Gigaword, but, notably, found a drop of over a point in BLEU score. This points forward to s"
W12-3152,N04-1035,0,0.0124259,"able 1: Languages. L1 is the worldwide number of native speakers according to Lewis (2009). 3 Data collection In addition to a general desire to collect suitable training corpora for low-resource languages, Indian languages demonstrate a variety of linguistic phenomena that are divergent from English and understudied. One example is head-finalness, exhibited most obviously in a subject-object-verb (SOV) pattern of sentence structure, in contrast to the general SVO ordering of English sentences. One of the motivations underlying linguistically-motivated syntactic translation systems like GHKM (Galley et al., 2004; Galley et al., 2006) or SAMT (Zollmann and Venugopal, 2006) is to describe such transformations. This difference in word order has the potential to serve as a better test bed for syntax-based MT3 compared to translating between English and European languages, most of which largely share its word order. Figure 1 contains an example of SOV reordering in Tamil. A second important phenomenon present in these languages is a high degree of morphological complexity relative to English (Figure 2). Indian languages can be highly agglutinative, which means that words are formed by concatenating morpho"
W12-3152,P06-1121,0,0.00996435,"is the worldwide number of native speakers according to Lewis (2009). 3 Data collection In addition to a general desire to collect suitable training corpora for low-resource languages, Indian languages demonstrate a variety of linguistic phenomena that are divergent from English and understudied. One example is head-finalness, exhibited most obviously in a subject-object-verb (SOV) pattern of sentence structure, in contrast to the general SVO ordering of English sentences. One of the motivations underlying linguistically-motivated syntactic translation systems like GHKM (Galley et al., 2004; Galley et al., 2006) or SAMT (Zollmann and Venugopal, 2006) is to describe such transformations. This difference in word order has the potential to serve as a better test bed for syntax-based MT3 compared to translating between English and European languages, most of which largely share its word order. Figure 1 contains an example of SOV reordering in Tamil. A second important phenomenon present in these languages is a high degree of morphological complexity relative to English (Figure 2). Indian languages can be highly agglutinative, which means that words are formed by concatenating morphological affixes that c"
W12-3152,W01-1409,0,0.0421683,"ally translated data. Therefore, we could not measure the BLEU score of the Turkers. Hindi 0 0 2 4 6 8 10 12 14 16 18 20 22 24 26 28 30 32 Figure 3: The total volume of translations (measured in English words) as a function of elapsed days. For Malayalam, we collected half a million words of translations in just under a week. week. For comparison, the Europarl corpus (Koehn, 2005) has about 50 million words of English for each of the Spanish and French parallel corpora. As has been previously reported (Zbib et al., 2012), cost is another advantage of building training data on Mechanical Turk. Germann (2001) puts the cost of professionally translated English at about $0.30 per word for translation from Tamil. Our translations were obtained for less than $0.01 per word. The rate of collection could likely be increased by raising these payments, but it is unclear whether quality would be affected by raising the base pay (although it could be improved by paying for subsequent quality control HITs, like editing). The tradeoff for low-cost translations is increased variance in translation quality when compared to the more consistently-good professional translations. Figure 4 contains some hand-picked"
W12-3152,N03-1017,0,0.0236059,"g data (plus dictionary), selected in two ways: best (result of vote), and random. There is little difference, suggesting quality control may not be terribly important. We did not collect votes for Malayalam. misspelling japenese japans japenes japenies japeneses japeneese japense count 91 40 9 3 3 1 1 Table 8: Misspellings of japanese (947) in the training portion of the Urdu-English data, along with their counts. 5.2 pairs and data conditions. Figure 6 (top) contains an example of a particularly poor alignment produced by the default alignment heuristic, the grow-diagand method described in Koehn et al. (2003). As a means of testing this, we varied the alignment combination heuristics using five alternatives described in Koehn et al. (2003) and available in the symal program distributed with Moses (Koehn et al., 2007). Experiments on Tamil produce a range of BLEU scores between 7.45 and 10.19 (each result is the average of three MERT runs). If we plot grammar size versus BLEU score, we observe a general trend that larger grammars seem to positively correlate with BLEU score. We tested this more generally across languages using the Berkeley aligner5 (Liang et al., 2006) instead of GIZA alignments, a"
W12-3152,P07-2045,1,0.0155163,"spelling japenese japans japenes japenies japeneses japeneese japense count 91 40 9 3 3 1 1 Table 8: Misspellings of japanese (947) in the training portion of the Urdu-English data, along with their counts. 5.2 pairs and data conditions. Figure 6 (top) contains an example of a particularly poor alignment produced by the default alignment heuristic, the grow-diagand method described in Koehn et al. (2003). As a means of testing this, we varied the alignment combination heuristics using five alternatives described in Koehn et al. (2003) and available in the symal program distributed with Moses (Koehn et al., 2007). Experiments on Tamil produce a range of BLEU scores between 7.45 and 10.19 (each result is the average of three MERT runs). If we plot grammar size versus BLEU score, we observe a general trend that larger grammars seem to positively correlate with BLEU score. We tested this more generally across languages using the Berkeley aligner5 (Liang et al., 2006) instead of GIZA alignments, and found a consistent increase in BLEU score for the Hiero grammars, often putting them on par with the original SAMT results (Table 9). Manual analysis suggests that the Berkeley aligner produces fewer, more rea"
W12-3152,2005.mtsummit-papers.11,0,0.0437349,"ask, and likely desire to maximize their throughput (and thus their wage). Unlike Zaidan and CallisonBurch (2011), who embed controls containing source language sentences with known professional translations, we had no professionally translated data. Therefore, we could not measure the BLEU score of the Turkers. Hindi 0 0 2 4 6 8 10 12 14 16 18 20 22 24 26 28 30 32 Figure 3: The total volume of translations (measured in English words) as a function of elapsed days. For Malayalam, we collected half a million words of translations in just under a week. week. For comparison, the Europarl corpus (Koehn, 2005) has about 50 million words of English for each of the Spanish and French parallel corpora. As has been previously reported (Zbib et al., 2012), cost is another advantage of building training data on Mechanical Turk. Germann (2001) puts the cost of professionally translated English at about $0.30 per word for translation from Tamil. Our translations were obtained for less than $0.01 per word. The rate of collection could likely be increased by raising these payments, but it is unclear whether quality would be affected by raising the base pay (although it could be improved by paying for subsequ"
W12-3152,N04-1022,0,0.0140679,"data quality on model quality? 4.1 Setup A principal point of comparison in this paper is between Hiero grammars (Chiang, 2007) and SAMT grammars (Zollmann and Venugopal, 2006), the latter of which make use of linguistic annotations to 405 improve nonterminal reordering. These grammars were trained with the Thrax grammar extractor using its default settings, and translated using Joshua (Weese et al., 2011). We tuned with minimum errorrate training (Och, 2003) using Z-MERT (Zaidan, 2009) and present the mean BLEU score on test data over three separate runs (Clark et al., 2011). MBR reranking (Kumar and Byrne, 2004) was applied to Joshua’s 300-best (unique) output, and evaluation was conducted with case-insensitive BLEU with four references. The training data was produced by pairing a source sentence with each of its four translations. We also added the dictionaries to the training data. We built five-gram language models from the target side of the training data using interpolated KneserNey smoothing. We also experimented with a largerscale language model built from English Gigaword, but, notably, found a drop of over a point in BLEU score. This points forward to some of the difficulties encountered wit"
W12-3152,N06-1014,0,0.0128778,"w-diagand method described in Koehn et al. (2003). As a means of testing this, we varied the alignment combination heuristics using five alternatives described in Koehn et al. (2003) and available in the symal program distributed with Moses (Koehn et al., 2007). Experiments on Tamil produce a range of BLEU scores between 7.45 and 10.19 (each result is the average of three MERT runs). If we plot grammar size versus BLEU score, we observe a general trend that larger grammars seem to positively correlate with BLEU score. We tested this more generally across languages using the Berkeley aligner5 (Liang et al., 2006) instead of GIZA alignments, and found a consistent increase in BLEU score for the Hiero grammars, often putting them on par with the original SAMT results (Table 9). Manual analysis suggests that the Berkeley aligner produces fewer, more reasonable-looking alignments than the Moses heuristics (Figure 6). This suggest a fruitful approaches in revisiting assumptions underlying alignment heuristics. 6 Related Work Alignments Inconsistent orthography fragments the training data, exacerbating problems already present due to morpohological richness. One place this is manifested is during alignment,"
W12-3152,N10-1024,1,0.728684,"(to increase quality) or translate more foreign sentences (to increase coverage). To test this, we constructed two smaller datasets, each making use of only one of the four translations of each source sentence: • Selected randomly • Selected by choosing the translation that received a plurality of the votes (§3.3), breaking ties randomly (best) We again included the dictionaries in the training data (where available). Table 7 contains results on the same test sets as before. These results do not clearly indicate that quality control through redundant translations are worth the extra expense. Novotney and Callison-Burch (2010) had a similar finding for crowdsourced transcriptions. 5 Further Analysis The previous section has shown that reasonable BLEU scores can be obtained from baseline translation systems built from these corpora. While translation quality is an issue (for example, very lit406 eral translations, etc), the previous section’s voted dataset experiments suggest this is not one of the most important issues to address. In this section, we undertake a manual analysis of the collected datasets to inform future work. There are a number of issues that arise due to non-Roman scripts, high-variance translatio"
W12-3152,P03-1021,0,0.0249439,". How well can we translate the test sets? 2. Do linguistically motivated translation models improve translation results? 3. What is the effect of data quality on model quality? 4.1 Setup A principal point of comparison in this paper is between Hiero grammars (Chiang, 2007) and SAMT grammars (Zollmann and Venugopal, 2006), the latter of which make use of linguistic annotations to 405 improve nonterminal reordering. These grammars were trained with the Thrax grammar extractor using its default settings, and translated using Joshua (Weese et al., 2011). We tuned with minimum errorrate training (Och, 2003) using Z-MERT (Zaidan, 2009) and present the mean BLEU score on test data over three separate runs (Clark et al., 2011). MBR reranking (Kumar and Byrne, 2004) was applied to Joshua’s 300-best (unique) output, and evaluation was conducted with case-insensitive BLEU with four references. The training data was produced by pairing a source sentence with each of its four translations. We also added the dictionaries to the training data. We built five-gram language models from the target side of the training data using interpolated KneserNey smoothing. We also experimented with a largerscale languag"
W12-3152,W11-2160,1,0.493032,"ance. The experiments aim to address the following questions: 1. How well can we translate the test sets? 2. Do linguistically motivated translation models improve translation results? 3. What is the effect of data quality on model quality? 4.1 Setup A principal point of comparison in this paper is between Hiero grammars (Chiang, 2007) and SAMT grammars (Zollmann and Venugopal, 2006), the latter of which make use of linguistic annotations to 405 improve nonterminal reordering. These grammars were trained with the Thrax grammar extractor using its default settings, and translated using Joshua (Weese et al., 2011). We tuned with minimum errorrate training (Och, 2003) using Z-MERT (Zaidan, 2009) and present the mean BLEU score on test data over three separate runs (Clark et al., 2011). MBR reranking (Kumar and Byrne, 2004) was applied to Joshua’s 300-best (unique) output, and evaluation was conducted with case-insensitive BLEU with four references. The training data was produced by pairing a source sentence with each of its four translations. We also added the dictionaries to the training data. We built five-gram language models from the target side of the training data using interpolated KneserNey smoo"
W12-3152,P11-1122,1,0.421888,"e m s s fu ov ll i fo e r ✓ ✓ ✓ X ✓ ✓ ✓ . pair Bengali Hindi GIZA++ Berkeley 15m 34m 12m 19m 28m 38m 27m 60m 27m 30m 46m 58m Malayalam Tamil Telugu Urdu BLEU 13.54 16.47 12.70 10.10 13.36 20.41 gain +0.82 +0.94 -1.02 +0.29 +0.90 +0.88 Table 9: Hiero translation results using Berkeley alignments instead of GIZA++ heuristics. The gain columns denotes improvements relative to the Hiero systems in Table 5. In many cases (bold gains), the BLEU scores are at or above even the SAMT models from that table. wages and collection rates. The techniques described here are similar to those அ&quot;# described in Zaidan and Callison-Burch (2011), who $மா&apos;( showed that crowdsourcing with appropriate quality controls could be used to produce professional-level )த+ translations for Urdu-English translation. This paெவ./0 per extends that work by applying their techniques to a larger set of Indian languages and scaling it to பட3 training-data-set sizes. ஆைச ✓ X X ✓ 7 Summary . Figure 6: A bad Tamil alignment produced with the grow-diag-and alignment combination heuristic (top); the Berkeley aligner is better (bottom). A ✓ is a correct guess, an X marks a false positive, and a • denotes a false negative. Hiero’s extraction heuristics yield"
W12-3152,N12-1006,1,0.732385,"taining source language sentences with known professional translations, we had no professionally translated data. Therefore, we could not measure the BLEU score of the Turkers. Hindi 0 0 2 4 6 8 10 12 14 16 18 20 22 24 26 28 30 32 Figure 3: The total volume of translations (measured in English words) as a function of elapsed days. For Malayalam, we collected half a million words of translations in just under a week. week. For comparison, the Europarl corpus (Koehn, 2005) has about 50 million words of English for each of the Spanish and French parallel corpora. As has been previously reported (Zbib et al., 2012), cost is another advantage of building training data on Mechanical Turk. Germann (2001) puts the cost of professionally translated English at about $0.30 per word for translation from Tamil. Our translations were obtained for less than $0.01 per word. The rate of collection could likely be increased by raising these payments, but it is unclear whether quality would be affected by raising the base pay (although it could be improved by paying for subsequent quality control HITs, like editing). The tradeoff for low-cost translations is increased variance in translation quality when compared to t"
W12-3152,W06-3119,0,0.0610624,"tive speakers according to Lewis (2009). 3 Data collection In addition to a general desire to collect suitable training corpora for low-resource languages, Indian languages demonstrate a variety of linguistic phenomena that are divergent from English and understudied. One example is head-finalness, exhibited most obviously in a subject-object-verb (SOV) pattern of sentence structure, in contrast to the general SVO ordering of English sentences. One of the motivations underlying linguistically-motivated syntactic translation systems like GHKM (Galley et al., 2004; Galley et al., 2006) or SAMT (Zollmann and Venugopal, 2006) is to describe such transformations. This difference in word order has the potential to serve as a better test bed for syntax-based MT3 compared to translating between English and European languages, most of which largely share its word order. Figure 1 contains an example of SOV reordering in Tamil. A second important phenomenon present in these languages is a high degree of morphological complexity relative to English (Figure 2). Indian languages can be highly agglutinative, which means that words are formed by concatenating morphological affixes that convey information such as tense, person"
W13-2201,W13-2205,0,0.0583032,"Missing"
W13-2201,S13-1034,0,0.0277746,"Missing"
W13-2201,C12-1008,0,0.0091484,"ignments is used to extract quantitative (amount and distribution of the alignments) and qualitative (importance of the aligned terms) features under the assumption that alignment information can help tasks where sentencelevel semantic relations need to be identified (Souza et al., 2013). Three similar EnglishSpanish systems are built and used to provide pseudo-references (Soricut et al., 2012) and back-translations, from which automatic MT evaluation metrics could be computed and used as features. DFKI (T1.2, T1.3): DFKI’s submission for Task 1.2 was based on decomposing rankings into pairs (Avramidis, 2012), where the best system for each pair was predicted with Logistic Regression (LogReg). For GermanEnglish, LogReg was trained with Stepwise Feature Selection (Hosmer, 1989) on two feature sets: Feature Set 24 includes basic counts augmented with PCFG parsing features (number of VPs, alternative parses, parse probability) on both source and target sentences (Avramidis et al., 2011), and pseudo-reference METEOR score; the most successful set, Feature Set 33 combines those 24 features with the 17 baseline features. For English-Spanish, LogReg was used with L2 Regularisation (Lin et al., 2007) and"
W13-2201,W13-2206,0,0.0913052,"ool of Data Analysis (Borisov et al., 2013) Carnegie Mellon University (Ammar et al., 2013) CMU - TREE - TO - TREE CU - BOJAR , Charles University in Prague (Bojar et al., 2013) CU - DEPFIX , CU - TAMCHYNA CU - KAREL , CU - ZEMAN CU - PHRASEFIX , Charles University in Prague (B´ılek and Zeman, 2013) Charles University in Prague (Galuˇscˇ a´ kov´a et al., 2013) CU - TECTOMT DCU DCU - FDA DCU - OKITA DESRT ITS - LATL JHU KIT LIA LIMSI MES -* OMNIFLUENT PROMT QCRI - MES QUAERO RWTH SHEF STANFORD TALP - UPC TUBITAK UCAM UEDIN , Dublin City University (Rubino et al., 2013a) Dublin City University (Bicici, 2013a) Dublin City University (Okita et al., 2013) Universit`a di Pisa (Miceli Barone and Attardi, 2013) University of Geneva Johns Hopkins University (Post et al., 2013) Karlsruhe Institute of Technology (Cho et al., 2013) Universit´e d’Avignon (Huet et al., 2013) LIMSI (Allauzen et al., 2013) Munich / Edinburgh / Stuttgart (Durrani et al., 2013a; Weller et al., 2013) SAIC (Matusov and Leusch, 2013) PROMT Automated Translations Solutions Qatar / Munich / Edinburgh / Stuttgart (Sajjad et al., 2013) QUAERO (Peitz et al., 2013a) RWTH Aachen (Peitz et al., 2013b) University of Sheffield Stanford Univ"
W13-2201,W13-2240,0,0.0186069,"Missing"
W13-2201,W13-2242,0,0.220429,"ool of Data Analysis (Borisov et al., 2013) Carnegie Mellon University (Ammar et al., 2013) CMU - TREE - TO - TREE CU - BOJAR , Charles University in Prague (Bojar et al., 2013) CU - DEPFIX , CU - TAMCHYNA CU - KAREL , CU - ZEMAN CU - PHRASEFIX , Charles University in Prague (B´ılek and Zeman, 2013) Charles University in Prague (Galuˇscˇ a´ kov´a et al., 2013) CU - TECTOMT DCU DCU - FDA DCU - OKITA DESRT ITS - LATL JHU KIT LIA LIMSI MES -* OMNIFLUENT PROMT QCRI - MES QUAERO RWTH SHEF STANFORD TALP - UPC TUBITAK UCAM UEDIN , Dublin City University (Rubino et al., 2013a) Dublin City University (Bicici, 2013a) Dublin City University (Okita et al., 2013) Universit`a di Pisa (Miceli Barone and Attardi, 2013) University of Geneva Johns Hopkins University (Post et al., 2013) Karlsruhe Institute of Technology (Cho et al., 2013) Universit´e d’Avignon (Huet et al., 2013) LIMSI (Allauzen et al., 2013) Munich / Edinburgh / Stuttgart (Durrani et al., 2013a; Weller et al., 2013) SAIC (Matusov and Leusch, 2013) PROMT Automated Translations Solutions Qatar / Munich / Edinburgh / Stuttgart (Sajjad et al., 2013) QUAERO (Peitz et al., 2013a) RWTH Aachen (Peitz et al., 2013b) University of Sheffield Stanford Univ"
W13-2201,W13-2207,0,0.0357036,"Missing"
W13-2201,W11-2104,0,0.00810596,"Missing"
W13-2201,P10-2016,1,0.826191,"Missing"
W13-2201,W13-2208,1,0.743011,"Missing"
W13-2201,P11-1022,0,0.112625,"the intended application. In Task 2 we tested binary word-level classification in a post-editing setting. If such annotation is presented through a user interface we imagine that words marked as incorrect would be hidden from the editor, highlighted as possibly wrong or that a list of alternatives would we generated. With respect to the poor improvements over trivial baselines, we consider that the results for word-level prediction could be mostly connected to limitations of the datasets provided, which are very small for word-level prediction, as compared to successful previous work such as (Bach et al., 2011). Despite the limited amount of training data, several systems were able to predict dubious words (binary variant of the task), showing that this can be a promising task. Extending the granularity even further by predicting the actual editing action necessary for a word yielded less positive results than the binary setting. We cannot directly compare sentence- and word-level results. However, since sentence-level predictions can benefit from more information available and therefore more signal on which the prediction is based, the natural conclusion is that, if there is a choice in the predict"
W13-2201,W13-2209,0,0.034542,"Missing"
W13-2201,W13-2241,1,0.0862471,"Missing"
W13-2201,W07-0718,1,0.697635,"to five alternative translations for a given source sentence, prediction of post-editing time for a sentence, and prediction of word-level scores for a given translation (correct/incorrect and types of edits). The datasets included English-Spanish and GermanEnglish news translations produced by a number of machine translation systems. This marks the second year we have conducted this task. Introduction We present the results of the shared tasks of the Workshop on Statistical Machine Translation (WMT) held at ACL 2013. This workshop builds on seven previous WMT workshops (Koehn and Monz, 2006; Callison-Burch et al., 2007, 2008, 2009, 2010, 2011, 2012). This year we conducted three official tasks: a translation task, a human evaluation of translation results, and a quality estimation task.1 In the translation task (§2), participants were asked to translate a shared test set, optionally restricting themselves to the provided training data. We held ten translation tasks this year, between English and each of Czech, French, German, Spanish, and Russian. The Russian translation tasks were new this year, and were also the most popular. The system outputs for each task were evaluated both automatically and manually."
W13-2201,W11-2103,1,0.5768,"Missing"
W13-2201,W13-2243,1,0.754887,"Missing"
W13-2201,W13-2213,0,0.0301468,"Missing"
W13-2201,P05-1022,0,0.0401846,"parses of the source and target sentence, the positions of the phrases with the lowest and highest probability and future cost estimate in the translation, the counts of phrases in the decoding graph whose probability or whether the future cost estimate is higher/lower than their standard deviation, counts of verbs and determiners, etc. The second submission (pls8) was trained with Partial Least Squares regression (Stone and Brooks, 1990) including more glass-box features. The following NLP tools were used in feature extraction: the Brown English Wall-StreetJournal-trained statistical parser (Charniak and Johnson, 2005), a Lexical Functional Grammar parser (XLE), together with a hand-crafted Lexical Functional Grammar, the English ParGram grammar (Kaplan et al., 2004), and the TreeTagger part-of-speech tagger (Schmidt, 1994) with off-the-shelf publicly available pre-trained tagging models for English and Spanish. For pseudoreference features, the Bing, Moses and Systran translation systems were used. The Mallet toolkit (McCallum, 2002) was used to build the topic models and features based on a grammar checker were extracted with LanguageTool.16 FBK-Uedin (T1.1, T1.3): The submissions explored features built"
W13-2201,W13-2210,0,0.0366902,"Missing"
W13-2201,W13-2212,1,0.1814,"Missing"
W13-2201,W13-2214,0,0.0302242,"Missing"
W13-2201,W13-2217,0,0.0313645,"Missing"
W13-2201,W13-2215,0,0.0174795,"Missing"
W13-2201,W13-2253,0,0.055006,"Missing"
W13-2201,W13-2246,0,0.0627442,"Missing"
W13-2201,N06-1058,0,0.0556187,"of the official test set size, and provide 4311 unique references. On average, one sentence in our set has 2.94±2.17 unique reference translations. Table 10 provides a histogram. It is well known that automatic MT evaluation methods perform better with more references, because a single one may not confirm a correct part of MT output. This issue is more severe for morphologically rich languages like Czech where about 1/3 of MT output was correct but not confirmed by the reference (Bojar et al., 2010). Advanced evaluation methods apply paraphrasing to smooth out some of the lexical divergence (Kauchak and Barzilay, 2006; Snover et al., 2009; Denkowski and Lavie, 2010). Simpler techniques such as lemmatizing are effective for morphologically rich languages (Tantug et al., 2008; Kos and Bojar, 2009) but they will lose resolution once the systems start performing generally well. WMTs have taken the stance that a big enough test set with just a single reference should compensate for the lack of other references. We use our post-edited reference translations to check this assumption for BLEU and NIST as implemented in mteval-13a (international tokenization switched on, which is not the default setting). We run ma"
W13-2201,W13-2248,0,0.0488662,"Missing"
W13-2201,2012.iwslt-papers.5,1,0.674636,"ts are somewhat artificially more diverse; in narrow domains, source sentences can repeat and even appear verbatim in the training data, and in natural test sets with multiple references, short sentences can receive several identical translations. For each probe, we measure the Spearman’s rank correlation coefficient ρ of the ranks proposed by BLEU or NIST and the manual ranks. We use the same implementation as applied in the WMT13 Shared Metrics Task (Mach´acˇ ek and Bojar, 2013). Note that the WMT13 metrics task still uses the WMT12 evaluation method ignoring ties, not the expected wins. As Koehn (2012) shows, the two methods do not differ much. Overall, the correlation is strongly impacted by Figure 5: Correlation of BLEU and WMT13 manual ranks for English→Czech translation Figure 6: Correlation of NIST and WMT13 manual ranks for English→Czech translation the particular choice of test sentences and reference translations. By picking sentences randomly, similarly or equally sized test sets can reach different correlations. Indeed, e.g. for a test set of about 1500 distinct sentences selected from the 3000-sentence official test set (1 reference translation), we obtain correlations for BLEU b"
W13-2201,W13-2202,1,0.761984,"Missing"
W13-2201,W06-3114,1,0.635019,"ntence, ranking of up to five alternative translations for a given source sentence, prediction of post-editing time for a sentence, and prediction of word-level scores for a given translation (correct/incorrect and types of edits). The datasets included English-Spanish and GermanEnglish news translations produced by a number of machine translation systems. This marks the second year we have conducted this task. Introduction We present the results of the shared tasks of the Workshop on Statistical Machine Translation (WMT) held at ACL 2013. This workshop builds on seven previous WMT workshops (Koehn and Monz, 2006; Callison-Burch et al., 2007, 2008, 2009, 2010, 2011, 2012). This year we conducted three official tasks: a translation task, a human evaluation of translation results, and a quality estimation task.1 In the translation task (§2), participants were asked to translate a shared test set, optionally restricting themselves to the provided training data. We held ten translation tasks this year, between English and each of Czech, French, German, Spanish, and Russian. The Russian translation tasks were new this year, and were also the most popular. The system outputs for each task were evaluated bot"
W13-2201,W13-2219,0,0.0360343,"Missing"
W13-2201,W13-2220,0,0.017834,"Missing"
W13-2201,W13-2255,0,0.0136612,"Missing"
W13-2201,W12-3113,0,0.012942,"Missing"
W13-2201,W13-2247,0,0.0208658,"Missing"
W13-2201,W13-2221,1,0.762444,"Missing"
W13-2201,W13-2228,0,0.0270572,"Missing"
W13-2201,W13-2224,0,0.0611782,"Missing"
W13-2201,2013.mtsummit-papers.21,1,0.577491,"ve learning to reduce the training set size (and therefore the annotation effort). The initial set features contains all black box and glass box features available within the Q U E ST framework (Specia et al., 2013) for the dataset at hand (160 in total for Task 1.1, and 80 for Task 1.3). The query selection strategy for active learning is based on the informativeness of the instances using Information Density, a measure that leverages between the variance among instances and how dense the region (in the feature space) where the instance is located is. To perform feature selection, following (Shah et al., 2013) features are ranked by the Gaussian Process 23 algorithm according to their learned length scales, which can be interpreted as the relevance of such feature for the model. This information was used for feature selection by discarding the lowest ranked (least useful) ones. based on empirical results found in (Shah et al., 2013), the top 25 features for both models were selected and used to retrain the same regression algorithm. Semantic Roles could bring marginally better accuracy. TCD-CNGL (T1.1) and TCD-DCU-CNGL (T1.3): The system is based on features which are commonly used for style classi"
W13-2201,2012.amta-papers.13,0,0.0208135,"Missing"
W13-2201,W13-2250,0,0.0320711,"Missing"
W13-2201,W13-2225,0,0.0376682,"Missing"
W13-2201,P13-1135,1,0.207781,"Missing"
W13-2201,W13-2226,1,0.759686,"Missing"
W13-2201,2006.amta-papers.25,0,0.295541,"ation Based on the data of Task 1.3, we define Task 2, a word-level annotation task for which participants are asked to produce a label for each token that indicates whether the word should be changed by a post-editor or kept in the final translation. We consider the following two sets of labels for prediction: Sentence-level Quality Estimation Task 1.1 Predicting Post-editing Distance This task is similar to the quality estimation task in WMT12, but with one important difference in the scoring variant: instead of using the post-editing effort scores in the [1, 2, 3, 4, 5] range, we use HTER (Snover et al., 2006) as quality score. This score is to be interpreted as the minimum edit distance between the machine translation and its manually post-edited version, and its range is [0, 1] (0 when no edit needs to be made, and 1 when all words need to be edited). Two variants of the results could be submitted in the shared task: • Binary classification: a keep/change label, the latter meaning that the token should be corrected in the post-editing process. • Multi-class classification: a label specifying the edit action that should be performed on the token (keep as is, delete, or substitute). 6.3 Datasets Ta"
W13-2201,W13-2227,0,0.0389834,"Missing"
W13-2201,W09-0441,0,0.0271208,"ze, and provide 4311 unique references. On average, one sentence in our set has 2.94±2.17 unique reference translations. Table 10 provides a histogram. It is well known that automatic MT evaluation methods perform better with more references, because a single one may not confirm a correct part of MT output. This issue is more severe for morphologically rich languages like Czech where about 1/3 of MT output was correct but not confirmed by the reference (Bojar et al., 2010). Advanced evaluation methods apply paraphrasing to smooth out some of the lexical divergence (Kauchak and Barzilay, 2006; Snover et al., 2009; Denkowski and Lavie, 2010). Simpler techniques such as lemmatizing are effective for morphologically rich languages (Tantug et al., 2008; Kos and Bojar, 2009) but they will lose resolution once the systems start performing generally well. WMTs have taken the stance that a big enough test set with just a single reference should compensate for the lack of other references. We use our post-edited reference translations to check this assumption for BLEU and NIST as implemented in mteval-13a (international tokenization switched on, which is not the default setting). We run many probes, randomly p"
W13-2201,W13-2230,0,0.0166145,"Missing"
W13-2201,W12-3118,1,0.609118,"1.1, T1.3): The submissions explored features built on MT engine resources including automatic word alignment, n-best candidate translation lists, back-translations and word posterior probabilities. Information about word alignments is used to extract quantitative (amount and distribution of the alignments) and qualitative (importance of the aligned terms) features under the assumption that alignment information can help tasks where sentencelevel semantic relations need to be identified (Souza et al., 2013). Three similar EnglishSpanish systems are built and used to provide pseudo-references (Soricut et al., 2012) and back-translations, from which automatic MT evaluation metrics could be computed and used as features. DFKI (T1.2, T1.3): DFKI’s submission for Task 1.2 was based on decomposing rankings into pairs (Avramidis, 2012), where the best system for each pair was predicted with Logistic Regression (LogReg). For GermanEnglish, LogReg was trained with Stepwise Feature Selection (Hosmer, 1989) on two feature sets: Feature Set 24 includes basic counts augmented with PCFG parsing features (number of VPs, alternative parses, parse probability) on both source and target sentences (Avramidis et al., 2011"
W13-2201,P13-2135,0,0.148019,"the difference between selecting the best or the worse translation. 19 ID CMU CNGL DCU DCU-SYMC DFKI FBK-UEdin LIG LIMSI LORIA SHEF TCD-CNGL TCD-DCU-CNGL UMAC UPC Participating team Carnegie Mellon University, USA (Hildebrand and Vogel, 2013) Centre for Next Generation Localization, Ireland (Bicici, 2013b) Dublin City University, Ireland (Almaghout and Specia, 2013) Dublin City University & Symantec, Ireland (Rubino et al., 2013b) German Research Centre for Artificial Intelligence, Germany (Avramidis and Popovic, 2013) Fondazione Bruno Kessler, Italy & University of Edinburgh, UK (Camargo de Souza et al., 2013) Laboratoire d’Informatique Grenoble, France (Luong et al., 2013) Laboratoire d’Informatique pour la M´ecanique et les Sciences de l’Ing´enieur, France (Singh et al., 2013) Lorraine Laboratory of Research in Computer Science and its Applications, France (Langlois and Smaili, 2013) University of Sheffield, UK (Beck et al., 2013) Trinity College Dublin & CNGL, Ireland (Moreau and Rubino, 2013) Trinity College Dublin, Dublin City University & CNGL, Ireland (Moreau and Rubino, 2013) University of Macau, China (Han et al., 2013) Universitat Politecnica de Catalunya, Spain (Formiga et al., 2013b) Ta"
W13-2201,2011.eamt-1.12,1,0.741789,"ar (CCG) features: CCG supertag language model perplexity and log probability, the number of maximal CCG constituents in the translation output which are the highestprobability minimum number of CCG constituents that span the translation output, the percentage of CCG argument mismatches between each subsequent CCG supertags, the percentage of CCG argument mismatches between each subsequent CCG maximal categories and the minimum number of phrases detected in the translation output. A second submission uses the aforementioned CCG features combined with 80 features from Q U E ST as described in (Specia, 2011). For the CCG features, the C&C parser was used to parse the translation output. Moses was used to build the phrase table from the SMT training corpus with maximum phrase length set to 7. The language model of supertags was built using the SRILM toolkit. As learning algorithm, Logistic Regression as provided by the SCIKIT- LEARN toolkit was used. The training data was prepared by converting each ranking of translation outputs to a set of pairwise comparisons according to the approach proposed by Avramidis et al. (2011). The rankings were generated back from pairwise comparisons predicted by th"
W13-2201,P13-4014,1,0.118757,"Missing"
W13-2201,W13-2229,0,0.0371208,"Missing"
W13-2201,tantug-etal-2008-bleu,0,0.0131442,"a histogram. It is well known that automatic MT evaluation methods perform better with more references, because a single one may not confirm a correct part of MT output. This issue is more severe for morphologically rich languages like Czech where about 1/3 of MT output was correct but not confirmed by the reference (Bojar et al., 2010). Advanced evaluation methods apply paraphrasing to smooth out some of the lexical divergence (Kauchak and Barzilay, 2006; Snover et al., 2009; Denkowski and Lavie, 2010). Simpler techniques such as lemmatizing are effective for morphologically rich languages (Tantug et al., 2008; Kos and Bojar, 2009) but they will lose resolution once the systems start performing generally well. WMTs have taken the stance that a big enough test set with just a single reference should compensate for the lack of other references. We use our post-edited reference translations to check this assumption for BLEU and NIST as implemented in mteval-13a (international tokenization switched on, which is not the default setting). We run many probes, randomly picking the test set size (number of distinct sentences) and the number of distinct references per sentence. Note that such test sets are s"
W13-2201,W10-1751,0,\N,Missing
W13-2201,de-marneffe-etal-2006-generating,0,\N,Missing
W13-2201,W13-2249,0,\N,Missing
W13-2201,N04-1013,0,\N,Missing
W13-2201,W02-1001,0,\N,Missing
W13-2201,W12-3102,1,\N,Missing
W13-2201,P12-3024,0,\N,Missing
W13-2201,W09-0401,1,\N,Missing
W13-2201,W13-2222,0,\N,Missing
W13-2201,W10-1711,0,\N,Missing
W13-2201,2010.iwslt-evaluation.22,0,\N,Missing
W13-2201,W10-1703,1,\N,Missing
W13-2201,W08-0309,1,\N,Missing
W13-2201,W13-2218,0,\N,Missing
W13-2201,P13-1004,1,\N,Missing
W13-2201,2013.mtsummit-papers.9,0,\N,Missing
W13-2201,W13-2216,1,\N,Missing
W13-2201,W13-2244,0,\N,Missing
W13-2226,D11-1125,0,0.266708,"rgument. 3 3.1 What’s New in Joshua 5.0 Sparse features The Joshua decoder is an implementation of the CKY+ algorithm (Chappelier et al., 1998), which generalizes CKY by removing the requirement Until a few years ago, machine translation systems were for the most part limited in the number of features they could employ, since the line-based optimization method, MERT (Och, 2003), was not able to efficiently search over more than tens of feature weights. The introduction of discriminative tuning methods for machine translation (Liang et al., 2006a; Tillmann and Zhang, 2006; Chiang et al., 2008; Hopkins and May, 2011) has made it possible to tune large numbers of features in statistical machine translation systems, and open2 nlp.stanford.edu/˜mgalley/software/ stanford-ghkm-latest.tar.gz 3 Roughly, the number of consecutive nonterminals in a rule (Hopkins and Langmead, 2010). 2.3 Decoding 207 source implementations such as Cherry and Foster (2012) have made it easy. Joshua 5.0 has moved to a sparse feature representation internally. First, to clarify terminology, a feature as implemented in the decoder is actually a template that can introduce any number of actual features (in the standard machine learning"
W13-2226,D11-1033,0,0.0297921,"uned our systems with kbMIRA. For truecasing, we used a monolingual translation system built on the training data, and finally detokenized with simple heuristics. Other features Joshua 5.0 also includes many features designed to increase its usability. These include: • A TCP/IP server architecture, designed to handle multiple sets of translation requests while ensuring fairness in thread assignment both across and within these connections. 5 • Intelligent selection of translation and language model training data using crossentropy difference to rank training candidates (Moore and Lewis, 2010; Axelrod et al., 2011) (described in detail in Orland (2013)). The 5.0 release of Joshua is the result of a significant year-long research, engineering, and usability effort that we hope will be of service to the research community. User-friendly packages of Joshua are available from joshua-decoder. org, while developers are encouraged to participate via github.com/joshua-decoder/ joshua. Mailing lists, linked from the main Joshua page, are available for both. • A bundler for easy packaging of trained models with all of its dependencies. • A year’s worth of improvements to the Joshua pipeline, including many new fe"
W13-2226,2005.mtsummit-papers.11,0,0.0472789,"Missing"
W13-2226,W09-0424,1,0.885997,"Missing"
W13-2226,N12-1047,0,0.254536,"search space. Other decoder options are too numerous to mention here, but are documented online. 2.2 2.4 Data preparation, alignment, and model building Tuning and testing The pipeline allows the specification (and optional linear interpolation) of an arbitrary number of language models. In addition, it builds an interpolated Kneser-Ney language model on the target side of the training data using KenLM (Heafield, 2011; Heafield et al., 2013), BerkeleyLM (Pauls and Klein, 2011) or SRILM (Stolcke, 2002). Joshua ships with MERT (Och, 2003) and PRO implementations. Tuning with k-best batch MIRA (Cherry and Foster, 2012) is also supported via callouts to Moses. Data preparation involves data normalization (e.g., collapsing certain punctuation symbols) and tokenization (with the Penn treebank or user-specified tokenizer). Alignment with GIZA++ (Och and Ney, 2000) and the Berkeley aligner (Liang et al., 2006b) are supported. Joshua’s builtin grammar extractor, Thrax, is a Hadoop-based extraction implementation that scales easily to large datasets (Ganitkevitch et al., 2013). It supports extraction of both Hiero (Chiang, 2005) and SAMT grammars (Zollmann and Venugopal, 2006) with extraction heuristics easily spe"
W13-2226,W10-1718,1,0.902749,"Missing"
W13-2226,D08-1024,0,0.0148575,"and supplied as an argument. 3 3.1 What’s New in Joshua 5.0 Sparse features The Joshua decoder is an implementation of the CKY+ algorithm (Chappelier et al., 1998), which generalizes CKY by removing the requirement Until a few years ago, machine translation systems were for the most part limited in the number of features they could employ, since the line-based optimization method, MERT (Och, 2003), was not able to efficiently search over more than tens of feature weights. The introduction of discriminative tuning methods for machine translation (Liang et al., 2006a; Tillmann and Zhang, 2006; Chiang et al., 2008; Hopkins and May, 2011) has made it possible to tune large numbers of features in statistical machine translation systems, and open2 nlp.stanford.edu/˜mgalley/software/ stanford-ghkm-latest.tar.gz 3 Roughly, the number of consecutive nonterminals in a rule (Hopkins and Langmead, 2010). 2.3 Decoding 207 source implementations such as Cherry and Foster (2012) have made it easy. Joshua 5.0 has moved to a sparse feature representation internally. First, to clarify terminology, a feature as implemented in the decoder is actually a template that can introduce any number of actual features (in the s"
W13-2226,P05-1033,0,0.0585045,"th MERT (Och, 2003) and PRO implementations. Tuning with k-best batch MIRA (Cherry and Foster, 2012) is also supported via callouts to Moses. Data preparation involves data normalization (e.g., collapsing certain punctuation symbols) and tokenization (with the Penn treebank or user-specified tokenizer). Alignment with GIZA++ (Och and Ney, 2000) and the Berkeley aligner (Liang et al., 2006b) are supported. Joshua’s builtin grammar extractor, Thrax, is a Hadoop-based extraction implementation that scales easily to large datasets (Ganitkevitch et al., 2013). It supports extraction of both Hiero (Chiang, 2005) and SAMT grammars (Zollmann and Venugopal, 2006) with extraction heuristics easily specified via a flexible configuration file. The pipeline also supports GHKM grammar extraction (Galley et al., 2006) using the extractors available from Michel Galley2 or Moses. SAMT and GHKM grammar extraction require a parse tree, which are produced using the Berkeley parser (Petrov et al., 2006), or can be done outside the pipeline and supplied as an argument. 3 3.1 What’s New in Joshua 5.0 Sparse features The Joshua decoder is an implementation of the CKY+ algorithm (Chappelier et al., 1998), which general"
W13-2226,P06-1096,0,0.127287,"Missing"
W13-2226,J07-2003,0,0.0799434,"binarization schemes (Zhang et al., 2006; DeNero et al., 2009). CKY+ maintains cubic-time parsing complexity (in the sentence length) with Earley-style implicit binarization of rules. Joshua permits arbitrary SCFGs, imposing no limitation on the rank or form of grammar rules. Parsing complexity is still exponential in the scope of the grammar,3 so grammar filtering remains important. The default Thrax settings extract only grammars with rank 2, and the pipeline implements scope-3 filtering (Hopkins and Langmead, 2010) when filtering grammars to test sets (for GHKM). Joshua uses cube pruning (Chiang, 2007) with a default pop limit of 100 to efficiently explore the search space. Other decoder options are too numerous to mention here, but are documented online. 2.2 2.4 Data preparation, alignment, and model building Tuning and testing The pipeline allows the specification (and optional linear interpolation) of an arbitrary number of language models. In addition, it builds an interpolated Kneser-Ney language model on the target side of the training data using KenLM (Heafield, 2011; Heafield et al., 2013), BerkeleyLM (Pauls and Klein, 2011) or SRILM (Stolcke, 2002). Joshua ships with MERT (Och, 200"
W13-2226,P09-2036,0,0.0202158,"steps in arbitrary acyclic dependency graphs (much like the U NIX make tool, but written with machine translation in mind). Joshua’s pipeline is more limited in that the basic pipeline skeleton is hard-coded, but reduced versatility covers many standard use cases and is arguably easier to use. The pipeline is parameterized in many ways, and all the options below are selectable with command-line switches. Pipeline documentation is available online. that the grammar first be converted to Chomsky Normal Form, thereby avoiding the complexities of explicit binarization schemes (Zhang et al., 2006; DeNero et al., 2009). CKY+ maintains cubic-time parsing complexity (in the sentence length) with Earley-style implicit binarization of rules. Joshua permits arbitrary SCFGs, imposing no limitation on the rank or form of grammar rules. Parsing complexity is still exponential in the scope of the grammar,3 so grammar filtering remains important. The default Thrax settings extract only grammars with rank 2, and the pipeline implements scope-3 filtering (Hopkins and Langmead, 2010) when filtering grammars to test sets (for GHKM). Joshua uses cube pruning (Chiang, 2007) with a default pop limit of 100 to efficiently ex"
W13-2226,N06-1014,0,0.0453991,"tion, it builds an interpolated Kneser-Ney language model on the target side of the training data using KenLM (Heafield, 2011; Heafield et al., 2013), BerkeleyLM (Pauls and Klein, 2011) or SRILM (Stolcke, 2002). Joshua ships with MERT (Och, 2003) and PRO implementations. Tuning with k-best batch MIRA (Cherry and Foster, 2012) is also supported via callouts to Moses. Data preparation involves data normalization (e.g., collapsing certain punctuation symbols) and tokenization (with the Penn treebank or user-specified tokenizer). Alignment with GIZA++ (Och and Ney, 2000) and the Berkeley aligner (Liang et al., 2006b) are supported. Joshua’s builtin grammar extractor, Thrax, is a Hadoop-based extraction implementation that scales easily to large datasets (Ganitkevitch et al., 2013). It supports extraction of both Hiero (Chiang, 2005) and SAMT grammars (Zollmann and Venugopal, 2006) with extraction heuristics easily specified via a flexible configuration file. The pipeline also supports GHKM grammar extraction (Galley et al., 2006) using the extractors available from Michel Galley2 or Moses. SAMT and GHKM grammar extraction require a parse tree, which are produced using the Berkeley parser (Petrov et al.,"
W13-2226,P10-2041,0,0.0180757,"ta (newstest2011). We tuned our systems with kbMIRA. For truecasing, we used a monolingual translation system built on the training data, and finally detokenized with simple heuristics. Other features Joshua 5.0 also includes many features designed to increase its usability. These include: • A TCP/IP server architecture, designed to handle multiple sets of translation requests while ensuring fairness in thread assignment both across and within these connections. 5 • Intelligent selection of translation and language model training data using crossentropy difference to rank training candidates (Moore and Lewis, 2010; Axelrod et al., 2011) (described in detail in Orland (2013)). The 5.0 release of Joshua is the result of a significant year-long research, engineering, and usability effort that we hope will be of service to the research community. User-friendly packages of Joshua are available from joshua-decoder. org, while developers are encouraged to participate via github.com/joshua-decoder/ joshua. Mailing lists, linked from the main Joshua page, are available for both. • A bundler for easy packaging of trained models with all of its dependencies. • A year’s worth of improvements to the Joshua pipeline"
W13-2226,P06-1121,0,0.0308244,"collapsing certain punctuation symbols) and tokenization (with the Penn treebank or user-specified tokenizer). Alignment with GIZA++ (Och and Ney, 2000) and the Berkeley aligner (Liang et al., 2006b) are supported. Joshua’s builtin grammar extractor, Thrax, is a Hadoop-based extraction implementation that scales easily to large datasets (Ganitkevitch et al., 2013). It supports extraction of both Hiero (Chiang, 2005) and SAMT grammars (Zollmann and Venugopal, 2006) with extraction heuristics easily specified via a flexible configuration file. The pipeline also supports GHKM grammar extraction (Galley et al., 2006) using the extractors available from Michel Galley2 or Moses. SAMT and GHKM grammar extraction require a parse tree, which are produced using the Berkeley parser (Petrov et al., 2006), or can be done outside the pipeline and supplied as an argument. 3 3.1 What’s New in Joshua 5.0 Sparse features The Joshua decoder is an implementation of the CKY+ algorithm (Chappelier et al., 1998), which generalizes CKY by removing the requirement Until a few years ago, machine translation systems were for the most part limited in the number of features they could employ, since the line-based optimization met"
W13-2226,W12-3018,0,0.0734936,"Missing"
W13-2226,W12-3134,1,0.899023,"Missing"
W13-2226,P00-1056,0,0.227105,"arbitrary number of language models. In addition, it builds an interpolated Kneser-Ney language model on the target side of the training data using KenLM (Heafield, 2011; Heafield et al., 2013), BerkeleyLM (Pauls and Klein, 2011) or SRILM (Stolcke, 2002). Joshua ships with MERT (Och, 2003) and PRO implementations. Tuning with k-best batch MIRA (Cherry and Foster, 2012) is also supported via callouts to Moses. Data preparation involves data normalization (e.g., collapsing certain punctuation symbols) and tokenization (with the Penn treebank or user-specified tokenizer). Alignment with GIZA++ (Och and Ney, 2000) and the Berkeley aligner (Liang et al., 2006b) are supported. Joshua’s builtin grammar extractor, Thrax, is a Hadoop-based extraction implementation that scales easily to large datasets (Ganitkevitch et al., 2013). It supports extraction of both Hiero (Chiang, 2005) and SAMT grammars (Zollmann and Venugopal, 2006) with extraction heuristics easily specified via a flexible configuration file. The pipeline also supports GHKM grammar extraction (Galley et al., 2006) using the extractors available from Michel Galley2 or Moses. SAMT and GHKM grammar extraction require a parse tree, which are produ"
W13-2226,N13-1092,1,0.786588,"Missing"
W13-2226,P03-1021,0,0.264651,"g, 2007) with a default pop limit of 100 to efficiently explore the search space. Other decoder options are too numerous to mention here, but are documented online. 2.2 2.4 Data preparation, alignment, and model building Tuning and testing The pipeline allows the specification (and optional linear interpolation) of an arbitrary number of language models. In addition, it builds an interpolated Kneser-Ney language model on the target side of the training data using KenLM (Heafield, 2011; Heafield et al., 2013), BerkeleyLM (Pauls and Klein, 2011) or SRILM (Stolcke, 2002). Joshua ships with MERT (Och, 2003) and PRO implementations. Tuning with k-best batch MIRA (Cherry and Foster, 2012) is also supported via callouts to Moses. Data preparation involves data normalization (e.g., collapsing certain punctuation symbols) and tokenization (with the Penn treebank or user-specified tokenizer). Alignment with GIZA++ (Och and Ney, 2000) and the Berkeley aligner (Liang et al., 2006b) are supported. Joshua’s builtin grammar extractor, Thrax, is a Hadoop-based extraction implementation that scales easily to large datasets (Ganitkevitch et al., 2013). It supports extraction of both Hiero (Chiang, 2005) and S"
W13-2226,P13-2121,0,0.0388401,"Missing"
W13-2226,P11-1027,0,0.0212052,"ltering grammars to test sets (for GHKM). Joshua uses cube pruning (Chiang, 2007) with a default pop limit of 100 to efficiently explore the search space. Other decoder options are too numerous to mention here, but are documented online. 2.2 2.4 Data preparation, alignment, and model building Tuning and testing The pipeline allows the specification (and optional linear interpolation) of an arbitrary number of language models. In addition, it builds an interpolated Kneser-Ney language model on the target side of the training data using KenLM (Heafield, 2011; Heafield et al., 2013), BerkeleyLM (Pauls and Klein, 2011) or SRILM (Stolcke, 2002). Joshua ships with MERT (Och, 2003) and PRO implementations. Tuning with k-best batch MIRA (Cherry and Foster, 2012) is also supported via callouts to Moses. Data preparation involves data normalization (e.g., collapsing certain punctuation symbols) and tokenization (with the Penn treebank or user-specified tokenizer). Alignment with GIZA++ (Och and Ney, 2000) and the Berkeley aligner (Liang et al., 2006b) are supported. Joshua’s builtin grammar extractor, Thrax, is a Hadoop-based extraction implementation that scales easily to large datasets (Ganitkevitch et al., 201"
W13-2226,W11-2123,0,0.0202034,"ope-3 filtering (Hopkins and Langmead, 2010) when filtering grammars to test sets (for GHKM). Joshua uses cube pruning (Chiang, 2007) with a default pop limit of 100 to efficiently explore the search space. Other decoder options are too numerous to mention here, but are documented online. 2.2 2.4 Data preparation, alignment, and model building Tuning and testing The pipeline allows the specification (and optional linear interpolation) of an arbitrary number of language models. In addition, it builds an interpolated Kneser-Ney language model on the target side of the training data using KenLM (Heafield, 2011; Heafield et al., 2013), BerkeleyLM (Pauls and Klein, 2011) or SRILM (Stolcke, 2002). Joshua ships with MERT (Och, 2003) and PRO implementations. Tuning with k-best batch MIRA (Cherry and Foster, 2012) is also supported via callouts to Moses. Data preparation involves data normalization (e.g., collapsing certain punctuation symbols) and tokenization (with the Penn treebank or user-specified tokenizer). Alignment with GIZA++ (Och and Ney, 2000) and the Berkeley aligner (Liang et al., 2006b) are supported. Joshua’s builtin grammar extractor, Thrax, is a Hadoop-based extraction implementation th"
W13-2226,P06-1055,0,0.0791186,"g et al., 2006b) are supported. Joshua’s builtin grammar extractor, Thrax, is a Hadoop-based extraction implementation that scales easily to large datasets (Ganitkevitch et al., 2013). It supports extraction of both Hiero (Chiang, 2005) and SAMT grammars (Zollmann and Venugopal, 2006) with extraction heuristics easily specified via a flexible configuration file. The pipeline also supports GHKM grammar extraction (Galley et al., 2006) using the extractors available from Michel Galley2 or Moses. SAMT and GHKM grammar extraction require a parse tree, which are produced using the Berkeley parser (Petrov et al., 2006), or can be done outside the pipeline and supplied as an argument. 3 3.1 What’s New in Joshua 5.0 Sparse features The Joshua decoder is an implementation of the CKY+ algorithm (Chappelier et al., 1998), which generalizes CKY by removing the requirement Until a few years ago, machine translation systems were for the most part limited in the number of features they could employ, since the line-based optimization method, MERT (Och, 2003), was not able to efficiently search over more than tens of feature weights. The introduction of discriminative tuning methods for machine translation (Liang et a"
W13-2226,D10-1063,0,0.0697106,"at the grammar first be converted to Chomsky Normal Form, thereby avoiding the complexities of explicit binarization schemes (Zhang et al., 2006; DeNero et al., 2009). CKY+ maintains cubic-time parsing complexity (in the sentence length) with Earley-style implicit binarization of rules. Joshua permits arbitrary SCFGs, imposing no limitation on the rank or form of grammar rules. Parsing complexity is still exponential in the scope of the grammar,3 so grammar filtering remains important. The default Thrax settings extract only grammars with rank 2, and the pipeline implements scope-3 filtering (Hopkins and Langmead, 2010) when filtering grammars to test sets (for GHKM). Joshua uses cube pruning (Chiang, 2007) with a default pop limit of 100 to efficiently explore the search space. Other decoder options are too numerous to mention here, but are documented online. 2.2 2.4 Data preparation, alignment, and model building Tuning and testing The pipeline allows the specification (and optional linear interpolation) of an arbitrary number of language models. In addition, it builds an interpolated Kneser-Ney language model on the target side of the training data using KenLM (Heafield, 2011; Heafield et al., 2013), Berk"
W13-2226,P06-1091,0,0.0258474,"done outside the pipeline and supplied as an argument. 3 3.1 What’s New in Joshua 5.0 Sparse features The Joshua decoder is an implementation of the CKY+ algorithm (Chappelier et al., 1998), which generalizes CKY by removing the requirement Until a few years ago, machine translation systems were for the most part limited in the number of features they could employ, since the line-based optimization method, MERT (Och, 2003), was not able to efficiently search over more than tens of feature weights. The introduction of discriminative tuning methods for machine translation (Liang et al., 2006a; Tillmann and Zhang, 2006; Chiang et al., 2008; Hopkins and May, 2011) has made it possible to tune large numbers of features in statistical machine translation systems, and open2 nlp.stanford.edu/˜mgalley/software/ stanford-ghkm-latest.tar.gz 3 Roughly, the number of consecutive nonterminals in a rule (Hopkins and Langmead, 2010). 2.3 Decoding 207 source implementations such as Cherry and Foster (2012) have made it easy. Joshua 5.0 has moved to a sparse feature representation internally. First, to clarify terminology, a feature as implemented in the decoder is actually a template that can introduce any number of actu"
W13-2226,W11-2160,1,0.923673,"Missing"
W13-2226,N06-1033,0,0.0332843,"rallel execution of steps in arbitrary acyclic dependency graphs (much like the U NIX make tool, but written with machine translation in mind). Joshua’s pipeline is more limited in that the basic pipeline skeleton is hard-coded, but reduced versatility covers many standard use cases and is arguably easier to use. The pipeline is parameterized in many ways, and all the options below are selectable with command-line switches. Pipeline documentation is available online. that the grammar first be converted to Chomsky Normal Form, thereby avoiding the complexities of explicit binarization schemes (Zhang et al., 2006; DeNero et al., 2009). CKY+ maintains cubic-time parsing complexity (in the sentence length) with Earley-style implicit binarization of rules. Joshua permits arbitrary SCFGs, imposing no limitation on the rank or form of grammar rules. Parsing complexity is still exponential in the scope of the grammar,3 so grammar filtering remains important. The default Thrax settings extract only grammars with rank 2, and the pipeline implements scope-3 filtering (Hopkins and Langmead, 2010) when filtering grammars to test sets (for GHKM). Joshua uses cube pruning (Chiang, 2007) with a default pop limit of"
W13-2226,W06-3119,0,0.0251012,"mentations. Tuning with k-best batch MIRA (Cherry and Foster, 2012) is also supported via callouts to Moses. Data preparation involves data normalization (e.g., collapsing certain punctuation symbols) and tokenization (with the Penn treebank or user-specified tokenizer). Alignment with GIZA++ (Och and Ney, 2000) and the Berkeley aligner (Liang et al., 2006b) are supported. Joshua’s builtin grammar extractor, Thrax, is a Hadoop-based extraction implementation that scales easily to large datasets (Ganitkevitch et al., 2013). It supports extraction of both Hiero (Chiang, 2005) and SAMT grammars (Zollmann and Venugopal, 2006) with extraction heuristics easily specified via a flexible configuration file. The pipeline also supports GHKM grammar extraction (Galley et al., 2006) using the extractors available from Michel Galley2 or Moses. SAMT and GHKM grammar extraction require a parse tree, which are produced using the Berkeley parser (Petrov et al., 2006), or can be done outside the pipeline and supplied as an argument. 3 3.1 What’s New in Joshua 5.0 Sparse features The Joshua decoder is an implementation of the CKY+ algorithm (Chappelier et al., 1998), which generalizes CKY by removing the requirement Until a few"
W14-3301,W11-2101,0,0.0261404,"Missing"
W14-3301,W11-2113,0,0.0533508,"Missing"
W14-3301,W12-3102,1,0.601062,"Missing"
W14-3301,P13-1139,0,0.553095,"§2). We evaluate on perplexity and also on accuracy, showing that the two are not always correlated, and arguing for the primacy of the latter (§3). We find that TrueSkill outperforms other models (§4). Moreover, TrueSkill also allows us to drastically reduce the amount of data that needs to be collected by sampling non-uniformly from the space of all competitions (§5), which also allows for greater separation of the systems into ranked clusters (§6). raised about how the evaluation is conducted (Bojar et al., 2011; Lopez, 2012). While these tweaks have been sensible (and later corroborated), Hopkins and May (2013) point out that this is essentially a model selection task, and should properly be driven by empirical performance on heldout data according to some metric. Instead of intuition, they suggest perplexity, and show that a novel graphical model outperforms existing approaches on that metric, with less amount of data. A second problem is the deficiency of the models used to produce the ranking, which work by computing simple ratios of wins (and, optionally, ties) to losses. Such approaches do not consider the relative difficulty of system matchups, and thus leave open the possibility that a system"
W14-3301,2012.iwslt-papers.5,0,0.340294,"ts metrics task, where evaluation metrics are evaluated. In machine translation, the longstanding disagreements about evaluation measures do not go away when moving from automatic metrics to human judges. This is due in no small part to the inherent ambiguity and subjectivity of the task, but also arises from the particular way that the WMT organizers produce the rankings. The systemlevel rankings are produced by collecting pairwise sentence-level comparisons between system outputs. These are then aggregated to produce a complete ordering of all systems, or, more recently, a partial ordering (Koehn, 2012), with systems clustered where they cannot be distinguished in a statistically significant way (Table 1, taken from Bojar et al. (2013)). Introduction The Workshop on Statistical Machine Translation (WMT) has long been a central event in the machine translation (MT) community for the evaluation of MT output. It hosts an annual set of shared translation tasks focused mostly on the translation of western European languages. One of its main functions is to publish a ranking of the systems for each task, which are produced by aggregating a large number of human judgments of sentencelevel pairwise"
W14-3301,W12-3101,0,0.0513987,"r provides an empirical comparison of a number of models of human evaluation (§2). We evaluate on perplexity and also on accuracy, showing that the two are not always correlated, and arguing for the primacy of the latter (§3). We find that TrueSkill outperforms other models (§4). Moreover, TrueSkill also allows us to drastically reduce the amount of data that needs to be collected by sampling non-uniformly from the space of all competitions (§5), which also allows for greater separation of the systems into ranked clusters (§6). raised about how the evaluation is conducted (Bojar et al., 2011; Lopez, 2012). While these tweaks have been sensible (and later corroborated), Hopkins and May (2013) point out that this is essentially a model selection task, and should properly be driven by empirical performance on heldout data according to some metric. Instead of intuition, they suggest perplexity, and show that a novel graphical model outperforms existing approaches on that metric, with less amount of data. A second problem is the deficiency of the models used to produce the ranking, which work by computing simple ratios of wins (and, optionally, ties) to losses. Such approaches do not consider the r"
W14-3301,P02-1040,0,0.103554,"is line of work by adapting the TrueSkillTM algorithm — an online approach for modeling the relative skills of players in ongoing competitions, such as Microsoft’s Xbox Live — to the human evaluation of machine translation output. Our experimental results show that TrueSkill outperforms other recently proposed models on accuracy, and also can significantly reduce the number of pairwise annotations that need to be collected by sampling non-uniformly from the space of system competitions. 1 score 0.638 0.604 0.591 0.571 0.562 0.541 0.512 0.486 0.439 0.429 0.420 0.389 0.322 reported (e.g., BLEU (Papineni et al., 2002)), the human evaluation is considered primary, and is in fact used as the gold standard for its metrics task, where evaluation metrics are evaluated. In machine translation, the longstanding disagreements about evaluation measures do not go away when moving from automatic metrics to human judges. This is due in no small part to the inherent ambiguity and subjectivity of the task, but also arises from the particular way that the WMT organizers produce the rankings. The systemlevel rankings are produced by collecting pairwise sentence-level comparisons between system outputs. These are then aggr"
W14-3301,W13-2201,1,\N,Missing
W14-3302,bojar-etal-2014-hindencorp,1,0.801715,"Missing"
W14-3302,W13-2242,0,0.189283,"Missing"
W14-3302,W14-3305,0,0.0227897,"Missing"
W14-3302,W14-3326,1,0.729814,"Missing"
W14-3302,W14-3313,0,0.0328356,"Missing"
W14-3302,W14-3342,0,0.0770851,"Missing"
W14-3302,2012.iwslt-papers.5,1,0.897779,"Each system SJ in the pool {Sj } is represented by an associated relative ability µj and a variance σa2 (fixed across all systems) which serve as the parameters of a Gaussian distribution. Samples from this distribution represent the quality of sentence translations, with higher quality samples having higher values. Pairwise annotations (S1 , S2 , π) are generated according to the following process: Method 1: Expected Wins (EW) Introduced for WMT13, the E XPECTED W INS has an intuitive score demonstrated to be accurate in ranking systems according to an underlying model of “relative ability” (Koehn, 2012a). The idea is to gauge the probability that a system Si will be ranked better than another system randomly chosen from a pool of opponents {Sj : j 6= i}. If we define the function win(A, B) as the number of times system A is ranked better than system B, 19 1. Select two systems S1 and S2 from the pool of systems {Sj } This score is then used to sort the systems and produce the ranking. 2. Draw two “translations”, adding random 2 to simulate Gaussian noise with variance σobs the subjectivity of the task and the differences among annotators: 3.4 Method Selection We have three methods which, pr"
W14-3302,W06-3114,1,0.176114,"estimation of machine translation quality, and a metrics task. This year, 143 machine translation systems from 23 institutions were submitted to the ten translation directions in the standard translation task. An additional 6 anonymized systems were included, and were then evaluated both automatically and manually. The quality estimation task had four subtasks, with a total of 10 teams, submitting 57 entries. 1 Introduction We present the results of the shared tasks of the Workshop on Statistical Machine Translation (WMT) held at ACL 2014. This workshop builds on eight previous WMT workshops (Koehn and Monz, 2006; Callison-Burch et al., 2007, 2008, 2009, 2010, 2011, 2012; Bojar et al., 2013). This year we conducted four official tasks: a translation task, a quality estimation task, a metrics task1 and a medical translation task. In the translation task (§2), participants were asked to translate a shared test set, optionally restricting themselves to the provided training data. We held ten translation tasks this year, between English and each of Czech, French, German, Hindi, and Russian. The Hindi translation tasks were new this year, providing a lesser resourced data condition on a challenging languag"
W14-3302,W12-3101,0,0.0505246,"Missing"
W14-3302,P02-1040,0,0.10401,"inuous-space language model is also used in a post-processing step for each system. POSTECH submitted a phrase-based SMT system and query translation system for the DE–EN language pair in both subtasks. They analysed three types of query formation, generated query translation candidates using term-to-term dictionaries and a phrase-based system, and then scored them using a co-occurrence word frequency measure to select the best candidate. UEDIN applied the Moses phrase-based system to 5.5 Results MT quality in the Medical Translation Task is evaluated using automatic evaluation metrics: BLEU (Papineni et al., 2002), TER (Snover et al., 2006), PER (Tillmann et al., 1997), and CDER (Leusch et al., 2006). BLEU scores are reported as percentage and all error rates are reported as one minus the original value, also as percentage, so that all metrics are in the 0-100 range, and higher scores indicate better translations. The main reason for not conducting human evaluation, as it happens in the standard Trans45 original ID BLEU normalized truecased normalized lowercased 1-TER 1-PER 1-CDER BLEU 1-TER 1-PER 1-CDER Czech→English CUNI 29.64 29.79±1.07 47.45±1.15 61.64±1.06 52.18±0.98 31.68±1.14 49.84±1.10 64.38±1."
W14-3302,W14-3328,0,0.0383178,"Missing"
W14-3302,W14-3301,1,0.485672,"guage pair by translating newspaper articles and provided training data. 2.1 2.2 As in past years we provided parallel corpora to train translation models, monolingual corpora to train language models, and development sets to tune system parameters. Some training corpora were identical from last year (Europarl4 , United Nations, French-English 109 corpus, CzEng, Common Crawl, Russian-English Wikipedia Headlines provided by CMU), some were updated (Russian-English parallel data provided by Yandex, News Commentary, monolingual data), and a new corpus was added (HindiEnglish corpus, Bojar et al. (2014)), Hindi-English Wikipedia Headline corpus). Some statistics about the training materials are given in Figure 1. Test data The test data for this year’s task was selected from news stories from online sources, as before. However, we changed our method to create the test sets. In previous years, we took equal amounts of source sentences from all six languages involved (around 500 sentences each), and translated them into all other languages. While this produced a multi-parallel test corpus that could be also used for language pairs (such as Czech-Russian) that we did not include in the evaluati"
W14-3302,W14-3330,0,0.0464303,"Missing"
W14-3302,W14-3320,0,0.0387654,"Missing"
W14-3302,W14-3317,0,0.0337314,"Missing"
W14-3302,W14-3343,1,0.726267,"Finally, with respect to out-of-domain (different 41 It is interesting to mention the indirect use of human translations by USHEFF for Tasks 1.1-1.3: given a translation for a source segment, all other translations for the same segment were used as pseudo-references. Apart from when this translation was actually the human translation, the human translation was effectively used as a reference. While this reference was mixed with 23 other pseudo-references (other machine translations) for the feature computations, these features led to significant gains in performance over the baseline features Scarton and Specia (2014). We believe that more investigation is needed for human translation quality prediction. Tasks dedicated to this type of data at both sentence- and word-level in the next editions of this shared task would be a possible starting point. The acquisition of such data is however much more costly, as it is arguably hard to find examples of low quality human translation, unless specific settings, such as translation learner corpora, are considered. text domain and MT system) test data, for Task 1.1, none of the papers submitted included experiments. (Shah and Specia, 2014) applied the models trained"
W14-3302,W02-1001,0,\N,Missing
W14-3302,E06-1031,0,\N,Missing
W14-3302,W12-3102,1,\N,Missing
W14-3302,P13-2135,0,\N,Missing
W14-3302,W14-3311,0,\N,Missing
W14-3302,W14-3314,0,\N,Missing
W14-3302,W09-0401,1,\N,Missing
W14-3302,W14-3319,0,\N,Missing
W14-3302,W14-3344,0,\N,Missing
W14-3302,W14-3327,0,\N,Missing
W14-3302,W07-0718,1,\N,Missing
W14-3302,P11-1132,0,\N,Missing
W14-3302,W13-2248,0,\N,Missing
W14-3302,W14-3307,0,\N,Missing
W14-3302,W10-1703,1,\N,Missing
W14-3302,W14-3323,0,\N,Missing
W14-3302,P13-4014,1,\N,Missing
W14-3302,W08-0309,1,\N,Missing
W14-3302,W14-3340,1,\N,Missing
W14-3302,W14-3312,0,\N,Missing
W14-3302,P13-1139,0,\N,Missing
W14-3302,W14-3310,1,\N,Missing
W14-3302,P13-1004,1,\N,Missing
W14-3302,W14-3304,0,\N,Missing
W14-3302,W14-3321,0,\N,Missing
W14-3302,W14-3308,0,\N,Missing
W14-3302,uresova-etal-2014-multilingual,1,\N,Missing
W14-3302,W14-3336,1,\N,Missing
W14-3302,W14-3331,0,\N,Missing
W14-3302,W14-3332,0,\N,Missing
W14-3302,W14-3325,0,\N,Missing
W14-3302,W14-3329,0,\N,Missing
W14-3302,W14-3315,0,\N,Missing
W14-3302,W13-2201,1,\N,Missing
W14-3302,W14-3339,0,\N,Missing
W14-3302,W14-3322,1,\N,Missing
W14-3302,W14-3303,0,\N,Missing
W14-3302,W14-3318,0,\N,Missing
W14-3302,W14-3341,0,\N,Missing
W14-3302,W11-2101,1,\N,Missing
W14-3302,W14-3338,1,\N,Missing
W15-3001,W05-0909,0,0.0473932,"asks 1 and 2 provide the same dataset with English-Spanish translations generated by the statistical machine translation (SMT) system, while Task 3 provides two different datasets, for two language pairs: English-German (EN-DE) and German-English (DE-EN) translations taken from all participating systems in WMT13 (Bojar et al., 2013). These datasets were annotated with different labels for quality: for Tasks 1 and 2, the labels were automatically derived from the post-editing of the machine translation output, while for Task 3, scores were computed based on reference translations using Meteor (Banerjee and Lavie, 2005). Any external resource, including additional quality estimation training data, could be used by participants (no distinction between open and close tracks was made). As presented in Section 4.1, participants were also provided with a baseline set of features for each task, and a software package to extract these and other quality estimation features and perform model learning, with suggested methods for all levels of prediction. Participants, described in Section 4.2, could submit up to two systems for each task. Data used to build MT systems or internal system information (such as model scor"
W15-3001,2011.mtsummit-papers.35,0,0.348896,"Missing"
W15-3001,W13-2241,1,0.851171,"Missing"
W15-3001,P13-2097,1,0.801912,"Missing"
W15-3001,W13-2242,0,0.0386206,"Missing"
W15-3001,W14-3339,0,0.0452066,"Missing"
W15-3001,W15-3035,0,0.054515,"Missing"
W15-3001,W11-2103,1,0.524514,"Missing"
W15-3001,W13-2201,1,0.499374,"Missing"
W15-3001,W14-3302,1,0.498006,"Missing"
W15-3001,W14-3340,1,0.54686,"Missing"
W15-3001,W15-3007,0,0.0166876,"Missing"
W15-3001,W15-3006,1,0.827804,"Missing"
W15-3001,W07-0718,1,0.664541,"om 24 institutions were submitted to the ten translation directions in the standard translation task. An additional 7 anonymized systems were included, and were then evaluated both automatically and manually. The quality estimation task had three subtasks, with a total of 10 teams, submitting 34 entries. The pilot automatic postediting task had a total of 4 teams, submitting 7 entries. 1 Introduction We present the results of the shared tasks of the Workshop on Statistical Machine Translation (WMT) held at EMNLP 2015. This workshop builds on eight previous WMT workshops (Koehn and Monz, 2006; Callison-Burch et al., 2007, 2008, 2009, 2010, 2011, 2012; Bojar et al., 2013, 2014). This year we conducted five official tasks: a translation task, a quality estimation task, a metrics task, a tuning task1 , and a automatic postediting task. In the translation task (§2), participants were asked to translate a shared test set, optionally restricting themselves to the provided training data. We held ten translation tasks this year, between English and each of Czech, French, German, Finnish, and Russian. The Finnish translation 1 The metrics and tuning tasks are reported in separate papers (Stanojevi´c et al., 2015a,b)."
W15-3001,2014.amta-researchers.13,0,0.0200349,"Missing"
W15-3001,W08-0309,1,0.406319,"2. machine translation and automatic evaluation or prediction of translation quality. 2 Overview of the Translation Task The recurring task of the workshop examines translation between English and other languages. As in the previous years, the other languages include German, French, Czech and Russian. Finnish replaced Hindi as the special language this year. Finnish is a lesser resourced language compared to the other languages and has challenging morphological properties. Finnish represents also a different language family that we had not tackled since we included Hungarian in 2008 and 2009 (Callison-Burch et al., 2008, 2009). We created a test set for each language pair by translating newspaper articles and provided training data, except for French, where the test set was drawn from user-generated comments on the news articles. 2.1 2.3 We received 68 submissions from 24 institutions. The participating institutions and their entry names are listed in Table 2; each system did not necessarily appear in all translation tasks. We also included 1 commercial off-the-shelf MT system and 6 online statistical MT systems, which we anonymized. For presentation of the results, systems are treated as either constrained"
W15-3001,W15-3025,1,0.914094,"teps aimed at removing duplicates and those triplets in which any of the elements (source, target, post-edition) was either too long or too short compared to the others, or included tags or special problematic symbols. The main reason for random sampling was to induce some homogeneity across the three datasets and, in turn, Automatic Post-editing Task This year WMT hosted for the first time a shared task on automatic post-editing (APE) for machine translation. The task requires to automatically correct the errors present in a machine translated text. As pointed out in Parton et al. (2012) and Chatterjee et al. (2015b), from the application point of view, APE components would make it possible to: • Improve MT output by exploiting information unavailable to the decoder, or by per22 The original triplets were provided by Unbabel (https: //unbabel.com/). 28 in terms of Translation Error Rate (TER) (Snover et al., 2006a), an evaluation metric commonly used in MT-related tasks (e.g. in quality estimation) to measure the minimum edit distance between an automatic translation and a reference translation.23 Systems are ranked based on the average TER calculated on the test set by using the TERcom24 software: lowe"
W15-3001,W10-1703,1,0.163282,"Missing"
W15-3001,P15-2026,1,0.797338,"teps aimed at removing duplicates and those triplets in which any of the elements (source, target, post-edition) was either too long or too short compared to the others, or included tags or special problematic symbols. The main reason for random sampling was to induce some homogeneity across the three datasets and, in turn, Automatic Post-editing Task This year WMT hosted for the first time a shared task on automatic post-editing (APE) for machine translation. The task requires to automatically correct the errors present in a machine translated text. As pointed out in Parton et al. (2012) and Chatterjee et al. (2015b), from the application point of view, APE components would make it possible to: • Improve MT output by exploiting information unavailable to the decoder, or by per22 The original triplets were provided by Unbabel (https: //unbabel.com/). 28 in terms of Translation Error Rate (TER) (Snover et al., 2006a), an evaluation metric commonly used in MT-related tasks (e.g. in quality estimation) to measure the minimum edit distance between an automatic translation and a reference translation.23 Systems are ranked based on the average TER calculated on the test set by using the TERcom24 software: lowe"
W15-3001,W12-3102,1,0.571688,"ator agreement, both for inter- and intra-annotator agreement scores. not included to make the graphs viewable). The plots cleary suggest that a fair comparison of systems of different kinds cannot rely on automatic scores. Rule-based systems receive a much lower BLEU score than statistical systems (see for instance English–German, e.g., PROMT- RULE). The same is true to a lesser degree for statistical syntax-based systems (see English–German, UEDIN - SYNTAX ) and online systems that were not tuned to the shared task (see Czech–English, CU TECTO vs. the cluster of tuning task systems TT*). 4 (Callison-Burch et al., 2012; Bojar et al., 2013, 2014), with tasks including both sentence and word-level estimation, using new training and test datasets, and an additional task: document-level prediction. The goals of this year’s shared task were: • Advance work on sentence- and wordlevel quality estimation by providing larger datasets. • Investigate the effectiveness of quality labels, features and learning methods for documentlevel prediction. Quality Estimation Task • Explore differences between sentence-level and document-level prediction. The fourth edition of the WMT shared task on quality estimation (QE) of mac"
W15-3001,W15-3008,0,0.0135172,"secondary key. The results for the scoring variant are presented in Table 9, sorted from best to worst by using the MAE metric scores as primary key and the RMSE metric scores as secondary key. Pearson’s r coefficients for all systems against HTER is given in Table 10. As discussed in (Graham, 2015), the results according to this metric can rank participating systems differently. In particular, we note the SHEF/GP submission, are which is deemed significantly worse than the baseline system according to MAE, but substantially better than the baseline according to Pearson’s correlation. Graham (2015) argues that the use of MAE as evaluation score for quality estimation tasks is inadequate, as MAE is very sensitive to variance. This means that a system that outputs predictions with high variance is more likely to have high MAE score, even if the distribution follows that of the true labels. Interestingly, according to Pearson’s correlation, the systems are • Source sentence (English). • Automatic translation (Spanish). • Manual post-edition of the automatic translation. • Word-level binary (“OK”/“BAD”) labelling of the automatic translation. The binary labels for the datasets were acquired"
W15-3001,W09-0401,1,0.251315,"Missing"
W15-3001,W15-3009,0,0.0460438,"Missing"
W15-3001,W15-3010,0,0.035772,"Missing"
W15-3001,P10-4002,0,0.00861255,"t sentences and number of punctuation marks in source and target sentences. All other features were averaged. The implementation for document-level feature extraction is available in Q U E ST ++ (Specia et al., 2015).12 These features were then used to train a SVR algorithm with RBF kernel using the SCIKIT- LEARN toolkit. The γ,  and C parameters were optimised via grid search with 5-fold cross validation on the training set. • Source token aligned to the target token, its left and right contexts of one word. The alignments were produced with the force align.py script, which is part of cdec (Dyer et al., 2010). It allows to align new parallel data with a pre-trained alignment model built with the cdec word aligner (fast align). The alignment model was trained on the Europarl corpus (Koehn, 2005). • Boolean dictionary features: whether target token is a stopword, a punctuation mark, a proper noun, a number. 4.2 Participants Table 7 lists all participating teams submitting systems to any of the tasks. Each team was allowed up to two submissions for each task and language pair. In the descriptions below, participation in specific tasks is denoted by a task identifier. • Target language model features:"
W15-3001,W15-3011,0,0.0373266,"Missing"
W15-3001,W15-3036,0,0.0738999,"Missing"
W15-3001,W15-3012,0,0.0168435,"secondary key. The results for the scoring variant are presented in Table 9, sorted from best to worst by using the MAE metric scores as primary key and the RMSE metric scores as secondary key. Pearson’s r coefficients for all systems against HTER is given in Table 10. As discussed in (Graham, 2015), the results according to this metric can rank participating systems differently. In particular, we note the SHEF/GP submission, are which is deemed significantly worse than the baseline system according to MAE, but substantially better than the baseline according to Pearson’s correlation. Graham (2015) argues that the use of MAE as evaluation score for quality estimation tasks is inadequate, as MAE is very sensitive to variance. This means that a system that outputs predictions with high variance is more likely to have high MAE score, even if the distribution follows that of the true labels. Interestingly, according to Pearson’s correlation, the systems are • Source sentence (English). • Automatic translation (Spanish). • Manual post-edition of the automatic translation. • Word-level binary (“OK”/“BAD”) labelling of the automatic translation. The binary labels for the datasets were acquired"
W15-3001,W15-4903,0,0.0623526,"Missing"
W15-3001,W15-3013,1,0.843414,"Missing"
W15-3001,W11-2123,0,0.0240468,"27,101 5,966 8,816 SRC 13,701 3,765 5,307 Lemmas TGT PE 7,624 7,689 2,810 2,819 3,778 3,814 Table 18: Data statistics. and classifying each word of a sentence as good or bad. An automatic translation to be post-edited is first decoded by our SPE system, then fed into one of the classifiers identified as SVM180feat and RNN. The HTER estimator selects the translation with the lower score while the binary word-level classifier selects the translation with the fewer amount of bad tags. The official evaluation of the shared task show an advantage of the RNN approach compared to SVM. KenLM toolkit (Heafield, 2011) for standard ngram modeling with an n-gram length of 5. Finally, the APE system was tuned on the development set, optimizing TER with Minimum Error Rate Training (Och, 2003). The results of this additional term of comparison, computed for both evaluation modalities (case sensitive/insensitive), are also reported in Tables 20 and 21. For each submitted run, the statistical significance of performance differences with respect to the baseline and the re-implementation of Simard et al. (2007) is calculated with the bootstrap test (Koehn, 2004). 5.2 FBK. The two runs submitted by FBK (Chatterjee e"
W15-3001,W08-0509,0,0.0268564,"ere collected by means of a crowdsourcing platform developed by the data provider. Monolingual translation as another term of comparison. To get further insights about the progress with respect to previous APE methods, participants’ results are also analysed with respect to another term of comparison: a reimplementation of the state-of-the-art approach firstly proposed by Simard et al. (2007).27 For this purpose, a phrase-based SMT system based on Moses (Koehn et al., 2007) is used. Translation and reordering models were estimated following the Moses protocol with default setup using MGIZA++ (Gao and Vogel, 2008) for word alignment. For language modeling we used the Test data (1, 817 instances) consists of (source, target) pairs having similar characteristics of those in the training set. Human post-editions of the test target instances were left apart to measure system performance. The data creation procedure adopted, as well as the origin and the domain of the texts pose specific challenges to the participating systems. As discussed in Section 5.4, the results of this pilot task can be partially explained in light of such challenges. This dataset, however, has three major advantages that made it sui"
W15-3001,W15-3014,0,0.0344469,"Missing"
W15-3001,P15-1174,0,0.0105686,"415 for test. Since no human annotation exists for the quality of entire paragraphs (or documents), Meteor against reference translations was used as quality label for this task. Meteor was calculated using its implementation within the Asyia toolkit, with the following settings: exact match, tokenised and case insensitive (Gim´enez and M`arquez, 2010). guage pairs. All systems were significantly better than the baseline. However, the difference between the baseline system and all submissions was much lower in the scoring evaluation than in the ranking evaluation. Following the suggestion in (Graham, 2015), Table 16 shows an alternative ranking of systems considering Pearson’s r correlation results. The alternative ranking differs from the official ranking in terms of MAE: for EN-DE, RTMDCU/RTM-FS-SVR is no longer in the winning group, while for DE-EN, USHEF/QUEST-DISCBO and USAAR-USHEF/BFF did not show statistically significant difference against the baseline. However, as with Task 1 these results are the same as the official ones in terms of DeltaAvg. 4.6 Discussion In what follows, we discuss the main findings of this year’s shared task based on the goals we had previously identified for it."
W15-3001,W04-3250,1,0.485885,"f the RNN approach compared to SVM. KenLM toolkit (Heafield, 2011) for standard ngram modeling with an n-gram length of 5. Finally, the APE system was tuned on the development set, optimizing TER with Minimum Error Rate Training (Och, 2003). The results of this additional term of comparison, computed for both evaluation modalities (case sensitive/insensitive), are also reported in Tables 20 and 21. For each submitted run, the statistical significance of performance differences with respect to the baseline and the re-implementation of Simard et al. (2007) is calculated with the bootstrap test (Koehn, 2004). 5.2 FBK. The two runs submitted by FBK (Chatterjee et al., 2015a) are based on combining the statistical phrase-based post-editing approach proposed by Simard et al. (2007) and its most significant variant proposed by B´echara et al. (2011). The APE systems are built-in an incremental manner. At each stage of the APE pipeline, the best configuration of a component is decided and then used in the next stage. The APE pipeline begins with the selection of the best language model from several language models trained on different types and quantities of data. The next stage addresses the possible"
W15-3001,2005.mtsummit-papers.11,1,0.0759255,"(Specia et al., 2015).12 These features were then used to train a SVR algorithm with RBF kernel using the SCIKIT- LEARN toolkit. The γ,  and C parameters were optimised via grid search with 5-fold cross validation on the training set. • Source token aligned to the target token, its left and right contexts of one word. The alignments were produced with the force align.py script, which is part of cdec (Dyer et al., 2010). It allows to align new parallel data with a pre-trained alignment model built with the cdec word aligner (fast align). The alignment model was trained on the Europarl corpus (Koehn, 2005). • Boolean dictionary features: whether target token is a stopword, a punctuation mark, a proper noun, a number. 4.2 Participants Table 7 lists all participating teams submitting systems to any of the tasks. Each team was allowed up to two submissions for each task and language pair. In the descriptions below, participation in specific tasks is denoted by a task identifier. • Target language model features: – The order of the highest order n-gram which starts or ends with the target token. – Backoff behaviour of the n-grams (ti−2 , ti−1 , ti ), (ti−1 , ti , ti+1 ), (ti , ti+1 , ti+2 ), where"
W15-3001,W15-3039,1,0.80659,"Missing"
W15-3001,J10-4005,0,0.0619684,"Missing"
W15-3001,J82-2005,0,0.818658,"Missing"
W15-3001,W14-3342,0,0.0353204,"t although the system is referred to as “baseline”, it is in fact a strong system. It has proved robust across a range of language pairs, MT systems, and text domains for predicting various forms of post-editing effort (Callison-Burch et al., 2012; Bojar et al., 2013, 2014). Word-level baseline system: For Task 2, the baseline features were extracted with the M AR MOT tool9 . For the baseline system we used a number of features that have been found the most informative in previous research on word-level quality estimation. Our baseline set of features is loosely based on the one described in (Luong et al., 2014). It contains the following 25 features: Baseline systems Sentence-level baseline system: For Task 1, Q U E ST7 (Specia et al., 2013) was used to extract 17 MT system-independent features from the source and translation (target) files and parallel corpora: • Word count in the source and target sentences, source and target token count ratio. Although these features are sentence-level (i.e. their values will be the same for all words in a sentence), but the length of a sentence might influence the probability of a word being incorrect. • Number of tokens in the source and target sentences. • Ave"
W15-3001,W13-2248,0,0.0824189,"Missing"
W15-3001,W06-3114,1,0.427665,"translation systems from 24 institutions were submitted to the ten translation directions in the standard translation task. An additional 7 anonymized systems were included, and were then evaluated both automatically and manually. The quality estimation task had three subtasks, with a total of 10 teams, submitting 34 entries. The pilot automatic postediting task had a total of 4 teams, submitting 7 entries. 1 Introduction We present the results of the shared tasks of the Workshop on Statistical Machine Translation (WMT) held at EMNLP 2015. This workshop builds on eight previous WMT workshops (Koehn and Monz, 2006; Callison-Burch et al., 2007, 2008, 2009, 2010, 2011, 2012; Bojar et al., 2013, 2014). This year we conducted five official tasks: a translation task, a quality estimation task, a metrics task, a tuning task1 , and a automatic postediting task. In the translation task (§2), participants were asked to translate a shared test set, optionally restricting themselves to the provided training data. We held ten translation tasks this year, between English and each of Czech, French, German, Finnish, and Russian. The Finnish translation 1 The metrics and tuning tasks are reported in separate papers (S"
W15-3001,W15-3015,0,0.0443946,"Missing"
W15-3001,W15-3016,0,0.0442638,"Missing"
W15-3001,W15-3037,0,0.0800739,"Missing"
W15-3001,P03-1021,0,0.0587969,"utomatic translation to be post-edited is first decoded by our SPE system, then fed into one of the classifiers identified as SVM180feat and RNN. The HTER estimator selects the translation with the lower score while the binary word-level classifier selects the translation with the fewer amount of bad tags. The official evaluation of the shared task show an advantage of the RNN approach compared to SVM. KenLM toolkit (Heafield, 2011) for standard ngram modeling with an n-gram length of 5. Finally, the APE system was tuned on the development set, optimizing TER with Minimum Error Rate Training (Och, 2003). The results of this additional term of comparison, computed for both evaluation modalities (case sensitive/insensitive), are also reported in Tables 20 and 21. For each submitted run, the statistical significance of performance differences with respect to the baseline and the re-implementation of Simard et al. (2007) is calculated with the bootstrap test (Koehn, 2004). 5.2 FBK. The two runs submitted by FBK (Chatterjee et al., 2015a) are based on combining the statistical phrase-based post-editing approach proposed by Simard et al. (2007) and its most significant variant proposed by B´echara"
W15-3001,padro-stanilovsky-2012-freeling,0,0.011024,"o rankings are not identical, none of the systems was particularly penalized by the case sensitive evaluation. Indeed, individual differences in the two modes are always close to the same value (∼ 0.7 TER difference) measured for the two baselines. USAAR-SAPE. The USAAR-SAPE system (Pal et al., 2015b) is designed with three basic components: corpus preprocessing, hybrid word alignment and a state-of-the-art phrase-based SMT system integrated with the hybrid word alignment. The preprocessing of the training corpus is carried out by stemming the Spanish MT output and the PE data using Freeling (Padr and Stanilovsky, 2012). The hybrid word alignment method combines different kinds of word alignment: GIZA++ word alignment with the 31 ID Baseline FBK Primary LIMSI Primary USAAR-SAPE LIMSI Contrastive Abu-MaTran Primary FBK Contrastive (Simard et al., 2007) Abu-MaTran Contrastive Avg. TER 22.913 23.228 23.331 23.426 23.573 23.639 23.649 23.839 24.715 ID Baseline LIMSI Primary FBK Primary USAAR-SAPE Abu-MaTran Primary LIMSI Contrastive FBK Contrastive (Simard et al., 2007) Abu-MaTran Contrastive Table 20: Official results for the WMT15 Automatic Post-editing task – average TER (↓) case sensitive. Table 21: Official"
W15-3001,W15-3038,0,0.0668224,"Missing"
W15-3001,W13-2814,0,0.0155219,"ject (Prompsit) Fondazione Bruno Kessler, Italy (Chatterjee et al., 2015a) Laboratoire d’Informatique pour la M´ecanique et les Sciences de l’Ing´enieur, France (Wisniewski et al., 2015) Saarland University, Germany & Jadavpur University, India (Pal et al., 2015b) Table 19: Participants in the WMT15 Automatic Post-editing pilot task. grow-diag-final-and (GDFA) heuristic (Koehn, 2010), SymGiza++ (Junczys-Dowmunt and Szal, 2011), the Berkeley aligner (Liang et al., 2006), and the edit distance-based aligners (Snover et al., 2006a; Lavie and Agarwal, 2007). These different word alignment tables (Pal et al., 2013) are combined by a mathematical union method. For the phrase-based SMT system various maximum phrase lengths for the translation model and n–gram settings for the language model are used. The best results in terms of BLEU (Papineni et al., 2002) score are achieved by a maximum phrase length of 7 for the translation model and a 5-gram language model. the model. These features measure the similarity and the reliability of the translation options and help to improve the precision of the resulting APE system. LIMSI. For the first edition of the APE shared task LIMSI submitted two systems (Wisniews"
W15-3001,W07-0734,0,0.0277649,"Abu-MaTran FBK LIMSI USAAR-SAPE Participating team Abu-MaTran Project (Prompsit) Fondazione Bruno Kessler, Italy (Chatterjee et al., 2015a) Laboratoire d’Informatique pour la M´ecanique et les Sciences de l’Ing´enieur, France (Wisniewski et al., 2015) Saarland University, Germany & Jadavpur University, India (Pal et al., 2015b) Table 19: Participants in the WMT15 Automatic Post-editing pilot task. grow-diag-final-and (GDFA) heuristic (Koehn, 2010), SymGiza++ (Junczys-Dowmunt and Szal, 2011), the Berkeley aligner (Liang et al., 2006), and the edit distance-based aligners (Snover et al., 2006a; Lavie and Agarwal, 2007). These different word alignment tables (Pal et al., 2013) are combined by a mathematical union method. For the phrase-based SMT system various maximum phrase lengths for the translation model and n–gram settings for the language model are used. The best results in terms of BLEU (Papineni et al., 2002) score are achieved by a maximum phrase length of 7 for the translation model and a 5-gram language model. the model. These features measure the similarity and the reliability of the translation options and help to improve the precision of the resulting APE system. LIMSI. For the first edition of"
W15-3001,W15-3017,0,0.0328586,"Missing"
W15-3001,W15-3026,0,0.0488223,"Missing"
W15-3001,W15-3040,1,0.889327,"HIDDEN Participating team Dublin City University, Ireland and University of Sheffield, UK (Logacheva et al., 2015) Heidelberg University, Germany (Kreutzer et al., 2015) Lorraine Laboratory of Research in Computer Science and its Applications, France (Langlois, 2015) Dublin City University, Ireland (Bicici et al., 2015) Shenyang Aerospace University, China (Shang et al., 2015) University of Sheffield Team 1, UK (Shah et al., 2015) Alicant University, Spain (Espl`a-Gomis et al., 2015a) Ghent University, Belgium (Tezcan et al., 2015) University of Sheffield, UK and Saarland University, Germany (Scarton et al., 2015a) University of Sheffield, UK (Scarton et al., 2015a) Undisclosed Table 7: Participants in the WMT15 quality estimation shared task. one from the official training data. Pseudoreferences were produced by three online systems. These features measure the intersection between n-gram sets of the target sentence and of the pseudo-references. Three sets of features were extracted from each online system, and a fourth feature was extracted measuring the inter-agreement among the three online systems and the target system. and fine-tuned for the quality estimation classification task by back-propagat"
W15-3001,W15-4916,1,0.80858,"Missing"
W15-3001,P02-1040,0,0.108957,"ndia (Pal et al., 2015b) Table 19: Participants in the WMT15 Automatic Post-editing pilot task. grow-diag-final-and (GDFA) heuristic (Koehn, 2010), SymGiza++ (Junczys-Dowmunt and Szal, 2011), the Berkeley aligner (Liang et al., 2006), and the edit distance-based aligners (Snover et al., 2006a; Lavie and Agarwal, 2007). These different word alignment tables (Pal et al., 2013) are combined by a mathematical union method. For the phrase-based SMT system various maximum phrase lengths for the translation model and n–gram settings for the language model are used. The best results in terms of BLEU (Papineni et al., 2002) score are achieved by a maximum phrase length of 7 for the translation model and a 5-gram language model. the model. These features measure the similarity and the reliability of the translation options and help to improve the precision of the resulting APE system. LIMSI. For the first edition of the APE shared task LIMSI submitted two systems (Wisniewski et al., 2015). The first one is based on the approach of Simard et al. (2007) and considers the APE task as a monolingual translation between a translation hypothesis and its post-edition. This straightforward approach does not succeed in imp"
W15-3001,2012.eamt-1.34,0,0.0799883,"Missing"
W15-3001,W15-3023,0,0.0267499,"Missing"
W15-3001,W15-3018,0,0.0412931,"Missing"
W15-3001,W15-3041,1,0.847113,"Missing"
W15-3001,potet-etal-2012-collection,0,0.011237,"Missing"
W15-3001,W15-3042,0,0.0858502,"Missing"
W15-3001,W15-3019,0,0.0362545,"Missing"
W15-3001,N07-1064,0,0.644928,"nsitive) are reported in Tables 20 and 21. • The target (TGT) is a tokenized Spanish translation of the source, produced by an unknown MT system; • The human post-edition (PE) is a manuallyrevised version of the target. PEs were collected by means of a crowdsourcing platform developed by the data provider. Monolingual translation as another term of comparison. To get further insights about the progress with respect to previous APE methods, participants’ results are also analysed with respect to another term of comparison: a reimplementation of the state-of-the-art approach firstly proposed by Simard et al. (2007).27 For this purpose, a phrase-based SMT system based on Moses (Koehn et al., 2007) is used. Translation and reordering models were estimated following the Moses protocol with default setup using MGIZA++ (Gao and Vogel, 2008) for word alignment. For language modeling we used the Test data (1, 817 instances) consists of (source, target) pairs having similar characteristics of those in the training set. Human post-editions of the test target instances were left apart to measure system performance. The data creation procedure adopted, as well as the origin and the domain of the texts pose specifi"
W15-3001,W15-3022,0,0.0629217,"Missing"
W15-3001,P15-4020,1,0.0689693,"://github.com/lspecia/quest 14 http://scikit-learn.org/ https://github.com/qe-team/marmot • Target token, its left and right contexts of one word. Document-level baseline system: For Task 3, the baseline features for sentence-level prediction were used. These are aggregated by summing or averaging their values for the entire document. Features that were summed: number of tokens in the source and target sentences and number of punctuation marks in source and target sentences. All other features were averaged. The implementation for document-level feature extraction is available in Q U E ST ++ (Specia et al., 2015).12 These features were then used to train a SVR algorithm with RBF kernel using the SCIKIT- LEARN toolkit. The γ,  and C parameters were optimised via grid search with 5-fold cross validation on the training set. • Source token aligned to the target token, its left and right contexts of one word. The alignments were produced with the force align.py script, which is part of cdec (Dyer et al., 2010). It allows to align new parallel data with a pre-trained alignment model built with the cdec word aligner (fast align). The alignment model was trained on the Europarl corpus (Koehn, 2005). • Boole"
W15-3001,W15-3027,0,0.0625354,"Missing"
W15-3001,P13-4014,1,0.762876,"Missing"
W15-3001,2013.mtsummit-papers.15,0,0.055239,"these rules is based on an analysis of the most frequent error corrections and aims at: i) predicting word case; ii) predicting exclamation and interrogation marks; and iii) predicting verbal endings. Experiments with this approach show that this system also hurts translation quality. An in-depth analysis revealed that this negative result is mainly explained by two reasons: i) most of the post-edition operations are nearly unique, which makes very difficult to generalize from a small amount of data; and ii) even when they are not, the high variability of post-editing, already pointed out by Wisniewski et al. (2013), results in predicting legitimate corrections that have not been made by the annotators, therefore preventing from improving over the baseline. 5.3 Results The official results achieved by the participating systems are reported in Tables 20 and 21. The seven runs submitted are sorted based on the average TER they achieve on test data. Table 20 shows the results computed in case sensitive mode, while Table 21 provides scores computed in the case insensitive mode. Both rankings reveal an unexpected outcome: none of the submitted runs was able to beat the baselines (i.e. average TER scores of 22"
W15-3001,W15-3032,1,0.810291,"Missing"
W15-3001,W15-3031,1,0.376914,"Missing"
W15-3001,W15-3020,1,0.808362,"Missing"
W15-3001,W15-3043,0,0.0443554,"Missing"
W15-3001,W15-3021,0,0.038175,"Missing"
W15-3001,P07-2045,1,\N,Missing
W15-3001,W15-3004,0,\N,Missing
W15-3001,2015.eamt-1.4,0,\N,Missing
W15-3001,N06-1014,0,\N,Missing
W15-3001,2015.eamt-1.17,1,\N,Missing
W15-3001,2012.iwslt-papers.12,0,\N,Missing
W16-2301,W13-3520,0,0.0148252,"Missing"
W16-2301,2011.mtsummit-papers.35,0,0.440323,"Missing"
W16-2301,W15-3001,1,0.419439,"Missing"
W16-2301,W16-2305,0,0.0273073,"Missing"
W16-2301,L16-1662,0,0.00889007,"introduce a new set of features for the sentence-Level QE task. The features extracted include three alignment-based features, three bilingual embedding-based features, two embeddingbased features constrained on alignment links, as well as a set of 74 bigrams used as boolean features. The set of bigrams represents the most frequent bigrams in translations that have changed after the post-edition, and they are compiled by aligning translations to their post-editions provided in the WMT QE datasets. To produce these features, GIZA++ (Och and Ney, 2003) was used for word alignment and Multivec (Berard et al., 2016) was used for the bilingual model, which jointly learns distributed representations for source and target languages using a parallel corpus. To build the bilingual model, domain-specific data compiled from the resources made available for the WMT 16 IT-Domain shared task was used. As prediction model, a Linear Regression model using scikit-learn was built using a combination of QuEst++ baseline features and the new features proposed. tional features for both tasks. UNBABEL (Task 2): Two systems were submitted for the word-level task. UNBABEL 2 linear is a feature-based linear sequential model."
W16-2301,W11-2101,1,0.851508,"Missing"
W16-2301,W16-2306,0,0.0263259,"Missing"
W16-2301,W16-2382,0,0.0451767,"Missing"
W16-2301,W16-2302,1,0.346609,"Missing"
W16-2301,W16-2307,1,0.756182,"Missing"
W16-2301,W16-2308,0,0.0373021,"Missing"
W16-2301,buck-etal-2014-n,0,0.0150538,"Missing"
W16-2301,W13-2201,1,0.287085,"Missing"
W16-2301,W14-3302,1,0.399147,"Missing"
W16-2301,W14-3340,1,0.419045,"Missing"
W16-2301,W07-0718,1,0.759293,"oth automatically and manually. The human evaluation (§3) involves asking human judges to rank sentences output by anonymized systems. We obtained large numbers of rankings from researchers who contributed The quality estimation task had three subtasks, with a total of 14 teams, submitting 39 entries. The automatic post-editing task had a total of 6 teams, submitting 11 entries. 1 Introduction We present the results of the shared tasks of the First Conference on Statistical Machine Translation (WMT) held at ACL 2016. This conference builds on nine previous WMT workshops (Koehn and Monz, 2006; Callison-Burch et al., 2007, 2008, 2009, 2010, 2011, 2012; Bojar et al., 2013, 2014, 2015). 131 Proceedings of the First Conference on Machine Translation, Volume 2: Shared Task Papers, pages 131–198, c Berlin, Germany, August 11-12, 2016. 2016 Association for Computational Linguistics to refine evaluation and estimation methodologies for machine translation. As before, all of the data, translations, and collected human judgments are publicly available.1 We hope these datasets serve as a valuable resource for research into statistical machine translation and automatic evaluation or prediction of translation quality. New"
W16-2301,W15-3025,1,0.888663,"used for the Task 2p. The submissions to the word-level task are modified in order to comply with the phrase-level task. sequences. Nevertheless, it is worth noticing the phraselevel QE systems introduced a number of interesting strategies that allowed them to outperform a strong baseline phrase-level model. Finally, we recall that the evaluation metric – word-level F1 mult – has difficulties to distinguish phrase-level systems. This suggests that we may need to find a different metric for evaluation of the phrase-level task, with phrase-level F1 -mult one of the candidates. 7 pointed out by Chatterjee et al. (2015b), from the application point of view the task is motivated by its possible uses to: • Improve MT output by exploiting information unavailable to the decoder, or by performing deeper text analysis that is too expensive at the decoding stage; • Cope with systematic errors of an MT system whose decoding process is not accessible; • Provide professional translators with improved MT output quality to reduce (human) post-editing effort; Automatic Post-editing Task This year WMT hosted the second round of the shared task on MT automatic post-editing (APE), which consists in automatically correcting"
W16-2301,P15-2026,1,0.895785,"used for the Task 2p. The submissions to the word-level task are modified in order to comply with the phrase-level task. sequences. Nevertheless, it is worth noticing the phraselevel QE systems introduced a number of interesting strategies that allowed them to outperform a strong baseline phrase-level model. Finally, we recall that the evaluation metric – word-level F1 mult – has difficulties to distinguish phrase-level systems. This suggests that we may need to find a different metric for evaluation of the phrase-level task, with phrase-level F1 -mult one of the candidates. 7 pointed out by Chatterjee et al. (2015b), from the application point of view the task is motivated by its possible uses to: • Improve MT output by exploiting information unavailable to the decoder, or by performing deeper text analysis that is too expensive at the decoding stage; • Cope with systematic errors of an MT system whose decoding process is not accessible; • Provide professional translators with improved MT output quality to reduce (human) post-editing effort; Automatic Post-editing Task This year WMT hosted the second round of the shared task on MT automatic post-editing (APE), which consists in automatically correcting"
W16-2301,W08-0309,1,0.809107,"Missing"
W16-2301,W16-2309,0,0.0305938,"Missing"
W16-2301,W10-1703,1,0.312318,"Missing"
W16-2301,W16-2336,0,0.0356983,"Missing"
W16-2301,W12-3102,1,0.235434,"ir native language. Consequently, health professionals may use the translated information to make clinical decisions impacting patients care. It is vital that translation systems do not contribute to the dissemination of incorrect clinical information. Therefore, the evaluation of biomedical translation systems should include an assessment at the document level indicating whether a translation conveyed erroneous clinical information. 6 Quality Estimation The fifth edition of the WMT shared task on quality estimation (QE) of machine translation (MT) builds on the previous editions of the task (Callison-Burch et al., 2012; Bojar et al., 2013, 2014, 2015), with “traditional” tasks at sentence and word levels, a new task for entire documents quality prediction, and a variant of the word-level task: phrase-level estimation. The goals of this year’s shared task were: • To advance work on sentence and wordlevel quality estimation by providing domainspecific, larger and professionally annotated datasets. • To analyse the effectiveness of different types of quality labels provided by humans for longer texts in document-level prediction. • To investigate quality estimation at a new level of granularity: phrases. Plan"
W16-2301,W16-2330,0,0.0328047,"Missing"
W16-2301,W16-2310,1,0.835279,"Missing"
W16-2301,W11-2103,1,0.65163,"Missing"
W16-2301,W16-2331,0,0.0203904,"cance test results for pairs of systems competing in the news domain translation task (en-ru), where a green cell denotes a significantly higher DA adequacy score for the system in a given row over the system in a given column. cs-en fi-en tr-en de-en ru-en ro-en 0.997 0.996 0.988 0.964 0.961 0.920 en-ru 0.975 DA Correlation with RR Table 11: Correlation between overall DA standardized mean adequacy scores and RR Trueskill scores. 150 4 IT Translation Task from the QTLeap project: HF&FCUL for Portuguese, UPV/EHU for Spanish and Basque, IICT-BAS for Bulgarian, CUNI for Czech and UG for Dutch). Duma and Menzel (2016) describe UHDS- DOC 2 VEC and UHBS- LMI (University of Hamburg). Pahari et al. (2016) describe JU-USAAR (Jadavpur University & Saarland University). Cuong et al. (2016) describe ILLC-U VA-S CORPIO (University of Amsterdam). IILC-U VA-DS is based on Hoang and Sima’an (2014). PROMT-RULE - BASED and PROMT-H YBRID systems were submitted by the PROMT LLC company and they are not described in any paper. QTL -M OSES is the standard Moses setup (MERT-tuned on the in-domain training data, but otherwise without any domain-adaptation) and serves as a baseline. The IT-domain translation task introduced th"
W16-2301,W15-3009,1,0.788321,"Missing"
W16-2301,P15-1174,1,0.28154,"endently inserted in another part of this sentence, i.e. to correct an unrelated error. The statistics of the datasets are outlined in Table 20. Evaluation Evaluation was performed against the true HTER label and/or ranking, using the following metrics: • Scoring: Pearson’s r correlation score (primary metric, official score for ranking submissions), Mean Average Error (MAE) and Root Mean Squared Error (RMSE). • Ranking: Spearman’s ρ rank correlation and DeltaAvg. Statistical significance on Pearson r and Spearman rho was computed using the William’s test, following the approach suggested in (Graham, 2015). Results Table 19 summarises the results for Task 1, ranking participating systems best to worst using Pearson’s r correlation as primary key. Spearman’s ρ correlation scores should be used to rank systems according to the ranking variant. We note that three systems have not submitted results ranking evaluation variant. 6.4 Task 2: Predicting word-level quality The goal of this task is to evaluate the extent to which we can detect word-level errors in MT output. Various classes of errors can be found in translations, but for this task we consider all error types together, aiming at making a b"
W16-2301,W16-2311,0,0.0122367,"016) PROMT Automated Translation Solutions (Molchanov and Bykov, 2016) QT21 System Combination (Peter et al., 2016b) RWTH Aachen (Peter et al., 2016a) ¨ TUBITAK (Bektas¸ et al., 2016) University of Edinburgh (Sennrich et al., 2016) University of Edinburgh (Williams et al., 2016) UEDIN - SYNTAX UEDIN - LMU UH -* USFD - RESCORING UUT YSDA ONLINE -[ A , B , F, G ] University of Edinburgh / University of Munich (Huck et al., 2016) University of Helsinki (Tiedemann et al., 2016) University of Sheffield (Blain et al., 2016) Uppsala University (Tiedemann et al., 2016) Yandex School of Data Analysis (Dvorkovich et al., 2016) Four online statistical machine translation systems Table 2: Participants in the shared translation task. Not all teams participated in all language pairs. The translations from the commercial and online systems were not submitted by their respective companies but were obtained by us, and are therefore anonymized in a fashion consistent with previous years of the workshop. 136 Figure 2: Screenshot of the Appraise interface used in the human evaluation campaign. The annotator is presented with a source segment, a reference translation, and up to five outputs from competing systems (anonymized"
W16-2301,W13-2305,1,0.509145,"Missing"
W16-2301,W15-3036,0,0.0603245,"Missing"
W16-2301,E14-1047,1,0.831614,"Missing"
W16-2301,W16-2378,0,0.168663,"ts for pairs of systems competing in the news domain translation task (en-ru), where a green cell denotes a significantly higher DA adequacy score for the system in a given row over the system in a given column. cs-en fi-en tr-en de-en ru-en ro-en 0.997 0.996 0.988 0.964 0.961 0.920 en-ru 0.975 DA Correlation with RR Table 11: Correlation between overall DA standardized mean adequacy scores and RR Trueskill scores. 150 4 IT Translation Task from the QTLeap project: HF&FCUL for Portuguese, UPV/EHU for Spanish and Basque, IICT-BAS for Bulgarian, CUNI for Czech and UG for Dutch). Duma and Menzel (2016) describe UHDS- DOC 2 VEC and UHBS- LMI (University of Hamburg). Pahari et al. (2016) describe JU-USAAR (Jadavpur University & Saarland University). Cuong et al. (2016) describe ILLC-U VA-S CORPIO (University of Amsterdam). IILC-U VA-DS is based on Hoang and Sima’an (2014). PROMT-RULE - BASED and PROMT-H YBRID systems were submitted by the PROMT LLC company and they are not described in any paper. QTL -M OSES is the standard Moses setup (MERT-tuned on the in-domain training data, but otherwise without any domain-adaptation) and serves as a baseline. The IT-domain translation task introduced th"
W16-2301,W11-2123,0,0.0116622,"s were also evaluated against a reimplementation of the approach firstly proposed by Simard et al. (2007).36 Last year, in fact, this statistical post-editing approach represented the common backbone of all submissions (this is also reflected by the close results achieved by participants in the pilot task). For this purpose, a phrasebased SMT system based on Moses (Koehn et al., 2007) was used. Translation and reordering models were estimated following the Moses protocol with default setup using MGIZA++ (Gao and Vogel, 2008) for word alignment. For language modeling we used the KenLM toolkit (Heafield, 2011) for standard n-gram modeling with an n-gram length of 5. Finally, the APE system was tuned on Adam Mickiewicz University. This system is among the very first ones exploring the application of neural translation models to the APE task. In particular, it investigates the following aspects: i) the use of artificially-created postedited data to train the neural models, ii) the loglinear combination of monolingual and bilingual models in an ensemble-like manner, iii) the addition of task-specific features in the log-linear model to control the final output quality. Concerning the data, in addition"
W16-2301,W16-2384,0,0.0416841,"Missing"
W16-2301,W13-0805,0,0.0110515,"of words in the reference. Lower TER values indicate lower distance from the reference as a proxy for higher MT quality. 33 http://www.cs.umd.edu/˜snover/tercom/ 34 https://github.com/moses-smt/mosesdecoder/ blob/master/scripts/generic/multi-bleu.perl 27 The source sentences (together with their reference translations which were not used for the task) were provided by TAUS (https://www.taus.net/) and originally come from a unique IT vendor. 28 It consists of a phrase-based machine translation system leveraging generic and in-domain parallel training data and using a pre-reordering technique (Herrmann et al., 2013). It takes also advantages of POS and word class-based language models. 29 German native speakers working at Text&Form https: //www.textform.com/. 176 Train (12,000) Dev (1,000) Test (2,000) SRC 201,505 17,827 31,477 Tokens TGT 210,573 19,355 34,332 PE 214,720 19,763 35,276 Table 31: Types SRC TGT 9,328 14,185 2,931 3,333 3,908 4,695 Data statistics. APE@WMT15 APE@WMT16 (EN-ES, news, crowd) (EN-DE, IT, prof.) SRC 2.905 6.616 TGT 3.312 8.845 PE 3.085 8.245 Table 32: Repetition Rate (RR) of the WMT15 (EnglishSpanish, news domain, crowdsourced post-edits) and WMT16 (English-German, IT domain, pro"
W16-2301,W16-2315,1,0.824148,"Missing"
W16-2301,P07-2045,1,0.017773,"gets unmodified.35 Baseline results are reported in Table 34. 7.2 Participants Monolingual translation as another term of comparison. To get some insights about the progress with respect to the first pilot task, participating systems were also evaluated against a reimplementation of the approach firstly proposed by Simard et al. (2007).36 Last year, in fact, this statistical post-editing approach represented the common backbone of all submissions (this is also reflected by the close results achieved by participants in the pilot task). For this purpose, a phrasebased SMT system based on Moses (Koehn et al., 2007) was used. Translation and reordering models were estimated following the Moses protocol with default setup using MGIZA++ (Gao and Vogel, 2008) for word alignment. For language modeling we used the KenLM toolkit (Heafield, 2011) for standard n-gram modeling with an n-gram length of 5. Finally, the APE system was tuned on Adam Mickiewicz University. This system is among the very first ones exploring the application of neural translation models to the APE task. In particular, it investigates the following aspects: i) the use of artificially-created postedited data to train the neural models, ii)"
W16-2301,W16-2337,0,0.0376798,"Missing"
W16-2301,W16-2303,1,0.821661,"Missing"
W16-2301,L16-1582,1,0.790825,"Missing"
W16-2301,W16-2385,0,0.0304494,"Missing"
W16-2301,P16-2095,1,0.885979,"e package to extract these and other quality estimation features and perform model learning, with suggested methods for all levels of prediction. Participants, described in Section 6.2, could submit up to two systems for each task. Data used to build MT systems or internal system information (such as model scores or n-best lists) were made available on request for Tasks 1 and 2. 6.1 The γ,  and C parameters were optimised via grid search with 5-fold cross validation on the training set. Word-level baseline system: For Tasks 2 and 2p, the baseline features were extracted with the Marmot tool (Logacheva et al., 2016b). For the baseline system we used a number of features that have been found the most informative in previous research on word-level QE. Our baseline set of features is loosely based on the one described in (Luong et al., 2014). It contains the following 22 features: • Word count in the source and target sentences, source and target token count ratio. Although these features are sentence-level (i.e. their values will be the same for all words in a sentence), the length of a sentence might influence the probability of a word being wrong. Baseline systems Sentence-level baseline system: For Tas"
W16-2301,W15-3037,0,0.00849978,"stems were submitted for the word-level task. UNBABEL 2 linear is a feature-based linear sequential model. It uses the baseline features provided by the shared task organisers (with slight changes) conjoined with individual labels and pairs of consecutive labels. It also uses various syntactic dependency-based features (dependency relations, heads, and second-order structures like siblings and grandparents). The syntactic dependencies are predicted with TurboParser trained on the TIGER German treebank. UNBABEL 2 ensemble uses a stacked architecture, inspired by the last year’s QUETCH+ system (Kreutzer et al., 2015), which combines three neural systems: one feedforward and two recurrent ones. The predictions of these systems are added as additional features in the linear system above. The following external resources were used: part-of-speech tags and extra syntactic dependency information obtained with TurboTagger and TurboParser (Martins et al., 2013), trained on the Penn Treebank (for English) and on the version of the German TIGER corpus used in the SPMRL shared task (Seddah et al., 2014) for German. For the neural models, pretrained word embeddings from Polyglot (AlRfou et al., 2013) and those produ"
W16-2301,W14-3342,0,0.0237077,"Data used to build MT systems or internal system information (such as model scores or n-best lists) were made available on request for Tasks 1 and 2. 6.1 The γ,  and C parameters were optimised via grid search with 5-fold cross validation on the training set. Word-level baseline system: For Tasks 2 and 2p, the baseline features were extracted with the Marmot tool (Logacheva et al., 2016b). For the baseline system we used a number of features that have been found the most informative in previous research on word-level QE. Our baseline set of features is loosely based on the one described in (Luong et al., 2014). It contains the following 22 features: • Word count in the source and target sentences, source and target token count ratio. Although these features are sentence-level (i.e. their values will be the same for all words in a sentence), the length of a sentence might influence the probability of a word being wrong. Baseline systems Sentence-level baseline system: For Task 1, QuEst++13 (Specia et al., 2015) was used to extract 17 features from the SMT source/target language training corpus: • Number of tokens in source & target sentences. • Target token, its left and right contexts of one word."
W16-2301,W16-2318,0,0.0145349,"Missing"
W16-2301,N06-1014,0,0.0215966,"d on n previous translation and reordering decisions. This technique is able to model both local and long-range reorderings that are quite useful when dealing with the German language. To improve the capability of choosing the correct edit to process, eight new features are added to the loglinear model. These features capture the cost of deleting a phrase and different information on possible gaps in reordering operations. The monolingual alignments between the MT outputs and their post-edits are computed using different methods based on TER, METEOR (Snover et al., 2006) and Berkeley Aligner (Liang et al., 2006). Only the task data is used for these submissions. 7.3 179 TER/BLEU results ID AMU Primary AMU Contrastive FBK Contrastive FBK Primary USAAR Primary USAAR Constrastive CUNI Primary (Simard et al., 2007) Baseline DCU Contrastive JUSAAR Primary JUSAAR Contrastive DCU Primary Avg. TER 21.52 23.06 23.92 23.94 24.14 24.14 24.31 24.64 24.76 26.79 26.92 26.97 28.97 BLEU 67.65 66.09 64.75 64.75 64.10 64.00 63.32 63.47 62.11 58.60 59.44 59.18 55.19 tained this year by the top runs can only be reached by moving from the basic statistical MT backbone shared by all last year’s participants to new and mor"
W16-2301,P13-2109,0,0.0179517,"eads, and second-order structures like siblings and grandparents). The syntactic dependencies are predicted with TurboParser trained on the TIGER German treebank. UNBABEL 2 ensemble uses a stacked architecture, inspired by the last year’s QUETCH+ system (Kreutzer et al., 2015), which combines three neural systems: one feedforward and two recurrent ones. The predictions of these systems are added as additional features in the linear system above. The following external resources were used: part-of-speech tags and extra syntactic dependency information obtained with TurboTagger and TurboParser (Martins et al., 2013), trained on the Penn Treebank (for English) and on the version of the German TIGER corpus used in the SPMRL shared task (Seddah et al., 2014) for German. For the neural models, pretrained word embeddings from Polyglot (AlRfou et al., 2013) and those produced with a neural MT system (Bahdanau et al., 2014) were used. UGENT-LT3 (Task 1, Task 2): The submissions for the word-level task use 41 features in combination with the baseline feature set to train binary classifiers. The 41 additional features attempt to capture accuracy errors (concerned with the meaning transfer from the source to targe"
W16-2301,W16-2387,0,0.0253511,"Missing"
W16-2301,W16-2317,0,0.0414065,"Missing"
W16-2301,N13-1090,0,0.00833788,"Missing"
W16-2301,2015.iwslt-papers.4,1,0.737239,"Missing"
W16-2301,W16-2319,0,0.0339318,"Missing"
W16-2301,W16-2386,1,0.814188,"e package to extract these and other quality estimation features and perform model learning, with suggested methods for all levels of prediction. Participants, described in Section 6.2, could submit up to two systems for each task. Data used to build MT systems or internal system information (such as model scores or n-best lists) were made available on request for Tasks 1 and 2. 6.1 The γ,  and C parameters were optimised via grid search with 5-fold cross validation on the training set. Word-level baseline system: For Tasks 2 and 2p, the baseline features were extracted with the Marmot tool (Logacheva et al., 2016b). For the baseline system we used a number of features that have been found the most informative in previous research on word-level QE. Our baseline set of features is loosely based on the one described in (Luong et al., 2014). It contains the following 22 features: • Word count in the source and target sentences, source and target token count ratio. Although these features are sentence-level (i.e. their values will be the same for all words in a sentence), the length of a sentence might influence the probability of a word being wrong. Baseline systems Sentence-level baseline system: For Tas"
W16-2301,L16-1470,1,0.826657,"Missing"
W16-2301,P03-1021,0,0.0340784,"7 Tokens TGT 210,573 19,355 34,332 PE 214,720 19,763 35,276 Table 31: Types SRC TGT 9,328 14,185 2,931 3,333 3,908 4,695 Data statistics. APE@WMT15 APE@WMT16 (EN-ES, news, crowd) (EN-DE, IT, prof.) SRC 2.905 6.616 TGT 3.312 8.845 PE 3.085 8.245 Table 32: Repetition Rate (RR) of the WMT15 (EnglishSpanish, news domain, crowdsourced post-edits) and WMT16 (English-German, IT domain, professional posteditors) APE Task data. 7.1.3 PE 16,388 3,506 5,047 SRC 5,628 1,922 2,479 Lemmas TGT PE 11,418 13,244 2,686 2,806 3,753 4,050 the development set, optimizing TER/BLEU with Minimum Error Rate Training (Och, 2003). The results of this additional term of comparison are also reported in Table 34. For each submitted run, the statistical significance of performance differences with respect to the baselines and the re-implementation of Simard et al. (2007) was calculated with the bootstrap test (Koehn, 2004). Baseline The official baseline results are the TER and BLEU scores calculated by comparing the raw MT output with the human post-edits. In practice, the baseline APE system is a system that leaves all the test targets unmodified.35 Baseline results are reported in Table 34. 7.2 Participants Monolingual"
W16-2301,W16-2338,0,0.0374669,"Missing"
W16-2301,J03-1002,0,0.0268639,"word alignments and bilingual distributed representations to introduce a new set of features for the sentence-Level QE task. The features extracted include three alignment-based features, three bilingual embedding-based features, two embeddingbased features constrained on alignment links, as well as a set of 74 bigrams used as boolean features. The set of bigrams represents the most frequent bigrams in translations that have changed after the post-edition, and they are compiled by aligning translations to their post-editions provided in the WMT QE datasets. To produce these features, GIZA++ (Och and Ney, 2003) was used for word alignment and Multivec (Berard et al., 2016) was used for the bilingual model, which jointly learns distributed representations for source and target languages using a parallel corpus. To build the bilingual model, domain-specific data compiled from the resources made available for the WMT 16 IT-Domain shared task was used. As prediction model, a Linear Regression model using scikit-learn was built using a combination of QuEst++ baseline features and the new features proposed. tional features for both tasks. UNBABEL (Task 2): Two systems were submitted for the word-level tas"
W16-2301,W16-2321,0,0.0372138,"Missing"
W16-2301,W16-2388,1,0.748184,"Missing"
W16-2301,W16-2333,0,0.0259256,"Missing"
W16-2301,W15-3026,0,0.0339859,"Missing"
W16-2301,W16-2379,1,0.819672,"Missing"
W16-2301,W16-2334,1,0.811656,"Missing"
W16-2301,P02-1040,0,0.105557,", eventually, make the APE task more feasible by automatic systems. Other changes concern the language combination and the evaluation mode. As regards the languages, we moved from English-Spanish to English-German, which is one of the language pairs covered by the QT21 Project26 that supported data collection and post-editing. Concerning the evaluation, we changed from TER scores computed both in case-sensitive and caseinsensitive mode to a single ranking based on case sensitive measurements. Besides these changes the new round of the APE task included some extensions in the evaluation. BLEU (Papineni et al., 2002) has been introduced as a secondary evaluation metric to measure the improvements over the rough MT output. In addition, to gain further insights on final output quality, a subset of the outputs of the submitted systems has also been manually evaluated. Based on these changes and extensions, the goals of this year’s shared task were to: i) improve and stabilize the evaluation framework in view of future rounds, ii) analyze the effect on task 26 feasibility of data coming from a narrow domain, iii) analyze the effect of post-edits collected from professional translators, iv) analyze how humans"
W16-2301,W16-2389,0,0.0210658,"Missing"
W16-2301,W16-2390,0,0.0283246,"Missing"
W16-2301,W16-2322,0,0.0139204,"ng and evaluating the manual translations has settled into the following pattern. We ask human annotators to rank the outputs of five systems. From these rankings, we produce pairwise translation comparisons, and then evaluate them with a version of the TrueSkill algorithm adapted to our task. We refer to this approach (described in Section 3.4) as the relative ranking approach (RR), so named because the pairwise comparisons denote only relative ability between a pair of systems, and cannot be used to infer their absolute quality. These results are used to produce the official ranking for the WMT 2016 tasks. However, work in evaluation over the past few years has provided fresh insight into ways to collect direct assessments (DA) of machine translation quality. In this setting, annotators are asked to provide an assessment of the direct quality of the output of a system relative to a reference translation. In order to evaluate the potential of this approach for future WMT evaluations, we conducted a direct assessment evaluation in parallel. This evaluation, together with a comparison of the official results, is described in Section 3.5. 2.3 3.1 2.2 Training data As in past years we provide"
W16-2301,W16-2392,1,0.772695,"Missing"
W16-2301,L16-1649,1,0.704564,"): one for the 17 baseline features and another for the 500 features from the embeddings. Since each kernel has its own set of hyperparameters, the full model can leverage the contributions from the two different sets. The second system (GRAPH-DISC) combines the baseline features with discourse-aware features. The discourse aware features are the same as the ones used by Scarton et al. (2015a) plus Latent Semantic Analysis (LSA) cohesion features (Scarton and Specia, 2014), number of subtrees and height of the Rhetorical Structure Theory (RST) tree and entity graph-based coherence scores (Sim Smith et al., 2016). Discourseaware and RST tree features were extracted only for English (tools are only available for this language), LSA features were extracted for both languages, and entity graph-based coherence scores were extracted for the target language only (Spanish), as the source documents are expected to be coherent. This QE model was trained with an SVR algorithm. YSDA (Task 1): The YSDA submission is based on a simple idea that the more complex the sentence is the more difficult it is to translate. For this purpose, it uses information provided by syntactic parsing (information from parsing trees,"
W16-2301,W16-2391,1,0.796337,"Missing"
W16-2301,2014.eamt-1.21,1,0.643048,"e extracted by averaging word embeddings in the document. The GP model was trained with two Rational Quadratic kernels (Rasmussen and Williams, 2006): one for the 17 baseline features and another for the 500 features from the embeddings. Since each kernel has its own set of hyperparameters, the full model can leverage the contributions from the two different sets. The second system (GRAPH-DISC) combines the baseline features with discourse-aware features. The discourse aware features are the same as the ones used by Scarton et al. (2015a) plus Latent Semantic Analysis (LSA) cohesion features (Scarton and Specia, 2014), number of subtrees and height of the Rhetorical Structure Theory (RST) tree and entity graph-based coherence scores (Sim Smith et al., 2016). Discourseaware and RST tree features were extracted only for English (tools are only available for this language), LSA features were extracted for both languages, and entity graph-based coherence scores were extracted for the target language only (Spanish), as the source documents are expected to be coherent. This QE model was trained with an SVR algorithm. YSDA (Task 1): The YSDA submission is based on a simple idea that the more complex the sentence"
W16-2301,W15-3040,1,0.909092,"on-word corpus,18 with a vocabulary size of 527K words. Document embeddings are extracted by averaging word embeddings in the document. The GP model was trained with two Rational Quadratic kernels (Rasmussen and Williams, 2006): one for the 17 baseline features and another for the 500 features from the embeddings. Since each kernel has its own set of hyperparameters, the full model can leverage the contributions from the two different sets. The second system (GRAPH-DISC) combines the baseline features with discourse-aware features. The discourse aware features are the same as the ones used by Scarton et al. (2015a) plus Latent Semantic Analysis (LSA) cohesion features (Scarton and Specia, 2014), number of subtrees and height of the Rhetorical Structure Theory (RST) tree and entity graph-based coherence scores (Sim Smith et al., 2016). Discourseaware and RST tree features were extracted only for English (tools are only available for this language), LSA features were extracted for both languages, and entity graph-based coherence scores were extracted for the target language only (Spanish), as the source documents are expected to be coherent. This QE model was trained with an SVR algorithm. YSDA (Task 1)"
W16-2301,2006.amta-papers.25,0,0.850121,"ombinations of several features. A regression model was training to predict BLEU as target metric instead HTER. The machine learning pipeline uses an SVR with RBF kernel to predict BLEU scores, followed by a linear SVR to predict HTER scores from BLEU scores. As external resources, the system uses a syntactic parser, pseudo-references and back-translation from web-scale MT system, and a web-scale language model. 6.3 Task 1: Predicting sentence-level quality This task consists in scoring (and ranking) translation sentences according to the percentage of their words that need to be fixed. HTER (Snover et al., 2006) is used as quality score, i.e. the minimum edit distance between the machine translation and its manually post-edited version in [0,1]. As in previous years, two variants of the results could be submitted: • Scoring: An absolute HTER score for each sentence translation, to be interpreted as an error metric: lower scores mean better translations. • Ranking: A ranking of sentence translations for all source sentences from best to worst. For this variant, it does not matter how the ranking is produced (from HTER predictions or by other means). The reference ranking is defined based on the true H"
W16-2301,W15-4916,1,0.824025,"Missing"
W16-2301,W16-2346,1,0.0602876,"Missing"
W16-2301,W16-2325,1,0.816227,"Missing"
W16-2301,W16-2393,0,0.033041,"Missing"
W16-2301,W16-2326,0,0.0221449,"Missing"
W16-2301,W16-2327,1,0.737603,"Missing"
W16-2301,W16-2328,0,0.0403946,"Missing"
W16-2301,C00-2137,0,0.0384283,"Missing"
W16-2301,D07-1091,1,\N,Missing
W16-2301,W09-0401,1,\N,Missing
W16-2301,P11-1105,0,\N,Missing
W16-2301,N07-1064,0,\N,Missing
W16-2301,W04-3250,1,\N,Missing
W16-2301,P15-4020,1,\N,Missing
W16-2301,W16-2377,1,\N,Missing
W16-2301,aziz-etal-2012-pet,1,\N,Missing
W16-2301,2012.eamt-1.31,0,\N,Missing
W16-2301,C14-1182,0,\N,Missing
W16-2301,W16-2316,0,\N,Missing
W16-2301,W16-2332,1,\N,Missing
W16-2301,W16-2320,1,\N,Missing
W16-2301,W16-2335,0,\N,Missing
W16-2301,W16-2329,0,\N,Missing
W16-2301,W16-2383,0,\N,Missing
W16-2301,W16-2381,1,\N,Missing
W16-2301,W16-2312,0,\N,Missing
W16-2301,P16-1162,1,\N,Missing
W16-2301,W13-2243,1,\N,Missing
W16-2301,W08-0509,0,\N,Missing
W16-2301,2015.eamt-1.17,1,\N,Missing
W16-2301,W14-6111,0,\N,Missing
W16-2301,2012.tc-1.5,1,\N,Missing
W16-2301,W16-2347,1,\N,Missing
W16-2301,W16-2314,0,\N,Missing
W16-2310,P14-1129,0,0.0580228,"Missing"
W16-2310,P13-2071,1,0.895955,"e investigated include: neural probabilistic language models, bilingual neural network language models, morphological segmentation, and the attentionbased neural machine translation model as reranking feature. 1 2.1 Basic Configuration We trained our systems with the following settings: a maximum sentence length of 80, growdiag-final-and symmetrization of GIZA++ alignments, an interpolated Kneser-Ney smoothed 5gram language model with KenLM (Heafield, 2011) used at runtime, hierarchical lexicalized reordering (Galley and Manning, 2008), a lexicallydriven 5-gram operation sequence model (OSM) (Durrani et al., 2013) with 4 count-based supportive features, sparse domain indicator, phrase length, and count bin features (Blunsom and Osborne, 2008; Chiang et al., 2009), a distortion limit of 6, maximum phrase-length of 5, 100-best translation options, compact phrase table (JunczysDowmunt, 2012) minimum Bayes risk decoding (Kumar and Byrne, 2004), cube pruning (Huang and Chiang, 2007), with a stack-size of 1000 during tuning and 5000 during test and the noreordering-over-punctuation heuristic (Koehn and Haddow, 2009). We optimize feature function weights with k-best MIRA (Cherry and Foster, 2012). We used POS"
W16-2310,D08-1023,0,0.228814,"tion, and the attentionbased neural machine translation model as reranking feature. 1 2.1 Basic Configuration We trained our systems with the following settings: a maximum sentence length of 80, growdiag-final-and symmetrization of GIZA++ alignments, an interpolated Kneser-Ney smoothed 5gram language model with KenLM (Heafield, 2011) used at runtime, hierarchical lexicalized reordering (Galley and Manning, 2008), a lexicallydriven 5-gram operation sequence model (OSM) (Durrani et al., 2013) with 4 count-based supportive features, sparse domain indicator, phrase length, and count bin features (Blunsom and Osborne, 2008; Chiang et al., 2009), a distortion limit of 6, maximum phrase-length of 5, 100-best translation options, compact phrase table (JunczysDowmunt, 2012) minimum Bayes risk decoding (Kumar and Byrne, 2004), cube pruning (Huang and Chiang, 2007), with a stack-size of 1000 during tuning and 5000 during test and the noreordering-over-punctuation heuristic (Koehn and Haddow, 2009). We optimize feature function weights with k-best MIRA (Cherry and Foster, 2012). We used POS and morphological tags as additional factors in phrase translation models (Koehn and Hoang, 2007) for the German-English language"
W16-2310,D08-1089,0,0.0787452,"ll 12 language pairs of this year’s evaluation campaign. Novel research directions we investigated include: neural probabilistic language models, bilingual neural network language models, morphological segmentation, and the attentionbased neural machine translation model as reranking feature. 1 2.1 Basic Configuration We trained our systems with the following settings: a maximum sentence length of 80, growdiag-final-and symmetrization of GIZA++ alignments, an interpolated Kneser-Ney smoothed 5gram language model with KenLM (Heafield, 2011) used at runtime, hierarchical lexicalized reordering (Galley and Manning, 2008), a lexicallydriven 5-gram operation sequence model (OSM) (Durrani et al., 2013) with 4 count-based supportive features, sparse domain indicator, phrase length, and count bin features (Blunsom and Osborne, 2008; Chiang et al., 2009), a distortion limit of 6, maximum phrase-length of 5, 100-best translation options, compact phrase table (JunczysDowmunt, 2012) minimum Bayes risk decoding (Kumar and Byrne, 2004), cube pruning (Huang and Chiang, 2007), with a stack-size of 1000 during tuning and 5000 during test and the noreordering-over-punctuation heuristic (Koehn and Haddow, 2009). We optimize"
W16-2310,buck-etal-2014-n,0,0.124801,"Missing"
W16-2310,W15-3013,1,0.821897,"l language pairs, the use of all fea275 Language Turkish Turkish Turkish Turkish Turkish Turkish English Threshold none 0 2 5 10 20 none Token count 8806 9606 9935 10169 10416 10720 11514 Method baseline Byte-Pair Chipmunk Chipmunk Chipmunk Chipmunk Morfessor Morfessor Morfessor Morfessor Morfessor Table 5: Token counts for different thresholds for Morfessor segmentation consecutive bytes with a symbol that does not occur elsewhere. Each such replacement is called a merge, and the number of merges is a tunable parameter. The original text can be recovered using a lookup-table. Sennrich et al. (2015) applied this to word segmentation, and demonstrate its success at solving the large vocabulary problem in neural machine translation. To create our training data for the Morfessor and ChipMunk experiments, we augment the original training data with a second copy that has been segmented. For the tuning and test data, we only segment words that occur infrequently. This allows frequent words to be translated directly, but also allows the system to learn from the subword units of all words, including frequent ones. The number and type of subword units in each word segmented by byte pair encoding"
W16-2310,N12-1047,0,0.270491,"ce model (OSM) (Durrani et al., 2013) with 4 count-based supportive features, sparse domain indicator, phrase length, and count bin features (Blunsom and Osborne, 2008; Chiang et al., 2009), a distortion limit of 6, maximum phrase-length of 5, 100-best translation options, compact phrase table (JunczysDowmunt, 2012) minimum Bayes risk decoding (Kumar and Byrne, 2004), cube pruning (Huang and Chiang, 2007), with a stack-size of 1000 during tuning and 5000 during test and the noreordering-over-punctuation heuristic (Koehn and Haddow, 2009). We optimize feature function weights with k-best MIRA (Cherry and Foster, 2012). We used POS and morphological tags as additional factors in phrase translation models (Koehn and Hoang, 2007) for the German-English language pairs. We also trained target sequence models on the in-domain subset of the parallel corpus using Kneser-Ney smoothed 7-gram models. We used syntactic preordering (Collins et al., 2005) Introduction The JHU 2016 WMT submission consists of phrase-based systems, hierarchical phrase-based systems, and syntax-based systems. In this paper we discuss features that we integrated into our system submissions. We also discuss the experiments we did with morphol"
W16-2310,W11-2123,0,0.143406,"phrase-based, hierarchical phrase-based and syntax-based systems for all 12 language pairs of this year’s evaluation campaign. Novel research directions we investigated include: neural probabilistic language models, bilingual neural network language models, morphological segmentation, and the attentionbased neural machine translation model as reranking feature. 1 2.1 Basic Configuration We trained our systems with the following settings: a maximum sentence length of 80, growdiag-final-and symmetrization of GIZA++ alignments, an interpolated Kneser-Ney smoothed 5gram language model with KenLM (Heafield, 2011) used at runtime, hierarchical lexicalized reordering (Galley and Manning, 2008), a lexicallydriven 5-gram operation sequence model (OSM) (Durrani et al., 2013) with 4 count-based supportive features, sparse domain indicator, phrase length, and count bin features (Blunsom and Osborne, 2008; Chiang et al., 2009), a distortion limit of 6, maximum phrase-length of 5, 100-best translation options, compact phrase table (JunczysDowmunt, 2012) minimum Bayes risk decoding (Kumar and Byrne, 2004), cube pruning (Huang and Chiang, 2007), with a stack-size of 1000 during tuning and 5000 during test and th"
W16-2310,P07-1019,0,0.143673,"nments, an interpolated Kneser-Ney smoothed 5gram language model with KenLM (Heafield, 2011) used at runtime, hierarchical lexicalized reordering (Galley and Manning, 2008), a lexicallydriven 5-gram operation sequence model (OSM) (Durrani et al., 2013) with 4 count-based supportive features, sparse domain indicator, phrase length, and count bin features (Blunsom and Osborne, 2008; Chiang et al., 2009), a distortion limit of 6, maximum phrase-length of 5, 100-best translation options, compact phrase table (JunczysDowmunt, 2012) minimum Bayes risk decoding (Kumar and Byrne, 2004), cube pruning (Huang and Chiang, 2007), with a stack-size of 1000 during tuning and 5000 during test and the noreordering-over-punctuation heuristic (Koehn and Haddow, 2009). We optimize feature function weights with k-best MIRA (Cherry and Foster, 2012). We used POS and morphological tags as additional factors in phrase translation models (Koehn and Hoang, 2007) for the German-English language pairs. We also trained target sequence models on the in-domain subset of the parallel corpus using Kneser-Ney smoothed 7-gram models. We used syntactic preordering (Collins et al., 2005) Introduction The JHU 2016 WMT submission consists of"
W16-2310,N09-1025,0,0.0734239,"Missing"
W16-2310,2012.eamt-1.58,0,0.410346,"Missing"
W16-2310,P05-1066,1,0.655469,"isk decoding (Kumar and Byrne, 2004), cube pruning (Huang and Chiang, 2007), with a stack-size of 1000 during tuning and 5000 during test and the noreordering-over-punctuation heuristic (Koehn and Haddow, 2009). We optimize feature function weights with k-best MIRA (Cherry and Foster, 2012). We used POS and morphological tags as additional factors in phrase translation models (Koehn and Hoang, 2007) for the German-English language pairs. We also trained target sequence models on the in-domain subset of the parallel corpus using Kneser-Ney smoothed 7-gram models. We used syntactic preordering (Collins et al., 2005) Introduction The JHU 2016 WMT submission consists of phrase-based systems, hierarchical phrase-based systems, and syntax-based systems. In this paper we discuss features that we integrated into our system submissions. We also discuss the experiments we did with morphological pre-processing and neural reranking. The JHU phrase-based translation systems for our participation in the WMT 2016 shared translation task1 are based on the open source Moses toolkit (Koehn et al., 2007). We built upon strong baselines of the Edinburgh-JHU joint WMT submissions from the last year (Haddow et al., 2015), t"
W16-2310,K15-1017,0,0.0842479,"Missing"
W16-2310,D07-1091,1,0.808965,"gth, and count bin features (Blunsom and Osborne, 2008; Chiang et al., 2009), a distortion limit of 6, maximum phrase-length of 5, 100-best translation options, compact phrase table (JunczysDowmunt, 2012) minimum Bayes risk decoding (Kumar and Byrne, 2004), cube pruning (Huang and Chiang, 2007), with a stack-size of 1000 during tuning and 5000 during test and the noreordering-over-punctuation heuristic (Koehn and Haddow, 2009). We optimize feature function weights with k-best MIRA (Cherry and Foster, 2012). We used POS and morphological tags as additional factors in phrase translation models (Koehn and Hoang, 2007) for the German-English language pairs. We also trained target sequence models on the in-domain subset of the parallel corpus using Kneser-Ney smoothed 7-gram models. We used syntactic preordering (Collins et al., 2005) Introduction The JHU 2016 WMT submission consists of phrase-based systems, hierarchical phrase-based systems, and syntax-based systems. In this paper we discuss features that we integrated into our system submissions. We also discuss the experiments we did with morphological pre-processing and neural reranking. The JHU phrase-based translation systems for our participation in t"
W16-2310,E03-1076,1,0.762597,"ny, August 11-12, 2016. 2016 Association for Computational Linguistics Language Pair German–English Czech–English Finnish–English Romanian–English Russian-English Turkish–English Language Czech German English Finnish Romanian Russian Turkish Sentences 19,074 19,074 1,500 943 9,006 500 Tokens 6.7 billion 65.2 billion 65.1 billion 2.9 billion 8.1 billion 23.3 billion 11.9 billion LM Size 13GB 107GB 89GB 8GB 13GB 41GB 23GB Table 1: Tuning set sizes for phrase-based system Table 2: Sizes of the language model trained on the monomlingual corpora extracted from Common Crawl. and compound splitting (Koehn and Knight, 2003) for the German-to-English systems. We did no language-specific processing for any other language. The systems were tuned on a very large tuning set consisting of the test sets from 2008-2014, with a total of 19,074 sentences (see Table 1). We used news-test 2015 as development test set. Significantly less tuning data was available for Finnish, Romanian, and Turkish. 2.2 2.3 Huge Language Model This year, large corpora of monolingual data were extracted from Common Crawl (Buck et al., 2014). We used this data to train 5-gram KneserNey smoothed language models, pruning out 3– 5 gram singletons."
W16-2310,P07-2045,1,0.0124021,"the in-domain subset of the parallel corpus using Kneser-Ney smoothed 7-gram models. We used syntactic preordering (Collins et al., 2005) Introduction The JHU 2016 WMT submission consists of phrase-based systems, hierarchical phrase-based systems, and syntax-based systems. In this paper we discuss features that we integrated into our system submissions. We also discuss the experiments we did with morphological pre-processing and neural reranking. The JHU phrase-based translation systems for our participation in the WMT 2016 shared translation task1 are based on the open source Moses toolkit (Koehn et al., 2007). We built upon strong baselines of the Edinburgh-JHU joint WMT submissions from the last year (Haddow et al., 2015), the Edinburgh syntax-based system submissions from the last year (Williams et al., 2015) as well as recent research in the field (Vaswani et al., 2013; Devlin et al., 2014). We also used the Apache Joshua translation toolkit (Post et al., 2015) to build hierarchical systems for two language tasks. 1 Moses Phrase-Based Systems http://www.statmt.org/wmt16 272 Proceedings of the First Conference on Machine Translation, Volume 2: Shared Task Papers, pages 272–280, c Berlin, Germany"
W16-2310,N04-1022,0,0.203242,"inal-and symmetrization of GIZA++ alignments, an interpolated Kneser-Ney smoothed 5gram language model with KenLM (Heafield, 2011) used at runtime, hierarchical lexicalized reordering (Galley and Manning, 2008), a lexicallydriven 5-gram operation sequence model (OSM) (Durrani et al., 2013) with 4 count-based supportive features, sparse domain indicator, phrase length, and count bin features (Blunsom and Osborne, 2008; Chiang et al., 2009), a distortion limit of 6, maximum phrase-length of 5, 100-best translation options, compact phrase table (JunczysDowmunt, 2012) minimum Bayes risk decoding (Kumar and Byrne, 2004), cube pruning (Huang and Chiang, 2007), with a stack-size of 1000 during tuning and 5000 during test and the noreordering-over-punctuation heuristic (Koehn and Haddow, 2009). We optimize feature function weights with k-best MIRA (Cherry and Foster, 2012). We used POS and morphological tags as additional factors in phrase translation models (Koehn and Hoang, 2007) for the German-English language pairs. We also trained target sequence models on the in-domain subset of the parallel corpus using Kneser-Ney smoothed 7-gram models. We used syntactic preordering (Collins et al., 2005) Introduction T"
W16-2310,E99-1010,0,0.314445,"lz, as we did all other language models. We compressed the language models with KenLM with 4-bit quantization and use of the trie data structure. The resulting size of the language model is listed in Table 2. The largest language model is the German model at 107GB, trained on 65.2 billion tokens, about an order of magnitude larger than previous data. Och Clusters As in last year’s system, we use word classes in four feature functions: (i) the language model, (ii) the operation sequence model, (iii) the reordering model, and the (iv) sparse word translation features. We generated Och clusters (Och, 1999) — a variant of Brown clusters — using mkcls. We have to choose a hyper parameter: the number of clusters. Our experiments and also prior work (Stewart et al., 2014) suggest that instead of committing to a single value, it is beneficial to use multiple numbers and use them in multiple feature functions concurrently. We used 50, 200, 600, and 2000 clusters, hence having 4 additional interpolated language models, 4 additional operation sequence models, 4 additional lexicalized reordering models, and 4 additional sets of sparse features. The feature functions for word classes were trained exactly"
W16-2310,2014.amta-researchers.3,0,0.0345605,"lting size of the language model is listed in Table 2. The largest language model is the German model at 107GB, trained on 65.2 billion tokens, about an order of magnitude larger than previous data. Och Clusters As in last year’s system, we use word classes in four feature functions: (i) the language model, (ii) the operation sequence model, (iii) the reordering model, and the (iv) sparse word translation features. We generated Och clusters (Och, 1999) — a variant of Brown clusters — using mkcls. We have to choose a hyper parameter: the number of clusters. Our experiments and also prior work (Stewart et al., 2014) suggest that instead of committing to a single value, it is beneficial to use multiple numbers and use them in multiple feature functions concurrently. We used 50, 200, 600, and 2000 clusters, hence having 4 additional interpolated language models, 4 additional operation sequence models, 4 additional lexicalized reordering models, and 4 additional sets of sparse features. The feature functions for word classes were trained exactly the same way as the corresponding feature functions for words. For instance, this means that the word class language model required training of individual models on"
W16-2310,D13-1140,0,0.0599603,"ystems. In this paper we discuss features that we integrated into our system submissions. We also discuss the experiments we did with morphological pre-processing and neural reranking. The JHU phrase-based translation systems for our participation in the WMT 2016 shared translation task1 are based on the open source Moses toolkit (Koehn et al., 2007). We built upon strong baselines of the Edinburgh-JHU joint WMT submissions from the last year (Haddow et al., 2015), the Edinburgh syntax-based system submissions from the last year (Williams et al., 2015) as well as recent research in the field (Vaswani et al., 2013; Devlin et al., 2014). We also used the Apache Joshua translation toolkit (Post et al., 2015) to build hierarchical systems for two language tasks. 1 Moses Phrase-Based Systems http://www.statmt.org/wmt16 272 Proceedings of the First Conference on Machine Translation, Volume 2: Shared Task Papers, pages 272–280, c Berlin, Germany, August 11-12, 2016. 2016 Association for Computational Linguistics Language Pair German–English Czech–English Finnish–English Romanian–English Russian-English Turkish–English Language Czech German English Finnish Romanian Russian Turkish Sentences 19,074 19,074 1,50"
W16-2310,W09-0429,1,\N,Missing
W16-2310,P03-1020,0,\N,Missing
W16-2310,P16-1162,0,\N,Missing
W16-2310,W14-3324,1,\N,Missing
W17-4717,W17-4758,0,0.0835228,"ese submissions investigate alternative machine learning models for the prediction of the HTER score on the sentencelevel task. Instead of directly predicting the HTER score, the systems use a single-layer perceptron with four outputs that jointly predict the number of each of the four distinct post-editing operations that are then used to calculate the HTER score. This also gives the possibility to correct invalid (e.g. negative) predicted values prior to the calculation of the HTER score. The two submissions use the baseline features and the EnglishGerman submission also uses features from (Avramidis, 2017a). JXNU (T1): The JXNU submissions use features extracted from a neural network, including embedding features and cross-entropy features of the source sentences and their machine translations. The sentence embedding features are extracted through global average pooling from word embedding, which are trained using the WORD 2 VEC toolkit. The sentence cross-entropy features are calculated by a recurrent neural network language model. They experimented with different sentence embedding dimensions of the source sentences and translation outputs, as well as different sizes of the training corpus."
W17-4717,W17-4772,0,0.0310859,"Missing"
W17-4717,W17-4759,0,0.0327025,"Missing"
W17-4717,L16-1356,1,0.770826,"or BAD. The ﬁrst two matrices are built from 300 dimension word vectors computed with pretrained in-domain word embeddings. They train their model over 100 iterations with the l2 norm as regulariser and using the forward-backward splitting algorithm (FOBOS) (Duchi and Singer, 2009) as optimisation method. They report results considering the word and its context versus the word in isolation, as well as variants with and without the gold labels at training time. Finally, for the phrase-level task, SHEF made use of predictions generated by BMAPS for task 2 and the phrase labelling approaches in (Blain et al., 2016). These approaches use the number of BAD word-level predictions in a phrase: an optimistic version labels the phrase as OK if at least half of the words in it are predicted to be OK, and a superpessimistic version labels the phrase as BAD if any word is in is predicted to be BAD. UHH (T1): The UHH-STK submission is based on sequence and tree kernels applied on the source and target input data for predicting the HTER score. The kernels use a backtranslation of the MT output into the source language as an additional input data representation. Further hand-crafted features were deﬁned in the form"
W17-4717,W17-4760,1,0.841464,"Missing"
W17-4717,W17-4755,1,0.0393895,"nd Conference on Statistical Machine Translation (WMT) held at EMNLP 2017. This conference builds on eleven previous editions of WMT as workshops and conference (Koehn and Monz, 2006; Callison-Burch et al., 2007, 2008, 2009, 2010, 2011, 2012; Bojar et al., 2013, 2014, 2015, 2016a). This year we conducted several ofﬁcial tasks. We report in this paper on three tasks: • news translation (Section 2, Section 3) • quality estimation (Section 4) • automatic post-editing (Section 5) The conference featured additional shared tasks that are described in separate papers in these proceedings: • metrics (Bojar et al., 2017a) • multimodal machine translation and multilingual image description (Elliott et al., 2017) • biomedical translation (Jimeno Yepes et al., 2017) In the news translation task (Section 2), participants were asked to translate a shared test set, optionally restricting themselves to the provided training data (constraint condition). We held 14 translation tasks this year, between English and each of Chinese, Czech, German, Finnish, Latvian, Russian, and Turkish. The Latvian and Chinese translation tasks were new this year. Latvian is a lesser resourced data condition on challenging language pair"
W17-4717,W07-0718,1,0.696979,") • bandit learning (Sokolov et al., 2017) This paper presents the results of the WMT17 shared tasks, which included three machine translation (MT) tasks (news, biomedical, and multimodal), two evaluation tasks (metrics and run-time estimation of MT quality), an automatic post-editing task, a neural MT training task, and a bandit learning task. 1 Introduction We present the results of the shared tasks of the Second Conference on Statistical Machine Translation (WMT) held at EMNLP 2017. This conference builds on eleven previous editions of WMT as workshops and conference (Koehn and Monz, 2006; Callison-Burch et al., 2007, 2008, 2009, 2010, 2011, 2012; Bojar et al., 2013, 2014, 2015, 2016a). This year we conducted several ofﬁcial tasks. We report in this paper on three tasks: • news translation (Section 2, Section 3) • quality estimation (Section 4) • automatic post-editing (Section 5) The conference featured additional shared tasks that are described in separate papers in these proceedings: • metrics (Bojar et al., 2017a) • multimodal machine translation and multilingual image description (Elliott et al., 2017) • biomedical translation (Jimeno Yepes et al., 2017) In the news translation task (Section 2), part"
W17-4717,W08-0309,1,0.537902,"Missing"
W17-4717,W10-1703,1,0.603181,"Missing"
W17-4717,W12-3102,1,0.508067,"Missing"
W17-4717,W17-4761,0,0.0349831,"Missing"
W17-4717,W17-4723,0,0.0362034,"Missing"
W17-4717,W17-4724,1,0.839009,"Missing"
W17-4717,W11-2103,1,0.744276,"Missing"
W17-4717,W17-4773,1,0.839713,"Missing"
W17-4717,W15-3025,1,0.905105,"Instead of predicting the HTER score, the systems attempted to predict the number of each of the four postediting operations (add, replace, shift, delete) at the sentence level. However, this did not lead to positive results. In future editions of the task, we plan to make this detailed post-editing information available again and suggest clear ways of using it. 5 Automatic Post-editing Task The WMT shared task on MT automatic postediting (APE), this year at its third round at WMT, aims to evaluate systems for the automatic correction of errors in a machine translated text. As pointed out by (Chatterjee et al., 2015b), from the application point of view the task is motivated by its possible uses to: • Improve MT output by exploiting information unavailable to the decoder, or by performing deeper text analysis that is too expensive at the decoding stage; • Cope with systematic errors of an MT system whose decoding process is not accessible; • Provide professional translators with improved MT output quality to reduce (human) post-editing effort; • Adapt the output of a general-purpose MT system to the lexicon/style requested in a speciﬁc application domain. The third round of the APE task proposed to parti"
W17-4717,W17-4718,1,0.0662673,"builds on eleven previous editions of WMT as workshops and conference (Koehn and Monz, 2006; Callison-Burch et al., 2007, 2008, 2009, 2010, 2011, 2012; Bojar et al., 2013, 2014, 2015, 2016a). This year we conducted several ofﬁcial tasks. We report in this paper on three tasks: • news translation (Section 2, Section 3) • quality estimation (Section 4) • automatic post-editing (Section 5) The conference featured additional shared tasks that are described in separate papers in these proceedings: • metrics (Bojar et al., 2017a) • multimodal machine translation and multilingual image description (Elliott et al., 2017) • biomedical translation (Jimeno Yepes et al., 2017) In the news translation task (Section 2), participants were asked to translate a shared test set, optionally restricting themselves to the provided training data (constraint condition). We held 14 translation tasks this year, between English and each of Chinese, Czech, German, Finnish, Latvian, Russian, and Turkish. The Latvian and Chinese translation tasks were new this year. Latvian is a lesser resourced data condition on challenging language pair. Chinese allowed us to co-operate with an ongoing evaluation campaign on Asian languages org"
W17-4717,W17-4725,0,0.0391077,"Missing"
W17-4717,W08-0509,0,0.00859402,"l post-editors) and WMT17 DE-EN (German-English, pharmacological domain, professional post-editors) APE task data. TER BLEU APE15 23.84 n/a APE16 24.76 62.11 APE17 EN-DE 24.48 62.49 APE17 DE-EN 15.55 79.54 Table 26: Translation quality (TER/BLEU of TGT and proportion of TGTs with TER=0) of the WMT15, WMT16, WMT17 EN-DE and WMT17 DE-EN data. Figure 7: TER distribution over the EN-DE test set Figure 8: TER distribution over the DE-EN test set TERcom27 software: lower average TER scores correspond to higher ranks. BLEU was computed using the multi-bleu.perl package28 available in MOSES. MGIZA++ (Gao and Vogel, 2008) for word alignment. For language modeling we used the KenLM toolkit (Heaﬁeld, 2011) for standard n-gram modeling with an n-gram length of 5. Finally, the system was tuned on the development set, optimizing TER/BLEU with Minimum Error Rate Training (Och, 2003). The results of this additional term of comparison are also reported in Tables 28-29. For each submitted run, the statistical signiﬁcance of performance differences with respect to the baseline and our re-implementation of Simard et al. (2007) was calculated with the bootstrap test (Koehn, 2004). 5.1.3 Baselines Also this year, the ofﬁci"
W17-4717,W17-4726,0,0.0387381,"Missing"
W17-4717,W13-2305,1,0.899702,"(RR) approach, so named because the pairwise comparisons denote only relative ability between a pair of systems, and cannot be used to infer absolute quality. For example, RR can be used to discover which systems perform better than others, but RR does not provide any information about the absolute quality of system translations, i.e. it provides no information about how far a given system is from producing perfect output according to a human user. Work on evaluation over the past few years has provided fresh insight into ways to collect direct assessments (DA) of machine translation quality (Graham et al., 2013, 2014, 2016), and last year’s evaluation campaign included parallel assessment of a subset of News task language pairs evaluated with RR and DA. DA has some clear advantages over RR, namely the evaluation of absolute translation quality and the ability to carry out evaluations through quality controlled crowd-sourcing. As established last year (Bojar et al., 2016a), DA results (via crowd-sourcing) and RR results (produced by researchers) correlate strongly, with Pearson correlation ranging from 0.920 to 0.997 across several source languages into English and at 0.975 for English-to-Russian (th"
W17-4717,E14-1047,1,0.508986,"Missing"
W17-4717,N15-1124,1,0.658439,"Missing"
W17-4717,W13-0805,0,0.0140095,"ely contain 25,000 and 1,000 triplets, while the test set consists of 2,000 instances. Table 24 provides some basic statistics about the data (the same used for the sentence-level quality estimation task), which has been released by the European Project QT21 (Specia et al., 2017b).25 In addition, Tables 25 and 26 provide a view of the data from a task difﬁculty standpoint. Table 25 shows the repetition rate (RR) values of the data sets released in the three rounds 24 We used phrase-based MT systems trained with generic and in-domain parallel training data, leveraging prereordering techniques (Herrmann et al., 2013), and taking advantage of POS and word class-based language models. 25 For both language directions, the source sentences and reference translations were provided by TAUS (https://www.taus.net/). 197 EN-DE Train (23,000) Dev (1,000) Test (2,000) DE-EN Train (25,000) Dev (1,000) Test (2,000) SRC Tokens TGT PE SRC Types TGT PE SRC Lemmas TGT PE 384448 17827 65120 403306 19355 69812 411246 19763 71483 18220 2931 8061 27382 3333 9765 31652 3506 10502 10946 1922 2626 21959 2686 3976 25550 2806 4282 437833 17578 35087 453096 18130 36082 456163 18313 36480 29745 4426 6987 19866 3583 5391 19172 3642 5"
W17-4717,W17-4775,0,0.0513454,"Missing"
W17-4717,W17-4730,1,0.829861,"Missing"
W17-4717,W17-4731,0,0.029579,"Missing"
W17-4717,W17-4727,0,0.046917,"Missing"
W17-4717,W16-2378,0,0.0869978,"a manuallyrevised version of the target, done by professional translators. Test data consists of (source, target) pairs having similar characteristics of those in the training set. Human post-edits of the test target instances were left apart to measure system performance. English-German data were drawn from the Information Technology (IT) domain. Training and test sets respectively contain 11,000 and 2,000 triplets. The data released for the 2016 round of the task (15,000 instances) and the artiﬁcially generated post-editing triplets (4 million instances) used by last year’s winning system (Junczys-Dowmunt and Grundkiewicz, 2016) were also provided as additional training material. German-English data were drawn from the Pharmacological domain. Training and development sets respectively contain 25,000 and 1,000 triplets, while the test set consists of 2,000 instances. Table 24 provides some basic statistics about the data (the same used for the sentence-level quality estimation task), which has been released by the European Project QT21 (Specia et al., 2017b).25 In addition, Tables 25 and 26 provide a view of the data from a task difﬁculty standpoint. Table 25 shows the repetition rate (RR) values of the data sets rele"
W17-4717,W11-2123,0,0.00943973,"-editors) APE task data. TER BLEU APE15 23.84 n/a APE16 24.76 62.11 APE17 EN-DE 24.48 62.49 APE17 DE-EN 15.55 79.54 Table 26: Translation quality (TER/BLEU of TGT and proportion of TGTs with TER=0) of the WMT15, WMT16, WMT17 EN-DE and WMT17 DE-EN data. Figure 7: TER distribution over the EN-DE test set Figure 8: TER distribution over the DE-EN test set TERcom27 software: lower average TER scores correspond to higher ranks. BLEU was computed using the multi-bleu.perl package28 available in MOSES. MGIZA++ (Gao and Vogel, 2008) for word alignment. For language modeling we used the KenLM toolkit (Heaﬁeld, 2011) for standard n-gram modeling with an n-gram length of 5. Finally, the system was tuned on the development set, optimizing TER/BLEU with Minimum Error Rate Training (Och, 2003). The results of this additional term of comparison are also reported in Tables 28-29. For each submitted run, the statistical signiﬁcance of performance differences with respect to the baseline and our re-implementation of Simard et al. (2007) was calculated with the bootstrap test (Koehn, 2004). 5.1.3 Baselines Also this year, the ofﬁcial baseline results are the TER and BLEU scores calculated by comparing the raw MT o"
W17-4717,I17-1013,0,0.0334345,"Missing"
W17-4717,W16-2384,0,0.0137431,"features are calculated by a recurrent neural network language model. They experimented with different sentence embedding dimensions of the source sentences and translation outputs, as well as different sizes of the training corpus. The experimental results show that the neural network features lead to signiﬁcant improvements over the baseline, and that combining the neural network features with baseline features leads to further improvement. POSTECH (T1, T2, T3): POSTECH’s submissions to the sentence/word/phrase-level QE tasks are based on predictor-estimator architecture (Kim et al., 2017; Kim and Lee, 2016), which is the two-stage end-to-end ID CDACM DCU DFKI JXNU POSTECH RTM SHEF UHH Unbabel Participating team Centre for Development of Advanced Computing, India (Patel and M, 2016) Dublin City University (Hokamp, 2017) German Research Centre for Artiﬁcial Intelligence, Germany (Avramidis, 2017b) Jiangxi Normal University, China (Chen et al., 2017) Pohang University of Science and Technology, Republic of Korea (Kim et al., 2017) Referential Translation Machines, Turkey (Bic¸ici, 2017) University of Shefﬁeld, UK (Blain et al., 2017; Paetzold and Specia, 2017) University of Hamburg, Germany (Duma a"
W17-4717,W17-4763,0,0.0300648,"ence cross-entropy features are calculated by a recurrent neural network language model. They experimented with different sentence embedding dimensions of the source sentences and translation outputs, as well as different sizes of the training corpus. The experimental results show that the neural network features lead to signiﬁcant improvements over the baseline, and that combining the neural network features with baseline features leads to further improvement. POSTECH (T1, T2, T3): POSTECH’s submissions to the sentence/word/phrase-level QE tasks are based on predictor-estimator architecture (Kim et al., 2017; Kim and Lee, 2016), which is the two-stage end-to-end ID CDACM DCU DFKI JXNU POSTECH RTM SHEF UHH Unbabel Participating team Centre for Development of Advanced Computing, India (Patel and M, 2016) Dublin City University (Hokamp, 2017) German Research Centre for Artiﬁcial Intelligence, Germany (Avramidis, 2017b) Jiangxi Normal University, China (Chen et al., 2017) Pohang University of Science and Technology, Republic of Korea (Kim et al., 2017) Referential Translation Machines, Turkey (Bic¸ici, 2017) University of Shefﬁeld, UK (Blain et al., 2017; Paetzold and Specia, 2017) University of Hamb"
W17-4717,W04-3250,1,0.380171,"age28 available in MOSES. MGIZA++ (Gao and Vogel, 2008) for word alignment. For language modeling we used the KenLM toolkit (Heaﬁeld, 2011) for standard n-gram modeling with an n-gram length of 5. Finally, the system was tuned on the development set, optimizing TER/BLEU with Minimum Error Rate Training (Och, 2003). The results of this additional term of comparison are also reported in Tables 28-29. For each submitted run, the statistical signiﬁcance of performance differences with respect to the baseline and our re-implementation of Simard et al. (2007) was calculated with the bootstrap test (Koehn, 2004). 5.1.3 Baselines Also this year, the ofﬁcial baseline results are the TER and BLEU scores calculated by comparing the raw MT output with the human post-edits. In practice, the baseline APE system is a “donothing” system that leaves all the test targets unmodiﬁed. Baseline results, the same shown in Table 26, are also reported in Tables 28-29 for comparison with participants’ submissions. In continuity with the previous rounds, we used as additional term of comparison a reimplementation of the method ﬁrstly proposed by Simard et al. (2007). It relies on a phrasebased post-editing approach to t"
W17-4717,P07-2045,1,0.0149362,"t with the human post-edits. In practice, the baseline APE system is a “donothing” system that leaves all the test targets unmodiﬁed. Baseline results, the same shown in Table 26, are also reported in Tables 28-29 for comparison with participants’ submissions. In continuity with the previous rounds, we used as additional term of comparison a reimplementation of the method ﬁrstly proposed by Simard et al. (2007). It relies on a phrasebased post-editing approach to the task, which represented the common backbone of APE systems before the spread of neural solutions. The system is based on Moses (Koehn et al., 2007); translation and reordering models were estimated following the Moses protocol with default setup using 27 http://www.cs.umd.edu/˜snover/tercom/ https://github.com/moses-smt/mosesdecoder/ blob/master/scripts/generic/multi-bleu.perl 28 5.2 Participants Seven teams participated in the English-German task by submitting a total of ﬁfteen runs. Two of them also participated in the German-English task with ﬁve submitted runs. Participants are listed in Table 27, and a short description of their systems is provided in the following. Adam Mickiewicz University. AMU’s (ENDE) participation explores and"
W17-4717,C14-1017,0,0.0141812,"ord embeddings approach used by (Scarton et al., 2016) for document-level QE. Here in-domain word embeddings are used instead of embeddings obtained general purpose data (same as in task 2, below). Word embeddings were averaged to generate a single vector for each sentence. Source and target word embeddings were then concatenated with the baseline features and given to an SVM regressor for model building. For the word-level task SHEF investigated a new approach based on predicting the strength of the lexical relationships between the source and target sentences (BMAPS). Following the work by (Madhyastha et al., 2014), a bilinear model is trained from three matrices corresponding to the training data, the development set and a “truth” matrix between them, which is built from the word alignments and the gold labels to indicate which lexical items form a pair, and whether or not their lexical relation is OK or BAD. The ﬁrst two matrices are built from 300 dimension word vectors computed with pretrained in-domain word embeddings. They train their model over 100 iterations with the l2 norm as regulariser and using the forward-backward splitting algorithm (FOBOS) (Duchi and Singer, 2009) as optimisation method."
W17-4717,Q17-1015,0,0.0112193,"ID CDACM DCU DFKI JXNU POSTECH RTM SHEF UHH Unbabel Participating team Centre for Development of Advanced Computing, India (Patel and M, 2016) Dublin City University (Hokamp, 2017) German Research Centre for Artiﬁcial Intelligence, Germany (Avramidis, 2017b) Jiangxi Normal University, China (Chen et al., 2017) Pohang University of Science and Technology, Republic of Korea (Kim et al., 2017) Referential Translation Machines, Turkey (Bic¸ici, 2017) University of Shefﬁeld, UK (Blain et al., 2017; Paetzold and Specia, 2017) University of Hamburg, Germany (Duma and Menzel, 2017) Unbabel, Portugal (Martins et al., 2017b) Table 10: Participants in the WMT17 quality estimation shared task. neural QE model. The predictor-estimator architecture consists of two types of stacked neural network models: 1) a word prediction model based on bidirectional and bilingual recurrent neural network language model trained on additional large-scale parallel corpora and 2) a neural quality estimation model trained on quality-annotated noisy parallel corpora. To jointly learn the two-stage model, a stack propagation method was applied (Zhang and Weiss, 2016). In addition, a “multilevel model” was developed where a task-speciﬁc"
W17-4717,W16-2387,0,0.0126259,"the SMT parallel corps. These features were used to train a Support Vector Regression (SVR) algorithm using a Radial Basis Function (RBF) kernel within the SCIKITLEARN toolkit.13 The γ, � and C parameters were optimised via grid search with 5-fold cross validation on the training set, resulting in γ=0.01, � = 0.0825, C = 20. This baseline system has proved robust across a range of language pairs, MT systems, and text domains for predicting various In addition to that, 6 new features were included which contain combinations of other features, and which proved useful in (Kreutzer et al., 2015; Martins et al., 2016): 12 https://github.com/ghpaetzold/ questplusplus 13 http://scikit-learn.org/ 184 • • • • Target word + left context. Target word + right context. Target word + aligned source word. POS of target word + POS of aligned source word. • Target word + left context + source word. • Target word + right context + source word. • Punctuation features: The baseline system models the task as a sequence prediction problem using the LinearChain Conditional Random Fields (CRF) algorithm within the CRFSuite tool (Okazaki, 2007). The model was trained using passive-aggressive optimisation algorithm. We note th"
W17-4717,W17-4764,0,0.0178514,"ID CDACM DCU DFKI JXNU POSTECH RTM SHEF UHH Unbabel Participating team Centre for Development of Advanced Computing, India (Patel and M, 2016) Dublin City University (Hokamp, 2017) German Research Centre for Artiﬁcial Intelligence, Germany (Avramidis, 2017b) Jiangxi Normal University, China (Chen et al., 2017) Pohang University of Science and Technology, Republic of Korea (Kim et al., 2017) Referential Translation Machines, Turkey (Bic¸ici, 2017) University of Shefﬁeld, UK (Blain et al., 2017; Paetzold and Specia, 2017) University of Hamburg, Germany (Duma and Menzel, 2017) Unbabel, Portugal (Martins et al., 2017b) Table 10: Participants in the WMT17 quality estimation shared task. neural QE model. The predictor-estimator architecture consists of two types of stacked neural network models: 1) a word prediction model based on bidirectional and bilingual recurrent neural network language model trained on additional large-scale parallel corpora and 2) a neural quality estimation model trained on quality-annotated noisy parallel corpora. To jointly learn the two-stage model, a stack propagation method was applied (Zhang and Weiss, 2016). In addition, a “multilevel model” was developed where a task-speciﬁc"
W17-4717,W06-3114,1,0.104699,"g (Bojar et al., 2017b) • bandit learning (Sokolov et al., 2017) This paper presents the results of the WMT17 shared tasks, which included three machine translation (MT) tasks (news, biomedical, and multimodal), two evaluation tasks (metrics and run-time estimation of MT quality), an automatic post-editing task, a neural MT training task, and a bandit learning task. 1 Introduction We present the results of the shared tasks of the Second Conference on Statistical Machine Translation (WMT) held at EMNLP 2017. This conference builds on eleven previous editions of WMT as workshops and conference (Koehn and Monz, 2006; Callison-Burch et al., 2007, 2008, 2009, 2010, 2011, 2012; Bojar et al., 2013, 2014, 2015, 2016a). This year we conducted several ofﬁcial tasks. We report in this paper on three tasks: • news translation (Section 2, Section 3) • quality estimation (Section 4) • automatic post-editing (Section 5) The conference featured additional shared tasks that are described in separate papers in these proceedings: • metrics (Bojar et al., 2017a) • multimodal machine translation and multilingual image description (Elliott et al., 2017) • biomedical translation (Jimeno Yepes et al., 2017) In the news trans"
W17-4717,P03-1021,0,0.144477,"ed better than monolingual models. The code for these models is freely available.15 DCU (T2): DCU’s submission is an ensemble of neural MT systems with different input factors, designed to jointly tackle both the automatic post-editing and word-level QE. 15 https://github.com/patelrajnath/rnn4nlp 186 Word-level features which have proven effective for QE, such as part-of-speech tags and dependency labels are included as input factors to NMT systems. NMT systems using different input representations are ensembled together in a log-linear model which is tuned for the F1 -mult metric using MERT (Och, 2003). The output of the ensemble is a pseudo-reference that is then TER aligned with the original MT to obtain OK/BAD tags for each word in the MT hypothesis. DFKI (T1): These submissions investigate alternative machine learning models for the prediction of the HTER score on the sentencelevel task. Instead of directly predicting the HTER score, the systems use a single-layer perceptron with four outputs that jointly predict the number of each of the four distinct post-editing operations that are then used to calculate the HTER score. This also gives the possibility to correct invalid (e.g. negativ"
W17-4717,W15-3037,0,0.0138062,"n in the source side of the SMT parallel corps. These features were used to train a Support Vector Regression (SVR) algorithm using a Radial Basis Function (RBF) kernel within the SCIKITLEARN toolkit.13 The γ, � and C parameters were optimised via grid search with 5-fold cross validation on the training set, resulting in γ=0.01, � = 0.0825, C = 20. This baseline system has proved robust across a range of language pairs, MT systems, and text domains for predicting various In addition to that, 6 new features were included which contain combinations of other features, and which proved useful in (Kreutzer et al., 2015; Martins et al., 2016): 12 https://github.com/ghpaetzold/ questplusplus 13 http://scikit-learn.org/ 184 • • • • Target word + left context. Target word + right context. Target word + aligned source word. POS of target word + POS of aligned source word. • Target word + left context + source word. • Target word + right context + source word. • Punctuation features: The baseline system models the task as a sequence prediction problem using the LinearChain Conditional Random Fields (CRF) algorithm within the CRFSuite tool (Okazaki, 2007). The model was trained using passive-aggressive optimisatio"
W17-4717,P16-1160,0,0.0330991,"t attention (looking at information anywhere in the source sequence during decoding) and hard monotonic attention (looking at one encoder state at a time from left to right, thus being more conservative and faithful to the original input), which are combined in different ways in the case of multi-source models. The artiﬁcial data provided by JunczysDowmunt and Grundkiewicz (2016) are used to boost performance by increasing the size of the corpus used for training. Univerzita Karlova v Praze. CUNI’s (EN-DE) system is based on the character-to-character neural network architecture described in (Lee et al., 2016). This architecture was compared with the standard neural network architecture proposed by Bahdanau et al. (2014) which uses byte-pair encoding (Sennrich et al., 2015) for generating translation tokens. During the experiments, two setups have been compared for each architecture: i) a single encoder with SRC and MT sentences concatenated, and ii) a two-encoder system, where each SRC and MT sentence is fed to a separate encoder. The submitted system uses the two-encoder architecture with a character-level encoder and decoder. The initial state of the decoder is a weighted combination of the ﬁnal"
W17-4717,W17-4733,0,0.0368107,"Missing"
W17-4717,W17-4765,1,0.786172,"Missing"
W17-4717,L16-1582,1,0.768758,"T 183 translations labelled with task-speciﬁc labels. Participants were also provided with a baseline set of features for each task, and a software package to extract these and other quality estimation features and perform model learning (Section 4.1). Participants (Section 4.2) could submit up to two systems for each task. A discussion on the main goals and ﬁndings from this year’s task is given in Section 4.7. 4.1 Baseline systems forms of post-editing effort (2012; 2013; 2014; 2015; 2016a). Word-level baseline system: For Task 2, the baseline features were extracted with the M AR MOT tool (Logacheva et al., 2016). These are 28 features that have been deemed the most informative in previous research on word-level QE. 22 of them were taken from the feature set described in (Luong et al., 2014), and had also been used as a baseline feature set at WMT16: Sentence-level baseline system: For Task 1, Q U E ST ++12 (2015) was used to extract 17 MT system-independent features from the source and translation (target) ﬁles and parallel corpora: • Word count in the source and target sentences, and source and target token count ratio. Although these features are sentencelevel (i.e. their values will be the same fo"
W17-4717,C16-1241,0,0.0353857,"Missing"
W17-4717,W14-3342,0,0.0181229,"lity estimation features and perform model learning (Section 4.1). Participants (Section 4.2) could submit up to two systems for each task. A discussion on the main goals and ﬁndings from this year’s task is given in Section 4.7. 4.1 Baseline systems forms of post-editing effort (2012; 2013; 2014; 2015; 2016a). Word-level baseline system: For Task 2, the baseline features were extracted with the M AR MOT tool (Logacheva et al., 2016). These are 28 features that have been deemed the most informative in previous research on word-level QE. 22 of them were taken from the feature set described in (Luong et al., 2014), and had also been used as a baseline feature set at WMT16: Sentence-level baseline system: For Task 1, Q U E ST ++12 (2015) was used to extract 17 MT system-independent features from the source and translation (target) ﬁles and parallel corpora: • Word count in the source and target sentences, and source and target token count ratio. Although these features are sentencelevel (i.e. their values will be the same for all words in a sentence), the length of a sentence might inﬂuence the probability of a word being wrong. • Target token, its left and right contexts of 1 word. • Source word aligne"
W17-4717,P16-2046,0,0.0171489,"Missing"
W17-4717,W16-2379,0,0.0228966,"Missing"
W17-4717,P02-1040,0,0.119222,"Missing"
W17-4717,W16-2389,0,0.038207,"Missing"
W17-4717,W17-4736,0,0.0268357,"Missing"
W17-4717,W17-4737,0,0.0383776,"Missing"
W17-4717,W17-4738,0,0.0427074,"Missing"
W17-4717,W14-3301,1,0.655758,"Missing"
W17-4717,W16-2391,1,0.761669,"target sentences into sequences of character embeddings, and then passes them through a series of deep parallel stacked convolution/max pooling layers. The baseline features are provided through a multi-layer perceptron, 187 and then concatenated with the characterlevel information. Finally, the concatenation is passed onto another multi-layer perceptron and the very last layer outputs HTER values. The two submissions differ in the the use of standard (CNN+BASE-Single) and multi-task learning (CNN+BASE-Multi) for training. The QUEST-EMB submission follows the word embeddings approach used by (Scarton et al., 2016) for document-level QE. Here in-domain word embeddings are used instead of embeddings obtained general purpose data (same as in task 2, below). Word embeddings were averaged to generate a single vector for each sentence. Source and target word embeddings were then concatenated with the baseline features and given to an SVM regressor for model building. For the word-level task SHEF investigated a new approach based on predicting the strength of the lexical relationships between the source and target sentences (BMAPS). Following the work by (Madhyastha et al., 2014), a bilinear model is trained"
W17-4717,W16-2323,1,0.349233,"theses. Concatenated source + MT hypothesis are also used as an input representation for some models. The system makes extensive use of the synthetic training data provided by Junczys-Dowmunt and Grundkiewicz (2016), as well as min-risk training for ﬁne-tuning (Shen et al., 2016). The neural systems, which use different input representations but share the same output vocabulary, are then ensembled together in a loglinear model which is tuned for the TER metric using MERT. Fondazione Bruno Kessler. FBK’s (EN-DE & DE-EN) submission extends the existing NMT implementation in the Nematus toolkit (Sennrich et al., 2016) to train an ensemble of multi-source neural APE systems. Building on previous participations based on the phrase-based paradigm (Chatterjee et al., 2015a, 2016), and similar to (Libovick´y et al., 2016), such systems jointly learn from source and target information in order to increase robustness and precision of the automatic corrections. The n-best hypotheses produced by 200 this ensemble are further re-ranked using features based on the edit distance between the original MT output and each APE hypothesis, as well as other statistical models (n-gram language model and operation sequence mod"
W17-4717,P16-1159,0,0.0156896,"different input factors, designed to jointly tackle both the APE task and the Word-Level QE task. Word-Level features which have proven effective for QE, such as word-alignments, partof-speech tags, and dependency labels, are included as input factors to neural machine translation systems, which are trained to output PostEdited MT hypotheses. Concatenated source + MT hypothesis are also used as an input representation for some models. The system makes extensive use of the synthetic training data provided by Junczys-Dowmunt and Grundkiewicz (2016), as well as min-risk training for ﬁne-tuning (Shen et al., 2016). The neural systems, which use different input representations but share the same output vocabulary, are then ensembled together in a loglinear model which is tuned for the TER metric using MERT. Fondazione Bruno Kessler. FBK’s (EN-DE & DE-EN) submission extends the existing NMT implementation in the Nematus toolkit (Sennrich et al., 2016) to train an ensemble of multi-source neural APE systems. Building on previous participations based on the phrase-based paradigm (Chatterjee et al., 2015a, 2016), and similar to (Libovick´y et al., 2016), such systems jointly learn from source and target inf"
W17-4717,2006.amta-papers.25,0,0.817721,"trast to last year, we also provide datasets for two language pairs. The structure used for the data have been the same since WMT15. Each data instance consists of (i) a source sentence, (ii) its automatic translation into the target language, (iii) the manually post-edited version of the automatic translation, (iv) a free reference translation of the source sentence. Post-edits are used to extract labels for the 4.4 Task 1: Predicting sentence-level quality This task consists in scoring (and ranking) translation sentences according to the proportion of their words that need to be ﬁxed. HTER (Snover et al., 2006b) is used as quality score, i.e. the minimum edit distance between the machine translation and its manually post-edited version. Labels HTER labels were computed using the TERCOM tool16 with default settings (tokenised, case insensitive, exact matching only), with scores capped to 1. 188 16 http://www.cs.umd.edu/˜snover/tercom/ Evaluation Evaluation was performed against the true HTER label and/or ranking, using the following metrics: with an edit operation: insertion, deletion, substitution or no edit (correct word). We mark each edited word as BAD, and the remainingn as OK. • Scoring: Pears"
W17-4717,W17-4756,0,0.0529328,"Missing"
W17-4717,P15-4020,1,0.738308,"Missing"
W17-4717,P13-4014,1,0.634274,"Missing"
W17-4717,W17-4720,1,0.830567,"Missing"
W17-4717,W17-4776,0,0.0596806,"Missing"
W17-4717,W17-4777,1,0.832756,"Missing"
W17-4717,W17-4742,0,0.0342495,"Missing"
W17-4717,W17-4744,0,0.0607458,"Missing"
W17-4717,C00-2137,0,0.0420443,"ch edited word as BAD, and the remainingn as OK. • Scoring: Pearson’s r correlation score (primary metric, ofﬁcial score for ranking submissions), Mean Average Error (MAE) and Root Mean Squared Error (RMSE). Evaluation Analogously to the last year’s task, the primary evaluation metric is the multiplication of F1 -scores for the OK and BAD classes, denoted as F1 -mult. Unlike previously used F1 BAD score this metric is not biased towards “pessimistic” labellings. We also report F1 -scores for individual classes for completeness. We test the signiﬁcance of the results using randomisation tests (Yeh, 2000) with Bonferroni correction (Abdi, 2007). • Ranking: Spearman’s ρ rank correlation and DeltaAvg. Statistical signiﬁcance on Pearson r was computed using the William’s test.17 Results Tables 13 and 14 summarise the results for Task 1 on German–English and English– German datasets, respectively, ranking participating systems best to worst using Pearson’s r correlation as primary key. Spearman’s ρ correlation scores should be used to rank systems for the ranking variant. The top three systems are the same for both datasets, and the ranking of systems according to their performance is similar for"
W17-4717,W17-4745,1,0.825998,"Missing"
W17-4717,P16-1147,0,0.0102821,"Missing"
W17-4724,W16-2310,1,0.682889,"tional language models trained on 50, 200, Introduction The JHU 2017 WMT submission consists of phrase-based systems, syntax-based systems and neural machine translation systems. In this paper we discuss features that we integrated into our system submissions. We also discuss lattice rescoring as a form of system combination of phrase-based and neural machine translation systems. The JHU phrase-based translation systems for our participation in the WMT 2017 shared translation task are based on the open source Moses toolkit (Koehn et al., 2007) and strong baselines of our submission last year (Ding et al., 2016). The JHU neural machine translation systems were built with the Nematus (Sennrich et al., 2016c) and Marian (Junczys-Dowmunt et al., 2016) toolkits. Our lattice rescoring experiments are also based on a combination of these three toolkits. 2 Configuration Phrase-Based Model Baselines Although the focus of research in machine translation has firmly moved onto neural machine translation, we still built traditional phrase-based statistical machine translation systems for all language pairs. These submissions also serve as a baseline 276 Proceedings of the Conference on Machine Translation (WMT),"
W17-4724,P13-2071,1,0.854174,"phrase-based, syntax-based and/or neural machine translation systems for all 14 language pairs of this year’s evaluation campaign. We also performed neural rescoring of phrasebased systems for English-Turkish and English-Finnish. 1 2.1 We trained our systems with the following settings: a maximum sentence length of 80, growdiag-final-and symmetrization of GIZA++ alignments, an interpolated Kneser-Ney smoothed 5gram language model with KenLM (Heafield, 2011) used at runtime, hierarchical lexicalized reordering (Galley and Manning, 2008), a lexicallydriven 5-gram operation sequence model (OSM) (Durrani et al., 2013) with 4 count-based supportive features, sparse domain indicator, phrase length, and count bin features (Blunsom and Osborne, 2008; Chiang et al., 2009), a distortion limit of 6, maximum phrase-length of 5, 100-best translation options, compact phrase table (JunczysDowmunt, 2012) minimum Bayes risk decoding (Kumar and Byrne, 2004), cube pruning (Huang and Chiang, 2007), with a stack-size of 1000 during tuning and 5000 during test and the noreordering-over-punctuation heuristic (Koehn and Haddow, 2009). We optimize feature function weights with k-best MIRA (Cherry and Foster, 2012). We used POS"
W17-4724,D08-1089,0,0.046349,"n task of EMNLP 2017 Second Conference on Machine Translation (WMT 2017). We set up phrase-based, syntax-based and/or neural machine translation systems for all 14 language pairs of this year’s evaluation campaign. We also performed neural rescoring of phrasebased systems for English-Turkish and English-Finnish. 1 2.1 We trained our systems with the following settings: a maximum sentence length of 80, growdiag-final-and symmetrization of GIZA++ alignments, an interpolated Kneser-Ney smoothed 5gram language model with KenLM (Heafield, 2011) used at runtime, hierarchical lexicalized reordering (Galley and Manning, 2008), a lexicallydriven 5-gram operation sequence model (OSM) (Durrani et al., 2013) with 4 count-based supportive features, sparse domain indicator, phrase length, and count bin features (Blunsom and Osborne, 2008; Chiang et al., 2009), a distortion limit of 6, maximum phrase-length of 5, 100-best translation options, compact phrase table (JunczysDowmunt, 2012) minimum Bayes risk decoding (Kumar and Byrne, 2004), cube pruning (Huang and Chiang, 2007), with a stack-size of 1000 during tuning and 5000 during test and the noreordering-over-punctuation heuristic (Koehn and Haddow, 2009). We optimize"
W17-4724,D08-1023,0,0.193388,"ign. We also performed neural rescoring of phrasebased systems for English-Turkish and English-Finnish. 1 2.1 We trained our systems with the following settings: a maximum sentence length of 80, growdiag-final-and symmetrization of GIZA++ alignments, an interpolated Kneser-Ney smoothed 5gram language model with KenLM (Heafield, 2011) used at runtime, hierarchical lexicalized reordering (Galley and Manning, 2008), a lexicallydriven 5-gram operation sequence model (OSM) (Durrani et al., 2013) with 4 count-based supportive features, sparse domain indicator, phrase length, and count bin features (Blunsom and Osborne, 2008; Chiang et al., 2009), a distortion limit of 6, maximum phrase-length of 5, 100-best translation options, compact phrase table (JunczysDowmunt, 2012) minimum Bayes risk decoding (Kumar and Byrne, 2004), cube pruning (Huang and Chiang, 2007), with a stack-size of 1000 during tuning and 5000 during test and the noreordering-over-punctuation heuristic (Koehn and Haddow, 2009). We optimize feature function weights with k-best MIRA (Cherry and Foster, 2012). We used POS and morphological tags as additional factors in phrase translation models (Koehn and Hoang, 2007) for the German-English language"
W17-4724,W11-2123,0,0.0496754,"ibes the Johns Hopkins University submissions to the shared translation task of EMNLP 2017 Second Conference on Machine Translation (WMT 2017). We set up phrase-based, syntax-based and/or neural machine translation systems for all 14 language pairs of this year’s evaluation campaign. We also performed neural rescoring of phrasebased systems for English-Turkish and English-Finnish. 1 2.1 We trained our systems with the following settings: a maximum sentence length of 80, growdiag-final-and symmetrization of GIZA++ alignments, an interpolated Kneser-Ney smoothed 5gram language model with KenLM (Heafield, 2011) used at runtime, hierarchical lexicalized reordering (Galley and Manning, 2008), a lexicallydriven 5-gram operation sequence model (OSM) (Durrani et al., 2013) with 4 count-based supportive features, sparse domain indicator, phrase length, and count bin features (Blunsom and Osborne, 2008; Chiang et al., 2009), a distortion limit of 6, maximum phrase-length of 5, 100-best translation options, compact phrase table (JunczysDowmunt, 2012) minimum Bayes risk decoding (Kumar and Byrne, 2004), cube pruning (Huang and Chiang, 2007), with a stack-size of 1000 during tuning and 5000 during test and th"
W17-4724,P07-1019,0,0.0412146,"nments, an interpolated Kneser-Ney smoothed 5gram language model with KenLM (Heafield, 2011) used at runtime, hierarchical lexicalized reordering (Galley and Manning, 2008), a lexicallydriven 5-gram operation sequence model (OSM) (Durrani et al., 2013) with 4 count-based supportive features, sparse domain indicator, phrase length, and count bin features (Blunsom and Osborne, 2008; Chiang et al., 2009), a distortion limit of 6, maximum phrase-length of 5, 100-best translation options, compact phrase table (JunczysDowmunt, 2012) minimum Bayes risk decoding (Kumar and Byrne, 2004), cube pruning (Huang and Chiang, 2007), with a stack-size of 1000 during tuning and 5000 during test and the noreordering-over-punctuation heuristic (Koehn and Haddow, 2009). We optimize feature function weights with k-best MIRA (Cherry and Foster, 2012). We used POS and morphological tags as additional factors in phrase translation models (Koehn and Hoang, 2007) for the German-English language pairs. We also trained target sequence models on the in-domain subset of the parallel corpus using Kneser-Ney smoothed 7-gram models. We used syntactic preordering (Collins et al., 2005) and compound splitting (Koehn and Knight, 2003) for t"
W17-4724,buck-etal-2014-n,0,0.0365342,"Missing"
W17-4724,W08-0336,0,0.0357741,"rs. These submissions also serve as a baseline 276 Proceedings of the Conference on Machine Translation (WMT), Volume 2: Shared Task Papers, pages 276–282 c Copenhagen, Denmark, September 711, 2017. 2017 Association for Computational Linguistics Language Pair German–English Czech–English Finnish–English Latvian–English Russian-English Turkish–English Chinese–English Sentences 21,243 21,730 2,870 984 11,824 1,001 1,000 For English data, we used the scripts from Moses (Koehn et al., 2007) to tokenize our data, while for Chinese data we carried out word segmentation with Stanford word segmenter (Chang et al., 2008). We also normalized all the Chinese punctuations to their English counterparts to avoid disagreement across sentences. We parsed the tokenized data with Berkeley Parser (Petrov and Klein, 2007) using the pre-trained grammar provided with the toolkit, followed by right binarization of the parse. Finally, truecasing was performed on all the English texts. Due to the lack of casing system, we did not perform truecasing for any Chinese texts. We performed word alignment with fast-align (Dyer et al., 2013) due to the huge scale of this year’s training data and grow-diag-final-and heuristic for ali"
W17-4724,2012.eamt-1.58,0,0.0622113,"Missing"
W17-4724,N12-1047,0,0.032334,"ce model (OSM) (Durrani et al., 2013) with 4 count-based supportive features, sparse domain indicator, phrase length, and count bin features (Blunsom and Osborne, 2008; Chiang et al., 2009), a distortion limit of 6, maximum phrase-length of 5, 100-best translation options, compact phrase table (JunczysDowmunt, 2012) minimum Bayes risk decoding (Kumar and Byrne, 2004), cube pruning (Huang and Chiang, 2007), with a stack-size of 1000 during tuning and 5000 during test and the noreordering-over-punctuation heuristic (Koehn and Haddow, 2009). We optimize feature function weights with k-best MIRA (Cherry and Foster, 2012). We used POS and morphological tags as additional factors in phrase translation models (Koehn and Hoang, 2007) for the German-English language pairs. We also trained target sequence models on the in-domain subset of the parallel corpus using Kneser-Ney smoothed 7-gram models. We used syntactic preordering (Collins et al., 2005) and compound splitting (Koehn and Knight, 2003) for the German-to-English systems. We did no language-specific processing for other languages. We included Och cluster language model, with 4 additional language models trained on 50, 200, Introduction The JHU 2017 WMT su"
W17-4724,N09-1025,0,0.0519466,"Missing"
W17-4724,D07-1091,1,0.65067,"gth, and count bin features (Blunsom and Osborne, 2008; Chiang et al., 2009), a distortion limit of 6, maximum phrase-length of 5, 100-best translation options, compact phrase table (JunczysDowmunt, 2012) minimum Bayes risk decoding (Kumar and Byrne, 2004), cube pruning (Huang and Chiang, 2007), with a stack-size of 1000 during tuning and 5000 during test and the noreordering-over-punctuation heuristic (Koehn and Haddow, 2009). We optimize feature function weights with k-best MIRA (Cherry and Foster, 2012). We used POS and morphological tags as additional factors in phrase translation models (Koehn and Hoang, 2007) for the German-English language pairs. We also trained target sequence models on the in-domain subset of the parallel corpus using Kneser-Ney smoothed 7-gram models. We used syntactic preordering (Collins et al., 2005) and compound splitting (Koehn and Knight, 2003) for the German-to-English systems. We did no language-specific processing for other languages. We included Och cluster language model, with 4 additional language models trained on 50, 200, Introduction The JHU 2017 WMT submission consists of phrase-based systems, syntax-based systems and neural machine translation systems. In this"
W17-4724,P16-1162,0,0.735349,"of phrase-based systems, syntax-based systems and neural machine translation systems. In this paper we discuss features that we integrated into our system submissions. We also discuss lattice rescoring as a form of system combination of phrase-based and neural machine translation systems. The JHU phrase-based translation systems for our participation in the WMT 2017 shared translation task are based on the open source Moses toolkit (Koehn et al., 2007) and strong baselines of our submission last year (Ding et al., 2016). The JHU neural machine translation systems were built with the Nematus (Sennrich et al., 2016c) and Marian (Junczys-Dowmunt et al., 2016) toolkits. Our lattice rescoring experiments are also based on a combination of these three toolkits. 2 Configuration Phrase-Based Model Baselines Although the focus of research in machine translation has firmly moved onto neural machine translation, we still built traditional phrase-based statistical machine translation systems for all language pairs. These submissions also serve as a baseline 276 Proceedings of the Conference on Machine Translation (WMT), Volume 2: Shared Task Papers, pages 276–282 c Copenhagen, Denmark, September 711, 2017. 2017 A"
W17-4724,P07-2045,1,0.0439969,"r other languages. We included Och cluster language model, with 4 additional language models trained on 50, 200, Introduction The JHU 2017 WMT submission consists of phrase-based systems, syntax-based systems and neural machine translation systems. In this paper we discuss features that we integrated into our system submissions. We also discuss lattice rescoring as a form of system combination of phrase-based and neural machine translation systems. The JHU phrase-based translation systems for our participation in the WMT 2017 shared translation task are based on the open source Moses toolkit (Koehn et al., 2007) and strong baselines of our submission last year (Ding et al., 2016). The JHU neural machine translation systems were built with the Nematus (Sennrich et al., 2016c) and Marian (Junczys-Dowmunt et al., 2016) toolkits. Our lattice rescoring experiments are also based on a combination of these three toolkits. 2 Configuration Phrase-Based Model Baselines Although the focus of research in machine translation has firmly moved onto neural machine translation, we still built traditional phrase-based statistical machine translation systems for all language pairs. These submissions also serve as a bas"
W17-4724,E03-1076,1,0.540903,"ning (Huang and Chiang, 2007), with a stack-size of 1000 during tuning and 5000 during test and the noreordering-over-punctuation heuristic (Koehn and Haddow, 2009). We optimize feature function weights with k-best MIRA (Cherry and Foster, 2012). We used POS and morphological tags as additional factors in phrase translation models (Koehn and Hoang, 2007) for the German-English language pairs. We also trained target sequence models on the in-domain subset of the parallel corpus using Kneser-Ney smoothed 7-gram models. We used syntactic preordering (Collins et al., 2005) and compound splitting (Koehn and Knight, 2003) for the German-to-English systems. We did no language-specific processing for other languages. We included Och cluster language model, with 4 additional language models trained on 50, 200, Introduction The JHU 2017 WMT submission consists of phrase-based systems, syntax-based systems and neural machine translation systems. In this paper we discuss features that we integrated into our system submissions. We also discuss lattice rescoring as a form of system combination of phrase-based and neural machine translation systems. The JHU phrase-based translation systems for our participation in the"
W17-4724,N04-1022,0,0.0775843,"inal-and symmetrization of GIZA++ alignments, an interpolated Kneser-Ney smoothed 5gram language model with KenLM (Heafield, 2011) used at runtime, hierarchical lexicalized reordering (Galley and Manning, 2008), a lexicallydriven 5-gram operation sequence model (OSM) (Durrani et al., 2013) with 4 count-based supportive features, sparse domain indicator, phrase length, and count bin features (Blunsom and Osborne, 2008; Chiang et al., 2009), a distortion limit of 6, maximum phrase-length of 5, 100-best translation options, compact phrase table (JunczysDowmunt, 2012) minimum Bayes risk decoding (Kumar and Byrne, 2004), cube pruning (Huang and Chiang, 2007), with a stack-size of 1000 during tuning and 5000 during test and the noreordering-over-punctuation heuristic (Koehn and Haddow, 2009). We optimize feature function weights with k-best MIRA (Cherry and Foster, 2012). We used POS and morphological tags as additional factors in phrase translation models (Koehn and Hoang, 2007) for the German-English language pairs. We also trained target sequence models on the in-domain subset of the parallel corpus using Kneser-Ney smoothed 7-gram models. We used syntactic preordering (Collins et al., 2005) and compound s"
W17-4724,N16-1046,0,0.0530386,"scores). Bold scores indicate best and submitted systems. 2 million sentences from news crawl 2016 monolingual corpus and 1.5 million sentences from preprocessed CWMT Chinese monolingual corpus from our syntax-based system run and backtranslated them with our trained base system. These back-translated pseudo-parallel data were then mixed with an equal amount of random samples from real parallel training data and used as the data for continued training. All the hyperparameters used for the continued training are exactly the same as those in the initial training stage. Following the effort of (Liu et al., 2016) and (Sennrich et al., 2016a), we also trained right-toleft (r2l) models with a random sample of 4 million sentence pairs for both translation directions of Chinese-English language pairs, in the hope that they could lead to better reordering on the target side. But they were not included in the final submission because they turned out to hurt the performance on development set. We conjecture that our r2l model is too weak compared to both base and back-trans models to yield good reordering hypotheses. We performed model averaging over the 4-best models for both base and back-trans systems as"
W17-4724,E99-1010,0,0.227759,"systems (Blunsom and Osborne, 2008)(Chiang et al., 2009). We used the same language model and tuning settings as the phrase-based systems. While BLEU score was used both for tuning and our development experiments, it is ambiguous when applied for Chinese outputs because Chinese does not have explicit word boundaries. For discriminative training and development tests, we evaluate the Chinese output against the automatically-segmented Chinese reference with multi-bleu.perl scripts in Moses (Koehn et al., 2007). Table 1: Tuning set sizes for phrase and syntaxbased system 500, and 2000 clusters (Och, 1999) using mkcls. In addition, we included a large language model based on the CommonCrawl monolingual data (Buck et al., 2014). The systems were tuned on a very large tuning set consisting of the test sets from 2008-2015, with a total of up to 21,730 sentences (see Table 1). We used newstest2016 as development test set. Significantly less tuning data was available for Finnish, Latvian, and Turkish. 2.2 Results Table 2 shows results for all language pairs, except for Chinese–English, for which we did not built phrase-based systems. Our phrase-based systems were clearly outperformed by NMT systems"
W17-4724,N07-1051,0,0.0711363,"1, 2017. 2017 Association for Computational Linguistics Language Pair German–English Czech–English Finnish–English Latvian–English Russian-English Turkish–English Chinese–English Sentences 21,243 21,730 2,870 984 11,824 1,001 1,000 For English data, we used the scripts from Moses (Koehn et al., 2007) to tokenize our data, while for Chinese data we carried out word segmentation with Stanford word segmenter (Chang et al., 2008). We also normalized all the Chinese punctuations to their English counterparts to avoid disagreement across sentences. We parsed the tokenized data with Berkeley Parser (Petrov and Klein, 2007) using the pre-trained grammar provided with the toolkit, followed by right binarization of the parse. Finally, truecasing was performed on all the English texts. Due to the lack of casing system, we did not perform truecasing for any Chinese texts. We performed word alignment with fast-align (Dyer et al., 2013) due to the huge scale of this year’s training data and grow-diag-final-and heuristic for alignment symmetrization. We used the GHKM rule extractor implemented in Moses to extract SCFG rules from the parallel corpus. We set the maximum number of nodes (except target words) in the rules"
W17-4724,E17-3017,0,0.0705342,"Missing"
W17-4724,W16-2323,0,0.530034,"of phrase-based systems, syntax-based systems and neural machine translation systems. In this paper we discuss features that we integrated into our system submissions. We also discuss lattice rescoring as a form of system combination of phrase-based and neural machine translation systems. The JHU phrase-based translation systems for our participation in the WMT 2017 shared translation task are based on the open source Moses toolkit (Koehn et al., 2007) and strong baselines of our submission last year (Ding et al., 2016). The JHU neural machine translation systems were built with the Nematus (Sennrich et al., 2016c) and Marian (Junczys-Dowmunt et al., 2016) toolkits. Our lattice rescoring experiments are also based on a combination of these three toolkits. 2 Configuration Phrase-Based Model Baselines Although the focus of research in machine translation has firmly moved onto neural machine translation, we still built traditional phrase-based statistical machine translation systems for all language pairs. These submissions also serve as a baseline 276 Proceedings of the Conference on Machine Translation (WMT), Volume 2: Shared Task Papers, pages 276–282 c Copenhagen, Denmark, September 711, 2017. 2017 A"
W18-1820,W17-3203,1,0.807221,"ing any optimizer from MXN ET’s library, including stochastic gradient descent (SGD) and Adam [Kingma and Ba, 2014]. S OCKEYE also includes its own implementation of the Eve optimizer, which extends Adam by incorporating information from the objective function [Koushik and Hayashi, 2016]. This allows learning to accelerate over ﬂat areas of the loss surface and decelerate when saddle points cause the objective to “bounce”. Learning schedules Recent work has shown the value of annealing the base learning rate α throughout training, even when using optimizers such as Adam [Vaswani et al., 2017, Denkowski and Neubig, 2017]. S OCKEYE’s ‘plateau-reduce’ scheduler implements rate annealing as follows. At each training checkpoint, the scheduler compares validation set perplexity against the best previous checkpoint. If perplexity has not surpassed the previous best in N checkpoints, the learning rate α is multiplied by a ﬁxed constant and the counter is reset. Optionally, the scheduler can reset model and optimizer parameters to the best previous point, simulating a perfect prediction of when to anneal the learning rate. 4 Note that the transformer and convolutional architectures cannot use these attention types."
W18-1820,D17-1300,0,0.0518893,"Missing"
W18-1820,N13-1073,0,0.076416,"Missing"
W18-1820,E17-3017,0,0.259665,"(version 1.12) 2 https://mxnet.incubator.apache.org/ Proceedings of AMTA 2018, vol. 1: MT Research Track Boston, March 17 - 21, 2018 |Page 200 2 Encoder-Decoder Models for NMT The main idea in neural sequence-to-sequence modeling is to encode a variable-length input sequence of tokens into a sequence of vector representations, and to then decode those representations into a sequence of output tokens. We give a brief description for the three encoder-decoder architectures as implemented in S OCKEYE, but refer the reader to the references for more details or to the arXiv version of this paper [Hieber et al., 2017]. 2.1 Stacked Recurrent Neural Network (RNN) with Attention [Bahdanau et al., 2014, Luong et al., 2015] The encoder consists of a bi-directional RNN followed by a stack of uni-directional RNNs. An RNN at layer l produces a sequence of hidden states hl1 . . . hln : l hli = fenc (hl−1 i , hi−1 ), (1) where frnn is some non-linear function, such as a Gated Recurrent Unit (GRU) or Long Short Term Memory (LSTM) cell, and hl−1 is the output from the lower layer at step i. The bii directional RNN on the lowest layer uses the embeddings ES xi as input and concatenates the → − ← − hidden states of a f"
W18-1820,D13-1176,0,0.0429926,"on quality and computational efﬁciency. In a trend that carries over from Statistical Machine Translation (SMT), the strongest NMT systems beneﬁt from subtle architecture modiﬁcations, hyper-parameter tuning, and empirically effective heuristics. To address these challenges, we introduce S OCKEYE, a neural sequence-to-sequence toolkit written in Python and built on Apache MXN ET2 [Chen et al., 2015]. To the best of our knowledge, S OCKEYE is the only toolkit that includes implementations of all three major neural translation architectures: attentional recurrent neural networks [Schwenk, 2012, Kalchbrenner and Blunsom, 2013, Sutskever et al., 2014, Bahdanau et al., 2014, Luong et al., 2015], selfattentional transformers [Vaswani et al., 2017], and fully convolutional networks [Gehring et al., 2017]. These implementations are supported by a wide and continually updated range of features reﬂecting the best ideas from recent literature. Users can easily train models based on the latest research, compare different architectures, and extend them by adding their own code. S OCKEYE is under active development that follows best practice for both research and production software, including clear coding and documentation"
W18-1820,P17-4012,0,0.0330319,"sh into German (EN→DE) and Latvian into English (LV→EN). Models in both language pairs were based on the complete parallel data provided for each task as part of the Second Conference on Machine Translation [Bojar et al., 2017]. We ran each toolkit in a “best available” conﬁguration, where we took settings from recent relevant papers that used the toolkit, or communicated directly with authors. Toolkits evaluated include FAIR S EQ [Gehring et al., 2017], M ARIAN [Junczys-Dowmunt et al., 2016], N EMATUS [Sennrich et al., 2017b], N EURAL M ONKEY [Helcl and Jindˇrich Libovický, 2017], O PEN NMT [Klein et al., 2017], and T ENSOR 2T ENSOR (T2T)8 .9 Shown in Table 1, S OCKEYE’s RNN model is competitive with M ARIAN, the top-performer, the CNN model outperforms FAIR S EQ, and the transformer outperforms all models from all other toolkits. See [Hieber et al., 2017] for more extensive comparisons and further details. 5 Summary We have presented S OCKEYE, a mature, open-source framework for neural sequence-to-sequence learning that implements the three major architectures for neural machine translation (the only one to do so, to our knowledge). Written in Python, it is easy to install, extend, and deploy; bui"
W18-1820,D15-1166,0,0.757671,"stical Machine Translation (SMT), the strongest NMT systems beneﬁt from subtle architecture modiﬁcations, hyper-parameter tuning, and empirically effective heuristics. To address these challenges, we introduce S OCKEYE, a neural sequence-to-sequence toolkit written in Python and built on Apache MXN ET2 [Chen et al., 2015]. To the best of our knowledge, S OCKEYE is the only toolkit that includes implementations of all three major neural translation architectures: attentional recurrent neural networks [Schwenk, 2012, Kalchbrenner and Blunsom, 2013, Sutskever et al., 2014, Bahdanau et al., 2014, Luong et al., 2015], selfattentional transformers [Vaswani et al., 2017], and fully convolutional networks [Gehring et al., 2017]. These implementations are supported by a wide and continually updated range of features reﬂecting the best ideas from recent literature. Users can easily train models based on the latest research, compare different architectures, and extend them by adding their own code. S OCKEYE is under active development that follows best practice for both research and production software, including clear coding and documentation guidelines, comprehensive automatic tests, and peer review for code"
W18-1820,C12-2104,0,0.0340888,"both translation quality and computational efﬁciency. In a trend that carries over from Statistical Machine Translation (SMT), the strongest NMT systems beneﬁt from subtle architecture modiﬁcations, hyper-parameter tuning, and empirically effective heuristics. To address these challenges, we introduce S OCKEYE, a neural sequence-to-sequence toolkit written in Python and built on Apache MXN ET2 [Chen et al., 2015]. To the best of our knowledge, S OCKEYE is the only toolkit that includes implementations of all three major neural translation architectures: attentional recurrent neural networks [Schwenk, 2012, Kalchbrenner and Blunsom, 2013, Sutskever et al., 2014, Bahdanau et al., 2014, Luong et al., 2015], selfattentional transformers [Vaswani et al., 2017], and fully convolutional networks [Gehring et al., 2017]. These implementations are supported by a wide and continually updated range of features reﬂecting the best ideas from recent literature. Users can easily train models based on the latest research, compare different architectures, and extend them by adding their own code. S OCKEYE is under active development that follows best practice for both research and production software, including"
W18-1820,C16-1165,0,0.0273615,"as source and references are typically tokenized and possibly byte-pair encoded. All statistics can also be written to a Tensorboard event ﬁle that can be rendered by a standalone Tensorboard fork.5 Regularization S OCKEYE supports standard techniques for regularization, such as dropout. This includes dropout on input embeddings for both the source and the target and the proposed dropout layers for the transformer architecture. One can also enable variational dropout [Gal and Ghahramani, 2016] to sample a ﬁxed dropout mask across recurrent time steps, or recurrent dropout without memory loss [Semeniuta et al., 2016]. S OCKEYE can also use MXN ET’s label smoothing [Pereyra et al., 2017] feature to efﬁciently back-propagate smoothed cross-entropy gradients without explicitly representing a dense, smoothed label distribution. Fault tolerance S OCKEYE saves the training state of all training components after every checkpoint, including elements like the shufﬂing data iterator and optimizer states. Training can therefore easily be continued from the last checkpoint in the case of aborted process. Mult-GPU training S OCKEYE can take advantage of multipe GPUs using MXN ET’s data parallelism mechanism. Training"
W18-1820,Q17-1007,0,0.039026,"Missing"
W18-1820,P12-1033,0,0.0764336,"Missing"
W18-6319,H91-1060,0,0.508206,"Missing"
W18-6319,E06-1032,0,0.276825,"Missing"
W18-6319,2004.iwslt-evaluation.1,0,0.193566,"Missing"
W18-6319,P05-1033,0,0.100948,"Missing"
W18-6319,J93-2004,0,0.0615117,"to compute preprocessing the F1 metric that is commonly reported and compared across parsing papers. Computing parser F1 comes with its own set of hidden parameters and edge cases. Should one count the TOP (ROOT) node? What about -NONE- nodes? Punctuation? Should any labels be considered equivalent? These boundary cases are resolved by that community’s adoption of a standard codebase, evalb,9 which included a parameters file that answers each of these questions.10 This has facilitated almost thirty years of comparisons on treebanks such as the Wall Street Journal portion of the Penn Treebank (Marcus et al., 1993). (no user processing) (user processing) source metric score Figure 1: The proper pipeline for computing reported BLEU scores. White boxes denote user-supplied processing, and the black box, metric-supplied. The user should not touch the reference, while the metric applies its own processing to the system output and reference. 2.4 Problem: Dataset specification Other tricky details exist in the management of datasets. It has been common over the past few years to report results on the English→German arc of the WMT’14 dataset. It is unfortunate, therefore, that for this track (and this track al"
W18-6319,P11-2031,0,0.0578645,"Missing"
W18-6319,P02-1040,0,0.106227,"reprocessing schemes have a large effect on scores (§2.2). Importantly, BLEU scores computed against differently-processed references are not comparable. • Papers vary in the hidden parameters and schemes they use, yet often do not report them (§2.3). Even when they do, it can be hard to discover the details. Introduction Science is the process of formulating hypotheses, making predictions, and measuring their outcomes. In machine translation research, the predictions are made by models whose development is the focus of the research, and the measurement, more often than not, is done via BLEU (Papineni et al., 2002). BLEU’s relative language independence, its ease of computation, and its reasonable correlation with human judgments have led to its adoption as the dominant metric for machine translation research. On the whole, it has been a boon to the community, providing a fast and cheap way for researchers to gauge the performance of their models. Together with larger-scale controlled manual evaluations, BLEU has shepTogether, these issues make it difficult to evaluate and compare BLEU scores across papers, which impedes comparison and replication. I quantify these issues and show that they are serious,"
W18-6319,D07-1066,0,0.0781756,"Missing"
W18-6319,P17-1012,0,0.0370647,"Missing"
W18-6319,J18-3002,0,0.052248,"Missing"
W18-6319,E14-1047,0,0.0537018,"Missing"
W18-6319,E17-3017,0,0.0952387,"Missing"
W18-6319,P15-1001,0,0.0295704,"Missing"
W18-6319,P16-1162,0,0.55957,"-supplied preprocessing were inadvertently applied to the reference. • metric. Only the metric-internal tokenization of the official WMT scoring script, mteval-v13a.pl, is applied.7 The changes in each column show the effect these different schemes have, as high as 1.8 for one arc, and averaging around 1.0. The biggest is the treatment of case, which is well known, yet many papers are not clear about whether they report cased or case-insensitive BLEU. Allowing the user to handle pre-processing of the reference has other traps. For example, many systems (particularly before sub-word splitting (Sennrich et al., 2016) was proposed) limited the vocabulary in their attempt to deal with unknown words. It’s possible that these papers applied this same unknown-word masking to the references, too, which would artificially inflate BLEU scores. Such mistakes are easy to introduce in researcher pipelines.8 2.3 configuration metriclc (unclear) user or metric (unclear) user user or userlc (unclear) user or userlc (unclear) user, metric Table 2: Benchmarks set by well-cited papers use different BLEU configurations (Table 1). Which one was used is often difficult to determine. least possible to reconstruct comparable n"
W18-6319,P07-2045,0,0.0153694,"Scoring the online-B system with one reference produces a BLEU score of 22.04, and with two, 25.25. As another example, the NIST OpenMT Arabic–English and Chinese– English test sets4 provided four references and consequently yielded BLEU scores in the high 40s (and now, low 50s). Since these numbers are all gathered together under the label “BLEU”, over time, they leave an impression in people’s minds of very high BLEU scores for some language pairs or test sets relative to others, but without this critical distinguishing detail. • basic. User-supplied preprocessing with the M OSES tokenizer (Koehn et al., 2007).5 • split. Splitting compounds, as in Luong et al. (2015a):6 e.g., rich-text → rich - text. • unk. All word types not appearing at least twice in the target side of the WMT training data (with “basic” tokenization) are mapped to UNK. This hypothetical scenario could 5 Arguments -q -no-escape -protected basic-protected-patterns -l LANG. 6 Their use of compound splitting is not mentioned in the paper, but only here: http://nlp.stanford.edu/ projects/nmt. 3 pip3 install sacrebleu https://catalog.ldc.upenn.edu/ LDC2010T21 4 187 config basic split unk metric range basiclc splitlc unklc metriclc ra"
W18-6319,D15-1166,0,0.361144,"BLEU score of 22.04, and with two, 25.25. As another example, the NIST OpenMT Arabic–English and Chinese– English test sets4 provided four references and consequently yielded BLEU scores in the high 40s (and now, low 50s). Since these numbers are all gathered together under the label “BLEU”, over time, they leave an impression in people’s minds of very high BLEU scores for some language pairs or test sets relative to others, but without this critical distinguishing detail. • basic. User-supplied preprocessing with the M OSES tokenizer (Koehn et al., 2007).5 • split. Splitting compounds, as in Luong et al. (2015a):6 e.g., rich-text → rich - text. • unk. All word types not appearing at least twice in the target side of the WMT training data (with “basic” tokenization) are mapped to UNK. This hypothetical scenario could 5 Arguments -q -no-escape -protected basic-protected-patterns -l LANG. 6 Their use of compound splitting is not mentioned in the paper, but only here: http://nlp.stanford.edu/ projects/nmt. 3 pip3 install sacrebleu https://catalog.ldc.upenn.edu/ LDC2010T21 4 187 config basic split unk metric range basiclc splitlc unklc metriclc rangelc en-cs 20.7 20.7 20.9 20.1 0.6 21.2 21.3 21.4 20.6 0"
W18-6319,P15-1002,0,\N,Missing
W18-6319,W17-4717,1,\N,Missing
W19-5301,W19-5424,1,0.858444,"Missing"
W19-5301,W19-5306,0,0.248769,"al., 2019) University of Kyoto (Cromieres and Kurohashi, 2019) Lingua Custodia (Burlot, 2019) LIUM (Bougares et al., 2019) LMU Munich (Stojanovski and Fraser, 2019; Stojanovski et al., 2019) MLLP, Technical University of Valencia (Iranzo-Sánchez et al., 2019) Microsoft Translator (Junczys-Dowmunt, 2019) Microsoft Research Asia (Xia et al., 2019) Northeastern University / NiuTrans Co., Ltd. (Li et al., 2019a) National Institute of Information and Communications Technology (Dabre et al., 2019; Marie et al., 2019b) National Research Council of Canada (Littell et al., 2019) Bo˘gaziçi University (Biçici, 2019) PROMT LLC (Molchanov, 2019) University of Groningen (Toral et al., 2019) RWTH Aachen (Rosendahl et al., 2019) TALP Research Center, Universitat Politècnica de Catalunya (Casas et al., 2019) University of Tartu (Tättar et al., 2019) Tilde (Pinnis et al., 2019) Universitat d’Alacant (Sánchez-Cartagena et al., 2019) University of Cambridge (Stahlberg et al., 2019) Saarland University, DFKI (España-Bonet and Ruiter, 2019) University of Edinburgh (Bawden et al., 2019a) University of Maryland (Briakou and Carpuat, 2019) (no associated paper) University of Sydney (Ding and Tao, 2019) (no associated"
W19-5301,D18-1549,0,0.116951,"s are available for this system. 2.5.7 BASELINE - RE - RERANK (no associated CUNI-T RANSFORMER -T2T2018 (Popel, 2018) is the exact same system as used last year. paper) BASELINE - RE - RERANK is a standard Transformer, with corpus filtering, pre-processing, postprocessing, averaging and ensembling as well as n-best list reranking. 2.5.8 CUNI-T RANSFORMER -M ARIAN (Popel et al., 2019) is a “reimplementation” of the last year’s system (Popel, 2018) in Marian (JunczysDowmunt et al., 2018). CA I RE (Liu et al., 2019) CUNI-U NSUPERVISED -NER- POST (Kvapilíková et al., 2019) follows the strategy of Artetxe et al. (2018), creating a seed phrase-based system where the phrase table is initialized from cross-lingual embedding mappings trained on monolingual data, followed by a neural machine translation system trained on synthetic parallel corpus. The synthetic corpus is produced by the seed phrase-based MT system or by a such a model refined through iterative back-translation. CUNI-U NSUPERVISED -NER- POST further focuses on the handling of named entities, i.e. the part of vocabulary where the cross-lingual embedding mapping suffer most. CA I RE is a hybrid system that took part only in the unsupervised track."
W19-5301,D18-1332,0,0.0215805,"et al., 2018). For English↔Gujarati, synthetic parallel data from two sources, backtranslation and pivoting through Hindi, is produced using unsupervised and semi-supervised NMT models, pre-trained using a cross-lingual language objective (Lample and Conneau, 2019) For German→English, the impact of vast amounts of back-translated training data on translation quality is studied, and some additional insights are gained over (Edunov et al., 2018). Towards the end of training, for German→English and Chinese↔English, the mini-batch size was increased up to fifty-fold by delaying gradient updates (Bogoychev et al., 2018) as an alternative to learning rate cooldown (Smith, 2018). For Chinese↔English, a comparison of different segmentation strategies showed that character-based decoding was superior to the translation of subwords when translating into Chinese. Pre-processing strategies were also investigated for English→Czech, showing that preprocessing can be simplified without loss to MT quality. UEDIN’s main findings on the Chinese↔English translation task are that character-level model on the Chinese side can be used when translating into Chinese to improve the BLEU score. The same does not hold when transl"
W19-5301,W19-5351,0,0.0505622,"Missing"
W19-5301,W19-5423,0,0.0419015,"Missing"
W19-5301,W18-6412,1,0.856623,"Missing"
W19-5301,W19-5305,0,0.0496912,"Missing"
W19-5301,W12-3102,1,0.474924,"Missing"
W19-5301,W19-5310,0,0.0432396,"Missing"
W19-5301,W19-5425,0,0.0236032,"ys-Dowmunt et al., 2018) and Phrase-based machine translation system (implemented with Moses) and for the Spanish-Portuguese task. The system combination included features formerly presented in (Marie and Fujita, 2018), including scores left-to-right and right-to-left, sentence level translation probabilities and language model scores. Also authors provide contrastive results with an unsupervised phrase-based MT system which achieves quite close results to their primary system. Authors associate high performance of the unsupervised system to the language similarity. Incomslav: Team INCOMSLAV (Chen and Avgustinova, 2019) by Saarlad University participated in the Czech to Polish translation task only. The team’s primary submission builds on a transformer-based NMT baseline with back translation which has been submitted one of their contrastive submission. Incomslav’s primary system is a phoneme-based system re-scored using their NMT baseline. A second contrastive submission builds our phrase-based SMT system combined with a joint BPE model. NITS-CNLP: The NITS-CNLP team (Laskar et al., 2019) by the National Institute of Technology Silchar in India submitted results to the HI-NE translation task in both directi"
W19-5301,W07-0718,1,0.530103,"ojar Charles University Yvette Graham Barry Haddow Dublin City University University of Edinburgh Philipp Koehn JHU / University of Edinburgh Mathias Müller University of Zurich Marta R. Costa-jussà Christian Federmann UPC Microsoft Cloud + AI Shervin Malmasi Harvard Medical School Santanu Pal Saarland University Matt Post JHU Abstract Introduction The Fourth Conference on Machine Translation (WMT) held at ACL 20191 hosts a number of shared tasks on various aspects of machine translation. This conference builds on 13 previous editions of WMT as workshops and conferences (Koehn and Monz, 2006; Callison-Burch et al., 2007, 2008, 2009, 2010, 2011, 2012; Bojar et al., 2013, 2014, 2015, 2016, 2017, 2018). This year we conducted several official tasks. We report in this paper on the news and similar translation tasks. Additional shared tasks are described in separate papers in these proceedings: • biomedical translation (Bawden et al., 2019b) • automatic post-editing (Chatterjee et al., 2019) • metrics (Ma et al., 2019) • quality estimation (Fonseca et al., 2019) • parallel corpus filtering (Koehn et al., 2019) • robustness (Li et al., 2019b) In the news translation task (Section 2), participants were asked to tra"
W19-5301,P16-2058,1,0.819865,"ipated with the Transformer (Vaswani et al., 2017) implemented in the OpenNMT toolkit. They focused on word segmentation methods and compared a cognate-aware segmentation method, Cognate Morfessor (Grönroos et al., 2018), with character segmentation and unsupervised segmentation methods. As primary submission they submitted this Cognate Morfessor that optimizes subword segmentations consistently for cognates. They participated for all translation directions in Spanish-Portuguese and Czech-Polish, and this Cognate Morfessor performed better for Czech-Polish, while characterbased segmentations (Costa-jussà and Fonollosa, 2016), while much more inefficient, were superior for Spanish-Portuguese. UPC-TALP: The UPC-TALP team (Biesialska et al., 2019) by the Universitat Politècnica de Catalunya submitted a Transformer (implemented with Fairseq (Ott et al., 2019)) for the Czechto-Polish task and a Phrase-based system (implemented with Moses (Koehn et al., 2007)) for Spanish-to-Portuguese. They tested adding monolingual data to the NMT system by copying the same data on the source and target sides, with negative results. Also, their system combination based on sentence-level BLEU in back-translation 5.4 Conclusion of Simi"
W19-5301,W08-0309,1,0.659809,"Missing"
W19-5301,W18-3931,1,0.820211,"or they use English as a pivot language to translate between resource-poorer languages. The interest in English is reflected, for example, in the WMT translation tasks (e.g. News, Biomedical) which have always included language pairs in which texts are translated to and/or from English. With the widespread use of MT technology, there is more and more interest in training systems to translate between languages other than English. One evidence of this is the need of directly translating between pairs of similar languages, varieties, and dialects (Zhang, 1998; Marujo et al., 2011; Hassani, 2017; Costa-jussà et al., 2018). The main challenge is to take advantage of the similarity between languages to overcome the limitation given the low amount of available parallel data to produce an accurate output. Given the interest of the community in this topic we organize, for the first time at WMT, a shared task on ""Similar Language Translation"" to evaluate the performance of state-of-the-art translation systems on translating between pairs of languages from the same language family. We provide participants with training and testing data from three language pairs: Spanish - Portuguese (Romance languages), Czech - Polis"
W19-5301,W19-5312,0,0.0773587,"Missing"
W19-5301,W19-5313,0,0.0931699,"University (Marchisio et al., 2019) (no associated paper) University of Saarland (Mondal et al., 2019) Kingsoft AI (Guo et al., 2019) University of Kyoto (Cromieres and Kurohashi, 2019) Lingua Custodia (Burlot, 2019) LIUM (Bougares et al., 2019) LMU Munich (Stojanovski and Fraser, 2019; Stojanovski et al., 2019) MLLP, Technical University of Valencia (Iranzo-Sánchez et al., 2019) Microsoft Translator (Junczys-Dowmunt, 2019) Microsoft Research Asia (Xia et al., 2019) Northeastern University / NiuTrans Co., Ltd. (Li et al., 2019a) National Institute of Information and Communications Technology (Dabre et al., 2019; Marie et al., 2019b) National Research Council of Canada (Littell et al., 2019) Bo˘gaziçi University (Biçici, 2019) PROMT LLC (Molchanov, 2019) University of Groningen (Toral et al., 2019) RWTH Aachen (Rosendahl et al., 2019) TALP Research Center, Universitat Politècnica de Catalunya (Casas et al., 2019) University of Tartu (Tättar et al., 2019) Tilde (Pinnis et al., 2019) Universitat d’Alacant (Sánchez-Cartagena et al., 2019) University of Cambridge (Stahlberg et al., 2019) Saarland University, DFKI (España-Bonet and Ruiter, 2019) University of Edinburgh (Bawden et al., 2019a) University of"
W19-5301,W19-5314,0,0.0200465,"Bo˘gaziçi University (Biçici, 2019) PROMT LLC (Molchanov, 2019) University of Groningen (Toral et al., 2019) RWTH Aachen (Rosendahl et al., 2019) TALP Research Center, Universitat Politècnica de Catalunya (Casas et al., 2019) University of Tartu (Tättar et al., 2019) Tilde (Pinnis et al., 2019) Universitat d’Alacant (Sánchez-Cartagena et al., 2019) University of Cambridge (Stahlberg et al., 2019) Saarland University, DFKI (España-Bonet and Ruiter, 2019) University of Edinburgh (Bawden et al., 2019a) University of Maryland (Briakou and Carpuat, 2019) (no associated paper) University of Sydney (Ding and Tao, 2019) (no associated paper) 8 participated in all language pairs. The translations from the Table 5: Participants in the shared translation task. Not all teams online systems were not submitted by their respective companies but were obtained by us, and are therefore anonymized in a fashion consistent with previous years of the workshop. 2.5.6 BTRANS only the middle sentence was considered for the final translation hypothesis, otherwise shorter context of two sentences or just a single sentence was used. Unfortunately, no details are available for this system. 2.5.7 BASELINE - RE - RERANK (no associ"
W19-5301,D18-1045,0,0.0285693,"xt was morphologically segmented with Apertium. The UEDIN systems are supervised NMT systems based on the transformer architecture and trained using Marian (Junczys-Dowmunt et al., 2018). For English↔Gujarati, synthetic parallel data from two sources, backtranslation and pivoting through Hindi, is produced using unsupervised and semi-supervised NMT models, pre-trained using a cross-lingual language objective (Lample and Conneau, 2019) For German→English, the impact of vast amounts of back-translated training data on translation quality is studied, and some additional insights are gained over (Edunov et al., 2018). Towards the end of training, for German→English and Chinese↔English, the mini-batch size was increased up to fifty-fold by delaying gradient updates (Bogoychev et al., 2018) as an alternative to learning rate cooldown (Smith, 2018). For Chinese↔English, a comparison of different segmentation strategies showed that character-based decoding was superior to the translation of subwords when translating into Chinese. Pre-processing strategies were also investigated for English→Czech, showing that preprocessing can be simplified without loss to MT quality. UEDIN’s main findings on the Chinese↔Engl"
W19-5301,W18-6410,0,0.0193718,"ormance can be found in Hindi-Nepali (both directions), where the best performing system is around 50 BLEU (53 for Hindi-to-Nepali and 49.1 for Nepali-toHindi), and the lowest entry is 1.4 for Hindi-toNepali and 0 for Nepali-to-Hindi. The lowest variance is for Polish-to-Czech and it may be because only two teams participated. UHelsinki: The University of Helsinki team (Scherrer et al., 2019) participated with the Transformer (Vaswani et al., 2017) implemented in the OpenNMT toolkit. They focused on word segmentation methods and compared a cognate-aware segmentation method, Cognate Morfessor (Grönroos et al., 2018), with character segmentation and unsupervised segmentation methods. As primary submission they submitted this Cognate Morfessor that optimizes subword segmentations consistently for cognates. They participated for all translation directions in Spanish-Portuguese and Czech-Polish, and this Cognate Morfessor performed better for Czech-Polish, while characterbased segmentations (Costa-jussà and Fonollosa, 2016), while much more inefficient, were superior for Spanish-Portuguese. UPC-TALP: The UPC-TALP team (Biesialska et al., 2019) by the Universitat Politècnica de Catalunya submitted a Transform"
W19-5301,W19-5317,0,0.114089,"ed paper) (Liu et al., 2019) Charles University (Popel et al., 2019; Kocmi and Bojar, 2019) and (Kvapilíková et al., 2019) Kumamoto University, Telkom University, Indonesian Institute of Sciences (Budiwati et al., 2019) DFKI (Zhang and van Genabith, 2019) eTranslation (Oravecz et al., 2019) Facebook AI Research (Ng et al., 2019) GTCOM (Bei et al., 2019) University of Helsinki (Talman et al., 2019) IIIT Hyderabad (Goyal and Sharma, 2019) IIT Patna (Sen et al., 2019) Johns Hopkins University (Marchisio et al., 2019) (no associated paper) University of Saarland (Mondal et al., 2019) Kingsoft AI (Guo et al., 2019) University of Kyoto (Cromieres and Kurohashi, 2019) Lingua Custodia (Burlot, 2019) LIUM (Bougares et al., 2019) LMU Munich (Stojanovski and Fraser, 2019; Stojanovski et al., 2019) MLLP, Technical University of Valencia (Iranzo-Sánchez et al., 2019) Microsoft Translator (Junczys-Dowmunt, 2019) Microsoft Research Asia (Xia et al., 2019) Northeastern University / NiuTrans Co., Ltd. (Li et al., 2019a) National Institute of Information and Communications Technology (Dabre et al., 2019; Marie et al., 2019b) National Research Council of Canada (Littell et al., 2019) Bo˘gaziçi University (Biçici, 201"
W19-5301,W19-5315,0,0.0266353,"Missing"
W19-5301,W19-5318,0,0.198338,"ance of the systems when translating from French to German seems to heavily depend on the 7 http://data.statmt.org/wmt19/ translation-task/dev.tgz 6 Systems MSRA.MADL eTranslation LIUM MLLP-UPV onlineA TartuNLP onlineB onlineY onlineG onlineX FULL 47.3 45.4 43.7 41.5 40.8 39.2 39.1 39.0 38.5 38.1 source FR 38.3 37.4 37.5 36.4 35.4 34.8 35.3 34.7 34.6 35.6 source DE 50.0 47.8 45.5 43.0 42.3 40.5 40.2 40.2 39.7 38.8 evaluations. In the rest of this sub-section, we provide brief details of the submitted systems, for those in cases where the authors provided such details. 2.5.1 AFRL - SYSCOMB 19 (Gwinnup et al., 2019) is a system combination of a Marian ensemble system, two distinct OpenNMT systems, a Sockeyebased Elastic Weight Consolidation system, and one Moses phrase-based system. Table 3: French→German Meteor scores. Systems MSRA.MADL LinguaCustodia MLLP_UPV Kyoto_University_T2T LIUM onlineY onlineB TartuNLP onlineA onlineX onlineG FULL 52.0 51.3 49.5 48.8 48.3 47.5 46.4 46.3 45.3 42.7 41.7 source FR 51.9 52.5 49.9 49.7 46.5 43.7 43.7 45.0 43.7 41.6 40.9 source DE 52.0 51.0 49.4 48.6 48.7 48.4 47.0 46.7 45.8 42.9 41.9 AFRL- EWC (Gwinnup et al., 2019) is a Sockeye Transformer system trained with the de"
W19-5301,W19-5316,0,0.109691,"boratory (Gwinnup et al., 2019) Apertium (Pirinen, 2019) Apprentice (Li and Specia, 2019) Aylien Ltd. (Hokamp et al., 2019) Baidu (Sun et al., 2019) (no associated paper) (no associated paper) (Liu et al., 2019) Charles University (Popel et al., 2019; Kocmi and Bojar, 2019) and (Kvapilíková et al., 2019) Kumamoto University, Telkom University, Indonesian Institute of Sciences (Budiwati et al., 2019) DFKI (Zhang and van Genabith, 2019) eTranslation (Oravecz et al., 2019) Facebook AI Research (Ng et al., 2019) GTCOM (Bei et al., 2019) University of Helsinki (Talman et al., 2019) IIIT Hyderabad (Goyal and Sharma, 2019) IIT Patna (Sen et al., 2019) Johns Hopkins University (Marchisio et al., 2019) (no associated paper) University of Saarland (Mondal et al., 2019) Kingsoft AI (Guo et al., 2019) University of Kyoto (Cromieres and Kurohashi, 2019) Lingua Custodia (Burlot, 2019) LIUM (Bougares et al., 2019) LMU Munich (Stojanovski and Fraser, 2019; Stojanovski et al., 2019) MLLP, Technical University of Valencia (Iranzo-Sánchez et al., 2019) Microsoft Translator (Junczys-Dowmunt, 2019) Microsoft Research Asia (Xia et al., 2019) Northeastern University / NiuTrans Co., Ltd. (Li et al., 2019a) National Institute of"
W19-5301,E14-1047,1,0.904335,"Missing"
W19-5301,W19-5427,0,0.0467733,"Missing"
W19-5301,W19-5322,1,0.807321,"Missing"
W19-5301,W19-5302,1,0.715203,"20191 hosts a number of shared tasks on various aspects of machine translation. This conference builds on 13 previous editions of WMT as workshops and conferences (Koehn and Monz, 2006; Callison-Burch et al., 2007, 2008, 2009, 2010, 2011, 2012; Bojar et al., 2013, 2014, 2015, 2016, 2017, 2018). This year we conducted several official tasks. We report in this paper on the news and similar translation tasks. Additional shared tasks are described in separate papers in these proceedings: • biomedical translation (Bawden et al., 2019b) • automatic post-editing (Chatterjee et al., 2019) • metrics (Ma et al., 2019) • quality estimation (Fonseca et al., 2019) • parallel corpus filtering (Koehn et al., 2019) • robustness (Li et al., 2019b) In the news translation task (Section 2), participants were asked to translate a shared test set, optionally restricting themselves to the provided training data (“constrained” condition). We 1 Christof Monz University of Amsterdam Marcos Zampieri University of Wolverhampton held 18 translation tasks this year, between English and each of Chinese, Czech (into Czech only), German, Finnish, Lithuanian, and Russian. New this year were Gujarati↔English and Kazakh↔English. B"
W19-5301,W19-5333,0,0.0923169,"Missing"
W19-5301,W19-5353,0,0.0658382,"Missing"
W19-5301,W19-5430,1,0.873847,"Missing"
W19-5301,W19-5431,0,0.0199622,"Universitat Politècnica de València (UPV) participated with a Transformer (implemented with FairSeq (Ott et al., 2019)) and a finetuning strategy for domain adaptaion in the task of Spanish-Portuguese. Fine-tunning on the development data provide improvements of almost 12 BLEU points, which may explain their clear best performance in the task for this language pair. As a contrastive system authors provided only for the Portuguese-to-Spanish a novel 2D alternating RNN model which did not respond so well when fine-tunning. UBC-NLP: Team UBC-NLP from the University of British Columbia in Canada (Przystupa and Abdul-Mageed, 2019) compared the performance of the LSTM plus attention (Bahdanau et al., 2015) and Transformer (Vaswani et al., 2017) (implemented in OpenNMT toolkit22 ) perform for the three tasks at hand. Authors use backtranslation to introduce monolingual data in their systems. LSTM plus attention outperformed Transformer for Hindi-Nepali, and viceversa for the other two tasks. As reported by the authors, Hindi-Nepali task provides much more shorter sentences than KYOTOUNIVERSITY: Kyoto University’s submission, listed simply as KYOTO in Table 25 for PT → ES task is based on transformer NMT system. They used"
W19-5301,P02-1040,0,0.11337,"ation of the source (CS), and a second encoder to encode sub-word (byte-pair-encoding) information of the source (CS). The results obtained by their system in translating from Czech→Polish and comment on the impact of out-of-domain test data in the performance of their system. UDSDFKI ranked second among ten teams in Czech– Polish translation. 5.3 Results We present results for the three language pairs, each of them in the two possible directions. For this first edition of the Similar Translation Task and differently from News task, evaluation was only performed on automatic basis using BLEU (Papineni et al., 2002) and TER (Snover et al., 2006) measures. Each language direction is reported in one different table which contain information of the team; type of system, either contrastive (C) or primary (P), and the BLEU and TER results. In general, primary systems tend to be better than contrastive systems, as expected, but there are some exceptions. Even if we are presenting 3 pairs of languages each pair belonging to the same family, translation quality in terms of BLEU varies signficantly. While the best systems for Spanish-Portuguese are above 64 BLEU and below 21 TER (see Tables 26 and 27), best syste"
W19-5301,W19-5354,0,0.0611791,"Missing"
W19-5301,W18-6486,0,0.0189853,"the agglutinative nature of Kazakh, (ii) data from an additional language (Russian), given the scarcity of English–Kazakh data and (iii) synthetic data for the source language filtered using language-independent sentence similarity. RUG _ KKEN _ MORFESSOR Tilde developed both constrained and unconstrained NMT systems for English-Lithuanian and Lithuanian-English using the Marian toolkit. All systems feature ensembles of four to five transformer models that were trained using the quasi-hyperbolic Adam optimiser (Ma and Yarats, 2018). Data for the systems were prepared using TildeMT filtering (Pinnis, 2018) and preprocessing (Pinnis et al., 2018) methods. For unconstrained systems, data were additionally filtered using dual conditional cross-entropy filtering (Junczys-Dowmunt, 2018a). All systems were trained using iterative back-translation (Rikters, 2018) and feature synthetic data that allows training NMT systems to support handling of unknown phenomena (Pinnis et al., 2017). During translation, automatic named entity and nontranslatable phrase post-editing were performed. For constrained systems, named entities and nontranslatable phrase lists were extracted from the parallel training data."
W19-5301,W19-5335,0,0.0408704,"Missing"
W19-5301,W19-5344,1,0.904781,"n Institute of Sciences (Budiwati et al., 2019) DFKI (Zhang and van Genabith, 2019) eTranslation (Oravecz et al., 2019) Facebook AI Research (Ng et al., 2019) GTCOM (Bei et al., 2019) University of Helsinki (Talman et al., 2019) IIIT Hyderabad (Goyal and Sharma, 2019) IIT Patna (Sen et al., 2019) Johns Hopkins University (Marchisio et al., 2019) (no associated paper) University of Saarland (Mondal et al., 2019) Kingsoft AI (Guo et al., 2019) University of Kyoto (Cromieres and Kurohashi, 2019) Lingua Custodia (Burlot, 2019) LIUM (Bougares et al., 2019) LMU Munich (Stojanovski and Fraser, 2019; Stojanovski et al., 2019) MLLP, Technical University of Valencia (Iranzo-Sánchez et al., 2019) Microsoft Translator (Junczys-Dowmunt, 2019) Microsoft Research Asia (Xia et al., 2019) Northeastern University / NiuTrans Co., Ltd. (Li et al., 2019a) National Institute of Information and Communications Technology (Dabre et al., 2019; Marie et al., 2019b) National Research Council of Canada (Littell et al., 2019) Bo˘gaziçi University (Biçici, 2019) PROMT LLC (Molchanov, 2019) University of Groningen (Toral et al., 2019) RWTH Aachen (Rosendahl et al., 2019) TALP Research Center, Universitat Politècnica de Catalunya (Casas e"
W19-5301,W19-5346,0,0.197908,"rtium (Pirinen, 2019) Apprentice (Li and Specia, 2019) Aylien Ltd. (Hokamp et al., 2019) Baidu (Sun et al., 2019) (no associated paper) (no associated paper) (Liu et al., 2019) Charles University (Popel et al., 2019; Kocmi and Bojar, 2019) and (Kvapilíková et al., 2019) Kumamoto University, Telkom University, Indonesian Institute of Sciences (Budiwati et al., 2019) DFKI (Zhang and van Genabith, 2019) eTranslation (Oravecz et al., 2019) Facebook AI Research (Ng et al., 2019) GTCOM (Bei et al., 2019) University of Helsinki (Talman et al., 2019) IIIT Hyderabad (Goyal and Sharma, 2019) IIT Patna (Sen et al., 2019) Johns Hopkins University (Marchisio et al., 2019) (no associated paper) University of Saarland (Mondal et al., 2019) Kingsoft AI (Guo et al., 2019) University of Kyoto (Cromieres and Kurohashi, 2019) Lingua Custodia (Burlot, 2019) LIUM (Bougares et al., 2019) LMU Munich (Stojanovski and Fraser, 2019; Stojanovski et al., 2019) MLLP, Technical University of Valencia (Iranzo-Sánchez et al., 2019) Microsoft Translator (Junczys-Dowmunt, 2019) Microsoft Research Asia (Xia et al., 2019) Northeastern University / NiuTrans Co., Ltd. (Li et al., 2019a) National Institute of Information and Communicatio"
W19-5301,W19-5341,0,0.0172601,"A,B,G,X,Y. For presentation of the results, systems are treated as either constrained or unconstrained, depending on whether their models were trained only on the provided data. Since we do not know how they were built, the online systems are treated as unconstrained during the automatic and human AYLIEN _ MULTILINGUAL (Hokamp et al., 2019) The Aylien research team built a Multilingual NMT system which is trained on all WMT2019 language pairs in all directions, then fine-tuned for a small number of iterations on Gujarati-English data only, including some self-backtranslated data. 2.5.5 BAIDU (Sun et al., 2019) Baidu systems are based on the Transformer architecture with several improvements. Data selection, back translation, data augmentation, knowledge distillation, domain adaptation, model ensemble and re-ranking are employed and proven effective in our experiments. 7 Team AFRL A PERTIUM - FIN - ENG A PPRENTICE - C AYLIEN _ MULTILINGUAL BAIDU BTRANS BASELINE - RE - RERANK CA I RE CUNI DBMS-KU DFKI - NMT E T RANSLATION FACEBOOK FAIR GTCOM H ELSINKI NLP IIITH-MT IITP JHU JUMT JU_S AARLAND KSAI K YOTO U NIVERSITY L INGUA C USTODIA LIUM LMU-NMT MLLP-UPV MS T RANSLATOR MSRA N IU T RANS NICT NRC PARFDA"
W19-5301,P16-1162,1,0.310296,"ssible, 2.5.13 E T RANSLATION (Oravecz et al., 2019) E T RANSLATION En-De E T RANSLATION ’s EnDe system is an ensemble of 3 base Transformers and a Transformer-type language model, trained from all available parallel data (cleaned up and filtered with dual conditional cross-entropy filtering) and with additional back-translated data generated 9 2.5.17 from monolingual news. Each Transformer model is fine tuned on previous years’ test sets. H ELSINKI NLP is a Transformer (Vaswani et al., 2017) style model implemented in OpenNMTpy using a variety of corpus filtering techniques, truecasing, BPE (Sennrich et al., 2016), backtranslation, ensembling and fine-tuning for domain adaptation. E T RANSLATION Fr-De The Fr-De system is an ensemble of 2 big Transformers (with size 8192 FFN layers). Back-translation data was selected using topic modelling techniques to tune the model towards the domain defined in the task. 2.5.18 En-Lt The En-Lt system is an ensemble of 2 big Transformers (as for Fr-De) and a Transformer type language model. The training data contains the Rapid corpus and the news domain back-translated data sets 2 times oversampled. E T RANSLATION 2.5.19 FACEBOOK FAIR (Ng et al., 2019) 2.5.20 JHU (Mar"
W19-5301,W19-5339,0,0.0767002,"Missing"
W19-5301,W19-5347,0,0.0328257,"Missing"
W19-5301,W19-5342,1,0.887858,"encia (Iranzo-Sánchez et al., 2019) Microsoft Translator (Junczys-Dowmunt, 2019) Microsoft Research Asia (Xia et al., 2019) Northeastern University / NiuTrans Co., Ltd. (Li et al., 2019a) National Institute of Information and Communications Technology (Dabre et al., 2019; Marie et al., 2019b) National Research Council of Canada (Littell et al., 2019) Bo˘gaziçi University (Biçici, 2019) PROMT LLC (Molchanov, 2019) University of Groningen (Toral et al., 2019) RWTH Aachen (Rosendahl et al., 2019) TALP Research Center, Universitat Politècnica de Catalunya (Casas et al., 2019) University of Tartu (Tättar et al., 2019) Tilde (Pinnis et al., 2019) Universitat d’Alacant (Sánchez-Cartagena et al., 2019) University of Cambridge (Stahlberg et al., 2019) Saarland University, DFKI (España-Bonet and Ruiter, 2019) University of Edinburgh (Bawden et al., 2019a) University of Maryland (Briakou and Carpuat, 2019) (no associated paper) University of Sydney (Ding and Tao, 2019) (no associated paper) 8 participated in all language pairs. The translations from the Table 5: Participants in the shared translation task. Not all teams online systems were not submitted by their respective companies but were obtained by us, and"
W19-5301,W19-5355,1,0.869964,"Missing"
W19-5301,W19-5350,0,0.0441915,"Missing"
W19-5301,P98-2238,0,0.38957,"n trained to translate texts from and to English or they use English as a pivot language to translate between resource-poorer languages. The interest in English is reflected, for example, in the WMT translation tasks (e.g. News, Biomedical) which have always included language pairs in which texts are translated to and/or from English. With the widespread use of MT technology, there is more and more interest in training systems to translate between languages other than English. One evidence of this is the need of directly translating between pairs of similar languages, varieties, and dialects (Zhang, 1998; Marujo et al., 2011; Hassani, 2017; Costa-jussà et al., 2018). The main challenge is to take advantage of the similarity between languages to overcome the limitation given the low amount of available parallel data to produce an accurate output. Given the interest of the community in this topic we organize, for the first time at WMT, a shared task on ""Similar Language Translation"" to evaluate the performance of state-of-the-art translation systems on translating between pairs of languages from the same language family. We provide participants with training and testing data from three language"
W19-5366,W18-2705,1,0.847977,"optimizer: 0.001, 0.0003, 0.0006 • Number of attention heads (head): 8, 16 • Number of layers (layer): 2, 4 Dataset sizes are shown in Table 4. JA→EN dataset Train-ALL Train-MTNT Valid-ALL Valid-MTNT Test18-MTNT EN→JA dataset Train-ALL Train-MTNT Valid-ALL Valid-MTNT Test18-MTNT Models segments words • Feed-forward layer size (ffsize): 1024, 2048 3.9m 6506 5416 965 1001 42.7m 155k 88k 23k 13k • Embedding and model size (embedding): 256, 518, 1024 segments words 3.9m 5775 5405 954 1002 42.9m 333k 111k 46k 13k The training process follows a continuedtraining procedure (c.f. Koehn et al. (2018); Khayrallah et al. (2018)): In Stage 1, we train systems from scratch on Train-ALL, and perform early stopping on Valid-ALL. This represents a mixed corpus with both in-domain and out-of-domain bitexts. For all models, we used batch sizes of 4,096 words, checkpointed every 2,000 updates, and stopped training with the bestperplexity checkpoint when validation perplexity on Valid-ALL had failed to improve for 16 consecutive checkpoints. In Stage 2, we fine-tuned the above systems by training on Train-MTNT, and perform early stopping on Valid-MTNT. Effectively, we initialize a new model with Stage 1 model weights, reset"
W19-5366,W18-6417,1,0.852469,"te (LR) for the ADAM optimizer: 0.001, 0.0003, 0.0006 • Number of attention heads (head): 8, 16 • Number of layers (layer): 2, 4 Dataset sizes are shown in Table 4. JA→EN dataset Train-ALL Train-MTNT Valid-ALL Valid-MTNT Test18-MTNT EN→JA dataset Train-ALL Train-MTNT Valid-ALL Valid-MTNT Test18-MTNT Models segments words • Feed-forward layer size (ffsize): 1024, 2048 3.9m 6506 5416 965 1001 42.7m 155k 88k 23k 13k • Embedding and model size (embedding): 256, 518, 1024 segments words 3.9m 5775 5405 954 1002 42.9m 333k 111k 46k 13k The training process follows a continuedtraining procedure (c.f. Koehn et al. (2018); Khayrallah et al. (2018)): In Stage 1, we train systems from scratch on Train-ALL, and perform early stopping on Valid-ALL. This represents a mixed corpus with both in-domain and out-of-domain bitexts. For all models, we used batch sizes of 4,096 words, checkpointed every 2,000 updates, and stopped training with the bestperplexity checkpoint when validation perplexity on Valid-ALL had failed to improve for 16 consecutive checkpoints. In Stage 2, we fine-tuned the above systems by training on Train-MTNT, and perform early stopping on Valid-MTNT. Effectively, we initialize a new model with Sta"
W19-5366,P07-2045,0,0.00872686,"1 We determined this threshold by eyeballing where in the ranked list the garbage started to thin out. • all of Europarl and News Commentary; 552 Proceedings of the Fourth Conference on Machine Translation (WMT), Volume 2: Shared Task Papers (Day 1) pages 552–558 c Florence, Italy, August 1-2, 2019. 2019 Association for Computational Linguistics training data sizes more manageable. We therefore did not compare against a model trained on all of the filtered data. We experimented with two preprocessing regimes. In the first, we applied standard preprocessing techniques from the Moses pipeline2 (Koehn et al., 2007), followed by subword splitting with BPE (Sennrich et al., 2016) using 32k merge operations. In the second scenario, we did not use any data preparation, instead applying sentencepiece (Kudo and Richardson, 2018) with subword regularization (Kudo, 2018) directly to the raw text. In this latter setting, we varied the size of the learned subword models, experimenting with 8k, 16k, 24k, and 32k. 2.2 Models 4 layers (BPE) 31.6 27.9 6 layers (BPE) + MTNT + filter + both 32.7 32.6 36.4 37.2 27.9 32.9 33.7 39.9 sp24k + filter sp24k + both 36.5 37.2 34.5 40.0 size WMT15 filter both MTNT18 filter both"
W19-5366,P18-1007,0,0.104464,"58 c Florence, Italy, August 1-2, 2019. 2019 Association for Computational Linguistics training data sizes more manageable. We therefore did not compare against a model trained on all of the filtered data. We experimented with two preprocessing regimes. In the first, we applied standard preprocessing techniques from the Moses pipeline2 (Koehn et al., 2007), followed by subword splitting with BPE (Sennrich et al., 2016) using 32k merge operations. In the second scenario, we did not use any data preparation, instead applying sentencepiece (Kudo and Richardson, 2018) with subword regularization (Kudo, 2018) directly to the raw text. In this latter setting, we varied the size of the learned subword models, experimenting with 8k, 16k, 24k, and 32k. 2.2 Models 4 layers (BPE) 31.6 27.9 6 layers (BPE) + MTNT + filter + both 32.7 32.6 36.4 37.2 27.9 32.9 33.7 39.9 sp24k + filter sp24k + both 36.5 37.2 34.5 40.0 size WMT15 filter both MTNT18 filter both 8k 16k 24k 36.0 36.2 36.5 33.9 33.9 34.5 36.5 36.9 37.2 38.7 39.7 40.0 Table 3: BLEU scores with the sentencepiece models and no other preprocessing. Observation 1 Improvements are to be had both from more data and from better (in-domain) data. Adding t"
W19-5366,D18-2012,0,0.0241351,"(WMT), Volume 2: Shared Task Papers (Day 1) pages 552–558 c Florence, Italy, August 1-2, 2019. 2019 Association for Computational Linguistics training data sizes more manageable. We therefore did not compare against a model trained on all of the filtered data. We experimented with two preprocessing regimes. In the first, we applied standard preprocessing techniques from the Moses pipeline2 (Koehn et al., 2007), followed by subword splitting with BPE (Sennrich et al., 2016) using 32k merge operations. In the second scenario, we did not use any data preparation, instead applying sentencepiece (Kudo and Richardson, 2018) with subword regularization (Kudo, 2018) directly to the raw text. In this latter setting, we varied the size of the learned subword models, experimenting with 8k, 16k, 24k, and 32k. 2.2 Models 4 layers (BPE) 31.6 27.9 6 layers (BPE) + MTNT + filter + both 32.7 32.6 36.4 37.2 27.9 32.9 33.7 39.9 sp24k + filter sp24k + both 36.5 37.2 34.5 40.0 size WMT15 filter both MTNT18 filter both 8k 16k 24k 36.0 36.2 36.5 33.9 33.9 34.5 36.5 36.9 37.2 38.7 39.7 40.0 Table 3: BLEU scores with the sentencepiece models and no other preprocessing. Observation 1 Improvements are to be had both from more data a"
W19-5366,D18-1050,0,0.109035,"irst performed word segmentation by Kytea (Neubig et al., 2011)6 , then ran the English Moses preprocessing pipeline to handle potential code-switched English/Japanese in the data. Finally, we induced BPE subword units with 10k, 30k, and 50k merge operations, independently for each side on the bitexts (JA→EN Train-ALL and EN→JA Train-ALL). Unlike the French-English systems, the JapaneseEnglish systems do not use shared BPE and embeddings. Training Data We trained systems using only the bitext data allowed in the shared task constrained setting: • The in-domain Reddit dataset–MTNT version 1.1 (Michel and Neubig, 2018)4 – consists of approximately 6k segments for training (which we label Train-MTNT) and 900 segments for validation (Valid-MTNT) in both JA→EN and EN→JA language directions. Additionally we use the included ”test set” (which we label Test18-MTNT) for internal BLEU benchmarks prior to submitting results for the official 2019 blindtest. We did not use the monolingual part of MTNT. 3.2 We use the Sockeye Transformer models for both JA→EN and EN→JA directions, similar to our French-English systems. The hyperparameter settings are different, however. We performed random search in the following hyper"
W19-5366,P11-2093,0,0.0126734,"model (with no other preprocessing) was just as good as the BPE model (with the Moses preprocessing pipeline). In one situation (adding the filtered data alone), it caused a gain of 0.8 over its BPE counterpart. We further conducted a small experiment varying the sentencepiece model size (Table 3). Larger sentencepiece models were consistently better in this relatively large-data setting. Our score on the official MTNT2019 blind test set was 40.2. 3 Japanese-English Systems 3.1 as the French–English system. For preprocessing on the Japanese side, we first performed word segmentation by Kytea (Neubig et al., 2011)6 , then ran the English Moses preprocessing pipeline to handle potential code-switched English/Japanese in the data. Finally, we induced BPE subword units with 10k, 30k, and 50k merge operations, independently for each side on the bitexts (JA→EN Train-ALL and EN→JA Train-ALL). Unlike the French-English systems, the JapaneseEnglish systems do not use shared BPE and embeddings. Training Data We trained systems using only the bitext data allowed in the shared task constrained setting: • The in-domain Reddit dataset–MTNT version 1.1 (Michel and Neubig, 2018)4 – consists of approximately 6k segmen"
W19-5366,E17-3017,0,0.0322505,"xt) helped on MTNT1 8 (+5) but caused a small drop on WMT15 (-0.1). Scoring At test time, we decoded with beam search using a beam of size 12. We scored with sacreBLEU (Post, 2018), with international tokenization.3 In the spirit of the robustness task, we measure BLEU not just on the reddit dataset, but also on the WMT15 newstest dataset, in order to examine how experimental variables vary in both in- and out-of-domain settings. We believe that testing both in- and outof-domain data is essential to measuring robustness. 2.4 MTNT18 Table 2: French–English translation results. We used Sockeye (Hieber et al., 2017), a sequence to sequence transduction framework written in Python and based on MXNet. Our models were variations of the Transformer architecture (Vaswani et al., 2017), mostly using default settings supplied with Sockeye: an embedding and model size of 512, a feed-forward layer size of 2048, 8 attention heads, and three-way tied embeddings. We used batch sizes of 4,096 words, checkpointed every 5,000 updates, and stopped training with the best-perplexity checkpoint when validation perplexity had failed to improve for 10 consecutive checkpoints. The initial learning rate was set to 0.0002, the"
W19-5366,W18-6319,1,0.893019,"33.7, +5.8) than adding the MTNT training data (27.9 → 32.9, +5), but the gains from both were even greater (+12). Observation 2 In order to ensure that our models did not increase accuracy on the MTNT data at the expense of in-domain data, we report scores on both WMT and MTNT test sets. In only one situation was there a problem: For the 6-layer Transformer, adding the MTNT data alone (without the large amount of filtered bitext) helped on MTNT1 8 (+5) but caused a small drop on WMT15 (-0.1). Scoring At test time, we decoded with beam search using a beam of size 12. We scored with sacreBLEU (Post, 2018), with international tokenization.3 In the spirit of the robustness task, we measure BLEU not just on the reddit dataset, but also on the WMT15 newstest dataset, in order to examine how experimental variables vary in both in- and out-of-domain settings. We believe that testing both in- and outof-domain data is essential to measuring robustness. 2.4 MTNT18 Table 2: French–English translation results. We used Sockeye (Hieber et al., 2017), a sequence to sequence transduction framework written in Python and based on MXNet. Our models were variations of the Transformer architecture (Vaswani et al."
W19-5366,W18-6478,0,0.0302442,"nese. Our goal was to evaluate the performance of reasonable state-of-the-art systems against both the robustness test set as well as more standard “general domain” test sets. We believe this is an important component of evaluating for actual robustness. In this way, we ensure that performance gains on robustness data are not purchased at the expense of this general-domain performance. Our systems used no monolingual data and relatively straightforward state-of-the-art techniques, and produced systems of roughly average performance. To filter the data, we applied dual cross-entropy filtering (Junczys-Dowmunt, 2018). We trained two smaller 4-layer Transformer models, one each for EN–FR and FR–EN, and used them to score the data according to the formula: exp(−(|s1 − s2 |+ 0.5 ∗ (s1 + s2 ))) where s1 is the score (a negative logprob) from the forward FR–EN model and s2 the score from the reverse EN–FR model. We then uniqued this data, sorted by score, and took a random sample of one million lines from the set of all sentence pairs with a score greater than 0.1.1 For all but FR–EN Gigaword, what remained was well less than a million lines. We did this both because prior work has indicated the utility of fil"
W19-5366,P16-1162,0,0.0537571,"ked list the garbage started to thin out. • all of Europarl and News Commentary; 552 Proceedings of the Fourth Conference on Machine Translation (WMT), Volume 2: Shared Task Papers (Day 1) pages 552–558 c Florence, Italy, August 1-2, 2019. 2019 Association for Computational Linguistics training data sizes more manageable. We therefore did not compare against a model trained on all of the filtered data. We experimented with two preprocessing regimes. In the first, we applied standard preprocessing techniques from the Moses pipeline2 (Koehn et al., 2007), followed by subword splitting with BPE (Sennrich et al., 2016) using 32k merge operations. In the second scenario, we did not use any data preparation, instead applying sentencepiece (Kudo and Richardson, 2018) with subword regularization (Kudo, 2018) directly to the raw text. In this latter setting, we varied the size of the learned subword models, experimenting with 8k, 16k, 24k, and 32k. 2.2 Models 4 layers (BPE) 31.6 27.9 6 layers (BPE) + MTNT + filter + both 32.7 32.6 36.4 37.2 27.9 32.9 33.7 39.9 sp24k + filter sp24k + both 36.5 37.2 34.5 40.0 size WMT15 filter both MTNT18 filter both 8k 16k 24k 36.0 36.2 36.5 33.9 33.9 34.5 36.5 36.9 37.2 38.7 39."
W19-5366,N19-1209,1,0.830789,"ining: Next, we perform continued training on the top 5 models. The results on Test18-MTNT are shown in Table 6. We observe consistent BLEU gains in these Stage 2 models, close to 2 or 3 points across all systems. This re-affirms the surprising effectiveness of a simple procedure such as continued training; but we should also note that preliminary efforts on English-French did not yield similar gains. Note that we do not measure Valid-ALL in this case since we now expect the models to be optimized specifically for MTNT; it is likely ValidALL scores will degrade due to catastrophic forgetting (Thompson et al., 2019). Results & Discussion The BLEU results for Stage 1 models are shown in Table 5. We performed random search in hyperparameter space, training approximately 40 models in each language-pair. The table is sorted by Test18-MTNT BLEU score and shows the top 5 models in terms of BLEU (id=a,b,c,d,e; id=z,y,x,w,v) as well as another 5 randomly selected model (id=e,f,g,h,i,j; id=u,t,s,r,q). Final Submission: In the final official submission, we performed an 4-ensemble of the Stage 2 Continued Training models of id=a,b,d,e for JA→EN and id=z,y,w,v for EN→JA. Note that the ensemble method in Sockeye curr"
W19-6618,D17-1098,0,0.0785982,"ms, but the approach was largely abandoned when sub-word methods (Sennrich et al., 2016) obviated the need. Most similar to this work in spirit is Crego et al. (2016), who mentioned that masking could be used to translate many “pass-through items” but did not conduct any further analysis towards the problem or the solution. Another solution for handling pass-through items is to add them as constraints during beam search. A number of approaches introduced modifications to beam search that ensured that desired words would be included in the output (Hokamp and Liu, 2017; Chatterjee et al., 2017; Anderson et al., 2017). One problem with these solutions is that decoding time generally grows very quickly with the number of constraints added. Hasler et al. (2018) showed that even two constraints cause decoding speed to increase by as much as five times. Post and Vilar (2018) introduced a fixed-beam-size variant which is constant in the number of constraints, but the constant overhead is still quite high. In terms of specific token types, Li et al. (2018), Ugawa et al. (2018) and Grundkiewicz and Heafield (2018) studied NMT models with better handling of named entities, either by adding named entity tags or emp"
W19-6618,J93-2003,0,0.102246,"ed multiple times. Obtaining weights Obtaining weights to use with bipartite matching is not straightforward. We experiment with two approaches: • Averaged attention scores. We average source attention scores across all decoder heads and layers in our model. • External aligner. We run a version of fastalign (Dyer et al., 2013). Both have problems. We use Transformers (Vaswani et al., 2017) in our experiments, but multihead Transformer attention is not the same thing as alignment (Jain and Wallace, 2019). Fast-align is fast and easy to use at inference time, but it is a variant of IBM Model 2 (Brown et al., 1993) and the HMM model (Vogel et al., 1996). Therefore, its translation model cannot distinguish among mask permutations, and its impoverished distortion model is not well-suited to the task of recovering permutations of identical masks. However, we consider both approaches worth testing on this coarser alignment task, where we are only concerned with Proceedings of MT Summit XVII, volume 1 Algorithm 1: Bipartite Matching Demasking Input: source sentence S = {s0 , . . . , sI−1 }, target sentence T = {t0 , . . . , tJ−1 }, soft alignment matrix A of size I × J Output: demasked target sentence T 0 T0"
W19-6618,2012.eamt-1.60,0,0.0135818,"t19/robustness.html Dublin, Aug. 19-23, 2019 |p. 184 Dataset French–English sents words Japanese–English sents words Europarl v.7 News commentary v.10 UN (complete) → UN (dict masks) KFTT JESC TED Talks 2.0m 200k 12.8m 1.1m - 50.2m 4.4m 316.2m 33.8m - 440k 3.2m 241k 9.7m 21m 4.0m newstest2014 MTNT1.1/valid 3,003 886 69k 34k 965 19k newstest2015 MTNT1.1/train MTNT1.1/test 1,500 19k 1,022 25k 660k 16k 6,506 1,001 128k 11k Table 2: Pre-tokenization data sizes in sentence and English words for FR–EN and JA–EN training (top), validation (middle), and testing (bottom). et al., 2017), and TED Talks (Cettolo et al., 2012) data. process. Each entity: • should be labeled as one of the following category: P ERSON, L OCATION, O RGANIZATION, C ITY, C OUNTRY; 4.2 Masks We obtain our set of mask types from two sources: a set of regular expressions, and a dictionary extracted from the training data. Regular expressions We built a set of regular expressions to identify the following mask types: N UMBER, E MOJI, E MAIL, and URL. A difficulty with developing these regular expressions is their interaction with other steps in the pipeline. One first has to choose whether to apply masking before or after tokenization. A nat"
W19-6618,W17-4716,0,0.0211124,"e words and technical items, but the approach was largely abandoned when sub-word methods (Sennrich et al., 2016) obviated the need. Most similar to this work in spirit is Crego et al. (2016), who mentioned that masking could be used to translate many “pass-through items” but did not conduct any further analysis towards the problem or the solution. Another solution for handling pass-through items is to add them as constraints during beam search. A number of approaches introduced modifications to beam search that ensured that desired words would be included in the output (Hokamp and Liu, 2017; Chatterjee et al., 2017; Anderson et al., 2017). One problem with these solutions is that decoding time generally grows very quickly with the number of constraints added. Hasler et al. (2018) showed that even two constraints cause decoding speed to increase by as much as five times. Post and Vilar (2018) introduced a fixed-beam-size variant which is constant in the number of constraints, but the constant overhead is still quite high. In terms of specific token types, Li et al. (2018), Ugawa et al. (2018) and Grundkiewicz and Heafield (2018) studied NMT models with better handling of named entities, either by adding"
W19-6618,N13-1073,0,0.0577586,"xecute the bipartite graph matching algorithm to find the best hard alignment scheme. These alignments can then be used to demask the output tokens. Our approach guarantees an alignment for each target mask. If there are fewer target than source masks, an input token will be erroneously used multiple times. Obtaining weights Obtaining weights to use with bipartite matching is not straightforward. We experiment with two approaches: • Averaged attention scores. We average source attention scores across all decoder heads and layers in our model. • External aligner. We run a version of fastalign (Dyer et al., 2013). Both have problems. We use Transformers (Vaswani et al., 2017) in our experiments, but multihead Transformer attention is not the same thing as alignment (Jain and Wallace, 2019). Fast-align is fast and easy to use at inference time, but it is a variant of IBM Model 2 (Brown et al., 1993) and the HMM model (Vogel et al., 1996). Therefore, its translation model cannot distinguish among mask permutations, and its impoverished distortion model is not well-suited to the task of recovering permutations of identical masks. However, we consider both approaches worth testing on this coarser alignmen"
W19-6618,gotti-etal-2014-hashtag,0,0.0130433,"ons is that decoding time generally grows very quickly with the number of constraints added. Hasler et al. (2018) showed that even two constraints cause decoding speed to increase by as much as five times. Post and Vilar (2018) introduced a fixed-beam-size variant which is constant in the number of constraints, but the constant overhead is still quite high. In terms of specific token types, Li et al. (2018), Ugawa et al. (2018) and Grundkiewicz and Heafield (2018) studied NMT models with better handling of named entities, either by adding named entity tags or employing transliteration models. Gotti et al. (2014) analyzed how hashtags are translated in the Canadian government tweet corpus and used insights from the analysis to improve their tweetoriented machine translation system. Radford et al. (2016) conducted corpus analysis on the alignment between natural language text with Emojis. Proceedings of MT Summit XVII, volume 1 match type examples template (regex) numbers, emoji, URLs, email addresses direct (dict) names, cities, states, locations Table 1: Pass-through candidates can be identified at the class level (via regular expressions) or type level (via direct match against a provided dictionary"
W19-6618,W18-2413,0,0.0627479,"(2016), who mentioned that masking could be used to translate many “pass-through items” but did not conduct any further analysis towards the problem or the solution. Another solution for handling pass-through items is to add them as constraints during beam search. A number of approaches introduced modifications to beam search that ensured that desired words would be included in the output (Hokamp and Liu, 2017; Chatterjee et al., 2017; Anderson et al., 2017). One problem with these solutions is that decoding time generally grows very quickly with the number of constraints added. Hasler et al. (2018) showed that even two constraints cause decoding speed to increase by as much as five times. Post and Vilar (2018) introduced a fixed-beam-size variant which is constant in the number of constraints, but the constant overhead is still quite high. In terms of specific token types, Li et al. (2018), Ugawa et al. (2018) and Grundkiewicz and Heafield (2018) studied NMT models with better handling of named entities, either by adding named entity tags or employing transliteration models. Gotti et al. (2014) analyzed how hashtags are translated in the Canadian government tweet corpus and used insight"
W19-6618,N18-2081,0,0.0366984,"Missing"
W19-6618,E17-3017,0,0.0261993,"sides of the data. For test sets, the counts are produced by matching only against the source. For most entity types, data is quite sparse. We then produce a new test set by repeating the following procedure 5,000 times: 1. Sample a sentence pair d ∈ Dm ; 2. Twenty separate times, do (a) Sample one of the positions with mask m in d (there may be only one); (b) Sample a term s ∈ Vm ; (c) Create a new sentence pair by inserting s into d. This yields synthetic datasets of 100k sentences. Table 7 contains examples. 4.4 Models Our baseline NMT system is a 4-layer transformer trained with Sockeye (Hieber et al., 2017). We use the following settings for training both French– English and Japanese–English models: eight attention heads, model size of 512, feed-forward layer size 2048, three-way tied embeddings, layer normalization applied before attention, dropout and a residual connection added afterwards, a batch size of 4096 words, and the learning rate initialized to 0.0002. We compute checkpoints every 5000 updates, and train until validation likelihood does not increase for ten consecutive checkpoints. For preprocessing, we first apply the Moses scripts that normalize punctuation, remove nonprinting char"
W19-6618,P17-1141,0,0.0236293,"the translation of rare words and technical items, but the approach was largely abandoned when sub-word methods (Sennrich et al., 2016) obviated the need. Most similar to this work in spirit is Crego et al. (2016), who mentioned that masking could be used to translate many “pass-through items” but did not conduct any further analysis towards the problem or the solution. Another solution for handling pass-through items is to add them as constraints during beam search. A number of approaches introduced modifications to beam search that ensured that desired words would be included in the output (Hokamp and Liu, 2017; Chatterjee et al., 2017; Anderson et al., 2017). One problem with these solutions is that decoding time generally grows very quickly with the number of constraints added. Hasler et al. (2018) showed that even two constraints cause decoding speed to increase by as much as five times. Post and Vilar (2018) introduced a fixed-beam-size variant which is constant in the number of constraints, but the constant overhead is still quite high. In terms of specific token types, Li et al. (2018), Ugawa et al. (2018) and Grundkiewicz and Heafield (2018) studied NMT models with better handling of named en"
W19-6618,N19-1357,0,0.0323982,"alignment for each target mask. If there are fewer target than source masks, an input token will be erroneously used multiple times. Obtaining weights Obtaining weights to use with bipartite matching is not straightforward. We experiment with two approaches: • Averaged attention scores. We average source attention scores across all decoder heads and layers in our model. • External aligner. We run a version of fastalign (Dyer et al., 2013). Both have problems. We use Transformers (Vaswani et al., 2017) in our experiments, but multihead Transformer attention is not the same thing as alignment (Jain and Wallace, 2019). Fast-align is fast and easy to use at inference time, but it is a variant of IBM Model 2 (Brown et al., 1993) and the HMM model (Vogel et al., 1996). Therefore, its translation model cannot distinguish among mask permutations, and its impoverished distortion model is not well-suited to the task of recovering permutations of identical masks. However, we consider both approaches worth testing on this coarser alignment task, where we are only concerned with Proceedings of MT Summit XVII, volume 1 Algorithm 1: Bipartite Matching Demasking Input: source sentence S = {s0 , . . . , sI−1 }, target s"
W19-6618,W17-3204,0,0.037952,"Missing"
W19-6618,2005.mtsummit-papers.11,0,0.11442,"e tj in T 0 with the unmasked source token corresponding to si ; end end return T 0 ; alignment of a handful of well-attested types, and not all the words in the sentence pair. 4 4.1 Experiment Setup Data Our evaluation follows the WMT 2019 Robustness Task,1 except that we use MTNT data (Michel and Neubig, 2018) for evaluation only. This includes MTNT/train, which we excluded from training in part because many of the masked items we would like to evaluate occur most frequently in this dataset. Table 2 contains information about all data sets. For French–English training data, we use Europarl (Koehn, 2005, v7) and News Commentary (v10), and a portion of the UN Corpus. Due to its large size, we do not add all of the UN data, but add only lines that have a mask other than N UMBER, which includes about 1.1 million lines. This is crucial for the experiments since there is not enough masked data without this addition. We also include the WMT 2015 newstest test set for evaluation. We also conduct limited experiments on Japanese–English. We follow Michel & Neubig in combining KFTT (Neubig, 2011), JESC (Pryzant 1 www.statmt.org/wmt19/robustness.html Dublin, Aug. 19-23, 2019 |p. 184 Dataset French–Engl"
W19-6618,W16-4602,0,0.201768,"opied to the output sentence. This includes many different token Dublin, Aug. 19-23, 2019 |p. 182 types that can be recognized by regular expressions (numbers, URLs, email addresses, and Emojis), as well as types for which we can provide a dictionary. We ask the following questions: • Are translation guarantees necessary for these types? • How effective is masking at producing these guarantees? We experiment in both high resource (FR→EN) and low-resource (JA→EN) language settings. 2 Related Work The first application of hard masking in neural machine translation was in Luong et al. (2015) and Long et al. (2016), which address the translation of rare words and technical items, but the approach was largely abandoned when sub-word methods (Sennrich et al., 2016) obviated the need. Most similar to this work in spirit is Crego et al. (2016), who mentioned that masking could be used to translate many “pass-through items” but did not conduct any further analysis towards the problem or the solution. Another solution for handling pass-through items is to add them as constraints during beam search. A number of approaches introduced modifications to beam search that ensured that desired words would be included"
W19-6618,P15-1002,0,0.0549974,", but which are simply copied to the output sentence. This includes many different token Dublin, Aug. 19-23, 2019 |p. 182 types that can be recognized by regular expressions (numbers, URLs, email addresses, and Emojis), as well as types for which we can provide a dictionary. We ask the following questions: • Are translation guarantees necessary for these types? • How effective is masking at producing these guarantees? We experiment in both high resource (FR→EN) and low-resource (JA→EN) language settings. 2 Related Work The first application of hard masking in neural machine translation was in Luong et al. (2015) and Long et al. (2016), which address the translation of rare words and technical items, but the approach was largely abandoned when sub-word methods (Sennrich et al., 2016) obviated the need. Most similar to this work in spirit is Crego et al. (2016), who mentioned that masking could be used to translate many “pass-through items” but did not conduct any further analysis towards the problem or the solution. Another solution for handling pass-through items is to add them as constraints during beam search. A number of approaches introduced modifications to beam search that ensured that desired"
W19-6618,C96-2141,0,0.411388,"taining weights to use with bipartite matching is not straightforward. We experiment with two approaches: • Averaged attention scores. We average source attention scores across all decoder heads and layers in our model. • External aligner. We run a version of fastalign (Dyer et al., 2013). Both have problems. We use Transformers (Vaswani et al., 2017) in our experiments, but multihead Transformer attention is not the same thing as alignment (Jain and Wallace, 2019). Fast-align is fast and easy to use at inference time, but it is a variant of IBM Model 2 (Brown et al., 1993) and the HMM model (Vogel et al., 1996). Therefore, its translation model cannot distinguish among mask permutations, and its impoverished distortion model is not well-suited to the task of recovering permutations of identical masks. However, we consider both approaches worth testing on this coarser alignment task, where we are only concerned with Proceedings of MT Summit XVII, volume 1 Algorithm 1: Bipartite Matching Demasking Input: source sentence S = {s0 , . . . , sI−1 }, target sentence T = {t0 , . . . , tJ−1 }, soft alignment matrix A of size I × J Output: demasked target sentence T 0 T0=T; for each unique mask label m in T d"
W19-6618,N18-1119,1,0.849806,"conduct any further analysis towards the problem or the solution. Another solution for handling pass-through items is to add them as constraints during beam search. A number of approaches introduced modifications to beam search that ensured that desired words would be included in the output (Hokamp and Liu, 2017; Chatterjee et al., 2017; Anderson et al., 2017). One problem with these solutions is that decoding time generally grows very quickly with the number of constraints added. Hasler et al. (2018) showed that even two constraints cause decoding speed to increase by as much as five times. Post and Vilar (2018) introduced a fixed-beam-size variant which is constant in the number of constraints, but the constant overhead is still quite high. In terms of specific token types, Li et al. (2018), Ugawa et al. (2018) and Grundkiewicz and Heafield (2018) studied NMT models with better handling of named entities, either by adding named entity tags or employing transliteration models. Gotti et al. (2014) analyzed how hashtags are translated in the Canadian government tweet corpus and used insights from the analysis to improve their tweetoriented machine translation system. Radford et al. (2016) conducted cor"
W19-6618,W18-6319,1,0.830223,"from annotations applied to the input tokens. For each of the types N UMBER, E MAIL, and URL, instead of masking, we added a distinct binary source factor. We also experimented with two ways of combining factors: concatenation and summing. Concatenation was described in Sennrich et al.; we learn an embedding of size 4 for each factor, and concatenate with the subword embeddings. For summing, we instead embed each factor to size 512, and sum together all factors for each input token. 5 Results We compute BLEU on detokenized, cased outputs using the standardized BLEU scoring script, sacreBLEU (Post, 2018).5 The results on all test sets can be found in Table 4. We provide the same-data baseline score from Michel and Neubig (2018) as an anchor point for evaluating the models. In no masking situation is there any improvement in BLEU score over the baseline system. In fact, adding masks seems to uniformly cost the models in 5 Shared portion of signature: BLEU +case.mixed +numrefs.1 +smooth.exp +tok.13a +version.1.2.20. Dublin, Aug. 19-23, 2019 |p. 186 System WMT15 Michel & Neubig (Base) French–English MTNTtest MTNTtrain JA–EN MTNTtest MTNTtrain - 23.2 - 6.6 - baseline indexed masking masking (fast"
W19-6618,U16-1018,0,0.0130502,"s five times. Post and Vilar (2018) introduced a fixed-beam-size variant which is constant in the number of constraints, but the constant overhead is still quite high. In terms of specific token types, Li et al. (2018), Ugawa et al. (2018) and Grundkiewicz and Heafield (2018) studied NMT models with better handling of named entities, either by adding named entity tags or employing transliteration models. Gotti et al. (2014) analyzed how hashtags are translated in the Canadian government tweet corpus and used insights from the analysis to improve their tweetoriented machine translation system. Radford et al. (2016) conducted corpus analysis on the alignment between natural language text with Emojis. Proceedings of MT Summit XVII, volume 1 match type examples template (regex) numbers, emoji, URLs, email addresses direct (dict) names, cities, states, locations Table 1: Pass-through candidates can be identified at the class level (via regular expressions) or type level (via direct match against a provided dictionary). 3 Masking Masking is the context-free replacement of a class of input tokens with a single mask token. The idea is to collapse collections of distributionally similar tokens into a single tok"
W19-6618,W16-2209,0,0.07364,"inting characters, and tokenize.4 We learn a subword model using byte-pair encoding (Sennrich et 4 With the options -no-escape and using a version of the Moses basic-protected-patterns file modified to protect masks. Proceedings of MT Summit XVII, volume 1 al., 2016) with 32k merge operations. No recasing is applied to either source- or target-language text. For alignment-based demasking, we trained two fast-align models, one in each language direction, using default parameters. We then combine them with the grow-diag-final-and heuristic. Source Factors We also experiment with source factors (Sennrich and Haddow, 2016) applied to the baseline (unmasked) system. Source factors are separate embeddings that are learned from annotations applied to the input tokens. For each of the types N UMBER, E MAIL, and URL, instead of masking, we added a distinct binary source factor. We also experimented with two ways of combining factors: concatenation and summing. Concatenation was described in Sennrich et al.; we learn an embedding of size 4 for each factor, and concatenate with the subword embeddings. For summing, we instead embed each factor to size 512, and sum together all factors for each input token. 5 Results We"
W19-6618,P16-1162,0,0.0760598,"ns (numbers, URLs, email addresses, and Emojis), as well as types for which we can provide a dictionary. We ask the following questions: • Are translation guarantees necessary for these types? • How effective is masking at producing these guarantees? We experiment in both high resource (FR→EN) and low-resource (JA→EN) language settings. 2 Related Work The first application of hard masking in neural machine translation was in Luong et al. (2015) and Long et al. (2016), which address the translation of rare words and technical items, but the approach was largely abandoned when sub-word methods (Sennrich et al., 2016) obviated the need. Most similar to this work in spirit is Crego et al. (2016), who mentioned that masking could be used to translate many “pass-through items” but did not conduct any further analysis towards the problem or the solution. Another solution for handling pass-through items is to add them as constraints during beam search. A number of approaches introduced modifications to beam search that ensured that desired words would be included in the output (Hokamp and Liu, 2017; Chatterjee et al., 2017; Anderson et al., 2017). One problem with these solutions is that decoding time generally"
