C04-1164,1998.amta-tutorials.5,0,0.101907,"Missing"
C04-1164,P98-1116,0,0.0171832,"endent ontologies with different semantic features. Over the last few years, significant effort has been made to construct the ontology manually according to the domain expert’s knowledge. Manual ontology merging using conventional editing tools without intelligent support is difficult, labor intensive and error prone. Therefore, several systems and frameworks for supporting the knowledge engineer in the ontology merging task have recently been proposed (Noy and Musen 2000). To avoid the reiteration in ontology construction, the algorithm of ontology merging (UMLS http://umlsks.nlm.nih.gov/) (Langkilde and Knight 1998) and ontology alignment (Vossen and Peters 1997) (Weigard and Hoppenbrouwers 1998) (Asanoma 2001) were invested. The final ontology is a merged version of the original ontologies. The two original ontologies persist, with aligned links between them. Alignment usually is performed when the ontologies cover domains that are complementary to each other. In the past, domain ontology was usually constructed manually according to the knowledge or experience of the experts or ontology engineers. Recently, automatic and semi-automatic methods have been developed. OntoExtract (Fensel et al. 2002) (Miss"
C04-1164,C98-1112,0,\N,Missing
C04-1164,W97-0801,0,\N,Missing
C04-1164,W04-1110,1,\N,Missing
C08-1133,S07-1074,0,0.0319516,"Missing"
C08-1133,W06-2911,0,0.0156954,"D system evaluates each instance in the second part. If the output of the WSD system disagrees with the two annotators, the instance is considered to be a suspicious candidate, otherwise it is considered to be clean and stored into the corpus. The set of suspicious candidates is collected and subsequently evaluated by the adjudicator to identify erroneous annotations. 3 ARG1 Word Sense Disambiguation This study takes a supervised learning approach to build a WSD system from the OntoNotes corpus. The feature set used herein is similar to several state-of-the-art WSD systems (Lee and Ng., 2002; Ando, 2006; Tratz et al., 2007; Cai et al., 2007; Agirre and Lopez de Lacalle, 2007; Specia et al., 2007), which is further integrated into a Naïve Bayes classifier (Lee and Ng., 2002; Mihalcea, 2007). In addition, a new feature, predicate-argument structure, provided by the OntoNotes corpus is also integrated. The feature set includes: Part-of-Speech (POS) tags: This feature includes the POS tags in the positions (P-3, P-2, P-1, P0, P1, P2, P3), relative to the POS tag of the target word. Local Collocations: This feature includes single words and multi-word n-grams. The single words include (W-3, W-2,"
C08-1133,D07-1108,0,0.0183533,"in the second part. If the output of the WSD system disagrees with the two annotators, the instance is considered to be a suspicious candidate, otherwise it is considered to be clean and stored into the corpus. The set of suspicious candidates is collected and subsequently evaluated by the adjudicator to identify erroneous annotations. 3 ARG1 Word Sense Disambiguation This study takes a supervised learning approach to build a WSD system from the OntoNotes corpus. The feature set used herein is similar to several state-of-the-art WSD systems (Lee and Ng., 2002; Ando, 2006; Tratz et al., 2007; Cai et al., 2007; Agirre and Lopez de Lacalle, 2007; Specia et al., 2007), which is further integrated into a Naïve Bayes classifier (Lee and Ng., 2002; Mihalcea, 2007). In addition, a new feature, predicate-argument structure, provided by the OntoNotes corpus is also integrated. The feature set includes: Part-of-Speech (POS) tags: This feature includes the POS tags in the positions (P-3, P-2, P-1, P0, P1, P2, P3), relative to the POS tag of the target word. Local Collocations: This feature includes single words and multi-word n-grams. The single words include (W-3, W-2, W-1, W0, W1, W2, W3), relative to the"
C08-1133,W02-0817,0,0.0470056,"Missing"
C08-1133,N06-2015,1,0.815676,"Missing"
C08-1133,S01-1004,0,0.0230452,"Missing"
C08-1133,W02-1006,0,0.0361716,"Missing"
C08-1133,W97-0207,0,0.205135,"Missing"
C08-1133,H93-1061,0,0.368997,"Missing"
C08-1133,P96-1006,0,0.206719,"Missing"
C08-1133,W04-2807,0,0.0754251,"Missing"
C08-1133,D07-1107,0,0.0332866,"Missing"
C08-1133,P07-1006,0,0.0170204,"disagrees with the two annotators, the instance is considered to be a suspicious candidate, otherwise it is considered to be clean and stored into the corpus. The set of suspicious candidates is collected and subsequently evaluated by the adjudicator to identify erroneous annotations. 3 ARG1 Word Sense Disambiguation This study takes a supervised learning approach to build a WSD system from the OntoNotes corpus. The feature set used herein is similar to several state-of-the-art WSD systems (Lee and Ng., 2002; Ando, 2006; Tratz et al., 2007; Cai et al., 2007; Agirre and Lopez de Lacalle, 2007; Specia et al., 2007), which is further integrated into a Naïve Bayes classifier (Lee and Ng., 2002; Mihalcea, 2007). In addition, a new feature, predicate-argument structure, provided by the OntoNotes corpus is also integrated. The feature set includes: Part-of-Speech (POS) tags: This feature includes the POS tags in the positions (P-3, P-2, P-1, P0, P1, P2, P3), relative to the POS tag of the target word. Local Collocations: This feature includes single words and multi-word n-grams. The single words include (W-3, W-2, W-1, W0, W1, W2, W3), relative to the target word W0. Similarly, the multi-word n-grams include"
C08-1133,S07-1057,0,0.0240467,"luates each instance in the second part. If the output of the WSD system disagrees with the two annotators, the instance is considered to be a suspicious candidate, otherwise it is considered to be clean and stored into the corpus. The set of suspicious candidates is collected and subsequently evaluated by the adjudicator to identify erroneous annotations. 3 ARG1 Word Sense Disambiguation This study takes a supervised learning approach to build a WSD system from the OntoNotes corpus. The feature set used herein is similar to several state-of-the-art WSD systems (Lee and Ng., 2002; Ando, 2006; Tratz et al., 2007; Cai et al., 2007; Agirre and Lopez de Lacalle, 2007; Specia et al., 2007), which is further integrated into a Naïve Bayes classifier (Lee and Ng., 2002; Mihalcea, 2007). In addition, a new feature, predicate-argument structure, provided by the OntoNotes corpus is also integrated. The feature set includes: Part-of-Speech (POS) tags: This feature includes the POS tags in the positions (P-3, P-2, P-1, P0, P1, P2, P3), relative to the POS tag of the target word. Local Collocations: This feature includes single words and multi-word n-grams. The single words include (W-3, W-2, W-1, W0, W1, W2, W3)"
C08-1133,D07-1082,1,0.928022,"or management. management.02: The people in charge. The ones actually doing the managing. Management wants to start downsizing. John was promoted to Management. I spoke to their management, and they&apos;re ready to make a deal. Table 2. Example sentence for the target word management along with its sense definitions. tags and definitions for the word arm (noun sense). The OntoNotes sense tags have been used for many applications, including the SemEval2007 evaluation (Pradhan et al., 2007b), sense merging (Snow et al., 2007), sense pool verification (Yu et al., 2007), and class imbalance problems (Zhu and Hovy, 2007). In creating OntoNotes, each word sense annotation involves two annotators and an adjudicator. First, all sentences containing the target word along with its sense distinctions are presented independently to two annotators for sense annotation. If the two annotators agree on the same sense for the target word in a given sentence, then their selection is stored in the corpus. Otherwise, this sentence is double-checked by the adjudicator for the final decision. The major problem of the above annotation scheme is that only the instances where the two annotators disagreed are double-checked, whil"
C08-1133,N07-1025,0,\N,Missing
C08-1133,S07-1016,0,\N,Missing
C10-1141,P06-1057,0,0.0141471,"earsynonym based on n-gram frequencies obtained by querying Google. A statistical test is then applied to determine whether or not a target word can be substituted by its near-synonyms. The dataset used in their experiments are derived from the OntoNotes copus (Hovy et al., 2006; Pradhan et al., 2007), where each near-synonym set corresponds to a sense pool in OntoNotes. Another direction to the task of near-synonym substitution is to identify the senses of a target word and its near-synonyms using word sense disambiguation (WSD), comparing whether they were of the same sense (McCarthy, 2002; Dagan et al., 2006). Dagan et al. (2006) described that the use of WSD is an indirect approach since it requires the intermediate sense identification step, and thus presented a sense matching technique to address the task directly. In this paper, we consider the near-synonym substitution task as a classification task, where a classifier is trained for each near-synonym set to classify test examples into one of the nearsynonyms in the set. However, near-synonyms share more common context words (features) than semantically dissimilar words in nature. Such similar contexts may decrease classifiers’ ability to disc"
C10-1141,P97-1067,0,0.266647,"thms to verify whether near-synonyms do match the given contexts. Applications can benefit from this ability to provide more effective services. For instance, a writing support system can assist users to select an alternative word that best fits a given context from a list of near-synonyms. In measuring the substitutability of words, the co-occurrence information between a target word 1254 Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 1254–1262, Beijing, August 2010 (the gap) and its context words is commonly used in statistical approaches. Edmonds (1997) built a lexical co-occurrence network from 1989 Wall Street Journal to determine the nearsynonym that is most typical or expected in a given context. Inkpen (2007) used the pointwise mutual information (PMI) formula to select the best near-synonym that can fill the gap in a given context. The PMI scores for each candidate near-synonym are computed using a larger web corpus, the Waterloo terabyte corpus, which can alleviate the data sparseness problem encountered in Edmonds’ approach. Following Inkpen’s approach, Gardiner and Dras (2007) also used the PMI formula with a different corpus (the W"
C10-1141,U07-1007,0,0.464628,"and its context words is commonly used in statistical approaches. Edmonds (1997) built a lexical co-occurrence network from 1989 Wall Street Journal to determine the nearsynonym that is most typical or expected in a given context. Inkpen (2007) used the pointwise mutual information (PMI) formula to select the best near-synonym that can fill the gap in a given context. The PMI scores for each candidate near-synonym are computed using a larger web corpus, the Waterloo terabyte corpus, which can alleviate the data sparseness problem encountered in Edmonds’ approach. Following Inkpen’s approach, Gardiner and Dras (2007) also used the PMI formula with a different corpus (the Web 1T 5-gram corpus) to explore whether near-synonyms differ in attitude. Yu et al. (2007) presented a method to compute the substitution scores for each nearsynonym based on n-gram frequencies obtained by querying Google. A statistical test is then applied to determine whether or not a target word can be substituted by its near-synonyms. The dataset used in their experiments are derived from the OntoNotes copus (Hovy et al., 2006; Pradhan et al., 2007), where each near-synonym set corresponds to a sense pool in OntoNotes. Another direct"
C10-1141,N06-2015,0,0.0219066,"alleviate the data sparseness problem encountered in Edmonds’ approach. Following Inkpen’s approach, Gardiner and Dras (2007) also used the PMI formula with a different corpus (the Web 1T 5-gram corpus) to explore whether near-synonyms differ in attitude. Yu et al. (2007) presented a method to compute the substitution scores for each nearsynonym based on n-gram frequencies obtained by querying Google. A statistical test is then applied to determine whether or not a target word can be substituted by its near-synonyms. The dataset used in their experiments are derived from the OntoNotes copus (Hovy et al., 2006; Pradhan et al., 2007), where each near-synonym set corresponds to a sense pool in OntoNotes. Another direction to the task of near-synonym substitution is to identify the senses of a target word and its near-synonyms using word sense disambiguation (WSD), comparing whether they were of the same sense (McCarthy, 2002; Dagan et al., 2006). Dagan et al. (2006) described that the use of WSD is an indirect approach since it requires the intermediate sense identification step, and thus presented a sense matching technique to address the task directly. In this paper, we consider the near-synonym su"
C10-1141,N07-1045,0,0.107541,"on Near-synonym sets represent groups of words with similar meaning, which are useful knowledge resources for many natural language applications. For instance, they can be used for query expansion in information retrieval (IR) (Moldovan and Mihalcea, 2000; Bhogal et al., 2007), where a query term can be expanded by its nearsynonyms to improve the recall rate. They can also be used in an intelligent thesaurus that can automatically suggest alternative words to avoid repeating the same word in the composing of text when there are suitable alternatives in its synonym set (Inkpen and Hirst, 2006; Inkpen, 2007). These near-synonym sets can be derived from manually constructed dictionaries such as WordNet (called synsets) (Fellbaum, 1998), EuroWordNet (Rodríguez et al., 1998), or clusters derived using statistical approaches (Lin, 1998). Although the words in a near-synonym set have similar meaning, they are not necessarily interchangeable in practical use due to their specific usage and collocational constraints. Pearce (2001) presented an example of collocational constraints for the context “ coffee”. In the given near-synonym set {strong, powerful}, the word “strong” is more suitable than “powerfu"
C10-1141,J06-2003,0,0.0274818,"us studies. 1 Introduction Near-synonym sets represent groups of words with similar meaning, which are useful knowledge resources for many natural language applications. For instance, they can be used for query expansion in information retrieval (IR) (Moldovan and Mihalcea, 2000; Bhogal et al., 2007), where a query term can be expanded by its nearsynonyms to improve the recall rate. They can also be used in an intelligent thesaurus that can automatically suggest alternative words to avoid repeating the same word in the composing of text when there are suitable alternatives in its synonym set (Inkpen and Hirst, 2006; Inkpen, 2007). These near-synonym sets can be derived from manually constructed dictionaries such as WordNet (called synsets) (Fellbaum, 1998), EuroWordNet (Rodríguez et al., 1998), or clusters derived using statistical approaches (Lin, 1998). Although the words in a near-synonym set have similar meaning, they are not necessarily interchangeable in practical use due to their specific usage and collocational constraints. Pearce (2001) presented an example of collocational constraints for the context “ coffee”. In the given near-synonym set {strong, powerful}, the word “strong” is more suitabl"
C10-1141,P98-2127,0,0.0480693,"ovan and Mihalcea, 2000; Bhogal et al., 2007), where a query term can be expanded by its nearsynonyms to improve the recall rate. They can also be used in an intelligent thesaurus that can automatically suggest alternative words to avoid repeating the same word in the composing of text when there are suitable alternatives in its synonym set (Inkpen and Hirst, 2006; Inkpen, 2007). These near-synonym sets can be derived from manually constructed dictionaries such as WordNet (called synsets) (Fellbaum, 1998), EuroWordNet (Rodríguez et al., 1998), or clusters derived using statistical approaches (Lin, 1998). Although the words in a near-synonym set have similar meaning, they are not necessarily interchangeable in practical use due to their specific usage and collocational constraints. Pearce (2001) presented an example of collocational constraints for the context “ coffee”. In the given near-synonym set {strong, powerful}, the word “strong” is more suitable than “powerful” to fill the gap, since “powerful coffee” is an anticollocation. Inkpen (2007) also presented several examples of collocations (e.g. ghastly mistake) and anti-collocations (e.g. ghastly error). Yu et al. (2007) described an exa"
C10-1141,W02-0816,0,0.0173461,"cores for each nearsynonym based on n-gram frequencies obtained by querying Google. A statistical test is then applied to determine whether or not a target word can be substituted by its near-synonyms. The dataset used in their experiments are derived from the OntoNotes copus (Hovy et al., 2006; Pradhan et al., 2007), where each near-synonym set corresponds to a sense pool in OntoNotes. Another direction to the task of near-synonym substitution is to identify the senses of a target word and its near-synonyms using word sense disambiguation (WSD), comparing whether they were of the same sense (McCarthy, 2002; Dagan et al., 2006). Dagan et al. (2006) described that the use of WSD is an indirect approach since it requires the intermediate sense identification step, and thus presented a sense matching technique to address the task directly. In this paper, we consider the near-synonym substitution task as a classification task, where a classifier is trained for each near-synonym set to classify test examples into one of the nearsynonyms in the set. However, near-synonyms share more common context words (features) than semantically dissimilar words in nature. Such similar contexts may decrease classif"
C10-1141,J90-1003,0,\N,Missing
C10-1141,C98-2122,0,\N,Missing
O00-1007,P81-1022,0,0.0882513,"Missing"
O00-1007,J95-2002,0,0.0432281,"Missing"
O03-1015,P97-1035,0,0.114774,"Missing"
O05-1011,W03-0204,0,0.0578214,"Missing"
O05-2003,1998.amta-tutorials.5,0,0.103331,"Missing"
O05-2003,P98-1116,0,0.0370073,"Missing"
O05-2003,W04-1110,1,0.51065,"Missing"
O05-3005,O97-3003,0,0.0418086,"Missing"
O06-1012,P92-1008,0,0.184611,"Missing"
O06-1012,N01-1016,0,0.0604128,"Missing"
O06-1012,N04-4040,0,0.0351336,"Missing"
O06-1012,N04-1018,0,0.0245156,"Missing"
O06-1012,N04-4032,0,\N,Missing
O07-3005,O04-3004,1,0.858189,"Missing"
O08-1011,C94-2167,0,0.189232,"Missing"
O08-1011,C04-1087,0,0.0456271,"Missing"
O08-1011,W95-0107,0,0.162773,"ly achieve a good performance on a test article composed of a relatively large amount of words. Without the use of large amount of words, this study proposes a method for extracting and weighting single- and multi-word propositional terms of varying syntactic structures. 2. System Design and Development This research extracts the propositional terms beyond simply the NP-based propositional terms from the abstract of technical papers and then regards propositional term extraction as a sequence labeling task. To this end, this approach employs an IOB (Inside, Outside, Beginning) encoding scheme [9] to specify the propositional term boundaries, and conditional random fields (CRFs) [10] to combine arbitrary observation features to find the globally optimal term boundaries. The combined association measure (CAM) [11] is further adopted to modify the propositional term boundaries. In other words, this research not only considers the multi-level contextual information of an RA (such as word statistics, tense, morphology, syntax, semantics, sentence structure, and cue words) but also computes the lexical cohesion of word sequences to determine whether or not a propositional term is formed, si"
O08-1011,W03-1806,0,0.0793031,"propositional terms of varying syntactic structures. 2. System Design and Development This research extracts the propositional terms beyond simply the NP-based propositional terms from the abstract of technical papers and then regards propositional term extraction as a sequence labeling task. To this end, this approach employs an IOB (Inside, Outside, Beginning) encoding scheme [9] to specify the propositional term boundaries, and conditional random fields (CRFs) [10] to combine arbitrary observation features to find the globally optimal term boundaries. The combined association measure (CAM) [11] is further adopted to modify the propositional term boundaries. In other words, this research not only considers the multi-level contextual information of an RA (such as word statistics, tense, morphology, syntax, semantics, sentence structure, and cue words) but also computes the lexical cohesion of word sequences to determine whether or not a propositional term is formed, since contextual information and lexical cohesion are two major factors for propositional term generation. Figure 2. The System Framework of Propositional Term Extraction The system framework essentially consists of a trai"
O08-1011,laparra-etal-2012-mapping,0,0.0414618,"chooses the ACM Computing Classification System (ACM CSS) [12] to serve as the domain terminology list for propositional term extraction from computer science RAs. The ACM CSS provides important subject descriptors for computer science, and was developed by the Association for Computing Machinery. The ACM CSS also provides a list of Implicit Subject Descriptors, which includes names of languages, people, and products in the field of computing. A mapping database, derived from WordNet (http://wordnet.princeton.edu/) and SUMO (Suggested Upper Merged Ontology) (http://ontology.teknowledge.com/) [13], supplies the semantic concept information of each word and the hierarchical concept information from the ontology. The AWL (Academic Words List) (http://www.vuw.ac.nz/lals/research/awl/) [14] is an academic word list containing 570 word families whose words are selected from different subjects. The syntactic level information of the RAs was obtained using Charniak’s parser [15], which is a “maximum-entropy inspired” probabilistic generative model parser for English. 2.2. Conditional Random Fields (CRFs) For this research goal, given a word sequence W = {w1, w2 ,..., wn } , the most likely pr"
O08-1011,N03-1028,0,0.0192861,"each fk , μk be the weight of gk and 1 Z0 be a normalization factor over all state sequences, ⎛ ⎞ where Z 0 = ∑ exp ⎜ ∑∑ λk f k ( st −1 , st , W ) + ∑∑ μ k g k ( st , W ) ⎟ . S t k ⎝ t k ⎠ ( k k ) , is usually estimated by maximizing the The set of weights in a CRF model, n conditional log-likelihood of the labeled sequences in the training data D = {S (i ) ,W (i ) } . Ψ = λ ,μ i =1 (Equation (3)) For fast training, parameter estimation was based on L-BFGS (the limited-memory BFGS) algorithm, a quasi-Newton algorithm for large scale numerical optimization problems [16]. The L-BFGS had proved [17] that converges significantly faster than Improved Iterative Scaling (IIS) and General Iterative Scaling (GIS). LΨ = ∑ log ( P ( S i =1... N Ψ (i ) |W (i ) ) ) (3) After the CRF model is trained to maximize the conditional log-likelihood of a given training set P(S|W), the test phase finds the most likely sequence using the combination of forward Viterbi and backward A* search [18]. The forward Viterbi search makes the labeling task more efficient and the backward A* search finds the n-best probable labels. 2.3. Multi-Level Features According to the properties of propositional term generation"
O08-1011,nenadic-etal-2002-automatic,0,0.0417437,"represented by connected words (CW) expressed with hyphenation, quotation marks or brackets. ACMCSS represents entries in the ACM Computing Classification System (ACM CSS). The last word of every entry in the ACM CSS (ACMCSSAff) satisfies the condition that it is a commonly occurring last word in scientific terminology. The existing propositional terms of the training data were the seeds of multiword terms (MTSeed). Words identified as acronyms were stored as useful features, consisting of IsNenadic, IsISD, and IsUC. IsNenadic was defined using the methodology of Nenadić, Spasić and Ananiadou [19] to acquire possible acronyms of a word sequence that was extracted by the C/NC value method. IsISD refers to the list of Implicit Subject Descriptors in the ACM CCS and IsUC signifies that all characters of the word were uppercase (2). Semantic Level: MeasureConcept infers that the word was found under SUMO’s “UNITS-OF-MEASURE” concept subclass and SeedConcept denotes that the concept of the word corresponded to the concept of a propositional term in the training data. (3). Frequency Level: A high frequency word list (HF) was generated from the top 5 percent of words in the training data. A s"
O08-1011,E95-1003,0,0.0880824,"Missing"
O08-6002,W06-2911,0,0.0394,"Missing"
O08-6002,P08-1077,0,0.0275794,"Missing"
O08-6002,D07-1108,0,0.0406125,"Missing"
O08-6002,W02-0817,0,0.0522505,"Missing"
O08-6002,N06-2015,1,0.801507,"Missing"
O08-6002,S01-1004,0,0.0257832,"Missing"
O08-6002,D07-1113,1,0.854748,"Missing"
O08-6002,W02-1006,0,0.0386323,"Missing"
O08-6002,W97-0207,0,0.201173,"Missing"
O08-6002,H93-1061,0,0.364807,"Missing"
O08-6002,P96-1006,0,0.176477,"Missing"
O08-6002,W04-2807,0,0.0764947,"Missing"
O08-6002,J05-1004,0,0.101837,"Missing"
O08-6002,S07-1016,0,0.0311514,"Missing"
O08-6002,D07-1107,0,0.0337662,"Missing"
O08-6002,P07-1006,0,0.0264734,"Missing"
O08-6002,S07-1057,0,0.0583238,"Missing"
O08-6002,C04-1164,1,0.900351,"Missing"
O08-6002,P08-1089,0,0.0257891,"Missing"
O08-6002,D07-1082,1,0.893155,"Missing"
O08-6002,N07-1025,0,\N,Missing
O08-6002,S07-1074,0,\N,Missing
O09-1017,W96-0204,0,0.017991,"Missing"
O09-1017,P03-1054,0,0.00625439,"Missing"
O10-5002,O10-5002,1,0.0513221,"Missing"
O10-5002,W06-2911,0,0.284892,"Missing"
O10-5002,D07-1108,0,0.067771,"Missing"
O10-5002,D07-1007,0,0.167846,"Missing"
O10-5002,N10-1030,0,0.232401,"Missing"
O10-5002,W02-0817,0,0.419747,"Missing"
O10-5002,P06-1057,0,0.272466,"Missing"
O10-5002,N09-1037,0,0.0867669,"Missing"
O10-5002,N06-2015,0,0.197466,"Missing"
O10-5002,W02-1006,0,0.31986,"Missing"
O10-5002,W02-0816,0,0.488792,"Missing"
O10-5002,H93-1061,0,0.177801,"Missing"
O10-5002,P96-1006,0,0.468206,"Missing"
O10-5002,W04-2807,0,0.374788,"Missing"
O10-5002,J05-1004,0,0.0606174,"Missing"
O10-5002,W11-1901,0,0.166828,"Missing"
O10-5002,D07-1107,0,0.214034,"Missing"
O10-5002,P07-1006,0,0.293679,"Missing"
O10-5002,S07-1057,0,0.24528,"Missing"
O10-5002,D07-1082,0,0.242821,"Missing"
O10-5002,S07-1074,0,\N,Missing
O10-5002,S07-1016,0,\N,Missing
O10-5002,P07-1005,0,\N,Missing
P06-2120,N03-2009,0,0.0137051,"evelopment of dialogue management for dealing with complex applications, speech act identification with semantic interpretation will be the most important topic with respect to the methods used to control the dialogue with the users. This paper proposes an approach integrating semantic dependency graph and history/discourse information to model the dialogue discourse (Kudo and Matsumoto, 2000; Hacioglu et al., 2003; Gao and Suzuki, 2003). Three major components, such as semantic relation, semantic class and semantic role are adopted in the semantic dependency graph (Gildea and Jurasfky, 2002; Hacioglu and Ward, 2003). The semantic relations constrain the word sense and provide the method for disambiguation. Semantic roles are assigned when the relation established among semantic objects. Both semantic relations and roles are defined in many knowledge resources or ontologies, such as FrameNet (Baker et al., 2004) and HowNet with 65,000 concepts in Chinese and close to 75,000 English equivalents, is a bilingual knowledge-base describing relations between concepts and relations between the attributes of concepts with ontological view (Dong and Dong 2006). Generally speaking, semantic class is defined as a se"
P06-2120,W00-1303,0,0.0171979,"first one is how to obtain the semantic object from the user’s utterances. The second is a more effective speech act identification approach for semantic understanding is needed. Since speech act plays an important role in the development of dialogue management for dealing with complex applications, speech act identification with semantic interpretation will be the most important topic with respect to the methods used to control the dialogue with the users. This paper proposes an approach integrating semantic dependency graph and history/discourse information to model the dialogue discourse (Kudo and Matsumoto, 2000; Hacioglu et al., 2003; Gao and Suzuki, 2003). Three major components, such as semantic relation, semantic class and semantic role are adopted in the semantic dependency graph (Gildea and Jurasfky, 2002; Hacioglu and Ward, 2003). The semantic relations constrain the word sense and provide the method for disambiguation. Semantic roles are assigned when the relation established among semantic objects. Both semantic relations and roles are defined in many knowledge resources or ontologies, such as FrameNet (Baker et al., 2004) and HowNet with 65,000 concepts in Chinese and close to 75,000 Englis"
P06-2120,J00-3003,0,0.14324,"Missing"
P06-2120,P97-1035,0,0.0350556,"ficients. The decoding strategy is based on a classical Viterbi algorithm. The evaluation results by the character error rate (CER) for a Chinese speech recognition system is 18.3 percent and the vocabulary size of the language is 25,132. 50 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 Length (Turns) Figure 4. The distribution of the number of turns per dialogue 3.2 Precision of speech act identification related to the corpus size 941 3.3 Performance analysis of semantic dependency graph To evaluate the performance, two systems were developed for comparison. One is based on the Bayes’ classifier (Walker et al., 1997), and the other is the use of the partial pattern tree (Wu et al., 2004) to identify the speech act of the user’s utterances. Since the dialogue discourse is defined as a sequence of speech acts. The prediction of speech Speech act Clinic information (26 sentences) Dr.’s information (42 sentences) Confirmation(others) (42 sentences) Others (14 sentences) FAQ (13 sentences) Clinic information (135 sentences) Time (38) Registration (75) Cancel registration (10) Average Precision act of the new input utterance becomes the core issue for discourse modeling. The accuracy for speech act identificati"
P06-2120,P98-1013,0,0.0201687,"Missing"
P06-2120,P03-1066,0,0.0278661,"rom the user’s utterances. The second is a more effective speech act identification approach for semantic understanding is needed. Since speech act plays an important role in the development of dialogue management for dealing with complex applications, speech act identification with semantic interpretation will be the most important topic with respect to the methods used to control the dialogue with the users. This paper proposes an approach integrating semantic dependency graph and history/discourse information to model the dialogue discourse (Kudo and Matsumoto, 2000; Hacioglu et al., 2003; Gao and Suzuki, 2003). Three major components, such as semantic relation, semantic class and semantic role are adopted in the semantic dependency graph (Gildea and Jurasfky, 2002; Hacioglu and Ward, 2003). The semantic relations constrain the word sense and provide the method for disambiguation. Semantic roles are assigned when the relation established among semantic objects. Both semantic relations and roles are defined in many knowledge resources or ontologies, such as FrameNet (Baker et al., 2004) and HowNet with 65,000 concepts in Chinese and close to 75,000 English equivalents, is a bilingual knowledge-base d"
P06-2120,J02-3001,0,0.117546,"Missing"
P06-2120,H01-1015,0,\N,Missing
P06-2120,C98-1013,0,\N,Missing
P06-2120,N04-1030,0,\N,Missing
P06-2121,P05-1046,0,0.0264135,"For some application domains, such annotated corpora may be unavailable. Therefore, we propose the use of web resources as the corpora. When facing with the web corpora, traditional corpus-based approaches may be infeasible. For example, it is impractical for health professionals to annotate the whole web corpora. Besides, it is also impractical to enumerate all possible combinations of words from the web corpora, and then search for the semantic patterns. To address the problems, we take the notion of weakly supervised (Stevenson and Greenwood, 2005) or unsupervised learning (Hasegawa, 2004; Grenager et al., 2005) to develop a framework able to bootstrap with a small set of seed patterns, and then induce more relevant patterns form the unannotated psychiatry web corpora. By this way, the reliance on annotated corpora can be significantly reduced. The proposed framework is divided into two parts: Hyperspace Analog to Language (HAL) model (Burgess et al., 1998; Bai et al., 2005), and a cascaded induction process (CIP). The HAL model, which is a cognitive motivated model, provides an informative infrastructure to make the CIP capable of learning from unannotated corpora. The CIP treats the variable-length"
P06-2121,P04-1053,0,0.0413752,"Missing"
P06-2121,P05-1047,0,0.0169164,"e corpora with annotated information to obtain more reliable parameters. For some application domains, such annotated corpora may be unavailable. Therefore, we propose the use of web resources as the corpora. When facing with the web corpora, traditional corpus-based approaches may be infeasible. For example, it is impractical for health professionals to annotate the whole web corpora. Besides, it is also impractical to enumerate all possible combinations of words from the web corpora, and then search for the semantic patterns. To address the problems, we take the notion of weakly supervised (Stevenson and Greenwood, 2005) or unsupervised learning (Hasegawa, 2004; Grenager et al., 2005) to develop a framework able to bootstrap with a small set of seed patterns, and then induce more relevant patterns form the unannotated psychiatry web corpora. By this way, the reliance on annotated corpora can be significantly reduced. The proposed framework is divided into two parts: Hyperspace Analog to Language (HAL) model (Burgess et al., 1998; Bai et al., 2005), and a cascaded induction process (CIP). The HAL model, which is a cognitive motivated model, provides an informative infrastructure to make the CIP capable of lear"
P06-2121,C04-1164,1,0.897763,"Missing"
P06-2121,M91-1033,0,\N,Missing
P06-2121,M92-1038,0,\N,Missing
P07-1129,H05-1121,0,0.0304208,"nts to identify the relevant documents is time consuming and tends to become overwhelming. Individuals need to be able to retrieve the relevant consultation documents efficiently and effectively. Therefore, this work presents a novel mechanism to automatically retrieve the relevant consultation documents with respect to users' problems. Traditional information retrieval systems represent queries and documents using a bag-of-words approach. Retrieval models, such as the vector space model (VSM) (Baeza-Yates and RibeiroNeto, 1999) and Okapi model (Robertson et al., 1995; Robertson et al., 1996; Okabe et al., 2005), are then adopted to estimate the relevance between queries and documents. The VSM represents each query and document as a vector of words, and adopts the cosine measure to estimate their relevance. The Okapi model, which has been used on the Text REtrieval Conference (TREC) collections, developed a family of word-weighting functions Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 1024–1031, c Prague, Czech Republic, June 2007. 2007 Association for Computational Linguistics Query: Consultation Document I broke up with my boyfriend. &lt;Depressed&gt; I o"
P07-1129,C04-1164,1,0.82514,"Missing"
P09-2051,P08-2006,0,0.0175839,"e life events within them. The class labels used herein are presented in Table 1, which are derived from Brostedt and Pedersen (2003). Traditional approaches to sentence classification (Khoo et al., 2006; Naughton et al., 2008) or text categorization (Sebastiani 2002) usually adopt bag-of-words as baseline features to train classifiers. Since the bag-of-words approach treats each word independently without considering the relationships of words in sentences, some researchers have investigated the use of n-grams to capture sequential relations between words to boost classification performance (Chitturi and Hansen, 2008; Li and Zong, 2008). The use of ngrams is effective in capturing local dependencies of words, but tends to suffer from data sparseness problem in capturing long-distance dependencies since higher-order n-grams require large training data to obtain reliable estimation. For our task, the expressions of negative life events can be characterized by association language patterns, i.e., meaningful combinations of words, such as <worry, children, health&gt;, <break up, boyfriend&gt;, <argue, friend&gt;, <loss, job&gt;, and 201 Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 201–204, c Suntec,"
P09-2051,U06-1005,0,0.0171689,"nally, a dialog system can generate supportive responses like “Don’t worry”, “That’s really sad” and “Cheer up” if it can understand the negative life events embedded in the example sentences shown in Table 1. Therefore, this study proposes a framework for negative life event classification. We formulate this problem as a sentence classification task; that is, classify sentences according to the type of negative life events within them. The class labels used herein are presented in Table 1, which are derived from Brostedt and Pedersen (2003). Traditional approaches to sentence classification (Khoo et al., 2006; Naughton et al., 2008) or text categorization (Sebastiani 2002) usually adopt bag-of-words as baseline features to train classifiers. Since the bag-of-words approach treats each word independently without considering the relationships of words in sentences, some researchers have investigated the use of n-grams to capture sequential relations between words to boost classification performance (Chitturi and Hansen, 2008; Li and Zong, 2008). The use of ngrams is effective in capturing local dependencies of words, but tends to suffer from data sparseness problem in capturing long-distance depende"
P09-2051,P08-2065,0,0.0267172,"The class labels used herein are presented in Table 1, which are derived from Brostedt and Pedersen (2003). Traditional approaches to sentence classification (Khoo et al., 2006; Naughton et al., 2008) or text categorization (Sebastiani 2002) usually adopt bag-of-words as baseline features to train classifiers. Since the bag-of-words approach treats each word independently without considering the relationships of words in sentences, some researchers have investigated the use of n-grams to capture sequential relations between words to boost classification performance (Chitturi and Hansen, 2008; Li and Zong, 2008). The use of ngrams is effective in capturing local dependencies of words, but tends to suffer from data sparseness problem in capturing long-distance dependencies since higher-order n-grams require large training data to obtain reliable estimation. For our task, the expressions of negative life events can be characterized by association language patterns, i.e., meaningful combinations of words, such as <worry, children, health&gt;, <break up, boyfriend&gt;, <argue, friend&gt;, <loss, job&gt;, and 201 Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 201–204, c Suntec, Singapore, 4 August"
P09-2051,C08-1078,0,0.0312278,"tem can generate supportive responses like “Don’t worry”, “That’s really sad” and “Cheer up” if it can understand the negative life events embedded in the example sentences shown in Table 1. Therefore, this study proposes a framework for negative life event classification. We formulate this problem as a sentence classification task; that is, classify sentences according to the type of negative life events within them. The class labels used herein are presented in Table 1, which are derived from Brostedt and Pedersen (2003). Traditional approaches to sentence classification (Khoo et al., 2006; Naughton et al., 2008) or text categorization (Sebastiani 2002) usually adopt bag-of-words as baseline features to train classifiers. Since the bag-of-words approach treats each word independently without considering the relationships of words in sentences, some researchers have investigated the use of n-grams to capture sequential relations between words to boost classification performance (Chitturi and Hansen, 2008; Li and Zong, 2008). The use of ngrams is effective in capturing local dependencies of words, but tends to suffer from data sparseness problem in capturing long-distance dependencies since higher-order"
P11-2106,P03-1056,0,0.0105222,"addition, words that are not confidently recognized are replaced by a special non-keyword token called Filler. Specifically, we compute the z-score (Larsen and Marx, 2000) of each word w in the ASR output. Figure 3 illustrates the PST for the sentences: Where is the Anping-Fort. There are two keywords Where and Anping-Fort and two non-keywords is and the. Note that with 2 non-keywords in the original sentence s, we have 22 = 4 partial sentences in the PST T (s). 4.2 Extraction of the Derivation Rules After text processing, a sentence s is parsed by the statistical Stanford parser (S-parser) (Levy and Manning, 2003). Let the grammar of the S-parser be denoted as a 5-tuple G = (V, Σ, P, S, D) where V is the variable (non-terminal) set,Σ is the terminal symbol set,P is the production rule set, S is the sentence symbol, and D is a function defined on P for rule probability (Jurafsky and Martin, 2009). A derivation rule is defined to be a derivation of the form A → B → w where A, B ∈ V and w ∈ Σ. The parsing result of the exemplar sentence s represented in the parenthesized expression is shown in Figure 4. From the parsing result, four DRs are extracted. Essentially, we have one DR for each lexical word in t"
W02-1904,O00-1007,1,0.887836,"Missing"
W04-1110,W97-0801,0,\N,Missing
W13-4420,W10-4107,0,0.243949,"common error type in hand-writings of second-language learners. However, since it only exists in hand-writings of humans and because all characters used in computers are legal ones, it is not necessary to address this kind of spelling errors when given erroneous texts are of electronic forms. The task addressed in SIGHAN-7 is a restricted type of Substitution errors, where there exists at most one continuous error (mis-spelled) character in its context within a sentence, with only one exception in which there is a two-character error (Chen, Wu, Yang, & Ku, 2011; C.-L. Liu et al., 2010; S.-H. Wu, Chen, Yang, Ku, & Liu, 2010). This allows the system to assume that when a character is to be corrected, its adjacent characters are correct. The correction procedure is comprised of two consecutive steps: 1) Providing candidate corrections for each character in the sentence, and 2) Scoring the altered correction sentences and identifying which is the best corrected sentence (C.-H. Liu et al., 2008; C.-H. Wu et al., 2010). In this paper, a web-based measure is employed in the second step to score and identify the best correction sentence (Macias, Wong, Thangarajah, & Cavedon, 2012). This paper is organized as follows. S"
W13-4420,C10-2085,0,0.465802,"Missing"
W13-4420,W03-1726,0,0.0190327,"uracy in SIGHAN-7 Sub-Tasks 1 and 2. (4) where is the “normalized web distance” and is defined in Equation 5. , log max ||, || log |∩ | log || log min ||, || (5) where ||is the number of Wikipedia Chinese pages, which is 3,063,936 as of the time the system is implemented. It should be noted that Macias-Galindo et al.’s original work is used in English texts. Currently we have not administered any preliminary experiment to find better setups of these equations. 4 Experiments and Discussions In the proposed system, Academia Sinica’s CKIP Chinese Segmenter is used to derive segmentation results (Ma & Chen, 2003) and the language model (trigrams using Chen and Goodman’s modified Kneser-Ney discounting) is trained using SRILM with Chinese Gigaword (LDC Catalog No.: LDC2003T09) (Stolcke, 2002). In a brief summary of the results, our system did not perform well in the final test of SIGHAN-7 bakeoff. The authors would like to defend the proposed method with a major problem in the runtime of the final test. In theory, the Sub-Task 1 (Detection) NCKU&YZU-1 NTHU-3 SinicaCKIP-3 SJTU-3 NCYU-2 NCYU-3 Error Location Accuracy 0.705 0.820 0.771 0.809 0.652 0.748 Sub-Task 2 (Correction) Location Accuracy NCKU&YZU-1"
W13-4420,W09-3412,0,\N,Missing
Y99-1035,O96-2002,0,0.05698,"Missing"
