2008.jeptalnrecital-long.17,abeille-barrier-2004-enriching,0,0.0656987,"Missing"
2008.jeptalnrecital-long.17,P05-1038,0,0.162403,"Missing"
2008.jeptalnrecital-long.17,A00-1031,0,0.216259,"Missing"
2008.jeptalnrecital-long.17,P05-1022,0,0.0658642,"Missing"
2008.jeptalnrecital-long.17,J03-4003,0,0.0674105,"Missing"
2008.jeptalnrecital-long.17,W01-0521,0,0.0616877,"Missing"
2008.jeptalnrecital-long.17,J98-4004,0,0.223944,"Missing"
2008.jeptalnrecital-long.17,P03-1054,0,0.087619,"Missing"
2008.jeptalnrecital-long.17,P05-1010,0,0.181101,"Missing"
2008.jeptalnrecital-long.17,P06-1055,0,0.187663,"Missing"
2009.jeptalnrecital-court.1,P05-1038,0,0.0413146,"Missing"
2009.jeptalnrecital-court.1,A00-1031,0,0.165214,"Missing"
2009.jeptalnrecital-court.1,P04-1041,0,0.0348247,"Missing"
2009.jeptalnrecital-court.1,A00-2018,0,0.240234,"Missing"
2009.jeptalnrecital-court.1,P00-1058,0,0.0518767,"Missing"
2009.jeptalnrecital-court.1,P03-1013,0,0.0578217,"Missing"
2009.jeptalnrecital-court.1,J98-4004,0,0.172518,"Missing"
2009.jeptalnrecital-court.1,J00-4006,0,0.0159887,"Missing"
2009.jeptalnrecital-court.1,P03-1054,0,0.0322081,"Missing"
2009.jeptalnrecital-court.1,P06-1055,0,0.281177,"Missing"
2009.jeptalnrecital-court.1,D07-1066,0,0.0506424,"Missing"
2009.jeptalnrecital-court.1,schluter-van-genabith-2008-treebank,0,0.257186,"Missing"
2009.jeptalnrecital-long.4,P05-1038,0,0.0779972,"Missing"
2009.jeptalnrecital-long.4,J96-1002,0,0.0126056,"Missing"
2009.jeptalnrecital-long.4,W09-1008,1,0.87588,"Missing"
2009.jeptalnrecital-long.4,de-marneffe-etal-2006-generating,0,0.109659,"Missing"
2009.jeptalnrecital-long.4,A00-2018,0,0.128627,"Missing"
2009.jeptalnrecital-long.4,2001.jeptalnrecital-tutoriel.3,0,0.368197,"Missing"
2009.jeptalnrecital-long.4,W03-2401,0,0.0761601,"Missing"
2009.jeptalnrecital-long.4,P95-1037,0,0.329975,"Missing"
2009.jeptalnrecital-long.4,schluter-van-genabith-2008-treebank,0,0.0611634,"Missing"
2017.jeptalnrecital-court.1,F12-2024,1,0.858093,"Missing"
2017.jeptalnrecital-court.1,C86-1001,0,0.547911,"Missing"
2017.jeptalnrecital-court.1,2010.jeptalnrecital-invite.1,0,0.108359,"Missing"
2017.jeptalnrecital-court.1,L16-1262,0,0.0611543,"Missing"
2017.jeptalnrecital-court.1,W17-1704,1,0.869636,"Missing"
2020.lrec-1.724,S14-1001,0,0.026174,"rained lexical distinctions, its hierarchical organization has been seen as different levels of semantic granularity. The top level of WordNet ontology, namely the “Unique Beginners” (UBs) have been taken as coarse-grained categories for both manual and automatic annotation, which has been proved to limit the difficulty of making fine-grained sense distinctions (Palmer et al., 2007; Navigli, 2006; Navigli, 2009). First used by Ciaramita and Johnson (2003) and Ciaramita and Altun (2006) as ”supersenses”, WordNet UBs are adopted in a growing number of studies devoted to coarse semantic tagging (Johannsen et al., 2014; Flekova and Gurevych, 2016). For semantic annotation projects based on WordNet UBs1 , the corpus may or may not be pre-tagged with UBs, depending on whether or not a WordNet lexicon is available with sufficient coverage for the target language. Pretagging was used for instance for Danish in the SemDax project (Pederson et al., 2016), but not used for Arabic in (Schneider et al., 2012)2 . French has a freely availaible Wordnet1 2 See Petrolito and Bond (2014) for an overview. Note that in the former case, UBs are generalizations over like resource, the Wolf, that has been automatically built"
2020.lrec-1.724,2016.gwc-1.30,0,0.0238749,"lso removed the Motive and Shape UBs. The former has 3 All other annotation projects using UBs as tags we know of also had to make adjustments, but to a more limited extent. In Schneider et al. (2012), extended definitions of WordNet UBs are proposed, illustrated by English examples, along with rule decisions such as “all man-made structures (buildings, rooms, bridges, etc.) are to be tagged as Artifact”. Pederson et al. (2016) extend the WordNet UB set by including more specific classes. For instance, they use Vehicle, Building and Container as subclasses of the Artifact supersense (Martinez Alonso et al., 2016). In both projects though, the definition and lexical scope of WordNet UBs are preserved. already been described as problematic in previous supersense annotation projects, and concerns very few words in WordNet. Shape was excluded because of its heterogeneity and fuzzy boundaries, and because its members could be easily dispatched in other existing categories, such as Artifact and Cognition. Novel classes Two supersenses absent from the WordNet inventory were used in the annotation. First, we adopted the Institution tag proposed in the SemDax corpus (Pederson et al., 2016), except that we used"
2020.lrec-1.724,P06-1014,0,0.0641723,"notated corpus of English, occurrences of a word are associated with a predefined list of senses (represented by synsets) from the Princeton WordNet (Miller et al., 1990). WordNet being known to include very fine-grained lexical distinctions, its hierarchical organization has been seen as different levels of semantic granularity. The top level of WordNet ontology, namely the “Unique Beginners” (UBs) have been taken as coarse-grained categories for both manual and automatic annotation, which has been proved to limit the difficulty of making fine-grained sense distinctions (Palmer et al., 2007; Navigli, 2006; Navigli, 2009). First used by Ciaramita and Johnson (2003) and Ciaramita and Altun (2006) as ”supersenses”, WordNet UBs are adopted in a growing number of studies devoted to coarse semantic tagging (Johannsen et al., 2014; Flekova and Gurevych, 2016). For semantic annotation projects based on WordNet UBs1 , the corpus may or may not be pre-tagged with UBs, depending on whether or not a WordNet lexicon is available with sufficient coverage for the target language. Pretagging was used for instance for Danish in the SemDax project (Pederson et al., 2016), but not used for Arabic in (Schneider e"
2020.lrec-1.724,L16-1136,0,0.094348,"-grained sense distinctions (Palmer et al., 2007; Navigli, 2006; Navigli, 2009). First used by Ciaramita and Johnson (2003) and Ciaramita and Altun (2006) as ”supersenses”, WordNet UBs are adopted in a growing number of studies devoted to coarse semantic tagging (Johannsen et al., 2014; Flekova and Gurevych, 2016). For semantic annotation projects based on WordNet UBs1 , the corpus may or may not be pre-tagged with UBs, depending on whether or not a WordNet lexicon is available with sufficient coverage for the target language. Pretagging was used for instance for Danish in the SemDax project (Pederson et al., 2016), but not used for Arabic in (Schneider et al., 2012)2 . French has a freely availaible Wordnet1 2 See Petrolito and Bond (2014) for an overview. Note that in the former case, UBs are generalizations over like resource, the Wolf, that has been automatically built from the Princeton WordNet and other resources, using translation techniques (Sagot and Fiˇser, 2008). We decided not to use it to pre-tag the corpus we annotated, because the Wolf has only been very partly manually validated and the gain of noisy data for speeding up annotation process is not obvious. Instead, following (Schneider et"
2020.lrec-1.724,W14-0132,0,0.0661046,"Missing"
2020.lrec-1.724,P12-2050,0,0.203838,"vigli, 2006; Navigli, 2009). First used by Ciaramita and Johnson (2003) and Ciaramita and Altun (2006) as ”supersenses”, WordNet UBs are adopted in a growing number of studies devoted to coarse semantic tagging (Johannsen et al., 2014; Flekova and Gurevych, 2016). For semantic annotation projects based on WordNet UBs1 , the corpus may or may not be pre-tagged with UBs, depending on whether or not a WordNet lexicon is available with sufficient coverage for the target language. Pretagging was used for instance for Danish in the SemDax project (Pederson et al., 2016), but not used for Arabic in (Schneider et al., 2012)2 . French has a freely availaible Wordnet1 2 See Petrolito and Bond (2014) for an overview. Note that in the former case, UBs are generalizations over like resource, the Wolf, that has been automatically built from the Princeton WordNet and other resources, using translation techniques (Sagot and Fiˇser, 2008). We decided not to use it to pre-tag the corpus we annotated, because the Wolf has only been very partly manually validated and the gain of noisy data for speeding up annotation process is not obvious. Instead, following (Schneider et al., 2012), we used UBs as lexicon-independent seman"
2020.lrec-1.724,W19-0422,1,0.798055,"Missing"
2020.mwe-1.14,calzolari-etal-2002-towards,0,0.0556036,"ary discovery methods. We released annotated and raw corpora in 14 languages, and this semi-supervised challenge attracted 7 teams who submitted 9 system results. This paper describes the effort of corpus creation, the task design, and the results obtained by the participating systems, especially their performance on unseen expressions. 1 Introduction Multiword expressions (MWEs) such as to throw someone under the bus ‘to cause one’s suffering to gain personal advantage’ are idiosyncratic word combinations which need to be identiﬁed prior to further semantic processing (Baldwin and Kim, 2010; Calzolari et al., 2002). The task of MWE identiﬁcation, that is, automatically locating instances of MWEs in running text (Constant et al., 2017) has received growing attention in the last 4 years. Progress on this task was especially motivated by shared tasks such as DiMSUM (Schneider et al., 2016), and two editions of the PARSEME shared tasks, edition 1.0 in 2017 (Savary et al., 2017), and edition 1.1 in 2018 (Ramisch et al., 2018). Previous editions of the PARSEME shared task focused on the identiﬁcation of verbal MWEs (VMWEs), because of their challenging traits: complex structure, discontinuities, variability,"
2020.mwe-1.14,J17-4005,1,0.888334,"Missing"
2020.mwe-1.14,W17-4418,0,0.065602,"Missing"
C10-2013,abeille-barrier-2004-enriching,0,0.0620534,"A syntactic analysis in terms of typed grammatical relations, whether encoded as functional annotations in syntagmatic trees or in labeled dependency trees, appears to be useful for many NLP tasks including question answering, information extraction, and lexical acquisition tasks like collocation extraction. This usefulness holds particularly for French, a language for which bare syntagmatic trees are often syntactically underspecied because of a rather free order of post-verbal complements/adjuncts and the possibility of subject inversion. Thus, the annotation scheme of the French Treebank (Abeillé and Barrier, 2004) makes use of at syntagmatic trees without VP joakim.nivre@lingl.uu.se nodes, with no structural distinction between complements, adjuncts or post-verbal subjects, but with additional functional annotations on dependents of verbs. Parsing is commonly enhanced by using more abstract lexical information, in the form of morphological features (Tsarfaty, 2006), lemmas (Seddah et al., 2010), or various forms of clusters (see (Candito and Seddah, 2010) for references). In this paper, we explore the integration of morphological features, lemmas, and linear context clusters. Typed dependencies can b"
C10-2013,J92-4003,0,0.152406,"Missing"
C10-2013,W06-2920,0,0.00795066,"e dominant paradigm for English has been to use constituency-based parsers, the output of which can be converted to typed dependencies using well-proven conversion procedures, as in the Stanford parser (Klein and Manning, 2003). In recent years, it has also become popular to use statistical dependency parsers, which are trained directly on labeled dependency trees and output such trees directly, such as MSTParser (McDonald, 2006) and MaltParser (Nivre et al., 2006). Dependency parsing has been applied to a fairly broad range of languages, especially in the CoNLL shared tasks in 2006 and 2007 (Buchholz and Marsi, 2006; Nivre et al., 2007). We present a comparison of three statistical parsing architectures that output typed dependencies for French: one constituency-based architecture featuring the Berkeley parser (Petrov et al., 2006), and two dependency-based systems using radically different parsing methods, MSTParser (McDonald et al., 2006) and MaltParser (Nivre et al., 2006). These three systems are compared both in terms of parsing accuracy and parsing times, in realistic settings that only use predicted information. By using freely available software packages that implement language-independent approa"
C10-2013,W09-3821,1,0.178099,"Missing"
C10-2013,W10-1409,1,0.292957,"ather free order of post-verbal complements/adjuncts and the possibility of subject inversion. Thus, the annotation scheme of the French Treebank (Abeillé and Barrier, 2004) makes use of at syntagmatic trees without VP joakim.nivre@lingl.uu.se nodes, with no structural distinction between complements, adjuncts or post-verbal subjects, but with additional functional annotations on dependents of verbs. Parsing is commonly enhanced by using more abstract lexical information, in the form of morphological features (Tsarfaty, 2006), lemmas (Seddah et al., 2010), or various forms of clusters (see (Candito and Seddah, 2010) for references). In this paper, we explore the integration of morphological features, lemmas, and linear context clusters. Typed dependencies can be derived using many different parsing architectures. As far as statistical approaches are concerned, the dominant paradigm for English has been to use constituency-based parsers, the output of which can be converted to typed dependencies using well-proven conversion procedures, as in the Stanford parser (Klein and Manning, 2003). In recent years, it has also become popular to use statistical dependency parsers, which are trained directly on labele"
C10-2013,candito-etal-2010-statistical,1,0.637544,"nks For training and testing the statistical parsers, we use treebanks that are automatically converted from the French Treebank (Abeillé and Barrier, 2004) (hereafter F TB), a constituency-based treebank made up of 12, 531 sentences from the Le Monde newspaper. Each sentence is annotated with a constituent structure and words bear the following features: gender, number, mood, tense, person, deniteness, wh-feature, and clitic case. Nodes representing dependents of a verb are labeled with one of 8 grammatical functions.1 We use two treebanks automatically obtained from F TB, both described in Candito et al. (2010). F TB - UC is a modied version of the original constituency-based treebank, where the rich morphological annotation has been mapped to a simple tagset of 28 part-of-speech tags, and where compounds with regular syntax are broken down into phrases containing several simple words while remaining sequences annotated as compounds in F TB are merged into a single token. Function labels are appended to syntactic category symbols and are either used or ignored, depending on the task. F TB - UC -D EP is a dependency treebank derived from F TB - UC using the classic technique of head propagation rule"
C10-2013,cer-etal-2010-parsing,0,0.0356489,"Missing"
C10-2013,A00-2018,0,0.108329,"Missing"
C10-2013,W06-2932,0,0.0627677,"ectly on labeled dependency trees and output such trees directly, such as MSTParser (McDonald, 2006) and MaltParser (Nivre et al., 2006). Dependency parsing has been applied to a fairly broad range of languages, especially in the CoNLL shared tasks in 2006 and 2007 (Buchholz and Marsi, 2006; Nivre et al., 2007). We present a comparison of three statistical parsing architectures that output typed dependencies for French: one constituency-based architecture featuring the Berkeley parser (Petrov et al., 2006), and two dependency-based systems using radically different parsing methods, MSTParser (McDonald et al., 2006) and MaltParser (Nivre et al., 2006). These three systems are compared both in terms of parsing accuracy and parsing times, in realistic settings that only use predicted information. By using freely available software packages that implement language-independent approaches 108 Coling 2010: Poster Volume, pages 108–116, Beijing, August 2010 and applying them to a language different from English, we also hope to shed some light on the capacity of different methods to cope with the challenges posed by different languages. Comparative evaluation of constituency-based and dependency-based parsers w"
C10-2013,de-marneffe-etal-2006-generating,0,0.00964342,"Missing"
C10-2013,Y09-1013,1,0.190601,"ed global inference for graph-based dependency parsing is NP-hard, and graph-based parsers like MSTParser therefore limit the scope of their features to a small number of adjacent arcs (usually two) and/or resort to approximate inference (McDonald and Pereira, 2006). For our experiments, we use MSTParser 0.4.3b4 with 1-best projective decoding, using the algorithm of Eisner (1996), and second order features. The labeling of dependencies is performed as a separate sequence classication step, following McDonald et al. (2006). To provide part-of-speech tags to MSTParser, we use the MElt tagger (Denis and Sagot, 2009), a Maximum Entropy Markov Model tagger enriched with information from a large-scale dictionary.5 The tagger was trained on the training set to provide POS tags for the dev and test sets, and we used 10-way jackkning to generate tags for the training set. 3.3 MaltParser MaltParser6 is a freely available implementation of the parsing models described in (Nivre, 2006) and (Nivre, 2008). These models are often characterized as transition-based, because they reduce the problem of parsing a sentence to the problem of nding an optimal path through an abstract transition system, or state machine. T"
C10-2013,C96-1058,0,0.154658,"arsers typically use global training algorithms, where the goal is to learn to score correct trees higher than incorrect trees. At parsing time a global search is run to nd the highest scoring dependency tree. However, unrestricted global inference for graph-based dependency parsing is NP-hard, and graph-based parsers like MSTParser therefore limit the scope of their features to a small number of adjacent arcs (usually two) and/or resort to approximate inference (McDonald and Pereira, 2006). For our experiments, we use MSTParser 0.4.3b4 with 1-best projective decoding, using the algorithm of Eisner (1996), and second order features. The labeling of dependencies is performed as a separate sequence classication step, following McDonald et al. (2006). To provide part-of-speech tags to MSTParser, we use the MElt tagger (Denis and Sagot, 2009), a Maximum Entropy Markov Model tagger enriched with information from a large-scale dictionary.5 The tagger was trained on the training set to provide POS tags for the dev and test sets, and we used 10-way jackkning to generate tags for the training set. 3.3 MaltParser MaltParser6 is a freely available implementation of the parsing models described in (Nivr"
C10-2013,P98-1106,0,0.0427852,"Missing"
C10-2013,P03-1054,0,0.00198719,"n the form of morphological features (Tsarfaty, 2006), lemmas (Seddah et al., 2010), or various forms of clusters (see (Candito and Seddah, 2010) for references). In this paper, we explore the integration of morphological features, lemmas, and linear context clusters. Typed dependencies can be derived using many different parsing architectures. As far as statistical approaches are concerned, the dominant paradigm for English has been to use constituency-based parsers, the output of which can be converted to typed dependencies using well-proven conversion procedures, as in the Stanford parser (Klein and Manning, 2003). In recent years, it has also become popular to use statistical dependency parsers, which are trained directly on labeled dependency trees and output such trees directly, such as MSTParser (McDonald, 2006) and MaltParser (Nivre et al., 2006). Dependency parsing has been applied to a fairly broad range of languages, especially in the CoNLL shared tasks in 2006 and 2007 (Buchholz and Marsi, 2006; Nivre et al., 2007). We present a comparison of three statistical parsing architectures that output typed dependencies for French: one constituency-based architecture featuring the Berkeley parser (Pet"
C10-2013,P08-1068,0,0.546912,"POS tags. Motivations for these features are rooted in the fact that French has a rather rich inectional morphology. The intuition behind using morphological features like tense, mood, gender, number, and person is that some of these are likely to provide additional cues for syntactic attachment or function type. This is especially true given that the 29 tags used by the MElt tagger are rather coarse-grained. The use of lemmas and word clusters, on the other hand, is motivated by data sparseness considerations: these provide various degrees of generalization over word forms. As suggested by Koo et al. (2008), the use of word clusters may also reduce the need for annotated data. All our features are automatically produced: no features except word forms originate from the treebank. Our aim was to assess the performance currently available for French in a realistic setting. Lemmas Lemmatized forms are extracted using Lefff (Sagot, 2010), a large-coverage morphosyntactic lexicon for French, and a set of heuristics for unknown words. More specically, Lefff is queried for each (word, pos), where pos is the tag predicted by the MElt tagger. If the pair is found, we use the longest lemma associated with"
C10-2013,P95-1037,0,0.137856,"Missing"
C10-2013,P05-1010,0,0.033322,"Missing"
C10-2013,D07-1013,1,0.235204,"Missing"
C10-2013,E06-1011,0,0.0377804,"a sentence to the problem of nding a directed maximum spanning tree in a dense graph representation of the sentence. Graph-based parsers typically use global training algorithms, where the goal is to learn to score correct trees higher than incorrect trees. At parsing time a global search is run to nd the highest scoring dependency tree. However, unrestricted global inference for graph-based dependency parsing is NP-hard, and graph-based parsers like MSTParser therefore limit the scope of their features to a small number of adjacent arcs (usually two) and/or resort to approximate inference (McDonald and Pereira, 2006). For our experiments, we use MSTParser 0.4.3b4 with 1-best projective decoding, using the algorithm of Eisner (1996), and second order features. The labeling of dependencies is performed as a separate sequence classication step, following McDonald et al. (2006). To provide part-of-speech tags to MSTParser, we use the MElt tagger (Denis and Sagot, 2009), a Maximum Entropy Markov Model tagger enriched with information from a large-scale dictionary.5 The tagger was trained on the training set to provide POS tags for the dev and test sets, and we used 10-way jackkning to generate tags for the t"
C10-2013,nivre-etal-2006-maltparser,1,0.27488,"ear context clusters. Typed dependencies can be derived using many different parsing architectures. As far as statistical approaches are concerned, the dominant paradigm for English has been to use constituency-based parsers, the output of which can be converted to typed dependencies using well-proven conversion procedures, as in the Stanford parser (Klein and Manning, 2003). In recent years, it has also become popular to use statistical dependency parsers, which are trained directly on labeled dependency trees and output such trees directly, such as MSTParser (McDonald, 2006) and MaltParser (Nivre et al., 2006). Dependency parsing has been applied to a fairly broad range of languages, especially in the CoNLL shared tasks in 2006 and 2007 (Buchholz and Marsi, 2006; Nivre et al., 2007). We present a comparison of three statistical parsing architectures that output typed dependencies for French: one constituency-based architecture featuring the Berkeley parser (Petrov et al., 2006), and two dependency-based systems using radically different parsing methods, MSTParser (McDonald et al., 2006) and MaltParser (Nivre et al., 2006). These three systems are compared both in terms of parsing accuracy and parsi"
C10-2013,J08-4003,1,0.445253,"rder features. The labeling of dependencies is performed as a separate sequence classication step, following McDonald et al. (2006). To provide part-of-speech tags to MSTParser, we use the MElt tagger (Denis and Sagot, 2009), a Maximum Entropy Markov Model tagger enriched with information from a large-scale dictionary.5 The tagger was trained on the training set to provide POS tags for the dev and test sets, and we used 10-way jackkning to generate tags for the training set. 3.3 MaltParser MaltParser6 is a freely available implementation of the parsing models described in (Nivre, 2006) and (Nivre, 2008). These models are often characterized as transition-based, because they reduce the problem of parsing a sentence to the problem of nding an optimal path through an abstract transition system, or state machine. This is sometimes equated with shift-reduce parsing, but in fact includes a much broader range of transition systems (Nivre, 2008). Transition-based parsers learn models that predict the next state given the current state of the system, including features over the history of parsing decisions and the input sentence. At parsing time, the parser starts in an initial state and greedily mo"
C10-2013,N07-1051,0,0.0285364,"Missing"
C10-2013,P06-1055,0,0.0252933,"three architectures in more detail.2 3 The Berkeley parser is a freely available implementation of the statistical training and parsing algorithms described in (Petrov et al., 2006) and (Petrov and Klein, 2007). It exploits the fact that PCFG learning can be improved by splitting symbols according to structural and/or lexical properties (Klein and Manning, 2003). Following Matsuzaki et al. (2005), the Berkeley learning algorithm uses EM to estimate probabilities on symbols that are automatically augmented with latent annotations, a process that can be viewed as symbol splitting. Petrov et al. (2006) proposed to score the splits in order to retain only the most benecial ones, and keep the grammar size manageable: the splits that induce the smallest losses in the likelihood of the treebank are merged back. The algorithm starts with a very general treebank-induced binarized PCFG, with order h horizontal markovisation. created, where at each level a symbol appears without track of its original siblings. Then the Berkeley algorithm performs split/merge/smooth cycles that iteratively rene the binarized grammar: it adds two latent annotations on each symbol, learns probabilities for the rene"
C10-2013,sagot-2010-lefff,0,0.0338599,"Missing"
C10-2013,W10-1410,1,0.880622,"Missing"
C10-2013,P06-3009,0,0.0189149,"which bare syntagmatic trees are often syntactically underspecied because of a rather free order of post-verbal complements/adjuncts and the possibility of subject inversion. Thus, the annotation scheme of the French Treebank (Abeillé and Barrier, 2004) makes use of at syntagmatic trees without VP joakim.nivre@lingl.uu.se nodes, with no structural distinction between complements, adjuncts or post-verbal subjects, but with additional functional annotations on dependents of verbs. Parsing is commonly enhanced by using more abstract lexical information, in the form of morphological features (Tsarfaty, 2006), lemmas (Seddah et al., 2010), or various forms of clusters (see (Candito and Seddah, 2010) for references). In this paper, we explore the integration of morphological features, lemmas, and linear context clusters. Typed dependencies can be derived using many different parsing architectures. As far as statistical approaches are concerned, the dominant paradigm for English has been to use constituency-based parsers, the output of which can be converted to typed dependencies using well-proven conversion procedures, as in the Stanford parser (Klein and Manning, 2003). In recent years, it has als"
C10-2013,W03-3023,0,0.125602,"ijing, August 2010 and applying them to a language different from English, we also hope to shed some light on the capacity of different methods to cope with the challenges posed by different languages. Comparative evaluation of constituency-based and dependency-based parsers with respect to labeled accuracy is rare, despite the fact that parser evaluation on typed dependencies has been advocated for a long time (Lin, 1995; Carroll et al., 1998). Early work on statistical dependency parsing often compared constituency-based and dependency-based methods with respect to their unlabeled accuracy (Yamada and Matsumoto, 2003), but comparison of different approaches with respect to labeled accuracy is more recent. Cer et al. (2010) present a thorough analysis of the best trade-off between speed and accuracy in deriving Stanford typed dependencies for English (de Marneffe et al., 2006), comparing a number of constituency-based and dependency-based parsers on data from the Wall Street Journal. They conclude that the highest accuracy is obtained using constituency-based parsers, although some of the dependency-based parsers are more efcient. For German, the 2008 ACL workshop on parsing German (Kübler, 2008) featured"
C10-2013,W08-1008,0,\N,Missing
C10-2013,C98-1102,0,\N,Missing
C10-2013,D07-1096,1,\N,Missing
C12-1149,W09-3821,1,0.874376,"cDonald, 2012). Needless to say, such observations are likely to be even more true on web data written in morphologically rich languages (MRLS). These languages are already known to be arguably harder to parse than English for a variety of reasons (e.g., small treebank size, rich inflexion, free word order, etc.) exposed in details in (Tsarfaty et al., 2010). However, a lot of progress has been made in parsing MRLS using, for examples, techniques built on richer syntactic models, lexical data sparseness reduction or rich feature set. See (Tsarfaty and Sima’an, 2008; Versley and Rehbein, 2009; Candito and Crabbé, 2009; Green and Manning, 2010) to name but a few. The questions are thus to know: (1) to what extend MRL user generated content is parsable? and (2) more importantly, what is needed to fill that performance gap? To answer question 1, we introduce the first release of the French Social Media Treebank, a representative gold standard treebank for French user-generated data. This treebank consists in around 1,700 sentences extracted from various types of French Web 2.0 user generated content (Facebook, Twitter, video games and medical board). This treebank was developed independently from the Google W"
C12-1149,W09-1008,1,0.895823,"Missing"
C12-1149,W11-2905,1,0.787828,"64.14 69.21 63.09 68.63 64.89 79.70 55.90 64.13 - 58.71 65.48 - 57.27 64.80 83.81 64.34 72.69 96.44 T EST LR SET LP F1 Pos acc. OOVs 70.10 70.59 71.68 71.44 70.88 71.02 79.14 75.70 15.42 19.88 31.50 24.70 54.67 71.29 58.16 73.45 56.36 72.35 64.40 78.88 32.84 24.47 38.25 23.40 5.2 55.26 60.98 66.69 - 59.23 61.79 68.50 - 57.18 61.38 67.58 84.10 54.64 70.68 74.43 96.97 50.40 29.52 22.81 4.89 Table 7: Baseline parsing results split by sub corpora and noisiness level word clustering within a PCFG-LA framework. Indeed, we have successfully applied these techniques for French out-of-domain parsing (Candito et al., 2011), as well as for parsing noisy English web data (Seddah et al., 2012). On the longer term we intend to apply our normalization and correction module before parsing. The parser will then be provided with corrected tokens, closely matching our regular training data, instead of unedited ones. This will compensate the lack of user generated content large unlabeled corpora, still lacking for French. 7 Conclusion As mentioned earlier, the French Social Media Bank shares with the Google web bank a common will to extend the traditional treebank domain towards user generated content. Although of a smal"
C12-1149,C10-2013,1,0.848791,"validation and correction by two annotators followed by an adjudication step. • Functional annotation followed by manual validation and correction by two annotators followed by and adjudication step. 5.1 Pre-annotation strategies for the tokenization and POS layers As mentioned above, we used two different strategies for tokenization and POS pre-annotation, depending on the noisiness score. For less noisy corpora (those with a noisiness score below 1), we used a slightly extended version of the tokenization and sentence splitting tools from our standard FTB-based parsing architecture, Bonsai (Candito et al., 2010). This is because we want to have a tokenization that is as close as possible from the principles underlying the FTB’s tokenization. Next, we used the POS-tagger MORFETTE (Chrupała et al., 2008) as a pre-annotator. For corpora with a high noisiness score, we used a specifically developed pre-annotation process. This is because in such corpora, spelling errors are even more frequent, but also because 15 In the Google Web Treebank, the counterpart of our tag Y is the tag GW. 2450 the original tokens rarely match sound linguistic units, as can be seen on the example in Table 4 taken from the DOCT"
C12-1149,W10-1409,1,0.828577,"ed over time and posters. The next step will involve collecting large unlabeled corpora to perform experiments with selftraining techniques (McClosky and Charniak, 2008; Foster et al., 2011b) and unsupervised 16 Simple linear regressions lead to the following results: without the normalization and correction wrapper, the slope is -4.8 and the correlation coefficient is 0.77; with the wrapper, the slope is -7.2 with a correlation coefficient as high as 0.88 (coefficients of determination are thus respectively 0.59 and 0.77). 17 For convenience, we provide also baseline results on the FTB, see (Candito and Seddah, 2010). 2453 DEV DOCTISSIMO high noisiness other JEUXVIDEOS. COM T WIT TER high noisiness other FACEBOOK high noisiness other all FTB SET LR LP F1 Pos acc. OOVs 37.22 69.68 66.56 41.20 70.19 66.46 39.11 69.94 66.51 51.72 77.96 74.56 40.47 15.56 20.46 62.07 68.06 64.14 69.21 63.09 68.63 64.89 79.70 55.90 64.13 - 58.71 65.48 - 57.27 64.80 83.81 64.34 72.69 96.44 T EST LR SET LP F1 Pos acc. OOVs 70.10 70.59 71.68 71.44 70.88 71.02 79.14 75.70 15.42 19.88 31.50 24.70 54.67 71.29 58.16 73.45 56.36 72.35 64.40 78.88 32.84 24.47 38.25 23.40 5.2 55.26 60.98 66.69 - 59.23 61.79 68.50 - 57.18 61.38 67.58 84.1"
C12-1149,F12-2024,1,0.873511,"iness score ‘Forplay have disappeared for at least 6 months, that is there is almost none.’ Parseval F-measure metric between two functionally annotated set of parses. Agreements range between 93.4 for FACEBOOK data and 97.44 for JEUXVIDEOS.COM (Table 5) and are on the same range than the DCU’s Twitter corpus agreement score (Foster et al., 2011a). Similarly to that corpus, the disagreements involve fragments, interjections and the syntactic status to assign to meta-tokens elements. We note that our agreement scores are higher than those reported in other out-of-domain initiatives for French (Candito and Seddah, 2012). This small annotation error rate comes from the fact that the same team annotated both treebanks and was thus highly trained for that task. Maybe more importantly, social media sentences tend to be shorter than their edited counterparts so once POS tagging errors are solved, the annotation task is made relatively easier. DCU ’ S DOCTISSIMO T WIT TER T WIT TERB ANK 95.05 95.40 95.8 JEUXVIDEOS. COM FACEBOOK - 97.44 93.40 - Table 5: Inter Annotator agreement 6 Preliminary experiments Experimental Protocol In the following experiments, we used the FTB -UC as training data set, in its classical s"
C12-1149,chrupala-etal-2008-learning,0,0.0250729,"Missing"
C12-1149,Y09-1013,1,0.861344,"evelopment corpus (all subcorpora but for the noisy Facebook subcorpus) n-gram sequences involving unknown tokens or occurring at an unexpectedly high frequency; then we manually selected the relevant ones and provided them manually with a corresponding “correction”. The number of “corrected tokens” obtained by applying these rules might be different from the number of original tokens. In such cases, we use 1-to-n or n-to-1 mappings. For example, the rule ni a pa → n’ y a pas explicitely states that ni is an amalgam for n’ and y, whereas pas is the correction of pa. 4. We use the MElt tagger (Denis and Sagot, 2009), trained on the FTB -UC and the Lefff lexicon (Sagot, 2010), for POS-tagging the sequence of corrected “tokens”. 5. We apply a set of 15 generic and almost language-independent manually crafted rewriting rules, originally developed for English data (see below), that aim at assigning the correct POS to tokens that belong to categories not found in MElt’s training corpus, i.e., the FTB; for example, all URLs and e-mail addresses are post-tagged as proper nouns whatever the tag provided by MElt; likewise, all smileys get the POS for interjections. 6. We assign POS tags to the original tokens bas"
C12-1149,P11-1118,0,0.0227658,"aptation. Yet, this is far from being the case as shown by Foster (2010). Indeed, in her seminal work on parsing web data, different issues preventing reasonably good parsing performance were highlighted; most of them were tied to lexical differences (coming from either genuine unknown words, typographical divergences, bad segmentation, etc.) or syntactic structures absent from training data (imperative usage, direct discourse, slang, etc.). This suboptimal parsing behavior on web data was in turn confirmed in follow-up works on Twitter and IRC chat (Foster et al., 2011a; Gimpel et al., 2010; Elsner and Charniak, 2011). They were again confirmed during the SANCL shared task, organized by Google, aimed at assessing the performances of parsers on various genres of Web texts (Petrov and McDonald, 2012). Needless to say, such observations are likely to be even more true on web data written in morphologically rich languages (MRLS). These languages are already known to be arguably harder to parse than English for a variety of reasons (e.g., small treebank size, rich inflexion, free word order, etc.) exposed in details in (Tsarfaty et al., 2010). However, a lot of progress has been made in parsing MRLS using, for"
C12-1149,N10-1060,0,0.537119,"is highlights the high difficulty of automatically processing such noisy data in a MRL. KEYWORDS: Treebanking, User Generated Content, Parsing, Social Media. Proceedings of COLING 2012: Technical Papers, pages 2441–2458, COLING 2012, Mumbai, December 2012. 2441 1 Introduction Complaining about the lack of robustness of statistical parsers whenever they are applied on out-of-domain text has almost became an overused cliché over the last few years. It remains true that such parsers only perform well on texts that are comparable to their training corpus, especially in terms of genre. As noted by Foster (2010) and Foster et al. (2011b), most studies on out-of-domain statistical parsing have been focusing mainly on slightly different newspaper texts (Gildea, 2001; McClosky et al., 2006a,b), biomedical data (Lease and Charniak, 2005; McClosky and Charniak, 2008) or balanced corpora mixing different genres (Foster et al., 2007). The common point between these corpora is that they are edited texts. This means that their underlying syntax, spelling, tokenization and typography remain standard, even if they slightly depart from the newspaper genre. Therefore, standard NLP tools can be used on such corpor"
C12-1149,I11-1100,0,0.0885155,"Missing"
C12-1149,W07-2204,1,0.926577,"Missing"
C12-1149,W01-0521,0,0.0471571,". Proceedings of COLING 2012: Technical Papers, pages 2441–2458, COLING 2012, Mumbai, December 2012. 2441 1 Introduction Complaining about the lack of robustness of statistical parsers whenever they are applied on out-of-domain text has almost became an overused cliché over the last few years. It remains true that such parsers only perform well on texts that are comparable to their training corpus, especially in terms of genre. As noted by Foster (2010) and Foster et al. (2011b), most studies on out-of-domain statistical parsing have been focusing mainly on slightly different newspaper texts (Gildea, 2001; McClosky et al., 2006a,b), biomedical data (Lease and Charniak, 2005; McClosky and Charniak, 2008) or balanced corpora mixing different genres (Foster et al., 2007). The common point between these corpora is that they are edited texts. This means that their underlying syntax, spelling, tokenization and typography remain standard, even if they slightly depart from the newspaper genre. Therefore, standard NLP tools can be used on such corpora. Now, new forms of electronic communication have emerged in the last few years,namely social media and Web 2.0 communication media, either synchronous (m"
C12-1149,P11-2008,0,0.148522,"Missing"
C12-1149,C10-1045,0,0.0162317,"o say, such observations are likely to be even more true on web data written in morphologically rich languages (MRLS). These languages are already known to be arguably harder to parse than English for a variety of reasons (e.g., small treebank size, rich inflexion, free word order, etc.) exposed in details in (Tsarfaty et al., 2010). However, a lot of progress has been made in parsing MRLS using, for examples, techniques built on richer syntactic models, lexical data sparseness reduction or rich feature set. See (Tsarfaty and Sima’an, 2008; Versley and Rehbein, 2009; Candito and Crabbé, 2009; Green and Manning, 2010) to name but a few. The questions are thus to know: (1) to what extend MRL user generated content is parsable? and (2) more importantly, what is needed to fill that performance gap? To answer question 1, we introduce the first release of the French Social Media Treebank, a representative gold standard treebank for French user-generated data. This treebank consists in around 1,700 sentences extracted from various types of French Web 2.0 user generated content (Facebook, Twitter, video games and medical board). This treebank was developed independently from the Google Web Treebank (Bies et al.,"
C12-1149,P08-2026,0,0.0646701,"Missing"
C12-1149,N06-1020,0,0.0516947,"of COLING 2012: Technical Papers, pages 2441–2458, COLING 2012, Mumbai, December 2012. 2441 1 Introduction Complaining about the lack of robustness of statistical parsers whenever they are applied on out-of-domain text has almost became an overused cliché over the last few years. It remains true that such parsers only perform well on texts that are comparable to their training corpus, especially in terms of genre. As noted by Foster (2010) and Foster et al. (2011b), most studies on out-of-domain statistical parsing have been focusing mainly on slightly different newspaper texts (Gildea, 2001; McClosky et al., 2006a,b), biomedical data (Lease and Charniak, 2005; McClosky and Charniak, 2008) or balanced corpora mixing different genres (Foster et al., 2007). The common point between these corpora is that they are edited texts. This means that their underlying syntax, spelling, tokenization and typography remain standard, even if they slightly depart from the newspaper genre. Therefore, standard NLP tools can be used on such corpora. Now, new forms of electronic communication have emerged in the last few years,namely social media and Web 2.0 communication media, either synchronous (micro-blogging) or async"
C12-1149,P06-1043,0,0.0320051,"of COLING 2012: Technical Papers, pages 2441–2458, COLING 2012, Mumbai, December 2012. 2441 1 Introduction Complaining about the lack of robustness of statistical parsers whenever they are applied on out-of-domain text has almost became an overused cliché over the last few years. It remains true that such parsers only perform well on texts that are comparable to their training corpus, especially in terms of genre. As noted by Foster (2010) and Foster et al. (2011b), most studies on out-of-domain statistical parsing have been focusing mainly on slightly different newspaper texts (Gildea, 2001; McClosky et al., 2006a,b), biomedical data (Lease and Charniak, 2005; McClosky and Charniak, 2008) or balanced corpora mixing different genres (Foster et al., 2007). The common point between these corpora is that they are edited texts. This means that their underlying syntax, spelling, tokenization and typography remain standard, even if they slightly depart from the newspaper genre. Therefore, standard NLP tools can be used on such corpora. Now, new forms of electronic communication have emerged in the last few years,namely social media and Web 2.0 communication media, either synchronous (micro-blogging) or async"
C12-1149,P06-1055,0,0.162253,"Missing"
C12-1149,N07-1051,0,0.0137649,"provided by MElt for the corrected token is assigned to the corresponding original token. This architecture is now available as part of the MElt distribution. It was also applied on English web data in the context of the SANCL shared task on parsing web data (Petrov and McDonald, 2012), with state-of-the-art results (Seddah et al., 2012). 5.2 Annotation strategy for constituency and functional annotation Parse pre-annotation was achieved using a state-of-the-art statistical parser trained on the FTB UC, provided with the manually validated tagging. The parser we used was the Berkeley parser (Petrov and Klein, 2007) adapted to French (Crabbé and Candito, 2008). Note that when the validated pos tags were discarded by the parser, in case of too many unknown word-pos pairs, those were reinserted. To assess the quality of annotation, we calculated the inter annotator agreement using the 2451 Original tokens Gold corrected “tokens” Automatically corrected and POS-tagged “tokens” sa fé o moin 6 mois qe les preliminaires sont sauté c a dire qil yen a presk pa ça fait au_moins 6 mois que les préliminaires sont sautés c’est-à-dire qu’ il y en a presque pas ça/PRO fait/V au/P+D moins/ADV 6/DET mois/NC que/PROREL l"
C12-1149,sagot-2010-lefff,1,0.839342,") n-gram sequences involving unknown tokens or occurring at an unexpectedly high frequency; then we manually selected the relevant ones and provided them manually with a corresponding “correction”. The number of “corrected tokens” obtained by applying these rules might be different from the number of original tokens. In such cases, we use 1-to-n or n-to-1 mappings. For example, the rule ni a pa → n’ y a pas explicitely states that ni is an amalgam for n’ and y, whereas pas is the correction of pa. 4. We use the MElt tagger (Denis and Sagot, 2009), trained on the FTB -UC and the Lefff lexicon (Sagot, 2010), for POS-tagging the sequence of corrected “tokens”. 5. We apply a set of 15 generic and almost language-independent manually crafted rewriting rules, originally developed for English data (see below), that aim at assigning the correct POS to tokens that belong to categories not found in MElt’s training corpus, i.e., the FTB; for example, all URLs and e-mail addresses are post-tagged as proper nouns whatever the tag provided by MElt; likewise, all smileys get the POS for interjections. 6. We assign POS tags to the original tokens based on the mappings between corrected POStagged tokens and or"
C12-1149,W10-1401,1,0.834989,"Twitter and IRC chat (Foster et al., 2011a; Gimpel et al., 2010; Elsner and Charniak, 2011). They were again confirmed during the SANCL shared task, organized by Google, aimed at assessing the performances of parsers on various genres of Web texts (Petrov and McDonald, 2012). Needless to say, such observations are likely to be even more true on web data written in morphologically rich languages (MRLS). These languages are already known to be arguably harder to parse than English for a variety of reasons (e.g., small treebank size, rich inflexion, free word order, etc.) exposed in details in (Tsarfaty et al., 2010). However, a lot of progress has been made in parsing MRLS using, for examples, techniques built on richer syntactic models, lexical data sparseness reduction or rich feature set. See (Tsarfaty and Sima’an, 2008; Versley and Rehbein, 2009; Candito and Crabbé, 2009; Green and Manning, 2010) to name but a few. The questions are thus to know: (1) to what extend MRL user generated content is parsable? and (2) more importantly, what is needed to fill that performance gap? To answer question 1, we introduce the first release of the French Social Media Treebank, a representative gold standard treeban"
C12-1149,C08-1112,0,0.0270883,"Missing"
C12-1149,W09-3820,0,0.0221679,"of Web texts (Petrov and McDonald, 2012). Needless to say, such observations are likely to be even more true on web data written in morphologically rich languages (MRLS). These languages are already known to be arguably harder to parse than English for a variety of reasons (e.g., small treebank size, rich inflexion, free word order, etc.) exposed in details in (Tsarfaty et al., 2010). However, a lot of progress has been made in parsing MRLS using, for examples, techniques built on richer syntactic models, lexical data sparseness reduction or rich feature set. See (Tsarfaty and Sima’an, 2008; Versley and Rehbein, 2009; Candito and Crabbé, 2009; Green and Manning, 2010) to name but a few. The questions are thus to know: (1) to what extend MRL user generated content is parsable? and (2) more importantly, what is needed to fill that performance gap? To answer question 1, we introduce the first release of the French Social Media Treebank, a representative gold standard treebank for French user-generated data. This treebank consists in around 1,700 sentences extracted from various types of French Web 2.0 user generated content (Facebook, Twitter, video games and medical board). This treebank was developed indep"
C12-1149,I05-1006,0,\N,Missing
C12-1149,N03-1031,0,\N,Missing
C12-1149,N10-1004,0,\N,Missing
C16-1040,abeille-barrier-2004-enriching,0,0.127213,"Missing"
C16-1040,P98-1013,0,0.401614,"ic phenomena such as alternations, control and raising tend to obfuscate the relation between syntax and semantics. In this paper we predict the semantic structure of a sentence using a deeper syntax than what is usually done. This deep syntactic representation abstracts away from purely syntactic phenomena and proposes a structural organization of the sentence that is closer to the semantic representation. Experiments conducted on a French corpus annotated with semantic frames showed that a semantic parser reaches better performances with such a deep syntactic input. 1 Introduction FrameNet (Baker et al., 1998) is an English resource containing a set of inter-related semantic frames, each frame containing a set of semantic roles (frame elements in FrameNet’s terminology). Frames offer semantic generalizations over individual predicates, since different lexical units can evoke the same frame, and semantic roles offer generalizations over syntactic arguments. Hence FrameNet parsing can be viewed as mixing predicate disambiguation and semantic role labelling.1 Although FrameNet is more semantically-oriented than other semantic role labeling resources such as PropBank (Palmer et al., 2005), syntactic in"
C16-1040,W13-4916,0,0.0644053,"Missing"
C16-1040,burchardt-etal-2006-salsa,0,0.0709356,"Missing"
C16-1040,P04-1041,0,0.257367,"Missing"
C16-1040,F12-2024,1,0.849637,"Missing"
C16-1040,candito-etal-2014-developing,1,0.868508,"Missing"
C16-1040,candito-etal-2014-deep,1,0.904832,"Missing"
C16-1040,J14-1002,0,0.278828,"y (Gildea and Hockenmaier, 2003) work shows that using CCG-derived predicate-argument features predicted by a CCG parser improves the identification of core PropBank arguments. Vickrey and Koller (2008) investigate the use of simplified syntactic paths and report a slight improvement when applying transformation rules to simplify phrase-structure parses. As far as FrameNet parsing is concerned, we don’t know of any work using more abstract syntactic input than plain “surface” syntactic trees, whether phrase-structure (Gildea and Jurafsky, 2002) or dependency trees (Johansson and Nugues, 2007; Das et al., 2014). We focus on French, first because of the availability of the afore-mentioned DSR, and second because in the French FrameNet corpus (Djemaa et al., 2016) the annotated semantic roles are restricted to essential arguments. On the contrary, both essential (“core”) and non essential participants are annotated in the English FrameNet, including modifiers such as time, location, purpose etc... But syntactic variation such as syntactic alternations, VP coordination, control etc... does concern primarily the most salient grammatical functions (subject, direct object, indirect object etc...), which a"
C16-1040,L16-1601,1,0.883433,"core PropBank arguments. Vickrey and Koller (2008) investigate the use of simplified syntactic paths and report a slight improvement when applying transformation rules to simplify phrase-structure parses. As far as FrameNet parsing is concerned, we don’t know of any work using more abstract syntactic input than plain “surface” syntactic trees, whether phrase-structure (Gildea and Jurafsky, 2002) or dependency trees (Johansson and Nugues, 2007; Das et al., 2014). We focus on French, first because of the availability of the afore-mentioned DSR, and second because in the French FrameNet corpus (Djemaa et al., 2016) the annotated semantic roles are restricted to essential arguments. On the contrary, both essential (“core”) and non essential participants are annotated in the English FrameNet, including modifiers such as time, location, purpose etc... But syntactic variation such as syntactic alternations, VP coordination, control etc... does concern primarily the most salient grammatical functions (subject, direct object, indirect object etc...), which are typically the ones that essential arguments bear. Hence, neutralizing syntactic variation is expected to have an impact primarily on essential semantic"
C16-1040,W03-1008,0,0.0542579,"ntations (DSR) to refer to surface syntactic trees and deep syntactic graphs. Using abstract syntactic representations as an intermediate representation level between syntax and semantics has been proposed in different theoretical frameworks, such as derived trees of Tree Adjoining Grammars (Joshi and Schabes, 1997) or deep syntactic structures of the Meaning Text Theory (Mel’ˇcuk, 1988). But we only found few works showing, empirically, that using such representations can effectively help predict the semantic roles of predicates.Two of them concern PropBank semantic role labeling. The early (Gildea and Hockenmaier, 2003) work shows that using CCG-derived predicate-argument features predicted by a CCG parser improves the identification of core PropBank arguments. Vickrey and Koller (2008) investigate the use of simplified syntactic paths and report a slight improvement when applying transformation rules to simplify phrase-structure parses. As far as FrameNet parsing is concerned, we don’t know of any work using more abstract syntactic input than plain “surface” syntactic trees, whether phrase-structure (Gildea and Jurafsky, 2002) or dependency trees (Johansson and Nugues, 2007; Das et al., 2014). We focus on F"
C16-1040,J02-3001,0,0.750944,"roles (frame elements in FrameNet’s terminology). Frames offer semantic generalizations over individual predicates, since different lexical units can evoke the same frame, and semantic roles offer generalizations over syntactic arguments. Hence FrameNet parsing can be viewed as mixing predicate disambiguation and semantic role labelling.1 Although FrameNet is more semantically-oriented than other semantic role labeling resources such as PropBank (Palmer et al., 2005), syntactic information has been shown to be decisive for predicting (FrameNet) semantic roles since the early days of the task (Gildea and Jurafsky, 2002). Linking regularities provide the theoretical justification of this result: there exist regularities in how semantic arguments are realized in syntax. Yet it is well known that the mapping from syntactic arguments to semantic ones is not straightforward. First, lexical idiosyncrasies can come into play, for instance the Addressee of communication verbs may correspond to the indirect object for verbs like to say and to the direct object for a verb like to inform. Second, it is also well known that surface syntax exhibits variation that can obfuscate regularities. For instance though the Speake"
C16-1040,hajic-etal-2012-announcing,0,0.071915,"Missing"
C16-1040,J07-3004,0,0.0194536,"additional information is sometimes viewed as pertaining to semantic representations, sometimes retained as still syntactic. English has been the first focus language, along with Czech thanks to the Prague Dependency Treebank (Hajiˇc et al., 2006). For English, several works automatically convert Penn Treebank constituency trees into deeper representations, based on lexicalized grammar formalisms such as LFG, CCG or HPSG. Cahill et al. (2004) automatically construct LFG f-structures from PTB trees, a work adapted for various other languages including French (Schluter and van Genabith, 2008). Hockenmaier and Steedman (2007) extracted a corpus of CCG derivations and dependency structures from the Penn Treebank. These two kinds of deeper representations do capture long distance dependencies, subjects of non finite verbs, argument sharing between coordinated verbs. When compared to the DSRs we use though, the main missing trait is the neutralization of syntactic alternations, which we believe is a major source for the syntactic path normalization effect described in section 2.38 . The Stanford dependencies (SD, De Marneffe and Manning (2008)) constitute another proposal for obtaining dependencies not directly prese"
C16-1040,S07-1048,0,0.0380114,"ntic role labeling. The early (Gildea and Hockenmaier, 2003) work shows that using CCG-derived predicate-argument features predicted by a CCG parser improves the identification of core PropBank arguments. Vickrey and Koller (2008) investigate the use of simplified syntactic paths and report a slight improvement when applying transformation rules to simplify phrase-structure parses. As far as FrameNet parsing is concerned, we don’t know of any work using more abstract syntactic input than plain “surface” syntactic trees, whether phrase-structure (Gildea and Jurafsky, 2002) or dependency trees (Johansson and Nugues, 2007; Das et al., 2014). We focus on French, first because of the availability of the afore-mentioned DSR, and second because in the French FrameNet corpus (Djemaa et al., 2016) the annotated semantic roles are restricted to essential arguments. On the contrary, both essential (“core”) and non essential participants are annotated in the English FrameNet, including modifiers such as time, location, purpose etc... But syntactic variation such as syntactic alternations, VP coordination, control etc... does concern primarily the most salient grammatical functions (subject, direct object, indirect obje"
C16-1040,P11-4015,1,0.84752,"Missing"
C16-1040,S14-2008,0,0.0709768,"Missing"
C16-1040,J05-1004,0,0.168366,"tion FrameNet (Baker et al., 1998) is an English resource containing a set of inter-related semantic frames, each frame containing a set of semantic roles (frame elements in FrameNet’s terminology). Frames offer semantic generalizations over individual predicates, since different lexical units can evoke the same frame, and semantic roles offer generalizations over syntactic arguments. Hence FrameNet parsing can be viewed as mixing predicate disambiguation and semantic role labelling.1 Although FrameNet is more semantically-oriented than other semantic role labeling resources such as PropBank (Palmer et al., 2005), syntactic information has been shown to be decisive for predicting (FrameNet) semantic roles since the early days of the task (Gildea and Jurafsky, 2002). Linking regularities provide the theoretical justification of this result: there exist regularities in how semantic arguments are realized in syntax. Yet it is well known that the mapping from syntactic arguments to semantic ones is not straightforward. First, lexical idiosyncrasies can come into play, for instance the Addressee of communication verbs may correspond to the indirect object for verbs like to say and to the direct object for"
C16-1040,W12-4625,1,0.848471,"Missing"
C16-1040,schluter-van-genabith-2008-treebank,0,0.0725663,"Missing"
C16-1040,P08-1040,0,0.0330744,"and semantics has been proposed in different theoretical frameworks, such as derived trees of Tree Adjoining Grammars (Joshi and Schabes, 1997) or deep syntactic structures of the Meaning Text Theory (Mel’ˇcuk, 1988). But we only found few works showing, empirically, that using such representations can effectively help predict the semantic roles of predicates.Two of them concern PropBank semantic role labeling. The early (Gildea and Hockenmaier, 2003) work shows that using CCG-derived predicate-argument features predicted by a CCG parser improves the identification of core PropBank arguments. Vickrey and Koller (2008) investigate the use of simplified syntactic paths and report a slight improvement when applying transformation rules to simplify phrase-structure parses. As far as FrameNet parsing is concerned, we don’t know of any work using more abstract syntactic input than plain “surface” syntactic trees, whether phrase-structure (Gildea and Jurafsky, 2002) or dependency trees (Johansson and Nugues, 2007; Das et al., 2014). We focus on French, first because of the availability of the afore-mentioned DSR, and second because in the French FrameNet corpus (Djemaa et al., 2016) the annotated semantic roles a"
C16-1040,C98-1013,0,\N,Missing
candito-etal-2010-statistical,levy-andrew-2006-tregex,0,\N,Missing
candito-etal-2010-statistical,de-marneffe-etal-2006-generating,0,\N,Missing
candito-etal-2010-statistical,W03-3023,0,\N,Missing
candito-etal-2010-statistical,E06-1011,0,\N,Missing
candito-etal-2010-statistical,W09-3821,1,\N,Missing
candito-etal-2010-statistical,W02-1001,0,\N,Missing
candito-etal-2010-statistical,C96-1058,0,\N,Missing
candito-etal-2010-statistical,J03-4003,0,\N,Missing
candito-etal-2010-statistical,P05-1038,0,\N,Missing
candito-etal-2010-statistical,P05-1012,0,\N,Missing
candito-etal-2010-statistical,Y09-1013,1,\N,Missing
candito-etal-2010-statistical,P06-1055,0,\N,Missing
candito-etal-2010-statistical,W06-2932,0,\N,Missing
candito-etal-2010-statistical,P05-1010,0,\N,Missing
candito-etal-2010-statistical,W07-2416,0,\N,Missing
candito-etal-2010-statistical,P95-1037,0,\N,Missing
candito-etal-2010-statistical,abeille-barrier-2004-enriching,0,\N,Missing
candito-etal-2010-statistical,D07-1096,0,\N,Missing
candito-etal-2014-deep,candito-etal-2010-statistical,1,\N,Missing
candito-etal-2014-deep,de-marneffe-etal-2006-generating,0,\N,Missing
candito-etal-2014-deep,J93-2004,0,\N,Missing
candito-etal-2014-deep,W09-4624,0,\N,Missing
candito-etal-2014-deep,H94-1020,0,\N,Missing
candito-etal-2014-deep,P05-1011,0,\N,Missing
candito-etal-2014-deep,P04-1041,0,\N,Missing
candito-etal-2014-deep,W00-1436,0,\N,Missing
candito-etal-2014-deep,J05-1004,0,\N,Missing
candito-etal-2014-deep,W13-3724,0,\N,Missing
candito-etal-2014-deep,abeille-barrier-2004-enriching,0,\N,Missing
candito-etal-2014-developing,mouton-etal-2010-framenet,1,\N,Missing
candito-etal-2014-developing,burchardt-etal-2006-salto,0,\N,Missing
candito-etal-2014-developing,burchardt-pennacchiotti-2008-fate,0,\N,Missing
candito-etal-2014-developing,sagot-etal-2010-lexicon,1,\N,Missing
candito-etal-2014-developing,N10-1138,0,\N,Missing
candito-etal-2014-developing,burchardt-etal-2006-salsa,0,\N,Missing
candito-etal-2014-developing,P98-1013,0,\N,Missing
candito-etal-2014-developing,C98-1013,0,\N,Missing
candito-etal-2014-developing,W13-4917,1,\N,Missing
candito-etal-2014-developing,P11-2023,1,\N,Missing
candito-etal-2014-developing,J02-3001,0,\N,Missing
candito-etal-2014-developing,J05-1004,0,\N,Missing
candito-etal-2014-developing,heppin-gronostaj-2012-rocky,0,\N,Missing
candito-etal-2014-developing,I11-1132,1,\N,Missing
candito-etal-2014-developing,abeille-barrier-2004-enriching,0,\N,Missing
D11-1113,abeille-barrier-2004-enriching,0,0.0543325,"Missing"
D11-1113,N07-1049,0,0.70456,"tactic dependency parse correction, attachments in an input parse tree are revised by selecting, for a given dependent, the best governor from within a small set of candidates. The motivation behind parse correction is that attachment decisions, especially traditionally difficult ones like pp-attachment and coordination, may require substantial contextual information in order to be made accurately. Because syntactic dependency parsers predict the parse tree for an entire sentence, they may not be able to take Related Work Previous research directly concerning parse correction includes that of Attardi and Ciaramita (2007), working on English and Swedish, who use an approach that considers a fixed set of revision rules: each rule describes movements in the parse tree leading from a dependent’s original governor to a new governor, and a classifier is trained to select the correct revision rule for a given dependent. One drawback of this approach is that the classes lack semantic coherence: a sequence of movements does not necessarily have the same meaning across differ1222 Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1222–1233, c Edinburgh, Scotland, UK, July 27–3"
D11-1113,N09-2066,0,0.0749889,"Missing"
D11-1113,J07-4002,0,0.292496,"Missing"
D11-1113,candito-etal-2010-statistical,1,0.847173,"Missing"
D11-1113,C10-2013,1,0.816747,"nts, they come at the cost of time complexity: for the projective case, adaptations of Eisner’s algorithm (Eisner, 1996) are O(n3 ) for 1-edge factors (McDonald et al., 2005) or sibling 2-edge factors (McDonald and Pereira, 2006), and O(n4 ) for general 2-edge factors (Carreras, 2007) or 3-edge factors (Koo and Collins, 2010). 3.3 Constituency-Based Parsing Beyond the two main approaches to dependency parsing, there is also the approach of constituencybased parsing followed by a conversion step to dependency structure. We use the three-step parsing architecture previously tested for French by Candito et al. (2010a): (i) A constituency parse tree is output by the BerkeleyParser, which has been trained to learn a probabilistic context-free grammar with latent annotations (Petrov et al., 2006) that has parsing time complexity O(n3 ) (Matsuzaki et al., 2005); (ii) A functional role labeler using a Maximum Entropy model adds functional annotations to links between a verb and its dependents; (iii) Constituency trees are automatically converted into projective dependency trees, with remaining unlabeled dependencies assigned labels using a rule-based approach. 3.4 Baseline Parsers In this paper, we use the fo"
D11-1113,D07-1101,0,0.0175588,"ed edges) between each pair of words (nodes) in a sentence. It finds the k-best scoring parse trees, both during training and at parse time, where the score of a tree is the sum of the scores of its factors (consisting of one or more linked edges). While large factors are desirable for capturing sophisticated linguistic constraints, they come at the cost of time complexity: for the projective case, adaptations of Eisner’s algorithm (Eisner, 1996) are O(n3 ) for 1-edge factors (McDonald et al., 2005) or sibling 2-edge factors (McDonald and Pereira, 2006), and O(n4 ) for general 2-edge factors (Carreras, 2007) or 3-edge factors (Koo and Collins, 2010). 3.3 Constituency-Based Parsing Beyond the two main approaches to dependency parsing, there is also the approach of constituencybased parsing followed by a conversion step to dependency structure. We use the three-step parsing architecture previously tested for French by Candito et al. (2010a): (i) A constituency parse tree is output by the BerkeleyParser, which has been trained to learn a probabilistic context-free grammar with latent annotations (Petrov et al., 2006) that has parsing time complexity O(n3 ) (Matsuzaki et al., 2005); (ii) A functional"
D11-1113,P05-1022,0,0.296207,"lowed to be rearranged in ways that introduce non-projectivity in order to increase its overall score. This rearrangement approach resembles parse correction because it is a second step that can revise attachments made in the first step, but it differs in a number of ways: it is dependent on a graph-based parsing approach, it does not model errors made by the parser, and it can only output non-projective variants of the predicted parse tree. As a process that revises the output of a syntactic parser, parse reranking is also similar to parse correction. A well-studied subject (e.g. the work of Charniak and Johnson (2005) and of Collins and Koo (2005)), parse reranking is concerned with the reordering of n-best ranked parse trees output by a syntactic parser. Parse correction has a number of advantages compared to reranking: it can be 1223 used with parsers that do not output n-best ranked parses, it can be easily restricted to specific attachment types, and its output space of parse trees is not limited to those appearing in an n-best list. However, parse reranking has the advantage of selecting the globally optimal parse for a sentence from an nbest list, while parse correction makes only locally optimal rev"
D11-1113,J05-1003,0,0.0554821,"introduce non-projectivity in order to increase its overall score. This rearrangement approach resembles parse correction because it is a second step that can revise attachments made in the first step, but it differs in a number of ways: it is dependent on a graph-based parsing approach, it does not model errors made by the parser, and it can only output non-projective variants of the predicted parse tree. As a process that revises the output of a syntactic parser, parse reranking is also similar to parse correction. A well-studied subject (e.g. the work of Charniak and Johnson (2005) and of Collins and Koo (2005)), parse reranking is concerned with the reordering of n-best ranked parse trees output by a syntactic parser. Parse correction has a number of advantages compared to reranking: it can be 1223 used with parsers that do not output n-best ranked parses, it can be easily restricted to specific attachment types, and its output space of parse trees is not limited to those appearing in an n-best list. However, parse reranking has the advantage of selecting the globally optimal parse for a sentence from an nbest list, while parse correction makes only locally optimal revisions in the predicted parse"
D11-1113,Y09-1013,0,0.0258359,"Missing"
D11-1113,C96-1058,0,0.0634776,"sing, whose seminal work is that of McDonald et al. (2005), the parsing process selects the globally optimal parse tree from a graph containing attachments (directed edges) between each pair of words (nodes) in a sentence. It finds the k-best scoring parse trees, both during training and at parse time, where the score of a tree is the sum of the scores of its factors (consisting of one or more linked edges). While large factors are desirable for capturing sophisticated linguistic constraints, they come at the cost of time complexity: for the projective case, adaptations of Eisner’s algorithm (Eisner, 1996) are O(n3 ) for 1-edge factors (McDonald et al., 2005) or sibling 2-edge factors (McDonald and Pereira, 2006), and O(n4 ) for general 2-edge factors (Carreras, 2007) or 3-edge factors (Koo and Collins, 2010). 3.3 Constituency-Based Parsing Beyond the two main approaches to dependency parsing, there is also the approach of constituencybased parsing followed by a conversion step to dependency structure. We use the three-step parsing architecture previously tested for French by Candito et al. (2010a): (i) A constituency parse tree is output by the BerkeleyParser, which has been trained to learn a"
D11-1113,W05-1505,0,0.566696,"Missing"
D11-1113,P09-1109,0,0.0154385,"the problem in isolation. Resnik (1999) uses semantic similarity to resolve nounphrase coordination of the form (n1 , cc, n2 , n3 ), where the coordinating conjunction cc coordinates either the heads n1 and n2 or the heads n1 and n3 . The same criticism as the one made by Atterer and Sch¨utze (2007) for pp-attachment might be applied to this approach to coordination resolution. In another formulation, the input consists of a raw sentence, and coordination structure is then detected and disambiguated using discriminative learning models (Shimbo and Hara, 2007) or coordination-specific parsers (Hara et al., 2009). Finally, other work has focused on introducing specialized features for coordination into existing syntactic parsing models (Hogan, 2007). Our approach is novel with respect to previous work by directly modeling the correction of coordination errors made by general-purpose dependency parsers. ouvrit Elle 3.1 Transition-Based Parsing porte avec la cl´e la Figure 1: An unlabeled dependency tree for: Elle ouvrit la porte avec la cl´e. (She opened the door with the key). 3 Dependency Parsing Dependency syntax involves the representation of syntactic information for a sentence in the form a direc"
D11-1113,P07-1086,0,0.020375,"ordinating conjunction cc coordinates either the heads n1 and n2 or the heads n1 and n3 . The same criticism as the one made by Atterer and Sch¨utze (2007) for pp-attachment might be applied to this approach to coordination resolution. In another formulation, the input consists of a raw sentence, and coordination structure is then detected and disambiguated using discriminative learning models (Shimbo and Hara, 2007) or coordination-specific parsers (Hara et al., 2009). Finally, other work has focused on introducing specialized features for coordination into existing syntactic parsing models (Hogan, 2007). Our approach is novel with respect to previous work by directly modeling the correction of coordination errors made by general-purpose dependency parsers. ouvrit Elle 3.1 Transition-Based Parsing porte avec la cl´e la Figure 1: An unlabeled dependency tree for: Elle ouvrit la porte avec la cl´e. (She opened the door with the key). 3 Dependency Parsing Dependency syntax involves the representation of syntactic information for a sentence in the form a directed graph, whose edges encode word-to-word relationships. An edge from a governor to a dependent indicates, roughly, that the presence of t"
D11-1113,W06-2930,0,0.0134884,"etc. In a greedy version of this process, the action to apply at each step is deterministically chosen to be the best-scoring action according to a classifier, which is trained on a dependency treebank converted into sequences of actions. The strengths of this framework are O(n) time complexity and a lack of restrictions on the locality of features. A major drawback is its greedy behavior: it can potentially make difficult attachment decisions early in the processing of a sentence, without being able to reconsider them when more information becomes available. Beamed versions of the algorithm (Johansson and Nugues, 2006) partially address this problem, but still do not provide a global optimization for selecting the output parse tree. 3.2 Graph-Based Parsing In graph-based dependency parsing, whose seminal work is that of McDonald et al. (2005), the parsing process selects the globally optimal parse tree from a graph containing attachments (directed edges) between each pair of words (nodes) in a sentence. It finds the k-best scoring parse trees, both during training and at parse time, where the score of a tree is the sum of the scores of its factors (consisting of one or more linked edges). While large factor"
D11-1113,P10-1001,0,0.0195711,"s (nodes) in a sentence. It finds the k-best scoring parse trees, both during training and at parse time, where the score of a tree is the sum of the scores of its factors (consisting of one or more linked edges). While large factors are desirable for capturing sophisticated linguistic constraints, they come at the cost of time complexity: for the projective case, adaptations of Eisner’s algorithm (Eisner, 1996) are O(n3 ) for 1-edge factors (McDonald et al., 2005) or sibling 2-edge factors (McDonald and Pereira, 2006), and O(n4 ) for general 2-edge factors (Carreras, 2007) or 3-edge factors (Koo and Collins, 2010). 3.3 Constituency-Based Parsing Beyond the two main approaches to dependency parsing, there is also the approach of constituencybased parsing followed by a conversion step to dependency structure. We use the three-step parsing architecture previously tested for French by Candito et al. (2010a): (i) A constituency parse tree is output by the BerkeleyParser, which has been trained to learn a probabilistic context-free grammar with latent annotations (Petrov et al., 2006) that has parsing time complexity O(n3 ) (Matsuzaki et al., 2005); (ii) A functional role labeler using a Maximum Entropy mode"
D11-1113,P05-1010,0,0.0221399,"or general 2-edge factors (Carreras, 2007) or 3-edge factors (Koo and Collins, 2010). 3.3 Constituency-Based Parsing Beyond the two main approaches to dependency parsing, there is also the approach of constituencybased parsing followed by a conversion step to dependency structure. We use the three-step parsing architecture previously tested for French by Candito et al. (2010a): (i) A constituency parse tree is output by the BerkeleyParser, which has been trained to learn a probabilistic context-free grammar with latent annotations (Petrov et al., 2006) that has parsing time complexity O(n3 ) (Matsuzaki et al., 2005); (ii) A functional role labeler using a Maximum Entropy model adds functional annotations to links between a verb and its dependents; (iii) Constituency trees are automatically converted into projective dependency trees, with remaining unlabeled dependencies assigned labels using a rule-based approach. 3.4 Baseline Parsers In this paper, we use the following baseline parsers: MaltParser (Nivre et al., 2007) for transition-based parsing; MSTParser (McDonald et al., 2005) (with sibling 2-edge factors) and BohnetParser (Bohnet, 2010) (with general 2-edge factors) for graph-based parsing; and Ber"
D11-1113,E06-1011,0,0.149853,"techniques similar to parse correction. Attardi and Dell’Orletta (2009) investigate reverse revision: a left-to-right transition-based model is first used to parse a sentence, then a right-to-left transition-based model is run with additional features taken from the left-toright model’s predicted parse. This approach leads to improved parsing results on a number of languages. While their approach is similar to parse correction in that it uses a predicted parse to inform a subsequent processing step, this information is used to improve a second parser rather than a model for correcting errors. McDonald and Pereira (2006) consider a method for recovering non-projective attachments from a graph representation of a sentence, in which an optimal projective parse tree has been identified. The parse tree’s edges are allowed to be rearranged in ways that introduce non-projectivity in order to increase its overall score. This rearrangement approach resembles parse correction because it is a second step that can revise attachments made in the first step, but it differs in a number of ways: it is dependent on a graph-based parsing approach, it does not model errors made by the parser, and it can only output non-project"
D11-1113,P05-1012,0,0.558987,"ions. The strengths of this framework are O(n) time complexity and a lack of restrictions on the locality of features. A major drawback is its greedy behavior: it can potentially make difficult attachment decisions early in the processing of a sentence, without being able to reconsider them when more information becomes available. Beamed versions of the algorithm (Johansson and Nugues, 2006) partially address this problem, but still do not provide a global optimization for selecting the output parse tree. 3.2 Graph-Based Parsing In graph-based dependency parsing, whose seminal work is that of McDonald et al. (2005), the parsing process selects the globally optimal parse tree from a graph containing attachments (directed edges) between each pair of words (nodes) in a sentence. It finds the k-best scoring parse trees, both during training and at parse time, where the score of a tree is the sum of the scores of its factors (consisting of one or more linked edges). While large factors are desirable for capturing sophisticated linguistic constraints, they come at the cost of time complexity: for the projective case, adaptations of Eisner’s algorithm (Eisner, 1996) are O(n3 ) for 1-edge factors (McDonald et a"
D11-1113,W03-3017,0,0.0329996,"ion-based parsing and graphbased parsing. Additionally, an alternative method for obtaining the dependency parse for a sentence is to parse the sentence with a constituency-based parser and then use an automatic process to convert the output into dependency structure. 1 Edges are generally labeled with the surface grammatical function that the dependent bears with respect to its governor. In this paper we focus on unlabeled dependency parsing, setting aside labeling as a separate task. 1224 In transition-based dependency parsing, whose seminal works are that of Yamada and Matsumoto (2003) and Nivre (2003), the parsing process applies a sequence of incremental actions, which typically manipulate a buffer position in the sentence and a stack for built sub-structures. Actions are of the type “read word from buffer”, “build a dependency from node on top of the stack to node that begins the buffer”, etc. In a greedy version of this process, the action to apply at each step is deterministically chosen to be the best-scoring action according to a classifier, which is trained on a dependency treebank converted into sequences of actions. The strengths of this framework are O(n) time complexity and a la"
D11-1113,H05-1035,0,0.0126375,"nking: it can be 1223 used with parsers that do not output n-best ranked parses, it can be easily restricted to specific attachment types, and its output space of parse trees is not limited to those appearing in an n-best list. However, parse reranking has the advantage of selecting the globally optimal parse for a sentence from an nbest list, while parse correction makes only locally optimal revisions in the predicted parse for a sentence. 2.1 Difficult Attachment Types Research on pp-attachment traditionally formulates the problem in isolation, as in the work of Pantel and Lin (2000) and of Olteanu and Moldovan (2005). Examples consist of tuples of the form (v, n1 , p, n2 ), where either v or n1 is the true governor of the pp comprising p and n2 , and the task is to choose between v and n1 . Recently, Atterer and Sch¨utze (2007) have criticized this formulation as unrealistic because it uses an oracle to select candidate governors, and they find that successful approaches for the isolated problem perform no better than stateof-the-art parsers on pp-attachment when evaluated on full sentences. With parse correction, candidate governors are identified automatically with no (v, n1 , p, n2 ) restriction, and f"
D11-1113,P00-1014,0,0.0614447,"f advantages compared to reranking: it can be 1223 used with parsers that do not output n-best ranked parses, it can be easily restricted to specific attachment types, and its output space of parse trees is not limited to those appearing in an n-best list. However, parse reranking has the advantage of selecting the globally optimal parse for a sentence from an nbest list, while parse correction makes only locally optimal revisions in the predicted parse for a sentence. 2.1 Difficult Attachment Types Research on pp-attachment traditionally formulates the problem in isolation, as in the work of Pantel and Lin (2000) and of Olteanu and Moldovan (2005). Examples consist of tuples of the form (v, n1 , p, n2 ), where either v or n1 is the true governor of the pp comprising p and n2 , and the task is to choose between v and n1 . Recently, Atterer and Sch¨utze (2007) have criticized this formulation as unrealistic because it uses an oracle to select candidate governors, and they find that successful approaches for the isolated problem perform no better than stateof-the-art parsers on pp-attachment when evaluated on full sentences. With parse correction, candidate governors are identified automatically with no"
D11-1113,P06-1055,0,0.0313993,"ling 2-edge factors (McDonald and Pereira, 2006), and O(n4 ) for general 2-edge factors (Carreras, 2007) or 3-edge factors (Koo and Collins, 2010). 3.3 Constituency-Based Parsing Beyond the two main approaches to dependency parsing, there is also the approach of constituencybased parsing followed by a conversion step to dependency structure. We use the three-step parsing architecture previously tested for French by Candito et al. (2010a): (i) A constituency parse tree is output by the BerkeleyParser, which has been trained to learn a probabilistic context-free grammar with latent annotations (Petrov et al., 2006) that has parsing time complexity O(n3 ) (Matsuzaki et al., 2005); (ii) A functional role labeler using a Maximum Entropy model adds functional annotations to links between a verb and its dependents; (iii) Constituency trees are automatically converted into projective dependency trees, with remaining unlabeled dependencies assigned labels using a rule-based approach. 3.4 Baseline Parsers In this paper, we use the following baseline parsers: MaltParser (Nivre et al., 2007) for transition-based parsing; MSTParser (McDonald et al., 2005) (with sibling 2-edge factors) and BohnetParser (Bohnet, 201"
D11-1113,H94-1048,0,0.509479,"Missing"
D11-1113,sagot-2010-lefff,0,0.0359643,"Missing"
D11-1113,D07-1064,0,0.0257684,"rch on coordination resolution has also often formulated the problem in isolation. Resnik (1999) uses semantic similarity to resolve nounphrase coordination of the form (n1 , cc, n2 , n3 ), where the coordinating conjunction cc coordinates either the heads n1 and n2 or the heads n1 and n3 . The same criticism as the one made by Atterer and Sch¨utze (2007) for pp-attachment might be applied to this approach to coordination resolution. In another formulation, the input consists of a raw sentence, and coordination structure is then detected and disambiguated using discriminative learning models (Shimbo and Hara, 2007) or coordination-specific parsers (Hara et al., 2009). Finally, other work has focused on introducing specialized features for coordination into existing syntactic parsing models (Hogan, 2007). Our approach is novel with respect to previous work by directly modeling the correction of coordination errors made by general-purpose dependency parsers. ouvrit Elle 3.1 Transition-Based Parsing porte avec la cl´e la Figure 1: An unlabeled dependency tree for: Elle ouvrit la porte avec la cl´e. (She opened the door with the key). 3 Dependency Parsing Dependency syntax involves the representation of syn"
D11-1113,W03-3023,0,0.0120504,"nto two main categories: transition-based parsing and graphbased parsing. Additionally, an alternative method for obtaining the dependency parse for a sentence is to parse the sentence with a constituency-based parser and then use an automatic process to convert the output into dependency structure. 1 Edges are generally labeled with the surface grammatical function that the dependent bears with respect to its governor. In this paper we focus on unlabeled dependency parsing, setting aside labeling as a separate task. 1224 In transition-based dependency parsing, whose seminal works are that of Yamada and Matsumoto (2003) and Nivre (2003), the parsing process applies a sequence of incremental actions, which typically manipulate a buffer position in the sentence and a stack for built sub-structures. Actions are of the type “read word from buffer”, “build a dependency from node on top of the stack to node that begins the buffer”, etc. In a greedy version of this process, the action to apply at each step is deterministically chosen to be the best-scoring action according to a classifier, which is trained on a dependency treebank converted into sequences of actions. The strengths of this framework are O(n) time co"
F12-2024,abeille-barrier-2004-enriching,0,0.275854,"Missing"
F12-2024,J92-4003,0,0.0401775,"Missing"
F12-2024,W09-3821,1,0.913196,"Missing"
F12-2024,candito-etal-2010-statistical,1,0.877951,"Missing"
F12-2024,W11-2905,1,0.840916,"Missing"
F12-2024,C10-2013,1,0.920944,"Missing"
F12-2024,A00-2018,0,0.350317,"Missing"
F12-2024,Y09-1013,0,0.157743,"Missing"
F12-2024,N10-1060,0,0.0568431,"Missing"
F12-2024,W07-2204,1,0.930035,"Missing"
F12-2024,P08-1068,0,0.0834377,"Missing"
F12-2024,J93-2004,0,0.0403036,"Missing"
F12-2024,P06-1043,0,0.10224,"Missing"
F12-2024,N03-4009,0,0.070537,"Missing"
F12-2024,N10-1003,0,0.0223409,"Missing"
F12-2024,N07-1051,0,0.0583566,"Missing"
F12-2024,W10-2606,0,0.0311906,"Missing"
F12-2024,sagot-2010-lefff,0,0.149665,"Missing"
F12-2024,N03-1031,0,0.157988,"Missing"
F12-2024,villemonte-de-la-clergerie-etal-2008-passage,0,0.166869,"Missing"
F14-2031,candito-etal-2010-statistical,1,0.895459,"Missing"
F14-2031,candito-etal-2014-deep,1,0.70798,"Missing"
F14-2031,F12-2024,1,0.886133,"Missing"
F14-2031,J05-1004,0,0.189665,"Missing"
L16-1375,abeille-barrier-2004-enriching,0,0.0129854,"h a complex wh-marker est-ce que: case (4) yes/no questions of the form est-ce que + SENT: Est-ce que Paul a déjà mangé ? lit. ’Is-it that Paul has already eaten?’ (Has Paul already eaten?) case (5) form qui/qu’ est-ce que/qui + SENT-with-gap: Qu’est-ce que Paul a mangé? lit. ’What is-it that Paul has eaten?’ (What has Paul eaten?) case (6) form qu’ est-ce que + NP: Qu’est-ce que le platine? lit. ’What is-it that platine’ (What is platine) 3. Questions in French corpora We now focus on questions in the French typical corpora usable for training statistical parsers. The French treebank (F TB) (Abeillé and Barrier, 2004)) is the most used treebank for that purpose, being both the first and the biggest. Other treebanks were developped later, in particular some out-of-domain treebanks using the same annotation scheme : the S EQUOIA treebank (Candito and Seddah, 2012), a well-edited out-of-domain small treebank, and the F RENCH S OCIAL M EDIA BANK, F SMB (Seddah et al., 2012), which originates in web forums and social media content. As already noted for English by Judge et al. (2006), questions are generally under-represented in treebanks. Indeed, this observation is confirmed the figures presented in Table 2366"
L16-1375,W09-3821,1,0.856077,"dard French counterparts. We left the web forum questions unedited so that the difficulties of handling noisy questions can be correctly assessed. SOURCE TREC 08-11 Faq GVT/NGOs CLEF03 sub-total Web In order to obtain evaluation treebanks compatible with parsers trained on the F TB, we have used as basis the F TB annotation scheme and followed as much as possible the corresponding annotation guidelines for morphology, phrase structure and functional annotation (Abeillé et al., 2003). More precisely, we started from a slight modification of this annotation scheme, referred to as the F TB - UC (Candito and Crabbé, 2009) and added specific guidelines for handling idiosyncrasies tied to question-phrase specificities.5 As far as grammatical function tags are concerned, we used an additional function label DIS for dislocated phrases. Such phrases appear either at the beginning or the end of a clause, and are coreferent with a (redundant) clitic appearing on the verb. It can occur in declarative sentences (e.g. Paul les a mangées, les fraises lit. ’Paul CL-ACC-pl has eaten, the strawberries’ (Paul has eaten the strawberries), but in the F QB it appears massively in questions of the form Qu’est-ce que NP whose par"
L16-1375,F12-2024,1,0.935337,"n size between our corpus parts (see Table 2). Let us note that the TREC part of the French Question Bank (F QB) is aligned with the first 1893 sentences of the QB. Joining those resources could prove useful for the evaluation of some syntax based machine translation system if not for the bootstrapping of such systems. 3 4. Annotation Scheme http://www-rali.iro.umontreal.ca/rali/?q=node/9 Social Welfare (CAF), IRS (Trésors public), employment agency (Pôle Emploi), National Statistics Agency (INSEE), UNESCO 4.1. Annotation Methodology and Evaluation We followed the same annotation protocol as (Candito and Seddah, 2012). Namely, two annotators working on the output of two parsers (the Berkeley parser (Petrov et al., 2006) and the first-phase parser of Charniak (2000)) fed with gold input (generated from a previous annotation phase). Resulting corrected parses were then adjudicated. To assess the quality of annotation, we calculated the interannotator agreement using the Parseval F-measure metric between two functionally annotated set of parses (Table 3). 4 5 Should this paper be accepted, we will provide more details on the annotation scheme. 2367 F QB We note that our agreement scores are higher than those"
L16-1375,candito-etal-2010-statistical,1,0.859166,"1 (cf. case 6 listed in section 2.). In order to prepare a further deep syntax annotation layer, we also annotated all long distance dependencies using functional paths, following, among others, (Schluter and van Genabith, 2008; Chrupała, 2008). The motivation lies in the need to closely follow the F TB annotation scheme, therefore avoiding empty elements and traces. Other modifications such as assigning function labels to pre-terminal and participle phrases were applied so that a dependency conversion will be less sensitive to structural ambiguities than the original conversion developed by Candito et al. (2010a). SENT VN NP-ATS # OF SENTENCES 1893 196 200 2289 285 PROWH V Qu’ est PONCT Ssub-DIS -ce ? NP CLS-SUJ CS que DET le NC platine Figure 1: Dislocated example for lit. What is-it that platine? (What is platine?) Table 2: Source of F QB sentences. The difficulties gathering question data in French entailed a relatively unbalanced corpus, compared for example to the Question Bank (QB) (Judge et al., 2006), as shown by the divergence in size between our corpus parts (see Table 2). Let us note that the TREC part of the French Question Bank (F QB) is aligned with the first 1893 sentences of the QB."
L16-1375,C10-2013,1,0.930963,"1 (cf. case 6 listed in section 2.). In order to prepare a further deep syntax annotation layer, we also annotated all long distance dependencies using functional paths, following, among others, (Schluter and van Genabith, 2008; Chrupała, 2008). The motivation lies in the need to closely follow the F TB annotation scheme, therefore avoiding empty elements and traces. Other modifications such as assigning function labels to pre-terminal and participle phrases were applied so that a dependency conversion will be less sensitive to structural ambiguities than the original conversion developed by Candito et al. (2010a). SENT VN NP-ATS # OF SENTENCES 1893 196 200 2289 285 PROWH V Qu’ est PONCT Ssub-DIS -ce ? NP CLS-SUJ CS que DET le NC platine Figure 1: Dislocated example for lit. What is-it that platine? (What is platine?) Table 2: Source of F QB sentences. The difficulties gathering question data in French entailed a relatively unbalanced corpus, compared for example to the Question Bank (QB) (Judge et al., 2006), as shown by the divergence in size between our corpus parts (see Table 2). Let us note that the TREC part of the French Question Bank (F QB) is aligned with the first 1893 sentences of the QB."
L16-1375,A00-2018,0,0.573725,"QB. Joining those resources could prove useful for the evaluation of some syntax based machine translation system if not for the bootstrapping of such systems. 3 4. Annotation Scheme http://www-rali.iro.umontreal.ca/rali/?q=node/9 Social Welfare (CAF), IRS (Trésors public), employment agency (Pôle Emploi), National Statistics Agency (INSEE), UNESCO 4.1. Annotation Methodology and Evaluation We followed the same annotation protocol as (Candito and Seddah, 2012). Namely, two annotators working on the output of two parsers (the Berkeley parser (Petrov et al., 2006) and the first-phase parser of Charniak (2000)) fed with gold input (generated from a previous annotation phase). Resulting corrected parses were then adjudicated. To assess the quality of annotation, we calculated the interannotator agreement using the Parseval F-measure metric between two functionally annotated set of parses (Table 3). 4 5 Should this paper be accepted, we will provide more details on the annotation scheme. 2367 F QB We note that our agreement scores are higher than those reported in other out-of-domain initiatives for French (Candito and Seddah, 2012; Seddah et al., 2012). This can be due to the smaller average sentenc"
L16-1375,P15-1118,0,0.0152027,"nk outside English, bringing a new genre to the existing French data set. Because statistical parsing models are notoriously biased toward the domain of their training model, the availability of a treebank made of questions for French will help building more robust parsers, useful for example in syntaxaugmented question answering system. However, we showed in this work how this data set could be used to close the question genre out-of-domain gap. Once more unlabeled question data are made available for French, complementary techniques, such as uptraining (Petrov et al., 2010) or paraphrasing (Choe and McClosky, 2015), will help to further improve question parsing for French. A large part of the F QB being aligned with the QB (Judge et al., 2006), this treebank will pave the way for crosslinguistics work. The F QB is freely available at http: //alpage.inria.fr/FrenchQuestionBank. Acknowledgment We thanks our anonymous reviewers for their comments. We are grateful to Benoit Crabbé for his remarks on a earlier version of this work and to Corentin Ribeyre for his generous help. This work was funded by the Program ""Investissements d’avenir"" managed by the Agence Nationale de la Recherche ANR-10-LABX-0083 (Labe"
L16-1375,Y09-1013,0,0.0415277,"Missing"
L16-1375,P06-1063,0,0.0209364,"Missing"
L16-1375,nivre-etal-2006-maltparser,0,0.104961,"Missing"
L16-1375,P06-1055,0,0.0267484,"QB) is aligned with the first 1893 sentences of the QB. Joining those resources could prove useful for the evaluation of some syntax based machine translation system if not for the bootstrapping of such systems. 3 4. Annotation Scheme http://www-rali.iro.umontreal.ca/rali/?q=node/9 Social Welfare (CAF), IRS (Trésors public), employment agency (Pôle Emploi), National Statistics Agency (INSEE), UNESCO 4.1. Annotation Methodology and Evaluation We followed the same annotation protocol as (Candito and Seddah, 2012). Namely, two annotators working on the output of two parsers (the Berkeley parser (Petrov et al., 2006) and the first-phase parser of Charniak (2000)) fed with gold input (generated from a previous annotation phase). Resulting corrected parses were then adjudicated. To assess the quality of annotation, we calculated the interannotator agreement using the Parseval F-measure metric between two functionally annotated set of parses (Table 3). 4 5 Should this paper be accepted, we will provide more details on the annotation scheme. 2367 F QB We note that our agreement scores are higher than those reported in other out-of-domain initiatives for French (Candito and Seddah, 2012; Seddah et al., 2012)."
L16-1375,D10-1069,0,0.0429632,"ion We introduced the first QuestionBank outside English, bringing a new genre to the existing French data set. Because statistical parsing models are notoriously biased toward the domain of their training model, the availability of a treebank made of questions for French will help building more robust parsers, useful for example in syntaxaugmented question answering system. However, we showed in this work how this data set could be used to close the question genre out-of-domain gap. Once more unlabeled question data are made available for French, complementary techniques, such as uptraining (Petrov et al., 2010) or paraphrasing (Choe and McClosky, 2015), will help to further improve question parsing for French. A large part of the F QB being aligned with the QB (Judge et al., 2006), this treebank will pave the way for crosslinguistics work. The F QB is freely available at http: //alpage.inria.fr/FrenchQuestionBank. Acknowledgment We thanks our anonymous reviewers for their comments. We are grateful to Benoit Crabbé for his remarks on a earlier version of this work and to Corentin Ribeyre for his generous help. This work was funded by the Program ""Investissements d’avenir"" managed by the Agence Nation"
L16-1375,schluter-van-genabith-2008-treebank,0,0.0605489,"Missing"
L16-1375,C12-1149,1,0.88215,"(Petrov et al., 2006) and the first-phase parser of Charniak (2000)) fed with gold input (generated from a previous annotation phase). Resulting corrected parses were then adjudicated. To assess the quality of annotation, we calculated the interannotator agreement using the Parseval F-measure metric between two functionally annotated set of parses (Table 3). 4 5 Should this paper be accepted, we will provide more details on the annotation scheme. 2367 F QB We note that our agreement scores are higher than those reported in other out-of-domain initiatives for French (Candito and Seddah, 2012; Seddah et al., 2012). This can be due to the smaller average sentence length of the F QB, and to the fact that the annotators were already trained for the task. 6 A vs B 97.54 A vs Gold 95.72 POS B vs Gold 97.21 none F TB gold none gold w/o funct with funct Bracketing Fmeasure (all sent.) 83.85 86.09 81.67 83.50 65.21 69.90 74.4 76.06 w/o funct with funct Bracketing Fmeasure (≤20 sent) 84.16 86.40 88.07 90.48 65.43 69.87 78.84 80.91 Pos accuracy (all sent.) 92.05 98.98 97.29 99.93 Table 3: Inter-annotator agreement Table 4: Baseline phrase-based results (B KY). 5. Parsability of the FQB As we said earlier, the mo"
L16-1601,abeille-barrier-2004-enriching,0,0.372156,"Missing"
L16-1601,P98-1013,0,0.899228,"nt status, there are 98 frames, 662 frame-evoking words, 872 senses, and about 13000 annotated frames, with their semantic roles assigned to portions of text. The French FrameNet is freely available at alpage.inria.fr/asfalda. Keywords: FrameNet, French, semantic roles, semantic frames, semantically-annotated corpus 1. Introduction The ASFALDA project1 aims to build semantic resources and a corresponding semantic analyzer for French, to capture generalizations both over predicates and over the semantic arguments of predicates. We chose to build on the work resulting from the FrameNet project (Baker et al., 1998), which provides a structured set of prototypical situations, called frames, along with a semantic characterization of the participants of these situations (called frame elements, but we’ll use roles for short). The corresponding English lexicon associates frames with the words that can evoke them (called frame-evoking elements, FEEs for short). While other English semantic resources, such as PropBank (Palmer et al., 2005) or VerbNet (Schuler, 2005), also provide semantic classes and/or semantic roles for predicate arguments, we chose FrameNet mainly because of its more semantic orientation, w"
L16-1601,burchardt-etal-2006-salto,0,0.512718,"Missing"
L16-1601,burchardt-etal-2006-salsa,0,0.147107,"Missing"
L16-1601,F12-2024,1,0.923551,"4), this paper concentrates on the subsequent corpus annotation phase, which focused on four notional domains (commercial transactions, cognitive stances, causality and verbal communication). Given full coverage is not reachable for a relatively “new” FrameNet project, we advocate that focusing on specific notional domains allowed us to obtain full lexical coverage for the frames of these domains, while partially reflecting word sense ambiguities. Furthermore, as frames and roles were annotated on two French Treebanks (the French Treebank (Abeill´e and Barrier, 2004) and the Sequoia Treebank (Candito and Seddah, 2012), we were able to extract a syntactico-semantic lexicon from the annotated frames. In the resource’s current status, there are 98 frames, 662 frame-evoking words, 872 senses, and about 13000 annotated frames, with their semantic roles assigned to portions of text. The French FrameNet is freely available at alpage.inria.fr/asfalda. Keywords: FrameNet, French, semantic roles, semantic frames, semantically-annotated corpus 1. Introduction The ASFALDA project1 aims to build semantic resources and a corresponding semantic analyzer for French, to capture generalizations both over predicates and over"
L16-1601,candito-etal-2014-developing,1,0.556471,"Missing"
L16-1601,N10-1138,0,0.0355403,"enced that using the frame-by-frame strategy, i.e. working in isolation on a frame, can result in missing some similarities with other frames, and thus artificially increasing polysemy: choosing the right frame for a given lemma’s occurrence can become quite difficult due to blurred frontiers between frames. Finally, a crucial characteristic of the different strategies concerns the use of the resulting annotations as training data for semantic parsers: even though they are much more numerous, Berkeley FrameNet’s lexicographic annotations have proved much less useful than their full-text ones (Das et al., 2010), which have the crucial trait of preserving the natural sense and role-realization probabilistic distributions. 3.2. In the current paper, we describe the subsequent corpus annotation phase. We used as target corpus two syntacticallyannotated corpora (see below section 3.3.), and basically aimed to annotate all occurrences of lemmas that potentially evoke a frame of one of 4 notional domains (cf. section 4.1.). Once disambiguated, the FEE occurrence is either associated with one relevant frame, or with a special Other sense frame, to indicate the meaning is outside of the targetted notional d"
L16-1601,mouton-etal-2010-framenet,0,0.524386,"Missing"
L16-1601,J05-1004,0,0.280343,"for French, to capture generalizations both over predicates and over the semantic arguments of predicates. We chose to build on the work resulting from the FrameNet project (Baker et al., 1998), which provides a structured set of prototypical situations, called frames, along with a semantic characterization of the participants of these situations (called frame elements, but we’ll use roles for short). The corresponding English lexicon associates frames with the words that can evoke them (called frame-evoking elements, FEEs for short). While other English semantic resources, such as PropBank (Palmer et al., 2005) or VerbNet (Schuler, 2005), also provide semantic classes and/or semantic roles for predicate arguments, we chose FrameNet mainly because of its more semantic orientation, which is crucial for portability to other languages. FrameNet offers generalization over not only syntactic variation (e.g. diathesis alternation) but also lexical variation (like VerbNet but unlike PropBank), and groups together lexical units of various categories, on the basis of criteria that are not primarily syntactic (unlike VerbNet). The resources built within ASFALDA consist of a set of frames, a French lexicon in w"
L16-1601,W13-4917,1,0.867681,"Missing"
L16-1601,C98-1013,0,\N,Missing
L16-1603,abeille-barrier-2004-enriching,0,0.037665,"Missing"
L16-1603,P98-1013,0,0.446604,"of (Wolff and Song, 2003), namely cause, enable, prevent. Other projects aim at annotating discourse relations between clauses, among which causal relations, marked or not by discourse connectives such as because or then (Prasad et al., 2008; Carlson et al., 2007). In contrast, larger lexical projects have on the one hand covered all sorts of POS expressing causality (including nouns, verbs, adverbs, conjunctions, prepositions, adjectives) and on the other hand, distinguished a much larger set of causal relationships involving events as well as facts: for instance in FrameNet (henceforth FN) (Baker et al., 1998), some frames are concerned with argumentation, where typical causal expressions introduce evidence for a claim, or reasons for an agent’s behaviour. This is consistent with more psycholinguistically oriented studies, which clearly distinguish the expression of factual and epistemic causality (Spooren et al., 2010). Examples in (1), taken from FN, show relationships between “situations”, events or facts, with names specific to each frame, and which parallel cause-effect relationships (in bold, the lexical unit triggering the frame annotation, called a frame-evoking element in FN).1 (1) a. [Wha"
L16-1603,P08-2045,0,0.416835,"rbs (Riaz and Girju, 2013; Do et al., 2011), or combinations of nouns and verbs (Riaz and Girju, 2014). Other researchers try instead to gather general knowledge about typical causal links, mostly expressed by verbs (Hashimoto et al., 2009; Szpektor and Dagan, 2008; Chklovski and Pantel, 2004), essentially for textual entailment or knowledge mining. They all target realized causalities: an event occurred and caused another event, or is likely to be the cause of another event. This is also the focus of annotation efforts, integrating time and causal information, as in (Mirza and Tonelli, 2014; Bethard and Martin, 2008). They make distinctions between groups of verbs expressing such causalities, based on the work of (Wolff and Song, 2003), namely cause, enable, prevent. Other projects aim at annotating discourse relations between clauses, among which causal relations, marked or not by discourse connectives such as because or then (Prasad et al., 2008; Carlson et al., 2007). In contrast, larger lexical projects have on the one hand covered all sorts of POS expressing causality (including nouns, verbs, adverbs, conjunctions, prepositions, adjectives) and on the other hand, distinguished a much larger set of ca"
L16-1603,burchardt-etal-2006-salto,0,0.083213,"Missing"
L16-1603,F12-2024,1,0.906389,"realizations of roles. FN later added full-text annotations to have a more representative coverage. We followed a similar path for the project, but aiming for a complete lexical coverage for causality: we first defined a lexicon of causal lexical units for French beforehand. The subsequent corpus annotation phase, performed within the ASFALDA French FrameNet project is described at length in (Djemaa et al., 2016) and concerns three other notional domains, on top of causality. We used two syntactically annotated corpus, the French Treebank (Abeill´e and Barrier, 2004) and the Sequoia treebank (Candito and Seddah, 2012), in which we isolated the occurrences of the lemmas of our causal LUs and had them disambiguated and 3809 Event Eventive_affecting Causation Reason Response Preventing Cause to start + Launch process (new) Cause enunciation Explaining the facts Contingency + Objective influence Transitive_action (new) Attributing cause Make possible to do Evidence Figure 1: Causal frame hierarchy; Grey nodes indicate frame changes from Framenet 1.5, either new, merged or redefined frames. Dotted relations are additions. Transitive action is an unlexicalised parent to more specific predicates. NB: Evidence is"
L16-1603,candito-etal-2014-developing,1,0.895062,"Missing"
L16-1603,W04-3205,0,0.0269633,"s also an important notion in discourse analysis, being one of the most important categories of discourse relations (Hovy and Maier, 1992; Mak and Sanders, 2013). Numerous studies in NLP have focused on extraction of cause-effect relationships between nouns denoting events (Kozareva, 2012), or relations between events denoted by verbs (Riaz and Girju, 2013; Do et al., 2011), or combinations of nouns and verbs (Riaz and Girju, 2014). Other researchers try instead to gather general knowledge about typical causal links, mostly expressed by verbs (Hashimoto et al., 2009; Szpektor and Dagan, 2008; Chklovski and Pantel, 2004), essentially for textual entailment or knowledge mining. They all target realized causalities: an event occurred and caused another event, or is likely to be the cause of another event. This is also the focus of annotation efforts, integrating time and causal information, as in (Mirza and Tonelli, 2014; Bethard and Martin, 2008). They make distinctions between groups of verbs expressing such causalities, based on the work of (Wolff and Song, 2003), namely cause, enable, prevent. Other projects aim at annotating discourse relations between clauses, among which causal relations, marked or not b"
L16-1603,L16-1601,1,0.887972,"usal lexical items with their corresponding semantic frames. The aim of our project is to have both the largest possible coverage of causal phenomena in French, across all parts of speech, and have it linked to a general semantic framework such as FN, to benefit in particular from the relations between other semantic frames, e.g., temporal ones or intentional ones, and the underlying upper lexical ontology that enables some forms of reasoning. This is part of the larger ASFALDA French FrameNet project, which focuses on a few different notional domains which are interesting in their own right (Djemaa et al., 2016), including cognitive positions and communication frames. In the process of building the French lexicon and preparing the annotation of the corpus, we had to remodel some of the frames proposed in FN based on English data, with hopefully more precise frame definitions to facilitate human annotation. This includes semantic clarifications of frames and frame elements, redundancy elimination, and added coverage. The result is arguably a significant improvement of the treatment of causality in FN itself. Keywords: semantic role labelling, causality, FrameNet 1. Introduction A lot of information in"
L16-1603,D11-1027,0,0.203789,"went as far as saying causality defines events, as “Events are identical if and only if they have exactly the same causes and effects”, and causality is an active area of research in linguistics (Neeleman and van de Koot, 2012). Causality is also an important notion in discourse analysis, being one of the most important categories of discourse relations (Hovy and Maier, 1992; Mak and Sanders, 2013). Numerous studies in NLP have focused on extraction of cause-effect relationships between nouns denoting events (Kozareva, 2012), or relations between events denoted by verbs (Riaz and Girju, 2013; Do et al., 2011), or combinations of nouns and verbs (Riaz and Girju, 2014). Other researchers try instead to gather general knowledge about typical causal links, mostly expressed by verbs (Hashimoto et al., 2009; Szpektor and Dagan, 2008; Chklovski and Pantel, 2004), essentially for textual entailment or knowledge mining. They all target realized causalities: an event occurred and caused another event, or is likely to be the cause of another event. This is also the focus of annotation efforts, integrating time and causal information, as in (Mirza and Tonelli, 2014; Bethard and Martin, 2008). They make distin"
L16-1603,W15-1622,0,0.220921,"ents or facts, with names specific to each frame, and which parallel cause-effect relationships (in bold, the lexical unit triggering the frame annotation, called a frame-evoking element in FN).1 (1) a. [What happened to these two chaps]support proves [the rumour is not true]proposition . E VIDENCE b. I like to think the main reason [we]Agent [have stayed together since the World Cup]action is [the great spirit within this Australian team]state of affairs . R EASON c. If [such a small earthquake]cause causes [problems]effect , just imagine a big one! C AUSATION Similar distinctions appear in (Dunietz et al., 2015), who annotated four kinds of causalities: consequence, which seems similar to the FN C AUSATION frame, motivation (similar to R EASON), purpose (similar to FN’s P URPOSE), and inference (similar to E VIDENCE). Unfortunately their effort was made without an explicit relation to an existing semantic lexicon, and has only a small dataset to support the framework. To our knowledge, FN is the semantic lexicon which has the largest coverage and the most detailed analysis of the expression of causality in English, equipped with annotation procedures.2 The detailed analysis relies 1 In the remaining"
L16-1603,S12-1052,0,0.0612495,"Missing"
L16-1603,D09-1122,0,0.071349,"Missing"
L16-1603,P00-1043,0,0.0792031,"h hopefully more precise frame definitions to facilitate human annotation. This includes semantic clarifications of frames and frame elements, redundancy elimination, and added coverage. The result is arguably a significant improvement of the treatment of causality in FN itself. Keywords: semantic role labelling, causality, FrameNet 1. Introduction A lot of information in natural language is of a causal nature: relations between events, explanations, argumentations are all important to the understanding of texts, and thus useful in question-answering (Oh et al., 2013), information extraction (Khoo et al., 2000; nan Cao et al., 2014) or textual entailment (Gordon et al., 2012). Davidson, in his work on individuation of events (Davidson, 1969), went as far as saying causality defines events, as “Events are identical if and only if they have exactly the same causes and effects”, and causality is an active area of research in linguistics (Neeleman and van de Koot, 2012). Causality is also an important notion in discourse analysis, being one of the most important categories of discourse relations (Hovy and Maier, 1992; Mak and Sanders, 2013). Numerous studies in NLP have focused on extraction of cause-e"
L16-1603,W12-4107,0,0.0127929,"on et al., 2012). Davidson, in his work on individuation of events (Davidson, 1969), went as far as saying causality defines events, as “Events are identical if and only if they have exactly the same causes and effects”, and causality is an active area of research in linguistics (Neeleman and van de Koot, 2012). Causality is also an important notion in discourse analysis, being one of the most important categories of discourse relations (Hovy and Maier, 1992; Mak and Sanders, 2013). Numerous studies in NLP have focused on extraction of cause-effect relationships between nouns denoting events (Kozareva, 2012), or relations between events denoted by verbs (Riaz and Girju, 2013; Do et al., 2011), or combinations of nouns and verbs (Riaz and Girju, 2014). Other researchers try instead to gather general knowledge about typical causal links, mostly expressed by verbs (Hashimoto et al., 2009; Szpektor and Dagan, 2008; Chklovski and Pantel, 2004), essentially for textual entailment or knowledge mining. They all target realized causalities: an event occurred and caused another event, or is likely to be the cause of another event. This is also the focus of annotation efforts, integrating time and causal in"
L16-1603,C14-1198,0,0.416049,"ween events denoted by verbs (Riaz and Girju, 2013; Do et al., 2011), or combinations of nouns and verbs (Riaz and Girju, 2014). Other researchers try instead to gather general knowledge about typical causal links, mostly expressed by verbs (Hashimoto et al., 2009; Szpektor and Dagan, 2008; Chklovski and Pantel, 2004), essentially for textual entailment or knowledge mining. They all target realized causalities: an event occurred and caused another event, or is likely to be the cause of another event. This is also the focus of annotation efforts, integrating time and causal information, as in (Mirza and Tonelli, 2014; Bethard and Martin, 2008). They make distinctions between groups of verbs expressing such causalities, based on the work of (Wolff and Song, 2003), namely cause, enable, prevent. Other projects aim at annotating discourse relations between clauses, among which causal relations, marked or not by discourse connectives such as because or then (Prasad et al., 2008; Carlson et al., 2007). In contrast, larger lexical projects have on the one hand covered all sorts of POS expressing causality (including nouns, verbs, adverbs, conjunctions, prepositions, adjectives) and on the other hand, distinguis"
L16-1603,mouton-etal-2010-framenet,0,0.0495501,"Missing"
L16-1603,P13-1170,0,0.0402666,"Missing"
L16-1603,prasad-etal-2008-penn,0,0.230152,"mining. They all target realized causalities: an event occurred and caused another event, or is likely to be the cause of another event. This is also the focus of annotation efforts, integrating time and causal information, as in (Mirza and Tonelli, 2014; Bethard and Martin, 2008). They make distinctions between groups of verbs expressing such causalities, based on the work of (Wolff and Song, 2003), namely cause, enable, prevent. Other projects aim at annotating discourse relations between clauses, among which causal relations, marked or not by discourse connectives such as because or then (Prasad et al., 2008; Carlson et al., 2007). In contrast, larger lexical projects have on the one hand covered all sorts of POS expressing causality (including nouns, verbs, adverbs, conjunctions, prepositions, adjectives) and on the other hand, distinguished a much larger set of causal relationships involving events as well as facts: for instance in FrameNet (henceforth FN) (Baker et al., 1998), some frames are concerned with argumentation, where typical causal expressions introduce evidence for a claim, or reasons for an agent’s behaviour. This is consistent with more psycholinguistically oriented studies, whic"
L16-1603,W13-4004,0,0.0168425,"nts (Davidson, 1969), went as far as saying causality defines events, as “Events are identical if and only if they have exactly the same causes and effects”, and causality is an active area of research in linguistics (Neeleman and van de Koot, 2012). Causality is also an important notion in discourse analysis, being one of the most important categories of discourse relations (Hovy and Maier, 1992; Mak and Sanders, 2013). Numerous studies in NLP have focused on extraction of cause-effect relationships between nouns denoting events (Kozareva, 2012), or relations between events denoted by verbs (Riaz and Girju, 2013; Do et al., 2011), or combinations of nouns and verbs (Riaz and Girju, 2014). Other researchers try instead to gather general knowledge about typical causal links, mostly expressed by verbs (Hashimoto et al., 2009; Szpektor and Dagan, 2008; Chklovski and Pantel, 2004), essentially for textual entailment or knowledge mining. They all target realized causalities: an event occurred and caused another event, or is likely to be the cause of another event. This is also the focus of annotation efforts, integrating time and causal information, as in (Mirza and Tonelli, 2014; Bethard and Martin, 2008)"
L16-1603,W14-0707,0,0.0119504,"ents are identical if and only if they have exactly the same causes and effects”, and causality is an active area of research in linguistics (Neeleman and van de Koot, 2012). Causality is also an important notion in discourse analysis, being one of the most important categories of discourse relations (Hovy and Maier, 1992; Mak and Sanders, 2013). Numerous studies in NLP have focused on extraction of cause-effect relationships between nouns denoting events (Kozareva, 2012), or relations between events denoted by verbs (Riaz and Girju, 2013; Do et al., 2011), or combinations of nouns and verbs (Riaz and Girju, 2014). Other researchers try instead to gather general knowledge about typical causal links, mostly expressed by verbs (Hashimoto et al., 2009; Szpektor and Dagan, 2008; Chklovski and Pantel, 2004), essentially for textual entailment or knowledge mining. They all target realized causalities: an event occurred and caused another event, or is likely to be the cause of another event. This is also the focus of annotation efforts, integrating time and causal information, as in (Mirza and Tonelli, 2014; Bethard and Martin, 2008). They make distinctions between groups of verbs expressing such causalities,"
L16-1603,C08-1107,0,0.011006,"e Koot, 2012). Causality is also an important notion in discourse analysis, being one of the most important categories of discourse relations (Hovy and Maier, 1992; Mak and Sanders, 2013). Numerous studies in NLP have focused on extraction of cause-effect relationships between nouns denoting events (Kozareva, 2012), or relations between events denoted by verbs (Riaz and Girju, 2013; Do et al., 2011), or combinations of nouns and verbs (Riaz and Girju, 2014). Other researchers try instead to gather general knowledge about typical causal links, mostly expressed by verbs (Hashimoto et al., 2009; Szpektor and Dagan, 2008; Chklovski and Pantel, 2004), essentially for textual entailment or knowledge mining. They all target realized causalities: an event occurred and caused another event, or is likely to be the cause of another event. This is also the focus of annotation efforts, integrating time and causal information, as in (Mirza and Tonelli, 2014; Bethard and Martin, 2008). They make distinctions between groups of verbs expressing such causalities, based on the work of (Wolff and Song, 2003), namely cause, enable, prevent. Other projects aim at annotating discourse relations between clauses, among which caus"
L16-1603,C98-1013,0,\N,Missing
L18-1718,P13-2107,0,0.0454926,"Missing"
L18-1718,W13-4916,0,0.0300012,"Missing"
L18-1718,W09-3821,1,0.740978,"Missing"
L18-1718,F12-2024,1,0.899058,"Missing"
L18-1718,candito-etal-2010-statistical,1,0.865662,"Missing"
L18-1718,C12-1052,0,0.052222,"Missing"
L18-1718,C12-1059,0,0.0416459,"Missing"
L18-1718,J93-2004,0,0.0629965,"Missing"
L18-1718,P09-2010,0,0.0726827,"Missing"
L18-1718,L16-1375,1,0.89553,"Missing"
L18-1718,W13-4917,1,0.856651,"Missing"
L18-1718,W14-6111,1,0.908556,"Missing"
L18-1718,W13-4906,1,0.857223,"Missing"
L18-1718,E17-1034,0,0.0185843,"an iterative fashion, or as new relevant conversion needs are identified. A full manual evaluation of a converted treebank could represent an effort comparable to full re-annotation of a large part of the data. Indeed, few of the UD-conversion papers provide accuracy scores of the conversion on a manually annotated testbench. For instance, The Danish conversion of Johannsen et al. (2015), uses a small set of hand-annotated sentences that reflect specific phenomena and hard cases that is used as held-out section during the iterative development of conversion rules. The Hungarian conversion of Vincze et al. (2017) uses a hand-corrected gold standard of 1,800 sentences. When comparing the quality of the conversion with the gold standard, they consider the accuracy (87.81 UAS and 75.99 LAS) not sufficient to release the resulting treebank. We draw inspiration on their method to develop a handcorrected sample to evaluate the quality of our conversion.One of the authors of the article, an expert in dependency annotation very familiar with the UD formalism, reviewed 100 sentences from the test section and 100 sentences from the dev section manually, correcting edges and labels that were either not properly"
L18-1718,K17-3001,1,0.897083,"Missing"
P14-1070,abeille-barrier-2004-enriching,0,0.0142472,"Missing"
P14-1070,P05-1038,0,0.0163086,"Missing"
P14-1070,W13-4916,0,0.0369316,"Missing"
P14-1070,C10-1011,0,0.0303605,"ly performs MWE segmentation and POS tagging (of simple tokens and of MWEs), both tasks mutually helping each other9 . The MWE analyzer integrates, among others, features computed from the external lexicons described in section 5.1, which greatly improve POS tagging (Denis and Sagot, 2009) and MWE segmentation (Constant and Tellier, 2012). The MWE analyzer also jointly classifies its predicted MWEs as regular or irregular (the distinction being learnt on gold training set, with structured MWEs cf. section 3.2). 5.3 Parser: We used the second-order graph-based parser available in Mate-tools12 (Bohnet, 2010). We used the Anna3.3 version, in projective mode, with default feature sets and parameters proposed in the documentation, augmented or not with MWE-specific features, depending on the experiments. MWE-specific features We introduce information from the external MWE resources in different ways: Flat MWE features: MWE information can be integrated as features to be used by the dependency parser. We tested to incorporate the MWE-specific features as defined in the gold flat representation (section 3.1): the mwehead=POS feature for the MWE head token, POS being the part-of-speech of the MWE; the"
P14-1070,candito-etal-2010-statistical,1,0.597504,"Missing"
P14-1070,chrupala-etal-2008-learning,0,0.027189,"Missing"
P14-1070,constant-tellier-2012-evaluating,1,0.822965,"cs MWE Analysis and Tagging: For the MWE analyzer, we used the tool lgtagger11 (version 1.1) with its default set of feature templates, and a 10-fold jackknifing on the training corpus. MWE Analyzer The MWE analyzer is a CRF-based sequential labeler, which, given a tokenized text, jointly performs MWE segmentation and POS tagging (of simple tokens and of MWEs), both tasks mutually helping each other9 . The MWE analyzer integrates, among others, features computed from the external lexicons described in section 5.1, which greatly improve POS tagging (Denis and Sagot, 2009) and MWE segmentation (Constant and Tellier, 2012). The MWE analyzer also jointly classifies its predicted MWEs as regular or irregular (the distinction being learnt on gold training set, with structured MWEs cf. section 3.2). 5.3 Parser: We used the second-order graph-based parser available in Mate-tools12 (Bohnet, 2010). We used the Anna3.3 version, in projective mode, with default feature sets and parameters proposed in the documentation, augmented or not with MWE-specific features, depending on the experiments. MWE-specific features We introduce information from the external MWE resources in different ways: Flat MWE features: MWE informat"
P14-1070,P12-1022,1,0.718786,"Alpage Paris Diderot Univ INRIA Matthieu Constant Universit´e Paris-Est LIGM CNRS marie.candito@ linguist.univ-paris-diderot.fr Matthieu.Constant@ u-pem.fr Abstract task when using external MWE lexicons to help English parsing, Constant et al. (2012) report results on the joint MWE recognition and parsing task, in which errors in MWE recognition alleviate their positive effect on parsing performance. While the realistic scenario of syntactic parsing with automatic MWE recognition (either done jointly or in a pipeline) has already been investigated in constituency parsing (Green et al., 2011; Constant et al., 2012; Green et al., 2013), the French dataset of the SPMRL 2013 Shared Task (Seddah et al., 2013) only recently provided the opportunity to evaluate this scenario within the framework of dependency syntax.2 In such a scenario, a system predicts dependency trees with marked groupings of tokens into MWEs. The trees show syntactic dependencies between semantically sound units (made of one or several tokens), and are thus particularly appealing for downstream semantic-oriented applications, as dependency trees are considered to be closer to predicate-argument structures. In this paper, we investigate"
P14-1070,W13-4905,1,0.88589,"dataset, and the modified representation for regular MWEs that we propose. for predicting both syntax and MWEs. Section 5 presents the external resources targeted to improve MWE recognition. We describe experiments and discuss their results in section 6 and conclude in section 7. 2 Related work We gave in introduction references to previous work on predicting MWEs and constituency parsing. To our knowledge, the first works3 on predicting both MWEs and dependency trees are those presented to the SPMRL 2013 Shared Task that provided scores for French (which is the only dataset containing MWEs). Constant et al. (2013) proposed to combine pipeline and joint systems in a reparser (Sagae and Lavie, 2006), and ranked first at the Shared Task. Our contribution with respect to that work is the representation of the internal syntactic structure of MWEs, and use of MWE-specific features for the joint system. The system of Bj¨orkelund et al. (2013) ranked second on French, though with close UAS/LAS scores. It is a less language-specific system that reranks nbest dependency parses from 3 parsers, informed with features from predicted constituency trees. It uses no feature nor treatment specific to MWEs as it focuses"
P14-1070,Y09-1013,0,0.0143371,"Missing"
P14-1070,D11-1067,0,0.0336593,"Missing"
P14-1070,J13-1009,0,0.481666,"Missing"
P14-1070,N06-2033,0,0.0359777,"ting both syntax and MWEs. Section 5 presents the external resources targeted to improve MWE recognition. We describe experiments and discuss their results in section 6 and conclude in section 7. 2 Related work We gave in introduction references to previous work on predicting MWEs and constituency parsing. To our knowledge, the first works3 on predicting both MWEs and dependency trees are those presented to the SPMRL 2013 Shared Task that provided scores for French (which is the only dataset containing MWEs). Constant et al. (2013) proposed to combine pipeline and joint systems in a reparser (Sagae and Lavie, 2006), and ranked first at the Shared Task. Our contribution with respect to that work is the representation of the internal syntactic structure of MWEs, and use of MWE-specific features for the joint system. The system of Bj¨orkelund et al. (2013) ranked second on French, though with close UAS/LAS scores. It is a less language-specific system that reranks nbest dependency parses from 3 parsers, informed with features from predicted constituency trees. It uses no feature nor treatment specific to MWEs as it focuses on the general aim of the Shared Task, namely coping with prediction of morphologica"
P14-1070,sagot-2010-lefff,0,0.0367674,"t label suffixing. • parsing: (i) MWE analysis and classification into regular or irregular, used for MWEspecific features, (ii) tagging and morphological prediction, (iii) parsing, We compare these four architectures between them and also with two simpler architectures used by (Constant et al., 2013) within the SPMRL 13 Shared Task, in which regular and irregular MWEs are not distinguished: 747 MWE lexicons are exploited as sources of features for both the dependency parser and the external MWE analyzer. In particular, two largecoverage general-language lexicons are used: the Lefff6 lexicon (Sagot, 2010), which contains approximately half a million inflected word forms, among which approx. 25, 000 are MWEs; and the DELA7 (Courtois, 2009; Courtois et al., 1997) lexicon, which contains approx. one million inflected forms, among which about 110, 000 are MWEs. These resources are completed with specific lexicons freely available in the platform Unitex8 : the toponym dictionary Prolex (Piton et al., 1999) and a dictionary of first names. Note that the lexicons do not include any information on the irregular or the regular status of the MWEs. In order to compare the MWEs present in the lexicons and"
P14-1070,P81-1022,0,0.727285,"Missing"
P14-1070,I13-1024,0,0.629295,"cuses on the general aim of the Shared Task, namely coping with prediction of morphological and syntactic analysis. Concerning related work on the representation of MWE internal structure, we can cite the Prague Dependency Bank, which captures both regular syntax of non-compositional MWEs and their MWE status, in two distinct annotation layers (Bejˇcek and Stranak, 2010). Our representation also resembles that of light-verb constructions (LVC) in the hungarian dependency treebank (Vincze et al., 2010): the construction has regular syntax, and a suffix is used on labels to express it is a LVC (Vincze et al., 2013). a. Flat representation: tp s p de au x mo d d cp d cp L’ de d p ep cp cpd d p de de t suj abus de biens sociaux fut d´enonc´e en vain b. Structured representation: au x de p mo d p de t de tp s suj d cp ob j.p mo d L’ abus de biens sociaux fut d´enonc´e en vain Figure 1: French dependency tree for L’abus de biens sociaux fut d´enonc´e en vain (literally the misuse of assets social was denounced in vain, meaning The misuse of corporate assets was denounced in vain), containing two MWEs (in red). Top: original flat representation. Bottom: Tree after regular MWEs structuring. 3.1 MWEs in Gold D"
P14-1070,W11-3806,0,\N,Missing
P14-1070,vincze-etal-2010-hungarian,0,\N,Missing
S19-2003,S07-1002,0,0.0531283,"of C OMMERCE SELL, be labeled with the same unsupervised tag.3 The task resembles word sense induction in that it assigns a class (or sense) label to a verb. In word sense induction (WSI), labels are determined and evaluated on word forms (lemma + part-ofspeech e.g., sell.v or auction.n). WSI evaluations assume that the inventory of senses (set Si s) for different word forms f is devised independently. For instance, assuming f1 is labeled with the set of senses S1 and f2 with S2 , then S1 ∩ S2 6= φ only if f1 = f2 ; and, if f1 6= f2 then S1 ∩ S2 = φ (as in other SemEval benchmarks, including Agirre and Soroa (2007); Manandhar et al. (2010); 3 Task B.1: Unsupervised Frame Semantic Argument Labeling Taking the frames as primary and defining roles relative to each frame, the aim of Task B.1 was to cluster prespecified verb-headed argument structures according to the principles of Frame Semantics, where FrameNet served as the reference for evaluation. This task amounted to unsupervised labeling of frames and core FEs (Figure 2b). Because FrameNet defines FEs frame-specifically, Task B.1 entails Task A. Given a set of semantically-unlabelled arguments as input (e.g., Figure 1a), the root nodes (i.e., verbs)"
S19-2003,W13-2322,0,0.0104004,"of the verb as the following graphic shows. T HEME Criticism S OURCE come from Guidelines The annotation guidelines for this task were slightly different from those of FrameNet and various semantic dependency treebanks. In contrast to FN, which annotates a full span of text as an argument filler, or PropBank, which annotates syntactic constituents of arguments of verbs (Palmer et al., 2005), we identified the text spans and only annotated a single word or a multi-word unit (MWU), i.e., the semantic head of the span, like annotations in Oepen et al. (2016) and Abstract Meaning Representation (Banarescu et al., 2013). To illustrate, in Example 1, FN would annotate Criticism of futures as filling the FE E NTITY. We only annotated Criticism, understanding it as the LU that evokes J UDGMENT COMMUNICATION, which in turn represents the meaning of the whole text span. Thus, we assumed that another frame fa fills an argument of a frame. We annotated only the main content word(s) that evoke(s) fa ; these main words are the semantic heads.4 Multi-word unit semantic heads (e.g., named entities, word form combinations) are annotated as if a single word form, such as Wall Street (# 1), excluding modifiers. In contras"
S19-2003,E17-1045,0,0.0303582,"The rest of this paper is organized as follows: Section 2 contextualizes this task; Section 3 offers a detailed task-description; Section 4 describes the data; Section 5 introduces the evaluation metrics and baselines; Section 6 characterizes the participating systems and unsupervised methods that participants used; Section 7 provides evaluation scores and additional insight about the data; and Section 8 presents concluding remarks. 2 best. This limitation does not hinder unsupervised methods, which will port and extend the coverage of semantic parsers, a common challenge in semantic parsing (Hartmann et al., 2017). Unsupervised frame induction methods can serve as an assistive semantic analytic tool, to build language resources and facilitate linguistic studies. Since the focus is usually to build language resources, most systems (Pennacchiotti et al. (2008); Green et al. (2004)) have used a lexical semantic resource like WordNet (Miller, 1995) to extend coverage of a resource like FrameNet. Some methods, e.g., Modi et al. (2012) and Kallmeyer et al. (2018), tried to extract FrameNetlike resources automatically without additional semantic information. Others (Ustalov et al. (2018); Materna (2012)) addr"
S19-2003,W06-3812,0,0.0665453,"vectors with concatenated verb representation vectors and vectors that represent usage context. Task B.2 employed hand crafted features, a method to encode syntactic information, and again an agglomerative clustering method. Ribeiro et al. (2019) also reported results for all subtasks using similar techniques to those reported in the other two submitted papers. Ribeiro et al. (2019) used the bidirectional neural language model BERT, which Arefyev et al. (2019) also used. Task A employed contextualized word representations proposed in (Ustalov et al., 2018), and Biemann’s clustering algorithm (Biemann, 2006). Compared to the two other systems, Ribeiro et al. (2019) exploited input structures, weighted them, and used them elegantly in its algorithm. With the same method but different hyper-parameters for B.2 along with combining results from Task A, Ribeiro et al. (2019) offered a solution to B.1. 7 BCF 70.70 68.10 65.32 65.35 Task A BCF 63.12 49.49 42.75 45.79 BCF 64.09 42.1 45.65 39.03 B.1 B.2 Table 2: Summary of Results. The BASELINE for Task A is 1C P H, and for B.1 and B.2 is 1C P HG. Best results appear in bold face; discarded results are crossed out. Table 6 lists all other baselines. of al"
S19-2003,N06-2015,0,0.0345463,"t. The task was unsupervised in that it forbade the use of any explicit semantic annotation (only permitting morphosyntactic annotation). Instead, we encouraged the use of unsupervised representation learning methods (e.g., word embeddings, brown clusters) to obtain semantic information. Hence, systems learn and assign semantic labels to test records without appealing to any explicit training labels. For development purposes, developers received a small labeled development set. 3.1 Jurgens and Klapaftis (2013); Navigli and Vannella (2013)). For instance, in WSI evaluations based on OntoNotes (Hovy et al., 2006), six different labels from Ssell are assigned to the lemma sell.v, and one label s0 is assigned to auction.v, knowing that s0 ∈ / Ssell . Typically, lexical semantic relationships among members of Si s (e.g., synonymy, antonymy) are then analyzed independently of WSI (e.g., Lenci and Benotto (2012); Girju et al. (2007); McCarthy and Navigli (2007)). In contrast, this task assumes that the sense inventory is defined independent of word forms. This task involves uncovering mapping between word forms f and members of S such that different word forms (i.e., fi 6= fj ) can be mapped to the same me"
S19-2003,W13-5503,0,0.0335597,"Missing"
S19-2003,J05-1004,0,0.358235,"re FEs, as Example 1 shows. (1) 4.2 O RIGIN Criticism O RIGIN come from Wall Street Also, using the set of 32 generic semantic role labels in VerbNet 3.2 and two additional roles, COG NIZER and CONTENT , we annotated arguments of the verb as the following graphic shows. T HEME Criticism S OURCE come from Guidelines The annotation guidelines for this task were slightly different from those of FrameNet and various semantic dependency treebanks. In contrast to FN, which annotates a full span of text as an argument filler, or PropBank, which annotates syntactic constituents of arguments of verbs (Palmer et al., 2005), we identified the text spans and only annotated a single word or a multi-word unit (MWU), i.e., the semantic head of the span, like annotations in Oepen et al. (2016) and Abstract Meaning Representation (Banarescu et al., 2013). To illustrate, in Example 1, FN would annotate Criticism of futures as filling the FE E NTITY. We only annotated Criticism, understanding it as the LU that evokes J UDGMENT COMMUNICATION, which in turn represents the meaning of the whole text span. Thus, we assumed that another frame fa fills an argument of a frame. We annotated only the main content word(s) that evo"
S19-2003,D08-1048,0,0.42248,"Missing"
S19-2003,S12-1012,0,0.0226833,", systems learn and assign semantic labels to test records without appealing to any explicit training labels. For development purposes, developers received a small labeled development set. 3.1 Jurgens and Klapaftis (2013); Navigli and Vannella (2013)). For instance, in WSI evaluations based on OntoNotes (Hovy et al., 2006), six different labels from Ssell are assigned to the lemma sell.v, and one label s0 is assigned to auction.v, knowing that s0 ∈ / Ssell . Typically, lexical semantic relationships among members of Si s (e.g., synonymy, antonymy) are then analyzed independently of WSI (e.g., Lenci and Benotto (2012); Girju et al. (2007); McCarthy and Navigli (2007)). In contrast, this task assumes that the sense inventory is defined independent of word forms. This task involves uncovering mapping between word forms f and members of S such that different word forms (i.e., fi 6= fj ) can be mapped to the same meaning (label), and the same meaning (label) can be mapped to several word forms. We defined S with respect to FrameNet and assumed that its typed-situation frames are units of meaning. So, C OMMERCE SELL captures the meaning associated with both sell.v and auction.v., as well as other selling-relate"
S19-2003,W12-1901,0,0.250272,"remarks. 2 best. This limitation does not hinder unsupervised methods, which will port and extend the coverage of semantic parsers, a common challenge in semantic parsing (Hartmann et al., 2017). Unsupervised frame induction methods can serve as an assistive semantic analytic tool, to build language resources and facilitate linguistic studies. Since the focus is usually to build language resources, most systems (Pennacchiotti et al. (2008); Green et al. (2004)) have used a lexical semantic resource like WordNet (Miller, 1995) to extend coverage of a resource like FrameNet. Some methods, e.g., Modi et al. (2012) and Kallmeyer et al. (2018), tried to extract FrameNetlike resources automatically without additional semantic information. Others (Ustalov et al. (2018); Materna (2012)) addressed frame induction only for verbs with two arguments. Lastly, unsupervised frame induction methods can also facilitate linguistic investigations by capturing information about the reciprocal relationships between statistical features and linguistic or extra-linguistic observations (e.g., Reisinger et al. (2015)). This task aimed to benchmark a class of such unsupervised frame induction methods. Background Frame Semant"
S19-2003,L16-1376,0,0.0261748,"Missing"
S19-2003,S13-2035,0,0.0202696,"ore, 1968) and against a set of generic semantic roles, taken primarily from VerbNet. The task was unsupervised in that it forbade the use of any explicit semantic annotation (only permitting morphosyntactic annotation). Instead, we encouraged the use of unsupervised representation learning methods (e.g., word embeddings, brown clusters) to obtain semantic information. Hence, systems learn and assign semantic labels to test records without appealing to any explicit training labels. For development purposes, developers received a small labeled development set. 3.1 Jurgens and Klapaftis (2013); Navigli and Vannella (2013)). For instance, in WSI evaluations based on OntoNotes (Hovy et al., 2006), six different labels from Ssell are assigned to the lemma sell.v, and one label s0 is assigned to auction.v, knowing that s0 ∈ / Ssell . Typically, lexical semantic relationships among members of Si s (e.g., synonymy, antonymy) are then analyzed independently of WSI (e.g., Lenci and Benotto (2012); Girju et al. (2007); McCarthy and Navigli (2007)). In contrast, this task assumes that the sense inventory is defined independent of word forms. This task involves uncovering mapping between word forms f and members of S suc"
S19-2003,D18-1412,0,0.147859,"format and for training data-driven machine learning systems, which is required for tasks such as information extraction, question-answering, text summarization, among others. However, manually developing frame semantic databases and annotating corpus-derived illustrative examples to support analyses of frames are resource-intensive tasks. The most well-known frame semantic (lexical) resource is FrameNet (Ruppenhofer et al., 2016), which only covers a (relatively) small set of the vocabulary of contemporary English. While NLP research has integrated FrameNet data into semantic parsing, e.g., Swayamdipta et al. (2018), these methods cannot extend beyond previously seen training labels, tagging out-of-domain semantics as unknown at 3 Task Description C OMMERCE SELL skyscraper dobj nsubj nmod:to Exxon Mobil sell company (a) Task A - Identifying Semantic Frames: Unsupervised learned labels evaluated against FN’s lexical units C OMMERCE SELL skyscraper Goods Seller dobj nsubj nmod:to Exxon Mobil sell Buyer company (b) Task B.1 - Full Frame Semantic Tagging: Unsupervised labels evaluated against FN’s frames Theme Agent nsubj skyscraper dobj nmod:to Exxon Mobil sell Recipient company (c) Task B.2 – Case Role Lab"
S19-2003,P18-2010,0,0.114883,"in semantic parsing (Hartmann et al., 2017). Unsupervised frame induction methods can serve as an assistive semantic analytic tool, to build language resources and facilitate linguistic studies. Since the focus is usually to build language resources, most systems (Pennacchiotti et al. (2008); Green et al. (2004)) have used a lexical semantic resource like WordNet (Miller, 1995) to extend coverage of a resource like FrameNet. Some methods, e.g., Modi et al. (2012) and Kallmeyer et al. (2018), tried to extract FrameNetlike resources automatically without additional semantic information. Others (Ustalov et al. (2018); Materna (2012)) addressed frame induction only for verbs with two arguments. Lastly, unsupervised frame induction methods can also facilitate linguistic investigations by capturing information about the reciprocal relationships between statistical features and linguistic or extra-linguistic observations (e.g., Reisinger et al. (2015)). This task aimed to benchmark a class of such unsupervised frame induction methods. Background Frame Semantics (Fillmore, 1976) and other theories (Gamerschlag et al., 2014) that adopt typed feature structures for representing knowledge and linguistic structure"
W09-1008,schluter-van-genabith-2008-treebank,0,\N,Missing
W09-1008,A00-2018,0,\N,Missing
W09-1008,J98-4004,0,\N,Missing
W09-1008,J04-4004,0,\N,Missing
W09-1008,W01-0521,0,\N,Missing
W09-1008,W06-1614,0,\N,Missing
W09-1008,J03-4003,0,\N,Missing
W09-1008,P03-1054,0,\N,Missing
W09-1008,P05-1038,0,\N,Missing
W09-1008,A00-1031,0,\N,Missing
W09-1008,P06-1055,0,\N,Missing
W09-1008,P05-1010,0,\N,Missing
W09-1008,P03-1013,0,\N,Missing
W09-1008,W04-3224,0,\N,Missing
W09-1008,N06-1020,0,\N,Missing
W09-3821,J92-4003,0,0.385914,"instance among a cluster of infinite verbs, one may find a present participle). The quality of the clusters is more crucial in our case than when clusters are features, whose informativity is discriminatively learnt. This observation led us to append a restricted set of suffixes to the clusters, which gives us the best results for now. token of a sentence, if unknown in the lexicon, the algorithm tries to desinflect the low case corresponding form. This desinflection reduces the number of distinct tokens in the F TB - UC from 27143 to 20268. 4 Unsupervised word clustering We chose to use the (Brown et al., 1992) hard clustering algorithm, which has proven useful for various NLP tasks, such as dependency parsing (Koo et al., 2008) or named entity recognition (Liang, 2005). The algorithm to obtain C clusters is as follows: each of the C most frequent tokens of the corpus is assigned its own distinct cluster. For the (C+1)th most frequent token, create a (C+1)th cluster. Then for each pair among the C+1 resulting clusters, merge the pair that minimizes the loss in the likelihood of the corpus, according to a bigram language model defined on the clusters. Repeat this operation for the (C+2)th most freque"
W09-3821,W09-1008,1,0.880808,"Missing"
W09-3821,W01-0521,0,0.187706,"Missing"
W09-3821,E09-1038,0,0.114879,"Missing"
W09-3821,P08-1068,0,0.747933,"o reported a slight improvement (F1 =88.18) when word forms are clustered on a morphological basis, into lemma+tag clusters. So PCFG-LA uses lexical information, but it is too sparse, hence it benefits from word clustering. Yet the use of lemma+tag terminals supposes tagging prior to parsing. We propose here to apply rather a deterministic supervised morphological clustering that preserves tagging ambiguities, leaving it to the parser to disambiguate POS tags. We also investigate the use of unsupervised word clustering, obtained from unannotated text. It has been proved useful for parsing by (Koo et al., 2008) and their work directly inspired ours. They have shown that parsing improves when cluster information is used as features in a discriminative training method that learns dependency parsers. We investigate in this paper the use of such clusters in a generative approach to probabilistic phrase-structure parsing, simply by replacing each token by its cluster. We present a semi-supervised method to improve statistical parsing performance. We focus on the well-known problem of lexical data sparseness and present experiments of word clustering prior to parsing. We use a combination of lexiconaided"
W09-3821,P98-2127,0,0.130231,"Missing"
W09-3821,P05-1010,0,0.104312,"ation is known crucial in natural language parsing. For probabilistic parsing, one main drawback of the plain PCFG approach is to lack sensitivity to the lexicon. The symbols accessible to context-free rules are part-of-speech tags, which encode generalizations that are too coarse for many parsing decisions (for instance subcategorization information is generally absent from tagsets). The lexicalized models first proposed by Collins reintroduced words at every depth of a parse tree, insuring that attachments receive probabilities that take lexical information into account. On the other hand, (Matsuzaki et al., 2005) have proposed probabilistic CFG learning with latent annotation (hereafter PCFG-LA), as a way to automate symbol splitting in unlexicalized probabilistic parsing (cf. adding latent annotations to a symbol is comparable to splitting this symbol). 138 Proceedings of the 11th International Conference on Parsing Technologies (IWPT), pages 138–141, c Paris, October 2009. 2009 Association for Computational Linguistics 3 Morphological clustering tering using lemmas is not possible, since lemma assignment supposes POS disambiguation. Further, information such as mood on verbs is necessary to capture"
W09-3821,P06-1055,0,0.0199032,"n be identified by its path within this binary tree. Hence, clusters can be used at various levels of granularity. 5 Experiments and discussion For the Brown clustering algorithm, we used Percy Liang’s code3 , run on the L’Est Républicain corpus, a 125 million word journalistic corpus, freely available at CNRTL4. The corpus was tokenised5 , segmented into sentences and desinflected using the process described in section 3. We ran the clustering into 1000 clusters for the desinflected forms appearing at least 20 times. We tested the use of word clusters for parsing with the Berkeley algorithm (Petrov et al., 2006). Clustering words in this case has a double advantage. First, it augments the known vocabulary, which is made of all the forms of all the clusters appearing in the treebank. Second, it reduces sparseness for the latent annotations learning on the lexical rules of the PCFG-LA grammar. 6 Related work We already mentioned that we were inspired by the success of (Koo et al., 2008) in using word clusters as features for the discriminative learning of dependency parsers. Another approach to augment the known vocabulary for a generative prob6 In all metrics punctuation tokens are ignored and all res"
W09-3821,C98-2122,0,\N,Missing
W09-3821,sagot-etal-2006-lefff,0,\N,Missing
W09-3824,abeille-barrier-2004-enriching,0,0.0870136,"this question, this paper reports a set of experiments where five algorithms, first designed for the purpose of parsing English, have been adapted to French: a P CFG parser with latent annotation (Petrov et al., 2006), a Stochastic Tree Adjoining Grammar parser (Chiang, 2003), the Charniak’s lexicalized parser (Charniak, 2000) and the Bikel’s implementation of Collins’ Model 1 and 2 (Collins, 1999) described in (Bikel, 2002). To ease further comparisons, we report results on two versions of the treebank: (1) the last version made available in December 2007, hereafter F TB , and described in (Abeillé and Barrier, 2004) and the (2) L FG inspired version of (Schluter and van Genabith, 2007). The paper is structured as follows : After a brief presentation of the treebanks, we discuss the useThis paper presents preliminary investigations on the statistical parsing of French by bringing a complete evaluation on French data of the main probabilistic lexicalized and unlexicalized parsers first designed on the Penn Treebank. We adapted the parsers on the two existing treebanks of French (Abeillé et al., 2003; Schluter and van Genabith, 2007). To our knowledge, mostly all of the results reported here are state-of-th"
W09-3824,P05-1038,0,0.0683898,"zation for the parsing of French. 1 Introduction The development of large scale symbolic grammars has long been a lively topic in the French NLP community. Surprisingly, the acquisition of probabilistic grammars aiming at stochastic parsing, using either supervised or unsupervised methods, has not attracted much attention despite the availability of large manually syntactic annotated data for French. Nevertheless, the availability of the Paris 7 French Treebank (Abeillé et al., 2003), allowed (Dybro-Johansen, 2004) to carry out the extraction of a Tree Adjoining Grammar (Joshi, 1987) and led (Arun and Keller, 2005) 1 This has been made available in December 2007. 150 Proceedings of the 11th International Conference on Parsing Technologies (IWPT), pages 150–161, c Paris, October 2009. 2009 Association for Computational Linguistics fulness of testing different parsing frameworks over two parsing paradigms before introducing our experimental protocol and presenting our results. Finally, we discuss and compare with related works on cross-language parser adaptation, then we conclude. 2 Treebanks for French This section provides a brief overview to the corpora on which we report results: the French Treebank ("
W09-3824,H91-1060,0,0.121862,"Missing"
W09-3824,P04-1041,0,0.0395878,"Missing"
W09-3824,W08-2102,0,0.0249898,"no substitution node. Moreover, the probability model, being split between lexical anchors and tree templates, allows a very coarse grammar that contains, for example, only 83 tree templates for one treebank instantiation, namely the F TB - CC (cf. section 5). This behavior, although not documented10 , is close to Collins’ model 1, which does not use any argument adjunct distinction information, and led to results interesting enough to be integrated as the “Chiang Spinal” model in our parser set. It should be noted that, recently, the use of similar models has been independently proposed in (Carreras et al., 2008) with the purpose of getting a richer parsing model that can use non local features and in (Sangati and Zuidema, 2009) as a mean of extracting a Lexicalized Tree Substitution Grammar. In their process, the first extracted grammar is actually a spinal STIG. 4 Experimental protocol In this section, we specify the settings of the parsers for French, the evaluation protocol and the different instantiations of the treebanks we used for conducting the experiments. 4.1 Parsers settings Head Propagation table All lexicalized parsers reported in this paper use head propagation tables. Adapting them to"
W09-3824,A00-2018,0,0.849433,"set of trees. This path has the advantage of an easier reproducibility and eases verification of reported results. With the problem of the usability of the data source being solved, the question of finding one or many accurate language models for parsing French raises. Thus, to answer this question, this paper reports a set of experiments where five algorithms, first designed for the purpose of parsing English, have been adapted to French: a P CFG parser with latent annotation (Petrov et al., 2006), a Stochastic Tree Adjoining Grammar parser (Chiang, 2003), the Charniak’s lexicalized parser (Charniak, 2000) and the Bikel’s implementation of Collins’ Model 1 and 2 (Collins, 1999) described in (Bikel, 2002). To ease further comparisons, we report results on two versions of the treebank: (1) the last version made available in December 2007, hereafter F TB , and described in (Abeillé and Barrier, 2004) and the (2) L FG inspired version of (Schluter and van Genabith, 2007). The paper is structured as follows : After a brief presentation of the treebanks, we discuss the useThis paper presents preliminary investigations on the statistical parsing of French by bringing a complete evaluation on French da"
W09-3824,C02-1126,0,0.0564763,"Missing"
W09-3824,N06-1020,0,0.0337046,"as well as (Bikel, 2002)’s implementation of the Collins’ models 1 & 2 (Collins, 1999). Most of the lexicalized parsers we use in this work are well known and since their releases, almost ten years ago, their core parsing models still provide state-of-the-art performance on the standard test set for English.6 We insist on the fact that one of the goals of this work was to evaluate raw performance of well known parsing models on French annotated data. Thus, we have not considered using more complex parsing architectures that makes use of reranking (Charniak and Johnson, 2005) or self-training (McClosky et al., 2006) in order to improve the performance of a raw parsing model. Furthermore, studying and designing a set of features for a reranking parser was beyond the scope of this work. However, we did use some of these models in a non classical way, leading us to explore a Collins’ model 2 variation, named model X, and a Stochastic Tree Adjoining Grammar (Schabes, 1992; Resnik, 1992) variant7 , named Spinal Stochastic Tree Insertion Grammars (hereafter S PINAL S TIG), which was first used to validate the heuristics used by our adaptation of the Bikel’s parser to French. The next two subsections introduce"
W09-3824,P06-1055,0,0.133959,"a new released and corrected version of the treebank1 it was possible to train statistical parsers from the original set of trees. This path has the advantage of an easier reproducibility and eases verification of reported results. With the problem of the usability of the data source being solved, the question of finding one or many accurate language models for parsing French raises. Thus, to answer this question, this paper reports a set of experiments where five algorithms, first designed for the purpose of parsing English, have been adapted to French: a P CFG parser with latent annotation (Petrov et al., 2006), a Stochastic Tree Adjoining Grammar parser (Chiang, 2003), the Charniak’s lexicalized parser (Charniak, 2000) and the Bikel’s implementation of Collins’ Model 1 and 2 (Collins, 1999) described in (Bikel, 2002). To ease further comparisons, we report results on two versions of the treebank: (1) the last version made available in December 2007, hereafter F TB , and described in (Abeillé and Barrier, 2004) and the (2) L FG inspired version of (Schluter and van Genabith, 2007). The paper is structured as follows : After a brief presentation of the treebanks, we discuss the useThis paper presents"
W09-3824,D07-1066,0,0.0373258,"Missing"
W09-3824,P03-1013,0,0.012629,"vercome P CFG’s problems (a) and (b)5 . M FT 4739 28.38 2.11 6944 39 27 3.1 Lexicalized algorithms The first class of algorithms used are lexicalized parsers of (Collins, 1999; Charniak, 2000; Chiang, 2003). The insight underlying the lexicalized algorithms is to model lexical dependencies between a governor and its dependants in order to improve attachment choices. Even though it has been proven numerous times that lexicalization was useful for parsing the Wall Street Journal corpus (Collins, 1999; Charniak, 2000), the question of its relevance for other languages has been raised for German (Dubey and Keller, 2003; Kübler et al., 2006) and for French Table 1: Treebanks Properties 3 F TB A ADV C CL D ET I N P P+D P+PRO PONCT PREF PRO V Parsing Algorithms Although Probabilistic Context Free Grammars (P CFG) are a baseline formalism for probabilistic parsing, it is well known that they suffer from two problems: (a) The independence assumptions made by the model are too strong, and (b) For Natural Language Parsing, they do not take into account lexical probabilities. To date, most of the results on statistical parsing have been reported for English. Here we propose to investigate how to apply these techniq"
W09-3824,E09-1080,0,0.0120162,"lows a very coarse grammar that contains, for example, only 83 tree templates for one treebank instantiation, namely the F TB - CC (cf. section 5). This behavior, although not documented10 , is close to Collins’ model 1, which does not use any argument adjunct distinction information, and led to results interesting enough to be integrated as the “Chiang Spinal” model in our parser set. It should be noted that, recently, the use of similar models has been independently proposed in (Carreras et al., 2008) with the purpose of getting a richer parsing model that can use non local features and in (Sangati and Zuidema, 2009) as a mean of extracting a Lexicalized Tree Substitution Grammar. In their process, the first extracted grammar is actually a spinal STIG. 4 Experimental protocol In this section, we specify the settings of the parsers for French, the evaluation protocol and the different instantiations of the treebanks we used for conducting the experiments. 4.1 Parsers settings Head Propagation table All lexicalized parsers reported in this paper use head propagation tables. Adapting them to the French language requires to design French specific head propagation rules. To this end, we used those described by"
W09-3824,J98-4004,0,0.0648842,"kes advantage of the function labels annotated in the treebank. This is one of the main differences with the experiments described in (Arun and Keller, 2005) and (DybroJohansen, 2004) where the authors had to rely only on the very flat treebank structure without function labels, to annotate the arguments of a head. 3.2 Unlexicalized Parser As an instance of an unlexicalized parser, the last algorithm we use is the Berkeley unlexicalized parser (B KY) of (Petrov et al., 2006). This algorithm is an evolution of treebank transformation principles aimed at reducing P CFG independence assumptions (Johnson, 1998; Klein and Manning, 2003). Treebank transformations may be of two kinds (1) structure transformation and (2) labelling transformations. The Berkeley parser concentrates on (2) by recasting the problem of acquiring an optimal set of non terminal symbols as an semisupervised learning problem by learning a P CFG with Latent annotations (P CFG -L A): given an observed P CFG induced from the treebank, the latent grammar is generated by combining every non terminal of the observed grammar to a predefined set H of latent symbols. The parameters of the latent grammar are estimated from the actual tre"
W09-3824,J95-4002,0,0.210274,"arsing Algorithms Although Probabilistic Context Free Grammars (P CFG) are a baseline formalism for probabilistic parsing, it is well known that they suffer from two problems: (a) The independence assumptions made by the model are too strong, and (b) For Natural Language Parsing, they do not take into account lexical probabilities. To date, most of the results on statistical parsing have been reported for English. Here we propose to investigate how to apply these techniques to another language – French – by testing two distinct enhancements 5 Except (Chiang, 2003) which is indeed a T REE I N (Schabes and Waters, 1995) parser but which must extract a lexicalized grammar from the set of context free rules underlying a treebank. SERTION GRAMMAR 153 a lexicalized PCFG can roughly be described as a set of stochastic rules of the form: P → Ln Ln−1 ..L1 H R1 .. Rm−1 Rm where Li , H, Ri and P are all lexicalized non terminals; P inherits its head from H (Bikel, 2004). The Collins’ model 2 deterministically labels some nodes of a rule to be arguments of a given Head and the remaining nodes are considered to be modifier non terminals (hereafter MNT). In this model, given a left-hand side symbol, the head and its arg"
W09-3824,C92-2066,0,0.129055,"Missing"
W09-3824,P03-1054,0,0.0141452,"f the function labels annotated in the treebank. This is one of the main differences with the experiments described in (Arun and Keller, 2005) and (DybroJohansen, 2004) where the authors had to rely only on the very flat treebank structure without function labels, to annotate the arguments of a head. 3.2 Unlexicalized Parser As an instance of an unlexicalized parser, the last algorithm we use is the Berkeley unlexicalized parser (B KY) of (Petrov et al., 2006). This algorithm is an evolution of treebank transformation principles aimed at reducing P CFG independence assumptions (Johnson, 1998; Klein and Manning, 2003). Treebank transformations may be of two kinds (1) structure transformation and (2) labelling transformations. The Berkeley parser concentrates on (2) by recasting the problem of acquiring an optimal set of non terminal symbols as an semisupervised learning problem by learning a P CFG with Latent annotations (P CFG -L A): given an observed P CFG induced from the treebank, the latent grammar is generated by combining every non terminal of the observed grammar to a predefined set H of latent symbols. The parameters of the latent grammar are estimated from the actual treebank Morphology and typog"
W09-3824,schluter-van-genabith-2008-treebank,0,0.0277974,"Missing"
W09-3824,W06-1614,0,0.0574525,"(a) and (b)5 . M FT 4739 28.38 2.11 6944 39 27 3.1 Lexicalized algorithms The first class of algorithms used are lexicalized parsers of (Collins, 1999; Charniak, 2000; Chiang, 2003). The insight underlying the lexicalized algorithms is to model lexical dependencies between a governor and its dependants in order to improve attachment choices. Even though it has been proven numerous times that lexicalization was useful for parsing the Wall Street Journal corpus (Collins, 1999; Charniak, 2000), the question of its relevance for other languages has been raised for German (Dubey and Keller, 2003; Kübler et al., 2006) and for French Table 1: Treebanks Properties 3 F TB A ADV C CL D ET I N P P+D P+PRO PONCT PREF PRO V Parsing Algorithms Although Probabilistic Context Free Grammars (P CFG) are a baseline formalism for probabilistic parsing, it is well known that they suffer from two problems: (a) The independence assumptions made by the model are too strong, and (b) For Natural Language Parsing, they do not take into account lexical probabilities. To date, most of the results on statistical parsing have been reported for English. Here we propose to investigate how to apply these techniques to another languag"
W09-3824,I08-3008,0,0.0258923,"’ S MODEL 1, S PINAL S TIG, C HAR NIAK and B KY ). For this earlier experiment, our implementation of the C OLLINS MODEL 1 actually corresponds to the MODEL X without an argument adjunct distinction table. More precisely, the absence of argument nodes, used for the acquisition of subcategorization frames features, makes the M ODEL X parsing model consider all the nodes of a rule, ex15 Due to the lack of function annotation labels in this treebank, (Arun and Keller, 2005)’s argument distinction table was used for this experiment. 16 Note that the C HARNIAK’s parser has been adapted for Danish (Zeman and Resnik, 2008) ; the authors report a 80.20 F1 score for a specific instance of the Danish Treebank. PARSER Arun (acl05) Arun (this paper) Schluter (pacling07) Collins (Mx) Collins (M2) Collins (M1) Charniak Chiang (Sp) Bky F TBA RUN 80.45 81.08 81.5 79.36 77.82 82.35 80.94 84.03 M FT S CHLU 79.95 80,96 79,91 82,66 81,86 82.86 Table 6: Labeled bracket scores on Arun’s F TB version and on the M FT In order to favour a “fair” comparison between our work and (Arun and Keller, 2005), we also ran their best adaptation of the C OLLINS MODEL 2 on their treebank version using our own head rules set15 and obtained 8"
W09-3824,J93-2004,0,\N,Missing
W09-3824,J04-4004,0,\N,Missing
W10-1401,P05-1038,0,0.0195671,"arily appropriate for parsing MRLs – but associated with this question are important questions concerning the annotation scheme of the related treebanks. Obviously, when annotating structures for languages with characteristics different than English one has to face different annotation decisions, and it comes as no surprise that the annotated structures for MRLs often differ from those employed in the PTB. 1 The shared tasks involved 18 languages, including many MRLs such as Arabic, Basque, Czech, Hungarian, and Turkish. For Spanish and French, it was shown by Cowan and Collins (2005) and in (Arun and Keller, 2005; Schluter and van Genabith, 2007), that restructuring the treebanks’ native annotation scheme to match the PTB annotation style led to a significant gain in parsing performance of Head-Driven models of the kind proposed in (Collins, 1997). For German, a language with four different treebanks and two substantially different annotation schemes, it has been shown that a PCFG parser is sensitive to the kind of representation employed in the treebank. Dubey and Keller (2003), for example, showed that a simple PCFG parser outperformed an emulation of Collins’ model 1 on N EGRA. They showed that usi"
W10-1401,W10-1408,1,0.785091,"Missing"
W10-1401,W10-1404,0,0.236375,"s is substantial lexical data sparseness due to high morphological variation in surface forms. The question is therefore, given our finite, and often fairly small, annotated sets of data, how can we guess the morphological analyses, including the PoS tag assignment and various features, of an OOV word? How can we learn the probabilities of such assignments? In a more general setup, this problem is akin to handling out-of-vocabulary or rare words for robust statistical parsing, and techniques for domain adaptation via lexicon enhance5 Constituency-Based Dependency-Based (Marton et al., 2010)† (Bengoetxea and Gojenola, 2010) - German Hebrew Hindi (Attia et al., 2010) (Attia et al., 2010) (Attia et al., 2010) (Seddah et al., 2010) (Candito and Seddah, 2010)† (Maier, 2010) (Tsarfaty and Sima’an, 2010) - Korean (Chung et al., 2010) Arabic Basque English French (Goldberg and Elhadad, 2010)† (Ambati et al., 2010a)† (Ambati et al., 2010b) - Table 1: An overview of SPMRL contributions. († report results also for non-gold standard input) ment (also explored for English and other morphologically impoverished languages). So, in fact, incorporating morphological information inside the syntactic model for the purpose of stat"
W10-1401,H91-1060,0,0.0333452,"Missing"
W10-1401,E03-1005,0,0.0521711,"Missing"
W10-1401,W06-2920,0,0.219462,"red MRL-friendly, due to its language agnostic design. The rise of dependency parsing: It is commonly assumed that dependency structures are better suited for representing the syntactic structures of free word order, morphologically rich, languages, because this representation format does not rely crucially on the position of words and the internal grouping of surface chunks (Mel’ˇcuk, 1988). It is an entirely different question, however, whether dependency parsers are in fact better suited for parsing such languages. The CoNLL shared tasks on multilingual dependency parsing in 2006 and 2007 (Buchholz and Marsi, 2006; Nivre et al., 2007a) demonstrated that dependency parsing for MRLs is quite challenging. While dependency parsers are adaptable to many languages, as reflected in the multiplicity of the languages covered,1 the analysis by Nivre et al. (2007b) shows that the best result was obtained for English, followed by Catalan, and that the most difficult languages to parse were Arabic, Basque, and Greek. Nivre et al. (2007a) drew a somewhat typological conclusion, that languages with rich morphology and free word order are the hardest to parse. This was shown to be the case for both MaltParser (Nivre e"
W10-1401,W10-1409,1,0.834713,"and often fairly small, annotated sets of data, how can we guess the morphological analyses, including the PoS tag assignment and various features, of an OOV word? How can we learn the probabilities of such assignments? In a more general setup, this problem is akin to handling out-of-vocabulary or rare words for robust statistical parsing, and techniques for domain adaptation via lexicon enhance5 Constituency-Based Dependency-Based (Marton et al., 2010)† (Bengoetxea and Gojenola, 2010) - German Hebrew Hindi (Attia et al., 2010) (Attia et al., 2010) (Attia et al., 2010) (Seddah et al., 2010) (Candito and Seddah, 2010)† (Maier, 2010) (Tsarfaty and Sima’an, 2010) - Korean (Chung et al., 2010) Arabic Basque English French (Goldberg and Elhadad, 2010)† (Ambati et al., 2010a)† (Ambati et al., 2010b) - Table 1: An overview of SPMRL contributions. († report results also for non-gold standard input) ment (also explored for English and other morphologically impoverished languages). So, in fact, incorporating morphological information inside the syntactic model for the purpose of statistical parsing is anything but trivial. In the next section we review the various approaches taken in the individual contributions of"
W10-1401,W08-2102,0,0.0218025,"Missing"
W10-1401,A00-2018,0,0.0303149,"Collins, 1997) on the ISST treebank, and obtained significantly lower results compared to English. It is notable that these models were applied without adding morphological signatures, using gold lemmas instead. Corazza et al. (2004) further tried different refinements including parent annotation and horizontal markovization, but none of them obtained the desired improvement. For French, Crabb´e and Candito (2008) and Seddah et al. (2010) show that, given a corpus comparable in size and properties (i.e. the number of tokens and grammar size), the performance level, both for Charniak’s parser (Charniak, 2000) and the Berke3 ley parser (Petrov et al., 2006) was higher for parsing the PTB than it was for French. The split-mergesmooth implementation of (Petrov et al., 2006) consistently outperform various lexicalized and unlexicalized models for French (Seddah et al., 2009) and for many other languages (Petrov and Klein, 2007). In this respect, (Petrov et al., 2006) is considered MRL-friendly, due to its language agnostic design. The rise of dependency parsing: It is commonly assumed that dependency structures are better suited for representing the syntactic structures of free word order, morphologic"
W10-1401,P00-1058,0,0.0168692,"Missing"
W10-1401,W10-1406,0,0.0567829,"Missing"
W10-1401,P99-1065,0,0.261618,"Missing"
W10-1401,P97-1003,0,0.183573,"r) are reflected in the form of words, morphological information is often secondary to other syntactic factors, such as the position of words and their arrangement into phrases. German, an Indo-European language closely related to English, already exhibits some of the properties that make parsing MRLs problematic. The Semitic languages Arabic and Hebrew show an even more extreme case in terms of the richness of their morphological forms and the flexibility in their syntactic ordering. 2.2 Parsing MRLs Pushing the envelope of constituency parsing: The Head-Driven models of the type proposed by Collins (1997) have been ported to parsing many MRLs, often via the implementation of Bikel (2002). For Czech, the adaptation by Collins et al. (1999) culminated in an 80 F1 -score. German has become almost an archetype of the problems caused by MRLs; even though German has a moderately rich morphology and a moderately free word order, parsing results are far from those for English (see (K¨ubler, 2008) and references therein). Dubey (2005) showed that, for German parsing, adding case and morphology information together with smoothed markovization and an adequate unknown-word model is more important than lex"
W10-1401,H05-1100,0,0.0111384,"ful in parsing English are necessarily appropriate for parsing MRLs – but associated with this question are important questions concerning the annotation scheme of the related treebanks. Obviously, when annotating structures for languages with characteristics different than English one has to face different annotation decisions, and it comes as no surprise that the annotated structures for MRLs often differ from those employed in the PTB. 1 The shared tasks involved 18 languages, including many MRLs such as Arabic, Basque, Czech, Hungarian, and Turkish. For Spanish and French, it was shown by Cowan and Collins (2005) and in (Arun and Keller, 2005; Schluter and van Genabith, 2007), that restructuring the treebanks’ native annotation scheme to match the PTB annotation style led to a significant gain in parsing performance of Head-Driven models of the kind proposed in (Collins, 1997). For German, a language with four different treebanks and two substantially different annotation schemes, it has been shown that a PCFG parser is sensitive to the kind of representation employed in the treebank. Dubey and Keller (2003), for example, showed that a simple PCFG parser outperformed an emulation of Collins’ model 1 o"
W10-1401,2008.jeptalnrecital-long.17,1,0.829354,"Missing"
W10-1401,P03-1013,0,0.0556766,"rted to parsing many MRLs, often via the implementation of Bikel (2002). For Czech, the adaptation by Collins et al. (1999) culminated in an 80 F1 -score. German has become almost an archetype of the problems caused by MRLs; even though German has a moderately rich morphology and a moderately free word order, parsing results are far from those for English (see (K¨ubler, 2008) and references therein). Dubey (2005) showed that, for German parsing, adding case and morphology information together with smoothed markovization and an adequate unknown-word model is more important than lexicalization (Dubey and Keller, 2003). For Modern Hebrew, Tsarfaty and Sima’an (2007) show that a simple treebank PCFG augmented with parent annotation and morphological information as state-splits significantly outperforms Head-Driven markovized models of the kind made popular by Klein and Manning (2003). Results for parsing Modern Standard Arabic using Bikel’s implementation on gold-standard tagging and segmentation have not improved substantially since the initial release of the treebank (Maamouri et al., 2004; Kulick et al., 2006; Maamouri et al., 2008). For Italian, Corazza et al. (2004) used the Stanford parser and Bikel’s"
W10-1401,P05-1039,0,0.0235903,"cal forms and the flexibility in their syntactic ordering. 2.2 Parsing MRLs Pushing the envelope of constituency parsing: The Head-Driven models of the type proposed by Collins (1997) have been ported to parsing many MRLs, often via the implementation of Bikel (2002). For Czech, the adaptation by Collins et al. (1999) culminated in an 80 F1 -score. German has become almost an archetype of the problems caused by MRLs; even though German has a moderately rich morphology and a moderately free word order, parsing results are far from those for English (see (K¨ubler, 2008) and references therein). Dubey (2005) showed that, for German parsing, adding case and morphology information together with smoothed markovization and an adequate unknown-word model is more important than lexicalization (Dubey and Keller, 2003). For Modern Hebrew, Tsarfaty and Sima’an (2007) show that a simple treebank PCFG augmented with parent annotation and morphological information as state-splits significantly outperforms Head-Driven markovized models of the kind made popular by Klein and Manning (2003). Results for parsing Modern Standard Arabic using Bikel’s implementation on gold-standard tagging and segmentation have not"
W10-1401,P08-1109,0,0.0528707,"Missing"
W10-1401,W10-1412,1,0.240586,"various features, of an OOV word? How can we learn the probabilities of such assignments? In a more general setup, this problem is akin to handling out-of-vocabulary or rare words for robust statistical parsing, and techniques for domain adaptation via lexicon enhance5 Constituency-Based Dependency-Based (Marton et al., 2010)† (Bengoetxea and Gojenola, 2010) - German Hebrew Hindi (Attia et al., 2010) (Attia et al., 2010) (Attia et al., 2010) (Seddah et al., 2010) (Candito and Seddah, 2010)† (Maier, 2010) (Tsarfaty and Sima’an, 2010) - Korean (Chung et al., 2010) Arabic Basque English French (Goldberg and Elhadad, 2010)† (Ambati et al., 2010a)† (Ambati et al., 2010b) - Table 1: An overview of SPMRL contributions. († report results also for non-gold standard input) ment (also explored for English and other morphologically impoverished languages). So, in fact, incorporating morphological information inside the syntactic model for the purpose of statistical parsing is anything but trivial. In the next section we review the various approaches taken in the individual contributions of the SPMRL workshop for addressing such challenges. 4 Parsing MRLs: Recurring Trends The first workshop on parsing MRLs features 11"
W10-1401,E09-1038,1,0.7929,"isambiguate the morphological analyses of input forms? Should we do that prior to parsing or perhaps jointly with it?2 Representation and Modeling: Assuming that the input to our system reflects morphological information, one way or another, which types of morpho2 Most studies on parsing MRLs nowadays assume the gold standard segmentation and disambiguated morphological information as input. This is the case, for instance, for the Arabic parsing at CoNLL 2007 (Nivre et al., 2007a). This practice deludes the community as to the validity of the parsing results reported for MRLs in shared tasks. Goldberg et al. (2009), for instance, show a gap of up to 6pt F1 -score between performance on gold standard segmentation vs. raw text. One way to overcome this is to devise joint morphological and syntactic disambiguation frameworks (cf. (Goldberg and Tsarfaty, 2008)). logical information should we include in the parsing model? Inflectional and/or derivational? Case information and/or agreement features? How can valency requirements reflected in derivational morphology affect the overall syntactic structure? In tandem with the decision concerning the morphological information to include, we face genuine challenges"
W10-1401,W05-0303,0,0.043074,"Missing"
W10-1401,P08-1067,0,0.0516751,"Missing"
W10-1401,P03-1054,0,0.00472369,"rphology and a moderately free word order, parsing results are far from those for English (see (K¨ubler, 2008) and references therein). Dubey (2005) showed that, for German parsing, adding case and morphology information together with smoothed markovization and an adequate unknown-word model is more important than lexicalization (Dubey and Keller, 2003). For Modern Hebrew, Tsarfaty and Sima’an (2007) show that a simple treebank PCFG augmented with parent annotation and morphological information as state-splits significantly outperforms Head-Driven markovized models of the kind made popular by Klein and Manning (2003). Results for parsing Modern Standard Arabic using Bikel’s implementation on gold-standard tagging and segmentation have not improved substantially since the initial release of the treebank (Maamouri et al., 2004; Kulick et al., 2006; Maamouri et al., 2008). For Italian, Corazza et al. (2004) used the Stanford parser and Bikel’s parser emulation of Collins’ model 2 (Collins, 1997) on the ISST treebank, and obtained significantly lower results compared to English. It is notable that these models were applied without adding morphological signatures, using gold lemmas instead. Corazza et al. (200"
W10-1401,W06-1614,0,0.154599,"Missing"
W10-1401,P95-1037,0,0.0299787,"Missing"
W10-1401,W10-1407,0,0.0148488,"elements into account, and thus learn the different distributions associated with morphologically marked elements in constituency structures, to improve performance. In addition to free word order, MRLs show higher degree of freedom in extraposition. Both of these phenomena can result in discontinuous structures. In constituency-based treebanks, this is either annotated as additional information which has to be recovered somehow (traces in the case of the PTB, complex edge labels in the German T¨uBa-D/Z), or as discontinuous phrase structures, which cannot be handled with current PCFG models. Maier (2010) suggests the use of Linear Context-Free Rewriting Systems (LCFRSs) in order to make discontinuous structure transparent to the parsing process and yet preserve familiar notions from constituency. Dependency representation uses non-projective dependencies to reflect discontinuities, which is problematic to parse with models that assume projectivity. Different ways have been proposed to deal with non-projectivity (Nivre and Nilsson, 2005; McDonald et al., 2005; McDonald and Pereira, 2006; Nivre, 2009). Bengoetxea and Gojenola (2010) discuss non-projective dependencies in Basque and show that th"
W10-1401,J93-2004,0,0.0355629,". We synthesize the contributions of researchers working on parsing Arabic, Basque, French, German, Hebrew, Hindi and Korean to point out shared solutions across languages. The overarching analysis suggests itself as a source of directions for future investigations. 1 Introduction The availability of large syntactically annotated corpora led to an explosion of interest in automatically inducing models for syntactic analysis and disambiguation called statistical parsers. The development of successful statistical parsing models for English focused on the Wall Street Journal Penn Treebank (PTB, (Marcus et al., 1993)) as the primary, and sometimes only, resource. Since the initial release of the Penn Treebank (PTB Marcus et Among the arguments that have been proposed to explain this performance gap are the impact of small data sets, differences in treebanks’ annotation schemes, and inadequacy of the widely used PARS E VAL evaluation metrics. None of these aspects in isolation can account for the systematic performance deterioration, but observed from a wider, crosslinguistic perspective, a picture begins to emerge – that the morphologically rich nature of some of the languages makes them inherently more s"
W10-1401,W10-1402,0,0.0399505,"Missing"
W10-1401,E06-1011,0,0.0266021,"labels in the German T¨uBa-D/Z), or as discontinuous phrase structures, which cannot be handled with current PCFG models. Maier (2010) suggests the use of Linear Context-Free Rewriting Systems (LCFRSs) in order to make discontinuous structure transparent to the parsing process and yet preserve familiar notions from constituency. Dependency representation uses non-projective dependencies to reflect discontinuities, which is problematic to parse with models that assume projectivity. Different ways have been proposed to deal with non-projectivity (Nivre and Nilsson, 2005; McDonald et al., 2005; McDonald and Pereira, 2006; Nivre, 2009). Bengoetxea and Gojenola (2010) discuss non-projective dependencies in Basque and show that the pseudo-projective transformation of (Nivre and Nilsson, 2005) improves accuracy for dependency parsing of Basque. Moreover, they show that in combination with other transformations, it improves the utility of these other ones, too. 4.4 Estimation and Smoothing: Coping with Lexical Sparsity Morphological word form variation augments the vocabulary size and thus worsens the problem of lexical data sparseness. Words occurring with mediumfrequency receive less reliable estimates, and the"
W10-1401,P05-1012,0,0.11798,"Missing"
W10-1401,P05-1013,0,0.0312259,"how (traces in the case of the PTB, complex edge labels in the German T¨uBa-D/Z), or as discontinuous phrase structures, which cannot be handled with current PCFG models. Maier (2010) suggests the use of Linear Context-Free Rewriting Systems (LCFRSs) in order to make discontinuous structure transparent to the parsing process and yet preserve familiar notions from constituency. Dependency representation uses non-projective dependencies to reflect discontinuities, which is problematic to parse with models that assume projectivity. Different ways have been proposed to deal with non-projectivity (Nivre and Nilsson, 2005; McDonald et al., 2005; McDonald and Pereira, 2006; Nivre, 2009). Bengoetxea and Gojenola (2010) discuss non-projective dependencies in Basque and show that the pseudo-projective transformation of (Nivre and Nilsson, 2005) improves accuracy for dependency parsing of Basque. Moreover, they show that in combination with other transformations, it improves the utility of these other ones, too. 4.4 Estimation and Smoothing: Coping with Lexical Sparsity Morphological word form variation augments the vocabulary size and thus worsens the problem of lexical data sparseness. Words occurring with medium"
W10-1401,P09-1040,0,0.0260548,"D/Z), or as discontinuous phrase structures, which cannot be handled with current PCFG models. Maier (2010) suggests the use of Linear Context-Free Rewriting Systems (LCFRSs) in order to make discontinuous structure transparent to the parsing process and yet preserve familiar notions from constituency. Dependency representation uses non-projective dependencies to reflect discontinuities, which is problematic to parse with models that assume projectivity. Different ways have been proposed to deal with non-projectivity (Nivre and Nilsson, 2005; McDonald et al., 2005; McDonald and Pereira, 2006; Nivre, 2009). Bengoetxea and Gojenola (2010) discuss non-projective dependencies in Basque and show that the pseudo-projective transformation of (Nivre and Nilsson, 2005) improves accuracy for dependency parsing of Basque. Moreover, they show that in combination with other transformations, it improves the utility of these other ones, too. 4.4 Estimation and Smoothing: Coping with Lexical Sparsity Morphological word form variation augments the vocabulary size and thus worsens the problem of lexical data sparseness. Words occurring with mediumfrequency receive less reliable estimates, and the number of rare"
W10-1401,N07-1051,0,0.0143897,"markovization, but none of them obtained the desired improvement. For French, Crabb´e and Candito (2008) and Seddah et al. (2010) show that, given a corpus comparable in size and properties (i.e. the number of tokens and grammar size), the performance level, both for Charniak’s parser (Charniak, 2000) and the Berke3 ley parser (Petrov et al., 2006) was higher for parsing the PTB than it was for French. The split-mergesmooth implementation of (Petrov et al., 2006) consistently outperform various lexicalized and unlexicalized models for French (Seddah et al., 2009) and for many other languages (Petrov and Klein, 2007). In this respect, (Petrov et al., 2006) is considered MRL-friendly, due to its language agnostic design. The rise of dependency parsing: It is commonly assumed that dependency structures are better suited for representing the syntactic structures of free word order, morphologically rich, languages, because this representation format does not rely crucially on the position of words and the internal grouping of surface chunks (Mel’ˇcuk, 1988). It is an entirely different question, however, whether dependency parsers are in fact better suited for parsing such languages. The CoNLL shared tasks on"
W10-1401,P06-1055,0,0.0941097,"tained significantly lower results compared to English. It is notable that these models were applied without adding morphological signatures, using gold lemmas instead. Corazza et al. (2004) further tried different refinements including parent annotation and horizontal markovization, but none of them obtained the desired improvement. For French, Crabb´e and Candito (2008) and Seddah et al. (2010) show that, given a corpus comparable in size and properties (i.e. the number of tokens and grammar size), the performance level, both for Charniak’s parser (Charniak, 2000) and the Berke3 ley parser (Petrov et al., 2006) was higher for parsing the PTB than it was for French. The split-mergesmooth implementation of (Petrov et al., 2006) consistently outperform various lexicalized and unlexicalized models for French (Seddah et al., 2009) and for many other languages (Petrov and Klein, 2007). In this respect, (Petrov et al., 2006) is considered MRL-friendly, due to its language agnostic design. The rise of dependency parsing: It is commonly assumed that dependency structures are better suited for representing the syntactic structures of free word order, morphologically rich, languages, because this representatio"
W10-1401,D07-1066,1,0.534932,"Missing"
W10-1401,P81-1022,0,0.830446,"Missing"
W10-1401,W07-2219,1,0.909816,"Missing"
W10-1401,C08-1112,1,0.709478,"Missing"
W10-1401,W10-1405,1,0.846235,"Missing"
W10-1401,W09-3820,1,0.856092,"e other ones, too. 4.4 Estimation and Smoothing: Coping with Lexical Sparsity Morphological word form variation augments the vocabulary size and thus worsens the problem of lexical data sparseness. Words occurring with mediumfrequency receive less reliable estimates, and the number of rare/unknown words is increased. One way to cope with the one of both aspects of this problem is through clustering, that is, providing an abstract representation over word forms that reflects their shared morphological and morphosyntactic aspects. This was done, for instance, in previous work on parsing German. Versley and Rehbein (2009) cluster words according to linear context features. These clusters include valency information added to verbs and morphological features such as case and number added to pre-terminal nodes. The clusters are then integrated as features in a discriminative parsing model to cope with unknown words. Their discriminative model thus obtains state-of-the-art results on parsing German. 8 Several contribution address similar challenges. For constituency-based generative parsers, the simple technique of replacing word forms with more abstract symbols is investigated by (Seddah et al., 2010; Candito and"
W10-1401,W10-1411,0,\N,Missing
W10-1401,W10-1403,0,\N,Missing
W10-1401,W10-1410,1,\N,Missing
W10-1401,W08-1008,0,\N,Missing
W10-1401,P05-1022,0,\N,Missing
W10-1401,P08-1043,1,\N,Missing
W10-1401,D07-1096,0,\N,Missing
W10-1409,P08-1037,0,0.0208222,"actic clustering to improve transition-based dependency parsing for English : using an available 30 million word corpus parsed with a constituency parser, words are represented as vectors of paths within the obtained constituency parses. Words are then clustered using a similarity metric between vectors of syntactic paths. The clusters are used as features to help a transition-based dependency parser. Note that the word representation for clustering is more complex (paths in parse trees), thus these authors have to cluster a smaller vocabulary : the top 5000 most frequent words are clustered. Agirre et al. (2008) use the same approach of replacing words by more general symbols, but these symbols are semantic classes. They test various methods to assign semantic classes (gold semantic class, most-frequent sense in sense-tagged data, or a fully unsupervised sense tagger). Though the method is very appealing, the reported improvement in parsing is rather small, especially for the fully unsupervised method. Versley and Rehbein (2009) cluster words according to linear context features, and use the clusters as features to boost discriminative German parsing for unknown words. Another approach to augment 83"
W10-1409,J92-4003,0,0.184561,"m Entropy training to learn PM and PL , we use the M ORFETTE models described in (Seddah et al., 2010), that are trained using the Averaged Sequence Perceptron algorithm (Freund and Schapire, 1999). The two classification models incorporate additional features calculated using the Lefff lexicon. Table 1 shows detailed results on dev set and test set of the F TB - UC, when M ORFETTE is trained on the F TB - UC training set. To the best of our knowledge the parts-of-speech tagging performance is state-of-the-art for French3 and the lemmatization performance has no comparable results. We use the Brown et al. (1992) hard clustering algorithm, which has proven useful for various NLP tasks such as dependency parsing (Koo et al., 2008) and named entity recognition (Liang, 2005). The algorithm to obtain C clusters is as follows: each of the C most frequent tokens of the corpus is assigned its own distinct cluster. For the (C + 1)th most frequent token, create a (C + 1)th cluster. Then for each pair among the C + 1 resulting clusters, merge the pair that minimizes the loss in the likelihood of the corpus, according to a bigram language model defined on the clusters. Repeat this operation for the (C + 2)th mos"
W10-1409,W09-3821,1,0.669433,"leads to better probability estimates for these words. 1 Introduction Statistical parsing techniques have dramatically improved over the last 15 years, yet lexical data sparseness remains a critical problem. And the richer the morphology of a language, the sparser the treebankdriven lexicons will be for that language. Koo et al. (2008) have proposed to use word clusters as features to improve graph-based statistical dependency parsing for English and Czech. Their clusters are obtained using unsupervised clustering, which makes it possible to use a raw corpus containing several million words. Candito and Crabbé (2009) applied clustering to generative constituency parsing for French. They use a desinflection step that removes some inflection marks from word forms and then replaces them with word clusters, resulting in a significant improvement in parsing performance. Clustering words seems useful as a way of addressing the lexical data sparseness problem, since counts on clusters are more reliable and lead to better probability estimates. Clustering also appears to address the mismatch of vocabularies between the original treebank and any external, potentially out-of-domain corpus: clusters operate as an in"
W10-1409,candito-etal-2010-statistical,1,0.0468684,"al symbols. F1 <40 is the F-Measure combining labeled precision and labeled recall for sentences of less than 40 words. All other metrics are for all sentences of the dev set/test set. UAS = Unlabeled attachement score of converted constituency trees into surface dependency trees. All metrics ignore punctuation tokens. recall) both for sentences of less than 40 words, and for all sentences7 . We also use the unlabeled attachment score (UAS), obtained when converting the constituency trees output by the B KY parsers into surface dependency trees, using the conversion procedure and software of (Candito et al., 2010)8 . Punctuation tokens are ignored in all metrics. 7 Discussion Results are shown in table 3. Our hope was that using lemmatization would improve overall accuracy of unsupervised clustering, hence leading to better parsing performance. However, results using both methods are comparable. 7 Note that often for statistical constituent parsing results are given for sentences of less than 40 words, whereas for dependency parsing, there is no such limitation. The experiment D FL and D FL +C LUST &gt;200 are reproduced from the previous work (Candito and Crabbé, 2009). More precisely, this previous work"
W10-1409,chrupala-etal-2008-learning,0,0.0884213,"Missing"
W10-1409,Y09-1013,0,0.0212872,"Missing"
W10-1409,E09-1038,0,0.0872854,"ls are semantic classes. They test various methods to assign semantic classes (gold semantic class, most-frequent sense in sense-tagged data, or a fully unsupervised sense tagger). Though the method is very appealing, the reported improvement in parsing is rather small, especially for the fully unsupervised method. Versley and Rehbein (2009) cluster words according to linear context features, and use the clusters as features to boost discriminative German parsing for unknown words. Another approach to augment 83 the known vocabulary for a generative probabilistic parser is the one pursued in (Goldberg et al., 2009). Within a plain PCFG, the lexical probabilities for words that are rare or absent in the treebank are taken from an external lexical probability distribution, estimated using a lexicon and the Baulm-Welch training of an HMM tagger. This is proven useful to better parse Hebrew. 9 Conclusion and future work We have provided a thorough study of the results of parsing word clusters for French. We showed that the clustering improves performance both for unseen and rare words and for medium- to highfrequency words. For French, preprocessing words with desinflection or with tagging+lemmatisation lea"
W10-1409,P03-1054,0,0.0192231,"Missing"
W10-1409,P08-1068,0,0.684428,"e for words that are originally either unknown or low-frequency, since these words are replaced by cluster symbols that tend to have higher frequencies. Furthermore, clustering also helps significantly for medium to high frequency words, suggesting that training on word clusters leads to better probability estimates for these words. 1 Introduction Statistical parsing techniques have dramatically improved over the last 15 years, yet lexical data sparseness remains a critical problem. And the richer the morphology of a language, the sparser the treebankdriven lexicons will be for that language. Koo et al. (2008) have proposed to use word clusters as features to improve graph-based statistical dependency parsing for English and Czech. Their clusters are obtained using unsupervised clustering, which makes it possible to use a raw corpus containing several million words. Candito and Crabbé (2009) applied clustering to generative constituency parsing for French. They use a desinflection step that removes some inflection marks from word forms and then replaces them with word clusters, resulting in a significant improvement in parsing performance. Clustering words seems useful as a way of addressing the le"
W10-1409,P95-1037,0,0.554961,"Missing"
W10-1409,P05-1010,0,0.0214152,"B KY, which is a constituent parser that has been proven to perform well for French (Crabbé and Candito, 2008; Seddah et al., 2009), 1 More precisely the partition is : first 1235 sentences for test, next 1235 sentences for development, and remaining 9881 sentences for training. 77 though a little lower than a combination of a tagger plus the dependency-based MST parser (Candito et al., 2010). Though PCFG-style parsers operate on too narrow a domain of locality, splitting symbols according to structural and/or lexical properties is known to help parsing (Klein and Manning., 2003). Following (Matsuzaki et al., 2005), the B KY algorithm uses EM to estimate probabilities on symbols that are automatically augmented with latent annotations, a process which can be viewed as symbol splitting. It iteratively evaluates each such split and merges back the less beneficial ones. Crabbé and Candito (2008) show that some of the information carried by the latent annotations is lexical, since replacing words by their gold part-of-speech tag leads to worse results than the corresponding perfect tagging test, with words unchanged. This is a clear indication that lexical distinctions are used, and percolate up the parse t"
W10-1409,P06-1055,0,0.0214456,"Missing"
W10-1409,W09-3829,0,0.0844018,".60 97.21 8.73 89.67 90.07 Table 5: Tagging accuracy and UAS scores for modified terminal symbols in the dev set, grouped by ranges of frequencies in the modified training sets. The “replaced by UNKC*” line corresponds to the case where the desinflected form or the POS+lemma pair does not appear more than 200 times in the L’est Républicain corpus. unsupervised Brown clustering, which uses very local information, the higher counts lead to better estimates even for high-frequency words. 8 Related work We have already cited the previous work of Koo et al. (2008) which has directly inspired ours. Sagae and Gordon (2009) explores the use of syntactic clustering to improve transition-based dependency parsing for English : using an available 30 million word corpus parsed with a constituency parser, words are represented as vectors of paths within the obtained constituency parses. Words are then clustered using a similarity metric between vectors of syntactic paths. The clusters are used as features to help a transition-based dependency parser. Note that the word representation for clustering is more complex (paths in parse trees), thus these authors have to cluster a smaller vocabulary : the top 5000 most frequ"
W10-1409,sagot-2010-lefff,0,0.0105068,"method, without resorting to part-ofspeech tagging. We propose an alternate method here, which uses lemmas and part-of-speech tags that are output by a tagger/lemmatizer. Because counts on lemmas are more reliable, clustering over lemmas presumably produces clusters that are more reliable than those produced by clustering over desinflected forms. However, this approach does create a constraint in which automatically tagged and lemmatized text is required as input to the parser, leading to the introduction of tagging errors. Both morphological clustering methods make use of the Lefff lexicon (Sagot, 2010). Before we describe these two methods, we briefly give basic information on French inflectional morphology and on the Lefff. 4.1 French inflection and the Lefff lexicon French nouns appear in singular and plural forms, and have an intrinsic gender. The number and gender of a noun determines the number and gender of determiners, adjectives, past participles that depend on it. Hence in the general case, past participles and adjectives have four different forms. The major inflectional variation appears for finite verbs that vary for tense, mood, person and number. A regular verb may correspond t"
W10-1409,P81-1022,0,0.817009,"Missing"
W10-1409,W09-3820,0,0.0944092,"word representation for clustering is more complex (paths in parse trees), thus these authors have to cluster a smaller vocabulary : the top 5000 most frequent words are clustered. Agirre et al. (2008) use the same approach of replacing words by more general symbols, but these symbols are semantic classes. They test various methods to assign semantic classes (gold semantic class, most-frequent sense in sense-tagged data, or a fully unsupervised sense tagger). Though the method is very appealing, the reported improvement in parsing is rather small, especially for the fully unsupervised method. Versley and Rehbein (2009) cluster words according to linear context features, and use the clusters as features to boost discriminative German parsing for unknown words. Another approach to augment 83 the known vocabulary for a generative probabilistic parser is the one pursued in (Goldberg et al., 2009). Within a plain PCFG, the lexical probabilities for words that are rare or absent in the treebank are taken from an external lexical probability distribution, estimated using a lexicon and the Baulm-Welch training of an HMM tagger. This is proven useful to better parse Hebrew. 9 Conclusion and future work We have provi"
W10-1409,W10-1410,1,\N,Missing
W10-1410,P04-1041,1,0.868231,"Missing"
W10-1410,W09-3821,1,0.89873,"Missing"
W10-1410,W09-1008,1,0.89151,"Missing"
W10-1410,A00-2018,0,0.735055,"rrections (referred to as the Modified French Treebank MFT) to support grammar acquisition for PCFG-based LFG Parsing (Cahill et al., 2004) while Crabbé and Candito (2008) slightly modified the original F TB POS tagset to optimize the grammar with latent annotations extracted by the Berkeley parser (B KY, (Petrov et al., 2006)). Moreover, research oriented towards adapting more complex parsing models to French showed that lexicalized models such as Collins’ model 2 (Collins, 1999) can be tuned to cope effectively with the flatness of the annotation scheme in the F TB, with the Charniak model (Charniak, 2000) performing particularly well, but outperformed by the B KY parser on French data (Seddah et al., 2009). Focusing on the lexicon, experiments have been carried out to study the impact of different forms of word clustering on the B KY parser trained on the F TB. Candito et al. (2009) showed that using gold lemmatization provides a significant increase in performance. Obviously, less sparse lexical data which retains critical pieces of information can only help a model to perform better. This was shown in (Candito and Crabbé, 2009) where distributional word clusters were acquired from a 125 mill"
W10-1410,chrupala-etal-2008-learning,1,0.815209,"Missing"
W10-1410,Y09-1013,0,0.14934,"Missing"
W10-1410,C94-2149,0,0.129478,"Missing"
W10-1410,E09-1038,0,0.139265,"Missing"
W10-1410,P06-1055,0,0.345326,"Missing"
W10-1410,D07-1066,1,0.886037,"Missing"
W10-1410,sagot-etal-2006-lefff,0,0.0720109,"Missing"
W10-1410,W09-3820,0,0.130258,"Missing"
W10-1410,J93-2004,0,\N,Missing
W10-1410,J03-4003,0,\N,Missing
W11-2905,W01-0521,0,0.149561,"Missing"
W11-2905,W10-1408,0,0.0471068,"Missing"
W11-2905,P08-1068,0,0.258581,"Missing"
W11-2905,J92-4003,0,0.155403,"learn a grammar from the word-clustered sentences in the training set; (iii) parse the word-clustered sentences in the test set; (iv) reintroduce the original tokens into the test sentences to obtain the final parsed output. The clustering is performed in two steps: (i) a morphological clustering is applied using the Lefff morphological lexicon (Sagot et al., 2006), where plural and feminine suffixes are removed from word forms and past/future tenses are mapped to present tense (provided this does not change the part-of-speech ambiguity of the form); (ii) an unsupervised clustering algorithm (Brown et al., 1992) is run on a large unlabeled corpus to learn clusters over the desinflected forms. Both clustering steps proved to be beneficial for parsing in-domain French text using the Berkeley parser. We apply a similar unsupervised word clustering technique to lexical domain adaptation, with the difference being that clusters are learned over a mixture of source-domain and target-domain text (hereafter mixed clusters). We test this technique when training a parser on the F TB training set as well as in self-training mode (McClosky and Charniak, 2008), where the parser is trained on both the source-domai"
W11-2905,P08-2026,0,0.575521,"guity of the form); (ii) an unsupervised clustering algorithm (Brown et al., 1992) is run on a large unlabeled corpus to learn clusters over the desinflected forms. Both clustering steps proved to be beneficial for parsing in-domain French text using the Berkeley parser. We apply a similar unsupervised word clustering technique to lexical domain adaptation, with the difference being that clusters are learned over a mixture of source-domain and target-domain text (hereafter mixed clusters). We test this technique when training a parser on the F TB training set as well as in self-training mode (McClosky and Charniak, 2008), where the parser is trained on both the source-domain training set and automatically parsed sentences from the target domain. 4 Symbols raw dfl clt-er clt-er-emea F-Measure on EMEA test set (≤ 40) No self-training 200k self-training 81.25 84.75 81.82 84.72 82.65 85.09 83.53 85.19 Table 2: F-Measure for sentences ≤ 40 tokens on the EMEA test set, both with self-training (200k autoparsed sentences from EmeaFrU) and without. freely available at CNRTL7 . Though this newspaper is less formal than Le Monde, it is still journalistic, so we consider it as being in the source domain. The mixed cluste"
W11-2905,W09-3821,1,0.958802,"the corpus, as many sentences provide general information or recommendations that are repeated in every EPAR document. In the end, the resulting preprocessed corpus (hereafter EmeaFrU) contains approximately 5.3 million tokens and 267 thousand sentences. 3 Lexical Domain Adaptation In our approach to domain adaption, we use unsupervised word clustering performed on a mixture of target-domain (biomedical) and source-domain (journalistic) text. The objective is to obtain clusters grouping together source-domain and targetdomain words, thus bridging the two vocabularies. We build on the work of Candito and Crabbé (2009), who proposed a technique to improve indomain parsing by reducing lexical data sparse2.3 Manual Bracketing Annotation To evaluate parsing performance, we manually annotated two extracts of the EmeaFrU corpus, cor3 Dev Set 574 16.2 9,346 Table 1: Statistics on the EMEA dev and test sets. alpha-lc stands for tokens converted to lowercase and containing at least one letter. Unknown tokens/types are those absent from the F TB training set. 2.2 Corpus Preprocessing 2 Test Set 544 21.5 11,679 4 We plan to make the manually-annotated corpus freely available, following a final validation step. Docume"
W11-2905,P06-1043,0,0.168543,"Missing"
W11-2905,N03-4009,0,0.0825179,"Missing"
W11-2905,C10-2013,1,0.909869,"Missing"
W11-2905,W07-2204,1,0.915983,"Missing"
W11-2905,N10-1060,0,0.231922,"Missing"
W11-2905,W10-2606,0,0.0467208,"set (Gildea, 2001; McClosky et al., 2006; Foster, 2010). However, the gap between this intrinsic evaluation methodology, which is only able to provide a ranking of some parser/treebank pairs using a given metric, and the growing need for accurate wide coverage parsers suitable for coping with an unlimited stream of new data, is currently being tackled more widely. Thus, the task of parsing out-of-domain text becomes crucial. Various techniques have been proposed to adapt existing parsing models to new genres: domain adaptation via self training (Bacchiani et al., 2006; McClosky et al., 2006; Sagae, 2010), co-training (Steedman et al., 2003), treebank and target transformation (Foster, 2010), source-domain target 2 Target Domain Corpus For our work on domain adaptation, we used the French Treebank (F TB) (Abeillé and Barrier, 2004) as the source domain corpus, which consists of 12,351 sentences from the Le Monde newspaper. For the target domain, we used biomedical texts from the European Medicines Agency, specifically the French part of the EMEA section1 of the OPUS corpus (Tiedemann, 2009). Although we chose the biomedical domain for this paper, our approach can be used for any target domain."
W11-2905,sagot-etal-2006-lefff,0,0.0230839,"Missing"
W11-2905,A97-1015,0,0.183391,"Missing"
W11-2905,N03-1031,0,0.175872,"y et al., 2006; Foster, 2010). However, the gap between this intrinsic evaluation methodology, which is only able to provide a ranking of some parser/treebank pairs using a given metric, and the growing need for accurate wide coverage parsers suitable for coping with an unlimited stream of new data, is currently being tackled more widely. Thus, the task of parsing out-of-domain text becomes crucial. Various techniques have been proposed to adapt existing parsing models to new genres: domain adaptation via self training (Bacchiani et al., 2006; McClosky et al., 2006; Sagae, 2010), co-training (Steedman et al., 2003), treebank and target transformation (Foster, 2010), source-domain target 2 Target Domain Corpus For our work on domain adaptation, we used the French Treebank (F TB) (Abeillé and Barrier, 2004) as the source domain corpus, which consists of 12,351 sentences from the Le Monde newspaper. For the target domain, we used biomedical texts from the European Medicines Agency, specifically the French part of the EMEA section1 of the OPUS corpus (Tiedemann, 2009). Although we chose the biomedical domain for this paper, our approach can be used for any target domain. 2.1 Corpus Characteristics The EMEA"
W11-2905,N07-1051,0,\N,Missing
W11-2905,I05-1006,0,\N,Missing
W11-2905,D10-1069,0,\N,Missing
W11-2905,W10-1409,1,\N,Missing
W11-2905,abeille-barrier-2004-enriching,0,\N,Missing
W12-3401,abeille-barrier-2004-enriching,0,0.0167263,"Missing"
W12-3401,P08-1037,0,0.0174213,"main setting. 6.1 Results in Constituency-Based Parsing The use of word classes for parsing dates back to the first works on generative constituency-based parsing, whether using semantic classes obtained from hand-built resources or less-informed classes created automatically. Bikel (2000) tried incorporating WordNet-based word sense disambiguation into a parser, but failed to obtain an improvement. Xiong et al. (2005) generalized bilexical dependencies in a generative parsing model using Chinese semantic resources (CiLin and HowNet), obtaining improvements for Chinese parsing. More recently, Agirre et al. (2008) show that replacing words with WordNet semantic classes improves English generative parsing. Lin et al. (2009) use the HowNet resource within the split-merge PCFG framework (Petrov et al., 2006) for Chinese parsing: they use the firstsense heuristic to append the most general hypernym to the POS of a token, obtaining a semanticallyinformed symbol refinement, and then guide further symbol splits using the HowNet hierarchy. Other work has used less-informed classes, notably unsupervised word clusters. Candito and Crabb´e (2009) use Brown clusters to replace words in a generative PCFG-LA framewo"
W12-3401,P11-2123,0,0.054594,"mmas, clusters, or synsets. Probabilistic lexical information is introduced into parser feature vectors by modifying the weights of lexical features. We obtain improvements in parsing accuracy with some lexical generalization configurations in experiments run on the French Treebank and two out-of-domain treebanks, with slightly better performance for the probabilistic lexical generalization approach compared to the standard single-mapping approach. 1 Past approaches for achieving lexical generalization in dependency parsing have used WordNet semantic senses in parsing experiments for English (Agirre et al., 2011), and word clustering over large corpora in parsing experiments for English (Koo et al., 2008) as well as for French (Candito et al., 2010b). These approaches map each word to a single corresponding generalized class (synset or cluster), and integrate generalized classes into parsing models in one of two ways: (i) the replacement strategy, where each word form is simply replaced with a corresponding generalized class; (ii) a strategy where an additional feature is created for the corresponding generalized class. Introduction In statistical, data-driven approaches to natural language syntactic"
W12-3401,W00-1320,0,0.0532964,"dical domain do both objectives appear to be fulfilled, as evidenced by our LAS improvements when parsing EMEA with lexical generalization. 6 Related Work We now discuss previous work concerning the use of lexical generalization for parsing, both in the classic in-domain setting and in the more recently popular out-of-domain setting. 6.1 Results in Constituency-Based Parsing The use of word classes for parsing dates back to the first works on generative constituency-based parsing, whether using semantic classes obtained from hand-built resources or less-informed classes created automatically. Bikel (2000) tried incorporating WordNet-based word sense disambiguation into a parser, but failed to obtain an improvement. Xiong et al. (2005) generalized bilexical dependencies in a generative parsing model using Chinese semantic resources (CiLin and HowNet), obtaining improvements for Chinese parsing. More recently, Agirre et al. (2008) show that replacing words with WordNet semantic classes improves English generative parsing. Lin et al. (2009) use the HowNet resource within the split-merge PCFG framework (Petrov et al., 2006) for Chinese parsing: they use the firstsense heuristic to append the most"
W12-3401,J92-4003,0,0.137867,"8), who tested the use of probabilistic part-ofspeech (POS) tags through an NLP pipeline. In this paper, we perform experiments for French that use the replacement strategy for integrating generalized classes into parsing models, comparing the single-mapping approach for lexical generalization with our probabilistic lexical generalization approach. In doing so, we provide first results on the application to French parsing of WordNet automatic sense ranking (ASR), using the method of McCarthy et al. (2004). For clustering we deviate from most previous work, which has integrated Brown clusters (Brown et al., 1992) into parsing models, and instead use distributional lexical semantics to create both a distributional thesaurus - for probabilistic generalization in the lemma space and ASR calculation and to perform hierarchical agglomerative clustering (HAC). Though unlexicalized syntactic HAC clustering has been used to improve English dependency parsing (Sagae and Gordon, 2009), we provide first results on using distributional lexical semantics for French parsing. We also include an out-of-domain evaluation on medical and parliamentary text in addition to an in-domain evaluation. In Section 2 we describe"
W12-3401,D08-1070,0,0.0166942,"distribution over a lexical target space of generalized classes, for which we consider the spaces of lemmas, synsets, and clusters. The standard single-mapping approach from previous work can be seen as a subcase: each categorical distribution assigns a probability of 1 to a single generalized class. The method 1 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 1–11, c Jeju, Republic of Korea, 12 July 2012. 2012 Association for Computational Linguistics we use for introducing probabilistic information into a feature vector is based on that used by Bunescu (2008), who tested the use of probabilistic part-ofspeech (POS) tags through an NLP pipeline. In this paper, we perform experiments for French that use the replacement strategy for integrating generalized classes into parsing models, comparing the single-mapping approach for lexical generalization with our probabilistic lexical generalization approach. In doing so, we provide first results on the application to French parsing of WordNet automatic sense ranking (ASR), using the method of McCarthy et al. (2004). For clustering we deviate from most previous work, which has integrated Brown clusters (Br"
W12-3401,W09-3821,1,0.89441,"Missing"
W12-3401,F12-2024,1,0.860785,"Missing"
W12-3401,candito-etal-2010-statistical,1,0.899228,"Missing"
W12-3401,C10-2013,1,0.919607,"l features. We obtain improvements in parsing accuracy with some lexical generalization configurations in experiments run on the French Treebank and two out-of-domain treebanks, with slightly better performance for the probabilistic lexical generalization approach compared to the standard single-mapping approach. 1 Past approaches for achieving lexical generalization in dependency parsing have used WordNet semantic senses in parsing experiments for English (Agirre et al., 2011), and word clustering over large corpora in parsing experiments for English (Koo et al., 2008) as well as for French (Candito et al., 2010b). These approaches map each word to a single corresponding generalized class (synset or cluster), and integrate generalized classes into parsing models in one of two ways: (i) the replacement strategy, where each word form is simply replaced with a corresponding generalized class; (ii) a strategy where an additional feature is created for the corresponding generalized class. Introduction In statistical, data-driven approaches to natural language syntactic parsing, a central problem is that of accurately modeling lexical relationships from potentially sparse counts within a training corpus. O"
W12-3401,W11-2905,1,0.845905,"plicate the improvements for English using semantic sense information (Agirre et al., 2011) or word clustering (Sagae and Gordon, 2009). The primary difference between our paper and previous work, though, is our evaluation of a novel probabilistic approach for lexical generalization. 6.3 Out-Of-Domain Parsing Concerning techniques for improving out-ofdomain parsing, a related approach has been to use self-training with auto-parsed out-of-domain data, as McClosky and Charniak (2008) do for English constituency parsing, though in that approach lexical generalization is not explicitly performed. Candito et al. (2011) use word clustering for domain adaptation of a PCFG-LA parser for French, deriving clusters from a corpus containing text from both the source and target domains, and they obtain parsing improvements in both domains. We are not aware of previous work on the use of lexical generalization for improving out-of-domain dependency parsing. 7 Conclusion We have investigated the use of probabilistic lexical target spaces for reducing lexical data sparseness in a transition-based dependency parser for French. We built a distributional thesaurus from an automatically-parsed large text corpus, using it"
W12-3401,Y09-1013,0,0.0204013,", and a parliamentary domain treebank containing 561 test sentences from the Europarl3 corpus. 4.2 Parser and Baseline Settings We use our own Python implementation of the arceager algorithm for transition-based parsing, based on the arc-eager setting of MaltParser (Nivre et al., 2007), and we train using the standard FTB training set. Our baseline feature templates and general settings correspond to those obtained in a benchmarking of parsers for French (Candito et al., 2010b), under the setting which combined lemmas and morphological features.4 Automatic POS-tagging is performed using MElt (Denis and Sagot, 2009), and lemmatization and morphological analysis are performed using the Lefff lexicon (Sagot, 2010). Table 1 lists our baseline parser’s feature templates. 4.3 Lexical Resource Construction We now describe the construction of our probabilistic lexical target space resources, whose prerequisites include the automatic parsing of a large corpus, the construction of a distributional thesaurus, the use of ASR on WordNet synsets, and the use of HAC clustering. 4.3.1 Automatically-Parsed Corpus The text corpus we use consists of 125 million words from the L’Est Republicain newspaper5 , 125 million wor"
W12-3401,O97-1002,0,0.155994,"Missing"
W12-3401,P08-1068,0,0.220952,"ectors by modifying the weights of lexical features. We obtain improvements in parsing accuracy with some lexical generalization configurations in experiments run on the French Treebank and two out-of-domain treebanks, with slightly better performance for the probabilistic lexical generalization approach compared to the standard single-mapping approach. 1 Past approaches for achieving lexical generalization in dependency parsing have used WordNet semantic senses in parsing experiments for English (Agirre et al., 2011), and word clustering over large corpora in parsing experiments for English (Koo et al., 2008) as well as for French (Candito et al., 2010b). These approaches map each word to a single corresponding generalized class (synset or cluster), and integrate generalized classes into parsing models in one of two ways: (i) the replacement strategy, where each word form is simply replaced with a corresponding generalized class; (ii) a strategy where an additional feature is created for the corresponding generalized class. Introduction In statistical, data-driven approaches to natural language syntactic parsing, a central problem is that of accurately modeling lexical relationships from potential"
W12-3401,D09-1135,0,0.0127277,"t works on generative constituency-based parsing, whether using semantic classes obtained from hand-built resources or less-informed classes created automatically. Bikel (2000) tried incorporating WordNet-based word sense disambiguation into a parser, but failed to obtain an improvement. Xiong et al. (2005) generalized bilexical dependencies in a generative parsing model using Chinese semantic resources (CiLin and HowNet), obtaining improvements for Chinese parsing. More recently, Agirre et al. (2008) show that replacing words with WordNet semantic classes improves English generative parsing. Lin et al. (2009) use the HowNet resource within the split-merge PCFG framework (Petrov et al., 2006) for Chinese parsing: they use the firstsense heuristic to append the most general hypernym to the POS of a token, obtaining a semanticallyinformed symbol refinement, and then guide further symbol splits using the HowNet hierarchy. Other work has used less-informed classes, notably unsupervised word clusters. Candito and Crabb´e (2009) use Brown clusters to replace words in a generative PCFG-LA framework, obtaining substantial parsing improvements for French. 6.2 Results in Dependency Parsing In dependency pars"
W12-3401,P98-2127,0,0.191661,"tag, t = fine POS tag, w = inflected word form, l = lemma, d = dependency label, mi = morphological feature from set M . For tokens, ni = ith token in the buffer, si = ith token on the stack. The token subscripts l, r, and h denote partially-constructed syntactic left-most dependent, right-most dependent, and head, respectively. preprocessed using the Bonsai tool7 , and parsed using our baseline parser. 4.3.2 Distributional Thesaurus We build separate distributional thesauri for nouns and for verbs,8 using straightforward methods in distributional lexical semantics based primarily on work by Lin (1998) and Curran (2004). We use the FreDist tool (Henestroza Anguiano and Denis, 2011) for thesaurus creation. First, syntactic contexts for each lemma are extracted from the corpus. We use all syntactic dependencies in which the secondary token has an open-class POS tag, with labels included in the contexts and two-edge dependencies used in the case of prepositional-phrase attachment and coordination. Example contexts are shown in Figure 2. For verb lemmas we limit contexts to dependencies in which the verb is governor, and we add unlexicalized versions of contexts to account for subcategorization"
W12-3401,P04-1036,0,0.202848,"ics we use for introducing probabilistic information into a feature vector is based on that used by Bunescu (2008), who tested the use of probabilistic part-ofspeech (POS) tags through an NLP pipeline. In this paper, we perform experiments for French that use the replacement strategy for integrating generalized classes into parsing models, comparing the single-mapping approach for lexical generalization with our probabilistic lexical generalization approach. In doing so, we provide first results on the application to French parsing of WordNet automatic sense ranking (ASR), using the method of McCarthy et al. (2004). For clustering we deviate from most previous work, which has integrated Brown clusters (Brown et al., 1992) into parsing models, and instead use distributional lexical semantics to create both a distributional thesaurus - for probabilistic generalization in the lemma space and ASR calculation and to perform hierarchical agglomerative clustering (HAC). Though unlexicalized syntactic HAC clustering has been used to improve English dependency parsing (Sagae and Gordon, 2009), we provide first results on using distributional lexical semantics for French parsing. We also include an out-of-domain"
W12-3401,P08-2026,0,0.0217052,"h, past improvements for indomain English dependency parsing with generalized lexical classes. Unfortunately, our results for French do not replicate the improvements for English using semantic sense information (Agirre et al., 2011) or word clustering (Sagae and Gordon, 2009). The primary difference between our paper and previous work, though, is our evaluation of a novel probabilistic approach for lexical generalization. 6.3 Out-Of-Domain Parsing Concerning techniques for improving out-ofdomain parsing, a related approach has been to use self-training with auto-parsed out-of-domain data, as McClosky and Charniak (2008) do for English constituency parsing, though in that approach lexical generalization is not explicitly performed. Candito et al. (2011) use word clustering for domain adaptation of a PCFG-LA parser for French, deriving clusters from a corpus containing text from both the source and target domains, and they obtain parsing improvements in both domains. We are not aware of previous work on the use of lexical generalization for improving out-of-domain dependency parsing. 7 Conclusion We have investigated the use of probabilistic lexical target spaces for reducing lexical data sparseness in a trans"
W12-3401,W06-2933,0,0.0176668,"ve multiple senses in Ωs , with probability mass functions pma and pav , then for each pair i, j we derive a feature of the form [xa =si ∧xb =sj ], with the following weighted indicator function:  pma (si )pav (sj ), if la =manger∧lb =avocat f (i,j) = 0, otherwise 3 In this paper we focus on transition-based parsing, whose seminal works are that of Yamada and Matsumoto (2003) and Nivre (2003). The parsing process applies a sequence of incremental actions, which typically manipulate a buffer position in the sentence and a stack for built sub-structures. In the arc-eager approach introduced by Nivre et al. (2006) the possible actions are as follows, with s0 being the token on top of the stack and n0 being the next token in the buffer: − SHIFT: Push n0 onto the stack. − REDUCE: Pop s0 from the stack. − RIGHT-ARC(r): Add an arc labeled r from s0 to n0 ; push n0 onto the stack. (8) − LEFT-ARC(r): Add an arc labeled r from n0 to s0 ; pop s0 from the stack. Dependency Parsing Dependency syntax involves the representation of syntactic information for a sentence in the form of a directed graph, whose edges encode word-to-word relationships. An edge from a governor to a dependent indicates, roughly, that the"
W12-3401,W03-3017,0,0.0508925,"pear ouvrit Elle 3.1 Transition-Based Parsing porte avec la cl´e la Figure 1: An unlabeled dependency tree for “Elle ouvrit la porte avec la cl´e” (“She opened the door with the key”). in our source vocabulary and have multiple senses in Ωs , with probability mass functions pma and pav , then for each pair i, j we derive a feature of the form [xa =si ∧xb =sj ], with the following weighted indicator function:  pma (si )pav (sj ), if la =manger∧lb =avocat f (i,j) = 0, otherwise 3 In this paper we focus on transition-based parsing, whose seminal works are that of Yamada and Matsumoto (2003) and Nivre (2003). The parsing process applies a sequence of incremental actions, which typically manipulate a buffer position in the sentence and a stack for built sub-structures. In the arc-eager approach introduced by Nivre et al. (2006) the possible actions are as follows, with s0 being the token on top of the stack and n0 being the next token in the buffer: − SHIFT: Push n0 onto the stack. − REDUCE: Pop s0 from the stack. − RIGHT-ARC(r): Add an arc labeled r from s0 to n0 ; push n0 onto the stack. (8) − LEFT-ARC(r): Add an arc labeled r from n0 to s0 ; pop s0 from the stack. Dependency Parsing Dependency"
W12-3401,P06-1055,0,0.018833,"obtained from hand-built resources or less-informed classes created automatically. Bikel (2000) tried incorporating WordNet-based word sense disambiguation into a parser, but failed to obtain an improvement. Xiong et al. (2005) generalized bilexical dependencies in a generative parsing model using Chinese semantic resources (CiLin and HowNet), obtaining improvements for Chinese parsing. More recently, Agirre et al. (2008) show that replacing words with WordNet semantic classes improves English generative parsing. Lin et al. (2009) use the HowNet resource within the split-merge PCFG framework (Petrov et al., 2006) for Chinese parsing: they use the firstsense heuristic to append the most general hypernym to the POS of a token, obtaining a semanticallyinformed symbol refinement, and then guide further symbol splits using the HowNet hierarchy. Other work has used less-informed classes, notably unsupervised word clusters. Candito and Crabb´e (2009) use Brown clusters to replace words in a generative PCFG-LA framework, obtaining substantial parsing improvements for French. 6.2 Results in Dependency Parsing In dependency parsing, word classes are integrated as features in underlying linear models. In a semin"
W12-3401,W09-3829,0,0.0131981,"e provide first results on the application to French parsing of WordNet automatic sense ranking (ASR), using the method of McCarthy et al. (2004). For clustering we deviate from most previous work, which has integrated Brown clusters (Brown et al., 1992) into parsing models, and instead use distributional lexical semantics to create both a distributional thesaurus - for probabilistic generalization in the lemma space and ASR calculation and to perform hierarchical agglomerative clustering (HAC). Though unlexicalized syntactic HAC clustering has been used to improve English dependency parsing (Sagae and Gordon, 2009), we provide first results on using distributional lexical semantics for French parsing. We also include an out-of-domain evaluation on medical and parliamentary text in addition to an in-domain evaluation. In Section 2 we describe the lexical target spaces used in this paper, as well as the method of integrating probabilistic lexical information into a feature vector for classification. In Section 3 we discuss dependency structure and transition-based parsing. In Section 4 we present the experimental setup, which includes our parser implementation, the construction of our probabilistic lexica"
W12-3401,sagot-2010-lefff,0,0.0500196,"d Baseline Settings We use our own Python implementation of the arceager algorithm for transition-based parsing, based on the arc-eager setting of MaltParser (Nivre et al., 2007), and we train using the standard FTB training set. Our baseline feature templates and general settings correspond to those obtained in a benchmarking of parsers for French (Candito et al., 2010b), under the setting which combined lemmas and morphological features.4 Automatic POS-tagging is performed using MElt (Denis and Sagot, 2009), and lemmatization and morphological analysis are performed using the Lefff lexicon (Sagot, 2010). Table 1 lists our baseline parser’s feature templates. 4.3 Lexical Resource Construction We now describe the construction of our probabilistic lexical target space resources, whose prerequisites include the automatic parsing of a large corpus, the construction of a distributional thesaurus, the use of ASR on WordNet synsets, and the use of HAC clustering. 4.3.1 Automatically-Parsed Corpus The text corpus we use consists of 125 million words from the L’Est Republicain newspaper5 , 125 million words of dispatches from the Agence France-Presse, and 225 million words from a French Wikipedia back"
W12-3401,I05-1007,0,0.0164769,"eneralization. 6 Related Work We now discuss previous work concerning the use of lexical generalization for parsing, both in the classic in-domain setting and in the more recently popular out-of-domain setting. 6.1 Results in Constituency-Based Parsing The use of word classes for parsing dates back to the first works on generative constituency-based parsing, whether using semantic classes obtained from hand-built resources or less-informed classes created automatically. Bikel (2000) tried incorporating WordNet-based word sense disambiguation into a parser, but failed to obtain an improvement. Xiong et al. (2005) generalized bilexical dependencies in a generative parsing model using Chinese semantic resources (CiLin and HowNet), obtaining improvements for Chinese parsing. More recently, Agirre et al. (2008) show that replacing words with WordNet semantic classes improves English generative parsing. Lin et al. (2009) use the HowNet resource within the split-merge PCFG framework (Petrov et al., 2006) for Chinese parsing: they use the firstsense heuristic to append the most general hypernym to the POS of a token, obtaining a semanticallyinformed symbol refinement, and then guide further symbol splits usi"
W12-3401,W03-3023,0,0.0375666,"both lemmas manger and avocat appear ouvrit Elle 3.1 Transition-Based Parsing porte avec la cl´e la Figure 1: An unlabeled dependency tree for “Elle ouvrit la porte avec la cl´e” (“She opened the door with the key”). in our source vocabulary and have multiple senses in Ωs , with probability mass functions pma and pav , then for each pair i, j we derive a feature of the form [xa =si ∧xb =sj ], with the following weighted indicator function:  pma (si )pav (sj ), if la =manger∧lb =avocat f (i,j) = 0, otherwise 3 In this paper we focus on transition-based parsing, whose seminal works are that of Yamada and Matsumoto (2003) and Nivre (2003). The parsing process applies a sequence of incremental actions, which typically manipulate a buffer position in the sentence and a stack for built sub-structures. In the arc-eager approach introduced by Nivre et al. (2006) the possible actions are as follows, with s0 being the token on top of the stack and n0 being the next token in the buffer: − SHIFT: Push n0 onto the stack. − REDUCE: Pop s0 from the stack. − RIGHT-ARC(r): Add an arc labeled r from s0 to n0 ; push n0 onto the stack. (8) − LEFT-ARC(r): Add an arc labeled r from n0 to s0 ; pop s0 from the stack. Dependency Pa"
W12-3401,C98-2122,0,\N,Missing
W13-4905,P05-1038,0,0.511442,"Missing"
W13-4905,D12-1133,0,0.0376615,"the pipeline systems is based on Conditional Random Fields (CRF) (Lafferty et al., 2001) and on external lexicons following (Constant and Tellier, 2012). Given a tokenized text, it jointly performs MWE segmentation and POS tagging (of simple tokens and of MWEs), both tasks mutually helping each other1 . CRF is a prominent statistical model for sequence segmenta1 Note though that we keep only the MWE segmentation, and use rather the Morfette tagger-lemmatizer, cf. section 4. 47 • Mate-tools 2, the joint POS tagger and transition-based parser with graph-based completion available in Mate-tools (Bohnet and Nivre, 2012). 2 We use the version available in the POS tagger MElt (Denis and Sagot, 2009). 3 We use the version in the platform Unitex (http://igm.univmlv.fr/˜unitex). We had to convert the DELA POS tagset to the FTB one. 4 http://igm.univ-mlv.fr/˜unitex 5 Available at http://code.google.com/p/mate-tools/. We used the Anna3.3 version. Such parsers require some preprocessing of the input text: lemmatization, POS tagging, morphology analyzer (except the joint POS tagger and transition-based parser that does not require preprocessed POS tagging). We competed for the scenario in which this information is no"
W13-4905,C10-1011,0,0.0317819,"ernor’s POS in the syntactic parse, the POS following the MWE, the POS preceding the MWE, the bigram of the POS following and preceding the MWE. e´ pargne avait ferm´e la 4 Dependency Parsers For our development, we trained 3 types of parsers, both for the pipeline and the joint architecture: veille Figure 1: French dependency tree for La caisse d’´epargne avait ferm´e la veille (The savings bank had closed the day before), containing two MWEs (in red). • MALT, a pure linear-complexity transitionbased parser (Nivre et al., 2006) • Mate-tools 1, the graph-based parser available in Mate-tools5 (Bohnet, 2010) 3 MWE Analyzer and MWE Tagger The MWE analyzer we used in the pipeline systems is based on Conditional Random Fields (CRF) (Lafferty et al., 2001) and on external lexicons following (Constant and Tellier, 2012). Given a tokenized text, it jointly performs MWE segmentation and POS tagging (of simple tokens and of MWEs), both tasks mutually helping each other1 . CRF is a prominent statistical model for sequence segmenta1 Note though that we keep only the MWE segmentation, and use rather the Morfette tagger-lemmatizer, cf. section 4. 47 • Mate-tools 2, the joint POS tagger and transition-based p"
W13-4905,C10-2013,1,0.853303,"relabeled ”dep cpd P”. At evaluation time, the output parse labels are remapped to the official annotation scheme. 8 More precisely, we based our implementation on the pseudo-code given in (McDonald, 2006). 9 http://igm.univ-mlv.fr/˜mconstan 48 plates. The MWE tagger model was trained using the Wapiti software(Lavergne et al., 2010). We used the default parameters and we forced the MaxEnt mode. Parsers For MALT (version 1.7.2), we used the arceager algorithm, and the liblinear library for training. As far as the features are concerned, we started with the feature templates given in Bonsai10 (Candito et al., 2010), and we added some templates (essentially lemma bigrams) during the development tests, that slightly improved performance. For the two Matetools parsers, we used the default feature sets and parameters proposed in the documentation. Morphological prediction Predicted lemmas, POS and morphology features are computed with Morfette version 0.3.5 (Chrupała et al., 2008; Seddah et al., 2010)11 , using 10 iterations for the tagging perceptron, 3 iterations for the lemmatization perceptron, default beam size for the decoding of the joint prediction, and the Lefff (Sagot, 2010) as external lexicon us"
W13-4905,constant-tellier-2012-evaluating,1,0.918112,"ation scheme. As shown in Figure 1, such trees contain not only syntactic dependencies, but also the grouping of tokens into MWEs, since the first component of an MWE bears dependencies to the subsequent components of the MWE with a specific label dep_cpd. At that stage, the only missing information is the POS of the MWEs, which we predict by applying a MWE tagger in a post-processing step. mo d d’ t de d cp d cp pc pd p p caisse de de la de au x tp s suj tion and labelling. External lexicons used as sources of features greatly improve POS tagging (Denis and Sagot, 2009) and MWE segmentation (Constant and Tellier, 2012). Our lexical resources are composed of two large-coverage general-language lexicons: the Lefff2 lexicon (Sagot, 2010), which contains approx. half a million inflected word forms, among which approx. 25, 000 are MWEs; and the DELA3 (Courtois, 2009; Courtois et al., 1997) lexicon, which contains approx. one million inflected forms, among which about 110, 000 are MWEs. These resources are completed with specific lexicons freely available in the platform Unitex4 : the toponym dictionary Prolex (Piton et al., 1999) and a dictionary of first names. The MWE tagger we used in the joint systems takes"
W13-4905,P12-1022,1,0.924909,"mance in tasks such as machine translation (Pal et al., 2011), there has been relatively little work exploiting MWE recognition to improve parsing performance. Indeed, a classical parsing scenario is to pregroup MWEs using gold MWE annotation (Arun Djam´e Seddah Alpage Paris Sorbonne Univ INRIA and Keller, 2005). This non-realistic scenario has been shown to help parsing (Nivre and Nilsson, 2004; Eryigit et al., 2011), but the situation is quite different when switching to automatic MWE prediction. In that case, errors in MWE recognition alleviate their positive effect on parsing performance (Constant et al., 2012). While the realistic scenario of syntactic parsing with automatic MWE recognition (either done jointly or in a pipeline) has already been investigated in constituency parsing (Cafferkey et al., 2007; Green et al., 2011; Constant et al., 2012; Green et al., 2013), the French dataset of the SPMRL 2013 Shared Task (Seddah et al., 2013) offers one of the first opportunities to evaluate this scenario within the framework of dependency syntax. In this paper, we discuss the systems we submitted to the SPMRL 2013 shared task. We focused our participation on the French dependency parsing track using t"
W13-4905,Y09-1013,0,0.259198,"tructure comply with the French dataset annotation scheme. As shown in Figure 1, such trees contain not only syntactic dependencies, but also the grouping of tokens into MWEs, since the first component of an MWE bears dependencies to the subsequent components of the MWE with a specific label dep_cpd. At that stage, the only missing information is the POS of the MWEs, which we predict by applying a MWE tagger in a post-processing step. mo d d’ t de d cp d cp pc pd p p caisse de de la de au x tp s suj tion and labelling. External lexicons used as sources of features greatly improve POS tagging (Denis and Sagot, 2009) and MWE segmentation (Constant and Tellier, 2012). Our lexical resources are composed of two large-coverage general-language lexicons: the Lefff2 lexicon (Sagot, 2010), which contains approx. half a million inflected word forms, among which approx. 25, 000 are MWEs; and the DELA3 (Courtois, 2009; Courtois et al., 1997) lexicon, which contains approx. one million inflected forms, among which about 110, 000 are MWEs. These resources are completed with specific lexicons freely available in the platform Unitex4 : the toponym dictionary Prolex (Piton et al., 1999) and a dictionary of first names."
W13-4905,C96-1058,0,0.0712235,"-lemmatizer Morfette (Chrupała et al., 2008; Seddah et al., 2010), in order to apply a jackknifing on the training set, so that parsers are made less sensitive to tagging errors. Note that no feature pertaining to MWEs are used at this stage. 5 Reparser The reparser is an adaptation to labeled dependency parsing of the simplest6 system proposed in (Sagae and Lavie, 2006). The principle is to build an arcfactored merge of the parses produced by n input parsers, and then to find the maximum spanning tree among the resulting merged graph7 . We implemented the maximum spanning tree algorithm8 of (Eisner, 1996) devoted to projective dependency parsing. During the parse merging, each arc is unlabeled, and is given a weight, which is the frequency it appears in the n input parses. Once the maximum spanning tree is found, each arc is labeled by its most voted label among the m input parses containing such an arc (with arbitrary choice in case of ties). 6 Experiments 6.1 Settings MWE Analysis and Tagging For the MWE analyzer, we used the tool lgtagger9 (version 1.1) with its default set of feature tem6 The other more complex systems were producing equivalent scores. 7 In order to account for labeled MWE"
W13-4905,W11-3806,0,0.126514,", English, or German, a large quantity of MWE resources have been generated (Baldwin and Nam, 2010). Yet, while special treatment of complex lexical units, such as MWEs, has been shown to boost performance in tasks such as machine translation (Pal et al., 2011), there has been relatively little work exploiting MWE recognition to improve parsing performance. Indeed, a classical parsing scenario is to pregroup MWEs using gold MWE annotation (Arun Djam´e Seddah Alpage Paris Sorbonne Univ INRIA and Keller, 2005). This non-realistic scenario has been shown to help parsing (Nivre and Nilsson, 2004; Eryigit et al., 2011), but the situation is quite different when switching to automatic MWE prediction. In that case, errors in MWE recognition alleviate their positive effect on parsing performance (Constant et al., 2012). While the realistic scenario of syntactic parsing with automatic MWE recognition (either done jointly or in a pipeline) has already been investigated in constituency parsing (Cafferkey et al., 2007; Green et al., 2011; Constant et al., 2012; Green et al., 2013), the French dataset of the SPMRL 2013 Shared Task (Seddah et al., 2013) offers one of the first opportunities to evaluate this scenario"
W13-4905,D11-1067,0,0.217349,"Missing"
W13-4905,J13-1009,0,0.219606,"Missing"
W13-4905,P10-1052,0,0.0698905,"Missing"
W13-4905,nivre-etal-2006-maltparser,0,0.0481328,"lemma, the sequence of POS of its components, the POS of its first component, its governor’s POS in the syntactic parse, the POS following the MWE, the POS preceding the MWE, the bigram of the POS following and preceding the MWE. e´ pargne avait ferm´e la 4 Dependency Parsers For our development, we trained 3 types of parsers, both for the pipeline and the joint architecture: veille Figure 1: French dependency tree for La caisse d’´epargne avait ferm´e la veille (The savings bank had closed the day before), containing two MWEs (in red). • MALT, a pure linear-complexity transitionbased parser (Nivre et al., 2006) • Mate-tools 1, the graph-based parser available in Mate-tools5 (Bohnet, 2010) 3 MWE Analyzer and MWE Tagger The MWE analyzer we used in the pipeline systems is based on Conditional Random Fields (CRF) (Lafferty et al., 2001) and on external lexicons following (Constant and Tellier, 2012). Given a tokenized text, it jointly performs MWE segmentation and POS tagging (of simple tokens and of MWEs), both tasks mutually helping each other1 . CRF is a prominent statistical model for sequence segmenta1 Note though that we keep only the MWE segmentation, and use rather the Morfette tagger-lemmatizer"
W13-4905,2011.mtsummit-papers.23,0,0.0370541,"both pipeline architecture (MWE recognition followed by parsing), and joint architecture (MWE recognition performed by the parser). 1 Introduction As shown by the remarkable permanence over the years of specialized workshops, multiword expressions (MWEs) identification is still receiving considerable attention. For some languages, such as Arabic, French, English, or German, a large quantity of MWE resources have been generated (Baldwin and Nam, 2010). Yet, while special treatment of complex lexical units, such as MWEs, has been shown to boost performance in tasks such as machine translation (Pal et al., 2011), there has been relatively little work exploiting MWE recognition to improve parsing performance. Indeed, a classical parsing scenario is to pregroup MWEs using gold MWE annotation (Arun Djam´e Seddah Alpage Paris Sorbonne Univ INRIA and Keller, 2005). This non-realistic scenario has been shown to help parsing (Nivre and Nilsson, 2004; Eryigit et al., 2011), but the situation is quite different when switching to automatic MWE prediction. In that case, errors in MWE recognition alleviate their positive effect on parsing performance (Constant et al., 2012). While the realistic scenario of synta"
W13-4905,N06-2033,0,0.0791526,"reprocessed POS tagging). We competed for the scenario in which this information is not gold but predicted. Instead of using the predicted POS, lemma and morphological features provided by the shared task organizers, we decided to retrain the tagger-lemmatizer Morfette (Chrupała et al., 2008; Seddah et al., 2010), in order to apply a jackknifing on the training set, so that parsers are made less sensitive to tagging errors. Note that no feature pertaining to MWEs are used at this stage. 5 Reparser The reparser is an adaptation to labeled dependency parsing of the simplest6 system proposed in (Sagae and Lavie, 2006). The principle is to build an arcfactored merge of the parses produced by n input parsers, and then to find the maximum spanning tree among the resulting merged graph7 . We implemented the maximum spanning tree algorithm8 of (Eisner, 1996) devoted to projective dependency parsing. During the parse merging, each arc is unlabeled, and is given a weight, which is the frequency it appears in the n input parses. Once the maximum spanning tree is found, each arc is labeled by its most voted label among the m input parses containing such an arc (with arbitrary choice in case of ties). 6 Experiments"
W13-4905,sagot-2010-lefff,0,0.124615,"nce the first component of an MWE bears dependencies to the subsequent components of the MWE with a specific label dep_cpd. At that stage, the only missing information is the POS of the MWEs, which we predict by applying a MWE tagger in a post-processing step. mo d d’ t de d cp d cp pc pd p p caisse de de la de au x tp s suj tion and labelling. External lexicons used as sources of features greatly improve POS tagging (Denis and Sagot, 2009) and MWE segmentation (Constant and Tellier, 2012). Our lexical resources are composed of two large-coverage general-language lexicons: the Lefff2 lexicon (Sagot, 2010), which contains approx. half a million inflected word forms, among which approx. 25, 000 are MWEs; and the DELA3 (Courtois, 2009; Courtois et al., 1997) lexicon, which contains approx. one million inflected forms, among which about 110, 000 are MWEs. These resources are completed with specific lexicons freely available in the platform Unitex4 : the toponym dictionary Prolex (Piton et al., 1999) and a dictionary of first names. The MWE tagger we used in the joint systems takes as input a MWE within a dependency tree, and outputs its POS. It is a pointwise classifier, based on a MaxEnt model th"
W13-4905,W10-1410,1,0.930496,"Missing"
W13-4905,chrupala-etal-2008-learning,0,\N,Missing
W13-4917,P06-1084,0,0.0139791,"s of incomplete lexicon coverage. The morphologically disambiguated input files for the Raw (1-best) scenario were produced by running the raw text through the morphological disam23 Note that this additional layer in the constituency treebank adds a relatively easy set of nodes to the trees, thus “inflating” the evaluation scores compared to previously reported results. To compensate, a stricter protocol than is used in this task would strip one of the two POS layers prior to evaluation. 24 This split is slightly different than the split in previous studies. 160 biguator (tagger) described in Adler and Elhadad (2006; Goldberg et al. (2008),Adler (2007). The disambiguator is based on the same lexicon that is used to produce the lattice files, but utilizes an extra module for dealing with unknown tokens Adler et al. (2008). The core of the disambiguator is an HMM tagger trained on about 70M unannotated tokens using EM, and being supervised by the lexicon. As in the case of Arabic, we also provided data for the Predicted (gold token / predicted morphology) scenario. We used the same sequence labeler, Morfette (Chrupała et al., 2008), trained on the concatenation of POS and morphological gold features, leadi"
W13-4917,P08-1083,1,0.743016,"in the constituency treebank adds a relatively easy set of nodes to the trees, thus “inflating” the evaluation scores compared to previously reported results. To compensate, a stricter protocol than is used in this task would strip one of the two POS layers prior to evaluation. 24 This split is slightly different than the split in previous studies. 160 biguator (tagger) described in Adler and Elhadad (2006; Goldberg et al. (2008),Adler (2007). The disambiguator is based on the same lexicon that is used to produce the lattice files, but utilizes an extra module for dealing with unknown tokens Adler et al. (2008). The core of the disambiguator is an HMM tagger trained on about 70M unannotated tokens using EM, and being supervised by the lexicon. As in the case of Arabic, we also provided data for the Predicted (gold token / predicted morphology) scenario. We used the same sequence labeler, Morfette (Chrupała et al., 2008), trained on the concatenation of POS and morphological gold features, leading to a model with respectable accuracy.25 4.7 The Hungarian Treebank Hungarian is an agglutinative language, thus a lemma can have hundreds of word forms due to derivational or inflectional affixation (nomina"
W13-4917,W13-4903,0,0.0228459,"such languages – are the word-based metrics used for English well-equipped to capture performance across frameworks, or performance in the face of morphological complexity? This event provoked active discussions and led to the establishment of a series of SPMRL events for the discussion of shared challenges and cross-fertilization among researchers working on parsing MRLs. The body of work on MRLs that was accumulated through the SPMRL workshops2 and hosting ACL venues contains new results for Arabic (Attia et al., 2010; Marton et al., 2013a), Basque (Bengoetxea and Gojenola, 2010), Croatian (Agic et al., 2013), French (Seddah et al., 2010; Candito and Seddah, 2010; Sigogne et al., 2011), German (Rehbein, 2011), Hebrew (Tsarfaty and Sima’an, 2010; Goldberg and 1 http://alpage.inria.fr/iwpt09/panel.en. html 2 See http://www.spmrl.org/ and related workshops. 148 Elhadad, 2010a), Hindi (Ambati et al., 2010), Korean (Chung et al., 2010; Choi and Palmer, 2011) and Spanish (Le Roux et al., 2012), Tamil (Green et al., 2012), amongst others. The awareness of the modeling challenges gave rise to new lines of work on topics such as joint morpho-syntactic processing (Goldberg and Tsarfaty, 2008), Relational-Re"
W13-4917,W10-1411,1,0.835873,"challenges and cross-fertilization among researchers working on parsing MRLs. The body of work on MRLs that was accumulated through the SPMRL workshops2 and hosting ACL venues contains new results for Arabic (Attia et al., 2010; Marton et al., 2013a), Basque (Bengoetxea and Gojenola, 2010), Croatian (Agic et al., 2013), French (Seddah et al., 2010; Candito and Seddah, 2010; Sigogne et al., 2011), German (Rehbein, 2011), Hebrew (Tsarfaty and Sima’an, 2010; Goldberg and 1 http://alpage.inria.fr/iwpt09/panel.en. html 2 See http://www.spmrl.org/ and related workshops. 148 Elhadad, 2010a), Hindi (Ambati et al., 2010), Korean (Chung et al., 2010; Choi and Palmer, 2011) and Spanish (Le Roux et al., 2012), Tamil (Green et al., 2012), amongst others. The awareness of the modeling challenges gave rise to new lines of work on topics such as joint morpho-syntactic processing (Goldberg and Tsarfaty, 2008), Relational-Realizational Parsing (Tsarfaty, 2010), EasyFirst Parsing (Goldberg, 2011), PLCFRS parsing (Kallmeyer and Maier, 2013), the use of factored lexica (Green et al., 2013), the use of bilingual data (Fraser et al., 2013), and more developments that are currently under way. With new models and data, and w"
W13-4917,W10-1408,1,0.383126,"Missing"
W13-4917,E12-2012,1,0.0774441,"parsing evaluation campaign SANCL 2012 (Petrov and McDonald, 2012). The present shared task was extremely demanding on our participants. From 30 individuals or teams who registered and obtained the data sets, we present results for the seven teams that accomplished successful executions on these data in the relevant scenarios in the given the time frame. 5.1 Dependency Track Seven teams participated in the dependency track. Two participating systems are based on MaltParser: M ALTOPTIMIZER (Ballesteros, 2013) and AI:KU (Cirik and Sensoy, ¸ 2013). M ALTOPTIMIZER uses a variant of MaltOptimizer (Ballesteros and Nivre, 2012) to explore features relevant for the processing of morphological information. AI:KU uses a combination of MaltParser and the original MaltOptimizer. Their system development has focused on the integration of an unsupervised word clustering method using contextual and morphological properties of the words, to help combat sparseness. Similarly to MaltParser A LPAGE :DYALOG (De La Clergerie, 2013) also uses a shift-reduce transition-based parser but its training and decoding algorithms are based on beam search. This parser is implemented on top of the tabular logic programming system DyALog. To"
W13-4917,W13-4907,0,0.0733412,"Missing"
W13-4917,W10-1404,0,0.0222482,"merged as to the evaluation of parsers in such languages – are the word-based metrics used for English well-equipped to capture performance across frameworks, or performance in the face of morphological complexity? This event provoked active discussions and led to the establishment of a series of SPMRL events for the discussion of shared challenges and cross-fertilization among researchers working on parsing MRLs. The body of work on MRLs that was accumulated through the SPMRL workshops2 and hosting ACL venues contains new results for Arabic (Attia et al., 2010; Marton et al., 2013a), Basque (Bengoetxea and Gojenola, 2010), Croatian (Agic et al., 2013), French (Seddah et al., 2010; Candito and Seddah, 2010; Sigogne et al., 2011), German (Rehbein, 2011), Hebrew (Tsarfaty and Sima’an, 2010; Goldberg and 1 http://alpage.inria.fr/iwpt09/panel.en. html 2 See http://www.spmrl.org/ and related workshops. 148 Elhadad, 2010a), Hindi (Ambati et al., 2010), Korean (Chung et al., 2010; Choi and Palmer, 2011) and Spanish (Le Roux et al., 2012), Tamil (Green et al., 2012), amongst others. The awareness of the modeling challenges gave rise to new lines of work on topics such as joint morpho-syntactic processing (Goldberg and"
W13-4917,W13-4916,1,0.230959,"Missing"
W13-4917,H91-1060,0,0.199934,"n the expected performance of parsers in real-world scenarios. Results reported for MRLs using gold morphological information are then, at best, optimistic. One reason for adopting this less-than-realistic evaluation scenario in previous tasks has been the lack of sound metrics for the more realistic scenario. Standard evaluation metrics assume that the number of terminals in the parse hypothesis equals the number of terminals in the gold tree. When the predicted morphological segmentation leads to a different number of terminals in the gold and parse trees, standard metrics such as ParsEval (Black et al., 1991) or Attachment Scores (Buchholz and Marsi, 2006) fail to produce a score. In this task, we use TedEval (Tsarfaty et al., 2012b), a metric recently suggested for joint morpho-syntactic evaluation, in which normalized tree-edit distance (Bille, 2005) on morphosyntactic trees allows us to quantify the success on the joint task in realistic parsing scenarios. Finally, the previous tasks focused on dependency parsing. When providing both constituency-based and dependency-based tracks, it is interesting to compare results across these frameworks so as to better understand the differences in performa"
W13-4917,D12-1133,1,0.807979,"cy-based and dependency-based parsing, have seen a surge of interest in the last two decades. These data-driven parsing approaches obtain state-of-the-art results on the de facto standard Wall Street Journal data set (Marcus et al., 1993) of English (Charniak, 2000; Collins, 2003; Charniak and Johnson, 2005; McDonald et al., 2005; McClosky et al., 2006; Petrov et al., 2006; Nivre et al., 2007b; Carreras et al., 2008; Finkel et al., 2008; ∗ Contact authors: djame.seddah@paris-sorbonne.fr, reut.tsarfaty@weizmann.ac.il, skuebler@indiana.edu Huang, 2008; Huang et al., 2010; Zhang and Nivre, 2011; Bohnet and Nivre, 2012; Shindo et al., 2012), and provide a foundation on which many tasks operating on semantic structure (e.g., recognizing textual entailments) or even discourse structure (coreference, summarization) crucially depend. While progress on parsing English — the main language of focus for the ACL community — has inspired some advances on other languages, it has not, by itself, yielded high-quality parsing for other languages and domains. This holds in particular for morphologically rich languages (MRLs), where important information concerning the predicate-argument structure of sentences is expressed"
W13-4917,C10-1011,0,0.0102695,"r system development has focused on the integration of an unsupervised word clustering method using contextual and morphological properties of the words, to help combat sparseness. Similarly to MaltParser A LPAGE :DYALOG (De La Clergerie, 2013) also uses a shift-reduce transition-based parser but its training and decoding algorithms are based on beam search. This parser is implemented on top of the tabular logic programming system DyALog. To the best of our knowledge, this is the first dependency parser capable of handling word lattice input. 163 Three participating teams use the MATE parser (Bohnet, 2010) in their systems: the BASQUE T EAM (Goenaga et al., 2013), IGM:A LPAGE (Constant et al., 2013) and IMS:S ZEGED :CIS (Björkelund et al., 2013). The BASQUE T EAM uses the MATE parser in combination with MaltParser (Nivre et al., 2007b). The system combines the parser outputs via MaltBlender (Hall et al., 2007). IGM:A LPAGE also uses MATE and MaltParser, once in a pipeline architecture and once in a joint model. The models are combined via a re-parsing strategy based on (Sagae and Lavie, 2006). This system mainly focuses on M WEs in French and uses a CRF tagger in combination with several large-"
W13-4917,W07-1506,0,0.220289,"s of all nodes were marked using a simple heuristic. In case there was a daughter with the edge label HD, this daughter was marked, i.e., existing head markings were honored. Otherwise, if existing, the rightmost daughter with edge label NK (noun kernel) was marked. Otherwise, as default, the leftmost daughter was marked. In a second step, for each continuous part of a discontinuous constituent, a separate node was introduced. This corresponds 21 This version is available from http://www.ims. uni-stuttgart.de/forschung/ressourcen/ korpora/tiger.html 159 to the &quot;raising&quot; algorithm described by Boyd (2007). In a third steps, all those newly introduced nodes that did not cover the head daughter of the original discontinuous node were deleted. For the second and the third step, we used the same script as for the Swedish constituency data. Predicted Morphology For the predicted scenario, a single sequence of POS tags and morphological features has been assigned using the MATE toolchain via a model trained on the train set via crossvalidation on the training set. The MATE toolchain was used to provide predicted annotation for lemmas, POS tags, morphology, and syntax. In order to achieve the best re"
W13-4917,W06-2920,0,0.827477,"ouri et al., 2004b), German (Kübler et al., 2006), French (Abeillé et al., 2003), Hebrew (Sima’an et al., 2001), Italian (Corazza et al., 2004), Spanish (Moreno et al., 2000), and more. It quickly became apparent that applying the phrase-based treebank grammar techniques is sensitive to language and annotation properties, and that these models are not easily portable across languages and schemes. An exception to that is the approach by Petrov (2009), who trained latentannotation treebank grammars and reported good accuracy on a range of languages. The CoNLL shared tasks on dependency parsing (Buchholz and Marsi, 2006; Nivre et al., 2007a) highlighted the usefulness of an alternative linguistic formalism for the development of competitive parsing models. Dependency relations are marked between input tokens directly, and allow the annotation of non-projective dependencies that are parseable efficiently. Dependency syntax was applied to the description of different types of languages (Tesnière, 1959; Mel’ˇcuk, 2001), which raised the hope that in these settings, parsing MRLs will further improve. However, the 2007 shared task organizers (Nivre et al., 2007a) concluded that: &quot;[Performance] classes are more ea"
W13-4917,W10-1409,1,0.0435485,"for English well-equipped to capture performance across frameworks, or performance in the face of morphological complexity? This event provoked active discussions and led to the establishment of a series of SPMRL events for the discussion of shared challenges and cross-fertilization among researchers working on parsing MRLs. The body of work on MRLs that was accumulated through the SPMRL workshops2 and hosting ACL venues contains new results for Arabic (Attia et al., 2010; Marton et al., 2013a), Basque (Bengoetxea and Gojenola, 2010), Croatian (Agic et al., 2013), French (Seddah et al., 2010; Candito and Seddah, 2010; Sigogne et al., 2011), German (Rehbein, 2011), Hebrew (Tsarfaty and Sima’an, 2010; Goldberg and 1 http://alpage.inria.fr/iwpt09/panel.en. html 2 See http://www.spmrl.org/ and related workshops. 148 Elhadad, 2010a), Hindi (Ambati et al., 2010), Korean (Chung et al., 2010; Choi and Palmer, 2011) and Spanish (Le Roux et al., 2012), Tamil (Green et al., 2012), amongst others. The awareness of the modeling challenges gave rise to new lines of work on topics such as joint morpho-syntactic processing (Goldberg and Tsarfaty, 2008), Relational-Realizational Parsing (Tsarfaty, 2010), EasyFirst Parsing"
W13-4917,candito-etal-2010-statistical,1,0.0386487,"g of 18,535 sentences,18 split into 14,759 sentences for training, 1,235 sentences for development, and 2,541 sentences for the final evaluation.19 Adapting the Data to the Shared Task The constituency trees are provided in an extended PTB bracketed format, with morphological features at the pre-terminal level only. They contain slight, automatically performed, modifications with respect to the original trees of the French treebank. The syntagmatic projection of prepositions and complementizers was normalized, in order to have prepositions and complementizers as heads in the dependency trees (Candito et al., 2010). The dependency representations are projective dependency trees, obtained through automatic conversion from the constituency trees. The conversion procedure is an enhanced version of the one described by Candito et al. (2010). Both the constituency and the dependency representations make use of coarse- and fine-grained POS tags (CPOS and FPOS respectively). The CPOS are the categories from the original treebank. The FPOS 18 The process of functional annotation is still ongoing, the objective of the FTB providers being to have all the 20000 sentences annotated with functional tags. 19 The firs"
W13-4917,W08-2102,0,0.0353476,"troduction Syntactic parsing consists of automatically assigning to a natural language sentence a representation of its grammatical structure. Data-driven approaches to this problem, both for constituency-based and dependency-based parsing, have seen a surge of interest in the last two decades. These data-driven parsing approaches obtain state-of-the-art results on the de facto standard Wall Street Journal data set (Marcus et al., 1993) of English (Charniak, 2000; Collins, 2003; Charniak and Johnson, 2005; McDonald et al., 2005; McClosky et al., 2006; Petrov et al., 2006; Nivre et al., 2007b; Carreras et al., 2008; Finkel et al., 2008; ∗ Contact authors: djame.seddah@paris-sorbonne.fr, reut.tsarfaty@weizmann.ac.il, skuebler@indiana.edu Huang, 2008; Huang et al., 2010; Zhang and Nivre, 2011; Bohnet and Nivre, 2012; Shindo et al., 2012), and provide a foundation on which many tasks operating on semantic structure (e.g., recognizing textual entailments) or even discourse structure (coreference, summarization) crucially depend. While progress on parsing English — the main language of focus for the ACL community — has inspired some advances on other languages, it has not, by itself, yielded high-quality par"
W13-4917,A00-2018,0,0.0705659,"n analysis and comparison of the parsers across languages and frameworks, reported for gold input as well as more realistic parsing scenarios. 1 Introduction Syntactic parsing consists of automatically assigning to a natural language sentence a representation of its grammatical structure. Data-driven approaches to this problem, both for constituency-based and dependency-based parsing, have seen a surge of interest in the last two decades. These data-driven parsing approaches obtain state-of-the-art results on the de facto standard Wall Street Journal data set (Marcus et al., 1993) of English (Charniak, 2000; Collins, 2003; Charniak and Johnson, 2005; McDonald et al., 2005; McClosky et al., 2006; Petrov et al., 2006; Nivre et al., 2007b; Carreras et al., 2008; Finkel et al., 2008; ∗ Contact authors: djame.seddah@paris-sorbonne.fr, reut.tsarfaty@weizmann.ac.il, skuebler@indiana.edu Huang, 2008; Huang et al., 2010; Zhang and Nivre, 2011; Bohnet and Nivre, 2012; Shindo et al., 2012), and provide a foundation on which many tasks operating on semantic structure (e.g., recognizing textual entailments) or even discourse structure (coreference, summarization) crucially depend. While progress on parsing E"
W13-4917,W11-3801,1,0.926035,"ers working on parsing MRLs. The body of work on MRLs that was accumulated through the SPMRL workshops2 and hosting ACL venues contains new results for Arabic (Attia et al., 2010; Marton et al., 2013a), Basque (Bengoetxea and Gojenola, 2010), Croatian (Agic et al., 2013), French (Seddah et al., 2010; Candito and Seddah, 2010; Sigogne et al., 2011), German (Rehbein, 2011), Hebrew (Tsarfaty and Sima’an, 2010; Goldberg and 1 http://alpage.inria.fr/iwpt09/panel.en. html 2 See http://www.spmrl.org/ and related workshops. 148 Elhadad, 2010a), Hindi (Ambati et al., 2010), Korean (Chung et al., 2010; Choi and Palmer, 2011) and Spanish (Le Roux et al., 2012), Tamil (Green et al., 2012), amongst others. The awareness of the modeling challenges gave rise to new lines of work on topics such as joint morpho-syntactic processing (Goldberg and Tsarfaty, 2008), Relational-Realizational Parsing (Tsarfaty, 2010), EasyFirst Parsing (Goldberg, 2011), PLCFRS parsing (Kallmeyer and Maier, 2013), the use of factored lexica (Green et al., 2013), the use of bilingual data (Fraser et al., 2013), and more developments that are currently under way. With new models and data, and with lingering interest in parsing non-standard Engli"
W13-4917,chrupala-etal-2008-learning,0,0.045003,"Missing"
W13-4917,W10-1406,0,0.0618994,"Missing"
W13-4917,W13-4909,0,0.199525,"derived from the Hebrew Treebank V2 (Sima’an et al., 2001; Guthmann et al., 2009). The treebank is based on just over 6000 sentences from the daily newspaper ‘Ha’aretz’, manually annotated with morphological information and phrase-structure trees and extended with head information as described in Tsarfaty (2010, ch. 5). The unlabeled dependency version was produced by conversion from the constituency treebank as described in Goldberg (2011). Both the constituency and dependency trees were annotated with a set grammatical function labels conforming to Unified Stanford Dependencies by Tsarfaty (2013). 22 We also provided a predicted-all scenario, in which we provided morphological analysis lattices with POS and morphological information derived from the analyses of the SMOR derivational morphology (Schmid et al., 2004). These lattices were not used by any of the participants. Adapting the Data to the Shared Task While based on the same trees, the dependency and constituency treebanks differ in their POS tag sets, as well as in some of the morphological segmentation decisions. The main effort towards the shared task was unifying the two resources such that the two treebanks share the same"
W13-4917,J03-4003,0,0.48866,"omparison of the parsers across languages and frameworks, reported for gold input as well as more realistic parsing scenarios. 1 Introduction Syntactic parsing consists of automatically assigning to a natural language sentence a representation of its grammatical structure. Data-driven approaches to this problem, both for constituency-based and dependency-based parsing, have seen a surge of interest in the last two decades. These data-driven parsing approaches obtain state-of-the-art results on the de facto standard Wall Street Journal data set (Marcus et al., 1993) of English (Charniak, 2000; Collins, 2003; Charniak and Johnson, 2005; McDonald et al., 2005; McClosky et al., 2006; Petrov et al., 2006; Nivre et al., 2007b; Carreras et al., 2008; Finkel et al., 2008; ∗ Contact authors: djame.seddah@paris-sorbonne.fr, reut.tsarfaty@weizmann.ac.il, skuebler@indiana.edu Huang, 2008; Huang et al., 2010; Zhang and Nivre, 2011; Bohnet and Nivre, 2012; Shindo et al., 2012), and provide a foundation on which many tasks operating on semantic structure (e.g., recognizing textual entailments) or even discourse structure (coreference, summarization) crucially depend. While progress on parsing English — the ma"
W13-4917,W13-4905,1,0.719588,"method using contextual and morphological properties of the words, to help combat sparseness. Similarly to MaltParser A LPAGE :DYALOG (De La Clergerie, 2013) also uses a shift-reduce transition-based parser but its training and decoding algorithms are based on beam search. This parser is implemented on top of the tabular logic programming system DyALog. To the best of our knowledge, this is the first dependency parser capable of handling word lattice input. 163 Three participating teams use the MATE parser (Bohnet, 2010) in their systems: the BASQUE T EAM (Goenaga et al., 2013), IGM:A LPAGE (Constant et al., 2013) and IMS:S ZEGED :CIS (Björkelund et al., 2013). The BASQUE T EAM uses the MATE parser in combination with MaltParser (Nivre et al., 2007b). The system combines the parser outputs via MaltBlender (Hall et al., 2007). IGM:A LPAGE also uses MATE and MaltParser, once in a pipeline architecture and once in a joint model. The models are combined via a re-parsing strategy based on (Sagae and Lavie, 2006). This system mainly focuses on M WEs in French and uses a CRF tagger in combination with several large-scale dictionaries to handle M WEs, which then serve as input for the two parsers. The IMS:S ZE"
W13-4917,W13-4906,1,0.680312,"dependency track. Two participating systems are based on MaltParser: M ALTOPTIMIZER (Ballesteros, 2013) and AI:KU (Cirik and Sensoy, ¸ 2013). M ALTOPTIMIZER uses a variant of MaltOptimizer (Ballesteros and Nivre, 2012) to explore features relevant for the processing of morphological information. AI:KU uses a combination of MaltParser and the original MaltOptimizer. Their system development has focused on the integration of an unsupervised word clustering method using contextual and morphological properties of the words, to help combat sparseness. Similarly to MaltParser A LPAGE :DYALOG (De La Clergerie, 2013) also uses a shift-reduce transition-based parser but its training and decoding algorithms are based on beam search. This parser is implemented on top of the tabular logic programming system DyALog. To the best of our knowledge, this is the first dependency parser capable of handling word lattice input. 163 Three participating teams use the MATE parser (Bohnet, 2010) in their systems: the BASQUE T EAM (Goenaga et al., 2013), IGM:A LPAGE (Constant et al., 2013) and IMS:S ZEGED :CIS (Björkelund et al., 2013). The BASQUE T EAM uses the MATE parser in combination with MaltParser (Nivre et al., 200"
W13-4917,W08-1301,0,0.0393335,"Missing"
W13-4917,P98-1062,0,0.0491049,"Missing"
W13-4917,P08-1109,0,0.0220424,"ences. In order to avoid comparing apples and oranges, we use the unlabeled TedEval metric, which converts all representation types internally into the same kind of structures, called function trees. Here we use TedEval’s crossframework protocol (Tsarfaty et al., 2012a), which accomodates annotation idiosyncrasies. • Cross-Language Evaluation. Here, we compare parsers for the same representation type across different languages. Conducting a complete and faithful evaluation across languages 151 would require a harmonized universal annotation scheme (possibly along the lines of (de Marneffe and Manning, 2008; McDonald et al., 2013; Tsarfaty, 2013)) or task based evaluation. As an approximation we use unlabeled TedEval. Since it is unlabeled, it is not sensitive to label set size. Since it internally uses function-trees, it is less sensitive to annotation idiosyncrasies (e.g., head choice) (Tsarfaty et al., 2011). The former two dimensions are evaluated on the full sets. The latter two are evaluated on smaller, comparable, test sets. For completeness, we provide below the formal definitions and essential modifications of the evaluation software that we used. 3.4.1 Evaluation Metrics for Phrase Str"
W13-4917,J13-1005,1,0.838989,"html 2 See http://www.spmrl.org/ and related workshops. 148 Elhadad, 2010a), Hindi (Ambati et al., 2010), Korean (Chung et al., 2010; Choi and Palmer, 2011) and Spanish (Le Roux et al., 2012), Tamil (Green et al., 2012), amongst others. The awareness of the modeling challenges gave rise to new lines of work on topics such as joint morpho-syntactic processing (Goldberg and Tsarfaty, 2008), Relational-Realizational Parsing (Tsarfaty, 2010), EasyFirst Parsing (Goldberg, 2011), PLCFRS parsing (Kallmeyer and Maier, 2013), the use of factored lexica (Green et al., 2013), the use of bilingual data (Fraser et al., 2013), and more developments that are currently under way. With new models and data, and with lingering interest in parsing non-standard English data, questions begin to emerge, such as: What is the realistic performance of parsing MRLs using today’s methods? How do the different models compare with one another? How do different representation types deal with parsing one particular language? Does the success of a parsing model on a language correlate with its representation type and learning method? How to parse effectively in the face of resource scarcity? The first step to answering all of these"
W13-4917,W13-4908,1,0.872762,"Missing"
W13-4917,W10-1412,1,0.789087,"Hall et al., 2007). IGM:A LPAGE also uses MATE and MaltParser, once in a pipeline architecture and once in a joint model. The models are combined via a re-parsing strategy based on (Sagae and Lavie, 2006). This system mainly focuses on M WEs in French and uses a CRF tagger in combination with several large-scale dictionaries to handle M WEs, which then serve as input for the two parsers. The IMS:S ZEGED :CIS team participated in both tracks, with an ensemble system. For the dependency track, the ensemble includes the MATE parser (Bohnet, 2010), a best-first variant of the easy-first parser by Goldberg and Elhadad (2010b), and turbo parser (Martins et al., 2010), in combination with a ranker that has the particularity of using features from the constituent parsed trees. C ADIM (Marton et al., 2013b) uses their variant of the easy-first parser combined with a feature-rich ensemble of lexical and syntactic resources. Four of the participating teams use external resources in addition to the parser. The IMS:S ZEGED :CIS team uses external morphological analyzers. C ADIM uses SAMA (Graff et al., 2009) for Arabic morphology. A LPAGE :DYALOG and IGM:A LPAGE use external lexicons for French. IGM:A LPAGE additionally"
W13-4917,N10-1115,1,0.576439,"Hall et al., 2007). IGM:A LPAGE also uses MATE and MaltParser, once in a pipeline architecture and once in a joint model. The models are combined via a re-parsing strategy based on (Sagae and Lavie, 2006). This system mainly focuses on M WEs in French and uses a CRF tagger in combination with several large-scale dictionaries to handle M WEs, which then serve as input for the two parsers. The IMS:S ZEGED :CIS team participated in both tracks, with an ensemble system. For the dependency track, the ensemble includes the MATE parser (Bohnet, 2010), a best-first variant of the easy-first parser by Goldberg and Elhadad (2010b), and turbo parser (Martins et al., 2010), in combination with a ranker that has the particularity of using features from the constituent parsed trees. C ADIM (Marton et al., 2013b) uses their variant of the easy-first parser combined with a feature-rich ensemble of lexical and syntactic resources. Four of the participating teams use external resources in addition to the parser. The IMS:S ZEGED :CIS team uses external morphological analyzers. C ADIM uses SAMA (Graff et al., 2009) for Arabic morphology. A LPAGE :DYALOG and IGM:A LPAGE use external lexicons for French. IGM:A LPAGE additionally"
W13-4917,P08-1085,1,0.364225,"overage. The morphologically disambiguated input files for the Raw (1-best) scenario were produced by running the raw text through the morphological disam23 Note that this additional layer in the constituency treebank adds a relatively easy set of nodes to the trees, thus “inflating” the evaluation scores compared to previously reported results. To compensate, a stricter protocol than is used in this task would strip one of the two POS layers prior to evaluation. 24 This split is slightly different than the split in previous studies. 160 biguator (tagger) described in Adler and Elhadad (2006; Goldberg et al. (2008),Adler (2007). The disambiguator is based on the same lexicon that is used to produce the lattice files, but utilizes an extra module for dealing with unknown tokens Adler et al. (2008). The core of the disambiguator is an HMM tagger trained on about 70M unannotated tokens using EM, and being supervised by the lexicon. As in the case of Arabic, we also provided data for the Predicted (gold token / predicted morphology) scenario. We used the same sequence labeler, Morfette (Chrupała et al., 2008), trained on the concatenation of POS and morphological gold features, leading to a model with respe"
W13-4917,E09-1038,1,0.867766,"ices with POS and morphological information derived from the analyses of the SMOR derivational morphology (Schmid et al., 2004). These lattices were not used by any of the participants. Adapting the Data to the Shared Task While based on the same trees, the dependency and constituency treebanks differ in their POS tag sets, as well as in some of the morphological segmentation decisions. The main effort towards the shared task was unifying the two resources such that the two treebanks share the same lexical yields, and the same pre-terminal labels. To this end, we took the layering approach of Goldberg et al. (2009), and included two levels of POS tags in the constituency trees. The lower level is lexical, conforming to the lexical resource used to build the lattices, and is shared by the two treebanks. The higher level is syntactic, and follows the tag set and annotation decisions of the original constituency treebank.23 In addition, we unified the representation of morphological features, and fixed inconsistencies and mistakes in the treebanks. Data Split The Hebrew treebank is one of the smallest in our language set, and hence it is provided in only the small (5k) setting. For the sake of comparabilit"
W13-4917,C10-1045,1,0.826872,"nflectional and derivational morphology. It exhibits a high degree of morphological ambiguity due to the absence of the diacritics and inconsistent spelling of letters, such as Alif and Ya. As a consequence, the Buckwalter Standard Arabic Morphological Analyzer (Buckwalter, 2004; Graff et al., 2009) produces an average of 12 analyses per word. Data Sets The Arabic data set contains two treebanks derived from the LDC Penn Arabic Treebanks (PATB) (Maamouri et al., 2004b):11 the Columbia Arabic Treebank (CATiB) (Habash and Roth, 2009), a dependency treebank, and the Stanford version of the PATB (Green and Manning, 2010), a phrasestructure treebank. We preprocessed the treebanks to obtain strict token matching between the treebanks and the morphological analyses. This required nontrivial synchronization at the tree token level between the PATB treebank, the CATiB treebank and the morphologically predicted data, using the PATB source tokens and CATiB feature word form as a dual synchronized pivot. The Columbia Arabic Treebank The Columbia Arabic Treebank (CATiB) uses a dependency representation that is based on traditional Arabic grammar and that emphasizes syntactic case relations (Habash and Roth, 2009; Haba"
W13-4917,W12-3410,0,0.0157938,"umulated through the SPMRL workshops2 and hosting ACL venues contains new results for Arabic (Attia et al., 2010; Marton et al., 2013a), Basque (Bengoetxea and Gojenola, 2010), Croatian (Agic et al., 2013), French (Seddah et al., 2010; Candito and Seddah, 2010; Sigogne et al., 2011), German (Rehbein, 2011), Hebrew (Tsarfaty and Sima’an, 2010; Goldberg and 1 http://alpage.inria.fr/iwpt09/panel.en. html 2 See http://www.spmrl.org/ and related workshops. 148 Elhadad, 2010a), Hindi (Ambati et al., 2010), Korean (Chung et al., 2010; Choi and Palmer, 2011) and Spanish (Le Roux et al., 2012), Tamil (Green et al., 2012), amongst others. The awareness of the modeling challenges gave rise to new lines of work on topics such as joint morpho-syntactic processing (Goldberg and Tsarfaty, 2008), Relational-Realizational Parsing (Tsarfaty, 2010), EasyFirst Parsing (Goldberg, 2011), PLCFRS parsing (Kallmeyer and Maier, 2013), the use of factored lexica (Green et al., 2013), the use of bilingual data (Fraser et al., 2013), and more developments that are currently under way. With new models and data, and with lingering interest in parsing non-standard English data, questions begin to emerge, such as: What is the realis"
W13-4917,J13-1009,1,0.747017,"Missing"
W13-4917,P09-2056,1,0.833708,".2 The Arabic Treebanks Arabic is a morphologically complex language which has rich inflectional and derivational morphology. It exhibits a high degree of morphological ambiguity due to the absence of the diacritics and inconsistent spelling of letters, such as Alif and Ya. As a consequence, the Buckwalter Standard Arabic Morphological Analyzer (Buckwalter, 2004; Graff et al., 2009) produces an average of 12 analyses per word. Data Sets The Arabic data set contains two treebanks derived from the LDC Penn Arabic Treebanks (PATB) (Maamouri et al., 2004b):11 the Columbia Arabic Treebank (CATiB) (Habash and Roth, 2009), a dependency treebank, and the Stanford version of the PATB (Green and Manning, 2010), a phrasestructure treebank. We preprocessed the treebanks to obtain strict token matching between the treebanks and the morphological analyses. This required nontrivial synchronization at the tree token level between the PATB treebank, the CATiB treebank and the morphologically predicted data, using the PATB source tokens and CATiB feature word form as a dual synchronized pivot. The Columbia Arabic Treebank The Columbia Arabic Treebank (CATiB) uses a dependency representation that is based on traditional A"
W13-4917,D07-1116,1,0.604822,"010), a phrasestructure treebank. We preprocessed the treebanks to obtain strict token matching between the treebanks and the morphological analyses. This required nontrivial synchronization at the tree token level between the PATB treebank, the CATiB treebank and the morphologically predicted data, using the PATB source tokens and CATiB feature word form as a dual synchronized pivot. The Columbia Arabic Treebank The Columbia Arabic Treebank (CATiB) uses a dependency representation that is based on traditional Arabic grammar and that emphasizes syntactic case relations (Habash and Roth, 2009; Habash et al., 2007). The CATiB treebank uses the word tokenization of the PATB 11 The LDC kindly provided their latest version of the Arabic Treebanks. In particular, we used PATB 1 v4.1 (Maamouri et al., 2005), PATB 2 v3.1 (Maamouri et al., 2004a) and PATB 3 v3.3. (Maamouri et al., 2009) train: #Sents #Tokens Lex. Size Avg. Length Ratio #NT/#Tokens Ratio #NT/#Sents #Non Terminals #POS tags #total NTs Dep. Label Set Size train5k: #Sents #Tokens Lex. Size Avg. Length Ratio #NT/#Tokens Ratio #NT/#Sents #Non Terminals #POS Tags #total NTs Dep. Label Set Size dev: #Sents #Tokens Lex. Size Avg. Length Ratio #NT/#Toke"
W13-4917,P07-2053,0,0.0323622,"Missing"
W13-4917,D07-1097,1,0.346865,"Missing"
W13-4917,D10-1002,0,0.0151688,"oaches to this problem, both for constituency-based and dependency-based parsing, have seen a surge of interest in the last two decades. These data-driven parsing approaches obtain state-of-the-art results on the de facto standard Wall Street Journal data set (Marcus et al., 1993) of English (Charniak, 2000; Collins, 2003; Charniak and Johnson, 2005; McDonald et al., 2005; McClosky et al., 2006; Petrov et al., 2006; Nivre et al., 2007b; Carreras et al., 2008; Finkel et al., 2008; ∗ Contact authors: djame.seddah@paris-sorbonne.fr, reut.tsarfaty@weizmann.ac.il, skuebler@indiana.edu Huang, 2008; Huang et al., 2010; Zhang and Nivre, 2011; Bohnet and Nivre, 2012; Shindo et al., 2012), and provide a foundation on which many tasks operating on semantic structure (e.g., recognizing textual entailments) or even discourse structure (coreference, summarization) crucially depend. While progress on parsing English — the main language of focus for the ACL community — has inspired some advances on other languages, it has not, by itself, yielded high-quality parsing for other languages and domains. This holds in particular for morphologically rich languages (MRLs), where important information concerning the predica"
W13-4917,P08-1067,0,0.0226773,"a-driven approaches to this problem, both for constituency-based and dependency-based parsing, have seen a surge of interest in the last two decades. These data-driven parsing approaches obtain state-of-the-art results on the de facto standard Wall Street Journal data set (Marcus et al., 1993) of English (Charniak, 2000; Collins, 2003; Charniak and Johnson, 2005; McDonald et al., 2005; McClosky et al., 2006; Petrov et al., 2006; Nivre et al., 2007b; Carreras et al., 2008; Finkel et al., 2008; ∗ Contact authors: djame.seddah@paris-sorbonne.fr, reut.tsarfaty@weizmann.ac.il, skuebler@indiana.edu Huang, 2008; Huang et al., 2010; Zhang and Nivre, 2011; Bohnet and Nivre, 2012; Shindo et al., 2012), and provide a foundation on which many tasks operating on semantic structure (e.g., recognizing textual entailments) or even discourse structure (coreference, summarization) crucially depend. While progress on parsing English — the main language of focus for the ACL community — has inspired some advances on other languages, it has not, by itself, yielded high-quality parsing for other languages and domains. This holds in particular for morphologically rich languages (MRLs), where important information co"
W13-4917,J98-4004,0,0.0891486,"ir strengths and weaknesses. Finally, we summarize and conclude with challenges to address in future shared tasks (§8). 2 2.1 Background A Brief History of the SPMRL Field Statistical parsing saw initial success upon the availability of the Penn Treebank (PTB, Marcus et al., 1994). With that large set of syntactically annotated sentences at their disposal, researchers could apply advanced statistical modeling and machine learning techniques in order to obtain high quality structure prediction. The first statistical parsing models were generative and based on treebank grammars (Charniak, 1997; Johnson, 1998; Klein and Manning, 2003; Collins, 2003; Petrov et al., 2006; McClosky et al., 2006), leading to high phrase-structure accuracy. Encouraged by the success of phrase-structure parsers for English, treebank grammars for additional languages have been developed, starting with Czech (Hajiˇc et al., 2000) then with treebanks of Chinese (Levy and Manning, 2003), Arabic (Maamouri et al., 2004b), German (Kübler et al., 2006), French (Abeillé et al., 2003), Hebrew (Sima’an et al., 2001), Italian (Corazza et al., 2004), Spanish (Moreno et al., 2000), and more. It quickly became apparent that applying t"
W13-4917,J13-1006,1,0.798597,"hbein, 2011), Hebrew (Tsarfaty and Sima’an, 2010; Goldberg and 1 http://alpage.inria.fr/iwpt09/panel.en. html 2 See http://www.spmrl.org/ and related workshops. 148 Elhadad, 2010a), Hindi (Ambati et al., 2010), Korean (Chung et al., 2010; Choi and Palmer, 2011) and Spanish (Le Roux et al., 2012), Tamil (Green et al., 2012), amongst others. The awareness of the modeling challenges gave rise to new lines of work on topics such as joint morpho-syntactic processing (Goldberg and Tsarfaty, 2008), Relational-Realizational Parsing (Tsarfaty, 2010), EasyFirst Parsing (Goldberg, 2011), PLCFRS parsing (Kallmeyer and Maier, 2013), the use of factored lexica (Green et al., 2013), the use of bilingual data (Fraser et al., 2013), and more developments that are currently under way. With new models and data, and with lingering interest in parsing non-standard English data, questions begin to emerge, such as: What is the realistic performance of parsing MRLs using today’s methods? How do the different models compare with one another? How do different representation types deal with parsing one particular language? Does the success of a parsing model on a language correlate with its representation type and learning method? Ho"
W13-4917,P03-1054,0,0.00438043,"d weaknesses. Finally, we summarize and conclude with challenges to address in future shared tasks (§8). 2 2.1 Background A Brief History of the SPMRL Field Statistical parsing saw initial success upon the availability of the Penn Treebank (PTB, Marcus et al., 1994). With that large set of syntactically annotated sentences at their disposal, researchers could apply advanced statistical modeling and machine learning techniques in order to obtain high quality structure prediction. The first statistical parsing models were generative and based on treebank grammars (Charniak, 1997; Johnson, 1998; Klein and Manning, 2003; Collins, 2003; Petrov et al., 2006; McClosky et al., 2006), leading to high phrase-structure accuracy. Encouraged by the success of phrase-structure parsers for English, treebank grammars for additional languages have been developed, starting with Czech (Hajiˇc et al., 2000) then with treebanks of Chinese (Levy and Manning, 2003), Arabic (Maamouri et al., 2004b), German (Kübler et al., 2006), French (Abeillé et al., 2003), Hebrew (Sima’an et al., 2001), Italian (Corazza et al., 2004), Spanish (Moreno et al., 2000), and more. It quickly became apparent that applying the phrase-based treebank"
W13-4917,W06-1614,1,0.812546,"nd machine learning techniques in order to obtain high quality structure prediction. The first statistical parsing models were generative and based on treebank grammars (Charniak, 1997; Johnson, 1998; Klein and Manning, 2003; Collins, 2003; Petrov et al., 2006; McClosky et al., 2006), leading to high phrase-structure accuracy. Encouraged by the success of phrase-structure parsers for English, treebank grammars for additional languages have been developed, starting with Czech (Hajiˇc et al., 2000) then with treebanks of Chinese (Levy and Manning, 2003), Arabic (Maamouri et al., 2004b), German (Kübler et al., 2006), French (Abeillé et al., 2003), Hebrew (Sima’an et al., 2001), Italian (Corazza et al., 2004), Spanish (Moreno et al., 2000), and more. It quickly became apparent that applying the phrase-based treebank grammar techniques is sensitive to language and annotation properties, and that these models are not easily portable across languages and schemes. An exception to that is the approach by Petrov (2009), who trained latentannotation treebank grammars and reported good accuracy on a range of languages. The CoNLL shared tasks on dependency parsing (Buchholz and Marsi, 2006; Nivre et al., 2007a) hi"
W13-4917,kubler-etal-2008-compare,1,0.91565,"node to the root node in the output tree and the corresponding path in the gold tree. The path consists of a sequence of node labels between the terminal node and the root node, and the similarity of two paths is calculated by using the Levenshtein distance. This distance is normalized by path length, and the score of the tree is an aggregated score of the values for all terminals in the tree (xt is the leaf-ancestor path of t in tree x). P LA(h, g) = t∈yield(g) Lv(ht ,gt )/(len(ht )+len(gt )) |yield(g)| This metric was shown to be less sensitive to differences between annotation schemes in (Kübler et al., 2008), and was shown by Rehbein and van Genabith (2007a) to evaluate trees more faithfully than ParsEval in the face of certain annotation decisions. We used the implementation of Wagner (2012).6 3.4.2 Evaluation Metrics for Dependency Structures Attachment Scores Labeled and Unlabeled Attachment scores have been proposed as evaluation metrics for dependency parsing in the CoNLL shared tasks (Buchholz and Marsi, 2006; Nivre et al., 2007a) and have since assumed the role of standard metrics in multiple shared tasks and independent studies. Assume that g, h are gold and hypothesized dependency trees"
W13-4917,W12-3408,1,0.878953,"Missing"
W13-4917,P03-1056,0,0.0207769,"disposal, researchers could apply advanced statistical modeling and machine learning techniques in order to obtain high quality structure prediction. The first statistical parsing models were generative and based on treebank grammars (Charniak, 1997; Johnson, 1998; Klein and Manning, 2003; Collins, 2003; Petrov et al., 2006; McClosky et al., 2006), leading to high phrase-structure accuracy. Encouraged by the success of phrase-structure parsers for English, treebank grammars for additional languages have been developed, starting with Czech (Hajiˇc et al., 2000) then with treebanks of Chinese (Levy and Manning, 2003), Arabic (Maamouri et al., 2004b), German (Kübler et al., 2006), French (Abeillé et al., 2003), Hebrew (Sima’an et al., 2001), Italian (Corazza et al., 2004), Spanish (Moreno et al., 2000), and more. It quickly became apparent that applying the phrase-based treebank grammar techniques is sensitive to language and annotation properties, and that these models are not easily portable across languages and schemes. An exception to that is the approach by Petrov (2009), who trained latentannotation treebank grammars and reported good accuracy on a range of languages. The CoNLL shared tasks on depend"
W13-4917,W12-4615,1,0.809959,"ly. The conversion of TiGer into dependencies is a variant of the one by Seeker and Kuhn (2012), which does not contain empty nodes. It is based on the same TiGer release as the one used for the constituency data. Punctuation was attached as high as possible, without creating any new non-projective edges. Adapting the Data to the Shared Task For the constituency version, punctuation and other unattached elements were first attached to the tree. As attachment target, we used roughly the respective least common ancestor node of the right and left terminal neighbor of the unattached element (see Maier et al. (2012) for details), and subsequently, the crossing branches were resolved. This was done in three steps. In the first step, the head daughters of all nodes were marked using a simple heuristic. In case there was a daughter with the edge label HD, this daughter was marked, i.e., existing head markings were honored. Otherwise, if existing, the rightmost daughter with edge label NK (noun kernel) was marked. Otherwise, as default, the leftmost daughter was marked. In a second step, for each continuous part of a discontinuous constituent, a separate node was introduced. This corresponds 21 This version"
W13-4917,J93-2004,0,0.0437888,"participants, and then provide an analysis and comparison of the parsers across languages and frameworks, reported for gold input as well as more realistic parsing scenarios. 1 Introduction Syntactic parsing consists of automatically assigning to a natural language sentence a representation of its grammatical structure. Data-driven approaches to this problem, both for constituency-based and dependency-based parsing, have seen a surge of interest in the last two decades. These data-driven parsing approaches obtain state-of-the-art results on the de facto standard Wall Street Journal data set (Marcus et al., 1993) of English (Charniak, 2000; Collins, 2003; Charniak and Johnson, 2005; McDonald et al., 2005; McClosky et al., 2006; Petrov et al., 2006; Nivre et al., 2007b; Carreras et al., 2008; Finkel et al., 2008; ∗ Contact authors: djame.seddah@paris-sorbonne.fr, reut.tsarfaty@weizmann.ac.il, skuebler@indiana.edu Huang, 2008; Huang et al., 2010; Zhang and Nivre, 2011; Bohnet and Nivre, 2012; Shindo et al., 2012), and provide a foundation on which many tasks operating on semantic structure (e.g., recognizing textual entailments) or even discourse structure (coreference, summarization) crucially depend."
W13-4917,D10-1004,0,0.0390834,"nd MaltParser, once in a pipeline architecture and once in a joint model. The models are combined via a re-parsing strategy based on (Sagae and Lavie, 2006). This system mainly focuses on M WEs in French and uses a CRF tagger in combination with several large-scale dictionaries to handle M WEs, which then serve as input for the two parsers. The IMS:S ZEGED :CIS team participated in both tracks, with an ensemble system. For the dependency track, the ensemble includes the MATE parser (Bohnet, 2010), a best-first variant of the easy-first parser by Goldberg and Elhadad (2010b), and turbo parser (Martins et al., 2010), in combination with a ranker that has the particularity of using features from the constituent parsed trees. C ADIM (Marton et al., 2013b) uses their variant of the easy-first parser combined with a feature-rich ensemble of lexical and syntactic resources. Four of the participating teams use external resources in addition to the parser. The IMS:S ZEGED :CIS team uses external morphological analyzers. C ADIM uses SAMA (Graff et al., 2009) for Arabic morphology. A LPAGE :DYALOG and IGM:A LPAGE use external lexicons for French. IGM:A LPAGE additionally uses Morfette (Chrupała et al., 2008) for"
W13-4917,J13-1008,1,0.913933,". Additionally, new questions emerged as to the evaluation of parsers in such languages – are the word-based metrics used for English well-equipped to capture performance across frameworks, or performance in the face of morphological complexity? This event provoked active discussions and led to the establishment of a series of SPMRL events for the discussion of shared challenges and cross-fertilization among researchers working on parsing MRLs. The body of work on MRLs that was accumulated through the SPMRL workshops2 and hosting ACL venues contains new results for Arabic (Attia et al., 2010; Marton et al., 2013a), Basque (Bengoetxea and Gojenola, 2010), Croatian (Agic et al., 2013), French (Seddah et al., 2010; Candito and Seddah, 2010; Sigogne et al., 2011), German (Rehbein, 2011), Hebrew (Tsarfaty and Sima’an, 2010; Goldberg and 1 http://alpage.inria.fr/iwpt09/panel.en. html 2 See http://www.spmrl.org/ and related workshops. 148 Elhadad, 2010a), Hindi (Ambati et al., 2010), Korean (Chung et al., 2010; Choi and Palmer, 2011) and Spanish (Le Roux et al., 2012), Tamil (Green et al., 2012), amongst others. The awareness of the modeling challenges gave rise to new lines of work on topics such as joint"
W13-4917,W13-4910,1,0.915357,". Additionally, new questions emerged as to the evaluation of parsers in such languages – are the word-based metrics used for English well-equipped to capture performance across frameworks, or performance in the face of morphological complexity? This event provoked active discussions and led to the establishment of a series of SPMRL events for the discussion of shared challenges and cross-fertilization among researchers working on parsing MRLs. The body of work on MRLs that was accumulated through the SPMRL workshops2 and hosting ACL venues contains new results for Arabic (Attia et al., 2010; Marton et al., 2013a), Basque (Bengoetxea and Gojenola, 2010), Croatian (Agic et al., 2013), French (Seddah et al., 2010; Candito and Seddah, 2010; Sigogne et al., 2011), German (Rehbein, 2011), Hebrew (Tsarfaty and Sima’an, 2010; Goldberg and 1 http://alpage.inria.fr/iwpt09/panel.en. html 2 See http://www.spmrl.org/ and related workshops. 148 Elhadad, 2010a), Hindi (Ambati et al., 2010), Korean (Chung et al., 2010; Choi and Palmer, 2011) and Spanish (Le Roux et al., 2012), Tamil (Green et al., 2012), amongst others. The awareness of the modeling challenges gave rise to new lines of work on topics such as joint"
W13-4917,N06-1020,0,0.225446,"for gold input as well as more realistic parsing scenarios. 1 Introduction Syntactic parsing consists of automatically assigning to a natural language sentence a representation of its grammatical structure. Data-driven approaches to this problem, both for constituency-based and dependency-based parsing, have seen a surge of interest in the last two decades. These data-driven parsing approaches obtain state-of-the-art results on the de facto standard Wall Street Journal data set (Marcus et al., 1993) of English (Charniak, 2000; Collins, 2003; Charniak and Johnson, 2005; McDonald et al., 2005; McClosky et al., 2006; Petrov et al., 2006; Nivre et al., 2007b; Carreras et al., 2008; Finkel et al., 2008; ∗ Contact authors: djame.seddah@paris-sorbonne.fr, reut.tsarfaty@weizmann.ac.il, skuebler@indiana.edu Huang, 2008; Huang et al., 2010; Zhang and Nivre, 2011; Bohnet and Nivre, 2012; Shindo et al., 2012), and provide a foundation on which many tasks operating on semantic structure (e.g., recognizing textual entailments) or even discourse structure (coreference, summarization) crucially depend. While progress on parsing English — the main language of focus for the ACL community — has inspired some advances on"
W13-4917,P05-1012,0,0.042194,"Missing"
W13-4917,moreno-etal-2000-treebank,0,0.0581254,"e generative and based on treebank grammars (Charniak, 1997; Johnson, 1998; Klein and Manning, 2003; Collins, 2003; Petrov et al., 2006; McClosky et al., 2006), leading to high phrase-structure accuracy. Encouraged by the success of phrase-structure parsers for English, treebank grammars for additional languages have been developed, starting with Czech (Hajiˇc et al., 2000) then with treebanks of Chinese (Levy and Manning, 2003), Arabic (Maamouri et al., 2004b), German (Kübler et al., 2006), French (Abeillé et al., 2003), Hebrew (Sima’an et al., 2001), Italian (Corazza et al., 2004), Spanish (Moreno et al., 2000), and more. It quickly became apparent that applying the phrase-based treebank grammar techniques is sensitive to language and annotation properties, and that these models are not easily portable across languages and schemes. An exception to that is the approach by Petrov (2009), who trained latentannotation treebank grammars and reported good accuracy on a range of languages. The CoNLL shared tasks on dependency parsing (Buchholz and Marsi, 2006; Nivre et al., 2007a) highlighted the usefulness of an alternative linguistic formalism for the development of competitive parsing models. Dependency"
W13-4917,nivre-etal-2006-talbanken05,1,0.442193,"subject agreement with respect to person and number has been dropped in modern Swedish. The Data Set The Swedish data sets are taken from the Talbanken section of the Swedish Treebank (Nivre and Megyesi, 2007). Talbanken is a syntactically annotated corpus developed in the 1970s, originally annotated according to the MAMBA scheme (Teleman, 1974) with a syntactic layer consisting of flat phrase structure and grammatical functions. The syntactic annotation was later automatically converted to full phrase structure with grammatical functions and from that to dependency structure, as described by Nivre et al. (2006). Both the phrase structure and the dependency version use the functional labels from the original MAMBA scheme, which provides a fine-grained classification of syntactic functions with 65 different labels, while the phrase structure annotation (which had to be inferred automatically) uses a coarse set of only 8 labels. For the release of the Swedish treebank, the POS level was re-annotated to conform to the current de facto standard for Swedish, which is the Stockholm-Umeå tagset (Ejerhed et al., 1992) with 25 base tags and 25 morpho-syntactic features, which together produce over 150 complex"
W13-4917,P06-1055,0,0.480329,"as more realistic parsing scenarios. 1 Introduction Syntactic parsing consists of automatically assigning to a natural language sentence a representation of its grammatical structure. Data-driven approaches to this problem, both for constituency-based and dependency-based parsing, have seen a surge of interest in the last two decades. These data-driven parsing approaches obtain state-of-the-art results on the de facto standard Wall Street Journal data set (Marcus et al., 1993) of English (Charniak, 2000; Collins, 2003; Charniak and Johnson, 2005; McDonald et al., 2005; McClosky et al., 2006; Petrov et al., 2006; Nivre et al., 2007b; Carreras et al., 2008; Finkel et al., 2008; ∗ Contact authors: djame.seddah@paris-sorbonne.fr, reut.tsarfaty@weizmann.ac.il, skuebler@indiana.edu Huang, 2008; Huang et al., 2010; Zhang and Nivre, 2011; Bohnet and Nivre, 2012; Shindo et al., 2012), and provide a foundation on which many tasks operating on semantic structure (e.g., recognizing textual entailments) or even discourse structure (coreference, summarization) crucially depend. While progress on parsing English — the main language of focus for the ACL community — has inspired some advances on other languages, it"
W13-4917,N10-1003,0,0.0195824,"2009) for Arabic morphology. A LPAGE :DYALOG and IGM:A LPAGE use external lexicons for French. IGM:A LPAGE additionally uses Morfette (Chrupała et al., 2008) for morphological analysis and POS tagging. Finally, as already mentioned, AI:KU clusters words and POS tags in an unsupervised fashion exploiting additional, un-annotated data. 5.2 Constituency Track A single team participated in the constituency parsing task, the IMS:S ZEGED :CIS team (Björkelund et al., 2013). Their phrase-structure parsing system uses a combination of 8 PCFG-LA parsers, trained using a product-of-grammars procedure (Petrov, 2010). The 50-best parses of this combination are then reranked by a model based on the reranker by Charniak and Johnson (2005).33 5.3 6.1 Baselines We additionally provide the results of two baseline systems for the nine languages, one for constituency parsing and one for dependency parsing. For the dependency track, our baseline system is MaltParser in its default configuration (the arc-eager algorithm and liblinear for training). Results marked as BASE :M ALT in the next two sections report the results of this baseline system in different scenarios. The constituency parsing baseline is based on"
W13-4917,W07-2460,0,0.109747,"Missing"
W13-4917,D07-1066,0,0.0884872,"Missing"
W13-4917,W11-3808,0,0.027114,"rameworks, or performance in the face of morphological complexity? This event provoked active discussions and led to the establishment of a series of SPMRL events for the discussion of shared challenges and cross-fertilization among researchers working on parsing MRLs. The body of work on MRLs that was accumulated through the SPMRL workshops2 and hosting ACL venues contains new results for Arabic (Attia et al., 2010; Marton et al., 2013a), Basque (Bengoetxea and Gojenola, 2010), Croatian (Agic et al., 2013), French (Seddah et al., 2010; Candito and Seddah, 2010; Sigogne et al., 2011), German (Rehbein, 2011), Hebrew (Tsarfaty and Sima’an, 2010; Goldberg and 1 http://alpage.inria.fr/iwpt09/panel.en. html 2 See http://www.spmrl.org/ and related workshops. 148 Elhadad, 2010a), Hindi (Ambati et al., 2010), Korean (Chung et al., 2010; Choi and Palmer, 2011) and Spanish (Le Roux et al., 2012), Tamil (Green et al., 2012), amongst others. The awareness of the modeling challenges gave rise to new lines of work on topics such as joint morpho-syntactic processing (Goldberg and Tsarfaty, 2008), Relational-Realizational Parsing (Tsarfaty, 2010), EasyFirst Parsing (Goldberg, 2011), PLCFRS parsing (Kallmeyer an"
W13-4917,N06-2033,0,0.0563478,"rst dependency parser capable of handling word lattice input. 163 Three participating teams use the MATE parser (Bohnet, 2010) in their systems: the BASQUE T EAM (Goenaga et al., 2013), IGM:A LPAGE (Constant et al., 2013) and IMS:S ZEGED :CIS (Björkelund et al., 2013). The BASQUE T EAM uses the MATE parser in combination with MaltParser (Nivre et al., 2007b). The system combines the parser outputs via MaltBlender (Hall et al., 2007). IGM:A LPAGE also uses MATE and MaltParser, once in a pipeline architecture and once in a joint model. The models are combined via a re-parsing strategy based on (Sagae and Lavie, 2006). This system mainly focuses on M WEs in French and uses a CRF tagger in combination with several large-scale dictionaries to handle M WEs, which then serve as input for the two parsers. The IMS:S ZEGED :CIS team participated in both tracks, with an ensemble system. For the dependency track, the ensemble includes the MATE parser (Bohnet, 2010), a best-first variant of the easy-first parser by Goldberg and Elhadad (2010b), and turbo parser (Martins et al., 2010), in combination with a ranker that has the particularity of using features from the constituent parsed trees. C ADIM (Marton et al., 2"
W13-4917,schmid-etal-2004-smor,0,0.00857226,"information and phrase-structure trees and extended with head information as described in Tsarfaty (2010, ch. 5). The unlabeled dependency version was produced by conversion from the constituency treebank as described in Goldberg (2011). Both the constituency and dependency trees were annotated with a set grammatical function labels conforming to Unified Stanford Dependencies by Tsarfaty (2013). 22 We also provided a predicted-all scenario, in which we provided morphological analysis lattices with POS and morphological information derived from the analyses of the SMOR derivational morphology (Schmid et al., 2004). These lattices were not used by any of the participants. Adapting the Data to the Shared Task While based on the same trees, the dependency and constituency treebanks differ in their POS tag sets, as well as in some of the morphological segmentation decisions. The main effort towards the shared task was unifying the two resources such that the two treebanks share the same lexical yields, and the same pre-terminal labels. To this end, we took the layering approach of Goldberg et al. (2009), and included two levels of POS tags in the constituency trees. The lower level is lexical, conforming t"
W13-4917,W10-1410,1,0.889145,"Missing"
W13-4917,seeker-kuhn-2012-making,1,0.106665,"n constituency data set is based on the TiGer treebank release 2.2.21 The original annotation scheme represents discontinuous constituents such that all arguments of a predicate are always grouped under a single node regardless of whether there is intervening material between them or not (Brants et al., 2002). Furthermore, punctuation and several other elements, such as parentheses, are not attached to the tree. In order to make the constituency treebank usable for PCFG parsing, we adapted this treebank as described shortly. The conversion of TiGer into dependencies is a variant of the one by Seeker and Kuhn (2012), which does not contain empty nodes. It is based on the same TiGer release as the one used for the constituency data. Punctuation was attached as high as possible, without creating any new non-projective edges. Adapting the Data to the Shared Task For the constituency version, punctuation and other unattached elements were first attached to the tree. As attachment target, we used roughly the respective least common ancestor node of the right and left terminal neighbor of the unattached element (see Maier et al. (2012) for details), and subsequently, the crossing branches were resolved. This w"
W13-4917,P12-1046,0,0.00731402,"based parsing, have seen a surge of interest in the last two decades. These data-driven parsing approaches obtain state-of-the-art results on the de facto standard Wall Street Journal data set (Marcus et al., 1993) of English (Charniak, 2000; Collins, 2003; Charniak and Johnson, 2005; McDonald et al., 2005; McClosky et al., 2006; Petrov et al., 2006; Nivre et al., 2007b; Carreras et al., 2008; Finkel et al., 2008; ∗ Contact authors: djame.seddah@paris-sorbonne.fr, reut.tsarfaty@weizmann.ac.il, skuebler@indiana.edu Huang, 2008; Huang et al., 2010; Zhang and Nivre, 2011; Bohnet and Nivre, 2012; Shindo et al., 2012), and provide a foundation on which many tasks operating on semantic structure (e.g., recognizing textual entailments) or even discourse structure (coreference, summarization) crucially depend. While progress on parsing English — the main language of focus for the ACL community — has inspired some advances on other languages, it has not, by itself, yielded high-quality parsing for other languages and domains. This holds in particular for morphologically rich languages (MRLs), where important information concerning the predicate-argument structure of sentences is expressed through word formatio"
W13-4917,W11-3803,0,0.0414253,"to capture performance across frameworks, or performance in the face of morphological complexity? This event provoked active discussions and led to the establishment of a series of SPMRL events for the discussion of shared challenges and cross-fertilization among researchers working on parsing MRLs. The body of work on MRLs that was accumulated through the SPMRL workshops2 and hosting ACL venues contains new results for Arabic (Attia et al., 2010; Marton et al., 2013a), Basque (Bengoetxea and Gojenola, 2010), Croatian (Agic et al., 2013), French (Seddah et al., 2010; Candito and Seddah, 2010; Sigogne et al., 2011), German (Rehbein, 2011), Hebrew (Tsarfaty and Sima’an, 2010; Goldberg and 1 http://alpage.inria.fr/iwpt09/panel.en. html 2 See http://www.spmrl.org/ and related workshops. 148 Elhadad, 2010a), Hindi (Ambati et al., 2010), Korean (Chung et al., 2010; Choi and Palmer, 2011) and Spanish (Le Roux et al., 2012), Tamil (Green et al., 2012), amongst others. The awareness of the modeling challenges gave rise to new lines of work on topics such as joint morpho-syntactic processing (Goldberg and Tsarfaty, 2008), Relational-Realizational Parsing (Tsarfaty, 2010), EasyFirst Parsing (Goldberg, 2011), PLCF"
W13-4917,W10-1405,1,0.891538,"Missing"
W13-4917,W10-1401,1,0.779419,"sentences is expressed through word formation, rather than constituent-order patterns as is the case in English and other configurational languages. MRLs express information concerning the grammatical function of a word and its grammatical relation to other words at the word level, via phenomena such as inflectional affixes, pronominal clitics, and so on (Tsarfaty et al., 2012c). The non-rigid tree structures and morphological ambiguity of input words contribute to the challenges of parsing MRLs. In addition, insufficient language resources were shown to also contribute to parsing difficulty (Tsarfaty et al., 2010; Tsarfaty et al., 2012c, and references therein). These challenges have initially been addressed by native-speaking experts using strong in-domain knowledge of the linguistic phenomena and annotation idiosyncrasies to improve the accuracy and efficiency of parsing models. More 146 Proceedings of the Fourth Workshop on Statistical Parsing of Morphologically Rich Languages, pages 146–182, c Seattle, Washington, USA, 18 October 2013. 2013 Association for Computational Linguistics recently, advances in PCFG-LA parsing (Petrov et al., 2006) and language-agnostic data-driven dependency parsing (McD"
W13-4917,D11-1036,1,0.926772,"dependency parsing. When providing both constituency-based and dependency-based tracks, it is interesting to compare results across these frameworks so as to better understand the differences in performance between parsers of different types. We are now faced with an additional question: how can we compare parsing results across different frameworks? Adopting standard metrics will not suffice as we would be comparing apples and oranges. In contrast, TedEval is defined for both phrase structures and dependency structures through the use of an intermediate representation called function trees (Tsarfaty et al., 2011; Tsarfaty et al., 2012a). Using TedEval thus allows us to explore both dependency and constituency parsing frameworks and meaningfully compare the performance of parsers of different types. 149 3 3.1 Defining the Shared-Task Input and Output We define a parser as a structure prediction function that maps sequences of space-delimited input tokens (henceforth, tokens) in a language to a set of parse trees that capture valid morpho-syntactic structures in that language. In the case of constituency parsing, the output structures are phrase-structure trees. In dependency parsing, the output consis"
W13-4917,E12-1006,1,0.148172,"er languages, it has not, by itself, yielded high-quality parsing for other languages and domains. This holds in particular for morphologically rich languages (MRLs), where important information concerning the predicate-argument structure of sentences is expressed through word formation, rather than constituent-order patterns as is the case in English and other configurational languages. MRLs express information concerning the grammatical function of a word and its grammatical relation to other words at the word level, via phenomena such as inflectional affixes, pronominal clitics, and so on (Tsarfaty et al., 2012c). The non-rigid tree structures and morphological ambiguity of input words contribute to the challenges of parsing MRLs. In addition, insufficient language resources were shown to also contribute to parsing difficulty (Tsarfaty et al., 2010; Tsarfaty et al., 2012c, and references therein). These challenges have initially been addressed by native-speaking experts using strong in-domain knowledge of the linguistic phenomena and annotation idiosyncrasies to improve the accuracy and efficiency of parsing models. More 146 Proceedings of the Fourth Workshop on Statistical Parsing of Morphologicall"
W13-4917,P13-2103,1,0.111695,"les and oranges, we use the unlabeled TedEval metric, which converts all representation types internally into the same kind of structures, called function trees. Here we use TedEval’s crossframework protocol (Tsarfaty et al., 2012a), which accomodates annotation idiosyncrasies. • Cross-Language Evaluation. Here, we compare parsers for the same representation type across different languages. Conducting a complete and faithful evaluation across languages 151 would require a harmonized universal annotation scheme (possibly along the lines of (de Marneffe and Manning, 2008; McDonald et al., 2013; Tsarfaty, 2013)) or task based evaluation. As an approximation we use unlabeled TedEval. Since it is unlabeled, it is not sensitive to label set size. Since it internally uses function-trees, it is less sensitive to annotation idiosyncrasies (e.g., head choice) (Tsarfaty et al., 2011). The former two dimensions are evaluated on the full sets. The latter two are evaluated on smaller, comparable, test sets. For completeness, we provide below the formal definitions and essential modifications of the evaluation software that we used. 3.4.1 Evaluation Metrics for Phrase Structures ParsEval The ParsEval metrics (B"
W13-4917,P11-2033,1,0.563308,"em, both for constituency-based and dependency-based parsing, have seen a surge of interest in the last two decades. These data-driven parsing approaches obtain state-of-the-art results on the de facto standard Wall Street Journal data set (Marcus et al., 1993) of English (Charniak, 2000; Collins, 2003; Charniak and Johnson, 2005; McDonald et al., 2005; McClosky et al., 2006; Petrov et al., 2006; Nivre et al., 2007b; Carreras et al., 2008; Finkel et al., 2008; ∗ Contact authors: djame.seddah@paris-sorbonne.fr, reut.tsarfaty@weizmann.ac.il, skuebler@indiana.edu Huang, 2008; Huang et al., 2010; Zhang and Nivre, 2011; Bohnet and Nivre, 2012; Shindo et al., 2012), and provide a foundation on which many tasks operating on semantic structure (e.g., recognizing textual entailments) or even discourse structure (coreference, summarization) crucially depend. While progress on parsing English — the main language of focus for the ACL community — has inspired some advances on other languages, it has not, by itself, yielded high-quality parsing for other languages and domains. This holds in particular for morphologically rich languages (MRLs), where important information concerning the predicate-argument structure o"
W13-4917,R13-1099,1,0.0375053,"orphology In order to provide the same POS tag set for the constituent and dependency treebanks, we used the dependency POS tagset for both treebank instances. Both versions of the treebank are available with gold standard and automatic morphological annotation. The automatic POS tagging was carried out by a 10-fold cross-validation on the shared task data set by magyarlanc, a natural language toolkit for processing Hungarian texts (segmentation, morphological analysis, POS tagging, and dependency parsing). The annotation provides POS tags and deep morphological features for each input token (Zsibrita et al., 2013).28 28 The full data sets of both the constituency and dependency versions of the Szeged Treebank are available at 161 4.8 The Korean Treebank The Treebank The Korean corpus is generated by collecting constituent trees from the K AIST Treebank (Choi et al., 1994), then converting the constituent trees to dependency trees using head-finding rules and heuristics. The K AIST Treebank consists of about 31K manually annotated constituent trees from 97 different sources (e.g., newspapers, novels, textbooks). After filtering out trees containing annotation errors, a total of 27,363 trees with 350,090"
W13-4917,E93-1064,0,\N,Missing
W13-4917,C00-1001,0,\N,Missing
W13-4917,C10-1061,1,\N,Missing
W13-4917,J13-1003,1,\N,Missing
W13-4917,C08-1112,1,\N,Missing
W13-4917,W08-1008,1,\N,Missing
W13-4917,P05-1022,0,\N,Missing
W13-4917,P98-1063,0,\N,Missing
W13-4917,C98-1060,0,\N,Missing
W13-4917,vincze-etal-2010-hungarian,1,\N,Missing
W13-4917,D07-1096,1,\N,Missing
W17-1704,J13-1009,0,0.0664231,"Missing"
W17-1704,W06-2408,0,0.14106,"Missing"
W17-1704,C14-1177,0,0.0618437,"Missing"
W17-1704,H05-1004,0,0.013573,"0. Note that these measures operate both on a micro scale (the optimal bijections are looked for within a given sentence) and a macro scale (the results are summed up for all sentences in the corpus). Alternatively, micro-only measures, i.e. the average values of precision and recall for individual sentences, could be considered. Given that the density of VMWEs per sentence can vary greatly, and in many languages the majority of sentences do not contain any VMWE, we believe that the macro measures are more appropriate. Note also that the measures in (2) are comparable to the CEAF-M measures (Luo, 2005) used in the coreference resolution task.20 There, mentions are grouped into entities (clusters) and the best bijection between gold and system entities is searched for. The main difference with our approach resides in the fact that, while coreference • T P 1max = |{t1,t2} ∩ {t1} |+ |{t3} ∩ {t2,t3} |= 2 R = T P 1max /||G ||= 2/3 P = T P 1max /||S1 ||= 2/3. • T P 2max = |{t1,t2} ∩ {t1} |+ |{t3} ∩ {t3} |+ |∅ ∩ {t2} |= 2 R = T P 2max /||G ||= 2/3 P = T P 2max /||S2 ||= 2/3. • T P 3max = |{t1,t2} ∩ {t1} |+ |{t3} ∩ {t3} |+ |∅ ∩ {t2} |+ |∅ ∩ {t1,t3} |= 2 R = T P 3max /||G ||= 2/3 P = T P 3max /||S3"
W17-1704,I11-1024,0,0.0544171,"Missing"
W17-1704,J15-3003,0,0.143842,"ation of this large project included the definition of roles – project leaders, technical experts, language group leaders (LGLs), language leaders (LLs) and annotators – and their tasks. Annotation Methodology In order to bring about substantial progress in the state of the art presented in the preceding section, the European PARSEME network5 , dedicated to parsing and MWEs, proposed a shared task on automatic identification of VMWEs. This initiative required the construction of a large multilingual VMWE-annotated corpus. Within the challenging features of linguistic annotation, as defined by Mathet et al. (2015), the VMWE annotation task is concerned by: 3.1 The biggest challenge in the initial phase of the project was the development of the annotation guidelines7 which would be as universal as possible but which would still allow for languagespecific categories and tests. To this end, a twophase pilot annotation in most of the participating languages was carried out. Some corpora were annotated at this stage not only by native but also by near-native speakers, so as to promote cross-language convergences. Each pilot annotation phase provided feedback from annotators and was followed by enhancements"
W17-1704,Q14-1016,0,0.0486965,"and/or verbal idioms. They also underline the heterogeneity of these MWE annotations. Nivre and Vincze (2015) show that this is also the case in the treebanks of Universal Dependencies (UD), despite the homogenizing objective of the UD project (McDonald et al., 2013). More recent efforts (Adalı et al., 2016), while addressing VMWEs in a comprehensive way, still suffer from missing annotation standards. 3 2 4 http://multiword.sourceforge.net/sharedtask2017 32 http://multiword.sf.net/ http://dimsum16.github.io (DE) auf|machen (lit. out|make) ’open’.6 discontinuous. They were annotated following Schneider et al. (2014b), and thus contain several VMWEs types on top of non-verbal MWEs. Links between MWE identification and syntactic parsing have also long been an issue. While the former has often been treated as a pre-processing step before the latter, both tasks are now more and more often integrated, in particular for continuous MWE categories (Finkel and Manning, 2009; Green et al., 2011; Green et al., 2013; Candito and Constant, 2014; Le Roux et al., 2014; Nasr et al., 2015; Constant and Nivre, 2016). Fewer works deal with verbal MWEs (Wehrli et al., 2010; Vincze et al., 2013; Wehrli, 2014; Waszczuk et al"
W17-1704,P15-1108,1,0.941692,"ultiword.sf.net/ http://dimsum16.github.io (DE) auf|machen (lit. out|make) ’open’.6 discontinuous. They were annotated following Schneider et al. (2014b), and thus contain several VMWEs types on top of non-verbal MWEs. Links between MWE identification and syntactic parsing have also long been an issue. While the former has often been treated as a pre-processing step before the latter, both tasks are now more and more often integrated, in particular for continuous MWE categories (Finkel and Manning, 2009; Green et al., 2011; Green et al., 2013; Candito and Constant, 2014; Le Roux et al., 2014; Nasr et al., 2015; Constant and Nivre, 2016). Fewer works deal with verbal MWEs (Wehrli et al., 2010; Vincze et al., 2013; Wehrli, 2014; Waszczuk et al., 2016). 3 This complexity is largely increased by the multilingual nature of the task, and calls for efficient project management. The 21 participating languages were divided into four language groups (LGs): Balto-Slavic: Bulgarian (BG), Croatian (HR), Czech (CS), Lithuanian (LT), Polish (PL) and Slovene (SL); Germanic: English (EN), German (DE), Swedish (SV) and Yiddish (YI); Romance: French (FR), Italian (IT), Romanian (RO), Spanish (ES) and Brazilian Portug"
W17-1704,C16-1042,1,0.824829,"r et al. (2014b), and thus contain several VMWEs types on top of non-verbal MWEs. Links between MWE identification and syntactic parsing have also long been an issue. While the former has often been treated as a pre-processing step before the latter, both tasks are now more and more often integrated, in particular for continuous MWE categories (Finkel and Manning, 2009; Green et al., 2011; Green et al., 2013; Candito and Constant, 2014; Le Roux et al., 2014; Nasr et al., 2015; Constant and Nivre, 2016). Fewer works deal with verbal MWEs (Wehrli et al., 2010; Vincze et al., 2013; Wehrli, 2014; Waszczuk et al., 2016). 3 This complexity is largely increased by the multilingual nature of the task, and calls for efficient project management. The 21 participating languages were divided into four language groups (LGs): Balto-Slavic: Bulgarian (BG), Croatian (HR), Czech (CS), Lithuanian (LT), Polish (PL) and Slovene (SL); Germanic: English (EN), German (DE), Swedish (SV) and Yiddish (YI); Romance: French (FR), Italian (IT), Romanian (RO), Spanish (ES) and Brazilian Portuguese (PT); and others: Farsi (FA), Greek (EL), Hebrew (HE), Hungarian (HU), Maltese (MT) and Turkish (TR). Note that the 4 last are non-Indo-E"
W17-1704,S16-1084,0,0.0544884,"Missing"
W17-1704,W10-3705,0,0.0808784,"Missing"
W17-1704,W11-0807,0,0.0214913,"Missing"
W17-1704,W14-0804,0,0.0310488,"owing Schneider et al. (2014b), and thus contain several VMWEs types on top of non-verbal MWEs. Links between MWE identification and syntactic parsing have also long been an issue. While the former has often been treated as a pre-processing step before the latter, both tasks are now more and more often integrated, in particular for continuous MWE categories (Finkel and Manning, 2009; Green et al., 2011; Green et al., 2013; Candito and Constant, 2014; Le Roux et al., 2014; Nasr et al., 2015; Constant and Nivre, 2016). Fewer works deal with verbal MWEs (Wehrli et al., 2010; Vincze et al., 2013; Wehrli, 2014; Waszczuk et al., 2016). 3 This complexity is largely increased by the multilingual nature of the task, and calls for efficient project management. The 21 participating languages were divided into four language groups (LGs): Balto-Slavic: Bulgarian (BG), Croatian (HR), Czech (CS), Lithuanian (LT), Polish (PL) and Slovene (SL); Germanic: English (EN), German (DE), Swedish (SV) and Yiddish (YI); Romance: French (FR), Italian (IT), Romanian (RO), Spanish (ES) and Brazilian Portuguese (PT); and others: Farsi (FA), Greek (EL), Hebrew (HE), Hungarian (HU), Maltese (MT) and Turkish (TR). Note that t"
W17-1704,S12-1010,0,0.026905,"Missing"
W17-1704,W16-1812,0,0.0461931,"Missing"
W17-1704,C10-1125,1,0.793436,"Missing"
W17-1704,R11-1040,1,0.890663,"Missing"
W17-1704,N09-1037,0,\N,Missing
W17-1704,P14-1070,1,\N,Missing
W17-1704,P16-1016,0,\N,Missing
W17-1717,W06-1620,0,0.135451,"81 0,7 0,65 0,86 0,48 0,25 0,58 0,484 System setting BCFILM BCDEFGJKLMO FHIKLNPQ EFGHJKLN BCDEFGHIJKLM EFGJKM EFGHJKLMN BCDEFLNP BCDEFGHIJLQ CFGHJKP BCDEFIKLMNOP BCDFGJKLOP DFGHJL BCDEFGHIJKLMNOQ BCDEFGIJKMN DFGJLP BCDEFGHIJKLMNO BCDEFGHJM Table 2: Detailed results of all experiments over all the languages. F columns provide F-score results and delta columns display the difference in F-score (times 10−2 ) between our system and the best other system of the shared task for the current evaluation/language configuration. general (including verbal expressions) ranging from contiguous expressions (Blunsom and Baldwin, 2006) to gappy ones (Schneider et al., 2014). A joint syntactic analysis and VMWE identification approach using off-the-shelf parsers is another interesting alternative that has shown to help VMWE identification such as light verb constructions (Eryi˘git et al., 2011; Vincze et al., 2013). robustness of our approach. Moreover, evaluation using per-MWE F-score (i.e. exact VMWE matching) ranks our system first on all languages but two (HU:2nd:, RO:3rd), displaying an average difference of 6.73 points with the best other system in the current evaluation/language pair. Concerning per-token scores (whic"
W17-1717,I13-1024,0,0.044249,"all the languages. F columns provide F-score results and delta columns display the difference in F-score (times 10−2 ) between our system and the best other system of the shared task for the current evaluation/language configuration. general (including verbal expressions) ranging from contiguous expressions (Blunsom and Baldwin, 2006) to gappy ones (Schneider et al., 2014). A joint syntactic analysis and VMWE identification approach using off-the-shelf parsers is another interesting alternative that has shown to help VMWE identification such as light verb constructions (Eryi˘git et al., 2011; Vincze et al., 2013). robustness of our approach. Moreover, evaluation using per-MWE F-score (i.e. exact VMWE matching) ranks our system first on all languages but two (HU:2nd:, RO:3rd), displaying an average difference of 6.73 points with the best other system in the current evaluation/language pair. Concerning per-token scores (which allow partial matchings), results are relatively lower: our system is ranked first for 12 languages (out of 18), with a positive average difference of 1.84 points as compared with the best other system. Such very enthusiastic results for per-MWE evaluations seem to show that our sy"
W17-1717,P16-1016,1,0.841262,"rd expressions. We participated in the closed track only, for all the 18 available languages. Our system is a robust greedy transition-based system, in which MWE are identified through a MERGE transition. The system was meant to accommodate the variety of linguistic resources provided for each language, in terms of accompanying morphological and syntactic information. Using per-MWE Fscore, the system was ranked first1 for all but two languages (Hungarian and Romanian). 1 2 System description The identification system we used is a simplified and partial implementation of the system proposed in Constant and Nivre (2016), which is in itself a mild extension of an arc-standard dependency parser (Nivre, 2004). Constant and Nivre (2016) proposed a parsing algorithm that jointly predicts a syntactic dependency tree and a forest of lexical units including MWEs. In particular, in line with Nivre (2014), this system integrates special parsing mechanisms to deal with lexical analysis. Given that the shared task focuses on the lexical task only and that datasets do not always provide syntactic annotations, we have modified the structure of the original system by removing syntax prediction, in order to use the same sys"
W17-1717,P11-2033,0,0.0262819,"n the number of training VMWEs and the performance. This suggests that the size of training datasets is not large enough as systems’ performance does not converge. We note though that some languages like CS and TR reach relatively low scores given the size of training data, which shows the high complexity of this task for these languages. When comparing to the other shared task systems, we can observe that our system is the only one that handled all 18 languages, showing the Distance-based Features Distance between sentence components is also known to help transition-based dependency parsing (Zhang and Nivre, 2011). We thus added the distance between S0 and B0 and the distance between S0 and S1 (cf. settings K and M in Table 1). Dictionary-based Features We also added features based on the VMWE dictionary automatically extracted from the training set. Such features inform the system when one of the focused elements (Si , Bj ) is a component of a VMWE present in the dictionary (cf. L in Table 1). Stack-length Features Using the length of the stack as an additional feature (cf. O in Table 1) has also proven beneficial during our feature tuning. 7 For the shared task, we used gold syntactic features for th"
W17-1717,W09-2903,0,0.0842377,"Missing"
W17-1717,J09-1005,0,0.0288306,"here feature template design and tuning could play a very important role in increasing the accuracy of system results. Basic Linguistic Features First of all, depending on their availability in the working dataset and on the activation of related settings (cf. G and J in Table 1), we extracted linguistic attributes in order to generate features such as S0 l, S0 p and S0 w where p, l and Syntax-based Features After integrating classical linguistic attributes, we investigated using more linguistically sophisticated features. First of all, syntactic structure is known to help MWE identification (Fazly et al., 2009; Seretan, 2011; Nagy T. and Vincze, 2014). We therefore inform the system with the 5 The whole system was developed using Python 2.7, with 2,200 lines of code, using the open-source Scikit-learn 0.19 libraries for the SVMs. The code is available on Github: https://goo.gl/EDFyiM 6 These correspond mainly to cases of verb-particle (tagged VPC in the data sets) in which the particle is not separated from the verb. 129 provided syntactic dependencies when available: for each token Bn that both appears in the buffer and is a syntactic dependent of S0 with label l, we capture the existence of the d"
W17-1717,W14-0803,0,0.0435038,"Missing"
W17-1717,W04-0308,0,0.089546,"tem is a robust greedy transition-based system, in which MWE are identified through a MERGE transition. The system was meant to accommodate the variety of linguistic resources provided for each language, in terms of accompanying morphological and syntactic information. Using per-MWE Fscore, the system was ranked first1 for all but two languages (Hungarian and Romanian). 1 2 System description The identification system we used is a simplified and partial implementation of the system proposed in Constant and Nivre (2016), which is in itself a mild extension of an arc-standard dependency parser (Nivre, 2004). Constant and Nivre (2016) proposed a parsing algorithm that jointly predicts a syntactic dependency tree and a forest of lexical units including MWEs. In particular, in line with Nivre (2014), this system integrates special parsing mechanisms to deal with lexical analysis. Given that the shared task focuses on the lexical task only and that datasets do not always provide syntactic annotations, we have modified the structure of the original system by removing syntax prediction, in order to use the same system for all 18 languages. A transition-based system consists in applying a sequence of a"
W17-1717,Q14-1016,0,0.130304,"em setting BCFILM BCDEFGJKLMO FHIKLNPQ EFGHJKLN BCDEFGHIJKLM EFGJKM EFGHJKLMN BCDEFLNP BCDEFGHIJLQ CFGHJKP BCDEFIKLMNOP BCDFGJKLOP DFGHJL BCDEFGHIJKLMNOQ BCDEFGIJKMN DFGJLP BCDEFGHIJKLMNO BCDEFGHJM Table 2: Detailed results of all experiments over all the languages. F columns provide F-score results and delta columns display the difference in F-score (times 10−2 ) between our system and the best other system of the shared task for the current evaluation/language configuration. general (including verbal expressions) ranging from contiguous expressions (Blunsom and Baldwin, 2006) to gappy ones (Schneider et al., 2014). A joint syntactic analysis and VMWE identification approach using off-the-shelf parsers is another interesting alternative that has shown to help VMWE identification such as light verb constructions (Eryi˘git et al., 2011; Vincze et al., 2013). robustness of our approach. Moreover, evaluation using per-MWE F-score (i.e. exact VMWE matching) ranks our system first on all languages but two (HU:2nd:, RO:3rd), displaying an average difference of 6.73 points with the best other system in the current evaluation/language pair. Concerning per-token scores (which allow partial matchings), results are"
W17-6507,W00-1436,0,0.0169751,", 1988)), as instanced in the recent AnCora-UPF treebank (Mille et al., 2013; Ballesteros et al., 2016), and our proposal. The MTT defines an explicit deep syntactic representation level13 , hereafter DSyntS. The AnCora-UPF Treebank follows its four layer model: morphological, surface-syntactic, deepsyntactic and semantic. The method used for annotating that corpus is similar to the procedure we used. Starting from the surface-syntactic level, the two other levels are automatically pre-annotated step by step: the annotation of a given level is rewritten to the next level using the MATE tools (Bohnet et al., 2000). Results and Error Analysis We evaluated the production of enhanced UD graphs in two settings, depending on whether the input UD trees do (PA+) or do not (PA−) contain manual disambiguation of cases (a), (b) and (c) described above. For the PA− case, we applied basic default rules instead, known to use insufficient information. Table 1 reports the F-measures (computed considering all edges or N ∪ A edges only). These results confirm the validity of our approach and highlight the consistency of the resulting graphbanks. Moreover, even if manual preannotations are required in theory, we empiric"
W17-6507,F12-2024,1,0.786077,"Missing"
W17-6507,J16-4009,0,0.0211653,"ations of large scale project such as the PDT (B¨ohmov´a et al., 2003), methods aiming at automatically enriching syntactic trees with deeper structures have peaked a decade ago (Hockenmaier, 2003; Cahill et al., 2004; Miyao and Tsujii, 2005) but have then been subsumed by purely data-driven methods when corpora with richer annotation have been made available (Hajic et al., 2006; Oepen et al., 2014; Mille et al., 2013). Space is missing for an in-depth comparison between these different annotation scheme, we refer the reader to (Rimell et al., 2009; Ivanova et al., 2012; Candito et al., 2014; Kuhlmann and Oepen, 2016) for a more complete overview. Here, we will focus on the differences between the Meaning Text Theory (MTT, (Melˇcuk, 1988)), as instanced in the recent AnCora-UPF treebank (Mille et al., 2013; Ballesteros et al., 2016), and our proposal. The MTT defines an explicit deep syntactic representation level13 , hereafter DSyntS. The AnCora-UPF Treebank follows its four layer model: morphological, surface-syntactic, deepsyntactic and semantic. The method used for annotating that corpus is similar to the procedure we used. Starting from the surface-syntactic level, the two other levels are automatical"
W17-6507,candito-etal-2014-deep,1,0.801748,"are implemented through diverse and, in some few cases, multilingual graphbanks. More clearly semantic schemes seem to depend on the needs of the downstream application or impose their own constraints on the syntactic layer it is either built upon or plugged in. See for example the differences between abstract meaning representations (Knight et al., 2014), designed with Machine Translation in sight, and the U DEP L AMBDA’s logical structures, very recently proposed by Reddy et al. (2017) and evaluated on a question-answering over a knowledge base task. In this paper, we build on the work of (Candito et al., 2014; Perrier et al., 2014) to propose an extension to the current enhanced dependency framework of Schuster and Manning (2016). First, we extend the types of argumental dependencies made explicit (taking into account participles, control nouns and adjectives, non-finite verbs and more cases of infinitive verbs). Second, we neutralize syntactic alternations, in order to make linking patterns more regular for a given verb form. We believe that making explicit and normalize the predicate-argument structures, still remaining at the syntactic level, can make downstream semantic analysis more straightf"
W17-6507,P13-2017,0,0.0733126,"Missing"
W17-6507,C16-1040,1,0.896954,"Missing"
W17-6507,W16-1715,0,0.0193644,"ransitives, then the canonical labels can be made explicit as shown in figure 9. Note that the canonical function of the Passive Passive is by far the most frequent syntactic alternation, and it is fortunately rather easy to identify in a language such as French. Note that because the UD scheme uses several labels for the same argumental slot, depending on the argument’s category, the basic rule of having the passive’s subject being the canonical direct object has to be split. The nsubj:pass dependent is considered the canonical obj. The csubj:pass dependent is 8 This is already identified by Gerdes and Kahane (2016), who advocate for directly adding the semantic argument rank (1,2,3...) on top of the syntactic label. 46 4.3 nsubj:pass argument is iobj if the verb has a direct object (Fig. 9a) or obj otherwise (Fig. 9b). nsubj:pass@iobj aux:pass (a) He was given case orders nsubj:pass@obj aux:pass (b) Orders Impersonal constructions can also be viewed as syntactic alternations: in French the postverbal complement has object-like properties (in particular the pronominalization with the quantitative clitic en (of-it)). obl@nsubj obj were by Impersonal them obl@iobj case given to nsubj@expl aux him Il It nsu"
W17-6507,W13-3724,0,0.0431398,"Missing"
W17-6507,P05-1011,0,0.012789,"Missing"
W17-6507,S14-2008,0,0.203066,"Missing"
W17-6507,F14-2031,1,0.923846,"gh diverse and, in some few cases, multilingual graphbanks. More clearly semantic schemes seem to depend on the needs of the downstream application or impose their own constraints on the syntactic layer it is either built upon or plugged in. See for example the differences between abstract meaning representations (Knight et al., 2014), designed with Machine Translation in sight, and the U DEP L AMBDA’s logical structures, very recently proposed by Reddy et al. (2017) and evaluated on a question-answering over a knowledge base task. In this paper, we build on the work of (Candito et al., 2014; Perrier et al., 2014) to propose an extension to the current enhanced dependency framework of Schuster and Manning (2016). First, we extend the types of argumental dependencies made explicit (taking into account participles, control nouns and adjectives, non-finite verbs and more cases of infinitive verbs). Second, we neutralize syntactic alternations, in order to make linking patterns more regular for a given verb form. We believe that making explicit and normalize the predicate-argument structures, still remaining at the syntactic level, can make downstream semantic analysis more straightforward (as shown for in"
W17-6507,W12-3602,0,0.0290507,"nnotated corpora and given the cost of annotations of large scale project such as the PDT (B¨ohmov´a et al., 2003), methods aiming at automatically enriching syntactic trees with deeper structures have peaked a decade ago (Hockenmaier, 2003; Cahill et al., 2004; Miyao and Tsujii, 2005) but have then been subsumed by purely data-driven methods when corpora with richer annotation have been made available (Hajic et al., 2006; Oepen et al., 2014; Mille et al., 2013). Space is missing for an in-depth comparison between these different annotation scheme, we refer the reader to (Rimell et al., 2009; Ivanova et al., 2012; Candito et al., 2014; Kuhlmann and Oepen, 2016) for a more complete overview. Here, we will focus on the differences between the Meaning Text Theory (MTT, (Melˇcuk, 1988)), as instanced in the recent AnCora-UPF treebank (Mille et al., 2013; Ballesteros et al., 2016), and our proposal. The MTT defines an explicit deep syntactic representation level13 , hereafter DSyntS. The AnCora-UPF Treebank follows its four layer model: morphological, surface-syntactic, deepsyntactic and semantic. The method used for annotating that corpus is similar to the procedure we used. Starting from the surface-synt"
W17-6507,petrov-etal-2012-universal,0,0.115553,"Missing"
W17-6507,D17-1009,0,0.0785386,"Missing"
W17-6507,D09-1085,0,0.0169912,"e the rise of large annotated corpora and given the cost of annotations of large scale project such as the PDT (B¨ohmov´a et al., 2003), methods aiming at automatically enriching syntactic trees with deeper structures have peaked a decade ago (Hockenmaier, 2003; Cahill et al., 2004; Miyao and Tsujii, 2005) but have then been subsumed by purely data-driven methods when corpora with richer annotation have been made available (Hajic et al., 2006; Oepen et al., 2014; Mille et al., 2013). Space is missing for an in-depth comparison between these different annotation scheme, we refer the reader to (Rimell et al., 2009; Ivanova et al., 2012; Candito et al., 2014; Kuhlmann and Oepen, 2016) for a more complete overview. Here, we will focus on the differences between the Meaning Text Theory (MTT, (Melˇcuk, 1988)), as instanced in the recent AnCora-UPF treebank (Mille et al., 2013; Ballesteros et al., 2016), and our proposal. The MTT defines an explicit deep syntactic representation level13 , hereafter DSyntS. The AnCora-UPF Treebank follows its four layer model: morphological, surface-syntactic, deepsyntactic and semantic. The method used for annotating that corpus is similar to the procedure we used. Starting"
W17-6507,L16-1376,0,0.398273,"dah@paris-sorbonne.fr bruno.guillaume@loria.fr marie.candito@linguist.univ-paris-diderot.fr Abstract released annotated versions of their treebanks, following the UD annotation scheme. Although UD has raised criticisms, both on the suitability of the scheme to meet linguistic typology (Croft et al., 2017) and on the current implementation of the UD treebanks (Gerdes and Kahane, 2016), the existence of many treebanks with same syntactic scheme does however ease crosslanguage linguistic analysis and enables parsers to generalize across languages at training time, as demonstrated by Ammar et al. (2016). The UD scheme favors dependencies between content words, in order to maximize parallelism between languages. Although this results in dependencies that are more semantic-oriented, the UD scheme lies at the surface syntax level and thus necessarily lacks abstraction over syntactic variation and does not fit all downstream applications’ needs (Schuster and Manning, 2016). This is partly why de Marneffe and Manning (2008) proposed a decade ago, in the Stanford Dependencies framework, several schemes with various semantic-oriented modifications of syntactic structures. Its graph-based, so-called"
W17-6507,P04-1041,0,\N,Missing
W17-6507,L16-1262,0,\N,Missing
W18-4925,W17-1717,1,0.829766,"on in syntactic parsing has also gained some popularity in recent years. While often treated as a pre-processing step for parsing, both tasks are now more and more integrated (Finkel and Manning, 2009; Green et al., 2011; Green et al., 2013; Candito and Constant, 2014; Le Roux et al., 2014; Nasr et al., 2015; Constant and Nivre, 2016). Although fewer works deal with verbal MWEs, there are some notable exceptions (Wehrli et al., 2010; Vincze et al., 2013; Wehrli, 2014; Waszczuk et al., 2016). Some systems that participated in edition 1.0 of the PARSEME Shared Task are also based on parsing (Al Saied et al., 2017; Nerima et al., 2017; Simkó et al., 2017). Other approaches to MWE identification include sequence labeling using CRFs (Boro¸s et al., 2017; Maldonado et al., 2017) and neural networks (Klyueva et al., 2017). 3 Enhanced Annotation Methodology The first PARSEME annotation campaign (Savary et al., forthcoming) generated a rich feedback from annotators and language team leaders. It also attracted the interest of new teams, working on languages not covered by the previous version of the PARSEME corpora. About 80 issues were raised and discussed among dozens of contributors.1 This boosted our effo"
W18-4925,W17-1716,1,0.892903,"Missing"
W18-4925,P14-1070,1,0.86208,"mon guidelines. They highlight the heterogeneity of MWE annotation practices. Similar conclusions have been drawn for Universal Dependencies (McDonald et al., 2013). With regard to these conclusions, we intended to provide unified guidelines for all the participating languages, in order to avoid heterogeneous, hence incomparable, datasets. MWE identification in syntactic parsing has also gained some popularity in recent years. While often treated as a pre-processing step for parsing, both tasks are now more and more integrated (Finkel and Manning, 2009; Green et al., 2011; Green et al., 2013; Candito and Constant, 2014; Le Roux et al., 2014; Nasr et al., 2015; Constant and Nivre, 2016). Although fewer works deal with verbal MWEs, there are some notable exceptions (Wehrli et al., 2010; Vincze et al., 2013; Wehrli, 2014; Waszczuk et al., 2016). Some systems that participated in edition 1.0 of the PARSEME Shared Task are also based on parsing (Al Saied et al., 2017; Nerima et al., 2017; Simkó et al., 2017). Other approaches to MWE identification include sequence labeling using CRFs (Boro¸s et al., 2017; Maldonado et al., 2017) and neural networks (Klyueva et al., 2017). 3 Enhanced Annotation Methodology The fi"
W18-4925,F12-2024,1,0.868442,"om the Croatian version of the SETimes corpora: mostly running text but also selected fragments, such as introductory blurbs and image descriptions characteristic of newswire text. The English corpus consists of 7,437 sentences taken from three of the UD: the Gold Standard Universal Dependencies Corpus for English, the LinES parallel corpus and the Parallel Universal Dependencies treebank. The Farsi corpus is built on top of the MULTEXT-East corpora (QasemiZadeh and Rahimi, 2006) and VMWE annotations are added to a portion of Orwell’s 1984 novel. The French corpus contains the Sequoia corpus (Candito and Seddah, 2012) converted to UD, the GDS French UD treebank, the French part of the Partut corpus, and part of the Parallel UD (PUD) corpus. The German corpus contains shuffled sentences crawled from online news, reviews and wikis, derived from the WMT16 shared task data (Bojar et al., 2016), and Universal Dependencies v2.0. The Greek corpus comprises Wikipedia articles and newswire texts from various on-line newspaper editions and news portals. The Hebrew corpus contains news and articles from Arutz 7 and HaAretz news websites, collected by the MILA Knowledge Center for Processing Hebrew. The Hindi corpus r"
W18-4925,P16-1016,0,0.0692628,"actices. Similar conclusions have been drawn for Universal Dependencies (McDonald et al., 2013). With regard to these conclusions, we intended to provide unified guidelines for all the participating languages, in order to avoid heterogeneous, hence incomparable, datasets. MWE identification in syntactic parsing has also gained some popularity in recent years. While often treated as a pre-processing step for parsing, both tasks are now more and more integrated (Finkel and Manning, 2009; Green et al., 2011; Green et al., 2013; Candito and Constant, 2014; Le Roux et al., 2014; Nasr et al., 2015; Constant and Nivre, 2016). Although fewer works deal with verbal MWEs, there are some notable exceptions (Wehrli et al., 2010; Vincze et al., 2013; Wehrli, 2014; Waszczuk et al., 2016). Some systems that participated in edition 1.0 of the PARSEME Shared Task are also based on parsing (Al Saied et al., 2017; Nerima et al., 2017; Simkó et al., 2017). Other approaches to MWE identification include sequence labeling using CRFs (Boro¸s et al., 2017; Maldonado et al., 2017) and neural networks (Klyueva et al., 2017). 3 Enhanced Annotation Methodology The first PARSEME annotation campaign (Savary et al., forthcoming) generat"
W18-4925,J17-4005,1,0.881403,"Missing"
W18-4925,N09-1037,0,0.0413995,"anks for 15 languages, collaboratively documented according to common guidelines. They highlight the heterogeneity of MWE annotation practices. Similar conclusions have been drawn for Universal Dependencies (McDonald et al., 2013). With regard to these conclusions, we intended to provide unified guidelines for all the participating languages, in order to avoid heterogeneous, hence incomparable, datasets. MWE identification in syntactic parsing has also gained some popularity in recent years. While often treated as a pre-processing step for parsing, both tasks are now more and more integrated (Finkel and Manning, 2009; Green et al., 2011; Green et al., 2013; Candito and Constant, 2014; Le Roux et al., 2014; Nasr et al., 2015; Constant and Nivre, 2016). Although fewer works deal with verbal MWEs, there are some notable exceptions (Wehrli et al., 2010; Vincze et al., 2013; Wehrli, 2014; Waszczuk et al., 2016). Some systems that participated in edition 1.0 of the PARSEME Shared Task are also based on parsing (Al Saied et al., 2017; Nerima et al., 2017; Simkó et al., 2017). Other approaches to MWE identification include sequence labeling using CRFs (Boro¸s et al., 2017; Maldonado et al., 2017) and neural netwo"
W18-4925,D11-1067,0,0.0237076,"Missing"
W18-4925,J13-1009,0,0.0221261,"Missing"
W18-4925,W17-1707,0,0.0299396,"en et al., 2011; Green et al., 2013; Candito and Constant, 2014; Le Roux et al., 2014; Nasr et al., 2015; Constant and Nivre, 2016). Although fewer works deal with verbal MWEs, there are some notable exceptions (Wehrli et al., 2010; Vincze et al., 2013; Wehrli, 2014; Waszczuk et al., 2016). Some systems that participated in edition 1.0 of the PARSEME Shared Task are also based on parsing (Al Saied et al., 2017; Nerima et al., 2017; Simkó et al., 2017). Other approaches to MWE identification include sequence labeling using CRFs (Boro¸s et al., 2017; Maldonado et al., 2017) and neural networks (Klyueva et al., 2017). 3 Enhanced Annotation Methodology The first PARSEME annotation campaign (Savary et al., forthcoming) generated a rich feedback from annotators and language team leaders. It also attracted the interest of new teams, working on languages not covered by the previous version of the PARSEME corpora. About 80 issues were raised and discussed among dozens of contributors.1 This boosted our efforts towards a better understanding of VMWErelated phenomena, and towards a better synergy of terminologies across languages and linguistic traditions. The annotation guidelines were gradually enhanced, so as"
W18-4925,C14-1177,0,0.023678,"Missing"
W18-4925,W14-0406,0,0.112167,"Missing"
W18-4925,W17-1715,0,0.019845,"ore integrated (Finkel and Manning, 2009; Green et al., 2011; Green et al., 2013; Candito and Constant, 2014; Le Roux et al., 2014; Nasr et al., 2015; Constant and Nivre, 2016). Although fewer works deal with verbal MWEs, there are some notable exceptions (Wehrli et al., 2010; Vincze et al., 2013; Wehrli, 2014; Waszczuk et al., 2016). Some systems that participated in edition 1.0 of the PARSEME Shared Task are also based on parsing (Al Saied et al., 2017; Nerima et al., 2017; Simkó et al., 2017). Other approaches to MWE identification include sequence labeling using CRFs (Boro¸s et al., 2017; Maldonado et al., 2017) and neural networks (Klyueva et al., 2017). 3 Enhanced Annotation Methodology The first PARSEME annotation campaign (Savary et al., forthcoming) generated a rich feedback from annotators and language team leaders. It also attracted the interest of new teams, working on languages not covered by the previous version of the PARSEME corpora. About 80 issues were raised and discussed among dozens of contributors.1 This boosted our efforts towards a better understanding of VMWErelated phenomena, and towards a better synergy of terminologies across languages and linguistic traditions. The annotation"
W18-4925,P15-1108,1,0.836029,"f MWE annotation practices. Similar conclusions have been drawn for Universal Dependencies (McDonald et al., 2013). With regard to these conclusions, we intended to provide unified guidelines for all the participating languages, in order to avoid heterogeneous, hence incomparable, datasets. MWE identification in syntactic parsing has also gained some popularity in recent years. While often treated as a pre-processing step for parsing, both tasks are now more and more integrated (Finkel and Manning, 2009; Green et al., 2011; Green et al., 2013; Candito and Constant, 2014; Le Roux et al., 2014; Nasr et al., 2015; Constant and Nivre, 2016). Although fewer works deal with verbal MWEs, there are some notable exceptions (Wehrli et al., 2010; Vincze et al., 2013; Wehrli, 2014; Waszczuk et al., 2016). Some systems that participated in edition 1.0 of the PARSEME Shared Task are also based on parsing (Al Saied et al., 2017; Nerima et al., 2017; Simkó et al., 2017). Other approaches to MWE identification include sequence labeling using CRFs (Boro¸s et al., 2017; Maldonado et al., 2017) and neural networks (Klyueva et al., 2017). 3 Enhanced Annotation Methodology The first PARSEME annotation campaign (Savary e"
W18-4925,W17-1706,0,0.0136581,"ing has also gained some popularity in recent years. While often treated as a pre-processing step for parsing, both tasks are now more and more integrated (Finkel and Manning, 2009; Green et al., 2011; Green et al., 2013; Candito and Constant, 2014; Le Roux et al., 2014; Nasr et al., 2015; Constant and Nivre, 2016). Although fewer works deal with verbal MWEs, there are some notable exceptions (Wehrli et al., 2010; Vincze et al., 2013; Wehrli, 2014; Waszczuk et al., 2016). Some systems that participated in edition 1.0 of the PARSEME Shared Task are also based on parsing (Al Saied et al., 2017; Nerima et al., 2017; Simkó et al., 2017). Other approaches to MWE identification include sequence labeling using CRFs (Boro¸s et al., 2017; Maldonado et al., 2017) and neural networks (Klyueva et al., 2017). 3 Enhanced Annotation Methodology The first PARSEME annotation campaign (Savary et al., forthcoming) generated a rich feedback from annotators and language team leaders. It also attracted the interest of new teams, working on languages not covered by the previous version of the PARSEME corpora. About 80 issues were raised and discussed among dozens of contributors.1 This boosted our efforts towards a better"
W18-4925,L16-1262,0,0.0499042,"Missing"
W18-4925,S16-1084,1,0.93644,"participating systems, their methods and obtained results are also presented and analysed. 1 Introduction Across languages, multiword expressions (MWEs) are widely recognized as a significant challenge for natural language processing (NLP) (Sag et al., 2002; Baldwin and Kim, 2010). An international and highly multilingual research community, forged via regular workshops and initiatives such as the PARSEME network (Savary et al., 2015), has rallied around the goals of characterizing MWEs in lexicons, grammars and corpora and enabling systems to process them. Recent shared tasks, namely DiMSUM (Schneider et al., 2016) and the first edition of the PARSEME Shared Task on automatic identification of verbal multiword expressions in 2017 (Savary et al., 2017), have helped drive MWE research forward, yielding new corpora and testbeds for MWEs identification systems. This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/. License details: http:// 222 Proceedings of the Joint Workshop on Linguistic Annotation, Multiword Expressions and Constructions (LAW-MWE-CxG-2018), pages 222–240 Santa Fe, New Mexico, USA, August 25-26, 2018. This paper describ"
W18-4925,W17-1705,1,0.741206,"ome popularity in recent years. While often treated as a pre-processing step for parsing, both tasks are now more and more integrated (Finkel and Manning, 2009; Green et al., 2011; Green et al., 2013; Candito and Constant, 2014; Le Roux et al., 2014; Nasr et al., 2015; Constant and Nivre, 2016). Although fewer works deal with verbal MWEs, there are some notable exceptions (Wehrli et al., 2010; Vincze et al., 2013; Wehrli, 2014; Waszczuk et al., 2016). Some systems that participated in edition 1.0 of the PARSEME Shared Task are also based on parsing (Al Saied et al., 2017; Nerima et al., 2017; Simkó et al., 2017). Other approaches to MWE identification include sequence labeling using CRFs (Boro¸s et al., 2017; Maldonado et al., 2017) and neural networks (Klyueva et al., 2017). 3 Enhanced Annotation Methodology The first PARSEME annotation campaign (Savary et al., forthcoming) generated a rich feedback from annotators and language team leaders. It also attracted the interest of new teams, working on languages not covered by the previous version of the PARSEME corpora. About 80 issues were raised and discussed among dozens of contributors.1 This boosted our efforts towards a better understanding of VMWE"
W18-4925,I13-1024,1,0.84782,"ns, we intended to provide unified guidelines for all the participating languages, in order to avoid heterogeneous, hence incomparable, datasets. MWE identification in syntactic parsing has also gained some popularity in recent years. While often treated as a pre-processing step for parsing, both tasks are now more and more integrated (Finkel and Manning, 2009; Green et al., 2011; Green et al., 2013; Candito and Constant, 2014; Le Roux et al., 2014; Nasr et al., 2015; Constant and Nivre, 2016). Although fewer works deal with verbal MWEs, there are some notable exceptions (Wehrli et al., 2010; Vincze et al., 2013; Wehrli, 2014; Waszczuk et al., 2016). Some systems that participated in edition 1.0 of the PARSEME Shared Task are also based on parsing (Al Saied et al., 2017; Nerima et al., 2017; Simkó et al., 2017). Other approaches to MWE identification include sequence labeling using CRFs (Boro¸s et al., 2017; Maldonado et al., 2017) and neural networks (Klyueva et al., 2017). 3 Enhanced Annotation Methodology The first PARSEME annotation campaign (Savary et al., forthcoming) generated a rich feedback from annotators and language team leaders. It also attracted the interest of new teams, working on lan"
W18-4925,C16-1042,1,0.834941,"guidelines for all the participating languages, in order to avoid heterogeneous, hence incomparable, datasets. MWE identification in syntactic parsing has also gained some popularity in recent years. While often treated as a pre-processing step for parsing, both tasks are now more and more integrated (Finkel and Manning, 2009; Green et al., 2011; Green et al., 2013; Candito and Constant, 2014; Le Roux et al., 2014; Nasr et al., 2015; Constant and Nivre, 2016). Although fewer works deal with verbal MWEs, there are some notable exceptions (Wehrli et al., 2010; Vincze et al., 2013; Wehrli, 2014; Waszczuk et al., 2016). Some systems that participated in edition 1.0 of the PARSEME Shared Task are also based on parsing (Al Saied et al., 2017; Nerima et al., 2017; Simkó et al., 2017). Other approaches to MWE identification include sequence labeling using CRFs (Boro¸s et al., 2017; Maldonado et al., 2017) and neural networks (Klyueva et al., 2017). 3 Enhanced Annotation Methodology The first PARSEME annotation campaign (Savary et al., forthcoming) generated a rich feedback from annotators and language team leaders. It also attracted the interest of new teams, working on languages not covered by the previous ver"
W18-4925,W10-3705,0,0.0133537,"rd to these conclusions, we intended to provide unified guidelines for all the participating languages, in order to avoid heterogeneous, hence incomparable, datasets. MWE identification in syntactic parsing has also gained some popularity in recent years. While often treated as a pre-processing step for parsing, both tasks are now more and more integrated (Finkel and Manning, 2009; Green et al., 2011; Green et al., 2013; Candito and Constant, 2014; Le Roux et al., 2014; Nasr et al., 2015; Constant and Nivre, 2016). Although fewer works deal with verbal MWEs, there are some notable exceptions (Wehrli et al., 2010; Vincze et al., 2013; Wehrli, 2014; Waszczuk et al., 2016). Some systems that participated in edition 1.0 of the PARSEME Shared Task are also based on parsing (Al Saied et al., 2017; Nerima et al., 2017; Simkó et al., 2017). Other approaches to MWE identification include sequence labeling using CRFs (Boro¸s et al., 2017; Maldonado et al., 2017) and neural networks (Klyueva et al., 2017). 3 Enhanced Annotation Methodology The first PARSEME annotation campaign (Savary et al., forthcoming) generated a rich feedback from annotators and language team leaders. It also attracted the interest of new"
W18-4925,W14-0804,0,0.0128345,"ovide unified guidelines for all the participating languages, in order to avoid heterogeneous, hence incomparable, datasets. MWE identification in syntactic parsing has also gained some popularity in recent years. While often treated as a pre-processing step for parsing, both tasks are now more and more integrated (Finkel and Manning, 2009; Green et al., 2011; Green et al., 2013; Candito and Constant, 2014; Le Roux et al., 2014; Nasr et al., 2015; Constant and Nivre, 2016). Although fewer works deal with verbal MWEs, there are some notable exceptions (Wehrli et al., 2010; Vincze et al., 2013; Wehrli, 2014; Waszczuk et al., 2016). Some systems that participated in edition 1.0 of the PARSEME Shared Task are also based on parsing (Al Saied et al., 2017; Nerima et al., 2017; Simkó et al., 2017). Other approaches to MWE identification include sequence labeling using CRFs (Boro¸s et al., 2017; Maldonado et al., 2017) and neural networks (Klyueva et al., 2017). 3 Enhanced Annotation Methodology The first PARSEME annotation campaign (Savary et al., forthcoming) generated a rich feedback from annotators and language team leaders. It also attracted the interest of new teams, working on languages not cov"
W19-0422,abeille-barrier-2004-enriching,0,0.0827286,"Missing"
W19-0422,P17-2094,0,0.0766744,"linguist.univ-paris-diderot.fr benoit.crabbe@linguist.univ-paris-diderot.fr Abstract As opposed to word sense induction, word sense disambiguation (WSD), whether supervised or semi-supervised, has the advantage of using interpretable senses, but requires annotated data, which are quite rare for most languages except English (Miller et al., 1993). In this paper, we investigate which strategy to adopt to achieve WSD for languages lacking data that was annotated specifically for the task, focusing on the particular case of verb disambiguation in French. We first study the usability of Eurosense (Bovi et al. 2017), a multilingual corpus extracted from Europarl (Kohen, 2005) and automatically annotated with BabelNet (Navigli and Ponzetto, 2010) senses. Such a resource opened up the way to supervised and semi-supervised WSD for resourceless languages like French. While this perspective looked promising, our evaluation showed the annotated senses’ quality was not sufficient for supervised WSD on French verbs. Instead, we propose to use Wiktionary, a collaboratively edited, multilingual online dictionary, as a new resource for WSD. Wiktionary provides both sense inventory and manually sense tagged examples"
W19-0422,F12-2024,1,0.87489,"rameNet data (Djemaa et al., 2016), but in such data, only some notional domains were considered, and verb occurrences not pertaining to such domains were not disambiguated. 7 Except for SensEval1 but only the English dataset was given to public domain. 8 The dataset is available here http://www.llf.cnrs.fr/dataset/fse/ 4 Number of sentences Number of annotated verb tokens Number of annotated verb types Mean number of annotations per verb type Mean number of senses per verb type 3121 3199 66 48.47 3.83 Table 2: Statistics for the FrenchSemEval corpus (FSE). and Barrier, 2004) and the Sequoia (Candito and Seddah, 2012) treebank9 ), supplementing the corpus when necessary by occurrences sampled from fr-Wikipedia, in order to reach 50 occurrences per verb. 4.2 Annotation process The annotation has been performed by three students10 for nearly a month. We used WebAnno (Yimam et al., 2014; de Castilho et al., 2016) an open-source adaptable annotation tool. Sentences had already been pre-processed into CoNLL format (Nivre et al., 2007) with the Mind The Gap (MTG) parser (Coavoux and Crabb´e, 2017) and were plugged in WebAnno. We were thus able to provide files (one file per verb) containing sentences in which oc"
W19-0422,E17-1118,1,0.841033,"Missing"
W19-0422,W16-4011,0,0.047586,"Missing"
W19-0422,L16-1601,1,0.859128,"val was built using the following steps: we first selected a vocabulary of verbs based on their frequency in corpus. We selected verbs appearing between 50 and 1000 times in the French Wikipedia (dumped on 2016-12-12 hereafter frWikipedia). Secondly, from this pre-selected list of verbs we extracted those having a number of senses comprised between two and ten in Wiktionary’s sense inventory. For these verbs, we chose to extract 50 occurrences primarily from corpora comprising other annotations (the French TreeBank (FTB) (Abeill´e 6 Verbs are annotated with frames in the French FrameNet data (Djemaa et al., 2016), but in such data, only some notional domains were considered, and verb occurrences not pertaining to such domains were not disambiguated. 7 Except for SensEval1 but only the English dataset was given to public domain. 8 The dataset is available here http://www.llf.cnrs.fr/dataset/fse/ 4 Number of sentences Number of annotated verb tokens Number of annotated verb types Mean number of annotations per verb type Mean number of senses per verb type 3121 3199 66 48.47 3.83 Table 2: Statistics for the FrenchSemEval corpus (FSE). and Barrier, 2004) and the Sequoia (Candito and Seddah, 2012) treebank"
W19-0422,S10-1003,0,0.0491334,"quasiinexistent6 , makes it a serious candidate for a new resource of WSD. To investigate this opportunity for our objective of French verb WSD, we present FrenchSemEval, a new dataset manually annotated for WSD of French verbs which we used to carry out several evaluations, we describe the new resource in the next section. 4 FrenchSemEval : An evaluation corpus for French verb disambiguation Since the first Senseval evaluation serie in 1998 (Kilgarrif, 1998), a various number of evaluation frameworks were proposed to evaluate different WSD tasks, but only a few include French test datasets (Lefever and Hoste, 2010; Navigli et al., 2013) and unfortunately these only focus on nouns7 . In this section we present FrenchSemEval8 a new French dataset in which verb occurrences were manually annotated with Wiktionary senses. Our objective was to evaluate whether Wiktionary’s sense inventory is operational for humans to sense-annotate a corpus, and if so, to use it as evaluation data for WSD experiments. We describe the annotation process along with several statistics about the resulting dataset and the quality of the annotations. 4.1 Data selection To build FrenchSemEval, we chose to focus on moderately freque"
W19-0422,P15-2068,0,0.132793,"type occurring in these sentences is 15,5. More importantly, we could notice that the frontiers of the various senses sometimes appeared difficult to judge, making it difficult to grasp the exact perimeter of a sense.These mixed results led us to investigate other sources of sense-annotated data for French. 3 Wiktionary as data for WSD Wiktionary is a collaboratively edited, open-source multilingual online dictionary, hosted by the Wikimedia Foundation. It provides an interesting open-source resource and several studies already showed its usefulness for various NLP tasks (e.g. lemmatization (Liebeck and Conrad, 2015)), especially in the lexical semantic field, for extracting or improving thesauri (Navarro et al., 2009; Henrich et al., 2011; Miller and Gurevych, 2014). In this section we briefly present Wiktionary’s most interesting features along with our motivations to investigate the use of this resource for WSD on French verbs. Wiktionary’s main advantages is that it is entirely open-source, multilingual and has a good coverage for a substantial number of languages (according to wiktionary statistics4 , 22 languages have more than 50, 000 wiktionary entries each). Each entry consists of a definition an"
W19-0422,K16-1006,0,0.373854,"ased WSD for French verbs, evaluated on FrenchSemEval (FSE), a new dataset of French verbs manually annotated with wiktionary senses. 1 Introduction Word Sense Disambiguation (WSD) is a NLP task aiming at identifying the sense of a word occurrence from its context, given a predefined sense inventory. Although the task emerged almost 70 years ago with the first work on Automatic Machine Translation (Weaver, 1955), it remains unresolved. The recent breakthrough in neural net models allowed a better representation of the context and thus improved the quality of supervised disambiguation systems (Melamud et al., 2016; Yuan et al., 2016; Peters et al., 2018). Nevertheless, although WSD has the advantage of providing interpretable senses (as opposed to the unsupervised task of word sense induction), it also has the drawback of heavily relying on the availability and quality of sense-annotated data, even in the semi-supervised setting. Now, such data is available in English, essentially with SemCor (Miller et al., 1993), a corpus manually sense-annotated with Wordnet (Miller, 1995) senses. But for most languages, sense disambiguated data are very rare or simply don’t exist. This is mainly due to the fact tha"
W19-0422,H93-1061,0,0.791687,", 1955), it remains unresolved. The recent breakthrough in neural net models allowed a better representation of the context and thus improved the quality of supervised disambiguation systems (Melamud et al., 2016; Yuan et al., 2016; Peters et al., 2018). Nevertheless, although WSD has the advantage of providing interpretable senses (as opposed to the unsupervised task of word sense induction), it also has the drawback of heavily relying on the availability and quality of sense-annotated data, even in the semi-supervised setting. Now, such data is available in English, essentially with SemCor (Miller et al., 1993), a corpus manually sense-annotated with Wordnet (Miller, 1995) senses. But for most languages, sense disambiguated data are very rare or simply don’t exist. This is mainly due to the fact that manual semantic annotation is very costly in time and resources (Navigli, 2009). Nevertheless, Bovi et al. (2017) recently presented Eurosense, a multilingual automatically sense-disambiguated corpus extracted from Europarl (Koehn, 2005) and annotated with BabelNet (Navigli and Ponzetto, 2012) senses. In this article, we focus on supervised WSD for French verbs and investigate a way to perform the task"
W19-0422,miller-gurevych-2014-wordnet,0,0.0314455,"udge, making it difficult to grasp the exact perimeter of a sense.These mixed results led us to investigate other sources of sense-annotated data for French. 3 Wiktionary as data for WSD Wiktionary is a collaboratively edited, open-source multilingual online dictionary, hosted by the Wikimedia Foundation. It provides an interesting open-source resource and several studies already showed its usefulness for various NLP tasks (e.g. lemmatization (Liebeck and Conrad, 2015)), especially in the lexical semantic field, for extracting or improving thesauri (Navarro et al., 2009; Henrich et al., 2011; Miller and Gurevych, 2014). In this section we briefly present Wiktionary’s most interesting features along with our motivations to investigate the use of this resource for WSD on French verbs. Wiktionary’s main advantages is that it is entirely open-source, multilingual and has a good coverage for a substantial number of languages (according to wiktionary statistics4 , 22 languages have more than 50, 000 wiktionary entries each). Each entry consists of a definition and one or several examples, either attested or created, each example being a potential sense-annotated example for the lemma at hand. Definitions and exam"
W19-0422,S15-2049,0,0.0667081,"Missing"
W19-0422,Q14-1019,0,0.0708881,"Missing"
W19-0422,S13-2040,0,0.098209,"it a serious candidate for a new resource of WSD. To investigate this opportunity for our objective of French verb WSD, we present FrenchSemEval, a new dataset manually annotated for WSD of French verbs which we used to carry out several evaluations, we describe the new resource in the next section. 4 FrenchSemEval : An evaluation corpus for French verb disambiguation Since the first Senseval evaluation serie in 1998 (Kilgarrif, 1998), a various number of evaluation frameworks were proposed to evaluate different WSD tasks, but only a few include French test datasets (Lefever and Hoste, 2010; Navigli et al., 2013) and unfortunately these only focus on nouns7 . In this section we present FrenchSemEval8 a new French dataset in which verb occurrences were manually annotated with Wiktionary senses. Our objective was to evaluate whether Wiktionary’s sense inventory is operational for humans to sense-annotate a corpus, and if so, to use it as evaluation data for WSD experiments. We describe the annotation process along with several statistics about the resulting dataset and the quality of the annotations. 4.1 Data selection To build FrenchSemEval, we chose to focus on moderately frequent and moderately ambig"
W19-0422,N18-1202,0,0.0184251,"renchSemEval (FSE), a new dataset of French verbs manually annotated with wiktionary senses. 1 Introduction Word Sense Disambiguation (WSD) is a NLP task aiming at identifying the sense of a word occurrence from its context, given a predefined sense inventory. Although the task emerged almost 70 years ago with the first work on Automatic Machine Translation (Weaver, 1955), it remains unresolved. The recent breakthrough in neural net models allowed a better representation of the context and thus improved the quality of supervised disambiguation systems (Melamud et al., 2016; Yuan et al., 2016; Peters et al., 2018). Nevertheless, although WSD has the advantage of providing interpretable senses (as opposed to the unsupervised task of word sense induction), it also has the drawback of heavily relying on the availability and quality of sense-annotated data, even in the semi-supervised setting. Now, such data is available in English, essentially with SemCor (Miller et al., 1993), a corpus manually sense-annotated with Wordnet (Miller, 1995) senses. But for most languages, sense disambiguated data are very rare or simply don’t exist. This is mainly due to the fact that manual semantic annotation is very cost"
W19-0422,E17-1010,0,0.309118,"very costly in time and resources (Navigli, 2009). Nevertheless, Bovi et al. (2017) recently presented Eurosense, a multilingual automatically sense-disambiguated corpus extracted from Europarl (Koehn, 2005) and annotated with BabelNet (Navigli and Ponzetto, 2012) senses. In this article, we focus on supervised WSD for French verbs and investigate a way to perform the task when no manually sense-annotated training data specifically designed for the task are available.We focus on verbs because they are known to be central to understanding tasks, but also known to lead to lower WSD performance (Raganato et al., 2017). In section 2 we report a study on the suitability of using Eurosense as training data for our task. Because the results of our evaluation were inconclusive, we decided to explore Wiktionary, a free collaboratively edited multilingual online dictionary which provides a sense inventory and manually sense tagged examples, as resource for WSD. We give a general description of Wiktionary in section 3. In section 4, we present FrenchSemEval, a new manually sense annotated dataset for French verbs, to serve as evaluation data for WSD experiments using Wiktionary as sense inventory and training exam"
W19-0422,serasset-2012-dbnary,0,0.486383,"Missing"
W19-0422,P14-5016,0,0.0329402,"Missing"
W19-0422,C16-1130,0,0.0783139,"rbs, evaluated on FrenchSemEval (FSE), a new dataset of French verbs manually annotated with wiktionary senses. 1 Introduction Word Sense Disambiguation (WSD) is a NLP task aiming at identifying the sense of a word occurrence from its context, given a predefined sense inventory. Although the task emerged almost 70 years ago with the first work on Automatic Machine Translation (Weaver, 1955), it remains unresolved. The recent breakthrough in neural net models allowed a better representation of the context and thus improved the quality of supervised disambiguation systems (Melamud et al., 2016; Yuan et al., 2016; Peters et al., 2018). Nevertheless, although WSD has the advantage of providing interpretable senses (as opposed to the unsupervised task of word sense induction), it also has the drawback of heavily relying on the availability and quality of sense-annotated data, even in the semi-supervised setting. Now, such data is available in English, essentially with SemCor (Miller et al., 1993), a corpus manually sense-annotated with Wordnet (Miller, 1995) senses. But for most languages, sense disambiguated data are very rare or simply don’t exist. This is mainly due to the fact that manual semantic a"
W19-0422,P10-4014,0,0.10803,"Missing"
W19-6109,W17-1717,1,0.895533,"distributed representations of atomic labels, their ability to capture contextual information. Moreover, neural methods supposedly learn combinations from simple feature templates, as an alternative to hand-crafted task-specific feature engineering. (Bergstra and Bengio, 2012); Yet, using neural methods for our task is challenging, the sizes of the available corpus are relatively modest (no ST.1 language has more than 5000 instances of training MWEs), albeit neural models generally have more parameters to learn than linear models. Indeed, the best systems at the shared tasks ST.0 and ST.1 (Al Saied et al., 2017; Waszczuk, 2018) (in closed track) are not neural and surpassed some neural approaches. In this paper, we carefully describe and compare the development and tuning of linear versus neural classifiers, to use in the transition system for MWE identification proposed in Al Saied et al. (2018), which itself built on the joint syntactic / MWE analyzer of Constant and Nivre (2016). We set ourselves the constraints (i) of building systems that are robust across languages, hence using the same hyperparameter configuration for all languages and (ii) of using lemma and POS information but not syntactic"
W19-6109,P16-1231,0,0.0366484,"l inflectional variation, allows adverbial modification and in some cases syntactic reordering such as relativization. Our starting point to address the MWE identification task is to reuse the system of Al Saied et al. (2018), an enhanced version of the winning system of ST.0, a transition system using a linear (SVM) model. Our objective has been to incorporate neural methods, which are overwhelming in current NLP systems. Neural networks have brought substantial performance improvements on a large variety of NLP tasks including transitionbased parsing (e.g. Kiperwasser and Goldberg (2016) or Andor et al. (2016)), in particular thanks to the use of distributed representations of atomic labels, their ability to capture contextual information. Moreover, neural methods supposedly learn combinations from simple feature templates, as an alternative to hand-crafted task-specific feature engineering. (Bergstra and Bengio, 2012); Yet, using neural methods for our task is challenging, the sizes of the available corpus are relatively modest (no ST.1 language has more than 5000 instances of training MWEs), albeit neural models generally have more parameters to learn than linear models. Indeed, the best systems"
W19-6109,W06-1620,0,0.0645785,"and neural models are described in Sections 5 and 6, and the tuning methodology in Section 7. We present experiments and discuss results in Sections 8 and 9, and conclude in Section 10. 2 Related work Supervised MWE identification has made significant progress in the last years thanks to the availability of new annotated resources (Schneider et al., 2016; Savary et al., 2017; Ramisch et al., 2018). Sequence tagging methods have been largely used for MWE identification. In particular, first studies experimented IOB or IOB-like annotated corpora to train conditional random fields (CRF) models (Blunsom and Baldwin, 2006; Constant and Sigogne, 2011; Vincze et al., 2011) or other linear models (Schneider et al., 2014). Recently, Gharbieh et al. (2017) experimented on the DiMSUM data set various IOB-based MWE taggers relying on different deep learning models, namely multilayer perceptron, recurrent neural networks and convolutional networks. They showed that convolutional networks achieve better results. On the other hand, Taslimipoor and Rohanian (2018) used pre-trained non-modifiable word embeddings, POS tags and other technical features to feed two convolutional layers with window sizes 2 and 3 in order to d"
W19-6109,J17-4005,1,0.904932,"Missing"
W19-6109,P16-1016,0,0.0719394,"relatively modest (no ST.1 language has more than 5000 instances of training MWEs), albeit neural models generally have more parameters to learn than linear models. Indeed, the best systems at the shared tasks ST.0 and ST.1 (Al Saied et al., 2017; Waszczuk, 2018) (in closed track) are not neural and surpassed some neural approaches. In this paper, we carefully describe and compare the development and tuning of linear versus neural classifiers, to use in the transition system for MWE identification proposed in Al Saied et al. (2018), which itself built on the joint syntactic / MWE analyzer of Constant and Nivre (2016). We set ourselves the constraints (i) of building systems that are robust across languages, hence using the same hyperparameter configuration for all languages and (ii) of using lemma and POS information but not syntactic parses provided in the PARSEME data sets, so that the resulting systems require limited preprocessing. We report a systematic work on designing and tuning linear and neural transition classifiers, including the use of resampling, vocabulary generalization and several strategies for the selection of the best hyperparameter configuration. We address both the open and closed tr"
W19-6109,W11-0809,0,0.0324256,"ribed in Sections 5 and 6, and the tuning methodology in Section 7. We present experiments and discuss results in Sections 8 and 9, and conclude in Section 10. 2 Related work Supervised MWE identification has made significant progress in the last years thanks to the availability of new annotated resources (Schneider et al., 2016; Savary et al., 2017; Ramisch et al., 2018). Sequence tagging methods have been largely used for MWE identification. In particular, first studies experimented IOB or IOB-like annotated corpora to train conditional random fields (CRF) models (Blunsom and Baldwin, 2006; Constant and Sigogne, 2011; Vincze et al., 2011) or other linear models (Schneider et al., 2014). Recently, Gharbieh et al. (2017) experimented on the DiMSUM data set various IOB-based MWE taggers relying on different deep learning models, namely multilayer perceptron, recurrent neural networks and convolutional networks. They showed that convolutional networks achieve better results. On the other hand, Taslimipoor and Rohanian (2018) used pre-trained non-modifiable word embeddings, POS tags and other technical features to feed two convolutional layers with window sizes 2 and 3 in order to detect n-grams. The concatena"
W19-6109,S17-1006,0,0.0205342,"ts in Sections 8 and 9, and conclude in Section 10. 2 Related work Supervised MWE identification has made significant progress in the last years thanks to the availability of new annotated resources (Schneider et al., 2016; Savary et al., 2017; Ramisch et al., 2018). Sequence tagging methods have been largely used for MWE identification. In particular, first studies experimented IOB or IOB-like annotated corpora to train conditional random fields (CRF) models (Blunsom and Baldwin, 2006; Constant and Sigogne, 2011; Vincze et al., 2011) or other linear models (Schneider et al., 2014). Recently, Gharbieh et al. (2017) experimented on the DiMSUM data set various IOB-based MWE taggers relying on different deep learning models, namely multilayer perceptron, recurrent neural networks and convolutional networks. They showed that convolutional networks achieve better results. On the other hand, Taslimipoor and Rohanian (2018) used pre-trained non-modifiable word embeddings, POS tags and other technical features to feed two convolutional layers with window sizes 2 and 3 in order to detect n-grams. The concatenation of the two layers is then passed to a Bi-LSTM layer. Legrand and Collobert (2016) used a phrase rep"
W19-6109,Q16-1023,0,0.178002,"f only the verb generally shows full inflectional variation, allows adverbial modification and in some cases syntactic reordering such as relativization. Our starting point to address the MWE identification task is to reuse the system of Al Saied et al. (2018), an enhanced version of the winning system of ST.0, a transition system using a linear (SVM) model. Our objective has been to incorporate neural methods, which are overwhelming in current NLP systems. Neural networks have brought substantial performance improvements on a large variety of NLP tasks including transitionbased parsing (e.g. Kiperwasser and Goldberg (2016) or Andor et al. (2016)), in particular thanks to the use of distributed representations of atomic labels, their ability to capture contextual information. Moreover, neural methods supposedly learn combinations from simple feature templates, as an alternative to hand-crafted task-specific feature engineering. (Bergstra and Bengio, 2012); Yet, using neural methods for our task is challenging, the sizes of the available corpus are relatively modest (no ST.1 language has more than 5000 instances of training MWEs), albeit neural models generally have more parameters to learn than linear models. In"
W19-6109,W16-1810,0,0.0323003,"Missing"
W19-6109,W04-0308,0,0.049407,"for the purpose of the paper. One MWE instance is either a set of several potentially non-continuous tokens, or a single token compounding multiple words (namely a multiword token, hereafter MWT).2 Data sets also contain rare MWEs embedded in another one, and overlapping MWEs. 4 System description Transition system A transition system incrementally builds the expected output structure by sequentially applying a transition to a configuration that encodes the state of the system, outputting a new configuration. It has been used in particular to build a syntactic tree for a given input sentence (Nivre, 2004), and to build both the syntactic tree and the MWE list (Constant and Nivre, 2016). We use such a system here to build the list of MWEs only.We reuse the transition system of Al Saied et al. (2018), simplified in that we do not predict the MWE types. In this system, a configuration is a triplet c = (σ, β, γ), where β is a buffer of (remaining) tokens, σ is a stack of ”elements”, which are either single tokens or binary trees of tokens, and γ is the list of elements that have been 1 We used all languages but Arabic due to licence issues. MWTs are extremely marginal for all ST.1 languages except"
W19-6109,Q14-1016,0,0.0258652,"esent experiments and discuss results in Sections 8 and 9, and conclude in Section 10. 2 Related work Supervised MWE identification has made significant progress in the last years thanks to the availability of new annotated resources (Schneider et al., 2016; Savary et al., 2017; Ramisch et al., 2018). Sequence tagging methods have been largely used for MWE identification. In particular, first studies experimented IOB or IOB-like annotated corpora to train conditional random fields (CRF) models (Blunsom and Baldwin, 2006; Constant and Sigogne, 2011; Vincze et al., 2011) or other linear models (Schneider et al., 2014). Recently, Gharbieh et al. (2017) experimented on the DiMSUM data set various IOB-based MWE taggers relying on different deep learning models, namely multilayer perceptron, recurrent neural networks and convolutional networks. They showed that convolutional networks achieve better results. On the other hand, Taslimipoor and Rohanian (2018) used pre-trained non-modifiable word embeddings, POS tags and other technical features to feed two convolutional layers with window sizes 2 and 3 in order to detect n-grams. The concatenation of the two layers is then passed to a Bi-LSTM layer. Legrand and"
W19-6109,S16-1084,0,0.013994,"essity to address unknown MWE (not seen in train); • a negative result concerning the basic semisupervised strategy of using pre-trained word embeddings. We discuss the related work in Section 2, data sets in Section 3 and the transition system in Section 4. Linear and neural models are described in Sections 5 and 6, and the tuning methodology in Section 7. We present experiments and discuss results in Sections 8 and 9, and conclude in Section 10. 2 Related work Supervised MWE identification has made significant progress in the last years thanks to the availability of new annotated resources (Schneider et al., 2016; Savary et al., 2017; Ramisch et al., 2018). Sequence tagging methods have been largely used for MWE identification. In particular, first studies experimented IOB or IOB-like annotated corpora to train conditional random fields (CRF) models (Blunsom and Baldwin, 2006; Constant and Sigogne, 2011; Vincze et al., 2011) or other linear models (Schneider et al., 2014). Recently, Gharbieh et al. (2017) experimented on the DiMSUM data set various IOB-based MWE taggers relying on different deep learning models, namely multilayer perceptron, recurrent neural networks and convolutional networks. They s"
W19-6109,W18-4930,0,0.0177369,"sition systems, introducing a greedy structured method that decomposes the MWE prediction problem into a sequence of local transition predictions. Constant and Nivre (2016) proposed a two-stack transition system to jointly perform MWE identification and syntactic parsing. Al Saied et al. (2017) experimented a partial implementation of this system for identifying and categorizing verbal MWEs. This system eliminates the syntactic aspects of Constant and Nivre (2016)’s system and learn a SVM model using linguistic and technical features to classify transitions. Relying on Al Saied et al. (2017), Stodden et al. (2018) replaced the linear model with a convolutional module that transforms the sparse feature vectors into continuous ones and connect them to a dense layer. Name S HIFT R EDUCE M ERGE M ARK Cond. β 6= ∅ σ 6= ∅ |σ |> 1 σ 6= ∅ Action (σ, i|β, γ) ⇒ (σ|i, β, γ) (σ|i, β, γ) ⇒ (σ, β, γ) (σ|i, j, β, γ) ⇒ (σ|(i, j), β, γ) (σ|i, β, γ) ⇒ (σ|i, β, γ ∪ (i)) Figure 1: Set of transitions, each with its precondition. 3 Data sets For our investigation, we focus on the data sets of the PARSEME Shared Task on verbal MWE identification edition 1.1 (Ramisch et al., 2018), thereafter ST.1. Table 1 provides statistics"
W19-6109,R11-1040,0,0.0287149,"nd the tuning methodology in Section 7. We present experiments and discuss results in Sections 8 and 9, and conclude in Section 10. 2 Related work Supervised MWE identification has made significant progress in the last years thanks to the availability of new annotated resources (Schneider et al., 2016; Savary et al., 2017; Ramisch et al., 2018). Sequence tagging methods have been largely used for MWE identification. In particular, first studies experimented IOB or IOB-like annotated corpora to train conditional random fields (CRF) models (Blunsom and Baldwin, 2006; Constant and Sigogne, 2011; Vincze et al., 2011) or other linear models (Schneider et al., 2014). Recently, Gharbieh et al. (2017) experimented on the DiMSUM data set various IOB-based MWE taggers relying on different deep learning models, namely multilayer perceptron, recurrent neural networks and convolutional networks. They showed that convolutional networks achieve better results. On the other hand, Taslimipoor and Rohanian (2018) used pre-trained non-modifiable word embeddings, POS tags and other technical features to feed two convolutional layers with window sizes 2 and 3 in order to detect n-grams. The concatenation of the two layers"
W19-6109,W18-4931,0,0.267401,"tations of atomic labels, their ability to capture contextual information. Moreover, neural methods supposedly learn combinations from simple feature templates, as an alternative to hand-crafted task-specific feature engineering. (Bergstra and Bengio, 2012); Yet, using neural methods for our task is challenging, the sizes of the available corpus are relatively modest (no ST.1 language has more than 5000 instances of training MWEs), albeit neural models generally have more parameters to learn than linear models. Indeed, the best systems at the shared tasks ST.0 and ST.1 (Al Saied et al., 2017; Waszczuk, 2018) (in closed track) are not neural and surpassed some neural approaches. In this paper, we carefully describe and compare the development and tuning of linear versus neural classifiers, to use in the transition system for MWE identification proposed in Al Saied et al. (2018), which itself built on the joint syntactic / MWE analyzer of Constant and Nivre (2016). We set ourselves the constraints (i) of building systems that are robust across languages, hence using the same hyperparameter configuration for all languages and (ii) of using lemma and POS information but not syntactic parses provided"
W19-6110,Q17-1010,0,0.0100065,"eature is considered present if it appears in at least one of the direct dependents of n or v. We consider only the top t features with the highest mutual information and whose underlying pairs appear in at least ` LVCs. While it is clear that LVC identification would greatly benefit from fine-grained semantic clues such as noun predicativeness, such information is not readily available for most languages under study. We consider instead on a set of unsupervised features that can be constructed for all languages based on distributional semantic models. In particular, we consider the fasttext (Bojanowski et al., 2017) set of pretrained word embeddings (which is also used by the SHOMA system) as a basis for semantic features. • FE : Word embeddings for the lemma of the verb and noun (300 dimensions each). • F1k : k-nearest neighbors of the underlying noun n. Considered neighbors are all nouns that are paired up with the underlying verb v in at least one LVC candidate in the training set, whether true LVC or not. We select the top k neighbors whose embedding has highest cosine against n’s embedding. Each neighbor is either seen-in-LVC (it is part of at least one true LVC) or an unseen-in-LVC (it is part of f"
W19-6110,L16-1628,0,0.0287611,"Missing"
W19-6110,I13-1038,0,0.0638099,"Missing"
W19-6110,W18-4932,0,0.0306641,"res. In the closed track, the TRAVERSAL system obtained the best overall results for MWEs in general as well as for LVCs. It uses a syntax-based approach, in which each node in the syntax tree was classified as part of an MWE or not (Waszczuk, 2018). The classifier resembles a second-order CRF, but rather than considering the previous 2 tokens at each point, it considers the parent and leftsibling. Features included the lemma, POS tag and dependency relation. Rather than predicting each token as being part of an LVC or not, the varIDE system use a Naive Bayes classifier to tag LVC candidates (Pasquer et al., 2018). These were extracted based on all possible token combinations whole multi-set of lemmas corresponded to an LVC that had been seen in the training corpus (no attempts were made at predicting unseen LVCs). Classifier features included POS tags and morphological information. Graph convolutional neural networks have also been used in the identification of VMWE candidates for subsequent classification (Rohanian et al., 2019). In this work, the network is combined with an attention mechanism so as to improve the accuracy of long-range predictions, and a Bi-LSTM layer is used to classify these pred"
W19-6110,W15-0903,0,0.0612786,"Missing"
W19-6110,N19-1275,0,0.064867,"Missing"
W19-6110,W11-0807,0,0.0312141,"apt a dependency parser so as to identify Hungarian LVC candidates as a byproduct of parsing, which they then evaluate on the Szeged Dependency Treebank with LVC annotations. Nagy T. et al. (2013) extract English LVC candidates involving a verb and a dependent noun with a specific dependency label. A J48 and an SVM classifier are then considered, using lexical and morphosyntactic features from the corpus, as well as semantic features from WordNet. The latter was found to contribute to better results when compared to earlier works that relied purely on morphosyntactic and statistical features (Tu and Roth, 2011). Chen et al. (2015) detect English LVCs in the BNC and OntoNotes corpora, using the PropBank layer to select LVC candidates composed of an eventive noun linked one of 6 known light verbs. The candidates are then filtered based on semantic features, including WordNet synsets and hypernym relations. More recently, the PARSEME shared-task saw 13 system submissions that tried to predict LVCs along with other VMWEs for annotated corpora in 19 languages (Ramisch et al., 2018). Overall, the best F1 scores across all languages in the open track were obtained by the SHOMA system, which employed a pipe"
W19-6110,I13-1024,0,0.0238498,"tially proposed based on lexicosyntactic patterns, and are then classified as LVC or non-LVC based on other criteria; (b) a variant of the BIO scheme (Ramshaw and Marcus, 1999) is employed so as to directly classify each token as belonging or not to an LVC. The former method allows the use of features that encompass the LVC as a whole, while the latter can be more easily implemented in the framework of some machine learning algorithms. Most works in the literature concerning LVC identification focus on annotations in a particular language, often with a language-specific understanding of LVCs. Vincze et al. (2013) adapt a dependency parser so as to identify Hungarian LVC candidates as a byproduct of parsing, which they then evaluate on the Szeged Dependency Treebank with LVC annotations. Nagy T. et al. (2013) extract English LVC candidates involving a verb and a dependent noun with a specific dependency label. A J48 and an SVM classifier are then considered, using lexical and morphosyntactic features from the corpus, as well as semantic features from WordNet. The latter was found to contribute to better results when compared to earlier works that relied purely on morphosyntactic and statistical feature"
W19-6110,W18-4931,0,0.0252879,"system, which employed a pipeline of CNNs, a Bi-LSTM, and and optional CRF layer (Taslimipoor and Rohanian, 2018). MWE prediction followed a variant of the BIO scheme that allowed multiple tags per token, with input features including a set of pretrained embeddings (leading the system to compete in the open track category), POS tags, and a set of word-shape features. In the closed track, the TRAVERSAL system obtained the best overall results for MWEs in general as well as for LVCs. It uses a syntax-based approach, in which each node in the syntax tree was classified as part of an MWE or not (Waszczuk, 2018). The classifier resembles a second-order CRF, but rather than considering the previous 2 tokens at each point, it considers the parent and leftsibling. Features included the lemma, POS tag and dependency relation. Rather than predicting each token as being part of an LVC or not, the varIDE system use a Naive Bayes classifier to tag LVC candidates (Pasquer et al., 2018). These were extracted based on all possible token combinations whole multi-set of lemmas corresponded to an LVC that had been seen in the training corpus (no attempts were made at predicting unseen LVCs). Classifier features in"
