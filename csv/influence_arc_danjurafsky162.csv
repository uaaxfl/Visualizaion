2007.sigdial-1.40,P04-1085,0,0.0388402,"use a label of 4 to represent addressing to the entire group. Baseline. We can build two baselines. The Next Speaker baseline always predicts the addressee to be the next (different) speaker (i.e. a label of 1). The Previous Speaker baseline predicts the addressee to be the most recent previous different speaker. Features. We expect that the structure of the dialog gives the most indicative cues to addressee: forward-looking dialog acts are likely to influence the addressee to speak next, while backward-looking acts might address a recent speaker. We therefore use similar features to those of Galley et al. (2004) for the related task of identifying the first half of an adjacency pair. However, since their task was retrospective, their features all involve facts about the previous discourse context. We therefore adapt the approach to examine features of subsequent as well as preceding utterances. For each utterance and potential addressee, we examine the pair made up of the original utterance A and the next (or previous) utterance B spoken by that potential addressee. We then extract features of the pair which might indicate the degree of relatedness of the utterances, including their overlap, separati"
2007.sigdial-1.40,P07-2027,1,0.87513,"Missing"
2007.sigdial-1.40,E06-1022,0,0.340956,"Missing"
2007.sigdial-1.40,E06-1007,0,0.0598088,"Missing"
2007.sigdial-1.40,J93-3003,0,\N,Missing
2015.lilt-12.3,D13-1181,0,0.0481039,"sed on the tenets of a specific literary style and investigated the impact that this style had on later work. Given this top-down approach, we hope that our findings are interpretable and useful to scholars in the humanities, since the relationship between a tenet—“to present an image”—and its operationalization—word-level concreteness ratings—was designed to be sufficiently clear. While we focused on poetic style in this paper, the results of our work regarding the differences between professional and amateur poetry are consistent with recent computational studies of literary style in prose. Ashok et al. (2013), for example, found lexical and syntactic features that correlate with literary success in novels: more successful novels used more verbs and discourse connectives, while less successful ones used more sentiment-laden words like “love”. On the other hand, we have not examined whether the differences we found between 19th century 24 / LiLT volume 12, issue 3 October 2015 and contemporary poems exist in other genres written in those times; as a result, our data does not allow us to conclude that these changes are specific to poetic language. It would be interesting for future work to compare hi"
2015.lilt-12.3,D08-1020,0,0.0570493,"Missing"
2015.lilt-12.3,J00-4001,0,0.141674,"rthermore, we showed that the lexicons used in psycholinguistics and natural language processing capture textual qualities that are important in literary analysis, such as imagery and sentiment, and that these measures can identify important trends in literary style. Finally, the computational nature of our analyses means that we can apply the same features and measurements to different set of poems and replicate or extend our findings. Computational techniques have been applied to analyze literary style in many ways. Holmes (1985) discussed the benefits of quantitative measures of style, and Stamatatos et al. (2000) used stylistic measures to automatically classify texts into different genres and authors. More recently, Kaplan and Blei (2007) developed a computer program to visualize and compare the styles of different American poets, some of the features of which we incorporated in this work. Although we drew upon the insights of these previous studies, our approach was somewhat more theory-driven. We selected measures directly based on the tenets of a specific literary style and investigated the impact that this style had on later work. Given this top-down approach, we hope that our findings are interp"
2020.acl-main.439,W01-1605,0,0.577465,"Missing"
2020.acl-main.439,D19-1060,0,0.0713573,"Missing"
2020.acl-main.439,N19-1423,0,0.209952,"ifying the type of relationship between a pair of text sequences (Penn Discourse Tree Bank Explicit and Implicit (PDTB-E/I) and Rhetorical structure theory (RST)). PDTB (Prasad et al., 2008) and RST (Carlson et al., 2001) are human annotated datasets. Both are multi-class classification tasks where PDTB is classifying a pair of sentences whereas RST is predicting the class of a node in a document-level discourse tree. Both classes of tasks are critical aspects of understanding discourse. Baselines: The previously best overall performing model from DiscoEval (Chen et al., 2019) was BERT-Large (Devlin et al., 2019). We also include the results for BERT-Base because our model is most comparable to BERT-Base in terms of parameter size, training data and training compute. We also evaluate RoBERTa-Base (Liu et al., 2019) because it was trained on more data, reported improvements over BERT-Base on other tasks but dropped the next sentence prediction objective entirely. We also compare against a BERT-Base model which we trained with binary sentence ordering (BERT-Base BSO) because this objective has been shown to be more useful than next sentence prediction (Lan et al., 2019). This BERT-Base BSO model was ini"
2020.acl-main.439,W07-1401,0,0.503285,"s We evaluate our model on DiscoEval (Chen et al., 2019), a recently published benchmark for evaluating and probing for various aspects of discourselevel semantics in representations output by discourse models. We observe that the representations learned with C ONPONO outperform BERT-Large and achieve a new state-of-the-art despite using fewer parameters and training on the same data. Furthermore, we show that our new objective improves model performance on other tasks including textual entailment, common-sense reasoning and reading comprehension. We compare C ONPONO against BERT-Base on RTE (Giampiccolo et al., 2007; Bentivogli et al., 2009), COPA (Roemmele et al., 2011) and ReCoRD (Zhang et al., 2018), while controlling for model size, training data and training time. Our main contributions are: 1. We describe a novel sentence-level discourse objective that is used in conjunction with a masked language model for unsupervised representation learning for text. We show that this objective can leverage the cross-attention and pretrained weights of a transformer model to learn discourse-level representations. 2. We show that our model achieves a new stateof-the-art on DiscoEval, improving the results on 5 of"
2020.acl-main.439,P19-1442,0,0.0707549,"., 2018), masked language models (Devlin et al., 2019), word order permutation (Yang et al., ∗ Work done during internship at Google. Code is available at https://github.com/googleresearch/language/tree/master/language/conpono and https://github.com/daniter-cu/DiscoEval 1 2019), more robust training (Liu et al., 2019) and more efficient architectures (Lan et al., 2019). However, little focus has been put on learning discourse coherence as part of the pretraining objective. While discourse coherence has been of great interest in recent natural language processing literature (Chen et al., 2019; Nie et al., 2019; Xu et al., 2019), its benefits have been questioned for pretrained language models, some even opting to remove any sentence ordering objective (Liu et al., 2019). However, in a recently published benchmark for evaluating discourse representations, Chen et al. (2019) found that the best performing model was surprisingly BERT, despite comparing against models specifically designed for discourse, such as DisSent (Nie et al., 2019) and a new recurrent network trained on a large range of sentence ordering objectives. We show that combining transformer encoders with our intersentence coherence obj"
2020.acl-main.439,N18-1202,0,0.0608877,"f a BSO example. Manually inspecting a sample of examples hints that there are often strong coreferPONO ences between the two input sentences that indicate the ordering. Table 6 shows two examples from the C ONPONO correct set which is drawn from the BSO-Wikipedia test data. In both examples, the discourse marker appears in the first sentence but the second sentence contains anaphora referring to an antecedent in the first sentence. 4 Related Work Some of the largest improvements on benchmarks such as GLUE (Wang et al., 2018) have come from ELMO’s large scale bi-directional language modeling (Peters et al., 2018), BERT’s masked language models (Devlin et al., 2019), XLNET’s generalized autoregressive pretraining (Yang et al., 2019), RoBERTa’s robust training (Liu et al., 2019) and ALBERT’s parameter reduction techniques (Lan et al., 2019). As discussed in Section 2.2, most language model were limited to NSP or BSO for inter-sentence representation learning. We showed that by comparing to BERT, which uses NSP and BERT-Base BSO which we train with the BSO objective that our objective is able to improve the discourse-level representations by training on more fine-grained sentence ordering, non-contiguous"
2020.acl-main.439,prasad-etal-2008-penn,0,0.102767,"Missing"
2020.acl-main.439,D16-1264,0,0.0524149,"standing. 3.2.1 ReCoRD results and models Model BERT-Base C ONPONO BERT-Large Accuracy 61.2 63.2 69.8 [EM] Table 4: C ONPONO is more effective at classifying the most plausible sentence from the extended context than BERT-Base. We report the BERT-Large exact match score, where the model selects only the target entity from the context, for reference. All scores are on the validation set. 4864 The task for the ReCoRD dataset is to select the correct entity from those that appear in the context to fill in the blank in the target. Previous models for ReCoRD have used a similar structure to SQuAD (Rajpurkar et al., 2016) where the model outputs a vector for each token and the model learns the best start and end position of the answer span based on the softmax over all the tokens. We, instead, generate all possible target sentences by filling the blank with each marked entity and discriminatively choose the sentence most likely to be the true “plausible” sentence from the context. This modified task evaluates how our model compares to BERTBase choosing the most coherent sentence from a set of nearly identical sentences. In Table 4 we show that C ONPONO does achieve a boost over BERT-Base but is still well belo"
2020.acl-main.439,W18-5446,0,0.0725612,"Missing"
2020.acl-main.439,P19-1067,0,0.217822,"anguage models (Devlin et al., 2019), word order permutation (Yang et al., ∗ Work done during internship at Google. Code is available at https://github.com/googleresearch/language/tree/master/language/conpono and https://github.com/daniter-cu/DiscoEval 1 2019), more robust training (Liu et al., 2019) and more efficient architectures (Lan et al., 2019). However, little focus has been put on learning discourse coherence as part of the pretraining objective. While discourse coherence has been of great interest in recent natural language processing literature (Chen et al., 2019; Nie et al., 2019; Xu et al., 2019), its benefits have been questioned for pretrained language models, some even opting to remove any sentence ordering objective (Liu et al., 2019). However, in a recently published benchmark for evaluating discourse representations, Chen et al. (2019) found that the best performing model was surprisingly BERT, despite comparing against models specifically designed for discourse, such as DisSent (Nie et al., 2019) and a new recurrent network trained on a large range of sentence ordering objectives. We show that combining transformer encoders with our intersentence coherence objective, we can fur"
2020.acl-main.486,D16-1129,0,0.0267646,"educating people on reducing unconscious biases in their language. 2 S OCIAL B IAS F RAMES Definition To better enable models to account for socially biased implications of language,2 we design a new pragmatic formalism that distinguishes several related but distinct inferences, shown in Figure 1. Given a natural language utterance, henceforth, post, we collect both categorical as well as free text inferences (described below), inspired by recent efforts in free-text annotations of commonsense knowledge (e.g., Speer and Havasi, 2012; Rashkin et al., 2018; Sap et al., 2019b) and argumentation (Habernal and Gurevych, 2016; Becker et al., 2017). The free-text explanations are crucial to our formalism, as they can both increase trust in predictions made by the machine (Kulesza et al., 2012; Bussone et al., 2015; Nguyen et al., 2018) and encourage a poster’s empathy towards a targeted group, thereby combating biases (CohenAlmagor, 2014). We base our initial frame design on social science literature of pragmatics (Lakoff, 1973; de Marneffe et al., 2012) and impoliteness (Kasper, 1990; Gabriel, 1998; Dynel, 2015; Vonasch and Baumeister, 2017). We then refine the frame structure (including number of possible answers"
2020.acl-main.486,N19-1169,0,0.0175417,"rated text candidate and variable probabilities.9 This can allow variables to be assigned an alternative value that is more globally optimal.10 4.1 Evaluation We evaluate performance of our models in the following ways. For classification, we report precision, recall, and F1 scores of the positive class. Following previous generative inference work (Sap et al., 2019b), we use automated metrics to evaluate model generations. We use BLEU2 and RougeL (F1 ) scores to capture word overlap between the generated inference and the references, which captures quality of generation (Galley et al., 2015; Hashimoto et al., 2019). We additionally compute word mover’s distance (WMD; Kusner et al., 2015), which uses distributed word representations to measure similarity between the generated and target text.11 4.2 Training Details As each post can contain multiple annotations, we define a training instance as containing one postgroup-statement triple (along with the five categorical annotations). We then split our dataset into train/dev./test (75:12.5:12.5), ensuring that no post is present in multiple splits. For evaluation (dev., test), we combine the categorical variables by averaging their binarized values and re-bi"
2020.acl-main.486,C92-2082,0,0.118608,"oke power dynamics between groups (e.g., “F*ck you” vs. “F*ck you, f*ggot”). This is a categorical variable with two possible answers: individualonly (no), group targeted (yes). Targeted group describes the social or demographic group that is referenced or targeted by the post. Here we collect free-text answers, but provide a seed list of demographic or social groups to encourage consistency. Implied statement represents the power dynamic or stereotype that is referenced in the post. We collect free-text answers in the form of simple Hearst-like patterns (e.g., “women are ADJ”, “gay men VBP”; Hearst, 1992). In-group language aims to capture whether the author of a post may be a member of the same social/demographic group that is targeted, as speaker identity changes how a statement is perceived (O’Dea et al., 2015). Specifically, in-group language (words or phrases that (re)establish belonging to a social group; Eble, 1996) can change the perceived offensiveness of a statement, such as reclaimed slurs (Croom, 2011; Galinsky et al., 2013) or self-deprecating language (Greengross and Miller, 2008). Note that we do not attempt to categorize the identity of the speaker. This variable takes three po"
2020.acl-main.486,D16-1230,0,0.0174402,"cs (Table 5). Overall, models do well at generating the targeted groups, likely because of the more limited generation space (there are only 1.4k possible groups in SBIC). Conversely, for implied statement generation (where output space is much larger), model performance is slightly worse. Similar to the classification tasks, SBF-GPT2 gdy shows a slight increase in RougeL score when using constrained decoding, but we see a slight drop in BLEU scores. Error analysis Since small differences in automated evaluation metrics for text generation sometimes only weakly correlate with human judgments (Liu et al., 2016), we manually perform an error analysis on a manually selected set of generated development-set examples from the SBFGPT2 -gdy-constr model (Table 6). Overall, the model seems to struggle with generating textual implications that are relevant to the post, instead generating very generic stereotypes about the demographic groups (e.g., in examples b and c). The model generates the correct stereotypes when there is high lexical overlap with the post (e.g., examples d and e). This is in line with previous research showing that large language models rely on correlational patterns in data (Sap et al"
2020.acl-main.486,J12-2003,0,0.0879155,"Missing"
2020.acl-main.486,D19-1474,0,0.0431593,"reased attention recently (Schmidt and Wiegand, 2017), and most dataset creation work has cast this detection problem as binary classification (Waseem and Hovy, 2016; Davidson et al., 2017; Founta et al., 2018). Moving beyond a single binary label, Wulczyn et al. (2017) and the PerspectiveAPI use a set of binary variables to annotate Wikipedia comments for several toxicityrelated categories (e.g., identity attack, profanity). Similarly, Zampieri et al. (2019) hierarchically annotate a dataset of tweets with offensiveness and whether a group or individual is targeted. Most related to our work, Ousidhoum et al. (2019) create a multilingual dataset of 13k tweets annotated for five different emotion- and toxicity-related aspects, including a 16-class variable representing social groups targeted. In comparison, S OCIAL B IAS F RAMES not only captures binary toxicity and hierarchical information about whether a group is targeted, but also free-text implications about 1.4k different targeted groups and the implied harm behind statements. Similar in spirit to this paper, recent work has tackled more subtle bias in language, such as microaggressions (Breitfeller et al., 2019) and condescension (Wang and Potts, 20"
2020.acl-main.486,P14-2056,0,0.0247509,"k different targeted groups and the implied harm behind statements. Similar in spirit to this paper, recent work has tackled more subtle bias in language, such as microaggressions (Breitfeller et al., 2019) and condescension (Wang and Potts, 2019). These types of biases are in line with the biases covered by S O CIAL B IAS F RAMES , but more narrowly scoped. 5484 Inference about social dynamics Various work has tackled the task of making inferences about power and social dynamics. Particularly, previous work has analyzed power dynamics about specific entities, either in conversation settings (Prabhakaran et al., 2014; Danescu-Niculescu-Mizil et al., 2012) or in narrative text (Sap et al., 2017; Field et al., 2019; Antoniak et al., 2019). Additionally, recent work in commonsense inference has focused on mental states of participants of a situation (e.g., Rashkin et al., 2018; Sap et al., 2019b). In contrast to reasoning about particular individuals, our work focuses on biased implications of social and demographic groups as a whole. 7 Ethical Considerations Risks in deployment Automatic detection of offensiveness or reasoning about harmful implications of language should be done with care. When deploying s"
2020.acl-main.486,D19-1482,0,0.0450103,"metric should be optimized (Corbett-Davies et al., 2017), as well as the fairness of the model on speech by different demographic groups or in different varieties of English (Mitchell et al., 2019). Additionally, deployment of such technology should discuss potential nefarious side effects, such as censorship (Ullmann and Tomalin, 2019) and dialect-based racial bias (Sap et al., 2019a; Davidson et al., 2019). Finally, offensiveness could be paired with promotions of positive online interactions, such as emphasis of community standards (Does et al., 2011) or counterspeech (Chung et al., 2019; Qian et al., 2019). Risks in annotation Recent work has highlighted various negative side effects caused by annotating potentially abusive or harmful content (e.g., acute stress; Roberts, 2016). We mitigated these by limiting the number of posts that one worker could annotate in one day, paying workers above minimum wage ($7–12), and providing crisis management resources to our annotators.13 Additionally, we acknowledge the implications of using data available on public forums for research (Zimmer, 2018) and urge researchers and practitioners to respect the privacy of the authors of posts in SBIC (Ayers et al.,"
2020.acl-main.486,P18-1043,1,0.933568,"tive analysis over large corpora can also be insightful for educating people on reducing unconscious biases in their language. 2 S OCIAL B IAS F RAMES Definition To better enable models to account for socially biased implications of language,2 we design a new pragmatic formalism that distinguishes several related but distinct inferences, shown in Figure 1. Given a natural language utterance, henceforth, post, we collect both categorical as well as free text inferences (described below), inspired by recent efforts in free-text annotations of commonsense knowledge (e.g., Speer and Havasi, 2012; Rashkin et al., 2018; Sap et al., 2019b) and argumentation (Habernal and Gurevych, 2016; Becker et al., 2017). The free-text explanations are crucial to our formalism, as they can both increase trust in predictions made by the machine (Kulesza et al., 2012; Bussone et al., 2015; Nguyen et al., 2018) and encourage a poster’s empathy towards a targeted group, thereby combating biases (CohenAlmagor, 2014). We base our initial frame design on social science literature of pragmatics (Lakoff, 1973; de Marneffe et al., 2012) and impoliteness (Kasper, 1990; Gabriel, 1998; Dynel, 2015; Vonasch and Baumeister, 2017). We th"
2020.acl-main.486,N16-3020,0,0.079531,"Missing"
2020.acl-main.486,P19-1163,1,0.544315,"a et al., 2016), and failure to do so can result in the deployment of harmful technologies (e.g., conversational AI systems turning sexist and racist; Vincent, 2016). Most previous approaches to understanding the implied harm in statements have cast this task as a simple toxicity classification (e.g., Waseem and Hovy, 2016; Founta et al., 2018; Davidson et al., 2017). However, simple classifications run the risk of discriminating against minority groups, due to high variation and identity-based biases in annotations (e.g., which cause models to learn associations between dialect and toxicity; Sap et al., 2019a; Davidson et al., 2019). In addition, detailed explanations are much more informative for people to understand and reason about why a statement is potentially harmful against other people (Gregor and Benbasat, 1999; Ribeiro et al., 2016). Thus, we propose S OCIAL B IAS F RAMES, a novel conceptual formalism that aims to model pragmatic frames in which people project social biases and stereotypes on others. Compared to semantic frames (Fillmore and Baker, 2001), the meanings projected by pragmatic frames are richer, and thus cannot be easily formalized using only categorical labels. Therefore,"
2020.acl-main.486,N19-1144,0,0.0277403,"(Sap et al., 2019c; Sakaguchi et al., 2020). 6 Related Work Bias and toxicity detection Detection of hateful, abusive, or other toxic language has received increased attention recently (Schmidt and Wiegand, 2017), and most dataset creation work has cast this detection problem as binary classification (Waseem and Hovy, 2016; Davidson et al., 2017; Founta et al., 2018). Moving beyond a single binary label, Wulczyn et al. (2017) and the PerspectiveAPI use a set of binary variables to annotate Wikipedia comments for several toxicityrelated categories (e.g., identity attack, profanity). Similarly, Zampieri et al. (2019) hierarchically annotate a dataset of tweets with offensiveness and whether a group or individual is targeted. Most related to our work, Ousidhoum et al. (2019) create a multilingual dataset of 13k tweets annotated for five different emotion- and toxicity-related aspects, including a 16-class variable representing social groups targeted. In comparison, S OCIAL B IAS F RAMES not only captures binary toxicity and hierarchical information about whether a group is targeted, but also free-text implications about 1.4k different targeted groups and the implied harm behind statements. Similar in spiri"
2020.acl-main.486,D17-1247,1,0.848658,"to this paper, recent work has tackled more subtle bias in language, such as microaggressions (Breitfeller et al., 2019) and condescension (Wang and Potts, 2019). These types of biases are in line with the biases covered by S O CIAL B IAS F RAMES , but more narrowly scoped. 5484 Inference about social dynamics Various work has tackled the task of making inferences about power and social dynamics. Particularly, previous work has analyzed power dynamics about specific entities, either in conversation settings (Prabhakaran et al., 2014; Danescu-Niculescu-Mizil et al., 2012) or in narrative text (Sap et al., 2017; Field et al., 2019; Antoniak et al., 2019). Additionally, recent work in commonsense inference has focused on mental states of participants of a situation (e.g., Rashkin et al., 2018; Sap et al., 2019b). In contrast to reasoning about particular individuals, our work focuses on biased implications of social and demographic groups as a whole. 7 Ethical Considerations Risks in deployment Automatic detection of offensiveness or reasoning about harmful implications of language should be done with care. When deploying such algorithms, ethical aspects should be considered including which performan"
2020.acl-main.486,D19-1454,1,0.861203,"a et al., 2016), and failure to do so can result in the deployment of harmful technologies (e.g., conversational AI systems turning sexist and racist; Vincent, 2016). Most previous approaches to understanding the implied harm in statements have cast this task as a simple toxicity classification (e.g., Waseem and Hovy, 2016; Founta et al., 2018; Davidson et al., 2017). However, simple classifications run the risk of discriminating against minority groups, due to high variation and identity-based biases in annotations (e.g., which cause models to learn associations between dialect and toxicity; Sap et al., 2019a; Davidson et al., 2019). In addition, detailed explanations are much more informative for people to understand and reason about why a statement is potentially harmful against other people (Gregor and Benbasat, 1999; Ribeiro et al., 2016). Thus, we propose S OCIAL B IAS F RAMES, a novel conceptual formalism that aims to model pragmatic frames in which people project social biases and stereotypes on others. Compared to semantic frames (Fillmore and Baker, 2001), the meanings projected by pragmatic frames are richer, and thus cannot be easily formalized using only categorical labels. Therefore,"
2020.acl-main.486,W17-1101,0,0.0384554,"to struggle with generating textual implications that are relevant to the post, instead generating very generic stereotypes about the demographic groups (e.g., in examples b and c). The model generates the correct stereotypes when there is high lexical overlap with the post (e.g., examples d and e). This is in line with previous research showing that large language models rely on correlational patterns in data (Sap et al., 2019c; Sakaguchi et al., 2020). 6 Related Work Bias and toxicity detection Detection of hateful, abusive, or other toxic language has received increased attention recently (Schmidt and Wiegand, 2017), and most dataset creation work has cast this detection problem as binary classification (Waseem and Hovy, 2016; Davidson et al., 2017; Founta et al., 2018). Moving beyond a single binary label, Wulczyn et al. (2017) and the PerspectiveAPI use a set of binary variables to annotate Wikipedia comments for several toxicityrelated categories (e.g., identity attack, profanity). Similarly, Zampieri et al. (2019) hierarchically annotate a dataset of tweets with offensiveness and whether a group or individual is targeted. Most related to our work, Ousidhoum et al. (2019) create a multilingual dataset"
2020.acl-main.486,speer-havasi-2012-representing,0,0.0296727,"In addition, the collective analysis over large corpora can also be insightful for educating people on reducing unconscious biases in their language. 2 S OCIAL B IAS F RAMES Definition To better enable models to account for socially biased implications of language,2 we design a new pragmatic formalism that distinguishes several related but distinct inferences, shown in Figure 1. Given a natural language utterance, henceforth, post, we collect both categorical as well as free text inferences (described below), inspired by recent efforts in free-text annotations of commonsense knowledge (e.g., Speer and Havasi, 2012; Rashkin et al., 2018; Sap et al., 2019b) and argumentation (Habernal and Gurevych, 2016; Becker et al., 2017). The free-text explanations are crucial to our formalism, as they can both increase trust in predictions made by the machine (Kulesza et al., 2012; Bussone et al., 2015; Nguyen et al., 2018) and encourage a poster’s empathy towards a targeted group, thereby combating biases (CohenAlmagor, 2014). We base our initial frame design on social science literature of pragmatics (Lakoff, 1973; de Marneffe et al., 2012) and impoliteness (Kasper, 1990; Gabriel, 1998; Dynel, 2015; Vonasch and Ba"
2020.acl-main.486,D19-1385,0,0.0579609,"houm et al. (2019) create a multilingual dataset of 13k tweets annotated for five different emotion- and toxicity-related aspects, including a 16-class variable representing social groups targeted. In comparison, S OCIAL B IAS F RAMES not only captures binary toxicity and hierarchical information about whether a group is targeted, but also free-text implications about 1.4k different targeted groups and the implied harm behind statements. Similar in spirit to this paper, recent work has tackled more subtle bias in language, such as microaggressions (Breitfeller et al., 2019) and condescension (Wang and Potts, 2019). These types of biases are in line with the biases covered by S O CIAL B IAS F RAMES , but more narrowly scoped. 5484 Inference about social dynamics Various work has tackled the task of making inferences about power and social dynamics. Particularly, previous work has analyzed power dynamics about specific entities, either in conversation settings (Prabhakaran et al., 2014; Danescu-Niculescu-Mizil et al., 2012) or in narrative text (Sap et al., 2017; Field et al., 2019; Antoniak et al., 2019). Additionally, recent work in commonsense inference has focused on mental states of participants of"
2020.acl-main.486,N16-2013,0,0.720936,"ly subtle) offensive implications about various demographic groups. recognize the implied demonizing stereotype that “Muslims are terrorists” (Figure 1). Understanding these biases with accurate underlying explanations is necessary for AI systems to adequately interact in the social world (Pereira et al., 2016), and failure to do so can result in the deployment of harmful technologies (e.g., conversational AI systems turning sexist and racist; Vincent, 2016). Most previous approaches to understanding the implied harm in statements have cast this task as a simple toxicity classification (e.g., Waseem and Hovy, 2016; Founta et al., 2018; Davidson et al., 2017). However, simple classifications run the risk of discriminating against minority groups, due to high variation and identity-based biases in annotations (e.g., which cause models to learn associations between dialect and toxicity; Sap et al., 2019a; Davidson et al., 2019). In addition, detailed explanations are much more informative for people to understand and reason about why a statement is potentially harmful against other people (Gregor and Benbasat, 1999; Ribeiro et al., 2016). Thus, we propose S OCIAL B IAS F RAMES, a novel conceptual formalis"
2020.emnlp-main.393,Q18-1018,0,0.0258812,"– we assume that what practitioners use reveals their latent preferences. Exploiting revealed preferences may be difficult in practice, however, given that usage statistics for models are not often made public and the decision to use a model might not be made with complete information. 5 Conclusion In this work, we offered several criticisms of leaderboard design in NLP. While it has helped create more accurate models, we argued that this has been at the expense of fairness, efficiency, and robustness, among other desiderata. We were not the first to criticize NLP leaderboards (Rogers, 2019; Crane, 2018; Linzen, 2020), but we were the first to do so under a framework of utility, which we used to study the divergence between what is incentivized by leaderboards and what is valued by practitioners. Given the diversity of NLP practitioners, there is no one-size-fits-all solution; rather, leaderboards should demand transparency, requiring the reporting of statistics that may be of practical concern. Equipped with these statistics, each user could then estimate the utility that each model provides to them and then re-rank accordingly, effectively creating a custom leaderboard for everyone. Acknow"
2020.emnlp-main.393,N19-1423,0,0.0943467,"Missing"
2020.emnlp-main.393,D19-1224,0,0.343521,"s better reflect that of the NLP community at large? Given that each practitioner has their own preferences, there is no way to rank models so that everyone is satisfied. Instead, we suggest that leaderboards demand transparency, requiring the reporting of statistics that are of practical concern (e.g., model size, energy efficiency). This is akin to the use of data statements for mitigating bias in NLP systems (Gebru et al., 2018; Mitchell et al., 2019; Bender and Friedman, 2018). This way, practitioners can determine the utility they receive from a given model with relatively little effort. Dodge et al. (2019) have suggested that model creators take it upon themselves to report these statistics, but without leaderboards requiring it, there is little incentive to do so. 2 Utility Functions In economics, the utility of a good denotes the benefit that a consumer receives from it (Mankiw, 2020). We specifically discuss the theory of cardinal utility, in which the amount of the good consumed can be mapped to a numerical value that quantifies its utility in utils (Mankiw, 2020). For example, a consumer might assign a value of 10 utils to two apples and 8 utils to one orange; we can infer both the directi"
2020.emnlp-main.393,W18-3012,1,0.809322,"ank the models. In effect, every user would have their own leaderboard. Ideally, users would even have the option of filtering out models that do not meet their criteria (e.g., those above a certain parameter count). This would have beneficial second-order effects as well. For example, reporting the costs of making predictions would put large institutions and poorlyresourced model creators on more equal footing (Rogers, 2019). This might motivate the creation of simpler methods whose ease-of-use makes up for weaker performance, such as weighted-average sentence embeddings (Arora et al., 2019; Ethayarajh, 2018). Even if a poorly-resourced creator could not afford to train the SOTA model du jour, they could at least compete on the basis of efficiency or create a minimally viable system that meets some desired threshold (Dodge et al., 2019; Dorr, 2011). Reporting the performance on the worst-off group, in the spirit of Rawlsian fairness (Rawls, 2001; Hashimoto et al., 2018), would also incentivize creators to improve worst-case performance. 4.2 A Leaderboard for Every Type of User While each practitioner may have their own utility function, groups of practitioners – characterized by a shared goal – ca"
2020.emnlp-main.393,D19-1354,1,0.828104,"ere found to be brittle or biased. The question-answering dataset SQuAD 2.0 was created in response to the observation that existing systems could not reliably demur when presented with an unanswerable question (Rajpurkar et al., 2016, 2018). The perplexity of language models rises when given out-of-domain text (Oren et al., 2019). Many types of bias have also been found in NLP systems, with models performing better on genderstereotypical inputs (Rudinger et al., 2018; Ethayarajh, 2020) and racial stereotypes being captured in embedding space (Manzini et al., 2019; Ethayarajh et al., 2019a,b; Ethayarajh, 2019). Moreover, repeated resubmissions allow for a model’s hyperparameters to be tuned to maximize performance, even on a private test set (Hardt, 2017). Note that leaderboards do not necessarily incentivize the creation of brittle and biased models; rather, because leaderboard utility is so parochial, these unintended consequences are relatively common. Some recent work has addressed the problem of brittleness by offering certificates of performance against adversarial examples (Raghunathan et al., 2018a,b; Jia et al., 2019). To tackle gender bias, the SuperGLUE leaderboard considers accuracy on"
2020.emnlp-main.393,2020.acl-main.262,1,0.773762,"A models is extensive (Jia and Liang, 2017; Zhang et al., 2020). There are many examples of state-of-the-art NLP models that were found to be brittle or biased. The question-answering dataset SQuAD 2.0 was created in response to the observation that existing systems could not reliably demur when presented with an unanswerable question (Rajpurkar et al., 2016, 2018). The perplexity of language models rises when given out-of-domain text (Oren et al., 2019). Many types of bias have also been found in NLP systems, with models performing better on genderstereotypical inputs (Rudinger et al., 2018; Ethayarajh, 2020) and racial stereotypes being captured in embedding space (Manzini et al., 2019; Ethayarajh et al., 2019a,b; Ethayarajh, 2019). Moreover, repeated resubmissions allow for a model’s hyperparameters to be tuned to maximize performance, even on a private test set (Hardt, 2017). Note that leaderboards do not necessarily incentivize the creation of brittle and biased models; rather, because leaderboard utility is so parochial, these unintended consequences are relatively common. Some recent work has addressed the problem of brittleness by offering certificates of performance against adversarial exa"
2020.emnlp-main.393,P19-1315,1,0.816999,"of-the-art NLP models that were found to be brittle or biased. The question-answering dataset SQuAD 2.0 was created in response to the observation that existing systems could not reliably demur when presented with an unanswerable question (Rajpurkar et al., 2016, 2018). The perplexity of language models rises when given out-of-domain text (Oren et al., 2019). Many types of bias have also been found in NLP systems, with models performing better on genderstereotypical inputs (Rudinger et al., 2018; Ethayarajh, 2020) and racial stereotypes being captured in embedding space (Manzini et al., 2019; Ethayarajh et al., 2019a,b; Ethayarajh, 2019). Moreover, repeated resubmissions allow for a model’s hyperparameters to be tuned to maximize performance, even on a private test set (Hardt, 2017). Note that leaderboards do not necessarily incentivize the creation of brittle and biased models; rather, because leaderboard utility is so parochial, these unintended consequences are relatively common. Some recent work has addressed the problem of brittleness by offering certificates of performance against adversarial examples (Raghunathan et al., 2018a,b; Jia et al., 2019). To tackle gender bias, the SuperGLUE leaderboard"
2020.emnlp-main.393,P19-1166,1,0.802661,"of-the-art NLP models that were found to be brittle or biased. The question-answering dataset SQuAD 2.0 was created in response to the observation that existing systems could not reliably demur when presented with an unanswerable question (Rajpurkar et al., 2016, 2018). The perplexity of language models rises when given out-of-domain text (Oren et al., 2019). Many types of bias have also been found in NLP systems, with models performing better on genderstereotypical inputs (Rudinger et al., 2018; Ethayarajh, 2020) and racial stereotypes being captured in embedding space (Manzini et al., 2019; Ethayarajh et al., 2019a,b; Ethayarajh, 2019). Moreover, repeated resubmissions allow for a model’s hyperparameters to be tuned to maximize performance, even on a private test set (Hardt, 2017). Note that leaderboards do not necessarily incentivize the creation of brittle and biased models; rather, because leaderboard utility is so parochial, these unintended consequences are relatively common. Some recent work has addressed the problem of brittleness by offering certificates of performance against adversarial examples (Raghunathan et al., 2018a,b; Jia et al., 2019). To tackle gender bias, the SuperGLUE leaderboard"
2020.emnlp-main.393,D17-1215,0,0.200271,"ethods in Natural Language Processing, pages 4846–4853, c November 16–20, 2020. 2020 Association for Computational Linguistics 3. Robustness: Practitioners receive higher utility from a model that is more robust to adversarial perturbations, generalizes better to outof-distribution data, and that is equally fair to all demographics. However, these benefits would leave leaderboard utility unchanged. We contextualize these limitations with examples from the ML fairness (Barocas et al., 2017; Hardt et al., 2016), Green AI (Strubell et al., 2019; Schwartz et al., 2019), and robustness literature (Jia and Liang, 2017). These three limitations are not comprehensive – other problems can also arise, which we leave to be discussed in future work. What changes can we make to leaderboards so that their utility functions better reflect that of the NLP community at large? Given that each practitioner has their own preferences, there is no way to rank models so that everyone is satisfied. Instead, we suggest that leaderboards demand transparency, requiring the reporting of statistics that are of practical concern (e.g., model size, energy efficiency). This is akin to the use of data statements for mitigating bias i"
2020.emnlp-main.393,D19-1423,0,0.030412,"Missing"
2020.emnlp-main.393,2020.acl-main.465,0,0.17775,"are many others like it as well (Zadeh and Moshovos, 2020; Hou et al., 2020; Mao et al., 2020). More efficiency and fewer parameters translate to lower costs. Our point is not that there is no incentive at all to build cheaper models, but rather that this incentive is not baked into leaderboards, which are an important artefact of the NLP community. Because lower prediction costs improve practitioner utility, practitioners build them despite the lack of incentive from leaderboards. If lower prediction costs also improved leaderboard utility, then there would be more interest in creating them (Linzen, 2020; Rogers, 2019; Dodge et al., 2019). At the very least, making prediction costs publicly available would allow users to better estimate the utility that they will get from a model, given that the leaderboard’s cost-ignorant ranking may be a poor proxy for their preferences. 3.3 Robustness Leaderboard utility only depends on model rank, which in turn only depends on the model’s performance on the test data. A typical leaderboard would gain no additional utility from a model that was robust to adversarial examples, generalized well to out-of-distribution data (Linzen, 2020), or was fair in a Raw"
2020.emnlp-main.393,N19-1062,0,0.0876279,". While this paradigm has been successful at driving the creation of more accurate models, the historical focus on performancebased evaluation has been at the expense of other attributes valued by the NLP community, such as fairness and energy efficiency (Bender and Friedman, 2018; Strubell et al., 2019). For example, a highly inefficient model would have limited use in practical applications, but this would not preclude it from reaching the top of most leaderboards. Similarly, models can reach the top while containing racial and gender biases – and indeed, some have (Bordia and Bowman, 2019; Manzini et al., 2019; Rudinger et al., 2018; Blodgett et al., 2020). Microeconomics provides a useful lens through which to study the divergence between what is incentivized by leaderboards and what is valued by practitioners. We can frame both the leaderboard and NLP practitioners as consumers of models and the benefit they receive from a model as its utility to them. Although leaderboards are inanimate, this framing allows us to make an apples-to-apples comparison: if the priorities of leaderboards and practitioners are perfectly aligned, their utility functions should be identical; the less aligned they are, t"
2020.emnlp-main.393,2020.coling-main.287,0,0.0372799,"Missing"
2020.emnlp-main.393,D19-1432,0,0.0214506,"at NLP practitioners care about, particularly those who deploy systems in real-world applications. In fact, the literature on the lack of robustness in many SOTA models is extensive (Jia and Liang, 2017; Zhang et al., 2020). There are many examples of state-of-the-art NLP models that were found to be brittle or biased. The question-answering dataset SQuAD 2.0 was created in response to the observation that existing systems could not reliably demur when presented with an unanswerable question (Rajpurkar et al., 2016, 2018). The perplexity of language models rises when given out-of-domain text (Oren et al., 2019). Many types of bias have also been found in NLP systems, with models performing better on genderstereotypical inputs (Rudinger et al., 2018; Ethayarajh, 2020) and racial stereotypes being captured in embedding space (Manzini et al., 2019; Ethayarajh et al., 2019a,b; Ethayarajh, 2019). Moreover, repeated resubmissions allow for a model’s hyperparameters to be tuned to maximize performance, even on a private test set (Hardt, 2017). Note that leaderboards do not necessarily incentivize the creation of brittle and biased models; rather, because leaderboard utility is so parochial, these unintende"
2020.emnlp-main.393,P18-2124,0,0.0652833,"Missing"
2020.emnlp-main.393,D16-1264,0,0.0160807,"f the worst-off group) (Rawls, 2001; Hashimoto et al., 2018). In contrast, these are all attributes that NLP practitioners care about, particularly those who deploy systems in real-world applications. In fact, the literature on the lack of robustness in many SOTA models is extensive (Jia and Liang, 2017; Zhang et al., 2020). There are many examples of state-of-the-art NLP models that were found to be brittle or biased. The question-answering dataset SQuAD 2.0 was created in response to the observation that existing systems could not reliably demur when presented with an unanswerable question (Rajpurkar et al., 2016, 2018). The perplexity of language models rises when given out-of-domain text (Oren et al., 2019). Many types of bias have also been found in NLP systems, with models performing better on genderstereotypical inputs (Rudinger et al., 2018; Ethayarajh, 2020) and racial stereotypes being captured in embedding space (Manzini et al., 2019; Ethayarajh et al., 2019a,b; Ethayarajh, 2019). Moreover, repeated resubmissions allow for a model’s hyperparameters to be tuned to maximize performance, even on a private test set (Hardt, 2017). Note that leaderboards do not necessarily incentivize the creation"
2020.emnlp-main.393,N18-2002,0,0.03967,"Missing"
2020.emnlp-main.393,P19-1355,0,0.1435,"n driven in part by benchmarks such as GLUE (Wang et al., 2018), whose leaderboards rank models by how well they perform on these diverse tasks. Performance-based evaluation on a shared task is not a recent idea either; this sort of shared challenge has been an important driver of progress since MUC (Sundheim, 1995). While this paradigm has been successful at driving the creation of more accurate models, the historical focus on performancebased evaluation has been at the expense of other attributes valued by the NLP community, such as fairness and energy efficiency (Bender and Friedman, 2018; Strubell et al., 2019). For example, a highly inefficient model would have limited use in practical applications, but this would not preclude it from reaching the top of most leaderboards. Similarly, models can reach the top while containing racial and gender biases – and indeed, some have (Bordia and Bowman, 2019; Manzini et al., 2019; Rudinger et al., 2018; Blodgett et al., 2020). Microeconomics provides a useful lens through which to study the divergence between what is incentivized by leaderboards and what is valued by practitioners. We can frame both the leaderboard and NLP practitioners as consumers of models"
2020.emnlp-main.393,W18-5446,0,0.0458885,"Missing"
2020.emnlp-main.393,N18-2003,0,0.0143412,"ow for a model’s hyperparameters to be tuned to maximize performance, even on a private test set (Hardt, 2017). Note that leaderboards do not necessarily incentivize the creation of brittle and biased models; rather, because leaderboard utility is so parochial, these unintended consequences are relatively common. Some recent work has addressed the problem of brittleness by offering certificates of performance against adversarial examples (Raghunathan et al., 2018a,b; Jia et al., 2019). To tackle gender bias, the SuperGLUE leaderboard considers accuracy on the WinoBias task (Wang et al., 2019; Zhao et al., 2018). Other work has proposed changes to prevent over-fitting via multiple resubmissions (Hardt and Blum, 2015; Hardt, 2017) while some have argued that this issue is overblown, given that question-answering systems submitted to the SQuAD leaderboard do not over-fit to the original test set (Miller et al., 2020). A novel approach even proposes using a dynamic benchmark instead of a static one, creating a moving target that is harder for models to overfit to (Nie et al., 2019). 4 4.1 The Future of Leaderboards A Leaderboard for Every User Given that each practitioner has their own utility function,"
2020.emnlp-main.554,2020.acl-main.421,0,0.175336,"guage processing. Lastly, in transfer experiments between different human languages, we find that transfer is better between languages that are syntactically typologically similar, even with no vocabulary overlap. This suggests that models have the ability to form 2 We use the MAESTRO music dataset, which utilizes an exact symbolic representation of music (like a music score) that is sequentialized for sequence modelling representations of typologically sensible properties rather than relying on ad-hoc or non-natural representations. For this result we draw on recent interlingual work such as Artetxe et al. (2020), Ponti et al. (2019), and Conneau et al. (2018b), extending it to use typological distance to turn these observations into quantitative probes. The TILT method allows us to ask a complementary set of questions to those answered by current analysis methods. TILTs demonstrate the abstract structural notions that LSTMs can learn, rather than probing for the manifestation of a particular known structure, as in most current methods. By examining the pretraining structures that give LSTMs a better ability to model language, we also contribute to the more general cognitive question of what structura"
2020.emnlp-main.554,W19-4828,0,0.0305843,"d that LSTM LMs learn representations that correlate with typological syntactic feature distance, allowing them to transfer more effectively from languages which are grammatically similar. Introduction Understanding how neural language models learn and represent syntactic structure is an important analytic question for NLP. Recent work has directly probed the internal activations of models (Conneau 1 Pretraining Language (L1) We release code to construct the corpora and run our experiments at https://github.com/toizzy/ tilt-transfer et al., 2018a; Dalvi et al., 2019; Hewitt and Manning, 2019; Clark et al., 2019), or fed them curated inputs that depend on complex syntax (Linzen et al., 2016; Gulordava et al., 2018; Talmor et al., 2019; McCoy et al., 2020), in order to uncover latent syntactic awareness. We propose a different approach: we measure the structural awareness of a language model by studying how much this structure acts as an inductive bias to improve learning when we transfer from one language or symbolic system to another. We train LSTM models on data with varying degrees of language-like structure (music, Java code, nested symbols), and then evaluate their performance on natural language"
2020.emnlp-main.554,P18-1198,0,0.0253514,"s between different human languages, we find that transfer is better between languages that are syntactically typologically similar, even with no vocabulary overlap. This suggests that models have the ability to form 2 We use the MAESTRO music dataset, which utilizes an exact symbolic representation of music (like a music score) that is sequentialized for sequence modelling representations of typologically sensible properties rather than relying on ad-hoc or non-natural representations. For this result we draw on recent interlingual work such as Artetxe et al. (2020), Ponti et al. (2019), and Conneau et al. (2018b), extending it to use typological distance to turn these observations into quantitative probes. The TILT method allows us to ask a complementary set of questions to those answered by current analysis methods. TILTs demonstrate the abstract structural notions that LSTMs can learn, rather than probing for the manifestation of a particular known structure, as in most current methods. By examining the pretraining structures that give LSTMs a better ability to model language, we also contribute to the more general cognitive question of what structural inductive biases a learner needs to be able t"
2020.emnlp-main.554,D18-1269,0,0.0301562,"s between different human languages, we find that transfer is better between languages that are syntactically typologically similar, even with no vocabulary overlap. This suggests that models have the ability to form 2 We use the MAESTRO music dataset, which utilizes an exact symbolic representation of music (like a music score) that is sequentialized for sequence modelling representations of typologically sensible properties rather than relying on ad-hoc or non-natural representations. For this result we draw on recent interlingual work such as Artetxe et al. (2020), Ponti et al. (2019), and Conneau et al. (2018b), extending it to use typological distance to turn these observations into quantitative probes. The TILT method allows us to ask a complementary set of questions to those answered by current analysis methods. TILTs demonstrate the abstract structural notions that LSTMs can learn, rather than probing for the manifestation of a particular known structure, as in most current methods. By examining the pretraining structures that give LSTMs a better ability to model language, we also contribute to the more general cognitive question of what structural inductive biases a learner needs to be able t"
2020.emnlp-main.554,N18-1108,0,0.0389094,"lowing them to transfer more effectively from languages which are grammatically similar. Introduction Understanding how neural language models learn and represent syntactic structure is an important analytic question for NLP. Recent work has directly probed the internal activations of models (Conneau 1 Pretraining Language (L1) We release code to construct the corpora and run our experiments at https://github.com/toizzy/ tilt-transfer et al., 2018a; Dalvi et al., 2019; Hewitt and Manning, 2019; Clark et al., 2019), or fed them curated inputs that depend on complex syntax (Linzen et al., 2016; Gulordava et al., 2018; Talmor et al., 2019; McCoy et al., 2020), in order to uncover latent syntactic awareness. We propose a different approach: we measure the structural awareness of a language model by studying how much this structure acts as an inductive bias to improve learning when we transfer from one language or symbolic system to another. We train LSTM models on data with varying degrees of language-like structure (music, Java code, nested symbols), and then evaluate their performance on natural language. Before evaluation, we freeze the LSTM parameters and fine-tune the word embeddings on the evaluation"
2020.emnlp-main.554,D19-1275,0,0.0401182,"ooks at a model’s treatment of structural features that are realized in specific input sentences, our method compares the encoding and transfer of general grammatical features of different languages. By using TILTs, we do not have to identify a structural feature of interest and investigate if it is being encoded, but instead asses if generalizable abstract structures are encoded in one language by examining if they can be used to model human language. Our work thus avoids known issues that have been pointed out with analytic methods like probing (Voita and Titov, 2020; Pimentel et al., 2020; Hewitt and Liang, 2019). We run experiments on natural languages, artificial languages, and non-linguistic corpora. Our non-linguistic and artificial language experiments suggest three facets of the structural encoding ability of LSTM LMs. First, that vocabulary distribution has a very minor effect for modelling human language compared to structural similarity. Second, that models can encode useful language modelling information from the latent structure inherent in non-linguistic structured data, even if the surface forms are vastly differing. Last, that encodings derived from hierarchically structured tokens are e"
2020.emnlp-main.554,N19-1419,0,0.0612416,"air structure. We also find that LSTM LMs learn representations that correlate with typological syntactic feature distance, allowing them to transfer more effectively from languages which are grammatically similar. Introduction Understanding how neural language models learn and represent syntactic structure is an important analytic question for NLP. Recent work has directly probed the internal activations of models (Conneau 1 Pretraining Language (L1) We release code to construct the corpora and run our experiments at https://github.com/toizzy/ tilt-transfer et al., 2018a; Dalvi et al., 2019; Hewitt and Manning, 2019; Clark et al., 2019), or fed them curated inputs that depend on complex syntax (Linzen et al., 2016; Gulordava et al., 2018; Talmor et al., 2019; McCoy et al., 2020), in order to uncover latent syntactic awareness. We propose a different approach: we measure the structural awareness of a language model by studying how much this structure acts as an inductive bias to improve learning when we transfer from one language or symbolic system to another. We train LSTM models on data with varying degrees of language-like structure (music, Java code, nested symbols), and then evaluate their performanc"
2020.emnlp-main.554,Q16-1037,0,0.0513922,"feature distance, allowing them to transfer more effectively from languages which are grammatically similar. Introduction Understanding how neural language models learn and represent syntactic structure is an important analytic question for NLP. Recent work has directly probed the internal activations of models (Conneau 1 Pretraining Language (L1) We release code to construct the corpora and run our experiments at https://github.com/toizzy/ tilt-transfer et al., 2018a; Dalvi et al., 2019; Hewitt and Manning, 2019; Clark et al., 2019), or fed them curated inputs that depend on complex syntax (Linzen et al., 2016; Gulordava et al., 2018; Talmor et al., 2019; McCoy et al., 2020), in order to uncover latent syntactic awareness. We propose a different approach: we measure the structural awareness of a language model by studying how much this structure acts as an inductive bias to improve learning when we transfer from one language or symbolic system to another. We train LSTM models on data with varying degrees of language-like structure (music, Java code, nested symbols), and then evaluate their performance on natural language. Before evaluation, we freeze the LSTM parameters and fine-tune the word embed"
2020.emnlp-main.554,E17-2002,0,0.0605452,"Missing"
2020.emnlp-main.554,2020.tacl-1.9,0,0.0220021,"languages which are grammatically similar. Introduction Understanding how neural language models learn and represent syntactic structure is an important analytic question for NLP. Recent work has directly probed the internal activations of models (Conneau 1 Pretraining Language (L1) We release code to construct the corpora and run our experiments at https://github.com/toizzy/ tilt-transfer et al., 2018a; Dalvi et al., 2019; Hewitt and Manning, 2019; Clark et al., 2019), or fed them curated inputs that depend on complex syntax (Linzen et al., 2016; Gulordava et al., 2018; Talmor et al., 2019; McCoy et al., 2020), in order to uncover latent syntactic awareness. We propose a different approach: we measure the structural awareness of a language model by studying how much this structure acts as an inductive bias to improve learning when we transfer from one language or symbolic system to another. We train LSTM models on data with varying degrees of language-like structure (music, Java code, nested symbols), and then evaluate their performance on natural language. Before evaluation, we freeze the LSTM parameters and fine-tune the word embeddings on the evaluation language. This lets us see if the training"
2020.emnlp-main.554,P13-2017,0,0.0459239,"Missing"
2020.emnlp-main.554,P13-2007,0,0.0273716,"doff (1996) for grammatical structure in music. the abstract structural features that these corpora share with natural language in a generalizable way that’s usable to model human language? 4.1 Data For our music data we use the MAESTRO dataset of Hawthorne et al. (2018). The MAESTRO dataset embeds MIDI files of many parallel notes into a linear format suitable for sequence modelling, without losing musical information. The final corpus has a vocabulary of 310 tokens, and encodes over 172 hours of classical piano performances. 6 For programming code data, we used the Habeas corpus released by Movshovitz-Attias and Cohen (2013), of tokenized and labelled Java code. We took out every token that was labelled as a comment so as to not contaminate the code corpus with natural language. 7 6 The MAESTRO dataset is available at https:// magenta.tensorflow.org/datasets/maestro 7 The Habeas corpus is available at https://github.com/habeascorpus/ 6832 ns are s tP Fla Pa ren de Ne sti ng Co sic Mu f Zip om nd Ra Ra nd om Un ifo rm Zero-shot ppl on Spanish (L2) (Lower is better) 500 450 400 350 300 250 200 150 100 50 Pretraining Language (L1) Figure 4: Results of Experiments 1 through 3, training on non-linguistic corpora. Erro"
2020.emnlp-main.554,2020.acl-main.420,0,0.0342515,"ties of neural models looks at a model’s treatment of structural features that are realized in specific input sentences, our method compares the encoding and transfer of general grammatical features of different languages. By using TILTs, we do not have to identify a structural feature of interest and investigate if it is being encoded, but instead asses if generalizable abstract structures are encoded in one language by examining if they can be used to model human language. Our work thus avoids known issues that have been pointed out with analytic methods like probing (Voita and Titov, 2020; Pimentel et al., 2020; Hewitt and Liang, 2019). We run experiments on natural languages, artificial languages, and non-linguistic corpora. Our non-linguistic and artificial language experiments suggest three facets of the structural encoding ability of LSTM LMs. First, that vocabulary distribution has a very minor effect for modelling human language compared to structural similarity. Second, that models can encode useful language modelling information from the latent structure inherent in non-linguistic structured data, even if the surface forms are vastly differing. Last, that encodings derived from hierarchicall"
2020.emnlp-main.554,D19-1288,0,0.0608911,"Missing"
2020.emnlp-main.554,K18-2016,0,0.0329015,"Missing"
2020.emnlp-main.554,W19-3905,0,0.0286591,"xts made up of pairs of tokens that are linked but non-hierarchical. Running experiments on a range of human languages, we conclude that the internal linguistic representation of LSTM LMs allows them to take advantage of structural similarities between languages even when unaided by lexical overlap. Our results on the parentheses corpora do not necessarily provide proof that the LSTMs trained on the Nesting Parentheses corpus aren’t encoding and utilizing hierarchical structure. In fact, previous research shows that LSTMs are able to suc6836 cessfully model stack-based hierarchical languages (Suzgun et al., 2019b; Yu et al., 2019; Suzgun et al., 2019a). What our results do indicate is that, in order for LSTMs to model human language, being able to model hierarchical structure is similar in utility to having access to a non-hierarchical ability to “look back” at one relevant dependency. These results shine light on the importance of considering other types of structural awareness that may be used by neural natural language models, even if those same models also demonstrate the ability to model pure hierarchical structure. Our method could be used to test many other hypotheses regarding neural language"
2020.emnlp-main.554,2020.emnlp-main.14,0,0.0680248,"ng the structural abilities of neural models looks at a model’s treatment of structural features that are realized in specific input sentences, our method compares the encoding and transfer of general grammatical features of different languages. By using TILTs, we do not have to identify a structural feature of interest and investigate if it is being encoded, but instead asses if generalizable abstract structures are encoded in one language by examining if they can be used to model human language. Our work thus avoids known issues that have been pointed out with analytic methods like probing (Voita and Titov, 2020; Pimentel et al., 2020; Hewitt and Liang, 2019). We run experiments on natural languages, artificial languages, and non-linguistic corpora. Our non-linguistic and artificial language experiments suggest three facets of the structural encoding ability of LSTM LMs. First, that vocabulary distribution has a very minor effect for modelling human language compared to structural similarity. Second, that models can encode useful language modelling information from the latent structure inherent in non-linguistic structured data, even if the surface forms are vastly differing. Last, that encodings der"
2020.emnlp-main.554,Q16-1035,0,0.0282849,"to model pure hierarchical structure. Our method could be used to test many other hypotheses regarding neural language models, by choosing a discerning set of pretraining languages. A first step in future work would be to test if the results of this paper hold on Transformer architectures, or if instead Transformers result in different patterns of structural encoding transfer. Future work expanding on our results could focus on ablating specific structural features by creating hypothetical languages that differ in single grammatical features from the L2, in the style of Galactic Dependencies (Wang and Eisner, 2016), and testing the effect of structured data that’s completely unrelated to language, such as images. Our results also contribute to the long-running nature-nurture debate in language acquisition: whether the success of neural models implies that unbiased learners can learn natural languages with enough data, or whether human abilities to acquire language given sparse stimulus implies a strong innate human learning bias (Linzen and Baroni, 2020). The results of our parentheses experiments suggest that simple structural head-dependent bias, which need not be hierarchical, goes a long way toward"
2020.emnlp-main.554,D19-1077,0,0.036064,"es fall on a smooth gradation of typological distance from it (see Table 1). We use the AWD-LM model (Merity et al., 2018) with the default parameters of 3 LSTM layers, 300dimensional word embeddings, a hidden size of 1,150 per layer, dropout of 0.65 for the word embedding matrices and dropout of 0.3 for the LSTM parameters. We used SGD and trained to convergence, starting the learning rate at the default of 30 and reducing it at loss plateau 5 times. Much of the work on multilingual transfer learning has speculated that successes in the field may be due to vocabulary overlap (see for example Wu and Dredze (2019)). Since our work focuses mostly on syntax, we wanted to remove this possibility. As such, we shuffle each word-to-index mapping to use disjoint vocabularies for all languages: the English word “Chile” and the Spanish word “Chile” would map to different integers. This addresses the confound of vocabulary overlap, as all language pairs have zero words in common from the point of view of the model. Since the vocabularies are totally separated between languages, we align the vocabularies for all L1-L2 pairs by finetuning the word embeddings of all the pretrained models on the Spanish (L2) trainin"
2020.emnlp-main.554,W19-4815,0,0.09044,"r experiments between different human languages, we find that transfer is better between languages that are syntactically typologically similar, even with no vocabulary overlap. This suggests that models have the ability to form 2 We use the MAESTRO music dataset, which utilizes an exact symbolic representation of music (like a music score) that is sequentialized for sequence modelling representations of typologically sensible properties rather than relying on ad-hoc or non-natural representations. For this result we draw on recent interlingual work such as Artetxe et al. (2020), Ponti et al. (2019), and Conneau et al. (2018b), extending it to use typological distance to turn these observations into quantitative probes. The TILT method allows us to ask a complementary set of questions to those answered by current analysis methods. TILTs demonstrate the abstract structural notions that LSTMs can learn, rather than probing for the manifestation of a particular known structure, as in most current methods. By examining the pretraining structures that give LSTMs a better ability to model language, we also contribute to the more general cognitive question of what structural inductive biases a"
2020.emnlp-main.554,D16-1163,0,0.0298599,"en. To model this successfully a model would have to have some ability to look back at previous tokens and determine which ones would likely have their match appear next. Our results suggest that this kind of ability is just as useful as potentially being able to model a simple stack-based grammar. 6 Experiment 4: Human Languages To further analyze what kinds of generalizable structure LSTMs can infer, we run experiments in transferring zero-shot between human languages. We ask: can LSTMs infer and use fine-grained syntactic similarities between typologically similar languages? Previous work (Zoph et al., 2016; Artetxe et al., 2020) indicates that transfer is more successful between related languages. We control for vocabulary overlap, and use typological syntactic difference as a quantitative probe to ask: are fine-grained syntactic similarities encoded in generalizable, transferrable ways? To answer this question, we investigate the extent to which fine-grained differences in syntactic structure cause different zero-shot transfer results. 6.1 Data We created our language corpora from Wikipedia, which offers both wide language variation as well as a generally consistent tone and subject domain. We"
2020.emnlp-main.745,2007.sigdial-1.23,0,0.0638819,"⇤ ), while ei is the observed proportion of people who prefer system B in sample i, again relative to 0.5. For extensions to estimate Type-M and Type-S error, see Appendix B. The key to generalizing this approach is to begin with the end in mind. In particular, if one plans to test for a difference between models, one needs to choose the statistical test that will be used. That test will determine the level of detail required in the generative process for simulating data. To return to the opening example of evaluating dialog systems, we want to test if people prefer one system over the other (Ai et al., 2007). If we ignore the nuances of human preference for now (but see §5 for a more nuanced approach), and simply assume that each person either prefers system A or system B, the only assumption we need to make for a power analysis in this setting is the proportion of people in the population who prefer system B. We can then simulate samples of n people (each of whom independently has the same probability of preferring system B) as a draw from a binomial distribution, and repeat this thousands of times.3 For each sample, we then test whether the proportion of people who prefer system B is significan"
2020.emnlp-main.745,2020.acl-main.506,0,0.0118047,"n n = 100 and more severely overestimates it when n = 25. Introduction Despite its importance to empirical evaluation, relatively little attention has been paid to statistical power in NLP. In particular, if it is the case that typical experiments in NLP are underpowered, not only would we expect many meaningful improvements to go undetected, we would also expect many apparently significant differences to be exaggerated (Gelman and Carlin, 2014). In this paper, we build on past work calling for greater rigor 1 https://github.com/dallascard/NLP-power-analysis in evaluation (McCoy et al., 2019; Azer et al., 2020), including the need for careful hypothesis testing (Koehn, 2004; Berg-Kirkpatrick et al., 2012; Søgaard et al., 2014; Dror et al., 2018), and show why and how power matters to NLP, addressing challenges unique to this domain. Roughly speaking, power is the probability that a statistical test will successfully detect a true effect. As an illustrative example, imagine comparing two dialog systems (see Figure 1). We want to know if people tend to prefer one system over the other. To test this, we will need multiple people to evaluate the systems. But how many? Once we have collected data, a stat"
2020.emnlp-main.745,D12-1091,0,0.0285023,"mportance to empirical evaluation, relatively little attention has been paid to statistical power in NLP. In particular, if it is the case that typical experiments in NLP are underpowered, not only would we expect many meaningful improvements to go undetected, we would also expect many apparently significant differences to be exaggerated (Gelman and Carlin, 2014). In this paper, we build on past work calling for greater rigor 1 https://github.com/dallascard/NLP-power-analysis in evaluation (McCoy et al., 2019; Azer et al., 2020), including the need for careful hypothesis testing (Koehn, 2004; Berg-Kirkpatrick et al., 2012; Søgaard et al., 2014; Dror et al., 2018), and show why and how power matters to NLP, addressing challenges unique to this domain. Roughly speaking, power is the probability that a statistical test will successfully detect a true effect. As an illustrative example, imagine comparing two dialog systems (see Figure 1). We want to know if people tend to prefer one system over the other. To test this, we will need multiple people to evaluate the systems. But how many? Once we have collected data, a statistical test will tell us if we can reject the null hypothesis the systems are equally good. As"
2020.emnlp-main.745,N19-1423,0,0.0234785,"t such a regression, from GLUE authors we obtain the model test set predictions on all tasks from a set of 10 highperforming models, which allows us to measure the extent to which their predictions overlap with each other. Using GLUE tasks which measure accuracy, we regress Pa on baseline accuracy and acc , and achieve an R2 of 0.97.7 Repeating this for SQuAD 2.0, we get an R2 of 0.94. See Appendix E.2 for regression coefficients and additional details. Typical improvements on popular tasks tend to be small (see mean improvements in Table 2). Except for rare transformative work, such as BERT (Devlin et al., 2019), it is generally difficult to do much better than a previous SOTA and thus improvements are likely to follow a trend, which is why we are able to use historical data as a guide. In cases where such data is not available or cannot be trusted, other methods are necessary. No prior: If no informative prior is available and the baseline model or can’t be used for comparison on a validation set, then we must fall back on middle of the road assumptions. Lachenbruch (1992) provides a suggested default prior, and we find that MDEs using this method are very similar to those found by using the regress"
2020.emnlp-main.745,D19-1224,1,0.890964,"Missing"
2020.emnlp-main.745,I05-5002,0,0.142453,"Missing"
2020.emnlp-main.745,P18-1128,0,0.0972888,"attention has been paid to statistical power in NLP. In particular, if it is the case that typical experiments in NLP are underpowered, not only would we expect many meaningful improvements to go undetected, we would also expect many apparently significant differences to be exaggerated (Gelman and Carlin, 2014). In this paper, we build on past work calling for greater rigor 1 https://github.com/dallascard/NLP-power-analysis in evaluation (McCoy et al., 2019; Azer et al., 2020), including the need for careful hypothesis testing (Koehn, 2004; Berg-Kirkpatrick et al., 2012; Søgaard et al., 2014; Dror et al., 2018), and show why and how power matters to NLP, addressing challenges unique to this domain. Roughly speaking, power is the probability that a statistical test will successfully detect a true effect. As an illustrative example, imagine comparing two dialog systems (see Figure 1). We want to know if people tend to prefer one system over the other. To test this, we will need multiple people to evaluate the systems. But how many? Once we have collected data, a statistical test will tell us if we can reject the null hypothesis the systems are equally good. Assuming the systems are not identical, stat"
2020.emnlp-main.745,W14-3346,0,0.0683659,"Missing"
2020.emnlp-main.745,D18-1045,0,0.0414174,"Missing"
2020.emnlp-main.745,P19-1595,1,0.876417,"Missing"
2020.emnlp-main.745,D19-1409,0,0.195321,"Missing"
2020.emnlp-main.745,W07-1401,0,0.187508,"Missing"
2020.emnlp-main.745,W14-3333,0,0.0532769,"Missing"
2020.emnlp-main.745,N19-1169,0,0.0170463,"ode for fitting such models as part of the online materials). Using this approach, we obtain an estimate of the relevant parameters from each of the large datasets. From these, we choose sets of parameters to be representative of experiments with high or low variance, with full results in Appendix H.3 (see Table 16 for parameter estimates). As before, we then use these estimates to simulate data, assess significance on the simulated data (here using mixed effect regression), and compute power as a function of mean difference and sample 12 We use publicly available or author-provided data from Hashimoto et al. (2019); Dathathri et al. (2020); Holtzman et al. (2020), and WMT19 (links in Appendix H.2). 9270 size.13 The resulting power estimates are shown in Figure 6, plotted in terms of effect size, sample size, and numbers of workers and items, for both the high and low variance scenarios. From this analysis, we highlight a few key takeaways: minimum detectable effect size, and should be chosen accordingly. • For tasks which no longer have adequate power to detect typical improvements (e.g., MRPC and SST-2), authors should consider expanding the test set or retiring the task. • Many human evaluation studie"
2020.emnlp-main.745,W04-3250,0,0.357718,"Despite its importance to empirical evaluation, relatively little attention has been paid to statistical power in NLP. In particular, if it is the case that typical experiments in NLP are underpowered, not only would we expect many meaningful improvements to go undetected, we would also expect many apparently significant differences to be exaggerated (Gelman and Carlin, 2014). In this paper, we build on past work calling for greater rigor 1 https://github.com/dallascard/NLP-power-analysis in evaluation (McCoy et al., 2019; Azer et al., 2020), including the need for careful hypothesis testing (Koehn, 2004; Berg-Kirkpatrick et al., 2012; Søgaard et al., 2014; Dror et al., 2018), and show why and how power matters to NLP, addressing challenges unique to this domain. Roughly speaking, power is the probability that a statistical test will successfully detect a true effect. As an illustrative example, imagine comparing two dialog systems (see Figure 1). We want to know if people tend to prefer one system over the other. To test this, we will need multiple people to evaluate the systems. But how many? Once we have collected data, a statistical test will tell us if we can reject the null hypothesis t"
2020.emnlp-main.745,W19-8643,0,0.0404486,"Missing"
2020.emnlp-main.745,W19-5333,0,0.0243468,"Missing"
2020.emnlp-main.745,N19-4009,0,0.0141871,"te power using the Algorithm in Figure 2. On each iteration, draw a simulated dataset from the generative process, compute the observed differP ence between models as ˆ B = 12 ni=1 i , and test if this is significantly different from zero using a modified randomization test, in which we assume that the net effect of swapping a subset of instances is simply the sum of the i ’s in the subset. (Please see online materials for an interactive example). Empirical estimates: In order to estimate reasonable values for the required parameters, we use several pretrained models from the FAIRSEQ library (Ott et al., 2019) for the WMT English-German translation task. We evaluate these models on the shared task test sets from 2016-2019 and compute BLEU scores using SACREBLEU (Post, 2018). Fitting a Delta-Laplace mixture to the effects of swapping individual output pairs, we estimate values for Pˆ0 and ˆb0 , reported in Table 3. (See also Figure 16 in Appendix G; code for computing estimates is provided in the online materials). 9 Note that swapping all n examples would reverse the model scores, equivalent to a net effect of 2 · B . 9268 M1 M2 ⇤ TF19 TF18 TF16 TF16 ⇤ TF18 TF16 Conv17 Conv14 Test set n 2019 2018 2"
2020.emnlp-main.745,W18-6301,0,0.0435725,"Missing"
2020.emnlp-main.745,P02-1040,0,0.112194,"ay be necessary to continue making wellpowered claims of SOTA improvement on individual tasks. For any comparisons which are likely to be underpowered, we should refrain from placing much emphasis on obtaining small improvements over the previously reported best model. In extreme cases, such as MRPC and SST-2, it is worth considering whether it is time to retire these datasets as the basis for model comparison.8 4 Machine Translation To show how our approach to power analysis can be applied to a more difficult setting, we consider automated evaluation of machine translation using BLEU scores (Papineni et al., 2002). As with accuracy, we would like to know what scale of improvements can be detected with reasonable power on typical test sets. This setting is more complicated because (1) BLEU is a corpus-level metric, rather than being averaged across instances, and (2) typical models are trained on vast amounts of parallel data, with little data available that has not been used in training, making it difficult to estimate variation in performance. Significance testing for BLEU: To test for a significant difference between two MT models we use the randomization test, as recommended in Dror et al. (2018): g"
2020.emnlp-main.745,W18-6319,0,0.0233202,"12 ni=1 i , and test if this is significantly different from zero using a modified randomization test, in which we assume that the net effect of swapping a subset of instances is simply the sum of the i ’s in the subset. (Please see online materials for an interactive example). Empirical estimates: In order to estimate reasonable values for the required parameters, we use several pretrained models from the FAIRSEQ library (Ott et al., 2019) for the WMT English-German translation task. We evaluate these models on the shared task test sets from 2016-2019 and compute BLEU scores using SACREBLEU (Post, 2018). Fitting a Delta-Laplace mixture to the effects of swapping individual output pairs, we estimate values for Pˆ0 and ˆb0 , reported in Table 3. (See also Figure 16 in Appendix G; code for computing estimates is provided in the online materials). 9 Note that swapping all n examples would reverse the model scores, equivalent to a net effect of 2 · B . 9268 M1 M2 ⇤ TF19 TF18 TF16 TF16 ⇤ TF18 TF16 Conv17 Conv14 Test set n 2019 2018 2017 2016 2K 3K 3K 3K B 4.3 4.2 1.3 7.6 Pˆ0 ˆb0 0.19 0.09 0.12 0.10 23.7 29.4 22.5 27.6 Table 3: Relevant parameters from four MT evaluations. TF are Transformer-based"
2020.emnlp-main.745,P18-2124,1,0.895149,"Missing"
2020.emnlp-main.745,D16-1264,0,0.135459,"Missing"
2020.emnlp-main.745,N18-1101,0,0.0199815,"Missing"
2020.emnlp-main.745,W05-0908,0,0.211067,"Missing"
2020.emnlp-main.745,D13-1170,0,0.00693889,"Missing"
2020.emnlp-main.745,W14-1601,0,0.0208414,"on, relatively little attention has been paid to statistical power in NLP. In particular, if it is the case that typical experiments in NLP are underpowered, not only would we expect many meaningful improvements to go undetected, we would also expect many apparently significant differences to be exaggerated (Gelman and Carlin, 2014). In this paper, we build on past work calling for greater rigor 1 https://github.com/dallascard/NLP-power-analysis in evaluation (McCoy et al., 2019; Azer et al., 2020), including the need for careful hypothesis testing (Koehn, 2004; Berg-Kirkpatrick et al., 2012; Søgaard et al., 2014; Dror et al., 2018), and show why and how power matters to NLP, addressing challenges unique to this domain. Roughly speaking, power is the probability that a statistical test will successfully detect a true effect. As an illustrative example, imagine comparing two dialog systems (see Figure 1). We want to know if people tend to prefer one system over the other. To test this, we will need multiple people to evaluate the systems. But how many? Once we have collected data, a statistical test will tell us if we can reject the null hypothesis the systems are equally good. Assuming the systems are"
2020.emnlp-main.745,W18-5446,0,0.0301509,"Missing"
2020.findings-emnlp.296,D18-1393,1,0.802438,"ds can express subjectivity or bias (Riloff and Wiebe, 2003; Recasens et al., 2013; Pryzant et al., 2020). Our current paper builds upon previous work by examining such triggers as opinion-framing devices in an argumentation context, where biases related to people’s prior beliefs may interact with the lexical effects of these words. Opinion-framing can be thought of as a special case of the broader phenomenon of framing as discussed in the communications and political science literatures (Entman, 2006; Lakoff and Ferguson, 2006; Chong and Druckman, 2007), as well as in NLP (Tsur et al., 2015; Field et al., 2018; Roy and 3297 3 https://github.com/yiweiluo/GWStance Goldwasser, 2020). Both phenomena serve to emphasize particular aspects of an issue, and are often used with the intent to influence perception of that issue. Our attention to the component of SOURCE in instances of opinion-framing is also informed by communications research on the messenger effect (that people’s perceptions of a message may depend heavily on the message source) (Bolsen et al., 2019; Myrick and Evans Comfort, 2020; Fielding et al., 2020; Esposo et al., 2013). Furthermore, our interest in predicates of opinion attribution is"
2020.findings-emnlp.296,P16-1150,0,0.0115489,"formance for classifying the stance of a sentence with respect to GW. 3. Lexicons of affirming and doubting PREDI CATES (e.g., know, claim) and SOURCE modifiers (e.g., peer-reviewed, misleading). 4. Analyses on a set of 500K opinions from GW news to illustrate the utility of our dataset and lexicons for studying opinion-framing. We release our dataset, model, and lexicons as part of this paper.3 2 Related work Our work is related to social psychology research on persuasion (Cialdini, 1993; Orji et al., 2015) and recent NLP research on argumentation, such as predicting argument convincingness (Habernal and Gurevych, 2016; Simpson and Gurevych, 2018) and studying discourse-level and non-linguistic features predictive of persuasion (Yang and Kraut, 2017; Zhang et al., 2016). The latter’s work on self- vs. opponent-coverage is particularly relevant to the GW debate and we apply a similar categorization to the stance of ascribed opinions. Also relevant is the literature on factuality and speaker commitment (de Marneffe et al., 2011; Soni et al., 2014; Werner et al., 2015; Rudinger et al., 2018; Jiang and de Marneffe, 2019), and relatedly, work studying how words can express subjectivity or bias (Riloff and Wiebe,"
2020.findings-emnlp.296,N13-1132,0,0.0102444,"(± 0.011 s.d.). This effect is robust to the inclusion of other variables, but should be interpreted with caution, as women were somewhat underrepresented in our study (see Tab. 7 in Appendix E for full modeling results). Regardless, this reinforces the importance of taking potential annotator biases into account (Cowan and Khatchadourian, 2003; Sap et al., 2019) and is suggestive for further research. 3.4 Aggregating annotations Because some workers are more reliable than others, we again make use of Bayesian modeling to aggregate the annotations for each item. Drawing inspiration from MACE (Hovy et al., 2013), we fit a model which includes a distribution over labels associated with each item (i.e., agree, neutral, disagree), corresponding biases for each annotator, and a parameter indicating the degree to which they are influenced by their own biases. Whereas MACE assumes that annotators sometimes choose labels at random on individual instances, but otherwise identify the true label, we assume that annotators are always somewhat influenced by their biases, but to differing degrees. This model allows us to simultaneously infer a distribution over labels for each instance (i.e., the probability of e"
2020.findings-emnlp.296,P19-1412,0,0.0612008,"Missing"
2020.findings-emnlp.296,P13-1162,1,0.80605,"on and Gurevych, 2018) and studying discourse-level and non-linguistic features predictive of persuasion (Yang and Kraut, 2017; Zhang et al., 2016). The latter’s work on self- vs. opponent-coverage is particularly relevant to the GW debate and we apply a similar categorization to the stance of ascribed opinions. Also relevant is the literature on factuality and speaker commitment (de Marneffe et al., 2011; Soni et al., 2014; Werner et al., 2015; Rudinger et al., 2018; Jiang and de Marneffe, 2019), and relatedly, work studying how words can express subjectivity or bias (Riloff and Wiebe, 2003; Recasens et al., 2013; Pryzant et al., 2020). Our current paper builds upon previous work by examining such triggers as opinion-framing devices in an argumentation context, where biases related to people’s prior beliefs may interact with the lexical effects of these words. Opinion-framing can be thought of as a special case of the broader phenomenon of framing as discussed in the communications and political science literatures (Entman, 2006; Lakoff and Ferguson, 2006; Chong and Druckman, 2007), as well as in NLP (Tsur et al., 2015; Field et al., 2018; Roy and 3297 3 https://github.com/yiweiluo/GWStance Goldwasser"
2020.findings-emnlp.296,W03-1014,0,0.0154306,"nd Gurevych, 2016; Simpson and Gurevych, 2018) and studying discourse-level and non-linguistic features predictive of persuasion (Yang and Kraut, 2017; Zhang et al., 2016). The latter’s work on self- vs. opponent-coverage is particularly relevant to the GW debate and we apply a similar categorization to the stance of ascribed opinions. Also relevant is the literature on factuality and speaker commitment (de Marneffe et al., 2011; Soni et al., 2014; Werner et al., 2015; Rudinger et al., 2018; Jiang and de Marneffe, 2019), and relatedly, work studying how words can express subjectivity or bias (Riloff and Wiebe, 2003; Recasens et al., 2013; Pryzant et al., 2020). Our current paper builds upon previous work by examining such triggers as opinion-framing devices in an argumentation context, where biases related to people’s prior beliefs may interact with the lexical effects of these words. Opinion-framing can be thought of as a special case of the broader phenomenon of framing as discussed in the communications and political science literatures (Entman, 2006; Lakoff and Ferguson, 2006; Chong and Druckman, 2007), as well as in NLP (Tsur et al., 2015; Field et al., 2018; Roy and 3297 3 https://github.com/yiwei"
2020.findings-emnlp.296,D19-1228,0,0.0154032,"on the weighted data offers a statistically significant improvement on validation accuracy, but the expected performance is statistically indistinguishable with respect to fine-tuning and/or incorporating the target opinion as an input. The best linear model was a simple l2 -weighted logistic regression classifier using unigrams and bigrams (details in Appendix H). Affirming devices We include factive and semifactive predicates (point out, understand (N=20)), studied extensively in de Marneffe et al. (2011), Saur´ı and Pustejovsky (2012), Rudinger et al. (2018), Jiang and de Marneffe (2019), Ross and Pavlick (2019), among others. We add verbs with connotations of factivity and/or high subject commitment (confirm, attest, certify, validate (N=7)). We also add high commitment adjectives (proven, settled (N=4)) and adjectives of “hyping” from Lerchenmueller et al. (2019) (breakthrough, expert (N=38)). To complement these adjectives that affirm the quality of evidence, we add modifiers that affirm the quantity of evidence and index consensus (many, numerous, dozens of (N=11)). Majority class Linear BERT Human acc FA FN FD Favg 0.43 0.62 0.75 0.71 0.0 0.55 0.68 0.52 0.66 0.76 0.0 0.56 0.75 0.17 0.60 0.73 Tab"
2020.findings-emnlp.296,J17-3005,0,0.039012,"g labels for outlet stance beyond the binary “right-” vs. “left-leaning.” We also categorized named entities as either activists or skeptics, which obscures distinctions between, e.g., corporations with economic incentives for GW skepticism vs. individuals that may be ideologically motivated. Our methodology may also be useful for work in argument mining: the main object of our inquiry—ascribed OPINIONS and the linguistic devices of SOURCE and PREDICATE used as syntactic markers of the attributive act—represents a novel dimension along which to analyze how premises are used to support claims (Stab and Gurevych, 2017). Our work also highlights challenges inherent to studying stance: we found that many items can be ambiguous at the sentence-level, without a single “true” stance, and that demographic attributes like party affiliation and gender can affect how people respond. At the same time, we showed how Bayesian modeling can be used to account for this variation. Such findings reinforce the idea that NLP should be conscious of who the training data comes from, and how a model might be biased as a result. We hope that future research can benefit from and extend the current work to study argumentation inclu"
2020.findings-emnlp.296,P15-1157,0,0.029906,"rk studying how words can express subjectivity or bias (Riloff and Wiebe, 2003; Recasens et al., 2013; Pryzant et al., 2020). Our current paper builds upon previous work by examining such triggers as opinion-framing devices in an argumentation context, where biases related to people’s prior beliefs may interact with the lexical effects of these words. Opinion-framing can be thought of as a special case of the broader phenomenon of framing as discussed in the communications and political science literatures (Entman, 2006; Lakoff and Ferguson, 2006; Chong and Druckman, 2007), as well as in NLP (Tsur et al., 2015; Field et al., 2018; Roy and 3297 3 https://github.com/yiweiluo/GWStance Goldwasser, 2020). Both phenomena serve to emphasize particular aspects of an issue, and are often used with the intent to influence perception of that issue. Our attention to the component of SOURCE in instances of opinion-framing is also informed by communications research on the messenger effect (that people’s perceptions of a message may depend heavily on the message source) (Bolsen et al., 2019; Myrick and Evans Comfort, 2020; Fielding et al., 2020; Esposo et al., 2013). Furthermore, our interest in predicates of op"
2020.findings-emnlp.296,W15-1304,0,0.0230617,"on persuasion (Cialdini, 1993; Orji et al., 2015) and recent NLP research on argumentation, such as predicting argument convincingness (Habernal and Gurevych, 2016; Simpson and Gurevych, 2018) and studying discourse-level and non-linguistic features predictive of persuasion (Yang and Kraut, 2017; Zhang et al., 2016). The latter’s work on self- vs. opponent-coverage is particularly relevant to the GW debate and we apply a similar categorization to the stance of ascribed opinions. Also relevant is the literature on factuality and speaker commitment (de Marneffe et al., 2011; Soni et al., 2014; Werner et al., 2015; Rudinger et al., 2018; Jiang and de Marneffe, 2019), and relatedly, work studying how words can express subjectivity or bias (Riloff and Wiebe, 2003; Recasens et al., 2013; Pryzant et al., 2020). Our current paper builds upon previous work by examining such triggers as opinion-framing devices in an argumentation context, where biases related to people’s prior beliefs may interact with the lexical effects of these words. Opinion-framing can be thought of as a special case of the broader phenomenon of framing as discussed in the communications and political science literatures (Entman, 2006; L"
2020.findings-emnlp.296,N18-1101,0,0.0283511,"Missing"
2020.findings-emnlp.296,N16-1017,0,0.021374,"e.g., peer-reviewed, misleading). 4. Analyses on a set of 500K opinions from GW news to illustrate the utility of our dataset and lexicons for studying opinion-framing. We release our dataset, model, and lexicons as part of this paper.3 2 Related work Our work is related to social psychology research on persuasion (Cialdini, 1993; Orji et al., 2015) and recent NLP research on argumentation, such as predicting argument convincingness (Habernal and Gurevych, 2016; Simpson and Gurevych, 2018) and studying discourse-level and non-linguistic features predictive of persuasion (Yang and Kraut, 2017; Zhang et al., 2016). The latter’s work on self- vs. opponent-coverage is particularly relevant to the GW debate and we apply a similar categorization to the stance of ascribed opinions. Also relevant is the literature on factuality and speaker commitment (de Marneffe et al., 2011; Soni et al., 2014; Werner et al., 2015; Rudinger et al., 2018; Jiang and de Marneffe, 2019), and relatedly, work studying how words can express subjectivity or bias (Riloff and Wiebe, 2003; Recasens et al., 2013; Pryzant et al., 2020). Our current paper builds upon previous work by examining such triggers as opinion-framing devices in"
2021.acl-long.130,P06-4018,0,0.0224482,"Missing"
2021.acl-long.130,D18-2029,0,0.0224483,"Missing"
2021.acl-long.130,W11-0609,0,0.0940382,"Missing"
2021.acl-long.130,P13-1025,1,0.779594,"s binary variables (Samei et al., 2014; Donnelly et al., 2017; Kelly et al., 2018; Stone et al., 2019; Jensen et al., 2020). Our labeled dataset, unsupervised approach (involving a state-of-the art pre-trained model), and careful analysis across domains are novel contributions that will enable a fine-grained and domain-adaptable measure of uptake that can support researchers and teachers. Our work aligns closely with research on the computational study of conversations. For example, measures have been developed to study constructiveness (Niculae and Danescu-Niculescu-Mizil, 2016), politeness (Danescu-Niculescu-Mizil et al., 2013) and persuasion (Tan et al., 2016) in conversations. Perhaps most similar to our work, Zhang and Danescu-Niculescu-Mizil (2020) develop an unsupervised method to identify therapists’ backwardand forward-looking utterances, with which they guide their conversations. We also draw on work measuring discourse coherence via embedding cosines (Xu et al., 2018; Ko et al., 2019), or via utterance classification (Xu et al., 2019; Iter et al., 2020), the latter of which is used also for building and evaluating dialog systems (Lowe et al., 2016; Wolf et al., 2019). Our work extends these two families of"
2021.acl-long.130,N19-1423,0,0.0099175,"Missing"
2021.acl-long.130,2020.acl-main.439,1,0.831043,"nversations. For example, measures have been developed to study constructiveness (Niculae and Danescu-Niculescu-Mizil, 2016), politeness (Danescu-Niculescu-Mizil et al., 2013) and persuasion (Tan et al., 2016) in conversations. Perhaps most similar to our work, Zhang and Danescu-Niculescu-Mizil (2020) develop an unsupervised method to identify therapists’ backwardand forward-looking utterances, with which they guide their conversations. We also draw on work measuring discourse coherence via embedding cosines (Xu et al., 2018; Ko et al., 2019), or via utterance classification (Xu et al., 2019; Iter et al., 2020), the latter of which is used also for building and evaluating dialog systems (Lowe et al., 2016; Wolf et al., 2019). Our work extends these two families of methods to human conversation and highlights the different linguistic phenomena they capture. Finally, our work shows the key role of coherence in the socially important task of studying uptake. 8 Conclusion We propose a framework for measuring uptake, a core conversational phenomenon with particularly high relevance in teaching contexts. We release an annotated dataset and develop and compare unsupervised measures of uptake, demonstrating"
2021.acl-long.130,N19-1349,0,0.0421349,"Missing"
2021.acl-long.130,2021.ccl-1.108,0,0.0616211,"Missing"
2021.acl-long.130,D14-1162,0,0.0857378,"Missing"
2021.acl-long.130,W16-3634,0,0.0206317,"escu-Niculescu-Mizil, 2016), politeness (Danescu-Niculescu-Mizil et al., 2013) and persuasion (Tan et al., 2016) in conversations. Perhaps most similar to our work, Zhang and Danescu-Niculescu-Mizil (2020) develop an unsupervised method to identify therapists’ backwardand forward-looking utterances, with which they guide their conversations. We also draw on work measuring discourse coherence via embedding cosines (Xu et al., 2018; Ko et al., 2019), or via utterance classification (Xu et al., 2019; Iter et al., 2020), the latter of which is used also for building and evaluating dialog systems (Lowe et al., 2016; Wolf et al., 2019). Our work extends these two families of methods to human conversation and highlights the different linguistic phenomena they capture. Finally, our work shows the key role of coherence in the socially important task of studying uptake. 8 Conclusion We propose a framework for measuring uptake, a core conversational phenomenon with particularly high relevance in teaching contexts. We release an annotated dataset and develop and compare unsupervised measures of uptake, demonstrating significant correlation with educational outcomes across three datasets. This lays the groundwo"
2021.acl-long.130,D19-1410,0,0.0214777,"Missing"
2021.acl-long.130,N16-1070,0,0.0266997,", elaborated feedback and uptake, treating these moves as binary variables (Samei et al., 2014; Donnelly et al., 2017; Kelly et al., 2018; Stone et al., 2019; Jensen et al., 2020). Our labeled dataset, unsupervised approach (involving a state-of-the art pre-trained model), and careful analysis across domains are novel contributions that will enable a fine-grained and domain-adaptable measure of uptake that can support researchers and teachers. Our work aligns closely with research on the computational study of conversations. For example, measures have been developed to study constructiveness (Niculae and Danescu-Niculescu-Mizil, 2016), politeness (Danescu-Niculescu-Mizil et al., 2013) and persuasion (Tan et al., 2016) in conversations. Perhaps most similar to our work, Zhang and Danescu-Niculescu-Mizil (2020) develop an unsupervised method to identify therapists’ backwardand forward-looking utterances, with which they guide their conversations. We also draw on work measuring discourse coherence via embedding cosines (Xu et al., 2018; Ko et al., 2019), or via utterance classification (Xu et al., 2019; Iter et al., 2020), the latter of which is used also for building and evaluating dialog systems (Lowe et al., 2016; Wolf et"
2021.acl-long.130,P02-1040,0,0.112228,"Missing"
2021.acl-long.130,P19-1067,0,0.0141798,"ional study of conversations. For example, measures have been developed to study constructiveness (Niculae and Danescu-Niculescu-Mizil, 2016), politeness (Danescu-Niculescu-Mizil et al., 2013) and persuasion (Tan et al., 2016) in conversations. Perhaps most similar to our work, Zhang and Danescu-Niculescu-Mizil (2020) develop an unsupervised method to identify therapists’ backwardand forward-looking utterances, with which they guide their conversations. We also draw on work measuring discourse coherence via embedding cosines (Xu et al., 2018; Ko et al., 2019), or via utterance classification (Xu et al., 2019; Iter et al., 2020), the latter of which is used also for building and evaluating dialog systems (Lowe et al., 2016; Wolf et al., 2019). Our work extends these two families of methods to human conversation and highlights the different linguistic phenomena they capture. Finally, our work shows the key role of coherence in the socially important task of studying uptake. 8 Conclusion We propose a framework for measuring uptake, a core conversational phenomenon with particularly high relevance in teaching contexts. We release an annotated dataset and develop and compare unsupervised measures of u"
2021.acl-long.130,D18-1432,0,0.0492561,"Missing"
2021.acl-long.130,2020.acl-main.470,0,0.0255361,"Our labeled dataset, unsupervised approach (involving a state-of-the art pre-trained model), and careful analysis across domains are novel contributions that will enable a fine-grained and domain-adaptable measure of uptake that can support researchers and teachers. Our work aligns closely with research on the computational study of conversations. For example, measures have been developed to study constructiveness (Niculae and Danescu-Niculescu-Mizil, 2016), politeness (Danescu-Niculescu-Mizil et al., 2013) and persuasion (Tan et al., 2016) in conversations. Perhaps most similar to our work, Zhang and Danescu-Niculescu-Mizil (2020) develop an unsupervised method to identify therapists’ backwardand forward-looking utterances, with which they guide their conversations. We also draw on work measuring discourse coherence via embedding cosines (Xu et al., 2018; Ko et al., 2019), or via utterance classification (Xu et al., 2019; Iter et al., 2020), the latter of which is used also for building and evaluating dialog systems (Lowe et al., 2016; Wolf et al., 2019). Our work extends these two families of methods to human conversation and highlights the different linguistic phenomena they capture. Finally, our work shows the key r"
2021.acl-short.8,2020.acl-main.385,0,0.370946,"uld extend such theoretical guarantees to them as well. In this work, we first prove that — save for the degenerate case — attention weights and leaveone-out values cannot be Shapley Values. More formally, there is no set of players (i.e., possible subjects of an explanation, such as tokens) and payoff (i.e., function defining prediction quality) such that the values induced by attention or leave-oneout also satisfy the definition of a Shapley Value. We then turn to attention flow, a post-processed variant of attention weights obtained by running the max-flow algorithm on the attention graph (Abnar and Zuidema, 2020). We prove that when the players all come from the same layer (e.g., tokens in the input layer), there exists a payoff function such that attention flows are Shapley Values. This means that under certain conditions, we can extend the theoretical guarantees associated with the Shapley Value to attention flow as well. As we show, these guarantees are axioms of faithful interpretation, and having them can increase Shapley Values, a solution to the credit assignment problem in cooperative game theory, are a popular type of explanation in machine learning, having been used to explain the importance"
2021.acl-short.8,D19-1006,1,0.800468,"ce with LOOi (v). In practice, this assumption is almost always satisfied. Note that leave-one-out tells us very little about player importance for discrete payoff functions. 52 For example, if the payoff were the correctness (i.e., 1 if correct and 0 otherwise), then the importance of a player would be binary: it would either be critically important to prediction or totally irrelevant. This provides an incomplete picture — while there is enough redundancy in BERT-based models to tolerate some missing embeddings, this does not mean those embeddings are of no importance (Kovaleva et al., 2019; Ethayarajh, 2019; Michel et al., 2019). For example, if two representations played a critical and identical role in a prediction — but only one was necessary — then leave-one-out would assign each a value of zero, despite both being important. In contrast, the Shapley Value of both players would be non-zero and identical. simultaneously be included or excluded from a coalition. • Recent work has used the Data Shapley — an extension of the Shapley Value — to estimate the contribution of each example in the training data to a model’s decision boundary (Ghorbani and Zou, 2019). If we’re finetuning BERT for senti"
2021.acl-short.8,P19-1282,0,0.0906793,"e value φi (v) of a player i is the share of the payoff allocated to it. In other words, it is the importance accorded to subject i of an explanation. φi (v) = 1 X [v(PR[:i] ∪ {i}) − v(PR[:i] )] (1) n! R There are other equivalent ways of expressing the Shapley Value, including as a sum over the 2n possible coalitions. In addition to satisfying our three criteria of equitable allocation (2.1), a Shapley Value distribution always exists and is unique for a TU-game (N, v). Unlike with attention weights, which have been criticized for allowing counterfactual explanations (Jain and Wallace, 2019; Serrano and Smith, 2019), there can thus be no counterfactual Shapley Value distribution for a given input and payoff function v. The distribution is also said to be efficient, since P it allocates all of the payoff: v(N ) = i∈N φi (v) (Myerson, 1977; Young, 1985). The Shapley Value can, in theory, be computed for any player set and payoff function. However, in practice, there are typically too many players to calculate this combinatorial expression exactly. Generally, estimates Definition 2.4. A game is defined by (N, v), a player set N and payoff function v. It is a transferable utility game (TU-game), where the pa"
2021.acl-short.8,D18-1407,0,0.0606216,"Missing"
2021.acl-short.8,D19-1002,0,0.0184353,"ulkerson, 1956) to this network to calculate the maximum flow on each edge. We prove (by construction) that these attention flows are Shapley Values when the players are restricted to those from the same layer and the payoff is the total flow, as visualized in Figure 1. Attention Weights Many have argued that attention weights are not a faithful explanation, on the basis of consistency (i.e., poor correlation with other importance measures) and non-exclusivity (i.e., multiple explanations leading to the same outcome) (Jain and Wallace, 2019). Others have countered that they have some utility (Wiegreffe and Pinter, 2019). Without making assumptions about their inherent utility, we prove in this section that they cannot be Shapley Value explanations, outside of the degenerate case. Proposition 1. If some player is attended to more than another, there is no TU-game (N, v) for which attention weights are Shapley Values. Proposition 2. Consider a TU-game (N, v), where N = {1, ..., n} players are all from the same layer. Let f denote the flow obtained by running a maxflow algorithm on the graph defined by the selfattention matrix, where the capacities are the attention weights. Let v(S) = |f (S)|, the value of the"
2021.acl-short.8,N19-1357,0,0.185473,"→ R, where v(∅) = 0. The value φi (v) of a player i is the share of the payoff allocated to it. In other words, it is the importance accorded to subject i of an explanation. φi (v) = 1 X [v(PR[:i] ∪ {i}) − v(PR[:i] )] (1) n! R There are other equivalent ways of expressing the Shapley Value, including as a sum over the 2n possible coalitions. In addition to satisfying our three criteria of equitable allocation (2.1), a Shapley Value distribution always exists and is unique for a TU-game (N, v). Unlike with attention weights, which have been criticized for allowing counterfactual explanations (Jain and Wallace, 2019; Serrano and Smith, 2019), there can thus be no counterfactual Shapley Value distribution for a given input and payoff function v. The distribution is also said to be efficient, since P it allocates all of the payoff: v(N ) = i∈N φi (v) (Myerson, 1977; Young, 1985). The Shapley Value can, in theory, be computed for any player set and payoff function. However, in practice, there are typically too many players to calculate this combinatorial expression exactly. Generally, estimates Definition 2.4. A game is defined by (N, v), a player set N and payoff function v. It is a transferable utility ga"
2021.acl-short.8,D19-1445,0,0.0211423,"o there is no equivalence with LOOi (v). In practice, this assumption is almost always satisfied. Note that leave-one-out tells us very little about player importance for discrete payoff functions. 52 For example, if the payoff were the correctness (i.e., 1 if correct and 0 otherwise), then the importance of a player would be binary: it would either be critically important to prediction or totally irrelevant. This provides an incomplete picture — while there is enough redundancy in BERT-based models to tolerate some missing embeddings, this does not mean those embeddings are of no importance (Kovaleva et al., 2019; Ethayarajh, 2019; Michel et al., 2019). For example, if two representations played a critical and identical role in a prediction — but only one was necessary — then leave-one-out would assign each a value of zero, despite both being important. In contrast, the Shapley Value of both players would be non-zero and identical. simultaneously be included or excluded from a coalition. • Recent work has used the Data Shapley — an extension of the Shapley Value — to estimate the contribution of each example in the training data to a model’s decision boundary (Ghorbani and Zou, 2019). If we’re finetun"
2021.conll-1.48,D19-1384,0,0.0282009,"nsion module, here again an LSTM. The final state of the LSTM is combined by a dot product with the output of the representation encoder for each image in the set of images I of a given game. The listener receives a different image of the target object as well as 3 distractor images. The combined message and image interpretation vectors are then used to predict which image contains the target object. The listener’s guess i0 is sampled from the predicted target distribution over the images in I: i0 ∼ pL (·|m, I). Our agents are symmetric (Cao et al., 2018; Bouchacourt and Baroni, 2019; Harding Graesser et al., 2019). This means they can alternate between module approach over previous approaches see Appendix D. 610 being speaker or listener. This approach allows us to use a shared set of embeddings for the production and comprehension modules within each individual agent in order to represent messages. Additionally, having symmetric agents allows us to distinguish between two types of learning phases, selfplay and play with other agents as described in §3.3. where b is the baseline representing the mean reward across previous batches. We add the speaker’s entropy `H as a regularizer, here c = 0.01. The li"
2021.emnlp-main.106,P19-1409,0,0.181437,", but there is no natural number of candidate mentions, making it incorollary to recency with multiple documents. 2 tractable to do the full n pairwise comparMost CDCR systems thus instead cluster the isons. Existing approaches simplify by considering coreference only within document clusdocuments and perform the full n2 comparisons ters, but this fails to handle inter-cluster corefonly within each cluster, disregarding inter-cluster erence, common in many applications. As a coreference (Lee et al., 2012; Yang et al., 2015; result cross-document coreference algorithms Choubey and Huang, 2017; Barhom et al., 2019; are rarely applied to downstream tasks. We Cattan et al., 2020; Yu et al., 2020; Caciularu et al., draw on an insight from discourse coherence 2021). This was effective for the ECB+ dataset, on theory: potential coreferences are constrained which most CDCR methods have been evaluated, by the reader’s discourse focus. We model the entities/events in a reader’s focus as a neighbecause ECB+ has lexically distinct topics with borhood within a learned latent embedding almost no inter-cluster coreference. space which minimizes the distance between Such document clustering, however, keeps mentions"
2021.emnlp-main.106,P10-1143,0,0.267398,"makes it feasible to apply CDCR to a wide variety of downstream tasks, without requiring expensive new coreference annotations to enable fine-tuning on each new corpus. (This has been a huge effort for the few tasks that have attempted it like multi-hop QA (Dhingra et al., 2018; Chen et al., 2019) and multi-document summarization (Falke et al., 2017).) 2 Related Work Cross-Document Coreference Many CDCR algorithms use hand engineered event features to perform classification. Such systems have a low pairwise classification cost and therefore ignore the quadratic scaling and perform no pruning (Bejan and Harabagiu, 2010; Yang et al., 2015; Vossen and Cybulska, 2016; Bugert et al., 2020a). Other such systems choose to include document clustering to Following the recommendations of Bugert et al. increase precision, which can be done with very little tradeoff for the ECB+ corpus (Lee et al., 2012; (2020a), we evaluate our method on multiple event and entity CDCR corpora, as well as on cross- Cremisini and Finlayson, 2020). corpus transfer for event CDCR. Our method Kenyon-Dean et al. (2018) explore an approach achieves state-of-the-art results on the ECB+ cor- that avoids pairwise classification entirely, inste"
2021.emnlp-main.106,2021.findings-emnlp.225,0,0.169457,"hoods aggressively constrain the search space for our second stage pairwise classifier. This classifier utilizes cross-attention between mention pairs and their local discourse features to capture the features important within an attentional state which are comparison specific (Grosz, 1978). By sampling from attentional state neighborhoods at training time, we train on only hard negatives such as shown in Table 1. We analyze the contribution of the local discourse features to our approach, providing an explanation for the empirical effectiveness of our classifier and that of earlier work like Caciularu et al. (2021). pus (+34.5 F1). We further improve average results by training across all event CDCR corpora, leading to a 17.2 F1 improvement for average performance across all tasks. Our robust model makes it feasible to apply CDCR to a wide variety of downstream tasks, without requiring expensive new coreference annotations to enable fine-tuning on each new corpus. (This has been a huge effort for the few tasks that have attempted it like multi-hop QA (Dhingra et al., 2018; Chen et al., 2019) and multi-document summarization (Falke et al., 2017).) 2 Related Work Cross-Document Coreference Many CDCR algor"
2021.emnlp-main.106,D17-1226,0,0.0368547,"Missing"
2021.emnlp-main.106,2020.nuse-1.1,0,0.123808,"ties/events in a reader’s focus as a neighbecause ECB+ has lexically distinct topics with borhood within a learned latent embedding almost no inter-cluster coreference. space which minimizes the distance between Such document clustering, however, keeps mentions and the centroids of their gold corefCDCR systems from being generally applicable. erence clusters. We then use these neighborBugert et al. (2020b) shows that inter-cluster corefhoods to sample only hard negatives to train erence makes up the majority of coreference in a fine-grained classifier on mention pairs and 1 many applications. Cremisini and Finlayson (2020) their local discourse features. Our approach achieves state-of-the-art results for both events note that document clustering methods are also unand entities on the ECB+, Gun Violence, Footlikely to generalize well to real data where docball Coreference, and Cross-Domain Crossuments lack the significant lexical differences of Document Coreference corpora. Furthermore, ECB+ topics. These issues present a major barrier training on multiple corpora improves averfor the general applicability of CDCR. age performance across all datasets by 17.2 F1 Human readers, by contrast, are able to perform poi"
2021.emnlp-main.106,N19-1423,0,0.0426206,"Missing"
2021.emnlp-main.106,N18-2007,0,0.0238067,"features to our approach, providing an explanation for the empirical effectiveness of our classifier and that of earlier work like Caciularu et al. (2021). pus (+34.5 F1). We further improve average results by training across all event CDCR corpora, leading to a 17.2 F1 improvement for average performance across all tasks. Our robust model makes it feasible to apply CDCR to a wide variety of downstream tasks, without requiring expensive new coreference annotations to enable fine-tuning on each new corpus. (This has been a huge effort for the few tasks that have attempted it like multi-hop QA (Dhingra et al., 2018; Chen et al., 2019) and multi-document summarization (Falke et al., 2017).) 2 Related Work Cross-Document Coreference Many CDCR algorithms use hand engineered event features to perform classification. Such systems have a low pairwise classification cost and therefore ignore the quadratic scaling and perform no pruning (Bejan and Harabagiu, 2010; Yang et al., 2015; Vossen and Cybulska, 2016; Bugert et al., 2020a). Other such systems choose to include document clustering to Following the recommendations of Bugert et al. increase precision, which can be done with very little tradeoff for the ECB"
2021.emnlp-main.106,I17-1081,0,0.0227693,"iveness of our classifier and that of earlier work like Caciularu et al. (2021). pus (+34.5 F1). We further improve average results by training across all event CDCR corpora, leading to a 17.2 F1 improvement for average performance across all tasks. Our robust model makes it feasible to apply CDCR to a wide variety of downstream tasks, without requiring expensive new coreference annotations to enable fine-tuning on each new corpus. (This has been a huge effort for the few tasks that have attempted it like multi-hop QA (Dhingra et al., 2018; Chen et al., 2019) and multi-document summarization (Falke et al., 2017).) 2 Related Work Cross-Document Coreference Many CDCR algorithms use hand engineered event features to perform classification. Such systems have a low pairwise classification cost and therefore ignore the quadratic scaling and perform no pruning (Bejan and Harabagiu, 2010; Yang et al., 2015; Vossen and Cybulska, 2016; Bugert et al., 2020a). Other such systems choose to include document clustering to Following the recommendations of Bugert et al. increase precision, which can be done with very little tradeoff for the ECB+ corpus (Lee et al., 2012; (2020a), we evaluate our method on multiple ev"
2021.emnlp-main.106,K19-1049,0,0.0239299,"instead optimize the distance metric directly by using the inner product as our scoring function. Before each epoch, we construct the representation of each mention ymi with the encoder from the previous epoch. Each gold coreference cluster yci is represented as the centroid of its component mentions ci : Using this scoring function, the model is trained to predict the correct cluster for a mention with respect to sampled negative clusters. We combine random in-batch negative clusters with hard negatives from the top 10 predicted gold clusters for each training sample in the batch, following Gillick et al. (2019). For each mention mi with true cluster c0 and negative clusters B, the loss is computed using Categorical Cross Entropy loss on the softmax of our score vector, which we express as: L(mi , c0 ) = −so (mi , c0 )+log X exp(so (mi , ci )) ci ∈B (3) This loss function can be interpreted intuitively as rewarding embeddings which form separable dense mention clusters according to their gold coreference labels. The left term in our loss function acts as an attractive component towards the centroid of the gold cluster, while the right term acts as a repulsive component away from the centroids of inco"
2021.emnlp-main.106,T78-1013,0,0.569407,"al state as the set of K nearest neighbors within a latent embedding space for mentions. This space is learned with a distance based classification loss to construct embeddings that minimize the distance between mentions and the centroid of all mentions which share their reference class. These attentional state neighborhoods aggressively constrain the search space for our second stage pairwise classifier. This classifier utilizes cross-attention between mention pairs and their local discourse features to capture the features important within an attentional state which are comparison specific (Grosz, 1978). By sampling from attentional state neighborhoods at training time, we train on only hard negatives such as shown in Table 1. We analyze the contribution of the local discourse features to our approach, providing an explanation for the empirical effectiveness of our classifier and that of earlier work like Caciularu et al. (2021). pus (+34.5 F1). We further improve average results by training across all event CDCR corpora, leading to a 17.2 F1 improvement for average performance across all tasks. Our robust model makes it feasible to apply CDCR to a wide variety of downstream tasks, without r"
2021.emnlp-main.106,J86-3001,0,0.600612,"Coreference, and Cross-Domain Crossuments lack the significant lexical differences of Document Coreference corpora. Furthermore, ECB+ topics. These issues present a major barrier training on multiple corpora improves averfor the general applicability of CDCR. age performance across all datasets by 17.2 F1 Human readers, by contrast, are able to perform points, leading to a robust coreference resolucoreference resolution with minimal pairwise comtion model for use in downstream tasks where link distribution is unknown. parisons. How do they do it? Discourse coherence theory (Grosz, 1977, 1978; Grosz and Sidner, 1986) proposes a simple mechanism: a reader focuses on 1 Introduction only a small set of entities/events from their full Cross-document coreference resolution of entities knowledge. This set, the attentional state, is conand events (CDCR) is an increasingly important structed as entities/events are brought into focus problem, as downstream tasks that benefit from either explicitly by reference or implicitly by their coreference annotations — such as question an- similarity to what has been referenced. Since attenswering, information extraction, and summariza- tional state is inherently dynamic — e"
2021.emnlp-main.106,S18-2001,0,0.0897322,"n. Such systems have a low pairwise classification cost and therefore ignore the quadratic scaling and perform no pruning (Bejan and Harabagiu, 2010; Yang et al., 2015; Vossen and Cybulska, 2016; Bugert et al., 2020a). Other such systems choose to include document clustering to Following the recommendations of Bugert et al. increase precision, which can be done with very little tradeoff for the ECB+ corpus (Lee et al., 2012; (2020a), we evaluate our method on multiple event and entity CDCR corpora, as well as on cross- Cremisini and Finlayson, 2020). corpus transfer for event CDCR. Our method Kenyon-Dean et al. (2018) explore an approach achieves state-of-the-art results on the ECB+ cor- that avoids pairwise classification entirely, instead pus for both events (+0.2 F1) and entities (+0.7 relying purely on representation learning and clusF1), the Gun Violence Corpus (+11.3 F1), the tering within an embedding space. They propose a Football Coreference Corpus (+13.3 F1), and the novel distance based regularization term for their Cross-Domain Cross-Document Coreference Cor- classifier that encourages representations that can 1407 Mention Type Event Entity Mention A preliminary magnitude of 2.0 struck near The"
2021.emnlp-main.106,D12-1045,1,0.75727,"he search space is pruned with simple lution across documents vastly increases the recency-based heuristics, but there is no natural number of candidate mentions, making it incorollary to recency with multiple documents. 2 tractable to do the full n pairwise comparMost CDCR systems thus instead cluster the isons. Existing approaches simplify by considering coreference only within document clusdocuments and perform the full n2 comparisons ters, but this fails to handle inter-cluster corefonly within each cluster, disregarding inter-cluster erence, common in many applications. As a coreference (Lee et al., 2012; Yang et al., 2015; result cross-document coreference algorithms Choubey and Huang, 2017; Barhom et al., 2019; are rarely applied to downstream tasks. We Cattan et al., 2020; Yu et al., 2020; Caciularu et al., draw on an insight from discourse coherence 2021). This was effective for the ECB+ dataset, on theory: potential coreferences are constrained which most CDCR methods have been evaluated, by the reader’s discourse focus. We model the entities/events in a reader’s focus as a neighbecause ECB+ has lexically distinct topics with borhood within a learned latent embedding almost no inter-clus"
2021.emnlp-main.106,D17-1018,0,0.0277648,"Me , s(·, ·)); likelyPairs ← scoreAndSort(pairs, p(·, ·)); C ← InitializeClustersAsSingletons(Me ); for pair ← likelyPairs do (ei , ej ) ← pair; ci ← currentCluster(C, ei ); cj ← currentCluster(C, ej ); if clusterScore(ci , cj ) > 0.5 then C ← mergeClusters(C, ci , cj ) else continue; end end return C; Figure 2: Clustering algorithm used at inference time RoBERTA-large pre-trained weights (Devlin et al., 2019; Liu et al., 2019). A mention is represented as the concatenation of the token-level representations at the boundaries of the mention, following the span boundary representations used by Lee et al. (2017). Optimization Similar to Kenyon-Dean et al. (2018), the network is trained to perform a multiclass classification problem where the classes are labels assigned to the gold coreference clusters, which are the connected components of the coreference graph. Rather than adding distance based regularization, we instead optimize the distance metric directly by using the inner product as our scoring function. Before each epoch, we construct the representation of each mention ymi with the encoder from the previous epoch. Each gold coreference cluster yci is represented as the centroid of its componen"
2021.emnlp-main.106,N18-2108,0,0.0604196,"Missing"
2021.emnlp-main.106,2020.findings-emnlp.440,0,0.0107797,"80.6 CD2CR F1 R P F1 70.9 72.7 57.0 35.0 44.0 82.1 80.3 67.7 72.8 70.2 82.8 77.4 79.7 78.5 Table 2: Evaluation Results using B 3 . For our approaches, (+ )/(− ) indicates usage of discourse or only a single sentence respectively. Methods marked with * perform all pairwise comparisons without pruning. be used for clustering. This approach is more scalable than pairwise classification approaches, but its performance lags behind the state-of-the-art as it cannot use pairwise information. Most recent systems use neural models for pairwise classification (Barhom et al., 2019; Cattan et al., 2020; Meged et al., 2020; Zeng et al., 2020; Yu et al., 2020; Caciularu et al., 2021). These algorithms each use document clustering, a pairwise neural classifier to construct distance matrices within each topic, and agglomerative clustering to compute the final clusters. Innovation has focused on the pairwise classification stage, with variants of document clustering as the only pruning option. Caciularu et al. (2021) sets the previous state of the art for both events and entities in ECB+ using a cross-document language model with a large context window to cross-encode and classify a pair of mentions with the full c"
2021.emnlp-main.106,P16-1060,0,0.0235183,"bulary. It contains 918 documents documents, made up of a 459 pairs of a scientific paper and a newspaper article covering the paper. These articles cover a variety of topics, but since documents come in automatically discovered pairs existing evaluations use the gold document pairs. It contains 13,169 links between 3102 entity mentions. 4.2 Evaluation and Results splits used by Bugert et al. (2020a). For CD2CR, we use the splits used by Ravenscroft et al. (2021). We compare the B 3 metric, since it is reported by baselines for all corpora and has the fewest applicable downsides identified by Moosavi and Strube (2016) since we do not perform mention identification (a full table of metrics for our corpus tailored systems can be found in Appendix A). We use a context window size of 5 sentences during candidate retrieval and of 3 sentences during pairwise classification for all experiments. For corpus tailored evaluations, we retrieve 15 pairs for each mention at training time and 5 pairs at inference time. For cross corpus evaluations, we retrieve 5 pairs for each mention for both training and inference. ECB+ Our approach achieves a new state of the art result on ECB+, which is the most widely used CDCR data"
2021.emnlp-main.106,2021.eacl-main.21,0,0.0309607,"y for CDCR systems to identify event coreference across subtopics. It contains 451 documents covering Football tournaments, where articles covering one tournament often refer to events from other tournaments. While it is the smallest corpus in terms of document size, it has the largest number of coreference links of any dataset we evaluate on with 145,272 coreference links between 3,563 event mentions. Bugert et al. (2020a) re-annotates this corpus at the token level and adds entity labels to enable easier validation between FCC and ECB+. Cross-Domain Cross-Document Coreference Corpus (CD2CR) Ravenscroft et al. (2021) presents a dataset which evaluates the ability for CDCR models to work across domains which vary significantly in style and vocabulary. It contains 918 documents documents, made up of a 459 pairs of a scientific paper and a newspaper article covering the paper. These articles cover a variety of topics, but since documents come in automatically discovered pairs existing evaluations use the gold document pairs. It contains 13,169 links between 3102 entity mentions. 4.2 Evaluation and Results splits used by Bugert et al. (2020a). For CD2CR, we use the splits used by Ravenscroft et al. (2021). We"
2021.emnlp-main.106,L18-1480,0,0.0210179,"pus has been the primary dataset used for evaluating CDCR. This corpus is based on the original Event Coreference Bank corpus from (Bejan and Harabagiu, 2010), with entity annotations added in Lee et al. (2012) to allow joint modeling and additional documents added by Cybulska and Vossen (2014). By number of documents, it is the largest corpus we evaluate on with 982 articles covering 43 diverse topics. It contains 26,712 coreference links between 6,833 event mentions and 69,050 coreference links between 8289 entity mentions. Gun Violence Corpus (GVC) The Gun Violence Corpus was introduced by Vossen et al. (2018) to present a greater challenge for CDCR by Optimization Once the training data has been curating a corpus with high similarity between all generated, we simply train the classifier in a binary mentions and documents covered. All 510 articles setup to classify a pair as either coreferring or non- in the dataset cover incidents of gun violence and coreferring. As with prior work, we optimize our are lexically similar which presents a greater chalpairwise classifier using binary cross-entropy loss. lenge for document clustering. It contains 29,398 1410 Test Dataset Model Baseline Ours Baseline O"
2021.emnlp-main.106,2020.emnlp-main.519,0,0.19186,"to-fine” approach in single document entity coreference resolution. The architecture utilises a bi-linear scoring function to generate a set of likely antecedents, which is then passed through a more expensive classifier which performs higher order inference on antecedent chains. Our work extends to multiple documents the idea of us3.1 Candidate Retrieval ing a high recall but low precision pruning function combined with expensive pairwise classification to Encoding Setup We feed the sentences from balance recall, precision, and runtime efficiency. a window surrounding the mention sentence to Wu et al. (2020) use a similar architecture to ours a fine-tuned BERT architecture initialized from 1408 Algorithm 1: Inference Algorithm Me : mentions; s(·, ·): bi-encoder scorer; p(·, ·): cross-encoder scorer; pairs ← nearestNeighborPairs(Me , s(·, ·)); likelyPairs ← scoreAndSort(pairs, p(·, ·)); C ← InitializeClustersAsSingletons(Me ); for pair ← likelyPairs do (ei , ej ) ← pair; ci ← currentCluster(C, ei ); cj ← currentCluster(C, ej ); if clusterScore(ci , cj ) > 0.5 then C ← mergeClusters(C, ci , cj ) else continue; end end return C; Figure 2: Clustering algorithm used at inference time RoBERTA-large pre"
2021.emnlp-main.106,Q15-1037,0,0.137003,"pruned with simple lution across documents vastly increases the recency-based heuristics, but there is no natural number of candidate mentions, making it incorollary to recency with multiple documents. 2 tractable to do the full n pairwise comparMost CDCR systems thus instead cluster the isons. Existing approaches simplify by considering coreference only within document clusdocuments and perform the full n2 comparisons ters, but this fails to handle inter-cluster corefonly within each cluster, disregarding inter-cluster erence, common in many applications. As a coreference (Lee et al., 2012; Yang et al., 2015; result cross-document coreference algorithms Choubey and Huang, 2017; Barhom et al., 2019; are rarely applied to downstream tasks. We Cattan et al., 2020; Yu et al., 2020; Caciularu et al., draw on an insight from discourse coherence 2021). This was effective for the ECB+ dataset, on theory: potential coreferences are constrained which most CDCR methods have been evaluated, by the reader’s discourse focus. We model the entities/events in a reader’s focus as a neighbecause ECB+ has lexically distinct topics with borhood within a learned latent embedding almost no inter-cluster coreference. sp"
2021.emnlp-main.106,2020.coling-main.275,0,0.0159836,"1 70.9 72.7 57.0 35.0 44.0 82.1 80.3 67.7 72.8 70.2 82.8 77.4 79.7 78.5 Table 2: Evaluation Results using B 3 . For our approaches, (+ )/(− ) indicates usage of discourse or only a single sentence respectively. Methods marked with * perform all pairwise comparisons without pruning. be used for clustering. This approach is more scalable than pairwise classification approaches, but its performance lags behind the state-of-the-art as it cannot use pairwise information. Most recent systems use neural models for pairwise classification (Barhom et al., 2019; Cattan et al., 2020; Meged et al., 2020; Zeng et al., 2020; Yu et al., 2020; Caciularu et al., 2021). These algorithms each use document clustering, a pairwise neural classifier to construct distance matrices within each topic, and agglomerative clustering to compute the final clusters. Innovation has focused on the pairwise classification stage, with variants of document clustering as the only pruning option. Caciularu et al. (2021) sets the previous state of the art for both events and entities in ECB+ using a cross-document language model with a large context window to cross-encode and classify a pair of mentions with the full context of their doc"
2021.naacl-main.323,D18-1488,0,0.527613,"gs of Z, Grimmer and Fong (2020) studied a closely related setting where text documents are randomly assigned to readers who produce outcomes. From this experiment, they discover text properties that cause the outcome. Their causal identification result requires an exclusion restriction assumption, which is related to the no unobserved confounding assumption that we make. 4097 This estimand only requires an adjustment for the ˜ We show how to exconfounding information Z. tract this information using pretrained language models in Section 5. Prior work on causal inference with proxy treatments (Wood-Doughty et al., 2018) requires an adjustment using the measurement model P (T˜ |Tˆ), i.e. the true relationship between the proxy label Tˆ and its target T˜, which is typically unobserved. In contrast, the estimand ψ proxy does not require the measurement model. The following result shows that the estimand ψ proxy only attenuates the ATE that we want, ψ rea. . That is, the bias due to proxy treatments is benign; it can only decrease the magnitude of the effect but it does not change the sign. ˜ and Theorem 2. Let 0 = Pr(T˜ = 0 |Tˆ = 1, Z) ˜ Then, let 1 = Pr(T˜ = 1 |Tˆ = 0, Z). h ˜ E[Y |T˜ = 1, Z]  i ˜ 0 + 1"
2021.naacl-main.323,J10-1004,0,0.0229047,"to be polite or not. The outcome is a variable Y , e.g., how long it took for this complaint to be serviced. Let Z be other linguistic properties that the writer communicated (consciously or unconsciously) via the text W , e.g. topic, brevity or eloquence. The linguistic properties T and Z are typically correlated, and both variables affect the outcome Y . 1 Literary theory argues that language is subject to two perspectives: the “artistic” pole – the text as intended by the author – and the “aesthetic” pole – the text as interpreted by the reader (Iser, 1974, 1979). The noisy channel model (Yuret and Yatbaz, 2010; Gibson et al., 2013) connects these poles by supposing that the reader perceives a noisy version of the author’s intent. This duality has also been modeled in linguistic pragmatics as the difference between speaker meaning and literal or utterance meaning (Potts, 2009; Levinson, 1995, 2000). Gricean pragmatic models like RSA (Goodman and Frank, 2016) similarly formalize this as the reader using the literal meaning to help make inferences about the speaker’s intent. 2 We leave higher-dimensional extensions to future work. We are interested in the causal effects of linguistic properties. To fo"
2021.naacl-main.416,D14-1162,0,0.0912178,"Missing"
2021.naacl-main.416,2020.acl-demos.14,1,0.827425,"K , Y¨ ) is further that depend on X. Mmesh (X processed with a feed forward layer, a residual connection, and a layer normalization to output Y˜ . As like in the encoder, the decoder can be stacked N times to output Y˜ N . Y˜ N is further passed to a feed forward layer to output report yˆ. precision (pr) and recall (rc) of entity match are calculated as P e∈Egen prENT = δ(e, Eref ) |Egen | e∈Eref δ(e, Egen ) (8) P rcENT = |Eref | ( δ(e, E) = 1, for e ∈ E 0, otherwise (9) (10) The harmonic mean of precision and recall is taken as factENT to reward a balanced match of entities. We used Stanza (Qi et al., 2020) and its clinical models (Zhang et al., 2020c) as a named entity recognizer for radiology reports. For example in the case of Figure 1, the common entities among the reference report and the generated report are pleural and effusion, resulting to factENT = 33.3. 3.2.2 Entailing Entity Match Reward (factENTNLI ) We additionally designed an F-score style reward that expands factENT with NLI to capture factual consistency. NLI is used to control the overestimation of disease when optimizing towards factENT . In factENTNLI , δ in Eq. 10 is expanded to    1,   φ(e,E)= 1,     0, for e ∈ E"
2021.naacl-main.416,2020.acl-main.450,0,0.14576,"2019), to adopt a hybrid retrieval-generation model (Li et al., 2018), or to consider structure information (Jing et al., 2019). More recent work has focused on generating reports that are clinically consistent and accurate. Liu et al. (2019) presented a system that generates accurate reports by fine-tuning it with their Clinically Although recent work in summarization, dialogue, and data-to-text generation has tried to address this problem of factual incompleteness and inconsistency by using natural language inference (NLI) (Falke et al., 2019; Welleck et al., 2019), question answering (QA) (Wang et al., 2020a), or content matching constraint (Wang et al., 2020b) approaches, they either show negative results or are not directly applicable to the generation of radiology reports due to a substantial task and do5289 1 https://github.com/ysmiura/ifcc Coherent Reward. Boag et al. (2020) evaluated several baseline generation systems with clinical metrics and found that standard NLG metrics are ill-equipped for this task. Very recently, Chen et al. (2020) proposed an approach to generate radiology reports with a memory-driven Transformer. Our work is most related to Liu et al. (2019); their system, howev"
2021.naacl-main.416,2020.acl-main.101,0,0.145008,"2019), to adopt a hybrid retrieval-generation model (Li et al., 2018), or to consider structure information (Jing et al., 2019). More recent work has focused on generating reports that are clinically consistent and accurate. Liu et al. (2019) presented a system that generates accurate reports by fine-tuning it with their Clinically Although recent work in summarization, dialogue, and data-to-text generation has tried to address this problem of factual incompleteness and inconsistency by using natural language inference (NLI) (Falke et al., 2019; Welleck et al., 2019), question answering (QA) (Wang et al., 2020a), or content matching constraint (Wang et al., 2020b) approaches, they either show negative results or are not directly applicable to the generation of radiology reports due to a substantial task and do5289 1 https://github.com/ysmiura/ifcc Coherent Reward. Boag et al. (2020) evaluated several baseline generation systems with clinical metrics and found that standard NLG metrics are ill-equipped for this task. Very recently, Chen et al. (2020) proposed an approach to generate radiology reports with a memory-driven Transformer. Our work is most related to Liu et al. (2019); their system, howev"
cer-etal-2010-parsing,de-marneffe-etal-2006-generating,1,\N,Missing
cer-etal-2010-parsing,A00-2018,0,\N,Missing
cer-etal-2010-parsing,nivre-etal-2006-maltparser,0,\N,Missing
cer-etal-2010-parsing,W07-1420,0,\N,Missing
cer-etal-2010-parsing,N03-1033,0,\N,Missing
cer-etal-2010-parsing,W09-1419,0,\N,Missing
cer-etal-2010-parsing,W07-1423,0,\N,Missing
cer-etal-2010-parsing,C96-1058,0,\N,Missing
cer-etal-2010-parsing,W09-1402,0,\N,Missing
cer-etal-2010-parsing,W08-1301,1,\N,Missing
cer-etal-2010-parsing,W07-1004,0,\N,Missing
cer-etal-2010-parsing,P03-1054,0,\N,Missing
cer-etal-2010-parsing,H05-1066,0,\N,Missing
cer-etal-2010-parsing,P05-1022,0,\N,Missing
cer-etal-2010-parsing,W07-1417,0,\N,Missing
cer-etal-2010-parsing,P06-1055,0,\N,Missing
cer-etal-2010-parsing,P08-1006,0,\N,Missing
cer-etal-2010-parsing,W03-3017,0,\N,Missing
cer-etal-2010-parsing,W04-3224,0,\N,Missing
cer-etal-2010-parsing,P08-1000,0,\N,Missing
chambers-jurafsky-2010-database,W04-1017,0,\N,Missing
chambers-jurafsky-2010-database,W99-0211,0,\N,Missing
chambers-jurafsky-2010-database,S07-1014,0,\N,Missing
chambers-jurafsky-2010-database,W09-1111,0,\N,Missing
chambers-jurafsky-2010-database,N04-1038,0,\N,Missing
chambers-jurafsky-2010-database,P98-1013,0,\N,Missing
chambers-jurafsky-2010-database,C98-1013,0,\N,Missing
chambers-jurafsky-2010-database,P06-1095,0,\N,Missing
chambers-jurafsky-2010-database,P07-2044,1,\N,Missing
chambers-jurafsky-2010-database,P08-1090,1,\N,Missing
chambers-jurafsky-2010-database,P09-1068,1,\N,Missing
D09-1035,N09-1072,1,0.447766,"transcripts, as described in the next section. We then trained four separate binary classifiers (for each gender for both perception and intention). specific lexical features would just be to throw counts for every word into a huge feature vector, but the curse of dimensionality rules this method out in small training set situations. We propose a new solution to this problem, using an unsupervised deep autoencoder to automatically compress and extract complex high level lexical features. 2 Dataset Our experiments make use of the SpeedDate Corpus collected by the third author, and described in Jurafsky et al. (2009). The corpus is based on three speed-dating sessions run at an American university in 2005, inspired by prior speeddating research (Madan et al., 2005). The graduate student participants volunteered to be in the study and were promised emails of persons with whom they reported mutual liking. All participants wore audio recorders on a shoulder sash, thus resulting in two audio recordings of the approximately 1100 4-minute dates. Each date was conducted in an open setting where there was substantial background noise. This noisy audio was thus hand-transcribed and turn start and end were hand-ali"
D09-1035,P08-1020,0,0.0206278,"Missing"
D10-1048,D08-1031,0,0.846856,"s. 1 The second attack occurred after some rocket firings aimed, apparently, toward [the israelis], apparently in retaliation. [we]’re checking our facts on that one. ... the president, quoted by ari fleischer, his spokesman, is saying he’s concerned the strike will undermine efforts by palestinian authorities to bring an end to terrorist attacks and does not contribute to the security of [israel]. Introduction Recent work on coreference resolution has shown that a rich feature space that models lexical, syntactic, semantic, and discourse phenomena is crucial to successfully address the task (Bengston and Roth, 2008; Haghighi and Klein, 2009; Haghighi and Klein, 2010). When such a rich representation Most state-of-the-art models will incorrectly link we to the israelis because of their proximity and compatibility of attributes (both we and the israelis are plural). In contrast, a more cautious approach is to first cluster the israelis with israel because the demonymy relation is highly precise. This initial clustering step will assign the correct animacy attribute (inanimate) to the corresponding geo-political entity, which will prevent the incorrect merging with the mention we (animate) in later steps."
D10-1048,P06-1005,0,0.370589,"ttributes. These are crucial factors for pronominal coreference. Like previous work, we implement pronominal coreference resolution by enforcing agreement constraints between the coreferent mentions. We use the following attributes for these constraints: Number – we assign number attributes based on: (a) a static list for pronouns; (b) NER labels: mentions marked as a named entity are considered singular with the exception of organizations, which can be both singular or plural; (c) part of speech tags: NN*S tags are plural and all other NN* tags are singular; and (d) a static dictionary from (Bergsma and Lin, 2006). Gender – we assign gender attributes from static lexicons from (Bergsma and Lin, 2006; Ji and Lin, 2009). Person – we assign person attributes only to pronouns. However, we do not enforce this constraint when linking two pronouns if one appears within quotes. This is a simple heuristic for speaker detection, e.g., I and she point to the same person in “[I] voted my conscience,” [she] said. Animacy – we set animacy attributes using: (a) a static list for pronouns; (b) NER labels, e.g., PERSON is animate whereas LOCATION is not; and (c) a dictionary boostrapped from the web (Ji and Lin, 2009)."
D10-1048,J93-2003,0,0.0155393,"ls entity clusters explicitly using a mostly-unsupervised generative model. As previously mentioned, our work is not constrained by first-order or Bayesian formalisms in how it uses cluster information. Additionally, the deterministic models in our tiered model are significantly simpler, yet perform generally better than the complex inference models proposed in these works. From a high level perspective, this work falls under the theory of shaping, defined as a “method of successive approximations” for learning (Skinner, 1938). This theory is known by different names in many NLP applications: Brown et al. (1993) used simple models as “stepping stones” for more complex word alignment models; Collins (1999) used “cautious” decision list learning for named entity classification; Spitkovsky et al. (2010) used “baby steps” for unsupervised dependency parsing, etc. To the best of our knowledge, we are the first to apply this theory to coreference resolution. 3 Description of the Task Intra-document coreference resolution clusters together textual mentions within a single document based on the underlying referent entity. Mentions are usually noun phrases (NPs) headed by nominal or pronominal terminals. To f"
D10-1048,W99-0613,0,0.0599343,"Missing"
D10-1048,N07-1011,0,0.106871,"in both supervised and unsupervised learning setups (Bengston and Roth, 2008; Haghighi and Klein, 2009). Our work reinforces this observation, and extends it by proposing a novel architecture that: (a) allows easy deployment of such features, and (b) infuses global information that can be readily exploited by these features or constraints. Most coreference resolution approaches perform the task by aggregating local decisions about pairs of mentions (Bengston and Roth, 2008; Finkel and Manning, 2008; Haghighi and Klein, 2009; Stoyanov, 2010). Two recent works that diverge from this pattern are Culotta et al. (2007) and Poon and 493 Domingos (2008). They perform coreference resolution jointly for all mentions in a document, using first-order probabilistic models in either supervised or unsupervised settings. Haghighi and Klein (2010) propose a generative approach that models entity clusters explicitly using a mostly-unsupervised generative model. As previously mentioned, our work is not constrained by first-order or Bayesian formalisms in how it uses cluster information. Additionally, the deterministic models in our tiered model are significantly simpler, yet perform generally better than the complex inf"
D10-1048,P10-2007,0,0.0457833,"Missing"
D10-1048,P05-1045,1,0.02431,"onference (MUC-6) evaluation. It contains 30 documents and 2,068 mentions. We used the first corpus (ACE2004-ROTH-DEV) for development. The other corpora are reserved for testing. We parse all documents using the Stanford parser (Klein and Manning, 2003). The syntactic information is used to identify the mention head words and to define the ordering of mentions in a given sentence (detailed in the next section). For a fair comparison with previous work, we do not use gold named entity labels or mention types but, instead, take the labels provided by the Stanford named entity recognizer (NER) (Finkel et al., 2005). 3.2 Evaluation Metrics We use three evaluation metrics widely used in the literature: (a) pairwise F1 (Ghosh, 2003) – computed over mention pairs in the same entity cluster; (b) MUC (Vilain et al., 1995) – which measures how many predicted clusters need to be merged to cover the gold clusters; and (c) B3 (Amit and Baldwin, 1998) – which uses the intersection between predicted and gold clusters for a given mention to mark correct mentions and the sizes of the the predicted and gold clusters as denominators for precision and recall, respectively. We refer the interested reader to (X. Luo, 2005"
D10-1048,P08-2012,1,0.850737,"rk This work builds upon the recent observation that strong features outweigh complex models for coreference resolution, in both supervised and unsupervised learning setups (Bengston and Roth, 2008; Haghighi and Klein, 2009). Our work reinforces this observation, and extends it by proposing a novel architecture that: (a) allows easy deployment of such features, and (b) infuses global information that can be readily exploited by these features or constraints. Most coreference resolution approaches perform the task by aggregating local decisions about pairs of mentions (Bengston and Roth, 2008; Finkel and Manning, 2008; Haghighi and Klein, 2009; Stoyanov, 2010). Two recent works that diverge from this pattern are Culotta et al. (2007) and Poon and 493 Domingos (2008). They perform coreference resolution jointly for all mentions in a document, using first-order probabilistic models in either supervised or unsupervised settings. Haghighi and Klein (2010) propose a generative approach that models entity clusters explicitly using a mostly-unsupervised generative model. As previously mentioned, our work is not constrained by first-order or Bayesian formalisms in how it uses cluster information. Additionally, the"
D10-1048,D09-1120,0,0.142423,"curred after some rocket firings aimed, apparently, toward [the israelis], apparently in retaliation. [we]’re checking our facts on that one. ... the president, quoted by ari fleischer, his spokesman, is saying he’s concerned the strike will undermine efforts by palestinian authorities to bring an end to terrorist attacks and does not contribute to the security of [israel]. Introduction Recent work on coreference resolution has shown that a rich feature space that models lexical, syntactic, semantic, and discourse phenomena is crucial to successfully address the task (Bengston and Roth, 2008; Haghighi and Klein, 2009; Haghighi and Klein, 2010). When such a rich representation Most state-of-the-art models will incorrectly link we to the israelis because of their proximity and compatibility of attributes (both we and the israelis are plural). In contrast, a more cautious approach is to first cluster the israelis with israel because the demonymy relation is highly precise. This initial clustering step will assign the correct animacy attribute (inanimate) to the corresponding geo-political entity, which will prevent the incorrect merging with the mention we (animate) in later steps. We propose an unsupervised"
D10-1048,N10-1061,0,0.468981,"Missing"
D10-1048,Y09-1024,0,0.0248102,"reference resolution by enforcing agreement constraints between the coreferent mentions. We use the following attributes for these constraints: Number – we assign number attributes based on: (a) a static list for pronouns; (b) NER labels: mentions marked as a named entity are considered singular with the exception of organizations, which can be both singular or plural; (c) part of speech tags: NN*S tags are plural and all other NN* tags are singular; and (d) a static dictionary from (Bergsma and Lin, 2006). Gender – we assign gender attributes from static lexicons from (Bergsma and Lin, 2006; Ji and Lin, 2009). Person – we assign person attributes only to pronouns. However, we do not enforce this constraint when linking two pronouns if one appears within quotes. This is a simple heuristic for speaker detection, e.g., I and she point to the same person in “[I] voted my conscience,” [she] said. Animacy – we set animacy attributes using: (a) a static list for pronouns; (b) NER labels, e.g., PERSON is animate whereas LOCATION is not; and (c) a dictionary boostrapped from the web (Ji and Lin, 2009). NER label – from the Stanford NER. If we cannot detect a value, we set attributes to unknown and treat th"
D10-1048,P03-1054,1,0.0100249,"a et al., 2007; Bengston and Roth, 2008; Haghighi and Klein, 2009). It consists of 107 documents and 5,469 mentions. • ACE2004-NWIRE – the newswire subset of the ACE 2004 corpus, utilized by Poon and Domingos (2008) and Haghighi and Klein (2009) for testing. It contains 128 documents and 11,413 mentions. • MUC6-TEST – test corpus from the sixth Message Understanding Conference (MUC-6) evaluation. It contains 30 documents and 2,068 mentions. We used the first corpus (ACE2004-ROTH-DEV) for development. The other corpora are reserved for testing. We parse all documents using the Stanford parser (Klein and Manning, 2003). The syntactic information is used to identify the mention head words and to define the ordering of mentions in a given sentence (detailed in the next section). For a fair comparison with previous work, we do not use gold named entity labels or mention types but, instead, take the labels provided by the Stanford named entity recognizer (NER) (Finkel et al., 2005). 3.2 Evaluation Metrics We use three evaluation metrics widely used in the literature: (a) pairwise F1 (Ghosh, 2003) – computed over mention pairs in the same entity cluster; (b) MUC (Vilain et al., 1995) – which measures how many pr"
D10-1048,H05-1004,0,0.787257,"l., 2005). 3.2 Evaluation Metrics We use three evaluation metrics widely used in the literature: (a) pairwise F1 (Ghosh, 2003) – computed over mention pairs in the same entity cluster; (b) MUC (Vilain et al., 1995) – which measures how many predicted clusters need to be merged to cover the gold clusters; and (c) B3 (Amit and Baldwin, 1998) – which uses the intersection between predicted and gold clusters for a given mention to mark correct mentions and the sizes of the the predicted and gold clusters as denominators for precision and recall, respectively. We refer the interested reader to (X. Luo, 2005; Finkel and Manning, 2008) for an analysis of these metrics. 4 Description of the Multi-Pass Sieve Our sieve framework is implemented as a succession of independent coreference models. We first describe how each model selects candidate mentions, and then describe the models themselves. 494 4.1 Mention Processing Given a mention mi , each model may either decline to propose a solution (in the hope that one of the subsequent models will solve it) or deterministically select a single best antecedent from a list of previous mentions m1 , . . . , mi−1 . We sort candidate antecedents using syntacti"
D10-1048,D08-1068,0,0.227799,"• ACE2004-ROTH-DEV2 – development split of Bengston and Roth (2008), from the corpus used in the 2004 Automatic Content Extraction (ACE) evaluation. It contains 68 documents and 4,536 mentions. 2 We use the same corpus names as (Haghighi and Klein, 2009) to facilitate comparison with previous work. • ACE2004-CULOTTA-TEST – partition of ACE 2004 corpus reserved for testing by several previous works (Culotta et al., 2007; Bengston and Roth, 2008; Haghighi and Klein, 2009). It consists of 107 documents and 5,469 mentions. • ACE2004-NWIRE – the newswire subset of the ACE 2004 corpus, utilized by Poon and Domingos (2008) and Haghighi and Klein (2009) for testing. It contains 128 documents and 11,413 mentions. • MUC6-TEST – test corpus from the sixth Message Understanding Conference (MUC-6) evaluation. It contains 30 documents and 2,068 mentions. We used the first corpus (ACE2004-ROTH-DEV) for development. The other corpora are reserved for testing. We parse all documents using the Stanford parser (Klein and Manning, 2003). The syntactic information is used to identify the mention head words and to define the ordering of mentions in a given sentence (detailed in the next section). For a fair comparison with pr"
D10-1048,N10-1116,0,0.0131313,"er information. Additionally, the deterministic models in our tiered model are significantly simpler, yet perform generally better than the complex inference models proposed in these works. From a high level perspective, this work falls under the theory of shaping, defined as a “method of successive approximations” for learning (Skinner, 1938). This theory is known by different names in many NLP applications: Brown et al. (1993) used simple models as “stepping stones” for more complex word alignment models; Collins (1999) used “cautious” decision list learning for named entity classification; Spitkovsky et al. (2010) used “baby steps” for unsupervised dependency parsing, etc. To the best of our knowledge, we are the first to apply this theory to coreference resolution. 3 Description of the Task Intra-document coreference resolution clusters together textual mentions within a single document based on the underlying referent entity. Mentions are usually noun phrases (NPs) headed by nominal or pronominal terminals. To facilitate comparison with most of the recent previous work, we report results using gold mention boundaries. However, our approach does not make any assumptions about the underlying mentions,"
D10-1048,M95-1005,0,0.961704,"ing the Stanford parser (Klein and Manning, 2003). The syntactic information is used to identify the mention head words and to define the ordering of mentions in a given sentence (detailed in the next section). For a fair comparison with previous work, we do not use gold named entity labels or mention types but, instead, take the labels provided by the Stanford named entity recognizer (NER) (Finkel et al., 2005). 3.2 Evaluation Metrics We use three evaluation metrics widely used in the literature: (a) pairwise F1 (Ghosh, 2003) – computed over mention pairs in the same entity cluster; (b) MUC (Vilain et al., 1995) – which measures how many predicted clusters need to be merged to cover the gold clusters; and (c) B3 (Amit and Baldwin, 1998) – which uses the intersection between predicted and gold clusters for a given mention to mark correct mentions and the sizes of the the predicted and gold clusters as denominators for precision and recall, respectively. We refer the interested reader to (X. Luo, 2005; Finkel and Manning, 2008) for an analysis of these metrics. 4 Description of the Multi-Pass Sieve Our sieve framework is implemented as a succession of independent coreference models. We first describe h"
D10-1048,P09-1074,0,\N,Missing
D12-1045,W99-0201,0,0.522129,"ooking at which semantic role the entity mentions can have and the verb pairs of their predicates. We confirm 490 that such features are useful but also show that the complementary features for verbal mentions lead to even better performance, especially when event and entity clusters are jointly modeled. Compared to the extensive work on entity coreference, the related problem of event coreference remains relatively under-explored, with minimal work on how entity and event coreference can be considered jointly on an open domain. Early work on event coreference for MUC (Humphreys et al., 1997; Bagga and Baldwin, 1999) focused on scenariospecific events. More recently, there have been approaches that looked at event coreference for wider domains. Chen and Ji (2009) proposed using spectral graph clustering to cluster events. Bejan and Harabagiu (2010) proposed a nonparametric Bayesian model for open-domain event resolution. However, most of this prior work focused only on event coreference, whereas we address both entities and events with a single model. Humphreys et al. (1997) considered entities as well as events, but due to the lack of a corpus annotated with event coreference, their approach was only eva"
D12-1045,P10-1143,0,0.762167,"ormance, especially when event and entity clusters are jointly modeled. Compared to the extensive work on entity coreference, the related problem of event coreference remains relatively under-explored, with minimal work on how entity and event coreference can be considered jointly on an open domain. Early work on event coreference for MUC (Humphreys et al., 1997; Bagga and Baldwin, 1999) focused on scenariospecific events. More recently, there have been approaches that looked at event coreference for wider domains. Chen and Ji (2009) proposed using spectral graph clustering to cluster events. Bejan and Harabagiu (2010) proposed a nonparametric Bayesian model for open-domain event resolution. However, most of this prior work focused only on event coreference, whereas we address both entities and events with a single model. Humphreys et al. (1997) considered entities as well as events, but due to the lack of a corpus annotated with event coreference, their approach was only evaluated implicitly in the MUC-6 template filling task. To our knowledge, the only previous work that considered entity and event coreference resolution jointly is He (2007), but limited to the medical domain and focused on just five sema"
D12-1045,J95-4004,0,0.0241778,"omic unit. On the other hand, our approach generates more training data than online learning, which trains using only the actual decisions taken during inference in each iteration (i.e., 2 We skip the pronoun sieve here because it does not affect the decisions taken during the iterative resolution steps. 493 the pair (e1 , e2 ) in step 13). After each epoch we have a new training corpus Γ, which we use to train the new linear regression model Θ’ (step 15), which is then interpolated with the old one (step 16). Our training procedure is similar in spirit to transformation based learning (TBL) (Brill, 1995). Similarly to TBL, our approach repeatedly applies the model over the training data and attempts to minimize the error rate of the current model. However, while TBL learns rules that directly minimize the current error rate, our approach achieves this indirectly, by incorporating the reduction in error rate in the score of the generated datums. This allows us to fit a linear regression to this task, which, as discussed before, is a better model for this task. Just like any hill-climbing algorithm, our approach has the risk of converging to a local maximum. To mitigate this risk, we do not ini"
D12-1045,W09-3208,0,0.403524,"ow that the complementary features for verbal mentions lead to even better performance, especially when event and entity clusters are jointly modeled. Compared to the extensive work on entity coreference, the related problem of event coreference remains relatively under-explored, with minimal work on how entity and event coreference can be considered jointly on an open domain. Early work on event coreference for MUC (Humphreys et al., 1997; Bagga and Baldwin, 1999) focused on scenariospecific events. More recently, there have been approaches that looked at event coreference for wider domains. Chen and Ji (2009) proposed using spectral graph clustering to cluster events. Bejan and Harabagiu (2010) proposed a nonparametric Bayesian model for open-domain event resolution. However, most of this prior work focused only on event coreference, whereas we address both entities and events with a single model. Humphreys et al. (1997) considered entities as well as events, but due to the lack of a corpus annotated with event coreference, their approach was only evaluated implicitly in the MUC-6 template filling task. To our knowledge, the only previous work that considered entity and event coreference resolutio"
D12-1045,N07-1030,0,0.00724113,"well studied problem with many successful techniques for identifying mention clusters (Ponzetto and Strube, 2006; Haghighi and Klein, 2009; Stoyanov et al., 2009; Haghighi and Klein, 2010; Raghunathan et al., 2010; Rahman and Ng, 2011, inter alia). Most of these techniques focus on matching compatible noun pairs using various syntactic and semantic features, with efforts targeted toward improving features and clustering models. Prior work showed that models that jointly resolve mentions across multiple entities result in better performance than simply resolving mentions in a pairwise fashion (Denis and Baldridge, 2007; Poon and Domingos, 2008; Wick et al., 2008; Lee et al., 2011, inter alia). A natural extension is to perform coreference jointly across both entities and events. Yet there has been little attempt in this direction. We know of only limited work that incorporates event-related information in entity coreference, typically by incorporating the verbs in context as features. For instance, Haghighi and Klein (2010) include the governor of the head of nominal mentions as features in their model. Rahman and Ng (2011) also used event-related information by looking at which semantic role the entity men"
D12-1045,D09-1120,0,0.0481785,"cies. • We annotate and release a new corpus with coreference relations between both entities and events across documents. The relations annotated are both intra and inter-document, which more accurately models real-world scenarios. • We evaluate our cross-document coreference resolution system on this corpus and show that our joint approach significantly outperforms two strong baselines that resolve entities and events separately. 2 Related Work Entity coreference resolution is a well studied problem with many successful techniques for identifying mention clusters (Ponzetto and Strube, 2006; Haghighi and Klein, 2009; Stoyanov et al., 2009; Haghighi and Klein, 2010; Raghunathan et al., 2010; Rahman and Ng, 2011, inter alia). Most of these techniques focus on matching compatible noun pairs using various syntactic and semantic features, with efforts targeted toward improving features and clustering models. Prior work showed that models that jointly resolve mentions across multiple entities result in better performance than simply resolving mentions in a pairwise fashion (Denis and Baldridge, 2007; Poon and Domingos, 2008; Wick et al., 2008; Lee et al., 2011, inter alia). A natural extension is to perform co"
D12-1045,N10-1061,0,0.0614507,"coreference relations between both entities and events across documents. The relations annotated are both intra and inter-document, which more accurately models real-world scenarios. • We evaluate our cross-document coreference resolution system on this corpus and show that our joint approach significantly outperforms two strong baselines that resolve entities and events separately. 2 Related Work Entity coreference resolution is a well studied problem with many successful techniques for identifying mention clusters (Ponzetto and Strube, 2006; Haghighi and Klein, 2009; Stoyanov et al., 2009; Haghighi and Klein, 2010; Raghunathan et al., 2010; Rahman and Ng, 2011, inter alia). Most of these techniques focus on matching compatible noun pairs using various syntactic and semantic features, with efforts targeted toward improving features and clustering models. Prior work showed that models that jointly resolve mentions across multiple entities result in better performance than simply resolving mentions in a pairwise fashion (Denis and Baldridge, 2007; Poon and Domingos, 2008; Wick et al., 2008; Lee et al., 2011, inter alia). A natural extension is to perform coreference jointly across both entities and events"
D12-1045,W97-1311,0,0.841147,"related information by looking at which semantic role the entity mentions can have and the verb pairs of their predicates. We confirm 490 that such features are useful but also show that the complementary features for verbal mentions lead to even better performance, especially when event and entity clusters are jointly modeled. Compared to the extensive work on entity coreference, the related problem of event coreference remains relatively under-explored, with minimal work on how entity and event coreference can be considered jointly on an open domain. Early work on event coreference for MUC (Humphreys et al., 1997; Bagga and Baldwin, 1999) focused on scenariospecific events. More recently, there have been approaches that looked at event coreference for wider domains. Chen and Ji (2009) proposed using spectral graph clustering to cluster events. Bejan and Harabagiu (2010) proposed a nonparametric Bayesian model for open-domain event resolution. However, most of this prior work focused only on event coreference, whereas we address both entities and events with a single model. Humphreys et al. (1997) considered entities as well as events, but due to the lack of a corpus annotated with event coreference, t"
D12-1045,W11-1902,1,0.774662,"Missing"
D12-1045,P98-2127,0,0.0174313,"and {acquired} in the above example is 1. E Indicator feature set to 1 if the two clusters have at least one coreferent predicate for a given role. For example, for the clusters {the man} and {the person}, extracted from the sentences helped [the man]Arg1 and helped [the person]Arg1 , the value of this feature is 1 if the two helped verbs were previously clustered together. E Cosine similarity of vectors containing words that are distributionally similar to words in the cluster mentions. We built these vectors by extracting the top-ten most-similar words in Dekang Lin’s similarity thesaurus (Lin, 1998) for all the nouns/adjectives/verbs in a cluster. For example, for the singleton cluster {a new home}, we construct this vector by expanding new and home to: {new:1, original:1, old:1, existing:1, current:1, unique:1, modern:1, different:1, special:1, major:1, small:1, home:1, house:1, apartment:1, building:1, hotel:1, residence:1, office:1, mansion:1, school:1, restaurant:1, hospital:1 }. E Cosine similarity of number, gender, animacy, and NE label vectors. For example, the number and gender vectors for the two-mention cluster {systems, a pen} are Number = {singular:1, plural:1}, Gender = {ne"
D12-1045,H05-1004,0,0.873901,"82.2 71.5 71.7 72.3 54.2 54.8 55.9 Table 4: Performance of the two baselines and our model. We report scores for entity clusters, event clusters and the complete task using five metrics. 6.2 Evaluation We use five coreference evaluation metrics widely used in the literature: MUC (Vilain et al., 1995) Link-based metric which measures how many predicted and gold clusters need to be merged to cover the gold and predicted clusters, respectively. B3 (Bagga and Baldwin, 1998) Mention-based metric which measures the proportion of overlap between predicted and gold clusters for a given mention. CEAF (Luo, 2005) Entity-based metric that, unlike B 3 , enforces a one-to-one alignment between gold and predicted clusters. We employ the entity-based version of CEAF. BLANC (Recasens and Hovy, 2011) Metric based on the Rand index (Rand, 1971) that considers both coreference and non-coreference links to address the imbalance between singleton and coreferent mentions. CoNLL F1 Average of MUC, B 3 , and CEAF-φ4 . This was the official metric in the CoNLL-2011 shared task (Pradhan et al., 2011). We followed the CoNLL-2011 evaluation methodology, that is, we removed all singleton clusters, and apposition/copular"
D12-1045,W04-2705,0,0.0538477,"al mention we consider the merge an operation between event (V) clusters; otherwise it is a merge between entity (E) clusters. We append to all entity features the suffix Proper or Common based on the type of the head word of the first mention in each of the two clusters. We use the suffix Proper only if both head words are proper nouns. paper we used a single heuristic: the possessor of a nominal event’s predicate is marked as its Arg0, e.g., Logan is the Arg0 to run in Logan’s run.4 4 A principled solution to this problem is to use an SRL system for nominal predicates trained using NomBank (Meyers et al., 2004). We will address this in future work. 494 We extracted named entity labels using the named entity recognizer from the Stanford CoreNLP suite. 6 Evaluation 6.1 Corpus The training and test data sets were derived from the EventCorefBank (ECB) corpus5 created by Bejan and Harabagiu (2010) to study event coreference since standard corpora such as OntoNotes (Pradhan et al., 2007) contain a small number of annotated event clusters. The ECB corpus consists of 482 documents from Google News clustered into 43 topics, where a topic is described as a seminal event. The reason for including comparable do"
D12-1045,passonneau-2004-computing,0,0.00676247,"ENTi hENTITY COREFID=“28”i rehab h/ENTITYi. Figure 1: Annotation example. Light verbs Verbs such as give and make followed by a noun (e.g., make an offer) were not annotated, but the noun was. Phrasal verbs We annotated the verb together with the preposition or adverb (e.g., check in). Idioms They were annotated with all their elements (e.g., booze it up). The first topic was annotated by all four annotators as burn-in. Afterwards, annotation disagreements were resolved between all annotators and the next three topics were annotated again by all four annotators to measure agreement. Following Passonneau (2004), we computed an inter-annotator agreement of α = 0.55 (Krippendorff, 2004) on these three topics, indicating moderate agreement among the annotators. Given the complexity of the task, we consider this to be a good score. For example, the average of the CoNLL F1 between any two annotators is 73.58, which is much higher than the system scores reported in the literature. After annotating the four topics, disagreements were resolved again and all the documents in the four topics were corrected to match the consensus. The rest of the corpus was split between the four annotators, and each document"
D12-1045,N06-1025,0,0.0653392,"Missing"
D12-1045,D08-1068,0,0.0185834,"any successful techniques for identifying mention clusters (Ponzetto and Strube, 2006; Haghighi and Klein, 2009; Stoyanov et al., 2009; Haghighi and Klein, 2010; Raghunathan et al., 2010; Rahman and Ng, 2011, inter alia). Most of these techniques focus on matching compatible noun pairs using various syntactic and semantic features, with efforts targeted toward improving features and clustering models. Prior work showed that models that jointly resolve mentions across multiple entities result in better performance than simply resolving mentions in a pairwise fashion (Denis and Baldridge, 2007; Poon and Domingos, 2008; Wick et al., 2008; Lee et al., 2011, inter alia). A natural extension is to perform coreference jointly across both entities and events. Yet there has been little attempt in this direction. We know of only limited work that incorporates event-related information in entity coreference, typically by incorporating the verbs in context as features. For instance, Haghighi and Klein (2010) include the governor of the head of nominal mentions as features in their model. Rahman and Ng (2011) also used event-related information by looking at which semantic role the entity mentions can have and the ve"
D12-1045,W11-1901,0,0.0618863,"998) Mention-based metric which measures the proportion of overlap between predicted and gold clusters for a given mention. CEAF (Luo, 2005) Entity-based metric that, unlike B 3 , enforces a one-to-one alignment between gold and predicted clusters. We employ the entity-based version of CEAF. BLANC (Recasens and Hovy, 2011) Metric based on the Rand index (Rand, 1971) that considers both coreference and non-coreference links to address the imbalance between singleton and coreferent mentions. CoNLL F1 Average of MUC, B 3 , and CEAF-φ4 . This was the official metric in the CoNLL-2011 shared task (Pradhan et al., 2011). We followed the CoNLL-2011 evaluation methodology, that is, we removed all singleton clusters, and apposition/copular relations before scoring. We evaluated the systems on three different settings: only on entity clusters, only on event clusters, and on the complete task, i.e., both entities and events. Note that the gold corpus separates clusters into entity and event clusters (see Table 3), but our 496 system does not make this distinction at runtime. In order to compute the entity-only and event-only scores in Table 4, we implemented the following procedure: (a) when scoring entity cluste"
D12-1045,D10-1048,1,0.937981,"ween both entities and events across documents. The relations annotated are both intra and inter-document, which more accurately models real-world scenarios. • We evaluate our cross-document coreference resolution system on this corpus and show that our joint approach significantly outperforms two strong baselines that resolve entities and events separately. 2 Related Work Entity coreference resolution is a well studied problem with many successful techniques for identifying mention clusters (Ponzetto and Strube, 2006; Haghighi and Klein, 2009; Stoyanov et al., 2009; Haghighi and Klein, 2010; Raghunathan et al., 2010; Rahman and Ng, 2011, inter alia). Most of these techniques focus on matching compatible noun pairs using various syntactic and semantic features, with efforts targeted toward improving features and clustering models. Prior work showed that models that jointly resolve mentions across multiple entities result in better performance than simply resolving mentions in a pairwise fashion (Denis and Baldridge, 2007; Poon and Domingos, 2008; Wick et al., 2008; Lee et al., 2011, inter alia). A natural extension is to perform coreference jointly across both entities and events. Yet there has been littl"
D12-1045,P11-1082,0,0.0281979,"nts across documents. The relations annotated are both intra and inter-document, which more accurately models real-world scenarios. • We evaluate our cross-document coreference resolution system on this corpus and show that our joint approach significantly outperforms two strong baselines that resolve entities and events separately. 2 Related Work Entity coreference resolution is a well studied problem with many successful techniques for identifying mention clusters (Ponzetto and Strube, 2006; Haghighi and Klein, 2009; Stoyanov et al., 2009; Haghighi and Klein, 2010; Raghunathan et al., 2010; Rahman and Ng, 2011, inter alia). Most of these techniques focus on matching compatible noun pairs using various syntactic and semantic features, with efforts targeted toward improving features and clustering models. Prior work showed that models that jointly resolve mentions across multiple entities result in better performance than simply resolving mentions in a pairwise fashion (Denis and Baldridge, 2007; Poon and Domingos, 2008; Wick et al., 2008; Lee et al., 2011, inter alia). A natural extension is to perform coreference jointly across both entities and events. Yet there has been little attempt in this dir"
D12-1045,P09-1074,0,0.0236089,"lease a new corpus with coreference relations between both entities and events across documents. The relations annotated are both intra and inter-document, which more accurately models real-world scenarios. • We evaluate our cross-document coreference resolution system on this corpus and show that our joint approach significantly outperforms two strong baselines that resolve entities and events separately. 2 Related Work Entity coreference resolution is a well studied problem with many successful techniques for identifying mention clusters (Ponzetto and Strube, 2006; Haghighi and Klein, 2009; Stoyanov et al., 2009; Haghighi and Klein, 2010; Raghunathan et al., 2010; Rahman and Ng, 2011, inter alia). Most of these techniques focus on matching compatible noun pairs using various syntactic and semantic features, with efforts targeted toward improving features and clustering models. Prior work showed that models that jointly resolve mentions across multiple entities result in better performance than simply resolving mentions in a pairwise fashion (Denis and Baldridge, 2007; Poon and Domingos, 2008; Wick et al., 2008; Lee et al., 2011, inter alia). A natural extension is to perform coreference jointly acros"
D12-1045,M95-1005,0,0.941575,"70.8 82.6 60.7 66.3 61.3 41.8 31.5 46.3 24.1 33.2 25.5 30.6 32.3 32.9 63.4 65.4 63.9 78.4 68.0 81.1 68.2 66.6 69.2 50.8 52.2 52.6 This paper Entity Event Both 60.7 62.7 61.2 70.6 62.8 75.9 65.2 62.7 67.8 55.5 62.5 53.9 74.9 73.9 79.0 63.7 67.7 64.1 39.3 34.0 45.2 29.5 33.9 30.0 33.7 33.9 35.8 66.9 67.6 67.1 79.6 78.5 82.2 71.5 71.7 72.3 54.2 54.8 55.9 Table 4: Performance of the two baselines and our model. We report scores for entity clusters, event clusters and the complete task using five metrics. 6.2 Evaluation We use five coreference evaluation metrics widely used in the literature: MUC (Vilain et al., 1995) Link-based metric which measures how many predicted and gold clusters need to be merged to cover the gold and predicted clusters, respectively. B3 (Bagga and Baldwin, 1998) Mention-based metric which measures the proportion of overlap between predicted and gold clusters for a given mention. CEAF (Luo, 2005) Entity-based metric that, unlike B 3 , enforces a one-to-one alignment between gold and predicted clusters. We employ the entity-based version of CEAF. BLANC (Recasens and Hovy, 2011) Metric based on the Rand index (Rand, 1971) that considers both coreference and non-coreference links to a"
D12-1045,P88-1014,0,0.616127,"(NPs). Focusing on NPs is a way to restrict the challenging problem of coreference resolution, but misses coreference relations like the one between hanged and his suicide in (1), and between placed and put in (2). 1. (a) One of the key suspected Mafia bosses arrested yesterday has hanged himself. (b) Police said Lo Presti had hanged himself. (c) His suicide appeared to be related to clan feuds. 2. (a) The New Orleans Saints placed Reggie Bush on the injured list on Wednesday. (b) Saints put Bush on I.R. As (1c) shows, NPs can also refer to events, and so corefer with phrases other than NPs (Webber, 1988). By being anchored in spatio-temporal dimensions, events represent the most frequent referent of verbal elements. In addition to time and location, events are characterized by their participants or arguments, which often correspond with discourse entities. This two-way feedback between events and their arguments (or entities) is the core of our approach. Since arguments play a key role in describing an event, knowing that two arguments corefer is useful for finding coreference relations between events, and knowing that two events corefer is useful for finding coreference relations between ent"
D12-1045,P95-1026,0,0.175973,", e.g., High-precision sieves Discourse processing sieve Exact string match sieve Relaxed string match sieve Precise constructs sieve (e.g., appositives) Strict head match sieves Proper head noun match sieve Relaxed head matching sieve Table 1: Deterministic sieves in step 6 of Algorithm 1. one sieve clusters together two entity mentions only when they have the same head word. Note that all these heuristics were designed for within-document coreference. They work well in our context because we apply them in individual document clusters, where the one-sense-per-discourse principle still holds (Yarowsky, 1995). Importantly, these sieves do not address verbal mentions. That is, all verbal mentions are still in singleton clusters after this step. Furthermore, none of these sieves use features that facilitate the joint resolution of nominal and verbal mentions (e.g., features from semantic role frames). All these limitations are addressed next. 3.4 Iterative Entity/Event Resolution In this stage (steps 7 – 9 in Algorithm 1), we construct entity and event clusters using a cautious or “baby steps” approach. We use a single linear regressor (Θ) to model cluster merge operations between both verbal and no"
D12-1045,C98-2122,0,\N,Missing
D15-1200,D14-1110,0,0.589369,"Missing"
D15-1200,S14-2001,0,0.0144059,"12). The path in the parse tree between the two nominals is retrieved, and the embedding is calculated based on recursive models and fed to a softmax classifier. For pure comparison purpose, we only use embeddings as features and do not explore other combination of artificial features. We adopt the same training strategy as for the sentiment task (e.g., Adagrad, minibatches, etc). Standard (50) 0.748 Standard(100) 0.770 Standard(300) 0.798 Greedy (50) 0.760 (+0.12) Global+G (100) 0.782 (+0.12) Sentence Semantic Relatedness We use the Sentences Involving Compositional Knowledge (SICK) dataset (Marelli et al., 2014) consisting of 9927 sentence pairs, split into training(4500)/development(500)/Testing(4927). Each sentence pair is associated with a gold-standard label ranging from 1 to 5, indicating how semantically related are the two sentences, from 1 (the two sentences are unrelated) to 5 (the two are very related). In our setting, the similarity between two sentences is measured based on sentence-level embeddings. Let s1 and s2 denote two sentences and es1 and es2 denote corresponding embeddings. es1 and es2 are achieved through recurrent or recursive models (as illustrated in Appendix section). Again,"
D15-1200,W09-2415,0,0.0878349,"Missing"
D15-1200,P12-1092,0,0.656101,"bank (with senses including ‘sloping land’ and ‘financial institution’) is forced to represent some uneasy central tendency between the various meanings. More fine-grained embeddings that represent more natural regions in semantic space could thus improve language understanding. Early research pointed out that embeddings could model aspects of word sense (Kintsch, 2001) and recent research has proposed a number of models that represent each word type by different senses, each sense associated with a sensespecific embedding (Kintsch, 2001; Reisinger and Mooney, 2010; Neelakantan et al., 2014; Huang et al., 2012; Chen et al., 2014; Pina and Johansson, 2014; Wu and Giles, 2015; Liu et al., 2015). Such sense-specific embeddings have shown improved performance on simple artificial tasks like matching human word similarity judgments— WS353 (Rubenstein and Goodenough, 1965) or MC30 (Huang et al., 2012). Incorporating multisense word embeddings into general NLP tasks requires a pipelined architecture that addresses three major steps: Learning a distinct representation for each sense of an ambiguous word could lead to more powerful and fine-grained models of vector-space representations. Yet while ‘multi-se"
D15-1200,D14-1113,0,0.655089,"for homonymous words like bank (with senses including ‘sloping land’ and ‘financial institution’) is forced to represent some uneasy central tendency between the various meanings. More fine-grained embeddings that represent more natural regions in semantic space could thus improve language understanding. Early research pointed out that embeddings could model aspects of word sense (Kintsch, 2001) and recent research has proposed a number of models that represent each word type by different senses, each sense associated with a sensespecific embedding (Kintsch, 2001; Reisinger and Mooney, 2010; Neelakantan et al., 2014; Huang et al., 2012; Chen et al., 2014; Pina and Johansson, 2014; Wu and Giles, 2015; Liu et al., 2015). Such sense-specific embeddings have shown improved performance on simple artificial tasks like matching human word similarity judgments— WS353 (Rubenstein and Goodenough, 1965) or MC30 (Huang et al., 2012). Incorporating multisense word embeddings into general NLP tasks requires a pipelined architecture that addresses three major steps: Learning a distinct representation for each sense of an ambiguous word could lead to more powerful and fine-grained models of vector-space representations."
D15-1200,W02-1011,0,0.019858,"–21 for validation and sections 22–24 for testing. Similar to NER, we trained 5layer neural models which take the concatenation of neighboring embeddings as inputs. We adopt a similar training and parameter tuning strategy as for POS tagging. Standard (50) 0.925 Standard (100) 0.940 Standard (300) 0.954 Greedy (50) 0.934 (+0.09) Global+G (100) 0.946 (+0.06) Expectation (50) 0.938 (+0.13) Global+E (100) 0.952 (+0.12) Table 4: Accuracy for Different Models on Part of Speech Tagging. P-value 0.033 for 50d and 0.031 for 100d. Sentence-level Sentiment Classification (Pang) The sentiment dataset of Pang et al. (2002) consists of movie reviews with a sentiment label for each sentence. We divide the original dataset into training(8101)/dev(500)/testing(2000). Word embeddings are initialized using the aforementioned types of embeddings and kept fixed in the learning procedure. Sentence level embeddings are achieved by using standard sequence recurrent neural models (Pearlmutter, 1989) (for details, please refer to Appendix section). The obtained embedding is then fed into a sigmoid classifier. Convolutional matrices at the word level are randomized from [-0.1, 0.1] and learned from sequence models. For train"
D15-1200,D14-1162,0,0.0815287,"increase embedding dimensionality. After describing related work, we introduce the new unsupervised sense-learning model in section 3, give our sense-induction algorithm in section 4, and then in following sections evaluate its performance for word similarity, and then various NLP tasks. 2 Related Work Neural embedding learning frameworks represent each token with a dense vector representation, optimized through predicting neighboring words or decomposing co-occurrence matrices (Bengio et al., 2006; Collobert and Weston, 2008; Mnih and Hinton, 2007; Mikolov et al., 2013; Mikolov et al., 2010; Pennington et al., 2014). Standard neural models represent each word with a single unique vector representation. Recent work has begun to augment the neural paradigm to address the multi-sense problem by associating each word with a series of sense specific embeddings. The central idea is to augment standard embedding learning models like skip-grams by disambiguating word senses based on local co-occurrence— e.g., the fruit “apple” tends to co-occur with the words “cider, tree, pear” while the homophonous IT company co-occurs with words like “iphone”, “Google” or “ipod”. For example Reisinger and Mooney (2010) and Hu"
D15-1200,N10-1013,0,0.83385,"(2006)). Thus the embedding for homonymous words like bank (with senses including ‘sloping land’ and ‘financial institution’) is forced to represent some uneasy central tendency between the various meanings. More fine-grained embeddings that represent more natural regions in semantic space could thus improve language understanding. Early research pointed out that embeddings could model aspects of word sense (Kintsch, 2001) and recent research has proposed a number of models that represent each word type by different senses, each sense associated with a sensespecific embedding (Kintsch, 2001; Reisinger and Mooney, 2010; Neelakantan et al., 2014; Huang et al., 2012; Chen et al., 2014; Pina and Johansson, 2014; Wu and Giles, 2015; Liu et al., 2015). Such sense-specific embeddings have shown improved performance on simple artificial tasks like matching human word similarity judgments— WS353 (Rubenstein and Goodenough, 1965) or MC30 (Huang et al., 2012). Incorporating multisense word embeddings into general NLP tasks requires a pipelined architecture that addresses three major steps: Learning a distinct representation for each sense of an ambiguous word could lead to more powerful and fine-grained models of vec"
D15-1200,D12-1110,0,0.026977,"Sentiment Treebank.). P-value 0.250 for 50d and 0.401 for 100d. Semantic Relationship Classification SemEval-2010 Task 8 (Hendrickx et al., 2009) is to find semantic relationships between pairs of nominals, e.g., in “My [apartment]e1 has a pretty large [kitchen]e2 ” classifying the relation between [apartment] and [kitchen] as component-whole. The dataset contains 9 ordered relationships, so the task is formalized as a 19-class classification problem, with directed relations treated as separate labels; see Hendrickx et al. (2009) for details. We follow the recursive implementations defined in Socher et al. (2012). The path in the parse tree between the two nominals is retrieved, and the embedding is calculated based on recursive models and fed to a softmax classifier. For pure comparison purpose, we only use embeddings as features and do not explore other combination of artificial features. We adopt the same training strategy as for the sentiment task (e.g., Adagrad, minibatches, etc). Standard (50) 0.748 Standard(100) 0.770 Standard(300) 0.798 Greedy (50) 0.760 (+0.12) Global+G (100) 0.782 (+0.12) Sentence Semantic Relatedness We use the Sentences Involving Compositional Knowledge (SICK) dataset (Mar"
D15-1200,D13-1170,0,0.0130186,"dels. For training, we adopt AdaGrad with mini-batch. Parameters (i.e., L2 penalty, learning rate and mini batch size) are tuned on the development set. Due to space limitations, we omit details of recurrent models and training. Standard (50) 0.750 Standard (100) 0.768 Standard (300) 0.774 Greedy (50) 0.752(+0.02) Global+G (100) 0.765(-0.03) Expectation (50) 0.750(+0.00) Global+E (100) 0.763(-0.05) Table 5: Accuracy for Different Models on Sentiment Analysis (Pang et al.’s dataset). P-value 0.442 for 50d and 0.375 for 100d. Sentiment Analysis–Stanford Treebank The Stanford Sentiment Treebank (Socher et al., 2013) contains gold-standard labels for each constituent in the parse tree (phrase level), thus allowing us to investigate a sentiment task at a finer granularity than the dataset in Pang et al. (2002) where labels are only found at the top of each sentence, The sentences in the treebank were split into a training(8544)/development(1101)/testing(2210) dataset. Following Socher et al. (2013) we obtained embeddings for tree nodes by using a recursive neural network model, where the embedding for parent node is obtained in a bottom-up fashion based on its children. The embeddings for each parse tree c"
D15-1200,P15-1150,0,0.00866231,"state-of-the-art results. Thus for example, in tagging tasks (e.g., NER, POS), we follow the protocols in (Collobert et al., 2011) using the concatenation of neighboring embeddings as input features rather than treating embeddings as auxiliary features which are fed into a CRF model along with other manually developed features as in Pennington et al. (2014). Or for experiments on sentiment and other tasks where sentence level embeddings are required we only employ standard recurrent or recursive models for sentence embedding rather than models with sophisticated state-of-theart methods (e.g., Tai et al. (2015; Irsoy and Cardie (2014)). Significance testing for comparing models is done via the bootstrap test (Efron and Tibshirani, 1994). Unless otherwise noted, significant testing is performed on one-word-one-vector embedding (50d) versus multi-sense embedding using Expectation inference (50d) and one-vector embedding (100d) versus Expectation (100d). 6.1 The Tasks Named Entity Recognition We use the CoNLL-2003 English benchmark for training, and test on the CoNLL-2003 test data. We follow the protocols in Collobert et al. (2011), using the concatenation of neighboring embeddings as input to a mult"
D15-1278,P14-2009,0,0.0279958,"based on the recursive structure of parse trees, starting from the leaves and proceeding recursively in a bottom-up fashion until the root of the parse tree is reached. For example, for the phrase the food is delicious, following the operation sequence ( (the food) (is delicious) ) rather than the sequential order (((the food) is) delicious). Many recursive models have been proposed (e.g., (Paulus et al., 2014; Irsoy and Cardie, 2014)), and applied to various NLP tasks, among them entailment (Bowman, 2013; Bowman et al., 2014), sentiment analysis (Socher et al., 2013; Irsoy and Cardie, 2013; Dong et al., 2014), question-answering (Iyyer et al., 2014), relation classification (Socher et al., 2012; Hashimoto et al., 2013), and discourse (Li and Hovy, 2014). 2304 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2304–2314, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational Linguistics. One possible advantage of recursive models is their potential for capturing long-distance dependencies: two tokens may be structurally close to each other, even though they are far away in word sequence. For example, a verb and its corresponding direc"
D15-1278,D13-1137,0,0.036062,"bottom-up fashion until the root of the parse tree is reached. For example, for the phrase the food is delicious, following the operation sequence ( (the food) (is delicious) ) rather than the sequential order (((the food) is) delicious). Many recursive models have been proposed (e.g., (Paulus et al., 2014; Irsoy and Cardie, 2014)), and applied to various NLP tasks, among them entailment (Bowman, 2013; Bowman et al., 2014), sentiment analysis (Socher et al., 2013; Irsoy and Cardie, 2013; Dong et al., 2014), question-answering (Iyyer et al., 2014), relation classification (Socher et al., 2012; Hashimoto et al., 2013), and discourse (Li and Hovy, 2014). 2304 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2304–2314, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational Linguistics. One possible advantage of recursive models is their potential for capturing long-distance dependencies: two tokens may be structurally close to each other, even though they are far away in word sequence. For example, a verb and its corresponding direct object can be far away in terms of tokens if many adjectives lies in between, but they are adjacent in the par"
D15-1278,W09-2415,0,0.0297319,"Missing"
D15-1278,D14-1070,0,0.0537703,"Missing"
D15-1278,D13-1176,0,0.00928056,"ly important, and if so for which tasks, or whether other issues are at play. Indeed, the reliance of recursive models on parsing is also a potential disadvantage, given that parsing is relatively slow, domain-dependent, and can be errorful. On the other hand, recent progress in multiple subfields of neural NLP has suggested that recurrent nets may be sufficient to deal with many of the tasks for which recursive models have been proposed. Recurrent models without parse structures have shown good results in sequenceto-sequence generation (Sutskever et al., 2014) for machine translation (e.g., (Kalchbrenner and Blunsom, 2013; 3; Luong et al., 2014)), parsing (Vinyals et al., 2014), and sentiment, where for example recurrent-based paragraph vectors (Le and Mikolov, 2014) outperform recursive models (Socher et al., 2013) on the Stanford sentimentbank dataset. Our goal in this paper is thus to investigate a number of tasks with the goal of understanding for which kinds of problems recurrent models may be sufficient, and for which kinds recursive models offer specific advantages. We investigate four tasks with different properties. • Binary sentiment classification at the sentence level (Pang et al., 2002) and phrase"
D15-1278,D14-1218,1,0.0712307,"parse tree is reached. For example, for the phrase the food is delicious, following the operation sequence ( (the food) (is delicious) ) rather than the sequential order (((the food) is) delicious). Many recursive models have been proposed (e.g., (Paulus et al., 2014; Irsoy and Cardie, 2014)), and applied to various NLP tasks, among them entailment (Bowman, 2013; Bowman et al., 2014), sentiment analysis (Socher et al., 2013; Irsoy and Cardie, 2013; Dong et al., 2014), question-answering (Iyyer et al., 2014), relation classification (Socher et al., 2012; Hashimoto et al., 2013), and discourse (Li and Hovy, 2014). 2304 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2304–2314, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational Linguistics. One possible advantage of recursive models is their potential for capturing long-distance dependencies: two tokens may be structurally close to each other, even though they are far away in word sequence. For example, a verb and its corresponding direct object can be far away in terms of tokens if many adjectives lies in between, but they are adjacent in the parse tree (Irsoy and Cardie, 2013). H"
D15-1278,D14-1220,1,0.925709,"3): comprehensive labels are found for words and phrases where local compositionally (such as from negation, mood, or others cued by phrase-structure) is to be learned. • Sentence-Target Matching on the UMDQA dataset (Iyyer et al., 2014): Learns matches between target and components in the source sentences, which are parse tree nodes for recursive models and different time-steps for recurrent models. • Semantic Relation Classification on the SemEval-2010 task (Hendrickx et al., 2009). Learns long-distance relationships between two words that may be far apart sequentially. • Discourse Parsing (Li et al., 2014; Hernault et al., 2010): Learns sentence-to-sentence relations based on calculated representations. In each case we followed the protocols described in the original papers. We first group the algorithm variants into two groups as follows: • Standard tree models vs standard sequence models vs standard bi-directional sequence models sentences with 215,154 phrases, the reconstructed dataset for recurrent models comprises 215,154 examples. Models are evaluated at both the phrase level (82,600 instances) and the sentence root level (2,210 instances). Tree Sequence P-value Bi-Sequence P-value • LST"
D15-1278,N13-1090,0,0.0122332,"ks (like semantic relation extraction) that require longdistance connection modeling, particularly on very long sequences. We then introduce a method for allowing recurrent models to achieve similar performance: breaking long sentences into clause-like units at punctuation and processing them separately before combining. Our results thus help understand the limitations of both classes of models, and suggest directions for improving recurrent models. 1 Introduction Deep learning based methods learn lowdimensional, real-valued vectors for word tokens, mostly from large-scale data corpus (e.g., (Mikolov et al., 2013; Le and Mikolov, 2014; Collobert et al., 2011)), successfully capturing syntactic and semantic aspects of text. For tasks where the inputs are larger text units (e.g., phrases, sentences or documents), a compositional model is first needed to aggregate tokens into a vector with fixed dimensionality that can be used as a feature for other NLP tasks. Models for achieving this usually fall into two categories: recurrent models and recursive models: Recurrent models (also referred to as sequence models) deal successfully with time-series data (Pearlmutter, 1989; Dorffner, 1996) like speech (Robin"
D15-1278,W02-1011,0,0.0304553,"alchbrenner and Blunsom, 2013; 3; Luong et al., 2014)), parsing (Vinyals et al., 2014), and sentiment, where for example recurrent-based paragraph vectors (Le and Mikolov, 2014) outperform recursive models (Socher et al., 2013) on the Stanford sentimentbank dataset. Our goal in this paper is thus to investigate a number of tasks with the goal of understanding for which kinds of problems recurrent models may be sufficient, and for which kinds recursive models offer specific advantages. We investigate four tasks with different properties. • Binary sentiment classification at the sentence level (Pang et al., 2002) and phrase level (Socher et al., 2013) that focus on understanding the role of recursive models in dealing with semantic compositionally in various scenarios such as different lengths of inputs and whether or not supervision is comprehensive. • Phrase Matching on the UMD-QA dataset (Iyyer et al., 2014) can help see the difference between outputs from intermediate components from different models, i.e., representations for intermediate parse tree nodes and outputs from recurrent models at different time steps. It also helps see whether parsing is useful for finding similarities between questio"
D15-1278,D13-1170,0,0.526172,"uentially, recursive models combine neighbors based on the recursive structure of parse trees, starting from the leaves and proceeding recursively in a bottom-up fashion until the root of the parse tree is reached. For example, for the phrase the food is delicious, following the operation sequence ( (the food) (is delicious) ) rather than the sequential order (((the food) is) delicious). Many recursive models have been proposed (e.g., (Paulus et al., 2014; Irsoy and Cardie, 2014)), and applied to various NLP tasks, among them entailment (Bowman, 2013; Bowman et al., 2014), sentiment analysis (Socher et al., 2013; Irsoy and Cardie, 2013; Dong et al., 2014), question-answering (Iyyer et al., 2014), relation classification (Socher et al., 2012; Hashimoto et al., 2013), and discourse (Li and Hovy, 2014). 2304 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2304–2314, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational Linguistics. One possible advantage of recursive models is their potential for capturing long-distance dependencies: two tokens may be structurally close to each other, even though they are far away in word sequence. For"
D15-1278,D11-1014,0,0.164886,"pacted into one component, and the error propagation is thus given by: error→ second-clause → first-clause → was→plot→the→as→simple. Propagation with clause segmentation consists of only 8 operations. Such a procedure thus tends to attenuate the gradient vanishing problem, potentially yielding better performance. 3.2 Binary Sentiment Classification (Pang) Task Description: The sentiment dataset of Pang et al. (2002) consists of sentences with a sentiment label for each sentence. We divide the original dataset into training(8101)/dev(500)/testing(2000). No pretraining procedure as described in Socher et al. (2011b) is employed. Word embeddings are initialized using skip-grams and kept fixed in the learning procedure. We trained skip-gram embeddings on the Wikipedia+Gigaword dataset using the word2vec package4 . Sentence level embeddings are fed into a sigmoid classifier. Performances for 50 dimensional vectors are given in the table below: Discussion Why don’t parse trees help on this task? One possible explanation is the distance 4 https://code.google.com/p/word2vec/ of the supervision signal from the local compositional structure. The Pang et al. dataset has an average sentence length of 22.5 words,"
D15-1278,D12-1110,0,0.765115,"ing recursively in a bottom-up fashion until the root of the parse tree is reached. For example, for the phrase the food is delicious, following the operation sequence ( (the food) (is delicious) ) rather than the sequential order (((the food) is) delicious). Many recursive models have been proposed (e.g., (Paulus et al., 2014; Irsoy and Cardie, 2014)), and applied to various NLP tasks, among them entailment (Bowman, 2013; Bowman et al., 2014), sentiment analysis (Socher et al., 2013; Irsoy and Cardie, 2013; Dong et al., 2014), question-answering (Iyyer et al., 2014), relation classification (Socher et al., 2012; Hashimoto et al., 2013), and discourse (Li and Hovy, 2014). 2304 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2304–2314, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational Linguistics. One possible advantage of recursive models is their potential for capturing long-distance dependencies: two tokens may be structurally close to each other, even though they are far away in word sequence. For example, a verb and its corresponding direct object can be far away in terms of tokens if many adjectives lies in between, but the"
D15-1278,W01-1605,0,\N,Missing
D15-1278,P15-1150,0,\N,Missing
D15-1278,P15-1002,1,\N,Missing
D15-1278,D14-1162,0,\N,Missing
D16-1005,W13-3520,0,0.0267057,"r each of the three classes, Past, Ongoing and Future. To alleviate overfitting of the CNN model, we applied dropout (Hinton et al., 2012) on the convolution layer and the following pooling layer with a keeping rate of 0.5. Our experiments used the 300-dimension English word2vec embeddings14 trained on 100 billion words of Google News. We trained our own 300dimension Spanish embeddings, running word2vec (Mikolov et al., 2013) over both Spanish Gigaword (Mendon et al., 2011)— tokenized using Stanford CoreNLP SpanishTokenizer (Manning et al., 2014)— and the pre-tokenized Spanish Wikipedia dump (Al-Rfou et al., 2013). The vectors were then tuned during backpropagation for our specific task. 13 Stanford CoreNLP has no support for generating syntactic dependencies for Spanish. 14 docs.google.com/uc?id=0B7XkCwpI5KDYNlNUTTlSS21pQmM. Row 1 2 3 4 5 6 7 8 Method TIPSem TIPSem with transitivity SVM with all features SVM with BOW features only +Tense/Aspect/Time +Governing Word +Future Oriented Lexicon Convolutional Neural Net PA 26/80/39 75/76/75 91/81/86 88/80/84 89/81/85 90/81/85 90/82/86 91/83/87 OG 8/32/13 14/22/17 33/47/39 37/46/41 40/50/44 43/56/48 44/56/49 46/57/51 FU 4/23/7 4/21/7 45/58/51 40/53/45 42/52/"
D16-1005,P98-1013,0,0.625075,"Missing"
D16-1005,bejan-harabagiu-2008-linguistic,0,0.332676,"Missing"
D16-1005,S13-2002,0,0.227721,"at they are much harder to label because tense and aspect are less available than for events realized as finite verbs. Fourth, the EventStatus data set is multilingual: we collected data from both English and Spanish texts, allowing us to compare events representing the same event frame across two languages that are known to differ in their typological properties for describing events (Talmy, 1985). Using the new EventStatus corpus, we investigate two approaches for recognizing the temporal status of events. We create a SVM classifier that incorporates features drawn from prior TempEval work (Bethard, 2013; Chambers et al., 2014; Llorens et al., 2010) as well as a new automatically induced lexicon of 411 English and 348 Spanish “futureoriented” matrix verbs—verbs like “threaten” and “fear” whose complement clause or nominal direct object argument is likely to describe a future event. We show that the SVM outperforms a state-of-theart TempEval system and that the induced lexicon further improves performance for both English and Spanish. We also introduce a Convolutional Neural Network (CNN) to detect the temporal status of events. Our analysis shows that it successfully models semantic compositi"
D16-1005,P07-1073,0,0.0844152,"Missing"
D16-1005,P15-2072,0,0.0288722,"riants. We then randomly selected 2954 and 14915 news stories from the English Gigaword 5th Ed. (Parker et al., 2011) and Spanish Gigaword 3rd Ed. (Mendon et al., 2011) corpora, respectively, that contain at least one civil unrest phrase. Events of a specific type are very sparsely distributed in a large corpus like the Gigaword, so we used keyword matching just as a first pass to identify candidate event mentions. 3 The English keywords are “protest”, “strike”, “march”, “rally”, “riot” and “occupy”. These correspond to the most frequent words in the relevant frame in the Media Frames corpus (Card et al., 2015). Because “march” most commonly refers to the month, we removed the word itself and only kept its other morphological variations. 4 Spanish keywords: “marchar”, “protestar”, “amotinar(se)”, “manifestar(se)”, “huelga”, “manifestaci´on”, “disturbio”, “mot´ın”, “ocupar * la calle”, “tomar * la calle”, “salir * las calles”, “lanzarse a las calles”, “cacerolas vac´ıas”, “cacerolazo”, “cacerolada”. Asterisks could be replaced by up to 4 words. The last three terms are common expressions for protest marches in many countries of Latin America and Spain. 5 46 (out of 3000) and 9 (out of 1500) stories w"
D16-1005,Q14-1022,0,0.0644833,"Missing"
D16-1005,P98-1067,0,0.168207,"Missing"
D16-1005,D14-1181,0,0.00628674,"For the Spanish data, we used Stanford CoreNLP to generate Partof-Speech tags13 and then applied the MaltParser (Nivre et al., 2004) to generate dependencies. 4 Convolutional Neural Network Model Convolutional neural networks (CNNs) have been shown to be effective in modeling natural language semantics (Collobert et al., 2011). We were especially keen to find out whether the convolution operations of CNNs can model the semantic compositionality needed to detect temporal-aspectual status. For our experiments, we trained a simple CNN with one convolution layer followed by one max pooling layer (Kim, 2014; Collobert et al., 2011), The convolution layer has 300 hidden units. In each unit, the same affine transformation is applied to every consecutive 5 words (a filter instance) in the input sequence of words. A different affine transformation is applied to each hidden unit. After each affine transformation, a Rectified Linear Units (ReLU) (Nair and Hinton, 2010) non-linearity is applied. For each hidden unit, the max pooling layer selects the maximum value from the pool of real values generated from each filter instance. After the max pooling layer, a softmax classifier predicts probabilites fo"
D16-1005,S10-1063,0,0.284343,"se tense and aspect are less available than for events realized as finite verbs. Fourth, the EventStatus data set is multilingual: we collected data from both English and Spanish texts, allowing us to compare events representing the same event frame across two languages that are known to differ in their typological properties for describing events (Talmy, 1985). Using the new EventStatus corpus, we investigate two approaches for recognizing the temporal status of events. We create a SVM classifier that incorporates features drawn from prior TempEval work (Bethard, 2013; Chambers et al., 2014; Llorens et al., 2010) as well as a new automatically induced lexicon of 411 English and 348 Spanish “futureoriented” matrix verbs—verbs like “threaten” and “fear” whose complement clause or nominal direct object argument is likely to describe a future event. We show that the SVM outperforms a state-of-theart TempEval system and that the induced lexicon further improves performance for both English and Spanish. We also introduce a Convolutional Neural Network (CNN) to detect the temporal status of events. Our analysis shows that it successfully models semantic compositionality for some challenging temporal contexts"
D16-1005,S15-2134,0,0.0138314,"English and Spanish (Verhagen et al., 2010), and can compute the relation of each event with the Document Creation 50 Time. We applied TIPSem to our test set, mapping the DCT relations to our three event status classes15 . Row 1 of Tables 6 and 7 shows TIPSem results. The columns show results for each category separately, as well as macro-average and microaverage results across the three categories. Each cell shows the Recall/Precision/F-score numbers. Since TIPSem linked relatively few event mentions to the DCT, we next leveraged the transitivity of temporal relations (UzZaman et al., 2012; Llorens et al., 2015), linking an event to a DCT if the temporal relation between another event in the same sentence and the DCT is transferable. For instance, if event A is AFTER its DCT, and event B is AFTER event A, then event B is also AFTER the DCT.16 Row 2 shows the results of TIPSem with temporal transitivity. Even augmented by transitivity, TIPSem fails to detect many Ongoing (OG) and Future (FU) events; most mislabeled OG and FU events were nominal. Confusion matrices (Table 8) show that most of the 15 We used the obvious mappings from TIPSem relations: “BEFORE” to “PA”, “AFTER” to “FU” , and “INCLUDES” ("
D16-1005,P14-5010,0,0.00419871,"the max pooling layer, a softmax classifier predicts probabilites for each of the three classes, Past, Ongoing and Future. To alleviate overfitting of the CNN model, we applied dropout (Hinton et al., 2012) on the convolution layer and the following pooling layer with a keeping rate of 0.5. Our experiments used the 300-dimension English word2vec embeddings14 trained on 100 billion words of Google News. We trained our own 300dimension Spanish embeddings, running word2vec (Mikolov et al., 2013) over both Spanish Gigaword (Mendon et al., 2011)— tokenized using Stanford CoreNLP SpanishTokenizer (Manning et al., 2014)— and the pre-tokenized Spanish Wikipedia dump (Al-Rfou et al., 2013). The vectors were then tuned during backpropagation for our specific task. 13 Stanford CoreNLP has no support for generating syntactic dependencies for Spanish. 14 docs.google.com/uc?id=0B7XkCwpI5KDYNlNUTTlSS21pQmM. Row 1 2 3 4 5 6 7 8 Method TIPSem TIPSem with transitivity SVM with all features SVM with BOW features only +Tense/Aspect/Time +Governing Word +Future Oriented Lexicon Convolutional Neural Net PA 26/80/39 75/76/75 91/81/86 88/80/84 89/81/85 90/81/85 90/82/86 91/83/87 OG 8/32/13 14/22/17 33/47/39 37/46/41 40/50/44"
D16-1005,de-marneffe-etal-2006-generating,0,0.174951,"Missing"
D16-1005,N15-2023,0,0.0645627,"Missing"
D16-1005,W04-2407,0,0.0618242,"wo events but not when relating an event to the Document Creation Time, for which tense, aspect, and time expression features were the most useful (Llorens et al., 2010; Bethard, 2013). 12 We did not imitate this procedure for Spanish because the quality of our generated Spanish dependencies is poor. 49 pairs the governing word of an event mention with the dependency relation in between. We used Stanford CoreNLP (Marneffe et al., 2006) to generate dependencies for the English data. For the Spanish data, we used Stanford CoreNLP to generate Partof-Speech tags13 and then applied the MaltParser (Nivre et al., 2004) to generate dependencies. 4 Convolutional Neural Network Model Convolutional neural networks (CNNs) have been shown to be effective in modeling natural language semantics (Collobert et al., 2011). We were especially keen to find out whether the convolution operations of CNNs can model the semantic compositionality needed to detect temporal-aspectual status. For our experiments, we trained a simple CNN with one convolution layer followed by one max pooling layer (Kim, 2014; Collobert et al., 2011), The convolution layer has 300 hidden units. In each unit, the same affine transformation is appl"
D16-1005,N15-1044,0,0.0295513,"Missing"
D16-1005,S13-2001,0,0.0942687,"GOING ), or may happen in the future ( FUTURE ). We introduce a new task and corpus for studying the temporal/aspectual properties of major events. The EventStatus corpus consists of 4500 English and Spanish news articles about civil unrest events, such as protests, demonstrations, marches, and strikes, in which each event is annotated as PAST, O N -G OING, or F UTURE (sublabeled as P LANNED, A LERT or P OSSIBLE). This task bridges event extraction research and temporal research in the tradition of TIMEBANK (Pustejovsky et al., 2003) and TempEval (Verhagen et al., 2007; Verhagen et al., 2010; UzZaman et al., 2013). Previous corpora have begun this association: TIMEBANK, for example, includes temporal relations linking events with Document Creation Times (DCT). But the EventStatus task and corpus offers several new research directions. First, major societal events are often discussed before they happen, or while they are still happening, because they have the potential to impact a large number of people. News outlets frequently report on impending natural disasters (e.g., hurricanes), anticipated disease outbreaks (e.g., Zika virus), threats of terrorism, and plans or warnings of potential civil unrest"
D16-1005,S07-1014,0,0.155943,"y happened (PAST), is currently happening (ON GOING ), or may happen in the future ( FUTURE ). We introduce a new task and corpus for studying the temporal/aspectual properties of major events. The EventStatus corpus consists of 4500 English and Spanish news articles about civil unrest events, such as protests, demonstrations, marches, and strikes, in which each event is annotated as PAST, O N -G OING, or F UTURE (sublabeled as P LANNED, A LERT or P OSSIBLE). This task bridges event extraction research and temporal research in the tradition of TIMEBANK (Pustejovsky et al., 2003) and TempEval (Verhagen et al., 2007; Verhagen et al., 2010; UzZaman et al., 2013). Previous corpora have begun this association: TIMEBANK, for example, includes temporal relations linking events with Document Creation Times (DCT). But the EventStatus task and corpus offers several new research directions. First, major societal events are often discussed before they happen, or while they are still happening, because they have the potential to impact a large number of people. News outlets frequently report on impending natural disasters (e.g., hurricanes), anticipated disease outbreaks (e.g., Zika virus), threats of terrorism, an"
D16-1005,D11-1040,0,0.0545468,"Missing"
D16-1005,C98-1013,0,\N,Missing
D16-1005,C98-1064,0,\N,Missing
D16-1057,D09-1062,0,0.118731,"patterns may rely on syntactic structures (Hatzivassiloglou and McKeown, 1997; Jijkoun et al., 2010; Rooth et al., 1999; Thelen and Riloff, 2002; Widdows and Dorow, 2002), which can be domain-specific and brittle (e.g., in social media lacking usual grammatical structures). Other models rely on general co-occurrence (Igo and Riloff, 2009; Riloff and Shepherd, 1997; Turney and Littman, 2003). Often corpus-based methods exploit distant-supervision signals (e.g., review scores, emoticons) specific to certain domains (Asghar et al., 2015; Blair-Goldensohn et al., 2008; Bravo-Marquez et al., 2015; Choi and Cardie, 2009; Severyn and Moschitti, 2015; Speriosu et al., 2011; Tang et al., 2014). An effective corpus-based approach that does not require distant-supervision— which we adapt here—is to construct lexical graphs using word co-occurrences and then to perform some form of label propagation over these graphs (Huang et al., 2014; Velikovich et al., 2010). Recent work has also learned transformations of wordvector representations in order to induce sentiment lexicons (Rothe et al., 2016). Fast et al. (2016) combine word vectors with crowdsourcing to produce domain-independent topic lexicons. Dictionary-base"
D16-1057,cook-stevenson-2010-automatically,0,0.031713,"r/TwoX). more similar sentiment in their language compared to communities that are entirely unrelated. 6 Inducing diachronic sentiment lexicons Sentiment also depends on the historical time-period in which a word is used. To investigate this dependency, we use our framework to analyze how word polarities have shifted over the last 150 years. The phenomena of amelioration (words becoming more positive) and pejoration (words becoming more negative) are well-discussed in the linguistic literature (Traugott and Dasher, 2001); however, no comprehensive polarity lexicons exist for historical data (Cook and Stevenson, 2010). Such lexicons are crucial to the growing body of work on NLP analyses of 602 historical text (Piotrowski, 2012) which are informing diachronic linguistics (Hamilton et al., 2016), the digital humanities (Muralidharan and Hearst, 2012), and history (Hendrickx et al., 2011). Our work is inspired by the only previous work on automatically inducing historical sentiment lexicons, Cook and Stevenson (2010); they use the PMI method and a full modern sentiment lexicon as their seed set, which relies on the assumption that all these words have not changed in sentiment. In contrast, in addition to our"
D16-1057,C14-1009,0,0.0192346,"bjective properties of texts like opinions and attitudes (Taboada et al., 2011). But lexical sentiment is hugely influenced by context. The word soft has a very different sentiment in an online sports community than it does in one dedicated to toy animals (Figure 1). Terrific once had a highly negative connotation; now it is essentially synonymous with good (Figure 2). Without domain-specific lexicons, social scientific analyses can be misled by sentiment assignments biased towards domain-general contexts, neglecting factors like genre, community-specific vernacular, or demographic variation (Deng et al., 2014; Hovy, 2015; Yang and Eisenstein, 2015). Using experts or crowdsourcing to construct domain-specific sentiment lexicons is expensive and often time-consuming (Mohammad and Turney, 2010; Fast et al., 2016), and is especially problematic when non-standard language (as in historical documents or obscure social media forums) prevents annotators from understanding the sociolinguistic context of the data. Web-scale sentiment lexicons can be automatically induced for large socially-diffuse domains, such as the internet-at-large (Velikovich et al., 2010) or all of Twitter (Tang et al., 2014). However"
D16-1057,esuli-sebastiani-2006-sentiwordnet,0,0.0933316,"t, delightful, perfect, loved, love, happy successful, excellent, profit, beneficial, improving, improved, success, gains, positive love, loved, loves, awesome, nice, amazing, best, fantastic, correct, happy bad, horrible, poor, unfortunate, unpleasant, disgusting, evil, hated, hate, unhappy negligent, loss, volatile, wrong, losses, damages, bad, litigation, failure, down, negative hate, hated, hates, terrible, nasty, awful, worst, horrible, wrong, sad Twitter Table 1: Seed words. The seed words were manually selected to be context insensitive (without knowledge of the test lexicons). labels (Esuli and Sebastiani, 2006; Hu and Liu, 2004; Kamps et al., 2004; Rao and Ravichandran, 2009; San Vicente et al., 2014; Takamura et al., 2005; Tai and Kao, 2013). There is an implicit consensus that dictionary-based approaches will generate higher-quality lexicons, due to their use of these clean, hand-curated resources; however, they are not applicable in domains lacking such a resource (e.g., most historical texts). Most previous work seeks to enrich or enlarge existing lexicons (Qiu et al., 2009; San Vicente et al., 2014; Velikovich et al., 2010), emphasizing recall over precision. This recall-oriented approach is m"
D16-1057,P16-1141,1,0.808922,"al time-period in which a word is used. To investigate this dependency, we use our framework to analyze how word polarities have shifted over the last 150 years. The phenomena of amelioration (words becoming more positive) and pejoration (words becoming more negative) are well-discussed in the linguistic literature (Traugott and Dasher, 2001); however, no comprehensive polarity lexicons exist for historical data (Cook and Stevenson, 2010). Such lexicons are crucial to the growing body of work on NLP analyses of 602 historical text (Piotrowski, 2012) which are informing diachronic linguistics (Hamilton et al., 2016), the digital humanities (Muralidharan and Hearst, 2012), and history (Hendrickx et al., 2011). Our work is inspired by the only previous work on automatically inducing historical sentiment lexicons, Cook and Stevenson (2010); they use the PMI method and a full modern sentiment lexicon as their seed set, which relies on the assumption that all these words have not changed in sentiment. In contrast, in addition to our different algorithm, we use a small seed set of words that were manually selected based on having strong and stable sentiment over the last 150 years (Table 1; confirmed via histo"
D16-1057,P97-1023,0,0.199073,"om/brandsavant/ the-hidden-bias-of-social-media-sentiment-analysis 2 http://nlp.stanford.edu/projects/socialsent 596 Figure 2: The sentiment of terrific changed from negative to positive over the last 150 years. Sentiment values and bootstrapped confidences were computed using S ENT P ROP on historical data (see Section 6). 2 Related work Our work builds upon a wealth of previous research on inducing sentiment lexicons, along two threads: Corpus-based approaches use seed words and patterns in unlabeled corpora to induce domainspecific lexicons. These patterns may rely on syntactic structures (Hatzivassiloglou and McKeown, 1997; Jijkoun et al., 2010; Rooth et al., 1999; Thelen and Riloff, 2002; Widdows and Dorow, 2002), which can be domain-specific and brittle (e.g., in social media lacking usual grammatical structures). Other models rely on general co-occurrence (Igo and Riloff, 2009; Riloff and Shepherd, 1997; Turney and Littman, 2003). Often corpus-based methods exploit distant-supervision signals (e.g., review scores, emoticons) specific to certain domains (Asghar et al., 2015; Blair-Goldensohn et al., 2008; Bravo-Marquez et al., 2015; Choi and Cardie, 2009; Severyn and Moschitti, 2015; Speriosu et al., 2011; Ta"
D16-1057,P15-1073,0,0.0271537,"of texts like opinions and attitudes (Taboada et al., 2011). But lexical sentiment is hugely influenced by context. The word soft has a very different sentiment in an online sports community than it does in one dedicated to toy animals (Figure 1). Terrific once had a highly negative connotation; now it is essentially synonymous with good (Figure 2). Without domain-specific lexicons, social scientific analyses can be misled by sentiment assignments biased towards domain-general contexts, neglecting factors like genre, community-specific vernacular, or demographic variation (Deng et al., 2014; Hovy, 2015; Yang and Eisenstein, 2015). Using experts or crowdsourcing to construct domain-specific sentiment lexicons is expensive and often time-consuming (Mohammad and Turney, 2010; Fast et al., 2016), and is especially problematic when non-standard language (as in historical documents or obscure social media forums) prevents annotators from understanding the sociolinguistic context of the data. Web-scale sentiment lexicons can be automatically induced for large socially-diffuse domains, such as the internet-at-large (Velikovich et al., 2010) or all of Twitter (Tang et al., 2014). However, to study s"
D16-1057,W09-1703,0,0.0442998,"g S ENT P ROP on historical data (see Section 6). 2 Related work Our work builds upon a wealth of previous research on inducing sentiment lexicons, along two threads: Corpus-based approaches use seed words and patterns in unlabeled corpora to induce domainspecific lexicons. These patterns may rely on syntactic structures (Hatzivassiloglou and McKeown, 1997; Jijkoun et al., 2010; Rooth et al., 1999; Thelen and Riloff, 2002; Widdows and Dorow, 2002), which can be domain-specific and brittle (e.g., in social media lacking usual grammatical structures). Other models rely on general co-occurrence (Igo and Riloff, 2009; Riloff and Shepherd, 1997; Turney and Littman, 2003). Often corpus-based methods exploit distant-supervision signals (e.g., review scores, emoticons) specific to certain domains (Asghar et al., 2015; Blair-Goldensohn et al., 2008; Bravo-Marquez et al., 2015; Choi and Cardie, 2009; Severyn and Moschitti, 2015; Speriosu et al., 2011; Tang et al., 2014). An effective corpus-based approach that does not require distant-supervision— which we adapt here—is to construct lexical graphs using word co-occurrences and then to perform some form of label propagation over these graphs (Huang et al., 2014;"
D16-1057,P10-1060,0,0.146611,"Missing"
D16-1057,kamps-etal-2004-using,0,0.352132,"Missing"
D16-1057,lee-etal-2014-importance,1,0.764746,"polarities. We set B = 50 and used 7 words per random subset (full seed sets are size 10; see Table 1). historical corpus, which is important since we will use the COHA corpus to infer historical sentiment lexicons (Section 6). Finance: Previous work found that general purpose sentiment lexicons performed very poorly on financial text (Loughran and McDonald, 2011), so a finance-specific sentiment lexicon (containing binary labels) was hand-constructed for this domain (ibid.). To test against this lexicon, we constructed embeddings using a dataset of ∼2×107 tokens from financial 8K documents (Lee et al., 2014). Twitter: Numerous works attempt to induce Twitter-specific sentiment lexicons using supervised approaches and features unique to that domain (e.g., follower graphs; Speriosu et al., 2011). Here, we emphasize that we can induce an accurate lexicon using a simple domain-independent and resourcelight approach, with the implication that lexicons can easily be induced for related social media domains without resorting to complex supervised frameworks. We evaluate our approach using the test set from the 2015 SemEval task 10E competition (Rosenthal et al., 2015), and we use the embeddings construc"
D16-1057,Q15-1016,0,0.0807569,"ting sentiment labels over this graph. 3.1 Constructing a lexical graph Lexical graphs are constructed from distributional word embeddings learned on unlabeled corpora. Distributional word embeddings The first step in our approach is to build highquality semantic representations for words using a vector space model (VSM). We embed each word wi ∈ V as a vector wi that captures information about its co-occurrence statistics with other words (Landauer and Dumais, 1997; Turney and Pantel, 2010). This VSM approach has a long history in NLP and has been highly successful in recent applications (see Levy et al., 2015 for a survey). When recreating known lexicons, we used a number of publicly available embeddings (Section 4). In the cases where we learned embeddings ourselves, we employed an SVD-based method to construct the word-vectors. First, we construct a matrix MP P M I ∈ R|V|×|V |with entries given by     pˆ(wi , wj ) P P MI Mi,j = max log , 0 , (1) pˆ(w)ˆ p(wj ) where pˆ denotes smoothed empirical probabilities of word (co-)occurrences within fixed-size sliding windows of text.3 MPi,jP M I is equal to a smoothed variant of the positive pointwise mutual information between words wi and wj (Levy"
D16-1057,W10-0204,0,0.486852,"ent in an online sports community than it does in one dedicated to toy animals (Figure 1). Terrific once had a highly negative connotation; now it is essentially synonymous with good (Figure 2). Without domain-specific lexicons, social scientific analyses can be misled by sentiment assignments biased towards domain-general contexts, neglecting factors like genre, community-specific vernacular, or demographic variation (Deng et al., 2014; Hovy, 2015; Yang and Eisenstein, 2015). Using experts or crowdsourcing to construct domain-specific sentiment lexicons is expensive and often time-consuming (Mohammad and Turney, 2010; Fast et al., 2016), and is especially problematic when non-standard language (as in historical documents or obscure social media forums) prevents annotators from understanding the sociolinguistic context of the data. Web-scale sentiment lexicons can be automatically induced for large socially-diffuse domains, such as the internet-at-large (Velikovich et al., 2010) or all of Twitter (Tang et al., 2014). However, to study sentiment in domain-specific cases—financial documents, historical texts, or tight-knit social me595 Proceedings of the 2016 Conference on Empirical Methods in Natural Langua"
D16-1057,D14-1162,0,0.0788717,"Missing"
D16-1057,E09-1077,0,0.139511,"profit, beneficial, improving, improved, success, gains, positive love, loved, loves, awesome, nice, amazing, best, fantastic, correct, happy bad, horrible, poor, unfortunate, unpleasant, disgusting, evil, hated, hate, unhappy negligent, loss, volatile, wrong, losses, damages, bad, litigation, failure, down, negative hate, hated, hates, terrible, nasty, awful, worst, horrible, wrong, sad Twitter Table 1: Seed words. The seed words were manually selected to be context insensitive (without knowledge of the test lexicons). labels (Esuli and Sebastiani, 2006; Hu and Liu, 2004; Kamps et al., 2004; Rao and Ravichandran, 2009; San Vicente et al., 2014; Takamura et al., 2005; Tai and Kao, 2013). There is an implicit consensus that dictionary-based approaches will generate higher-quality lexicons, due to their use of these clean, hand-curated resources; however, they are not applicable in domains lacking such a resource (e.g., most historical texts). Most previous work seeks to enrich or enlarge existing lexicons (Qiu et al., 2009; San Vicente et al., 2014; Velikovich et al., 2010), emphasizing recall over precision. This recall-oriented approach is motivated by the need for massive polarity lexicons in tasks like w"
D16-1057,W97-0313,0,0.31143,"rical data (see Section 6). 2 Related work Our work builds upon a wealth of previous research on inducing sentiment lexicons, along two threads: Corpus-based approaches use seed words and patterns in unlabeled corpora to induce domainspecific lexicons. These patterns may rely on syntactic structures (Hatzivassiloglou and McKeown, 1997; Jijkoun et al., 2010; Rooth et al., 1999; Thelen and Riloff, 2002; Widdows and Dorow, 2002), which can be domain-specific and brittle (e.g., in social media lacking usual grammatical structures). Other models rely on general co-occurrence (Igo and Riloff, 2009; Riloff and Shepherd, 1997; Turney and Littman, 2003). Often corpus-based methods exploit distant-supervision signals (e.g., review scores, emoticons) specific to certain domains (Asghar et al., 2015; Blair-Goldensohn et al., 2008; Bravo-Marquez et al., 2015; Choi and Cardie, 2009; Severyn and Moschitti, 2015; Speriosu et al., 2011; Tang et al., 2014). An effective corpus-based approach that does not require distant-supervision— which we adapt here—is to construct lexical graphs using word co-occurrences and then to perform some form of label propagation over these graphs (Huang et al., 2014; Velikovich et al., 2010)."
D16-1057,P99-1014,0,0.139709,"analysis 2 http://nlp.stanford.edu/projects/socialsent 596 Figure 2: The sentiment of terrific changed from negative to positive over the last 150 years. Sentiment values and bootstrapped confidences were computed using S ENT P ROP on historical data (see Section 6). 2 Related work Our work builds upon a wealth of previous research on inducing sentiment lexicons, along two threads: Corpus-based approaches use seed words and patterns in unlabeled corpora to induce domainspecific lexicons. These patterns may rely on syntactic structures (Hatzivassiloglou and McKeown, 1997; Jijkoun et al., 2010; Rooth et al., 1999; Thelen and Riloff, 2002; Widdows and Dorow, 2002), which can be domain-specific and brittle (e.g., in social media lacking usual grammatical structures). Other models rely on general co-occurrence (Igo and Riloff, 2009; Riloff and Shepherd, 1997; Turney and Littman, 2003). Often corpus-based methods exploit distant-supervision signals (e.g., review scores, emoticons) specific to certain domains (Asghar et al., 2015; Blair-Goldensohn et al., 2008; Bravo-Marquez et al., 2015; Choi and Cardie, 2009; Severyn and Moschitti, 2015; Speriosu et al., 2011; Tang et al., 2014). An effective corpus-base"
D16-1057,N16-1091,0,0.352861,"emoticons) specific to certain domains (Asghar et al., 2015; Blair-Goldensohn et al., 2008; Bravo-Marquez et al., 2015; Choi and Cardie, 2009; Severyn and Moschitti, 2015; Speriosu et al., 2011; Tang et al., 2014). An effective corpus-based approach that does not require distant-supervision— which we adapt here—is to construct lexical graphs using word co-occurrences and then to perform some form of label propagation over these graphs (Huang et al., 2014; Velikovich et al., 2010). Recent work has also learned transformations of wordvector representations in order to induce sentiment lexicons (Rothe et al., 2016). Fast et al. (2016) combine word vectors with crowdsourcing to produce domain-independent topic lexicons. Dictionary-based approaches use hand-curated lexical resources—usually WordNet (Fellbaum, 1998)—in order to propagate sentiment from seed Domain Positive seed words Negative seed words Standard English Finance good, lovely, excellent, fortunate, pleasant, delightful, perfect, loved, love, happy successful, excellent, profit, beneficial, improving, improved, success, gains, positive love, loved, loves, awesome, nice, amazing, best, fantastic, correct, happy bad, horrible, poor, unfortunate"
D16-1057,E14-1010,0,0.447059,"mproved, success, gains, positive love, loved, loves, awesome, nice, amazing, best, fantastic, correct, happy bad, horrible, poor, unfortunate, unpleasant, disgusting, evil, hated, hate, unhappy negligent, loss, volatile, wrong, losses, damages, bad, litigation, failure, down, negative hate, hated, hates, terrible, nasty, awful, worst, horrible, wrong, sad Twitter Table 1: Seed words. The seed words were manually selected to be context insensitive (without knowledge of the test lexicons). labels (Esuli and Sebastiani, 2006; Hu and Liu, 2004; Kamps et al., 2004; Rao and Ravichandran, 2009; San Vicente et al., 2014; Takamura et al., 2005; Tai and Kao, 2013). There is an implicit consensus that dictionary-based approaches will generate higher-quality lexicons, due to their use of these clean, hand-curated resources; however, they are not applicable in domains lacking such a resource (e.g., most historical texts). Most previous work seeks to enrich or enlarge existing lexicons (Qiu et al., 2009; San Vicente et al., 2014; Velikovich et al., 2010), emphasizing recall over precision. This recall-oriented approach is motivated by the need for massive polarity lexicons in tasks like web-advertising (Velikovich"
D16-1057,N15-1159,0,0.0313517,"ntactic structures (Hatzivassiloglou and McKeown, 1997; Jijkoun et al., 2010; Rooth et al., 1999; Thelen and Riloff, 2002; Widdows and Dorow, 2002), which can be domain-specific and brittle (e.g., in social media lacking usual grammatical structures). Other models rely on general co-occurrence (Igo and Riloff, 2009; Riloff and Shepherd, 1997; Turney and Littman, 2003). Often corpus-based methods exploit distant-supervision signals (e.g., review scores, emoticons) specific to certain domains (Asghar et al., 2015; Blair-Goldensohn et al., 2008; Bravo-Marquez et al., 2015; Choi and Cardie, 2009; Severyn and Moschitti, 2015; Speriosu et al., 2011; Tang et al., 2014). An effective corpus-based approach that does not require distant-supervision— which we adapt here—is to construct lexical graphs using word co-occurrences and then to perform some form of label propagation over these graphs (Huang et al., 2014; Velikovich et al., 2010). Recent work has also learned transformations of wordvector representations in order to induce sentiment lexicons (Rothe et al., 2016). Fast et al. (2016) combine word vectors with crowdsourcing to produce domain-independent topic lexicons. Dictionary-based approaches use hand-curated"
D16-1057,W11-2207,0,0.186686,"iloglou and McKeown, 1997; Jijkoun et al., 2010; Rooth et al., 1999; Thelen and Riloff, 2002; Widdows and Dorow, 2002), which can be domain-specific and brittle (e.g., in social media lacking usual grammatical structures). Other models rely on general co-occurrence (Igo and Riloff, 2009; Riloff and Shepherd, 1997; Turney and Littman, 2003). Often corpus-based methods exploit distant-supervision signals (e.g., review scores, emoticons) specific to certain domains (Asghar et al., 2015; Blair-Goldensohn et al., 2008; Bravo-Marquez et al., 2015; Choi and Cardie, 2009; Severyn and Moschitti, 2015; Speriosu et al., 2011; Tang et al., 2014). An effective corpus-based approach that does not require distant-supervision— which we adapt here—is to construct lexical graphs using word co-occurrences and then to perform some form of label propagation over these graphs (Huang et al., 2014; Velikovich et al., 2010). Recent work has also learned transformations of wordvector representations in order to induce sentiment lexicons (Rothe et al., 2016). Fast et al. (2016) combine word vectors with crowdsourcing to produce domain-independent topic lexicons. Dictionary-based approaches use hand-curated lexical resources—usua"
D16-1057,J11-2001,0,0.255458,"cally between different communities. 1 6 4 2 0 −2 −4 −6 r/spo r ts o ittlep r/myl ny Figure 1: The sentiment of soft in different online communities. Sentiment values computed using S ENT P ROP (Section 3) on comments from Reddit communities illustrate how sentiment depends on social context. Bootstrap-sampled standard deviations provide a measure of confidence with the scores. Introduction Inducing domain-specific sentiment lexicons is crucial to computational social science (CSS) research. Sentiment lexicons allow us to analyze key subjective properties of texts like opinions and attitudes (Taboada et al., 2011). But lexical sentiment is hugely influenced by context. The word soft has a very different sentiment in an online sports community than it does in one dedicated to toy animals (Figure 1). Terrific once had a highly negative connotation; now it is essentially synonymous with good (Figure 2). Without domain-specific lexicons, social scientific analyses can be misled by sentiment assignments biased towards domain-general contexts, neglecting factors like genre, community-specific vernacular, or demographic variation (Deng et al., 2014; Hovy, 2015; Yang and Eisenstein, 2015). Using experts or cro"
D16-1057,P05-1017,0,0.06914,"s, positive love, loved, loves, awesome, nice, amazing, best, fantastic, correct, happy bad, horrible, poor, unfortunate, unpleasant, disgusting, evil, hated, hate, unhappy negligent, loss, volatile, wrong, losses, damages, bad, litigation, failure, down, negative hate, hated, hates, terrible, nasty, awful, worst, horrible, wrong, sad Twitter Table 1: Seed words. The seed words were manually selected to be context insensitive (without knowledge of the test lexicons). labels (Esuli and Sebastiani, 2006; Hu and Liu, 2004; Kamps et al., 2004; Rao and Ravichandran, 2009; San Vicente et al., 2014; Takamura et al., 2005; Tai and Kao, 2013). There is an implicit consensus that dictionary-based approaches will generate higher-quality lexicons, due to their use of these clean, hand-curated resources; however, they are not applicable in domains lacking such a resource (e.g., most historical texts). Most previous work seeks to enrich or enlarge existing lexicons (Qiu et al., 2009; San Vicente et al., 2014; Velikovich et al., 2010), emphasizing recall over precision. This recall-oriented approach is motivated by the need for massive polarity lexicons in tasks like web-advertising (Velikovich et al., 2010). In cont"
D16-1057,C14-1018,0,0.376599,"ariation (Deng et al., 2014; Hovy, 2015; Yang and Eisenstein, 2015). Using experts or crowdsourcing to construct domain-specific sentiment lexicons is expensive and often time-consuming (Mohammad and Turney, 2010; Fast et al., 2016), and is especially problematic when non-standard language (as in historical documents or obscure social media forums) prevents annotators from understanding the sociolinguistic context of the data. Web-scale sentiment lexicons can be automatically induced for large socially-diffuse domains, such as the internet-at-large (Velikovich et al., 2010) or all of Twitter (Tang et al., 2014). However, to study sentiment in domain-specific cases—financial documents, historical texts, or tight-knit social me595 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 595–605, c Austin, Texas, November 1-5, 2016. 2016 Association for Computational Linguistics dia forums—such generic lexicons may be inaccurate, and even introduce harmful biases (Loughran and McDonald, 2011).1 Researchers need a principled and accurate framework for inducing lexicons that are specific to their domain of study. To meet these needs, we introduce S ENT P ROP, a framew"
D16-1057,W02-1028,0,0.102517,"p.stanford.edu/projects/socialsent 596 Figure 2: The sentiment of terrific changed from negative to positive over the last 150 years. Sentiment values and bootstrapped confidences were computed using S ENT P ROP on historical data (see Section 6). 2 Related work Our work builds upon a wealth of previous research on inducing sentiment lexicons, along two threads: Corpus-based approaches use seed words and patterns in unlabeled corpora to induce domainspecific lexicons. These patterns may rely on syntactic structures (Hatzivassiloglou and McKeown, 1997; Jijkoun et al., 2010; Rooth et al., 1999; Thelen and Riloff, 2002; Widdows and Dorow, 2002), which can be domain-specific and brittle (e.g., in social media lacking usual grammatical structures). Other models rely on general co-occurrence (Igo and Riloff, 2009; Riloff and Shepherd, 1997; Turney and Littman, 2003). Often corpus-based methods exploit distant-supervision signals (e.g., review scores, emoticons) specific to certain domains (Asghar et al., 2015; Blair-Goldensohn et al., 2008; Bravo-Marquez et al., 2015; Choi and Cardie, 2009; Severyn and Moschitti, 2015; Speriosu et al., 2011; Tang et al., 2014). An effective corpus-based approach that does not"
D16-1057,N10-1119,0,0.200746,"munity-specific vernacular, or demographic variation (Deng et al., 2014; Hovy, 2015; Yang and Eisenstein, 2015). Using experts or crowdsourcing to construct domain-specific sentiment lexicons is expensive and often time-consuming (Mohammad and Turney, 2010; Fast et al., 2016), and is especially problematic when non-standard language (as in historical documents or obscure social media forums) prevents annotators from understanding the sociolinguistic context of the data. Web-scale sentiment lexicons can be automatically induced for large socially-diffuse domains, such as the internet-at-large (Velikovich et al., 2010) or all of Twitter (Tang et al., 2014). However, to study sentiment in domain-specific cases—financial documents, historical texts, or tight-knit social me595 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 595–605, c Austin, Texas, November 1-5, 2016. 2016 Association for Computational Linguistics dia forums—such generic lexicons may be inaccurate, and even introduce harmful biases (Loughran and McDonald, 2011).1 Researchers need a principled and accurate framework for inducing lexicons that are specific to their domain of study. To meet these nee"
D16-1057,C02-1114,0,0.0505076,"ocialsent 596 Figure 2: The sentiment of terrific changed from negative to positive over the last 150 years. Sentiment values and bootstrapped confidences were computed using S ENT P ROP on historical data (see Section 6). 2 Related work Our work builds upon a wealth of previous research on inducing sentiment lexicons, along two threads: Corpus-based approaches use seed words and patterns in unlabeled corpora to induce domainspecific lexicons. These patterns may rely on syntactic structures (Hatzivassiloglou and McKeown, 1997; Jijkoun et al., 2010; Rooth et al., 1999; Thelen and Riloff, 2002; Widdows and Dorow, 2002), which can be domain-specific and brittle (e.g., in social media lacking usual grammatical structures). Other models rely on general co-occurrence (Igo and Riloff, 2009; Riloff and Shepherd, 1997; Turney and Littman, 2003). Often corpus-based methods exploit distant-supervision signals (e.g., review scores, emoticons) specific to certain domains (Asghar et al., 2015; Blair-Goldensohn et al., 2008; Bravo-Marquez et al., 2015; Choi and Cardie, 2009; Severyn and Moschitti, 2015; Speriosu et al., 2011; Tang et al., 2014). An effective corpus-based approach that does not require distant-supervisio"
D16-1057,S15-2078,0,\N,Missing
D16-1127,D11-1054,1,\N,Missing
D16-1127,W00-0306,0,\N,Missing
D16-1127,P02-1040,0,\N,Missing
D16-1127,P10-1083,0,\N,Missing
D16-1127,P15-1152,0,\N,Missing
D16-1127,P16-1094,1,\N,Missing
D16-1127,D16-1230,0,\N,Missing
D16-1127,P11-1028,0,\N,Missing
D16-1127,P16-1153,1,\N,Missing
D16-1229,P16-1229,0,0.0281699,"(t) similarity vector for wi from these neighbor sets with entries defined as (t) (t) s(t) (j) = cos-sim(wi , wj ) (t) (t+1) ∀wj ∈ Nk (wi ) ∪ Nk (wi ), (2) (t+1) and we compute an analogous vector for wi . (t) The second-order vector, si , contains the cosine similarity of wi and the vectors of all wi ’s nearest semantic neighbors in the the time-periods t and t + 1. Working with variants of these second-order vectors has been a popular approach in many recent works, though most of these works define these vectors against the full vocabulary and not just a word’s nearest neighbors (del Prado Martin and Brendel, 2016; Eger and Mehler, 2016; Rodda et al., 2016). Finally, we compute the local neighborhood change as (t) (t+1) dL (wi , wi (t) (t+1) ) = cos-dist(si , si ). (3) This measures the extent to which wi ’s similarity with its nearest neighbors has changed. The local neighborhood measure defined in (3) captures strong shifts in a word’s paradigmatic relations but is less sensitive to global shifts in syntagmatic contexts (Schutze and Pedersen, 1993). We 2118 Dataset Google English All Google English Fic. German French COHA (Word) COHA (Lemma) # Nouns # Verbs 5299 4941 5443 2310 4077 3389 2722 3128 184"
D16-1229,P16-2009,0,0.0346925,"wi from these neighbor sets with entries defined as (t) (t) s(t) (j) = cos-sim(wi , wj ) (t) (t+1) ∀wj ∈ Nk (wi ) ∪ Nk (wi ), (2) (t+1) and we compute an analogous vector for wi . (t) The second-order vector, si , contains the cosine similarity of wi and the vectors of all wi ’s nearest semantic neighbors in the the time-periods t and t + 1. Working with variants of these second-order vectors has been a popular approach in many recent works, though most of these works define these vectors against the full vocabulary and not just a word’s nearest neighbors (del Prado Martin and Brendel, 2016; Eger and Mehler, 2016; Rodda et al., 2016). Finally, we compute the local neighborhood change as (t) (t+1) dL (wi , wi (t) (t+1) ) = cos-dist(si , si ). (3) This measures the extent to which wi ’s similarity with its nearest neighbors has changed. The local neighborhood measure defined in (3) captures strong shifts in a word’s paradigmatic relations but is less sensitive to global shifts in syntagmatic contexts (Schutze and Pedersen, 1993). We 2118 Dataset Google English All Google English Fic. German French COHA (Word) COHA (Lemma) # Nouns # Verbs 5299 4941 5443 2310 4077 3389 2722 3128 1844 4992 1267 783 Table 1"
D16-1229,W11-2508,0,0.522202,"caused by more regular processes of semantic change (e.g., grammaticalization or subjectification). This distinction is essential for research on linguistic and cultural evolution. Detecting cultural shifts in language use is crucial to computational studies of history and other digital humanities projects. By contrast, for advancing historical linguistics, cultural shifts amount to noise and only the more regular shifts matter. Introduction Distributional methods of embedding words in vector spaces according to their co-occurrence statistics are a promising new tool for diachronic semantics (Gulordava and Baroni, 2011; Jatowt and Duh, 2014; Kulkarni et al., 2014; Xu and Kemp, 2015; Hamilton et al., 2016). Previous work, however, does not consider the underlying causes of semanOur work builds on two intuitions: that distributional models can highlight syntagmatic versus paradigmatic relations with neighboring words (Schutze and Pedersen, 1993) and that nouns are more likely to undergo changes due to irregular cultural shifts while verbs more readily participate in regular processes of semantic change (Gentner and France, 1988; Traugott and Dasher, 2001). We use this noun vs. verb mapping as a proxy to compa"
D16-1229,P16-1141,1,0.821567,"ation). This distinction is essential for research on linguistic and cultural evolution. Detecting cultural shifts in language use is crucial to computational studies of history and other digital humanities projects. By contrast, for advancing historical linguistics, cultural shifts amount to noise and only the more regular shifts matter. Introduction Distributional methods of embedding words in vector spaces according to their co-occurrence statistics are a promising new tool for diachronic semantics (Gulordava and Baroni, 2011; Jatowt and Duh, 2014; Kulkarni et al., 2014; Xu and Kemp, 2015; Hamilton et al., 2016). Previous work, however, does not consider the underlying causes of semanOur work builds on two intuitions: that distributional models can highlight syntagmatic versus paradigmatic relations with neighboring words (Schutze and Pedersen, 1993) and that nouns are more likely to undergo changes due to irregular cultural shifts while verbs more readily participate in regular processes of semantic change (Gentner and France, 1988; Traugott and Dasher, 2001). We use this noun vs. verb mapping as a proxy to compare our two measures’ sensitivities to cultural vs. linguistic shifts. Sensitivity to nom"
D16-1229,W14-2517,0,0.501926,"ican English (COHA), which is smaller than Google N-grams but was carefully constructed to be genre balanced and contains word lemmas as well as surface forms (Davies, 2010). We examined all decades from 1850 through 2000 using the COHA dataset and used the part-of-speech tags provided with the corpora. 2.1 Measuring semantic change We examine two different ways to measure semantic change (Figure 1). Global measure The first measure analyzes global shifts in a word’s vector semantics and is identical to the measure used in most previous works (Gulordava and Baroni, 2011; Jatowt and Duh, 2014; Kim et al., 2014; Hamilton et al., 2016). We simply take a word’s vectors for two consecutive decades and measure the cosine distance between them, i.e. (t) (t+1) dG (wi , wi (t) (t+1) ) = cos-dist(wi , wi ). (1) (Verb - noun) change 0.2 0.1 0.0 −0.1 −0.2 Global measure Local measure −0.3 −0.4 English (All) German English (Fic.) French COHA (word) COHA (lemma) Figure 2: The global measure is more sensitive to semantic changes in verbs while the local neighborhood measure is more sensitive to noun changes. Examining how much nouns change relative to verbs (using coefficients from mixed-model regressions) revea"
D16-1229,P12-3029,0,0.020043,"constructed using the skip-gram with negative sampling (SGNS) algorithm (Mikolov et al., 2013) and post-processed to align the semantic spaces between years. Measuring the distance between word vectors for consecutive decades allows us to compute the rate at which the different words 1 http://nlp.stanford.edu/projects/histwords/. This URL also links to detailed dataset descriptions and the code needed to replicate the experiments in this paper. 2117 change in meaning (Gulordava and Baroni, 2011). We analyzed the decades from 1800 to 1990 using vectors derived from the Google N-gram datasets (Lin et al., 2012) that have large amounts of historical text (English, French, German, and English Fiction). We also used vectors derived from the Corpus of Historical American English (COHA), which is smaller than Google N-grams but was carefully constructed to be genre balanced and contains word lemmas as well as surface forms (Davies, 2010). We examined all decades from 1850 through 2000 using the COHA dataset and used the part-of-speech tags provided with the corpora. 2.1 Measuring semantic change We examine two different ways to measure semantic change (Figure 1). Global measure The first measure analyzes"
D17-1019,P04-1051,0,0.0280782,"Missing"
D17-1019,J08-1001,0,0.935501,"model is reprinted from Li and Hovy (2014), Entity Grid Model from Louis and Nenkova (2012), HMM, HMM+Entity and HMM+Content from Louis and Nenkova (2012), Graph from Guinaudeau and Strube (2013), and the final two lexical models are recomputed using Glove and LDA to replace the original LSA model of Foltz et al. (1998). 4.1 Sentence Ordering, Domain-specific Data Dataset We first evaluate the proposed algorithms on the task of predicting the correct ordering of pairs of sentences predicated on the assumption that an article is always more coherent than a random permutation of its sentences (Barzilay and Lapata, 2008). A detailed description of this commonly used dataset and training/testing are found in the Appendix. We report the performance of the following baselines widely used in the coherence literature. (1) Entity Grid Model: The grid model presented in Barzilay and Lapata (2008). Results are directly taken from Barzilay and Lapata’s (2008) paper. We also consider variations of entity grid models, such as Louis and Nenkova (2012) which models the cluster transition probability and the Graph Based Approach which uses a graph to represent the entity transitions needed for local coherence computation ("
D17-1019,N04-1015,0,0.0306036,"Missing"
D17-1019,J05-3002,0,0.0368956,"entences. We study both discriminative models that learn to distinguish coherent from incoherent discourse, and generative models that produce coherent text, including a novel neural latentvariable Markovian generative model that captures the latent discourse dependencies between sentences in a text. Our work achieves state-of-the-art performance on multiple coherence evaluations, and marks an initial step in generating coherent texts given discourse contexts. 1 Introduction Modeling discourse coherence (the way parts of a text are linked into a coherent whole) is essential for summarization (Barzilay and McKeown, 2005), text planning (Hovy, 1988; Marcu, 1997) question-answering (Verberne et al., 2007), and even psychiatric diagnosis (Elvev˚ag et al., 2007; Bedi et al., 2015). Various frameworks exist, each tackling aspects of coherence. Lexical cohesion (Halliday and Hasan, 1976; Morris and Hirst, 1991) models chains of words and synonyms. Psychological models of discourse (Foltz et al., 1998; Foltz, 2007; McNamara et al., 2010) use LSA embeddings to generalize lexical cohesion. Relational models like RST (Mann and Thompson, 1988; Lascarides and Our generative models are based on augumenting encoder-decoder"
D17-1019,E12-1032,0,0.686483,"Missing"
D17-1019,P13-1010,0,0.170712,"dels obtain the best result on a large open-domain setting, including on the difficult task of reconstructing the order of every sentence in a paragraph, and our latent variable generative model significantly improves the coherence of text generated by the model. 198 Our work marks an initial step in building endto-end systems to evaluate open-domain discourse coherence, and more importantly, generating coherent texts given discourse contexts. 1 Adding coreference (Elsner and Charniak, 2008), named entities (Eisner and Charniak, 2011), discourse relations (Lin et al., 2011) and entity graphs (Guinaudeau and Strube, 2013). Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 198–209 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics 2 The Discriminative Model The discriminative model treats cliques (sets of sentences surrounding a center sentence) taken from the original articles as coherent positive examples and cliques with random replacements of the center sentence as negative examples. The discriminative model can be viewed as an extended version of Li and Hovy’s (2014) model but is practical at large scale2 . We thus make th"
D17-1019,P88-1020,0,0.699611,"s that learn to distinguish coherent from incoherent discourse, and generative models that produce coherent text, including a novel neural latentvariable Markovian generative model that captures the latent discourse dependencies between sentences in a text. Our work achieves state-of-the-art performance on multiple coherence evaluations, and marks an initial step in generating coherent texts given discourse contexts. 1 Introduction Modeling discourse coherence (the way parts of a text are linked into a coherent whole) is essential for summarization (Barzilay and McKeown, 2005), text planning (Hovy, 1988; Marcu, 1997) question-answering (Verberne et al., 2007), and even psychiatric diagnosis (Elvev˚ag et al., 2007; Bedi et al., 2015). Various frameworks exist, each tackling aspects of coherence. Lexical cohesion (Halliday and Hasan, 1976; Morris and Hirst, 1991) models chains of words and synonyms. Psychological models of discourse (Foltz et al., 1998; Foltz, 2007; McNamara et al., 2010) use LSA embeddings to generalize lexical cohesion. Relational models like RST (Mann and Thompson, 1988; Lascarides and Our generative models are based on augumenting encoder-decoder models with latent variabl"
D17-1019,N16-1037,0,0.0494803,"Missing"
D17-1019,P03-1069,0,0.0819021,"0.108 0.101 adver-2 0.120 0.104 0.090 0.078 0.068 adver-3 0.054 0.043 0.039 0.030 0.024 Table 4: Adversarial Success for different models. 4.2.2 Paragraph Reconstruction The accuracy of our models on the binary task of detecting the original sentence ordering is very high, on both the prior small task and our large open-domain version. We therefore believe it is time for the community to move to a more difficult task for measuring coherence. We suggest the task of reconstructing an original paragraph from a bag of constituent sentences, which has been previously used in coherence evaluation (Lapata, 2003). More formally, given a set of permuted sentences s1 , s2 , ..., sN (N the number of sentences in the original document), our goal is return the original (presumably most coherent) ordering of s. Because the discriminative model calculates the coherence of a sentence given the known previous and following sentences, it cannot be applied to this task since we don’t know the surrounding context. Hence, we only use the generative model. The first sentence of a paragraph is given: for each step, we compute the coherence score of placing each remaining candidate sentence to the right of the partia"
D17-1019,J06-4002,0,0.157342,"Missing"
D17-1019,P91-1008,0,0.6263,"Missing"
D17-1019,D14-1218,1,0.901993,"S EQ model, the only difference being that the current token to predict not only depends on the LSTM output ht , but also zn . Given the sampled zn , the KL-divergence can be readily computed, and we update the model using standard gradient decent (details shown in the Appendix). 202 Acci 0.930 0.755 0.770 0.864 0.904 0.822 0.842 0.742 0.846 0.705 0.660 Earthq 0.992 0.930 0.931 0.976 0.872 0.938 0.911 0.953 0.635 0.682 0.667 Aver 0.956 0.842 0.851 0.920 0.888 0.880 0.876 0.847 0.740 0.688 0.664 Table 1: Results from different coherence models. Results for the Recursive model is reprinted from Li and Hovy (2014), Entity Grid Model from Louis and Nenkova (2012), HMM, HMM+Entity and HMM+Content from Louis and Nenkova (2012), Graph from Guinaudeau and Strube (2013), and the final two lexical models are recomputed using Glove and LDA to replace the original LSA model of Foltz et al. (1998). 4.1 Sentence Ordering, Domain-specific Data Dataset We first evaluate the proposed algorithms on the task of predicting the correct ordering of pairs of sentences predicated on the assumption that an article is always more coherent than a random permutation of its sentences (Barzilay and Lapata, 2008). A detailed desc"
D17-1019,P15-1107,1,0.740173,"Missing"
D17-1019,P11-1100,0,0.126817,"Missing"
D17-1019,D12-1106,0,0.0954984,"the current token to predict not only depends on the LSTM output ht , but also zn . Given the sampled zn , the KL-divergence can be readily computed, and we update the model using standard gradient decent (details shown in the Appendix). 202 Acci 0.930 0.755 0.770 0.864 0.904 0.822 0.842 0.742 0.846 0.705 0.660 Earthq 0.992 0.930 0.931 0.976 0.872 0.938 0.911 0.953 0.635 0.682 0.667 Aver 0.956 0.842 0.851 0.920 0.888 0.880 0.876 0.847 0.740 0.688 0.664 Table 1: Results from different coherence models. Results for the Recursive model is reprinted from Li and Hovy (2014), Entity Grid Model from Louis and Nenkova (2012), HMM, HMM+Entity and HMM+Content from Louis and Nenkova (2012), Graph from Guinaudeau and Strube (2013), and the final two lexical models are recomputed using Glove and LDA to replace the original LSA model of Foltz et al. (1998). 4.1 Sentence Ordering, Domain-specific Data Dataset We first evaluate the proposed algorithms on the task of predicting the correct ordering of pairs of sentences predicated on the assumption that an article is always more coherent than a random permutation of its sentences (Barzilay and Lapata, 2008). A detailed description of this commonly used dataset and trainin"
D17-1019,D15-1166,0,0.0247322,"Kiros et al., 2015), which build an encoder-decoder model by predicting tokens in neighboring sentences. As shown in Figure 1a, given two consecutive sentences [si , si+1 ], one can measure the coherence by the likelihood of generating si+1 given its preceding sentence si (denoted by uni). This likelihood is scaled by the number of words in si+1 (denoted by Ni+1 ) to avoid favoring short sequences. L(si , si+1 ) = 1 log p(si+1 |si ) Ni+1 (1) The probability can be directly computed using a pretrained S EQ 2S EQ model (Sutskever et al., 2014) or an attention-based model (Bahdanau et al., 2015; Luong et al., 2015). In a coherent context, a machine should not only be able to guess the next utterance given the preceding ones, but also the preceding one given the following ones. This gives rise to the coherence model (denoted by bi) that measures the bidirectional dependency between the two consecutive sentences: L(si , si+1 ) = 1 log pB (si |si+1 ) Ni 1 + log pF (si+1 |si ) Ni+1 (2) We separately train two models: a forward model pF (si+1 |si ) that predicts the next sentence based on the previous one and a backward model pB (si |si+1 ) that predicts the previous sentence given the next sentence. pB (si"
D17-1019,J91-1002,0,0.597882,"a text. Our work achieves state-of-the-art performance on multiple coherence evaluations, and marks an initial step in generating coherent texts given discourse contexts. 1 Introduction Modeling discourse coherence (the way parts of a text are linked into a coherent whole) is essential for summarization (Barzilay and McKeown, 2005), text planning (Hovy, 1988; Marcu, 1997) question-answering (Verberne et al., 2007), and even psychiatric diagnosis (Elvev˚ag et al., 2007; Bedi et al., 2015). Various frameworks exist, each tackling aspects of coherence. Lexical cohesion (Halliday and Hasan, 1976; Morris and Hirst, 1991) models chains of words and synonyms. Psychological models of discourse (Foltz et al., 1998; Foltz, 2007; McNamara et al., 2010) use LSA embeddings to generalize lexical cohesion. Relational models like RST (Mann and Thompson, 1988; Lascarides and Our generative models are based on augumenting encoder-decoder models with latent variables to model discourse relationships across sentences, including (1) a model that incorporates an HMMLDA topic model into the generative model and (2) an end-to-end model that introduces a Markovstructured neural latent variable, inspired by recent work on trainin"
D17-1019,P11-1153,0,0.0213961,"proposed generative models for discourse coherence modeling. global meaning that it should convey at each wordgeneration step, a global meaning which can capture the state of the discourse across the sentences of a text. We propose two models of this global meaning, a pipelined approach based on HMMbased topic models (Blei et al., 2003; Gruber et al., 2007), and an end-to-end generative model with variational latent variables. 3.2 HMM-LDA based Generative Models (HMM-LDA-GM) In Markov topic models the topic depends on the previous topics in context (Ritter et al., 2010; Paul and Girju, 2010; Wang et al., 2011; Gruber et al., 2007; Paul, 2012). The topic for the current sentence is drawn based on the topic of the preceding sentence (or word) rather than on the global document-level topic distribution in vanilla LDA. Our first model is a pipelined one (the HMMLDA-GM in Fig. 1b), in which an HMM-LDA model provides the S EQ 2S EQ model with global information for token generation, with two components: (1) Running HMM-LDA: we first run a sentence-level HMM-LDA similar to Gruber et al. (2007). Our implementation forces all words in a sentence to be generated from the same topic, and this topic is sample"
D17-1019,D12-1009,0,0.0165449,"e coherence modeling. global meaning that it should convey at each wordgeneration step, a global meaning which can capture the state of the discourse across the sentences of a text. We propose two models of this global meaning, a pipelined approach based on HMMbased topic models (Blei et al., 2003; Gruber et al., 2007), and an end-to-end generative model with variational latent variables. 3.2 HMM-LDA based Generative Models (HMM-LDA-GM) In Markov topic models the topic depends on the previous topics in context (Ritter et al., 2010; Paul and Girju, 2010; Wang et al., 2011; Gruber et al., 2007; Paul, 2012). The topic for the current sentence is drawn based on the topic of the preceding sentence (or word) rather than on the global document-level topic distribution in vanilla LDA. Our first model is a pipelined one (the HMMLDA-GM in Fig. 1b), in which an HMM-LDA model provides the S EQ 2S EQ model with global information for token generation, with two components: (1) Running HMM-LDA: we first run a sentence-level HMM-LDA similar to Gruber et al. (2007). Our implementation forces all words in a sentence to be generated from the same topic, and this topic is sampled from a distribution based on the"
D17-1019,D14-1162,0,0.0976568,"Missing"
D17-1019,N10-1020,0,0.0213572,"decoder about the Figure 1: Overview of the proposed generative models for discourse coherence modeling. global meaning that it should convey at each wordgeneration step, a global meaning which can capture the state of the discourse across the sentences of a text. We propose two models of this global meaning, a pipelined approach based on HMMbased topic models (Blei et al., 2003; Gruber et al., 2007), and an end-to-end generative model with variational latent variables. 3.2 HMM-LDA based Generative Models (HMM-LDA-GM) In Markov topic models the topic depends on the previous topics in context (Ritter et al., 2010; Paul and Girju, 2010; Wang et al., 2011; Gruber et al., 2007; Paul, 2012). The topic for the current sentence is drawn based on the topic of the preceding sentence (or word) rather than on the global document-level topic distribution in vanilla LDA. Our first model is a pipelined one (the HMMLDA-GM in Fig. 1b), in which an HMM-LDA model provides the S EQ 2S EQ model with global information for token generation, with two components: (1) Running HMM-LDA: we first run a sentence-level HMM-LDA similar to Gruber et al. (2007). Our implementation forces all words in a sentence to be generated from"
D17-1019,D11-1116,0,0.0523017,"Missing"
D17-1019,P11-2022,0,\N,Missing
D17-1019,P08-2011,0,\N,Missing
D17-1230,N16-1014,1,0.34709,"Jurafsky 1 1 Stanford University, Stanford, CA, USA 2 New York University, NY, USA 3 Ohio State University, OH, USA jiweil,wmonroe4,tianlins,jurafsky@stanford.edu sebastien@cs.nyu.edu ritter.1492@osu.edu Abstract 2015; Vinyals and Le, 2015; Li et al., 2016a; Yao et al., 2015; Luan et al., 2016) approximate such a goal by predicting the next dialogue utterance given the dialogue history using the maximum likelihood estimation (MLE) objective. Despite its success, this over-simplified training objective leads to problems: responses are dull, generic (Sordoni et al., 2015; Serban et al., 2016a; Li et al., 2016a), repetitive, and short-sighted (Li et al., 2016d). In this paper, drawing intuition from the Turing test, we propose using adversarial training for open-domain dialogue generation: the system is trained to produce sequences that are indistinguishable from human-generated dialogue utterances. We cast the task as a reinforcement learning (RL) problem where we jointly train two systems, a generative model to produce response sequences, and a discriminator—analagous to the human evaluator in the Turing test— to distinguish between the human-generated dialogues and the machine-generated ones. Th"
D17-1230,P16-1094,1,0.386843,"Jurafsky 1 1 Stanford University, Stanford, CA, USA 2 New York University, NY, USA 3 Ohio State University, OH, USA jiweil,wmonroe4,tianlins,jurafsky@stanford.edu sebastien@cs.nyu.edu ritter.1492@osu.edu Abstract 2015; Vinyals and Le, 2015; Li et al., 2016a; Yao et al., 2015; Luan et al., 2016) approximate such a goal by predicting the next dialogue utterance given the dialogue history using the maximum likelihood estimation (MLE) objective. Despite its success, this over-simplified training objective leads to problems: responses are dull, generic (Sordoni et al., 2015; Serban et al., 2016a; Li et al., 2016a), repetitive, and short-sighted (Li et al., 2016d). In this paper, drawing intuition from the Turing test, we propose using adversarial training for open-domain dialogue generation: the system is trained to produce sequences that are indistinguishable from human-generated dialogue utterances. We cast the task as a reinforcement learning (RL) problem where we jointly train two systems, a generative model to produce response sequences, and a discriminator—analagous to the human evaluator in the Turing test— to distinguish between the human-generated dialogues and the machine-generated ones. Th"
D17-1230,K16-1002,0,0.642309,"mpirical Methods in Natural Language Processing, pages 2157–2169 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics human-generated one. The output from the discriminator is used as a reward to the generator, pushing it to generate utterances indistinguishable from human-generated dialogues. The idea of a Turing test—employing an evaluator to distinguish machine-generated texts from human-generated ones—can be applied not only to training but also testing, where it goes by the name of adversarial evaluation. Adversarial evaluation was first employed in Bowman et al. (2016) to evaluate sentence generation quality, and preliminarily studied for dialogue generation by Kannan and Vinyals (2016). In this paper, we discuss potential pitfalls of adversarial evaluations and necessary steps to avoid them and make evaluation reliable. Experimental results demonstrate that our approach produces more interactive, interesting, and non-repetitive responses than standard S EQ 2S EQ models trained using the MLE objective function. 2 Related Work Dialogue generation Response generation for dialogue can be viewed as a source-to-target transduction problem. Ritter et al. (2011) f"
D17-1230,D16-1137,0,0.00721876,"sequence generation, Chen et al. (2016b) apply the idea of adversarial training to sentiment analysis and Zhang et al. (2017) apply the idea to domain adaptation tasks. Our work is distantly related to recent work that formalizes sequence generation as an action-taking problem in reinforcement learning. Ranzato et al. (2016) train RNN decoders in a S EQ 2S EQ model using policy gradient to obtain competitive machine translation results. Bahdanau et al. (2017) take this a step further by training an actor-critic RL model for machine translation. Also related is recent work (Shen et al., 2016; Wiseman and Rush, 2016) to address the issues of exposure bias and loss-evaluation mismatch in neural translation. 3 Adversarial Training for Dialogue Generation In this section, we describe in detail the components of the proposed adversarial reinforcement 2158 learning model. The problem can be framed as follows: given a dialogue history x consisting of a sequence of dialogue utterances,1 the model needs to generate a response y = {y1 , y2 , ..., yT }. We view the process of sentence generation as a sequence of actions that are taken according to a policy defined by an encoder-decoder recurrent neural network. 3.1"
D17-1230,P15-1152,0,0.321537,"Missing"
D18-1008,D14-1159,1,0.817362,"and discourse parsing on the other. The former produces predicate-argument representations of individual facts in a text (Baker et al., 1998; Gildea and Jurafsky, 2002; Palmer et al., 2005); the latter identifies discourse relations between syntactic clauses (Taboada and Mann, 2006; Prasad et al., 2007; Pitler et al., 2009; Prasad et al., 2010; Surdeanu et al., 2015). TAP first maps from syntax to a set of SRL-style representations, and then identifies structurallyconstrained, higher-order relations among them. It is in this sense reminiscent of, but distinct from, work on causal processes by Berant et al. (2014). Related Work Analogy. In the cognitive science literature, analogy is a general form of relational reasoning unique to human cognition (Tversky and Gati, 1978; Holyoak and Thagard, 1996; Goldstone and Son, 2005; Penn et al., 2008; Holyoak, 2012). Our model of textual analogy is particularly influenced by Structure Mapping Theory (Falkenhainer et al., 1989; Gentner and Markman, 1997), an influential cognitive model of analogy as a structurepreserving map between concepts. Within the NLP community, there has been Numbers in NLP. There has been some work on understanding numbers in text. This i"
D18-1008,P16-1055,1,0.768267,"and Son, 2005; Penn et al., 2008; Holyoak, 2012). Our model of textual analogy is particularly influenced by Structure Mapping Theory (Falkenhainer et al., 1989; Gentner and Markman, 1997), an influential cognitive model of analogy as a structurepreserving map between concepts. Within the NLP community, there has been Numbers in NLP. There has been some work on understanding numbers in text. This includes quantitative reasoning (Kushman et al., 2014; Roy et al., 2015), numerical information extraction (Madaan et al., 2016), and techniques for making numbers more easily interpretable in text (Chaganty and Liang, 2016; Kim et al., 2016). If pursued further, the application of plotting 89 References quantitative text that we discuss in this paper could help to clarify quantitative text on the web (Larkin and Simon, 1987; Barrio et al., 2016). Collin F Baker, Charles J Fillmore, and John B Lowe. 1998. The Berkeley FrameNet Project. In Proceedings of the 17th International Conference on Computational Linguistics-Volume 1, pages 86–90. Neural modeling. Recent work has shown the promise of sophisticated neural models on semantic role labeling (He et al., 2017). Similar to other such sequence prediction models,"
D18-1008,D12-1062,0,0.0197056,"he most likely role for each span and edge and then discarding any edges and spans that violate the wellformedness (1) and typing constraints (2). We then enforce transitivity constraints (4) by incrementally building a cluster of analogous and equivalent spans. We then resolve the unique facts constraint (3) by keeping only the span with highest FACT edge score. Finally, for every cluster of analogous VALUE spans, we check that the analogy constraint (5) holds and if not, discard the cluster. We also implement an optimal decoder that encodes the TAP constraints as an ILP (Roth and Yih, 2004; Do et al., 2012). The ILP tries to find an optimal decoding according to the model, subject to hard constraints imposed on the solution space. For example, we require that solutions satisfy the ‘connected spans’ constraint: which defines a joint distribution over per-token role labels. We thus obtain spans from this distribution corresponding to vertices of the graph described in Section 4 by merging contiguous rolelabels in the maximum likelihood label sequence predicted by the CRF. Edge prediction with PATH M AX features. For edge prediction, we use the spans identified above to construct span and edge embe"
D18-1008,J02-3001,0,0.03861,"system. Note that with the imposition of global constraints reflecting the structure of analogy, the system yields well-formed charts. Without these constraints, generated charts either have multiple yaxis values assigned to the same x-axis value, or have floating y-axis values with no grounding on the x-axis. 8 Discourse and Information Extraction. TAP is an information extraction task that synthesizes ideas from semantic role labeling on the one hand and discourse parsing on the other. The former produces predicate-argument representations of individual facts in a text (Baker et al., 1998; Gildea and Jurafsky, 2002; Palmer et al., 2005); the latter identifies discourse relations between syntactic clauses (Taboada and Mann, 2006; Prasad et al., 2007; Pitler et al., 2009; Prasad et al., 2010; Surdeanu et al., 2015). TAP first maps from syntax to a set of SRL-style representations, and then identifies structurallyconstrained, higher-order relations among them. It is in this sense reminiscent of, but distinct from, work on causal processes by Berant et al. (2014). Related Work Analogy. In the cognitive science literature, analogy is a general form of relational reasoning unique to human cognition (Tversky a"
D18-1008,P17-1044,0,0.0145695,"numbers more easily interpretable in text (Chaganty and Liang, 2016; Kim et al., 2016). If pursued further, the application of plotting 89 References quantitative text that we discuss in this paper could help to clarify quantitative text on the web (Larkin and Simon, 1987; Barrio et al., 2016). Collin F Baker, Charles J Fillmore, and John B Lowe. 1998. The Berkeley FrameNet Project. In Proceedings of the 17th International Conference on Computational Linguistics-Volume 1, pages 86–90. Neural modeling. Recent work has shown the promise of sophisticated neural models on semantic role labeling (He et al., 2017). Similar to other such sequence prediction models, e.g., those for named entity recognition (Lample et al., 2016) or semantic role labeling (Zhou and Xu, 2015), our span prediction utilizes a neural CRF. Our model also has an edge-prediction component, which benefits from a simplified version of the PathLSTM model of Roth and Lapata (2016). Our edge-prediction model also uses an embedding concatenation component, which was inspired by recent work on neural coreference resolution (Lee et al., 2017). He et al. (2017) also impose semantic constraints during prediction, but use A∗ search instead"
D18-1008,P14-5010,1,0.00906289,"raphs as defined in Section 4. Given a sentence, the neural model predicts a distribution over role-labeled spans with edges denoting semantic relations between them. Then, we use an ILP to decode while enforcing the TAP constraints defined in Section 4. Figure 4 presents an overview of the architecture. Context-sensitive word embeddings. We first encode the words in a sentence by embedding each token using fixed word embeddings. We also concatenate a few linguistic features to the word embeddings, such as named entity tags and dependency relations. These features are generated using CoreNLP (Manning et al., 2014) and represented by randomly-initialized, learned embeddings for symbols together with the fixed word embedding of each token’s dependency head and the dependency path length between adjacent tokens. The token embeddings are then passed through several stacked convolutional layers (Kim, 2014). While the first convolutional layer can only capture local information, subsequent layers allow for longer-distance reasoning. def l ∈ LR = {FACT, EQUIVALENCE, ANALOGY}. For G so defined to encode a set of valid TAP frames, it must satisfy certain constraints: 1. Well-formedness constraints. For any two"
D18-1008,N13-1090,0,0.0224717,"Missing"
D18-1008,D14-1181,0,0.00241209,". Context-sensitive word embeddings. We first encode the words in a sentence by embedding each token using fixed word embeddings. We also concatenate a few linguistic features to the word embeddings, such as named entity tags and dependency relations. These features are generated using CoreNLP (Manning et al., 2014) and represented by randomly-initialized, learned embeddings for symbols together with the fixed word embedding of each token’s dependency head and the dependency path length between adjacent tokens. The token embeddings are then passed through several stacked convolutional layers (Kim, 2014). While the first convolutional layer can only capture local information, subsequent layers allow for longer-distance reasoning. def l ∈ LR = {FACT, EQUIVALENCE, ANALOGY}. For G so defined to encode a set of valid TAP frames, it must satisfy certain constraints: 1. Well-formedness constraints. For any two vertices v, v 0 ∈ V , their associated spans must not overlap. Furthermore, every vertex must participate in at least one FACT edge, i.e., no disconnected vertices. 2. Typing constraints. FACT relations are always drawn from a VALUE vertex to a nonVALUE vertex. ANALOGY and EQUIVA LENCE are on"
D18-1008,P14-1026,0,0.0350441,"ognitive science literature, analogy is a general form of relational reasoning unique to human cognition (Tversky and Gati, 1978; Holyoak and Thagard, 1996; Goldstone and Son, 2005; Penn et al., 2008; Holyoak, 2012). Our model of textual analogy is particularly influenced by Structure Mapping Theory (Falkenhainer et al., 1989; Gentner and Markman, 1997), an influential cognitive model of analogy as a structurepreserving map between concepts. Within the NLP community, there has been Numbers in NLP. There has been some work on understanding numbers in text. This includes quantitative reasoning (Kushman et al., 2014; Roy et al., 2015), numerical information extraction (Madaan et al., 2016), and techniques for making numbers more easily interpretable in text (Chaganty and Liang, 2016; Kim et al., 2016). If pursued further, the application of plotting 89 References quantitative text that we discuss in this paper could help to clarify quantitative text on the web (Larkin and Simon, 1987; Barrio et al., 2016). Collin F Baker, Charles J Fillmore, and John B Lowe. 1998. The Berkeley FrameNet Project. In Proceedings of the 17th International Conference on Computational Linguistics-Volume 1, pages 86–90. Neural"
D18-1008,J05-1004,0,0.011188,"imposition of global constraints reflecting the structure of analogy, the system yields well-formed charts. Without these constraints, generated charts either have multiple yaxis values assigned to the same x-axis value, or have floating y-axis values with no grounding on the x-axis. 8 Discourse and Information Extraction. TAP is an information extraction task that synthesizes ideas from semantic role labeling on the one hand and discourse parsing on the other. The former produces predicate-argument representations of individual facts in a text (Baker et al., 1998; Gildea and Jurafsky, 2002; Palmer et al., 2005); the latter identifies discourse relations between syntactic clauses (Taboada and Mann, 2006; Prasad et al., 2007; Pitler et al., 2009; Prasad et al., 2010; Surdeanu et al., 2015). TAP first maps from syntax to a set of SRL-style representations, and then identifies structurallyconstrained, higher-order relations among them. It is in this sense reminiscent of, but distinct from, work on causal processes by Berant et al. (2014). Related Work Analogy. In the cognitive science literature, analogy is a general form of relational reasoning unique to human cognition (Tversky and Gati, 1978; Holyoak"
D18-1008,D14-1162,1,0.105743,"rning a sentence embedding or hidden layers, the log-linear model simply uses a CRF to predict span labels directly from fixed input features, and then uses a single sigmoid layer to predict edge labels from deterministic edge embeddings, emn . For the neural models, we used three convolutional layers for sentence embedding with a filter size of 3. Every layer other than the input layer used a hidden dimension of 50 with ReLU nonlinearities. We introduced a single dropout layer (p = 0.5) between every two layers in the network (including at the input). We used 50-dimensional GloVe embeddings (Pennington et al., 2014) learned from Wikipedia 2014 and Gigaword 5 as pre-trained word embeddings, and initialized the embeddings for the features randomly. We chose relatively low input- and hidden-vector dimension because of the size of our data. The network was trained for 15 epochs using ADADELTA (Zeiler, 2012) with a learning rate of 1.0. All models were implemented in PyTorch (Paszke et al., 2017). 7 Model Model Log-linear (all feats.) Neural (no feats.) Neural (all feats.) w/o NER w/o dep. w/o CRF P R F1 42.8 41.7 41.5 41.6 41.2 36.1 82.3 79.1 79.2 79.1 77.5 73.1 56.3 54.6 54.4 54.5 53.8 48.3 Table 4: Perform"
D18-1008,N16-1030,0,0.0241819,"Missing"
D18-1008,P09-1077,0,0.0142039,"generated charts either have multiple yaxis values assigned to the same x-axis value, or have floating y-axis values with no grounding on the x-axis. 8 Discourse and Information Extraction. TAP is an information extraction task that synthesizes ideas from semantic role labeling on the one hand and discourse parsing on the other. The former produces predicate-argument representations of individual facts in a text (Baker et al., 1998; Gildea and Jurafsky, 2002; Palmer et al., 2005); the latter identifies discourse relations between syntactic clauses (Taboada and Mann, 2006; Prasad et al., 2007; Pitler et al., 2009; Prasad et al., 2010; Surdeanu et al., 2015). TAP first maps from syntax to a set of SRL-style representations, and then identifies structurallyconstrained, higher-order relations among them. It is in this sense reminiscent of, but distinct from, work on causal processes by Berant et al. (2014). Related Work Analogy. In the cognitive science literature, analogy is a general form of relational reasoning unique to human cognition (Tversky and Gati, 1978; Holyoak and Thagard, 1996; Goldstone and Son, 2005; Penn et al., 2008; Holyoak, 2012). Our model of textual analogy is particularly influenced"
D18-1008,D17-1018,0,0.0241294,"Missing"
D18-1008,prasad-etal-2010-exploiting,0,0.0244782,"er have multiple yaxis values assigned to the same x-axis value, or have floating y-axis values with no grounding on the x-axis. 8 Discourse and Information Extraction. TAP is an information extraction task that synthesizes ideas from semantic role labeling on the one hand and discourse parsing on the other. The former produces predicate-argument representations of individual facts in a text (Baker et al., 1998; Gildea and Jurafsky, 2002; Palmer et al., 2005); the latter identifies discourse relations between syntactic clauses (Taboada and Mann, 2006; Prasad et al., 2007; Pitler et al., 2009; Prasad et al., 2010; Surdeanu et al., 2015). TAP first maps from syntax to a set of SRL-style representations, and then identifies structurallyconstrained, higher-order relations among them. It is in this sense reminiscent of, but distinct from, work on causal processes by Berant et al. (2014). Related Work Analogy. In the cognitive science literature, analogy is a general form of relational reasoning unique to human cognition (Tversky and Gati, 1978; Holyoak and Thagard, 1996; Goldstone and Son, 2005; Penn et al., 2008; Holyoak, 2012). Our model of textual analogy is particularly influenced by Structure Mapping"
D18-1008,W04-2401,0,0.0606951,"e begin by picking the most likely role for each span and edge and then discarding any edges and spans that violate the wellformedness (1) and typing constraints (2). We then enforce transitivity constraints (4) by incrementally building a cluster of analogous and equivalent spans. We then resolve the unique facts constraint (3) by keeping only the span with highest FACT edge score. Finally, for every cluster of analogous VALUE spans, we check that the analogy constraint (5) holds and if not, discard the cluster. We also implement an optimal decoder that encodes the TAP constraints as an ILP (Roth and Yih, 2004; Do et al., 2012). The ILP tries to find an optimal decoding according to the model, subject to hard constraints imposed on the solution space. For example, we require that solutions satisfy the ‘connected spans’ constraint: which defines a joint distribution over per-token role labels. We thus obtain spans from this distribution corresponding to vertices of the graph described in Section 4 by merging contiguous rolelabels in the maximum likelihood label sequence predicted by the CRF. Edge prediction with PATH M AX features. For edge prediction, we use the spans identified above to construct"
D18-1008,P16-1113,0,0.0219536,"Missing"
D18-1008,Q15-1001,0,0.0128036,"ature, analogy is a general form of relational reasoning unique to human cognition (Tversky and Gati, 1978; Holyoak and Thagard, 1996; Goldstone and Son, 2005; Penn et al., 2008; Holyoak, 2012). Our model of textual analogy is particularly influenced by Structure Mapping Theory (Falkenhainer et al., 1989; Gentner and Markman, 1997), an influential cognitive model of analogy as a structurepreserving map between concepts. Within the NLP community, there has been Numbers in NLP. There has been some work on understanding numbers in text. This includes quantitative reasoning (Kushman et al., 2014; Roy et al., 2015), numerical information extraction (Madaan et al., 2016), and techniques for making numbers more easily interpretable in text (Chaganty and Liang, 2016; Kim et al., 2016). If pursued further, the application of plotting 89 References quantitative text that we discuss in this paper could help to clarify quantitative text on the web (Larkin and Simon, 1987; Barrio et al., 2016). Collin F Baker, Charles J Fillmore, and John B Lowe. 1998. The Berkeley FrameNet Project. In Proceedings of the 17th International Conference on Computational Linguistics-Volume 1, pages 86–90. Neural modeling. Recent wo"
D18-1008,N15-3001,0,0.0187186,"s values assigned to the same x-axis value, or have floating y-axis values with no grounding on the x-axis. 8 Discourse and Information Extraction. TAP is an information extraction task that synthesizes ideas from semantic role labeling on the one hand and discourse parsing on the other. The former produces predicate-argument representations of individual facts in a text (Baker et al., 1998; Gildea and Jurafsky, 2002; Palmer et al., 2005); the latter identifies discourse relations between syntactic clauses (Taboada and Mann, 2006; Prasad et al., 2007; Pitler et al., 2009; Prasad et al., 2010; Surdeanu et al., 2015). TAP first maps from syntax to a set of SRL-style representations, and then identifies structurallyconstrained, higher-order relations among them. It is in this sense reminiscent of, but distinct from, work on causal processes by Berant et al. (2014). Related Work Analogy. In the cognitive science literature, analogy is a general form of relational reasoning unique to human cognition (Tversky and Gati, 1978; Holyoak and Thagard, 1996; Goldstone and Son, 2005; Penn et al., 2008; Holyoak, 2012). Our model of textual analogy is particularly influenced by Structure Mapping Theory (Falkenhainer et"
D18-1008,Q13-1029,0,0.052752,"Missing"
D18-1008,P15-1109,0,0.0332622,"Missing"
D18-1008,P98-1013,0,\N,Missing
D18-1008,C98-1013,0,\N,Missing
D18-1393,N15-1171,0,0.0790227,"Missing"
D18-1393,P15-2072,0,0.263398,"inally, unlike classification tasks where each article is assigned to a single category, most articles employ a variety of frames (Ghanem and McCombs, 2001). Recent work has attempted to address these conceptual challenges by defining broad framing categories. The Policy Frames Codebook defines a set of 15 frames (one of which is “Other”) commonly used in media for a broad range of issues (Boydstun et al., 2013). In a follow-up work, the authors use these frames to build The Media Frames Corpus (MFC), which consists of articles related to 3 issues: immigration, tobacco, and same-sex marriage (Card et al., 2015). About 11,900 articles are hand-annotated with frames: annotators highlight spans of text related to each frame in the codebook and assign a single “primary frame” to each document. However, the MFC, like other prior framing analyses, relies heavily on labor-intensive manual annotations. The primary automated methods have relied on probabilistic topic models (Tsur et al., 2015; Boydstun et al., 2013; Nguyen et al., 2013; Roberts et al., 2013). Although topic models can show researchers what themes are salient in a corpus, they have two main drawbacks: they tend to be corpus-specific and hard"
D18-1393,D16-1148,0,0.270422,"omated methods have relied on probabilistic topic models (Tsur et al., 2015; Boydstun et al., 2013; Nguyen et al., 2013; Roberts et al., 2013). Although topic models can show researchers what themes are salient in a corpus, they have two main drawbacks: they tend to be corpus-specific and hard to interpret. Topics discovered in one corpus are likely not relevant to a different corpus, and it is difficult to compare the outputs of topic models run on different corpora. Other automated framing analyses have used the annotations of the Media Frame Corpus to predict the primary frame of articles (Card et al., 2016; Ji and Smith, 2017), or used classifiers to identify language specifically related to framing (Baumer et al., 2015). Importantly, all of these methods focus exclusively on English data sets. While unsupervised methods like topic models can be applied to other languages, any supervised method requires annotated data, which does not exist in other languages. 3.2 Framing Analysis Methodology Our goal is to develop a method that is easy to interpret and applicable across-languages. In order to ensure our analysis is interpretable, we ground our method using the annotations of the Media Frames Co"
D18-1393,J90-1003,0,0.15656,"Missing"
D18-1393,P17-1092,0,0.195841,"relied on probabilistic topic models (Tsur et al., 2015; Boydstun et al., 2013; Nguyen et al., 2013; Roberts et al., 2013). Although topic models can show researchers what themes are salient in a corpus, they have two main drawbacks: they tend to be corpus-specific and hard to interpret. Topics discovered in one corpus are likely not relevant to a different corpus, and it is difficult to compare the outputs of topic models run on different corpora. Other automated framing analyses have used the annotations of the Media Frame Corpus to predict the primary frame of articles (Card et al., 2016; Ji and Smith, 2017), or used classifiers to identify language specifically related to framing (Baumer et al., 2015). Importantly, all of these methods focus exclusively on English data sets. While unsupervised methods like topic models can be applied to other languages, any supervised method requires annotated data, which does not exist in other languages. 3.2 Framing Analysis Methodology Our goal is to develop a method that is easy to interpret and applicable across-languages. In order to ensure our analysis is interpretable, we ground our method using the annotations of the Media Frames Corpus. However, becaus"
D18-1393,D17-1292,0,0.014393,"d terrorist will simply be replaced “and everything will start afresh - explosions, chases, roundups...unlucky businessmen, successful terrorists”. The articles 3577 portray the U.S. as an unsafe place to live, making Russia seem like a preferable home. A third type of article also presents Russia as safe by downplaying U.S. military threat: “the missile defense system of the USA does not pose a real threat to Russia’s strategic nuclear forces.” or describing the growth of Russian technology compared to ‘impotent’ American counterparts. to sentiment of social media posts (Nardo et al., 2016). Kang et al. (2017) combine text and Granger causality for a different task: automatically explaining causes of time series events. Our study differs from past work in that we reverse the direction: rather than using news articles to model changes in economic data, we use economic data to show changes in news articles. 6 7 Related Work Most studies on Russian media manipulation focus on state-owned television networks, such as Channel 1 and RT. Strategies identified in these outlets include spreading confusion (Paul and Matthews, 2016) and “selection attribution”, in which negative economic events are attributed"
D18-1393,P15-1157,0,0.566412,"uarterly or yearly level. 3572 of examples. Finally, we use this method to contextualize strategies of media manipulation in the Izvestia corpus. 3.1 Background on Framing Analyses While agenda-setting broadly refers to what topics a text covers, framing refers to which attributes of those topics are highlighted. Several aspects of framing make the concept difficult to analyze. First, just defining framing has been “notoriously slippery” (Boydstun et al., 2013). Frames can occur as stock phrases, i.e. “death tax” vs. “estate tax”, but they can also occur as broader associations or sub-topics (Tsur et al., 2015; McCombs, 2002). Frames also need to be distinguished from similar concepts, like sentiment and stance. For example, the same frame can be used to take different stances on an issue: one politician might argue that immigrants boost the economy by starting new companies that create jobs, while another might argue that immigrants hurt the economy by taking jobs away from U.S. citizens (Baumer et al., 2015; Gamson and Modigliani, 1989). Finally, unlike classification tasks where each article is assigned to a single category, most articles employ a variety of frames (Ghanem and McCombs, 2001). Re"
D19-1469,N19-1056,0,0.120414,"Missing"
D19-1469,N19-1412,0,0.0177554,"the focus on information sharing and constructions of self-image. Computational approaches to multi-modal document understanding have focused on key problems such as image captioning (Chen et al., 2015; Faghri et al., 2018), visual question answering (Goyal et al., 2017; Zellers et al., 2018; Hudson and Manning, 2019), or extracting the literal or connotative meaning of a post (Soleymani et al., 2017). More recent work has explored the role of image as context for interaction and pragmatics, either in dialog (Mostafazadeh et al., 2016, 2017), or as a prompt for users to generate descriptions (Bisk et al., 2019). Another important direction has looked at an image’s perlocutionary force (how it is perceived by its audience), including aspects such as memorability (Khosla et al., 2015), saliency (Bylinskii et al., 2018), popularity (Khosla et al., 2014) and virality (Deza and Parikh, 2015; Alameda-Pineda et al., 2017). Some prior work has focused on intention. Joo et al. (2014) and Huang and Kovashka (2016) study prediction of intent behind politician portraits in the news. Hussain et al. (2017) study the understanding of image and video advertisements, predicting topic, sentiment, and intent. Alikhani"
D19-1469,I17-1047,0,0.0587702,"Missing"
D19-1469,P16-1170,0,0.0311619,"ia. For example, we rarely see commissive posts on Instagram and Facebook because of the focus on information sharing and constructions of self-image. Computational approaches to multi-modal document understanding have focused on key problems such as image captioning (Chen et al., 2015; Faghri et al., 2018), visual question answering (Goyal et al., 2017; Zellers et al., 2018; Hudson and Manning, 2019), or extracting the literal or connotative meaning of a post (Soleymani et al., 2017). More recent work has explored the role of image as context for interaction and pragmatics, either in dialog (Mostafazadeh et al., 2016, 2017), or as a prompt for users to generate descriptions (Bisk et al., 2019). Another important direction has looked at an image’s perlocutionary force (how it is perceived by its audience), including aspects such as memorability (Khosla et al., 2015), saliency (Bylinskii et al., 2018), popularity (Khosla et al., 2014) and virality (Deza and Parikh, 2015; Alameda-Pineda et al., 2017). Some prior work has focused on intention. Joo et al. (2014) and Huang and Kovashka (2016) study prediction of intent behind politician portraits in the news. Hussain et al. (2017) study the understanding of ima"
D19-1469,N18-1202,0,0.0301579,"and to further explore our hypothesis about meaning multiplication. Our model can take as input either image (Img), text (Txt) or both (Img + Txt), and consists of modality specific encoders, a fusion layer, and a class prediction layer. We use the ResNet-18 network pre-trained on ImageNet as the image encoder (He et al., 2016). For encoding captions, we use a standard pipeline that employs a RNN model on word embeddings. We experiment with both word2vec type (word token-based) embeddings trained from scratch (Mikolov et al., 2013) and pre-trained character-based contextual embeddings (ELMo) (Peters et al., 2018). For our purpose ELMo character embeddings are more useful since they increase robustness to noisy and often misspelled Instagram captions. For the combined model, we implement a simple fusion strategy that first linearly projects encoded vectors from both the modalities in the same embedding space and then adds the two vectors. Although naive, this strategy has been shown to be effective at a variety of tasks such as Visual Question Answering (Nguyen and Okatani, 2018) and image-caption matching (Ahuja et al., 2018). We then use the fused vector to predict class-wise scores using a fully con"
E17-1044,E14-1005,0,0.379369,"Missing"
E17-1044,P14-1035,0,0.0647256,"2000 tokens on both sides. ES 98.4 92.1 97.5 AS(p) 77.3 62.5 67.0 AS(o) 42.9 35.0 14.9 IS 82.3 71.5 60.4 All 85.1 75.9 72.7 Table 9: Breakdown of the accuracy of our system per type of quote (see Table 3) in each test set. Exact Name Match If the mention that a quote is linked to matches a character name or alias in our character list, label the quote with that speaker. we use the Exact Name Match and Coreference Disambiguation sieves. Coreference Disambiguation If the mention is a pronoun, we attempt to disambiguate it to a specific character using the coreference labels provided by BookNLP (Bamman et al., 2014). 6.2 Table 7 shows a direct comparison of our work versus the previous systems. We replicate the test conditions used by He et al. (2013) as closely as possible in this comparison. In this comparison, the evaluations based on CQSC are of non-contiguous subsets of the quotes that are also not necessarily the same between our work and the previous work. As discussed in section 3, CQSC provides an incomplete set of quotespeaker labels. In this work we follow the same methodology as He et al. (2013) to extract a test set of unambiguously labeled quotes by using a list of character names to identi"
E17-1044,P06-1005,0,0.107909,"Missing"
E17-1044,Q14-1022,0,0.0243667,"ble to leverage additional contextual information, resulting in a richer, labeled output. Its modular design means that it can be easily updated to account for improvements in various sub-areas such as coreference resolution. We use a sievebased architecture because having accurate labels for the easy cases allows us to first find anchors that help resolve harder, often conversational, cases. Sieve-based systems have been shown to work well for tasks like coreference resolution (Raghunathan et al., 2010; Lee et al., 2013), entity linking (Hajishirzi et al., 2013), and event temporal ordering (Chambers et al., 2014). 5.1 Quote→Mention The quote→mention stage is a series of deterministic sieves. We describe each in detail in the following sections and show examples in Table 5. Trigram Matching This sieve is similar to patterns used in Elson and McKeown (2010). It uses patterns like Quote-Mention-Verb (e.g ‘‘...’’ she said) where the mention is either a character name or pronoun to isolate the mention. Other patterns include Quote-Verb-Mention, MentionVerb-Quote, and Verb-Mention-Quote. Dependency Parses The next sieve in our pipeline inspects the dependency parses of the sentences surrounding the target q"
E17-1044,D14-1082,0,0.0242219,"tail in the following sections and show examples in Table 5. Trigram Matching This sieve is similar to patterns used in Elson and McKeown (2010). It uses patterns like Quote-Mention-Verb (e.g ‘‘...’’ she said) where the mention is either a character name or pronoun to isolate the mention. Other patterns include Quote-Verb-Mention, MentionVerb-Quote, and Verb-Mention-Quote. Dependency Parses The next sieve in our pipeline inspects the dependency parses of the sentences surrounding the target quote. We use the enhanced dependency parses (Schuster and Manning, 2016) produced by Stanford CoreNLP (Chen and Manning, 2014) to extract all verbs and their dependent nsubj nodes. If the verb is a common speech verb8 and its nsubj relation points to a 7 Character lists are available on sites like sparknotes.com. The automatic extraction of characters from a novel has been identified as a separate problem (Vala et al., 2015). 8 This list of verbs as well as the family relation nouns list are available in supplemental section A.4. 464 Sieve Trigram Matching Dependency Parses Single Mention Detection Vocative Detection Paragraph Final Mention Linking Supervised Sieve Conversation Detection Loose Conversation Detection"
E17-1044,P16-1061,0,0.0515512,"Missing"
E17-1044,D10-1048,1,0.846089,"s mentions to the entity that they refer to. By doing both quote→mention and mention→entity linking, our system is able to leverage additional contextual information, resulting in a richer, labeled output. Its modular design means that it can be easily updated to account for improvements in various sub-areas such as coreference resolution. We use a sievebased architecture because having accurate labels for the easy cases allows us to first find anchors that help resolve harder, often conversational, cases. Sieve-based systems have been shown to work well for tasks like coreference resolution (Raghunathan et al., 2010; Lee et al., 2013), entity linking (Hajishirzi et al., 2013), and event temporal ordering (Chambers et al., 2014). 5.1 Quote→Mention The quote→mention stage is a series of deterministic sieves. We describe each in detail in the following sections and show examples in Table 5. Trigram Matching This sieve is similar to patterns used in Elson and McKeown (2010). It uses patterns like Quote-Mention-Verb (e.g ‘‘...’’ she said) where the mention is either a character name or pronoun to isolate the mention. Other patterns include Quote-Verb-Mention, MentionVerb-Quote, and Verb-Mention-Quote. Depende"
E17-1044,P16-1164,0,0.150685,"Missing"
E17-1044,D13-1029,0,0.0243498,"ote→mention and mention→entity linking, our system is able to leverage additional contextual information, resulting in a richer, labeled output. Its modular design means that it can be easily updated to account for improvements in various sub-areas such as coreference resolution. We use a sievebased architecture because having accurate labels for the easy cases allows us to first find anchors that help resolve harder, often conversational, cases. Sieve-based systems have been shown to work well for tasks like coreference resolution (Raghunathan et al., 2010; Lee et al., 2013), entity linking (Hajishirzi et al., 2013), and event temporal ordering (Chambers et al., 2014). 5.1 Quote→Mention The quote→mention stage is a series of deterministic sieves. We describe each in detail in the following sections and show examples in Table 5. Trigram Matching This sieve is similar to patterns used in Elson and McKeown (2010). It uses patterns like Quote-Mention-Verb (e.g ‘‘...’’ she said) where the mention is either a character name or pronoun to isolate the mention. Other patterns include Quote-Verb-Mention, MentionVerb-Quote, and Verb-Mention-Quote. Dependency Parses The next sieve in our pipeline inspects the depend"
E17-1044,L16-1376,0,0.0208846,"Missing"
E17-1044,P13-1129,0,0.143068,"h for Quote Attribution Grace Muzny1 , Michael Fang1 , Angel X. Chang1,2 , and Dan Jurafsky1 1 Stanford University, Stanford, CA 94305 2 Princeton University, Princeton, NJ 08544 {muzny,mjfang,angelx,jurafsky}@cs.stanford.edu Abstract speaker only indirectly via anaphora, or even omit mention of the speaker entirely (Table 1). Prior work has produced important datasets labeling quotes in novels, providing training data for supervised methods. But some of these model the quote-attribution task at the mention-level (Elson and McKeown, 2010; O’Keefe et al., 2012), and others at the entity-level (He et al., 2013), leading to labels that are inconsistent across datasets. We propose entity-level quote attribution as the end goal but with mention-level quote attribution as an important intermediary step. Our first contribution is the QuoteLi3 dataset, a unified combination of data from Elson and McKeown (2010) and He et al. (2013) with the addition of more than 3,000 new labels from expert annotators. This dataset provides both mention and entity labels for Pride and Prejudice, Emma, and The Steppe. Next, we describe a new deterministic system that models quote attribution as a two-step process that i) u"
E17-1044,E12-2021,0,0.0400605,"Missing"
E17-1044,Y09-1024,0,0.0304359,"be determined either by the gender of a pronoun mention or the gender of an animate noun (Bergsma and Paragraph Final Mention Linking If the target quote occurs at the end of a paragraph, link it to the final mention occurring in the preceding sentence. Conversational Pattern If a quote in paragraph n has been linked to mention mn , then this sieve links an unattributed quote two paragraphs ahead, n + 2, to mention mn if they appear to be in conversation. We consider two quotes “in conversation” if the paragraph between is also a quote, and 9 Mention→Speaker The list of animate nouns is from Ji and Lin (2009). 465 Test P&P Emma The Steppe Lin, 2006), this information is used to filter the candidate speakers in the top speakers list. We use a window size from 2000 tokens before the target quote to 500 tokens after the target quote. If no speakers matching in gender can be found in this window, it is expanded by 2000 tokens on both sides. ES 98.4 92.1 97.5 AS(p) 77.3 62.5 67.0 AS(o) 42.9 35.0 14.9 IS 82.3 71.5 60.4 All 85.1 75.9 72.7 Table 9: Breakdown of the accuracy of our system per type of quote (see Table 3) in each test set. Exact Name Match If the mention that a quote is linked to matches a c"
E17-1044,D15-1088,0,0.0473102,"clude Quote-Verb-Mention, MentionVerb-Quote, and Verb-Mention-Quote. Dependency Parses The next sieve in our pipeline inspects the dependency parses of the sentences surrounding the target quote. We use the enhanced dependency parses (Schuster and Manning, 2016) produced by Stanford CoreNLP (Chen and Manning, 2014) to extract all verbs and their dependent nsubj nodes. If the verb is a common speech verb8 and its nsubj relation points to a 7 Character lists are available on sites like sparknotes.com. The automatic extraction of characters from a novel has been identified as a separate problem (Vala et al., 2015). 8 This list of verbs as well as the family relation nouns list are available in supplemental section A.4. 464 Sieve Trigram Matching Dependency Parses Single Mention Detection Vocative Detection Paragraph Final Mention Linking Supervised Sieve Conversation Detection Loose Conversation Detection Example “They have none of them much to recommend them,” replied he. Mrs. Bennet said only, “Nonsense, nonsense!” ...Elizabeth impatiently. “There has been many a one, I fancy, overcome in the same way. I wonder who first discovered the efficacy of poetry in driving away love!” “My dear Mr. Bennet,..."
E17-1044,J13-4004,1,0.816088,"hat they refer to. By doing both quote→mention and mention→entity linking, our system is able to leverage additional contextual information, resulting in a richer, labeled output. Its modular design means that it can be easily updated to account for improvements in various sub-areas such as coreference resolution. We use a sievebased architecture because having accurate labels for the easy cases allows us to first find anchors that help resolve harder, often conversational, cases. Sieve-based systems have been shown to work well for tasks like coreference resolution (Raghunathan et al., 2010; Lee et al., 2013), entity linking (Hajishirzi et al., 2013), and event temporal ordering (Chambers et al., 2014). 5.1 Quote→Mention The quote→mention stage is a series of deterministic sieves. We describe each in detail in the following sections and show examples in Table 5. Trigram Matching This sieve is similar to patterns used in Elson and McKeown (2010). It uses patterns like Quote-Mention-Verb (e.g ‘‘...’’ she said) where the mention is either a character name or pronoun to isolate the mention. Other patterns include Quote-Verb-Mention, MentionVerb-Quote, and Verb-Mention-Quote. Dependency Parses The next"
E17-1044,L16-1028,0,0.121495,"ark and Manning, 2016). 4 For first-level quotes, there is typically just one speaker per paragraph. This assumption breaks down in some cases and it is very rarely true for nested quotes. 5 462 See supplemental section A.1. Figure 1: Conversation from Pride and Prejudice annotated with our annotation tool. Speakers are indicated by color, mentions are marked by dashed outlines, and quote-to-mention links by blue lines. notation tool developed by the authors. Previously developed tools were either not designed for the task (BRAT (Stenetorp et al., 2012), WebAnno (Yimam et al., 2013), CHARLES (Vala et al., 2016)) or unavailable (He et al., 2013). One problem with the CQSC annotations was that the annotators were shown short snippets that lacked the context to determine the speaker and no character list. We designed our tool to provide context and a character list including name, aliases, gender, and description of the character. Similar to CHARLES, the character list is not static and the annotator can add to the list of characters. Our tool also features automatic data consistency checks such as ensuring that all quotes are linked to a mention. Our expert annotators achieved high interannotator agre"
E17-1044,P16-1094,0,0.0461109,"Missing"
E17-1044,D12-1072,0,0.349651,"Missing"
E17-1044,P13-4001,0,0.0611843,"Missing"
E17-1044,D13-1101,0,0.14513,"Missing"
E17-1044,pareti-2012-database,0,0.045456,"Missing"
J13-4004,W97-1306,0,0.430065,"le-based systems relied on hand-tuned weights and were not capable of global inference, two factors that led to poor performance and replacement by machine learning. We propose a new approach that brings together the insights of these modern supervised and unsupervised models with the advantages of deterministic, rule-based systems. We introduce a model that performs entity-centric coreference, where all mentions that point to the same real-world entity are jointly modeled, in a rich feature space using solely simple, deterministic rules. Our work is inspired both by the seminal early work of Baldwin (1997), who first proposed that a series of high-precision rules could be used to build a high-precision, low-recall system for anaphora resolution, and by more recent work that has suggested that deterministic rules can outperform machine learning models for coreference (Zhou and Su 2004; Haghighi and Klein 2009) and for named entity recognition (Chiticariu et al. 2010). Figure 1 illustrates the two main stages of our new deterministic model: mention detection and coreference resolution, as well as a smaller post-processing step. In the mention detection stage, nominal and pronominal mentions are i"
J13-4004,P12-1041,0,0.0506069,"Missing"
J13-4004,D08-1031,0,0.592969,"Missing"
J13-4004,P06-1005,0,0.313052,"rs for pronominal coreference. We implement pronominal coreference resolution using an approach standard for many decades: enforcing agreement constraints between the coreferent mentions. We use the following attributes for these constraints: r r r r r r Number – we assign number attributes based on: (a) a static list for pronouns; (b) NER labels: mentions marked as a named entity are considered singular with the exception of organizations, which can be both singular and plural; (c) part of speech tags: NN*S tags are plural and all other NN* tags are singular; and (d) a static dictionary from Bergsma and Lin (2006). Gender – we assign gender attributes from static lexicons from Bergsma and Lin (2006), and Ji and Lin (2009). Person – we assign person attributes only to pronouns. We do not enforce this constraint when linking two pronouns, however, if one appears within quotes. This is a simple heuristic for speaker detection (e.g., I and she point to the same person in “[I] voted my conscience,” [she] said). Animacy – we set animacy attributes using: (a) a static list for pronouns; (b) NER labels (e.g., PERSON is animate whereas LOCATION is not); and (c) a dictionary bootstrapped from the Web (Ji and Lin"
J13-4004,P08-1002,0,0.0325892,"Missing"
J13-4004,C82-1006,0,0.137371,"coreference work mentioned earlier and discussed in Section 6, we draw on classic ideas that have proved to be important again and again in the history of natural language processing. The idea of beginning with the most accurate models or starting with smaller subproblems that allow for high-precision solutions combines the intuitions of “shaping” or “successive approximations” first proposed for learning by Skinner (1938), and widely used in NLP (e.g., the successively trained IBM MT models of Brown et al. [1993]) and the “islands of reliability” approaches to parsing and speech recognition [Borghesi and Favareto 1982; Corazza et al. 1991]). The idea of beginning with a high-recall list of candidates that are followed by a series of high-precision filters dates back to one of the earliest architectures in natural language processing, the part of speech tagging algorithm of the Computational Grammar Coder (Klein and Simmons 887 Computational Linguistics Volume 39, Number 4 1963) and the TAGGIT tagger (Greene and Rubin 1971), which begin with a high-recall list of all possible tags for words, and then used high-precision rules to filter likely tags based on context. In the next section we walk through an exa"
J13-4004,W05-0406,0,0.105348,"Missing"
J13-4004,P87-1022,0,0.348698,"Missing"
J13-4004,J93-2003,0,0.0334508,"Missing"
J13-4004,W11-1907,0,0.0406925,"Missing"
J13-4004,W99-0611,0,0.559667,"Missing"
J13-4004,W11-1904,0,0.0068184,"tter we used gold mentions. The only reason for this distinction is to facilitate comparison with previous work (all systems listed in Table 5 used gold mention boundaries). The two tables show that, regardless of evaluation corpus and methodology, our system generally outperforms the previous state of the art. In the CoNLL shared task, 900 Lee et al. Deterministic Coreference Resolution Based on Entity-Centric, Precision-Ranked Rules our system scores 1.8 CoNLL F1 points higher than the next system in the closed track and 2.6 points higher than the second-ranked system in the open track. The Chang et al. (2011) system has marginally higher B3 and BLANC F1 scores, but does not outperform our model on the other two metrics and the average F1 score. Table 5 shows that our model has higher B3 F1 scores than all the other models in the two ACE corpora. The model of Haghighi and Klein (2009) minimally outperforms ours by 0.6 B3 F1 points in the MUC corpus. All in all, these results prove that our approach compares favorably with a wide range of models, which include most aspects deemed important for coreference resolution, among other things, supervised learning using ´ and Turmo 2011; Chang et al. 2011),"
J13-4004,W12-4504,0,0.0360779,"Missing"
J13-4004,D10-1098,0,0.062478,"Missing"
J13-4004,W99-0613,0,0.44242,"Missing"
J13-4004,1991.iwpt-1.24,0,0.0410389,"Missing"
J13-4004,N07-1011,0,0.284716,"or example, even if we start with a perfect set of gold mentions, if we miss all coreference relations in a text, every mention will remain as a singleton and will be removed by the OntoNotes post Table 5 Comparison of our system with the other reported results on the ACE and MUC corpora. All these systems use gold mention boundaries. System B3 MUC R P F1 R P F1 74.5 78.5 73.2 74.5 88.7 79.6 86.7 88.3 81.0 79.0 79.3 80.8 74.1 74.5 – 65.2 87.3 79.4 – 86.8 80.2 76.9 – 74.5 63.1 67.3 – 49.7 90.6 84.7 – 90.9 74.4 75.0 – 64.3 ACE2004-Culotta-Test This paper Haghighi and Klein (2009) Culotta et al. (2007) Bengston and Roth (2008) 70.2 77.7 – 69.9 82.7 74.8 – 82.7 75.9 79.6 – 75.8 ACE2004-nwire This paper Haghighi and Klein (2009) Poon and Domingos (2008) Finkel and Manning (2008) 75.1 75.9 70.5 58.5 84.6 77.0 71.3 78.7 79.6 76.5 70.9 67.1 MUC6-Test This paper Haghighi and Klein (2009) Poon and Domingos (2008) Finkel and Manning (2008) 69.1 77.3 75.8 55.1 90.6 87.2 83.0 89.7 78.4 81.9 79.2 68.3 901 Computational Linguistics Volume 39, Number 4 processing, resulting in zero mentions in the final output. Therefore, we included the score using gold mention boundaries in the last part of Table 4 (“"
J13-4004,H05-1013,0,0.0270679,"Missing"
J13-4004,W08-1301,0,0.0240857,"Missing"
J13-4004,N07-1030,0,0.190945,"Missing"
J13-4004,doddington-etal-2004-automatic,0,0.0156715,"ible that . . . , It seems that . . . , It turns out . . . ). The complete set of patterns, using the tregex2 notation, is shown in Appendix B. 5. We discard adjectival forms of nations or nationality acronyms (e.g., American, U.S., U.K.), following the OntoNotes annotation guidelines. 6. We remove stop words from the following list determined by error analysis on mention detection: there, ltd., etc, ’s, hmm. Note that some rules change depending on the corpus we use for evaluation. In particular, adjectival forms of nations are valid mentions in the Automated Content Extraction (ACE) corpus (Doddington et al. 2004), thus they would not be removed when processing this corpus. 3.2 Resolution Architecture Traditionally, coreference resolution is implemented as a quadratic problem, where potential coreference links between any two mentions in a document are considered. This is not ideal, however, as it increases both the likelihood of errors and the processing time. In this article, we argue that it is better to cautiously construct high-quality mention clusters,3 and use an entity-centric model that allows the sharing of information across these incrementally constructed clusters. We achieve these goals by"
J13-4004,P10-2007,0,0.0175971,"Notes corpus, this sieve does not enhance recall significantly, mainly because appositions and predicate nominatives are not annotated in this corpus (they are annotated in ACE). Regardless of annotation standard, however, this sieve is important because it grows entities with high quality elements, which has a significant impact on the entity’s features (as discussed in Section 3.2.3). 3.3.5 Pass 5 – Strict Head Match. Linking a mention to an antecedent based on the naive matching of their head words generates many spurious links because it completely ignores possibly incompatible modifiers (Elsner and Charniak 2010). For example, Yale University and Harvard University have similar head words, but they are obviously different entities. To address this issue, this pass implements several constraints that must all be matched in order to yield a link: r Entity head match – the mention head word matches any head word of mentions in the antecedent entity. Note that this feature is actually more relaxed than naive head matching in a pair of mentions because here it is satisfied when the mention’s head matches the head of any mention in the candidate entity. We constrain this feature by enforcing a conjunction w"
J13-4004,W12-4502,0,0.381614,"Missing"
J13-4004,P05-1045,0,0.191986,"Missing"
J13-4004,P08-2012,0,0.146569,"Missing"
J13-4004,P07-2027,1,0.571167,"Missing"
J13-4004,P07-1107,0,0.299269,"Missing"
J13-4004,D09-1120,0,0.188162,"ng knowledge in rule-based systems that has implications throughout computational linguistics. 1. Introduction Coreference resolution, the task of finding all expressions that refer to the same entity in a discourse, is important for natural language understanding tasks like summarization, question answering, and information extraction. The long history of coreference resolution has shown that the use of highly precise lexical and syntactic features is crucial to high quality resolution (Ng and Cardie 2002b; Lappin and Leass 1994; Poesio et al. 2004a; Zhou and Su 2004; Bengtson and Roth 2008; Haghighi and Klein 2009). Recent work has also shown the importance of global inference—performing coreference resolution jointly for several or all mentions in a document—rather than greedily disambiguating individual pairs of mentions (Morton 2000; Luo et al. 2004; Yang et al. 2004; Culotta et al. 2007; Yang et al. 2008; Poon and Domingos 2008; Denis and Baldridge 2009; Rahman and Ng 2009; Haghighi and Klein 2010; Cai, Mujdricza-Maydt, and Strube 2011). Modern systems have met this need for carefully designed features and global or entity-centric inference with machine learning approaches to coreference resolution."
J13-4004,N10-1061,0,0.481142,"Missing"
J13-4004,Y09-1024,0,0.646111,"ecades: enforcing agreement constraints between the coreferent mentions. We use the following attributes for these constraints: r r r r r r Number – we assign number attributes based on: (a) a static list for pronouns; (b) NER labels: mentions marked as a named entity are considered singular with the exception of organizations, which can be both singular and plural; (c) part of speech tags: NN*S tags are plural and all other NN* tags are singular; and (d) a static dictionary from Bergsma and Lin (2006). Gender – we assign gender attributes from static lexicons from Bergsma and Lin (2006), and Ji and Lin (2009). Person – we assign person attributes only to pronouns. We do not enforce this constraint when linking two pronouns, however, if one appears within quotes. This is a simple heuristic for speaker detection (e.g., I and she point to the same person in “[I] voted my conscience,” [she] said). Animacy – we set animacy attributes using: (a) a static list for pronouns; (b) NER labels (e.g., PERSON is animate whereas LOCATION is not); and (c) a dictionary bootstrapped from the Web (Ji and Lin 2009). NER label – from the Stanford NER. Pronoun distance - sentence distance between a pronoun and its ante"
J13-4004,W97-0319,0,0.282039,"Missing"
J13-4004,P03-1054,0,0.0128291,"ition of the ACE 2004 corpus reserved for testing by several previous studies (Culotta et al. 2007; Bengtson and Roth 2008; Haghighi and Klein 2009). ACE2004-nwire – newswire subset of the ACE 2004 corpus, utilized by Poon and Domingos (2008) and Haghighi and Klein (2009) for testing. MUC6-Test – test corpus from the sixth Message Understanding Conference (MUC-6) evaluation. The corpora statistics are shown in Table 3. We used the first corpus (OntoNotes-Dev) for development and all others for the formal evaluation. We parsed all documents in the ACE and MUC corpora using the Stanford parser (Klein and Manning 2003) and the Stanford NER (Finkel, Grenager, and Manning 2005). We used the provided parse Table 3 Corpora statistics. Corpora OntoNotes-Dev OntoNotes-Test ACE2004-Culotta-Test ACE2004-nwire MUC6-Test 898 # Documents # Sentences # Words # Entities # Mentions 303 322 107 128 30 6,894 8,262 1,993 3,594 576 136K 142K 33K 74K 13K 3,752 3,926 2,576 4,762 496 14,291 16,291 5,455 11,398 2,136 Lee et al. Deterministic Coreference Resolution Based on Entity-Centric, Precision-Ranked Rules trees and named entity labels (not gold) in the OntoNotes corpora to facilitate the comparison with other systems. 4.2"
J13-4004,P11-1079,0,0.0279236,"Missing"
J13-4004,J94-4002,0,0.268022,"guistics Volume 39, Number 4 Arabic. Our system thus offers a new paradigm for combining knowledge in rule-based systems that has implications throughout computational linguistics. 1. Introduction Coreference resolution, the task of finding all expressions that refer to the same entity in a discourse, is important for natural language understanding tasks like summarization, question answering, and information extraction. The long history of coreference resolution has shown that the use of highly precise lexical and syntactic features is crucial to high quality resolution (Ng and Cardie 2002b; Lappin and Leass 1994; Poesio et al. 2004a; Zhou and Su 2004; Bengtson and Roth 2008; Haghighi and Klein 2009). Recent work has also shown the importance of global inference—performing coreference resolution jointly for several or all mentions in a document—rather than greedily disambiguating individual pairs of mentions (Morton 2000; Luo et al. 2004; Yang et al. 2004; Culotta et al. 2007; Yang et al. 2008; Poon and Domingos 2008; Denis and Baldridge 2009; Rahman and Ng 2009; Haghighi and Klein 2010; Cai, Mujdricza-Maydt, and Strube 2011). Modern systems have met this need for carefully designed features and globa"
J13-4004,W11-1902,1,0.676764,"Missing"
J13-4004,H05-1004,0,0.725061,"Missing"
J13-4004,P04-1018,0,0.0538826,"understanding tasks like summarization, question answering, and information extraction. The long history of coreference resolution has shown that the use of highly precise lexical and syntactic features is crucial to high quality resolution (Ng and Cardie 2002b; Lappin and Leass 1994; Poesio et al. 2004a; Zhou and Su 2004; Bengtson and Roth 2008; Haghighi and Klein 2009). Recent work has also shown the importance of global inference—performing coreference resolution jointly for several or all mentions in a document—rather than greedily disambiguating individual pairs of mentions (Morton 2000; Luo et al. 2004; Yang et al. 2004; Culotta et al. 2007; Yang et al. 2008; Poon and Domingos 2008; Denis and Baldridge 2009; Rahman and Ng 2009; Haghighi and Klein 2010; Cai, Mujdricza-Maydt, and Strube 2011). Modern systems have met this need for carefully designed features and global or entity-centric inference with machine learning approaches to coreference resolution. But machine learning, although powerful, has limitations. Supervised machine learning systems rely on expensive hand-labeled data sets and generalize poorly to new words or domains. Unsupervised systems are increasingly more complex, making"
J13-4004,P00-1023,0,0.102077,"ral language understanding tasks like summarization, question answering, and information extraction. The long history of coreference resolution has shown that the use of highly precise lexical and syntactic features is crucial to high quality resolution (Ng and Cardie 2002b; Lappin and Leass 1994; Poesio et al. 2004a; Zhou and Su 2004; Bengtson and Roth 2008; Haghighi and Klein 2009). Recent work has also shown the importance of global inference—performing coreference resolution jointly for several or all mentions in a document—rather than greedily disambiguating individual pairs of mentions (Morton 2000; Luo et al. 2004; Yang et al. 2004; Culotta et al. 2007; Yang et al. 2008; Poon and Domingos 2008; Denis and Baldridge 2009; Rahman and Ng 2009; Haghighi and Klein 2010; Cai, Mujdricza-Maydt, and Strube 2011). Modern systems have met this need for carefully designed features and global or entity-centric inference with machine learning approaches to coreference resolution. But machine learning, although powerful, has limitations. Supervised machine learning systems rely on expensive hand-labeled data sets and generalize poorly to new words or domains. Unsupervised systems are increasingly more"
J13-4004,D08-1067,0,0.32593,"Missing"
J13-4004,N09-1065,0,0.134029,"shown that the use of highly precise lexical and syntactic features is crucial to high quality resolution (Ng and Cardie 2002b; Lappin and Leass 1994; Poesio et al. 2004a; Zhou and Su 2004; Bengtson and Roth 2008; Haghighi and Klein 2009). Recent work has also shown the importance of global inference—performing coreference resolution jointly for several or all mentions in a document—rather than greedily disambiguating individual pairs of mentions (Morton 2000; Luo et al. 2004; Yang et al. 2004; Culotta et al. 2007; Yang et al. 2008; Poon and Domingos 2008; Denis and Baldridge 2009; Rahman and Ng 2009; Haghighi and Klein 2010; Cai, Mujdricza-Maydt, and Strube 2011). Modern systems have met this need for carefully designed features and global or entity-centric inference with machine learning approaches to coreference resolution. But machine learning, although powerful, has limitations. Supervised machine learning systems rely on expensive hand-labeled data sets and generalize poorly to new words or domains. Unsupervised systems are increasingly more complex, making them hard to tune and difficult to apply to new problems and genres as well. Rule-based models like Lappin and Leass (1994) wer"
J13-4004,P10-1142,0,0.165739,"Missing"
J13-4004,C02-1139,0,0.652854,"ics Computational Linguistics Volume 39, Number 4 Arabic. Our system thus offers a new paradigm for combining knowledge in rule-based systems that has implications throughout computational linguistics. 1. Introduction Coreference resolution, the task of finding all expressions that refer to the same entity in a discourse, is important for natural language understanding tasks like summarization, question answering, and information extraction. The long history of coreference resolution has shown that the use of highly precise lexical and syntactic features is crucial to high quality resolution (Ng and Cardie 2002b; Lappin and Leass 1994; Poesio et al. 2004a; Zhou and Su 2004; Bengtson and Roth 2008; Haghighi and Klein 2009). Recent work has also shown the importance of global inference—performing coreference resolution jointly for several or all mentions in a document—rather than greedily disambiguating individual pairs of mentions (Morton 2000; Luo et al. 2004; Yang et al. 2004; Culotta et al. 2007; Yang et al. 2008; Poon and Domingos 2008; Denis and Baldridge 2009; Rahman and Ng 2009; Haghighi and Klein 2010; Cai, Mujdricza-Maydt, and Strube 2011). Modern systems have met this need for carefully des"
J13-4004,P02-1014,0,0.574409,"Missing"
J13-4004,P04-1019,0,0.0115911,"ber 4 Arabic. Our system thus offers a new paradigm for combining knowledge in rule-based systems that has implications throughout computational linguistics. 1. Introduction Coreference resolution, the task of finding all expressions that refer to the same entity in a discourse, is important for natural language understanding tasks like summarization, question answering, and information extraction. The long history of coreference resolution has shown that the use of highly precise lexical and syntactic features is crucial to high quality resolution (Ng and Cardie 2002b; Lappin and Leass 1994; Poesio et al. 2004a; Zhou and Su 2004; Bengtson and Roth 2008; Haghighi and Klein 2009). Recent work has also shown the importance of global inference—performing coreference resolution jointly for several or all mentions in a document—rather than greedily disambiguating individual pairs of mentions (Morton 2000; Luo et al. 2004; Yang et al. 2004; Culotta et al. 2007; Yang et al. 2008; Poon and Domingos 2008; Denis and Baldridge 2009; Rahman and Ng 2009; Haghighi and Klein 2010; Cai, Mujdricza-Maydt, and Strube 2011). Modern systems have met this need for carefully designed features and global or entity-centric"
J13-4004,W04-0707,0,0.046836,"ber 4 Arabic. Our system thus offers a new paradigm for combining knowledge in rule-based systems that has implications throughout computational linguistics. 1. Introduction Coreference resolution, the task of finding all expressions that refer to the same entity in a discourse, is important for natural language understanding tasks like summarization, question answering, and information extraction. The long history of coreference resolution has shown that the use of highly precise lexical and syntactic features is crucial to high quality resolution (Ng and Cardie 2002b; Lappin and Leass 1994; Poesio et al. 2004a; Zhou and Su 2004; Bengtson and Roth 2008; Haghighi and Klein 2009). Recent work has also shown the importance of global inference—performing coreference resolution jointly for several or all mentions in a document—rather than greedily disambiguating individual pairs of mentions (Morton 2000; Luo et al. 2004; Yang et al. 2004; Culotta et al. 2007; Yang et al. 2008; Poon and Domingos 2008; Denis and Baldridge 2009; Rahman and Ng 2009; Haghighi and Klein 2010; Cai, Mujdricza-Maydt, and Strube 2011). Modern systems have met this need for carefully designed features and global or entity-centric"
J13-4004,D08-1068,0,0.551661,"n extraction. The long history of coreference resolution has shown that the use of highly precise lexical and syntactic features is crucial to high quality resolution (Ng and Cardie 2002b; Lappin and Leass 1994; Poesio et al. 2004a; Zhou and Su 2004; Bengtson and Roth 2008; Haghighi and Klein 2009). Recent work has also shown the importance of global inference—performing coreference resolution jointly for several or all mentions in a document—rather than greedily disambiguating individual pairs of mentions (Morton 2000; Luo et al. 2004; Yang et al. 2004; Culotta et al. 2007; Yang et al. 2008; Poon and Domingos 2008; Denis and Baldridge 2009; Rahman and Ng 2009; Haghighi and Klein 2010; Cai, Mujdricza-Maydt, and Strube 2011). Modern systems have met this need for carefully designed features and global or entity-centric inference with machine learning approaches to coreference resolution. But machine learning, although powerful, has limitations. Supervised machine learning systems rely on expensive hand-labeled data sets and generalize poorly to new words or domains. Unsupervised systems are increasingly more complex, making them hard to tune and difficult to apply to new problems and genres as well. Rule"
J13-4004,W12-4501,0,0.631873,"Missing"
J13-4004,W11-1901,0,0.834001,"ang Zhou and a consultant. . . are removed in this stage. 4. Experimental Results We start this section with overall results on three corpora widely used for the evaluation of coreference resolution systems. We continue with a series of ablative experiments that analyze the contribution of each aspect of our approach and conclude with error analysis, which highlights cases currently not solved by our approach. 4.1 Corpora We used the following corpora for development and formal evaluation: r r r r r OntoNotes-Dev – development partition of OntoNotes v4.0 provided in the CoNLL2011 shared task (Pradhan et al. 2011). OntoNotes-Test – test partition of OntoNotes v4.0 provided in the CoNLL-2011 shared task. ACE2004-Culotta-Test – partition of the ACE 2004 corpus reserved for testing by several previous studies (Culotta et al. 2007; Bengtson and Roth 2008; Haghighi and Klein 2009). ACE2004-nwire – newswire subset of the ACE 2004 corpus, utilized by Poon and Domingos (2008) and Haghighi and Klein (2009) for testing. MUC6-Test – test corpus from the sixth Message Understanding Conference (MUC-6) evaluation. The corpora statistics are shown in Table 3. We used the first corpus (OntoNotes-Dev) for development a"
J13-4004,D10-1048,1,0.78311,"s of i and j but also any information (head word, named entity type, gender, or number) about the other mentions already linked to i and j in previous steps. Finally, the architecture is highly modular, which means that additional coreference resolution models can be easily integrated. The two stage architecture offers a powerful way to balance both high recall and precision in the system and make use of entity-level information with rule-based architecture. The mention detection stage heavily favors recall, and the following sieves favor precision. Our results here and in our earlier papers (Raghunathan et al. 2010; Lee et al. 2011) show that this design leads to state-of-the-art performance despite the simplicity of the individual components, and that the lack of language-specific lexical features makes the system easy to port to other languages. The intuition is not new; in addition to the prior coreference work mentioned earlier and discussed in Section 6, we draw on classic ideas that have proved to be important again and again in the history of natural language processing. The idea of beginning with the most accurate models or starting with smaller subproblems that allow for high-precision solution"
J13-4004,D09-1101,0,0.0519169,"Missing"
J13-4004,P10-1144,0,0.0676267,"Missing"
J13-4004,N13-1110,0,0.0170762,"Missing"
J13-4004,W11-1903,0,0.0811866,"Missing"
J13-4004,W12-4514,0,0.0215518,"Missing"
J13-4004,J01-4004,0,0.898996,"Missing"
J13-4004,N10-1116,0,0.0385068,"Missing"
J13-4004,C12-1154,0,0.107033,"Missing"
J13-4004,J00-4003,0,0.143805,"Labor Party wants credit controls. • Parser or NER error: Um alright uh Mister Zalisko do you know anything from your personal experience of having been on the cruise as to what happened? – Mister Zalisko is not recognized as a PERSON • Enumerations: This year, the economies of the five large special economic zones, namely, Shenzhen, Zhuhai, Shantou, Xiamen and Hainan, have maintained strong growth momentum. . . . A three dimensional traffic frame in Zhuhai has preliminarily taken shape and the investment environment improves daily. add more sophisticated anaphoricity detection to our system (Vieira and Poesio 2000; Ng and Cardie 2002a; Poesio et al. 2004b; Boyd, Gegg-Harrison, and Byron 2005; Gupta, Purver, and Jurafsky 2007; Bergsma, Lin, and Goebel 2008; Ng 2009). Event mentions. Our system was tailored for the resolution of entity coreference and does not have any event-specific features, such as, for example, matching event participants. Furthermore, our model considers only noun phrases as antecedent candidates, thus missing all mentions that are verbal phrases. Therefore, our system misses most coreference links between event mentions. For example, in Table 12 the pronoun That 907 Computational L"
J13-4004,M95-1005,0,0.945762,"entions 303 322 107 128 30 6,894 8,262 1,993 3,594 576 136K 142K 33K 74K 13K 3,752 3,926 2,576 4,762 496 14,291 16,291 5,455 11,398 2,136 Lee et al. Deterministic Coreference Resolution Based on Entity-Centric, Precision-Ranked Rules trees and named entity labels (not gold) in the OntoNotes corpora to facilitate the comparison with other systems. 4.2 Evaluation Metrics We use five evaluation metrics widely used in the literature. B3 and CEAF have implementation variations in how to take system mentions into account. We followed the same implementation as used in CoNLL-2011 shared task. r MUC (Vilain et al. 1995) – link-based metric which measures how many predicted and gold mention clusters need to be merged to cover the gold and predicted clusters, respectively.  (|Gi |−|p(Gi ) |)  (Gi : a gold mention cluster, p(Gi ): partitions of Gi ) R= (|G |−1)  r r r r P= F1 = i (|Si |−|p(Si ) |)  (|Si |−1) 2PR P+ R (Si : a system mention cluster, p(Si ): partitions of Si ) B3 (Bagga and Baldwin 1998) – mention-based metric which measures the proportion of overlap between predicted and gold mention clusters for a given mention. When Gmi is the gold cluster of mention mi and Smi is the system cluster of men"
J13-4004,W12-3204,1,0.511811,"62.1 60.1 65.5 61.4 56.8 55.0 58.8 68.4 37.2 63.9 62.1 59.8 59.5 59.6 61.5 51.6 55.5 35.2 69.5 68.3 62.2 64.5 73.2 77.1 53.9 54.4 55.5 70.6 65.2 76.7 70.3 62.2 52.5 73.4 70.2 68.2 70.0 66.7 68.7 67.3 67.3 62.5 62.2 61.3 61.2 resources. For the closed track, the organizers provided dictionaries for gender and number information, in addition to parse trees and named entity labels (Pradhan et al. 2011). For the open track, we used the following additional resources: (a) a hand-built list of genders of first names that we created, incorporating frequent names from census lists and other sources (Vogel and Jurafsky 2012) (b) an animacy list (Ji and Lin 2009), (c) a country and state gazetteer, and (d) a demonym list. These resources were also used for the results reported in Table 5. A significant difference between Tables 4 and 5 is that in the former (other than its last block) we used predicted mentions (detected with the algorithm described in Section 3.1), whereas in the latter we used gold mentions. The only reason for this distinction is to facilitate comparison with previous work (all systems listed in Table 5 used gold mention boundaries). The two tables show that, regardless of evaluation corpus and"
J13-4004,W12-4506,0,0.0271675,"Missing"
J13-4004,P07-1067,0,0.0314736,"Missing"
J13-4004,P08-1096,0,0.147619,"ng, and information extraction. The long history of coreference resolution has shown that the use of highly precise lexical and syntactic features is crucial to high quality resolution (Ng and Cardie 2002b; Lappin and Leass 1994; Poesio et al. 2004a; Zhou and Su 2004; Bengtson and Roth 2008; Haghighi and Klein 2009). Recent work has also shown the importance of global inference—performing coreference resolution jointly for several or all mentions in a document—rather than greedily disambiguating individual pairs of mentions (Morton 2000; Luo et al. 2004; Yang et al. 2004; Culotta et al. 2007; Yang et al. 2008; Poon and Domingos 2008; Denis and Baldridge 2009; Rahman and Ng 2009; Haghighi and Klein 2010; Cai, Mujdricza-Maydt, and Strube 2011). Modern systems have met this need for carefully designed features and global or entity-centric inference with machine learning approaches to coreference resolution. But machine learning, although powerful, has limitations. Supervised machine learning systems rely on expensive hand-labeled data sets and generalize poorly to new words or domains. Unsupervised systems are increasingly more complex, making them hard to tune and difficult to apply to new problems"
J13-4004,C04-1033,0,0.0520591,"ks like summarization, question answering, and information extraction. The long history of coreference resolution has shown that the use of highly precise lexical and syntactic features is crucial to high quality resolution (Ng and Cardie 2002b; Lappin and Leass 1994; Poesio et al. 2004a; Zhou and Su 2004; Bengtson and Roth 2008; Haghighi and Klein 2009). Recent work has also shown the importance of global inference—performing coreference resolution jointly for several or all mentions in a document—rather than greedily disambiguating individual pairs of mentions (Morton 2000; Luo et al. 2004; Yang et al. 2004; Culotta et al. 2007; Yang et al. 2008; Poon and Domingos 2008; Denis and Baldridge 2009; Rahman and Ng 2009; Haghighi and Klein 2010; Cai, Mujdricza-Maydt, and Strube 2011). Modern systems have met this need for carefully designed features and global or entity-centric inference with machine learning approaches to coreference resolution. But machine learning, although powerful, has limitations. Supervised machine learning systems rely on expensive hand-labeled data sets and generalize poorly to new words or domains. Unsupervised systems are increasingly more complex, making them hard to tune"
J13-4004,W12-4507,0,0.0515344,"Missing"
J13-4004,W12-4510,0,0.0210587,"Missing"
J13-4004,C04-1075,0,0.276472,"tem thus offers a new paradigm for combining knowledge in rule-based systems that has implications throughout computational linguistics. 1. Introduction Coreference resolution, the task of finding all expressions that refer to the same entity in a discourse, is important for natural language understanding tasks like summarization, question answering, and information extraction. The long history of coreference resolution has shown that the use of highly precise lexical and syntactic features is crucial to high quality resolution (Ng and Cardie 2002b; Lappin and Leass 1994; Poesio et al. 2004a; Zhou and Su 2004; Bengtson and Roth 2008; Haghighi and Klein 2009). Recent work has also shown the importance of global inference—performing coreference resolution jointly for several or all mentions in a document—rather than greedily disambiguating individual pairs of mentions (Morton 2000; Luo et al. 2004; Yang et al. 2004; Culotta et al. 2007; Yang et al. 2008; Poon and Domingos 2008; Denis and Baldridge 2009; Rahman and Ng 2009; Haghighi and Klein 2010; Cai, Mujdricza-Maydt, and Strube 2011). Modern systems have met this need for carefully designed features and global or entity-centric inference with mach"
J13-4004,D12-1045,1,\N,Missing
J13-4004,J14-4004,0,\N,Missing
J14-3009,P98-1013,0,0.494273,"Missing"
J14-3009,P09-1068,1,0.811703,"applied widely to improve the state of the art in tasks across NLP such as question answering (Shen and Lapata 2007; Surdeanu, Ciaramita, and Zaragoza 2011) and machine translation (Liu and Gildea 2010; Lo et al. 2013). Fillmore’s FrameNet project also led to the development of FrameNets for many other languages including Spanish, German, Japanese, Portuguese, Italian, and Chinese. And in a perhaps appropriate return to the discovery procedures that first inspired Fillmore, modern work has focused on ways to induce semantic roles from corpora without role annotation (Swier and Stevenson 2004; Chambers and Jurafsky 2009, 2011; Lang and Lapata 2014). In addition to his work in semantics, Fillmore had significant contributions to syntax and pragmatics, including the influential Santa Cruz Lectures on Deixis (Fillmore 1975b) and a long-standing research project in developing Construction Grammar, a theory—or perhaps more accurately family of theories—that represented a grammar as a collection of constructions, pairings of meaning, and form (Fillmore, Kay, and O’Connor 1988). He also contributed to the application of linguistics to other disciplines including cognitive science, education, and law. Ackerman, Kay,"
J14-3009,P11-1098,1,0.897849,"Missing"
J14-3009,J14-3006,0,0.0129936,"e of the art in tasks across NLP such as question answering (Shen and Lapata 2007; Surdeanu, Ciaramita, and Zaragoza 2011) and machine translation (Liu and Gildea 2010; Lo et al. 2013). Fillmore’s FrameNet project also led to the development of FrameNets for many other languages including Spanish, German, Japanese, Portuguese, Italian, and Chinese. And in a perhaps appropriate return to the discovery procedures that first inspired Fillmore, modern work has focused on ways to induce semantic roles from corpora without role annotation (Swier and Stevenson 2004; Chambers and Jurafsky 2009, 2011; Lang and Lapata 2014). In addition to his work in semantics, Fillmore had significant contributions to syntax and pragmatics, including the influential Santa Cruz Lectures on Deixis (Fillmore 1975b) and a long-standing research project in developing Construction Grammar, a theory—or perhaps more accurately family of theories—that represented a grammar as a collection of constructions, pairings of meaning, and form (Fillmore, Kay, and O’Connor 1988). He also contributed to the application of linguistics to other disciplines including cognitive science, education, and law. Ackerman, Kay, and O’Connor (2014) offer mo"
J14-3009,C10-1081,0,0.0258461,"ling, first on FrameNet (Gildea and Jurafsky 2000) and then on PropBank data (Gildea and Palmer 2002, inter alia). The problem first addressed in the 1970s by hand-written rules was thus now generally recast as one of supervised machine learning. The resulting plethora of systems for performing automatic semantic role labeling (see the surveys in Palmer, Gildea, and Xue (2010) and M`arquez et al. (2008)) have been applied widely to improve the state of the art in tasks across NLP such as question answering (Shen and Lapata 2007; Surdeanu, Ciaramita, and Zaragoza 2011) and machine translation (Liu and Gildea 2010; Lo et al. 2013). Fillmore’s FrameNet project also led to the development of FrameNets for many other languages including Spanish, German, Japanese, Portuguese, Italian, and Chinese. And in a perhaps appropriate return to the discovery procedures that first inspired Fillmore, modern work has focused on ways to induce semantic roles from corpora without role annotation (Swier and Stevenson 2004; Chambers and Jurafsky 2009, 2011; Lang and Lapata 2014). In addition to his work in semantics, Fillmore had significant contributions to syntax and pragmatics, including the influential Santa Cruz Lect"
J14-3009,P13-2067,0,0.018231,"et (Gildea and Jurafsky 2000) and then on PropBank data (Gildea and Palmer 2002, inter alia). The problem first addressed in the 1970s by hand-written rules was thus now generally recast as one of supervised machine learning. The resulting plethora of systems for performing automatic semantic role labeling (see the surveys in Palmer, Gildea, and Xue (2010) and M`arquez et al. (2008)) have been applied widely to improve the state of the art in tasks across NLP such as question answering (Shen and Lapata 2007; Surdeanu, Ciaramita, and Zaragoza 2011) and machine translation (Liu and Gildea 2010; Lo et al. 2013). Fillmore’s FrameNet project also led to the development of FrameNets for many other languages including Spanish, German, Japanese, Portuguese, Italian, and Chinese. And in a perhaps appropriate return to the discovery procedures that first inspired Fillmore, modern work has focused on ways to induce semantic roles from corpora without role annotation (Swier and Stevenson 2004; Chambers and Jurafsky 2009, 2011; Lang and Lapata 2014). In addition to his work in semantics, Fillmore had significant contributions to syntax and pragmatics, including the influential Santa Cruz Lectures on Deixis (F"
J14-3009,J08-2001,0,0.0826346,"Missing"
J14-3009,J05-1004,0,0.192175,"Missing"
J14-3009,D07-1002,0,0.0302723,"sbury, and Gildea 2005), led to a revival of automatic approaches to semantic role labeling, first on FrameNet (Gildea and Jurafsky 2000) and then on PropBank data (Gildea and Palmer 2002, inter alia). The problem first addressed in the 1970s by hand-written rules was thus now generally recast as one of supervised machine learning. The resulting plethora of systems for performing automatic semantic role labeling (see the surveys in Palmer, Gildea, and Xue (2010) and M`arquez et al. (2008)) have been applied widely to improve the state of the art in tasks across NLP such as question answering (Shen and Lapata 2007; Surdeanu, Ciaramita, and Zaragoza 2011) and machine translation (Liu and Gildea 2010; Lo et al. 2013). Fillmore’s FrameNet project also led to the development of FrameNets for many other languages including Spanish, German, Japanese, Portuguese, Italian, and Chinese. And in a perhaps appropriate return to the discovery procedures that first inspired Fillmore, modern work has focused on ways to induce semantic roles from corpora without role annotation (Swier and Stevenson 2004; Chambers and Jurafsky 2009, 2011; Lang and Lapata 2014). In addition to his work in semantics, Fillmore had signifi"
J14-3009,J11-2003,0,0.0366482,"Missing"
L18-1182,P06-4018,0,0.138422,"ogically analyzing Japanese and English captions and discarding all but the content words, then (2) stemming these content words, (3) translating the Japanese to English with simple dictionary lookups, (4) averaging the GLoVE vectors for each caption’s words, and (5) computing the cosine similarity between these vector representations. We used the Rakuten and JUMAN morphological analyzers to extract content words from Japanese captions, and the Stanford POS tagger for English (Hagiwara and Sekine, 2014; Manning et al., 2014). We used JUMANPP (Morita et al., 2015) and NLTK to stem these words (Bird, 2006), and JMdict/EDICT to map Japanese words to their English equivalents (Breen, 2004; Matsumoto et al., 1991). Phrases without translations were skipped. Note that our method introduces a bias in the phrase pairs of resultant matches, namely those pairs that would score highly under a lexicon, but we assume that JMdict/EDICT is nearcomplete with respect to common content words. 4.3. Filtering The document- and caption-matching procedures outlined above produced 27,716,868 matches between English and Figure 3: JESC exhibits a right-skewed sentence length distribution. 83 English and 114 Japanese"
L18-1182,D17-1151,1,0.875563,"Missing"
L18-1182,chu-etal-2014-constructing,0,0.171932,"s contents and evaluate its quality using human experts and baseline machine translation (MT) systems. Keywords: parallel corpus, asian languages, machine translation 1. Introduction time-based alignment feasible (Tiedemann, 2008). There is a strong need for large parallel corpora from new domains. Modern machine translation (MT) systems are fundamentally constrained by the availability and quantity of parallel corpora. Apart from the exceptions of EnglishArabic, English-Chinese, and several European pairs, parallel corpora remain a scarce resource due to the high cost of manual construction (Chu et al., 2014). Furthermore, despite promising work in domain adaptation, MT systems struggle to generalize to new domains that are disparate from their training data (Pryzant et al., 2017). This need for large, novel-domain data is especially evident in the resource-poor Japanese-English (JA-EN) language pair. Only two large (>1M phrase pairs) and free datasets exist for this language pair (Neubig, 2017; Tiedemann, 2017; Moses, 2017). The first is called ASPEC. It consists of 3M examples and it originates from scientific papers, a highly formalized and written domain (all other JA-EN datasets have similar"
L18-1182,C14-2009,0,0.025458,"e the highest-scoring match of this window. We score the quality of an English-Japanese caption pairing by (1) morphologically analyzing Japanese and English captions and discarding all but the content words, then (2) stemming these content words, (3) translating the Japanese to English with simple dictionary lookups, (4) averaging the GLoVE vectors for each caption’s words, and (5) computing the cosine similarity between these vector representations. We used the Rakuten and JUMAN morphological analyzers to extract content words from Japanese captions, and the Stanford POS tagger for English (Hagiwara and Sekine, 2014; Manning et al., 2014). We used JUMANPP (Morita et al., 2015) and NLTK to stem these words (Bird, 2006), and JMdict/EDICT to map Japanese words to their English equivalents (Breen, 2004; Matsumoto et al., 1991). Phrases without translations were skipped. Note that our method introduces a bias in the phrase pairs of resultant matches, namely those pairs that would score highly under a lexicon, but we assume that JMdict/EDICT is nearcomplete with respect to common content words. 4.3. Filtering The document- and caption-matching procedures outlined above produced 27,716,868 matches between Engli"
L18-1182,D09-1129,0,0.0404481,"t Correction Next, we preprocessed the English documents by performing syntax correction on each caption. Many fanmade subtitles were created by non-native English speakers and as such contained typographical and spelling mistakes. We developed a laplace-smoothed statistical error model P (w|w∗ ) that scores the probability of a word w∗ being misspelled as w. This model was trained by observing relative misspelling frequencies on the Birkbeck corpus (Mitton, 1985). We then developed two additional laplace-smoothed frequency-based models using unigrams and bigrams from Google’s Web 1T N-grams (Islam and Inkpen, 2009). These are language models that score the prior probability of n-gram occurrence, P (w), and the transition probability P (wi |wi−1 ). We used a smoothing factor of α = 1 for all of these models. Next, for each possibly misspelled token ti of a caption c, we performed depth-4 uniform cost search on the space of edits to produce candidate replacements t∗i . Armed with the error model P (ti |t∗i ) and language model P (t∗i )P (t∗i |ti−1 ), we scored the probability of each candidate by applying Bayes rule, similar to (Lison and Tiedemann, 2016): 2 http://www.yaml.org/ Cross-lingual Alignment On"
L18-1182,L16-1147,0,0.369203,"is especially evident in the resource-poor Japanese-English (JA-EN) language pair. Only two large (>1M phrase pairs) and free datasets exist for this language pair (Neubig, 2017; Tiedemann, 2017; Moses, 2017). The first is called ASPEC. It consists of 3M examples and it originates from scientific papers, a highly formalized and written domain (all other JA-EN datasets have similar language) (Nakazawa et al., 2016). The other, OpenSubtitles, is a multi-language dataset of aligned subtitles authored by professional translators; the JA-EN subset of these data contains approximately 1M examples (Lison and Tiedemann, 2016). OpenSubtitles is to the best of these authors knowledge the only parallel corpus to cover the unrepresented domains of conversational speech and informal writing. This dearth of large-scale and informal data is especially problematic because colloquial Japanese has significant structural characteristics which can preclude cross-domain translation (Tsujimura, 2013). We hope to alleviate this problem by building off the work of (Lison and Tiedemann, 2016) to construct a larger corpus that incorporates the vast number of unofficial and fan-made subtitles on the web. Subtitles are an excellent s"
L18-1182,D15-1166,0,0.130338,"Missing"
L18-1182,P14-5010,0,0.00460209,"of this window. We score the quality of an English-Japanese caption pairing by (1) morphologically analyzing Japanese and English captions and discarding all but the content words, then (2) stemming these content words, (3) translating the Japanese to English with simple dictionary lookups, (4) averaging the GLoVE vectors for each caption’s words, and (5) computing the cosine similarity between these vector representations. We used the Rakuten and JUMAN morphological analyzers to extract content words from Japanese captions, and the Stanford POS tagger for English (Hagiwara and Sekine, 2014; Manning et al., 2014). We used JUMANPP (Morita et al., 2015) and NLTK to stem these words (Bird, 2006), and JMdict/EDICT to map Japanese words to their English equivalents (Breen, 2004; Matsumoto et al., 1991). Phrases without translations were skipped. Note that our method introduces a bias in the phrase pairs of resultant matches, namely those pairs that would score highly under a lexicon, but we assume that JMdict/EDICT is nearcomplete with respect to common content words. 4.3. Filtering The document- and caption-matching procedures outlined above produced 27,716,868 matches between English and Figure 3: JESC e"
L18-1182,D15-1276,0,0.0277796,"an English-Japanese caption pairing by (1) morphologically analyzing Japanese and English captions and discarding all but the content words, then (2) stemming these content words, (3) translating the Japanese to English with simple dictionary lookups, (4) averaging the GLoVE vectors for each caption’s words, and (5) computing the cosine similarity between these vector representations. We used the Rakuten and JUMAN morphological analyzers to extract content words from Japanese captions, and the Stanford POS tagger for English (Hagiwara and Sekine, 2014; Manning et al., 2014). We used JUMANPP (Morita et al., 2015) and NLTK to stem these words (Bird, 2006), and JMdict/EDICT to map Japanese words to their English equivalents (Breen, 2004; Matsumoto et al., 1991). Phrases without translations were skipped. Note that our method introduces a bias in the phrase pairs of resultant matches, namely those pairs that would score highly under a lexicon, but we assume that JMdict/EDICT is nearcomplete with respect to common content words. 4.3. Filtering The document- and caption-matching procedures outlined above produced 27,716,868 matches between English and Figure 3: JESC exhibits a right-skewed sentence length"
L18-1182,L16-1350,0,0.0405928,"re, despite promising work in domain adaptation, MT systems struggle to generalize to new domains that are disparate from their training data (Pryzant et al., 2017). This need for large, novel-domain data is especially evident in the resource-poor Japanese-English (JA-EN) language pair. Only two large (>1M phrase pairs) and free datasets exist for this language pair (Neubig, 2017; Tiedemann, 2017; Moses, 2017). The first is called ASPEC. It consists of 3M examples and it originates from scientific papers, a highly formalized and written domain (all other JA-EN datasets have similar language) (Nakazawa et al., 2016). The other, OpenSubtitles, is a multi-language dataset of aligned subtitles authored by professional translators; the JA-EN subset of these data contains approximately 1M examples (Lison and Tiedemann, 2016). OpenSubtitles is to the best of these authors knowledge the only parallel corpus to cover the unrepresented domains of conversational speech and informal writing. This dearth of large-scale and informal data is especially problematic because colloquial Japanese has significant structural characteristics which can preclude cross-domain translation (Tsujimura, 2013). We hope to alleviate t"
L18-1182,P02-1040,0,0.112074,"Missing"
L18-1182,W17-4712,1,0.863002,"ntroduction time-based alignment feasible (Tiedemann, 2008). There is a strong need for large parallel corpora from new domains. Modern machine translation (MT) systems are fundamentally constrained by the availability and quantity of parallel corpora. Apart from the exceptions of EnglishArabic, English-Chinese, and several European pairs, parallel corpora remain a scarce resource due to the high cost of manual construction (Chu et al., 2014). Furthermore, despite promising work in domain adaptation, MT systems struggle to generalize to new domains that are disparate from their training data (Pryzant et al., 2017). This need for large, novel-domain data is especially evident in the resource-poor Japanese-English (JA-EN) language pair. Only two large (>1M phrase pairs) and free datasets exist for this language pair (Neubig, 2017; Tiedemann, 2017; Moses, 2017). The first is called ASPEC. It consists of 3M examples and it originates from scientific papers, a highly formalized and written domain (all other JA-EN datasets have similar language) (Nakazawa et al., 2016). The other, OpenSubtitles, is a multi-language dataset of aligned subtitles authored by professional translators; the JA-EN subset of these d"
L18-1182,tiedemann-2008-synchronizing,0,0.038135,"ented domain of conversational dialogue. It consists of more than 3.2 million examples, making it the largest freely available dataset of its kind. The corpus was assembled by crawling and aligning subtitles found on the web. The assembly process incorporates a number of novel preprocessing elements to ensure high monolingual fluency and accurate bilingual alignments. We summarize its contents and evaluate its quality using human experts and baseline machine translation (MT) systems. Keywords: parallel corpus, asian languages, machine translation 1. Introduction time-based alignment feasible (Tiedemann, 2008). There is a strong need for large parallel corpora from new domains. Modern machine translation (MT) systems are fundamentally constrained by the availability and quantity of parallel corpora. Apart from the exceptions of EnglishArabic, English-Chinese, and several European pairs, parallel corpora remain a scarce resource due to the high cost of manual construction (Chu et al., 2014). Furthermore, despite promising work in domain adaptation, MT systems struggle to generalize to new domains that are disparate from their training data (Pryzant et al., 2017). This need for large, novel-domain da"
L18-1182,L16-1559,0,0.314739,"ally evident in the resource-poor Japanese-English (JA-EN) language pair. Only two large (>1M phrase pairs) and free datasets exist for this language pair (Neubig, 2017; Tiedemann, 2017; Moses, 2017). The first is called ASPEC. It consists of 3M examples and it originates from scientific papers, a highly formalized and written domain (all other JA-EN datasets have similar language) (Nakazawa et al., 2016). The other, OpenSubtitles, is a multi-language dataset of aligned subtitles authored by professional translators; the JA-EN subset of these data contains approximately 1M examples (Lison and Tiedemann, 2016). OpenSubtitles is to the best of these authors knowledge the only parallel corpus to cover the unrepresented domains of conversational speech and informal writing. This dearth of large-scale and informal data is especially problematic because colloquial Japanese has significant structural characteristics which can preclude cross-domain translation (Tsujimura, 2013). We hope to alleviate this problem by building off the work of (Lison and Tiedemann, 2016) to construct a larger corpus that incorporates the vast number of unofficial and fan-made subtitles on the web. Subtitles are an excellent s"
L18-1182,2007.mtsummit-papers.63,0,0.0208494,"Missing"
L18-1445,N15-1084,0,0.0182101,"social issues. For instance, women1 journalists reach a smaller audience in terms of social media impressions (Matias and Wallach, 2012), and traditional gender stereotypes and unbalanced gender representation occur even in contemporary stories and movies (Fast et al., 2016; Sap et al., 2017). Large datasets are particularly of use in this context due to the complex nature of differential responses to gender. However, previous computational work on language and gender has focused mainly on language about or portraying persons of a particular gender (Wagner et al., 2015; Flekova et al., 2016; Agarwal et al., 2015). We thus present a large multi-genre dataset of online communication to enable research in a category of gender difference understudied in computational work: responses to gender in language. These include posts and talks labeled for the gender of the source,2 along with comments given in response to the source texts. We collect such data from a variety of contexts, including: • Facebook (Politicians): Responses to Facebook posts from members of the U.S. House and Senate • Facebook (Public Figures): Responses to Facebook posts from other public figures, e.g., television hosts, journalists, an"
L18-1445,P06-1005,0,0.0356371,"tion are all public; however, to protect the anonymity of Facebook users in our dataset we remove all identifying user information as well as Facebook-internal information such as User IDs and Post IDs, replacing these with randomized ID numbers. Therefore users whose comments appear multiple times in our dataset may be compared, but without revealing their identity. We also only report commenter first names, since this is less identifying but still allows for running genderidentification algorithms. As a baseline for convenience we provide masculine/feminine ratios for these first names from Bergsma and Lin (2006). We collect posts and their associated top-level comments for the categories of speakers described below. In each case we find the page for the speaker with a novel method for finding gender-labeled speakers from Wikipedia. Specifically, our method takes as input a Wikipedia category page such as https://en.wikipedia.org/wiki/ Category:American_female_tennis_players, and for each name listed runs a search for public pages using Facebook’s Graph API. If an exact match for the name appears in the top three results, and the category of the page matches a relevant category (for instance, ”Public"
L18-1445,P05-1054,0,0.231484,"ource and responder may know one another and have an ongoing interaction afterwards. 2. Responses to Gender Here we aim to encourage research on responses to gender. Contrasting with language about or portraying a given gender which address abstract representations of social categories, responses to gender are directed towards an individual person. We know that social characteristics of the addressee influence linguistic behavior; existing computational work has shown, for instance, that the gender of the interlocutor influences lexical choices of a speaker in spoken and written interactions (Boulis and Ostendorf, 2005; Jurgens et al., 2017; Prabhakaran and Rambow, 2017). Looking at responses to gender also allows us to consider the important social issue of gender bias. Since important forms of bias (e.g., dehumanization or treating a person as their social category) often happen at the level of individual responses, responses to gender are an understudied but critical lens for studying gender bias. The issue is related to that of abusive language (Xu et al., 2012; Clarke and Grieve, 2017), though often gender bias takes a less overt form than straightforward abuse. Social issues like gender bias are often"
L18-1445,W17-3001,0,0.0230235,"nce, that the gender of the interlocutor influences lexical choices of a speaker in spoken and written interactions (Boulis and Ostendorf, 2005; Jurgens et al., 2017; Prabhakaran and Rambow, 2017). Looking at responses to gender also allows us to consider the important social issue of gender bias. Since important forms of bias (e.g., dehumanization or treating a person as their social category) often happen at the level of individual responses, responses to gender are an understudied but critical lens for studying gender bias. The issue is related to that of abusive language (Xu et al., 2012; Clarke and Grieve, 2017), though often gender bias takes a less overt form than straightforward abuse. Social issues like gender bias are often not just about hostility but also behaviors such as stereotype-reinforcing benevolence (Eagly and Mladinic, 1989; Glick and Fiske, 1996; 2814 B ROADCAST P ERSONAL Dataset Facebook (Politicians) Facebook (Public Figures) TED Talks Fitocracy Reddit Source Individuals M: 306 W: 96 M: 41 W: 64 M: 1,071 W: 349 M: 52,432 W: 47,498 M: 19,010 W: 11,116 Source Text Count 399,037 117,811 1,671 318,535 1,453,512 Response Count 13,866,507 10,667,500 190,425 318,535 1,453,512 Response Wor"
L18-1445,P16-1080,0,0.0474796,"Missing"
L18-1445,W17-2902,0,0.116166,"about hostility but also behaviors such as stereotype-reinforcing benevolence (Eagly and Mladinic, 1989; Glick and Fiske, 1996; 2814 B ROADCAST P ERSONAL Dataset Facebook (Politicians) Facebook (Public Figures) TED Talks Fitocracy Reddit Source Individuals M: 306 W: 96 M: 41 W: 64 M: 1,071 W: 349 M: 52,432 W: 47,498 M: 19,010 W: 11,116 Source Text Count 399,037 117,811 1,671 318,535 1,453,512 Response Count 13,866,507 10,667,500 190,425 318,535 1,453,512 Response Word Count 376,114,950 123,753,913 15,549,984 6,606,087 44,537,612 Table 1: Basic statistics about the subcorpora within RtGender. Jha and Mamidi, 2017). Nevertheless, biased responses to social categories like gender can lead to marginalization (Sue, 2010) and negatively impact a person’s self-esteem and ability through mechanisms such as stereotype threat (Spencer et al., 1999). Perhaps most related to our work, Fu et al. (2016) analyze questions directed at men and women tennis players, finding that questions directed at men tend to be more about the game while questions directed at women are more likely to stray to topics about their appearance and off-court relationships. Tsou et al. (2014) similarly find comments on TED talks are more l"
L18-1445,D15-1130,0,0.0450648,"Missing"
L18-1445,D17-1247,0,0.0271873,"nstruction of identity and social categories like gender; social issues such as gender bias, in turn, often take form in language. Linguistic datasets have been used both to debunk gender-biased myths — for example, contrary to stereotype women are not actually more talkative than men (Mehl et al., 2007) — and to identify social issues. For instance, women1 journalists reach a smaller audience in terms of social media impressions (Matias and Wallach, 2012), and traditional gender stereotypes and unbalanced gender representation occur even in contemporary stories and movies (Fast et al., 2016; Sap et al., 2017). Large datasets are particularly of use in this context due to the complex nature of differential responses to gender. However, previous computational work on language and gender has focused mainly on language about or portraying persons of a particular gender (Wagner et al., 2015; Flekova et al., 2016; Agarwal et al., 2015). We thus present a large multi-genre dataset of online communication to enable research in a category of gender difference understudied in computational work: responses to gender in language. These include posts and talks labeled for the gender of the source,2 along with"
L18-1445,D13-1170,0,0.00495629,"Missing"
L18-1445,N12-1084,0,0.027848,"shown, for instance, that the gender of the interlocutor influences lexical choices of a speaker in spoken and written interactions (Boulis and Ostendorf, 2005; Jurgens et al., 2017; Prabhakaran and Rambow, 2017). Looking at responses to gender also allows us to consider the important social issue of gender bias. Since important forms of bias (e.g., dehumanization or treating a person as their social category) often happen at the level of individual responses, responses to gender are an understudied but critical lens for studying gender bias. The issue is related to that of abusive language (Xu et al., 2012; Clarke and Grieve, 2017), though often gender bias takes a less overt form than straightforward abuse. Social issues like gender bias are often not just about hostility but also behaviors such as stereotype-reinforcing benevolence (Eagly and Mladinic, 1989; Glick and Fiske, 1996; 2814 B ROADCAST P ERSONAL Dataset Facebook (Politicians) Facebook (Public Figures) TED Talks Fitocracy Reddit Source Individuals M: 306 W: 96 M: 41 W: 64 M: 1,071 W: 349 M: 52,432 W: 47,498 M: 19,010 W: 11,116 Source Text Count 399,037 117,811 1,671 318,535 1,453,512 Response Count 13,866,507 10,667,500 190,425 318,"
lee-etal-2014-importance,P13-1086,0,\N,Missing
lee-etal-2014-importance,P12-2018,0,\N,Missing
lee-etal-2014-importance,W02-1011,0,\N,Missing
lee-etal-2014-importance,D11-1121,0,\N,Missing
lee-etal-2014-importance,N09-1031,0,\N,Missing
N04-1030,A00-2031,0,0.0077464,"Missing"
N04-1030,P01-1017,0,0.09119,"Missing"
N04-1030,W03-1006,0,0.234797,"Missing"
N04-1030,C92-3145,0,0.0180452,"Missing"
N04-1030,N03-2008,0,0.0468064,"Missing"
N04-1030,W03-1008,0,0.271799,"Missing"
N04-1030,P00-1065,0,0.0435523,"Missing"
N04-1030,J02-3001,0,0.853591,"Missing"
N04-1030,P02-1031,0,0.133044,"Missing"
N04-1030,N03-2009,1,0.474549,"Missing"
N04-1030,W00-0730,0,0.418325,"Missing"
N04-1030,N01-1025,0,0.381966,"Missing"
N04-1030,P98-2127,0,0.297893,"Missing"
N04-1030,H94-1020,0,0.017141,"Missing"
N04-1030,P03-1002,0,0.75586,"Missing"
N04-1030,J03-4003,0,\N,Missing
N04-1030,C98-2122,0,\N,Missing
N07-1002,P04-1086,0,0.537501,"Missing"
N07-1002,nissim-etal-2004-annotation,0,0.211469,"nd contrast information) to investigate the relative usefulness of both linguistic and shallow features, as well as how well different features combine with each other. 9 Proceedings of NAACL HLT 2007, pages 9–16, c Rochester, NY, April 2007. 2007 Association for Computational Linguistics 2 Data and features For our experiments we use 12 Switchboard (Godfrey et al., 1992) conversations, 14,555 tokens in total. Each word was manually labeled for presence or absence of pitch accent1 , as well as additional features including information status (or givenness), contrast and animacy distinctions, (Nissim et al., 2004; Calhoun et al., 2005; Zaenen et al., 2004), features that linguistic literature suggests are predictive of prominence (Bolinger, 1961; Chafe, 1976). All of the features described in detail below have been shown to have statistically significant correlation with prominence (Brenier et al., 2006). Information status The information status (IS), or givenness, of discourse entities is important for choosing appropriate reference form (Prince, 1992; Gundel et al., 1993) and possibly plays a role in prominence decisions as well (Brown, 1983). No previous studies have examined the usefulness of inf"
N07-1002,W06-1612,0,0.117177,"Missing"
N07-1002,P00-1030,0,0.0580454,"Missing"
N07-1002,W99-0619,0,0.0323707,"res. theyold have all the WATERnew theyold WANT. theyold can ACTUALLY PUMP waterold. 1 Introduction Being able to predict the prominence or pitch accent status of a word in conversational speech is important for implementing text-to-speech in dialog systems, as well as in detection of prosody in conversational speech recognition. Previous investigations of prominence prediction from text have primarily relied on robust surface features with some deeper information structure features. Surface features like a word’s part-of-speech (POS) (Hirschberg, 1993) and its unigram and bigram probability (Pan and McKeown, 1999; Pan and 0 Thanks to the Edinburgh-Stanford Link and ONR (MURI award N000140510388) for generous support. While previous models have attempted to capture global properties of words (via POS or unigram probability), they have not in general used word identity as a predictive feature, assuming either that current supervised training sets would be too small or that word identity would not be robust across genres (Pan et al., 2002). In this paper, we show a way to capture word identity in a feature, accent ratio, that works well with current small supervised training sets, and is robust to genre"
N07-1002,W05-0307,1,0.918816,"on) to investigate the relative usefulness of both linguistic and shallow features, as well as how well different features combine with each other. 9 Proceedings of NAACL HLT 2007, pages 9–16, c Rochester, NY, April 2007. 2007 Association for Computational Linguistics 2 Data and features For our experiments we use 12 Switchboard (Godfrey et al., 1992) conversations, 14,555 tokens in total. Each word was manually labeled for presence or absence of pitch accent1 , as well as additional features including information status (or givenness), contrast and animacy distinctions, (Nissim et al., 2004; Calhoun et al., 2005; Zaenen et al., 2004), features that linguistic literature suggests are predictive of prominence (Bolinger, 1961; Chafe, 1976). All of the features described in detail below have been shown to have statistically significant correlation with prominence (Brenier et al., 2006). Information status The information status (IS), or givenness, of discourse entities is important for choosing appropriate reference form (Prince, 1992; Gundel et al., 1993) and possibly plays a role in prominence decisions as well (Brown, 1983). No previous studies have examined the usefulness of information status in nat"
N07-1002,W04-0216,0,\N,Missing
N09-1072,W98-0319,0,0.201975,"otal time for a speaker for a conversation side, in seconds RATE OF number of words in turn divided by SPEECH duration of turn in seconds, averaged over turns Table 1: Prosodic features for each conversation side, extracted using Praat from the hand-segmented turns of each side. tive or story-telling behavior, and Metadate, for discussion about the speed-date itself. The features are summarized in Table 2. 4.3 Dialogue Act and Adjacency Pair Features A number of discourse features were extracted, drawing from the conversation analysis, disfluency and dialog act literature (Sacks et al., 1974; Jurafsky et al., 1998; Jurafsky, 2001). While discourse features are clearly important for extracting social meaning, previous work on social meaning has met with less success in use of such features (with the exception of the ‘critical segments’ work of (Enos et al., 2007)), presumably because discourse feaTOTAL WORDS PAST TENSE M ETADATE YOU WE I A SSENT S WEAR I NSIGHT A NGER N EGEMOTION S EXUAL I NGEST total number of words uses of past tense auxiliaries was, were, had horn, date, bell, survey, speed, form, questionnaire, rushed, study, research you, you’d, you’ll, your, you’re, yours, you’ve (not counting you"
N09-1072,P08-1020,0,0.0403728,"ing conversational style, and will play an important role in creating more natural dialogue agents (Pentland, 2005; Nass and Brave, 2005; Brave et al., 2005). 638 Cues for social meaning permeate speech at every level of linguistic structure. Acoustic cues such as low and high F0 or energy and spectral tilt are important in detecting emotions such as annoyance, anger, sadness, or boredom (Ang et al., 2002; Lee and Narayanan, 2002; Liscombe et al., 2003), speaker characteristics such as charisma (Rosenberg and Hirschberg, 2005), or personality features like extroversion (Mairesse et al., 2007; Mairesse and Walker, 2008). Lexical cues to social meaning abound. Speakers with links to depression or speakers who are under stress use more first person singular pronouns (Rude et al., 2004; Pennebaker and Lay, 2002; Cohn et al., 2004), positive emotion words are cues to agreeableness (Mairesse et al., 2007), and negative emotion words are useful cues to deceptive speech (Newman et al., 2003). The number of words in a sentence can be a useful feature for extroverted personality (Mairesse et al., 2007). Finally, dialog features such as the presence of disfluencies can inform listeners about speakers’ problems in utte"
N15-1038,P13-2121,0,0.0197951,"s compared with standard stochastic gradient descent for deep recurrent neural network training (Sutskever et al., 2013). The NAG algorithm uses a step size of 10−5 and a momentum of 0.95. After each epoch we divide the learning rate by 1.3. Training for 10 epochs on a single GTX 570 GPU takes approximately one week. 4.3 Character Language Model Training The Switchboard corpus transcripts alone are too small to build CLMs which accurately model general orthography in English. To learn how to spell words more generally we train our CLMs using a corpus of 31 billion words gathered from the web (Heafield et al., 2013). Our language models use sentence start and end tokens, &lt;s&gt; and &lt;/s&gt;, as well as a &lt;null&gt; token for cases when our context window extends past the start of a sentence. We build 5-gram and 7-gram CLMs with modified Kneser-Ney smoothing using the KenLM toolkit (Heafield et al., 2013). Building traditional n-gram CLMs is for n &gt; 7 becomes increasingly difficult as the model free parameters and memory footprint become unwieldy. Our 7-gram CLM is already 21GB; we were not able to build higher order n-gram models to compare against our neural network CLMs. Following work illustrating the effectiven"
N16-1082,E14-1049,0,0.00880291,"tasets and the adopted neural models in Section 3. Different visualization strategies and correspondent analytical results are presented 681 Proceedings of NAACL-HLT 2016, pages 681–691, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics separately in Section 4,5,6, followed by a brief conclusion. 2 A Brief Review of Neural Visualization Similarity is commonly visualized graphically, generally by projecting the embedding space into two dimensions and observing that similar words tend to be clustered together (e.g., Elman (1989), Ji and Eisenstein (2014), Faruqui and Dyer (2014)). (Karpathy et al., 2015) attempts to interpret recurrent neural models from a statical point of view and does deeply touch compositionally of meanings. Other relevant attempts include (Fyshe et al., 2015; Faruqui et al., 2015). Methods for interpreting and visualizing neural models have been much more significantly explored in vision, especially for Convolutional Neural Networks (CNNs or ConvNets) (Krizhevsky et al., 2012), multi-layer neural networks in which the original matrix of image pixels is convolved and pooled as it is passed on to hidden layers. ConvNet visualizing techniques consi"
N16-1082,P15-1144,0,0.130602,"2016 Association for Computational Linguistics separately in Section 4,5,6, followed by a brief conclusion. 2 A Brief Review of Neural Visualization Similarity is commonly visualized graphically, generally by projecting the embedding space into two dimensions and observing that similar words tend to be clustered together (e.g., Elman (1989), Ji and Eisenstein (2014), Faruqui and Dyer (2014)). (Karpathy et al., 2015) attempts to interpret recurrent neural models from a statical point of view and does deeply touch compositionally of meanings. Other relevant attempts include (Fyshe et al., 2015; Faruqui et al., 2015). Methods for interpreting and visualizing neural models have been much more significantly explored in vision, especially for Convolutional Neural Networks (CNNs or ConvNets) (Krizhevsky et al., 2012), multi-layer neural networks in which the original matrix of image pixels is convolved and pooled as it is passed on to hidden layers. ConvNet visualizing techniques consist mainly in mapping the different layers of the network (or other features like SIFT (Lowe, 2004) and HOG (Dalal and Triggs, 2005)) back to the initial image input, thus capturing the humaninterpretable information they represe"
N16-1082,N15-1004,0,0.0189816,", June 12-17, 2016. 2016 Association for Computational Linguistics separately in Section 4,5,6, followed by a brief conclusion. 2 A Brief Review of Neural Visualization Similarity is commonly visualized graphically, generally by projecting the embedding space into two dimensions and observing that similar words tend to be clustered together (e.g., Elman (1989), Ji and Eisenstein (2014), Faruqui and Dyer (2014)). (Karpathy et al., 2015) attempts to interpret recurrent neural models from a statical point of view and does deeply touch compositionally of meanings. Other relevant attempts include (Fyshe et al., 2015; Faruqui et al., 2015). Methods for interpreting and visualizing neural models have been much more significantly explored in vision, especially for Convolutional Neural Networks (CNNs or ConvNets) (Krizhevsky et al., 2012), multi-layer neural networks in which the original matrix of image pixels is convolved and pooled as it is passed on to hidden layers. ConvNet visualizing techniques consist mainly in mapping the different layers of the network (or other features like SIFT (Lowe, 2004) and HOG (Dalal and Triggs, 2005)) back to the initial image input, thus capturing the humaninterpretable i"
N16-1082,P14-1002,0,0.00988943,"this work. We describe datasets and the adopted neural models in Section 3. Different visualization strategies and correspondent analytical results are presented 681 Proceedings of NAACL-HLT 2016, pages 681–691, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics separately in Section 4,5,6, followed by a brief conclusion. 2 A Brief Review of Neural Visualization Similarity is commonly visualized graphically, generally by projecting the embedding space into two dimensions and observing that similar words tend to be clustered together (e.g., Elman (1989), Ji and Eisenstein (2014), Faruqui and Dyer (2014)). (Karpathy et al., 2015) attempts to interpret recurrent neural models from a statical point of view and does deeply touch compositionally of meanings. Other relevant attempts include (Fyshe et al., 2015; Faruqui et al., 2015). Methods for interpreting and visualizing neural models have been much more significantly explored in vision, especially for Convolutional Neural Networks (CNNs or ConvNets) (Krizhevsky et al., 2012), multi-layer neural networks in which the original matrix of image pixels is convolved and pooled as it is passed on to hidden layers. ConvNet vis"
N16-1082,C12-1118,0,0.0188047,"ire the work we present in this paper, there are fundamental differences between vision and NLP. In NLP words function as basic units, and hence (word) vectors rather than single pixels are the basic units. Sequences of words (e.g., phrases and sentences) are also presented in a more structured way than arrangements of pixels. In parallel to our research, independent researches (Karpathy et al., 2015) have been conducted to explore similar direction from an error-analysis point of view, by analyzing predictions and errors from a recurrent neural models. Other distantly relevant works include: Murphy et al. (2012; Fyshe et al. (2015) used an manual task to quantify the interpretability of semantic dimensions by presetting human users with a list of words and ask them to choose the one that does not belong to the list. Faruqui et al. (2015). Similar strategy is adopted in (Faruqui et al., 2015) by extracting top-ranked words in each vector dimension. 3 Datasets and Neural Models We explored two datasets on which neural models are trained, one of which is of relatively small scale and the other of large scale. 3.1 Stanford Sentiment Treebank Stanford Sentiment Treebank is a benchmark dataset widely used"
N16-1082,D13-1170,0,0.0729884,"atively small scale and the other of large scale. 3.1 Stanford Sentiment Treebank Stanford Sentiment Treebank is a benchmark dataset widely used for neural model evaluations. The dataset contains gold-standard sentiment labels for every parse tree constituent, from sentences to phrases to individual words, for 215,154 phrases in 11,855 sentences. The task is to perform both finegrained (very positive, positive, neutral, negative and very negative) and coarse-grained (positive vs negative) classification at both the phrase and sentence level. For more details about the dataset, please refer to Socher et al. (2013). While many studies on this dataset use recursive parse-tree models, in this work we employ only standard sequence models (RNNs and LSTMs) since these are the most widely used current neural models, and sequential visualization is more straightforward. We therefore first transform each parse tree node to a sequence of tokens. The sequence is first mapped to a phrase/sentence representation and fed into a softmax classifier. Phrase/sentence representations are built with the following three models: Standard Recurrent Sequence with TANH activation functions, LSTMs and Bidirectional LSTMs. For d"
N16-1082,P15-1002,0,\N,Missing
N16-1082,N16-1014,1,\N,Missing
N18-1057,D16-1161,0,0.540927,"test set. We believe this is due to a mismatch between the CoNLL 2013 dev and 2014 Experiments To determine the effectiveness of the described noising schemes, we synthesize additional data using each and evaluate the performance of models trainined using the additional data on two benchmarks. Datasets For training our sequence transduction models, we combine the publicly available English Lang-8 dataset, a parallel corpus collected from a language learner forum, with training data from the CoNLL 2014 challenge (Mizumoto et al., 2011; Ng et al., 2014). We refer to this as the “base” dataset. Junczys-Dowmunt and Grundkiewicz (2016) additionally scraped 3.3M pairs of sentences from Lang-8. Although this expanded dataset, which we call the “expanded” dataset, is not typically used when comparing performance 623 Method Dev (no LM) Dev Test P R F0.5 P R F0.5 P R F0.5 none clean token reverse rank top random 50.7 56.1 49.7 53.1 51.3 49.1 50.0 10.5 9.4 11.9 13.0 12.3 17.4 17.9 28.7 28.1 30.4 32.8 31.4 36.0 36.8 48.4 47.5 47.7 50.5 51.0 47.7 48.9 17.2 16.9 18.7 19.1 18.3 23.9 23.0 35.5 34.8 36.4 38.0 37.6 39.8 39.9 52.7 52.3 51.4 54.7 54.3 50.9 54.2 27.5 27.5 30.3 29.6 29.3 34.7 35.4 44.5 44.3 45.1 46.8 46.4 46.6 49.0 expanded"
N18-1057,W17-3204,0,0.0323766,"F0.5 score, which is standard for the task, as precision is valued over recall. On JFLEG, we report results with the GLEU metric (similar to BLEU) developed for the dataset. Training and decoding details All models are trained using stochastic gradient descent with annealing based on validation perplexity on a small held-out subset of the Lang-8 corpus. We apply both dropout and weight decay regularization. We observed that performance tended to saturate after 30 epochs. Decoding is done with a beam size of 8; in early experiments, we did not observe significant gains with larger beam sizes (Koehn and Knowles, 2017). CoNLL Results for the CoNLL 2013 (dev) and 2014 (test) datasets but with and without language model reranking are given in Table 2. In general, adding noised data helps, while simply adding clean data leads the model to be too conservative. Overall, we find that the random noising scheme yields the most significant gain of 4.5 F -score. Surprisingly, we find that augmenting the base dataset with synthesized data generated with random noising yields nearly the same performance when compared to using only nonsynthesized examples. To determine whether this might be due to overfitting, we reduce"
N18-1057,P06-1032,0,0.246179,"phrasing or errors in subject-verb agreement. Existing methods, however, are often only able to correct highly local errors, such as spelling errors or errors involving articles or prepositions. Classifier-based approaches to error correction are limited in their ability to capture a broad range of error types (Ng et al., 2014). Machine translation-based approaches—that instead trans619 Proceedings of NAACL-HLT 2018, pages 619–628 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics with lexical or part-of-speech features based on a small context window (Brockett et al., 2006; Felice, 2016). While these methods can introduce many possible edits, they are not as flexible as our approach inspired by the backtranslation procedure for machine translation (Sennrich et al., 2015). This is important as neural language models not explicitly trained to track long-range linguistic dependencies can fail to capture even simple noun-verb errors (Linzen et al., 2016). Recently, in the work perhaps most similar to ours, Rei et al. (2017) propose using statistical machine translation and backtranslation along with syntactic patterns for generating errors, albeit for the error det"
N18-1057,J81-4005,0,0.683475,"Missing"
N18-1057,Q16-1037,0,0.0308116,"s619 Proceedings of NAACL-HLT 2018, pages 619–628 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics with lexical or part-of-speech features based on a small context window (Brockett et al., 2006; Felice, 2016). While these methods can introduce many possible edits, they are not as flexible as our approach inspired by the backtranslation procedure for machine translation (Sennrich et al., 2015). This is important as neural language models not explicitly trained to track long-range linguistic dependencies can fail to capture even simple noun-verb errors (Linzen et al., 2016). Recently, in the work perhaps most similar to ours, Rei et al. (2017) propose using statistical machine translation and backtranslation along with syntactic patterns for generating errors, albeit for the error detection task. Neural machine translation Recent end-toend neural network-based approaches to machine translation have demonstrated strong empirical results (Sutskever et al., 2014; Cho et al., 2014). Building off of these strong results on machine translation, we use neural encoder-decoder models with attention (Bahdanau et al., 2014) for both our data synthesis (noising) and grammar"
N18-1057,I11-1017,0,0.600827,"op noising scheme matches the best performance on the development set but not the test set. We believe this is due to a mismatch between the CoNLL 2013 dev and 2014 Experiments To determine the effectiveness of the described noising schemes, we synthesize additional data using each and evaluate the performance of models trainined using the additional data on two benchmarks. Datasets For training our sequence transduction models, we combine the publicly available English Lang-8 dataset, a parallel corpus collected from a language learner forum, with training data from the CoNLL 2014 challenge (Mizumoto et al., 2011; Ng et al., 2014). We refer to this as the “base” dataset. Junczys-Dowmunt and Grundkiewicz (2016) additionally scraped 3.3M pairs of sentences from Lang-8. Although this expanded dataset, which we call the “expanded” dataset, is not typically used when comparing performance 623 Method Dev (no LM) Dev Test P R F0.5 P R F0.5 P R F0.5 none clean token reverse rank top random 50.7 56.1 49.7 53.1 51.3 49.1 50.0 10.5 9.4 11.9 13.0 12.3 17.4 17.9 28.7 28.1 30.4 32.8 31.4 36.0 36.8 48.4 47.5 47.7 50.5 51.0 47.7 48.9 17.2 16.9 18.7 19.1 18.3 23.9 23.0 35.5 34.8 36.4 38.0 37.6 39.8 39.9 52.7 52.3 51.4"
N18-1057,E17-2037,0,0.649681,"λ log pLM (h) 4.1 where λ is a hyperparameter and pLM (h) is given by the language model. 4 on grammar correction benchmarks, we use it instead to compare performance when training on additional synthesized data versus nonsynthesized data. For clean text to be noised, we use the LDC New York Times corpus for 2007, which yields roughly 1 million sentences. A summary of the data used for training is given in Table 1. We use the CoNLL 2013 evaluation set as our development set in all cases (Ng et al., 2013). Our test sets are the CoNLL 2014 evaluation set and the JFLEG test set (Ng et al., 2014; Napoles et al., 2017). Because CoNLL 2013 only has a single set of gold annotations while CoNLL 2014 has two, performance metrics tend to be significantly higher on CoNLL 2014. We report precision, recall, and F0.5 score, which is standard for the task, as precision is valued over recall. On JFLEG, we report results with the GLEU metric (similar to BLEU) developed for the dataset. Training and decoding details All models are trained using stochastic gradient descent with annealing based on validation perplexity on a small held-out subset of the Lang-8 corpus. We apply both dropout and weight decay regularization."
N18-1057,W14-1701,0,0.547232,"ing noisy, ungrammatical text remains a challenging task in natural language processing. Ideally, given some piece of writing, an error correction system would be able to fix minor typographical errors, as well as grammatical errors that involve longer dependencies such as nonidiomatic phrasing or errors in subject-verb agreement. Existing methods, however, are often only able to correct highly local errors, such as spelling errors or errors involving articles or prepositions. Classifier-based approaches to error correction are limited in their ability to capture a broad range of error types (Ng et al., 2014). Machine translation-based approaches—that instead trans619 Proceedings of NAACL-HLT 2018, pages 619–628 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics with lexical or part-of-speech features based on a small context window (Brockett et al., 2006; Felice, 2016). While these methods can introduce many possible edits, they are not as flexible as our approach inspired by the backtranslation procedure for machine translation (Sennrich et al., 2015). This is important as neural language models not explicitly trained to track long-range linguistic depende"
N18-1057,N16-1042,0,0.299469,"Missing"
N18-1057,W13-3601,0,0.118233,"model during final reranking by modifying the score for a completed hypothesis s(h) to be sLM (h) = s(h) + λ log pLM (h) 4.1 where λ is a hyperparameter and pLM (h) is given by the language model. 4 on grammar correction benchmarks, we use it instead to compare performance when training on additional synthesized data versus nonsynthesized data. For clean text to be noised, we use the LDC New York Times corpus for 2007, which yields roughly 1 million sentences. A summary of the data used for training is given in Table 1. We use the CoNLL 2013 evaluation set as our development set in all cases (Ng et al., 2013). Our test sets are the CoNLL 2014 evaluation set and the JFLEG test set (Ng et al., 2014; Napoles et al., 2017). Because CoNLL 2013 only has a single set of gold annotations while CoNLL 2014 has two, performance metrics tend to be significantly higher on CoNLL 2014. We report precision, recall, and F0.5 score, which is standard for the task, as precision is valued over recall. On JFLEG, we report results with the GLEU metric (similar to BLEU) developed for the dataset. Training and decoding details All models are trained using stochastic gradient descent with annealing based on validation per"
N18-1057,W17-5032,0,0.0795041,", June 1 - 6, 2018. 2018 Association for Computational Linguistics with lexical or part-of-speech features based on a small context window (Brockett et al., 2006; Felice, 2016). While these methods can introduce many possible edits, they are not as flexible as our approach inspired by the backtranslation procedure for machine translation (Sennrich et al., 2015). This is important as neural language models not explicitly trained to track long-range linguistic dependencies can fail to capture even simple noun-verb errors (Linzen et al., 2016). Recently, in the work perhaps most similar to ours, Rei et al. (2017) propose using statistical machine translation and backtranslation along with syntactic patterns for generating errors, albeit for the error detection task. Neural machine translation Recent end-toend neural network-based approaches to machine translation have demonstrated strong empirical results (Sutskever et al., 2014; Cho et al., 2014). Building off of these strong results on machine translation, we use neural encoder-decoder models with attention (Bahdanau et al., 2014) for both our data synthesis (noising) and grammar correction (denoising) models. Although many recent works on NMT have"
N18-1057,I17-2062,0,0.152533,"Missing"
N18-1057,P16-1009,0,0.266903,"Missing"
N18-1057,N15-1020,0,0.0715226,"Missing"
N18-1057,1983.tc-1.13,0,0.334578,"Missing"
N18-1146,W17-4712,1,0.839877,"ctor of word frequencies. It learns adversarial encodings of T which are useful for predicting Y , but not useful for predicting C. It is depicted in Figure 2. Description. First, we encode T into e ∈ Rd via the same mechanisms as the Deep Residualizer of Section 3.1. e is then passed to a series of FFNNs (“prediction heads”) which are trained to predict each target and confound with the same loss functions as that of Section 3.1. As gradients backpropagate from the confound prediction heads to the encoder, we pass them through a gradient reversal layer in the style of Ganin et al. (2016) and Britz et al. (2017), which multiplies gradients by −1. If the cumulative loss of the target variables is Lt and that of the confounds is Lc , then the loss which is implicitly used to train the encoder is Le = Lt − Lc , thereby encouraging the encoder to learn representations of the text which are not useful for predicting the confounds. Lexicons are elicited from this model via the same mechanism as the Deep Residualizer of Section 3.1. 4 Experiments We evaluate the approaches described in Sections 3 and 5 by generating and evaluating deconfounded lexicons in three domains: financial complaints, e-commerce prod"
N18-1146,D12-1124,0,0.0317016,"of related work which we draw on. We address these in turn. Lexicon induction. Some work in lexicon induction is intended to help interpret the subjective properties of a text or make make machine learning models more interpretable, i.e. so that practitioners can know why their system works. For example, Taboada et al. (2011); Hamilton et al. (2016) induce sentiment lexicons, and Mohammad and Turney (2010); Hu et al. (2009) induce emotion lexicons. Practitioners often get these words by considering the high-scoring features of regressions trained to predict an outcome (McFarland et al., 2013; Chahuneau et al., 2012; Ranganath et al., 2013; Kang et al., 2013). They account for confounds through manual inspection, residualizing (Jaeger et al., 2009; Baayen et al., 2010), hierarchical modeling (Bates, 2010; Gustarini, 2016; Schillebeeckx et al., 2016), log-odds (Szumilas, 2010; Monroe et al., 2008), mutual information (Berg, 2004), or matching (Tan et al., 2014; DiNardo, 2010). Many of these methods are manual processes or have known limitations, mostly due to multicollinearity (Imai and Kim, 2016; Chatelain and Ralf, 2012; Wurm and Fisicaro, 2014). Furthermore, these methods have not been tested in a comp"
N18-1146,P16-1150,0,0.0179045,"d method related to our Deep Residualization (DR) method (Section 3.1), and Egami et al. (2017) explore how to make causal inferences from text through careful data splitting. Unlike us, these papers are largely unconcerned with the underlying features and algorithmic interpretability. Athey (2017) has a recent survey of machine learning problems where causal modeling is important. Persuasion. Our experiments touch on the mechanism of persuasion, which has been widely studied. Most of this prior work uses lexical, syntactic, discourse, and dialog interactive features (Stab and Gurevych, 2014; Habernal and Gurevych, 2016; Wei et al., 2016), power dynamics (Rosenthal and Mckeown, 2017; Moore, 2012), or diction (Wei et al., 2016) to study discourse persuasion as manifested in argument. We study narrative persuasion as manifested in everyday decisions. This important mode of persuasion is understudied because researchers have struggled to isolate the “active ingredient” of persuasive narratives (Green, 2008; De Graaf et al., 2012), a problem that the formal framework of deconfounded lexicon induction (Section 2) may help alleviate. 6 Conclusion Computational social scientists frequently develop algorithms to fin"
N18-1146,D16-1057,1,0.854738,"anese shoppers (Pryzant et al., 2017). On the other hand, RR selected sev1621 eral numbers and failed to avoid brand indicators: “nichiban”, a large company which specializes in medical adhesives, is one of the highest-scoring words. 5 Related Work There are three areas of related work which we draw on. We address these in turn. Lexicon induction. Some work in lexicon induction is intended to help interpret the subjective properties of a text or make make machine learning models more interpretable, i.e. so that practitioners can know why their system works. For example, Taboada et al. (2011); Hamilton et al. (2016) induce sentiment lexicons, and Mohammad and Turney (2010); Hu et al. (2009) induce emotion lexicons. Practitioners often get these words by considering the high-scoring features of regressions trained to predict an outcome (McFarland et al., 2013; Chahuneau et al., 2012; Ranganath et al., 2013; Kang et al., 2013). They account for confounds through manual inspection, residualizing (Jaeger et al., 2009; Baayen et al., 2010), hierarchical modeling (Bates, 2010; Gustarini, 2016; Schillebeeckx et al., 2016), log-odds (Szumilas, 2010; Monroe et al., 2008), mutual information (Berg, 2004), or match"
N18-1146,D13-1150,0,0.129943,"words are useful in their own right (to develop causal domain theories or for linguistic analysis) but also as interpretable features for down-stream modeling. Such work could help widely in applications of NLP to tasks like linking text to sales figures (Ho and Wu, 1999), to voter preference (Luntz, 2007; Ansolabehere and Iyengar, 1995), to moral belief (Giles et al., 2008; Keele et al., 2009), to police respect (Voigt et al., 2017), to financial outlooks (Grinblatt and Keloharju, 2001; Chatelain and Ralf, 2012), to stock prices (Lee et al., 2014), and even to restaurant health inspections (Kang et al., 2013). Identifying linguistic features that are indicative of such outcomes and decorrelated with confounds is a common activity among social scientists, data scientists, and other machine learning practitioners. Indeed, it is essential for developing transpar1615 Proceedings of NAACL-HLT 2018, pages 1615–1625 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics ent and interpretable machine learning NLP models. Yet there is no generally accepted and rigorously evaluated procedure for the activity. Practitioners have conducted it on a largely ad-hoc basis, appl"
N18-1146,lee-etal-2014-importance,1,0.821754,"rom confounding information. The lexicons constituted by these words are useful in their own right (to develop causal domain theories or for linguistic analysis) but also as interpretable features for down-stream modeling. Such work could help widely in applications of NLP to tasks like linking text to sales figures (Ho and Wu, 1999), to voter preference (Luntz, 2007; Ansolabehere and Iyengar, 1995), to moral belief (Giles et al., 2008; Keele et al., 2009), to police respect (Voigt et al., 2017), to financial outlooks (Grinblatt and Keloharju, 2001; Chatelain and Ralf, 2012), to stock prices (Lee et al., 2014), and even to restaurant health inspections (Kang et al., 2013). Identifying linguistic features that are indicative of such outcomes and decorrelated with confounds is a common activity among social scientists, data scientists, and other machine learning practitioners. Indeed, it is essential for developing transpar1615 Proceedings of NAACL-HLT 2018, pages 1615–1625 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics ent and interpretable machine learning NLP models. Yet there is no generally accepted and rigorously evaluated procedure for the activity."
N18-1146,W10-0204,0,0.0172197,"nd, RR selected sev1621 eral numbers and failed to avoid brand indicators: “nichiban”, a large company which specializes in medical adhesives, is one of the highest-scoring words. 5 Related Work There are three areas of related work which we draw on. We address these in turn. Lexicon induction. Some work in lexicon induction is intended to help interpret the subjective properties of a text or make make machine learning models more interpretable, i.e. so that practitioners can know why their system works. For example, Taboada et al. (2011); Hamilton et al. (2016) induce sentiment lexicons, and Mohammad and Turney (2010); Hu et al. (2009) induce emotion lexicons. Practitioners often get these words by considering the high-scoring features of regressions trained to predict an outcome (McFarland et al., 2013; Chahuneau et al., 2012; Ranganath et al., 2013; Kang et al., 2013). They account for confounds through manual inspection, residualizing (Jaeger et al., 2009; Baayen et al., 2010), hierarchical modeling (Bates, 2010; Gustarini, 2016; Schillebeeckx et al., 2016), log-odds (Szumilas, 2010; Monroe et al., 2008), mutual information (Berg, 2004), or matching (Tan et al., 2014; DiNardo, 2010). Many of these metho"
N18-1146,D14-1006,0,0.0246327,"13) advocate a lasso-based method related to our Deep Residualization (DR) method (Section 3.1), and Egami et al. (2017) explore how to make causal inferences from text through careful data splitting. Unlike us, these papers are largely unconcerned with the underlying features and algorithmic interpretability. Athey (2017) has a recent survey of machine learning problems where causal modeling is important. Persuasion. Our experiments touch on the mechanism of persuasion, which has been widely studied. Most of this prior work uses lexical, syntactic, discourse, and dialog interactive features (Stab and Gurevych, 2014; Habernal and Gurevych, 2016; Wei et al., 2016), power dynamics (Rosenthal and Mckeown, 2017; Moore, 2012), or diction (Wei et al., 2016) to study discourse persuasion as manifested in argument. We study narrative persuasion as manifested in everyday decisions. This important mode of persuasion is understudied because researchers have struggled to isolate the “active ingredient” of persuasive narratives (Green, 2008; De Graaf et al., 2012), a problem that the formal framework of deconfounded lexicon induction (Section 2) may help alleviate. 6 Conclusion Computational social scientists frequen"
N18-1146,J11-2001,0,0.0496509,"shown to appeal to Japanese shoppers (Pryzant et al., 2017). On the other hand, RR selected sev1621 eral numbers and failed to avoid brand indicators: “nichiban”, a large company which specializes in medical adhesives, is one of the highest-scoring words. 5 Related Work There are three areas of related work which we draw on. We address these in turn. Lexicon induction. Some work in lexicon induction is intended to help interpret the subjective properties of a text or make make machine learning models more interpretable, i.e. so that practitioners can know why their system works. For example, Taboada et al. (2011); Hamilton et al. (2016) induce sentiment lexicons, and Mohammad and Turney (2010); Hu et al. (2009) induce emotion lexicons. Practitioners often get these words by considering the high-scoring features of regressions trained to predict an outcome (McFarland et al., 2013; Chahuneau et al., 2012; Ranganath et al., 2013; Kang et al., 2013). They account for confounds through manual inspection, residualizing (Jaeger et al., 2009; Baayen et al., 2010), hierarchical modeling (Bates, 2010; Gustarini, 2016; Schillebeeckx et al., 2016), log-odds (Szumilas, 2010; Monroe et al., 2008), mutual informatio"
N18-1146,P14-1017,0,0.0436176,"ce sentiment lexicons, and Mohammad and Turney (2010); Hu et al. (2009) induce emotion lexicons. Practitioners often get these words by considering the high-scoring features of regressions trained to predict an outcome (McFarland et al., 2013; Chahuneau et al., 2012; Ranganath et al., 2013; Kang et al., 2013). They account for confounds through manual inspection, residualizing (Jaeger et al., 2009; Baayen et al., 2010), hierarchical modeling (Bates, 2010; Gustarini, 2016; Schillebeeckx et al., 2016), log-odds (Szumilas, 2010; Monroe et al., 2008), mutual information (Berg, 2004), or matching (Tan et al., 2014; DiNardo, 2010). Many of these methods are manual processes or have known limitations, mostly due to multicollinearity (Imai and Kim, 2016; Chatelain and Ralf, 2012; Wurm and Fisicaro, 2014). Furthermore, these methods have not been tested in a comparative setting: this work is the first to offer an experimental analysis of their abilities. Causal inference. Our methods for lexicon induction have connections to recent advances in the causal inference literature. In particular, Johansson et al. (2016) and Shalit et al. (2016) propose an algorithm for counterfactual inference which bear similar"
N18-1146,P16-2032,0,0.0255622,"Residualization (DR) method (Section 3.1), and Egami et al. (2017) explore how to make causal inferences from text through careful data splitting. Unlike us, these papers are largely unconcerned with the underlying features and algorithmic interpretability. Athey (2017) has a recent survey of machine learning problems where causal modeling is important. Persuasion. Our experiments touch on the mechanism of persuasion, which has been widely studied. Most of this prior work uses lexical, syntactic, discourse, and dialog interactive features (Stab and Gurevych, 2014; Habernal and Gurevych, 2016; Wei et al., 2016), power dynamics (Rosenthal and Mckeown, 2017; Moore, 2012), or diction (Wei et al., 2016) to study discourse persuasion as manifested in argument. We study narrative persuasion as manifested in everyday decisions. This important mode of persuasion is understudied because researchers have struggled to isolate the “active ingredient” of persuasive narratives (Green, 2008; De Graaf et al., 2012), a problem that the formal framework of deconfounded lexicon induction (Section 2) may help alleviate. 6 Conclusion Computational social scientists frequently develop algorithms to find words that are re"
N19-1304,P15-2072,0,0.0651016,"number of followed politicians from the user’s preferred party, one more followed politician is associated with a decrease of .02 SD in the leave-out. 4 Topics and Framing Topic choice can be a tool for agenda-setting by establishing what an author or institution deems worthy of discussion (McCombs, 2002), and works in NLP have used topic modeling as an approach to measure this effect (Tsur et al., 2015; Field et al., 2018). The strategy of highlighting particular aspects within topics as a means of framing (Entman, 2007) has also been quantified in the NLP literature (Boydstun et al., 2013; Card et al., 2015; Naderi and Hirst, 2017). Previous work largely focuses on the relation between topic and framing in the news media; we study social media, proposing methods to identify general, non-event-specific topics and to quantify between- and within-topic polarization. 4.1 Methods Topic assignment. Our goal is to induce topics that are salient in our narrow domain and comparable across events. This presents a challenge for traditional topic modeling approaches, since the discourse surrounding these events is inherently tied to concrete aspects of the events that tend to covary with topic usage, like l"
N19-1304,D18-1393,1,0.865898,"ates imply that, fixing the total number of followed politicians, one more followed politician from one’s preferred party is associated with an increase of .009 SD in the leaveout. Fixing the number of followed politicians from the user’s preferred party, one more followed politician is associated with a decrease of .02 SD in the leave-out. 4 Topics and Framing Topic choice can be a tool for agenda-setting by establishing what an author or institution deems worthy of discussion (McCombs, 2002), and works in NLP have used topic modeling as an approach to measure this effect (Tsur et al., 2015; Field et al., 2018). The strategy of highlighting particular aspects within topics as a means of framing (Entman, 2007) has also been quantified in the NLP literature (Boydstun et al., 2013; Card et al., 2015; Naderi and Hirst, 2017). Previous work largely focuses on the relation between topic and framing in the news media; we study social media, proposing methods to identify general, non-event-specific topics and to quantify between- and within-topic polarization. 4.1 Methods Topic assignment. Our goal is to induce topics that are salient in our narrow domain and comparable across events. This presents a challe"
N19-1304,naderi-hirst-2017-classifying,0,0.0305199,"politicians from the user’s preferred party, one more followed politician is associated with a decrease of .02 SD in the leave-out. 4 Topics and Framing Topic choice can be a tool for agenda-setting by establishing what an author or institution deems worthy of discussion (McCombs, 2002), and works in NLP have used topic modeling as an approach to measure this effect (Tsur et al., 2015; Field et al., 2018). The strategy of highlighting particular aspects within topics as a means of framing (Entman, 2007) has also been quantified in the NLP literature (Boydstun et al., 2013; Card et al., 2015; Naderi and Hirst, 2017). Previous work largely focuses on the relation between topic and framing in the news media; we study social media, proposing methods to identify general, non-event-specific topics and to quantify between- and within-topic polarization. 4.1 Methods Topic assignment. Our goal is to induce topics that are salient in our narrow domain and comparable across events. This presents a challenge for traditional topic modeling approaches, since the discourse surrounding these events is inherently tied to concrete aspects of the events that tend to covary with topic usage, like location, setting, and dem"
N19-1304,D16-1106,0,0.022179,"nalyzing polarization within and across events. Framing and polarization in the context of mass shootings is well-studied, though much of the literature studies the role of media (Chyi and McCombs, 2004; Schildkraut and Elsass, 2016) and politicians (Johnson et al., 2017). Several works find that frames have changed over time and between such events (Muschert and Carr, 2006; Schildkraut and Muschert, 2014), and that frames influence opinions on gun policies (HaiderMarkel and Joslyn, 2001). Prior NLP work in this area has considered how to extract factual information on gun violence from news (Pavlick et al., 2016) as well as quantify stance and public opinion on Twitter (Benton et al., 2016) and across the web (Ayers et al., 2016); here we advance NLP approaches to the public discourse surrounding gun violence by introducing methods to analyze other linguistic manifestations of polarization. 1.2 The Role of the Shooter’s Race We are particularly interested in the role of the shooter’s race in shaping polarized responses to these events. Implicit or explicit racial biases can be central in people’s understanding of social 2970 Proceedings of NAACL-HLT 2019, pages 2970–3005 c Minneapolis, Minnesota, June"
N19-1304,D14-1162,0,0.0832406,"developed specifically for tweets. For all of these methods, we first randomly sample 10k tweets from each event forming our subset S of all tweets T ; then, we create a vocabulary V of word stems that occur at least ten times in at least three events within S (⇠2000 word stems) and remove all stems from T are not part of V . Sampling is crucial for encouraging event-independent topics given the large disparity among event-level tweet counts (the largest event, Orlando, has 225⇥ more tweets than the smallest event, Burlington). For the embedding-based approach, we: 1. Train GloVe embeddings (Pennington et al., 2014) on V based on 11-50k random samples of tweets from each event.9 2. Create sentence embeddings et , 8t 2 T using Arora et al. (2017)’s method, by computing the weighted average vt of the embeddings of stems within t and removing vt ’s projection onto the first principal component of the matrix the rows of which are vt , 8t 2 S. Stem weights are set to be inversely proportional to their frequencies in S. 3. Jointly cluster the embeddings et , 8t 2 S via k-means using cosine distance and assign all tweet embeddings et , 8t 2 T to the centroids to which they are closest. We also trained MALLET an"
N19-1304,P17-1068,0,0.0746506,"Missing"
N19-1304,P15-1157,0,0.0426658,"dicators. The estimates imply that, fixing the total number of followed politicians, one more followed politician from one’s preferred party is associated with an increase of .009 SD in the leaveout. Fixing the number of followed politicians from the user’s preferred party, one more followed politician is associated with a decrease of .02 SD in the leave-out. 4 Topics and Framing Topic choice can be a tool for agenda-setting by establishing what an author or institution deems worthy of discussion (McCombs, 2002), and works in NLP have used topic modeling as an approach to measure this effect (Tsur et al., 2015; Field et al., 2018). The strategy of highlighting particular aspects within topics as a means of framing (Entman, 2007) has also been quantified in the NLP literature (Boydstun et al., 2013; Card et al., 2015; Naderi and Hirst, 2017). Previous work largely focuses on the relation between topic and framing in the news media; we study social media, proposing methods to identify general, non-event-specific topics and to quantify between- and within-topic polarization. 4.1 Methods Topic assignment. Our goal is to induce topics that are salient in our narrow domain and comparable across events. T"
N19-1304,P14-1018,0,0.0705562,"Missing"
N19-1364,P18-1058,0,0.0436454,"(2017) utilized the persuasive modes—ethos, logos, pathos—to model premises and the semantic types of argument components in an online persuasive forum. While most computational argumentation focuses on the relational support structures and factual evidence to make claims, persuasion focuses more on language cues aimed at shaping, reinforcing and changing people’s opinions and beliefs. How language changes people’s attitudes and behaviors have received less attention from the computational community than argumentation, although there have been important preliminary work (Persing and Ng, 2017; Carlile et al., 2018). Farra et al., (2015) built regression models to predict essay scores based on features extracted from opinion expressions and topical elements. Chatterjee et al., (2014) used verbal descriptors and para-verbal markers of hesitation to predict speakers’ persuasiveness on website housing videos of product reviews. When looking at persuasion in the context of online forum discussions (Wei et al., 2016), Tan et al., (2016) found that on the Change My View subreddit, interaction dynamics such as the language interplay between opinion holders and other participants provides highly predictive cues"
N19-1364,W14-5908,0,0.0359094,"putational argumentation focuses on the relational support structures and factual evidence to make claims, persuasion focuses more on language cues aimed at shaping, reinforcing and changing people’s opinions and beliefs. How language changes people’s attitudes and behaviors have received less attention from the computational community than argumentation, although there have been important preliminary work (Persing and Ng, 2017; Carlile et al., 2018). Farra et al., (2015) built regression models to predict essay scores based on features extracted from opinion expressions and topical elements. Chatterjee et al., (2014) used verbal descriptors and para-verbal markers of hesitation to predict speakers’ persuasiveness on website housing videos of product reviews. When looking at persuasion in the context of online forum discussions (Wei et al., 2016), Tan et al., (2016) found that on the Change My View subreddit, interaction dynamics such as the language interplay between opinion holders and other participants provides highly predictive cues for persuasiveness. Using the same dataset, Wel et al., (2016) extracted a set of textual information and social interaction features to identify persuasive posts. Recentl"
N19-1364,W14-2106,0,0.0327216,"or Computational Linguistics show that our semi-supervised model outperforms several baselines. We then apply this automated model to unseen requests from different domains and obtain nuanced findings of the importance of different strategies on persuasion success. Our model can be useful in any situation in which we have exogenous document-level supervision, but only small amounts of expensive human-annotated sentence labels. 2 Related Work Computational argumentation has received much recent attention (Ghosh et al., 2016; Stab and Gurevych, 2017; Peldszus and Stede, 2013; Stab et al., 2018; Ghosh et al., 2014). Most work has either identified the arguments in news articles (Sardianos et al., 2015) or user-generated web content (Habernal and Gurevych, 2017; Musi et al., 2018), or classified argument components (Zhang and Litman, 2015) into claims and premises, supporting and opposing claims, or backings, rebuttals and refutations . For example, Stab and Gurevych (2014) proposed structural, lexical, syntactic and contextual features to identify convincing components of Web arguments including claim, major claim, and premise. Similarly, Zhang and Litman (2015) studied student essay revisions and class"
N19-1364,P16-1150,0,0.0244722,"7; Musi et al., 2018), or classified argument components (Zhang and Litman, 2015) into claims and premises, supporting and opposing claims, or backings, rebuttals and refutations . For example, Stab and Gurevych (2014) proposed structural, lexical, syntactic and contextual features to identify convincing components of Web arguments including claim, major claim, and premise. Similarly, Zhang and Litman (2015) studied student essay revisions and classified a set of argumentative actions associated with successful writing such as warrant/reasoning/backing, rebuttal/reservation, and claims/ideas. Habernal and Gurevych (2016) investigated the persuasiveness of arguments in any given argument pair using bidirectional LSTM. Hidey et al., (2017) utilized the persuasive modes—ethos, logos, pathos—to model premises and the semantic types of argument components in an online persuasive forum. While most computational argumentation focuses on the relational support structures and factual evidence to make claims, persuasion focuses more on language cues aimed at shaping, reinforcing and changing people’s opinions and beliefs. How language changes people’s attitudes and behaviors have received less attention from the comput"
N19-1364,J17-1004,0,0.0462079,"requests from different domains and obtain nuanced findings of the importance of different strategies on persuasion success. Our model can be useful in any situation in which we have exogenous document-level supervision, but only small amounts of expensive human-annotated sentence labels. 2 Related Work Computational argumentation has received much recent attention (Ghosh et al., 2016; Stab and Gurevych, 2017; Peldszus and Stede, 2013; Stab et al., 2018; Ghosh et al., 2014). Most work has either identified the arguments in news articles (Sardianos et al., 2015) or user-generated web content (Habernal and Gurevych, 2017; Musi et al., 2018), or classified argument components (Zhang and Litman, 2015) into claims and premises, supporting and opposing claims, or backings, rebuttals and refutations . For example, Stab and Gurevych (2014) proposed structural, lexical, syntactic and contextual features to identify convincing components of Web arguments including claim, major claim, and premise. Similarly, Zhang and Litman (2015) studied student essay revisions and classified a set of argumentative actions associated with successful writing such as warrant/reasoning/backing, rebuttal/reservation, and claims/ideas. H"
N19-1364,W17-5102,0,0.488347,"sing claims, or backings, rebuttals and refutations . For example, Stab and Gurevych (2014) proposed structural, lexical, syntactic and contextual features to identify convincing components of Web arguments including claim, major claim, and premise. Similarly, Zhang and Litman (2015) studied student essay revisions and classified a set of argumentative actions associated with successful writing such as warrant/reasoning/backing, rebuttal/reservation, and claims/ideas. Habernal and Gurevych (2016) investigated the persuasiveness of arguments in any given argument pair using bidirectional LSTM. Hidey et al., (2017) utilized the persuasive modes—ethos, logos, pathos—to model premises and the semantic types of argument components in an online persuasive forum. While most computational argumentation focuses on the relational support structures and factual evidence to make claims, persuasion focuses more on language cues aimed at shaping, reinforcing and changing people’s opinions and beliefs. How language changes people’s attitudes and behaviors have received less attention from the computational community than argumentation, although there have been important preliminary work (Persing and Ng, 2017; Carlil"
N19-1364,P14-5010,0,0.00443789,"Missing"
N19-1364,W15-0608,0,0.0135574,"suasive modes—ethos, logos, pathos—to model premises and the semantic types of argument components in an online persuasive forum. While most computational argumentation focuses on the relational support structures and factual evidence to make claims, persuasion focuses more on language cues aimed at shaping, reinforcing and changing people’s opinions and beliefs. How language changes people’s attitudes and behaviors have received less attention from the computational community than argumentation, although there have been important preliminary work (Persing and Ng, 2017; Carlile et al., 2018). Farra et al., (2015) built regression models to predict essay scores based on features extracted from opinion expressions and topical elements. Chatterjee et al., (2014) used verbal descriptors and para-verbal markers of hesitation to predict speakers’ persuasiveness on website housing videos of product reviews. When looking at persuasion in the context of online forum discussions (Wei et al., 2016), Tan et al., (2016) found that on the Change My View subreddit, interaction dynamics such as the language interplay between opinion holders and other participants provides highly predictive cues for persuasiveness. Us"
N19-1364,P16-2089,0,0.0142165,"2019, pages 3620–3630 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics show that our semi-supervised model outperforms several baselines. We then apply this automated model to unseen requests from different domains and obtain nuanced findings of the importance of different strategies on persuasion success. Our model can be useful in any situation in which we have exogenous document-level supervision, but only small amounts of expensive human-annotated sentence labels. 2 Related Work Computational argumentation has received much recent attention (Ghosh et al., 2016; Stab and Gurevych, 2017; Peldszus and Stede, 2013; Stab et al., 2018; Ghosh et al., 2014). Most work has either identified the arguments in news articles (Sardianos et al., 2015) or user-generated web content (Habernal and Gurevych, 2017; Musi et al., 2018), or classified argument components (Zhang and Litman, 2015) into claims and premises, supporting and opposing claims, or backings, rebuttals and refutations . For example, Stab and Gurevych (2014) proposed structural, lexical, syntactic and contextual features to identify convincing components of Web arguments including claim, major claim"
N19-1364,N18-1146,1,0.848177,"s highly predictive cues for persuasiveness. Using the same dataset, Wel et al., (2016) extracted a set of textual information and social interaction features to identify persuasive posts. Recently, Pryzant et al., (2017) introduced a neural network with an adversarial objective to select text features that are predictive of some outcomes but decorrelated with others and further analyzed the narratives highlighted by such text features. Further work extended the model to induce narrative persuasion lexicons predictive of enrollment from course descriptions and sales from product descriptions (Pryzant et al., 2018a), and the efficacy of search advertisements (Pryzant et al., 2018b). Similar to their settings, we use the outcomes of a persuasive description to supervise the learning of persuasion tactics, and our model can similarly induce lexicons associated with successful narrative persuasion by examining highly attentional words associated with persuasion outcomes. Our work differs both in our semisupervised method and also because we explicitly draw on the theoretical literature to model the persuasion strategy for each sentence in requests, allowing requests to have multiple persuasion strategies;"
N19-1364,P16-2032,0,0.0625096,"s people’s attitudes and behaviors have received less attention from the computational community than argumentation, although there have been important preliminary work (Persing and Ng, 2017; Carlile et al., 2018). Farra et al., (2015) built regression models to predict essay scores based on features extracted from opinion expressions and topical elements. Chatterjee et al., (2014) used verbal descriptors and para-verbal markers of hesitation to predict speakers’ persuasiveness on website housing videos of product reviews. When looking at persuasion in the context of online forum discussions (Wei et al., 2016), Tan et al., (2016) found that on the Change My View subreddit, interaction dynamics such as the language interplay between opinion holders and other participants provides highly predictive cues for persuasiveness. Using the same dataset, Wel et al., (2016) extracted a set of textual information and social interaction features to identify persuasive posts. Recently, Pryzant et al., (2017) introduced a neural network with an adversarial objective to select text features that are predictive of some outcomes but decorrelated with others and further analyzed the narratives highlighted by such tex"
N19-1364,W18-5415,0,0.0153708,"s highly predictive cues for persuasiveness. Using the same dataset, Wel et al., (2016) extracted a set of textual information and social interaction features to identify persuasive posts. Recently, Pryzant et al., (2017) introduced a neural network with an adversarial objective to select text features that are predictive of some outcomes but decorrelated with others and further analyzed the narratives highlighted by such text features. Further work extended the model to induce narrative persuasion lexicons predictive of enrollment from course descriptions and sales from product descriptions (Pryzant et al., 2018a), and the efficacy of search advertisements (Pryzant et al., 2018b). Similar to their settings, we use the outcomes of a persuasive description to supervise the learning of persuasion tactics, and our model can similarly induce lexicons associated with successful narrative persuasion by examining highly attentional words associated with persuasion outcomes. Our work differs both in our semisupervised method and also because we explicitly draw on the theoretical literature to model the persuasion strategy for each sentence in requests, allowing requests to have multiple persuasion strategies;"
N19-1364,N16-1174,1,0.694918,"Missing"
N19-1364,D14-1006,0,0.0449685,", but only small amounts of expensive human-annotated sentence labels. 2 Related Work Computational argumentation has received much recent attention (Ghosh et al., 2016; Stab and Gurevych, 2017; Peldszus and Stede, 2013; Stab et al., 2018; Ghosh et al., 2014). Most work has either identified the arguments in news articles (Sardianos et al., 2015) or user-generated web content (Habernal and Gurevych, 2017; Musi et al., 2018), or classified argument components (Zhang and Litman, 2015) into claims and premises, supporting and opposing claims, or backings, rebuttals and refutations . For example, Stab and Gurevych (2014) proposed structural, lexical, syntactic and contextual features to identify convincing components of Web arguments including claim, major claim, and premise. Similarly, Zhang and Litman (2015) studied student essay revisions and classified a set of argumentative actions associated with successful writing such as warrant/reasoning/backing, rebuttal/reservation, and claims/ideas. Habernal and Gurevych (2016) investigated the persuasiveness of arguments in any given argument pair using bidirectional LSTM. Hidey et al., (2017) utilized the persuasive modes—ethos, logos, pathos—to model premises a"
N19-1364,W15-0616,0,0.0135048,"fferent strategies on persuasion success. Our model can be useful in any situation in which we have exogenous document-level supervision, but only small amounts of expensive human-annotated sentence labels. 2 Related Work Computational argumentation has received much recent attention (Ghosh et al., 2016; Stab and Gurevych, 2017; Peldszus and Stede, 2013; Stab et al., 2018; Ghosh et al., 2014). Most work has either identified the arguments in news articles (Sardianos et al., 2015) or user-generated web content (Habernal and Gurevych, 2017; Musi et al., 2018), or classified argument components (Zhang and Litman, 2015) into claims and premises, supporting and opposing claims, or backings, rebuttals and refutations . For example, Stab and Gurevych (2014) proposed structural, lexical, syntactic and contextual features to identify convincing components of Web arguments including claim, major claim, and premise. Similarly, Zhang and Litman (2015) studied student essay revisions and classified a set of argumentative actions associated with successful writing such as warrant/reasoning/backing, rebuttal/reservation, and claims/ideas. Habernal and Gurevych (2016) investigated the persuasiveness of arguments in any"
N19-1364,C16-1246,0,0.0447825,"Missing"
N19-1364,J17-3005,0,0.021144,"30 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics show that our semi-supervised model outperforms several baselines. We then apply this automated model to unseen requests from different domains and obtain nuanced findings of the importance of different strategies on persuasion success. Our model can be useful in any situation in which we have exogenous document-level supervision, but only small amounts of expensive human-annotated sentence labels. 2 Related Work Computational argumentation has received much recent attention (Ghosh et al., 2016; Stab and Gurevych, 2017; Peldszus and Stede, 2013; Stab et al., 2018; Ghosh et al., 2014). Most work has either identified the arguments in news articles (Sardianos et al., 2015) or user-generated web content (Habernal and Gurevych, 2017; Musi et al., 2018), or classified argument components (Zhang and Litman, 2015) into claims and premises, supporting and opposing claims, or backings, rebuttals and refutations . For example, Stab and Gurevych (2014) proposed structural, lexical, syntactic and contextual features to identify convincing components of Web arguments including claim, major claim, and premise. Similarly,"
N19-1365,W06-3907,1,0.738151,"Missing"
N19-1365,D14-1162,0,0.0807548,"Missing"
N19-1365,N18-1202,0,0.10909,"Missing"
N19-1365,S12-1020,1,0.887325,"Missing"
N19-1365,N18-1101,0,0.568933,"Routing Networks with an EM-like training approach. We show how to incorporate RRNs into different neural components (word representation layers, recurrent network hidden layers, classifier layers), and we study their application to natural language inference (NLI), in which premise–hypothesis pairs are labeled for whether the premise entails, contradicts, or is neutral with respect to the hypothesis. We chose this task because reasoning in natural language involves context-sensitive interpretation of words and sentences as well as compositional structure. We make use of the MULTINLI corpus (Williams et al., 2018), which includes text from multiple genres that we expect to condition linguistic senses in complex ways. Our experiments show that RRNs learn policies and components that reflect this genre structure, which leads to superior performance. We also introduce a new corpus of NLI examples involving implicative constructions like manage to, be able to, and fail to (Karttunen, 1971, 2012). This corpus follows the design of many recent NLI corpora, but with the added challenges of reasoning about implicatives, which have logical signatures that interact compositionally with each other and with surrou"
N19-1365,P18-2104,0,0.018209,"wo examples are learned using different weights, there is low potential for transfer and interference. This is beneficial for unrelated examples, but limits the potential for learning about commonalities between related ones. RRNs extend vanilla neural networks by granting them the leverage to navigate this trade-off by making global functional decisions at the module level. As a result, RRNs explicitly make decisions to compress or orthogonalize knowledge between examples by deciding whether to share specific weights. Thus far, hard-selection routing has not been applied to language domains. Zaremoodi et al. (2018) introduced a soft version of routing that falls within a larger class of Mixtures of Experts (MoE) models (Jacobs et al., 1991). However, M o E models differ from RRN s in two crucial ways. First, MoE models generally do not consider the recursive application of functions. The promise is that we can compose functions to reflect the compositional aspects of a problem. Imagine we have a sentence encoding and we want to answer a particular question. We can now condition the router on the question, so that it applies exactly the functions required that translate the encoding to extract the answer"
P07-2027,E06-1022,0,0.14175,"Missing"
P07-2027,H94-1020,0,0.0378558,"resents our features in detail. N 2 N 2 2 2 2 2 Table 1: Number of cases found. originally referential use (as the original addressee may not be the current addressee – see example (4)). We allowed a separate class for genuinely ambiguous cases. Switchboard explicitly tags “you know” when used as a discourse marker; as this (generic) case is common and seems trivial we removed it from our data. B: (4) A: Features All features used for classifier experiments were extracted from the Switchboard LDC Treebank 3 release, which includes transcripts, part of speech information using the Penn tagset (Marcus et al., 1994) and dialog act tags (Jurafsky et al., 1997). Features fell into four main categories:2 sentential features which capture lexical features of the utterance itself; part-of-speech features which capture shallow syntactic patterns; dialog act features capturing the discourse function of the current utterance and surrounding context; and context features which give oracle information (i.e., the correct generic/referential label) about preceding uses 2 46 46 46 2 2 Well, uh, I guess probably the last one I went to I met so many people that I had not seen in probably ten, over ten years. It was lik"
P07-2027,E06-1007,0,0.178699,"Missing"
P07-2027,W04-2319,0,0.0257365,"me. Oh yeah uh O K. It can also be important to distinguish between singular and plural reference, as in example (2) where the task is assigned to more than one person: A: (2) B: So y- so you guys will send to the rest of us um a version of um, this, and - the - uh, description With sugge- yeah, suggested improvements and - Use of “you” might therefore help us both in de∗ This work was supported by the CALO project (DARPA grant NBCH-D-03-0010) and ONR (MURI award N000140510388). The authors also thank John Niekrasz for annotating our test data. 1 (1,2) are taken from the ICSI Meeting Corpus (Shriberg et al., 2004); (3,4) from Switchboard (Godfrey et al., 1992). tecting the fact that a task is being assigned, and in identifying the owner. While there is an increasing body of work concerning addressee identification (Katzenmaier et al., 2004; Jovanovic et al., 2006), there is very little investigating the problem of second-person pronoun resolution, and it is this that we address here. Most cases of “you” do not in fact refer to the addressee but are generic, as in example (3); automatic referentiality classification is therefore very important. B: (3) 2 Well, usually what you do is just wait until you t"
P07-2027,J00-3003,0,0.16253,"Missing"
P07-2044,P06-1095,0,0.36598,"built a MaxEnt classifier that assigns each pair of events one of 6 relations from an augmented Timebank corpus. Their classifier relies on perfect features that were hand-tagged in the corpus, including tense, aspect, modality, polarity and event class. Pairwise agreement on tense and aspect are also included. In a second study, they applied rules of temporal transitivity to greatly expand the corpus, providing different results on this enlarged dataset. We could not duplicate their reported performance on this enlarged data, and instead focus on performing well on the Timebank data itself. Lapata and Lascarides (2006) trained an event classifier for inter-sentential events. They built a corpus by saving sentences that contained two events, one of which is triggered by a key time word (e.g. after and before). Their learner was based on syntax and clausal ordering features. Boguraev and Ando (2005) evaluated machine learning on related tasks, but not relevant to event-event classification. Our work is most similar to Mani’s in that we are Proceedings of the ACL 2007 Demo and Poster Sessions, pages 173–176, c Prague, June 2007. 2007 Association for Computational Linguistics learning relations given event pair"
P07-2049,P06-2020,0,0.200415,"Missing"
P07-2049,C00-1072,0,0.710836,"st, when the summary is produced in response to a user query or topic (query-focused, topic-focused, or generally focused summary), the topic/query determines what information is appropriate for inclusion in the summary, making the task potentially more challenging. In this paper we present an analytical study of two questions regarding aspects of the topic-focused scenario. First, two estimates of importance on words have been used very successfully both in generic and query-focused summarization: frequency (Luhn, 1958; Nenkova et al., 2006; Vanderwende et al., 2006) and loglikelihood ratio (Lin and Hovy, 2000; Conroy et al., 2006; Lacatusu et al., 2006). While both schemes have proved to be suitable for sum193 marization, with generally better results from loglikelihood ratio, no study has investigated in what respects and by how much they differ. Second, there are many little-understood aspects of the differences between generic and query-focused summarization. For example, we’d like to know if a particular word weighting scheme is more suitable for focused summarization than others. More significantly, previous studies show that generic and focused systems perform very similarly to each other in"
P07-2049,W04-1013,0,0.0342145,"nouns, verbs, adjectives and adverbs are considered and a short list of light verbs are excluded: “has, was, have, are, will, were, do, been, say, said, says”. For FOCUSED summarization, we modify this algorithm merely by running the sentence selection algorithm on only those sentences in the input that are relevent to the user query. In some previous DUC evaluations, relevant sentences are explicitly marked by annotators and given to systems. In our version here, a sentence in the input is considered relevant if it contains at least one word from the user query. For evaluation we use ROUGE (Lin, 2004) SU4 recall metric1 , which was among the official automatic evaluation metrics for DUC. 4 Results The results are shown in Table 1. The focused summarizer using LLR(CQ) is the best, and it significantly outperforms the focused summarizer based on frequency. Also, LLR (using log-likelihood ratio to assign weights to all words) perfroms significantly worse than LLR(C). We can observe some trends even from the results for which there is no significance. Both LLR and LLR(C) are sensitive to the introduction of topic relevance, producing somewhat better summaries in the FOCUSED scenario 1 -n 2 -x"
P08-1044,W96-0213,0,0.0946589,"nels (uh-huh, mm-hm), guesses (where the transcribers were unsure of the correct words), and full words (everything else). Error rates for each of these types can be found in Table 1. The remainder of our analysis considers only the 36159 invocabulary full words in the reference transcriptions (70 OOV full words are excluded). We collected the following features for these words: Speaker sex Male or female. Broad syntactic class Open class (e.g., nouns and verbs), closed class (e.g., prepositions and articles), or discourse marker (e.g., okay, well). Classes were identified using a POS tagger (Ratnaparkhi, 1996) trained on the tagged Switchboard corpus. Log probability The unigram log probability of each word, as listed in the system’s language model. Word length The length of each word (in phones), determined using the most frequent pronunciation BefRep FirRep MidRep LastRep AfRep BefFP AfFP BefFr AfFr yeah i i i think you should um ask for the ref- recommendation Figure 1: Example illustrating disfluency features: words occurring before and after repetitions, filled pauses, and fragments; first, middle, and last words in a repeated sequence. found for that word in the recognition lattices. 3.2 Resu"
P08-1090,P98-1013,0,0.262015,"eframe networks as a kind of contextual role knoweldge for anaphora resolution. A caseframe is a verb/event and a semantic role (e.g. &lt;patient&gt; kidnapped). Caseframe networks are relations between caseframes that may represent synonymy (&lt;patient&gt; kidnapped and &lt;patient&gt; abducted) or related events (&lt;patient&gt; kidnapped and &lt;patient&gt; released). Bean and Riloff learn these networks from two topic-specific texts and apply them to the problem of anaphora resolution. Our work can be seen as an attempt to generalize the intuition of caseframes (finding an entire set of events 1 We analyzed FrameNet (Baker et al., 1998) for insight, but found that very few of the frames are event sequences of the type characterizing narratives and scripts. 790 rather than just pairs of related frames) and apply it to a different task (finding a coherent structured narrative in non-topic-specific text). More recently, Brody (2007) proposed an approach similar to caseframes that discovers highlevel relatedness between verbs by grouping verbs that share the same lexical items in subject/object positions. He calls these shared arguments anchors. Brody learns pairwise relations between clusters of related verbs, similar to the re"
P08-1090,P05-1018,0,0.00605106,"rather than only pairwise lexical decisions. Finally, Fujiki et al. (2003) investigated script acquisition by extracting the 41 most frequent pairs of events from the first paragraph of newswire articles, using the assumption that the paragraph’s textual order follows temporal order. Our model, by contrast, learns entire event chains, uses more sophisticated probabilistic measures, and uses temporal ordering models instead of relying on document order. 3 3.1 The Narrative Chain Model Definition Our model is inspired by Centering (Grosz et al., 1995) and other entity-based models of coherence (Barzilay and Lapata, 2005) in which an entity is in focus through a sequence of sentences. We propose to use this same intuition to induce narrative chains. We assume that although a narrative has several participants, there is a central actor who characterizes a narrative chain: the protagonist. Narrative chains are thus structured by the protagonist’s grammatical roles in the events. In addition, narrative events are ordered by some theory of time. This paper describes a partial ordering with the before (no overlap) relation. Our task, therefore, is to learn events that constitute narrative chains. Formally, a narrat"
P08-1090,N04-1038,0,0.0112226,"nts. Finally, the space of narrative events and temporal orders is clustered and pruned to create discrete sets of narrative chains. 2 Previous Work While previous work hasn’t focused specifically on learning narratives1 , our work draws from two lines of research in summarization and anaphora resolution. In summarization, topic signatures are a set of terms indicative of a topic (Lin and Hovy, 2000). They are extracted from hand-sorted (by topic) sets of documents using log-likelihood ratios. These terms can capture some narrative relations, but the model requires topic-sorted training data. Bean and Riloff (2004) proposed the use of caseframe networks as a kind of contextual role knoweldge for anaphora resolution. A caseframe is a verb/event and a semantic role (e.g. &lt;patient&gt; kidnapped). Caseframe networks are relations between caseframes that may represent synonymy (&lt;patient&gt; kidnapped and &lt;patient&gt; abducted) or related events (&lt;patient&gt; kidnapped and &lt;patient&gt; released). Bean and Riloff learn these networks from two topic-specific texts and apply them to the problem of anaphora resolution. Our work can be seen as an attempt to generalize the intuition of caseframes (finding an entire set of events"
P08-1090,P07-1057,0,0.00731615,"nt&gt; kidnapped and &lt;patient&gt; released). Bean and Riloff learn these networks from two topic-specific texts and apply them to the problem of anaphora resolution. Our work can be seen as an attempt to generalize the intuition of caseframes (finding an entire set of events 1 We analyzed FrameNet (Baker et al., 1998) for insight, but found that very few of the frames are event sequences of the type characterizing narratives and scripts. 790 rather than just pairs of related frames) and apply it to a different task (finding a coherent structured narrative in non-topic-specific text). More recently, Brody (2007) proposed an approach similar to caseframes that discovers highlevel relatedness between verbs by grouping verbs that share the same lexical items in subject/object positions. He calls these shared arguments anchors. Brody learns pairwise relations between clusters of related verbs, similar to the results with caseframes. A human evaluation of these pairs shows an improvement over baseline. This and previous caseframe work lend credence to learning relations from verbs with common arguments. We also draw from lexical chains (Morris and Hirst, 1991), indicators of text coherence from word overl"
P08-1090,P07-2044,1,0.741104,"Missing"
P08-1090,W04-3205,0,0.0171432,"s in subject/object positions. He calls these shared arguments anchors. Brody learns pairwise relations between clusters of related verbs, similar to the results with caseframes. A human evaluation of these pairs shows an improvement over baseline. This and previous caseframe work lend credence to learning relations from verbs with common arguments. We also draw from lexical chains (Morris and Hirst, 1991), indicators of text coherence from word overlap/similarity. We use a related notion of protagonist overlap to motivate narrative chain learning. Work on semantic similarity learning such as Chklovski and Pantel (2004) also automatically learns relations between verbs. We use similar distributional scoring metrics, but differ with our use of a protagonist as the indicator of relatedness. We also use typed dependencies and the entire space of events for similarity judgements, rather than only pairwise lexical decisions. Finally, Fujiki et al. (2003) investigated script acquisition by extracting the 41 most frequent pairs of events from the first paragraph of newswire articles, using the assumption that the paragraph’s textual order follows temporal order. Our model, by contrast, learns entire event chains, u"
P08-1090,de-marneffe-etal-2006-generating,0,0.0116931,"Missing"
P08-1090,E03-1061,0,0.0333632,"guments. We also draw from lexical chains (Morris and Hirst, 1991), indicators of text coherence from word overlap/similarity. We use a related notion of protagonist overlap to motivate narrative chain learning. Work on semantic similarity learning such as Chklovski and Pantel (2004) also automatically learns relations between verbs. We use similar distributional scoring metrics, but differ with our use of a protagonist as the indicator of relatedness. We also use typed dependencies and the entire space of events for similarity judgements, rather than only pairwise lexical decisions. Finally, Fujiki et al. (2003) investigated script acquisition by extracting the 41 most frequent pairs of events from the first paragraph of newswire articles, using the assumption that the paragraph’s textual order follows temporal order. Our model, by contrast, learns entire event chains, uses more sophisticated probabilistic measures, and uses temporal ordering models instead of relying on document order. 3 3.1 The Narrative Chain Model Definition Our model is inspired by Centering (Grosz et al., 1995) and other entity-based models of coherence (Barzilay and Lapata, 2005) in which an entity is in focus through a sequen"
P08-1090,J95-2003,0,0.065801,"ncies and the entire space of events for similarity judgements, rather than only pairwise lexical decisions. Finally, Fujiki et al. (2003) investigated script acquisition by extracting the 41 most frequent pairs of events from the first paragraph of newswire articles, using the assumption that the paragraph’s textual order follows temporal order. Our model, by contrast, learns entire event chains, uses more sophisticated probabilistic measures, and uses temporal ordering models instead of relying on document order. 3 3.1 The Narrative Chain Model Definition Our model is inspired by Centering (Grosz et al., 1995) and other entity-based models of coherence (Barzilay and Lapata, 2005) in which an entity is in focus through a sequence of sentences. We propose to use this same intuition to induce narrative chains. We assume that although a narrative has several participants, there is a central actor who characterizes a narrative chain: the protagonist. Narrative chains are thus structured by the protagonist’s grammatical roles in the events. In addition, narrative events are ordered by some theory of time. This paper describes a partial ordering with the before (no overlap) relation. Our task, therefore,"
P08-1090,P06-1095,0,0.0121413,"Missing"
P08-1090,C00-1072,0,0.00547159,"te partial orders of our learned events. We show, using a coherence-based evaluation of temporal ordering, that our partial orders lead to better coherence judgements of real narrative instances extracted from documents. Finally, the space of narrative events and temporal orders is clustered and pruned to create discrete sets of narrative chains. 2 Previous Work While previous work hasn’t focused specifically on learning narratives1 , our work draws from two lines of research in summarization and anaphora resolution. In summarization, topic signatures are a set of terms indicative of a topic (Lin and Hovy, 2000). They are extracted from hand-sorted (by topic) sets of documents using log-likelihood ratios. These terms can capture some narrative relations, but the model requires topic-sorted training data. Bean and Riloff (2004) proposed the use of caseframe networks as a kind of contextual role knoweldge for anaphora resolution. A caseframe is a verb/event and a semantic role (e.g. &lt;patient&gt; kidnapped). Caseframe networks are relations between caseframes that may represent synonymy (&lt;patient&gt; kidnapped and &lt;patient&gt; abducted) or related events (&lt;patient&gt; kidnapped and &lt;patient&gt; released). Bean and Ril"
P08-1090,1985.tmi-1.17,0,0.0542763,"s can be filled in and instantiated in a particular text situation to draw inferences. Chains focus on a single actor to faciliIt would be useful for question answering or textual entailment to know that ‘X denied ’ is also a likely event in the left chain, while ‘ replaces W’ temporally follows the right. Narrative chains (such as Firing of Employee or Executive Resigns) offer the structure and power to directly infer these new subevents by providing critical background knowledge. In part due to its complexity, automatic induction has not been addressed since the early nonstatistical work of Mooney and DeJong (1985). The first step to narrative induction uses an entitybased model for learning narrative relations by fol789 Proceedings of ACL-08: HLT, pages 789–797, c Columbus, Ohio, USA, June 2008. 2008 Association for Computational Linguistics lowing a protagonist. As a narrative progresses through a series of events, each event is characterized by the grammatical role played by the protagonist, and by the protagonist’s shared connection to surrounding events. Our algorithm is an unsupervised distributional learning approach that uses coreferring arguments as evidence of a narrative relation. We show, us"
P08-1090,J91-1002,0,0.0323474,"ed narrative in non-topic-specific text). More recently, Brody (2007) proposed an approach similar to caseframes that discovers highlevel relatedness between verbs by grouping verbs that share the same lexical items in subject/object positions. He calls these shared arguments anchors. Brody learns pairwise relations between clusters of related verbs, similar to the results with caseframes. A human evaluation of these pairs shows an improvement over baseline. This and previous caseframe work lend credence to learning relations from verbs with common arguments. We also draw from lexical chains (Morris and Hirst, 1991), indicators of text coherence from word overlap/similarity. We use a related notion of protagonist overlap to motivate narrative chain learning. Work on semantic similarity learning such as Chklovski and Pantel (2004) also automatically learns relations between verbs. We use similar distributional scoring metrics, but differ with our use of a protagonist as the indicator of relatedness. We also use typed dependencies and the entire space of events for similarity judgements, rather than only pairwise lexical decisions. Finally, Fujiki et al. (2003) investigated script acquisition by extracting"
P08-1090,N04-1041,0,0.00835113,"ence). Given a list of observed verb/dependency counts, we approximate the pointwise mutual information (PMI) by: pmi(e(w, d), e(v, g)) = log P (e(w, d), e(v, g)) (1) P (e(w, d))P (e(v, g)) where e(w, d) is the verb/dependency pair w and d (e.g. e(push,subject)). The numerator is defined by: C(e(w, d), e(v, g)) P x,y d,f C(e(x, d), e(y, f )) (2) P (e(w, d), e(v, g)) = P where C(e(x, d), e(y, f )) is the number of times the two events e(x, d) and e(y, f ) had a coreferring entity filling the values of the dependencies d and f . We also adopt the ‘discount score’ to penalize low occuring words (Pantel and Ravichandran, 2004). Given the debate over appropriate metrics for distributional learning, we also experimented with the t-test. Our experiments found that PMI outperforms the t-test on this task by itself and when interpolated together using various mixture weights. Once pairwise relation scores are calculated, a global narrative score can then be built such that all events provide feedback on the event in question. For instance, given all narrative events in a document, we can find the next most likely event to occur by maximizing: max j:0&lt;j&lt;m n X pmi(ei , fj ) (3) i=0 where n is the number of events in our c"
P08-1090,C98-1013,0,\N,Missing
P08-1090,J08-1001,0,\N,Missing
P09-1034,P06-2003,0,0.0627553,"Missing"
P09-1034,W05-0909,0,0.0605981,"e system level; (4) scores are biased towards statistical MT; (5) the quality gap between MT and human translations is not reflected in equally large BLEU differences. ∗ This paper is based on work funded by the Defense Advanced Research Projects Agency through IBM. The content does not necessarily reflect the views of the U.S. Government, and no official endorsement should be inferred. A number of metrics have been designed to account for paraphrase, either by making the matching more intelligent (TER, Snover et al. (2006)), or by using linguistic evidence, mostly lexical similarity (METEOR, Banerjee and Lavie (2005); MaxSim, Chan and Ng (2008)), or syntactic overlap (Owczarzak et al. (2008); Liu and Gildea (2005)). Unfortunately, each metrics tend to concentrate on one particular type of linguistic information, none of which always correlates well with human judgments. Our paper proposes two strategies. We first explore the combination of traditional scores into a more robust ensemble metric with linear regression. Our second, more fundamental, strategy replaces the use of loose surrogates of translation quality with a model that attempts to comprehensively assess meaning equivalence between references a"
P09-1034,2005.mtsummit-papers.11,0,0.00383163,"Missing"
P09-1034,C04-1072,0,0.0380796,"y 5–10 points.1 4.3 Baseline Metrics We consider four baselines. They are small regression models as described in Section 2 over component scores of four widely used MT metrics. To alleviate possible nonlinearity, we add all features in linear and log space. Each baselines carries the name of the underlying metric plus the suffix -R.2 B LEU R includes the following 18 sentence-level scores: BLEU-n and n-gram precision scores (1 ≤ n ≤ 4); BLEU brevity penalty (BP); BLEU score divided by BP. To counteract BLEU’s brittleness at the sentence level, we also smooth BLEU-n and n-gram precision as in Lin and Och (2004). N IST R consists of 16 features. NIST-n scores (1 ≤ n ≤ 10) and information-weighted n-gram precision scores (1 ≤ n ≤ 4); NIST brevity penalty (BP); and NIST score divided by BP. 1 Due to space constraints, we only show results for “tieaware” predictions. See Pad´o et al. (2009) for a discussion. 2 The regression models can simulate the behaviour of each component by setting the weights appropriately, but are strictly more powerful. A possible danger is that the parameters overfit on the training set. We therefore verified that the three non-trivial “baseline” regression models indeed confer"
P09-1034,W05-0904,0,0.324729,"translations is not reflected in equally large BLEU differences. ∗ This paper is based on work funded by the Defense Advanced Research Projects Agency through IBM. The content does not necessarily reflect the views of the U.S. Government, and no official endorsement should be inferred. A number of metrics have been designed to account for paraphrase, either by making the matching more intelligent (TER, Snover et al. (2006)), or by using linguistic evidence, mostly lexical similarity (METEOR, Banerjee and Lavie (2005); MaxSim, Chan and Ng (2008)), or syntactic overlap (Owczarzak et al. (2008); Liu and Gildea (2005)). Unfortunately, each metrics tend to concentrate on one particular type of linguistic information, none of which always correlates well with human judgments. Our paper proposes two strategies. We first explore the combination of traditional scores into a more robust ensemble metric with linear regression. Our second, more fundamental, strategy replaces the use of loose surrogates of translation quality with a model that attempts to comprehensively assess meaning equivalence between references and MT hypotheses. We operationalize meaning equivalence by bidirectional textual entailment (RTE, D"
P09-1034,E06-1032,0,0.104031,"tnesses are terming it terrorism. Introduction Constant evaluation is vital to the progress of machine translation (MT). Since human evaluation is costly and difficult to do reliably, a major focus of research has been on automatic measures of MT quality, pioneered by BLEU (Papineni et al., 2002) and NIST (Doddington, 2002). BLEU and NIST measure MT quality by using the strong correlation between human judgments and the degree of n-gram overlap between a system hypothesis translation and one or more reference translations. The resulting scores are cheap and objective. However, studies such as Callison-Burch et al. (2006) have identified a number of problems with BLEU and related n-gram-based scores: (1) BLEUlike metrics are unreliable at the level of individual sentences due to data sparsity; (2) BLEU metrics can be “gamed” by permuting word order; (3) for some corpora and languages, the correlation to human ratings is very low even at the system level; (4) scores are biased towards statistical MT; (5) the quality gap between MT and human translations is not reflected in equally large BLEU differences. ∗ This paper is based on work funded by the Defense Advanced Research Projects Agency through IBM. The conte"
P09-1034,C08-1066,1,0.800593,"ile the original RTE task is asymmetric, MT evaluation needs to determine meaning equivalence, which is a symmetric relation. We do this by checking for entailment in both directions (see Figure 1). Operationally, this ensures we detect translations which either delete or insert material. Clearly, there are also differences between the two tasks. An important one is that RTE assumes the well-formedness of the two sentences. This is not generally true in MT, and could lead to degraded linguistic analyses. However, entailment relations are more sensitive to the contribution of individual words (MacCartney and Manning, 2008). In Example 2, the modal modifiers break the entailment between two otherwise identical sentences: (2) HYP: Peter is certainly from Lincolnshire. REF: Peter is possibly from Lincolnshire. This means that the prediction of TE hinges on correct semantic analysis and is sensitive to misanalyses. In contrast, human MT judgments behave robustly. Translations that involve individual errors, like (2), are judged lower than perfect ones, but usually not crucially so, since most aspects are still rendered correctly. We thus expect even noisy RTE features to be predictive for translation quality. This"
P09-1034,W08-0309,0,0.0715095,"Gibbs sampling (see de Marneffe et al. (2007)). Entailment features. In the third stage, the system produces roughly 100 features for each aligned premise-hypothesis pair. A small number of them are real-valued (mostly quality scores), but most are binary implementations of small linguistic theories whose activation indicates syntactic and se4 4.1 Experimental Evaluation Experiments Traditionally, human ratings for MT quality have been collected in the form of absolute scores on a five- or seven-point Likert scale, but low reliability numbers for this type of annotation have raised concerns (Callison-Burch et al., 2008). An alternative that has been adopted by the yearly WMT evaluation shared tasks since 2008 is the collection of pairwise preference judgments between pairs of MT hypotheses which can be elicited (somewhat) more reliably. We demonstrate that our approach works well for both types of annotation and different corpora. Experiment 1 models absolute scores on Asian newswire, and Experiment 2 pairwise preferences on European speech and news data. 4.2 Evaluation We evaluate the output of our models both on the sentence and on the system level. At the sentence level, we can correlate predictions in Ex"
P09-1034,N06-1006,1,0.583147,"Missing"
P09-1034,P08-1007,0,0.286549,"ed towards statistical MT; (5) the quality gap between MT and human translations is not reflected in equally large BLEU differences. ∗ This paper is based on work funded by the Defense Advanced Research Projects Agency through IBM. The content does not necessarily reflect the views of the U.S. Government, and no official endorsement should be inferred. A number of metrics have been designed to account for paraphrase, either by making the matching more intelligent (TER, Snover et al. (2006)), or by using linguistic evidence, mostly lexical similarity (METEOR, Banerjee and Lavie (2005); MaxSim, Chan and Ng (2008)), or syntactic overlap (Owczarzak et al. (2008); Liu and Gildea (2005)). Unfortunately, each metrics tend to concentrate on one particular type of linguistic information, none of which always correlates well with human judgments. Our paper proposes two strategies. We first explore the combination of traditional scores into a more robust ensemble metric with linear regression. Our second, more fundamental, strategy replaces the use of loose surrogates of translation quality with a model that attempts to comprehensively assess meaning equivalence between references and MT hypotheses. We operati"
P09-1034,I08-1042,0,0.0473301,"Missing"
P09-1034,P06-1114,0,0.0466837,". 3 Textual Entailment vs. MT Evaluation Our novel approach to MT evaluation exploits the similarity between MT evaluation and textual entailment (TE). TE was introduced by Dagan et al. (2005) as a concept that corresponds more closely to “common sense” reasoning patterns than classical, strict logical entailment. Textual entailment is defined informally as a relation between two natural language sentences (a premise P and a hypothesis H) that holds if “a human reading P would infer that H is most likely true”. Knowledge about entailment is beneficial for NLP tasks such as Question Answering (Harabagiu and Hickl, 2006). The relation between textual entailment and MT evaluation is shown in Figure 1. Perfect MT output and the reference translation entail each other (top). Translation problems that impact semantic equivalence, e.g., deletion or addition of material, can break entailment in one or both directions (bottom). On the modelling level, there is common ground between RTE and MT evaluation: Both have to distinguish between valid and invalid variation to determine whether two texts convey the same information or not. For example, to recognize the bidirectional entailment in Ex. (1), RTE must account for"
P09-1034,hovy-etal-2006-automated,0,0.0208131,"a translation may contain many long matches while lacking coherence and grammaticality (cf. the bottom example in Table 2). Thus, incorporation of syntactic knowledge has been the focus of another line of research. Amig´o et al. (2006) use the degree of overlap between the dependency trees of reference and hypothesis as a predictor of translation quality. Similar ideas have been applied by Owczarzak et al. (2008) to LFG parses, and by Liu and Gildea (2005) to features derived from phrase-structure tress. This approach has also been successful for the related task of summarization evaluation (Hovy et al., 2006). The most comparable work to ours is Gim´enez and M´arquez (2008). Our results agree on the crucial point that the use of a wide range of linguistic knowledge in MT evaluation is desirable and important. However, Gim´enez and M´arquez advocate the use of a bottom-up development process that builds on a set of “heterogeneous”, independent metrics each of which measures overlap with respect to one linguistic level. In contrast, our aim is to provide a “top-down”, integrated motivation for the features we integrate through the textual entailment recognition paradigm. 8 Conclusion and Outlook In"
P09-1034,N06-1058,0,0.0523827,"ependents of predicates is unreliable on the WMT data. Other differences do reflect more fundamental differences between the two tasks (cf. Section 3). For example, RTE puts high weights onto quantifier and polarity features, both of which have the potential of influencing entailment decisions, but are (at least currently) unimportant for MT evaluation. 7 Related Work Researchers have exploited various resources to enable the matching between words or n-grams that are semantically close but not identical. Banerjee and Lavie (2005) and Chan and Ng (2008) use WordNet, and Zhou et al. (2006) and Kauchak and Barzilay (2006) exploit large collections of automatically-extracted paraphrases. These approaches reduce the risk that a good translation is rated poorly due to lexical deviation, but do not address the problem that a translation may contain many long matches while lacking coherence and grammaticality (cf. the bottom example in Table 2). Thus, incorporation of syntactic knowledge has been the focus of another line of research. Amig´o et al. (2006) use the degree of overlap between the dependency trees of reference and hypothesis as a predictor of translation quality. Similar ideas have been applied by Owcza"
P09-1034,P03-1021,0,0.00335176,"Missing"
P09-1034,W09-0404,1,0.763215,"Missing"
P09-1034,P02-1040,0,0.104893,"fferent settings. The combination metric outperforms the individual scores, but is bested by the entailment-based metric. Combining the entailment and traditional features yields further improvements. 1 (1) HYP: However, this was declared terrorism by observers and witnesses. REF: Nevertheless, commentators as well as eyewitnesses are terming it terrorism. Introduction Constant evaluation is vital to the progress of machine translation (MT). Since human evaluation is costly and difficult to do reliably, a major focus of research has been on automatic measures of MT quality, pioneered by BLEU (Papineni et al., 2002) and NIST (Doddington, 2002). BLEU and NIST measure MT quality by using the strong correlation between human judgments and the degree of n-gram overlap between a system hypothesis translation and one or more reference translations. The resulting scores are cheap and objective. However, studies such as Callison-Burch et al. (2006) have identified a number of problems with BLEU and related n-gram-based scores: (1) BLEUlike metrics are unreliable at the level of individual sentences due to data sparsity; (2) BLEU metrics can be “gamed” by permuting word order; (3) for some corpora and languages,"
P09-1034,2007.tmi-papers.19,0,0.082174,"ment REF: Three aid workers were kidnapped by pirates. Figure 1: Entailment status between an MT system hypothesis and a reference translation for equivalent (top) and non-equivalent (bottom) translations. 2 Regression-based MT Quality Prediction Current MT metrics tend to focus on a single dimension of linguistic information. Since the importance of these dimensions tends not to be stable across language pairs, genres, and systems, performance of these metrics varies substantially. A simple strategy to overcome this problem could be to combine the judgments of different metrics. For example, Paul et al. (2007) train binary classifiers on a feature set formed by a number of MT metrics. We follow a similar idea, but use a regularized linear regression to directly predict human ratings. Feature combination via regression is a supervised approach that requires labeled data. As we show in Section 5, this data is available, and the resulting model generalizes well from relatively small amounts of training data. 3 Textual Entailment vs. MT Evaluation Our novel approach to MT evaluation exploits the similarity between MT evaluation and textual entailment (TE). TE was introduced by Dagan et al. (2005) as a"
P09-1034,2006.amta-papers.25,0,0.0532152,"(3) for some corpora and languages, the correlation to human ratings is very low even at the system level; (4) scores are biased towards statistical MT; (5) the quality gap between MT and human translations is not reflected in equally large BLEU differences. ∗ This paper is based on work funded by the Defense Advanced Research Projects Agency through IBM. The content does not necessarily reflect the views of the U.S. Government, and no official endorsement should be inferred. A number of metrics have been designed to account for paraphrase, either by making the matching more intelligent (TER, Snover et al. (2006)), or by using linguistic evidence, mostly lexical similarity (METEOR, Banerjee and Lavie (2005); MaxSim, Chan and Ng (2008)), or syntactic overlap (Owczarzak et al. (2008); Liu and Gildea (2005)). Unfortunately, each metrics tend to concentrate on one particular type of linguistic information, none of which always correlates well with human judgments. Our paper proposes two strategies. We first explore the combination of traditional scores into a more robust ensemble metric with linear regression. Our second, more fundamental, strategy replaces the use of loose surrogates of translation quali"
P09-1034,W09-0441,0,0.0147185,"tness and improved correlations for the regression models. An exception is BLEU-1 and NIST-4 on Expt. 1 (Ar, Ch), which perform 0.5–1 point better at the sentence level. T ER R includes 50 features. We start with the standard TER score and the number of each of the four edit operations. Since the default uniform cost does not always correlate well with human judgment, we duplicate these features for 9 non-uniform edit costs. We find it effective to set insertion cost close to 0, as a way of enabling surface variation, and indeed the new TERp metric uses a similarly low default insertion cost (Snover et al., 2009). M ETEOR R 4.4 consists of METEOR v0.7. Combination Metrics The following three regression models implement the methods discussed in Sections 2 and 3. M T R combines the 85 features of the four baseline models. It uses no entailment features. RTE R uses the 70 entailment features described in Section 3.1, but no M T R features. M T +RTE R uses all M T R and RTE R features, combining matching and entailment evidence.3 5 Expt. 1: Predicting Absolute Scores Data. Our first experiment evaluates the models we have proposed on a corpus with traditional annotation on a seven-point scale, namely the"
P09-1034,W06-1610,0,0.214402,"or mismatches between dependents of predicates is unreliable on the WMT data. Other differences do reflect more fundamental differences between the two tasks (cf. Section 3). For example, RTE puts high weights onto quantifier and polarity features, both of which have the potential of influencing entailment decisions, but are (at least currently) unimportant for MT evaluation. 7 Related Work Researchers have exploited various resources to enable the matching between words or n-grams that are semantically close but not identical. Banerjee and Lavie (2005) and Chan and Ng (2008) use WordNet, and Zhou et al. (2006) and Kauchak and Barzilay (2006) exploit large collections of automatically-extracted paraphrases. These approaches reduce the risk that a good translation is rated poorly due to lexical deviation, but do not address the problem that a translation may contain many long matches while lacking coherence and grammaticality (cf. the bottom example in Table 2). Thus, incorporation of syntactic knowledge has been the focus of another line of research. Amig´o et al. (2006) use the degree of overlap between the dependency trees of reference and hypothesis as a predictor of translation quality. Similar"
P09-1034,W07-1401,0,\N,Missing
P09-1068,P98-1013,0,0.820001,"ive interpretation of syntax and word use. Knowledge structures such as these provided the interpreter rich information about many aspects of meaning. The problem with these rich knowledge structures is that the need for hand construction, specificity, and domain dependence prevents robust and flexible language understanding. Instead, modern work on understanding has focused on shallower representations like semantic roles, which express at least one aspect of the semantics of events and have proved amenable to supervised learning from corpora like PropBank (Palmer et al., 2005) and Framenet (Baker et al., 1998). Unfortunately, creating these supervised corpora is an expensive and difficult multi-year effort, requiring complex decisions about the exact set of roles to Events Roles A search B A = Police B = Suspect C = Plea D = Jury A arrest B B plead C D acquit B D convict B D sentence B Being able to robustly learn sets of related events (left) and frame-specific role information about the argument types that fill them (right) could assist a variety of NLP applications, from question answering to machine translation. Our previous work (Chambers and Jurafsky, 2008) relied on the intuition that in a c"
P09-1068,N04-1038,0,0.00846275,"t the argument types that fill them (right) could assist a variety of NLP applications, from question answering to machine translation. Our previous work (Chambers and Jurafsky, 2008) relied on the intuition that in a coherent text, any two events that are about the same participants are likely to be part of the same story or narrative. The model learned simple aspects of narrative structure (‘narrative chains’) by extracting events that share a single participant, the protagonist. In this paper we extend this work to represent sets of situation-specific events not unlike scripts, caseframes (Bean and Riloff, 2004), and FrameNet frames (Baker et al., 1998). This paper shows that verbs in distinct narrative chains can be merged into an improved single narrative schema, while the shared arguments across verbs can provide rich information for inducing semantic roles. 602 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 602–610, c Suntec, Singapore, 2-7 August 2009. 2009 ACL and AFNLP 2 Background Our previous work, however, has two major limitations. First, the model did not express any information about the protagonist, such as its type or role. Role information (su"
P09-1068,P08-1090,1,0.409958,"PropBank (Palmer et al., 2005) and Framenet (Baker et al., 1998). Unfortunately, creating these supervised corpora is an expensive and difficult multi-year effort, requiring complex decisions about the exact set of roles to Events Roles A search B A = Police B = Suspect C = Plea D = Jury A arrest B B plead C D acquit B D convict B D sentence B Being able to robustly learn sets of related events (left) and frame-specific role information about the argument types that fill them (right) could assist a variety of NLP applications, from question answering to machine translation. Our previous work (Chambers and Jurafsky, 2008) relied on the intuition that in a coherent text, any two events that are about the same participants are likely to be part of the same story or narrative. The model learned simple aspects of narrative structure (‘narrative chains’) by extracting events that share a single participant, the protagonist. In this paper we extend this work to represent sets of situation-specific events not unlike scripts, caseframes (Bean and Riloff, 2004), and FrameNet frames (Baker et al., 1998). This paper shows that verbs in distinct narrative chains can be merged into an improved single narrative schema, whil"
P09-1068,de-marneffe-etal-2006-generating,0,0.0120698,"Missing"
P09-1068,W05-1007,0,0.0167345,"meNet (Baker et al., 1998) or VerbNet (Kipper et al., 2000) as gold standard roles and training data. More recent learning work has applied bootstrapping approaches (Swier and Stevenson, 2004; He and Gildea, 2006), but these still rely on a hand labeled seed corpus as well as a pre-defined set of roles. Grenegar and Manning (2006) use the EM algorithm to learn PropBank roles from unlabeled data, and unlike bootstrapping, they don’t need a labeled corpus from which to start. However, they do require a predefined set of roles (arg0, arg1, etc.) to define the domain of their probabilistic model. Green and Dorr (2005) use WordNet’s graph structure to cluster its verbs into FrameNet frames, using glosses to name potential slots. We differ in that we attempt to learn frame-like narrative structure from untagged newspaper text. Most similar to us, Alishahi and Stevenson (2007) learn verb specific semantic profiles of arguments using WordNet classes to define the roles. We learn situation-specific classes of roles shared by multiple verbs. Thus, two open goals in role learning include (1) unsupervised learning and (2) learning the roles themselves rather than relying on pre-defined role classes. As just descri"
P09-1068,W06-1601,0,0.0646395,"Missing"
P09-1068,J05-1004,0,0.334704,"heir participants) and frames to drive interpretation of syntax and word use. Knowledge structures such as these provided the interpreter rich information about many aspects of meaning. The problem with these rich knowledge structures is that the need for hand construction, specificity, and domain dependence prevents robust and flexible language understanding. Instead, modern work on understanding has focused on shallower representations like semantic roles, which express at least one aspect of the semantics of events and have proved amenable to supervised learning from corpora like PropBank (Palmer et al., 2005) and Framenet (Baker et al., 1998). Unfortunately, creating these supervised corpora is an expensive and difficult multi-year effort, requiring complex decisions about the exact set of roles to Events Roles A search B A = Police B = Suspect C = Plea D = Jury A arrest B B plead C D acquit B D convict B D sentence B Being able to robustly learn sets of related events (left) and frame-specific role information about the argument types that fill them (right) could assist a variety of NLP applications, from question answering to machine translation. Our previous work (Chambers and Jurafsky, 2008) r"
P09-1068,W04-3213,0,\N,Missing
P09-1068,C98-1013,0,\N,Missing
P11-1098,P98-1013,0,0.048966,"discovery that focuses on learning atomic facts (Banko et al., 2007a; Carlson et al., 2010), templates can extract a richer representation of a particular domain. However, unlike relation discovery, most template-based IE approaches assume foreknowledge of the domain’s templates. Very little work addresses how to learn the template structure 976 itself. Our goal in this paper is to perform the standard template filling task, but to first automatically induce the templates from an unlabeled corpus. There are many ways to represent events, ranging from role-based representations such as frames (Baker et al., 1998) to sequential events in scripts (Schank and Abelson, 1977) and narrative schemas (Chambers and Jurafsky, 2009; Kasch and Oates, 2010). Our approach learns narrative-like knowledge in the form of IE templates; we learn sets of related events and semantic roles, as shown in this sample output from our system: Bombing Template {detonate, blow up, plant, explode, defuse, destroy} Perpetrator: Person who detonates, plants, blows up Instrument: Object that is planted, detonated, defused Target: Object that is destroyed, is blown up A semantic role, such as target, is a cluster of syntactic function"
P11-1098,P04-1056,0,0.0983946,"his paper focuses on how to characterize a domain-specific corpus by learning rich template structure. We describe how to first expand the small corpus’ size, how to cluster its events, and finally how to induce semantic roles. Section 5 then describes the extraction algorithm, followed by evaluations against previous work in section 6 and 7. 2 Previous Work Many template extraction algorithms require full knowledge of the templates and labeled corpora, such as in rule-based systems (Chinchor et al., 1993; Rau et al., 1992) and modern supervised classifiers (Freitag, 1998; Chieu et al., 2003; Bunescu and Mooney, 2004; Patwardhan and Riloff, 2009). Classifiers rely on the labeled examples’ surrounding context for features such as nearby tokens, document position, syntax, named entities, semantic classes, and discourse relations (Maslennikov and Chua, 2007). Ji and Grishman (2008) also supplemented labeled with unlabeled data. Weakly supervised approaches remove some of the need for fully labeled data. Most still require the templates and their slots. One common approach is to begin with unlabeled, but clustered event-specific documents, and extract common word patterns as extractors (Riloff and Schmelzenba"
P11-1098,P08-1090,1,0.888564,"a of using unlabeled documents to discover relations in text, and of defining semantic roles by sets of entities. However, the limitations to their approach are that (1) redundant documents about specific events are required, (2) relations are binary, and (3) only slots with named entities are learned. We will extend their work by showing how to learn without these assumptions, obviating the need for redundant documents, and learning templates with any type and any number of slots. Large-scale learning of scripts and narrative schemas also captures template-like knowledge from unlabeled text (Chambers and Jurafsky, 2008; Kasch and Oates, 2010). Scripts are sets of related event words and semantic roles learned by linking syntactic functions with coreferring arguments. While they learn interesting event structure, the structures are limited to frequent topics in a large corpus. We borrow ideas from this work as well, but our goal is to instead characterize a specific domain with limited data. Further, we are the first to apply this knowledge to the IE task of filling in template mentions in documents. In summary, our work extends previous work on unsupervised IE in a number of ways. We are the first to learn"
P11-1098,P09-1068,1,0.897926,"can extract a richer representation of a particular domain. However, unlike relation discovery, most template-based IE approaches assume foreknowledge of the domain’s templates. Very little work addresses how to learn the template structure 976 itself. Our goal in this paper is to perform the standard template filling task, but to first automatically induce the templates from an unlabeled corpus. There are many ways to represent events, ranging from role-based representations such as frames (Baker et al., 1998) to sequential events in scripts (Schank and Abelson, 1977) and narrative schemas (Chambers and Jurafsky, 2009; Kasch and Oates, 2010). Our approach learns narrative-like knowledge in the form of IE templates; we learn sets of related events and semantic roles, as shown in this sample output from our system: Bombing Template {detonate, blow up, plant, explode, defuse, destroy} Perpetrator: Person who detonates, plants, blows up Instrument: Object that is planted, detonated, defused Target: Object that is destroyed, is blown up A semantic role, such as target, is a cluster of syntactic functions of the template’s event words (e.g., the objects of detonate and explode). Our goal is to characterize a dom"
P11-1098,P03-1028,0,0.228003,"stems. The core of this paper focuses on how to characterize a domain-specific corpus by learning rich template structure. We describe how to first expand the small corpus’ size, how to cluster its events, and finally how to induce semantic roles. Section 5 then describes the extraction algorithm, followed by evaluations against previous work in section 6 and 7. 2 Previous Work Many template extraction algorithms require full knowledge of the templates and labeled corpora, such as in rule-based systems (Chinchor et al., 1993; Rau et al., 1992) and modern supervised classifiers (Freitag, 1998; Chieu et al., 2003; Bunescu and Mooney, 2004; Patwardhan and Riloff, 2009). Classifiers rely on the labeled examples’ surrounding context for features such as nearby tokens, document position, syntax, named entities, semantic classes, and discourse relations (Maslennikov and Chua, 2007). Ji and Grishman (2008) also supplemented labeled with unlabeled data. Weakly supervised approaches remove some of the need for fully labeled data. Most still require the templates and their slots. One common approach is to begin with unlabeled, but clustered event-specific documents, and extract common word patterns as extracto"
P11-1098,J93-3001,0,0.72418,". We evaluate on the MUC-4 terrorism corpus with results approaching those of supervised systems. The core of this paper focuses on how to characterize a domain-specific corpus by learning rich template structure. We describe how to first expand the small corpus’ size, how to cluster its events, and finally how to induce semantic roles. Section 5 then describes the extraction algorithm, followed by evaluations against previous work in section 6 and 7. 2 Previous Work Many template extraction algorithms require full knowledge of the templates and labeled corpora, such as in rule-based systems (Chinchor et al., 1993; Rau et al., 1992) and modern supervised classifiers (Freitag, 1998; Chieu et al., 2003; Bunescu and Mooney, 2004; Patwardhan and Riloff, 2009). Classifiers rely on the labeled examples’ surrounding context for features such as nearby tokens, document position, syntax, named entities, semantic classes, and discourse relations (Maslennikov and Chua, 2007). Ji and Grishman (2008) also supplemented labeled with unlabeled data. Weakly supervised approaches remove some of the need for fully labeled data. Most still require the templates and their slots. One common approach is to begin with unlabel"
P11-1098,D08-1094,0,0.0073989,"In the sentence, he ran and then he fell, the subjects of run and fall corefer, and so they likely belong to the same scenario-specific semantic role. We applied this idea to a new vector similarity framework. We represent a relation as a vector of all relations with which their arguments coreferred. For instance, arguments of the relation go off:s were seen coreferring with mentions in plant:o, set off:o and injure:s. We represent go off:s as a vector of these relation counts, calling this its coref vector representation. Selectional preferences (SPs) are also useful in measuring similarity (Erk and Pado, 2008). A relation can be represented as a vector of its observed arguments during training. The SPs for go off:s in our data include {bomb, device, charge, explosion}. We measure similarity using cosine similarity between the vectors in both approaches. However, 2 coreference and SPs measure different types of similarity. Coreference is a looser narrative similarity (bombings cause injuries), while SPs capture synonymy (plant and place have similar arguments). We observed that many narrative relations are not synonymous, and vice versa. We thus take the maximum of either cosine score as our final s"
P11-1098,P06-2027,0,0.403729,"nding context for features such as nearby tokens, document position, syntax, named entities, semantic classes, and discourse relations (Maslennikov and Chua, 2007). Ji and Grishman (2008) also supplemented labeled with unlabeled data. Weakly supervised approaches remove some of the need for fully labeled data. Most still require the templates and their slots. One common approach is to begin with unlabeled, but clustered event-specific documents, and extract common word patterns as extractors (Riloff and Schmelzenbach, 1998; Sudo et al., 2003; Riloff et al., 2005; Patwardhan and Riloff, 2007). Filatova et al. (2006) integrate named entities into pattern learning (PERSON won) to approximate unknown semantic roles. Bootstrapping with seed examples of known slot fillers has been shown to be effective (Surdeanu et al., 2006; Yangarber et al., 2000). In contrast, this paper removes these data assumptions, learning instead from a corpus of unknown events and unclustered documents, without seed examples. Shinyama and Sekine (2006) describe an approach to template learning without labeled data. They present unrestricted relation discovery as a means of discovering relations in unlabeled documents, and extract th"
P11-1098,P98-1067,0,0.221888,"f supervised systems. The core of this paper focuses on how to characterize a domain-specific corpus by learning rich template structure. We describe how to first expand the small corpus’ size, how to cluster its events, and finally how to induce semantic roles. Section 5 then describes the extraction algorithm, followed by evaluations against previous work in section 6 and 7. 2 Previous Work Many template extraction algorithms require full knowledge of the templates and labeled corpora, such as in rule-based systems (Chinchor et al., 1993; Rau et al., 1992) and modern supervised classifiers (Freitag, 1998; Chieu et al., 2003; Bunescu and Mooney, 2004; Patwardhan and Riloff, 2009). Classifiers rely on the labeled examples’ surrounding context for features such as nearby tokens, document position, syntax, named entities, semantic classes, and discourse relations (Maslennikov and Chua, 2007). Ji and Grishman (2008) also supplemented labeled with unlabeled data. Weakly supervised approaches remove some of the need for fully labeled data. Most still require the templates and their slots. One common approach is to begin with unlabeled, but clustered event-specific documents, and extract common word"
P11-1098,W06-1601,0,0.028342,"and we did not experiment with other quantities. A search for optimum parameter values may lead to better results. 4.3 Inducing Semantic Roles (Slots) Having successfully clustered event words and retrieved an IR-corpus for each cluster, we now address the problem of inducing semantic roles. Our learned roles will then extract entities in the next section and we will evaluate their per-role accuracy. Most work on unsupervised role induction focuses on learning verb-specific roles, starting with seed examples (Swier and Stevenson, 2004; He and Gildea, 2006) and/or knowing the number of roles (Grenager and Manning, 2006; Lang and Lapata, 2010). Our previous work (Chambers and Jurafsky, 2009) learned situation-specific roles over narrative schemas, similar to frame roles in FrameNet (Baker et al., 1998). Schemas link the syntactic relations of verbs by clustering them based on observing coreferring arguments in those positions. This paper extends this intuition by introducing a new vectorbased approach to coreference similarity. 4.3.1 Syntactic Relations as Roles We learn the roles of cluster C by clustering the syntactic relations RC of its words. Consider the following example: C = {go off, explode, set off"
P11-1098,P08-1030,0,0.204695,"lgorithm, followed by evaluations against previous work in section 6 and 7. 2 Previous Work Many template extraction algorithms require full knowledge of the templates and labeled corpora, such as in rule-based systems (Chinchor et al., 1993; Rau et al., 1992) and modern supervised classifiers (Freitag, 1998; Chieu et al., 2003; Bunescu and Mooney, 2004; Patwardhan and Riloff, 2009). Classifiers rely on the labeled examples’ surrounding context for features such as nearby tokens, document position, syntax, named entities, semantic classes, and discourse relations (Maslennikov and Chua, 2007). Ji and Grishman (2008) also supplemented labeled with unlabeled data. Weakly supervised approaches remove some of the need for fully labeled data. Most still require the templates and their slots. One common approach is to begin with unlabeled, but clustered event-specific documents, and extract common word patterns as extractors (Riloff and Schmelzenbach, 1998; Sudo et al., 2003; Riloff et al., 2005; Patwardhan and Riloff, 2007). Filatova et al. (2006) integrate named entities into pattern learning (PERSON won) to approximate unknown semantic roles. Bootstrapping with seed examples of known slot fillers has been s"
P11-1098,W10-0905,0,0.124693,"ntation of a particular domain. However, unlike relation discovery, most template-based IE approaches assume foreknowledge of the domain’s templates. Very little work addresses how to learn the template structure 976 itself. Our goal in this paper is to perform the standard template filling task, but to first automatically induce the templates from an unlabeled corpus. There are many ways to represent events, ranging from role-based representations such as frames (Baker et al., 1998) to sequential events in scripts (Schank and Abelson, 1977) and narrative schemas (Chambers and Jurafsky, 2009; Kasch and Oates, 2010). Our approach learns narrative-like knowledge in the form of IE templates; we learn sets of related events and semantic roles, as shown in this sample output from our system: Bombing Template {detonate, blow up, plant, explode, defuse, destroy} Perpetrator: Person who detonates, plants, blows up Instrument: Object that is planted, detonated, defused Target: Object that is destroyed, is blown up A semantic role, such as target, is a cluster of syntactic functions of the template’s event words (e.g., the objects of detonate and explode). Our goal is to characterize a domain by learning this tem"
P11-1098,N10-1137,0,0.0246136,"ith other quantities. A search for optimum parameter values may lead to better results. 4.3 Inducing Semantic Roles (Slots) Having successfully clustered event words and retrieved an IR-corpus for each cluster, we now address the problem of inducing semantic roles. Our learned roles will then extract entities in the next section and we will evaluate their per-role accuracy. Most work on unsupervised role induction focuses on learning verb-specific roles, starting with seed examples (Swier and Stevenson, 2004; He and Gildea, 2006) and/or knowing the number of roles (Grenager and Manning, 2006; Lang and Lapata, 2010). Our previous work (Chambers and Jurafsky, 2009) learned situation-specific roles over narrative schemas, similar to frame roles in FrameNet (Baker et al., 1998). Schemas link the syntactic relations of verbs by clustering them based on observing coreferring arguments in those positions. This paper extends this intuition by introducing a new vectorbased approach to coreference similarity. 4.3.1 Syntactic Relations as Roles We learn the roles of cluster C by clustering the syntactic relations RC of its words. Consider the following example: C = {go off, explode, set off, damage, destroy} RC ="
P11-1098,D07-1075,0,0.885082,"n the labeled examples’ surrounding context for features such as nearby tokens, document position, syntax, named entities, semantic classes, and discourse relations (Maslennikov and Chua, 2007). Ji and Grishman (2008) also supplemented labeled with unlabeled data. Weakly supervised approaches remove some of the need for fully labeled data. Most still require the templates and their slots. One common approach is to begin with unlabeled, but clustered event-specific documents, and extract common word patterns as extractors (Riloff and Schmelzenbach, 1998; Sudo et al., 2003; Riloff et al., 2005; Patwardhan and Riloff, 2007). Filatova et al. (2006) integrate named entities into pattern learning (PERSON won) to approximate unknown semantic roles. Bootstrapping with seed examples of known slot fillers has been shown to be effective (Surdeanu et al., 2006; Yangarber et al., 2000). In contrast, this paper removes these data assumptions, learning instead from a corpus of unknown events and unclustered documents, without seed examples. Shinyama and Sekine (2006) describe an approach to template learning without labeled data. They present unrestricted relation discovery as a means of discovering relations in unlabeled d"
P11-1098,D09-1016,0,0.798865,"o characterize a domain-specific corpus by learning rich template structure. We describe how to first expand the small corpus’ size, how to cluster its events, and finally how to induce semantic roles. Section 5 then describes the extraction algorithm, followed by evaluations against previous work in section 6 and 7. 2 Previous Work Many template extraction algorithms require full knowledge of the templates and labeled corpora, such as in rule-based systems (Chinchor et al., 1993; Rau et al., 1992) and modern supervised classifiers (Freitag, 1998; Chieu et al., 2003; Bunescu and Mooney, 2004; Patwardhan and Riloff, 2009). Classifiers rely on the labeled examples’ surrounding context for features such as nearby tokens, document position, syntax, named entities, semantic classes, and discourse relations (Maslennikov and Chua, 2007). Ji and Grishman (2008) also supplemented labeled with unlabeled data. Weakly supervised approaches remove some of the need for fully labeled data. Most still require the templates and their slots. One common approach is to begin with unlabeled, but clustered event-specific documents, and extract common word patterns as extractors (Riloff and Schmelzenbach, 1998; Sudo et al., 2003; R"
P11-1098,M92-1008,0,0.539077,"C-4 terrorism corpus with results approaching those of supervised systems. The core of this paper focuses on how to characterize a domain-specific corpus by learning rich template structure. We describe how to first expand the small corpus’ size, how to cluster its events, and finally how to induce semantic roles. Section 5 then describes the extraction algorithm, followed by evaluations against previous work in section 6 and 7. 2 Previous Work Many template extraction algorithms require full knowledge of the templates and labeled corpora, such as in rule-based systems (Chinchor et al., 1993; Rau et al., 1992) and modern supervised classifiers (Freitag, 1998; Chieu et al., 2003; Bunescu and Mooney, 2004; Patwardhan and Riloff, 2009). Classifiers rely on the labeled examples’ surrounding context for features such as nearby tokens, document position, syntax, named entities, semantic classes, and discourse relations (Maslennikov and Chua, 2007). Ji and Grishman (2008) also supplemented labeled with unlabeled data. Weakly supervised approaches remove some of the need for fully labeled data. Most still require the templates and their slots. One common approach is to begin with unlabeled, but clustered e"
P11-1098,W98-1106,0,0.165524,"nescu and Mooney, 2004; Patwardhan and Riloff, 2009). Classifiers rely on the labeled examples’ surrounding context for features such as nearby tokens, document position, syntax, named entities, semantic classes, and discourse relations (Maslennikov and Chua, 2007). Ji and Grishman (2008) also supplemented labeled with unlabeled data. Weakly supervised approaches remove some of the need for fully labeled data. Most still require the templates and their slots. One common approach is to begin with unlabeled, but clustered event-specific documents, and extract common word patterns as extractors (Riloff and Schmelzenbach, 1998; Sudo et al., 2003; Riloff et al., 2005; Patwardhan and Riloff, 2007). Filatova et al. (2006) integrate named entities into pattern learning (PERSON won) to approximate unknown semantic roles. Bootstrapping with seed examples of known slot fillers has been shown to be effective (Surdeanu et al., 2006; Yangarber et al., 2000). In contrast, this paper removes these data assumptions, learning instead from a corpus of unknown events and unclustered documents, without seed examples. Shinyama and Sekine (2006) describe an approach to template learning without labeled data. They present unrestricted"
P11-1098,N06-1039,0,0.397191,"t clustered event-specific documents, and extract common word patterns as extractors (Riloff and Schmelzenbach, 1998; Sudo et al., 2003; Riloff et al., 2005; Patwardhan and Riloff, 2007). Filatova et al. (2006) integrate named entities into pattern learning (PERSON won) to approximate unknown semantic roles. Bootstrapping with seed examples of known slot fillers has been shown to be effective (Surdeanu et al., 2006; Yangarber et al., 2000). In contrast, this paper removes these data assumptions, learning instead from a corpus of unknown events and unclustered documents, without seed examples. Shinyama and Sekine (2006) describe an approach to template learning without labeled data. They present unrestricted relation discovery as a means of discovering relations in unlabeled documents, and extract their fillers. Central to the algorithm is collecting multiple documents describ977 ing the same exact event (e.g. Hurricane Ivan), and observing repeated word patterns across documents connecting the same proper nouns. Learned patterns represent binary relations, and they show how to construct tables of extracted entities for these relations. Our approach draws on this idea of using unlabeled documents to discover"
P11-1098,P03-1029,0,0.0623833,"an and Riloff, 2009). Classifiers rely on the labeled examples’ surrounding context for features such as nearby tokens, document position, syntax, named entities, semantic classes, and discourse relations (Maslennikov and Chua, 2007). Ji and Grishman (2008) also supplemented labeled with unlabeled data. Weakly supervised approaches remove some of the need for fully labeled data. Most still require the templates and their slots. One common approach is to begin with unlabeled, but clustered event-specific documents, and extract common word patterns as extractors (Riloff and Schmelzenbach, 1998; Sudo et al., 2003; Riloff et al., 2005; Patwardhan and Riloff, 2007). Filatova et al. (2006) integrate named entities into pattern learning (PERSON won) to approximate unknown semantic roles. Bootstrapping with seed examples of known slot fillers has been shown to be effective (Surdeanu et al., 2006; Yangarber et al., 2000). In contrast, this paper removes these data assumptions, learning instead from a corpus of unknown events and unclustered documents, without seed examples. Shinyama and Sekine (2006) describe an approach to template learning without labeled data. They present unrestricted relation discovery"
P11-1098,H91-1059,0,0.864192,"ling in template mentions in documents. In summary, our work extends previous work on unsupervised IE in a number of ways. We are the first to learn MUC-4 templates, and we are the first to extract entities without knowing how many templates exist, without examples of slot fillers, and without event-clustered documents. 3 The Domain and its Templates Our goal is to learn the general event structure of a domain, and then extract the instances of each learned event. In order to measure performance in both tasks (learning structure and extracting instances), we use the terrorism corpus of MUC-4 (Sundheim, 1991) as our target domain. This corpus was chosen because it is annotated with templates that describe all of the entities involved in each event. An example snippet from a bombing document is given here: The terrorists used explosives against the town hall. El Comercio reported that alleged Shining Path members also attacked public facilities in huarpacha, Ambo, tomayquichua, and kichki. Municipal official Sergio Horna was seriously wounded in an explosion in Ambo. The entities from this document fill the following slots in a MUC-4 bombing template. Perp: Shining Path members Victim: Sergio Horna"
P11-1098,W06-2207,0,0.137821,"eled with unlabeled data. Weakly supervised approaches remove some of the need for fully labeled data. Most still require the templates and their slots. One common approach is to begin with unlabeled, but clustered event-specific documents, and extract common word patterns as extractors (Riloff and Schmelzenbach, 1998; Sudo et al., 2003; Riloff et al., 2005; Patwardhan and Riloff, 2007). Filatova et al. (2006) integrate named entities into pattern learning (PERSON won) to approximate unknown semantic roles. Bootstrapping with seed examples of known slot fillers has been shown to be effective (Surdeanu et al., 2006; Yangarber et al., 2000). In contrast, this paper removes these data assumptions, learning instead from a corpus of unknown events and unclustered documents, without seed examples. Shinyama and Sekine (2006) describe an approach to template learning without labeled data. They present unrestricted relation discovery as a means of discovering relations in unlabeled documents, and extract their fillers. Central to the algorithm is collecting multiple documents describ977 ing the same exact event (e.g. Hurricane Ivan), and observing repeated word patterns across documents connecting the same prop"
P11-1098,C04-1078,0,0.0550395,"orpus is to report the F1 score for slot type accuracy, ignoring the template type. For instance, a perpetrator of a bombing and a perpetrator of an attack are treated the same. This allows supervised classifiers to train on all perpetrators at once, rather than template-specific learners. Although not ideal for our learning goals, we report it for comparison against previous work. Several supervised approaches have presented results on MUC-4, but unfortunately we cannot compare against them. Maslennikov and Chua (2006; 2007) evaluated a random subset of test (they report .60 and .63 F1), and Xiao et al. (2004) did not evaluate all slot types (they report .57 F1). Figure 5 thus shows our results with previous work that is comparable: the fully supervised and 983 Patwardhan & Riloff-09 : Supervised Patwardhan & Riloff-07 : Weak-Sup Our Results (1 attack) Our Results (5 attack) P 48 42 48 44 R 59 48 25 36 F1 53 44 33 40 Figure 5: MUC-4 extraction, ignoring template type. F1 Score Results Kidnap .53 Bomb .43 Arson .42 Attack .16 / .25 Figure 6: Performance of individual templates. Attack compares our 1 vs 5 best templates. weakly supervised approaches of Patwardhan and Riloff (2009; 2007). We give two"
P11-1098,C00-2136,0,0.288082,"a. Weakly supervised approaches remove some of the need for fully labeled data. Most still require the templates and their slots. One common approach is to begin with unlabeled, but clustered event-specific documents, and extract common word patterns as extractors (Riloff and Schmelzenbach, 1998; Sudo et al., 2003; Riloff et al., 2005; Patwardhan and Riloff, 2007). Filatova et al. (2006) integrate named entities into pattern learning (PERSON won) to approximate unknown semantic roles. Bootstrapping with seed examples of known slot fillers has been shown to be effective (Surdeanu et al., 2006; Yangarber et al., 2000). In contrast, this paper removes these data assumptions, learning instead from a corpus of unknown events and unclustered documents, without seed examples. Shinyama and Sekine (2006) describe an approach to template learning without labeled data. They present unrestricted relation discovery as a means of discovering relations in unlabeled documents, and extract their fillers. Central to the algorithm is collecting multiple documents describ977 ing the same exact event (e.g. Hurricane Ivan), and observing repeated word patterns across documents connecting the same proper nouns. Learned pattern"
P11-1098,W04-3213,0,\N,Missing
P11-1098,C98-1013,0,\N,Missing
P11-1098,C98-1064,0,\N,Missing
P13-1025,abdul-mageed-diab-2012-awatif,0,0.018015,"lone we obtained the same qualitative results, with statistical significance always lower than 0.01.12 Ide, 1989; Blum-Kulka and Kasper, 1990; BlumKulka, 2003; Watts, 2003; Byon, 2006). The starting point for most research is the theory of Brown and Levinson (1987). Aspects of this theory have been explored from game-theoretic perspectives (Van Rooy, 2003) and implemented in language generation systems for interactive narratives (Walker et al., 1997), cooking instructions, (Gupta et al., 2007), translation (Faruqui and Pado, 2012), spoken dialog (Wang et al., 2012), and subjectivity analysis (Abdul-Mageed and Diab, 2012), among others. In recent years, politeness has been studied in online settings. Researchers have identified variation in politeness marking across different contexts and media types (Herring, 1994; Brennan and Ohaeri, 1999; Duthler, 2006) and between different social groups (Burke and Kraut, 2008a). The present paper pursues similar goals using orders of magnitude more data, which facilitates a fuller survey of different politeness strategies. Politeness marking is one aspect of the broader issue of how language relates to power and status, which has been studied in the context of workplace d"
P13-1025,de-marneffe-etal-2006-generating,0,0.0364066,"Missing"
P13-1025,E12-1064,0,0.0158263,"the newly designed experiments. When we re-ran our experiments on human-labeled data alone we obtained the same qualitative results, with statistical significance always lower than 0.01.12 Ide, 1989; Blum-Kulka and Kasper, 1990; BlumKulka, 2003; Watts, 2003; Byon, 2006). The starting point for most research is the theory of Brown and Levinson (1987). Aspects of this theory have been explored from game-theoretic perspectives (Van Rooy, 2003) and implemented in language generation systems for interactive narratives (Walker et al., 1997), cooking instructions, (Gupta et al., 2007), translation (Faruqui and Pado, 2012), spoken dialog (Wang et al., 2012), and subjectivity analysis (Abdul-Mageed and Diab, 2012), among others. In recent years, politeness has been studied in online settings. Researchers have identified variation in politeness marking across different contexts and media types (Herring, 1994; Brennan and Ohaeri, 1999; Duthler, 2006) and between different social groups (Burke and Kraut, 2008a). The present paper pursues similar goals using orders of magnitude more data, which facilitates a fuller survey of different politeness strategies. Politeness marking is one aspect of the broader issue of ho"
P13-1025,W10-0719,1,0.284783,"Missing"
P13-1025,W11-0711,0,0.19197,"been studied in online settings. Researchers have identified variation in politeness marking across different contexts and media types (Herring, 1994; Brennan and Ohaeri, 1999; Duthler, 2006) and between different social groups (Burke and Kraut, 2008a). The present paper pursues similar goals using orders of magnitude more data, which facilitates a fuller survey of different politeness strategies. Politeness marking is one aspect of the broader issue of how language relates to power and status, which has been studied in the context of workplace discourse (Bramsen et al., ; Diehl et al., 2007; Peterson et al., 2011; Prabhakaran et al., 2012; Gilbert, 2012; McCallum et al., 2007) and social networking (Scholand et al., 2010). However, this research focusses on domain-specific textual cues, whereas the present work seeks to leverage domain-independent politeness cues, building on the literature on how politeness affects worksplace social dynamics and power structures (Gyasi Obeng, 1997; Chilton, 1990; Andersson and Pearson, 1999; Rogers and Lee-Wong, 2003; Holmes and Stubbe, 2005). Burke and Kraut (2008b) study the question of how and why specific individuals rise to administrative positions on Wikipedia,"
P13-1025,N12-1057,0,0.0212787,"settings. Researchers have identified variation in politeness marking across different contexts and media types (Herring, 1994; Brennan and Ohaeri, 1999; Duthler, 2006) and between different social groups (Burke and Kraut, 2008a). The present paper pursues similar goals using orders of magnitude more data, which facilitates a fuller survey of different politeness strategies. Politeness marking is one aspect of the broader issue of how language relates to power and status, which has been studied in the context of workplace discourse (Bramsen et al., ; Diehl et al., 2007; Peterson et al., 2011; Prabhakaran et al., 2012; Gilbert, 2012; McCallum et al., 2007) and social networking (Scholand et al., 2010). However, this research focusses on domain-specific textual cues, whereas the present work seeks to leverage domain-independent politeness cues, building on the literature on how politeness affects worksplace social dynamics and power structures (Gyasi Obeng, 1997; Chilton, 1990; Andersson and Pearson, 1999; Rogers and Lee-Wong, 2003; Holmes and Stubbe, 2005). Burke and Kraut (2008b) study the question of how and why specific individuals rise to administrative positions on Wikipedia, and Danescu-Niculescu-Miz"
P13-1025,W12-1603,0,0.0397809,"re-ran our experiments on human-labeled data alone we obtained the same qualitative results, with statistical significance always lower than 0.01.12 Ide, 1989; Blum-Kulka and Kasper, 1990; BlumKulka, 2003; Watts, 2003; Byon, 2006). The starting point for most research is the theory of Brown and Levinson (1987). Aspects of this theory have been explored from game-theoretic perspectives (Van Rooy, 2003) and implemented in language generation systems for interactive narratives (Walker et al., 1997), cooking instructions, (Gupta et al., 2007), translation (Faruqui and Pado, 2012), spoken dialog (Wang et al., 2012), and subjectivity analysis (Abdul-Mageed and Diab, 2012), among others. In recent years, politeness has been studied in online settings. Researchers have identified variation in politeness marking across different contexts and media types (Herring, 1994; Brennan and Ohaeri, 1999; Duthler, 2006) and between different social groups (Burke and Kraut, 2008a). The present paper pursues similar goals using orders of magnitude more data, which facilitates a fuller survey of different politeness strategies. Politeness marking is one aspect of the broader issue of how language relates to power and sta"
P13-1162,W11-1701,0,0.0288479,"or more opposing perspectives, like the Israeli-Palestinian conflict (Lin et al., 2006). 6 See Choi et al. (2012) for an exploration of the interface between hedging and framing. b. Colombian paramilitary groups. Framing bias has been studied within the literature on stance recognition and arguing subjectivity. Because this literature has focused on identifying which side an article takes on a two-sided debate such as the Israeli-Palestinian conflict (Lin et al., 2006), most studies cast the problem as a two-way classification of documents or sentences into for/positive vs. against/negative (Anand et al., 2011; Conrad et al., 2012; Somasundaran and Wiebe, 2010), or into one of two opposing views (Yano et al., 2010; Park et al., 2011). The features used by these models include subjectivity and sentiment lexicons, counts of unigrams and bigrams, distributional similarity, discourse relationships, and so on. The datasets used by these studies come from genres that overtly take a specific stance (e.g., debates, editorials, blog posts). In contrast, Wikipedia editors are asked not to advocate a particular point of view, but to provide a balanced account of the different available perspectives. For this"
P13-1162,P12-1013,0,0.0177111,"he left of the word under analysis, and two to the right. Taking context into account is important given that biases can be context-dependent, especially epistemological bias since it depends on the truth of a proposition. To define some of the features like POS and grammatical relation, we used the Stanford’s CoreNLP tagger and dependency parser (de Marneffe et al., 2006). Features 9–10 use the list of hedges from Hyland (2005), features 11–14 use the factives and assertives from Hooper (1975), features 15–16 use the implicatives from Karttunen (1971), features 19–20 use the entailments from Berant et al. (2012), features 21–25 employ the subjectivity lexicon from Riloff and Wiebe (2003), and features 26–29 use the sentiment lexicon—positive and negative words—from Liu et al. (2005). If the word (or a word in the context) is in the lexicon, then the feature is true, otherwise it is false. We also included a “bias lexicon” (feature 31) that we built based on our NPOV corpus from Wikipedia. We used the training set to extract the lemmas of words that were the before form of at least two NPOV edits, and that occurred in at least two different articles. Of the 654 words included in this lexicon, 433 were"
P13-1162,W12-3809,1,0.739524,"meaning of a phrase or proposition. (11) a. Schnabel himself did the fantastic reproductions of Basquiat’s work. b. Schnabel himself did the accurate reproductions of Basquiat’s work. (12) a. Shwekey’s albums are arranged by many talented arrangers. b. Shwekey’s albums are arranged by many different arrangers. 2. One-sided terms reflect only one of the sides of a contentious issue. They often belong to controversial subjects (e.g., religion, terrorism, etc.) where the same event can be seen from two or more opposing perspectives, like the Israeli-Palestinian conflict (Lin et al., 2006). 6 See Choi et al. (2012) for an exploration of the interface between hedging and framing. b. Colombian paramilitary groups. Framing bias has been studied within the literature on stance recognition and arguing subjectivity. Because this literature has focused on identifying which side an article takes on a two-sided debate such as the Israeli-Palestinian conflict (Lin et al., 2006), most studies cast the problem as a two-way classification of documents or sentences into for/positive vs. against/negative (Anand et al., 2011; Conrad et al., 2012; Somasundaran and Wiebe, 2010), or into one of two opposing views (Yano et"
P13-1162,W12-3810,0,0.0333565,"b. Kuypers stated that the mainstream press in America tends to favor liberal viewpoints. Bias is linked to the lexical and grammatical cues identified by the literature on subjectivity (Wiebe et al., 2004; Lin et al., 2011), sentiment (Liu et al., 2005; Turney, 2002), and especially stance 1650 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1650–1659, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics or “arguing subjectivity” (Lin et al., 2006; Somasundaran and Wiebe, 2010; Yano et al., 2010; Park et al., 2011; Conrad et al., 2012). For example, like stance, framing bias is realized when the writer of a text takes a particular position on a controversial topic and uses its metaphors and vocabulary. But unlike the product reviews or debate articles that overtly use subjective language, editors in Wikipedia are actively trying to avoid bias, and hence biases may appear more subtly, in the form of covert framing language, or presuppositions and entailments that may not play as important a role in other genres. Our linguistic analysis identifies common classes of these subtle bias cues, including factive verbs, implicatives"
P13-1162,de-marneffe-etal-2006-generating,0,0.0520295,"Missing"
P13-1162,J12-2003,0,0.0253518,"Missing"
P13-1162,P09-2044,0,0.0147186,"Missing"
P13-1162,N09-1057,0,0.0272342,"Missing"
P13-1162,P02-1053,0,0.0329254,"duced by claimed, which casts doubt on Kuypers’ statement. (1) a. Usually, smaller cottage-style houses have been demolished to make way for these McMansions. b. Usually, smaller cottage-style houses have been demolished to make way for these homes. (2) a. Kuypers claimed that the mainstream press in America tends to favor liberal viewpoints. b. Kuypers stated that the mainstream press in America tends to favor liberal viewpoints. Bias is linked to the lexical and grammatical cues identified by the literature on subjectivity (Wiebe et al., 2004; Lin et al., 2011), sentiment (Liu et al., 2005; Turney, 2002), and especially stance 1650 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1650–1659, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics or “arguing subjectivity” (Lin et al., 2006; Somasundaran and Wiebe, 2010; Yano et al., 2010; Park et al., 2011; Conrad et al., 2012). For example, like stance, framing bias is realized when the writer of a text takes a particular position on a controversial topic and uses its metaphors and vocabulary. But unlike the product reviews or debate articles that overtly use subjective"
P13-1162,J04-3002,0,0.027316,"f a proposition. In (2), the assertive stated removes the bias introduced by claimed, which casts doubt on Kuypers’ statement. (1) a. Usually, smaller cottage-style houses have been demolished to make way for these McMansions. b. Usually, smaller cottage-style houses have been demolished to make way for these homes. (2) a. Kuypers claimed that the mainstream press in America tends to favor liberal viewpoints. b. Kuypers stated that the mainstream press in America tends to favor liberal viewpoints. Bias is linked to the lexical and grammatical cues identified by the literature on subjectivity (Wiebe et al., 2004; Lin et al., 2011), sentiment (Liu et al., 2005; Turney, 2002), and especially stance 1650 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1650–1659, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics or “arguing subjectivity” (Lin et al., 2006; Somasundaran and Wiebe, 2010; Yano et al., 2010; Park et al., 2011; Conrad et al., 2012). For example, like stance, framing bias is realized when the writer of a text takes a particular position on a controversial topic and uses its metaphors and vocabulary. But unlike the"
P13-1162,W06-2915,0,0.0561372,"uypers claimed that the mainstream press in America tends to favor liberal viewpoints. b. Kuypers stated that the mainstream press in America tends to favor liberal viewpoints. Bias is linked to the lexical and grammatical cues identified by the literature on subjectivity (Wiebe et al., 2004; Lin et al., 2011), sentiment (Liu et al., 2005; Turney, 2002), and especially stance 1650 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1650–1659, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics or “arguing subjectivity” (Lin et al., 2006; Somasundaran and Wiebe, 2010; Yano et al., 2010; Park et al., 2011; Conrad et al., 2012). For example, like stance, framing bias is realized when the writer of a text takes a particular position on a controversial topic and uses its metaphors and vocabulary. But unlike the product reviews or debate articles that overtly use subjective language, editors in Wikipedia are actively trying to avoid bias, and hence biases may appear more subtly, in the form of covert framing language, or presuppositions and entailments that may not play as important a role in other genres. Our linguistic analysis"
P13-1162,I11-1129,0,0.0404358,"Missing"
P13-1162,W10-0723,0,0.0186022,"Missing"
P13-1162,max-wisniewski-2010-mining,0,0.0267364,"c theory; automatically detecting these biases is equally significant for computational linguistics. We propose to address both by using a powerful resource: edits in Wikipedia that are specifically designed to remove bias. Since Wikipedia maintains a complete revision history, the edits associated with NPOV tags allow us to compare the text in its biased (before) and unbiased (after) form, helping us better understand the linguistic realization of bias. Our work thus shares the intuition of prior NLP work applying Wikipedia’s revision history (Nelken and Yamangil, 2008; Yatskar et al., 2010; Max and Wisniewski, 2010; Zanzotto and Pennacchiotti, 2010). The analysis of Wikipedia’s edits provides valuable linguistic insights into the nature of biased language. We find two major classes of bias-driven edits. The first, framing bias, is realized by subjective words or phrases linked with a particular point of view. In (1), the term McMansion, unlike homes, appeals to a negative attitude toward large and pretentious houses. The second class, epistemological bias, is related to linguistic features that subtly (often via presupposition) focus on the believability of a proposition. In (2), the assertive stated re"
P13-1162,W10-0719,0,0.0318312,"Missing"
P13-1162,N10-1056,1,0.304118,"mportant for linguistic theory; automatically detecting these biases is equally significant for computational linguistics. We propose to address both by using a powerful resource: edits in Wikipedia that are specifically designed to remove bias. Since Wikipedia maintains a complete revision history, the edits associated with NPOV tags allow us to compare the text in its biased (before) and unbiased (after) form, helping us better understand the linguistic realization of bias. Our work thus shares the intuition of prior NLP work applying Wikipedia’s revision history (Nelken and Yamangil, 2008; Yatskar et al., 2010; Max and Wisniewski, 2010; Zanzotto and Pennacchiotti, 2010). The analysis of Wikipedia’s edits provides valuable linguistic insights into the nature of biased language. We find two major classes of bias-driven edits. The first, framing bias, is realized by subjective words or phrases linked with a particular point of view. In (1), the term McMansion, unlike homes, appeals to a negative attitude toward large and pretentious houses. The second class, epistemological bias, is related to linguistic features that subtly (often via presupposition) focus on the believability of a proposition. In (2"
P13-1162,P11-1035,0,0.02445,"liberal viewpoints. b. Kuypers stated that the mainstream press in America tends to favor liberal viewpoints. Bias is linked to the lexical and grammatical cues identified by the literature on subjectivity (Wiebe et al., 2004; Lin et al., 2011), sentiment (Liu et al., 2005; Turney, 2002), and especially stance 1650 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1650–1659, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics or “arguing subjectivity” (Lin et al., 2006; Somasundaran and Wiebe, 2010; Yano et al., 2010; Park et al., 2011; Conrad et al., 2012). For example, like stance, framing bias is realized when the writer of a text takes a particular position on a controversial topic and uses its metaphors and vocabulary. But unlike the product reviews or debate articles that overtly use subjective language, editors in Wikipedia are actively trying to avoid bias, and hence biases may appear more subtly, in the form of covert framing language, or presuppositions and entailments that may not play as important a role in other genres. Our linguistic analysis identifies common classes of these subtle bias cues, including facti"
P13-1162,W03-1014,0,0.0404817,"nto account is important given that biases can be context-dependent, especially epistemological bias since it depends on the truth of a proposition. To define some of the features like POS and grammatical relation, we used the Stanford’s CoreNLP tagger and dependency parser (de Marneffe et al., 2006). Features 9–10 use the list of hedges from Hyland (2005), features 11–14 use the factives and assertives from Hooper (1975), features 15–16 use the implicatives from Karttunen (1971), features 19–20 use the entailments from Berant et al. (2012), features 21–25 employ the subjectivity lexicon from Riloff and Wiebe (2003), and features 26–29 use the sentiment lexicon—positive and negative words—from Liu et al. (2005). If the word (or a word in the context) is in the lexicon, then the feature is true, otherwise it is false. We also included a “bias lexicon” (feature 31) that we built based on our NPOV corpus from Wikipedia. We used the training set to extract the lemmas of words that were the before form of at least two NPOV edits, and that occurred in at least two different articles. Of the 654 words included in this lexicon, 433 were unique to this lexicon (i.e., recorded in neither Riloff and Wiebe’s (2003)"
P13-1162,W10-0214,0,0.250341,"t the mainstream press in America tends to favor liberal viewpoints. b. Kuypers stated that the mainstream press in America tends to favor liberal viewpoints. Bias is linked to the lexical and grammatical cues identified by the literature on subjectivity (Wiebe et al., 2004; Lin et al., 2011), sentiment (Liu et al., 2005; Turney, 2002), and especially stance 1650 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1650–1659, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics or “arguing subjectivity” (Lin et al., 2006; Somasundaran and Wiebe, 2010; Yano et al., 2010; Park et al., 2011; Conrad et al., 2012). For example, like stance, framing bias is realized when the writer of a text takes a particular position on a controversial topic and uses its metaphors and vocabulary. But unlike the product reviews or debate articles that overtly use subjective language, editors in Wikipedia are actively trying to avoid bias, and hence biases may appear more subtly, in the form of covert framing language, or presuppositions and entailments that may not play as important a role in other genres. Our linguistic analysis identifies common classes of t"
P13-1162,W10-3504,0,\N,Missing
P13-2014,D10-1040,0,0.484734,"Missing"
P13-2014,J12-1006,0,0.0376672,"Missing"
P13-2014,N13-1127,1,0.921432,"ut implicitly assuming they have mastered challenging linguistic structures (Stiller et al., 2011). In this paper, we extend these results beyond simple reference games to full decision-problems in which the agents reason about language and action together over time. To do this, we use the Decentralized Partially Observable Markov Decision Process (Dec-POMDP) to implement agents that are capable of manipulating the multiply-nested belief structures required for implicature calculation. Optimal decision making in Dec-POMDPs is NEXP complete, so we employ the single-agent POMDP approximation of Vogel et al. (2013). We show that agents in the Dec-POMDP reach implicature-rich interpretations simply as a byproduct of the way they reason about each other to maximize joint utility. Our simulations involve a reference game and a dynamic, interactional scenario involving implemented artificial agents. Conversational implicatures involve reasoning about multiply nested belief structures. This complexity poses significant challenges for computational models of conversation and cognition. We show that agents in the multi-agent DecentralizedPOMDP reach implicature-rich interpretations simply as a by-product of th"
P13-2089,N10-1122,0,0.496701,"ramen vs. sushi). By identifying these subcategories, we enable questions which probe one step deeper than the top-level category label. To identify these subcategories, we run Latent Dirichlet Analysis (LDA) (Blei et al., 2003) on the reviews of each set of businesses in the twenty most common top-level categories, using 10 topics and concatenating all of a business’s reviews into one document.2 Several researchers have used sentence-level documents to model topics in reviews, but these tend to generate topics about finegrained aspects of the sort we discuss in Section 2.2 (Jo and Oh, 2011; Brody and Elhadad, 2010). We then manually labeled the topics, discarding junk topics and merging similar topics. Table 1 displays sample extracted subcategories. Using these topic models, we assign a business 2 We use the Topic Modeling Toolkit implementation: http://nlp.stanford.edu/software/tmt https://www.yelp.com/dataset_challenge/ 499 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 499–504, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics Category Italian American (New) Delis Japanese Topic Label pizza traditional bistro deli brew"
P13-2089,P04-1009,0,0.0319064,"10) select questions from a small fixed hierarchy, which is not applicable to our large set of attributes. Information Gain Agent The information gain recommendation agent chooses questions to ask the user by selecting question attributes that maximize the entropy of the resulting document set, in a manner similar to decision tree learning (Mitchell, 1997). Formally, we define a function infogain : Att × P(B) → R: infogain(att, B) = X − vals∈P(dom(att)) Experimental Setup We follow the standard approach of using the attributes of an individual business as a simulation of a user’s preferences (Chung, 2004; Young et al., 2010). For every business b ∈ B we form an information need composed of all of b’s attributes: [ Ib = (att, att(b)) the user about, then uses the answer value to narrow the set of businesses to those with the desired attribute value, and selects another query. Algorithm 1 presents this process more formally. The recommendation agent can use both the set of businesses B and the history of question and answers H from the user to select the next query. Thus, formally a recommendation agent is a function π : B × H → Att. The dialog ends after a fixed number of queries K. 3.2 Evalua"
P13-2089,P97-1023,0,0.0322051,"lad delicious mediterranean wrap deli sandwich meats cannoli cheeses authentic sausage deli beef sandwich pastrami corned fries waitress bagel sandwiches toasted lox delicious donuts yummy pita lemonade falafel hummus delicious salad bakery sandwich subs sauce beef tasty meats delicious sushi kyoto zen rolls tuna sashimi spicy sapporo chef teppanyaki sushi drinks shrimp fried teriyaki sauce beef bowls veggies spicy grill noodles udon dishes blossom delicious soup ramen Table 1: A sample of subcategory topics with hand-labels and top words. require that the adjectives also be conjoined by and (Hatzivassiloglou and McKeown, 1997). This reduces problems like propagating positive sentiment to orange in good orange chicken. We marked adjectives that follow too or lie in the scope of negation with special prefixes and treated them as distinct lexical entries. to a subcategory based on the topic with highest probability in that business’s topic distribution. Finally, we use these subcategory topics to generate questions for our recommender dialog system. Each top-level category corresponds to a single question whose potential answers are the set of subcategories: e.g., “What type of Japanese cuisine do you want?” 2.2 Senti"
P13-2089,P03-1054,0,0.0425597,"nally, we extract NPs that appear as direct object to one of our evaluative verbs (e.g., We loved the fried chicken). 2.2.3 Extraction Patterns Aspects as Questions We generate questions from these extracted aspects using simple templates. For example, the aspect +burritos yields the question: Do you want a place with good burritos? To identify noun-phrases which are targeted by predicates in our sentiment lexicon, we develop hand-crafted extraction patterns defined over syntactic dependency parses (Blair-Goldensohn et al., 2008; Somasundaran and Wiebe, 2009) generated by the Stanford parser (Klein and Manning, 2003). Table 3 shows a sample of the aspects generated by these methods. 3 Question Selection for Dialog To utilize the questions generated from reviews in recommendation dialogs, we first formalize the dialog optimization task and then offer a solution. Adj + NP It is common practice to extract any NP modified by a sentiment adjective. However, this simple extraction rule suffers from precision problems. First, reviews often contain sentiment toward irrelevant, non-business targets (Wayne is the target of excellent job in (1)). Second, hypothetical contexts lead to spurious extractions. In (2), th"
P13-2089,P09-1026,0,0.0239453,"ss VerbNet predicates (Kipper-Schuler, 2005). 2.2.2 Verb + NP Finally, we extract NPs that appear as direct object to one of our evaluative verbs (e.g., We loved the fried chicken). 2.2.3 Extraction Patterns Aspects as Questions We generate questions from these extracted aspects using simple templates. For example, the aspect +burritos yields the question: Do you want a place with good burritos? To identify noun-phrases which are targeted by predicates in our sentiment lexicon, we develop hand-crafted extraction patterns defined over syntactic dependency parses (Blair-Goldensohn et al., 2008; Somasundaran and Wiebe, 2009) generated by the Stanford parser (Klein and Manning, 2003). Table 3 shows a sample of the aspects generated by these methods. 3 Question Selection for Dialog To utilize the questions generated from reviews in recommendation dialogs, we first formalize the dialog optimization task and then offer a solution. Adj + NP It is common practice to extract any NP modified by a sentiment adjective. However, this simple extraction rule suffers from precision problems. First, reviews often contain sentiment toward irrelevant, non-business targets (Wayne is the target of excellent job in (1)). Second, hyp"
P13-2089,P12-1065,0,0.0195314,"n. Second, we apply syntactic patterns to identify NPs targeted by these sentiment predicates. 2.2.1 Sentiment Lexicon Coordination Graph We generate a list of domain-specific sentiment adjectives using graph propagation. We begin with a seed set combining PARADIGM+ (Jo and Oh, 2011) with ‘strongly subjective’ adjectives from the OpinionFinder lexicon (Wilson et al., 2005), yielding 1342 seeds. Like Brody and Elhadad (2010), we then construct a coordination graph that links adjectives modifying the same noun, but to increase precision we 3 Our results are consistent with the recent finding of Whitney and Sarkar (2012) that cautious systems are better when bootstrapping from seeds. 500 Negative Sentiment institutional, underwhelming, not nice, burntish, unidentifiable, inefficient, not attentive, grotesque, confused, trashy, insufferable, grandiose, not pleasant, timid, degrading, laughable, under-seasoned, dismayed, torn Positive Sentiment decadent, satisfied, lovely, stupendous, sizable, nutritious, intense, peaceful, not expensive, elegant, rustic, fast, affordable, efficient, congenial, rich, not too heavy, wholesome, bustling, lush We address these problems by filtering out sentences in hypothetical co"
P13-2089,H05-2018,0,0.0186113,"edicate. For example, from the sentence “The place had great atmosphere, but the service was slow.” we extract two aspects: +atmosphere and –service. Our aspect extraction system has two steps. First we develop a domain specific sentiment lexicon. Second, we apply syntactic patterns to identify NPs targeted by these sentiment predicates. 2.2.1 Sentiment Lexicon Coordination Graph We generate a list of domain-specific sentiment adjectives using graph propagation. We begin with a seed set combining PARADIGM+ (Jo and Oh, 2011) with ‘strongly subjective’ adjectives from the OpinionFinder lexicon (Wilson et al., 2005), yielding 1342 seeds. Like Brody and Elhadad (2010), we then construct a coordination graph that links adjectives modifying the same noun, but to increase precision we 3 Our results are consistent with the recent finding of Whitney and Sarkar (2012) that cautious systems are better when bootstrapping from seeds. 500 Negative Sentiment institutional, underwhelming, not nice, burntish, unidentifiable, inefficient, not attentive, grotesque, confused, trashy, insufferable, grandiose, not pleasant, timid, degrading, laughable, under-seasoned, dismayed, torn Positive Sentiment decadent, satisfied,"
P15-1107,J08-1001,0,0.439993,"nguage generation and summarization1 . 1 Introduction Generating coherent text is a central task in natural language processing. A wide variety of theories exist for representing relationships between text units, such as Rhetorical Structure Theory (Mann and Thompson, 1988) or Discourse Representation Theory (Lascarides and Asher, 1991), for extracting these relations from text units (Marcu, 2000; LeThanh et al., 2004; Hernault et al., 2010; Feng and Hirst, 2012, inter alia), and for extracting other coherence properties characterizing the role each text unit plays with others in a discourse (Barzilay and Lapata, 2008; Barzilay and Lee, 1 Code for models described in this paper are available at www.stanford.edu/˜jiweil/. 2004; Elsner and Charniak, 2008; Li and Hovy, 2014, inter alia). However, applying these to text generation remains difficult. To understand how discourse units are connected, one has to understand the communicative function of each unit, and the role it plays within the context that encapsulates it, recursively all the way up for the entire text. Identifying increasingly sophisticated human-developed features may be insufficient for capturing these patterns. But developing neuralbased alt"
P15-1107,N04-1015,0,0.0207602,"er BLEU nor ROUGE attempts to evaluate true coherence. There is no generally accepted and readily available coherence evaluation metric.3 Because of the difficulty of developing a universal coherence evaluation metric, we proposed here only a tailored metric specific to our case. Based on the assumption that human-generated texts (i.e., input documents in our tasks) are coherent (Barzilay and Lapata, 2008), we compare generated outputs with input documents in terms of how much original text order is preserved. We develop a grid evaluation metric similar to the entity transition algorithms in (Barzilay and Lee, 2004; Lapata and Barzilay, 2005). The key idea of Barzilay and Lapata’s models is to first identify grammatical roles (i.e., object and subject) that entities play and then model the transition probability over entities and roles across sentences. We represent each sentence as a featurevector consisting of verbs and nouns in the sentence. Next we align sentences from output documents to input sentences based on sentence-tosentence F1 scores (precision and recall are computed similarly to ROUGE and BLEU but at sentence level) using feature vectors. Note that multiple output sentences can be matched"
P15-1107,P11-1050,0,0.0188877,"Missing"
P15-1107,P08-2011,0,0.00764121,"ariety of theories exist for representing relationships between text units, such as Rhetorical Structure Theory (Mann and Thompson, 1988) or Discourse Representation Theory (Lascarides and Asher, 1991), for extracting these relations from text units (Marcu, 2000; LeThanh et al., 2004; Hernault et al., 2010; Feng and Hirst, 2012, inter alia), and for extracting other coherence properties characterizing the role each text unit plays with others in a discourse (Barzilay and Lapata, 2008; Barzilay and Lee, 1 Code for models described in this paper are available at www.stanford.edu/˜jiweil/. 2004; Elsner and Charniak, 2008; Li and Hovy, 2014, inter alia). However, applying these to text generation remains difficult. To understand how discourse units are connected, one has to understand the communicative function of each unit, and the role it plays within the context that encapsulates it, recursively all the way up for the entire text. Identifying increasingly sophisticated human-developed features may be insufficient for capturing these patterns. But developing neuralbased alternatives has also been difficult. Although neural representations for sentences can capture aspects of coherent sentence structure (Ji a"
P15-1107,P12-1007,0,0.00679266,"coherence. While only a first step toward generating coherent text units from neural models, our work has the potential to significantly impact natural language generation and summarization1 . 1 Introduction Generating coherent text is a central task in natural language processing. A wide variety of theories exist for representing relationships between text units, such as Rhetorical Structure Theory (Mann and Thompson, 1988) or Discourse Representation Theory (Lascarides and Asher, 1991), for extracting these relations from text units (Marcu, 2000; LeThanh et al., 2004; Hernault et al., 2010; Feng and Hirst, 2012, inter alia), and for extracting other coherence properties characterizing the role each text unit plays with others in a discourse (Barzilay and Lapata, 2008; Barzilay and Lee, 1 Code for models described in this paper are available at www.stanford.edu/˜jiweil/. 2004; Elsner and Charniak, 2008; Li and Hovy, 2014, inter alia). However, applying these to text generation remains difficult. To understand how discourse units are connected, one has to understand the communicative function of each unit, and the role it plays within the context that encapsulates it, recursively all the way up for th"
P15-1107,P14-1002,0,0.0129671,"2008; Li and Hovy, 2014, inter alia). However, applying these to text generation remains difficult. To understand how discourse units are connected, one has to understand the communicative function of each unit, and the role it plays within the context that encapsulates it, recursively all the way up for the entire text. Identifying increasingly sophisticated human-developed features may be insufficient for capturing these patterns. But developing neuralbased alternatives has also been difficult. Although neural representations for sentences can capture aspects of coherent sentence structure (Ji and Eisenstein, 2014; Li et al., 2014; Li and Hovy, 2014), it’s not clear how they could help in generating more broadly coherent text. Recent LSTM models (Hochreiter and Schmidhuber, 1997) have shown powerful results on generating meaningful and grammatical sentences in sequence generation tasks like machine translation (Sutskever et al., 2014; Bahdanau et al., 2014; Luong et al., 2015) or parsing (Vinyals et al., 2014). This performance is at least partially attributable to the ability of these systems to capture local compositionally: the way neighboring words are combined semantically and syntactically to for"
P15-1107,P91-1008,0,0.143169,"e ROUGE and Entity Grid, showing that neural models are able to encode texts in a way that preserve syntactic, semantic, and discourse coherence. While only a first step toward generating coherent text units from neural models, our work has the potential to significantly impact natural language generation and summarization1 . 1 Introduction Generating coherent text is a central task in natural language processing. A wide variety of theories exist for representing relationships between text units, such as Rhetorical Structure Theory (Mann and Thompson, 1988) or Discourse Representation Theory (Lascarides and Asher, 1991), for extracting these relations from text units (Marcu, 2000; LeThanh et al., 2004; Hernault et al., 2010; Feng and Hirst, 2012, inter alia), and for extracting other coherence properties characterizing the role each text unit plays with others in a discourse (Barzilay and Lapata, 2008; Barzilay and Lee, 1 Code for models described in this paper are available at www.stanford.edu/˜jiweil/. 2004; Elsner and Charniak, 2008; Li and Hovy, 2014, inter alia). However, applying these to text generation remains difficult. To understand how discourse units are connected, one has to understand the commu"
P15-1107,D14-1218,1,0.360495,"r representing relationships between text units, such as Rhetorical Structure Theory (Mann and Thompson, 1988) or Discourse Representation Theory (Lascarides and Asher, 1991), for extracting these relations from text units (Marcu, 2000; LeThanh et al., 2004; Hernault et al., 2010; Feng and Hirst, 2012, inter alia), and for extracting other coherence properties characterizing the role each text unit plays with others in a discourse (Barzilay and Lapata, 2008; Barzilay and Lee, 1 Code for models described in this paper are available at www.stanford.edu/˜jiweil/. 2004; Elsner and Charniak, 2008; Li and Hovy, 2014, inter alia). However, applying these to text generation remains difficult. To understand how discourse units are connected, one has to understand the communicative function of each unit, and the role it plays within the context that encapsulates it, recursively all the way up for the entire text. Identifying increasingly sophisticated human-developed features may be insufficient for capturing these patterns. But developing neuralbased alternatives has also been difficult. Although neural representations for sentences can capture aspects of coherent sentence structure (Ji and Eisenstein, 2014"
P15-1107,D14-1220,1,0.434012,"inter alia). However, applying these to text generation remains difficult. To understand how discourse units are connected, one has to understand the communicative function of each unit, and the role it plays within the context that encapsulates it, recursively all the way up for the entire text. Identifying increasingly sophisticated human-developed features may be insufficient for capturing these patterns. But developing neuralbased alternatives has also been difficult. Although neural representations for sentences can capture aspects of coherent sentence structure (Ji and Eisenstein, 2014; Li et al., 2014; Li and Hovy, 2014), it’s not clear how they could help in generating more broadly coherent text. Recent LSTM models (Hochreiter and Schmidhuber, 1997) have shown powerful results on generating meaningful and grammatical sentences in sequence generation tasks like machine translation (Sutskever et al., 2014; Bahdanau et al., 2014; Luong et al., 2015) or parsing (Vinyals et al., 2014). This performance is at least partially attributable to the ability of these systems to capture local compositionally: the way neighboring words are combined semantically and syntactically to form meanings that t"
P15-1107,N03-1020,0,0.305314,"ed our models for a total of 7 epochs. • Batch size is set to 32 (32 documents). • Decoding algorithm allows generating at most 1.5 times the number of words in inputs. • 0.2 dropout rate. • Gradient clipping is adopted by scaling gradients when the norm exceeded a threshold of 5. Our implementation on a single GPU2 processes a speed of approximately 600-1,200 tokens per second. We trained our models for a total of 7 iterations. 4.3 Evaluations We need to measure the closeness of the output (candidate) to the input (reference). We first adopt two standard evaluation metrics, ROUGE (Lin, 2004; Lin and Hovy, 2003) and BLEU (Papineni et al., 2002). ROUGE is a recall-oriented measure widely used in the summarization literature. It measures the n-gram recall between the candidate text and the reference text(s). In this work, we only have one reference document (the input document) and ROUGE score is therefore given by: P gram ∈input countmatch (gramn ) ROUGEn = P n gramn ∈input count(gramn ) (16) 2 Tesla K40m, 1 Kepler GK110B, 2880 Cuda cores. where countmatch denotes the number of n-grams co-occurring in the input and output. We report ROUGE-1, 2 and W (based on weighted longest common subsequence). BLEU"
P15-1107,P11-1100,0,0.00981517,"Missing"
P15-1107,W04-1013,0,0.0284614,"s. We trained our models for a total of 7 epochs. • Batch size is set to 32 (32 documents). • Decoding algorithm allows generating at most 1.5 times the number of words in inputs. • 0.2 dropout rate. • Gradient clipping is adopted by scaling gradients when the norm exceeded a threshold of 5. Our implementation on a single GPU2 processes a speed of approximately 600-1,200 tokens per second. We trained our models for a total of 7 iterations. 4.3 Evaluations We need to measure the closeness of the output (candidate) to the input (reference). We first adopt two standard evaluation metrics, ROUGE (Lin, 2004; Lin and Hovy, 2003) and BLEU (Papineni et al., 2002). ROUGE is a recall-oriented measure widely used in the summarization literature. It measures the n-gram recall between the candidate text and the reference text(s). In this work, we only have one reference document (the input document) and ROUGE score is therefore given by: P gram ∈input countmatch (gramn ) ROUGEn = P n gramn ∈input count(gramn ) (16) 2 Tesla K40m, 1 Kepler GK110B, 2880 Cuda cores. where countmatch denotes the number of n-grams co-occurring in the input and output. We report ROUGE-1, 2 and W (based on weighted longest comm"
P15-1107,P15-1002,1,0.430297,"ped features may be insufficient for capturing these patterns. But developing neuralbased alternatives has also been difficult. Although neural representations for sentences can capture aspects of coherent sentence structure (Ji and Eisenstein, 2014; Li et al., 2014; Li and Hovy, 2014), it’s not clear how they could help in generating more broadly coherent text. Recent LSTM models (Hochreiter and Schmidhuber, 1997) have shown powerful results on generating meaningful and grammatical sentences in sequence generation tasks like machine translation (Sutskever et al., 2014; Bahdanau et al., 2014; Luong et al., 2015) or parsing (Vinyals et al., 2014). This performance is at least partially attributable to the ability of these systems to capture local compositionally: the way neighboring words are combined semantically and syntactically to form meanings that they wish to express. Could these models be extended to deal with generation of larger structures like paragraphs or even entire documents? In standard sequenceto-sequence generation tasks, an input sequence is mapped to a vector embedding that represents the sequence, and then to an output string of words. Multi-text generation tasks like summarizatio"
P15-1107,J00-3005,0,0.0201056,"in a way that preserve syntactic, semantic, and discourse coherence. While only a first step toward generating coherent text units from neural models, our work has the potential to significantly impact natural language generation and summarization1 . 1 Introduction Generating coherent text is a central task in natural language processing. A wide variety of theories exist for representing relationships between text units, such as Rhetorical Structure Theory (Mann and Thompson, 1988) or Discourse Representation Theory (Lascarides and Asher, 1991), for extracting these relations from text units (Marcu, 2000; LeThanh et al., 2004; Hernault et al., 2010; Feng and Hirst, 2012, inter alia), and for extracting other coherence properties characterizing the role each text unit plays with others in a discourse (Barzilay and Lapata, 2008; Barzilay and Lee, 1 Code for models described in this paper are available at www.stanford.edu/˜jiweil/. 2004; Elsner and Charniak, 2008; Li and Hovy, 2014, inter alia). However, applying these to text generation remains difficult. To understand how discourse units are connected, one has to understand the communicative function of each unit, and the role it plays within"
P15-1107,P02-1040,0,0.127401,"epochs. • Batch size is set to 32 (32 documents). • Decoding algorithm allows generating at most 1.5 times the number of words in inputs. • 0.2 dropout rate. • Gradient clipping is adopted by scaling gradients when the norm exceeded a threshold of 5. Our implementation on a single GPU2 processes a speed of approximately 600-1,200 tokens per second. We trained our models for a total of 7 iterations. 4.3 Evaluations We need to measure the closeness of the output (candidate) to the input (reference). We first adopt two standard evaluation metrics, ROUGE (Lin, 2004; Lin and Hovy, 2003) and BLEU (Papineni et al., 2002). ROUGE is a recall-oriented measure widely used in the summarization literature. It measures the n-gram recall between the candidate text and the reference text(s). In this work, we only have one reference document (the input document) and ROUGE score is therefore given by: P gram ∈input countmatch (gramn ) ROUGEn = P n gramn ∈input count(gramn ) (16) 2 Tesla K40m, 1 Kepler GK110B, 2880 Cuda cores. where countmatch denotes the number of n-grams co-occurring in the input and output. We report ROUGE-1, 2 and W (based on weighted longest common subsequence). BLEU Purely measuring recall will ina"
P15-1107,J05-2005,0,0.00782263,"zilay, 2005). The key idea of Barzilay and Lapata’s models is to first identify grammatical roles (i.e., object and subject) that entities play and then model the transition probability over entities and roles across sentences. We represent each sentence as a featurevector consisting of verbs and nouns in the sentence. Next we align sentences from output documents to input sentences based on sentence-tosentence F1 scores (precision and recall are computed similarly to ROUGE and BLEU but at sentence level) using feature vectors. Note that multiple output sentences can be matched to one input 3 Wolf and Gibson (2005) and Lin et al. (2011) proposed metrics based on discourse relations, but these are hard to apply widely since identifying discourse relations is a difficult problem. Indeed sophisticated coherence evaluation metrics are seldom adopted in real-world applications, and summarization researchers tend to use simple approximations like number of overlapped tokens or topic distribution similarity (e.g., (Yan et al., 2011b; Yan et al., 2011a; Celikyilmaz and Hakkani-T¨ur, 2011)). 1111 Input-Wiki Output-Wiki Input-Wiki Output-Wiki Input-Wiki Output-Wiki Input-Review Output-Review washington was unanim"
P15-1107,D11-1040,0,0.0223866,"(precision and recall are computed similarly to ROUGE and BLEU but at sentence level) using feature vectors. Note that multiple output sentences can be matched to one input 3 Wolf and Gibson (2005) and Lin et al. (2011) proposed metrics based on discourse relations, but these are hard to apply widely since identifying discourse relations is a difficult problem. Indeed sophisticated coherence evaluation metrics are seldom adopted in real-world applications, and summarization researchers tend to use simple approximations like number of overlapped tokens or topic distribution similarity (e.g., (Yan et al., 2011b; Yan et al., 2011a; Celikyilmaz and Hakkani-T¨ur, 2011)). 1111 Input-Wiki Output-Wiki Input-Wiki Output-Wiki Input-Wiki Output-Wiki Input-Review Output-Review washington was unanimously elected President by the electors in both the 1788 – 1789 and 1792 elections . he oversaw the creation of a strong, well-financed national government that maintained neutrality in the french revolutionary wars , suppressed the whiskey rebellion , and won acceptance among Americans of all types . washington established many forms in government still used today , such as the cabinet system and inaugural address"
P15-1107,C04-1048,0,\N,Missing
P15-2052,P07-2027,1,0.865273,"referential senses are found. Restaurant reviews provide an important new test case, and resolving who a reviewer wants to address could have important implications for coreference resolution or sentiment analysis of reviews, as well as downstream tasks like information extraction. Introduction and Task Description Detecting whether a given entity is referential is an important question in computational discourse processing. Linguistic features in the local context of a given mention have been successfully used for determining whether a second-person pronoun (you) in dialogue is referential (Gupta et al., 2007b; Frampton et al., 2009; Purver et al., 2009). The related task of anaphoricity detection is an important subtask of coreference resolution (Ng and Cardie, 2002; Ng, 2004; Luo, 2007; Zhou and Kong, 2009; Recasens et al., 2013). In this paper we consider the task of audience identification in review texts, using restaurant reviews written in Chinese. Our task is to disambiguate a mention of the Chinese second-person pronoun 你 (ni, “you”) into the following four labels that we found to occur commonly in reviews: 2 Related Work A number of closely related earlier papers have focused on disambigu"
P15-2052,E06-1022,0,0.0803233,"Missing"
P15-2052,P03-1054,0,0.00518544,"ncorporating features having to do with acoustic prosody, gaze, and head movements (Jovanovi´c et al., 2006; Takemae and Ozawa, 2006; Gupta et al., 2007b; Frampton et al., 2009). Of course in the review domain we have no access to such information; as we’ll see, however, we can exploit other unique properties of reviews to make up for this lack. 3 Preprocessing We apply the Stanford CRF Word Segmenter (Tseng et al., 2005) to segment the text of each review into words, and use simple heuristics based on whitespace and punctuation to extract sentences or sentence fragments. The Stanford Parser (Klein and Manning, 2003; Levy and Manning, 2003) is then run on each extracted sentence or fragment containing a ni to produce a dependency graph and set of part-of-speech (POS) tags for later use in feature extraction. 3.2 Annotation We hand-annotated 701 examples of ni tokens (including both singular and plural cases), placing them into one of seven categories: generic, writerreferential, reader-referential, shop-referential, idiomatic, non-“you”, and other. The idiomatic and non-“you” cases are commonly comprised of set phrases such as 你 好 (nihao, “hello”) or 迷 你 (mini, “mini”) and are therefore relatively trivia"
P15-2052,D13-1028,0,0.0171324,"ance-level lexical features help, suggesting that different uses of ‘you’ are associated with distinct vocabularies. Reiter and Frank (2010) investigate the more general question of identifying genericity for noun phrases, showing the usefuleness of linguistic features such as syntactic dependency relations. Similar local structural cues like phrase-structure positioning, head word identity, and distance to surrounding clauses have been used as features in machine learning approaches for anaphoricity detection as one stage in a coreference resolution (Kong and Zhou, 2010; Zhou and Kong, 2011; Kong and Ng, 2013). Prior work has also shown improvements in performance in the dialogue domain from incorporating features having to do with acoustic prosody, gaze, and head movements (Jovanovi´c et al., 2006; Takemae and Ozawa, 2006; Gupta et al., 2007b; Frampton et al., 2009). Of course in the review domain we have no access to such information; as we’ll see, however, we can exploit other unique properties of reviews to make up for this lack. 3 Preprocessing We apply the Stanford CRF Word Segmenter (Tseng et al., 2005) to segment the text of each review into words, and use simple heuristics based on whitesp"
P15-2052,D10-1086,0,0.0254601,"resolve the referent. They show that utterance-level lexical features help, suggesting that different uses of ‘you’ are associated with distinct vocabularies. Reiter and Frank (2010) investigate the more general question of identifying genericity for noun phrases, showing the usefuleness of linguistic features such as syntactic dependency relations. Similar local structural cues like phrase-structure positioning, head word identity, and distance to surrounding clauses have been used as features in machine learning approaches for anaphoricity detection as one stage in a coreference resolution (Kong and Zhou, 2010; Zhou and Kong, 2011; Kong and Ng, 2013). Prior work has also shown improvements in performance in the dialogue domain from incorporating features having to do with acoustic prosody, gaze, and head movements (Jovanovi´c et al., 2006; Takemae and Ozawa, 2006; Gupta et al., 2007b; Frampton et al., 2009). Of course in the review domain we have no access to such information; as we’ll see, however, we can exploit other unique properties of reviews to make up for this lack. 3 Preprocessing We apply the Stanford CRF Word Segmenter (Tseng et al., 2005) to segment the text of each review into words, a"
P15-2052,P03-1056,0,0.0436916,"ing to do with acoustic prosody, gaze, and head movements (Jovanovi´c et al., 2006; Takemae and Ozawa, 2006; Gupta et al., 2007b; Frampton et al., 2009). Of course in the review domain we have no access to such information; as we’ll see, however, we can exploit other unique properties of reviews to make up for this lack. 3 Preprocessing We apply the Stanford CRF Word Segmenter (Tseng et al., 2005) to segment the text of each review into words, and use simple heuristics based on whitespace and punctuation to extract sentences or sentence fragments. The Stanford Parser (Klein and Manning, 2003; Levy and Manning, 2003) is then run on each extracted sentence or fragment containing a ni to produce a dependency graph and set of part-of-speech (POS) tags for later use in feature extraction. 3.2 Annotation We hand-annotated 701 examples of ni tokens (including both singular and plural cases), placing them into one of seven categories: generic, writerreferential, reader-referential, shop-referential, idiomatic, non-“you”, and other. The idiomatic and non-“you” cases are commonly comprised of set phrases such as 你 好 (nihao, “hello”) or 迷 你 (mini, “mini”) and are therefore relatively trivial to filter; and the “oth"
P15-2052,D09-1102,0,0.0228148,"ent analysis of reviews, as well as downstream tasks like information extraction. Introduction and Task Description Detecting whether a given entity is referential is an important question in computational discourse processing. Linguistic features in the local context of a given mention have been successfully used for determining whether a second-person pronoun (you) in dialogue is referential (Gupta et al., 2007b; Frampton et al., 2009; Purver et al., 2009). The related task of anaphoricity detection is an important subtask of coreference resolution (Ng and Cardie, 2002; Ng, 2004; Luo, 2007; Zhou and Kong, 2009; Recasens et al., 2013). In this paper we consider the task of audience identification in review texts, using restaurant reviews written in Chinese. Our task is to disambiguate a mention of the Chinese second-person pronoun 你 (ni, “you”) into the following four labels that we found to occur commonly in reviews: 2 Related Work A number of closely related earlier papers have focused on disambiguating ‘you’ in English. Gupta et al. (2007b) annotated the Switchboard corpus of telephone dialogue, showing that features based on specific lexical patterns, adjacent partsof-speech, punctuation, and di"
P15-2052,N07-1010,0,0.0126911,"n or sentiment analysis of reviews, as well as downstream tasks like information extraction. Introduction and Task Description Detecting whether a given entity is referential is an important question in computational discourse processing. Linguistic features in the local context of a given mention have been successfully used for determining whether a second-person pronoun (you) in dialogue is referential (Gupta et al., 2007b; Frampton et al., 2009; Purver et al., 2009). The related task of anaphoricity detection is an important subtask of coreference resolution (Ng and Cardie, 2002; Ng, 2004; Luo, 2007; Zhou and Kong, 2009; Recasens et al., 2013). In this paper we consider the task of audience identification in review texts, using restaurant reviews written in Chinese. Our task is to disambiguate a mention of the Chinese second-person pronoun 你 (ni, “you”) into the following four labels that we found to occur commonly in reviews: 2 Related Work A number of closely related earlier papers have focused on disambiguating ‘you’ in English. Gupta et al. (2007b) annotated the Switchboard corpus of telephone dialogue, showing that features based on specific lexical patterns, adjacent partsof-speech"
P15-2052,C02-1139,0,0.0720089,"tions for coreference resolution or sentiment analysis of reviews, as well as downstream tasks like information extraction. Introduction and Task Description Detecting whether a given entity is referential is an important question in computational discourse processing. Linguistic features in the local context of a given mention have been successfully used for determining whether a second-person pronoun (you) in dialogue is referential (Gupta et al., 2007b; Frampton et al., 2009; Purver et al., 2009). The related task of anaphoricity detection is an important subtask of coreference resolution (Ng and Cardie, 2002; Ng, 2004; Luo, 2007; Zhou and Kong, 2009; Recasens et al., 2013). In this paper we consider the task of audience identification in review texts, using restaurant reviews written in Chinese. Our task is to disambiguate a mention of the Chinese second-person pronoun 你 (ni, “you”) into the following four labels that we found to occur commonly in reviews: 2 Related Work A number of closely related earlier papers have focused on disambiguating ‘you’ in English. Gupta et al. (2007b) annotated the Switchboard corpus of telephone dialogue, showing that features based on specific lexical patterns, ad"
P15-2052,P04-1020,0,0.0343094,"resolution or sentiment analysis of reviews, as well as downstream tasks like information extraction. Introduction and Task Description Detecting whether a given entity is referential is an important question in computational discourse processing. Linguistic features in the local context of a given mention have been successfully used for determining whether a second-person pronoun (you) in dialogue is referential (Gupta et al., 2007b; Frampton et al., 2009; Purver et al., 2009). The related task of anaphoricity detection is an important subtask of coreference resolution (Ng and Cardie, 2002; Ng, 2004; Luo, 2007; Zhou and Kong, 2009; Recasens et al., 2013). In this paper we consider the task of audience identification in review texts, using restaurant reviews written in Chinese. Our task is to disambiguate a mention of the Chinese second-person pronoun 你 (ni, “you”) into the following four labels that we found to occur commonly in reviews: 2 Related Work A number of closely related earlier papers have focused on disambiguating ‘you’ in English. Gupta et al. (2007b) annotated the Switchboard corpus of telephone dialogue, showing that features based on specific lexical patterns, adjacent par"
P15-2052,W09-3944,0,0.0460309,"Missing"
P15-2052,N13-1071,0,0.0698419,"Missing"
P15-2052,P10-1005,0,0.0214732,"pers), pages 314–319, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics 3.1 teractions significantly better than a simple baseline. Frampton et al. (2009) combine discourse features with acoustic and visual information for four-way interactions to resolve participant reference, and in the same setting Purver et al. (2009) employ cascaded classifiers that first establish referentiality and then attempt to resolve the referent. They show that utterance-level lexical features help, suggesting that different uses of ‘you’ are associated with distinct vocabularies. Reiter and Frank (2010) investigate the more general question of identifying genericity for noun phrases, showing the usefuleness of linguistic features such as syntactic dependency relations. Similar local structural cues like phrase-structure positioning, head word identity, and distance to surrounding clauses have been used as features in machine learning approaches for anaphoricity detection as one stage in a coreference resolution (Kong and Zhou, 2010; Zhou and Kong, 2011; Kong and Ng, 2013). Prior work has also shown improvements in performance in the dialogue domain from incorporating features having to do wi"
P15-2052,E09-1032,0,\N,Missing
P15-2052,H92-1045,0,\N,Missing
P16-1111,W12-3202,1,0.343253,"ity. 2 Related Work Our work builds upon a wealth of previous literature in both topic modeling and scientific discourse analysis, which we discuss in this section. We also discuss how our work relates to prior work on analyzing scientific trends. 2.1 Topic Modeling Topic modeling has a long history of applications to scientific literature, including studies of temporal scientific trends (Griffiths and Steyvers, 2004; Steyvers et al., 2004; Wang and McCallum, 2006), article recommendation (Wang and Blei, 2011), and impact prediction (Yogatama et al., 2011). For example, Hall et al. (2008) and Anderson et al. (2012) show how tracking topic popularities over time can produce a ‘computational history’ of a particular scientific field (in their case ACL, where they tracked the rise of statistical NLP, among other dramatic changes). Technical advancements in these areas usually correspond to modifications or extensions of the topic modeling (i.e., LDA) framework itself, such as by incorporating citation (Nallapati et al., 2008) or co-authorship information (Mei et al., 2008) directly into the topic model; Nallapati et al. (2011) employ such an extension to estimate the temporal “lead” or “lag” of different s"
P16-1111,P79-1022,0,0.401947,"Missing"
P16-1111,I11-1001,0,0.173992,"ant clues about the stage or development of an intellectual movement they stand to represent. For example, a topic that shifts over time from being employed as a method to being mentioned as background may signal an increase in its maturity and perhaps a corresponding decrease in its popularity among new research. In this paper, we introduce a new algorithm to determine the rhetorical functions of topics associated with an abstract. There is much work on annotating and automatically parsing the rhetorical functions or narrative structure of scientific writing (e.g., Teufel, 2000; Chung, 2009; Gupta and Manning, 2011; de Waard and Maat, 2012). We derive insights from this prior work, but since we desire to apply our analysis to a broad range of domains, we build our narrative structure model based on over 83,000 self-labeled abstracts extracted from a variety of domains in the Web of Science corpus. Figure 2 shows an example of an abstract in which the authors have labeled the different narrative sections explicitly and identified the rhetorical functions. We use our narrative structure model to assign rhetorical function labels to scientific topics and show that these labels offer important clues indicat"
P16-1111,D08-1038,0,0.0356745,"hold important clues about the dynamics involved in the evolution of science; clues that may help predict the rise and fall of scientific ideas, methods and even fields. Being able to predict scientific trends in advance could potentially revolutionize the way science is done, for instance, by enabling funding agencies to optimize allocation of resources towards promising research areas. Prior studies have often tracked scientific trends by applying topic modeling (Blei et al., 2003) based techniques to large corpora of scientific texts (Griffiths and Steyvers, 2004; Blei and Lafferty, 2006; Hall et al., 2008). They capture scientific ideas, methods, and fields in terms of topics, modeled as distributions over collection of words. These approaches usually adopt a decontextualized view of text and its usage, associating topics to documents based solely on word occurrences, disregarding where or how the words were employed. In reality, however, scientific abstracts often follow narrative structures (Crookes, 1986; Latour, 1987) that signal the specific rhetorical roles that different topics play within the research (Figure 1). The rhetorical role of a topic is the purpose or role it plays in the pape"
P16-1111,C12-1041,0,0.0151564,"12). A wide range of techniques have been used in prior work to parse scientific abstracts, from fully supervised techniques (Chung, 2009; Guo et al., 2010) to semi-supervised (Guo et al., 2011c; Guo et al., 2013) and unsupervised techniques (Kiela et al., 2015). Scientific discourse parsing has also been applied to other downstream tasks within the biomedical domain, such as information retrieval from randomized controlled trials in evidence based medicine (Chung, 2009; Kim et al., 2011; Verbeke et al., 2012), cancer risk assessment (Guo et al., 2011b), summarization (Teufel and Moens, 2002; Contractor et al., 2012), and question answering (Guo et al., 2013). Our work also falls in this category in the sense that our goal is to apply the rhetorical function parser to better understand the link between rhetoric and the historical trajectory of scientific ideas. 2.3 Scientific Trends Analysis There is also a large body of literature in bibliometrics and scientometrics on tracking scientific trends using various citation patterns. Researchers have attempted to detect emerging research fronts using topological measures of citation networks (Shibata et al., 2008) as well as co-citation clusters (Small, 2006;"
P16-1111,I08-1050,0,0.303802,"Missing"
P16-1111,W10-1913,0,0.211338,"LDA 1171 framework—by overlaying rhetorical roles—and how this allows us to not only detect the growth and decline of scientific topics but also to predict these trends based upon the rhetorical roles being employed. Since our framework is structured as a pipeline (Figure 3) and works with the output of a topic modeling system, it is compatible with the vast majority of these extended topic models. 2.2 Scientific Discourse Analysis Scientific discourse analysis is an active area of research with many different proposed schema of analysis — Argument Zones (Teufel, 2000), Information Structure (Guo et al., 2010), Core Scientific Concepts (Liakata, 2010), Research Aspects (Gupta and Manning, 2011), Discourse Segments (de Waard and Maat, 2012), Relation Structures (Tateisi et al., 2014), and Rhetorical Roles (Chung, 2009) to name a few. Most studies in this area focus on improving automatic discourse parsing of scientific text, while some works also focus on the linguistic patterns and psychological effects of scientific argumentation (e.g., de Waard and Maat, 2012). A wide range of techniques have been used in prior work to parse scientific abstracts, from fully supervised techniques (Chung, 2009; Guo"
P16-1111,D11-1025,0,0.0873972,", 2010), Research Aspects (Gupta and Manning, 2011), Discourse Segments (de Waard and Maat, 2012), Relation Structures (Tateisi et al., 2014), and Rhetorical Roles (Chung, 2009) to name a few. Most studies in this area focus on improving automatic discourse parsing of scientific text, while some works also focus on the linguistic patterns and psychological effects of scientific argumentation (e.g., de Waard and Maat, 2012). A wide range of techniques have been used in prior work to parse scientific abstracts, from fully supervised techniques (Chung, 2009; Guo et al., 2010) to semi-supervised (Guo et al., 2011c; Guo et al., 2013) and unsupervised techniques (Kiela et al., 2015). Scientific discourse parsing has also been applied to other downstream tasks within the biomedical domain, such as information retrieval from randomized controlled trials in evidence based medicine (Chung, 2009; Kim et al., 2011; Verbeke et al., 2012), cancer risk assessment (Guo et al., 2011b), summarization (Teufel and Moens, 2002; Contractor et al., 2012), and question answering (Guo et al., 2013). Our work also falls in this category in the sense that our goal is to apply the rhetorical function parser to better underst"
P16-1111,W10-3101,0,0.013179,"oles—and how this allows us to not only detect the growth and decline of scientific topics but also to predict these trends based upon the rhetorical roles being employed. Since our framework is structured as a pipeline (Figure 3) and works with the output of a topic modeling system, it is compatible with the vast majority of these extended topic models. 2.2 Scientific Discourse Analysis Scientific discourse analysis is an active area of research with many different proposed schema of analysis — Argument Zones (Teufel, 2000), Information Structure (Guo et al., 2010), Core Scientific Concepts (Liakata, 2010), Research Aspects (Gupta and Manning, 2011), Discourse Segments (de Waard and Maat, 2012), Relation Structures (Tateisi et al., 2014), and Rhetorical Roles (Chung, 2009) to name a few. Most studies in this area focus on improving automatic discourse parsing of scientific text, while some works also focus on the linguistic patterns and psychological effects of scientific argumentation (e.g., de Waard and Maat, 2012). A wide range of techniques have been used in prior work to parse scientific abstracts, from fully supervised techniques (Chung, 2009; Guo et al., 2010) to semi-supervised (Guo et"
P16-1111,D12-1053,0,0.0560339,"Missing"
P16-1111,W09-3603,0,0.0637304,"Missing"
P16-1111,D11-1055,0,0.0299571,"Missing"
P16-1111,tateisi-etal-2014-annotation,0,0.0126253,"ed upon the rhetorical roles being employed. Since our framework is structured as a pipeline (Figure 3) and works with the output of a topic modeling system, it is compatible with the vast majority of these extended topic models. 2.2 Scientific Discourse Analysis Scientific discourse analysis is an active area of research with many different proposed schema of analysis — Argument Zones (Teufel, 2000), Information Structure (Guo et al., 2010), Core Scientific Concepts (Liakata, 2010), Research Aspects (Gupta and Manning, 2011), Discourse Segments (de Waard and Maat, 2012), Relation Structures (Tateisi et al., 2014), and Rhetorical Roles (Chung, 2009) to name a few. Most studies in this area focus on improving automatic discourse parsing of scientific text, while some works also focus on the linguistic patterns and psychological effects of scientific argumentation (e.g., de Waard and Maat, 2012). A wide range of techniques have been used in prior work to parse scientific abstracts, from fully supervised techniques (Chung, 2009; Guo et al., 2010) to semi-supervised (Guo et al., 2011c; Guo et al., 2013) and unsupervised techniques (Kiela et al., 2015). Scientific discourse parsing has also been applied to"
P16-1111,J02-4002,0,0.135814,"., de Waard and Maat, 2012). A wide range of techniques have been used in prior work to parse scientific abstracts, from fully supervised techniques (Chung, 2009; Guo et al., 2010) to semi-supervised (Guo et al., 2011c; Guo et al., 2013) and unsupervised techniques (Kiela et al., 2015). Scientific discourse parsing has also been applied to other downstream tasks within the biomedical domain, such as information retrieval from randomized controlled trials in evidence based medicine (Chung, 2009; Kim et al., 2011; Verbeke et al., 2012), cancer risk assessment (Guo et al., 2011b), summarization (Teufel and Moens, 2002; Contractor et al., 2012), and question answering (Guo et al., 2013). Our work also falls in this category in the sense that our goal is to apply the rhetorical function parser to better understand the link between rhetoric and the historical trajectory of scientific ideas. 2.3 Scientific Trends Analysis There is also a large body of literature in bibliometrics and scientometrics on tracking scientific trends using various citation patterns. Researchers have attempted to detect emerging research fronts using topological measures of citation networks (Shibata et al., 2008) as well as co-citati"
P16-1141,P12-1015,0,0.026974,"by looking at the displacement between consecutive time-points. 3 Comparison of different approaches We compare the different distributional approaches on a set of benchmarks designed to test their scientific utility. We evaluate both their synchronic accuracy (i.e., ability to capture word similarity within individual time-periods) and their diachronic validity (i.e., ability to quantify semantic changes over time). 3.1 Synchronic Accuracy We evaluated the synchronic (within-time-period) accuracy of the methods using a standard modern benchmark and the 1990s portion of the E NG A LL data. On Bruni et al. (2012)’s MEN similarity task of matching human judgments of word similarities, SVD performed best (ρ = 0.739), followed by PPMI (ρ = 0.687) and SGNS (ρ = 0.649). These results echo the findings of Levy et al. (2015), who found SVD to perform best on similarity tasks while SGNS performed best on analogy tasks (which are not the focus of this work). 3.2 Diachronic Validity We evaluate the diachronic validity of the methods on two historical semantic tasks: detecting known shifts and discovering shifts from data. For both these tasks, we performed detailed evaluations on a small set of examples (28 kno"
P16-1141,C14-1154,0,0.188006,"Missing"
P16-1141,E03-1020,0,0.0608566,"wi ) |− 1) (10) where NPPMI (wi ) = {wj : PPMI(wi , wj ) &gt; 0}. This measure counts the proportion of wi ’s neighbors that are also neighbors of each other. According to this measure, a word will have a high clustering coefficient (and thus a low polysemy score) if the words that it co-occurs with also tend to cooccur with each other. Polysemous words that are contextually diverse will have low clustering coefficients, since they appear in disjointed or unrelated contexts. Variants of this measure are often used in wordsense discrimination and correlate with, e.g., number of senses in WordNet (Dorow and Widdows, 2003; Ferret, 2004). However, we found that it was slightly biased towards rating contextually diverse discourse function words (e.g., also) as highly polysemous, which needs to be taken into account when interpreting our results. We opted to use this measure, despite this bias, because it has the strong benefit of being clearly interpretable: it simply measures the extent to which a word appears in diverse textual contexts. Table 6 gives examples of the least and most polysemous words in the E NG F IC data, according to this score. As expected, this measure has significant intrinsic positive corr"
P16-1141,W11-2508,0,0.839944,"nge in English using SGNS vectors.2 a, The word gay shifted from meaning “cheerful” or “frolicsome” to referring to homosexuality. b, In the early 20th century broadcast referred to “casting out seeds”; with the rise of television and radio its meaning shifted to “transmitting signals”. c, Awful underwent a process of pejoration, as it shifted from meaning “full of awe” to meaning “terrible or appalling” (Simpson et al., 1989). are then compared across time-periods. This new direction has been effectively demonstrated in a number of case-studies (Sagi et al., 2011; Wijaya and Yeniterzi, 2011; Gulordava and Baroni, 2011; Jatowt and Duh, 2014) and used to perform largescale linguistic change-point detection (Kulkarni et al., 2014) as well as to test a few specific hypotheses, such as whether English synonyms tend to change meaning in similar ways (Xu and Kemp, 2015). However, these works employ widely different embedding approaches and test their approaches only on English. In this work, we develop a robust methodology for quantifying semantic change using embeddings by comparing state-of-the-art approaches (PPMI, SVD, word2vec) on novel benchmarks. We then apply this methodology in a large-scale cross-lingui"
P16-1141,W14-2517,0,0.70973,"Missing"
P16-1141,P12-3029,0,0.203679,"Missing"
P16-1141,P14-1096,0,0.27948,"Missing"
P16-1141,Q15-1016,0,\N,Missing
P16-1141,E12-1060,0,\N,Missing
P16-1141,C04-1194,0,\N,Missing
P16-1141,Q16-1003,0,\N,Missing
P17-2009,D11-1145,0,0.0532572,"Missing"
P17-2009,P16-2096,0,0.0351595,". Furthermore, in a case study using Twitter for health tracking, our method substantially increases the availability of texts written by underrepresented populations, enabling the development of “socially inclusive” NLP tools. 1 Figure 1: Challenges for socially-equitable LID in Twitter include dialectal text, shown from Nigeria (#1) and Ireland (#2), and multilingual text (Indonesian and English) in #3. graphic and dialectal variation. As a result, these systems systematically misclassify texts from populations with millions of speakers whose local speech differs from the majority dialects (Hovy and Spruit, 2016; Blodgett et al., 2016). Multiple systems have been proposed for broadcoverage LID at the global level (McCandless, 2010; Lui and Baldwin, 2012; Brown, 2014; Jaech et al., 2016). However, only a handful of techniques have addressed the challenge of linguistic variability of global data, such as the dialectal variability and multilingual text seen in Figure 1. These techniques have typically focused only on limited aspects of variability, e.g., individual dialects like African American Vernacular English (Blodgett et al., 2016), online speech (Nguyen and Do˘gru¨oz, 2013), similar languages (Be"
P17-2009,W16-6212,0,0.269445,"development of “socially inclusive” NLP tools. 1 Figure 1: Challenges for socially-equitable LID in Twitter include dialectal text, shown from Nigeria (#1) and Ireland (#2), and multilingual text (Indonesian and English) in #3. graphic and dialectal variation. As a result, these systems systematically misclassify texts from populations with millions of speakers whose local speech differs from the majority dialects (Hovy and Spruit, 2016; Blodgett et al., 2016). Multiple systems have been proposed for broadcoverage LID at the global level (McCandless, 2010; Lui and Baldwin, 2012; Brown, 2014; Jaech et al., 2016). However, only a handful of techniques have addressed the challenge of linguistic variability of global data, such as the dialectal variability and multilingual text seen in Figure 1. These techniques have typically focused only on limited aspects of variability, e.g., individual dialects like African American Vernacular English (Blodgett et al., 2016), online speech (Nguyen and Do˘gru¨oz, 2013), similar languages (Bergsma et al., 2012; Zampieri et al., 2014a), or word-level code switching (Solorio et al., 2014; Rijhwani et al., 2017). Introduction Language identification (LID) is an essentia"
P17-2009,P15-1120,0,0.0212281,"tudies employ off-the-shelf LID systems without considering how they were trained. We aim to create a socially-representative corpus for LID that captures the variation within a language, such as orthography, dialect, formality, topic, and spelling. Motivated by the recent language survey of Twitter (Trampus, 2016), we next describe how we construct this corpus for 70 languages along three dimensions: geography, social and topical diversity, and multilinguality. Geographic Diversity We create a large-scale dataset of geographically-diverse text by bootstrapping with a people-centric approach (Bamman, 2015) that treats location and languagesspoken as demographic attributes to be inferred for authors. By inferring both for Twitter users and then collecting documents from monolingual users, we ensure that we capture regional variation in a language, rather than focusing on a particular aspect of linguistic variety. Individuals’ locations are inferred using the method of Compton et al. (2014) as implemented by Jurgens et al. (2015). The method first identifies the individuals who have reliable ground truth locations from geotagged tweets and then infers the locations of other individuals as the geo"
P17-2009,W12-2108,0,0.0272177,"16; Blodgett et al., 2016). Multiple systems have been proposed for broadcoverage LID at the global level (McCandless, 2010; Lui and Baldwin, 2012; Brown, 2014; Jaech et al., 2016). However, only a handful of techniques have addressed the challenge of linguistic variability of global data, such as the dialectal variability and multilingual text seen in Figure 1. These techniques have typically focused only on limited aspects of variability, e.g., individual dialects like African American Vernacular English (Blodgett et al., 2016), online speech (Nguyen and Do˘gru¨oz, 2013), similar languages (Bergsma et al., 2012; Zampieri et al., 2014a), or word-level code switching (Solorio et al., 2014; Rijhwani et al., 2017). Introduction Language identification (LID) is an essential first step for NLP on multilingual text. In global settings like Twitter, this text is written by authors from diverse linguistic backgrounds, who may communicate with regional dialects (Gonc¸alves and S´anchez, 2014) or even include parallel translations in the same message to address different audiences (Ling et al., 2013, 2016). Such dialectal variation is frequent in all languages and even macro-dialects such as American and Briti"
P17-2009,D16-1120,0,0.0307458,"Missing"
P17-2009,D14-1069,0,0.044983,"enabling the development of “socially inclusive” NLP tools. 1 Figure 1: Challenges for socially-equitable LID in Twitter include dialectal text, shown from Nigeria (#1) and Ireland (#2), and multilingual text (Indonesian and English) in #3. graphic and dialectal variation. As a result, these systems systematically misclassify texts from populations with millions of speakers whose local speech differs from the majority dialects (Hovy and Spruit, 2016; Blodgett et al., 2016). Multiple systems have been proposed for broadcoverage LID at the global level (McCandless, 2010; Lui and Baldwin, 2012; Brown, 2014; Jaech et al., 2016). However, only a handful of techniques have addressed the challenge of linguistic variability of global data, such as the dialectal variability and multilingual text seen in Figure 1. These techniques have typically focused only on limited aspects of variability, e.g., individual dialects like African American Vernacular English (Blodgett et al., 2016), online speech (Nguyen and Do˘gru¨oz, 2013), similar languages (Bergsma et al., 2012; Zampieri et al., 2014a), or word-level code switching (Solorio et al., 2014; Rijhwani et al., 2017). Introduction Language identification"
P17-2009,W16-5801,0,0.0440216,"Missing"
P17-2009,N13-1097,0,0.0695273,"Missing"
P17-2009,W14-3356,0,0.055626,"Missing"
P17-2009,W16-5806,0,0.00821164,"Missing"
P17-2009,P12-3005,0,0.0943464,"epresented populations, enabling the development of “socially inclusive” NLP tools. 1 Figure 1: Challenges for socially-equitable LID in Twitter include dialectal text, shown from Nigeria (#1) and Ireland (#2), and multilingual text (Indonesian and English) in #3. graphic and dialectal variation. As a result, these systems systematically misclassify texts from populations with millions of speakers whose local speech differs from the majority dialects (Hovy and Spruit, 2016; Blodgett et al., 2016). Multiple systems have been proposed for broadcoverage LID at the global level (McCandless, 2010; Lui and Baldwin, 2012; Brown, 2014; Jaech et al., 2016). However, only a handful of techniques have addressed the challenge of linguistic variability of global data, such as the dialectal variability and multilingual text seen in Figure 1. These techniques have typically focused only on limited aspects of variability, e.g., individual dialects like African American Vernacular English (Blodgett et al., 2016), online speech (Nguyen and Do˘gru¨oz, 2013), similar languages (Bergsma et al., 2012; Zampieri et al., 2014a), or word-level code switching (Solorio et al., 2014; Rijhwani et al., 2017). Introduction Language i"
P17-2009,W14-1303,0,0.0375212,"Missing"
P17-2009,D16-1217,0,0.0683736,"Missing"
P17-2009,Q14-1003,0,0.100672,"hich is roughly three epochs. Comparison Systems We compare against two broad-coverage LID systems, langid.py (Lui and Baldwin, 2012) and CLD2 (McCandless, 2010), both of which have been widely used for Twitter within in the NLP community. CLD2 is trained on web page text, while langid.py was trained on newswire, JRC-Acquis, web pages, and Wikipedia. As neither was designed for Twitter, we preprocess text to remove user mentions, hashtags, and URLs for a more fair comparison. For multilingual documents, we substitute langid.py (Lui and Baldwin, 2012) with its extension, Polyglot, described in Lui et al. (2014) and designed for that particular task. Equitable LID Classifier We introduce E QUI LID, and evaluate it on monolingual and multilingual tweet-length text. Model Character-based neural network architectures are particularly suitable for LID, as they facilitate modeling nuanced orthographic and phonological properties of languages (Jaech et al., 2016; Samih et al., 2016), e.g., capturing regular morpheme occurrences within the words of a language. Further, character-based methods significantly reduce the model complexity compared to word-based methods; the latter require separate neural represe"
P17-2009,W14-3907,0,0.0299577,"rage LID at the global level (McCandless, 2010; Lui and Baldwin, 2012; Brown, 2014; Jaech et al., 2016). However, only a handful of techniques have addressed the challenge of linguistic variability of global data, such as the dialectal variability and multilingual text seen in Figure 1. These techniques have typically focused only on limited aspects of variability, e.g., individual dialects like African American Vernacular English (Blodgett et al., 2016), online speech (Nguyen and Do˘gru¨oz, 2013), similar languages (Bergsma et al., 2012; Zampieri et al., 2014a), or word-level code switching (Solorio et al., 2014; Rijhwani et al., 2017). Introduction Language identification (LID) is an essential first step for NLP on multilingual text. In global settings like Twitter, this text is written by authors from diverse linguistic backgrounds, who may communicate with regional dialects (Gonc¸alves and S´anchez, 2014) or even include parallel translations in the same message to address different audiences (Ling et al., 2013, 2016). Such dialectal variation is frequent in all languages and even macro-dialects such as American and British English are composed of local dialects that vary across city and socioecon"
P17-2009,D13-1084,0,0.131253,"Missing"
P17-2009,W14-5307,0,0.0170451,"Missing"
P17-2009,W15-5401,0,0.0296329,"Missing"
P17-2009,P17-1180,0,0.0156764,"level (McCandless, 2010; Lui and Baldwin, 2012; Brown, 2014; Jaech et al., 2016). However, only a handful of techniques have addressed the challenge of linguistic variability of global data, such as the dialectal variability and multilingual text seen in Figure 1. These techniques have typically focused only on limited aspects of variability, e.g., individual dialects like African American Vernacular English (Blodgett et al., 2016), online speech (Nguyen and Do˘gru¨oz, 2013), similar languages (Bergsma et al., 2012; Zampieri et al., 2014a), or word-level code switching (Solorio et al., 2014; Rijhwani et al., 2017). Introduction Language identification (LID) is an essential first step for NLP on multilingual text. In global settings like Twitter, this text is written by authors from diverse linguistic backgrounds, who may communicate with regional dialects (Gonc¸alves and S´anchez, 2014) or even include parallel translations in the same message to address different audiences (Ling et al., 2013, 2016). Such dialectal variation is frequent in all languages and even macro-dialects such as American and British English are composed of local dialects that vary across city and socioeconomic development level ("
P17-2009,P13-1018,0,\N,Missing
P18-1027,P17-1033,0,0.0553559,"Missing"
P18-1027,N16-1082,1,0.82789,"rsity {urvashik,hehe,pengqi,jurafsky}@stanford.edu Abstract provement often attributed to their ability to model long-range dependencies in faraway context. Yet, how these NLMs use the context is largely unexplained. Recent studies have begun to shed light on the information encoded by Long Short-Term Memory (LSTM) networks. They can remember sentence lengths, word identity, and word order (Adi et al., 2017), can capture some syntactic structures such as subject-verb agreement (Linzen et al., 2016), and can model certain kinds of semantic compositionality such as negation and intensification (Li et al., 2016). However, all of the previous work studies LSTMs at the sentence level, even though they can potentially encode longer context. Our goal is to complement the prior work to provide a richer understanding of the role of context, in particular, long-range context beyond a sentence. We aim to answer the following questions: (i) How much context is used by NLMs, in terms of the number of tokens? (ii) Within this range, are nearby and long-range contexts represented differently? (iii) How do copy mechanisms help the model use different regions of context? We investigate these questions via ablation"
P18-1027,Q16-1037,0,0.0798862,"nguage Models Use Context Urvashi Khandelwal, He He, Peng Qi, Dan Jurafsky Computer Science Department Stanford University {urvashik,hehe,pengqi,jurafsky}@stanford.edu Abstract provement often attributed to their ability to model long-range dependencies in faraway context. Yet, how these NLMs use the context is largely unexplained. Recent studies have begun to shed light on the information encoded by Long Short-Term Memory (LSTM) networks. They can remember sentence lengths, word identity, and word order (Adi et al., 2017), can capture some syntactic structures such as subject-verb agreement (Linzen et al., 2016), and can model certain kinds of semantic compositionality such as negation and intensification (Li et al., 2016). However, all of the previous work studies LSTMs at the sentence level, even though they can potentially encode longer context. Our goal is to complement the prior work to provide a richer understanding of the role of context, in particular, long-range context beyond a sentence. We aim to answer the following questions: (i) How much context is used by NLMs, in terms of the number of tokens? (ii) Within this range, are nearby and long-range contexts represented differently? (iii) Ho"
P18-1027,P14-5010,0,0.0058666,"Missing"
P18-1027,J93-2004,0,0.0606706,"er bounds which would likely be lower, were the model also trained to handle such perturbations. We use a standard LSTM language model, trained and finetuned using the Averaging SGD optimizer (Merity et al., 2018).2 We also augment the model with a cache only for Section 6.2, in order to investigate why an external copy mechanism is helpful. A short description of the architecture and a detailed list of hyperparameters is listed in Appendix A, and we refer the reader to the original paper for additional details. We analyze two datasets commonly used for language modeling, Penn Treebank (PTB) (Marcus et al., 1993; Mikolov et al., 2010) and Wikitext-2 (Wiki) (Merity et al., 2017). PTB consists of Wall Street Journal news articles with 0.9M tokens for training and a 10K vocabulary. Wiki is a larger and more diverse dataset, containing Wikipedia articles across many topics with 2.1M tokens for training and a 33K vocabulary. Additional dataset statistics are provided in Ta1 , . . . , w1 ), and language models compute the conditional probability of a target word wt given its preceding context, w1 , . . . , wt 1 . Language models are trained to minimize the negative log likelihood of the training corpus: NL"
P18-1027,E17-2025,0,0.0431037,"Missing"
P18-1027,P16-1125,0,\N,Missing
Q18-1028,N12-1009,0,0.0606437,"Missing"
Q18-1028,N13-1067,0,0.64401,"Missing"
Q18-1028,W12-3202,1,0.843103,"a variance inflation factor of < 10 for all variables. 400 their work as C OMPARISON OR C ONTRAST or E X TENDS , with both having significant positive effects. 8 The Growth of Rapid Discovery Science As scientific fields evolve, new subfields initially emerge around methods or technologies which become a focus of collective puzzle-solving and continual improvement (Moody, 2004). NLP has witnessed the emergence of several such subfields from the early grammar based approaches in the 1950s1970s, to the statistical revolution in the 1990s, to the recent deep learning models (Sp¨arck Jones, 2001; Anderson et al., 2012). Collins (1994) proposed that a field can undergo a particular shift, referring to it as rapid discovery science, when the field (a) reaches high consensus on research topics as well as methods and technologies, and (b) then develops genealogies of methods and technologies that continually improve on one another. Over time, there is increased consensus on core approaches, and the field’s periphery is extended to new research puzzles rather than contesting prior efforts. Collins claims this shift characterizes natural sciences, but not many social sciences, which are instead more likely to eng"
Q18-1028,N12-1073,0,0.05741,"es with the most-frequent arguments seen for each paths. Each dependency path feature value reflects the similarity of (i) the average word vector for that path’s arguments with (ii) the vector of the path’s argument in a given context, if the path is present. average similarity of an instance’s arguments with the class’s preferences for all observed syntactic relationships (i.e., how similar are the syntacticallyrelated words to the function’s preferences). Our work differs from dependency-based features from prior work that use separate features for each unique dependency path and argument (Athar and Teufel, 2012; Abu-Jbara et al., 2013); in contrast, we use a single feature for each path with distributed representation for its arguments, which allows our features to generalize to similar words that are unseen in the training data. 3.2 Experimental Setup Models All models were trained using a Random Forest classifier, which is robust to overfitting even with large numbers of features (Fern´andez-Delgado et al., 2014). After limited grid search over possible configurations,5 we set parameter values as follows. The number of random trees is 2500 and we required each leaf to match at least 5 instances. T"
Q18-1028,bird-etal-2008-acl,0,0.0719884,"UTURE class to indicate that authors have forward-looking references for how their work might be applied later; these references are important for establishing a temporal lineage between works, and as we show later in §4, are the most frequent citation type in papers’ Conclusion sections. Our adapted scheme enables us to conduct detailed analyses of the narrative structure of papers, venue citation pattern and evolution, and modeling the evolution of the whole field. 2.2 Annotation guidelines were created using a pilot study of 10 papers sampled from the ACL Anthology Reference Corpus (ARC) (Bird et al., 2008). 2 2.1 Classification Scheme Our classification captures the broad thematic functions a citation can serve in the discourse, e.g., pro1 For notational clarity, we use the term reference for the work that is cited and citation for the mention of it in the text. 392 Annotation Process and Dataset Another potential theme is citation sentiment (Athar, 2014; Kumar, 2016), but we omit this theme from our field-scale analysis because researchers have shown that negative sentiment is rare in practice (Chubin and Moitra, 1975; Vinkler, 1998; Case and Higgins, 2000) and can be quite subjective to class"
Q18-1028,W99-0629,0,0.0781822,"e rise in expected incom9 Note that this change acts against the general decrease in non-methodological; considering only 1980-2009, the decrease in non-methodological is even larger (r= -0.568, p ≤ 0.01 ). ing citations to those works compared against (r= 0.734, p ≤ 0.01) and used (r=0.889, p ≤ 0.01). For example, in 1991, authors compared with a diffuse group of parsing papers, e.g., (Shieber, 1988; Pereira and Warren, 1983; Haas, 1989), with such papers receiving at most three citations that year; whereas in 2000, most comparisons were to a core set of parsing papers, e.g., (Collins, 1999; Buchholz et al., 1999; Collins, 1997), with a much sharper (lower entropy) distribution of citations. These trends show the increased incorporation of prior work to form a lineage of method technologies as well as show increased consensus on which works are sufficient for comparing against in order to establish a claim. These results also empirically confirm the observation of Sp¨arck Jones (2001) that a major trend in NLP in the 1990s was an increase in reusable technologies and evaluations, like the BNC (Leech, 1992) and the Penn Treebank (Marcus et al., 1993). More broadly, our work points to the future of NLP"
Q18-1028,P97-1003,0,0.075739,"m9 Note that this change acts against the general decrease in non-methodological; considering only 1980-2009, the decrease in non-methodological is even larger (r= -0.568, p ≤ 0.01 ). ing citations to those works compared against (r= 0.734, p ≤ 0.01) and used (r=0.889, p ≤ 0.01). For example, in 1991, authors compared with a diffuse group of parsing papers, e.g., (Shieber, 1988; Pereira and Warren, 1983; Haas, 1989), with such papers receiving at most three citations that year; whereas in 2000, most comparisons were to a core set of parsing papers, e.g., (Collins, 1999; Buchholz et al., 1999; Collins, 1997), with a much sharper (lower entropy) distribution of citations. These trends show the increased incorporation of prior work to form a lineage of method technologies as well as show increased consensus on which works are sufficient for comparing against in order to establish a claim. These results also empirically confirm the observation of Sp¨arck Jones (2001) that a major trend in NLP in the 1990s was an increase in reusable technologies and evaluations, like the BNC (Leech, 1992) and the Penn Treebank (Marcus et al., 1993). More broadly, our work points to the future of NLP as a quickly mov"
Q18-1028,councill-etal-2008-parscit,0,0.0313905,"by two trained annotators with expertise in NLP using the Brat tool (Stenetorp et al., 2012) and were then fully adjudicated to ensure quality. Following best practices for annotating citations (Athar, 2014), annotators saw an extended context before and after the citing sentence, provided from the output of ParsCit. Annotators were instructed to skip any instances whose context was corrupted or whose citance text did not match the regular citation style for ACL venues.3 The citation scheme was applied to a random sample of 52 papers drawn from the ARC. Each paper was processed using ParsCit (Councill et al., 2008) to extract citations and their references. As expected from prior studies (Teufel et al., 2006a; Dong and 3 A small number of citation instances in our sample occurred in contexts where the surrounding text was malformed, which we attribute to being OCR errors, the citation being in the middle of a math-related context whose symbols were not converted, or where the citation occurred within a table or figure whose structure was treated as the surrounding text. In all cases, we viewed in the instance as unsuitable for use as a training example since it contained little meaningful context. These"
Q18-1028,I11-1070,0,0.0470054,"Missing"
Q18-1028,P07-1028,0,0.0195428,"ormances and results is likely to be a C OMPARE OR C ONTRAST, whereas one describing methodology is more likely to be U SES. We quantify this thematic framing by using features based on topic models, computed over the sentence containing the citation and also over the paragraph containing the citing sentence. For each type of context, a topic model is trained over 321,129 respective contexts from the ARC. Table 5 shows example topics. Prototypical Argument Features We also explored richer grammatical features, drawing on selectional preferences reflecting expectations for predicate arguments (Erk, 2007). We construct a prototype for each citation function by identifying the frequent arguments seen in different syntactic positions. For example, E XTENDS citations occur frequently as objects of verbs such as “follow” and “use”, whereas U SES citations have techniques or artifact words as dependents; Table 6 shows more examples. Each class’s selectional preferences are represented using a vector for the argument at each relation type, constructed by summing the vectors of all words appearing in it. Each function is represented as a separate feature whose value is the 395 Function M OTIVATION M"
Q18-1028,J89-4001,0,0.029991,"ortionally more non-methodological citations.9 In the second trend, authors are more likely to use and compare against the same set of papers, as shown in Figure 6 by the rise in expected incom9 Note that this change acts against the general decrease in non-methodological; considering only 1980-2009, the decrease in non-methodological is even larger (r= -0.568, p ≤ 0.01 ). ing citations to those works compared against (r= 0.734, p ≤ 0.01) and used (r=0.889, p ≤ 0.01). For example, in 1991, authors compared with a diffuse group of parsing papers, e.g., (Shieber, 1988; Pereira and Warren, 1983; Haas, 1989), with such papers receiving at most three citations that year; whereas in 2000, most comparisons were to a core set of parsing papers, e.g., (Collins, 1999; Buchholz et al., 1999; Collins, 1997), with a much sharper (lower entropy) distribution of citations. These trends show the increased incorporation of prior work to form a lineage of method technologies as well as show increased consensus on which works are sufficient for comparing against in order to establish a claim. These results also empirically confirm the observation of Sp¨arck Jones (2001) that a major trend in NLP in the 1990s wa"
Q18-1028,D08-1038,0,0.140784,"Missing"
Q18-1028,P14-5010,0,0.00248399,"depth of the decision tree as 10 or n, where n is the number of features; minimum leaf size in decision tree [2, . . . , 10]; number of topics [50, 100, 250, 500]; and whether to use Smote (Chawla et al., 2002). System Macro F1 This work 0.530 without topic features 0.502 without selectional prefs. 0.464 without bootstrapped pats. 0.457 without any novel features 0.474 Abu-Jbara et al. (2013) 0.410 Teufel (2000) 0.273 Dong and Sch¨afer (2011) 0.233 Majority-Class 0.092 Random 0.138 classifier is implemented using SciKit (Pedregosa et al., 2011) and syntactic processing was done using CoreNLP (Manning et al., 2014). Selectional preferences used pretrained 300-dimensional GloVe vectors from the 840B token Common Crawl (Pennington et al., 2014). The topic model features used an LDA with 100 topics. Data Annotated data is crucial for developing high accuracy for rare citation classes. Therefore, we integrate portions of the dataset of Teufel (2010),6 which has fine-grained citation function labeled for ACL-related documents using the annotation scheme of Teufel et al. (2006b). We map their 12 function classes into our six classes (see Appendix A). When combining the two datasets, we omit the data labeled w"
Q18-1028,J93-2004,0,0.0640913,"a core set of parsing papers, e.g., (Collins, 1999; Buchholz et al., 1999; Collins, 1997), with a much sharper (lower entropy) distribution of citations. These trends show the increased incorporation of prior work to form a lineage of method technologies as well as show increased consensus on which works are sufficient for comparing against in order to establish a claim. These results also empirically confirm the observation of Sp¨arck Jones (2001) that a major trend in NLP in the 1990s was an increase in reusable technologies and evaluations, like the BNC (Leech, 1992) and the Penn Treebank (Marcus et al., 1993). More broadly, our work points to the future of NLP as a quickly moving field of high consensus and suggests that artifacts that facilitate consensus such as shared tasks and open source research software will be necessary to continue this trend. 9 Conclusion pus annotated with citation function and by developing a state-of-the-art classifier for revealing scientific framing. In doing so, we demonstrate the importance of novel unsupervised features related to topic models and argument structure, and label all the citations for an entire field. We then show that citation framing reveals salien"
Q18-1028,D14-1162,0,0.0800752,"Missing"
Q18-1028,P83-1021,0,0.551973,"hors chose to include proportionally more non-methodological citations.9 In the second trend, authors are more likely to use and compare against the same set of papers, as shown in Figure 6 by the rise in expected incom9 Note that this change acts against the general decrease in non-methodological; considering only 1980-2009, the decrease in non-methodological is even larger (r= -0.568, p ≤ 0.01 ). ing citations to those works compared against (r= 0.734, p ≤ 0.01) and used (r=0.889, p ≤ 0.01). For example, in 1991, authors compared with a diffuse group of parsing papers, e.g., (Shieber, 1988; Pereira and Warren, 1983; Haas, 1989), with such papers receiving at most three citations that year; whereas in 2000, most comparisons were to a core set of parsing papers, e.g., (Collins, 1999; Buchholz et al., 1999; Collins, 1997), with a much sharper (lower entropy) distribution of citations. These trends show the increased incorporation of prior work to form a lineage of method technologies as well as show increased consensus on which works are sufficient for comparing against in order to establish a claim. These results also empirically confirm the observation of Sp¨arck Jones (2001) that a major trend in NLP in"
Q18-1028,C88-2128,0,0.308164,"ited space, authors chose to include proportionally more non-methodological citations.9 In the second trend, authors are more likely to use and compare against the same set of papers, as shown in Figure 6 by the rise in expected incom9 Note that this change acts against the general decrease in non-methodological; considering only 1980-2009, the decrease in non-methodological is even larger (r= -0.568, p ≤ 0.01 ). ing citations to those works compared against (r= 0.734, p ≤ 0.01) and used (r=0.889, p ≤ 0.01). For example, in 1991, authors compared with a diffuse group of parsing papers, e.g., (Shieber, 1988; Pereira and Warren, 1983; Haas, 1989), with such papers receiving at most three citations that year; whereas in 2000, most comparisons were to a core set of parsing papers, e.g., (Collins, 1999; Buchholz et al., 1999; Collins, 1997), with a much sharper (lower entropy) distribution of citations. These trends show the increased incorporation of prior work to form a lineage of method technologies as well as show increased consensus on which works are sufficient for comparing against in order to establish a claim. These results also empirically confirm the observation of Sp¨arck Jones (2001) th"
Q18-1028,E12-2021,0,0.105368,"Missing"
Q18-1028,W06-1312,0,0.173037,"f citation framing by first developing a state-of-the-art method for automatically classifying citation function and then applying this method to an entire field’s literature to quantify the effects and evolution of framing. Analyzing large-scale changes in citation framing requires an accurate method for classifying the function a citation plays towards furthering an argument. Due to the difficulty of interpreting citation intent, many prior works performed manual analysis (Moravcsik and Murugesan, 1975; Swales, 1990; Harwood, 2009) and only recently have automated approaches been developed (Teufel et al., 2006b; Valenzuela et al., 2015). Here, we unify core aspects of several prior citation annotation schemes (White, 2004; Ding et al., 2014; Hern´andez-Alvarez and Gomez, 2016). Using this scheme, we create 391 Transactions of the Association for Computational Linguistics, vol. 6, pp. 391–406, 2018. Action Editor: Katrin Erk. Submission batch: 8/2017; Revision batch: 12/2017; Published 6/2018. c 2018 Association for Computational Linguistics. Distributed under a CC-BY 4.0 license. one of the largest annotated corpora of citations and use it to train a high-accuracy method for automatically labeling"
Q18-1028,W06-1613,0,0.186058,"f citation framing by first developing a state-of-the-art method for automatically classifying citation function and then applying this method to an entire field’s literature to quantify the effects and evolution of framing. Analyzing large-scale changes in citation framing requires an accurate method for classifying the function a citation plays towards furthering an argument. Due to the difficulty of interpreting citation intent, many prior works performed manual analysis (Moravcsik and Murugesan, 1975; Swales, 1990; Harwood, 2009) and only recently have automated approaches been developed (Teufel et al., 2006b; Valenzuela et al., 2015). Here, we unify core aspects of several prior citation annotation schemes (White, 2004; Ding et al., 2014; Hern´andez-Alvarez and Gomez, 2016). Using this scheme, we create 391 Transactions of the Association for Computational Linguistics, vol. 6, pp. 391–406, 2018. Action Editor: Katrin Erk. Submission batch: 8/2017; Revision batch: 12/2017; Published 6/2018. c 2018 Association for Computational Linguistics. Distributed under a CC-BY 4.0 license. one of the largest annotated corpora of citations and use it to train a high-accuracy method for automatically labeling"
Q18-1028,D11-1055,0,0.0604872,"Missing"
Q18-1028,P84-1044,0,0.0986665,"enable future research. 2 A Corpus for Citation Function Citations play a key role in supporting authors’ contributions throughout a scientific paper.1 Multiple schemes have been proposed on how to classify these different roles, ranging from a handful of classes (Nanba and Okumura, 1999; Pham and Hoffmann, 2003) to twenty or more (Garfield, 1979; Garzone and Mercer, 2000). While suitable for expert manual analysis, many schemes include either fine-grained distinctions that are too rare to reliably identify or subjective classifications that require detailed knowledge of the field or author (Ziman, 1968; Swales, 1990; Harwood, 2009). Motivated by the desire to automatically examine large-scale trends in scholarly behavior, we address these issues by unifying the common aspects of multiple approaches in a single classification. viding background or serving as contrast (Oppenheim and Renn, 1978; Spiegel-R¨using, 1977; Teufel et al., 2006a; Garfield, 1979; Garzone and Mercer, 2000; Abu-Jbara et al., 2013).2 Citation function reflects the specific purpose a citation plays with respect to the current paper’s contributions. We unify the functional roles common in several classifications, e.g., (Sp"
Q18-1033,Q16-1033,0,0.0176208,"nstrate the importance of understanding the role of institutional context in shaping conversation structure. In doing so, our paper also draws on recent research on automatically extracting structure from human-human dialog. Drawing on Grosz’s original insights, Bangalore et al. (2006) show how to extract a hierarchical task structure for catalog ordering dialogs with subtasks like opening, contact-information, order-item, relatedoffers, and summary. Prabhakaran et al. (2012) and Prabhakaran et al. (2014) employ dialog act analysis to study correlates of gender and power in work emails, while Althoff et al. (2016) studied structural aspects of successful counseling conversations, and Yang et al. (2013) and Chandrasekaran et al. (2017) investigated structures in online classroom conversations that predict success or need for intervention. Our work also draws on an important line of unsupervised work that models topical structure of conversations (Blei and Moreno, 2001; Eisenstein and Barzilay, 2008; Paul, 2012; Nguyen et al., 2012). Our work is closely related to the active line of research in NLP on dialog act classification. Recently, recurrent neural network-based dialog act taggers, e.g., Khanpour e"
Q18-1033,P06-1026,0,0.0470307,"Missing"
Q18-1033,cieri-etal-2004-fisher,0,0.109806,"eraged F-scores on institutional act prediction using different ASR sources. Table 6 shows word error rates under different settings. Overall, we obtain relatively high error rates, largely due to the noisy environment of the audio in this domain. BLSTM performs better than DNNHMM, consistent with prior research (Mohamed et al., 2015; Sak et al., 2014).11 Interpolating Switchboard and Fisher language models provides a further boost of 0.7 percentage points. 6.3 Language Model Data Augmentation 6.5 To mitigate language model data scarcity, we use transcriptions from the Switchboard and Fisher (Cieri et al., 2004) corpora, adding about 3.12M and 21.1M words, respectively. Separate language models are trained on these datasets, and then interpolated with the traffic stop language model; interpolation weights were chosen by minimizing perplexity on a separate Dev set. Table 5 shows the perplexities of different language models on this Dev set. We now use text generated by ASR to train and test the institutional act tagger of Section 4. To increase recall, we also made use of N-best list output from the ASR systems, collecting ngram and pattern features from the top 10 candidate transcriptions. The L1 pen"
Q18-1033,W04-3240,0,0.202657,"Missing"
Q18-1033,D08-1035,0,0.0210716,"e opening, contact-information, order-item, relatedoffers, and summary. Prabhakaran et al. (2012) and Prabhakaran et al. (2014) employ dialog act analysis to study correlates of gender and power in work emails, while Althoff et al. (2016) studied structural aspects of successful counseling conversations, and Yang et al. (2013) and Chandrasekaran et al. (2017) investigated structures in online classroom conversations that predict success or need for intervention. Our work also draws on an important line of unsupervised work that models topical structure of conversations (Blei and Moreno, 2001; Eisenstein and Barzilay, 2008; Paul, 2012; Nguyen et al., 2012). Our work is closely related to the active line of research in NLP on dialog act classification. Recently, recurrent neural network-based dialog act taggers, e.g., Khanpour et al. (2016), Li and Wu (2016) and Liu et al. (2017), have posted state-of-the-art performance on benchmark datasets such as the Switchboard corpus (Jurafsky et al., 1997) and MRDA (Ang et al., 2005). Since these corpora come from significantly different domains (telephone conversations and meeting transcripts, respectively) than ours, and since we are interested specifically in the insti"
Q18-1033,C16-1189,0,0.0224325,"al. (2016) studied structural aspects of successful counseling conversations, and Yang et al. (2013) and Chandrasekaran et al. (2017) investigated structures in online classroom conversations that predict success or need for intervention. Our work also draws on an important line of unsupervised work that models topical structure of conversations (Blei and Moreno, 2001; Eisenstein and Barzilay, 2008; Paul, 2012; Nguyen et al., 2012). Our work is closely related to the active line of research in NLP on dialog act classification. Recently, recurrent neural network-based dialog act taggers, e.g., Khanpour et al. (2016), Li and Wu (2016) and Liu et al. (2017), have posted state-of-the-art performance on benchmark datasets such as the Switchboard corpus (Jurafsky et al., 1997) and MRDA (Ang et al., 2005). Since these corpora come from significantly different domains (telephone conversations and meeting transcripts, respectively) than ours, and since we are interested specifically in the institutional acts (e.g., did the officer request documentation from the driver?) rather than the general dialog acts (did the officer issue a request?), these taggers do not directly serve our purpose. Furthermore, our data i"
Q18-1033,D10-1084,0,0.150742,"s a tool for police departments to assess and improve police community interactions. 2 Background Computational work on human-human conversation has long focused on dialog structure, beginning with the influential work of Grosz showing the homology between dialog and task structure (Grosz, 1977). Recent work has integrated speech act theory (Austin, 1975) and conversational analysis (Schegloff and Sacks, 1973; Sacks et al., 1974; Schegloff, 1979) into models of dialog acts for domains like meetings (Ang et al., 2005), telephone calls (Stolcke et al., 2006), emails (Cohen et al., 2004), chats (Kim et al., 2010), and Twitter (Ritter et al., 2010). Our models extend this work by drawing on the notion of institutional talk (Atkinson and Drew, 1979), an application of conversational analysis to environments in which the goals of participants are institution-specific. Actions, their sequences, and interpretations during institutional talk depend not 468 only on the speaker (as speech act theory suggests) or the dialog (as conversational analysts argue), but they are inherently tied to the institutional context. Institutional talk has been used as a tool to understand the work of social institutions. For"
Q18-1033,Y12-1050,0,0.0147698,"onversations and meeting transcripts, respectively) than ours, and since we are interested specifically in the institutional acts (e.g., did the officer request documentation from the driver?) rather than the general dialog acts (did the officer issue a request?), these taggers do not directly serve our purpose. Furthermore, our data is an order of magnitude smaller (around 7K sentences) than these corpora; making it infeasible to train in-domain recurrent networks. Prior to neural network approaches, support vector machines and conditional random fields (Cohen et al., 2004; Kim et al., 2010; Kim et al., 2012; Omuya et al., 2013) were the state-of-the-art algorithms on this task. These approaches also incorporated contextual and structural information into the classifier. For instance, Kim et al. (2012) used lexical information from previous utterances in predicting the dialog act of a current utterance; and Omuya et al. (2013) uses features such as the relative position of an utterance w.r.t the whole dialog. We draw from this line of work; we also experiment with positional and contextual features in addition to lexical features. Furthermore, we use features that capture the institutional contex"
Q18-1033,D14-1181,0,0.00499336,"Missing"
Q18-1033,C16-1185,0,0.0186278,"tural aspects of successful counseling conversations, and Yang et al. (2013) and Chandrasekaran et al. (2017) investigated structures in online classroom conversations that predict success or need for intervention. Our work also draws on an important line of unsupervised work that models topical structure of conversations (Blei and Moreno, 2001; Eisenstein and Barzilay, 2008; Paul, 2012; Nguyen et al., 2012). Our work is closely related to the active line of research in NLP on dialog act classification. Recently, recurrent neural network-based dialog act taggers, e.g., Khanpour et al. (2016), Li and Wu (2016) and Liu et al. (2017), have posted state-of-the-art performance on benchmark datasets such as the Switchboard corpus (Jurafsky et al., 1997) and MRDA (Ang et al., 2005). Since these corpora come from significantly different domains (telephone conversations and meeting transcripts, respectively) than ours, and since we are interested specifically in the institutional acts (e.g., did the officer request documentation from the driver?) rather than the general dialog acts (did the officer issue a request?), these taggers do not directly serve our purpose. Furthermore, our data is an order of magn"
Q18-1033,D17-1231,0,0.0147971,"essful counseling conversations, and Yang et al. (2013) and Chandrasekaran et al. (2017) investigated structures in online classroom conversations that predict success or need for intervention. Our work also draws on an important line of unsupervised work that models topical structure of conversations (Blei and Moreno, 2001; Eisenstein and Barzilay, 2008; Paul, 2012; Nguyen et al., 2012). Our work is closely related to the active line of research in NLP on dialog act classification. Recently, recurrent neural network-based dialog act taggers, e.g., Khanpour et al. (2016), Li and Wu (2016) and Liu et al. (2017), have posted state-of-the-art performance on benchmark datasets such as the Switchboard corpus (Jurafsky et al., 1997) and MRDA (Ang et al., 2005). Since these corpora come from significantly different domains (telephone conversations and meeting transcripts, respectively) than ours, and since we are interested specifically in the institutional acts (e.g., did the officer request documentation from the driver?) rather than the general dialog acts (did the officer issue a request?), these taggers do not directly serve our purpose. Furthermore, our data is an order of magnitude smaller (around"
Q18-1033,P12-1009,0,0.0263844,"relatedoffers, and summary. Prabhakaran et al. (2012) and Prabhakaran et al. (2014) employ dialog act analysis to study correlates of gender and power in work emails, while Althoff et al. (2016) studied structural aspects of successful counseling conversations, and Yang et al. (2013) and Chandrasekaran et al. (2017) investigated structures in online classroom conversations that predict success or need for intervention. Our work also draws on an important line of unsupervised work that models topical structure of conversations (Blei and Moreno, 2001; Eisenstein and Barzilay, 2008; Paul, 2012; Nguyen et al., 2012). Our work is closely related to the active line of research in NLP on dialog act classification. Recently, recurrent neural network-based dialog act taggers, e.g., Khanpour et al. (2016), Li and Wu (2016) and Liu et al. (2017), have posted state-of-the-art performance on benchmark datasets such as the Switchboard corpus (Jurafsky et al., 1997) and MRDA (Ang et al., 2005). Since these corpora come from significantly different domains (telephone conversations and meeting transcripts, respectively) than ours, and since we are interested specifically in the institutional acts (e.g., did the offic"
Q18-1033,N13-1099,1,0.848533,"eeting transcripts, respectively) than ours, and since we are interested specifically in the institutional acts (e.g., did the officer request documentation from the driver?) rather than the general dialog acts (did the officer issue a request?), these taggers do not directly serve our purpose. Furthermore, our data is an order of magnitude smaller (around 7K sentences) than these corpora; making it infeasible to train in-domain recurrent networks. Prior to neural network approaches, support vector machines and conditional random fields (Cohen et al., 2004; Kim et al., 2010; Kim et al., 2012; Omuya et al., 2013) were the state-of-the-art algorithms on this task. These approaches also incorporated contextual and structural information into the classifier. For instance, Kim et al. (2012) used lexical information from previous utterances in predicting the dialog act of a current utterance; and Omuya et al. (2013) uses features such as the relative position of an utterance w.r.t the whole dialog. We draw from this line of work; we also experiment with positional and contextual features in addition to lexical features. Furthermore, we use features that capture the institutional context of the conversation"
Q18-1033,D12-1009,0,0.114192,"order-item, relatedoffers, and summary. Prabhakaran et al. (2012) and Prabhakaran et al. (2014) employ dialog act analysis to study correlates of gender and power in work emails, while Althoff et al. (2016) studied structural aspects of successful counseling conversations, and Yang et al. (2013) and Chandrasekaran et al. (2017) investigated structures in online classroom conversations that predict success or need for intervention. Our work also draws on an important line of unsupervised work that models topical structure of conversations (Blei and Moreno, 2001; Eisenstein and Barzilay, 2008; Paul, 2012; Nguyen et al., 2012). Our work is closely related to the active line of research in NLP on dialog act classification. Recently, recurrent neural network-based dialog act taggers, e.g., Khanpour et al. (2016), Li and Wu (2016) and Liu et al. (2017), have posted state-of-the-art performance on benchmark datasets such as the Switchboard corpus (Jurafsky et al., 1997) and MRDA (Ang et al., 2005). Since these corpora come from significantly different domains (telephone conversations and meeting transcripts, respectively) than ours, and since we are interested specifically in the institutional act"
Q18-1033,N12-1057,1,0.813135,"(e.g. do they listen, take civilian views into account) predict civilian’s attitudes towards the police (Giles et al., 2006). These findings demonstrate the importance of understanding the role of institutional context in shaping conversation structure. In doing so, our paper also draws on recent research on automatically extracting structure from human-human dialog. Drawing on Grosz’s original insights, Bangalore et al. (2006) show how to extract a hierarchical task structure for catalog ordering dialogs with subtasks like opening, contact-information, order-item, relatedoffers, and summary. Prabhakaran et al. (2012) and Prabhakaran et al. (2014) employ dialog act analysis to study correlates of gender and power in work emails, while Althoff et al. (2016) studied structural aspects of successful counseling conversations, and Yang et al. (2013) and Chandrasekaran et al. (2017) investigated structures in online classroom conversations that predict success or need for intervention. Our work also draws on an important line of unsupervised work that models topical structure of conversations (Blei and Moreno, 2001; Eisenstein and Barzilay, 2008; Paul, 2012; Nguyen et al., 2012). Our work is closely related to t"
Q18-1033,D14-1211,1,0.859702,"ilian views into account) predict civilian’s attitudes towards the police (Giles et al., 2006). These findings demonstrate the importance of understanding the role of institutional context in shaping conversation structure. In doing so, our paper also draws on recent research on automatically extracting structure from human-human dialog. Drawing on Grosz’s original insights, Bangalore et al. (2006) show how to extract a hierarchical task structure for catalog ordering dialogs with subtasks like opening, contact-information, order-item, relatedoffers, and summary. Prabhakaran et al. (2012) and Prabhakaran et al. (2014) employ dialog act analysis to study correlates of gender and power in work emails, while Althoff et al. (2016) studied structural aspects of successful counseling conversations, and Yang et al. (2013) and Chandrasekaran et al. (2017) investigated structures in online classroom conversations that predict success or need for intervention. Our work also draws on an important line of unsupervised work that models topical structure of conversations (Blei and Moreno, 2001; Eisenstein and Barzilay, 2008; Paul, 2012; Nguyen et al., 2012). Our work is closely related to the active line of research in"
Q18-1033,N10-1020,0,0.0835973,"to assess and improve police community interactions. 2 Background Computational work on human-human conversation has long focused on dialog structure, beginning with the influential work of Grosz showing the homology between dialog and task structure (Grosz, 1977). Recent work has integrated speech act theory (Austin, 1975) and conversational analysis (Schegloff and Sacks, 1973; Sacks et al., 1974; Schegloff, 1979) into models of dialog acts for domains like meetings (Ang et al., 2005), telephone calls (Stolcke et al., 2006), emails (Cohen et al., 2004), chats (Kim et al., 2010), and Twitter (Ritter et al., 2010). Our models extend this work by drawing on the notion of institutional talk (Atkinson and Drew, 1979), an application of conversational analysis to environments in which the goals of participants are institution-specific. Actions, their sequences, and interpretations during institutional talk depend not 468 only on the speaker (as speech act theory suggests) or the dialog (as conversational analysts argue), but they are inherently tied to the institutional context. Institutional talk has been used as a tool to understand the work of social institutions. For example, Whalen and Zimmerman (1987"
W03-1706,P95-1037,0,0.12758,"Missing"
W03-1706,A88-1019,0,0.312544,"Missing"
W03-1706,P97-1003,0,0.210726,"Missing"
W03-1706,1997.iwpt-1.13,0,0.0913361,"Missing"
W03-1706,W95-0107,0,0.0160218,"rivate 经 济 /economy 发展/develop 的/AUX 势头/trend 很 /very 好 /good”(‘The developmental trend of private economy is very good.’), “私营/private 经 济 /economy 发 展 /develop” is a content word sequence, but it’s not a phrase; only “私营/private 经济/economy” in it is a phrase. The purpose of content chunk parsing is to recognize phrases in a sequence of content words. Specifically speaking, the content chunking contains two subtasks: (1) to recognize the maximum phrase in a sequence of content words; (2) to analyze the hierarchical structure within the phrase down to words. Like baseNP chunking(Church, 1988; Ramshaw & Marcus 1995), content chunk parsing is also a kind of shallow parsing. Content chunk parsing is deeper than baseNP chunking in two aspects: (1) a content chunk may contain verb phrases and other phrases even a full sentence as long as the all the components are content words; (2) it may contain recursive NPs. Thus the content chunk can supply more structural information than a baseNP. The motives for content chunk parsing are twofold: (1) Like other shallow parsing tasks, it can simplify the parsing task. This can be explained in two aspects. First, it can avoid the ambiguities brought up by functional wo"
W08-0304,P07-2045,0,0.0546783,"h all other candidate translations that have yet to be selected as the 1-best. And, for each of the n n-best lists, this may have to be done up to m − 1 times. 2.2 Search Strategies In this section, we review two search strategies that, in conjunction with the line search just described, can be used to drive MERT. The first, Powell’s method, was advocated by Och (2003) when MERT was first introduced for statistical machine translation. The second, which we call Koehn-coordinate descent (KCD)6 , is used by the MERT utility packaged with the popular Moses statistical machine translation system (Koehn et al., 2007). 6 Moses uses David Chiang’s CMERT package. Within the source file mert.c, the function that implements the overall search strategy, optimize koehn(), is based on Philipp Koehn’s Perl script for MERT optimization that was distributed with Pharaoh. 2.2.1 Powell’s Method 3.1 Powell’s method (Press et al., 2007) attempts to efficiently search the objective by constructing a set of mutually non-interfering search directions. The basic procedure is as follows: (i) A collection of search directions is initialized to be the coordinates of the space being searched; (ii) The objective is minimized by"
W08-0304,N03-1017,0,0.0204375,"hich the results are influenced by some runs receiving starting points that are better in general or perhaps better/worse w.r.t. their specific optimization strategy. 32 Dev MT02 30.967 30.638 31.681 Test MT03 30.778 30.692 31.754 Test MT05 29.580 29.780 30.191 Table 3: BLEU scores obtained by models trained using the three different parameter search strategies: Powell’s method, KCD, and stochastic search. data. The Chinese data was word segmented using the GALE Y2 retest release of the Stanford CRF segmenter (Tseng et al., 2005). Phrases were extracted using the typical approach described in Koehn et al. (2003) of running GIZA++ (Och & Ney, 2003) in both directions and then merging the alignments using the grow-diag-final heuristic. From the merged alignments we also extracted a bidirectional lexical reordering model conditioned on the source and the target phrases (Tillmann, 2004) (Koehn et al., 2007). A 5-gram language model was created using the SRI language modeling toolkit (Stolcke, 2002) and trained using the Gigaword corpus and English sentences from the parallel data. 5 Results As illustrated in table 3, Powell’s method and KCD achieve a very similar level of performance, with KCD modestly o"
W08-0304,P03-1021,0,0.778611,"Minimum error rate training (MERT) is a widely used learning procedure for statistical machine translation models. We contrast three search strategies for MERT: Powell’s method, the variant of coordinate descent found in the Moses MERT utility, and a novel stochastic method. It is shown that the stochastic method obtains test set gains of +0.98 BLEU on MT03 and +0.61 BLEU on MT05. We also present a method for regularizing the MERT objective that achieves statistically significant gains when combined with both Powell’s method and coordinate descent. 1 2 Minimum Error Rate Training Introduction Och (2003) introduced minimum error rate training (MERT) as an alternative training regime to the conditional likelihood objective previously used with log-linear translation models (Och & Ney, 2002). This approach attempts to improve translation quality by optimizing an automatic translation evaluation metric, such as the BLEU score (Papineni et al., 2002). This is accomplished by either directly walking the error surface provided by an evaluation metric w.r.t. the model weights or by using gradientbased techniques on a continuous approximation of such a surface. While the former is piecewise constant"
W08-0304,P02-1038,0,0.0634091,"ariant of coordinate descent found in the Moses MERT utility, and a novel stochastic method. It is shown that the stochastic method obtains test set gains of +0.98 BLEU on MT03 and +0.61 BLEU on MT05. We also present a method for regularizing the MERT objective that achieves statistically significant gains when combined with both Powell’s method and coordinate descent. 1 2 Minimum Error Rate Training Introduction Och (2003) introduced minimum error rate training (MERT) as an alternative training regime to the conditional likelihood objective previously used with log-linear translation models (Och & Ney, 2002). This approach attempts to improve translation quality by optimizing an automatic translation evaluation metric, such as the BLEU score (Papineni et al., 2002). This is accomplished by either directly walking the error surface provided by an evaluation metric w.r.t. the model weights or by using gradientbased techniques on a continuous approximation of such a surface. While the former is piecewise constant and thus cannot be optimized using gradient techniques, Och (2003) provides an approach that performs such training efficiently. In this paper we explore a number of variations on MERT. Fir"
W08-0304,J03-1002,0,0.00325925,"runs receiving starting points that are better in general or perhaps better/worse w.r.t. their specific optimization strategy. 32 Dev MT02 30.967 30.638 31.681 Test MT03 30.778 30.692 31.754 Test MT05 29.580 29.780 30.191 Table 3: BLEU scores obtained by models trained using the three different parameter search strategies: Powell’s method, KCD, and stochastic search. data. The Chinese data was word segmented using the GALE Y2 retest release of the Stanford CRF segmenter (Tseng et al., 2005). Phrases were extracted using the typical approach described in Koehn et al. (2003) of running GIZA++ (Och & Ney, 2003) in both directions and then merging the alignments using the grow-diag-final heuristic. From the merged alignments we also extracted a bidirectional lexical reordering model conditioned on the source and the target phrases (Tillmann, 2004) (Koehn et al., 2007). A 5-gram language model was created using the SRI language modeling toolkit (Stolcke, 2002) and trained using the Gigaword corpus and English sentences from the parallel data. 5 Results As illustrated in table 3, Powell’s method and KCD achieve a very similar level of performance, with KCD modestly outperforming Powell on the MT03 test"
W08-0304,P02-1040,0,0.106415,"of +0.98 BLEU on MT03 and +0.61 BLEU on MT05. We also present a method for regularizing the MERT objective that achieves statistically significant gains when combined with both Powell’s method and coordinate descent. 1 2 Minimum Error Rate Training Introduction Och (2003) introduced minimum error rate training (MERT) as an alternative training regime to the conditional likelihood objective previously used with log-linear translation models (Och & Ney, 2002). This approach attempts to improve translation quality by optimizing an automatic translation evaluation metric, such as the BLEU score (Papineni et al., 2002). This is accomplished by either directly walking the error surface provided by an evaluation metric w.r.t. the model weights or by using gradientbased techniques on a continuous approximation of such a surface. While the former is piecewise constant and thus cannot be optimized using gradient techniques, Och (2003) provides an approach that performs such training efficiently. In this paper we explore a number of variations on MERT. First, it is shown that performance gains can be had by making use of a stochastic search strategy as compare to that obtained by Powell’s method and Let F be a co"
W08-0304,W05-0908,0,0.0266415,"20 30.191 29.529 29.963 30.674 Table 4: BLEU scores obtained when regularizing using the average loss of adjacent plateaus, left, and the maximum loss of adjacent plateaus, right. The none entry for each search strategy represents the baseline where no regularization is used. Statistically significant test set gains, p &lt; 0.01, over the respective baselines are in bold face. of Powell’s method, diagonal search, with coordinate descent’s robustness to the sudden jumps between regions that result from global line minimization. Using an approximate randomization test for statistical significance (Riezler & Maxwell, 2005), and with KCD as a baseline, the gains obtained by stochastic search on MT03 are statistically significant (p = 0.002), as are the gains on MT05 (p = 0.005). Table 4 indicates that performing regularization by either averaging or taking the maximum of adjacent plateaus during the line search leads to gains for both Powell’s method and KCD. However, no reliable additional gains appear to be had when stochastic search is combined with regularization. It may seem surprising that the regularization gains for Powell & KCD are seen not only in the test sets but on the dev set as well. That is, in t"
W08-0304,P06-2101,0,0.284978,"tion to other data sets. This could be avoided by regularizing the surface in order to eliminate such spurious minima. One candidate for performing such regularization is the continuous approximation of the MERT objective, O = Epw (`). Och (2003) claimed that this approximation achieved essentially equivalent performance to that obtained when directly using the loss as the objective, O = `. However, Zens et al. (2007) found that O = Epw (`) achieved substantially better test set performance than O = `, even though it performs slightly worse on the data used to train the parameters. Similarly, Smith and Eisner (2006) reported test set gains for the related technique of minimum risk annealing, which incorporates a temper8 However, we speculate that similar results could be obtained using a uniform distribution over (−1, 1) 31 ature parameter that trades off between the smoothness of the objective and the degree it reflects the underlying piecewise constant error surface. However, the most straightforward implementation of such methods requires a loss that can be applied at the sentence level. If the evaluation metric of interest does not have this property (e.g. BLEU), the loss must be approximated using s"
W08-0304,N04-4026,0,0.0132182,"es obtained by models trained using the three different parameter search strategies: Powell’s method, KCD, and stochastic search. data. The Chinese data was word segmented using the GALE Y2 retest release of the Stanford CRF segmenter (Tseng et al., 2005). Phrases were extracted using the typical approach described in Koehn et al. (2003) of running GIZA++ (Och & Ney, 2003) in both directions and then merging the alignments using the grow-diag-final heuristic. From the merged alignments we also extracted a bidirectional lexical reordering model conditioned on the source and the target phrases (Tillmann, 2004) (Koehn et al., 2007). A 5-gram language model was created using the SRI language modeling toolkit (Stolcke, 2002) and trained using the Gigaword corpus and English sentences from the parallel data. 5 Results As illustrated in table 3, Powell’s method and KCD achieve a very similar level of performance, with KCD modestly outperforming Powell on the MT03 test set while Powell modestly outperforms coordinate descent on the MT05 test set. Moreover, the fact that Powell’s algorithm did not perform better than KCD on the training data10 , and in fact actually performed modestly worse, suggests that"
W08-0304,I05-3027,1,0.564376,"izable number of random restarts should be used in order to minimize the degree to which the results are influenced by some runs receiving starting points that are better in general or perhaps better/worse w.r.t. their specific optimization strategy. 32 Dev MT02 30.967 30.638 31.681 Test MT03 30.778 30.692 31.754 Test MT05 29.580 29.780 30.191 Table 3: BLEU scores obtained by models trained using the three different parameter search strategies: Powell’s method, KCD, and stochastic search. data. The Chinese data was word segmented using the GALE Y2 retest release of the Stanford CRF segmenter (Tseng et al., 2005). Phrases were extracted using the typical approach described in Koehn et al. (2003) of running GIZA++ (Och & Ney, 2003) in both directions and then merging the alignments using the grow-diag-final heuristic. From the merged alignments we also extracted a bidirectional lexical reordering model conditioned on the source and the target phrases (Tillmann, 2004) (Koehn et al., 2007). A 5-gram language model was created using the SRI language modeling toolkit (Stolcke, 2002) and trained using the Gigaword corpus and English sentences from the parallel data. 5 Results As illustrated in table 3, Powe"
W08-0304,D07-1055,0,0.249878,"be 39.1 while all surrounding plateaus have a BLEU score that is &lt; 10. Intuitively, such a minima would be a very bad solution, as the resulting parameters would likely exhibit very poor generalization to other data sets. This could be avoided by regularizing the surface in order to eliminate such spurious minima. One candidate for performing such regularization is the continuous approximation of the MERT objective, O = Epw (`). Och (2003) claimed that this approximation achieved essentially equivalent performance to that obtained when directly using the loss as the objective, O = `. However, Zens et al. (2007) found that O = Epw (`) achieved substantially better test set performance than O = `, even though it performs slightly worse on the data used to train the parameters. Similarly, Smith and Eisner (2006) reported test set gains for the related technique of minimum risk annealing, which incorporates a temper8 However, we speculate that similar results could be obtained using a uniform distribution over (−1, 1) 31 ature parameter that trades off between the smoothness of the objective and the degree it reflects the underlying piecewise constant error surface. However, the most straightforward imp"
W09-2307,W08-0336,1,0.374339,"tion such as in the loc (localizer) relation. For the example in Figure 1, if we look at the sentence structure from the typed dependency parse (bottom of Figure 1), “d d d” is connected to the main verb dd (finish) by a loc (localizer) relation, and the structure is the same for sentences (a) and (b). This suggests that this kind of semantic and syntactic representation could have more benefit than phrase structure parses. Our Chinese typed dependencies are automatically extracted from phrase structure parses. In English, this kind of typed dependencies has been introduced by de Marneffe and Manning (2008) and de Marneffe et al. (2006). Using typed dependencies, it is easier to read out relations between words, and thus the typed dependencies have been used in meaning extraction tasks. We design features over the Chinese typed dependencies and use them in a phrase-based MT system when deciding whether one chunk of Chinese words (MT system statistical phrase) should appear before or after another. To achieve this, we train a discriminative phrase orientation classifier following the work by Zens and Ney (2006), and we use the grammatical relations between words as extra features to build the cla"
W09-2307,W08-1301,1,0.0607658,"Missing"
W09-2307,de-marneffe-etal-2006-generating,1,0.0159854,"Missing"
W09-2307,2006.amta-papers.8,0,0.0161748,"ish are a major factor in the difficulty of machine translation from Chinese to English. The wide variety of such Chinese-English differences include the ordering of head nouns and relative clauses, and the ordering of prepositional phrases and the heads they modify. Previous studies have shown that using syntactic structures from the source side can help MT performance on these constructions. Most of the previous syntactic MT work has used phrase structure parses in various ways, either by doing syntaxdirected translation to directly translate parse trees into strings in the target language (Huang et al., 2006), or by using source-side parses to preprocess the source sentences (Wang et al., 2007). One intuition for using syntax is to capture different Chinese structures that might have the same 51 ࡐ ࡐ (fixed) Figure 1: Sentences (a) and (b) have the same meaning, but different phrase structure parses. Both sentences, however, have the same typed dependencies shown at the bottom of the figure. meaning and hence the same translation in English. But it turns out that phrase structure (and linear order) are not sufficient to capture this meaning relation. Two sentences with the same meaning can have d"
W09-2307,N03-1017,0,0.00387383,"n a phrase-based MT system when deciding whether one chunk of Chinese words (MT system statistical phrase) should appear before or after another. To achieve this, we train a discriminative phrase orientation classifier following the work by Zens and Ney (2006), and we use the grammatical relations between words as extra features to build the classifier. We then apply the phrase orientation classifier as a feature in a phrasebased MT system to help reordering. 2 Discriminative Reordering Model Basic reordering models in phrase-based systems use linear distance as the cost for phrase movements (Koehn et al., 2003). The disadvantage of these models is their insensitivity to the content of the words or phrases. More recent work (Tillman, 2004; Och et al., 2004; Koehn et al., 2007) has in52 troduced lexicalized reordering models which estimate reordering probabilities conditioned on the actual phrases. Lexicalized reordering models have brought significant gains over the baseline reordering models, but one concern is that data sparseness can make estimation less reliable. Zens and Ney (2006) proposed a discriminatively trained phrase orientation model and evaluated its performance as a classifier and when"
W09-2307,P07-2045,0,0.00675784,"ain a discriminative phrase orientation classifier following the work by Zens and Ney (2006), and we use the grammatical relations between words as extra features to build the classifier. We then apply the phrase orientation classifier as a feature in a phrasebased MT system to help reordering. 2 Discriminative Reordering Model Basic reordering models in phrase-based systems use linear distance as the cost for phrase movements (Koehn et al., 2003). The disadvantage of these models is their insensitivity to the content of the words or phrases. More recent work (Tillman, 2004; Och et al., 2004; Koehn et al., 2007) has in52 troduced lexicalized reordering models which estimate reordering probabilities conditioned on the actual phrases. Lexicalized reordering models have brought significant gains over the baseline reordering models, but one concern is that data sparseness can make estimation less reliable. Zens and Ney (2006) proposed a discriminatively trained phrase orientation model and evaluated its performance as a classifier and when plugged into a phrase-based MT system. Their framework allows us to easily add in extra features. Therefore we use it as a testbed to see if we can effectively use fea"
W09-2307,N06-1014,0,0.0173938,"Missing"
W09-2307,P03-1021,0,0.0105591,"Missing"
W09-2307,W05-0908,0,0.0493934,"Missing"
W09-2307,N04-4026,0,0.091796,"r another. To achieve this, we train a discriminative phrase orientation classifier following the work by Zens and Ney (2006), and we use the grammatical relations between words as extra features to build the classifier. We then apply the phrase orientation classifier as a feature in a phrasebased MT system to help reordering. 2 Discriminative Reordering Model Basic reordering models in phrase-based systems use linear distance as the cost for phrase movements (Koehn et al., 2003). The disadvantage of these models is their insensitivity to the content of the words or phrases. More recent work (Tillman, 2004; Och et al., 2004; Koehn et al., 2007) has in52 troduced lexicalized reordering models which estimate reordering probabilities conditioned on the actual phrases. Lexicalized reordering models have brought significant gains over the baseline reordering models, but one concern is that data sparseness can make estimation less reliable. Zens and Ney (2006) proposed a discriminatively trained phrase orientation model and evaluated its performance as a classifier and when plugged into a phrase-based MT system. Their framework allows us to easily add in extra features. Therefore we use it as a testb"
W09-2307,D07-1077,0,0.0483917,"Missing"
W09-2307,W06-3108,0,0.371343,") (VP (ADVP (AD ี儳)) (VP (VV )ګݙ (NP (NP (ADJP (JJ ࡐ)) (NP (NN 凹䣈))) (NP (NN ދ凹))) (QP (CD ԫۍԲԼ䣐) (CLP (M ց))))) (PU Ζ))) The prevalence in Chinese of grammatical structures that translate into English in different word orders is an important cause of translation difficulty. While previous work has used phrase-structure parses to deal with such ordering problems, we introduce a richer set of Chinese grammatical relations that describes more semantically abstract relations between words. Using these Chinese grammatical relations, we improve a phrase orientation classifier (introduced by Zens and Ney (2006)) that decides the ordering of two phrases when translated into English by adding path features designed over the Chinese typed dependencies. We then apply the log probability of the phrase orientation classifier as an extra feature in a phrase-based MT system, and get significant BLEU point gains on three test sets: MT02 (+0.59), MT03 (+1.00) and MT05 (+0.77). Our Chinese grammatical relations are also likely to be useful for other NLP tasks. ګݙ ګݙ (complete) loc 䝢 (over; 䝢 in) lobj ڣڣ (year) nummod nsubj ৄؑ (city) ৄؑ det 㪤ࠄ 㪤ࠄ (these) advmod ี儳 ี儳 (collectively) dobj range ދ凹 ދ凹 (i"
W09-2307,N04-1021,0,\N,Missing
W09-2307,D08-1076,0,\N,Missing
W11-0116,W09-3208,0,0.0257341,"of events. Since an event’s duration is highly dependent on context, our algorithm models this aspectual property as a distribution over durations rather than a single mean duration. For example, a “war” typically lasts years, sometimes months, but almost never seconds, while “look” typically lasts seconds or minutes, but rarely years or decades. Our approach uses web queries to model an event’s typical distribution in the real world. Learning such rich aspectual properties of events is an important area for computational semantics, and should enrich applications like event coreference (e.g., Chen and Ji, 2009) in much the same way that gender has benefited nominal coreference systems. Event durations are also key to building event timelines and other deeper temporal understandings of a text (Verhagen et al., 2007; Pustejovsky and Verhagen, 2009). The contributions of this work are: • Demonstrating how to acquire event duration distributions by querying the web with patterns. • Showing that a system that predicts event durations based only on our web count distributions can outperform a supervised system that requires manually annotated training data. • Making available an event duration lexicon wit"
W11-0116,W04-3205,0,0.104792,"pired by the standard use of web patterns for the acquisition of relational lexical knowledge. Hearst (1998) first observed that a phrase like “. . . algae, such as Gelidium. . . ” indicates that “Gelidium” is a type of “algae”, and so hypernym-hyponym relations can be identified by querying a text collection with patterns like “such <noun> as <noun>” and “<noun> , including <noun>”. A wide variety of pattern-based work followed, including the application of the idea in VerbOcean to acquire aspects and temporal structure such as happens-before, using patterns like “to <verb> and then <verb>” (Chklovski and Pantel, 2004). More recent work has learned nominal gender and animacy by matching patterns like “<noun> * himself” and “<noun> and her” to a corpus of Web n-grams (Bergsma, 2005; Ji and Lin, 2009). Phrases like “John Joseph”, which were observed often with masculine pronouns and never with feminine or neuter pronouns, can thus have their gender identified as masculine. Ji and Lin found that such web-counts can predict person names as well as a fully supervised named entity recognition system. Our goal, then, is to integrate these two strands in the literature, applying pattern/web approaches to the task o"
W11-0116,D09-1120,0,0.00678247,"ration using a corpus collected through Amazon’s Mechanical Turk. We make available a new database of events and their duration distributions for use in research involving the temporal and aspectual properties of events. 1 Introduction Bridging the gap between lexical knowledge and world knowledge is crucial for achieving natural language understanding. For example, knowing whether a nominal is a person or organization and whether a person is male or female substantially improves coreference resolution, even when such knowledge is gathered through noisy unsupervised approaches (Bergsma, 2005; Haghighi and Klein, 2009). However, existing algorithms and resources for such semantic knowledge have focused primarily on static properties of nominals (e.g. gender or entity type), not dynamic properties of verbs and events. This paper shows how to learn one such property: the typical duration of events. Since an event’s duration is highly dependent on context, our algorithm models this aspectual property as a distribution over durations rather than a single mean duration. For example, a “war” typically lasts years, sometimes months, but almost never seconds, while “look” typically lasts seconds or minutes, but rar"
W11-0116,Y09-1024,0,0.0116617,"t “Gelidium” is a type of “algae”, and so hypernym-hyponym relations can be identified by querying a text collection with patterns like “such <noun> as <noun>” and “<noun> , including <noun>”. A wide variety of pattern-based work followed, including the application of the idea in VerbOcean to acquire aspects and temporal structure such as happens-before, using patterns like “to <verb> and then <verb>” (Chklovski and Pantel, 2004). More recent work has learned nominal gender and animacy by matching patterns like “<noun> * himself” and “<noun> and her” to a corpus of Web n-grams (Bergsma, 2005; Ji and Lin, 2009). Phrases like “John Joseph”, which were observed often with masculine pronouns and never with feminine or neuter pronouns, can thus have their gender identified as masculine. Ji and Lin found that such web-counts can predict person names as well as a fully supervised named entity recognition system. Our goal, then, is to integrate these two strands in the literature, applying pattern/web approaches to the task of estimating event durations. One difference from previous work is the distributional nature of the extracted knowledge. In the time domain, unlike in most previous relation-extraction"
W11-0116,P03-1054,0,0.00293524,"ion, maximum entropy and support vector machine classifiers, but as discussed in Section 8, the maximum entropy model performed best in cross-validations on the training data. 5 Unsupervised Approach While supervised learning is effective for many NLP tasks, it is sensitive to the amount of available training data. Unfortunately, the training data for event durations is very small, consisting of only 58 news articles (Pan et al., 2006), and labeling further data is quite expensive. This motivates our desire to find an 1 We parsed the documents into typed dependencies with the Stanford Parser (Klein and Manning, 2003). 147 approach that does not rely on labeled data, but instead utilizes the large amounts of text available on the Web to search for duration-specific patterns. This section describes our web-based approach to learning event durations. 5.1 Web Query Patterns Temporal properties of events are often described explicitly in language-specific constructions which can help us infer an event’s duration. Consider the following two sentences from our corpus: • Many spend hours surfing the Internet. • The answer is coming up in a few minutes. These sentences explicitly describe the duration of the event"
W11-0116,W10-0719,1,0.809058,"Missing"
W11-0116,P06-1050,0,0.150641,"g event distributions based on web counts. We then evaluate both of these models on an existing annotated corpus of event durations and make comparisons to durations we collected using Amazon’s Mechanical Turk. Finally, we present a generated database of event durations. 145 2 Previous Work Early work on extracting event properties focused on linguistic aspect, for example, automatically distinguishing culminated events that have an end point from non-culminated events that do not (Siegel and McKeown, 2000). The more fine-grained task of predicting the duration of events was first proposed by Pan et al. (2006), who annotated each event in a small section of the TimeBank (Pustejovsky et al., 2003) with duration lower and upper bounds. They then trained support vector machines on their annotated corpus for two prediction tasks: less-than-a-day vs. more-than-a-day, and bins like seconds, minutes, hours, etc. Their models used features like bags of words, heads of syntactic subjects and objects, and WordNet hypernyms of the events. This work provides a valuable resource in its annotated corpus and is also a good baseline. We replicate their work and also add new features as described below. Our approac"
W11-0116,W09-2418,0,0.0138796,"sometimes months, but almost never seconds, while “look” typically lasts seconds or minutes, but rarely years or decades. Our approach uses web queries to model an event’s typical distribution in the real world. Learning such rich aspectual properties of events is an important area for computational semantics, and should enrich applications like event coreference (e.g., Chen and Ji, 2009) in much the same way that gender has benefited nominal coreference systems. Event durations are also key to building event timelines and other deeper temporal understandings of a text (Verhagen et al., 2007; Pustejovsky and Verhagen, 2009). The contributions of this work are: • Demonstrating how to acquire event duration distributions by querying the web with patterns. • Showing that a system that predicts event durations based only on our web count distributions can outperform a supervised system that requires manually annotated training data. • Making available an event duration lexicon with duration distributions for common English events. We first review previous work and describe our re-implementation and augmentation of the latest supervised system for predicting event durations. Next, we present our approach to learning"
W11-0116,J00-4004,0,0.0166472,"entation of the latest supervised system for predicting event durations. Next, we present our approach to learning event distributions based on web counts. We then evaluate both of these models on an existing annotated corpus of event durations and make comparisons to durations we collected using Amazon’s Mechanical Turk. Finally, we present a generated database of event durations. 145 2 Previous Work Early work on extracting event properties focused on linguistic aspect, for example, automatically distinguishing culminated events that have an end point from non-culminated events that do not (Siegel and McKeown, 2000). The more fine-grained task of predicting the duration of events was first proposed by Pan et al. (2006), who annotated each event in a small section of the TimeBank (Pustejovsky et al., 2003) with duration lower and upper bounds. They then trained support vector machines on their annotated corpus for two prediction tasks: less-than-a-day vs. more-than-a-day, and bins like seconds, minutes, hours, etc. Their models used features like bags of words, heads of syntactic subjects and objects, and WordNet hypernyms of the events. This work provides a valuable resource in its annotated corpus and i"
W11-0116,D08-1027,0,0.0272319,"Missing"
W11-0116,S07-1014,0,0.011816,"typically lasts years, sometimes months, but almost never seconds, while “look” typically lasts seconds or minutes, but rarely years or decades. Our approach uses web queries to model an event’s typical distribution in the real world. Learning such rich aspectual properties of events is an important area for computational semantics, and should enrich applications like event coreference (e.g., Chen and Ji, 2009) in much the same way that gender has benefited nominal coreference systems. Event durations are also key to building event timelines and other deeper temporal understandings of a text (Verhagen et al., 2007; Pustejovsky and Verhagen, 2009). The contributions of this work are: • Demonstrating how to acquire event duration distributions by querying the web with patterns. • Showing that a system that predicts event durations based only on our web count distributions can outperform a supervised system that requires manually annotated training data. • Making available an event duration lexicon with duration distributions for common English events. We first review previous work and describe our re-implementation and augmentation of the latest supervised system for predicting event durations. Next, we"
W11-0116,P87-1001,0,\N,Missing
W11-1902,W97-1306,0,0.302132,"rmance despite the simplicity of the individual components. This strategy has been successfully used before for information extraction, e.g., in the BioNLP 2009 event extraction shared task (Kim et al., 2009), several of the top systems had a first high-recall component to identify event anchors, followed by high-precision classifiers, which identified event arguments and removed unlikely event candidates (Bj¨orne et al., 2009). In the coreference resolution space, several works have shown that applying a list of rules from highest to lowest precision is beneficial for coreference resolution (Baldwin, 1997; Raghunathan el al., 2010). However, we believe we are the first to show that this high-recall/high-precision strategy yields competitive results for the complete task of coreference resolution, i.e., including mention detection and both nominal and pronominal coreference. 2.1 Mention Detection Sieve In our particular setup, the recall of the mention detection component is more important than its precision, because any missed mentions are guaranteed to affect the final score, but spurious mentions may not impact the overall score if they are left as singletons, which are discarded by our post"
W11-1902,D08-1031,0,0.585246,"ld-out corpus (see Table 1 for the complete list of sieves deployed in our system). Since these two sieves use 3 We initialize the clusters as singletons and grow them progressively in each sieve. 30 Bare plurals - bare plurals are generic and cannot have a coreferent antecedent. 2.3.2 Semantic-Similarity Sieves We first extend the above system with two new sieves that exploit semantics from WordNet, Wikipedia infoboxes, and Freebase records, drawing on previous coreference work using these databases (Ng & Cardie, 2002; Daum´e & Marcu, 2005; Ponzetto & Strube, 2006; Ng, 2007; Yang & Su, 2007; Bengston & Roth, 2008; Huang et al., 2009; inter alia). Since the input to a sieve is a collection of mention clusters built by the previous (more precise) sieves, we need to link mention clusters (rather than individual mentions) to records in these three knowledge bases. The following steps generate a query for these resources from a mention cluster. First, we select the most representative mention in a cluster by preferring mentions headed by proper nouns to mentions headed by common nouns, and nominal mentions to pronominal ones. In case of ties, we select the longer string. For example, the mention selected f"
W11-1902,W09-1402,0,0.0144679,"Missing"
W11-1902,H05-1013,0,0.0128588,"Missing"
W11-1902,N10-1061,0,0.158402,"68.3 68.9 70.0 70.8 R 43.4 43.3 46.3 46.3 CEAFE P 47.8 46.8 50.5 49.6 F1 45.5 45.0 48.3 47.9 R 70.6 71.9 72.0 73.4 BLANC P 76.2 76.6 78.6 79.0 F1 73.0 74.0 74.8 75.8 avg F1 57.8 58.3 60.7 61.4 Table 4: Results on the official test set. as well, whereas in development (lines 6 and 7 in Table 3), gold mentions included only mentions part of an actual coreference chain. This explains the large difference between, say, line 6 in Table 3 and line 4 in Table 4. Our scores are comparable to previously reported state-of-the-art results for coreference resolution with predicted mentions. For example, Haghighi and Klein (2010) compare four state-of-the-art systems on three different corpora and report B3 scores between 63 and 77 points. While the corpora used in (Haghighi and Klein, 2010) are different from the one in this shared task, our result of 68 B3 suggests that our system’s performance is competitive. In this task, our submissions in both the open and the closed track obtained the highest scores. 4 Conclusion In this work we showed how a competitive end-toend coreference resolution system can be built using only deterministic models (or sieves). Our approach starts with a high-recall mention detection compo"
W11-1902,N06-2015,0,0.132313,"i Surdeanu, Dan Jurafsky Stanford NLP Group Stanford University, Stanford, CA 94305 {heeyoung,peirsman,angelx,natec,mihais,jurafsky}@stanford.edu Abstract standing, e.g., linking speakers to compatible pronouns. Second, we incorporated a mention detection sieve at the beginning of the processing flow. This sieve filters our syntactic constituents unlikely to be mentions using a simple set of rules on top of the syntactic analysis of text. And lastly, we added a post-processing step, which guarantees that the output of our system is compatible with the shared task and OntoNotes specifications (Hovy et al., 2006; Pradhan et al., 2007). This paper details the coreference resolution system submitted by Stanford at the CoNLL2011 shared task. Our system is a collection of deterministic coreference resolution models that incorporate lexical, syntactic, semantic, and discourse information. All these models use global document-level information by sharing mention attributes, such as gender and number, across mentions in the same cluster. We participated in both the open and closed tracks and submitted results using both predicted and gold mentions. Our system was ranked first in both tracks, with a score of"
W11-1902,D09-1128,0,0.0144125,"e 1 for the complete list of sieves deployed in our system). Since these two sieves use 3 We initialize the clusters as singletons and grow them progressively in each sieve. 30 Bare plurals - bare plurals are generic and cannot have a coreferent antecedent. 2.3.2 Semantic-Similarity Sieves We first extend the above system with two new sieves that exploit semantics from WordNet, Wikipedia infoboxes, and Freebase records, drawing on previous coreference work using these databases (Ng & Cardie, 2002; Daum´e & Marcu, 2005; Ponzetto & Strube, 2006; Ng, 2007; Yang & Su, 2007; Bengston & Roth, 2008; Huang et al., 2009; inter alia). Since the input to a sieve is a collection of mention clusters built by the previous (more precise) sieves, we need to link mention clusters (rather than individual mentions) to records in these three knowledge bases. The following steps generate a query for these resources from a mention cluster. First, we select the most representative mention in a cluster by preferring mentions headed by proper nouns to mentions headed by common nouns, and nominal mentions to pronominal ones. In case of ties, we select the longer string. For example, the mention selected from the cluster {Pre"
W11-1902,Y09-1024,0,0.0223833,"for the gold mention the working meeting of the 32 ”863 Program”. Due to this boundary mismatch, all mentions found to be coreferent with this predicted mention are counted as precision errors, and all mentions in the same coreference cluster with the gold mention are counted as recall errors. Table 3 lists the results of our end-to-end system on the development partition. “External Resources”, which were used only in the open track, includes: (a) a hand-built list of genders of first names that we created, incorporating frequent names from census lists and other sources, (b) an animacy list (Ji and Lin, 2009), (c) a country and state gazetteer, and (d) a demonym list. “Discourse” stands for the sieve introduced in Section 2.3.3. “Semantics” stands for the sieves presented in Section 2.3.2. The table shows that the discourse sieve yields an improvement of almost 2 points to the overall score (row 1 versus 3), and external resources contribute 0.5 points. On the other hand, the semantic sieves do not help (row 3 versus 4). The latter result contradicts our initial experiments, where we measured a minor improvement when these sieves were enabled and gold mentions were used. Our hypothesis is that, wh"
W11-1902,P07-1068,0,0.0061436,"on their precision in a held-out corpus (see Table 1 for the complete list of sieves deployed in our system). Since these two sieves use 3 We initialize the clusters as singletons and grow them progressively in each sieve. 30 Bare plurals - bare plurals are generic and cannot have a coreferent antecedent. 2.3.2 Semantic-Similarity Sieves We first extend the above system with two new sieves that exploit semantics from WordNet, Wikipedia infoboxes, and Freebase records, drawing on previous coreference work using these databases (Ng & Cardie, 2002; Daum´e & Marcu, 2005; Ponzetto & Strube, 2006; Ng, 2007; Yang & Su, 2007; Bengston & Roth, 2008; Huang et al., 2009; inter alia). Since the input to a sieve is a collection of mention clusters built by the previous (more precise) sieves, we need to link mention clusters (rather than individual mentions) to records in these three knowledge bases. The following steps generate a query for these resources from a mention cluster. First, we select the most representative mention in a cluster by preferring mentions headed by proper nouns to mentions headed by common nouns, and nominal mentions to pronominal ones. In case of ties, we select the longer str"
W11-1902,P02-1014,0,0.574048,"wo new sieves that address nominal mentions and are inserted based on their precision in a held-out corpus (see Table 1 for the complete list of sieves deployed in our system). Since these two sieves use 3 We initialize the clusters as singletons and grow them progressively in each sieve. 30 Bare plurals - bare plurals are generic and cannot have a coreferent antecedent. 2.3.2 Semantic-Similarity Sieves We first extend the above system with two new sieves that exploit semantics from WordNet, Wikipedia infoboxes, and Freebase records, drawing on previous coreference work using these databases (Ng & Cardie, 2002; Daum´e & Marcu, 2005; Ponzetto & Strube, 2006; Ng, 2007; Yang & Su, 2007; Bengston & Roth, 2008; Huang et al., 2009; inter alia). Since the input to a sieve is a collection of mention clusters built by the previous (more precise) sieves, we need to link mention clusters (rather than individual mentions) to records in these three knowledge bases. The following steps generate a query for these resources from a mention cluster. First, we select the most representative mention in a cluster by preferring mentions headed by proper nouns to mentions headed by common nouns, and nominal mentions to p"
W11-1902,W11-1901,0,0.492284,"c coreference resolution models that incorporate lexical, syntactic, semantic, and discourse information. All these models use global document-level information by sharing mention attributes, such as gender and number, across mentions in the same cluster. We participated in both the open and closed tracks and submitted results using both predicted and gold mentions. Our system was ranked first in both tracks, with a score of 57.8 in the closed track and 58.3 in the open track. 1 Introduction This paper describes the coreference resolution system used by Stanford at the CoNLL-2011 shared task (Pradhan et al., 2011). Our system extends the multi-pass sieve system of Raghunathan et al. (2010), which applies tiers of deterministic coreference models one at a time from highest to lowest precision. Each tier builds on the entity clusters constructed by previous models in the sieve, guaranteeing that stronger features are given precedence over weaker ones. Furthermore, this model propagates global information by sharing attributes (e.g., gender and number) across mentions in the same cluster. We made three considerable extensions to the Raghunathan et al. (2010) model. First, we added five additional sieves,"
W11-1902,D10-1048,1,0.816894,"ntic, and discourse information. All these models use global document-level information by sharing mention attributes, such as gender and number, across mentions in the same cluster. We participated in both the open and closed tracks and submitted results using both predicted and gold mentions. Our system was ranked first in both tracks, with a score of 57.8 in the closed track and 58.3 in the open track. 1 Introduction This paper describes the coreference resolution system used by Stanford at the CoNLL-2011 shared task (Pradhan et al., 2011). Our system extends the multi-pass sieve system of Raghunathan et al. (2010), which applies tiers of deterministic coreference models one at a time from highest to lowest precision. Each tier builds on the entity clusters constructed by previous models in the sieve, guaranteeing that stronger features are given precedence over weaker ones. Furthermore, this model propagates global information by sharing attributes (e.g., gender and number) across mentions in the same cluster. We made three considerable extensions to the Raghunathan et al. (2010) model. First, we added five additional sieves, the majority of which address the semantic similarity between mentions, e.g.,"
W11-1902,P07-1067,0,0.0116669,"precision in a held-out corpus (see Table 1 for the complete list of sieves deployed in our system). Since these two sieves use 3 We initialize the clusters as singletons and grow them progressively in each sieve. 30 Bare plurals - bare plurals are generic and cannot have a coreferent antecedent. 2.3.2 Semantic-Similarity Sieves We first extend the above system with two new sieves that exploit semantics from WordNet, Wikipedia infoboxes, and Freebase records, drawing on previous coreference work using these databases (Ng & Cardie, 2002; Daum´e & Marcu, 2005; Ponzetto & Strube, 2006; Ng, 2007; Yang & Su, 2007; Bengston & Roth, 2008; Huang et al., 2009; inter alia). Since the input to a sieve is a collection of mention clusters built by the previous (more precise) sieves, we need to link mention clusters (rather than individual mentions) to records in these three knowledge bases. The following steps generate a query for these resources from a mention cluster. First, we select the most representative mention in a cluster by preferring mentions headed by proper nouns to mentions headed by common nouns, and nominal mentions to pronominal ones. In case of ties, we select the longer string. For example,"
W11-1902,W09-1401,0,\N,Missing
W11-1902,N06-1025,0,\N,Missing
W12-2502,W09-4104,0,0.0509494,"mpirical research has been done to examine the textual features or mental processes that engender such a sensation. In this paper, we propose a computational framework for analyzing textual features that may be responsible for generating Computational aesthetics Previous research on the computational analysis of poetry focused on quantifying poetic devices such as rhyme and meter (Hayward, 1996; Greene et al., 2010; Genzel et al., 2010), tracking stylistic influence between authors (Forstall et al., 2011), or classifying poems based on the poet and style (Kaplan & Blei, 2007; He et al., 2007; Fang et al., 2009). These studies showed that computational methods can reveal interesting statistical properties in poetic language that allow us to better understand and categorize great works of literature (Fabb, 2006). However, there has been very little work using computational techniques to answer an important question in 8 Workshop on Computational Linguistics for Literature, pages 8–17, c Montr´eal, Canada, June 8, 2012. 2012 Association for Computational Linguistics both poetics and linguistics (Jakobson, 1960): what makes one poem more aesthetically appealing than another? One such attempt is the “aes"
W12-2502,D10-1016,0,0.0612699,"ogether by the delicate, tough skin of words. —Paul Engle (1908 -1991) Many people have experienced the astounding and transformational power of a beautiful poem. However, little empirical research has been done to examine the textual features or mental processes that engender such a sensation. In this paper, we propose a computational framework for analyzing textual features that may be responsible for generating Computational aesthetics Previous research on the computational analysis of poetry focused on quantifying poetic devices such as rhyme and meter (Hayward, 1996; Greene et al., 2010; Genzel et al., 2010), tracking stylistic influence between authors (Forstall et al., 2011), or classifying poems based on the poet and style (Kaplan & Blei, 2007; He et al., 2007; Fang et al., 2009). These studies showed that computational methods can reveal interesting statistical properties in poetic language that allow us to better understand and categorize great works of literature (Fabb, 2006). However, there has been very little work using computational techniques to answer an important question in 8 Workshop on Computational Linguistics for Literature, pages 8–17, c Montr´eal, Canada, June 8, 2012. 2012 As"
W12-2502,D10-1051,0,0.0564933,"with emotions, held together by the delicate, tough skin of words. —Paul Engle (1908 -1991) Many people have experienced the astounding and transformational power of a beautiful poem. However, little empirical research has been done to examine the textual features or mental processes that engender such a sensation. In this paper, we propose a computational framework for analyzing textual features that may be responsible for generating Computational aesthetics Previous research on the computational analysis of poetry focused on quantifying poetic devices such as rhyme and meter (Hayward, 1996; Greene et al., 2010; Genzel et al., 2010), tracking stylistic influence between authors (Forstall et al., 2011), or classifying poems based on the poet and style (Kaplan & Blei, 2007; He et al., 2007; Fang et al., 2009). These studies showed that computational methods can reveal interesting statistical properties in poetic language that allow us to better understand and categorize great works of literature (Fabb, 2006). However, there has been very little work using computational techniques to answer an important question in 8 Workshop on Computational Linguistics for Literature, pages 8–17, c Montr´eal, Canada,"
W12-2502,D11-1014,0,0.00390316,"ry as professional or amateur is a rather coarse measure of quality. In order to identify defining features of more fine-grained levels of poetic skill, future work could compare awardwinning poetry with poems written by less prestigious but also professionally trained poets. Experimenting with different databases and lexicons for affect and imagery could also be helpful, such as word-emotion associations (Mohammad & Turney, 2011) and imageability ratings (Coltheart, 1981). In addition, more sophisticated methods that consider sense ambiguities and meaning compositionality in affective words (Socher et al., 2011) should be applied to help enhance and improve upon our current analyses. While our approach reveals interesting patterns that shed light on elements of poetic sophistication, conclusions from the analysis need to be tested using controlled experiments. For example, does modifying a professional poem to include less concrete words make people perceive it as less beautiful? Investigating these questions using psychology experiments could help identify causal relationships between linguistic elements and sensations of poetic beauty. In summary, our framework provides a novel way to discover pote"
W12-2502,D08-1020,0,0.0178725,"ntemporary American English (COCA) (Davies, 2011). An average log word frequency was obtained for each poem by looking up each word in the poem in the word list and summing up the log word frequencies. The total log frequency was then divided by the number of words in the poem to obtain the average. Type-token ratio: Readability measures and automatic essay grading systems often use the ratio of total word types to total number of words in order to evaluate vocabulary sophistication, with higher type-token ratios indicating more diverse and sophisticated vocabulary (Ben-Simon & Bennett, 2007; Pitler & Nenkova, 2008). We predict that professional poets utilize a larger and more varied vocabulary and avoid using the same word several times throughout a poem. A type-token ratio score was calculated for each poem by counting all the separate instances of words and dividing that number by the total number of words in the poem. 3.2 Sound Device Poetry has a rich oral tradition that predates literacy, and traces of this aspect of poetic history can be found in sound devices such as rhyme, repetition, and meter. How a poem sounds is critical to how it is perceived, understood, and remembered. Indeed, most contem"
W12-2503,J08-1001,0,0.408255,"rlie the surface text, are mutually accessible and relevant.&quot; Cohesion considers the limited human capacity for storing the “surface materials” of a text long enough to relate them semantically during the act of reading. We therefore propose to study referential cohesion (Halliday and Hasan 1976), the relation between co-referring entities in a narrative, as an important component of cohesion. Referential cohesion has a significant literature in natural language processing (Grosz et al. 1995, Mani et al. 1998, Marcu 2000, Karamanis et al. 2004, Kibble and Power 2004, Elsner and Charniak 2008, Barzilay and Lapata 2008, inter alia) as does automatic coreference resolution, which has significantly increased in accuracy in recent years (Bengston and Roth 2008, Haghighi and Klein 2009, Haghighi and Klein 2010, Rahman and Ng 2011, Pradhan et al. 2011, Lee et al. 2011). We formulate and test two hypotheses in this position paper: First, we anticipate that given stylistic considerations and their fundamental narrative function, prose literary texts are inherently “more cohesive” than news. Second, in light of the aforementioned necessity for “dynamic equivalence” in the literary translation, we anticipate that cu"
W12-2503,P08-2011,0,0.0252761,"cations and Future Research We found in two separate analyses that literary texts had more dense reference chains than informative texts. This result supports our hypothesis that literary texts are indeed more cohesive in general than informative texts; that is to say, the stylistic and narrative demands of literature lead to prose being more cohesively “about” its subjects than news. It remains to replicate this experiment on a large, carefully sampled cross-genre corpus to confirm these preliminary findings, perhaps integrating a more 23 complex measure of cohesion as in Barzilay and Lapata (2008). We also found that MT systems had difficulty in conveying the cohesion in literary texts. Of course these results are preliminary and may be confounded by the nature of the training data used by modern MT systems. The uses of Google Translate as an MT system and longerform magazine articles as our informative texts were aimed at mitigating these concerns to some extent, but for now these results primarily serve as indicative of the need for further research in this area. Cohesion, as well, is only one of the seven “standards of textuality” put forth by Beaugrande and Dressler (1981) and take"
W12-2503,P10-1015,0,0.0264426,"y” put forth by Beaugrande and Dressler (1981) and taken up by Hatim and Mason (1997) in the translation context. Some of these have an existing literature addressing their computational identification and analysis (eg. Morris and Hirst 1991), in which cases we might apply existing methods to identify genre effects in literary text. For others, such as situationality, it remains to investigate appropriate computational analogues for large-scale automatic analysis and application to literary text. Studies addressing relevant textual-level concerns in literature show increasing promise, such as Elson et al. (2010)&apos;s work in automatically extracting social networks from fiction. Once these sorts of genre effects in literature are more clearly understood, they can be addressed on a large scale for comparisons between machine- and human-translated literary texts in the manner carried out in this paper, in order to identify further potential stumbling blocks for machine translation on the textual level as regards literary texts. Our preliminary work as presented here suggests, at the very least, the potential value and necessity of such analyses if we are to make progress towards a true literary machine tr"
W12-2503,J95-2003,0,0.212996,"[ing] the ways in which the components of the textual world, i.e., the configuration of concepts and relations which underlie the surface text, are mutually accessible and relevant.&quot; Cohesion considers the limited human capacity for storing the “surface materials” of a text long enough to relate them semantically during the act of reading. We therefore propose to study referential cohesion (Halliday and Hasan 1976), the relation between co-referring entities in a narrative, as an important component of cohesion. Referential cohesion has a significant literature in natural language processing (Grosz et al. 1995, Mani et al. 1998, Marcu 2000, Karamanis et al. 2004, Kibble and Power 2004, Elsner and Charniak 2008, Barzilay and Lapata 2008, inter alia) as does automatic coreference resolution, which has significantly increased in accuracy in recent years (Bengston and Roth 2008, Haghighi and Klein 2009, Haghighi and Klein 2010, Rahman and Ng 2011, Pradhan et al. 2011, Lee et al. 2011). We formulate and test two hypotheses in this position paper: First, we anticipate that given stylistic considerations and their fundamental narrative function, prose literary texts are inherently “more cohesive” than new"
W12-2503,D09-1120,0,0.0149941,"o relate them semantically during the act of reading. We therefore propose to study referential cohesion (Halliday and Hasan 1976), the relation between co-referring entities in a narrative, as an important component of cohesion. Referential cohesion has a significant literature in natural language processing (Grosz et al. 1995, Mani et al. 1998, Marcu 2000, Karamanis et al. 2004, Kibble and Power 2004, Elsner and Charniak 2008, Barzilay and Lapata 2008, inter alia) as does automatic coreference resolution, which has significantly increased in accuracy in recent years (Bengston and Roth 2008, Haghighi and Klein 2009, Haghighi and Klein 2010, Rahman and Ng 2011, Pradhan et al. 2011, Lee et al. 2011). We formulate and test two hypotheses in this position paper: First, we anticipate that given stylistic considerations and their fundamental narrative function, prose literary texts are inherently “more cohesive” than news. Second, in light of the aforementioned necessity for “dynamic equivalence” in the literary translation, we anticipate that current machine translation systems, built with newswire texts in mind, will be less successful at conveying cohesion for literary texts than for news. 2. Investigating"
W12-2503,N10-1061,0,0.0128452,"y during the act of reading. We therefore propose to study referential cohesion (Halliday and Hasan 1976), the relation between co-referring entities in a narrative, as an important component of cohesion. Referential cohesion has a significant literature in natural language processing (Grosz et al. 1995, Mani et al. 1998, Marcu 2000, Karamanis et al. 2004, Kibble and Power 2004, Elsner and Charniak 2008, Barzilay and Lapata 2008, inter alia) as does automatic coreference resolution, which has significantly increased in accuracy in recent years (Bengston and Roth 2008, Haghighi and Klein 2009, Haghighi and Klein 2010, Rahman and Ng 2011, Pradhan et al. 2011, Lee et al. 2011). We formulate and test two hypotheses in this position paper: First, we anticipate that given stylistic considerations and their fundamental narrative function, prose literary texts are inherently “more cohesive” than news. Second, in light of the aforementioned necessity for “dynamic equivalence” in the literary translation, we anticipate that current machine translation systems, built with newswire texts in mind, will be less successful at conveying cohesion for literary texts than for news. 2. Investigating Literary Cohesion Our fi"
W12-2503,J04-4001,0,0.032987,"onfiguration of concepts and relations which underlie the surface text, are mutually accessible and relevant.&quot; Cohesion considers the limited human capacity for storing the “surface materials” of a text long enough to relate them semantically during the act of reading. We therefore propose to study referential cohesion (Halliday and Hasan 1976), the relation between co-referring entities in a narrative, as an important component of cohesion. Referential cohesion has a significant literature in natural language processing (Grosz et al. 1995, Mani et al. 1998, Marcu 2000, Karamanis et al. 2004, Kibble and Power 2004, Elsner and Charniak 2008, Barzilay and Lapata 2008, inter alia) as does automatic coreference resolution, which has significantly increased in accuracy in recent years (Bengston and Roth 2008, Haghighi and Klein 2009, Haghighi and Klein 2010, Rahman and Ng 2011, Pradhan et al. 2011, Lee et al. 2011). We formulate and test two hypotheses in this position paper: First, we anticipate that given stylistic considerations and their fundamental narrative function, prose literary texts are inherently “more cohesive” than news. Second, in light of the aforementioned necessity for “dynamic equivalence"
W12-2503,D10-1086,0,0.0418024,"Missing"
W12-2503,J91-1002,0,0.585747,"re of the training data used by modern MT systems. The uses of Google Translate as an MT system and longerform magazine articles as our informative texts were aimed at mitigating these concerns to some extent, but for now these results primarily serve as indicative of the need for further research in this area. Cohesion, as well, is only one of the seven “standards of textuality” put forth by Beaugrande and Dressler (1981) and taken up by Hatim and Mason (1997) in the translation context. Some of these have an existing literature addressing their computational identification and analysis (eg. Morris and Hirst 1991), in which cases we might apply existing methods to identify genre effects in literary text. For others, such as situationality, it remains to investigate appropriate computational analogues for large-scale automatic analysis and application to literary text. Studies addressing relevant textual-level concerns in literature show increasing promise, such as Elson et al. (2010)&apos;s work in automatically extracting social networks from fiction. Once these sorts of genre effects in literature are more clearly understood, they can be addressed on a large scale for comparisons between machine- and huma"
W12-2503,J04-3003,0,0.0641343,"Missing"
W12-2503,P11-1082,0,0.0131046,"ng. We therefore propose to study referential cohesion (Halliday and Hasan 1976), the relation between co-referring entities in a narrative, as an important component of cohesion. Referential cohesion has a significant literature in natural language processing (Grosz et al. 1995, Mani et al. 1998, Marcu 2000, Karamanis et al. 2004, Kibble and Power 2004, Elsner and Charniak 2008, Barzilay and Lapata 2008, inter alia) as does automatic coreference resolution, which has significantly increased in accuracy in recent years (Bengston and Roth 2008, Haghighi and Klein 2009, Haghighi and Klein 2010, Rahman and Ng 2011, Pradhan et al. 2011, Lee et al. 2011). We formulate and test two hypotheses in this position paper: First, we anticipate that given stylistic considerations and their fundamental narrative function, prose literary texts are inherently “more cohesive” than news. Second, in light of the aforementioned necessity for “dynamic equivalence” in the literary translation, we anticipate that current machine translation systems, built with newswire texts in mind, will be less successful at conveying cohesion for literary texts than for news. 2. Investigating Literary Cohesion Our first preliminary expe"
W12-2503,J85-1001,0,0.257793,"Missing"
W12-2503,D07-1057,0,0.0217558,"Missing"
W12-2503,D08-1031,0,\N,Missing
W12-2503,P04-1050,0,\N,Missing
W12-2503,W11-1901,0,\N,Missing
W12-2503,W11-1902,1,\N,Missing
W12-3202,bird-etal-2008-acl,0,0.423443,"Missing"
W12-3202,H94-1010,0,0.0266631,"rticipation in a required evaluation (bakeoff) task of filling slots in templates about events, and began (after an exploratory MUC-1 in 1987) with MUC-2 in 1989, followed by MUC-3 (1991), MUC-4 (1992), MUC-5 (1993) and MUC6 (1995) (Grishman and Sundheim, 1996). The Air Travel Information System (ATIS) was a task for measuring progress in spoken language under16 standing, sponsored by DARPA (Hemphill et al., 1990; Price, 1990). Subjects talked with a system to answer questions about flight schedules and airline fares from a database; there were evaluations in 1990, 1991, 1992, 1993, and 1994 (Dahl et al., 1994). The ATIS systems were described in papers at the DARPA Speech and Natural Language Workshops, a series of DARPA-sponsored workshsop held from 1989–1994 to which DARPA grantees were strongly encouraged to participate, with the goal of bringing together the speech and natural language processing communities. After the MUC and ATIS bakeoffs and the DARPA workshops ended, the field largely stopped publishing in the bakeoff topics and transitioned to other topics; participation by researchers in speech recognition also dropped off significantly. From 2002 onward, the field settled into the modern"
W12-3202,C96-1079,0,0.166245,"nds to a number of important US government initiatives: MUC, ATIS, and the DARPA workshops. The Message Understanding Conferences (MUC) were an early initiative in information extraction, set up by the United States Naval Oceans Systems Center with the support of DARPA, the Defense Advanced Research Projects Agency. A condition of attending the MUC workshops was participation in a required evaluation (bakeoff) task of filling slots in templates about events, and began (after an exploratory MUC-1 in 1987) with MUC-2 in 1989, followed by MUC-3 (1991), MUC-4 (1992), MUC-5 (1993) and MUC6 (1995) (Grishman and Sundheim, 1996). The Air Travel Information System (ATIS) was a task for measuring progress in spoken language under16 standing, sponsored by DARPA (Hemphill et al., 1990; Price, 1990). Subjects talked with a system to answer questions about flight schedules and airline fares from a database; there were evaluations in 1990, 1991, 1992, 1993, and 1994 (Dahl et al., 1994). The ATIS systems were described in papers at the DARPA Speech and Natural Language Workshops, a series of DARPA-sponsored workshsop held from 1989–1994 to which DARPA grantees were strongly encouraged to participate, with the goal of bringin"
W12-3202,D08-1038,0,0.126842,"apers has made it possible to develop a computational history of science. Methods from natural language processing and other areas of computer science can be naturally applied to study the ways a field and its ideas develop and expand (Au Yeung and Jatowt, 2011; Gerrish and Blei, 2010; Tu et al., 2010; Aris et al., 2009). One particular direction in computational history has been the use of topic models (Blei et al., 2003) to analyze the rise and fall of research topics to study the progress of science, both in general (Griffiths and Steyvers, 2004) and more specifically in the ACL Anthology (Hall et al., 2008). We extend this work with a more people-centered view of computational history. In this framework, we examine the trajectories of individual authors across research topics in the field of computational linguistics. By examining a single author’s paper topics over time, we can trace the evolution of her academic efforts; by superimposing these individual traces over each other, we can learn how the entire field progressed over time. One goal is to investigate the use of these techniques for computational history in general. A second goal is to use the ACL Anthology Network Corpus (Radev et al."
W12-3202,H90-1021,0,0.0724601,"n information extraction, set up by the United States Naval Oceans Systems Center with the support of DARPA, the Defense Advanced Research Projects Agency. A condition of attending the MUC workshops was participation in a required evaluation (bakeoff) task of filling slots in templates about events, and began (after an exploratory MUC-1 in 1987) with MUC-2 in 1989, followed by MUC-3 (1991), MUC-4 (1992), MUC-5 (1993) and MUC6 (1995) (Grishman and Sundheim, 1996). The Air Travel Information System (ATIS) was a task for measuring progress in spoken language under16 standing, sponsored by DARPA (Hemphill et al., 1990; Price, 1990). Subjects talked with a system to answer questions about flight schedules and airline fares from a database; there were evaluations in 1990, 1991, 1992, 1993, and 1994 (Dahl et al., 1994). The ATIS systems were described in papers at the DARPA Speech and Natural Language Workshops, a series of DARPA-sponsored workshsop held from 1989–1994 to which DARPA grantees were strongly encouraged to participate, with the goal of bringing together the speech and natural language processing communities. After the MUC and ATIS bakeoffs and the DARPA workshops ended, the field largely stopped"
W12-3202,H90-1020,0,0.0849225,"n, set up by the United States Naval Oceans Systems Center with the support of DARPA, the Defense Advanced Research Projects Agency. A condition of attending the MUC workshops was participation in a required evaluation (bakeoff) task of filling slots in templates about events, and began (after an exploratory MUC-1 in 1987) with MUC-2 in 1989, followed by MUC-3 (1991), MUC-4 (1992), MUC-5 (1993) and MUC6 (1995) (Grishman and Sundheim, 1996). The Air Travel Information System (ATIS) was a task for measuring progress in spoken language under16 standing, sponsored by DARPA (Hemphill et al., 1990; Price, 1990). Subjects talked with a system to answer questions about flight schedules and airline fares from a database; there were evaluations in 1990, 1991, 1992, 1993, and 1994 (Dahl et al., 1994). The ATIS systems were described in papers at the DARPA Speech and Natural Language Workshops, a series of DARPA-sponsored workshsop held from 1989–1994 to which DARPA grantees were strongly encouraged to participate, with the goal of bringing together the speech and natural language processing communities. After the MUC and ATIS bakeoffs and the DARPA workshops ended, the field largely stopped publishing in"
W12-3202,W09-3607,0,0.0852082,"Missing"
W12-3202,C10-2145,0,0.0606128,"searchers to the field, and bridged early topics to modern probabilistic approaches. Last, we identify steep increases in author retention during the bakeoff era and the modern era, suggesting two points at which the field became more integrated. 1 Introduction The rise of vast on-line collections of scholarly papers has made it possible to develop a computational history of science. Methods from natural language processing and other areas of computer science can be naturally applied to study the ways a field and its ideas develop and expand (Au Yeung and Jatowt, 2011; Gerrish and Blei, 2010; Tu et al., 2010; Aris et al., 2009). One particular direction in computational history has been the use of topic models (Blei et al., 2003) to analyze the rise and fall of research topics to study the progress of science, both in general (Griffiths and Steyvers, 2004) and more specifically in the ACL Anthology (Hall et al., 2008). We extend this work with a more people-centered view of computational history. In this framework, we examine the trajectories of individual authors across research topics in the field of computational linguistics. By examining a single author’s paper topics over time, we can trace"
W12-3204,W12-3202,1,0.865988,"Missing"
W12-3204,bird-etal-2008-acl,0,0.337763,"Missing"
W12-3204,D08-1038,0,0.136566,"Missing"
W12-3204,W09-3607,0,0.13574,"Missing"
W13-1403,W09-4104,0,0.244451,"expect its total absence? Does more contemporary poetry still draw connections to classical language? 17 Proceedings of the Second Workshop on Computational Linguistics for Literature, pages 17–22, c Atlanta, Georgia, June 14, 2013. 2013 Association for Computational Linguistics 2 Prior Work on Chinese Poetry in NLP The majority of existing studies in NLP on Chinese poetry deal exclusively with the classical language. Jiang and Zhou (2008) explore the problem of classical Chinese poetic couplets, and to develop a system to generate them automatically using techniques from machine translation. Fang et al. (2009) use an ontology of imagery developed by Lo (2008) to identify imagery in classical Chinese poems, and develop a parser that is able to extract tree structures that identify complex imagistic language in the same. More recent work develops useful resources for understanding classical poetry. Lee (2012) develops a corpus of classical Chinese poems that are wordsegmented and annotated with nested part-of-speech tags that allow for different interpretations of “wordhood” - a non-trivial concept in considering Chinese texts classical and modern. Lee and Kong (2012) introduce a large-scale dependen"
W13-1403,C08-1048,0,0.062435,"s and classical language immediately discarded with the advent of vernacular poetry? What is the status of classical language after 1949 and amidst the Maoist era, when we might expect its total absence? Does more contemporary poetry still draw connections to classical language? 17 Proceedings of the Second Workshop on Computational Linguistics for Literature, pages 17–22, c Atlanta, Georgia, June 14, 2013. 2013 Association for Computational Linguistics 2 Prior Work on Chinese Poetry in NLP The majority of existing studies in NLP on Chinese poetry deal exclusively with the classical language. Jiang and Zhou (2008) explore the problem of classical Chinese poetic couplets, and to develop a system to generate them automatically using techniques from machine translation. Fang et al. (2009) use an ontology of imagery developed by Lo (2008) to identify imagery in classical Chinese poems, and develop a parser that is able to extract tree structures that identify complex imagistic language in the same. More recent work develops useful resources for understanding classical poetry. Lee (2012) develops a corpus of classical Chinese poems that are wordsegmented and annotated with nested part-of-speech tags that al"
W13-1403,W12-1011,0,0.100581,"in NLP The majority of existing studies in NLP on Chinese poetry deal exclusively with the classical language. Jiang and Zhou (2008) explore the problem of classical Chinese poetic couplets, and to develop a system to generate them automatically using techniques from machine translation. Fang et al. (2009) use an ontology of imagery developed by Lo (2008) to identify imagery in classical Chinese poems, and develop a parser that is able to extract tree structures that identify complex imagistic language in the same. More recent work develops useful resources for understanding classical poetry. Lee (2012) develops a corpus of classical Chinese poems that are wordsegmented and annotated with nested part-of-speech tags that allow for different interpretations of “wordhood” - a non-trivial concept in considering Chinese texts classical and modern. Lee and Kong (2012) introduce a large-scale dependency treebank annotated on a corpus of 8th-century poems. To our knowledge, there is no existing computational work that attempts to understand the development of modern Chinese poetry over time. 3 Data Collection For this project, we use a corpus of modern poems collected on the site “Chinese Poetry Tre"
W13-1403,N12-1020,0,0.175322,"hniques from machine translation. Fang et al. (2009) use an ontology of imagery developed by Lo (2008) to identify imagery in classical Chinese poems, and develop a parser that is able to extract tree structures that identify complex imagistic language in the same. More recent work develops useful resources for understanding classical poetry. Lee (2012) develops a corpus of classical Chinese poems that are wordsegmented and annotated with nested part-of-speech tags that allow for different interpretations of “wordhood” - a non-trivial concept in considering Chinese texts classical and modern. Lee and Kong (2012) introduce a large-scale dependency treebank annotated on a corpus of 8th-century poems. To our knowledge, there is no existing computational work that attempts to understand the development of modern Chinese poetry over time. 3 Data Collection For this project, we use a corpus of modern poems collected on the site “Chinese Poetry Treasury” (中国诗歌库, www.shigeku.com) entitled the “Selected Database of Chinese Modern Poetry” (中国 现代诗歌精品资料库). It is important to note that the poems in this collection were hand-selected by the group running the site for their canonicity, so our data are biased toward"
W13-2239,P96-1041,0,0.0262624,"16.3 16.1 16.2 15.9 16.3 16.4 6 15.9 16.1 16.4 7 15.4 16.2 16.3 8 16.1 16.2 16.4 9 15.9 16.4 16.5 10 16.2 16.1 16.3 Table 3: BLEU scores on BOLT dev12 dev achieved by the individual PDT systems tuned on GALE dev10 web tune. Scores report individual system performance before system combination. tics were used to extract a phrase-table over word alignments symmetrized using grow-diag (Koehn et al., 2003). We made use of a hierarchical reordering model (Galley and Manning, 2008) as well as a 5-gram language model trained on the target side of the bi-text and smoothed using modified Kneeser-Ney (Chen and Goodman, 1996). Individual PDT systems were tuned on the GALE dev10 web tune set using online-PRO (Green et al., 2013; Hopkins and May, 2011) to the Positive Diversity Tuning criterion.4 The Multi-Engine Machine Translation (MEMT) package was used for system combination (Heafield and Lavie, 2010a). We used BOLT dev12 dev as a development test set to explore different α parameterizations of the Positive Diversity criteria. 7 equation (3), we measure the diversity of translations produced by an individual system as the negative BLEU score of the translations with respect to the translations from systems built"
W13-2239,N12-1047,0,0.0167,") (Och, 2003), attempts to maximize the correctness objective directly. Popular alternatives such as pairwise ranking objective (PRO) (Hopkins and May, 2011), MIRA (Chiang et al., 2008), and RAMPION (Gimpel and Smith, 2012) use surrogate optimization objectives that indirectly attempt to maximize the correctness function by using it to select targets for training discriminative classification models. In practice, either optimizing correctness directly or optimizing a surrogate objective that uses correctness to choose optimization targets results in roughly equivalent translation performance (Cherry and Foster, 2012). Even when individual systems are being built to be used in a larger combined system, they are still usually tuned to maximize their isolated individual system performance rather than to maxi1 The exception being Xiao et al. (2013)’s work using boosting for error-driven reweighting of the tuning set 2 Other system combination techniques exist such as candidate selection systems, whereby the combination model attempts to find the best single candidate produced by one of the translation engines (Paul et al., 2005; Nomoto, 2004; Zwarts and Dras, 2008), decoder chaining (Aikawa and Ruopp, 2009),"
W13-2239,D08-1024,0,0.0206523,"orrectness measure that systems are typically tuned toward is BLEU (Papineni et al., 2002), which measures the fraction of the n-grams that are both present in the reference translations and the translations produced by a system. The BLEU score is computed as the geometric mean of the resulting n-gram precisions scaled by a brevity penalty. The most widely used machine translation tuning algorithm, minimum error rate training (MERT) (Och, 2003), attempts to maximize the correctness objective directly. Popular alternatives such as pairwise ranking objective (PRO) (Hopkins and May, 2011), MIRA (Chiang et al., 2008), and RAMPION (Gimpel and Smith, 2012) use surrogate optimization objectives that indirectly attempt to maximize the correctness function by using it to select targets for training discriminative classification models. In practice, either optimizing correctness directly or optimizing a surrogate objective that uses correctness to choose optimization targets results in roughly equivalent translation performance (Cherry and Foster, 2012). Even when individual systems are being built to be used in a larger combined system, they are still usually tuned to maximize their isolated individual system"
W13-2239,N10-1141,0,0.110527,"uires the construction of multiple systems that are simultaneously diverse and well-performing. If the systems are not distinct enough, they will bring very little value during system combination. However, if some of the systems produce diverse translations but achieve lower overall translation quality, their contributions risk being ignored during system combination. Prior work has approached the need for diverse systems by using different system architectures, model components, system build parameters, decoder hyperparameters, as well as data selection and weighting (Macherey and Och, 2007; DeNero et al., 2010; Xiao et al., 2013). However, during tuning, each individual system is still just trained to maximize its own isolated performance on a tune set, or at best an error-driven reweighting of the tune set, without explicitly taking into account the diversity of the resulting translations. Such tuning does not encourage systems to rigorously explore model variations that achieve both good translation quality and diversity with respect to the other systems. It is reasonable to suspect that this results in individual systems that under exploit the amount of diversity possible, given the characterist"
W13-2239,W11-2107,0,0.0279539,"erates by aligning the translations produced by two or more individual translation systems and then using the alignments to construct a search space that allows new translations to be pieced together by picking and choosing parts of the material from the original translations (Bangalore et al., 2001; Matusov et al., 2006; Rosti et al., 2007a; Rosti et al., 2007b; Karakos et al., 2008; Heafield and Lavie, 2010a).2 The alignment of the individual system translations can be performed using alignment driven evaluation metrics such as invWER, TERp, METEOR (Leusch et al., 2003; Snover et al., 2009; Denkowski and Lavie, 2011). The piecewise selection of material from the original translations is performed using the combination model’s scoring features such as n-gram language models, confidence models over the individual systems, and consensus features that score a combined translation using n-grams matches to the individual system translations (Rosti et al., 2007b; Zhao and He, 2009; Heafield and Lavie, 2010b). Both system confidence model features and ngram consensus features score contributions based in part on how confident the system combination model is in each individual machine translation system. This mean"
W13-2239,W00-0405,0,0.0118247,"tion with other systems. They constructed chains of systems whereby the output of one decoder is feed as input to the next decoder in the pipeline. The downstream systems are built and tuned to correct errors produced by the preceding system. In this approach, the downstream decoder acts as a machine learning based post editing system. Related Work While the idea of encouraging diversity in individual systems that will be used for system combination has been proven effective in speech recognition and document summarization (Hinton, 2002; Breslin and Gales, 2007; Carbonell and Goldstein, 1998; Goldstein et al., 2000), there has only been a modest amount of prior work exploring such approaches for machine translation. Prior work within machine translation has investigated adapting machine learning techniques for building ensembles of classifiers to translation system tuning, encouraging diversity by varying both the hyperparameters and the data used to build the individual systems, and chaining together individual translation systems. Xiao et al. (2013) explores using boosting to train an ensemble of machine translation systems. Following the standard Adaboost algorithm, each system was trained in sequence"
W13-2239,P13-1031,1,0.839248,"6.1 16.3 Table 3: BLEU scores on BOLT dev12 dev achieved by the individual PDT systems tuned on GALE dev10 web tune. Scores report individual system performance before system combination. tics were used to extract a phrase-table over word alignments symmetrized using grow-diag (Koehn et al., 2003). We made use of a hierarchical reordering model (Galley and Manning, 2008) as well as a 5-gram language model trained on the target side of the bi-text and smoothed using modified Kneeser-Ney (Chen and Goodman, 1996). Individual PDT systems were tuned on the GALE dev10 web tune set using online-PRO (Green et al., 2013; Hopkins and May, 2011) to the Positive Diversity Tuning criterion.4 The Multi-Engine Machine Translation (MEMT) package was used for system combination (Heafield and Lavie, 2010a). We used BOLT dev12 dev as a development test set to explore different α parameterizations of the Positive Diversity criteria. 7 equation (3), we measure the diversity of translations produced by an individual system as the negative BLEU score of the translations with respect to the translations from systems built during prior iterations. For clarity of presentation, these diversity scores are reported as 1.0−BLEU."
W13-2239,2009.mtsummit-posters.1,0,0.1431,"(Cherry and Foster, 2012). Even when individual systems are being built to be used in a larger combined system, they are still usually tuned to maximize their isolated individual system performance rather than to maxi1 The exception being Xiao et al. (2013)’s work using boosting for error-driven reweighting of the tuning set 2 Other system combination techniques exist such as candidate selection systems, whereby the combination model attempts to find the best single candidate produced by one of the translation engines (Paul et al., 2005; Nomoto, 2004; Zwarts and Dras, 2008), decoder chaining (Aikawa and Ruopp, 2009), re-decoding informed by the decoding paths taken by other systems (Huang and Papineni, 2007), and decoding model combination (DeNero et al., 2010). 321 Input : systems [], tune(), source, refs [], α, EvalMetric (), SimMetric () Output: models [] // start with an empty set of translations from prior iterations other_sys [] ← [] for i ← 1 to len(systems []) do // new Positive Diversity measure using prior translations PDα,i () ← new PD(α, EvalMetric(), SimMetric(), refs [], other_sys []) // tune a new model to fit PDα,i // e.g., using MERT, PRO, MIRA, RAMPION, etc. models [i] ← tune(systems [i"
W13-2239,W10-1744,0,0.0728713,"are different from those produced by other systems. 2 Similar to speech recognition’s Recognizer Output Voting Error Reduction (ROVER) algorithm (Fiscus, 1997), machine translation system combination typically operates by aligning the translations produced by two or more individual translation systems and then using the alignments to construct a search space that allows new translations to be pieced together by picking and choosing parts of the material from the original translations (Bangalore et al., 2001; Matusov et al., 2006; Rosti et al., 2007a; Rosti et al., 2007b; Karakos et al., 2008; Heafield and Lavie, 2010a).2 The alignment of the individual system translations can be performed using alignment driven evaluation metrics such as invWER, TERp, METEOR (Leusch et al., 2003; Snover et al., 2009; Denkowski and Lavie, 2011). The piecewise selection of material from the original translations is performed using the combination model’s scoring features such as n-gram language models, confidence models over the individual systems, and consensus features that score a combined translation using n-grams matches to the individual system translations (Rosti et al., 2007b; Zhao and He, 2009; Heafield and Lavie,"
W13-2239,2010.amta-papers.34,0,0.519175,"are different from those produced by other systems. 2 Similar to speech recognition’s Recognizer Output Voting Error Reduction (ROVER) algorithm (Fiscus, 1997), machine translation system combination typically operates by aligning the translations produced by two or more individual translation systems and then using the alignments to construct a search space that allows new translations to be pieced together by picking and choosing parts of the material from the original translations (Bangalore et al., 2001; Matusov et al., 2006; Rosti et al., 2007a; Rosti et al., 2007b; Karakos et al., 2008; Heafield and Lavie, 2010a).2 The alignment of the individual system translations can be performed using alignment driven evaluation metrics such as invWER, TERp, METEOR (Leusch et al., 2003; Snover et al., 2009; Denkowski and Lavie, 2011). The piecewise selection of material from the original translations is performed using the combination model’s scoring features such as n-gram language models, confidence models over the individual systems, and consensus features that score a combined translation using n-grams matches to the individual system translations (Rosti et al., 2007b; Zhao and He, 2009; Heafield and Lavie,"
W13-2239,D11-1125,0,0.174735,"ness(ref[], sysΘ ) (1) Θ The correctness measure that systems are typically tuned toward is BLEU (Papineni et al., 2002), which measures the fraction of the n-grams that are both present in the reference translations and the translations produced by a system. The BLEU score is computed as the geometric mean of the resulting n-gram precisions scaled by a brevity penalty. The most widely used machine translation tuning algorithm, minimum error rate training (MERT) (Och, 2003), attempts to maximize the correctness objective directly. Popular alternatives such as pairwise ranking objective (PRO) (Hopkins and May, 2011), MIRA (Chiang et al., 2008), and RAMPION (Gimpel and Smith, 2012) use surrogate optimization objectives that indirectly attempt to maximize the correctness function by using it to select targets for training discriminative classification models. In practice, either optimizing correctness directly or optimizing a surrogate objective that uses correctness to choose optimization targets results in roughly equivalent translation performance (Cherry and Foster, 2012). Even when individual systems are being built to be used in a larger combined system, they are still usually tuned to maximize their"
W13-2239,N10-2003,1,0.839823,"n be a heterogeneous collection of substantially different systems (e.g., phrase-based, hierarchical, syntactic, or tunable hybrid systems) or even multiple copies of a single machine translation system. In all cases, systems later in the list will be trained to produce translations that both fit the references and are encouraged to be distinct from the systems earlier in the list. During each iteration, the system constructs a 6 Experiments Experiments are performed using a single phrase-based Chinese-to-English translation system, built with the Stanford Phrasal machine translation toolkit (Cer et al., 2010). The system was built using all of the parallel data available for Phase 2 of the DARPA BOLT program. The Chinese data was segmented to the Chinese TreeBank (CTB) standard using a maximum match word segmenter, trained on the output of a CRF segmenter (Xiang et al., 2013). The bitext was word aligned using the Berkeley aligner (Liang et al., 2006). Standard phrase-pair extraction heuris323 PDT System α = 0.95 α = 0.97 α = 0.99 BLEU scores from individual systems tuned during iteration i of PDT 0 1 2 3 4 5 16.2 16.0 15.7 15.9 16.1 16.1 16.4 15.8 15.8 15.9 16.0 16.2 16.3 16.1 16.2 15.9 16.3 16.4"
W13-2239,D07-1029,0,0.019307,"er combined system, they are still usually tuned to maximize their isolated individual system performance rather than to maxi1 The exception being Xiao et al. (2013)’s work using boosting for error-driven reweighting of the tuning set 2 Other system combination techniques exist such as candidate selection systems, whereby the combination model attempts to find the best single candidate produced by one of the translation engines (Paul et al., 2005; Nomoto, 2004; Zwarts and Dras, 2008), decoder chaining (Aikawa and Ruopp, 2009), re-decoding informed by the decoding paths taken by other systems (Huang and Papineni, 2007), and decoding model combination (DeNero et al., 2010). 321 Input : systems [], tune(), source, refs [], α, EvalMetric (), SimMetric () Output: models [] // start with an empty set of translations from prior iterations other_sys [] ← [] for i ← 1 to len(systems []) do // new Positive Diversity measure using prior translations PDα,i () ← new PD(α, EvalMetric(), SimMetric(), refs [], other_sys []) // tune a new model to fit PDα,i // e.g., using MERT, PRO, MIRA, RAMPION, etc. models [i] ← tune(systems [i], source, PDα,i ()) // Save translations from tuned modeli for use during // the diversity co"
W13-2239,C08-1145,0,0.0181633,"oughly equivalent translation performance (Cherry and Foster, 2012). Even when individual systems are being built to be used in a larger combined system, they are still usually tuned to maximize their isolated individual system performance rather than to maxi1 The exception being Xiao et al. (2013)’s work using boosting for error-driven reweighting of the tuning set 2 Other system combination techniques exist such as candidate selection systems, whereby the combination model attempts to find the best single candidate produced by one of the translation engines (Paul et al., 2005; Nomoto, 2004; Zwarts and Dras, 2008), decoder chaining (Aikawa and Ruopp, 2009), re-decoding informed by the decoding paths taken by other systems (Huang and Papineni, 2007), and decoding model combination (DeNero et al., 2010). 321 Input : systems [], tune(), source, refs [], α, EvalMetric (), SimMetric () Output: models [] // start with an empty set of translations from prior iterations other_sys [] ← [] for i ← 1 to len(systems []) do // new Positive Diversity measure using prior translations PDα,i () ← new PD(α, EvalMetric(), SimMetric(), refs [], other_sys []) // tune a new model to fit PDα,i // e.g., using MERT, PRO, MIRA,"
W13-2239,P08-2021,0,0.016728,"uce translations that are different from those produced by other systems. 2 Similar to speech recognition’s Recognizer Output Voting Error Reduction (ROVER) algorithm (Fiscus, 1997), machine translation system combination typically operates by aligning the translations produced by two or more individual translation systems and then using the alignments to construct a search space that allows new translations to be pieced together by picking and choosing parts of the material from the original translations (Bangalore et al., 2001; Matusov et al., 2006; Rosti et al., 2007a; Rosti et al., 2007b; Karakos et al., 2008; Heafield and Lavie, 2010a).2 The alignment of the individual system translations can be performed using alignment driven evaluation metrics such as invWER, TERp, METEOR (Leusch et al., 2003; Snover et al., 2009; Denkowski and Lavie, 2011). The piecewise selection of material from the original translations is performed using the combination model’s scoring features such as n-gram language models, confidence models over the individual systems, and consensus features that score a combined translation using n-grams matches to the individual system translations (Rosti et al., 2007b; Zhao and He,"
W13-2239,N03-1017,0,0.00817328,"rd phrase-pair extraction heuris323 PDT System α = 0.95 α = 0.97 α = 0.99 BLEU scores from individual systems tuned during iteration i of PDT 0 1 2 3 4 5 16.2 16.0 15.7 15.9 16.1 16.1 16.4 15.8 15.8 15.9 16.0 16.2 16.3 16.1 16.2 15.9 16.3 16.4 6 15.9 16.1 16.4 7 15.4 16.2 16.3 8 16.1 16.2 16.4 9 15.9 16.4 16.5 10 16.2 16.1 16.3 Table 3: BLEU scores on BOLT dev12 dev achieved by the individual PDT systems tuned on GALE dev10 web tune. Scores report individual system performance before system combination. tics were used to extract a phrase-table over word alignments symmetrized using grow-diag (Koehn et al., 2003). We made use of a hierarchical reordering model (Galley and Manning, 2008) as well as a 5-gram language model trained on the target side of the bi-text and smoothed using modified Kneeser-Ney (Chen and Goodman, 1996). Individual PDT systems were tuned on the GALE dev10 web tune set using online-PRO (Green et al., 2013; Hopkins and May, 2011) to the Positive Diversity Tuning criterion.4 The Multi-Engine Machine Translation (MEMT) package was used for system combination (Heafield and Lavie, 2010a). We used BOLT dev12 dev as a development test set to explore different α parameterizations of the"
W13-2239,2003.mtsummit-papers.32,0,0.0342342,"ranslation system combination typically operates by aligning the translations produced by two or more individual translation systems and then using the alignments to construct a search space that allows new translations to be pieced together by picking and choosing parts of the material from the original translations (Bangalore et al., 2001; Matusov et al., 2006; Rosti et al., 2007a; Rosti et al., 2007b; Karakos et al., 2008; Heafield and Lavie, 2010a).2 The alignment of the individual system translations can be performed using alignment driven evaluation metrics such as invWER, TERp, METEOR (Leusch et al., 2003; Snover et al., 2009; Denkowski and Lavie, 2011). The piecewise selection of material from the original translations is performed using the combination model’s scoring features such as n-gram language models, confidence models over the individual systems, and consensus features that score a combined translation using n-grams matches to the individual system translations (Rosti et al., 2007b; Zhao and He, 2009; Heafield and Lavie, 2010b). Both system confidence model features and ngram consensus features score contributions based in part on how confident the system combination model is in each"
W13-2239,N06-1014,0,0.0127583,"from the systems earlier in the list. During each iteration, the system constructs a 6 Experiments Experiments are performed using a single phrase-based Chinese-to-English translation system, built with the Stanford Phrasal machine translation toolkit (Cer et al., 2010). The system was built using all of the parallel data available for Phase 2 of the DARPA BOLT program. The Chinese data was segmented to the Chinese TreeBank (CTB) standard using a maximum match word segmenter, trained on the output of a CRF segmenter (Xiang et al., 2013). The bitext was word aligned using the Berkeley aligner (Liang et al., 2006). Standard phrase-pair extraction heuris323 PDT System α = 0.95 α = 0.97 α = 0.99 BLEU scores from individual systems tuned during iteration i of PDT 0 1 2 3 4 5 16.2 16.0 15.7 15.9 16.1 16.1 16.4 15.8 15.8 15.9 16.0 16.2 16.3 16.1 16.2 15.9 16.3 16.4 6 15.9 16.1 16.4 7 15.4 16.2 16.3 8 16.1 16.2 16.4 9 15.9 16.4 16.5 10 16.2 16.1 16.3 Table 3: BLEU scores on BOLT dev12 dev achieved by the individual PDT systems tuned on GALE dev10 web tune. Scores report individual system performance before system combination. tics were used to extract a phrase-table over word alignments symmetrized using gro"
W13-2239,D07-1105,0,0.616982,"l system combination requires the construction of multiple systems that are simultaneously diverse and well-performing. If the systems are not distinct enough, they will bring very little value during system combination. However, if some of the systems produce diverse translations but achieve lower overall translation quality, their contributions risk being ignored during system combination. Prior work has approached the need for diverse systems by using different system architectures, model components, system build parameters, decoder hyperparameters, as well as data selection and weighting (Macherey and Och, 2007; DeNero et al., 2010; Xiao et al., 2013). However, during tuning, each individual system is still just trained to maximize its own isolated performance on a tune set, or at best an error-driven reweighting of the tune set, without explicitly taking into account the diversity of the resulting translations. Such tuning does not encourage systems to rigorously explore model variations that achieve both good translation quality and diversity with respect to the other systems. It is reasonable to suspect that this results in individual systems that under exploit the amount of diversity possible, g"
W13-2239,E06-1005,0,0.0255359,"ould not only obtain good translation performance, but also produce translations that are different from those produced by other systems. 2 Similar to speech recognition’s Recognizer Output Voting Error Reduction (ROVER) algorithm (Fiscus, 1997), machine translation system combination typically operates by aligning the translations produced by two or more individual translation systems and then using the alignments to construct a search space that allows new translations to be pieced together by picking and choosing parts of the material from the original translations (Bangalore et al., 2001; Matusov et al., 2006; Rosti et al., 2007a; Rosti et al., 2007b; Karakos et al., 2008; Heafield and Lavie, 2010a).2 The alignment of the individual system translations can be performed using alignment driven evaluation metrics such as invWER, TERp, METEOR (Leusch et al., 2003; Snover et al., 2009; Denkowski and Lavie, 2011). The piecewise selection of material from the original translations is performed using the combination model’s scoring features such as n-gram language models, confidence models over the individual systems, and consensus features that score a combined translation using n-grams matches to the in"
W13-2239,P04-1063,0,0.028282,"s results in roughly equivalent translation performance (Cherry and Foster, 2012). Even when individual systems are being built to be used in a larger combined system, they are still usually tuned to maximize their isolated individual system performance rather than to maxi1 The exception being Xiao et al. (2013)’s work using boosting for error-driven reweighting of the tuning set 2 Other system combination techniques exist such as candidate selection systems, whereby the combination model attempts to find the best single candidate produced by one of the translation engines (Paul et al., 2005; Nomoto, 2004; Zwarts and Dras, 2008), decoder chaining (Aikawa and Ruopp, 2009), re-decoding informed by the decoding paths taken by other systems (Huang and Papineni, 2007), and decoding model combination (DeNero et al., 2010). 321 Input : systems [], tune(), source, refs [], α, EvalMetric (), SimMetric () Output: models [] // start with an empty set of translations from prior iterations other_sys [] ← [] for i ← 1 to len(systems []) do // new Positive Diversity measure using prior translations PDα,i () ← new PD(α, EvalMetric(), SimMetric(), refs [], other_sys []) // tune a new model to fit PDα,i // e.g."
W13-2239,P03-1021,0,0.0583526,"ing parameter values Θ that produce translations sysΘ that in turn achieve a high score on some correctness measure: arg max Correctness(ref[], sysΘ ) (1) Θ The correctness measure that systems are typically tuned toward is BLEU (Papineni et al., 2002), which measures the fraction of the n-grams that are both present in the reference translations and the translations produced by a system. The BLEU score is computed as the geometric mean of the resulting n-gram precisions scaled by a brevity penalty. The most widely used machine translation tuning algorithm, minimum error rate training (MERT) (Och, 2003), attempts to maximize the correctness objective directly. Popular alternatives such as pairwise ranking objective (PRO) (Hopkins and May, 2011), MIRA (Chiang et al., 2008), and RAMPION (Gimpel and Smith, 2012) use surrogate optimization objectives that indirectly attempt to maximize the correctness function by using it to select targets for training discriminative classification models. In practice, either optimizing correctness directly or optimizing a surrogate objective that uses correctness to choose optimization targets results in roughly equivalent translation performance (Cherry and Fo"
W13-2239,P02-1040,0,0.106954,"when combining a good system with poor performing systems even if the systems col3 System Combination Tuning Individual Translation Systems Machine translation systems are tuned toward some measure of the correctness of the translations produced by the system according to one or more manually translated references. As shown in equation (1), this can be written as finding parameter values Θ that produce translations sysΘ that in turn achieve a high score on some correctness measure: arg max Correctness(ref[], sysΘ ) (1) Θ The correctness measure that systems are typically tuned toward is BLEU (Papineni et al., 2002), which measures the fraction of the n-grams that are both present in the reference translations and the translations produced by a system. The BLEU score is computed as the geometric mean of the resulting n-gram precisions scaled by a brevity penalty. The most widely used machine translation tuning algorithm, minimum error rate training (MERT) (Och, 2003), attempts to maximize the correctness objective directly. Popular alternatives such as pairwise ranking objective (PRO) (Hopkins and May, 2011), MIRA (Chiang et al., 2008), and RAMPION (Gimpel and Smith, 2012) use surrogate optimization obje"
W13-2239,N07-1029,0,0.0983508,"ood translation performance, but also produce translations that are different from those produced by other systems. 2 Similar to speech recognition’s Recognizer Output Voting Error Reduction (ROVER) algorithm (Fiscus, 1997), machine translation system combination typically operates by aligning the translations produced by two or more individual translation systems and then using the alignments to construct a search space that allows new translations to be pieced together by picking and choosing parts of the material from the original translations (Bangalore et al., 2001; Matusov et al., 2006; Rosti et al., 2007a; Rosti et al., 2007b; Karakos et al., 2008; Heafield and Lavie, 2010a).2 The alignment of the individual system translations can be performed using alignment driven evaluation metrics such as invWER, TERp, METEOR (Leusch et al., 2003; Snover et al., 2009; Denkowski and Lavie, 2011). The piecewise selection of material from the original translations is performed using the combination model’s scoring features such as n-gram language models, confidence models over the individual systems, and consensus features that score a combined translation using n-grams matches to the individual system tran"
W13-2239,P07-1040,0,0.021501,"ood translation performance, but also produce translations that are different from those produced by other systems. 2 Similar to speech recognition’s Recognizer Output Voting Error Reduction (ROVER) algorithm (Fiscus, 1997), machine translation system combination typically operates by aligning the translations produced by two or more individual translation systems and then using the alignments to construct a search space that allows new translations to be pieced together by picking and choosing parts of the material from the original translations (Bangalore et al., 2001; Matusov et al., 2006; Rosti et al., 2007a; Rosti et al., 2007b; Karakos et al., 2008; Heafield and Lavie, 2010a).2 The alignment of the individual system translations can be performed using alignment driven evaluation metrics such as invWER, TERp, METEOR (Leusch et al., 2003; Snover et al., 2009; Denkowski and Lavie, 2011). The piecewise selection of material from the original translations is performed using the combination model’s scoring features such as n-gram language models, confidence models over the individual systems, and consensus features that score a combined translation using n-grams matches to the individual system tran"
W13-2239,W09-0441,0,0.0194723,"bination typically operates by aligning the translations produced by two or more individual translation systems and then using the alignments to construct a search space that allows new translations to be pieced together by picking and choosing parts of the material from the original translations (Bangalore et al., 2001; Matusov et al., 2006; Rosti et al., 2007a; Rosti et al., 2007b; Karakos et al., 2008; Heafield and Lavie, 2010a).2 The alignment of the individual system translations can be performed using alignment driven evaluation metrics such as invWER, TERp, METEOR (Leusch et al., 2003; Snover et al., 2009; Denkowski and Lavie, 2011). The piecewise selection of material from the original translations is performed using the combination model’s scoring features such as n-gram language models, confidence models over the individual systems, and consensus features that score a combined translation using n-grams matches to the individual system translations (Rosti et al., 2007b; Zhao and He, 2009; Heafield and Lavie, 2010b). Both system confidence model features and ngram consensus features score contributions based in part on how confident the system combination model is in each individual machine t"
W13-2239,P13-1081,0,0.0124211,"translations that both fit the references and are encouraged to be distinct from the systems earlier in the list. During each iteration, the system constructs a 6 Experiments Experiments are performed using a single phrase-based Chinese-to-English translation system, built with the Stanford Phrasal machine translation toolkit (Cer et al., 2010). The system was built using all of the parallel data available for Phase 2 of the DARPA BOLT program. The Chinese data was segmented to the Chinese TreeBank (CTB) standard using a maximum match word segmenter, trained on the output of a CRF segmenter (Xiang et al., 2013). The bitext was word aligned using the Berkeley aligner (Liang et al., 2006). Standard phrase-pair extraction heuris323 PDT System α = 0.95 α = 0.97 α = 0.99 BLEU scores from individual systems tuned during iteration i of PDT 0 1 2 3 4 5 16.2 16.0 15.7 15.9 16.1 16.1 16.4 15.8 15.8 15.9 16.0 16.2 16.3 16.1 16.2 15.9 16.3 16.4 6 15.9 16.1 16.4 7 15.4 16.2 16.3 8 16.1 16.2 16.4 9 15.9 16.4 16.5 10 16.2 16.1 16.3 Table 3: BLEU scores on BOLT dev12 dev achieved by the individual PDT systems tuned on GALE dev10 web tune. Scores report individual system performance before system combination. tics w"
W13-2239,N09-2052,0,0.01801,"et al., 2008; Heafield and Lavie, 2010a).2 The alignment of the individual system translations can be performed using alignment driven evaluation metrics such as invWER, TERp, METEOR (Leusch et al., 2003; Snover et al., 2009; Denkowski and Lavie, 2011). The piecewise selection of material from the original translations is performed using the combination model’s scoring features such as n-gram language models, confidence models over the individual systems, and consensus features that score a combined translation using n-grams matches to the individual system translations (Rosti et al., 2007b; Zhao and He, 2009; Heafield and Lavie, 2010b). Both system confidence model features and ngram consensus features score contributions based in part on how confident the system combination model is in each individual machine translation system. This means that little or no gains will typically be seen when combining a good system with poor performing systems even if the systems col3 System Combination Tuning Individual Translation Systems Machine translation systems are tuned toward some measure of the correctness of the translations produced by the system according to one or more manually translated references"
W13-2239,D08-1089,1,\N,Missing
W13-2239,2005.iwslt-1.5,0,\N,Missing
W13-2239,N12-1023,0,\N,Missing
W18-0615,D14-1162,0,0.0822593,"lines, the resulting algorithms differentiate between our two subject groups with statistical significance and are strong predictors of schizophrenia. The preprocessing changes are simple. First, we remove all filler words (i.e., various forms of uh, um, you know, etc.) and sentences entirely composed of stop words. Second, we replace the sliding window in the Tangentiality Model with sentence tokenization to capture semantically meaningful chunks of the response, obviating the need to tune the window size parameter. Second, we draw on recent advances in word embeddings (Mikolov et al., 2013; Pennington et al., 2014) and sentence embeddings (Arora et al., 2016; Pagliardini et al., 2018). These are The three sentence embedding techniques mentioned above are intended to improve the em141 Distinguishing Schizophrenics from Controls Incoherence Model Sentence Word t-test Stat LSA 0.594 Mean Vector Glove 0.514 Word2Vec 1.147 LSA 1.142 TD-IDF Glove 0.935 Word2Vec 1.957 LSA 1.517 SIF Glove 2.139 Word2Vec 2.432* Sent2Vec Sent2Vec 2.067 Tangentiality Model Sentence Word t-test Stat LSA 0.588 Mean Vector Glove 1.820 Word2Vec 1.689 LSA 2.173* TF-IDF Glove 0.718 Word2Vec 1.372 LSA 1.930 SIF Glove 2.207* Word2Vec 2.35"
W18-0615,P91-1008,0,0.299457,"guistics and Clinical Psychology: From Keyboard to Clinic, pages 136–146 c New Orleans, Louisiana, June 5, 2018. 2018 Association for Computational Linguistics well-written texts that makes them easier to ... understand than a sequence of randomly strung sentences” (Lapata and Barzilay, 2005). Various aspects of discourse are associated with coherence. Lexical cohesion models chains of words and synonyms (Halliday and Hasan, 2014; Morris and Hirst, 1991). Relational models like rhetorical structure theory define discourse relations that hierarchically structure texts (Mann and Thompson, 1988; Lascaridest and Asher, 1991). Referential coherence focuses on the coherence of entities moving in and out of focus across a text (Grosz and Sidner, 1986; Barzilay and Lapata, 2008). these coherence models outperform the previously proposed methods and prove to be statistically significant discriminators between our schizophrenic and control groups. We also investigate the use of referential incoherence in our schizophrenic groups. FTD has been reported to coincide with anomalies in deictic noun phrase usage, including various unusual uses of pronouns (Hinzen and Rossell´o, 2015). We observed that referential incoherence"
W18-0615,W15-1202,0,0.378711,"word-document co-occurrences, and was applied early on as a model of discourse coherence, using cosines between embeddings for text regions as a measure of concept overlap or lexical cohesion (Foltz et al., 1998; McNamara et al., 2010). Joe Montana having a remarkable season coming off his Super Bowl Win where they upset the Cincinnati Bengals is off to another fabulous year Various other computational models have shown features of text and speech that can be automatically extracted and are associated with schizophrenia, including lexical features drawn from lexicons (Hong et al., 2012, 2015; Mitchell et al., 2015) and acoustic features (Covington et al., 2012). We focus in this paper on coherence metrics, but in the future will be exploring the role of these additional linguistic features on our dataset as well. Figure 4 shows more examples of ambiguous pronoun use in our dataset. Based on this observation, we propose automatically measuring ambiguous pronoun usage as a novel computational model for referential incoherence in FTD and show its ability to predict schizophrenia in our pilot study. 2 Related Work Speech analysis and coherence. FTD is typically diagnosed on the basis of the clinical observa"
W18-0615,J91-1002,0,0.770481,"oherence. Psychiatrists diagnose schizophrenia by assessing subjects in a clinical setting and noting abnor136 Proceedings of the Fifth Workshop on Computational Linguistics and Clinical Psychology: From Keyboard to Clinic, pages 136–146 c New Orleans, Louisiana, June 5, 2018. 2018 Association for Computational Linguistics well-written texts that makes them easier to ... understand than a sequence of randomly strung sentences” (Lapata and Barzilay, 2005). Various aspects of discourse are associated with coherence. Lexical cohesion models chains of words and synonyms (Halliday and Hasan, 2014; Morris and Hirst, 1991). Relational models like rhetorical structure theory define discourse relations that hierarchically structure texts (Mann and Thompson, 1988; Lascaridest and Asher, 1991). Referential coherence focuses on the coherence of entities moving in and out of focus across a text (Grosz and Sidner, 1986; Barzilay and Lapata, 2008). these coherence models outperform the previously proposed methods and prove to be statistically significant discriminators between our schizophrenic and control groups. We also investigate the use of referential incoherence in our schizophrenic groups. FTD has been reported"
W18-0615,N18-1049,0,0.141232,"rip then. Um, badum badum. Narrative meaning? A little ball hitting the other ball but I don’t know the name. Hexagonal? yeah. Um So, all right. Um, badum badum. A vacation. 0.406 0.387 0.380 0.355 0.350 0.332 0.323 0.284 0.218 0.184 Table 2: The 10 lowest scoring pairs of sentences in our corpus. Less coherent pairs have lower scores. known to provide superior representations, such as correcting for sentence embeddings that contain “semantically meaningless directions” (Arora et al., 2016). We test a number of sentence embeddings, which we refer to as TF-IDF (Lintean et al., 2010), Sent2Vec (Pagliardini et al., 2018) and Smooth Inverse Frequency (SIF) (Arora et al., 2016). TF-IDF is a traditional vector weighting scheme; in using it to create sentence embeddings we follow the parameterization of Lintean et al. (2010), proposed originally to create sentence embeddings for LSA: multiplying each word embedding by the raw (non-logged) term frequency (# of times that word occurs in the sentence) and dividing by the (non-logged) document frequency (# of documents in which the term is used in a corpus). Typically, for small corpora the denominator term is taken from a large corpus; we chose the enwiki dataset (W"
W19-2309,D17-1070,0,0.0268179,"ch style transfer task. All reported numbers scaled by 102 for display. NYT↔BNC NYT↔Reddit Pop↔Hip hop → ← → ← → ← Pre-Transfer 5.763 3.891 4.609 5.763 2.470 1.453 Baseline Style Rerank Meaning Rerank 4.016 3.877 3.874 4.012 3.992 3.743 3.920 3.603 3.808 5.506 5.194 5.395 2.112 1.930 1.915 1.429 1.310 1.284 Method Table 4: Fluency of each style transfer task. All reported numbers scaled by 103 for display. To compute the embeddings r, we use the sentence encoder provided by the InferSent library, which has demonstrated excellent performance on a number of natural language understanding tasks (Conneau et al., 2017). 3. Fluency The post-transfer sentence should remain grammatical and fluent. We use the average log probability of the sentence posttransfer with respect to a language model trained on CommonCrawl as our measure of fluency. Task Base Rerank No Pref NYT → BNC BNC → NYT 6.00 10.8 6.25 6.5 87.8 82.8 NYT → Reddit Reddit → NYT 6.75 9.75 9.5 18.3 83.8 72.0 Pop → Hip Hop Hip Hop → Pop 5.25 7.5 6.50 10.3 88.3 82.3 Table 5: Human evaluation results for style transfer strength. Entries give percentage of time where annotator preferred base vs. rerank (combined for 2 annotators). 3.3 Pairwise Human Eval"
W19-2309,P18-1097,0,0.0249571,"ethods or datasets in order to consistently encourage more syntactically rich edits. Noising and denoising To our knowledge, there has been no prior work formulating style transfer as a denoising task outside of using token corruptions to avoid copying between source and target. Our style transfer method borrows techniques from the field of noising and denoising to correct errors in text. We apply the noising technique in Xie et al. (2018) that requires an initial noise seed corpus instead of dictionaries or aligned embeddings. Similar work for using noise to create a parallel corpus includes Ge et al. (2018). References Style transfer Existing work for style transfer often takes the approach of separating content and style, for example by encoding a sentence into some latent space (Bowman et al., 2015; Hu et al., 2017; Shen et al., 2017) and then modifying or augmenting that space towards a different style. Hu et al. (2017) base their method on variational autoencoders (Kingma and Welling, 2014), while Shen et al. (2017) instead propose two constrained variants of the autoencoder. Yang et al. (2018) use language models as discriminators instead of a binary classifier as they hypothesize language"
W19-2309,P12-2039,0,0.0139967,", we apply the noising method described in Xie et al. (2018). This 75 We then join the corpora to obtain the (clean, noisy) sentence pairs, A summary of the datasets used for the three tasks is provided in Table 1. We use The New York Times for the American English data, the British National Corpus for the British English data, and the Reddit comments dataset for informal forum data. The pop and hip hop lyrics are gathered from MetroLyrics.1 For the parallel seed corpus used to train the noising model, we use a dataset of roughly 1MM sentences collected from an English language learner forum (Tajiri et al., 2012). ˜ = {(Xk , X ˜ k )}MR+S , (X, X) k=1 from which we will learn our denoising model. Our denoising model learns the probabilistic map˜ obtaining model parameters θ∗ by ping P (X|X), minimizing the loss function: ∑ MR+S L(θ) = − ˜ k ; θ) log P (Xk |X 3.2 Evaluation k=1 We define effective style transfer using the following criteria: For our experiments we use the Transformer encoder-decoder model (Vaswani et al., 2017) with byte-pair encoding (Sennrich et al., 2015b) with vocabulary size of 30000. We follow the usual training procedure of minibatch gradient descent to minimize negative log-like"
W19-2309,P13-2121,0,0.0135617,"YT → BNC BNC → NYT 6.00 10.8 6.25 6.5 87.8 82.8 NYT → Reddit Reddit → NYT 6.75 9.75 9.5 18.3 83.8 72.0 Pop → Hip Hop Hip Hop → Pop 5.25 7.5 6.50 10.3 88.3 82.3 Table 5: Human evaluation results for style transfer strength. Entries give percentage of time where annotator preferred base vs. rerank (combined for 2 annotators). 3.3 Pairwise Human Evaluation of Reranking The source and target language models are 4-gram (in the case of music lyrics) or 5-gram (in the case of other datasets) language models trained on a held-out subset of each corpus, estimated with Kneser-Ney smoothing using KenLM (Heafield et al., 2013). While language model likelihood is an established measure of fluency or grammaticality, and InferSent has been used as an effective sentence representation on a number of natural language un77 derstanding tasks (Conneau et al., 2017), we wish to validate our transfer strength results for our proposed reranking method using human evaluation as well. For each of the six tasks (3 pairs crossed with 2 directions), we randomly selected 200 sentences, then took the outputs with models trained using style reranking and without style reranking. We then randomized the outputs such that the human eval"
W19-2309,Q17-1024,0,0.0399512,"slation, in particular recent work on machine translation without parallel data using only a dictionary or aligned word embeddings (Lample et al., 2017; Artetxe et al., 2017). These approaches also use backtranslation while introducing token-level corruptions to avoid the problem of copying during an initial autoencoder training phase. They additionally use an initial dictionary or embedding alignments which may be infeasible to collect for many style transfer tasks. Finally, our work also draws from work on zero-shot translation between languages given parallel corpora with a pivot language (Johnson et al., 2017). 6 Conclusion In this paper, we propose a denoising method for performing text style transfer by treating the source text as a noisy version of the desired target. Our method can generate rich edits to map inputs to the target style. We additionally propose two reranking methods during the data synthesis phase intended to encourage meaning preservation as well as modulate the strength of style transfer, then examine their effects across three varied datasets. An exciting future direction is to develop other noising methods or datasets in order to consistently encourage more syntactically rich"
W19-2309,N18-1057,1,0.929064,"ulti-task learning often improves the performance for each of the separate tasks (Luong et al., 2016). 2.1 Noising We first synthesize noisy versions of R and S. We first obtain a seed noise corpus of (clean, noisy) sentence pairs from a language learner forum. Using the seed noise corpus, we train a neural sequence transduction model to learn the mapping from clean to noisy C → N from our (clean, noisy) sentence pairs. Then, we decode R and S using the noising model to synthesize the ˜ and S. ˜ corresponding noisy versions, R • Baseline As a baseline, we apply the noising method described in Xie et al. (2018). This 75 We then join the corpora to obtain the (clean, noisy) sentence pairs, A summary of the datasets used for the three tasks is provided in Table 1. We use The New York Times for the American English data, the British National Corpus for the British English data, and the Reddit comments dataset for informal forum data. The pop and hip hop lyrics are gathered from MetroLyrics.1 For the parallel seed corpus used to train the noising model, we use a dataset of roughly 1MM sentences collected from an English language learner forum (Tajiri et al., 2012). ˜ = {(Xk , X ˜ k )}MR+S , (X, X) k=1 f"
W19-2309,N18-1169,0,0.0706253,"Missing"
W19-2309,P18-1080,0,0.0465819,"of generated outputs, we demonstrate that our method is able both to produce high-quality output while maintaining the flexibility to suggest syntactically rich stylistic edits. Cseed R S Noise (Decode) ˜ R S˜ 3. Train denoising model ˜ ∪ S˜ R Denoise (Train) R∪S 4. Style transfer/decode denoising model Introduction S Following exciting work on style transfer for images (Gatys et al., 2016), neural style transfer for text has gained research interest as an application and testbed for syntactic and semantic understanding of natural language (Li et al., 2018; Shen et al., 2017; Hu et al., 2017; Prabhumoye et al., 2018). Unfortunately, unlike image style transfer, which often requires only a single reference image in the desired style, neural text style transfer typically requires a large parallel corpus of sentences in the source and target style to train a neural machine translation model (Sutskever et al., 2014; Bahdanau et al., 2014). One approach to mitigate the need for a large parallel corpus is to develop methods to disentangle stylistic attributes from semantic content, for example by using adversarial classifiers (Shen et al., 2017) or by predefining markers associated with stylistic attributes (Li"
W19-2309,P16-1009,0,0.0708243,"Missing"
W19-4701,P16-1141,1,0.91716,"00) that allow a manner adverb to be reinterpreted as an intensifying adverb similar to very. 1 Introduction Developments in computational semantics and availability of large diachronic corpora have renewed interest in studying historical semantic change. Recent work has moved away from documenting and qualitatively categorizing types of changes (Br´eal, 1964; Stern, 1931) to focus on detecting semantic shifts (Gulordava and Baroni, 2011; Rosenfeld and Erk, 2018; Frermann and Lapata, 2016; Mitra et al., 2014; Kulkarni et al., 2015), distinguishing gradual linguistic drifts from cultural ones (Hamilton et al., 2016a) and assessing laws of change (Hamilton et al., 2016b; Dubossarsky et al., 2017; Xu and Kemp, 2015; Ramiro et al., 2018; Luo and Xu, 2018). 1 Though he focuses on synchronic properties of degree words, Bolinger (1972, 18) observes: “[Intensifiers] afford 1 Proceedings of the 1st International Workshop on Computational Approaches to Historical Language Change, pages 1–13 c Florence, Italy, August 2, 2019. 2019 Association for Computational Linguistics Original usage awfully behaved wildly flailing insanely muttering abundantly endow singing terribly aggressively demanded Bleached usage awfull"
W19-4701,P12-3029,0,0.020086,"Missing"
