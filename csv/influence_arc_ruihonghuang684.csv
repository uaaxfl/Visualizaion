2020.aacl-main.81,W06-0901,0,0.173113,"ic English MUC-4 dataset and a large-scale Chinese CFEED dataset. 2 http://www.nlpr.ia.ac.cn/cip/ liukang/dataset/documentevent1.html ˜ 812 2 Related Work Sentence-level EE has achieved a lot of advancement in recent work (Chen et al., 2015; Nguyen et al., 2016; Chen et al., 2018) and can be classified into template-based approaches (Jungermann and Morik, 2008; Bjorne et al., 2010; Hogenboom et al., 2016) and statistical approaches. Templatebased methods require human-crafted templates to match the events. Most of the statistical methods are supervised and either based on feature engineering (Ahn, 2006; Ji and Grishman, 2008; Liao and Grishman, 2010; Reichart and Barzilay, 2012) or Neural network algorithm (Chen et al., 2015; Nguyen et al., 2016; Chen et al., 2018; Liu et al., 2018; Sha et al., 2018; Liu et al., 2018). However, these supervised methods rely on intensive manual annotations. To alleviate this problem, many weak supervised methods (Chen et al., 2017; Zeng et al., 2018) have arisen and achieved good performance in ACE 2005 evaluation. However, most of the time, people care about the events discussed across a whole document. So research on document-level EE also prevails. Tradit"
2020.aacl-main.81,D17-1209,0,0.0554986,"Missing"
2020.aacl-main.81,D14-1199,0,0.038221,"Missing"
2020.aacl-main.81,P17-1038,1,0.848953,"08; Bjorne et al., 2010; Hogenboom et al., 2016) and statistical approaches. Templatebased methods require human-crafted templates to match the events. Most of the statistical methods are supervised and either based on feature engineering (Ahn, 2006; Ji and Grishman, 2008; Liao and Grishman, 2010; Reichart and Barzilay, 2012) or Neural network algorithm (Chen et al., 2015; Nguyen et al., 2016; Chen et al., 2018; Liu et al., 2018; Sha et al., 2018; Liu et al., 2018). However, these supervised methods rely on intensive manual annotations. To alleviate this problem, many weak supervised methods (Chen et al., 2017; Zeng et al., 2018) have arisen and achieved good performance in ACE 2005 evaluation. However, most of the time, people care about the events discussed across a whole document. So research on document-level EE also prevails. Traditionally, pattern-based and classifier-based methods are popular to solve this task. Systems like AutoSlog (Riloff et al., 1993) and AutoSlogTS (Riloff, 1996) directly applied regular patterns to extract role fillers. Many works (Patwardhan and Riloff, 2007, 2009; Huang and Riloff, 2011, 2012; Boros et al., 2014) relied on feature-based classifiers to distinguish can"
2020.aacl-main.81,P15-1017,1,0.877144,"relevant sources. • We propose an edge-enriched graph attention algorithm that can blend both the local clues and global context to enforce semantic representations for each candidate and help to filter noises in the event regions. • Experimental results show that our method outperforms the existing state-of-the-arts on two datasets with different languages, including a public English MUC-4 dataset and a large-scale Chinese CFEED dataset. 2 http://www.nlpr.ia.ac.cn/cip/ liukang/dataset/documentevent1.html ˜ 812 2 Related Work Sentence-level EE has achieved a lot of advancement in recent work (Chen et al., 2015; Nguyen et al., 2016; Chen et al., 2018) and can be classified into template-based approaches (Jungermann and Morik, 2008; Bjorne et al., 2010; Hogenboom et al., 2016) and statistical approaches. Templatebased methods require human-crafted templates to match the events. Most of the statistical methods are supervised and either based on feature engineering (Ahn, 2006; Ji and Grishman, 2008; Liao and Grishman, 2010; Reichart and Barzilay, 2012) or Neural network algorithm (Chen et al., 2015; Nguyen et al., 2016; Chen et al., 2018; Liu et al., 2018; Sha et al., 2018; Liu et al., 2018). However,"
2020.aacl-main.81,D18-1158,1,0.821423,"nriched graph attention algorithm that can blend both the local clues and global context to enforce semantic representations for each candidate and help to filter noises in the event regions. • Experimental results show that our method outperforms the existing state-of-the-arts on two datasets with different languages, including a public English MUC-4 dataset and a large-scale Chinese CFEED dataset. 2 http://www.nlpr.ia.ac.cn/cip/ liukang/dataset/documentevent1.html ˜ 812 2 Related Work Sentence-level EE has achieved a lot of advancement in recent work (Chen et al., 2015; Nguyen et al., 2016; Chen et al., 2018) and can be classified into template-based approaches (Jungermann and Morik, 2008; Bjorne et al., 2010; Hogenboom et al., 2016) and statistical approaches. Templatebased methods require human-crafted templates to match the events. Most of the statistical methods are supervised and either based on feature engineering (Ahn, 2006; Ji and Grishman, 2008; Liao and Grishman, 2010; Reichart and Barzilay, 2012) or Neural network algorithm (Chen et al., 2015; Nguyen et al., 2016; Chen et al., 2018; Liu et al., 2018; Sha et al., 2018; Liu et al., 2018). However, these supervised methods rely on intensiv"
2020.aacl-main.81,W04-1000,0,0.595724,"a graph into rich vector representations to facilitate event region identification. The experimental results on two datasets of two languages show that our approach yields new state-of-the-art performance for the challenging event extraction task. 1 Event Template Event Extraction (EE), a challenging task in Natural Language Processing, aims to extract key types of information (aka event roles, e.g., perpetrators and victims of an attack event) that can represent an event in texts and plays a critical role in downstream applications such as Question Answer (Yang et al., 2003) and Summarizing (Filatova and Hatzivassiloglou, 2004). Existing research on EE mostly focused on sentence-level, such as the evaluation in Automatic Content Extraction (ACE) 20051 . However, an event is usually described in Most of the work was done when the first author was a research engineer in the Institute of Automation, CAS. 1 http://projects.ldc.upenn.edu/ace/ Role Fillers PerpInd TERRORISTS, HOODED INDIVIDUALS PerpOrg SHINING PATH Victim DOLORES HINOSTROZA, HINOSTROZA Figure 1: An example of document-level event extraction. We need to extract noun phrases from the document as role fillers for the event roles in the predefined event templ"
2020.aacl-main.81,P11-1114,1,0.959214,"intensive manual annotations. To alleviate this problem, many weak supervised methods (Chen et al., 2017; Zeng et al., 2018) have arisen and achieved good performance in ACE 2005 evaluation. However, most of the time, people care about the events discussed across a whole document. So research on document-level EE also prevails. Traditionally, pattern-based and classifier-based methods are popular to solve this task. Systems like AutoSlog (Riloff et al., 1993) and AutoSlogTS (Riloff, 1996) directly applied regular patterns to extract role fillers. Many works (Patwardhan and Riloff, 2007, 2009; Huang and Riloff, 2011, 2012; Boros et al., 2014) relied on feature-based classifiers to distinguish candidate role fillers from texts and achieved better performance. Until recent years, researchers (Hsi, 2018; Yang et al., 2018; Zheng et al., 2019) began to utilize multiple neuralbased methods to solve the task. Notably, among the document-level EE research, some works (Patwardhan and Riloff, 2009; Huang and Riloff, 2012; Yang et al., 2018) have noticed the importance of identifying event regions to improve performance. Traditional neural networks such as Convolutional Neural Networks and Recursive Neural Network"
2020.aacl-main.81,P08-1030,0,0.140204,"MUC-4 dataset and a large-scale Chinese CFEED dataset. 2 http://www.nlpr.ia.ac.cn/cip/ liukang/dataset/documentevent1.html ˜ 812 2 Related Work Sentence-level EE has achieved a lot of advancement in recent work (Chen et al., 2015; Nguyen et al., 2016; Chen et al., 2018) and can be classified into template-based approaches (Jungermann and Morik, 2008; Bjorne et al., 2010; Hogenboom et al., 2016) and statistical approaches. Templatebased methods require human-crafted templates to match the events. Most of the statistical methods are supervised and either based on feature engineering (Ahn, 2006; Ji and Grishman, 2008; Liao and Grishman, 2010; Reichart and Barzilay, 2012) or Neural network algorithm (Chen et al., 2015; Nguyen et al., 2016; Chen et al., 2018; Liu et al., 2018; Sha et al., 2018; Liu et al., 2018). However, these supervised methods rely on intensive manual annotations. To alleviate this problem, many weak supervised methods (Chen et al., 2017; Zeng et al., 2018) have arisen and achieved good performance in ACE 2005 evaluation. However, most of the time, people care about the events discussed across a whole document. So research on document-level EE also prevails. Traditionally, pattern-based"
2020.aacl-main.81,P10-1081,0,0.195906,"ge-scale Chinese CFEED dataset. 2 http://www.nlpr.ia.ac.cn/cip/ liukang/dataset/documentevent1.html ˜ 812 2 Related Work Sentence-level EE has achieved a lot of advancement in recent work (Chen et al., 2015; Nguyen et al., 2016; Chen et al., 2018) and can be classified into template-based approaches (Jungermann and Morik, 2008; Bjorne et al., 2010; Hogenboom et al., 2016) and statistical approaches. Templatebased methods require human-crafted templates to match the events. Most of the statistical methods are supervised and either based on feature engineering (Ahn, 2006; Ji and Grishman, 2008; Liao and Grishman, 2010; Reichart and Barzilay, 2012) or Neural network algorithm (Chen et al., 2015; Nguyen et al., 2016; Chen et al., 2018; Liu et al., 2018; Sha et al., 2018; Liu et al., 2018). However, these supervised methods rely on intensive manual annotations. To alleviate this problem, many weak supervised methods (Chen et al., 2017; Zeng et al., 2018) have arisen and achieved good performance in ACE 2005 evaluation. However, most of the time, people care about the events discussed across a whole document. So research on document-level EE also prevails. Traditionally, pattern-based and classifier-based meth"
2020.aacl-main.81,D17-1159,0,0.0728135,"Missing"
2020.aacl-main.81,N16-1034,0,0.0729328,"We propose an edge-enriched graph attention algorithm that can blend both the local clues and global context to enforce semantic representations for each candidate and help to filter noises in the event regions. • Experimental results show that our method outperforms the existing state-of-the-arts on two datasets with different languages, including a public English MUC-4 dataset and a large-scale Chinese CFEED dataset. 2 http://www.nlpr.ia.ac.cn/cip/ liukang/dataset/documentevent1.html ˜ 812 2 Related Work Sentence-level EE has achieved a lot of advancement in recent work (Chen et al., 2015; Nguyen et al., 2016; Chen et al., 2018) and can be classified into template-based approaches (Jungermann and Morik, 2008; Bjorne et al., 2010; Hogenboom et al., 2016) and statistical approaches. Templatebased methods require human-crafted templates to match the events. Most of the statistical methods are supervised and either based on feature engineering (Ahn, 2006; Ji and Grishman, 2008; Liao and Grishman, 2010; Reichart and Barzilay, 2012) or Neural network algorithm (Chen et al., 2015; Nguyen et al., 2016; Chen et al., 2018; Liu et al., 2018; Sha et al., 2018; Liu et al., 2018). However, these supervised meth"
2020.aacl-main.81,D07-1075,0,0.0602653,", these supervised methods rely on intensive manual annotations. To alleviate this problem, many weak supervised methods (Chen et al., 2017; Zeng et al., 2018) have arisen and achieved good performance in ACE 2005 evaluation. However, most of the time, people care about the events discussed across a whole document. So research on document-level EE also prevails. Traditionally, pattern-based and classifier-based methods are popular to solve this task. Systems like AutoSlog (Riloff et al., 1993) and AutoSlogTS (Riloff, 1996) directly applied regular patterns to extract role fillers. Many works (Patwardhan and Riloff, 2007, 2009; Huang and Riloff, 2011, 2012; Boros et al., 2014) relied on feature-based classifiers to distinguish candidate role fillers from texts and achieved better performance. Until recent years, researchers (Hsi, 2018; Yang et al., 2018; Zheng et al., 2019) began to utilize multiple neuralbased methods to solve the task. Notably, among the document-level EE research, some works (Patwardhan and Riloff, 2009; Huang and Riloff, 2012; Yang et al., 2018) have noticed the importance of identifying event regions to improve performance. Traditional neural networks such as Convolutional Neural Network"
2020.aacl-main.81,D09-1016,0,0.768247,"mentions the target event twice in two regions. The correct role fillers are crowding in the first event region S1, S2, S3 and the second one S5, S6 respectively. Nevertheless, the sentence-level extractor will extract noise from both the event regions like HOUSE from S3 and irrelevant sentence like FATHER in S4, destroying the layout of the original regions. Many previous efforts try to avoid aggregating the noisy candidates by detecting such event regions. The popular approach is to apply sentential classification to filter the sentences and recognize role fillers from the chosen sentences (Patwardhan and Riloff, 2009; Huang and Riloff, 2012). However, these approaches only detect regions at single sentence-level and ignore the crowding of relevant sentences. Also, they also suffer from the accumulative error of sentential classification. For example, they may identify S2 as a relevant event region but S3 as irrelevant because they fail to take into account the similarity of S2 and S3. Another solution proposed by Yang et al. (2018) tries to detect the primary event description sentence and supplement the missing event roles with fillers from adjacent sentences. This method considers the multiple sentences"
2020.aacl-main.81,N12-1008,0,0.0667316,"taset. 2 http://www.nlpr.ia.ac.cn/cip/ liukang/dataset/documentevent1.html ˜ 812 2 Related Work Sentence-level EE has achieved a lot of advancement in recent work (Chen et al., 2015; Nguyen et al., 2016; Chen et al., 2018) and can be classified into template-based approaches (Jungermann and Morik, 2008; Bjorne et al., 2010; Hogenboom et al., 2016) and statistical approaches. Templatebased methods require human-crafted templates to match the events. Most of the statistical methods are supervised and either based on feature engineering (Ahn, 2006; Ji and Grishman, 2008; Liao and Grishman, 2010; Reichart and Barzilay, 2012) or Neural network algorithm (Chen et al., 2015; Nguyen et al., 2016; Chen et al., 2018; Liu et al., 2018; Sha et al., 2018; Liu et al., 2018). However, these supervised methods rely on intensive manual annotations. To alleviate this problem, many weak supervised methods (Chen et al., 2017; Zeng et al., 2018) have arisen and achieved good performance in ACE 2005 evaluation. However, most of the time, people care about the events discussed across a whole document. So research on document-level EE also prevails. Traditionally, pattern-based and classifier-based methods are popular to solve this"
2020.aacl-main.81,P18-4009,1,0.243235,"detecting such event regions. The popular approach is to apply sentential classification to filter the sentences and recognize role fillers from the chosen sentences (Patwardhan and Riloff, 2009; Huang and Riloff, 2012). However, these approaches only detect regions at single sentence-level and ignore the crowding of relevant sentences. Also, they also suffer from the accumulative error of sentential classification. For example, they may identify S2 as a relevant event region but S3 as irrelevant because they fail to take into account the similarity of S2 and S3. Another solution proposed by Yang et al. (2018) tries to detect the primary event description sentence and supplement the missing event roles with fillers from adjacent sentences. This method considers the multiple sentences in an event region but is limited to one region per document. For instance, it may detect S1 as the primary sentence and supplement it with S2, missing the valid items like SHINING PATH from region 2. Moreover, it also suffers from the errors selecting primary sentence, and the supplementing strategy is coarse-grained and fails to take into account every candidate filler individually. We build a graph for each document"
2020.aacl-main.81,D19-1032,0,0.443645,"Missing"
2020.acl-main.478,S17-1027,0,0.110158,") 316(4.3) 47(2.1) D3 1876(32.1) 953(19.5) 1867(25.2) 598(26.9) D4 532(9.1) 525(10.7) 924(12.5) 148(6.7) N/A 197(3.3) 736(15.0) 686(9.3) 141(6.3) Table 3: Distribution of Content type labels across media sources, with percentages shown within parentheses. vs. not speech). In the created corpus, 5535 out of 18,155 sentences are labeled as speech. 5 Document-level Neural Network Model for Discourse Profiling A wide range of computational models has been applied for extracting different forms of discourse structures. However, across several tasks, neural network methods (Ji and Eisenstein, 2015; Becker et al., 2017) are found the most effective, with relatively superior performance obtained by modeling discourse-level context (Dai and Huang, 2018a,b). As an initial attempt, we use a hierarchical neural network to derive sentence representations and a document encoding, and model associations between each sentence and the main topic of the document when determining content types for sentences. Shown in Figure 1, it first uses a wordlevel bi-LSTM layer (Hochreiter and Schmidhuber, 1997) with soft-attention over word representations to generate intermediate sentence representations which are further enriche"
2020.acl-main.478,doddington-etal-2004-automatic,0,0.210598,"Missing"
2020.acl-main.478,D15-1263,0,0.0510906,"Missing"
2020.acl-main.478,P18-1045,1,0.727743,"types, seven types specifically, therefore, if main events do not belong to those seven types, they are not annotated as events, which also contributes to the imperfect percentage of main event sentences containing a headline event. 6 All the KBP corpora include documents from both discussion forum and news articles. But as the goal of this study is to leverage discourse structures specific to news articles for improving event coreference resolution performance, we only evaluate the ILP system using news articles in the KBP corpora. This evaluation setting is consistent with our previous work Choubey and Huang (2018). For direct comparisons, the results reported for all the systems and baselines are based on news articles in the test datasets as well 7 The classifier can be obtained from https://git. io/JeDw3 5381 Model Local classifier +Content Structure -Singletons -Main Events -Intra-type Events Lu and Ng (2017) Choubey and Huang (2018) B3 51.47 52.78 51.47 52.65 52.62 50.16 51.67 CEAFe 47.96 49.7 47.96 49.35 49.63 48.59 49.1 KBP 2016 MUC 26.29 34.62 31.42 32.56 32.97 32.41 34.08 BLANC 30.82 34.49 32.89 33.69 34.07 32.72 34.08 AV G 39.13 42.9 40.94 42.06 42.32 40.97 42.23 B3 50.24 51.68 51.17 51.4 51.6"
2020.acl-main.478,N18-2055,1,0.78336,"l network classifier to make content type predictions. 6.2 Proposed Document-level Models Document LSTM adds the sentence-level BiLSTM over sentence representations obtained from the word-level BiLSTM to enrich sentence representations with local contextual information. +Document Encoding uses document encoding for modeling associations with the main topic and obtains the final sentence representations as described previously. +Headline replaces document encoding with headline sentence encoding generated from the wordlevel biLSTM. Headline is known to be a strong predictor for the main event (Choubey et al., 2018). CRF Fine-grained and CRF Coarse-grained adds a CRF layer to make content type predictions for sentences which models dependencies among fine-grained (eight content types) and coarse-grained (main vs. context-informing vs. supportive contents) content types respectively. 6.3 Implementation Details We set hidden states dimension to 512 for both word-level and sentence-level biLSTMs in all our models. Similarly, we use two-layered feed forward networks with 1024-512-1 units to calculate attention weights for both the BiLSTMs. The final classifier uses two-layer feed forward networks with 3072-1"
2020.acl-main.478,D18-1368,1,0.852282,"3) 141(6.3) Table 3: Distribution of Content type labels across media sources, with percentages shown within parentheses. vs. not speech). In the created corpus, 5535 out of 18,155 sentences are labeled as speech. 5 Document-level Neural Network Model for Discourse Profiling A wide range of computational models has been applied for extracting different forms of discourse structures. However, across several tasks, neural network methods (Ji and Eisenstein, 2015; Becker et al., 2017) are found the most effective, with relatively superior performance obtained by modeling discourse-level context (Dai and Huang, 2018a,b). As an initial attempt, we use a hierarchical neural network to derive sentence representations and a document encoding, and model associations between each sentence and the main topic of the document when determining content types for sentences. Shown in Figure 1, it first uses a wordlevel bi-LSTM layer (Hochreiter and Schmidhuber, 1997) with soft-attention over word representations to generate intermediate sentence representations which are further enriched with the context information using another sentence-level bi-LSTM. Enriched sentence representations are then averaged with their s"
2020.acl-main.478,N18-1013,1,0.883146,"3) 141(6.3) Table 3: Distribution of Content type labels across media sources, with percentages shown within parentheses. vs. not speech). In the created corpus, 5535 out of 18,155 sentences are labeled as speech. 5 Document-level Neural Network Model for Discourse Profiling A wide range of computational models has been applied for extracting different forms of discourse structures. However, across several tasks, neural network methods (Ji and Eisenstein, 2015; Becker et al., 2017) are found the most effective, with relatively superior performance obtained by modeling discourse-level context (Dai and Huang, 2018a,b). As an initial attempt, we use a hierarchical neural network to derive sentence representations and a document encoding, and model associations between each sentence and the main topic of the document when determining content types for sentences. Shown in Figure 1, it first uses a wordlevel bi-LSTM layer (Hochreiter and Schmidhuber, 1997) with soft-attention over word representations to generate intermediate sentence representations which are further enriched with the context information using another sentence-level bi-LSTM. Enriched sentence representations are then averaged with their s"
2020.acl-main.478,D08-1035,0,0.496944,"Missing"
2020.acl-main.478,P12-1007,0,0.0610047,"Missing"
2020.acl-main.478,W14-4921,0,0.0633311,"Missing"
2020.acl-main.478,P94-1002,0,0.886448,"atically construct news content structures. Finally, we demonstrate that incorporating system predicted news structures yields new state-of-theart performance for event coreference resolution. The news documents we annotated are openly available and the annotations are publicly released for future research1 . 1 Introduction Detecting and incorporating discourse structures is important for achieving text-level language understanding. Several well-studied discourse analysis tasks, such as RST (Mann and Thompson, 1988) and PDTB style (Prasad et al., 2008) discourse parsing and text segmentation (Hearst, 1994), generate rhetorical and content structures that have been shown useful for many NLP applications. But these widely applicable discourse structures overlook genre specialties. In this paper, we focus on studying content structures specific to news articles, a broadly studied text genre for many NLP tasks and applications. We believe that genre-specific discourse structures can effectively complement genre independent discourse structures and are essential for achieving deep story-level text understanding. What is in a news article? Normally, we expect a news article to describe well verified"
2020.acl-main.478,P14-1002,0,0.112372,"Missing"
2020.acl-main.478,Q15-1024,0,0.0297046,"(17.3) D2 197(3.4) 96(2.0) 316(4.3) 47(2.1) D3 1876(32.1) 953(19.5) 1867(25.2) 598(26.9) D4 532(9.1) 525(10.7) 924(12.5) 148(6.7) N/A 197(3.3) 736(15.0) 686(9.3) 141(6.3) Table 3: Distribution of Content type labels across media sources, with percentages shown within parentheses. vs. not speech). In the created corpus, 5535 out of 18,155 sentences are labeled as speech. 5 Document-level Neural Network Model for Discourse Profiling A wide range of computational models has been applied for extracting different forms of discourse structures. However, across several tasks, neural network methods (Ji and Eisenstein, 2015; Becker et al., 2017) are found the most effective, with relatively superior performance obtained by modeling discourse-level context (Dai and Huang, 2018a,b). As an initial attempt, we use a hierarchical neural network to derive sentence representations and a document encoding, and model associations between each sentence and the main topic of the document when determining content types for sentences. Shown in Figure 1, it first uses a wordlevel bi-LSTM layer (Hochreiter and Schmidhuber, 1997) with soft-attention over word representations to generate intermediate sentence representations whi"
2020.acl-main.478,P17-1092,0,0.0329297,"Missing"
2020.acl-main.478,N18-2075,0,0.0839722,"Missing"
2020.acl-main.478,D14-1220,0,0.0414483,"Missing"
2020.acl-main.478,P14-2047,0,0.0604871,"Missing"
2020.acl-main.478,P11-1100,0,0.0877525,"Missing"
2020.acl-main.478,W10-4327,0,0.0877911,"Missing"
2020.acl-main.478,P17-1009,0,0.0829367,"Missing"
2020.acl-main.478,N18-1202,0,0.0294844,"r enriched with the context information using another sentence-level bi-LSTM. Enriched sentence representations are then averaged with their soft-attention weights to generate document encoding. The final prediction layers model associations between the document encoding and each sentence encoding to predict sentence types. Context-aware sentence encoding: Let a document be a sequence of sentences {s1 , s2 ..sn }, which in turn are sequences of words {(w11 , w12 ..) .. (wn1 , wn2 , ..)}. We first transform a sequence of words in each sentence to contextualized word representations using ELMo (Peters et al., 2018) followed by a word-level biLSTM layer to obtain their hidden state representations Hs . Then, we take weighted sums of hidden representations using soft-attention scores to obtain intermediate senFigure 1: Neural-Network Architecture Incorporating Document Encoding for Content Type Classification tence encodings (Si ) that are uninformed of the contextual information. Therefore, we apply another sentence-level biLSTM over the sequence of sentence encodings to model interactions among sentences and smoothen context flow from the headline until the last sentence in a document. The hidden states"
2020.acl-main.478,P09-2004,0,0.0719949,"Missing"
2020.acl-main.478,prasad-etal-2008-penn,0,0.495448,"e propose several documentlevel neural-network models to automatically construct news content structures. Finally, we demonstrate that incorporating system predicted news structures yields new state-of-theart performance for event coreference resolution. The news documents we annotated are openly available and the annotations are publicly released for future research1 . 1 Introduction Detecting and incorporating discourse structures is important for achieving text-level language understanding. Several well-studied discourse analysis tasks, such as RST (Mann and Thompson, 1988) and PDTB style (Prasad et al., 2008) discourse parsing and text segmentation (Hearst, 1994), generate rhetorical and content structures that have been shown useful for many NLP applications. But these widely applicable discourse structures overlook genre specialties. In this paper, we focus on studying content structures specific to news articles, a broadly studied text genre for many NLP tasks and applications. We believe that genre-specific discourse structures can effectively complement genre independent discourse structures and are essential for achieving deep story-level text understanding. What is in a news article? Normal"
2020.acl-main.478,D16-1246,0,0.0442148,"Missing"
2020.acl-main.478,K16-2007,0,0.0278748,"Missing"
2020.acl-main.478,D10-1037,0,0.0978756,"Missing"
2020.acl-main.478,N19-1178,0,0.0417004,"Missing"
2020.acl-main.478,N03-1030,0,0.333129,"Missing"
2020.acl-main.478,E99-1015,0,0.404529,"Missing"
2020.acl-main.478,W09-3742,0,0.0837645,"Missing"
2020.acl-main.478,D18-1079,0,0.0206765,"Missing"
2020.emnlp-main.430,P19-1471,0,0.144824,"Missing"
2020.emnlp-main.430,araki-etal-2014-detecting,0,0.312589,"a wide range of event types. The acquired subevent knowledge has been shown useful for discourse analysis and identifying a range of event-event relations1 . 1 While being in high demand, little subevent knowledge can be found in existing knowledge bases. Therefore, we aim to extract subevent knowledge from text and build the first subevent knowledge base covering a large number of commonly seen events and their rich subevents. Little research has focused on identifying the subevent relation between two events in a text. Several datasets annotated with subevent relations (Glavaš et al., 2014; Araki et al., 2014; O’Gorman et al., 2016) exist, but they are extremely small and usually contain dozens to one/two hundred documents. Subevent relation classifiers trained on these small datasets are not suitable to use to extract subevent knowledge from text, considering that subevent relations can appear in dramatically different contexts depending on topics and events. Introduction A subevent is the event that happens as a part of the other event (i.e., parent event) spatio-temporally (Glavaš and Šnajder, 2014). Subevents, which elaborate and expand an event, widely exist in event descriptions. For instanc"
2020.emnlp-main.430,D16-1088,1,0.818216,"challenging cross-sentence cases where we usually have little contextual clues to rely on. Furthermore, when incorporated into a recent neural discourse 2 While subevents are also spatially contained by the parent event, we did not use this observation to identify candidate subevent relations because the spatial contained_by relation between two events is not frequently stated in text. 2 Related Work Subevent Knowledge Acquisition: Considering the generalizability issue of supervised contextual classifiers trained on small annotated data, our pilot research on subevent knowledge acquisition (Badgett and Huang, 2016) relies on heuristics, where we first identify sentences in news articles that are likely to contain subevents by exploiting a sentential pattern3 , and then, we extract subevent phrases from those sentences using a phrasal pattern4 . In addition, this pilot work does not aim to acquire the parent event together with subevents, instead, 3 Subevents often appear in sentences that start or end with characteristic phrases such as “media reports” and “witness said”. 4 Subevent phrases often occur together in conjunction constructions as a sequence of subevent phrases. 5346 Candidate Seed Pairs Con"
2020.emnlp-main.430,S13-2002,0,0.0290476,"5). Then, the trained contextual classifier will be used to identify new event pairs of the subevent relation by examining multiple occurrences of an event pair in text (Section 6). We use the English Gigaword (Napoles et al., 2012) as the text corpus. Identification and Acquisition of other Event Relations: Compared to relatively little research devoted to subevent identification and acquisition, significantly more research has been done for identifying and extracting several other types of event relations, especially temporal relations (Pustejovsky et al., 2003; Chklovski and Pantel, 2004; Bethard, 2013; Llorens et al., 2010; D’Souza and Ng, 2013; Chambers et al., 2014) and causal relations (Girju, 2003; Bethard and Martin, 2008; Riaz and Girju, 2010; Do et al., 2011; Riaz and Girju, 2013; Mirza and Tonelli, 2014, 2016). 4 3 Overview of the Weakly Supervised Approach Figure 1 shows the overview of the weakly supervised learning approach for subevent knowledge 4.1 Weak Supervision Seed Event Pair Identification We use six preposition patterns (i.e., during, in, amid, throughout, including, and within) to extract candidate seed event pairs. Specifically, we use dependency relations5 to recogni"
2020.emnlp-main.430,P08-2045,0,0.0431013,"mining multiple occurrences of an event pair in text (Section 6). We use the English Gigaword (Napoles et al., 2012) as the text corpus. Identification and Acquisition of other Event Relations: Compared to relatively little research devoted to subevent identification and acquisition, significantly more research has been done for identifying and extracting several other types of event relations, especially temporal relations (Pustejovsky et al., 2003; Chklovski and Pantel, 2004; Bethard, 2013; Llorens et al., 2010; D’Souza and Ng, 2013; Chambers et al., 2014) and causal relations (Girju, 2003; Bethard and Martin, 2008; Riaz and Girju, 2010; Do et al., 2011; Riaz and Girju, 2013; Mirza and Tonelli, 2014, 2016). 4 3 Overview of the Weakly Supervised Approach Figure 1 shows the overview of the weakly supervised learning approach for subevent knowledge 4.1 Weak Supervision Seed Event Pair Identification We use six preposition patterns (i.e., during, in, amid, throughout, including, and within) to extract candidate seed event pairs. Specifically, we use dependency relations5 to recognize preposition patterns, and extract the governor word and dependent word of each pattern. We then check whether both words are"
2020.emnlp-main.430,P19-1470,0,0.0224991,"vision (Section 4) Apply Identifying New Pairs ... a campaign in which ... by persuading voters ... Populate ... organized a campaign ... persuaded Congress ... ... after a disputed election ... would begin his term ... Candidate New Pairs Distill Contexts (Section 6) campaign, persuade New Pairs Figure 1: Overview of the Subevent Knowledge Acquisition System it learns a list of subevent phrases from documents that are known to describe a certain type of event. Specifically, in this work, we only acquired 610 subevent phrases for one type of parent event, civil unrest events. The recent work (Bosselut et al., 2019; Sap et al., 2019) uses generative language models to generate subevent knowledge among many other types of commonsense knowledge. We can potentially incorporate our learned subevent knowledge into a general event ontology to enrich subevent links in the ontology. For instance, the Rich Event Ontology (REO) (Brown et al., 2017) unifies two existing knowledge resources (i.e., FrameNet (Fillmore et al., 2003) and VerbNet (Kipper et al., 2008)) and two event annotated datasets (i.e., ACE (Doddington et al., 2004) and ERE (Song et al., 2015)) to allow users to query multiple linguistic resources"
2020.emnlp-main.430,W17-2712,0,0.0162615,"Knowledge Acquisition System it learns a list of subevent phrases from documents that are known to describe a certain type of event. Specifically, in this work, we only acquired 610 subevent phrases for one type of parent event, civil unrest events. The recent work (Bosselut et al., 2019; Sap et al., 2019) uses generative language models to generate subevent knowledge among many other types of commonsense knowledge. We can potentially incorporate our learned subevent knowledge into a general event ontology to enrich subevent links in the ontology. For instance, the Rich Event Ontology (REO) (Brown et al., 2017) unifies two existing knowledge resources (i.e., FrameNet (Fillmore et al., 2003) and VerbNet (Kipper et al., 2008)) and two event annotated datasets (i.e., ACE (Doddington et al., 2004) and ERE (Song et al., 2015)) to allow users to query multiple linguistic resources and combine event annotations. However, REO contains few subevent relation links between events. acquisition. The key of this approach is to identify seed event pairs that are likely to be of the subevent relation in a two-step procedure (Section 4). We first use several temporal relation patterns (e.g., ei during ej ) to identi"
2020.emnlp-main.430,W18-4306,0,0.0234925,"see that compared to the basic BERT classifier, incorporating learned subevent knowledge achieves better performance on both datasets, for both intra-sentence and cross-sentence cases. 8.2 Temporal and Causal Relation Identification Subevents indicate how an event emerges and develops, and therefore, the learned subevent knowledge can further be used to identify other semantic relations between events, such as temporal and causal relations. For evaluation, we use the same RED 17 dataset plus two more datasets, TimeBank v1.218 (Pustejovsky et al., 2003) and Event Storyline Corpus (ESC) v1.519 (Caselli and Inel, 2018), 15 The BiLSTM has the hidden size of 50 and uses maxpooling to encode an event phrase. 16 We trained TransE for 20 iterations. 17 RED has 1104 (1010) intra-sentence and 182 (119) crosssentence temporal (causal) relations. We consider all the annotated event-event relations in RED other than temporal (causal) relations as others. 18 TimeBank has 1,122 intra-sentence and 247 crosssentence “before/after” temporal relations. We consider all the annotated event-event relations in TimeBank other than “before/after” relations as others. 19 ESC has 1,649 intra-sentence and 3,952 cross-sentence causa"
2020.emnlp-main.430,Q14-1022,0,0.0239876,"o identify new event pairs of the subevent relation by examining multiple occurrences of an event pair in text (Section 6). We use the English Gigaword (Napoles et al., 2012) as the text corpus. Identification and Acquisition of other Event Relations: Compared to relatively little research devoted to subevent identification and acquisition, significantly more research has been done for identifying and extracting several other types of event relations, especially temporal relations (Pustejovsky et al., 2003; Chklovski and Pantel, 2004; Bethard, 2013; Llorens et al., 2010; D’Souza and Ng, 2013; Chambers et al., 2014) and causal relations (Girju, 2003; Bethard and Martin, 2008; Riaz and Girju, 2010; Do et al., 2011; Riaz and Girju, 2013; Mirza and Tonelli, 2014, 2016). 4 3 Overview of the Weakly Supervised Approach Figure 1 shows the overview of the weakly supervised learning approach for subevent knowledge 4.1 Weak Supervision Seed Event Pair Identification We use six preposition patterns (i.e., during, in, amid, throughout, including, and within) to extract candidate seed event pairs. Specifically, we use dependency relations5 to recognize preposition patterns, and extract the governor word and dependent"
2020.emnlp-main.430,W04-3205,0,0.201367,"relation classifier (Section 5). Then, the trained contextual classifier will be used to identify new event pairs of the subevent relation by examining multiple occurrences of an event pair in text (Section 6). We use the English Gigaword (Napoles et al., 2012) as the text corpus. Identification and Acquisition of other Event Relations: Compared to relatively little research devoted to subevent identification and acquisition, significantly more research has been done for identifying and extracting several other types of event relations, especially temporal relations (Pustejovsky et al., 2003; Chklovski and Pantel, 2004; Bethard, 2013; Llorens et al., 2010; D’Souza and Ng, 2013; Chambers et al., 2014) and causal relations (Girju, 2003; Bethard and Martin, 2008; Riaz and Girju, 2010; Do et al., 2011; Riaz and Girju, 2013; Mirza and Tonelli, 2014, 2016). 4 3 Overview of the Weakly Supervised Approach Figure 1 shows the overview of the weakly supervised learning approach for subevent knowledge 4.1 Weak Supervision Seed Event Pair Identification We use six preposition patterns (i.e., during, in, amid, throughout, including, and within) to extract candidate seed event pairs. Specifically, we use dependency relati"
2020.emnlp-main.430,D19-1295,1,0.84284,"s noticeably improved the system performance on both datasets. This is true for both temporal relations and causal relations. Overall, the systems achieved the best performance when using the event embedding approach to incorporate subevent knowledge. 8.3 Implicit Discourse Relation Classification We expect subevent knowledge to be useful for classifying discourse relations between two text units in general because subevent descriptions often elaborate and provide a continued discussion of a parent event introduced earlier in text. For experiments, we used our recent discourse parsing system (Dai and Huang, 2019) that easily incorporates external event knowledge as a regularizer into a two-level hierarchical BiLSTM model (Base Model) for paragraph-level discourse parsing. The experimental setting is exactly the same as in (Dai and Huang, 2019). Table 3 reports the performance of implicit discourse relation classification on PDTB 2.0 (Prasad et al., 2008). Incorporating the acquired subevent pairs (239K) into the Base Model improves the overall macro-average F1-score and accuracy by 2.0 and 2.6 points respectively, which is non-trivial considering the challenges of implicit discourse relation identific"
2020.emnlp-main.430,N19-1423,0,0.0988807,"e optimal number of clusters, with no pre-defined number of clusters needed. Event clusters often become stable soon after 50 iterations, to ensure convergence, we ran the algorithm for 60 iterations. After performing the semantic consistency 5348 6 We used word2vec word embeddings. check, we retained around 30K seed event pairs. We find occurrences of these event pairs in the Gigaword corpus and obtained around 388K7 sentences containing an event pair. These sentences will be used as positive instances to train the contextual classifier. 5 The Contextual Classifier Using BERT Recently, BERT (Devlin et al., 2019) pretrained on massive data has achieved high performance on various NLP tasks. We fine-tune a pretrained BERT model to build the contextual classifier for subevent relation identification. BERT model is essentially a bi-directional Transformer-based encoder that consists of multiple layers where each layer has multiple attention heads. Formally, given a sentence with N tokens, each attention head transforms a token vector ti into query, key, and value vectors qi , ki , vi through three linear transformers. Next, for each token, the head calculates the self-attention scores for all other token"
2020.emnlp-main.430,D11-1027,0,0.0340348,"text (Section 6). We use the English Gigaword (Napoles et al., 2012) as the text corpus. Identification and Acquisition of other Event Relations: Compared to relatively little research devoted to subevent identification and acquisition, significantly more research has been done for identifying and extracting several other types of event relations, especially temporal relations (Pustejovsky et al., 2003; Chklovski and Pantel, 2004; Bethard, 2013; Llorens et al., 2010; D’Souza and Ng, 2013; Chambers et al., 2014) and causal relations (Girju, 2003; Bethard and Martin, 2008; Riaz and Girju, 2010; Do et al., 2011; Riaz and Girju, 2013; Mirza and Tonelli, 2014, 2016). 4 3 Overview of the Weakly Supervised Approach Figure 1 shows the overview of the weakly supervised learning approach for subevent knowledge 4.1 Weak Supervision Seed Event Pair Identification We use six preposition patterns (i.e., during, in, amid, throughout, including, and within) to extract candidate seed event pairs. Specifically, we use dependency relations5 to recognize preposition patterns, and extract the governor word and dependent word of each pattern. We then check whether both words are event triggering words, and try to atta"
2020.emnlp-main.430,doddington-etal-2004-automatic,0,0.137782,"0 subevent phrases for one type of parent event, civil unrest events. The recent work (Bosselut et al., 2019; Sap et al., 2019) uses generative language models to generate subevent knowledge among many other types of commonsense knowledge. We can potentially incorporate our learned subevent knowledge into a general event ontology to enrich subevent links in the ontology. For instance, the Rich Event Ontology (REO) (Brown et al., 2017) unifies two existing knowledge resources (i.e., FrameNet (Fillmore et al., 2003) and VerbNet (Kipper et al., 2008)) and two event annotated datasets (i.e., ACE (Doddington et al., 2004) and ERE (Song et al., 2015)) to allow users to query multiple linguistic resources and combine event annotations. However, REO contains few subevent relation links between events. acquisition. The key of this approach is to identify seed event pairs that are likely to be of the subevent relation in a two-step procedure (Section 4). We first use several temporal relation patterns (e.g., ei during ej ) to identify candidate seed pairs since a child event is usually temporally contained by its parent event; and then, we conduct a definitionguided semantic consistency check to remove spurious sub"
2020.emnlp-main.430,N13-1112,0,0.053133,"Missing"
2020.emnlp-main.430,W03-1210,0,0.209226,"lation by examining multiple occurrences of an event pair in text (Section 6). We use the English Gigaword (Napoles et al., 2012) as the text corpus. Identification and Acquisition of other Event Relations: Compared to relatively little research devoted to subevent identification and acquisition, significantly more research has been done for identifying and extracting several other types of event relations, especially temporal relations (Pustejovsky et al., 2003; Chklovski and Pantel, 2004; Bethard, 2013; Llorens et al., 2010; D’Souza and Ng, 2013; Chambers et al., 2014) and causal relations (Girju, 2003; Bethard and Martin, 2008; Riaz and Girju, 2010; Do et al., 2011; Riaz and Girju, 2013; Mirza and Tonelli, 2014, 2016). 4 3 Overview of the Weakly Supervised Approach Figure 1 shows the overview of the weakly supervised learning approach for subevent knowledge 4.1 Weak Supervision Seed Event Pair Identification We use six preposition patterns (i.e., during, in, amid, throughout, including, and within) to extract candidate seed event pairs. Specifically, we use dependency relations5 to recognize preposition patterns, and extract the governor word and dependent word of each pattern. We then che"
2020.emnlp-main.430,W14-3705,0,0.639145,"between two events in a text. Several datasets annotated with subevent relations (Glavaš et al., 2014; Araki et al., 2014; O’Gorman et al., 2016) exist, but they are extremely small and usually contain dozens to one/two hundred documents. Subevent relation classifiers trained on these small datasets are not suitable to use to extract subevent knowledge from text, considering that subevent relations can appear in dramatically different contexts depending on topics and events. Introduction A subevent is the event that happens as a part of the other event (i.e., parent event) spatio-temporally (Glavaš and Šnajder, 2014). Subevents, which elaborate and expand an event, widely exist in event descriptions. For instance, when describing election events, people usually describe typical subevents such as “nominate candidates”, “debates” and “people vote”. Knowing typical subevents of an event can help with analyzing several discourse relations (such as expansion and temporal relations) between text units. Furthermore, knowing typical 1 Code and the knowledge base are available at https://github.com/wenlinyao/ EMNLP20-SubeventAcquisition We propose to conduct weakly supervised learning and train a wide-coverage con"
2020.emnlp-main.430,glavas-etal-2014-hieve,0,0.420128,"accuracy) and cover a wide range of event types. The acquired subevent knowledge has been shown useful for discourse analysis and identifying a range of event-event relations1 . 1 While being in high demand, little subevent knowledge can be found in existing knowledge bases. Therefore, we aim to extract subevent knowledge from text and build the first subevent knowledge base covering a large number of commonly seen events and their rich subevents. Little research has focused on identifying the subevent relation between two events in a text. Several datasets annotated with subevent relations (Glavaš et al., 2014; Araki et al., 2014; O’Gorman et al., 2016) exist, but they are extremely small and usually contain dozens to one/two hundred documents. Subevent relation classifiers trained on these small datasets are not suitable to use to extract subevent knowledge from text, considering that subevent relations can appear in dramatically different contexts depending on topics and events. Introduction A subevent is the event that happens as a part of the other event (i.e., parent event) spatio-temporally (Glavaš and Šnajder, 2014). Subevents, which elaborate and expand an event, widely exist in event descr"
2020.emnlp-main.430,S10-1063,0,0.0475087,"trained contextual classifier will be used to identify new event pairs of the subevent relation by examining multiple occurrences of an event pair in text (Section 6). We use the English Gigaword (Napoles et al., 2012) as the text corpus. Identification and Acquisition of other Event Relations: Compared to relatively little research devoted to subevent identification and acquisition, significantly more research has been done for identifying and extracting several other types of event relations, especially temporal relations (Pustejovsky et al., 2003; Chklovski and Pantel, 2004; Bethard, 2013; Llorens et al., 2010; D’Souza and Ng, 2013; Chambers et al., 2014) and causal relations (Girju, 2003; Bethard and Martin, 2008; Riaz and Girju, 2010; Do et al., 2011; Riaz and Girju, 2013; Mirza and Tonelli, 2014, 2016). 4 3 Overview of the Weakly Supervised Approach Figure 1 shows the overview of the weakly supervised learning approach for subevent knowledge 4.1 Weak Supervision Seed Event Pair Identification We use six preposition patterns (i.e., during, in, amid, throughout, including, and within) to extract candidate seed event pairs. Specifically, we use dependency relations5 to recognize preposition pattern"
2020.emnlp-main.430,P14-5010,0,0.00247317,"erns (i.e., during, in, amid, throughout, including, and within) to extract candidate seed event pairs. Specifically, we use dependency relations5 to recognize preposition patterns, and extract the governor word and dependent word of each pattern. We then check whether both words are event triggering words, and try to attach an argument to an event word to form an event phrase that tends to be more expressive and self-contained than a single event word, e.g., sign agreement vs sign, or, attack on troops vs attack. We consider both verb event phrases and 5 We use Stanford dependency relations (Manning et al., 2014), e.g., prep_during. 5347 noun event phrases (Appendix A provides more details). We further require that at least one argument is included in an event pair which may be attached to the first or the second event. In other words, we do not consider event pairs in which neither event has an argument. To select seed subevent pairs, we consider event pairs that co-occur with at least two different patterns for at least three times. In this way, we identified around 43K candidate seed pairs from the Gigaword corpus. However, many candidate seed pairs identified by the preposition patterns only have"
2020.emnlp-main.430,C14-1198,0,0.0981608,"gaword (Napoles et al., 2012) as the text corpus. Identification and Acquisition of other Event Relations: Compared to relatively little research devoted to subevent identification and acquisition, significantly more research has been done for identifying and extracting several other types of event relations, especially temporal relations (Pustejovsky et al., 2003; Chklovski and Pantel, 2004; Bethard, 2013; Llorens et al., 2010; D’Souza and Ng, 2013; Chambers et al., 2014) and causal relations (Girju, 2003; Bethard and Martin, 2008; Riaz and Girju, 2010; Do et al., 2011; Riaz and Girju, 2013; Mirza and Tonelli, 2014, 2016). 4 3 Overview of the Weakly Supervised Approach Figure 1 shows the overview of the weakly supervised learning approach for subevent knowledge 4.1 Weak Supervision Seed Event Pair Identification We use six preposition patterns (i.e., during, in, amid, throughout, including, and within) to extract candidate seed event pairs. Specifically, we use dependency relations5 to recognize preposition patterns, and extract the governor word and dependent word of each pattern. We then check whether both words are event triggering words, and try to attach an argument to an event word to form an even"
2020.emnlp-main.430,C16-1007,0,0.0377751,"Missing"
2020.emnlp-main.430,W12-3018,0,0.0606155,"Missing"
2020.emnlp-main.430,W16-5706,0,0.174023,"Missing"
2020.emnlp-main.430,W15-0812,0,0.0252615,"parent event, civil unrest events. The recent work (Bosselut et al., 2019; Sap et al., 2019) uses generative language models to generate subevent knowledge among many other types of commonsense knowledge. We can potentially incorporate our learned subevent knowledge into a general event ontology to enrich subevent links in the ontology. For instance, the Rich Event Ontology (REO) (Brown et al., 2017) unifies two existing knowledge resources (i.e., FrameNet (Fillmore et al., 2003) and VerbNet (Kipper et al., 2008)) and two event annotated datasets (i.e., ACE (Doddington et al., 2004) and ERE (Song et al., 2015)) to allow users to query multiple linguistic resources and combine event annotations. However, REO contains few subevent relation links between events. acquisition. The key of this approach is to identify seed event pairs that are likely to be of the subevent relation in a two-step procedure (Section 4). We first use several temporal relation patterns (e.g., ei during ej ) to identify candidate seed pairs since a child event is usually temporally contained by its parent event; and then, we conduct a definitionguided semantic consistency check to remove spurious subevent pairs that are semanti"
2020.emnlp-main.430,prasad-etal-2008-penn,0,0.0426992,"ssifying discourse relations between two text units in general because subevent descriptions often elaborate and provide a continued discussion of a parent event introduced earlier in text. For experiments, we used our recent discourse parsing system (Dai and Huang, 2019) that easily incorporates external event knowledge as a regularizer into a two-level hierarchical BiLSTM model (Base Model) for paragraph-level discourse parsing. The experimental setting is exactly the same as in (Dai and Huang, 2019). Table 3 reports the performance of implicit discourse relation classification on PDTB 2.0 (Prasad et al., 2008). Incorporating the acquired subevent pairs (239K) into the Base Model improves the overall macro-average F1-score and accuracy by 2.0 and 2.6 points respectively, which is non-trivial considering the challenges of implicit discourse relation identification. The performance improvements are noticeable on both the expansion relation and the temporal relation categories. among all the annotated event mentions in ESC and consider all the mention pairs that were not annotated with the causal relation as others. In this way, we generated 4.1K intra-sentence and 34K cross-sentence event mention pair"
2020.emnlp-main.611,J08-4004,0,0.0281721,"to provide more specific judgments than just true paraphrase or not. The annotation criteria are presented in Table 4: Completely equivalent (3), Mostly equivalent (2), Roughly equivalent (1), and Not equivalent (0). Labels of 3 and 2 are considered paraphrases, while 0 and 1 are non-paraphrases. 4.3 Annotation Quality Control Annotators are asked to carefully read the annotation criteria before starting annotations. Each pair is randomly assigned to three annotators; the final ground-truth is decided by majority vote. We evaluate annotation quality of each annotator via Cohen’s Kappa score (Artstein and Poesio, 2008) against the ground-truth. The average Cohen’s Kappa score of the annotators is 0.65. Following Lan et al. (2017), we re-assign the data instances that were assigned to 2 annotators with low annotation quality (Cohen’s Kappa score<0.4) to the best 5 annotators (Cohen’s Kappa score>0.75) and ask them to re-label (give labels without seeing old labels) these data instances. 7576 5 Experiments In this section, we present experiments that aim to answer the following research questions (RQs): • RQ1: How do BERT and other neural models perform on PARADE? Better or worse than their performance on tra"
2020.emnlp-main.611,W03-1004,0,0.156315,"s encouraging, and suggests the need for further enhancements in incorporating domain knowledge into NLP models. 2 Related Work Framework for Collecting Paraphrases: The basic idea of collecting a paraphrase dataset is to connect parallel data that are related to the same reference, like different news articles reporting the same event (MRPC) (Dolan and Brockett, 2005; Dolan et al., 2004), multiple descriptions of the same video clip (Chen and Dolan, 2011), multiple phrasal paraphrases on the web to describe the same concept (Hashimoto et al., 2011), different translations of a foreign novel (Barzilay and Elhadad, 2003), and multiple tweets that relate to the same topic (Xu et al., 2014) or contain the same URL (Lan et al., 2017). In this paper, we propose a novel framework to collect sentential paraphrases from online usergenerated flashcards, where different definitions (on the back of flashcards) of the same entity (on the front of flashcards) are probably paraphrases. The main advantage of this framework is that it can easily collect domain-specific paraphrases. Since flashcard websites like Quizlet are mainly used by students to prepare for quizzes and exams, these flashcards are often organized by subj"
2020.emnlp-main.611,D19-1371,0,0.0541412,"Missing"
2020.emnlp-main.611,N18-3011,0,0.019781,"Missing"
2020.emnlp-main.611,P17-1152,0,0.309918,"ecifically, we require a dataset of paraphrases that overlap very little but are semantically equivalent, and of nonparaphrases that have overlap greatly but are not semantically equivalent based on computer science domain knowledge. Correspondingly, there is a research gap in understanding if modern neural models can achieve exemplary performance on such a dataset, especially in comparison with existing paraphrase identification datasets (that lack such specialized domain knowledge). In sum, this paper makes four contributions: Recent neural models (Nie and Bansal, 2017; Parikh et al., 2016; Chen et al., 2017) that go beyond traditional approaches based on lexical and syntactic features have demonstrated state-of-theart performance on paraphrase identification. For example, BERT and its variants (Devlin et al., 2018; Liu et al., 2019; Yang et al., 2019; Lan et al., 2019; Raffel et al., 2019) have achieved the best results on the General Language Understanding Evaluation (GLUE) benchmark (Wang et al., 2018) on two paraphrase identification datasets: the Microsoft Research Paraphrase Corpus (MRPC) and Quora Question Pairs (QQP). Using massive pre-training data and a flexible bidirectional self-attent"
2020.emnlp-main.611,P09-1053,0,0.0532455,"g (Bhagat and Hovy, 2013). Automatically identifying paraphrases and non-paraphrases has proven useful for a wide range of natural language processing (NLP) applications, including question answering, semantic parsing, information extraction, machine translation, textual entailment, and semantic textual similarity. Paraphrase identification (PI) is typically formalized as a binary classification problem: given two sentences, determine if they roughly express the same meaning. Traditional paraphrase identification approaches (Mihalcea et al., 2006; Kozareva and Montoyo, 2006; Wan et al., 2006; Das and Smith, 2009; Xu et al., 2014) mainly rely on lexical and syntactic overlap features to measure the 1 https://github.com/heyunh2015/PARADE_ dataset Table 1: Examples of paraphrases and non-paraphrases from the computer science domain. Judgments are made based on domain knowledge rather than lexical or syntactic features. Overlapping words (other than stop-words) are in bold and key different words are underlined. However, these shallow lexical and syntactic overlap features may not effectively capture the domain-specific semantics of the two sentences. A typical situation where models based on these overl"
2020.emnlp-main.611,D19-1109,0,0.0567743,"example, BERT and its variants (Devlin et al., 2018; Liu et al., 2019; Yang et al., 2019; Lan et al., 2019; Raffel et al., 2019) have achieved the best results on the General Language Understanding Evaluation (GLUE) benchmark (Wang et al., 2018) on two paraphrase identification datasets: the Microsoft Research Paraphrase Corpus (MRPC) and Quora Question Pairs (QQP). Using massive pre-training data and a flexible bidirectional self-attention mechanism, BERT and its variants are able to better model the semantic relationship between sentences. Moreover, two recent studies (Petroni et al., 2019; Davison et al., 2019) observe that BERT without fine-tuning can even capture world knowledge and can answer factual questions like “place of birth” and “who developed the theory of relativity.” Naturally, we are curious to know if these neural models can correctly identify paraphrases that require specialized domain knowledge like the examples shown in Table 1. Hence, our overarching research goal is to create new datasets and enable new models for highquality paraphrase identification based on domain knowledge. Because previous paraphrase datasets (Dolan and Brockett, 2005; Dolan et al., 2004; Xu et al., 2014; La"
2020.emnlp-main.611,C04-1051,0,0.4048,"oni et al., 2019; Davison et al., 2019) observe that BERT without fine-tuning can even capture world knowledge and can answer factual questions like “place of birth” and “who developed the theory of relativity.” Naturally, we are curious to know if these neural models can correctly identify paraphrases that require specialized domain knowledge like the examples shown in Table 1. Hence, our overarching research goal is to create new datasets and enable new models for highquality paraphrase identification based on domain knowledge. Because previous paraphrase datasets (Dolan and Brockett, 2005; Dolan et al., 2004; Xu et al., 2014; Lan et al., 2017; Iyer et al., 2017; Zhang et al., 2019) were not originally designed and constructed from the perspective of domain knowledge, to date there is no such dataset that requires specialized domain knowledge to discern the quality of two candidate sentences as para7573 • First, we propose a novel extensible framework for inexpensively collecting domainspecific sentential candidate paraphrases that are characterized by specialized knowledge. The key idea is to leverage large-scale online collections of user-generated flashcards. We treat definitions on each flashc"
2020.emnlp-main.611,I05-5002,0,0.742564,", two recent studies (Petroni et al., 2019; Davison et al., 2019) observe that BERT without fine-tuning can even capture world knowledge and can answer factual questions like “place of birth” and “who developed the theory of relativity.” Naturally, we are curious to know if these neural models can correctly identify paraphrases that require specialized domain knowledge like the examples shown in Table 1. Hence, our overarching research goal is to create new datasets and enable new models for highquality paraphrase identification based on domain knowledge. Because previous paraphrase datasets (Dolan and Brockett, 2005; Dolan et al., 2004; Xu et al., 2014; Lan et al., 2017; Iyer et al., 2017; Zhang et al., 2019) were not originally designed and constructed from the perspective of domain knowledge, to date there is no such dataset that requires specialized domain knowledge to discern the quality of two candidate sentences as para7573 • First, we propose a novel extensible framework for inexpensively collecting domainspecific sentential candidate paraphrases that are characterized by specialized knowledge. The key idea is to leverage large-scale online collections of user-generated flashcards. We treat defini"
2020.emnlp-main.611,P11-1109,0,0.0833302,"Missing"
2020.emnlp-main.611,N16-1108,0,0.0948008,"ed; words were stemmed. 5.1 Experimental Setup We first introduce our experimental setup here, including paraphrase identification models, other PI datasets and their partition and reproducibility. Models for Binary Paraphrase Identification: We test seven different approaches on PARADE. The Decomposable Attention Model (DecAtt, 380K parameters) (Parikh et al., 2016) is one of the earliest models to apply attention for modeling sentence pairs. It computes the word pair interaction between the two sentences in a candidate paraphrase. The Pairwise Word Interaction Model (PWIM, 2.2M parameters) (He and Lin, 2016) uses Bi-LSTM to model the context of each word and then uses cosine similarity, Euclidean distance and dot product together to model word pair interactions. The Enhanced Sequential Inference Model (ESIM, 7.7M parameters) (Chen et al., 2017) first encodes sentences by using Bi-LSTM and then also calculates the word pair interaction between the two sentences like DecAtt. The Shortcut-Stacked Sentence Encoder (SSE, 140M parameters) (Nie and Bansal, 2017) applies a stacked Bi-LSTM with skip connections as the sentence encoder. Recently, the Bidirectional Encoder Representations from Transformer ("
2020.emnlp-main.611,D17-1126,0,0.139588,"Missing"
2020.emnlp-main.611,C18-1328,0,0.125414,"set at GLUE as the test set and sample another part from its training set as the validation set. Details of these previous PI datasets can be found in Section 2. Reproducibility: PARADE and its split in this paper is released.11 For BERT, we use a widely used pytorch implementation12 and Adam optimizer with batch size 32 and learning rate 2e-5. We fine-tuned BERT for 20 epochs. We selected the BERRT hyper-parameters from the range as recommended in Devlin et al. (2018) and based on the performance in terms of F1 on the validation set. The implementations13 of the other neural models are from Lan and Xu (2018), and we use the same hyper-parameters as recommended by Lan and Xu (2018). 5.2 RQ1: Paraphrase Identification Comparison We first present the performance of BERT-large on PARADE and previous PI datasets in Table 6. Compared to datasets that lack domain knowledge, we observe that BERT yields the lowest performance on PARADE across all metrics. For example, BERT obtains 0.709 in terms of F1, which 6 https://gluebenchmark.com/tasks https://github.com/ google-research-datasets/paws 8 https://github.com/lanwuwei/ Twitter-URL-Corpus 9 https://cocoxu.github.io/ #publications 10 https://gluebenchmark"
2020.emnlp-main.611,2021.ccl-1.108,0,0.161745,"Missing"
2020.emnlp-main.611,D19-5307,0,0.0330919,"Missing"
2020.emnlp-main.611,W17-5308,0,0.196771,"Missing"
2020.emnlp-main.611,D16-1244,0,0.189466,"Missing"
2020.emnlp-main.611,P15-2010,0,0.0695365,"Missing"
2020.emnlp-main.611,D19-1250,0,0.0575959,"Missing"
2020.emnlp-main.611,U06-1019,0,0.225805,"g different wording (Bhagat and Hovy, 2013). Automatically identifying paraphrases and non-paraphrases has proven useful for a wide range of natural language processing (NLP) applications, including question answering, semantic parsing, information extraction, machine translation, textual entailment, and semantic textual similarity. Paraphrase identification (PI) is typically formalized as a binary classification problem: given two sentences, determine if they roughly express the same meaning. Traditional paraphrase identification approaches (Mihalcea et al., 2006; Kozareva and Montoyo, 2006; Wan et al., 2006; Das and Smith, 2009; Xu et al., 2014) mainly rely on lexical and syntactic overlap features to measure the 1 https://github.com/heyunh2015/PARADE_ dataset Table 1: Examples of paraphrases and non-paraphrases from the computer science domain. Judgments are made based on domain knowledge rather than lexical or syntactic features. Overlapping words (other than stop-words) are in bold and key different words are underlined. However, these shallow lexical and syntactic overlap features may not effectively capture the domain-specific semantics of the two sentences. A typical situation where models"
2020.emnlp-main.611,W18-5446,0,0.0713888,"Missing"
2020.emnlp-main.611,S15-2001,0,0.0420122,"Missing"
2020.emnlp-main.611,Q14-1034,0,0.719156,"013). Automatically identifying paraphrases and non-paraphrases has proven useful for a wide range of natural language processing (NLP) applications, including question answering, semantic parsing, information extraction, machine translation, textual entailment, and semantic textual similarity. Paraphrase identification (PI) is typically formalized as a binary classification problem: given two sentences, determine if they roughly express the same meaning. Traditional paraphrase identification approaches (Mihalcea et al., 2006; Kozareva and Montoyo, 2006; Wan et al., 2006; Das and Smith, 2009; Xu et al., 2014) mainly rely on lexical and syntactic overlap features to measure the 1 https://github.com/heyunh2015/PARADE_ dataset Table 1: Examples of paraphrases and non-paraphrases from the computer science domain. Judgments are made based on domain knowledge rather than lexical or syntactic features. Overlapping words (other than stop-words) are in bold and key different words are underlined. However, these shallow lexical and syntactic overlap features may not effectively capture the domain-specific semantics of the two sentences. A typical situation where models based on these overlap features may fa"
2020.emnlp-main.611,W16-2343,0,0.0564951,"Missing"
2020.emnlp-main.611,N19-1131,0,0.0383938,"Missing"
2020.lrec-1.732,N19-1423,0,0.0137976,"lossΘ = −(log(Ts ) + X (1 − log(fs ))) fs ∈Fs 5981 2. Single ClassifierBiLST M +Gloss : a variation of the shared model, with no regularization based on sense examples. using ADAM (Kingma and Ba, 2014) optimizer with learning rate set to 0.0001. For regularization, we use dropout rate (Srivastava et al., 2014) of 0.5 on the output activations of all encoders and neural layers. We use pre-trained word embeddings which are kept fixed during training. To ensure fair comparison with previous works, we evaluate our system using GloVe (Pennington et al., 2014), ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019) word embeddings. Training runs for 20 epochs for models with GloVe embeddings and 6 epochs for models with ELMo and BERT embeddings. In addition, we implemented our own word-specific model, Word-Specific ClassifierBiLST M +Gloss , which is equivalent to the Single ClassifierBiLST M +Gloss , except that the word-specific model uses word-specific sense prediction layers. This model can be seen as a simplified implementation of the previous system GAS (Luo et al., 2018b) after ignoring hyper- and hypo-nyms and memory modules. 4.3. 4.5. Baseline Systems We compare our proposed model with a heuris"
2020.lrec-1.732,S01-1001,0,0.25611,"grow linearly with the number of ambiguous words. 4. 4.1. Evaluation Dataset Training: We use the SemCor (Miller et al., 1993) dataset for training our neural network. SemCor is the largest manually annotated English corpus for word sense disambiguation. It consists of 352 documents from the Brown corpus with 226,036 sense annotations based on WordNet 1.6 (Miller et al., 1990) which was later mapped to WordNet 3.0 (Raganato et al., 2017b). Validation and Evaluation: We evaluate our models on the benchmark fine-grained English all-words WSD dataset that includes test datasets from Senseval-2 (Edmonds and Cotton, 2001), Senseval-3 (Mihalcea et al., 2004), SemEval-2007 (Pradhan et al., 2007), SemEval 2013 (Navigli et al., 2013) and SemEval-2015 (Moro and Navigli, 2015). All these test sets were originally annotated with different versions of WordNet senses which were later standardized to WordNet 3.0 by Raganato et al. (2017b). Since both the training and test datasets are mapped to WordNet 3.0, we use WordNet 3.0 for extracting sense definitions and an example sentence for each sense, if available. Also, following the previous work on supervised WSD (Luo et al., 2018a; Luo et al., 2018b; Raganato et al., 20"
2020.lrec-1.732,P16-1085,0,0.0125529,"annotationlean words. 2. Related Work Knowledge-based word sense disambiguation approaches (Lesk, 1986; Agirre et al., 2014; Basile et al., 2014) rely on sense definitions and lexico-semantic resources to measure overlap between sense representations and a word context, which is flexible to handle infrequent words. However, in the absence of any supervision, knowledge-based approaches may suffer from a mismatch between sense representations and a word context and their performance consistently falls behind the data-driven approaches. Data-driven methods (Zhong and Ng, 2010; Shen et al., 2013; Iacobacci et al., 2016) mostly build word specific classifiers (word-experts). Recent neural network models (Raganato et al., 2017a; K˚ageb¨ack and Salomonsson, 2016; Vial et al., 2018) use common first layers for all words and learn generalized low-level context representations. However, while using common context encoding layers, neural network models build word specific sense prediction layers and none of them completely pull out from the word expert notion. The crucial limitation of word expert models is their lack of capability of generalizing across words and their senses. In line with the recent neural networ"
2020.lrec-1.732,W16-5307,0,0.0564262,"Missing"
2020.lrec-1.732,kipper-etal-2006-extending,0,0.0647287,"Missing"
2020.lrec-1.732,P19-1568,0,0.198146,"al. (2019) merges senses across words guided by the hypernym-hyponym relation in WordNet to ease the problem of sparsity, but the fundamental generalization issue that results from word specific sense prediction layers remains. The proposed shared model for word sense disambiguation combines the best of both types of approaches. Specifically, by utilizing both the available sense-annotated data and knowledge resources, the sense-context resemblance model learns to generate and attend to word sense representations for disambiguating a word context. Similar to our approach, a concurrent work by Kumar et al. (2019) has also shown the benefit of learning sense embeddings in a shared model with common parameters for zero-shot WSD. In addition, we study how correlated and uncorrelated word senses, identified based on VerbNet, contribute to the improvement in the model’s performance on annotation-lean word senses. 3. A Shared Neural Network Model for Word Sense Disambiguation Our approach for word sense disambiguation measures the appropriateness of a word sense in a word context through a unified neural network that uses the same set of parameters for all the words and senses. Specifically, as shown in Fig"
2020.lrec-1.732,W02-0109,0,0.174591,"Missing"
2020.lrec-1.732,D18-1170,0,0.123031,"hare two related senses depending on whether the argument refers to concrete objects or abstract concepts. Modeling systematic polysemy (Pustejovsky, 1995; Utt and Pad´o, 2011) and accurately identifying words with similar senses is challenging though, which is not our focus here. Instead, we aim to directly address the limitation of word-specific classifiers for WSD that completely isolate a word from others and build a single classifier for No. F1 1-2 13.8 3-5 39.7 6-10 51 11-40 63.9 41-70 86.1 71-200 89.1 ≥ 200 93 Table 1: Performance of the state-of-the-art word-specific classifier model (Luo et al., 2018b) on word senses with a different number of training instances. The model was trained and evaluated on standard WSD training and test datasets, described further in the Evaluation Section. WSD that is shared across words and senses. The shared model allows utilizing sense correlations across words and therefore allows to transfer common disambiguation rules learned from disambiguating annotation-rich words and applies the rules for improving the disambiguation of annotation-lean words that share a sense alternation pattern. Specifically, we build a single neural network model for WSD that der"
2020.lrec-1.732,P18-1230,0,0.342571,"Missing"
2020.lrec-1.732,W04-0807,0,0.152784,"uous words. 4. 4.1. Evaluation Dataset Training: We use the SemCor (Miller et al., 1993) dataset for training our neural network. SemCor is the largest manually annotated English corpus for word sense disambiguation. It consists of 352 documents from the Brown corpus with 226,036 sense annotations based on WordNet 1.6 (Miller et al., 1990) which was later mapped to WordNet 3.0 (Raganato et al., 2017b). Validation and Evaluation: We evaluate our models on the benchmark fine-grained English all-words WSD dataset that includes test datasets from Senseval-2 (Edmonds and Cotton, 2001), Senseval-3 (Mihalcea et al., 2004), SemEval-2007 (Pradhan et al., 2007), SemEval 2013 (Navigli et al., 2013) and SemEval-2015 (Moro and Navigli, 2015). All these test sets were originally annotated with different versions of WordNet senses which were later standardized to WordNet 3.0 by Raganato et al. (2017b). Since both the training and test datasets are mapped to WordNet 3.0, we use WordNet 3.0 for extracting sense definitions and an example sentence for each sense, if available. Also, following the previous work on supervised WSD (Luo et al., 2018a; Luo et al., 2018b; Raganato et al., 2017a), we use the SemEval-2007 datase"
2020.lrec-1.732,H93-1061,0,0.763309,"pectively. We estimate the parameters of the shared model, Θ, by minimizing the following loss function: (9) The single classifier approach uses all the words and senses to train a shared neural network model, which can better capture structural regularities across correlated senses and sense alternation patterns across words. Meanwhile, the shared model uses many less parameters in a single neural net, unlike the word expert approach that uses word-specific classifiers and the parameters grow linearly with the number of ambiguous words. 4. 4.1. Evaluation Dataset Training: We use the SemCor (Miller et al., 1993) dataset for training our neural network. SemCor is the largest manually annotated English corpus for word sense disambiguation. It consists of 352 documents from the Brown corpus with 226,036 sense annotations based on WordNet 1.6 (Miller et al., 1990) which was later mapped to WordNet 3.0 (Raganato et al., 2017b). Validation and Evaluation: We evaluate our models on the benchmark fine-grained English all-words WSD dataset that includes test datasets from Senseval-2 (Edmonds and Cotton, 2001), Senseval-3 (Mihalcea et al., 2004), SemEval-2007 (Pradhan et al., 2007), SemEval 2013 (Navigli et al"
2020.lrec-1.732,S15-2049,0,0.142113,"eural network. SemCor is the largest manually annotated English corpus for word sense disambiguation. It consists of 352 documents from the Brown corpus with 226,036 sense annotations based on WordNet 1.6 (Miller et al., 1990) which was later mapped to WordNet 3.0 (Raganato et al., 2017b). Validation and Evaluation: We evaluate our models on the benchmark fine-grained English all-words WSD dataset that includes test datasets from Senseval-2 (Edmonds and Cotton, 2001), Senseval-3 (Mihalcea et al., 2004), SemEval-2007 (Pradhan et al., 2007), SemEval 2013 (Navigli et al., 2013) and SemEval-2015 (Moro and Navigli, 2015). All these test sets were originally annotated with different versions of WordNet senses which were later standardized to WordNet 3.0 by Raganato et al. (2017b). Since both the training and test datasets are mapped to WordNet 3.0, we use WordNet 3.0 for extracting sense definitions and an example sentence for each sense, if available. Also, following the previous work on supervised WSD (Luo et al., 2018a; Luo et al., 2018b; Raganato et al., 2017a), we use the SemEval-2007 dataset, the smallest among all, for validation and parameter tuning. 4.2. Model Settings and Model Training (8) We determ"
2020.lrec-1.732,Q14-1019,0,0.241817,"knowledge based methods and data-driven methods. 1. Heuristic MFS predicts the most frequent sense of a word in the training dataset. 2. Leskext+emb uses a word similarity function defined in distributional semantics space to score glosscontext overlap and uses that to identify the most appropriate sense (Basile et al., 2014). Our models are directly comparable to this system, except that we leverage sense-annotated corpora to learn to attend to gloss. 3. Babelfy creates semantic interpretations of the input text and uses a densest subgraph heuristic to jointly perform WSD and entity linking (Moro et al., 2014). 4. IMS uses linear support vector machines on lexical and syntactic features defined on the context of target word (Zhong and Ng, 2010). 5. BiLSTM+att.+LEX+P OS combines BiLSTM model with self-attention and uses multi-task learning framework for WSD, parts-of-speech tagging and semantic labeling (Raganato et al., 2017a). 6. GAS models semantic relationship between the context, gloss and hyper- and hypo-nyms of target word using memory modules in a word-specific classifier framework (Luo et al., 2018b). 7. HCAN extends the GAS model and uses the sophisticated hierarchical co-attention mechani"
2020.lrec-1.732,S13-2040,0,0.128306,"Missing"
2020.lrec-1.732,D14-1162,0,0.0864285,"Missing"
2020.lrec-1.732,N18-1202,0,0.0376429,"ansformers (Wolf et al., 2019). lossΘ = −(log(Ts ) + X (1 − log(fs ))) fs ∈Fs 5981 2. Single ClassifierBiLST M +Gloss : a variation of the shared model, with no regularization based on sense examples. using ADAM (Kingma and Ba, 2014) optimizer with learning rate set to 0.0001. For regularization, we use dropout rate (Srivastava et al., 2014) of 0.5 on the output activations of all encoders and neural layers. We use pre-trained word embeddings which are kept fixed during training. To ensure fair comparison with previous works, we evaluate our system using GloVe (Pennington et al., 2014), ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019) word embeddings. Training runs for 20 epochs for models with GloVe embeddings and 6 epochs for models with ELMo and BERT embeddings. In addition, we implemented our own word-specific model, Word-Specific ClassifierBiLST M +Gloss , which is equivalent to the Single ClassifierBiLST M +Gloss , except that the word-specific model uses word-specific sense prediction layers. This model can be seen as a simplified implementation of the previous system GAS (Luo et al., 2018b) after ignoring hyper- and hypo-nyms and memory modules. 4.3. 4.5. Baseline Systems We compare o"
2020.lrec-1.732,S07-1016,0,0.305767,"Training: We use the SemCor (Miller et al., 1993) dataset for training our neural network. SemCor is the largest manually annotated English corpus for word sense disambiguation. It consists of 352 documents from the Brown corpus with 226,036 sense annotations based on WordNet 1.6 (Miller et al., 1990) which was later mapped to WordNet 3.0 (Raganato et al., 2017b). Validation and Evaluation: We evaluate our models on the benchmark fine-grained English all-words WSD dataset that includes test datasets from Senseval-2 (Edmonds and Cotton, 2001), Senseval-3 (Mihalcea et al., 2004), SemEval-2007 (Pradhan et al., 2007), SemEval 2013 (Navigli et al., 2013) and SemEval-2015 (Moro and Navigli, 2015). All these test sets were originally annotated with different versions of WordNet senses which were later standardized to WordNet 3.0 by Raganato et al. (2017b). Since both the training and test datasets are mapped to WordNet 3.0, we use WordNet 3.0 for extracting sense definitions and an example sentence for each sense, if available. Also, following the previous work on supervised WSD (Luo et al., 2018a; Luo et al., 2018b; Raganato et al., 2017a), we use the SemEval-2007 dataset, the smallest among all, for valida"
2020.lrec-1.732,D17-1120,0,0.223862,"e et al., 2014; Basile et al., 2014) rely on sense definitions and lexico-semantic resources to measure overlap between sense representations and a word context, which is flexible to handle infrequent words. However, in the absence of any supervision, knowledge-based approaches may suffer from a mismatch between sense representations and a word context and their performance consistently falls behind the data-driven approaches. Data-driven methods (Zhong and Ng, 2010; Shen et al., 2013; Iacobacci et al., 2016) mostly build word specific classifiers (word-experts). Recent neural network models (Raganato et al., 2017a; K˚ageb¨ack and Salomonsson, 2016; Vial et al., 2018) use common first layers for all words and learn generalized low-level context representations. However, while using common context encoding layers, neural network models build word specific sense prediction layers and none of them completely pull out from the word expert notion. The crucial limitation of word expert models is their lack of capability of generalizing across words and their senses. In line with the recent neural network-based models, Luo et al. (2018b) and Luo et al. (2018a) use both sense definitions and sense-annotated da"
2020.lrec-1.732,E17-1010,0,0.274082,"e et al., 2014; Basile et al., 2014) rely on sense definitions and lexico-semantic resources to measure overlap between sense representations and a word context, which is flexible to handle infrequent words. However, in the absence of any supervision, knowledge-based approaches may suffer from a mismatch between sense representations and a word context and their performance consistently falls behind the data-driven approaches. Data-driven methods (Zhong and Ng, 2010; Shen et al., 2013; Iacobacci et al., 2016) mostly build word specific classifiers (word-experts). Recent neural network models (Raganato et al., 2017a; K˚ageb¨ack and Salomonsson, 2016; Vial et al., 2018) use common first layers for all words and learn generalized low-level context representations. However, while using common context encoding layers, neural network models build word specific sense prediction layers and none of them completely pull out from the word expert notion. The crucial limitation of word expert models is their lack of capability of generalizing across words and their senses. In line with the recent neural network-based models, Luo et al. (2018b) and Luo et al. (2018a) use both sense definitions and sense-annotated da"
2020.lrec-1.732,W08-1206,0,0.108929,"Missing"
2020.lrec-1.732,S13-1003,0,0.0310427,"WSD performance on annotationlean words. 2. Related Work Knowledge-based word sense disambiguation approaches (Lesk, 1986; Agirre et al., 2014; Basile et al., 2014) rely on sense definitions and lexico-semantic resources to measure overlap between sense representations and a word context, which is flexible to handle infrequent words. However, in the absence of any supervision, knowledge-based approaches may suffer from a mismatch between sense representations and a word context and their performance consistently falls behind the data-driven approaches. Data-driven methods (Zhong and Ng, 2010; Shen et al., 2013; Iacobacci et al., 2016) mostly build word specific classifiers (word-experts). Recent neural network models (Raganato et al., 2017a; K˚ageb¨ack and Salomonsson, 2016; Vial et al., 2018) use common first layers for all words and learn generalized low-level context representations. However, while using common context encoding layers, neural network models build word specific sense prediction layers and none of them completely pull out from the word expert notion. The crucial limitation of word expert models is their lack of capability of generalizing across words and their senses. In line with"
2020.lrec-1.732,W11-0128,0,0.0732492,"Missing"
2020.lrec-1.732,2019.gwc-1.14,0,0.280343,"se common first layers for all words and learn generalized low-level context representations. However, while using common context encoding layers, neural network models build word specific sense prediction layers and none of them completely pull out from the word expert notion. The crucial limitation of word expert models is their lack of capability of generalizing across words and their senses. In line with the recent neural network-based models, Luo et al. (2018b) and Luo et al. (2018a) use both sense definitions and sense-annotated data in a neural network classifier-based models. Further, Vial et al. (2019) merges senses across words guided by the hypernym-hyponym relation in WordNet to ease the problem of sparsity, but the fundamental generalization issue that results from word specific sense prediction layers remains. The proposed shared model for word sense disambiguation combines the best of both types of approaches. Specifically, by utilizing both the available sense-annotated data and knowledge resources, the sense-context resemblance model learns to generate and attend to word sense representations for disambiguating a word context. Similar to our approach, a concurrent work by Kumar et a"
2020.lrec-1.732,P10-4014,0,0.508861,"-sets, and improves WSD performance on annotationlean words. 2. Related Work Knowledge-based word sense disambiguation approaches (Lesk, 1986; Agirre et al., 2014; Basile et al., 2014) rely on sense definitions and lexico-semantic resources to measure overlap between sense representations and a word context, which is flexible to handle infrequent words. However, in the absence of any supervision, knowledge-based approaches may suffer from a mismatch between sense representations and a word context and their performance consistently falls behind the data-driven approaches. Data-driven methods (Zhong and Ng, 2010; Shen et al., 2013; Iacobacci et al., 2016) mostly build word specific classifiers (word-experts). Recent neural network models (Raganato et al., 2017a; K˚ageb¨ack and Salomonsson, 2016; Vial et al., 2018) use common first layers for all words and learn generalized low-level context representations. However, while using common context encoding layers, neural network models build word specific sense prediction layers and none of them completely pull out from the word expert notion. The crucial limitation of word expert models is their lack of capability of generalizing across words and their s"
2021.acl-short.93,D19-1367,0,0.0754154,"uistics and the 11th International Joint Conference on Natural Language Processing (Short Papers), pages 735–742 August 1–6, 2021. ©2021 Association for Computational Linguistics Exact Match Compound Lemma Match Figure 1: An example of NER with both discourse-level and sentence-level entity relations. domain-specific NER, we plot learning curves and show that leveraging relations between entity mentions can effectively and consistently improve the NER performance when limited annotations are available. 2 Related Work NER research has a long history and recent approaches (Yang and Zhang, 2018; Jiang et al., 2019; Jie and Lu, 2019; Li et al., 2020) using Neural Network models like BiLSTM-CNN-CRF (Ma and Hovy, 2016) and contextual embeddings such as BERT (Devlin et al., 2019) and FLAIR (Akbik et al., 2018) have improved the NER performance in the general domain to the human-level. However, the NER performance for specific domains is still moderate due to the challenges of limited annotations and dealing with complicated domain-specific contexts. We aim to further improve NER performance by considering coreference relations and semantic relations between entity mentions. This is in contrast to the usual"
2021.acl-short.93,D19-1399,0,0.0268292,"International Joint Conference on Natural Language Processing (Short Papers), pages 735–742 August 1–6, 2021. ©2021 Association for Computational Linguistics Exact Match Compound Lemma Match Figure 1: An example of NER with both discourse-level and sentence-level entity relations. domain-specific NER, we plot learning curves and show that leveraging relations between entity mentions can effectively and consistently improve the NER performance when limited annotations are available. 2 Related Work NER research has a long history and recent approaches (Yang and Zhang, 2018; Jiang et al., 2019; Jie and Lu, 2019; Li et al., 2020) using Neural Network models like BiLSTM-CNN-CRF (Ma and Hovy, 2016) and contextual embeddings such as BERT (Devlin et al., 2019) and FLAIR (Akbik et al., 2018) have improved the NER performance in the general domain to the human-level. However, the NER performance for specific domains is still moderate due to the challenges of limited annotations and dealing with complicated domain-specific contexts. We aim to further improve NER performance by considering coreference relations and semantic relations between entity mentions. This is in contrast to the usual way of thinking a"
2021.acl-short.93,2020.acl-main.519,0,0.0306585,"nt Conference on Natural Language Processing (Short Papers), pages 735–742 August 1–6, 2021. ©2021 Association for Computational Linguistics Exact Match Compound Lemma Match Figure 1: An example of NER with both discourse-level and sentence-level entity relations. domain-specific NER, we plot learning curves and show that leveraging relations between entity mentions can effectively and consistently improve the NER performance when limited annotations are available. 2 Related Work NER research has a long history and recent approaches (Yang and Zhang, 2018; Jiang et al., 2019; Jie and Lu, 2019; Li et al., 2020) using Neural Network models like BiLSTM-CNN-CRF (Ma and Hovy, 2016) and contextual embeddings such as BERT (Devlin et al., 2019) and FLAIR (Akbik et al., 2018) have improved the NER performance in the general domain to the human-level. However, the NER performance for specific domains is still moderate due to the challenges of limited annotations and dealing with complicated domain-specific contexts. We aim to further improve NER performance by considering coreference relations and semantic relations between entity mentions. This is in contrast to the usual way of thinking about NER as an up-"
2021.acl-short.93,D19-1488,0,0.0282742,"entence S3, “bone marrow” of the type Multi-tissue Structure and “endothelial progenitors” of the type Cell, are the subject and object of the predicate “contains” respectively in the dependency tree. If the system can reliably predict the type of one entity, we can infer the type of the other entity more easily, knowing that they are closely related on the dependency tree. We incorporate both relations by using Graph Neural Networks (GNNs), specifically, we use the Graph Attention Networks (GATs) (Velickovic et al., 2018) that have been shown effective for a range of tasks (Sui et al., 2019; Linmei et al., 2019). Empirical results show that our lightweight method can learn better word representations for sequence tagging models and further improve the NER performance over strong LMs-based baselines on two datasets, the AnatEM (Pyysalo and Ananiadou, 2014) dataset from the biomedical domain and the Mars (Wagstaff et al., 2018) dataset from the planetary science domain. In addition, considering the lack of annotations challenge for 735 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Short"
2021.acl-short.93,N19-1308,0,0.0180647,"erformance for specific domains is still moderate due to the challenges of limited annotations and dealing with complicated domain-specific contexts. We aim to further improve NER performance by considering coreference relations and semantic relations between entity mentions. This is in contrast to the usual way of thinking about NER as an up-stream task conducted before coreference resolution or entity relation extraction. The idea aligns with recent works that conduct joint inferences among multiple information extraction tasks (Miwa and Bansal, 2016; Li et al., 2017; Bekoulis et al., 2018; Luan et al., 2019; Sui et al., 2020; Yuan et al., 2020), including NER, coreference resolution and relation extraction, by mining dependencies among the extractions. However, joint inference approaches require annotations for all the target tasks and aim to improve performance for all the tasks as well, while our lightweight approach aims to improve the performance of the basic NER task requiring no additional annotations (usually unavailable for specific domains). Our approach is also related to several recent neural approaches for NER that encourage label dependencies among entity mentions. The Pooled FLAIR"
2021.acl-short.93,P16-1101,0,0.308077,"noticeably improves the NER performance on two datasets from different domains. We further show that the proposed lightweight system can effectively elevate the NER performance to a higher level even when only a tiny amount of labeled data is available, which is desirable for domain-specific NER.1 1 Introduction Named entity recognition (NER) has been well studied for the general domain, and recent systems have achieved close to human-level performance for identifying a small number of common NER types, such as Person and Organization, mainly benefiting from the use of Neural Network models (Ma and Hovy, 2016; Yang and Zhang, 2018) and pretrained Language Models (LMs) (Akbik et al., 2018; Devlin et al., 2019). However, the performance is still moderate for specialized domains that tend to feature diverse and complicated contexts as well as a richer set of semantically related entity types (e.g., Cell, Tissue, Organ etc. for the biomedical domain). With these challenges in view, we hypothesize that being aware of the 1 The code for the system is available here: https:// github.com/brickee/EnRel-G re-occurrences of the same entity as well as semantically related entities will lead to better NER perf"
2021.acl-short.93,P16-1105,0,0.0158754,"ce in the general domain to the human-level. However, the NER performance for specific domains is still moderate due to the challenges of limited annotations and dealing with complicated domain-specific contexts. We aim to further improve NER performance by considering coreference relations and semantic relations between entity mentions. This is in contrast to the usual way of thinking about NER as an up-stream task conducted before coreference resolution or entity relation extraction. The idea aligns with recent works that conduct joint inferences among multiple information extraction tasks (Miwa and Bansal, 2016; Li et al., 2017; Bekoulis et al., 2018; Luan et al., 2019; Sui et al., 2020; Yuan et al., 2020), including NER, coreference resolution and relation extraction, by mining dependencies among the extractions. However, joint inference approaches require annotations for all the target tasks and aim to improve performance for all the tasks as well, while our lightweight approach aims to improve the performance of the basic NER task requiring no additional annotations (usually unavailable for specific domains). Our approach is also related to several recent neural approaches for NER that encourage"
2021.acl-short.93,D19-1519,0,0.0153557,"types. 4.1 Baselines NCRF++ (Yang and Zhang, 2018) is an opensource Neural Sequence Labelling Toolkit. We use 3 Previous systems on the AnatEM dataset either evaluate the NER performance by head match or only evaluate the performance on span identification; therefore, so we do not include their results here. 4 More details about the datasets, data preprocessing, and model settings can be found in the appendices. the BiLSTM-CNN-CRF structrue as a baseline. FLAIR (Akbik et al., 2018) is a character-level pretrained LM based on BiLSTM, which has been used in many NER systems (Jiang et al., 2019; Wang et al., 2019). We use the embeddings from it with a BiLSTM-CRF architecture as a baseline. Pooled FLAIR (Akbik et al., 2019) is an extended version of the FLAIR model with global memory and pooling mechanism for the same word, which helps consistent predictions of coreferential entity mentions. We also use the embeddings from it with a BiLSTM-CRF architecture as a baseline. Tuning Bio/SciBERT We also use Bio/SciBERT with a BiLSTM-CRF architecture as baselines for the AnatEM/Mars datasets, which do not have the GNNs layer or Fusion layer as compared with our system. 4.2 Results To alleviate random turbulenc"
2021.acl-short.93,W12-4304,0,0.0553466,"Missing"
2021.acl-short.93,P18-4013,0,0.381409,"s the NER performance on two datasets from different domains. We further show that the proposed lightweight system can effectively elevate the NER performance to a higher level even when only a tiny amount of labeled data is available, which is desirable for domain-specific NER.1 1 Introduction Named entity recognition (NER) has been well studied for the general domain, and recent systems have achieved close to human-level performance for identifying a small number of common NER types, such as Person and Organization, mainly benefiting from the use of Neural Network models (Ma and Hovy, 2016; Yang and Zhang, 2018) and pretrained Language Models (LMs) (Akbik et al., 2018; Devlin et al., 2019). However, the performance is still moderate for specialized domains that tend to feature diverse and complicated contexts as well as a richer set of semantically related entity types (e.g., Cell, Tissue, Organ etc. for the biomedical domain). With these challenges in view, we hypothesize that being aware of the 1 The code for the system is available here: https:// github.com/brickee/EnRel-G re-occurrences of the same entity as well as semantically related entities will lead to better NER performance for specific do"
2021.acl-short.93,D14-1162,0,0.0969084,"Missing"
2021.acl-short.93,N19-1082,0,0.0134968,"get tasks and aim to improve performance for all the tasks as well, while our lightweight approach aims to improve the performance of the basic NER task requiring no additional annotations (usually unavailable for specific domains). Our approach is also related to several recent neural approaches for NER that encourage label dependencies among entity mentions. The Pooled FLAIR model (Akbik et al., 2019) proposed a global pooling mechanism to learn word representations. Dai et al. (2019) used a coreference layer with a regularizer to harmonize word representations. Closely related to our work, Qian et al. (2019) used graph neural nets to capture repetitions of the same word as well, but in a denser graph that includes edges between adjacent words and is meant to completely overlay the lower encoding layers. Memory networks (Gui et al., 2020; Luo et al., 2020) were also used to store and refine predictions of a base model by considering repetitions or co-occurrences of words. In addition, dependency relations have been commonly used to connect entities for relation extraction (Zhang et al., 2018; Bunescu and Mooney, 2005), but we aim to better infer the type of an entity by associating it with other c"
2021.acl-short.93,D19-1396,0,0.0230415,"two entities in sentence S3, “bone marrow” of the type Multi-tissue Structure and “endothelial progenitors” of the type Cell, are the subject and object of the predicate “contains” respectively in the dependency tree. If the system can reliably predict the type of one entity, we can infer the type of the other entity more easily, knowing that they are closely related on the dependency tree. We incorporate both relations by using Graph Neural Networks (GNNs), specifically, we use the Graph Attention Networks (GATs) (Velickovic et al., 2018) that have been shown effective for a range of tasks (Sui et al., 2019; Linmei et al., 2019). Empirical results show that our lightweight method can learn better word representations for sequence tagging models and further improve the NER performance over strong LMs-based baselines on two datasets, the AnatEM (Pyysalo and Ananiadou, 2014) dataset from the biomedical domain and the Mars (Wagstaff et al., 2018) dataset from the planetary science domain. In addition, considering the lack of annotations challenge for 735 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Lang"
2021.eacl-main.101,P19-1409,0,0.106384,"atures comprising of lemma and part-of-speech tag similarity of event words (Bejan and Harabagiu, 2010; Lee et al., 2012; Liu et al., 2014; Yang et al., 2015; Lu et al., 2016; Cremisini and Finlayson, 2020), argument overlap (Chen et al., 2009; McConky et al., 2012; Sangeetha and Arock, 2012; Bejan and Harabagiu, 2014; Yang et al., 2015; Lu et al., 2016; Choubey and Huang, 2017), semantic similarity based on lexical resources such as wordnet (Bejan and Harabagiu, 2010; Liu et al., 2014; Yu et al., 2016) and word embeddings (Yang et al., 2015; Choubey and Huang, 2017; Kenyon-Dean et al., 2018; Barhom et al., 2019; Zuo et al., 2019; Pandian et al., 2020; Sahlani et al., 2020; Lu et al., 2020), and discourse features such as token and sentence distance (Liu et al., 2014; Cybulska and Vossen, 2015). The resulting classifier is used to cluster event mentions. The commonly used strategies include agglomerative clustering that selects the antecedent closest in mention distance that is classified as coreferent or the antecedent with the highest coreference likelihood (Chen et al., 2009; Chen and Ng, 2014), hierarchical bayesian (Yang et al., 2015) or spectral clustering algorithms (Chen and Ji, 2009). In thi"
2021.eacl-main.101,P10-1143,0,0.119896,"match baseline. Thus, increasing training data size does not improve the performance of an event coreference resolution system on a new text genre. We suspect that, for generalization across different text genres, we may require specialized learning algorithms, e.g., text style adaptation, which is not in the scope of this work. 2 Related Work The existing literature on supervised event coreference resolution primarily focuses on designing pairwise classifier based on the surface linguistic features such as lexical features comprising of lemma and part-of-speech tag similarity of event words (Bejan and Harabagiu, 2010; Lee et al., 2012; Liu et al., 2014; Yang et al., 2015; Lu et al., 2016; Cremisini and Finlayson, 2020), argument overlap (Chen et al., 2009; McConky et al., 2012; Sangeetha and Arock, 2012; Bejan and Harabagiu, 2014; Yang et al., 2015; Lu et al., 2016; Choubey and Huang, 2017), semantic similarity based on lexical resources such as wordnet (Bejan and Harabagiu, 2010; Liu et al., 2014; Yu et al., 2016) and word embeddings (Yang et al., 2015; Choubey and Huang, 2017; Kenyon-Dean et al., 2018; Barhom et al., 2019; Zuo et al., 2019; Pandian et al., 2020; Sahlani et al., 2020; Lu et al., 2020), a"
2021.eacl-main.101,J14-2004,0,0.0222854,"may require specialized learning algorithms, e.g., text style adaptation, which is not in the scope of this work. 2 Related Work The existing literature on supervised event coreference resolution primarily focuses on designing pairwise classifier based on the surface linguistic features such as lexical features comprising of lemma and part-of-speech tag similarity of event words (Bejan and Harabagiu, 2010; Lee et al., 2012; Liu et al., 2014; Yang et al., 2015; Lu et al., 2016; Cremisini and Finlayson, 2020), argument overlap (Chen et al., 2009; McConky et al., 2012; Sangeetha and Arock, 2012; Bejan and Harabagiu, 2014; Yang et al., 2015; Lu et al., 2016; Choubey and Huang, 2017), semantic similarity based on lexical resources such as wordnet (Bejan and Harabagiu, 2010; Liu et al., 2014; Yu et al., 2016) and word embeddings (Yang et al., 2015; Choubey and Huang, 2017; Kenyon-Dean et al., 2018; Barhom et al., 2019; Zuo et al., 2019; Pandian et al., 2020; Sahlani et al., 2020; Lu et al., 2020), and discourse features such as token and sentence distance (Liu et al., 2014; Cybulska and Vossen, 2015). The resulting classifier is used to cluster event mentions. The commonly used strategies include agglomerative c"
2021.eacl-main.101,chen-ng-2014-sinocoreferencer,0,0.0200977,"t al., 2016) and word embeddings (Yang et al., 2015; Choubey and Huang, 2017; Kenyon-Dean et al., 2018; Barhom et al., 2019; Zuo et al., 2019; Pandian et al., 2020; Sahlani et al., 2020; Lu et al., 2020), and discourse features such as token and sentence distance (Liu et al., 2014; Cybulska and Vossen, 2015). The resulting classifier is used to cluster event mentions. The commonly used strategies include agglomerative clustering that selects the antecedent closest in mention distance that is classified as coreferent or the antecedent with the highest coreference likelihood (Chen et al., 2009; Chen and Ng, 2014), hierarchical bayesian (Yang et al., 2015) or spectral clustering algorithms (Chen and Ji, 2009). In this work, we use the pre-trained BERT model to extract both event and context features and use agglomerative clustering to form event coreference chains. Supervised models suffer from a lack of human1186 annotated event coreference data. To address the annotation scarcity problem, Peng et al. (2016) proposed to learn structured event representations on large amounts of text and use the similarity score between two event representations to form event coreference chains. Their model uses a smal"
2021.eacl-main.101,P15-2053,0,0.0259811,"uffer from a lack of human1186 annotated event coreference data. To address the annotation scarcity problem, Peng et al. (2016) proposed to learn structured event representations on large amounts of text and use the similarity score between two event representations to form event coreference chains. Their model uses a small human-annotated event coreference dataset to find the appropriate similarity score threshold for linking two events. Unsupervised models based on probabilistic generative modeling have also been successfully used for event coreference resolution (Bejan and Harabagiu, 2010; Chen and Ng, 2015). However, both semi-supervised and unsupervised approaches have been found empirically lagging behind the supervised models (Lu and Ng, 2018). The closest to our work are weakly-supervised and self-training methods that have been shown useful for many information extraction and classification tasks (Riloff, 1996; Riloff and Wiebe, 2003; Xie et al., 2019). But, to the best of our knowledge, we are the first to explore discourse-aware strategies to automatically label event coreference relations and use them exclusively or use them to augment existing human-annotated data for training event cor"
2021.eacl-main.101,W09-3208,0,0.0567636,"2018; Barhom et al., 2019; Zuo et al., 2019; Pandian et al., 2020; Sahlani et al., 2020; Lu et al., 2020), and discourse features such as token and sentence distance (Liu et al., 2014; Cybulska and Vossen, 2015). The resulting classifier is used to cluster event mentions. The commonly used strategies include agglomerative clustering that selects the antecedent closest in mention distance that is classified as coreferent or the antecedent with the highest coreference likelihood (Chen et al., 2009; Chen and Ng, 2014), hierarchical bayesian (Yang et al., 2015) or spectral clustering algorithms (Chen and Ji, 2009). In this work, we use the pre-trained BERT model to extract both event and context features and use agglomerative clustering to form event coreference chains. Supervised models suffer from a lack of human1186 annotated event coreference data. To address the annotation scarcity problem, Peng et al. (2016) proposed to learn structured event representations on large amounts of text and use the similarity score between two event representations to form event coreference chains. Their model uses a small human-annotated event coreference dataset to find the appropriate similarity score threshold fo"
2021.eacl-main.101,W09-4303,0,0.048121,"e suspect that, for generalization across different text genres, we may require specialized learning algorithms, e.g., text style adaptation, which is not in the scope of this work. 2 Related Work The existing literature on supervised event coreference resolution primarily focuses on designing pairwise classifier based on the surface linguistic features such as lexical features comprising of lemma and part-of-speech tag similarity of event words (Bejan and Harabagiu, 2010; Lee et al., 2012; Liu et al., 2014; Yang et al., 2015; Lu et al., 2016; Cremisini and Finlayson, 2020), argument overlap (Chen et al., 2009; McConky et al., 2012; Sangeetha and Arock, 2012; Bejan and Harabagiu, 2014; Yang et al., 2015; Lu et al., 2016; Choubey and Huang, 2017), semantic similarity based on lexical resources such as wordnet (Bejan and Harabagiu, 2010; Liu et al., 2014; Yu et al., 2016) and word embeddings (Yang et al., 2015; Choubey and Huang, 2017; Kenyon-Dean et al., 2018; Barhom et al., 2019; Zuo et al., 2019; Pandian et al., 2020; Sahlani et al., 2020; Lu et al., 2020), and discourse features such as token and sentence distance (Liu et al., 2014; Cybulska and Vossen, 2015). The resulting classifier is used to"
2021.eacl-main.101,D17-1226,1,0.91629,"Missing"
2021.eacl-main.101,P18-1045,1,0.618339,"ision and recall scores. 5.2 Implementation Details We use an ensemble of multi-layer feed-forward neural network classifiers to identify event men11 ECB+ (Cybulska and Vossen, 2014) is another popular dataset for evaluating event coreference resolution. However, documents in ECB+ are selectively annotated, comprising only of event mentions and within-document coreference chains that are relevant to cross-document event coreference chains. Since our data acquisition methodology is designed for collecting within-document event pairs, we decided to exclude evaluations on the ECB+ corpus. tions (Choubey and Huang, 2018) for both news and discussion forum documents in KBP 2017 corpus. For the RED corpus, we use gold event mentions as that event extraction system can identify events from only eight event types annotated in KBP 2015 corpus. The coreference classifier uses a three-layer feed-forward neural network with 1024512-1 units for scoring coreference likelihood. Two single-neural layers, used to transform elementwise dot product and difference between two event embeddings used for controlling context input, use 1024 units each. All hidden activations are followed by dropout with the rate of 0.1 for regul"
2021.eacl-main.101,2020.acl-main.478,1,0.872732,"events, and other historical or future projected events 1185 Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics, pages 1185–1196 April 19 - 23, 2021. ©2021 Association for Computational Linguistics event in a news article. In general, we can recognize pairs of sentences in news articles that are likely to contain coreferential or non-coreferential event mention pairs by knowing the sentence’s discourse function following Van Dijk’s theory. To ascertain our hypothesis, we first use the discourse profiling system and dataset introduced by Choubey et al. (2020) to identify the discourse role for each sentence in a news article. Then, we use multiple rules to capture the distributional correlation between event coreference chains and discourse roles of sentences and collect a diverse set of 9,210 coreferential and 232,135 non-coreferential event pairs2 . To assess the reliability of the proposed data augmentation strategy, we perform manual validation on subsets of both coreferential and non-coreferential event pairs. Then, we train event coreference resolution systems using the acquired data alone or using the acquired data to augment a human-annota"
2021.eacl-main.101,2020.nuse-1.1,0,0.017423,"reference resolution system on a new text genre. We suspect that, for generalization across different text genres, we may require specialized learning algorithms, e.g., text style adaptation, which is not in the scope of this work. 2 Related Work The existing literature on supervised event coreference resolution primarily focuses on designing pairwise classifier based on the surface linguistic features such as lexical features comprising of lemma and part-of-speech tag similarity of event words (Bejan and Harabagiu, 2010; Lee et al., 2012; Liu et al., 2014; Yang et al., 2015; Lu et al., 2016; Cremisini and Finlayson, 2020), argument overlap (Chen et al., 2009; McConky et al., 2012; Sangeetha and Arock, 2012; Bejan and Harabagiu, 2014; Yang et al., 2015; Lu et al., 2016; Choubey and Huang, 2017), semantic similarity based on lexical resources such as wordnet (Bejan and Harabagiu, 2010; Liu et al., 2014; Yu et al., 2016) and word embeddings (Yang et al., 2015; Choubey and Huang, 2017; Kenyon-Dean et al., 2018; Barhom et al., 2019; Zuo et al., 2019; Pandian et al., 2020; Sahlani et al., 2020; Lu et al., 2020), and discourse features such as token and sentence distance (Liu et al., 2014; Cybulska and Vossen, 2015)."
2021.eacl-main.101,cybulska-vossen-2014-using,0,0.0627842,"coreference resolution, we evaluate all the event coreference resolution systems using the official KBP 2017 scorer v1.8. The scorer employs four coreference scoring measures, namely B 3 (Bagga and Baldwin, 1998), CEAFe (Luo, 2005), MUC (Vilain et al., 1995) and BLANC (Recasens and Hovy, 2011) and the unweighted average of their F1 scores AV GF 1 . In addition, since MUC directly evaluates pairwise coreference links, we also report MUC precision and recall scores. 5.2 Implementation Details We use an ensemble of multi-layer feed-forward neural network classifiers to identify event men11 ECB+ (Cybulska and Vossen, 2014) is another popular dataset for evaluating event coreference resolution. However, documents in ECB+ are selectively annotated, comprising only of event mentions and within-document coreference chains that are relevant to cross-document event coreference chains. Since our data acquisition methodology is designed for collecting within-document event pairs, we decided to exclude evaluations on the ECB+ corpus. tions (Choubey and Huang, 2018) for both news and discussion forum documents in KBP 2017 corpus. For the RED corpus, we use gold event mentions as that event extraction system can identify"
2021.eacl-main.101,W15-0801,0,0.0154078,"misini and Finlayson, 2020), argument overlap (Chen et al., 2009; McConky et al., 2012; Sangeetha and Arock, 2012; Bejan and Harabagiu, 2014; Yang et al., 2015; Lu et al., 2016; Choubey and Huang, 2017), semantic similarity based on lexical resources such as wordnet (Bejan and Harabagiu, 2010; Liu et al., 2014; Yu et al., 2016) and word embeddings (Yang et al., 2015; Choubey and Huang, 2017; Kenyon-Dean et al., 2018; Barhom et al., 2019; Zuo et al., 2019; Pandian et al., 2020; Sahlani et al., 2020; Lu et al., 2020), and discourse features such as token and sentence distance (Liu et al., 2014; Cybulska and Vossen, 2015). The resulting classifier is used to cluster event mentions. The commonly used strategies include agglomerative clustering that selects the antecedent closest in mention distance that is classified as coreferent or the antecedent with the highest coreference likelihood (Chen et al., 2009; Chen and Ng, 2014), hierarchical bayesian (Yang et al., 2015) or spectral clustering algorithms (Chen and Ji, 2009). In this work, we use the pre-trained BERT model to extract both event and context features and use agglomerative clustering to form event coreference chains. Supervised models suffer from a la"
2021.eacl-main.101,N19-1423,0,0.0215123,"Missing"
2021.eacl-main.101,P18-1128,0,0.0140707,"5.34 45.36 45.20 45.11 38.52 39.43 44.76 42.01 44.32 48.91 49.61 47.23 47.29 64.34 64.88 65.77 63.09 63.75 66.74 67.50 66.70 67.44 68.78 68.65 71.08 65.96 71.0 72.96 73.29 71.70 71.94 19.69 20.14 24.52 24.48 18.89 24.09 21.19 25.38 21.29 18.25 18.14 20.10 19.19 18.81 20.01 19.19 20.08 17.51 28.59 28.44 30.35 28.39 29.59 29.92 29.59 29.73 28.47 Table 3: Results for event coreference resolution systems on the KBP 2017 and RED corpora. Feature-based Classifier results are directly taken from Choubey and Huang (2018). The results are statistically significant using bootstrap and permutation test (Dror et al., 2018) with p<0.01 between Post-Filtering Paraphrase pairs and Paraphrase-based Pairs and p<0.002 between KBP 2015+Post-Filtering Paraphrase pairs+Masked Training and KBP 2015 models on both KBP 2017 and RED news articles test sets. Further, results for KBP 2015+PostFiltering Paraphrase pairs+Masked Training are statistically significant compared to both Student Training and Student Training+Masked Training with p<0.002 on the RED news articles test set. for all models on discussion forum documents, with the increased data size, also indicate the need for specialized learning algorithms to build a m"
2021.eacl-main.101,N13-1092,0,0.102524,"Missing"
2021.eacl-main.101,S18-2001,0,0.0120335,"eatures such as lexical features comprising of lemma and part-of-speech tag similarity of event words (Bejan and Harabagiu, 2010; Lee et al., 2012; Liu et al., 2014; Yang et al., 2015; Lu et al., 2016; Cremisini and Finlayson, 2020), argument overlap (Chen et al., 2009; McConky et al., 2012; Sangeetha and Arock, 2012; Bejan and Harabagiu, 2014; Yang et al., 2015; Lu et al., 2016; Choubey and Huang, 2017), semantic similarity based on lexical resources such as wordnet (Bejan and Harabagiu, 2010; Liu et al., 2014; Yu et al., 2016) and word embeddings (Yang et al., 2015; Choubey and Huang, 2017; Kenyon-Dean et al., 2018; Barhom et al., 2019; Zuo et al., 2019; Pandian et al., 2020; Sahlani et al., 2020; Lu et al., 2020), and discourse features such as token and sentence distance (Liu et al., 2014; Cybulska and Vossen, 2015). The resulting classifier is used to cluster event mentions. The commonly used strategies include agglomerative clustering that selects the antecedent closest in mention distance that is classified as coreferent or the antecedent with the highest coreference likelihood (Chen et al., 2009; Chen and Ng, 2014), hierarchical bayesian (Yang et al., 2015) or spectral clustering algorithms (Chen"
2021.eacl-main.101,D12-1045,0,0.0343159,"asing training data size does not improve the performance of an event coreference resolution system on a new text genre. We suspect that, for generalization across different text genres, we may require specialized learning algorithms, e.g., text style adaptation, which is not in the scope of this work. 2 Related Work The existing literature on supervised event coreference resolution primarily focuses on designing pairwise classifier based on the surface linguistic features such as lexical features comprising of lemma and part-of-speech tag similarity of event words (Bejan and Harabagiu, 2010; Lee et al., 2012; Liu et al., 2014; Yang et al., 2015; Lu et al., 2016; Cremisini and Finlayson, 2020), argument overlap (Chen et al., 2009; McConky et al., 2012; Sangeetha and Arock, 2012; Bejan and Harabagiu, 2014; Yang et al., 2015; Lu et al., 2016; Choubey and Huang, 2017), semantic similarity based on lexical resources such as wordnet (Bejan and Harabagiu, 2010; Liu et al., 2014; Yu et al., 2016) and word embeddings (Yang et al., 2015; Choubey and Huang, 2017; Kenyon-Dean et al., 2018; Barhom et al., 2019; Zuo et al., 2019; Pandian et al., 2020; Sahlani et al., 2020; Lu et al., 2020), and discourse featu"
2021.eacl-main.101,liu-etal-2014-supervised,0,0.0215365,"a size does not improve the performance of an event coreference resolution system on a new text genre. We suspect that, for generalization across different text genres, we may require specialized learning algorithms, e.g., text style adaptation, which is not in the scope of this work. 2 Related Work The existing literature on supervised event coreference resolution primarily focuses on designing pairwise classifier based on the surface linguistic features such as lexical features comprising of lemma and part-of-speech tag similarity of event words (Bejan and Harabagiu, 2010; Lee et al., 2012; Liu et al., 2014; Yang et al., 2015; Lu et al., 2016; Cremisini and Finlayson, 2020), argument overlap (Chen et al., 2009; McConky et al., 2012; Sangeetha and Arock, 2012; Bejan and Harabagiu, 2014; Yang et al., 2015; Lu et al., 2016; Choubey and Huang, 2017), semantic similarity based on lexical resources such as wordnet (Bejan and Harabagiu, 2010; Liu et al., 2014; Yu et al., 2016) and word embeddings (Yang et al., 2015; Choubey and Huang, 2017; Kenyon-Dean et al., 2018; Barhom et al., 2019; Zuo et al., 2019; Pandian et al., 2020; Sahlani et al., 2020; Lu et al., 2020), and discourse features such as token"
2021.eacl-main.101,C16-1308,0,0.0148581,"ce of an event coreference resolution system on a new text genre. We suspect that, for generalization across different text genres, we may require specialized learning algorithms, e.g., text style adaptation, which is not in the scope of this work. 2 Related Work The existing literature on supervised event coreference resolution primarily focuses on designing pairwise classifier based on the surface linguistic features such as lexical features comprising of lemma and part-of-speech tag similarity of event words (Bejan and Harabagiu, 2010; Lee et al., 2012; Liu et al., 2014; Yang et al., 2015; Lu et al., 2016; Cremisini and Finlayson, 2020), argument overlap (Chen et al., 2009; McConky et al., 2012; Sangeetha and Arock, 2012; Bejan and Harabagiu, 2014; Yang et al., 2015; Lu et al., 2016; Choubey and Huang, 2017), semantic similarity based on lexical resources such as wordnet (Bejan and Harabagiu, 2010; Liu et al., 2014; Yu et al., 2016) and word embeddings (Yang et al., 2015; Choubey and Huang, 2017; Kenyon-Dean et al., 2018; Barhom et al., 2019; Zuo et al., 2019; Pandian et al., 2020; Sahlani et al., 2020; Lu et al., 2020), and discourse features such as token and sentence distance (Liu et al., 2"
2021.eacl-main.101,H05-1004,0,0.13394,"ely annotated with event coreference relations with no restriction on event types or subtypes, thus, allowing us to evaluate coreference resolution performance on a broad range of events. Besides, we evaluate the performance of models across text genres by evaluating our models trained with news articles on KBP 2017 discussion forum documents. Following previous work on event coreference resolution, we evaluate all the event coreference resolution systems using the official KBP 2017 scorer v1.8. The scorer employs four coreference scoring measures, namely B 3 (Bagga and Baldwin, 1998), CEAFe (Luo, 2005), MUC (Vilain et al., 1995) and BLANC (Recasens and Hovy, 2011) and the unweighted average of their F1 scores AV GF 1 . In addition, since MUC directly evaluates pairwise coreference links, we also report MUC precision and recall scores. 5.2 Implementation Details We use an ensemble of multi-layer feed-forward neural network classifiers to identify event men11 ECB+ (Cybulska and Vossen, 2014) is another popular dataset for evaluating event coreference resolution. However, documents in ECB+ are selectively annotated, comprising only of event mentions and within-document coreference chains that"
2021.eacl-main.101,2020.findings-emnlp.440,0,0.0151863,"86; Choubey et al., 2020) to identify seed coreferential and non-coreferential event pairs followed by a single bootstrapping iteration to collect additional non-coreferential event pairs. 3.1 Identifying Coreferential Event Trigger Candidates using The PPDB Database We collect lexically diverse candidate coreferential event pairs using the paraphrases from PPDB-2.0-slexical (Pavlick et al., 2015) database. The corpus7 contains 213,716 highest scoring lexical paraphrase pairs, each annotated with one of the equivalence, forward or reverse entailment, and contradiction 6 A contemporary work by Meged et al. (2020) has also studied the potential correlation between coreferential event trigger words and predicate paraphrases. 7 http://nlpgrid.seas.upenn.edu/PPDB/ eng/ppdb-2.0-tldr.gz Post-Filtering Paraphrase-based Event Pairs using Functional News Discourse Structure To generate the news discourse structure proposed by Van Dijk (Teun A, 1986; Van Dijk, 1988a,b) and specify the discourse role of a sentence with respect to events in the document, we use the discourse profiling system proposed by Choubey et al. (2020). Note that the above discourse structure is functional (Webber and Joshi, 2012) and does"
2021.eacl-main.101,W12-3018,0,0.0288701,"Missing"
2021.eacl-main.101,W16-5706,0,0.0541663,"Missing"
2021.eacl-main.101,P15-2070,0,0.0248961,"Missing"
2021.eacl-main.101,D16-1038,0,0.263022,"Missing"
2021.eacl-main.101,D14-1162,0,0.0877882,"Missing"
2021.eacl-main.101,W03-1014,0,0.266614,"notated event coreference dataset to find the appropriate similarity score threshold for linking two events. Unsupervised models based on probabilistic generative modeling have also been successfully used for event coreference resolution (Bejan and Harabagiu, 2010; Chen and Ng, 2015). However, both semi-supervised and unsupervised approaches have been found empirically lagging behind the supervised models (Lu and Ng, 2018). The closest to our work are weakly-supervised and self-training methods that have been shown useful for many information extraction and classification tasks (Riloff, 1996; Riloff and Wiebe, 2003; Xie et al., 2019). But, to the best of our knowledge, we are the first to explore discourse-aware strategies to automatically label event coreference relations and use them exclusively or use them to augment existing human-annotated data for training event coreference resolution systems. 3 relation classes. First, we extract all the verb paraphrase pairs as the potential event trigger words. While event mentions can take other part of speech types, we limit our paraphrase pairs to verbs to ensure high precision among the collected event trigger words. Additionally, many of the verb paraphras"
2021.eacl-main.101,M95-1005,0,0.239809,"h event coreference relations with no restriction on event types or subtypes, thus, allowing us to evaluate coreference resolution performance on a broad range of events. Besides, we evaluate the performance of models across text genres by evaluating our models trained with news articles on KBP 2017 discussion forum documents. Following previous work on event coreference resolution, we evaluate all the event coreference resolution systems using the official KBP 2017 scorer v1.8. The scorer employs four coreference scoring measures, namely B 3 (Bagga and Baldwin, 1998), CEAFe (Luo, 2005), MUC (Vilain et al., 1995) and BLANC (Recasens and Hovy, 2011) and the unweighted average of their F1 scores AV GF 1 . In addition, since MUC directly evaluates pairwise coreference links, we also report MUC precision and recall scores. 5.2 Implementation Details We use an ensemble of multi-layer feed-forward neural network classifiers to identify event men11 ECB+ (Cybulska and Vossen, 2014) is another popular dataset for evaluating event coreference resolution. However, documents in ECB+ are selectively annotated, comprising only of event mentions and within-document coreference chains that are relevant to cross-docum"
2021.eacl-main.101,W12-3205,0,0.0199994,"orary work by Meged et al. (2020) has also studied the potential correlation between coreferential event trigger words and predicate paraphrases. 7 http://nlpgrid.seas.upenn.edu/PPDB/ eng/ppdb-2.0-tldr.gz Post-Filtering Paraphrase-based Event Pairs using Functional News Discourse Structure To generate the news discourse structure proposed by Van Dijk (Teun A, 1986; Van Dijk, 1988a,b) and specify the discourse role of a sentence with respect to events in the document, we use the discourse profiling system proposed by Choubey et al. (2020). Note that the above discourse structure is functional (Webber and Joshi, 2012) and does not specify relations between two discourse units. Instead, it classifies each sentence in a document into one of the eight content types. Each content type describes the specific role of a sentence in describing the main event, context informing events, and other historical or future projected events. The eight content types include main event (M1) sentences that describe the most newsworthy event of a news article. Sentences describing events that happen recently and act as triggers for the main event and events that are triggered by the main event constitute the previous event (C1"
2021.eacl-main.101,Q15-1037,0,0.0184175,"prove the performance of an event coreference resolution system on a new text genre. We suspect that, for generalization across different text genres, we may require specialized learning algorithms, e.g., text style adaptation, which is not in the scope of this work. 2 Related Work The existing literature on supervised event coreference resolution primarily focuses on designing pairwise classifier based on the surface linguistic features such as lexical features comprising of lemma and part-of-speech tag similarity of event words (Bejan and Harabagiu, 2010; Lee et al., 2012; Liu et al., 2014; Yang et al., 2015; Lu et al., 2016; Cremisini and Finlayson, 2020), argument overlap (Chen et al., 2009; McConky et al., 2012; Sangeetha and Arock, 2012; Bejan and Harabagiu, 2014; Yang et al., 2015; Lu et al., 2016; Choubey and Huang, 2017), semantic similarity based on lexical resources such as wordnet (Bejan and Harabagiu, 2010; Liu et al., 2014; Yu et al., 2016) and word embeddings (Yang et al., 2015; Choubey and Huang, 2017; Kenyon-Dean et al., 2018; Barhom et al., 2019; Zuo et al., 2019; Pandian et al., 2020; Sahlani et al., 2020; Lu et al., 2020), and discourse features such as token and sentence distan"
2021.findings-acl.295,D15-1263,0,0.0248621,"ment as well as the overall news structure type. We built a joint model for these two tasks to preserve the two-way dependencies and constraints between them, and have empirically improved the performance of both tasks. In the previous work, several well-studied genreindependent discourse structures have been explored for improving many NLP applications. For example, discourse structures including the RSTstyle tree structure (Mann and Thompson, 1988) and the PDTB-style discourse relations (Prasad et al., 2008) have been shown useful for a range of NLP applications, such as sentiment analysis (Bhatia et al., 2015; M¨arkle-Huß et al., 2017), text 3333 3 Paragraph-level News Element Tags summarization (Marcu, 1997; Louis et al., 2010) and machine translation (Li et al., 2014; Guzm´an et al., 2014). In addition, text segmentation (Hearst, 1994) that divides a text into a sequence of topically coherent segments by detecting topic transition boundaries have been shown useful for text summarization (Barzilay and Lee, 2004), sentiment analysis (Sauper et al., 2010) and dialogue systems (Shi et al., 2019). We believe that the genre-speciﬁc news structures can effectively complement the genre-independent disco"
2021.findings-acl.295,N18-2055,1,0.878684,"Missing"
2021.findings-acl.295,W18-4308,1,0.616365,"amid is not the only news structure, there exist several other commonly used news structures as well, for example, a structure called Kabob is commonly used to present a narrative hook (Myers and Wukasch, 2003) ﬁrst and then report the main story, where the narrative hook catches the reader’s attention so that reader is willing to keep reading. Recognizing the overall structure of a news article can beneﬁt many NLP tasks and applications, such as text summarization, text segmentation, discourse analysis, information extraction and text quality assessment, and many others. Our recent research (Dai et al., 2018) ﬁrst deﬁnes a small set of news elements, speciﬁcally ﬁve news elements, and then formally deﬁnes four commonly used news structures based on their different ways to select and organize news elements. News elements are deﬁned based on their functions in a news story (introducing the main story or event, catching the reader’s attention or providing details, etc.) as well as their writing styles (narrative or expository, also known as modes of discourse). Speciﬁcally, ﬁve news elements are deﬁned, including two ledes, Standard Lede and Image Lede, with their functions as either introducing the"
2021.findings-acl.295,N19-1423,0,0.038788,"tractive summarization, which aims to extract a summary by identifying the most important sentences in a news article. Nallapati et al. (2017) presents one of the earliest neural network systems for extractive summarization that adopt an RNN-based encoder for abstracting sentence representations. More recent work achieves higher performance for extractive summarization using more sophisticated neural network structures. SUMO (Liu et al., 2019) introduces structured attention to induce a dependency tree representation of a document while generating a summary. Liu and Lapata (2019) adapts BERT (Devlin et al., 2019) to text summarization which obtains contextualized representations of a document and its sentences using BERT’s encoder by stacking several inter-sentence Transformer layers. Dong et al. (2019) ﬁne-tunes a new Uniﬁed pre-trained Language Model (UniLM) for text summarization by employing a shared Transformer network and utilizing speciﬁc self-attention masks to control which context the predicting summary conditions on. The extrinsic evaluation on text summarization using (Liu and Lapata, 2019) as baseline demonstrates the usefulness of our system predicted genre-speciﬁc news structure tags in"
2021.findings-acl.295,P14-1065,0,0.0631744,"Missing"
2021.findings-acl.295,P94-1002,0,0.753993,"k, several well-studied genreindependent discourse structures have been explored for improving many NLP applications. For example, discourse structures including the RSTstyle tree structure (Mann and Thompson, 1988) and the PDTB-style discourse relations (Prasad et al., 2008) have been shown useful for a range of NLP applications, such as sentiment analysis (Bhatia et al., 2015; M¨arkle-Huß et al., 2017), text 3333 3 Paragraph-level News Element Tags summarization (Marcu, 1997; Louis et al., 2010) and machine translation (Li et al., 2014; Guzm´an et al., 2014). In addition, text segmentation (Hearst, 1994) that divides a text into a sequence of topically coherent segments by detecting topic transition boundaries have been shown useful for text summarization (Barzilay and Lee, 2004), sentiment analysis (Sauper et al., 2010) and dialogue systems (Shi et al., 2019). We believe that the genre-speciﬁc news structures can effectively complement the genre-independent discourse structures, and both of them are essential for achieving deep story-level text understanding. In this work, we further apply our system predicted news structure and news element tags to help the task of extractive summarization,"
2021.findings-acl.295,P14-2047,0,0.0129309,"empirically improved the performance of both tasks. In the previous work, several well-studied genreindependent discourse structures have been explored for improving many NLP applications. For example, discourse structures including the RSTstyle tree structure (Mann and Thompson, 1988) and the PDTB-style discourse relations (Prasad et al., 2008) have been shown useful for a range of NLP applications, such as sentiment analysis (Bhatia et al., 2015; M¨arkle-Huß et al., 2017), text 3333 3 Paragraph-level News Element Tags summarization (Marcu, 1997; Louis et al., 2010) and machine translation (Li et al., 2014; Guzm´an et al., 2014). In addition, text segmentation (Hearst, 1994) that divides a text into a sequence of topically coherent segments by detecting topic transition boundaries have been shown useful for text summarization (Barzilay and Lee, 2004), sentiment analysis (Sauper et al., 2010) and dialogue systems (Shi et al., 2019). We believe that the genre-speciﬁc news structures can effectively complement the genre-independent discourse structures, and both of them are essential for achieving deep story-level text understanding. In this work, we further apply our system predicted news structu"
2021.findings-acl.295,W04-1013,0,0.0365391,"element called Synopsis) will summarize the main story. Therefore, this news genre brings additional difﬁculty to locate the correct paragraphs for extracting summary, and accordingly, recognizing this genre and its news elements is likely to noticeably improve text summarization performance on documents with the Kabob structure the most. Indeed, the extrinsic evaluation on the CNN/DailyMail dataset (Hermann et al., 2015) shows that a simple method for incorporating news genre tags as word features into a recent extractive summarization system (Liu and Lapata, 2019) improves the three ROUGE (Lin, 2004) scores, R-1, R-2 and R-L, consistently for all four types of news structure genres, with the Kabob structure receiving the largest improvements of 0.37, 0.14 and 0.34 points on R-1, R-2 and R-L respectively. 2 Related Work News structures have been extensively studied in the area of linguistics and journalism (Schokkenbroek, 1999; Van Dijk, 1985; Ytreberg, 2001). However, few computational studies tried to automatically categorize news articles according to news structures using data-driven methods. Our previous work (Dai et al., 2018) is the ﬁrst work we are aware of that formulated four new"
2021.findings-acl.295,D19-1387,0,0.111108,"he following paragraphs (corresponding to a news element called Synopsis) will summarize the main story. Therefore, this news genre brings additional difﬁculty to locate the correct paragraphs for extracting summary, and accordingly, recognizing this genre and its news elements is likely to noticeably improve text summarization performance on documents with the Kabob structure the most. Indeed, the extrinsic evaluation on the CNN/DailyMail dataset (Hermann et al., 2015) shows that a simple method for incorporating news genre tags as word features into a recent extractive summarization system (Liu and Lapata, 2019) improves the three ROUGE (Lin, 2004) scores, R-1, R-2 and R-L, consistently for all four types of news structure genres, with the Kabob structure receiving the largest improvements of 0.37, 0.14 and 0.34 points on R-1, R-2 and R-L respectively. 2 Related Work News structures have been extensively studied in the area of linguistics and journalism (Schokkenbroek, 1999; Van Dijk, 1985; Ytreberg, 2001). However, few computational studies tried to automatically categorize news articles according to news structures using data-driven methods. Our previous work (Dai et al., 2018) is the ﬁrst work we"
2021.findings-acl.295,N19-1173,0,0.099411,"sential for achieving deep story-level text understanding. In this work, we further apply our system predicted news structure and news element tags to help the task of extractive summarization, which aims to extract a summary by identifying the most important sentences in a news article. Nallapati et al. (2017) presents one of the earliest neural network systems for extractive summarization that adopt an RNN-based encoder for abstracting sentence representations. More recent work achieves higher performance for extractive summarization using more sophisticated neural network structures. SUMO (Liu et al., 2019) introduces structured attention to induce a dependency tree representation of a document while generating a summary. Liu and Lapata (2019) adapts BERT (Devlin et al., 2019) to text summarization which obtains contextualized representations of a document and its sentences using BERT’s encoder by stacking several inter-sentence Transformer layers. Dong et al. (2019) ﬁne-tunes a new Uniﬁed pre-trained Language Model (UniLM) for text summarization by employing a shared Transformer network and utilizing speciﬁc self-attention masks to control which context the predicting summary conditions on. The"
2021.findings-acl.295,P14-5010,0,0.00250699,"en a sequence of feature-rich word vectors (w1 , w2 , ..., wL ) as the input, the word-level BiLSTM layer will reﬁne the word wi ’s hidden representation (wi′ ) by modeling the word-level inter-dependencies: wi′ = BiLST M (w1 , ..., wi , ..., wL ) of CNN with 50 hidden units followed by a max-pooling layer. 4 For word-level features, we collected the corresponding paragraph’s position (PARA) index, capitalization (CAP) ﬂag, Part-of-speech (POS) tag and named entity (NER) tag of each word. The embedding sizes for PARA/CAP/POS/NER were 20/5/35/20 respectively. We used Standford CoreNLP toolkit (Manning et al., 2014) to generate POS and NER tags. 5 GloVe embeddings were ﬁxed during training. For ELMo and BERT, we also froze its parameters during model training. Paragraph-level BiLSTM Layer: Given a sequence of word representations (w1′ , w2′ , ..., wL′ ), we build the paragraph representation (pj ) for the j-th paragraph in the document, by applying maxpooling operation over the sequence of word representations for all words within the j-th paragraph: pj = max wi′ wi ∈pj Then, the paragraph-level BiLSTM layer will update the j-th paragraph’s hidden representation (p′j ) by modeling the paragraph-level int"
2021.findings-acl.295,W97-0713,0,0.62898,"wo-way dependencies and constraints between them, and have empirically improved the performance of both tasks. In the previous work, several well-studied genreindependent discourse structures have been explored for improving many NLP applications. For example, discourse structures including the RSTstyle tree structure (Mann and Thompson, 1988) and the PDTB-style discourse relations (Prasad et al., 2008) have been shown useful for a range of NLP applications, such as sentiment analysis (Bhatia et al., 2015; M¨arkle-Huß et al., 2017), text 3333 3 Paragraph-level News Element Tags summarization (Marcu, 1997; Louis et al., 2010) and machine translation (Li et al., 2014; Guzm´an et al., 2014). In addition, text segmentation (Hearst, 1994) that divides a text into a sequence of topically coherent segments by detecting topic transition boundaries have been shown useful for text summarization (Barzilay and Lee, 2004), sentiment analysis (Sauper et al., 2010) and dialogue systems (Shi et al., 2019). We believe that the genre-speciﬁc news structures can effectively complement the genre-independent discourse structures, and both of them are essential for achieving deep story-level text understanding. In"
2021.findings-acl.295,D14-1162,0,0.084614,"Missing"
2021.findings-acl.295,N18-1202,0,0.00907871,"yer Max-pooling p1 ... ... p2 Word representations w&apos;i pj Word-level BiLSTM Layer Word vectors Char-level CNN Char emb ... wi Word emb Features emb (GloVe/ELMo/BERT) (POS/NER/...) Figure 2: The Joint Model Architecture for both Document-level and Paragraph-level News Genre Tags Prediction. word-level features embedding 4 as: wi = [wiword ; wichar ; wif eatures ] To take advantage of the recent progress about contextualized word representation from pre-trained language models, our framework supports three options including 300 dimensional GloVe (Pennington et al., 2014), 1024 dimensional ELMo (Peters et al., 2018) and the “bert-basecased” version of BERT (Devlin et al., 2019) to initialize 5 the wiword . Word-level BiLSTM Layer: Given a sequence of feature-rich word vectors (w1 , w2 , ..., wL ) as the input, the word-level BiLSTM layer will reﬁne the word wi ’s hidden representation (wi′ ) by modeling the word-level inter-dependencies: wi′ = BiLST M (w1 , ..., wi , ..., wL ) of CNN with 50 hidden units followed by a max-pooling layer. 4 For word-level features, we collected the corresponding paragraph’s position (PARA) index, capitalization (CAP) ﬂag, Part-of-speech (POS) tag and named entity (NER) tag"
2021.findings-acl.295,prasad-etal-2008-penn,0,0.0108798,"of downstream applications, we developed a computational system to recognize news elements within a document as well as the overall news structure type. We built a joint model for these two tasks to preserve the two-way dependencies and constraints between them, and have empirically improved the performance of both tasks. In the previous work, several well-studied genreindependent discourse structures have been explored for improving many NLP applications. For example, discourse structures including the RSTstyle tree structure (Mann and Thompson, 1988) and the PDTB-style discourse relations (Prasad et al., 2008) have been shown useful for a range of NLP applications, such as sentiment analysis (Bhatia et al., 2015; M¨arkle-Huß et al., 2017), text 3333 3 Paragraph-level News Element Tags summarization (Marcu, 1997; Louis et al., 2010) and machine translation (Li et al., 2014; Guzm´an et al., 2014). In addition, text segmentation (Hearst, 1994) that divides a text into a sequence of topically coherent segments by detecting topic transition boundaries have been shown useful for text summarization (Barzilay and Lee, 2004), sentiment analysis (Sauper et al., 2010) and dialogue systems (Shi et al., 2019)."
2021.findings-acl.295,W09-1119,0,0.103969,"s Narration Body Section # 519 244 237 113 746 Table 1: Data Statistics of the News Genre Dataset. 4 Model 4.1 The Joint Model for Predicting both News Structures and News Elements Figure 2 illustrates the overall architecture of our joint model, which can simultaneously predict both document-level news structure label and paragraphlevel news element tags. The model processes a whole news article containing a sequence of paragraphs each time, and predicts a document-level label as well as a sequence of paragraph-level tags with one tag for each paragraph using the standard BIO tagging schema (Ratinov and Roth, 2009) for sequence labeling. Speciﬁcally, we treat the news element Body Section as the “other” (or ‘O’) tag since this tag can’t help determine document-level news structure type (shown in Figure 1) and was used as a catch-all “other” label during the data annotation as well. For other paragraph-level news element tags except for the Body Section, we assign a “B-” preﬁx to the ﬁrst paragraph that starts the news element and assign “I-” preﬁx to other paragraphs inside the same news element. The model employs the two-level hierarchical BiLSTM layers (Schuster and Paliwal, 1997) with max-pooling (Co"
2021.findings-acl.295,N19-1178,0,0.0211977,"rasad et al., 2008) have been shown useful for a range of NLP applications, such as sentiment analysis (Bhatia et al., 2015; M¨arkle-Huß et al., 2017), text 3333 3 Paragraph-level News Element Tags summarization (Marcu, 1997; Louis et al., 2010) and machine translation (Li et al., 2014; Guzm´an et al., 2014). In addition, text segmentation (Hearst, 1994) that divides a text into a sequence of topically coherent segments by detecting topic transition boundaries have been shown useful for text summarization (Barzilay and Lee, 2004), sentiment analysis (Sauper et al., 2010) and dialogue systems (Shi et al., 2019). We believe that the genre-speciﬁc news structures can effectively complement the genre-independent discourse structures, and both of them are essential for achieving deep story-level text understanding. In this work, we further apply our system predicted news structure and news element tags to help the task of extractive summarization, which aims to extract a summary by identifying the most important sentences in a news article. Nallapati et al. (2017) presents one of the earliest neural network systems for extractive summarization that adopt an RNN-based encoder for abstracting sentence rep"
2021.findings-emnlp.137,Q19-1011,0,0.0115023,"ons and projected consequences that are labeled as Expectation (D4). For automatic text segmentation, multitude of approaches such as lexical overlap, bayesian learning or dynamic programming (Hearst, 1997; Choi, 2000; Utiyama and Isahara, 2001; Eisenstein and Barzilay, 2008; Du et al., 2013) have been proposed. The recent works rely on neural network 4 Model models to learn different aspects of text segmentation such as coherence and cohesion (Wang et al., We model the discourse profiling task as a two 2017; Sehikh et al., 2017; Bahdanau et al., 2016a; step process. Given a news document X : Arnold et al., 2019). {H, x1 , x2 , .., xn } comprising of headline H and 1596 Figure 2: Neural-Network Architecture, including Gradient Flow Paths, for Incorporating Document-level Content Structures in a Discourse Profiling System n sentences with their content-type labels Y : {y1 , y2 , .., yn }, our main goal is to learn a model f : X → Y that classifies each sentence xi in the document X to its content type yi . In the first step, a latent function fT : X → T ∈ {1, 2, .., n}k , a classifier, is used to identify k subtopic boundary sentences in the document. These boundary sentences are used to partition docu"
2021.findings-emnlp.137,S17-1027,0,0.0220468,"most newsworthy event, i.e. main content. This aligns with our inverted pyramid structure based on rule of relevance ordering, where a main sentence following non-main sentences indicates the segment boundaries. Besides, feature news also follow welldefined content organization structures such as having an introductory anecdotes or back-grounding which is taken care by our rules to separate historical and anecdotal contents from the relevance ordering. Neural Models for Discourse Modeling Deep neural networks have been successfully explored for modeling discourse (Ji and Eisenstein, 2014a,b; Becker et al., 2017), including hierarchical models (Li et al., 2016b; Liu and Lapata, 2017; Dai and Huang, 2018) to induce hierarchical structure. Morey et al. (2017), however, found that some of the improvements from neural models on RST parsing are attributed to differences in evaluation procedures. Nonetheless, Morey et al. (2017) concluded that neural models are more effective in modeling discourse, though the relative error reduction rates are lower than reported. The better discourse modeling capabilities of neural models are also evident from their widespread adoption in follow-up works such as Lin et al."
2021.findings-emnlp.137,2021.eacl-main.101,1,0.75087,"Missing"
2021.findings-emnlp.137,2020.acl-main.478,1,0.92318,"its fine-grained discourse function within into different content types, where each content local subtopical context. Further interactions betype characterizes the specific discourse role of a sentence in describing a news story (Choubey et al., tween subtopics and the document vector helps to determine the broader role of a subtopic with re2020). It is vital to effectively contextualize the spect to the main content. For instance, in news occurrence of a news event, which has been shown useful for extracting event structures from a docu- document in Figure 1, we can identify discourse ment (Choubey et al., 2020; Choubey and Huang, role of sentence S7 by combining the two levels of information. First, sentences S6 and S8 describe 2021). Furthermore, this task is likely to benefit a events that happened years before the main event range of other NLP applications that require deep which can be modelled through interaction between story-level text understanding, such as text summadocument and subtopic embedding corresponding rization and complex question answering. As the discourse roles are interpreted with re- to [S6-S8]. Then, events in S7 has temporal proximity with the events in sentences S6 and S8"
2021.findings-emnlp.137,N18-1013,1,0.85287,"sed on rule of relevance ordering, where a main sentence following non-main sentences indicates the segment boundaries. Besides, feature news also follow welldefined content organization structures such as having an introductory anecdotes or back-grounding which is taken care by our rules to separate historical and anecdotal contents from the relevance ordering. Neural Models for Discourse Modeling Deep neural networks have been successfully explored for modeling discourse (Ji and Eisenstein, 2014a,b; Becker et al., 2017), including hierarchical models (Li et al., 2016b; Liu and Lapata, 2017; Dai and Huang, 2018) to induce hierarchical structure. Morey et al. (2017), however, found that some of the improvements from neural models on RST parsing are attributed to differences in evaluation procedures. Nonetheless, Morey et al. (2017) concluded that neural models are more effective in modeling discourse, though the relative error reduction rates are lower than reported. The better discourse modeling capabilities of neural models are also evident from their widespread adoption in follow-up works such as Lin et al. (2019); Zhang et al. (2020) and Koto et al. (2021). Reinforcement Learning for NLP Applicati"
2021.findings-emnlp.137,W18-4308,1,0.803048,"requently used to set the tone for a news article or to highlight main argument with personal experiences or historical events. Algorithm 1 Rules to identify subtopic boundary sentences for the Inverted Pyramid structure xi xi 3: xi 4: xi 1: 2: 6 ∈ {M 1, M 2} and xi−1 ∈ {C1 − D4} ∈ {C1, C2} and xi−1 ∈ {D1 − D4} ∈ {D1, D2} and xi−1 ∈ / {D1, D2} ∈ / {D1, D2} and xi−1 ∈ {D1, D2} Evaluation 6.1 Dataset We also consider the inverted pyramid structure We evaluate our content organization structure(Po¨ ttker, 2003), that is most often used in news aware model on the NewsDiscourse Corpus2 . It media (Dai et al., 2018). It organizes the news consists of 802 English news articles taken from content in decreasing order of relevance, placing three different news sources, NYT, Xinhua and the most relevant information at the top and then Reuters, and covers business, crime, disaster and arranging the remaining details in an decreasing politics domains. Each sentence in a news docuorder of relevance. While the inverted pyramid is ment is annotated with one of the eight discourse a global content organization structure, we made a content types (described in §3), and additionally simplifying assumption that a docum"
2021.findings-emnlp.137,N13-1019,0,0.0172223,"situations that are often fictional or personal accounts of incidents of an unknown person (Anecdotal Event (D2)). Lastly, opinionated contents including reactions from immediate participants, experts, known personalities as well as journalists or news sources are covered in the Evaluation (D3) category, except speculations and projected consequences that are labeled as Expectation (D4). For automatic text segmentation, multitude of approaches such as lexical overlap, bayesian learning or dynamic programming (Hearst, 1997; Choi, 2000; Utiyama and Isahara, 2001; Eisenstein and Barzilay, 2008; Du et al., 2013) have been proposed. The recent works rely on neural network 4 Model models to learn different aspects of text segmentation such as coherence and cohesion (Wang et al., We model the discourse profiling task as a two 2017; Sehikh et al., 2017; Bahdanau et al., 2016a; step process. Given a news document X : Arnold et al., 2019). {H, x1 , x2 , .., xn } comprising of headline H and 1596 Figure 2: Neural-Network Architecture, including Gradient Flow Paths, for Incorporating Document-level Content Structures in a Discourse Profiling System n sentences with their content-type labels Y : {y1 , y2 , .."
2021.findings-emnlp.137,D08-1035,0,0.078602,"cal Event (D1)) or unverifiable situations that are often fictional or personal accounts of incidents of an unknown person (Anecdotal Event (D2)). Lastly, opinionated contents including reactions from immediate participants, experts, known personalities as well as journalists or news sources are covered in the Evaluation (D3) category, except speculations and projected consequences that are labeled as Expectation (D4). For automatic text segmentation, multitude of approaches such as lexical overlap, bayesian learning or dynamic programming (Hearst, 1997; Choi, 2000; Utiyama and Isahara, 2001; Eisenstein and Barzilay, 2008; Du et al., 2013) have been proposed. The recent works rely on neural network 4 Model models to learn different aspects of text segmentation such as coherence and cohesion (Wang et al., We model the discourse profiling task as a two 2017; Sehikh et al., 2017; Bahdanau et al., 2016a; step process. Given a news document X : Arnold et al., 2019). {H, x1 , x2 , .., xn } comprising of headline H and 1596 Figure 2: Neural-Network Architecture, including Gradient Flow Paths, for Incorporating Document-level Content Structures in a Discourse Profiling System n sentences with their content-type labels"
2021.findings-emnlp.137,J97-1003,0,0.921124,"ures depending on the used ment embedding to obtain the underlying main segmentation criteria. In this paper, we consider 1 Code and data are available at https://github. two subtopic structures: 1) broad-genre topic segcom/prafulla77/Discoure_Profiling_RL_ EMNLP21Findings ments generated by the TextTiling algorithm; and 1594 Findings of the Association for Computational Linguistics: EMNLP 2021, pages 1594–1605 November 7–11, 2021. ©2021 Association for Computational Linguistics Figure 1: An example document annotated with three different subtopic structures. The first is based on TextTiling (Hearst, 1997) and is shown with the black-solid line ([S1-S8],[S9-S11]). The second structure is based on locally inverted pyramid structure (discussed in § 5.2) and is shown through red-dashed lines ([S1-S5],[S6-S8],[S9-S11]). The third, shown by colored boxes, segments document based on the temporal position where the first segment (S1, S2) focuses on the main event, second segment (S3, S4, S5) describes events following the main event, third segment (S6, S7, S8) describes historical events and the last segment (S9, S10, S11) again covers current context. 2) news genre-specific inverted pyramid structure"
2021.findings-emnlp.137,P14-1002,0,0.161904,"vent with repetitions of the most newsworthy event, i.e. main content. This aligns with our inverted pyramid structure based on rule of relevance ordering, where a main sentence following non-main sentences indicates the segment boundaries. Besides, feature news also follow welldefined content organization structures such as having an introductory anecdotes or back-grounding which is taken care by our rules to separate historical and anecdotal contents from the relevance ordering. Neural Models for Discourse Modeling Deep neural networks have been successfully explored for modeling discourse (Ji and Eisenstein, 2014a,b; Becker et al., 2017), including hierarchical models (Li et al., 2016b; Liu and Lapata, 2017; Dai and Huang, 2018) to induce hierarchical structure. Morey et al. (2017), however, found that some of the improvements from neural models on RST parsing are attributed to differences in evaluation procedures. Nonetheless, Morey et al. (2017) concluded that neural models are more effective in modeling discourse, though the relative error reduction rates are lower than reported. The better discourse modeling capabilities of neural models are also evident from their widespread adoption in follow-up"
2021.findings-emnlp.137,2021.eacl-main.60,0,0.0138809,"Li et al., 2016b; Liu and Lapata, 2017; Dai and Huang, 2018) to induce hierarchical structure. Morey et al. (2017), however, found that some of the improvements from neural models on RST parsing are attributed to differences in evaluation procedures. Nonetheless, Morey et al. (2017) concluded that neural models are more effective in modeling discourse, though the relative error reduction rates are lower than reported. The better discourse modeling capabilities of neural models are also evident from their widespread adoption in follow-up works such as Lin et al. (2019); Zhang et al. (2020) and Koto et al. (2021). Reinforcement Learning for NLP Applications Reinforcement learning has been frequently used for sequence generation tasks to mitigate exposure bias or to directly optimize task-specific evaluation metrics such as BLEU score (Ranzato et al., 2015; Henß et al., 2015; Bahdanau et al., 2016b; Paulus et al., 2017; Fedus et al., 2018). In addition, RL have been explored for range of NLP tasks such as question-answering (Xiong et al., 2017), dialog generation (Li et al., 2016a), text summarization (Chen and Bansal, 2018), knowledge-graph reasoning (Lin et al., 2018) and relation extraction (Qin et"
2021.findings-emnlp.137,D16-1127,0,0.0367357,"s with our inverted pyramid structure based on rule of relevance ordering, where a main sentence following non-main sentences indicates the segment boundaries. Besides, feature news also follow welldefined content organization structures such as having an introductory anecdotes or back-grounding which is taken care by our rules to separate historical and anecdotal contents from the relevance ordering. Neural Models for Discourse Modeling Deep neural networks have been successfully explored for modeling discourse (Ji and Eisenstein, 2014a,b; Becker et al., 2017), including hierarchical models (Li et al., 2016b; Liu and Lapata, 2017; Dai and Huang, 2018) to induce hierarchical structure. Morey et al. (2017), however, found that some of the improvements from neural models on RST parsing are attributed to differences in evaluation procedures. Nonetheless, Morey et al. (2017) concluded that neural models are more effective in modeling discourse, though the relative error reduction rates are lower than reported. The better discourse modeling capabilities of neural models are also evident from their widespread adoption in follow-up works such as Lin et al. (2019); Zhang et al. (2020) and Koto et al. (20"
2021.findings-emnlp.137,D16-1035,0,0.0223648,"s with our inverted pyramid structure based on rule of relevance ordering, where a main sentence following non-main sentences indicates the segment boundaries. Besides, feature news also follow welldefined content organization structures such as having an introductory anecdotes or back-grounding which is taken care by our rules to separate historical and anecdotal contents from the relevance ordering. Neural Models for Discourse Modeling Deep neural networks have been successfully explored for modeling discourse (Ji and Eisenstein, 2014a,b; Becker et al., 2017), including hierarchical models (Li et al., 2016b; Liu and Lapata, 2017; Dai and Huang, 2018) to induce hierarchical structure. Morey et al. (2017), however, found that some of the improvements from neural models on RST parsing are attributed to differences in evaluation procedures. Nonetheless, Morey et al. (2017) concluded that neural models are more effective in modeling discourse, though the relative error reduction rates are lower than reported. The better discourse modeling capabilities of neural models are also evident from their widespread adoption in follow-up works such as Lin et al. (2019); Zhang et al. (2020) and Koto et al. (20"
2021.findings-emnlp.137,D18-1362,0,0.0140948,"(2019); Zhang et al. (2020) and Koto et al. (2021). Reinforcement Learning for NLP Applications Reinforcement learning has been frequently used for sequence generation tasks to mitigate exposure bias or to directly optimize task-specific evaluation metrics such as BLEU score (Ranzato et al., 2015; Henß et al., 2015; Bahdanau et al., 2016b; Paulus et al., 2017; Fedus et al., 2018). In addition, RL have been explored for range of NLP tasks such as question-answering (Xiong et al., 2017), dialog generation (Li et al., 2016a), text summarization (Chen and Bansal, 2018), knowledge-graph reasoning (Lin et al., 2018) and relation extraction (Qin et al., 2018). To the best of our knowledge, we are the first to explore RL techniques for exposing underlying content organization structures in news articles as well as using a linguistically motivated critic to reduce the variance of reinforce algorithm. 3 Task Description News discourse profiling categorizes sentences in news articles into eight schematic categories that are defined following the news content schemata proposed by Van Dijk (Teun A, 1986; Van Dijk, 1988a,b; Choubey et al., 2020). The eight content types describe the common discourse roles of sen"
2021.findings-emnlp.137,P19-1410,0,0.0176089,"al., 2017), including hierarchical models (Li et al., 2016b; Liu and Lapata, 2017; Dai and Huang, 2018) to induce hierarchical structure. Morey et al. (2017), however, found that some of the improvements from neural models on RST parsing are attributed to differences in evaluation procedures. Nonetheless, Morey et al. (2017) concluded that neural models are more effective in modeling discourse, though the relative error reduction rates are lower than reported. The better discourse modeling capabilities of neural models are also evident from their widespread adoption in follow-up works such as Lin et al. (2019); Zhang et al. (2020) and Koto et al. (2021). Reinforcement Learning for NLP Applications Reinforcement learning has been frequently used for sequence generation tasks to mitigate exposure bias or to directly optimize task-specific evaluation metrics such as BLEU score (Ranzato et al., 2015; Henß et al., 2015; Bahdanau et al., 2016b; Paulus et al., 2017; Fedus et al., 2018). In addition, RL have been explored for range of NLP tasks such as question-answering (Xiong et al., 2017), dialog generation (Li et al., 2016a), text summarization (Chen and Bansal, 2018), knowledge-graph reasoning (Lin et"
2021.findings-emnlp.137,D17-1133,0,0.0161435,"d pyramid structure based on rule of relevance ordering, where a main sentence following non-main sentences indicates the segment boundaries. Besides, feature news also follow welldefined content organization structures such as having an introductory anecdotes or back-grounding which is taken care by our rules to separate historical and anecdotal contents from the relevance ordering. Neural Models for Discourse Modeling Deep neural networks have been successfully explored for modeling discourse (Ji and Eisenstein, 2014a,b; Becker et al., 2017), including hierarchical models (Li et al., 2016b; Liu and Lapata, 2017; Dai and Huang, 2018) to induce hierarchical structure. Morey et al. (2017), however, found that some of the improvements from neural models on RST parsing are attributed to differences in evaluation procedures. Nonetheless, Morey et al. (2017) concluded that neural models are more effective in modeling discourse, though the relative error reduction rates are lower than reported. The better discourse modeling capabilities of neural models are also evident from their widespread adoption in follow-up works such as Lin et al. (2019); Zhang et al. (2020) and Koto et al. (2021). Reinforcement Lear"
2021.findings-emnlp.137,D17-1136,0,0.0154633,"e following non-main sentences indicates the segment boundaries. Besides, feature news also follow welldefined content organization structures such as having an introductory anecdotes or back-grounding which is taken care by our rules to separate historical and anecdotal contents from the relevance ordering. Neural Models for Discourse Modeling Deep neural networks have been successfully explored for modeling discourse (Ji and Eisenstein, 2014a,b; Becker et al., 2017), including hierarchical models (Li et al., 2016b; Liu and Lapata, 2017; Dai and Huang, 2018) to induce hierarchical structure. Morey et al. (2017), however, found that some of the improvements from neural models on RST parsing are attributed to differences in evaluation procedures. Nonetheless, Morey et al. (2017) concluded that neural models are more effective in modeling discourse, though the relative error reduction rates are lower than reported. The better discourse modeling capabilities of neural models are also evident from their widespread adoption in follow-up works such as Lin et al. (2019); Zhang et al. (2020) and Koto et al. (2021). Reinforcement Learning for NLP Applications Reinforcement learning has been frequently used fo"
2021.findings-emnlp.137,N18-1202,0,0.0314783,"Missing"
2021.findings-emnlp.137,P18-1199,0,0.0182044,"(2021). Reinforcement Learning for NLP Applications Reinforcement learning has been frequently used for sequence generation tasks to mitigate exposure bias or to directly optimize task-specific evaluation metrics such as BLEU score (Ranzato et al., 2015; Henß et al., 2015; Bahdanau et al., 2016b; Paulus et al., 2017; Fedus et al., 2018). In addition, RL have been explored for range of NLP tasks such as question-answering (Xiong et al., 2017), dialog generation (Li et al., 2016a), text summarization (Chen and Bansal, 2018), knowledge-graph reasoning (Lin et al., 2018) and relation extraction (Qin et al., 2018). To the best of our knowledge, we are the first to explore RL techniques for exposing underlying content organization structures in news articles as well as using a linguistically motivated critic to reduce the variance of reinforce algorithm. 3 Task Description News discourse profiling categorizes sentences in news articles into eight schematic categories that are defined following the news content schemata proposed by Van Dijk (Teun A, 1986; Van Dijk, 1988a,b; Choubey et al., 2020). The eight content types describe the common discourse roles of sentences in telling a news story. Specificall"
2021.findings-emnlp.137,P01-1064,0,0.305197,"n months and years (Historical Event (D1)) or unverifiable situations that are often fictional or personal accounts of incidents of an unknown person (Anecdotal Event (D2)). Lastly, opinionated contents including reactions from immediate participants, experts, known personalities as well as journalists or news sources are covered in the Evaluation (D3) category, except speculations and projected consequences that are labeled as Expectation (D4). For automatic text segmentation, multitude of approaches such as lexical overlap, bayesian learning or dynamic programming (Hearst, 1997; Choi, 2000; Utiyama and Isahara, 2001; Eisenstein and Barzilay, 2008; Du et al., 2013) have been proposed. The recent works rely on neural network 4 Model models to learn different aspects of text segmentation such as coherence and cohesion (Wang et al., We model the discourse profiling task as a two 2017; Sehikh et al., 2017; Bahdanau et al., 2016a; step process. Given a news document X : Arnold et al., 2019). {H, x1 , x2 , .., xn } comprising of headline H and 1596 Figure 2: Neural-Network Architecture, including Gradient Flow Paths, for Incorporating Document-level Content Structures in a Discourse Profiling System n sentences"
2021.findings-emnlp.137,D17-1139,0,0.0244083,"Missing"
2021.findings-emnlp.137,2020.acl-main.569,0,0.0145983,"ng hierarchical models (Li et al., 2016b; Liu and Lapata, 2017; Dai and Huang, 2018) to induce hierarchical structure. Morey et al. (2017), however, found that some of the improvements from neural models on RST parsing are attributed to differences in evaluation procedures. Nonetheless, Morey et al. (2017) concluded that neural models are more effective in modeling discourse, though the relative error reduction rates are lower than reported. The better discourse modeling capabilities of neural models are also evident from their widespread adoption in follow-up works such as Lin et al. (2019); Zhang et al. (2020) and Koto et al. (2021). Reinforcement Learning for NLP Applications Reinforcement learning has been frequently used for sequence generation tasks to mitigate exposure bias or to directly optimize task-specific evaluation metrics such as BLEU score (Ranzato et al., 2015; Henß et al., 2015; Bahdanau et al., 2016b; Paulus et al., 2017; Fedus et al., 2018). In addition, RL have been explored for range of NLP tasks such as question-answering (Xiong et al., 2017), dialog generation (Li et al., 2016a), text summarization (Chen and Bansal, 2018), knowledge-graph reasoning (Lin et al., 2018) and relat"
C16-1136,P05-1074,0,0.026393,"l models, which can help to reproduce training data of monolingual model. Che et al. (2013) exploited the complementary cues between two languages as bilingual constraints to help detect errors in a mono-lingual tagger task, which can improve the annotation quality of named entities. Zhu et al. (2013) translated English sentences into Chinese sentences (with the same topic) in ACE 2005 evaluation data with google machine translation system as a second text representation feature 1448 so as to alleviate the data sparseness problem effectively. Our method is also related to paraphrase learning (Bannard and Callison-Burch, 2005; Callison-Burch, 2008; Zhao et al., 2008; Snover et al., 2009; Ganitkevitch et al., 2013). However, there are two significant differences. First, paraphrase learning translates phrases strictly via word alignments while we use word alignments to find phrase spans on the target language. Second, our purpose is to obtain structured phrases (with syntactic constraints) rather than plain phrases as structured phrases can help us find new phrase structures as shown in Section 3. 7 Conclusion and Future Work We have presented a bilingual structure projection algorithm that explores structural diver"
C16-1136,W10-2906,0,0.0962151,"n word phrase (e.g., “sitins”) or phrases starting with a noun (e.g., “disobedience of order”), even a passive form phrase structure like “rallies held (in)”. In order to address this issue, we propose a simple yet effective bilingual structure projection method that explores syntactic divergences (Georgi et al., 2012) between two languages and mines new syntactic structures for event expressions and event facet phrases effectively using parallel corpora. This is inspired by many recent cross-lingual research that utilize the second language to provide a different view (Balcan and Blum, 2005; Burkett et al., 2010; Ganchev et al., 2012) and complementary cues (Che et al., 2013; Wang et al., 2013) in improving Natural language Processing (NLP) tasks for the target language, analogous to co-training (Chen and Ji, 2009; Wan, 2009; Hajmohammadi et al., 2015) but between two different languages. In order to learn new event phrases and their syntactic structures, we map phrases2 back and ∗ Corresponding author This work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details: http://creativecommons.org/licenses/by/4.0/ 1 “xcomp” is a dependency relation between a verb or a"
C16-1136,D08-1021,0,0.0361866,"uce training data of monolingual model. Che et al. (2013) exploited the complementary cues between two languages as bilingual constraints to help detect errors in a mono-lingual tagger task, which can improve the annotation quality of named entities. Zhu et al. (2013) translated English sentences into Chinese sentences (with the same topic) in ACE 2005 evaluation data with google machine translation system as a second text representation feature 1448 so as to alleviate the data sparseness problem effectively. Our method is also related to paraphrase learning (Bannard and Callison-Burch, 2005; Callison-Burch, 2008; Zhao et al., 2008; Snover et al., 2009; Ganitkevitch et al., 2013). However, there are two significant differences. First, paraphrase learning translates phrases strictly via word alignments while we use word alignments to find phrase spans on the target language. Second, our purpose is to obtain structured phrases (with syntactic constraints) rather than plain phrases as structured phrases can help us find new phrase structures as shown in Section 3. 7 Conclusion and Future Work We have presented a bilingual structure projection algorithm that explores structural divergences between languag"
C16-1136,W09-2307,0,0.0251991,"auses, LC: localizers, PU: punctuations, CD: cardinal numbers, MSP: some particles. 1445 Method H&R’s Iter #4 Iteration 1 Iteration 2 Iteration 3 Iteration 4 Phrases EP:623 PP:569 EP:1096 PP:2219 EP:4273 PP:4597 EP:8041 PP:9169 EP:9868 PP:11705 Recall Precision F1 71 88 79 76.2 86.5 81.1 79.2 86.0 82.5 79.2 86.0 82.5 79.2 86.0 82.5 Table 1: Results of the projection method using H&R’s phrase lists as seed phrases for expansion and projection Figure 5: F1-score curve against the number of iterations LDC2004T07. We ran Giza++ (Och, 2003) and Stanford dependency parser (De Marneffe et al., 2006; Chang et al., 2009) on the parallel sentence pairs to obtain word alignments and dependency trees. In addition, we used the same evaluation method and data as H&R’s. The evaluation data contains 400 news articles that were randomly sampled from the English Gigaword Fifth Edition corpora (Parker et al., 2011). Each article contains one of six commonly used civil unrest keywords or their morphological variations. The development set contains 100 documents and the rest 300 documents are used as the test set. 4.2 Event Recognition with Expanded Phrases We examine the effectiveness of our bilingual structure projecti"
C16-1136,N13-1006,0,0.136997,", “disobedience of order”), even a passive form phrase structure like “rallies held (in)”. In order to address this issue, we propose a simple yet effective bilingual structure projection method that explores syntactic divergences (Georgi et al., 2012) between two languages and mines new syntactic structures for event expressions and event facet phrases effectively using parallel corpora. This is inspired by many recent cross-lingual research that utilize the second language to provide a different view (Balcan and Blum, 2005; Burkett et al., 2010; Ganchev et al., 2012) and complementary cues (Che et al., 2013; Wang et al., 2013) in improving Natural language Processing (NLP) tasks for the target language, analogous to co-training (Chen and Ji, 2009; Wan, 2009; Hajmohammadi et al., 2015) but between two different languages. In order to learn new event phrases and their syntactic structures, we map phrases2 back and ∗ Corresponding author This work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details: http://creativecommons.org/licenses/by/4.0/ 1 “xcomp” is a dependency relation between a verb or an adjective and its open clausal complement in a dependency tree"
C16-1136,W09-2209,0,0.180693,"e yet effective bilingual structure projection method that explores syntactic divergences (Georgi et al., 2012) between two languages and mines new syntactic structures for event expressions and event facet phrases effectively using parallel corpora. This is inspired by many recent cross-lingual research that utilize the second language to provide a different view (Balcan and Blum, 2005; Burkett et al., 2010; Ganchev et al., 2012) and complementary cues (Che et al., 2013; Wang et al., 2013) in improving Natural language Processing (NLP) tasks for the target language, analogous to co-training (Chen and Ji, 2009; Wan, 2009; Hajmohammadi et al., 2015) but between two different languages. In order to learn new event phrases and their syntactic structures, we map phrases2 back and ∗ Corresponding author This work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details: http://creativecommons.org/licenses/by/4.0/ 1 “xcomp” is a dependency relation between a verb or an adjective and its open clausal complement in a dependency tree. In sentence “Workers came out to demonstrate”, the relation between verb “came” and verb “demonstrate” is “xcomp”. 2 We start with initial p"
C16-1136,P11-1061,0,0.0145497,"allies held in”. In addition, we have seen some new verb structures in English phrases that consist of a single verb or a verb with complex objects as shown in Table 5. 6 Related Work Recent years have witnessed increasing interests in leveraging bilingual corpora or resources to improve performance of monolingual NLP tasks. Generally, The introduction of bilingual corpora or resources serves two purposes. The first purpose is to alleviate the problem that we have few labeled instances in some resource-impoverished languages by a resource-rich language (Hwa et al., 2005; Ganchev et al., 2009; Das and Petrov, 2011; He et al., 2015). The second purpose is to leverage divergences found in different languages to obtain complementary cues (Li et al., 2012; Wang et al., 2013; Che et al., 2013) or extra information (Snyder et al., 2009; Burkett et al., 2010) from another language. Our projection method follows the latter. In the first purpose, Das and Petrov (2011) explored existing abundant English labeled resources as features to assist building tools for eight European languages. Different to projecting labels as feature, Wang and Manning (2014) proposed a method that projected model expectations as featu"
C16-1136,de-marneffe-etal-2006-generating,0,0.0528503,"Missing"
C16-1136,P09-1042,0,0.0313473,"voiced verb phrase “rallies held in”. In addition, we have seen some new verb structures in English phrases that consist of a single verb or a verb with complex objects as shown in Table 5. 6 Related Work Recent years have witnessed increasing interests in leveraging bilingual corpora or resources to improve performance of monolingual NLP tasks. Generally, The introduction of bilingual corpora or resources serves two purposes. The first purpose is to alleviate the problem that we have few labeled instances in some resource-impoverished languages by a resource-rich language (Hwa et al., 2005; Ganchev et al., 2009; Das and Petrov, 2011; He et al., 2015). The second purpose is to leverage divergences found in different languages to obtain complementary cues (Li et al., 2012; Wang et al., 2013; Che et al., 2013) or extra information (Snyder et al., 2009; Burkett et al., 2010) from another language. Our projection method follows the latter. In the first purpose, Das and Petrov (2011) explored existing abundant English labeled resources as features to assist building tools for eight European languages. Different to projecting labels as feature, Wang and Manning (2014) proposed a method that projected model"
C16-1136,N13-1092,0,0.0388965,"Missing"
C16-1136,georgi-etal-2012-measuring,0,0.024285,"d are ignored by the proposed algorithm. For instance, a verb phrase where two verbs are connected with a particular dependency relation “xcomp”1 , (e.g., “came out to demonstrate”) is one of these structures. Civil unrest events can also be invoked by some noun structure phrases, such as just a noun word phrase (e.g., “sitins”) or phrases starting with a noun (e.g., “disobedience of order”), even a passive form phrase structure like “rallies held (in)”. In order to address this issue, we propose a simple yet effective bilingual structure projection method that explores syntactic divergences (Georgi et al., 2012) between two languages and mines new syntactic structures for event expressions and event facet phrases effectively using parallel corpora. This is inspired by many recent cross-lingual research that utilize the second language to provide a different view (Balcan and Blum, 2005; Burkett et al., 2010; Ganchev et al., 2012) and complementary cues (Che et al., 2013; Wang et al., 2013) in improving Natural language Processing (NLP) tasks for the target language, analogous to co-training (Chen and Ji, 2009; Wan, 2009; Hajmohammadi et al., 2015) but between two different languages. In order to learn"
C16-1136,N13-1005,1,0.716283,"languages (Chinese and English) and mines new phrases with new syntactic structures, which have been ignored in the previous work. Experiments show that our approach can successfully find novel event phrases and structures, e.g., phrases headed by nouns. Furthermore, the newly mined phrases are capable of recognizing additional event descriptions and increasing the recall of event recognition. 1 Introduction Event recognition aims to identify documents that describe a specific type of event. Accurate event recognition is challenging due to ambiguities of event keywords. In the previous work, Huang and Riloff (2013) (hereafter H&R) proposed multi-faceted event recognition method that uses event expressions as well as event defining characteristics (aka “event facets”, such as “agents” and “purpose”) to achieve high accuracy in identifying civil unrest events. They also presented a bootstrapping solution that can learn event expressions and event facet phrases from unannotated texts. However, to achieve high quality phrases, strict syntactic constraints have been enforced and their bootstrapping algorithm can only learn two particular types of V-O (Verb-Object) Structure for both event expressions and fac"
C16-1136,P03-1021,0,0.0690839,"Missing"
C16-1136,P09-1009,0,0.0352952,"g interests in leveraging bilingual corpora or resources to improve performance of monolingual NLP tasks. Generally, The introduction of bilingual corpora or resources serves two purposes. The first purpose is to alleviate the problem that we have few labeled instances in some resource-impoverished languages by a resource-rich language (Hwa et al., 2005; Ganchev et al., 2009; Das and Petrov, 2011; He et al., 2015). The second purpose is to leverage divergences found in different languages to obtain complementary cues (Li et al., 2012; Wang et al., 2013; Che et al., 2013) or extra information (Snyder et al., 2009; Burkett et al., 2010) from another language. Our projection method follows the latter. In the first purpose, Das and Petrov (2011) explored existing abundant English labeled resources as features to assist building tools for eight European languages. Different to projecting labels as feature, Wang and Manning (2014) proposed a method that projected model expectations as feature for training. He et al. (2015) transferred the sentiment information of a resource-rich language to replenish the lost information of the target language. In the second purpose, Chen and Ji (2009) proposed a bootstrap"
C16-1136,P09-1027,0,0.0438148,"ingual structure projection method that explores syntactic divergences (Georgi et al., 2012) between two languages and mines new syntactic structures for event expressions and event facet phrases effectively using parallel corpora. This is inspired by many recent cross-lingual research that utilize the second language to provide a different view (Balcan and Blum, 2005; Burkett et al., 2010; Ganchev et al., 2012) and complementary cues (Che et al., 2013; Wang et al., 2013) in improving Natural language Processing (NLP) tasks for the target language, analogous to co-training (Chen and Ji, 2009; Wan, 2009; Hajmohammadi et al., 2015) but between two different languages. In order to learn new event phrases and their syntactic structures, we map phrases2 back and ∗ Corresponding author This work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details: http://creativecommons.org/licenses/by/4.0/ 1 “xcomp” is a dependency relation between a verb or an adjective and its open clausal complement in a dependency tree. In sentence “Workers came out to demonstrate”, the relation between verb “came” and verb “demonstrate” is “xcomp”. 2 We start with initial phrases lear"
C16-1136,Q14-1005,0,0.0156564,"resource-rich language (Hwa et al., 2005; Ganchev et al., 2009; Das and Petrov, 2011; He et al., 2015). The second purpose is to leverage divergences found in different languages to obtain complementary cues (Li et al., 2012; Wang et al., 2013; Che et al., 2013) or extra information (Snyder et al., 2009; Burkett et al., 2010) from another language. Our projection method follows the latter. In the first purpose, Das and Petrov (2011) explored existing abundant English labeled resources as features to assist building tools for eight European languages. Different to projecting labels as feature, Wang and Manning (2014) proposed a method that projected model expectations as feature for training. He et al. (2015) transferred the sentiment information of a resource-rich language to replenish the lost information of the target language. In the second purpose, Chen and Ji (2009) proposed a bootstrap framework of co-training among two languages, which uses Chinese event extraction as a case study and bilingual texts as a new source of information. Burkett et al. (2010) attached a bilingual model as a second view (Balcan and Blum, 2005; Ganchev et al., 2012) onto original monolingual models, and used rich features"
C16-1136,P08-1089,0,0.0303828,"onolingual model. Che et al. (2013) exploited the complementary cues between two languages as bilingual constraints to help detect errors in a mono-lingual tagger task, which can improve the annotation quality of named entities. Zhu et al. (2013) translated English sentences into Chinese sentences (with the same topic) in ACE 2005 evaluation data with google machine translation system as a second text representation feature 1448 so as to alleviate the data sparseness problem effectively. Our method is also related to paraphrase learning (Bannard and Callison-Burch, 2005; Callison-Burch, 2008; Zhao et al., 2008; Snover et al., 2009; Ganitkevitch et al., 2013). However, there are two significant differences. First, paraphrase learning translates phrases strictly via word alignments while we use word alignments to find phrase spans on the target language. Second, our purpose is to obtain structured phrases (with syntactic constraints) rather than plain phrases as structured phrases can help us find new phrase structures as shown in Section 3. 7 Conclusion and Future Work We have presented a bilingual structure projection algorithm that explores structural divergences between languages and can effectiv"
C16-2054,W12-0515,0,0.0171139,"tational Linguistics: System Demonstrations, pages 258–262, Osaka, Japan, December 11-17 2016. Figure 1: An example of an abbreviation being extracted with its source text. The algorithm attempts to match the POS of the head word of a parenthetical phrase with the first matching POS before the abbreviation. methods have been developed to overcome a reliance on language-dependent keywords using graph-based ranking (Mihalcea, 2005; Wong et al., 2008). A large body of recent work has been presented by Galgani and Hoffmann through LEXA, a system which uses citation analysis to generate summaries (Galgani et al., 2012a; Galgani and Hoffmann, 2010). LEXA includes an interface for continued system learning using Ripple-down Rules (RDR), which allows domain experts to evaluate sentence selections live and agree or disagree with the selections. When the experts agree on a relevant sentence, a new extraction pattern is added (Galgani et al., 2015). Galgani et al. continued their work in this domain with the development of a multi-technique approach to summarization, including ‘catchphrase’ analysis (Galgani et al., 2012b). CaseSummarizer is a multi-technique approach with a goal of providing a comprehensive int"
C16-2054,W04-1013,0,0.0402146,"Missing"
C16-2054,P08-2051,0,0.035848,"Missing"
C16-2054,P05-3013,0,0.045473,"ommons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/ License details: http:// 258 Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: System Demonstrations, pages 258–262, Osaka, Japan, December 11-17 2016. Figure 1: An example of an abbreviation being extracted with its source text. The algorithm attempts to match the POS of the head word of a parenthetical phrase with the first matching POS before the abbreviation. methods have been developed to overcome a reliance on language-dependent keywords using graph-based ranking (Mihalcea, 2005; Wong et al., 2008). A large body of recent work has been presented by Galgani and Hoffmann through LEXA, a system which uses citation analysis to generate summaries (Galgani et al., 2012a; Galgani and Hoffmann, 2010). LEXA includes an interface for continued system learning using Ripple-down Rules (RDR), which allows domain experts to evaluate sentence selections live and agree or disagree with the selections. When the experts agree on a relevant sentence, a new extraction pattern is added (Galgani et al., 2015). Galgani et al. continued their work in this domain with the development of a mu"
C16-2054,C08-1124,0,0.0189393,"on 4.0 International License. creativecommons.org/licenses/by/4.0/ License details: http:// 258 Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: System Demonstrations, pages 258–262, Osaka, Japan, December 11-17 2016. Figure 1: An example of an abbreviation being extracted with its source text. The algorithm attempts to match the POS of the head word of a parenthetical phrase with the first matching POS before the abbreviation. methods have been developed to overcome a reliance on language-dependent keywords using graph-based ranking (Mihalcea, 2005; Wong et al., 2008). A large body of recent work has been presented by Galgani and Hoffmann through LEXA, a system which uses citation analysis to generate summaries (Galgani et al., 2012a; Galgani and Hoffmann, 2010). LEXA includes an interface for continued system learning using Ripple-down Rules (RDR), which allows domain experts to evaluate sentence selections live and agree or disagree with the selections. When the experts agree on a relevant sentence, a new extraction pattern is added (Galgani et al., 2015). Galgani et al. continued their work in this domain with the development of a multi-technique approa"
D13-1066,D12-1091,0,0.0212027,"Missing"
D13-1066,W10-2914,0,0.141457,"sions, represented using punctuation and other keyboard characters, to be more predictive of irony1 in contrast to features representing structured linguistic knowledge in Por1 They adopted the term ‘irony’ instead of ‘sarcasm’ to refer to the case when a word or expression with prior positive polarity is figuratively used to express a negative opinion. 705 tuguese. Filatova (2012) presented a detailed description of sarcasm corpus creation with sarcasm annotations of Amazon product reviews. Their annotations capture sarcasm both at the document level and the text utterance level. Tsur et al. (2010) presented a semi-supervised learning framework that exploits syntactic and pattern based features in sarcastic sentences of Amazon product reviews. They observed correlated sentiment words such as “yay!” or “great!” often occurring in their most useful patterns. Davidov et al. (2010) used sarcastic tweets and sarcastic Amazon product reviews to train a sarcasm classifier with syntactic and pattern-based features. They examined whether tweets with a sarcasm hashtag are reliable enough indicators of sarcasm to be used as a gold standard for evaluation, but found that sarcasm hashtags are noisy"
D13-1066,filatova-2012-irony,0,0.509927,"ial dialogue to learn lexical N-gram cues associated with sarcasm (e.g., “oh really”, “I get it”, “no way”, etc.) as well as lexico-syntactic patterns. In opinionated user posts, Carvalho et al. (2009) found oral or gestural expressions, represented using punctuation and other keyboard characters, to be more predictive of irony1 in contrast to features representing structured linguistic knowledge in Por1 They adopted the term ‘irony’ instead of ‘sarcasm’ to refer to the case when a word or expression with prior positive polarity is figuratively used to express a negative opinion. 705 tuguese. Filatova (2012) presented a detailed description of sarcasm corpus creation with sarcasm annotations of Amazon product reviews. Their annotations capture sarcasm both at the document level and the text utterance level. Tsur et al. (2010) presented a semi-supervised learning framework that exploits syntactic and pattern based features in sarcastic sentences of Amazon product reviews. They observed correlated sentiment words such as “yay!” or “great!” often occurring in their most useful patterns. Davidov et al. (2010) used sarcastic tweets and sarcastic Amazon product reviews to train a sarcasm classifier wit"
D13-1066,P11-2102,0,0.64887,"c sentences of Amazon product reviews. They observed correlated sentiment words such as “yay!” or “great!” often occurring in their most useful patterns. Davidov et al. (2010) used sarcastic tweets and sarcastic Amazon product reviews to train a sarcasm classifier with syntactic and pattern-based features. They examined whether tweets with a sarcasm hashtag are reliable enough indicators of sarcasm to be used as a gold standard for evaluation, but found that sarcasm hashtags are noisy and possibly biased towards the hardest form of sarcasm (where even humans have difficulty). Gonz´alez-Ib´an˜ ez et al. (2011) explored the usefulness of lexical and pragmatic features for sarcasm detection in tweets. They used sarcasm hashtags as gold labels. They found positive and negative emotions in tweets, determined through fixed word dictionaries, to have a strong correlation with sarcasm. Liebrecht et al. (2013) explored Ngram features from 1 to 3-grams to build a classifier to recognize sarcasm in Dutch tweets. They made an interesting observation from their most effective Ngram features that people tend to be more sarcastic towards specific topics such as school, homework, weather, returning from vacation,"
D13-1066,W07-0101,0,0.680534,"follow a positive sentiment (initially, the seed word “love”). Second, we learn positive sentiment phrases that occur near a negative situation phrase. The bootstrapping process iterates, alternately learning new negative situations and new positive sentiment phrases. Finally, we use the learned lists of sentiment and situation phrases to recognize sarcasm in new tweets by identifying contexts that contain a positive sentiment in close proximity to a negative situation phrase. 2 Related Work Researchers have investigated the use of lexical and syntactic features to recognize sarcasm in text. Kreuz and Caucci (2007) studied the role that different lexical factors play, such as interjections (e.g., “gee” or “gosh”) and punctuation symbols (e.g., ‘?’) in recognizing sarcasm in narratives. Lukin and Walker (2013) explored the potential of a bootstrapping method for sarcasm classification in social dialogue to learn lexical N-gram cues associated with sarcasm (e.g., “oh really”, “I get it”, “no way”, etc.) as well as lexico-syntactic patterns. In opinionated user posts, Carvalho et al. (2009) found oral or gestural expressions, represented using punctuation and other keyboard characters, to be more predictiv"
D13-1066,W13-1605,0,0.583115,"Missing"
D13-1066,W13-1104,0,0.489692,"ly learning new negative situations and new positive sentiment phrases. Finally, we use the learned lists of sentiment and situation phrases to recognize sarcasm in new tweets by identifying contexts that contain a positive sentiment in close proximity to a negative situation phrase. 2 Related Work Researchers have investigated the use of lexical and syntactic features to recognize sarcasm in text. Kreuz and Caucci (2007) studied the role that different lexical factors play, such as interjections (e.g., “gee” or “gosh”) and punctuation symbols (e.g., ‘?’) in recognizing sarcasm in narratives. Lukin and Walker (2013) explored the potential of a bootstrapping method for sarcasm classification in social dialogue to learn lexical N-gram cues associated with sarcasm (e.g., “oh really”, “I get it”, “no way”, etc.) as well as lexico-syntactic patterns. In opinionated user posts, Carvalho et al. (2009) found oral or gestural expressions, represented using punctuation and other keyboard characters, to be more predictive of irony1 in contrast to features representing structured linguistic knowledge in Por1 They adopted the term ‘irony’ instead of ‘sarcasm’ to refer to the case when a word or expression with prior"
D13-1066,N13-1039,0,0.209974,"egative situations and new positive sentiment phrases. Finally, we use the learned lists of sentiment and situation phrases to recognize sarcasm in new tweets by identifying contexts that contain a positive sentiment in close proximity to a negative situation phrase. 2 Related Work Researchers have investigated the use of lexical and syntactic features to recognize sarcasm in text. Kreuz and Caucci (2007) studied the role that different lexical factors play, such as interjections (e.g., “gee” or “gosh”) and punctuation symbols (e.g., ‘?’) in recognizing sarcasm in narratives. Lukin and Walker (2013) explored the potential of a bootstrapping method for sarcasm classification in social dialogue to learn lexical N-gram cues associated with sarcasm (e.g., “oh really”, “I get it”, “no way”, etc.) as well as lexico-syntactic patterns. In opinionated user posts, Carvalho et al. (2009) found oral or gestural expressions, represented using punctuation and other keyboard characters, to be more predictive of irony1 in contrast to features representing structured linguistic knowledge in Por1 They adopted the term ‘irony’ instead of ‘sarcasm’ to refer to the case when a word or expression with prior"
D13-1066,N13-1059,0,0.0107603,"that the negative situation phrase is preceded by a positive sentiment. We harvest the n-grams that precede the negative situation phrases as positive sentiment candidates, score and select the best candidates, and add them to a list of positive sentiment phrases. The bootstrapping process then iterates, alternately learning more positive sentiment phrases and more negative situation phrases. We also observed that positive sentiments are frequently expressed as predicative phrases (i.e., predicate adjectives and predicate nominals). For example: “I’m taking calculus. It is awesome. #sarcasm”. Wiegand et al. (2013) offered a related observation that adjectives occurring in predicate adjective constructions are more likely to convey subjectivity than adjectives occurring in non-predicative structures. Therefore we also include a step in the learning process to harvest predicative phrases that occur in close proximity to a negative situation phrase. In the following sections, we explain each step of the bootstrapping process in more detail. 3.4 3.2 We extract three n-grams as candidate negative situation phrases: Bootstrapping Data For the learning process, we used Twitter’s streaming API to obtain a larg"
D13-1066,H05-2018,1,0.557344,"Missing"
D13-1066,H05-1044,0,0.175212,"Missing"
D13-1162,D12-1091,0,0.0223871,"Missing"
D13-1162,P08-1119,1,0.790288,"nt-based text classification (Riloff and Lehnert, 1994) and web page classification (Furnkranz et al., 1998). Semantic information has also been incorporated for text classification. However, most previous work relies on existing semantic resources, such as Wordnet (Scott and Stan, 1998; Bloehdorn and Hotho, 2006) or Wikipedia (Wang et al., 2009). There is also a rich history of automatic lexicon induction from text corpora (e.g., (Roark and Charniak, 1998; Riloff and Jones, 1999; McIntosh and Curran, 2009)), Wikipedia (e.g., (Vyas and Pantel, 2009)), and the Web (e.g., (Etzioni et al., 2005; Kozareva et al., 2008; Carlson et al., 2010)). The novel aspects of our work are in using an IE tagger to harvest a domain-specific lexicon from unannotated texts, and using the induced lexicon to encode domain-specific features for text classification. existing resources such as Wordnet (Miller, 1990), our veterinary message board posts are filled with informal and unconventional vocabulary. For example, one might naively assume that “male” and “female” are sufficient to identify gender. But the gender of animals is often revealed by describing their spayed/neutered status, often indicated with shorthand notation"
D13-1162,P09-1045,0,0.0152593,"Lodhi et al., 2001). Information extraction techniques have been used previously to create richer features for event-based text classification (Riloff and Lehnert, 1994) and web page classification (Furnkranz et al., 1998). Semantic information has also been incorporated for text classification. However, most previous work relies on existing semantic resources, such as Wordnet (Scott and Stan, 1998; Bloehdorn and Hotho, 2006) or Wikipedia (Wang et al., 2009). There is also a rich history of automatic lexicon induction from text corpora (e.g., (Roark and Charniak, 1998; Riloff and Jones, 1999; McIntosh and Curran, 2009)), Wikipedia (e.g., (Vyas and Pantel, 2009)), and the Web (e.g., (Etzioni et al., 2005; Kozareva et al., 2008; Carlson et al., 2010)). The novel aspects of our work are in using an IE tagger to harvest a domain-specific lexicon from unannotated texts, and using the induced lexicon to encode domain-specific features for text classification. existing resources such as Wordnet (Miller, 1990), our veterinary message board posts are filled with informal and unconventional vocabulary. For example, one might naively assume that “male” and “female” are sufficient to identify gender. But the gender of"
D13-1162,P98-2182,0,0.0992692,"ation (LDA) (Br et al., 2008) and string kernels (Lodhi et al., 2001). Information extraction techniques have been used previously to create richer features for event-based text classification (Riloff and Lehnert, 1994) and web page classification (Furnkranz et al., 1998). Semantic information has also been incorporated for text classification. However, most previous work relies on existing semantic resources, such as Wordnet (Scott and Stan, 1998; Bloehdorn and Hotho, 2006) or Wikipedia (Wang et al., 2009). There is also a rich history of automatic lexicon induction from text corpora (e.g., (Roark and Charniak, 1998; Riloff and Jones, 1999; McIntosh and Curran, 2009)), Wikipedia (e.g., (Vyas and Pantel, 2009)), and the Web (e.g., (Etzioni et al., 2005; Kozareva et al., 2008; Carlson et al., 2010)). The novel aspects of our work are in using an IE tagger to harvest a domain-specific lexicon from unannotated texts, and using the induced lexicon to encode domain-specific features for text classification. existing resources such as Wordnet (Miller, 1990), our veterinary message board posts are filled with informal and unconventional vocabulary. For example, one might naively assume that “male” and “female” a"
D13-1162,W98-0706,0,0.0998684,"d features. Researchers have also investigated clustering (Baker and McCallum, 1998), Latent Semantic Indexing (LSI) (Zelikovitz and Hirsh, 2001), Latent Dirichlet Allocation (LDA) (Br et al., 2008) and string kernels (Lodhi et al., 2001). Information extraction techniques have been used previously to create richer features for event-based text classification (Riloff and Lehnert, 1994) and web page classification (Furnkranz et al., 1998). Semantic information has also been incorporated for text classification. However, most previous work relies on existing semantic resources, such as Wordnet (Scott and Stan, 1998; Bloehdorn and Hotho, 2006) or Wikipedia (Wang et al., 2009). There is also a rich history of automatic lexicon induction from text corpora (e.g., (Roark and Charniak, 1998; Riloff and Jones, 1999; McIntosh and Curran, 2009)), Wikipedia (e.g., (Vyas and Pantel, 2009)), and the Web (e.g., (Etzioni et al., 2005; Kozareva et al., 2008; Carlson et al., 2010)). The novel aspects of our work are in using an IE tagger to harvest a domain-specific lexicon from unannotated texts, and using the induced lexicon to encode domain-specific features for text classification. existing resources such as Wordne"
D13-1162,N09-1033,0,0.0136629,"hniques have been used previously to create richer features for event-based text classification (Riloff and Lehnert, 1994) and web page classification (Furnkranz et al., 1998). Semantic information has also been incorporated for text classification. However, most previous work relies on existing semantic resources, such as Wordnet (Scott and Stan, 1998; Bloehdorn and Hotho, 2006) or Wikipedia (Wang et al., 2009). There is also a rich history of automatic lexicon induction from text corpora (e.g., (Roark and Charniak, 1998; Riloff and Jones, 1999; McIntosh and Curran, 2009)), Wikipedia (e.g., (Vyas and Pantel, 2009)), and the Web (e.g., (Etzioni et al., 2005; Kozareva et al., 2008; Carlson et al., 2010)). The novel aspects of our work are in using an IE tagger to harvest a domain-specific lexicon from unannotated texts, and using the induced lexicon to encode domain-specific features for text classification. existing resources such as Wordnet (Miller, 1990), our veterinary message board posts are filled with informal and unconventional vocabulary. For example, one might naively assume that “male” and “female” are sufficient to identify gender. But the gender of animals is often revealed by describing the"
D13-1162,C98-2177,0,\N,Missing
D16-1005,W13-3520,0,0.0267057,"r each of the three classes, Past, Ongoing and Future. To alleviate overfitting of the CNN model, we applied dropout (Hinton et al., 2012) on the convolution layer and the following pooling layer with a keeping rate of 0.5. Our experiments used the 300-dimension English word2vec embeddings14 trained on 100 billion words of Google News. We trained our own 300dimension Spanish embeddings, running word2vec (Mikolov et al., 2013) over both Spanish Gigaword (Mendon et al., 2011)— tokenized using Stanford CoreNLP SpanishTokenizer (Manning et al., 2014)— and the pre-tokenized Spanish Wikipedia dump (Al-Rfou et al., 2013). The vectors were then tuned during backpropagation for our specific task. 13 Stanford CoreNLP has no support for generating syntactic dependencies for Spanish. 14 docs.google.com/uc?id=0B7XkCwpI5KDYNlNUTTlSS21pQmM. Row 1 2 3 4 5 6 7 8 Method TIPSem TIPSem with transitivity SVM with all features SVM with BOW features only +Tense/Aspect/Time +Governing Word +Future Oriented Lexicon Convolutional Neural Net PA 26/80/39 75/76/75 91/81/86 88/80/84 89/81/85 90/81/85 90/82/86 91/83/87 OG 8/32/13 14/22/17 33/47/39 37/46/41 40/50/44 43/56/48 44/56/49 46/57/51 FU 4/23/7 4/21/7 45/58/51 40/53/45 42/52/"
D16-1005,P98-1013,0,0.625075,"Missing"
D16-1005,bejan-harabagiu-2008-linguistic,0,0.332676,"Missing"
D16-1005,S13-2002,0,0.227721,"at they are much harder to label because tense and aspect are less available than for events realized as finite verbs. Fourth, the EventStatus data set is multilingual: we collected data from both English and Spanish texts, allowing us to compare events representing the same event frame across two languages that are known to differ in their typological properties for describing events (Talmy, 1985). Using the new EventStatus corpus, we investigate two approaches for recognizing the temporal status of events. We create a SVM classifier that incorporates features drawn from prior TempEval work (Bethard, 2013; Chambers et al., 2014; Llorens et al., 2010) as well as a new automatically induced lexicon of 411 English and 348 Spanish “futureoriented” matrix verbs—verbs like “threaten” and “fear” whose complement clause or nominal direct object argument is likely to describe a future event. We show that the SVM outperforms a state-of-theart TempEval system and that the induced lexicon further improves performance for both English and Spanish. We also introduce a Convolutional Neural Network (CNN) to detect the temporal status of events. Our analysis shows that it successfully models semantic compositi"
D16-1005,P07-1073,0,0.0844152,"Missing"
D16-1005,P15-2072,0,0.0288722,"riants. We then randomly selected 2954 and 14915 news stories from the English Gigaword 5th Ed. (Parker et al., 2011) and Spanish Gigaword 3rd Ed. (Mendon et al., 2011) corpora, respectively, that contain at least one civil unrest phrase. Events of a specific type are very sparsely distributed in a large corpus like the Gigaword, so we used keyword matching just as a first pass to identify candidate event mentions. 3 The English keywords are “protest”, “strike”, “march”, “rally”, “riot” and “occupy”. These correspond to the most frequent words in the relevant frame in the Media Frames corpus (Card et al., 2015). Because “march” most commonly refers to the month, we removed the word itself and only kept its other morphological variations. 4 Spanish keywords: “marchar”, “protestar”, “amotinar(se)”, “manifestar(se)”, “huelga”, “manifestaci´on”, “disturbio”, “mot´ın”, “ocupar * la calle”, “tomar * la calle”, “salir * las calles”, “lanzarse a las calles”, “cacerolas vac´ıas”, “cacerolazo”, “cacerolada”. Asterisks could be replaced by up to 4 words. The last three terms are common expressions for protest marches in many countries of Latin America and Spain. 5 46 (out of 3000) and 9 (out of 1500) stories w"
D16-1005,Q14-1022,0,0.0644833,"Missing"
D16-1005,P98-1067,0,0.168207,"Missing"
D16-1005,D14-1181,0,0.00628674,"For the Spanish data, we used Stanford CoreNLP to generate Partof-Speech tags13 and then applied the MaltParser (Nivre et al., 2004) to generate dependencies. 4 Convolutional Neural Network Model Convolutional neural networks (CNNs) have been shown to be effective in modeling natural language semantics (Collobert et al., 2011). We were especially keen to find out whether the convolution operations of CNNs can model the semantic compositionality needed to detect temporal-aspectual status. For our experiments, we trained a simple CNN with one convolution layer followed by one max pooling layer (Kim, 2014; Collobert et al., 2011), The convolution layer has 300 hidden units. In each unit, the same affine transformation is applied to every consecutive 5 words (a filter instance) in the input sequence of words. A different affine transformation is applied to each hidden unit. After each affine transformation, a Rectified Linear Units (ReLU) (Nair and Hinton, 2010) non-linearity is applied. For each hidden unit, the max pooling layer selects the maximum value from the pool of real values generated from each filter instance. After the max pooling layer, a softmax classifier predicts probabilites fo"
D16-1005,S10-1063,0,0.284343,"se tense and aspect are less available than for events realized as finite verbs. Fourth, the EventStatus data set is multilingual: we collected data from both English and Spanish texts, allowing us to compare events representing the same event frame across two languages that are known to differ in their typological properties for describing events (Talmy, 1985). Using the new EventStatus corpus, we investigate two approaches for recognizing the temporal status of events. We create a SVM classifier that incorporates features drawn from prior TempEval work (Bethard, 2013; Chambers et al., 2014; Llorens et al., 2010) as well as a new automatically induced lexicon of 411 English and 348 Spanish “futureoriented” matrix verbs—verbs like “threaten” and “fear” whose complement clause or nominal direct object argument is likely to describe a future event. We show that the SVM outperforms a state-of-theart TempEval system and that the induced lexicon further improves performance for both English and Spanish. We also introduce a Convolutional Neural Network (CNN) to detect the temporal status of events. Our analysis shows that it successfully models semantic compositionality for some challenging temporal contexts"
D16-1005,S15-2134,0,0.0138314,"English and Spanish (Verhagen et al., 2010), and can compute the relation of each event with the Document Creation 50 Time. We applied TIPSem to our test set, mapping the DCT relations to our three event status classes15 . Row 1 of Tables 6 and 7 shows TIPSem results. The columns show results for each category separately, as well as macro-average and microaverage results across the three categories. Each cell shows the Recall/Precision/F-score numbers. Since TIPSem linked relatively few event mentions to the DCT, we next leveraged the transitivity of temporal relations (UzZaman et al., 2012; Llorens et al., 2015), linking an event to a DCT if the temporal relation between another event in the same sentence and the DCT is transferable. For instance, if event A is AFTER its DCT, and event B is AFTER event A, then event B is also AFTER the DCT.16 Row 2 shows the results of TIPSem with temporal transitivity. Even augmented by transitivity, TIPSem fails to detect many Ongoing (OG) and Future (FU) events; most mislabeled OG and FU events were nominal. Confusion matrices (Table 8) show that most of the 15 We used the obvious mappings from TIPSem relations: “BEFORE” to “PA”, “AFTER” to “FU” , and “INCLUDES” ("
D16-1005,P14-5010,0,0.00419871,"the max pooling layer, a softmax classifier predicts probabilites for each of the three classes, Past, Ongoing and Future. To alleviate overfitting of the CNN model, we applied dropout (Hinton et al., 2012) on the convolution layer and the following pooling layer with a keeping rate of 0.5. Our experiments used the 300-dimension English word2vec embeddings14 trained on 100 billion words of Google News. We trained our own 300dimension Spanish embeddings, running word2vec (Mikolov et al., 2013) over both Spanish Gigaword (Mendon et al., 2011)— tokenized using Stanford CoreNLP SpanishTokenizer (Manning et al., 2014)— and the pre-tokenized Spanish Wikipedia dump (Al-Rfou et al., 2013). The vectors were then tuned during backpropagation for our specific task. 13 Stanford CoreNLP has no support for generating syntactic dependencies for Spanish. 14 docs.google.com/uc?id=0B7XkCwpI5KDYNlNUTTlSS21pQmM. Row 1 2 3 4 5 6 7 8 Method TIPSem TIPSem with transitivity SVM with all features SVM with BOW features only +Tense/Aspect/Time +Governing Word +Future Oriented Lexicon Convolutional Neural Net PA 26/80/39 75/76/75 91/81/86 88/80/84 89/81/85 90/81/85 90/82/86 91/83/87 OG 8/32/13 14/22/17 33/47/39 37/46/41 40/50/44"
D16-1005,de-marneffe-etal-2006-generating,0,0.174951,"Missing"
D16-1005,N15-2023,0,0.0645627,"Missing"
D16-1005,W04-2407,0,0.0618242,"wo events but not when relating an event to the Document Creation Time, for which tense, aspect, and time expression features were the most useful (Llorens et al., 2010; Bethard, 2013). 12 We did not imitate this procedure for Spanish because the quality of our generated Spanish dependencies is poor. 49 pairs the governing word of an event mention with the dependency relation in between. We used Stanford CoreNLP (Marneffe et al., 2006) to generate dependencies for the English data. For the Spanish data, we used Stanford CoreNLP to generate Partof-Speech tags13 and then applied the MaltParser (Nivre et al., 2004) to generate dependencies. 4 Convolutional Neural Network Model Convolutional neural networks (CNNs) have been shown to be effective in modeling natural language semantics (Collobert et al., 2011). We were especially keen to find out whether the convolution operations of CNNs can model the semantic compositionality needed to detect temporal-aspectual status. For our experiments, we trained a simple CNN with one convolution layer followed by one max pooling layer (Kim, 2014; Collobert et al., 2011), The convolution layer has 300 hidden units. In each unit, the same affine transformation is appl"
D16-1005,N15-1044,0,0.0295513,"Missing"
D16-1005,S13-2001,0,0.0942687,"GOING ), or may happen in the future ( FUTURE ). We introduce a new task and corpus for studying the temporal/aspectual properties of major events. The EventStatus corpus consists of 4500 English and Spanish news articles about civil unrest events, such as protests, demonstrations, marches, and strikes, in which each event is annotated as PAST, O N -G OING, or F UTURE (sublabeled as P LANNED, A LERT or P OSSIBLE). This task bridges event extraction research and temporal research in the tradition of TIMEBANK (Pustejovsky et al., 2003) and TempEval (Verhagen et al., 2007; Verhagen et al., 2010; UzZaman et al., 2013). Previous corpora have begun this association: TIMEBANK, for example, includes temporal relations linking events with Document Creation Times (DCT). But the EventStatus task and corpus offers several new research directions. First, major societal events are often discussed before they happen, or while they are still happening, because they have the potential to impact a large number of people. News outlets frequently report on impending natural disasters (e.g., hurricanes), anticipated disease outbreaks (e.g., Zika virus), threats of terrorism, and plans or warnings of potential civil unrest"
D16-1005,S07-1014,0,0.155943,"y happened (PAST), is currently happening (ON GOING ), or may happen in the future ( FUTURE ). We introduce a new task and corpus for studying the temporal/aspectual properties of major events. The EventStatus corpus consists of 4500 English and Spanish news articles about civil unrest events, such as protests, demonstrations, marches, and strikes, in which each event is annotated as PAST, O N -G OING, or F UTURE (sublabeled as P LANNED, A LERT or P OSSIBLE). This task bridges event extraction research and temporal research in the tradition of TIMEBANK (Pustejovsky et al., 2003) and TempEval (Verhagen et al., 2007; Verhagen et al., 2010; UzZaman et al., 2013). Previous corpora have begun this association: TIMEBANK, for example, includes temporal relations linking events with Document Creation Times (DCT). But the EventStatus task and corpus offers several new research directions. First, major societal events are often discussed before they happen, or while they are still happening, because they have the potential to impact a large number of people. News outlets frequently report on impending natural disasters (e.g., hurricanes), anticipated disease outbreaks (e.g., Zika virus), threats of terrorism, an"
D16-1005,D11-1040,0,0.0545468,"Missing"
D16-1005,C98-1013,0,\N,Missing
D16-1005,C98-1064,0,\N,Missing
D16-1088,araki-etal-2014-detecting,0,0.065634,"nt extraction research (Appelt et al., 1993; Riloff, 1993; Soderland et al., 1995; Sudo et al., 2003; Li et al., 2005; Yu et al., 2005; Gu and Cercone, 2006; Maslennikov and Chua, 2007; S. and E., 2009; Liao and Grishman, 2010; Huang and Riloff, 2011; Chambers and Jurafsky, 2011; Huang and Riloff, 2012; Huang et al., 2016). Subevents as a theme has been discussed in the past three Event workshops (Eve, 2013), (Eve, 2014), (Eve, 2015). However, despite the great potential of using subevents to improve event detection and extraction (Hakeem 907 and Shah, 2005), and event coreference resolution (Araki et al., 2014), there is little existing research on automatically learning subevent phrases, partially because researchers have not agreed upon the definition of subevents. Much recent research in event timeline generation (Huang and Huang, 2013) suggests the usefulness of subevents in improving quality and completeness of automatically generated event summaries. However, they often focus on a different notion of subevents that broadly covers pre-condition events and consequence events and is temporally-based. Subevents have been studied for event tracking applications (Shen et al., 2013; Meladianos et al."
D16-1088,P11-1098,0,0.060448,"Missing"
D16-1088,P06-1061,0,0.0971061,"Missing"
D16-1088,D13-1068,0,0.0282315,"ang and Riloff, 2011; Chambers and Jurafsky, 2011; Huang and Riloff, 2012; Huang et al., 2016). Subevents as a theme has been discussed in the past three Event workshops (Eve, 2013), (Eve, 2014), (Eve, 2015). However, despite the great potential of using subevents to improve event detection and extraction (Hakeem 907 and Shah, 2005), and event coreference resolution (Araki et al., 2014), there is little existing research on automatically learning subevent phrases, partially because researchers have not agreed upon the definition of subevents. Much recent research in event timeline generation (Huang and Huang, 2013) suggests the usefulness of subevents in improving quality and completeness of automatically generated event summaries. However, they often focus on a different notion of subevents that broadly covers pre-condition events and consequence events and is temporally-based. Subevents have been studied for event tracking applications (Shen et al., 2013; Meladianos et al., 2015). However, most current research is specifically related to social media applications, like Twitter, in terms of both its definition of subevents and methodologies. For example, in previous research by (Shen et al., 2013), a s"
D16-1088,P11-1114,1,0.833094,"Missing"
D16-1088,N13-1005,1,0.929258,"se Approach for Subevent Extraction As illustrated in Figure 1, We use a two-phase algorithm to identify subevent phrases from our domainspecific corpus. For the first stage, we implemented a bootstrapped artificial neural network in order to identify sentences that are likely to contain a subevent phrase. In the second stage, we identify phrases fitting a predetermined conjunction pattern within the sentences classified by the first-stage neural network. 3.1 3.1.1 Phase 1: Identifying Subevent Sentences Domain-specific Corpus Thanks to previous research on multi-faceted event recognition by (Huang and Riloff, 2013), we compiled our own domain-specific corpus that describes civil unrest events. Using civil unrest events as an example, (Huang and Riloff, 2013) demonFigure 1: The Two-step Subevent Learning Paradigm strated that we can use automatically-learned event facet phrases (event agents and purposes) and main event expressions to find event articles with a high accuracy. We first obtained their learned event facet phrases and event expressions, most of which refer to general events. Then we followed their paper and identified two types of news articles that are likely to describe a civil unrest even"
D16-1088,P16-1025,0,0.048638,"Missing"
D16-1088,W05-0610,0,0.113406,"Missing"
D16-1088,P10-1081,0,0.0609152,"Missing"
D16-1088,P07-1075,0,0.0799682,"Missing"
D16-1088,D14-1162,0,0.0786509,"Missing"
D16-1088,D09-1016,0,0.0697451,"Missing"
D16-1088,N13-1135,0,0.069914,"Missing"
D16-1088,P03-1029,0,0.136937,"Missing"
D16-1088,P05-1062,0,0.118801,"Missing"
D17-1190,S13-2002,0,0.0354596,"previous works on temporal relation classification are based on feature-based classifiers. Mani et al. (2006) built MaxEnt classifier on hand-tagged features in the corpus, including tense, aspect, modality, polarity and event class for classifying temporal relations. Later Chambers et al. (2007) used a two-stage classifier which first learned imperfect event attributes and then combined them with other linguistic features in the second stage to perform the classification. The following works mostly expanded the feature sets (Cheng et al., 2007; Bethard and Martin, 2007; UzZaman et al., 2012; Bethard, 2013; Kolomiyets et al., 2012; Chambers, 2013; Laokulrat et al., 2013). Specifically, Chambers (2013) used direct dependency path between event pairs to capture syntactic context. Laokulrat et al. (2013) used 3-grams of paths between two event mentions in a dependency tree as features instead of full paths as those are too sparse. We found that modeling the entire path as one sequence provides greater compositional evidence on the temporal relation. In addition, modifiers attached to the words in a path with specific dependency relations like nmod:tmod are also informative. Ng (2013) proposed a hy"
D17-1190,S07-1025,0,0.0331526,"ature based models. 2 Related Works Most of the previous works on temporal relation classification are based on feature-based classifiers. Mani et al. (2006) built MaxEnt classifier on hand-tagged features in the corpus, including tense, aspect, modality, polarity and event class for classifying temporal relations. Later Chambers et al. (2007) used a two-stage classifier which first learned imperfect event attributes and then combined them with other linguistic features in the second stage to perform the classification. The following works mostly expanded the feature sets (Cheng et al., 2007; Bethard and Martin, 2007; UzZaman et al., 2012; Bethard, 2013; Kolomiyets et al., 2012; Chambers, 2013; Laokulrat et al., 2013). Specifically, Chambers (2013) used direct dependency path between event pairs to capture syntactic context. Laokulrat et al. (2013) used 3-grams of paths between two event mentions in a dependency tree as features instead of full paths as those are too sparse. We found that modeling the entire path as one sequence provides greater compositional evidence on the temporal relation. In addition, modifiers attached to the words in a path with specific dependency relations like nmod:tmod are also"
D17-1190,S13-2012,0,0.0460367,"sification are based on feature-based classifiers. Mani et al. (2006) built MaxEnt classifier on hand-tagged features in the corpus, including tense, aspect, modality, polarity and event class for classifying temporal relations. Later Chambers et al. (2007) used a two-stage classifier which first learned imperfect event attributes and then combined them with other linguistic features in the second stage to perform the classification. The following works mostly expanded the feature sets (Cheng et al., 2007; Bethard and Martin, 2007; UzZaman et al., 2012; Bethard, 2013; Kolomiyets et al., 2012; Chambers, 2013; Laokulrat et al., 2013). Specifically, Chambers (2013) used direct dependency path between event pairs to capture syntactic context. Laokulrat et al. (2013) used 3-grams of paths between two event mentions in a dependency tree as features instead of full paths as those are too sparse. We found that modeling the entire path as one sequence provides greater compositional evidence on the temporal relation. In addition, modifiers attached to the words in a path with specific dependency relations like nmod:tmod are also informative. Ng (2013) proposed a hybrid system for temporal relation classif"
D17-1190,P07-2044,0,0.238662,"ification between events. Our complete neural net model taking all the three types of sequences per1 In this paper, we restrict ourselves to study temporal relation classification between event mentions that are within one sentence. forms the best, which clearly outperforms feature based models. 2 Related Works Most of the previous works on temporal relation classification are based on feature-based classifiers. Mani et al. (2006) built MaxEnt classifier on hand-tagged features in the corpus, including tense, aspect, modality, polarity and event class for classifying temporal relations. Later Chambers et al. (2007) used a two-stage classifier which first learned imperfect event attributes and then combined them with other linguistic features in the second stage to perform the classification. The following works mostly expanded the feature sets (Cheng et al., 2007; Bethard and Martin, 2007; UzZaman et al., 2012; Bethard, 2013; Kolomiyets et al., 2012; Chambers, 2013; Laokulrat et al., 2013). Specifically, Chambers (2013) used direct dependency path between event pairs to capture syntactic context. Laokulrat et al. (2013) used 3-grams of paths between two event mentions in a dependency tree as features in"
D17-1190,D14-1082,0,0.0298994,"ng the sequence, we inverted the relation types in cases where relation type was annotated in opposite order. Final distribution of dataset is given in Table 1. 3.3 First, we extract words that are in the dependency path between two event mentions. However, event pairs can be very far in a sentence and are involved in complex syntactic structures. Therefore, we also apply two heuristic rules to deal with complex syntactic structures, e.g., two event mentions are in separate clauses and have a punctuation sign in their context. We describe our specific rules below. We used the Stanford parser (Chen and Manning, 2014) for generating dependency relations and parts-of-speech tags and all notations follow enhanced universal dependencies (De Marneffe and Manning, 2008). Rule 1 (punctuation): Comma directly influences the meaning in text and omitting it may alter the meaning of phrase. Therefore, include comma if it precedes or follows e1 , e2 or their modifiers. Rule 2 (children): Modifiers like now, then, will, yesterday, subsequent, when, was, etc. contains information on the temporal order of events and help in grounding events to the timeline. These modifiers are often related to event mentions with a spec"
D17-1190,S07-1052,0,0.0304655,"early outperforms feature based models. 2 Related Works Most of the previous works on temporal relation classification are based on feature-based classifiers. Mani et al. (2006) built MaxEnt classifier on hand-tagged features in the corpus, including tense, aspect, modality, polarity and event class for classifying temporal relations. Later Chambers et al. (2007) used a two-stage classifier which first learned imperfect event attributes and then combined them with other linguistic features in the second stage to perform the classification. The following works mostly expanded the feature sets (Cheng et al., 2007; Bethard and Martin, 2007; UzZaman et al., 2012; Bethard, 2013; Kolomiyets et al., 2012; Chambers, 2013; Laokulrat et al., 2013). Specifically, Chambers (2013) used direct dependency path between event pairs to capture syntactic context. Laokulrat et al. (2013) used 3-grams of paths between two event mentions in a dependency tree as features instead of full paths as those are too sparse. We found that modeling the entire path as one sequence provides greater compositional evidence on the temporal relation. In addition, modifiers attached to the words in a path with specific dependency relatio"
D17-1190,W04-3205,0,0.045667,"ll model: our complete sequential model considering POS, dependency and word forms sequences. Direct dependency path: the same as Full model except that the two heuristic rules were not applied in extracting sequences. Baseline I: a neural network classifier using discrete features described in Mirza and Tonelli (2014); Ng (2013). The features used are: POS tag, dependency relation, token and lemma of e1 (e2 ); dependency relations between e1 (e2 ) and their children; binary features indicating if e1 and e2 are related with the ’happensbefore’ or the ’similar’ relation according to VerbOcean (Chklovski and Pantel, 2004), if e1 and e2 have the same POS tag, or if e1 (e2 ) is the root and e1 modifies (or governs) e2 ; the dependency relation between e1 and e2 if they are directly connected in the dependency parse tree; prepositions that modify (or govern) e1 (e2 ); signal words (Derczynski and Gaizauskas, 2012) and entity distance between e1 and e2 . These features are concatenated and fed into an output neural layer with 14 neurons. Baseline II: a neural network classifier using POS tags and word forms of words in the surface path as input. The surface path consists of words that lie in between two event ment"
D17-1190,P06-1095,0,0.269721,"ree sequences and model compositional structural information, both syntactically and semantically. The evaluation shows that each type of sequences is useful to temporal relation classification between events. Our complete neural net model taking all the three types of sequences per1 In this paper, we restrict ourselves to study temporal relation classification between event mentions that are within one sentence. forms the best, which clearly outperforms feature based models. 2 Related Works Most of the previous works on temporal relation classification are based on feature-based classifiers. Mani et al. (2006) built MaxEnt classifier on hand-tagged features in the corpus, including tense, aspect, modality, polarity and event class for classifying temporal relations. Later Chambers et al. (2007) used a two-stage classifier which first learned imperfect event attributes and then combined them with other linguistic features in the second stage to perform the classification. The following works mostly expanded the feature sets (Cheng et al., 2007; Bethard and Martin, 2007; UzZaman et al., 2012; Bethard, 2013; Kolomiyets et al., 2012; Chambers, 2013; Laokulrat et al., 2013). Specifically, Chambers (2013"
D17-1190,E14-1033,0,0.133129,"elation. In addition, modifiers attached to the words in a path with specific dependency relations like nmod:tmod are also informative. Ng (2013) proposed a hybrid system for temporal relation classification that combines the learned classifier with 437 hand-coded rules. Their system first applied high-accuracy rules and then used the learned classifier, trained on rich features including those high-accuracy rules as features, to classify the cases that were not handled by the rules. Ng et al. (2013) also showed the effectiveness of different discourse analysis frameworks for this task. Later Mirza and Tonelli (2014) showed that a simpler approach based on lexico-syntactic features achieved results comparable to Ng (2013). They also reported that dependency order between events, either governordependent or dependent-governor, was not useful in their experiments. However, we show that dependency relations, when modeled as a sequence, contribute significantly to this task. 3 Temporal Link Labeling In this section, we describe the task of temporal relation classification, dataset, context words se1797 quence extraction model and the used recurrent neural net based classifier. 3.1 Task description Early works"
D17-1190,D13-1002,0,0.0134727,"e found that modeling the entire path as one sequence provides greater compositional evidence on the temporal relation. In addition, modifiers attached to the words in a path with specific dependency relations like nmod:tmod are also informative. Ng (2013) proposed a hybrid system for temporal relation classification that combines the learned classifier with 437 hand-coded rules. Their system first applied high-accuracy rules and then used the learned classifier, trained on rich features including those high-accuracy rules as features, to classify the cases that were not handled by the rules. Ng et al. (2013) also showed the effectiveness of different discourse analysis frameworks for this task. Later Mirza and Tonelli (2014) showed that a simpler approach based on lexico-syntactic features achieved results comparable to Ng (2013). They also reported that dependency order between events, either governordependent or dependent-governor, was not useful in their experiments. However, we show that dependency relations, when modeled as a sequence, contribute significantly to this task. 3 Temporal Link Labeling In this section, we describe the task of temporal relation classification, dataset, context wo"
D17-1190,N13-1112,0,0.215857,"l., 2012; Bethard, 2013; Kolomiyets et al., 2012; Chambers, 2013; Laokulrat et al., 2013). Specifically, Chambers (2013) used direct dependency path between event pairs to capture syntactic context. Laokulrat et al. (2013) used 3-grams of paths between two event mentions in a dependency tree as features instead of full paths as those are too sparse. We found that modeling the entire path as one sequence provides greater compositional evidence on the temporal relation. In addition, modifiers attached to the words in a path with specific dependency relations like nmod:tmod are also informative. Ng (2013) proposed a hybrid system for temporal relation classification that combines the learned classifier with 437 hand-coded rules. Their system first applied high-accuracy rules and then used the learned classifier, trained on rich features including those high-accuracy rules as features, to classify the cases that were not handled by the rules. Ng et al. (2013) also showed the effectiveness of different discourse analysis frameworks for this task. Later Mirza and Tonelli (2014) showed that a simpler approach based on lexico-syntactic features achieved results comparable to Ng (2013). They also re"
D17-1190,D14-1162,0,0.0840849,"during, during inv, identity. Six pairs among them are inverse of each other and other two types are commutative (e1 Re2 ≡ e2 Re1 , R ∈ {identical, simultaneous}). Our sequential model requires that relation Extracting Context Word Sequence Sequences and Classifier We form three sequences on the extracted context words (with t words), which are based on (i) parts-of-speech tags: PT = p1 , p2 , ..., pt (ii) dependency relations: DT = d1 , d2 , ..., dn 2 and (iii) word forms: WT = w1 , w2 , ..., wt . We transform each pi and di to a one-hot vector and each wi to a pre-trained embedding vector (Pennington et al., 2014). Then each sequence of vectors are encoded using their corresponding forward (LST Mf ) and backward (LST Mb ) LSTM layers. Classifier: Figure 2 shows an overview of our model. It consists of six LSTM (Hochreiter and 2 we only consider dependency relations for words in path connecting e1 and e2 . 1798 Figure 2: Bi-directional LSTM based classifier used for temporal relations classification. Schmidhuber, 1997) layers, three of them encode feature sequences in forward order and remaining in reverse order. LSTM layers for POS tag and dependency relation have 50 neurons and have dropouts of 0.20."
D17-1190,Q16-1010,0,0.0233678,"Missing"
D17-1190,Q16-1023,0,0.0301751,"an output neural layer with 14 neurons. Baseline II: a neural network classifier using POS tags and word forms of words in the surface path as input. The surface path consists of words that lie in between two event mentions based on the original sentence. The classifier uses four LSTM layers to encode both POS tag and word sequences in forward and backward order. The output neural layer and parameters for all LSTM layers are kept the same as the Full model. Baseline III: a neural network classifier based on event embeddings for both event mentions that were learned using bidirectional LSTMs (Kiperwasser and Goldberg, 2016). The learning uses two LSTM layers, each with 150 neurons and dropout of 0.2, to embed the forward and backward representations for each event mention. The input to LSTM layers are sequences of concatenated word embeddings and POS tags; each sequence corresponding to 19 context words to the left or to the right side of an event mention for the forward or the backward LSTM layer respectively. Event embeddings are then concatenated and fed into an output neural layer with 14 neurons. 1799 All baselines are trained using rmsprop optimizer on an objective function defined by categorical cross ent"
D17-1190,S13-2001,0,0.0417186,"this task. 3 Temporal Link Labeling In this section, we describe the task of temporal relation classification, dataset, context words se1797 quence extraction model and the used recurrent neural net based classifier. 3.1 Task description Early works on temporal relation classification Mani et al. (2006); Chambers et al. (2007) and the first two versions of TempEval (Verhagen et al., 2007, 2010) simplified the task by considering only six relation types. They combined the pair of relation types that are the inverse of each other and ignored the relations during and during inv. Then TempEval-3 (Uzzaman et al., 2013) extended the task to complete 14 class classification problem and all later works have considered all 14 relations. Our model performs 14-class classification following the recent works, as this is arguably more challenging (Ng, 2013). Also, we consider gold annotated event pairs, mainly because the corpus is small and distribution of relations is very skewed. All previous works focusing on the problem of classifying temporal relation types assumed gold annotation. 3.2 Dataset Relations After Before Simultaneous Identity Includes IS included Ended by During inv Begun by Begins IBefore IAfter"
D17-1190,P12-1010,0,0.0186739,"on temporal relation classification are based on feature-based classifiers. Mani et al. (2006) built MaxEnt classifier on hand-tagged features in the corpus, including tense, aspect, modality, polarity and event class for classifying temporal relations. Later Chambers et al. (2007) used a two-stage classifier which first learned imperfect event attributes and then combined them with other linguistic features in the second stage to perform the classification. The following works mostly expanded the feature sets (Cheng et al., 2007; Bethard and Martin, 2007; UzZaman et al., 2012; Bethard, 2013; Kolomiyets et al., 2012; Chambers, 2013; Laokulrat et al., 2013). Specifically, Chambers (2013) used direct dependency path between event pairs to capture syntactic context. Laokulrat et al. (2013) used 3-grams of paths between two event mentions in a dependency tree as features instead of full paths as those are too sparse. We found that modeling the entire path as one sequence provides greater compositional evidence on the temporal relation. In addition, modifiers attached to the words in a path with specific dependency relations like nmod:tmod are also informative. Ng (2013) proposed a hybrid system for temporal"
D17-1190,S07-1014,0,0.0847373,"eported that dependency order between events, either governordependent or dependent-governor, was not useful in their experiments. However, we show that dependency relations, when modeled as a sequence, contribute significantly to this task. 3 Temporal Link Labeling In this section, we describe the task of temporal relation classification, dataset, context words se1797 quence extraction model and the used recurrent neural net based classifier. 3.1 Task description Early works on temporal relation classification Mani et al. (2006); Chambers et al. (2007) and the first two versions of TempEval (Verhagen et al., 2007, 2010) simplified the task by considering only six relation types. They combined the pair of relation types that are the inverse of each other and ignored the relations during and during inv. Then TempEval-3 (Uzzaman et al., 2013) extended the task to complete 14 class classification problem and all later works have considered all 14 relations. Our model performs 14-class classification following the recent works, as this is arguably more challenging (Ng, 2013). Also, we consider gold annotated event pairs, mainly because the corpus is small and distribution of relations is very skewed. All p"
D17-1190,S10-1010,0,\N,Missing
D17-1226,W06-0901,0,0.646747,"addition, recognizing second order relations between event chains relies on adequate number of event mentions that are already linked. Therefore, our model conducts event coreference in two stages. In the first stage, it iteratively conducts WD and CD merges as suggested by pairwise WD and CD merging classifiers respectively. Argument features of individual event mentions are propagated Related Work Different approaches, focusing on either of WD or CD coreference chains, have been proposed for event coreference resolution. Works specific to WD event coreference includes pairwise classifiers (Ahn, 2006; Chen et al., 2009) graph based clustering method (Chen and Ji, 2009), information propagation (Liu et al., 2014), and markov logic networks Lu et al. (2016). As to only CD event coreference, Cybulska and Vossen (2015a) 3 2125 System Overview and A Worked Example Figure 1: An example of Event Coreference using the iterative two stage model. All event mentions are boldfaced; solid arrow line between event mentions show second order relations between them; dashed lines link coreferent event mentions and are tagged with the type of merge. within a cluster after each merge operation. In the secon"
D17-1226,P10-1143,0,0.895508,"and their participants and arguments are combined within a cluster, CD clustering can be performed with ease as sufficient evidences are collected through initial WD clustering. Therefore, another very common practice for event coreference is to first group event mentions within a document and then group WD clusters across documents (Yang et al., 2015). Nonetheless, WD coreference chains are equally hard to resolve. Event mentions in the same document can look very dissimilar (”killed/ VB” and ”murder/ NN”), have event arguments (i.e., participants and spatio-temporal information of an event (Bejan and Harabagiu, 2010)) partially or entirely omitted, or appear in distinct contexts compared to their antecedent event mentions, partially to avoid repetitions. Under this irresolute state, approaching WD and CD individually is incompetent. While CD coreference resolution is overall difficult, we observe that some CD coreferent event mentions, especially the ones that appear at the beginning of documents, share sufficient contexts and are relatively easier to resolve. At the same 2124 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2124–2133 c Copenhagen, Denmark, Sep"
D17-1226,J14-2004,0,0.433537,"Missing"
D17-1226,D14-1082,0,0.0286928,"Table 2 shows the distribution of the corpus. #Documents #Sentences #Event Mentions #CD Chains #WD Chains Avg. WD chain length Avg. CD chain length Train 462 7,294 3,555 687 2,499 2.835 5.17 Dev 73 649 441 47 316 2.589 9.39 Test 447 7,867 3,290 486 2,137 2.553 6.77 Total 982 15,810 7,286 1,220 4,952 2.686 5.98 Table 2: ECB+ Corpus Statistics. We used event mentions identified by CRF based event extractor used in Yang et al. (2015) and extracted event arguments by applying state-of-the-art semantic role labeling system (SwiRL (Surdeanu et al., 2007)). In addition, we used the Stanford parser (Chen and Manning, 2014) for generating dependency relations, partsof-speech tags and lemmas. We use pre-trained Glove vectors (Pennington et al., 2014)5 for word representation and one-hot vectors for parts-ofspeech tags. We evaluate our model using four commonly adopted event coreference evaluation metrics, namely, MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998), CEAFe (Luo, 2005) and CoNLL F1 (Pradhan et al., 2014). We used the publicly available official implementation of revised coreference scorer (v8.01).6 6.1 Baseline Systems We compare our iterative event coreference resolution model with five baselin"
D17-1226,W09-3208,0,0.463628,"hains relies on adequate number of event mentions that are already linked. Therefore, our model conducts event coreference in two stages. In the first stage, it iteratively conducts WD and CD merges as suggested by pairwise WD and CD merging classifiers respectively. Argument features of individual event mentions are propagated Related Work Different approaches, focusing on either of WD or CD coreference chains, have been proposed for event coreference resolution. Works specific to WD event coreference includes pairwise classifiers (Ahn, 2006; Chen et al., 2009) graph based clustering method (Chen and Ji, 2009), information propagation (Liu et al., 2014), and markov logic networks Lu et al. (2016). As to only CD event coreference, Cybulska and Vossen (2015a) 3 2125 System Overview and A Worked Example Figure 1: An example of Event Coreference using the iterative two stage model. All event mentions are boldfaced; solid arrow line between event mentions show second order relations between them; dashed lines link coreferent event mentions and are tagged with the type of merge. within a cluster after each merge operation. In the second stage, it explores second order relations across event clusters w.r."
D17-1226,W09-4303,0,0.891208,"recognizing second order relations between event chains relies on adequate number of event mentions that are already linked. Therefore, our model conducts event coreference in two stages. In the first stage, it iteratively conducts WD and CD merges as suggested by pairwise WD and CD merging classifiers respectively. Argument features of individual event mentions are propagated Related Work Different approaches, focusing on either of WD or CD coreference chains, have been proposed for event coreference resolution. Works specific to WD event coreference includes pairwise classifiers (Ahn, 2006; Chen et al., 2009) graph based clustering method (Chen and Ji, 2009), information propagation (Liu et al., 2014), and markov logic networks Lu et al. (2016). As to only CD event coreference, Cybulska and Vossen (2015a) 3 2125 System Overview and A Worked Example Figure 1: An example of Event Coreference using the iterative two stage model. All event mentions are boldfaced; solid arrow line between event mentions show second order relations between them; dashed lines link coreferent event mentions and are tagged with the type of merge. within a cluster after each merge operation. In the second stage, it explores"
D17-1226,P15-1136,0,0.0238008,"meta-document by concatenating topic-relevant documents and treated both as an identical task. Most recently, Yang et al. (2015) applied a two-level clustering model that first groups event mentions within a document and then groups WD clusters across documents in a joint inference process. Our approach advances these works and emphasizes on different natures of WD and CD clusters along with the benefits of distinguishing WD merges from CD merges and exploiting their mutual dependencies. Iterative models, in general, have been applied to both entity coreference resolution (Singh et al., 2009; Clark and Manning, 2015, 2016; Wiseman et al., 2016) and prior event coreference resolution (Lee et al., 2012) works, which gradually build clusters and enable later merges to benefit from earlier ones. Especially, Lee et al. (2012) used an iterative model to jointly build entity and event clusters and showed the advantages of information flow between entity and event clusters through semantic role features. Our model, by alternating between WD and CD merges, allows the multi-level flow of first order interdependencies. Moreover, additional cross cluster merges based on 2nd order interdependencies effectively exploi"
D17-1226,P16-1061,0,0.0782732,"Missing"
D17-1226,cybulska-vossen-2014-using,0,0.519926,"WD and CD coreferent event pairs. Intuitively, if two event mentions are related to the same set of events, it is likely that the two event mentions refer to the same real world event, even when their word forms and local contexts are distinct. Specifically, we merge event clusters if their event mentions are tightly associated (i.e., having the same dependency relations) or loosely associated (i.e., co-occurring in the same sentential context) with enough (i.e., passing a threshold) other events that are known coreferent. Experimental results on the benchmark event coreference dataset, ECB+ (Cybulska and Vossen, 2014b,a), show that our model extensively exploits inter-dependencies between events and outperforms the state-of-the-art methods for both WD and CD event coreference resolution. created pairwise classifiers using features indicating granularities of event slots and in another work (2015b), grouped events based on compatibilities of event contexts. Like this work, several studies have considered both WD and CD event coreference resolution task together. However to simplify the problem, they (Lee et al., 2012; Bejan and Harabagiu, 2010, 2014) created a meta-document by concatenating topic-relevant"
D17-1226,W15-0801,0,0.451456,"the first stage, it iteratively conducts WD and CD merges as suggested by pairwise WD and CD merging classifiers respectively. Argument features of individual event mentions are propagated Related Work Different approaches, focusing on either of WD or CD coreference chains, have been proposed for event coreference resolution. Works specific to WD event coreference includes pairwise classifiers (Ahn, 2006; Chen et al., 2009) graph based clustering method (Chen and Ji, 2009), information propagation (Liu et al., 2014), and markov logic networks Lu et al. (2016). As to only CD event coreference, Cybulska and Vossen (2015a) 3 2125 System Overview and A Worked Example Figure 1: An example of Event Coreference using the iterative two stage model. All event mentions are boldfaced; solid arrow line between event mentions show second order relations between them; dashed lines link coreferent event mentions and are tagged with the type of merge. within a cluster after each merge operation. In the second stage, it explores second order relations across event clusters w.r.t context event mentions in order to carefully generate candidate event clusters and perform further merging. The example in Figure 1 illustrates th"
D17-1226,W03-0502,0,0.0759429,"ts. Experiments on the ECB+ corpus show that our model outperforms state-of-the-art methods in joint task of WD and CD event coreference resolution. 1 Introduction Event coreference resolution is the task of identifying event mentions and clustering them such that each cluster represents a unique real world event. The capability of resolving links among coreferring event identities is vital for information aggregation and many NLP applications, including topic detection and tracking, information extraction, question answering and text summarization (Humphreys et al., 1997; Allan et al., 1998; Daniel et al., 2003; Narayanan and Harabagiu, 2004; Mayfield et al., 2009; Zhang et al., 2015). Yet, studies on event coreference are few compared to the well-studied entity coreference resolution. Event mentions that refer to the same event can occur both within a document (WD) and across multiple documents (CD). One common practice (Lee et al., 2012) to approach CD coreference task is to resolve event coreference in a megadocument created by concatenating topic-relevant documents, which essentially does not distinguish WD and CD event links. However, intuitively, recognizing CD coreferent event pairs requires"
D17-1226,W97-1311,0,0.327683,"ely related to a set of other chains of events. Experiments on the ECB+ corpus show that our model outperforms state-of-the-art methods in joint task of WD and CD event coreference resolution. 1 Introduction Event coreference resolution is the task of identifying event mentions and clustering them such that each cluster represents a unique real world event. The capability of resolving links among coreferring event identities is vital for information aggregation and many NLP applications, including topic detection and tracking, information extraction, question answering and text summarization (Humphreys et al., 1997; Allan et al., 1998; Daniel et al., 2003; Narayanan and Harabagiu, 2004; Mayfield et al., 2009; Zhang et al., 2015). Yet, studies on event coreference are few compared to the well-studied entity coreference resolution. Event mentions that refer to the same event can occur both within a document (WD) and across multiple documents (CD). One common practice (Lee et al., 2012) to approach CD coreference task is to resolve event coreference in a megadocument created by concatenating topic-relevant documents, which essentially does not distinguish WD and CD event links. However, intuitively, recogn"
D17-1226,D12-1045,0,0.548448,"olving links among coreferring event identities is vital for information aggregation and many NLP applications, including topic detection and tracking, information extraction, question answering and text summarization (Humphreys et al., 1997; Allan et al., 1998; Daniel et al., 2003; Narayanan and Harabagiu, 2004; Mayfield et al., 2009; Zhang et al., 2015). Yet, studies on event coreference are few compared to the well-studied entity coreference resolution. Event mentions that refer to the same event can occur both within a document (WD) and across multiple documents (CD). One common practice (Lee et al., 2012) to approach CD coreference task is to resolve event coreference in a megadocument created by concatenating topic-relevant documents, which essentially does not distinguish WD and CD event links. However, intuitively, recognizing CD coreferent event pairs requires stricter evidence compared to WD event linking because it is riskier to link two event mentions from two distinct documents rather than the same document. In a perfect scenario where all WD event mentions are properly clustered and their participants and arguments are combined within a cluster, CD clustering can be performed with eas"
D17-1226,liu-etal-2014-supervised,0,0.274073,"ions that are already linked. Therefore, our model conducts event coreference in two stages. In the first stage, it iteratively conducts WD and CD merges as suggested by pairwise WD and CD merging classifiers respectively. Argument features of individual event mentions are propagated Related Work Different approaches, focusing on either of WD or CD coreference chains, have been proposed for event coreference resolution. Works specific to WD event coreference includes pairwise classifiers (Ahn, 2006; Chen et al., 2009) graph based clustering method (Chen and Ji, 2009), information propagation (Liu et al., 2014), and markov logic networks Lu et al. (2016). As to only CD event coreference, Cybulska and Vossen (2015a) 3 2125 System Overview and A Worked Example Figure 1: An example of Event Coreference using the iterative two stage model. All event mentions are boldfaced; solid arrow line between event mentions show second order relations between them; dashed lines link coreferent event mentions and are tagged with the type of merge. within a cluster after each merge operation. In the second stage, it explores second order relations across event clusters w.r.t context event mentions in order to careful"
D17-1226,C16-1308,0,0.571682,"model conducts event coreference in two stages. In the first stage, it iteratively conducts WD and CD merges as suggested by pairwise WD and CD merging classifiers respectively. Argument features of individual event mentions are propagated Related Work Different approaches, focusing on either of WD or CD coreference chains, have been proposed for event coreference resolution. Works specific to WD event coreference includes pairwise classifiers (Ahn, 2006; Chen et al., 2009) graph based clustering method (Chen and Ji, 2009), information propagation (Liu et al., 2014), and markov logic networks Lu et al. (2016). As to only CD event coreference, Cybulska and Vossen (2015a) 3 2125 System Overview and A Worked Example Figure 1: An example of Event Coreference using the iterative two stage model. All event mentions are boldfaced; solid arrow line between event mentions show second order relations between them; dashed lines link coreferent event mentions and are tagged with the type of merge. within a cluster after each merge operation. In the second stage, it explores second order relations across event clusters w.r.t context event mentions in order to carefully generate candidate event clusters and per"
D17-1226,H05-1004,0,0.716553,"CRF based event extractor used in Yang et al. (2015) and extracted event arguments by applying state-of-the-art semantic role labeling system (SwiRL (Surdeanu et al., 2007)). In addition, we used the Stanford parser (Chen and Manning, 2014) for generating dependency relations, partsof-speech tags and lemmas. We use pre-trained Glove vectors (Pennington et al., 2014)5 for word representation and one-hot vectors for parts-ofspeech tags. We evaluate our model using four commonly adopted event coreference evaluation metrics, namely, MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998), CEAFe (Luo, 2005) and CoNLL F1 (Pradhan et al., 2014). We used the publicly available official implementation of revised coreference scorer (v8.01).6 6.1 Baseline Systems We compare our iterative event coreference resolution model with five baseline systems. LEMMA: The Lemma match baseline links event mentions within- or cross- documents which have the same lemmatized head word. It is often considered a strong baseline for this task. HDDCRP (Yang et al., 2015): The second baseline is the supervised Hierarchical Distance Dependent Bayesian Model, the most recent event coreference system evaluated on the same EC"
D17-1226,C04-1100,0,0.0192293,"e ECB+ corpus show that our model outperforms state-of-the-art methods in joint task of WD and CD event coreference resolution. 1 Introduction Event coreference resolution is the task of identifying event mentions and clustering them such that each cluster represents a unique real world event. The capability of resolving links among coreferring event identities is vital for information aggregation and many NLP applications, including topic detection and tracking, information extraction, question answering and text summarization (Humphreys et al., 1997; Allan et al., 1998; Daniel et al., 2003; Narayanan and Harabagiu, 2004; Mayfield et al., 2009; Zhang et al., 2015). Yet, studies on event coreference are few compared to the well-studied entity coreference resolution. Event mentions that refer to the same event can occur both within a document (WD) and across multiple documents (CD). One common practice (Lee et al., 2012) to approach CD coreference task is to resolve event coreference in a megadocument created by concatenating topic-relevant documents, which essentially does not distinguish WD and CD event links. However, intuitively, recognizing CD coreferent event pairs requires stricter evidence compared to W"
D17-1226,D14-1162,0,0.0850708,"Avg. CD chain length Train 462 7,294 3,555 687 2,499 2.835 5.17 Dev 73 649 441 47 316 2.589 9.39 Test 447 7,867 3,290 486 2,137 2.553 6.77 Total 982 15,810 7,286 1,220 4,952 2.686 5.98 Table 2: ECB+ Corpus Statistics. We used event mentions identified by CRF based event extractor used in Yang et al. (2015) and extracted event arguments by applying state-of-the-art semantic role labeling system (SwiRL (Surdeanu et al., 2007)). In addition, we used the Stanford parser (Chen and Manning, 2014) for generating dependency relations, partsof-speech tags and lemmas. We use pre-trained Glove vectors (Pennington et al., 2014)5 for word representation and one-hot vectors for parts-ofspeech tags. We evaluate our model using four commonly adopted event coreference evaluation metrics, namely, MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998), CEAFe (Luo, 2005) and CoNLL F1 (Pradhan et al., 2014). We used the publicly available official implementation of revised coreference scorer (v8.01).6 6.1 Baseline Systems We compare our iterative event coreference resolution model with five baseline systems. LEMMA: The Lemma match baseline links event mentions within- or cross- documents which have the same lemmatized head"
D17-1226,P14-2006,0,0.10195,"r used in Yang et al. (2015) and extracted event arguments by applying state-of-the-art semantic role labeling system (SwiRL (Surdeanu et al., 2007)). In addition, we used the Stanford parser (Chen and Manning, 2014) for generating dependency relations, partsof-speech tags and lemmas. We use pre-trained Glove vectors (Pennington et al., 2014)5 for word representation and one-hot vectors for parts-ofspeech tags. We evaluate our model using four commonly adopted event coreference evaluation metrics, namely, MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998), CEAFe (Luo, 2005) and CoNLL F1 (Pradhan et al., 2014). We used the publicly available official implementation of revised coreference scorer (v8.01).6 6.1 Baseline Systems We compare our iterative event coreference resolution model with five baseline systems. LEMMA: The Lemma match baseline links event mentions within- or cross- documents which have the same lemmatized head word. It is often considered a strong baseline for this task. HDDCRP (Yang et al., 2015): The second baseline is the supervised Hierarchical Distance Dependent Bayesian Model, the most recent event coreference system evaluated on the same ECB+ dataset. This model uses distance"
D17-1226,N03-1033,0,0.182914,"Missing"
D17-1226,M95-1005,0,0.392622,"CB+ Corpus Statistics. We used event mentions identified by CRF based event extractor used in Yang et al. (2015) and extracted event arguments by applying state-of-the-art semantic role labeling system (SwiRL (Surdeanu et al., 2007)). In addition, we used the Stanford parser (Chen and Manning, 2014) for generating dependency relations, partsof-speech tags and lemmas. We use pre-trained Glove vectors (Pennington et al., 2014)5 for word representation and one-hot vectors for parts-ofspeech tags. We evaluate our model using four commonly adopted event coreference evaluation metrics, namely, MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998), CEAFe (Luo, 2005) and CoNLL F1 (Pradhan et al., 2014). We used the publicly available official implementation of revised coreference scorer (v8.01).6 6.1 Baseline Systems We compare our iterative event coreference resolution model with five baseline systems. LEMMA: The Lemma match baseline links event mentions within- or cross- documents which have the same lemmatized head word. It is often considered a strong baseline for this task. HDDCRP (Yang et al., 2015): The second baseline is the supervised Hierarchical Distance Dependent Bayesian Model, the most recent"
D17-1226,N16-1114,0,0.0209154,"topic-relevant documents and treated both as an identical task. Most recently, Yang et al. (2015) applied a two-level clustering model that first groups event mentions within a document and then groups WD clusters across documents in a joint inference process. Our approach advances these works and emphasizes on different natures of WD and CD clusters along with the benefits of distinguishing WD merges from CD merges and exploiting their mutual dependencies. Iterative models, in general, have been applied to both entity coreference resolution (Singh et al., 2009; Clark and Manning, 2015, 2016; Wiseman et al., 2016) and prior event coreference resolution (Lee et al., 2012) works, which gradually build clusters and enable later merges to benefit from earlier ones. Especially, Lee et al. (2012) used an iterative model to jointly build entity and event clusters and showed the advantages of information flow between entity and event clusters through semantic role features. Our model, by alternating between WD and CD merges, allows the multi-level flow of first order interdependencies. Moreover, additional cross cluster merges based on 2nd order interdependencies effectively exploits the semantic relations amo"
D17-1226,Q15-1037,0,0.634268,"CD coreferent event pairs requires stricter evidence compared to WD event linking because it is riskier to link two event mentions from two distinct documents rather than the same document. In a perfect scenario where all WD event mentions are properly clustered and their participants and arguments are combined within a cluster, CD clustering can be performed with ease as sufficient evidences are collected through initial WD clustering. Therefore, another very common practice for event coreference is to first group event mentions within a document and then group WD clusters across documents (Yang et al., 2015). Nonetheless, WD coreference chains are equally hard to resolve. Event mentions in the same document can look very dissimilar (”killed/ VB” and ”murder/ NN”), have event arguments (i.e., participants and spatio-temporal information of an event (Bejan and Harabagiu, 2010)) partially or entirely omitted, or appear in distinct contexts compared to their antecedent event mentions, partially to avoid repetitions. Under this irresolute state, approaching WD and CD individually is incompetent. While CD coreference resolution is overall difficult, we observe that some CD coreferent event mentions, es"
D17-1226,D15-1020,0,0.0140903,"-the-art methods in joint task of WD and CD event coreference resolution. 1 Introduction Event coreference resolution is the task of identifying event mentions and clustering them such that each cluster represents a unique real world event. The capability of resolving links among coreferring event identities is vital for information aggregation and many NLP applications, including topic detection and tracking, information extraction, question answering and text summarization (Humphreys et al., 1997; Allan et al., 1998; Daniel et al., 2003; Narayanan and Harabagiu, 2004; Mayfield et al., 2009; Zhang et al., 2015). Yet, studies on event coreference are few compared to the well-studied entity coreference resolution. Event mentions that refer to the same event can occur both within a document (WD) and across multiple documents (CD). One common practice (Lee et al., 2012) to approach CD coreference task is to resolve event coreference in a megadocument created by concatenating topic-relevant documents, which essentially does not distinguish WD and CD event links. However, intuitively, recognizing CD coreferent event pairs requires stricter evidence compared to WD event linking because it is riskier to lin"
D18-1368,S17-1027,0,0.451148,"mbeddings in a paragraph and generate refined hidden states −−−−−→ ←−−−−− hClause t and hClause t at each clause position t. Then, we concatenate the two hidden states for a clause to get the final clause representation −−−−−→ ←−−−−− hClause t = [hClause t , hClause t ]. Situation Entity Type Classification: Finally, the prediction layer will predict the situation entity type for each clause by applying the softmax function to its clause representation: yt = sof tmax(Wy ∗ hClause t + by ) 3.1 (3) Fine-tune Situation Entity Predictions with a CRF Layer Previous studies (Friedrich et al., 2016; Becker et al., 2017) show that there exist common SE label patterns between adjacent clauses. For example, Friedrich et al. (2016) reported the fact that GENERIC sentences usually occur together in a paragraph. Following (Friedrich et al., 2016), in order to capture SE label patterns in our hierarchical recurrent neural network model, we add a CRF layer at the top of the softmax prediction layer (shown in figure 2) to fine-tune predicted situation entity types. The CRF layer will update a state-transition matrix, which can effectively adjust the current label depending on its preceding and following labels. Both"
D18-1368,Q16-1026,0,0.0177455,"f clauses. Other related tasks include predicting aspectual classes of verbs (Friedrich and Palmer, 2014a), classifying genericity of noun phrases (Reiter and Frank, 2010) and predicting clause habituality (Friedrich and Pinkal, 2015). 2.3 Paragraph-level Sequence Labeling Learning latent representations and predicting a sequence of labels from a long sequence of sentences (clauses), such as a paragraph, is a challenging task. Recently, various neural network models, including Convolution Neural Network (CNN) (Wang and Lu, 2017), Recurrent Neural Network (RNN) based models (Wang et al., 2015; Chiu and Nichols, 2016; Huang et al., 2015; Ma and Hovy, 2016; Lample et al., 2016) and Sequence to Sequence models (Vaswani et al., 2016; Zheng et al., 2017), have been applied to the general task of sequence labeling. Among them, the bidirectional LSTM (Bi-LSTM) model (Schuster and Paliwal, 1997) has been widely used to process a paragraph for applications such as language generation (Li et al., 2015), dialogue systems (Serban et al., 2016) and text summarization (Nallapati et al., 2016), because of its capabilities in modeling long-distance dependencies between words. In this work, we use two levels of Bi-LSTMs"
D18-1368,D17-1070,0,0.075398,"Missing"
D18-1368,P14-2085,0,0.275028,"erty and more specifically, based on the type of Situation Entity (SE)1 (e.g., events, states, generalizing statements and generic statements) the clause introduces to the discourse, following the recent work by (Friedrich et al., 2016). Understanding SE types of clauses is beneficial for many NLP tasks, including discourse mode identi1 The Situation Entity (SE) type of a clause is defined with respect to three situation-related features: the main NP referent type (specific or generic), fundamental aspectual class (stative or dynamic), and whether the situation evoked is episodic or habitual (Friedrich and Palmer, 2014b). fication2 (Smith, 2003, 2005), text summarization, information extraction and question answering. The situation entity type of a clause reflects discourse roles the clause plays in a paragraph and discourse role interpretation depends heavily on paragraph-wide contexts. Recently, Friedrich et al. (2016) used insightful syntactic-semantic features extracted from the target clause itself for SE type classification, which has achieved good performance across several genres when evaluated on the newly created large dataset MASC+Wiki. In addition, Friedrich et al. (2016) implemented a sequence"
D18-1368,P16-1101,0,0.0249034,"cting aspectual classes of verbs (Friedrich and Palmer, 2014a), classifying genericity of noun phrases (Reiter and Frank, 2010) and predicting clause habituality (Friedrich and Pinkal, 2015). 2.3 Paragraph-level Sequence Labeling Learning latent representations and predicting a sequence of labels from a long sequence of sentences (clauses), such as a paragraph, is a challenging task. Recently, various neural network models, including Convolution Neural Network (CNN) (Wang and Lu, 2017), Recurrent Neural Network (RNN) based models (Wang et al., 2015; Chiu and Nichols, 2016; Huang et al., 2015; Ma and Hovy, 2016; Lample et al., 2016) and Sequence to Sequence models (Vaswani et al., 2016; Zheng et al., 2017), have been applied to the general task of sequence labeling. Among them, the bidirectional LSTM (Bi-LSTM) model (Schuster and Paliwal, 1997) has been widely used to process a paragraph for applications such as language generation (Li et al., 2015), dialogue systems (Serban et al., 2016) and text summarization (Nallapati et al., 2016), because of its capabilities in modeling long-distance dependencies between words. In this work, we use two levels of Bi-LSTMs connected by a max-pooling layer to abs"
D18-1368,W14-4921,0,0.657361,"erty and more specifically, based on the type of Situation Entity (SE)1 (e.g., events, states, generalizing statements and generic statements) the clause introduces to the discourse, following the recent work by (Friedrich et al., 2016). Understanding SE types of clauses is beneficial for many NLP tasks, including discourse mode identi1 The Situation Entity (SE) type of a clause is defined with respect to three situation-related features: the main NP referent type (specific or generic), fundamental aspectual class (stative or dynamic), and whether the situation evoked is episodic or habitual (Friedrich and Palmer, 2014b). fication2 (Smith, 2003, 2005), text summarization, information extraction and question answering. The situation entity type of a clause reflects discourse roles the clause plays in a paragraph and discourse role interpretation depends heavily on paragraph-wide contexts. Recently, Friedrich et al. (2016) used insightful syntactic-semantic features extracted from the target clause itself for SE type classification, which has achieved good performance across several genres when evaluated on the newly created large dataset MASC+Wiki. In addition, Friedrich et al. (2016) implemented a sequence"
D18-1368,P16-1166,0,0.0802919,"ieves the state-of-the-art performance for clause-level situation entity classification on the genrerich MASC+Wiki corpus, which approaches human-level performance. 1 Introduction Clauses in a paragraph play different discourse and pragmatic roles and have different aspectual properties (Smith, 1997; Verkuyl, 2013) accordingly. We aim to categorize a clause based on its aspectual property and more specifically, based on the type of Situation Entity (SE)1 (e.g., events, states, generalizing statements and generic statements) the clause introduces to the discourse, following the recent work by (Friedrich et al., 2016). Understanding SE types of clauses is beneficial for many NLP tasks, including discourse mode identi1 The Situation Entity (SE) type of a clause is defined with respect to three situation-related features: the main NP referent type (specific or generic), fundamental aspectual class (stative or dynamic), and whether the situation evoked is episodic or habitual (Friedrich and Palmer, 2014b). fication2 (Smith, 2003, 2005), text summarization, information extraction and question answering. The situation entity type of a clause reflects discourse roles the clause plays in a paragraph and discourse"
D18-1368,D15-1294,0,0.013795,"e content of the target clause using a GRU and incorporating several sources of context information, includ3306 ing contents and labels of preceding clauses as well as genre information, using additional separate GRUs (Chung et al., 2014). This model is different from our approach that processes one paragraph (with a sequence of clauses) at a time and extensively models inter-dependencies of clauses. Other related tasks include predicting aspectual classes of verbs (Friedrich and Palmer, 2014a), classifying genericity of noun phrases (Reiter and Frank, 2010) and predicting clause habituality (Friedrich and Pinkal, 2015). 2.3 Paragraph-level Sequence Labeling Learning latent representations and predicting a sequence of labels from a long sequence of sentences (clauses), such as a paragraph, is a challenging task. Recently, various neural network models, including Convolution Neural Network (CNN) (Wang and Lu, 2017), Recurrent Neural Network (RNN) based models (Wang et al., 2015; Chiu and Nichols, 2016; Huang et al., 2015; Ma and Hovy, 2016; Lample et al., 2016) and Sequence to Sequence models (Vaswani et al., 2016; Zheng et al., 2017), have been applied to the general task of sequence labeling. Among them, th"
D18-1368,ide-etal-2008-masc,0,0.0211836,"ng rate of 0.001 and the batch size5 of 128. All our proposed models were implemented with Pytorch6 and converged to the best result within 40 epochs. Note that to diminish the effects of randomness in training neural network models and report stable experimental results, we ran each of the proposed models as well as our own baseline models ten times and reported the averaged performance across the ten runs. 4 4.1 Evaluation than 40,000 clauses and is the largest annotated dataset for situation entity type classification. The MASC+Wiki dataset is composed of documents from Wikipedia and MASC (Ide et al., 2008) covering as many as 13 written genres (e.g., news, essays, fiction, etc). Table 1 shows statistics of the dataset, from which you can see that the SE type distribution is highly imbalanced. The majority SE type of MASC documents is STATE while the majority SE type of Wikipedia documents is GENERIC. To make our results comparable with previous works (Friedrich et al., 2016; Becker et al., 2017), we used the same 80:20 traintest split with balanced genre distributions. Preprocessing: As described in (Friedrich et al., 2016), texts were split into clauses using SPADE (Soricut and Marcu, 2003). T"
D18-1368,N16-1030,0,0.0203517,"sses of verbs (Friedrich and Palmer, 2014a), classifying genericity of noun phrases (Reiter and Frank, 2010) and predicting clause habituality (Friedrich and Pinkal, 2015). 2.3 Paragraph-level Sequence Labeling Learning latent representations and predicting a sequence of labels from a long sequence of sentences (clauses), such as a paragraph, is a challenging task. Recently, various neural network models, including Convolution Neural Network (CNN) (Wang and Lu, 2017), Recurrent Neural Network (RNN) based models (Wang et al., 2015; Chiu and Nichols, 2016; Huang et al., 2015; Ma and Hovy, 2016; Lample et al., 2016) and Sequence to Sequence models (Vaswani et al., 2016; Zheng et al., 2017), have been applied to the general task of sequence labeling. Among them, the bidirectional LSTM (Bi-LSTM) model (Schuster and Paliwal, 1997) has been widely used to process a paragraph for applications such as language generation (Li et al., 2015), dialogue systems (Serban et al., 2016) and text summarization (Nallapati et al., 2016), because of its capabilities in modeling long-distance dependencies between words. In this work, we use two levels of Bi-LSTMs connected by a max-pooling layer to abstract clause represent"
D18-1368,P15-1107,0,0.0355209,"h as a paragraph, is a challenging task. Recently, various neural network models, including Convolution Neural Network (CNN) (Wang and Lu, 2017), Recurrent Neural Network (RNN) based models (Wang et al., 2015; Chiu and Nichols, 2016; Huang et al., 2015; Ma and Hovy, 2016; Lample et al., 2016) and Sequence to Sequence models (Vaswani et al., 2016; Zheng et al., 2017), have been applied to the general task of sequence labeling. Among them, the bidirectional LSTM (Bi-LSTM) model (Schuster and Paliwal, 1997) has been widely used to process a paragraph for applications such as language generation (Li et al., 2015), dialogue systems (Serban et al., 2016) and text summarization (Nallapati et al., 2016), because of its capabilities in modeling long-distance dependencies between words. In this work, we use two levels of Bi-LSTMs connected by a max-pooling layer to abstract clause representations by extensively modeling paragraph-wide contexts and inter-dependencies between clauses. 3 The Hierarchical Recurrent Neural Network for SE Type Classification We design an unified neural network to extensively model word-level dependencies as well as clause-level dependencies in deriving clause representations for"
D18-1368,K16-1028,0,0.146561,"de identi1 The Situation Entity (SE) type of a clause is defined with respect to three situation-related features: the main NP referent type (specific or generic), fundamental aspectual class (stative or dynamic), and whether the situation evoked is episodic or habitual (Friedrich and Palmer, 2014b). fication2 (Smith, 2003, 2005), text summarization, information extraction and question answering. The situation entity type of a clause reflects discourse roles the clause plays in a paragraph and discourse role interpretation depends heavily on paragraph-wide contexts. Recently, Friedrich et al. (2016) used insightful syntactic-semantic features extracted from the target clause itself for SE type classification, which has achieved good performance across several genres when evaluated on the newly created large dataset MASC+Wiki. In addition, Friedrich et al. (2016) implemented a sequence labeling model with conditional random fields (CRF) (Lafferty et al., 2001) for finetuning a sequence of predicted SE types. However, other than leveraging common SE label patterns (e.g., GENERIC clauses tend to cluster together.), this approach largely ignored the wider contexts a clause appears in when pr"
D18-1368,P07-1113,0,0.0192026,"robust performance close to human level. In addition, the CRF layer further improves the SE type classification results, but by a small margin. We hypothesize that situation entity type patterns across clauses may have been largely captured by allowing the preceding and following clauses to influence semantic representation building for a clause in the paragraph-level neural net model. 2 Related Work 2.1 Linguistic Categories of SE Types The situation entity types annotated in the MASC+Wiki corpus (Friedrich et al., 2016) were initially introduced by Smith (2003), which were then extended by (Palmer et al., 2007; Friedrich and Palmer, 2014b). The situation entity types can be divided into the following broad categories: • Eventualities (EVENT, STATE and REPORT): for clauses representing actual happenings and world states. STATE and EVENT are two fundamental aspectual classes of a clause (Siegel and McKeown, 2000) which can be distinguished by the semantic property of dynamism. REPORT is a subtype of EVENT for quoted speech. • General Statives (GENERIC and GENERALIZING): for clauses that express general information over classes or kinds, or regularities related to specific main referents. The type GEN"
D18-1368,prasad-etal-2008-penn,0,0.222996,"ining the SE type of clauses they connect. Our assumption is that discourse connectives are important to glue clauses together and removing them affects text coherence and information flow between clauses. Intuitively, the connective “and” may occur between two clauses with the same SE type; “for example” may indicate that the following clause is not GENERIC. Therefore, we designed a pilot experiment to see whether discourse connective phrases are indispensable in building clause representations. In this pilot experiment, we extracted a list of 100 explicit discourse connectives. PDTB corpus (Prasad et al., 2008) and identified clauses that start with a discourse connecte9 . Then we ran the full paragraph-level model with one modification, i.e., disregarding words in connective phrases when conducting the max-pooling operation in equation (1), thus we did not consider discourse connective phrases directly when building a clause representation. As shown in Table 6, for clauses containing a discourse connective phrase, both macro-average F1-score and accuracy dropped due to the exclusion of discourse connective phrases. The performance was negatively influenced across all the SE types except the type of"
D18-1368,P10-1005,0,0.0323075,"edicts the SE type for one clause each time, by encoding the content of the target clause using a GRU and incorporating several sources of context information, includ3306 ing contents and labels of preceding clauses as well as genre information, using additional separate GRUs (Chung et al., 2014). This model is different from our approach that processes one paragraph (with a sequence of clauses) at a time and extensively models inter-dependencies of clauses. Other related tasks include predicting aspectual classes of verbs (Friedrich and Palmer, 2014a), classifying genericity of noun phrases (Reiter and Frank, 2010) and predicting clause habituality (Friedrich and Pinkal, 2015). 2.3 Paragraph-level Sequence Labeling Learning latent representations and predicting a sequence of labels from a long sequence of sentences (clauses), such as a paragraph, is a challenging task. Recently, various neural network models, including Convolution Neural Network (CNN) (Wang and Lu, 2017), Recurrent Neural Network (RNN) based models (Wang et al., 2015; Chiu and Nichols, 2016; Huang et al., 2015; Ma and Hovy, 2016; Lample et al., 2016) and Sequence to Sequence models (Vaswani et al., 2016; Zheng et al., 2017), have been a"
D18-1368,J00-4004,0,0.052392,"semantic representation building for a clause in the paragraph-level neural net model. 2 Related Work 2.1 Linguistic Categories of SE Types The situation entity types annotated in the MASC+Wiki corpus (Friedrich et al., 2016) were initially introduced by Smith (2003), which were then extended by (Palmer et al., 2007; Friedrich and Palmer, 2014b). The situation entity types can be divided into the following broad categories: • Eventualities (EVENT, STATE and REPORT): for clauses representing actual happenings and world states. STATE and EVENT are two fundamental aspectual classes of a clause (Siegel and McKeown, 2000) which can be distinguished by the semantic property of dynamism. REPORT is a subtype of EVENT for quoted speech. • General Statives (GENERIC and GENERALIZING): for clauses that express general information over classes or kinds, or regularities related to specific main referents. The type GENERIC is for utterances describing a general class or kind rather than any specific individuals (e.g., People love dogs.). The type GENERALIZING is for habitual utterances that refer to ongoing actions or properties of specific individuals (e.g., Audubon educates the public.). • Speech Acts (QUESTION and IM"
D18-1368,N03-1030,0,0.364834,"and MASC (Ide et al., 2008) covering as many as 13 written genres (e.g., news, essays, fiction, etc). Table 1 shows statistics of the dataset, from which you can see that the SE type distribution is highly imbalanced. The majority SE type of MASC documents is STATE while the majority SE type of Wikipedia documents is GENERIC. To make our results comparable with previous works (Friedrich et al., 2016; Becker et al., 2017), we used the same 80:20 traintest split with balanced genre distributions. Preprocessing: As described in (Friedrich et al., 2016), texts were split into clauses using SPADE (Soricut and Marcu, 2003). There are 4,784 paragraphs in total in the corpus; and on average, each paragraph contains 9.6 clauses. In figure 4, the horizontal axis shows the distribution of paragraphs based on the number of clauses in a paragraph. The annotations of clauses are stored in separate files from the text files. To recover the paragraph contexts for each clause, we matched its content with the corresponding raw document. 4.2 Systems for Comparisons We compare the performance of our neural network model with two recent SE type classification models on the MASC+Wiki corpus as well as humans’ performance (uppe"
D18-1368,N16-1027,0,0.0262751,"ng genericity of noun phrases (Reiter and Frank, 2010) and predicting clause habituality (Friedrich and Pinkal, 2015). 2.3 Paragraph-level Sequence Labeling Learning latent representations and predicting a sequence of labels from a long sequence of sentences (clauses), such as a paragraph, is a challenging task. Recently, various neural network models, including Convolution Neural Network (CNN) (Wang and Lu, 2017), Recurrent Neural Network (RNN) based models (Wang et al., 2015; Chiu and Nichols, 2016; Huang et al., 2015; Ma and Hovy, 2016; Lample et al., 2016) and Sequence to Sequence models (Vaswani et al., 2016; Zheng et al., 2017), have been applied to the general task of sequence labeling. Among them, the bidirectional LSTM (Bi-LSTM) model (Schuster and Paliwal, 1997) has been widely used to process a paragraph for applications such as language generation (Li et al., 2015), dialogue systems (Serban et al., 2016) and text summarization (Nallapati et al., 2016), because of its capabilities in modeling long-distance dependencies between words. In this work, we use two levels of Bi-LSTMs connected by a max-pooling layer to abstract clause representations by extensively modeling paragraph-wide contexts"
D18-1368,P17-1113,0,0.0173117,"phrases (Reiter and Frank, 2010) and predicting clause habituality (Friedrich and Pinkal, 2015). 2.3 Paragraph-level Sequence Labeling Learning latent representations and predicting a sequence of labels from a long sequence of sentences (clauses), such as a paragraph, is a challenging task. Recently, various neural network models, including Convolution Neural Network (CNN) (Wang and Lu, 2017), Recurrent Neural Network (RNN) based models (Wang et al., 2015; Chiu and Nichols, 2016; Huang et al., 2015; Ma and Hovy, 2016; Lample et al., 2016) and Sequence to Sequence models (Vaswani et al., 2016; Zheng et al., 2017), have been applied to the general task of sequence labeling. Among them, the bidirectional LSTM (Bi-LSTM) model (Schuster and Paliwal, 1997) has been widely used to process a paragraph for applications such as language generation (Li et al., 2015), dialogue systems (Serban et al., 2016) and text summarization (Nallapati et al., 2016), because of its capabilities in modeling long-distance dependencies between words. In this work, we use two levels of Bi-LSTMs connected by a max-pooling layer to abstract clause representations by extensively modeling paragraph-wide contexts and inter-dependenci"
D19-1295,D15-1262,0,0.0278663,"Missing"
D19-1295,D14-1168,0,0.182408,"Missing"
D19-1295,C18-1046,0,0.0222702,"Missing"
D19-1295,P15-1067,0,0.0388176,"ors, but it performed significantly worse. 6 We tried to employ weights with soft-attention mechanism, but it did not show improvement in our experiments. Experiments 4.1 Dataset and Preprocessing Dataset: We evaluate our model on PDTB v2.0 (Prasad et al., 2008), which is the largest annotated dataset containing 19K explicit discourse relations and 17K implicit discourse relations. To make our experimental results directly comparable with previous work, we adopted the mostused dataset splitting “PDTB-Ji” (Ji and Eisenstein, 2015) that uses sections 2-20, 0-1, and 21-22 7 We also tried TransD (Ji et al., 2015) and TransR (Lin et al., 2015) for knowledge regularization, but none of them showed clear improvement over TransE in our experiments. 8 Note that cosine similarity performed better than L1 or L2 distance in our experiments. 2980 Relation Type Coreference Event Temporal Event Causal Event Subevent Source CoreNLP Yao et al. ConceptNet ConceptNet # 19,819 18,515 947 626 Discourse Exp & Cont Temp Cont Exp & Temp Table 1: Overview of relation types. # is the number of matched triplets (clusters for coreference). The last column summarizes relevant discourse relation classes each knowledge type may"
D19-1295,Q15-1024,0,0.193248,"International Joint Conference on Natural Language Processing, pages 2976–2987, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics events (situations in general) as described in two discourse units. As shown in the above example, the “Cause” discourse relation between the two DUs depends on the relation between two events “smoking” and “lung-cancer” with one event in each DU. Second, we consider entity coreference relations as a useful form of linguistic constraints in inferring discourse relations. This is motivated by prior work (Rutherford and Xue, 2014; Ji and Eisenstein, 2015) showing that coreference based features can improve entity mention representations within a DU, which facilitates recognizing coherence and discourse relations between DUs. In this paper, we investigate how to incorporate external event knowledge and entity coreference relation based linguistic constraints into neural network models for discourse parsing. One key difficulty we want to address is that external knowledge derived event relations or hard linguistic constraints may not always apply for interpreting a particular context, and may hurt performance if used blindly (Kishimoto et al., 2"
D19-1295,N16-1037,0,0.026287,"d explicit discourse relation recognition compared to all previous work. 2 2.1 Related Work Discourse Parsing on PDTB With the release of Penn Discourse Treebank (PDTB) (Prasad et al., 2008), the task of discourse parsing, especially implicit discourse relation recognition, has received a lot of attention from the NLP community and researchers (Pitler and Nenkova, 2009; Lin et al., 2014; Xue et al., 2015; Rutherford and Xue, 2016). A large number of previous work attempted to model the semantic meanings of two discourse units using latest and advanced neural network models (Chen et al., 2016; Ji et al., 2016; Rutherford et al., 2017; Qin et al., 2017; Guo et al., 2018; Bai and Zhao, 2018). Paragraph-wide contexts were considered for building better discourse unit representations in Dai and Huang (2018). Another research direction for improving implicit discourse relation classification is to expand the training data by leveraging explicit relations (Liu et al., 2016; Lan et al., 2017) or discourse connective informed unlabeled data (Rutherford and Xue, 2015; Xu et al., 2018). 2.2 Incorporate Knowledge into Discourse Parsing Only a few previous work (Park and Cardie, 2012; Biran and McKeown, 2013;"
D19-1295,C18-1049,0,0.0557632,"d Eisenstein, 2015) showing that coreference based features can improve entity mention representations within a DU, which facilitates recognizing coherence and discourse relations between DUs. In this paper, we investigate how to incorporate external event knowledge and entity coreference relation based linguistic constraints into neural network models for discourse parsing. One key difficulty we want to address is that external knowledge derived event relations or hard linguistic constraints may not always apply for interpreting a particular context, and may hurt performance if used blindly (Kishimoto et al., 2018). Therefore, we propose to tightly integrate these constraints into the discourse relation inference process by manipulating hidden word representations to reflect relations between words, and meanwhile balance attentions to contexts and constraints through adding a knowledge regularization term in the final objective function. Specifically, we choose the paragraph-level model we proposed (Dai and Huang, 2018) as the base model, which exploits wider paragraph-level contexts and has been shown effective for PDTBstyle discourse parsing. The model mainly consists of a two-level hierarchical BiLST"
D19-1295,P16-1163,0,0.0131186,"of both implicit and explicit discourse relation recognition compared to all previous work. 2 2.1 Related Work Discourse Parsing on PDTB With the release of Penn Discourse Treebank (PDTB) (Prasad et al., 2008), the task of discourse parsing, especially implicit discourse relation recognition, has received a lot of attention from the NLP community and researchers (Pitler and Nenkova, 2009; Lin et al., 2014; Xue et al., 2015; Rutherford and Xue, 2016). A large number of previous work attempted to model the semantic meanings of two discourse units using latest and advanced neural network models (Chen et al., 2016; Ji et al., 2016; Rutherford et al., 2017; Qin et al., 2017; Guo et al., 2018; Bai and Zhao, 2018). Paragraph-wide contexts were considered for building better discourse unit representations in Dai and Huang (2018). Another research direction for improving implicit discourse relation classification is to expand the training data by leveraging explicit relations (Liu et al., 2016; Lan et al., 2017) or discourse connective informed unlabeled data (Rutherford and Xue, 2015; Xu et al., 2018). 2.2 Incorporate Knowledge into Discourse Parsing Only a few previous work (Park and Cardie, 2012; Biran a"
D19-1295,P18-1224,0,0.0248559,"within DUj ; and the other counts relation triplets that have one node in DUj and the other node in an adjacent DU. We concatenate these DU features with the hidden DU representation hDUj . Adding either word features or DU features is to imitate traditional feature-based approaches and incorporate event knowledge and coreference relation constraints as features. • Base Model + two-step approach: our own baseline that follows the two-step approach for incorporating relational constraints, including both event relations and coreference relations. We re-implement the inference model proposed by Chen et al. (2018)14 , 14 We followed Chen et al. (2018) to tune the hyper• • • • • 4.4 which first employs TransE to obtain relation representations, and then uses attention mechanism to incorporate relations and build knowledge-enhanced DU representations. (Rutherford and Xue, 2015): a feature-based classifier that utilizes explicit discourse connectives for creating more implicit relations. (Liu et al., 2016): CNN based multi-task joint learning model that leverages both PDTB and RST (Carlson et al., 2003) datasets. (Liu and Li, 2016): a hierarchical attentionover-attention neural network model for implicit"
D19-1295,D17-1134,0,0.139942,"., 2014; Xue et al., 2015; Rutherford and Xue, 2016). A large number of previous work attempted to model the semantic meanings of two discourse units using latest and advanced neural network models (Chen et al., 2016; Ji et al., 2016; Rutherford et al., 2017; Qin et al., 2017; Guo et al., 2018; Bai and Zhao, 2018). Paragraph-wide contexts were considered for building better discourse unit representations in Dai and Huang (2018). Another research direction for improving implicit discourse relation classification is to expand the training data by leveraging explicit relations (Liu et al., 2016; Lan et al., 2017) or discourse connective informed unlabeled data (Rutherford and Xue, 2015; Xu et al., 2018). 2.2 Incorporate Knowledge into Discourse Parsing Only a few previous work (Park and Cardie, 2012; Biran and McKeown, 2013; Lei et al., 2018) has exploited external knowledge, including WordNet features (e.g., Antonyms and Hypernyms) and Verb Class (Levin, 1993), in discourse parsing by deriving discrete indicator features and then feed them into feature-based classifiers. Incorporating knowledge as additional features into neural network models often generalize poorly due to the sparsity of features,"
D19-1295,W04-3205,0,0.340713,"Missing"
D19-1295,P16-1061,0,0.028981,"subevent in this work) between the head event xh at the position h and tail event xt at the position t. For each triplet Em = (h, r, t), we use a feedforward neural network5 fr () to update the hidden word representations of head and tail events: fr (hxh ); fr (hxt ) = tanh(Wr [hxh ; hxt ] + br ) where Wr and br are relation-specific weights and bias learned for each type of event relation r only. Coreference Relations: Our system assumes that coreference relations in each paragraph are given in the form of coreference clusters, which are generated by running an existing coreference resolver (Clark and Manning, 2016) from the latest version (3.9.2) of Stanford CoreNLP toolkit. Let C = (C1 , C2 , ..., CK ) denote coreference clusters in one paragraph, where Ck contains the word indices with corresponding words referring to the same entity. Similar as above, we use one feedforward neural network fcoref () to update the hidden word representation hxi for words within each coreference cluster. Specifically, the output word vector has the following form: hidden representation of head h plus the relationspecific vector hr in vector space if (h, r, t) holds. To guide the knowledge-aware word representation learn"
D19-1295,D17-1070,0,0.0431231,"the input paragraph, for each word xi , we construct the expanded word vector by concatenating its word embedding wiword with its character-level representation and extra word-level features4 as: wi = [wiword ; wichar ; wif eatures ] The word-level BiLSTM layer will process the sequence of expanded word vectors (w1 , w2 , ..., wL ) and compute the word xi ’s hidden representation at each word index i: hxi = BiLST M (w1 , w2 , ..., wL ) DU-level BiLSTM Layer: Given the output of word-level BiLSTM (hx1 , hx2 , ..., hxL ), we calculate the raw DU representation by applying max-pooling operation (Conneau et al., 2017) over the sequence of word representations for all words within a discourse unit: h0DUj = maxxi 2DUj hxi Then, the DU-level BiLSTM will process the sequence of raw DU representations and obtain the refined DU representation hDUj for the j-th discourse unit in a paragraph: hDUj = BiLST M (h0DU1 , h0DU2 , ..., h0DUT +1 ) 3 Both character embedding and CNN hidden size is 50. In this work, we used capitalization (Cap) flag, Partof-speech (POS) tag and named entity (NER) tag of each word as extra word-level features. The embedding size for Cap/POS/NER is 5/35/20. We used Standford CoreNLP toolkit ("
D19-1295,N18-1013,1,0.480843,"is that external knowledge derived event relations or hard linguistic constraints may not always apply for interpreting a particular context, and may hurt performance if used blindly (Kishimoto et al., 2018). Therefore, we propose to tightly integrate these constraints into the discourse relation inference process by manipulating hidden word representations to reflect relations between words, and meanwhile balance attentions to contexts and constraints through adding a knowledge regularization term in the final objective function. Specifically, we choose the paragraph-level model we proposed (Dai and Huang, 2018) as the base model, which exploits wider paragraph-level contexts and has been shown effective for PDTBstyle discourse parsing. The model mainly consists of a two-level hierarchical BiLSTMs (Schuster and Paliwal, 1997) for modeling both wordlevel and DU-level inter-dependencies (with a brief description in section 3.1). To implement the knowledge guided regularization for discourse parsing, we first insert a new knowledge layer between the word-level BiLSTM and DU-level BiLSTM layer. This knowledge layer modifies hidden representations of words that participate in an event or coreference relat"
D19-1295,D09-1036,0,0.0418847,"ge Layer Word-level BiLSTM Layer Word vectors xi Char-level CNN Char emb ... f r() ... ... Word emb Features emb (GloVe/ELMo) (POS/NER) coreference relations event temporal ... event causal knowledge knowledge External Knowledge Figure 1: Model Architecture for Paragraph-level Discourse Parsing. The left part is the base model. The right part with colored arrows and neurons show how to incorporate coreference and event knowledge into base model. Untied (Explicit vs. Implicit) Prediction Layer: Considering the different natures of explicit and implicit discourse relations (Pitler et al., 2009; Lin et al., 2009), the base model trains two independent linear layers with untied parameters for predicting explicit or implicit discourse relations between each two adjacent DUs respectively: ( Wexp [hDUt ; hDUt+1 ] + bexp , if yt 2 exp h yt = Wimp [hDUt ; hDUt+1 ] + bimp , if yt 2 imp CRF Layer for Discourse Relation Sequence Labeling: A CRF layer (Biran and McKeown, 2015) is added on top of the prediction layer to fine-tune the predicted sequence of discourse relations by capturing continuity and transition patterns (e.g., a temporal relation is likely to follow another temporal relation). Given the hidden"
D19-1295,D16-1130,0,0.0646515,"i GPU. 4.3 Models for Comparison We compare our proposed regularization models with the following base model, our own baselines and recent published discourse parsing systems: • (Dai and Huang, 2018): the original model for paragraph-level discourse parsing. 13 We followed Ji and Eisenstein (2015) to exclude 5 minor second-level classes in our experiments because none of these classes appear in the test or dev sets. 2981 Implicit Explicit Macro Acc Comp Cont Exp Temp Macro Acc Previous work with the same evaluation setting (Rutherford and Xue, 2015) 40.50 57.10 (Liu et al., 2016) 44.98 57.25 (Liu and Li, 2016) 46.29 57.57 (Lan et al., 2017) 47.80 57.39 (Dai and Huang, 2018) 48.82 57.44 37.72 49.39 67.45 40.70 93.21 93.98 (Bai and Zhao, 2018) (ELMo) 51.06 Our models using GloVe word embeddings Base Model 48.96 56.42 41.29 47.77 66.16 40.60 93.78 94.64 Base Model + Coreference (C) 49.42 57.15 41.39 49.26 66.89 40.14 93.73 94.62 Base Model + Event Temporal 49.58 57.31 41.52 46.49 67.50 42.83 93.87 94.72 Base Model + Event (E) 50.02 58.22 40.20 48.06 68.35 43.45 93.63 94.46 Full Model (Base Model + C&E) 50.49 58.32 39.61 49.29 68.23 44.83 94.32 95.07 Our own baselines with knowledge features using GloV"
D19-1295,P84-1076,0,0.424936,"Missing"
D19-1295,P14-5010,0,0.00375105,"over the sequence of word representations for all words within a discourse unit: h0DUj = maxxi 2DUj hxi Then, the DU-level BiLSTM will process the sequence of raw DU representations and obtain the refined DU representation hDUj for the j-th discourse unit in a paragraph: hDUj = BiLST M (h0DU1 , h0DU2 , ..., h0DUT +1 ) 3 Both character embedding and CNN hidden size is 50. In this work, we used capitalization (Cap) flag, Partof-speech (POS) tag and named entity (NER) tag of each word as extra word-level features. The embedding size for Cap/POS/NER is 5/35/20. We used Standford CoreNLP toolkit (Manning et al., 2014) to generate POS and NER tags. 2978 4 CRF loss Incorporate Event Knowledge and Coreference Relations into Base Model CRF layer Discourse relation representations Untied Prediction Layer y1 y2 DU representations ... ... Knowledge Regularization yT TransE score function: dtransE(h, r, t) DU-level BiLSTM Layer DU1 Max-pooling DU2 DUT+1 ... ... Knowledge-aware word vectors f coref () Knowledge Layer Word-level BiLSTM Layer Word vectors xi Char-level CNN Char emb ... f r() ... ... Word emb Features emb (GloVe/ELMo) (POS/NER) coreference relations event temporal ... event causal knowledge knowledge"
D19-1295,W12-1614,0,0.0135321,"work models (Chen et al., 2016; Ji et al., 2016; Rutherford et al., 2017; Qin et al., 2017; Guo et al., 2018; Bai and Zhao, 2018). Paragraph-wide contexts were considered for building better discourse unit representations in Dai and Huang (2018). Another research direction for improving implicit discourse relation classification is to expand the training data by leveraging explicit relations (Liu et al., 2016; Lan et al., 2017) or discourse connective informed unlabeled data (Rutherford and Xue, 2015; Xu et al., 2018). 2.2 Incorporate Knowledge into Discourse Parsing Only a few previous work (Park and Cardie, 2012; Biran and McKeown, 2013; Lei et al., 2018) has exploited external knowledge, including WordNet features (e.g., Antonyms and Hypernyms) and Verb Class (Levin, 1993), in discourse parsing by deriving discrete indicator features and then feed them into feature-based classifiers. Incorporating knowledge as additional features into neural network models often generalize poorly due to the sparsity of features, as also shown in our experiments. Recently, Kishimoto et al. (2018) incorporated the whole of ConceptNet into a MAGEGRU (Dhingra et al., 2017) based neural networks, but their experiments sh"
D19-1295,D16-1246,0,0.0360644,"Missing"
D19-1295,P17-1093,0,0.0132466,"compared to all previous work. 2 2.1 Related Work Discourse Parsing on PDTB With the release of Penn Discourse Treebank (PDTB) (Prasad et al., 2008), the task of discourse parsing, especially implicit discourse relation recognition, has received a lot of attention from the NLP community and researchers (Pitler and Nenkova, 2009; Lin et al., 2014; Xue et al., 2015; Rutherford and Xue, 2016). A large number of previous work attempted to model the semantic meanings of two discourse units using latest and advanced neural network models (Chen et al., 2016; Ji et al., 2016; Rutherford et al., 2017; Qin et al., 2017; Guo et al., 2018; Bai and Zhao, 2018). Paragraph-wide contexts were considered for building better discourse unit representations in Dai and Huang (2018). Another research direction for improving implicit discourse relation classification is to expand the training data by leveraging explicit relations (Liu et al., 2016; Lan et al., 2017) or discourse connective informed unlabeled data (Rutherford and Xue, 2015; Xu et al., 2018). 2.2 Incorporate Knowledge into Discourse Parsing Only a few previous work (Park and Cardie, 2012; Biran and McKeown, 2013; Lei et al., 2018) has exploited external k"
D19-1295,E17-1027,0,0.0123723,"rse relation recognition compared to all previous work. 2 2.1 Related Work Discourse Parsing on PDTB With the release of Penn Discourse Treebank (PDTB) (Prasad et al., 2008), the task of discourse parsing, especially implicit discourse relation recognition, has received a lot of attention from the NLP community and researchers (Pitler and Nenkova, 2009; Lin et al., 2014; Xue et al., 2015; Rutherford and Xue, 2016). A large number of previous work attempted to model the semantic meanings of two discourse units using latest and advanced neural network models (Chen et al., 2016; Ji et al., 2016; Rutherford et al., 2017; Qin et al., 2017; Guo et al., 2018; Bai and Zhao, 2018). Paragraph-wide contexts were considered for building better discourse unit representations in Dai and Huang (2018). Another research direction for improving implicit discourse relation classification is to expand the training data by leveraging explicit relations (Liu et al., 2016; Lan et al., 2017) or discourse connective informed unlabeled data (Rutherford and Xue, 2015; Xu et al., 2018). 2.2 Incorporate Knowledge into Discourse Parsing Only a few previous work (Park and Cardie, 2012; Biran and McKeown, 2013; Lei et al., 2018) has ex"
D19-1295,E14-1068,0,0.026011,"ge Processing and the 9th International Joint Conference on Natural Language Processing, pages 2976–2987, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics events (situations in general) as described in two discourse units. As shown in the above example, the “Cause” discourse relation between the two DUs depends on the relation between two events “smoking” and “lung-cancer” with one event in each DU. Second, we consider entity coreference relations as a useful form of linguistic constraints in inferring discourse relations. This is motivated by prior work (Rutherford and Xue, 2014; Ji and Eisenstein, 2015) showing that coreference based features can improve entity mention representations within a DU, which facilitates recognizing coherence and discourse relations between DUs. In this paper, we investigate how to incorporate external event knowledge and entity coreference relation based linguistic constraints into neural network models for discourse parsing. One key difficulty we want to address is that external knowledge derived event relations or hard linguistic constraints may not always apply for interpreting a particular context, and may hurt performance if used bl"
D19-1295,N15-1081,0,0.307269,"r of previous work attempted to model the semantic meanings of two discourse units using latest and advanced neural network models (Chen et al., 2016; Ji et al., 2016; Rutherford et al., 2017; Qin et al., 2017; Guo et al., 2018; Bai and Zhao, 2018). Paragraph-wide contexts were considered for building better discourse unit representations in Dai and Huang (2018). Another research direction for improving implicit discourse relation classification is to expand the training data by leveraging explicit relations (Liu et al., 2016; Lan et al., 2017) or discourse connective informed unlabeled data (Rutherford and Xue, 2015; Xu et al., 2018). 2.2 Incorporate Knowledge into Discourse Parsing Only a few previous work (Park and Cardie, 2012; Biran and McKeown, 2013; Lei et al., 2018) has exploited external knowledge, including WordNet features (e.g., Antonyms and Hypernyms) and Verb Class (Levin, 1993), in discourse parsing by deriving discrete indicator features and then feed them into feature-based classifiers. Incorporating knowledge as additional features into neural network models often generalize poorly due to the sparsity of features, as also shown in our experiments. Recently, Kishimoto et al. (2018) incorp"
D19-1295,K16-2007,0,0.0147679,"ledge regularization approach can effectively utilize several types of externally obtained event knowledge and entity coreference relations1 , and improves the performance of both implicit and explicit discourse relation recognition compared to all previous work. 2 2.1 Related Work Discourse Parsing on PDTB With the release of Penn Discourse Treebank (PDTB) (Prasad et al., 2008), the task of discourse parsing, especially implicit discourse relation recognition, has received a lot of attention from the NLP community and researchers (Pitler and Nenkova, 2009; Lin et al., 2014; Xue et al., 2015; Rutherford and Xue, 2016). A large number of previous work attempted to model the semantic meanings of two discourse units using latest and advanced neural network models (Chen et al., 2016; Ji et al., 2016; Rutherford et al., 2017; Qin et al., 2017; Guo et al., 2018; Bai and Zhao, 2018). Paragraph-wide contexts were considered for building better discourse unit representations in Dai and Huang (2018). Another research direction for improving implicit discourse relation classification is to expand the training data by leveraging explicit relations (Liu et al., 2016; Lan et al., 2017) or discourse connective informed u"
D19-1295,D14-1162,0,0.0820152,"e made to the base model: (1) we insert a novel knowledge layer between the two BiLSTM layers of the base mode; (2) we add a regularizer into the overall objective function. We will first briefly describe the base model, a replication2 of our recently proposed paragraph-level discourse parsing model (Dai and Huang, 2018). We will then explain the knowledge layer and knowledge regularizer we added. 2 In our re-implementation, we made several minor modifications to the original base model by using character-level features as well as supporting both traditional fixed word embeddings (300D GloVe (Pennington et al., 2014)) and latest context-dependent word embeddings (1024D ELMo (Peters et al., 2018)) for word embedding initialization. 3.1 Base Model The base model processes a paragraph containing a sequence of discourse units each time, and predicts a sequence of discourse relations (both implicit and explicit relations) with one relation between each pair of adjacent discourse units (DU). The base model utilizes a hierarchical BiLSTM to calculate both word-level and DU-level representations, followed by a prediction layer and Conditional Random Field (CRF) layer (Lafferty et al., 2001) for jointly predicting"
D19-1295,speer-havasi-2012-representing,0,0.0105065,"sets respectively. Knowledge Preprocessing: Table 1 gives an overview of the relation types used in our experiments and the number of triplets (clusters) identified in the PDTB dataset. Specifically, for coreference relations, we utilized the Stanford CoreNLP coreference resolver to identify coreference clusters in each paragraph. For event knowledge, we considered three major event relation types including temporal, causal and subevent. We obtained event temporal knowledge from a previous work (Yao and Huang, 2018)10 and we retrieved the latter two types of event knowledge from ConceptNet11 (Speer and Havasi, 2012), which is a widelyused commonsense knowledge base. 4.2 Experiment Setting Evaluation Setting: Annotated discourse relation labels in PDTB v2.0 are organized in a three-level hierarchy. The top-level coarse-grained discourse relation classes include Comparison (Comp), Contingency (Cont), Expansion (Exp) and Temporal (Temp), which are further split into 16 finegrained classes at the second-level. To compare with previous work, we report the macro-average F1-score and accuracy12 on the top-level multi9 Available at https://github.com/ZeyuDai/ paragraph_implicit_discourse_relations. Relations (0."
D19-1295,N18-1202,0,0.02381,"TM layers of the base mode; (2) we add a regularizer into the overall objective function. We will first briefly describe the base model, a replication2 of our recently proposed paragraph-level discourse parsing model (Dai and Huang, 2018). We will then explain the knowledge layer and knowledge regularizer we added. 2 In our re-implementation, we made several minor modifications to the original base model by using character-level features as well as supporting both traditional fixed word embeddings (300D GloVe (Pennington et al., 2014)) and latest context-dependent word embeddings (1024D ELMo (Peters et al., 2018)) for word embedding initialization. 3.1 Base Model The base model processes a paragraph containing a sequence of discourse units each time, and predicts a sequence of discourse relations (both implicit and explicit relations) with one relation between each pair of adjacent discourse units (DU). The base model utilizes a hierarchical BiLSTM to calculate both word-level and DU-level representations, followed by a prediction layer and Conditional Random Field (CRF) layer (Lafferty et al., 2001) for jointly predicting a sequence of discourse relations within a paragraph. The base model consists o"
D19-1295,P09-1077,0,0.0462635,"rs f coref () Knowledge Layer Word-level BiLSTM Layer Word vectors xi Char-level CNN Char emb ... f r() ... ... Word emb Features emb (GloVe/ELMo) (POS/NER) coreference relations event temporal ... event causal knowledge knowledge External Knowledge Figure 1: Model Architecture for Paragraph-level Discourse Parsing. The left part is the base model. The right part with colored arrows and neurons show how to incorporate coreference and event knowledge into base model. Untied (Explicit vs. Implicit) Prediction Layer: Considering the different natures of explicit and implicit discourse relations (Pitler et al., 2009; Lin et al., 2009), the base model trains two independent linear layers with untied parameters for predicting explicit or implicit discourse relations between each two adjacent DUs respectively: ( Wexp [hDUt ; hDUt+1 ] + bexp , if yt 2 exp h yt = Wimp [hDUt ; hDUt+1 ] + bimp , if yt 2 imp CRF Layer for Discourse Relation Sequence Labeling: A CRF layer (Biran and McKeown, 2015) is added on top of the prediction layer to fine-tune the predicted sequence of discourse relations by capturing continuity and transition patterns (e.g., a temporal relation is likely to follow another temporal relation"
D19-1295,P09-2004,0,0.0254546,"he experiments on PDTB v2.0 demonstrate that our proposed knowledge regularization approach can effectively utilize several types of externally obtained event knowledge and entity coreference relations1 , and improves the performance of both implicit and explicit discourse relation recognition compared to all previous work. 2 2.1 Related Work Discourse Parsing on PDTB With the release of Penn Discourse Treebank (PDTB) (Prasad et al., 2008), the task of discourse parsing, especially implicit discourse relation recognition, has received a lot of attention from the NLP community and researchers (Pitler and Nenkova, 2009; Lin et al., 2014; Xue et al., 2015; Rutherford and Xue, 2016). A large number of previous work attempted to model the semantic meanings of two discourse units using latest and advanced neural network models (Chen et al., 2016; Ji et al., 2016; Rutherford et al., 2017; Qin et al., 2017; Guo et al., 2018; Bai and Zhao, 2018). Paragraph-wide contexts were considered for building better discourse unit representations in Dai and Huang (2018). Another research direction for improving implicit discourse relation classification is to expand the training data by leveraging explicit relations (Liu et"
D19-1295,prasad-etal-2008-penn,0,0.899352,"into the objective function. Experiments show that our knowledge regularization approach outperforms all previous systems on the benchmark dataset PDTB for discourse parsing. 1 Introduction Discourse parsing and identifying rhetorical discourse relations between two text spans (i.e., discourse units, either clauses or sentences) is crucial and beneficial for a wide variety of downstream tasks and applications such as machine translation (Webber et al., 2017), text generation (Mann, 1984; Bosselut et al., 2018) and text summarization (Gerani et al., 2014). In the PDTB-style discourse parsing (Prasad et al., 2008), we commonly distinguish implicit discourse relations from explicit relations, depending on whether a discourse connective (e.g., “because”, “however”) appears between two discourse units. In general, recognizing implicit discourse relations is more challenging due to the lack of connective, which has recently drawn significant attention from the NLP researchers. Recent research for implicit discourse relation classification has mostly focused on applying powerful neural network models (Qin et al., 2016a,b; Liu and Li, 2016; Lei et al., 2017; Bai and Zhao, 2018) for modeling compositional mea"
D19-1295,C16-1180,0,0.0268433,"Missing"
D19-1295,W17-4800,0,0.0852247,"Missing"
D19-1295,D18-1079,0,0.0236334,"ed to model the semantic meanings of two discourse units using latest and advanced neural network models (Chen et al., 2016; Ji et al., 2016; Rutherford et al., 2017; Qin et al., 2017; Guo et al., 2018; Bai and Zhao, 2018). Paragraph-wide contexts were considered for building better discourse unit representations in Dai and Huang (2018). Another research direction for improving implicit discourse relation classification is to expand the training data by leveraging explicit relations (Liu et al., 2016; Lan et al., 2017) or discourse connective informed unlabeled data (Rutherford and Xue, 2015; Xu et al., 2018). 2.2 Incorporate Knowledge into Discourse Parsing Only a few previous work (Park and Cardie, 2012; Biran and McKeown, 2013; Lei et al., 2018) has exploited external knowledge, including WordNet features (e.g., Antonyms and Hypernyms) and Verb Class (Levin, 1993), in discourse parsing by deriving discrete indicator features and then feed them into feature-based classifiers. Incorporating knowledge as additional features into neural network models often generalize poorly due to the sparsity of features, as also shown in our experiments. Recently, Kishimoto et al. (2018) incorporated the whole o"
D19-1295,P17-1132,0,0.0301331,"es, as also shown in our experiments. Recently, Kishimoto et al. (2018) incorporated the whole of ConceptNet into a MAGEGRU (Dhingra et al., 2017) based neural networks, but their experiments show that it did not work well for improving implicit discourse relation identification compared with their own base1 Entity coreference relations were generated using an existing coreference resolver from Standford CoreNLP toolkit. 2977 line. We interpret this negative result as the consequence of using irrelevant (noisy) knowledge types blindly without proper regularization. There are also recent work (Yang and Mitchell, 2017; Xu et al., 2017; Zhou et al., 2018) that incorporate external knowledge into neural network models for improving several other NLP tasks, including information extraction and conversation generation, which mostly followed the twostep approach that first obtained representations of knowledge (with triplet format) from knowledge base using knowledge graph embedding methods such as TransE (Bordes et al., 2013), and then utilized attention mechanism (or added gates in a RNN cell (Ma et al., 2018)) to integrate knowledge representations with hidden word vectors. This approach has two main drawbac"
D19-1295,P18-1050,1,0.835419,"code9 of Dai and Huang (2018), and obtained 12,037/1222/1050 paragraph instances in train/dev/test sets respectively. Knowledge Preprocessing: Table 1 gives an overview of the relation types used in our experiments and the number of triplets (clusters) identified in the PDTB dataset. Specifically, for coreference relations, we utilized the Stanford CoreNLP coreference resolver to identify coreference clusters in each paragraph. For event knowledge, we considered three major event relation types including temporal, causal and subevent. We obtained event temporal knowledge from a previous work (Yao and Huang, 2018)10 and we retrieved the latter two types of event knowledge from ConceptNet11 (Speer and Havasi, 2012), which is a widelyused commonsense knowledge base. 4.2 Experiment Setting Evaluation Setting: Annotated discourse relation labels in PDTB v2.0 are organized in a three-level hierarchy. The top-level coarse-grained discourse relation classes include Comparison (Comp), Contingency (Cont), Expansion (Exp) and Temporal (Temp), which are further split into 16 finegrained classes at the second-level. To compare with previous work, we report the macro-average F1-score and accuracy12 on the top-level"
D19-1664,P15-2072,0,0.124163,"0), and verb transitivity (Greene and Resnik, 2009). However, such studies fail to take into consideration biases that depend on a larger context, which is what we try to address in this work. Our work is also in line with framing analysis in social science theory, or the concept of selecting and signifying specific aspects of an event to promote a particular interpretation (Entman, 1993). In fact, informational bias can be considered a specific form of framing where the author intends to influence the reader’s opinion of an entity. The relationship between framing and news is investigated by Card et al. (2015), in which news articles are annotated with framing dimensions like “legality” and “public opinion.” BASIL contains richer information that allows us to study the purpose of “frames,” i.e., how biased content is invoked to support or oppose the issue at hand. Research in political science has also studied bias induced by the inclusion or omission of certain facts (Entman, 2007; Gentzkow and Shapiro, 2006, 2010; Prat and Str¨omberg, 2013). However, their definition of bias is typically grounded in how a reader perceives the ideological leaning of the article and news outlet, whereas our informa"
D19-1664,D14-1125,0,0.057938,"Missing"
D19-1664,N19-1423,0,0.136633,"l Level), of 300 news articles with lexical and informational bias spans. To examine how media sources encode bias differently, the dataset uses 100 triplets of articles, each reporting the same event from three outlets of different ideology. Based on our annotations, we find that all three sources use more informational bias than lexical bias, and informational bias is embedded uniformly across the entire article, while lexical bias is frequently observed at the beginning. We further explore the challenges in bias detection and benchmark BASIL using rule-based classifiers and the BERT model (Devlin et al., 2019) fine-tuned on our data. Results show that identifying informational bias poses additional difficulty and suggest future directions of encoding contextual knowledge from the full articles as well as reporting by other media. 2 Related Work Prior work on automatic bias detection based on natural language processing methods primarily deals with finding sentence-level bias and considers linguistic attributes like word polarity (Recasens et al., 2013), partisan phrases (Yano et al., 2010), and verb transitivity (Greene and Resnik, 2009). However, such studies fail to take into consideration biases"
D19-1664,N09-1057,0,0.516464,"term ∗ Equal contribution. Lisa Fan focused on annotation schema design and writing, Marshall White focused on data collection and statistical analysis. 1 Dataset can be found at www.ccs.neu.edu/home/ luwang/data.html. Figure 1: Examples of negative bias from Huffington Post (HPO), Fox News (FOX), and New York Times (NYT) discussing the same event. Informational bias and lexical bias are highlighted. The target of the bias is noted at the end of each span. Intermediary targets of indirect bias spans are underlined. lexical bias: bias stemming from content realization, or how things are said (Greene and Resnik, 2009; Hube and Fetahu, 2019; Iyyer et al., 2014; Recasens et al., 2013; Yano et al., 2010). Such forms of bias typically do not depend on context outside of the sentence and can be alleviated while maintaining its semantics: polarized words can be removed or replaced, and clauses written in active voice can be rewritten in passive voice. However, political science researchers find that news bias can also be characterized by decisions 6343 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing,"
D19-1664,P14-1105,0,0.354244,"annotation schema design and writing, Marshall White focused on data collection and statistical analysis. 1 Dataset can be found at www.ccs.neu.edu/home/ luwang/data.html. Figure 1: Examples of negative bias from Huffington Post (HPO), Fox News (FOX), and New York Times (NYT) discussing the same event. Informational bias and lexical bias are highlighted. The target of the bias is noted at the end of each span. Intermediary targets of indirect bias spans are underlined. lexical bias: bias stemming from content realization, or how things are said (Greene and Resnik, 2009; Hube and Fetahu, 2019; Iyyer et al., 2014; Recasens et al., 2013; Yano et al., 2010). Such forms of bias typically do not depend on context outside of the sentence and can be alleviated while maintaining its semantics: polarized words can be removed or replaced, and clauses written in active voice can be rewritten in passive voice. However, political science researchers find that news bias can also be characterized by decisions 6343 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 6343–6349, c Hong Kong, China, Novem"
D19-1664,P13-1162,0,0.281463,"sign and writing, Marshall White focused on data collection and statistical analysis. 1 Dataset can be found at www.ccs.neu.edu/home/ luwang/data.html. Figure 1: Examples of negative bias from Huffington Post (HPO), Fox News (FOX), and New York Times (NYT) discussing the same event. Informational bias and lexical bias are highlighted. The target of the bias is noted at the end of each span. Intermediary targets of indirect bias spans are underlined. lexical bias: bias stemming from content realization, or how things are said (Greene and Resnik, 2009; Hube and Fetahu, 2019; Iyyer et al., 2014; Recasens et al., 2013; Yano et al., 2010). Such forms of bias typically do not depend on context outside of the sentence and can be alleviated while maintaining its semantics: polarized words can be removed or replaced, and clauses written in active voice can be rewritten in passive voice. However, political science researchers find that news bias can also be characterized by decisions 6343 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 6343–6349, c Hong Kong, China, November 3–7, 2019. 2019 Ass"
D19-1664,P10-1059,0,0.0333047,"or indirect), and whether the bias is part of a quote. Bias aim investigates the case where the main entity is indirectly targeted through an intermediary figure (see the HPO example in Figure 1, where the sentiment towards the intermediary entity “Trump Administration” is transferred to the main target, “Donald Trump”). Statistics are presented in Table 1. Inter-annotator Agreement (IAA). Two annotators individually annotate each article triplet before discussing their annotations together to resolve conflicts and agree on “gold-standard” labels. We measure span-level agreement according to Toprak et al. (2010), where we calculate the F1 score of span overlaps between two sets of annotations (details are in the Supplementary). Although the F1 scores of IAA are unsurprisingly low for this highly variable task, the score dramatically in2 The likely effect of annotators’ prior beliefs on their perception of bias will be investigated in future work. creases when agreement is calculated between individual annotations and the gold standard—from 0.34 to 0.70 for informational bias spans and from 0.14 to 0.56 for the sparser lexical spans, demonstrating the effectiveness of resolution discussions. During th"
D19-1664,H05-1044,0,0.0128664,"le with the lowest average TF-IDF token scores as containing 4 BERT’s maximum input length is 512 tokens, which is shorter than most articles in BASIL. We thus treat sentences as passages, rather than using text of fixed length. 5 BASIL averages 4.1 informational bias spans per article. Token-level Classifier. From Table 2, we see that the BERT lexical sequence tagger produces better recall and F1 than the informational tagger, highlighting the additional difficulty of accurately identifying spans of informational bias. We also use the polarity and subjectivity lexicons from the MPQA website (Wilson et al., 2005; Choi and Wiebe, 2014) as a simple baseline for lexical bias tagging and find that these word-level cues, though widely used in prior sentiment analysis studies, are insufficient to fully capture lexical bias. In order to evaluate token-level prediction on the larger original test set, we conduct a pipeline experiment with the fine-tuned BERT models where sentences predicted as containing bias by the best sentence-level classifier from cross validation are tagged by the best token-level model. The results reaffirm our hypothesis that while both tasks are extremely difficult, informational bia"
D19-1664,W10-0723,0,0.028067,"Missing"
E12-1029,P11-1098,0,0.301261,"action patterns in an unsupervised way (e.g., (Shinyama and Sekine, 2006; Sekine, 2006)). But these efforts target open domain information extraction. To extract domainspecific event information, domain experts are needed to select the pattern subsets to use. There have also been weakly supervised approaches that use more than just local context. (Patwardhan and Riloff, 2007) uses a semantic affinity measure to learn primary and secondary patterns, and the secondary patterns are applied only to event sentences. The event sentence classifier is self-trained using seed patterns. Most recently, (Chambers and Jurafsky, 2011) acquire event words from an external resource, group the event words to form event scenarios, and group extraction patterns for different event roles. However, these weakly supervised systems produce substantially lower performance than the best supervised systems. 3 Overview of TIER The goal of our research is to develop a weakly supervised training process that can successfully train a state-of-the-art event extraction system for a new domain with minimal human input. We decided to focus our efforts on the TIER event extraction model because it recently produced better performance on the MU"
E12-1029,P05-1045,0,0.00660023,"the negative:positive ratio to be 10:1. Once the classifier is trained, it is applied to the unlabeled noun phrases in the relevant documents. Noun phrases that are assigned role filler labels by the classifier with high confidence (using the sliding threshold) are added to the set of positive instances. New negative instances are drawn randomly from the irrelevant documents to maintain the 10:1 (negative:positive) ratio. We extract features from each noun phrase (NP) and its surrounding context. The features include the NP head noun and its premodifiers. We also use the Stanford NER tagger (Finkel et al., 2005) to identify Named Entities within the NP. The context features include four words to the left of the NP, four words to the right of the NP, and the lexico-syntactic patterns generated by AutoSlog to capture expressions around the NP (see (Riloff, 1993) for details). 4.2.2 Event Sentence Classifier The event sentence classifier is responsible for identifying sentences that describe a relevant event. Similar to the noun phrase classifier training, positive training instances are selected from the relevant documents and negative instances are drawn from the irrelevant documents. All sentences in"
E12-1029,P98-1067,0,0.414988,"xtraction systems process stories about domain-relevant events and identify the role fillers of each event. A key challenge for event extraction is that recognizing role fillers is inherently contextual. For example, a PERSON can be a perpetrator or a victim in different contexts (e.g., “John Smith assassinated the mayor” vs. “John Smith was assassinated”). Similarly, any COM PANY can be an acquirer or an acquiree depending on the context. Many supervised learning techniques have been used to create event extraction systems using gold standard “answer key” event templates for training (e.g., (Freitag, 1998a; Chieu and Ng, The goal of our research is to use bootstrapping techniques to automatically train a state-ofthe-art event extraction system without humangenerated answer key templates. The focus of our work is the TIER event extraction model, which is a multi-layered architecture for event extraction (Huang and Riloff, 2011). TIER’s innovation over previous techniques is the use of four different classifiers that analyze a document at increasing levels of granularity. TIER progressively zooms in on event information using a pipeline of classifiers that perform document-level classification,"
E12-1029,P06-1061,0,0.186873,"3; Riloff, 1993; Soderland et al., 1995; Huffman, 1996; Freitag, 1998b; Ciravegna, 2001; Califf and Mooney, 2003; Riloff, 1996; Riloff and Jones, 1999; Yangarber et al., 2000; Sudo et al., 2003; Stevenson and Greenwood, 2005) or classifierbased (e.g., (Freitag, 1998a; Chieu and Ng, 2002; Finn and Kushmerick, 2004; Li et al., 2005; Yu et al., 2005)). Recently, several approaches have been proposed to address the insufficiency of using only local context to identify role fillers. Some approaches look at the broader sentential context around a potential role filler when making a decision (e.g., (Gu and Cercone, 2006; Patwardhan and Riloff, 2009)). Other systems take a more global view and consider discourse properties of the document as a whole to improve performance (e.g., (Maslennikov and Chua, 2007; Ji and Grishman, 2008; Liao and Grishman, 2010; Huang and Riloff, 2011)). Currently, the learning-based event extraction systems that perform best all use supervised learning techniques that require a large number of texts coupled with manually-generated annotations or answer key templates. A variety of techniques have been explored for weakly supervised training of event extraction systems, primarily in t"
E12-1029,P10-1029,1,0.736206,"nouns or if the semantic class of the head noun is compatible with the corresponding event role. In the previous example, tsunami will not be extracted as a weapon because it has an incompatible semantic class (EVENT), but bomb will be extracted because it has a compatible semantic class (WEAPON). We use the semantic class labels assigned by the Sundance parser (Riloff and Phillips, 2004) in our experiments. Sundance looks up each noun in a semantic dictionary to assign the semantic class labels. As an alternative, general resources (e.g., WordNet (Miller, 1990)) or a semantic tagger (e.g., (Huang and Riloff, 2010)) could be used. 289 men = Human building = Object ... Semantic Dictionary Constraints terrorists assassins snipers ... Role−Identifying Noun Constraints was killed by <np> <subject> attacked <subject> fired shots ... Role−Identifying Patterns men John Smith was killed by two armed 1 in broad daylight this morning. The assassins attacked the mayor as he 2 left his house to go to work about 8:00 am. Police arrested the unidentified men 3 an hour later. Figure 3: Automatic Training Data Creation 4.1.3 Propagating Labels with Coreference To enrich the automatically labeled training instances, we"
E12-1029,P11-1114,1,0.328003,"ayor” vs. “John Smith was assassinated”). Similarly, any COM PANY can be an acquirer or an acquiree depending on the context. Many supervised learning techniques have been used to create event extraction systems using gold standard “answer key” event templates for training (e.g., (Freitag, 1998a; Chieu and Ng, The goal of our research is to use bootstrapping techniques to automatically train a state-ofthe-art event extraction system without humangenerated answer key templates. The focus of our work is the TIER event extraction model, which is a multi-layered architecture for event extraction (Huang and Riloff, 2011). TIER’s innovation over previous techniques is the use of four different classifiers that analyze a document at increasing levels of granularity. TIER progressively zooms in on event information using a pipeline of classifiers that perform document-level classification, sentence classification, and noun phrase classification. TIER outperformed previous event extraction systems on the MUC-4 data set, but relied heavily on a large collection of 1,300 documents coupled with answer key templates to train its four classifiers. In this paper, we present a bootstrapping solution that exploits a larg"
E12-1029,P08-1030,0,0.476072,"eenwood, 2005) or classifierbased (e.g., (Freitag, 1998a; Chieu and Ng, 2002; Finn and Kushmerick, 2004; Li et al., 2005; Yu et al., 2005)). Recently, several approaches have been proposed to address the insufficiency of using only local context to identify role fillers. Some approaches look at the broader sentential context around a potential role filler when making a decision (e.g., (Gu and Cercone, 2006; Patwardhan and Riloff, 2009)). Other systems take a more global view and consider discourse properties of the document as a whole to improve performance (e.g., (Maslennikov and Chua, 2007; Ji and Grishman, 2008; Liao and Grishman, 2010; Huang and Riloff, 2011)). Currently, the learning-based event extraction systems that perform best all use supervised learning techniques that require a large number of texts coupled with manually-generated annotations or answer key templates. A variety of techniques have been explored for weakly supervised training of event extraction systems, primarily in the realm of pattern or rule-based approaches (e.g., (Riloff, 1996; Riloff and Jones, 1999; Yangarber et al., 2000; Sudo et al., 2003; Stevenson and Greenwood, 2005)). In some of these approaches, a human must man"
E12-1029,W05-0610,0,0.0413844,"s comparable to supervised training with 700 manually annotated documents. 2 Related Work Event extraction techniques have largely focused on detecting event “triggers” with their arguments for extracting role fillers. Classical methods are either pattern-based (Kim and Moldovan, 1993; Riloff, 1993; Soderland et al., 1995; Huffman, 1996; Freitag, 1998b; Ciravegna, 2001; Califf and Mooney, 2003; Riloff, 1996; Riloff and Jones, 1999; Yangarber et al., 2000; Sudo et al., 2003; Stevenson and Greenwood, 2005) or classifierbased (e.g., (Freitag, 1998a; Chieu and Ng, 2002; Finn and Kushmerick, 2004; Li et al., 2005; Yu et al., 2005)). Recently, several approaches have been proposed to address the insufficiency of using only local context to identify role fillers. Some approaches look at the broader sentential context around a potential role filler when making a decision (e.g., (Gu and Cercone, 2006; Patwardhan and Riloff, 2009)). Other systems take a more global view and consider discourse properties of the document as a whole to improve performance (e.g., (Maslennikov and Chua, 2007; Ji and Grishman, 2008; Liao and Grishman, 2010; Huang and Riloff, 2011)). Currently, the learning-based event extraction"
E12-1029,P10-1081,0,0.293509,"ifierbased (e.g., (Freitag, 1998a; Chieu and Ng, 2002; Finn and Kushmerick, 2004; Li et al., 2005; Yu et al., 2005)). Recently, several approaches have been proposed to address the insufficiency of using only local context to identify role fillers. Some approaches look at the broader sentential context around a potential role filler when making a decision (e.g., (Gu and Cercone, 2006; Patwardhan and Riloff, 2009)). Other systems take a more global view and consider discourse properties of the document as a whole to improve performance (e.g., (Maslennikov and Chua, 2007; Ji and Grishman, 2008; Liao and Grishman, 2010; Huang and Riloff, 2011)). Currently, the learning-based event extraction systems that perform best all use supervised learning techniques that require a large number of texts coupled with manually-generated annotations or answer key templates. A variety of techniques have been explored for weakly supervised training of event extraction systems, primarily in the realm of pattern or rule-based approaches (e.g., (Riloff, 1996; Riloff and Jones, 1999; Yangarber et al., 2000; Sudo et al., 2003; Stevenson and Greenwood, 2005)). In some of these approaches, a human must manually review and “clean”"
E12-1029,P07-1075,0,0.0273379,"al., 2003; Stevenson and Greenwood, 2005) or classifierbased (e.g., (Freitag, 1998a; Chieu and Ng, 2002; Finn and Kushmerick, 2004; Li et al., 2005; Yu et al., 2005)). Recently, several approaches have been proposed to address the insufficiency of using only local context to identify role fillers. Some approaches look at the broader sentential context around a potential role filler when making a decision (e.g., (Gu and Cercone, 2006; Patwardhan and Riloff, 2009)). Other systems take a more global view and consider discourse properties of the document as a whole to improve performance (e.g., (Maslennikov and Chua, 2007; Ji and Grishman, 2008; Liao and Grishman, 2010; Huang and Riloff, 2011)). Currently, the learning-based event extraction systems that perform best all use supervised learning techniques that require a large number of texts coupled with manually-generated annotations or answer key templates. A variety of techniques have been explored for weakly supervised training of event extraction systems, primarily in the realm of pattern or rule-based approaches (e.g., (Riloff, 1996; Riloff and Jones, 1999; Yangarber et al., 2000; Sudo et al., 2003; Stevenson and Greenwood, 2005)). In some of these appro"
E12-1029,D07-1075,1,0.961594,"Jones, 1999; Yangarber et al., 2000; Sudo et al., 2003; Stevenson and Greenwood, 2005)). In some of these approaches, a human must manually review and “clean” the learned patterns to obtain good performance. Research has also been done to learn extraction patterns in an unsupervised way (e.g., (Shinyama and Sekine, 2006; Sekine, 2006)). But these efforts target open domain information extraction. To extract domainspecific event information, domain experts are needed to select the pattern subsets to use. There have also been weakly supervised approaches that use more than just local context. (Patwardhan and Riloff, 2007) uses a semantic affinity measure to learn primary and secondary patterns, and the secondary patterns are applied only to event sentences. The event sentence classifier is self-trained using seed patterns. Most recently, (Chambers and Jurafsky, 2011) acquire event words from an external resource, group the event words to form event scenarios, and group extraction patterns for different event roles. However, these weakly supervised systems produce substantially lower performance than the best supervised systems. 3 Overview of TIER The goal of our research is to develop a weakly supervised train"
E12-1029,D09-1016,1,0.88382,"land et al., 1995; Huffman, 1996; Freitag, 1998b; Ciravegna, 2001; Califf and Mooney, 2003; Riloff, 1996; Riloff and Jones, 1999; Yangarber et al., 2000; Sudo et al., 2003; Stevenson and Greenwood, 2005) or classifierbased (e.g., (Freitag, 1998a; Chieu and Ng, 2002; Finn and Kushmerick, 2004; Li et al., 2005; Yu et al., 2005)). Recently, several approaches have been proposed to address the insufficiency of using only local context to identify role fillers. Some approaches look at the broader sentential context around a potential role filler when making a decision (e.g., (Gu and Cercone, 2006; Patwardhan and Riloff, 2009)). Other systems take a more global view and consider discourse properties of the document as a whole to improve performance (e.g., (Maslennikov and Chua, 2007; Ji and Grishman, 2008; Liao and Grishman, 2010; Huang and Riloff, 2011)). Currently, the learning-based event extraction systems that perform best all use supervised learning techniques that require a large number of texts coupled with manually-generated annotations or answer key templates. A variety of techniques have been explored for weakly supervised training of event extraction systems, primarily in the realm of pattern or rule-ba"
E12-1029,P06-2094,0,0.0874073,"re a large number of texts coupled with manually-generated annotations or answer key templates. A variety of techniques have been explored for weakly supervised training of event extraction systems, primarily in the realm of pattern or rule-based approaches (e.g., (Riloff, 1996; Riloff and Jones, 1999; Yangarber et al., 2000; Sudo et al., 2003; Stevenson and Greenwood, 2005)). In some of these approaches, a human must manually review and “clean” the learned patterns to obtain good performance. Research has also been done to learn extraction patterns in an unsupervised way (e.g., (Shinyama and Sekine, 2006; Sekine, 2006)). But these efforts target open domain information extraction. To extract domainspecific event information, domain experts are needed to select the pattern subsets to use. There have also been weakly supervised approaches that use more than just local context. (Patwardhan and Riloff, 2007) uses a semantic affinity measure to learn primary and secondary patterns, and the secondary patterns are applied only to event sentences. The event sentence classifier is self-trained using seed patterns. Most recently, (Chambers and Jurafsky, 2011) acquire event words from an external resour"
E12-1029,N06-1039,0,0.057139,"es that require a large number of texts coupled with manually-generated annotations or answer key templates. A variety of techniques have been explored for weakly supervised training of event extraction systems, primarily in the realm of pattern or rule-based approaches (e.g., (Riloff, 1996; Riloff and Jones, 1999; Yangarber et al., 2000; Sudo et al., 2003; Stevenson and Greenwood, 2005)). In some of these approaches, a human must manually review and “clean” the learned patterns to obtain good performance. Research has also been done to learn extraction patterns in an unsupervised way (e.g., (Shinyama and Sekine, 2006; Sekine, 2006)). But these efforts target open domain information extraction. To extract domainspecific event information, domain experts are needed to select the pattern subsets to use. There have also been weakly supervised approaches that use more than just local context. (Patwardhan and Riloff, 2007) uses a semantic affinity measure to learn primary and secondary patterns, and the secondary patterns are applied only to event sentences. The event sentence classifier is self-trained using seed patterns. Most recently, (Chambers and Jurafsky, 2011) acquire event words from an external resour"
E12-1029,P05-1047,0,0.332208,"trapped system, TIERlite , outperforms previous weakly supervised event extraction systems and achieves performance levels comparable to supervised training with 700 manually annotated documents. 2 Related Work Event extraction techniques have largely focused on detecting event “triggers” with their arguments for extracting role fillers. Classical methods are either pattern-based (Kim and Moldovan, 1993; Riloff, 1993; Soderland et al., 1995; Huffman, 1996; Freitag, 1998b; Ciravegna, 2001; Califf and Mooney, 2003; Riloff, 1996; Riloff and Jones, 1999; Yangarber et al., 2000; Sudo et al., 2003; Stevenson and Greenwood, 2005) or classifierbased (e.g., (Freitag, 1998a; Chieu and Ng, 2002; Finn and Kushmerick, 2004; Li et al., 2005; Yu et al., 2005)). Recently, several approaches have been proposed to address the insufficiency of using only local context to identify role fillers. Some approaches look at the broader sentential context around a potential role filler when making a decision (e.g., (Gu and Cercone, 2006; Patwardhan and Riloff, 2009)). Other systems take a more global view and consider discourse properties of the document as a whole to improve performance (e.g., (Maslennikov and Chua, 2007; Ji and Grishma"
E12-1029,P03-1029,0,0.113861,"show that the bootstrapped system, TIERlite , outperforms previous weakly supervised event extraction systems and achieves performance levels comparable to supervised training with 700 manually annotated documents. 2 Related Work Event extraction techniques have largely focused on detecting event “triggers” with their arguments for extracting role fillers. Classical methods are either pattern-based (Kim and Moldovan, 1993; Riloff, 1993; Soderland et al., 1995; Huffman, 1996; Freitag, 1998b; Ciravegna, 2001; Califf and Mooney, 2003; Riloff, 1996; Riloff and Jones, 1999; Yangarber et al., 2000; Sudo et al., 2003; Stevenson and Greenwood, 2005) or classifierbased (e.g., (Freitag, 1998a; Chieu and Ng, 2002; Finn and Kushmerick, 2004; Li et al., 2005; Yu et al., 2005)). Recently, several approaches have been proposed to address the insufficiency of using only local context to identify role fillers. Some approaches look at the broader sentential context around a potential role filler when making a decision (e.g., (Gu and Cercone, 2006; Patwardhan and Riloff, 2009)). Other systems take a more global view and consider discourse properties of the document as a whole to improve performance (e.g., (Maslenniko"
E12-1029,C00-2136,0,0.730164,"n research. Our results show that the bootstrapped system, TIERlite , outperforms previous weakly supervised event extraction systems and achieves performance levels comparable to supervised training with 700 manually annotated documents. 2 Related Work Event extraction techniques have largely focused on detecting event “triggers” with their arguments for extracting role fillers. Classical methods are either pattern-based (Kim and Moldovan, 1993; Riloff, 1993; Soderland et al., 1995; Huffman, 1996; Freitag, 1998b; Ciravegna, 2001; Califf and Mooney, 2003; Riloff, 1996; Riloff and Jones, 1999; Yangarber et al., 2000; Sudo et al., 2003; Stevenson and Greenwood, 2005) or classifierbased (e.g., (Freitag, 1998a; Chieu and Ng, 2002; Finn and Kushmerick, 2004; Li et al., 2005; Yu et al., 2005)). Recently, several approaches have been proposed to address the insufficiency of using only local context to identify role fillers. Some approaches look at the broader sentential context around a potential role filler when making a decision (e.g., (Gu and Cercone, 2006; Patwardhan and Riloff, 2009)). Other systems take a more global view and consider discourse properties of the document as a whole to improve performance"
E12-1029,P05-1062,0,0.0134343,"upervised training with 700 manually annotated documents. 2 Related Work Event extraction techniques have largely focused on detecting event “triggers” with their arguments for extracting role fillers. Classical methods are either pattern-based (Kim and Moldovan, 1993; Riloff, 1993; Soderland et al., 1995; Huffman, 1996; Freitag, 1998b; Ciravegna, 2001; Califf and Mooney, 2003; Riloff, 1996; Riloff and Jones, 1999; Yangarber et al., 2000; Sudo et al., 2003; Stevenson and Greenwood, 2005) or classifierbased (e.g., (Freitag, 1998a; Chieu and Ng, 2002; Finn and Kushmerick, 2004; Li et al., 2005; Yu et al., 2005)). Recently, several approaches have been proposed to address the insufficiency of using only local context to identify role fillers. Some approaches look at the broader sentential context around a potential role filler when making a decision (e.g., (Gu and Cercone, 2006; Patwardhan and Riloff, 2009)). Other systems take a more global view and consider discourse properties of the document as a whole to improve performance (e.g., (Maslennikov and Chua, 2007; Ji and Grishman, 2008; Liao and Grishman, 2010; Huang and Riloff, 2011)). Currently, the learning-based event extraction systems that perf"
E12-1029,C98-1064,0,\N,Missing
gao-huang-2017-detecting,N16-2013,0,\N,Missing
gao-huang-2017-detecting,W17-1101,0,\N,Missing
I08-4020,W06-0132,1,0.549935,"Missing"
I17-1078,D14-1162,0,0.0810561,"s and capture composite meanings of tweets using a sequence neural net classifier. Specifically, our LSTM classifier has a single layer of LSTM units. The output dimension size of the LSTM layer is 100. A sigmoid layer is built on the top of the LSTM layer to generate predictions. The input dropout rate and recurrent state dropout rate are both set to 0.2. In each iteration of the bootstrapping process, the training of the LSTM classifier runs for 10 epochs. The input to our LSTM classifier is a sequence of words. We pre-process and normalize tokens in tweets following the steps suggested in (Pennington et al., 2014). In addition, we used the pre-processing of emoji and smiley described in a preprocess tool 4 . Then we retrieve word vector representations from the downloaded5 pre-trained word2vec embeddings (Mikolov et al., 2013). The LSTM classifier is trained using the automatically labeled hateful tweets as positive instances and randomly sampled tweets as negative instances, with the ratio of POS:NEG as 1:10. Then the classifier is used to identify additional hateful tweets from the large set of unlabeled tweets. The LSTM classifier will deem a tweet as hateful if the tweet receives a confidence score"
I17-1078,W12-2103,0,0.321236,"apture both explicit and implicit inflammatory language. Explicit hate speech is easily identifiable by recognizing a clearly hateful word or phrase. For example: 2 Related Work Previous studies on hate speech recognition mostly used supervised approaches. Due to the sparsity of hate speech overall in reality, the data selection methods and annotations are often biased towards a specific type of hate speech or hate speech generated in certain scenarios. For instance, Razavi et al. (2010) conducted their experiments on 1525 annotated sentences from a company’s log file and a certain newsgroup. Warner and Hirschberg (2012) labeled around 9000 human labeled paragraphs from Yahoo!’s news group post and American Jewish Congress’s website, and the labeling is restricted to antiSemitic hate speech. Sood et al. (2012) studied use of profanity on a dataset of 6,500 labeled comments from Yahoo! Buzz. Kwok and Wang (2013) built a balanced corpus of 24582 tweets consisting of anti-black and non-anti black tweets. The tweets were manually selected from Twitter accounts that were believed to be racist based upon their reactions to anti-Obama articles. Burnap and Williams (2014) collected hateful tweets related to the murde"
I17-1078,N16-2013,0,0.541687,"inflammatory language. 1 Ruihong Huang Texas A&M University huangrh@cse.tamu.edu Introduction Following a turbulent election season, 2016’s digital footprint is awash with hate speech. Apart from censorship, the goals of enabling computers to understand inflammatory language are many. Sensing increased proliferation of hate speech can elucidate public opinion surrounding polarizing events. Identifying hateful declarations can bolster security in revealing individuals harboring malicious intentions towards specific groups. Recent studies on supervised methods for online hate speech detection (Waseem and Hovy, 2016; Nobata et al., 2016) have relied on manually annotated data sets, which are not only costly to create but also likely to be insufficient to obtain wide-coverage hate speech detection systems. This is mainly because online hate speech is relatively infrequent (among large amounts of online To address the various limitations of supervised hate speech detection methods, we present a weakly supervised two-path bootstrapping approach for online hate speech detection that requires minimal human supervision and can be easily retrained and adapted to capture new types of inflammatory language. Our t"
I17-1078,D14-1181,0,0.00775571,"Missing"
I17-1078,D16-1076,0,0.0516894,"Missing"
I17-2040,D17-1190,1,0.704636,"ese prior studies is on designing new neural network architectures (e.g., tree-structured LSTMs) corresponding to the parse tree structure. In contrast, our method aims at extracting appropriate event-centered data representations from dependency trees so that the neural net models can effectively concentrate on relevant regions of contexts. Similar to our dependency chains, dependency paths between two nodes in a dependency tree have been widely used as features for various NLP tasks and applications, including relation extraction (Bunescu and Mooney, 2005), temporal relation identification (Choubey and Huang, 2017) semantic parsing (Moschitti, 2004) and question 1 We used the Stanford CoreNLP to generate dependency parse trees. 2 http://nlp.stanford.edu/software/ dependencies_manual.pdf 235 Model PA CNN (Huang et al., 2016) LSTM CNN LSTM tree-LSTM OG FU Local Contexts 91/83/87 46/57/51 49/67/57 88/83/85 47/54/51 52/62/57 Dependency Chains 91/84/87 49/63/55 60/65/62 92/85/88 49/63/55 63/71/67 Full Dependency Trees 92/80/86 47/59/53 30/58/40 Macro Micro 62/69/65 63/66/64 77/77/76.9 75/75/75.5 67/71/68 68/73/70 79/79/78.6 80/80/79.6 56/66/60 75/75/75.1 Table 1: Classification results on the test set. Each"
I17-2040,D16-1005,1,0.82101,"put in neural network models, which consistently outperform previous models using local context words as input. Visualization verifies that the dependency chain representation can effectively capture the context events which are closely related to the target event and play key roles in predicting event temporal status. 1 Introduction Event temporal status identification aims to recognize whether an event has happened (PAST), is currently happening (ON-GOING) or has not happened yet (FUTURE), which can be crucial for event prediction, timeline generation and news summarization. Our prior work (Huang et al., 2016) showed that linguistic features, such as tense, aspect and time expressions, are insufficient for this challenging task, which instead requires accurate understanding of composite meanings from wide sentential contexts. However surprisingly, the best performance for event temporal status identification was achieved by a Convolutional Neural Network (CNN) running on local contexts (seven words to each side) surrounding the target event (Huang et al., 2016). Considering the following sentence with a future event “protest”: (1) Climate activists from around the world will launch a hunger strike"
I17-2040,D14-1181,0,0.00758563,"Missing"
I17-2040,N16-1082,0,0.0256255,"Missing"
I17-2040,P04-1043,0,0.0157069,"l network architectures (e.g., tree-structured LSTMs) corresponding to the parse tree structure. In contrast, our method aims at extracting appropriate event-centered data representations from dependency trees so that the neural net models can effectively concentrate on relevant regions of contexts. Similar to our dependency chains, dependency paths between two nodes in a dependency tree have been widely used as features for various NLP tasks and applications, including relation extraction (Bunescu and Mooney, 2005), temporal relation identification (Choubey and Huang, 2017) semantic parsing (Moschitti, 2004) and question 1 We used the Stanford CoreNLP to generate dependency parse trees. 2 http://nlp.stanford.edu/software/ dependencies_manual.pdf 235 Model PA CNN (Huang et al., 2016) LSTM CNN LSTM tree-LSTM OG FU Local Contexts 91/83/87 46/57/51 49/67/57 88/83/85 47/54/51 52/62/57 Dependency Chains 91/84/87 49/63/55 60/65/62 92/85/88 49/63/55 63/71/67 Full Dependency Trees 92/80/86 47/59/53 30/58/40 Macro Micro 62/69/65 63/66/64 77/77/76.9 75/75/75.5 67/71/68 68/73/70 79/79/78.6 80/80/79.6 56/66/60 75/75/75.1 Table 1: Classification results on the test set. Each cell shows Recall/Precision/F1 scor"
I17-2040,W15-4002,0,0.0129004,"n2 , aux, auxpass or cop, with a word that is already in the chain after the first stage, then we include this word in the chain as well. For the example (1), the word “will” is inserted into the dependency chain in the second stage. The reason we perform this additional step is that context words identified with one of the above three dependency relations usually indicate Related Work Constituency-based and dependency-based parse trees have been explored and applied to improve performance of neural nets for the task of sentiment analysis and semantic relation extraction (Socher et al., 2013; Bowman and Potts, 2015; Tai et al., 2015). The focus of these prior studies is on designing new neural network architectures (e.g., tree-structured LSTMs) corresponding to the parse tree structure. In contrast, our method aims at extracting appropriate event-centered data representations from dependency trees so that the neural net models can effectively concentrate on relevant regions of contexts. Similar to our dependency chains, dependency paths between two nodes in a dependency tree have been widely used as features for various NLP tasks and applications, including relation extraction (Bunescu and Mooney, 2005)"
I17-2040,H05-1091,0,0.0723492,"3; Bowman and Potts, 2015; Tai et al., 2015). The focus of these prior studies is on designing new neural network architectures (e.g., tree-structured LSTMs) corresponding to the parse tree structure. In contrast, our method aims at extracting appropriate event-centered data representations from dependency trees so that the neural net models can effectively concentrate on relevant regions of contexts. Similar to our dependency chains, dependency paths between two nodes in a dependency tree have been widely used as features for various NLP tasks and applications, including relation extraction (Bunescu and Mooney, 2005), temporal relation identification (Choubey and Huang, 2017) semantic parsing (Moschitti, 2004) and question 1 We used the Stanford CoreNLP to generate dependency parse trees. 2 http://nlp.stanford.edu/software/ dependencies_manual.pdf 235 Model PA CNN (Huang et al., 2016) LSTM CNN LSTM tree-LSTM OG FU Local Contexts 91/83/87 46/57/51 49/67/57 88/83/85 47/54/51 52/62/57 Dependency Chains 91/84/87 49/63/55 60/65/62 92/85/88 49/63/55 63/71/67 Full Dependency Trees 92/80/86 47/59/53 30/58/40 Macro Micro 62/69/65 63/66/64 77/77/76.9 75/75/75.5 67/71/68 68/73/70 79/79/78.6 80/80/79.6 56/66/60 75/75"
I17-2040,D13-1170,0,0.00273195,"ar dependency relation2 , aux, auxpass or cop, with a word that is already in the chain after the first stage, then we include this word in the chain as well. For the example (1), the word “will” is inserted into the dependency chain in the second stage. The reason we perform this additional step is that context words identified with one of the above three dependency relations usually indicate Related Work Constituency-based and dependency-based parse trees have been explored and applied to improve performance of neural nets for the task of sentiment analysis and semantic relation extraction (Socher et al., 2013; Bowman and Potts, 2015; Tai et al., 2015). The focus of these prior studies is on designing new neural network architectures (e.g., tree-structured LSTMs) corresponding to the parse tree structure. In contrast, our method aims at extracting appropriate event-centered data representations from dependency trees so that the neural net models can effectively concentrate on relevant regions of contexts. Similar to our dependency chains, dependency paths between two nodes in a dependency tree have been widely used as features for various NLP tasks and applications, including relation extraction (B"
I17-2040,P15-1150,0,0.0819908,"Missing"
I17-2040,D16-1058,0,0.0148301,"Huang et al., 2016) with a filter size of 5. For LSTMs, we implemented a simple architecture that consists of one LSTM layer and one output layer with softmax function. For tree-LSTMs, we replicated the Dependency tree-LSTMs3 from (Tai et al., 2015) and added an output layer on top of it. Both of the two latter neural nets used the same number (300) of hidden units as CNNs. Note that we have also experimented with complex LSTM models, including the ones with multiple layers, with bidirectional inferencing (Schuster and Paliwal, 1997) as well as with attention mechanism (Bahdanau et al., 2015; Wang et al., 2016), however none of these complex models improve the event temporal status prediction performance. Experiments 4.1 The EventStatus Corpus We experiment on the EventStatus corpus (Huang et al., 2016), which contains 4500 English and Spanish news articles about civil unrest events (e.g., protest and march), where each civil unrest event mention has been annotated with three categories, Past (PA), On-Going (OG) and Future (FU), to indicate if the event has concluded, is currently ongoing or has not happened yet. We only use the English portion of the corpus which include 2364 documents because our"
N13-1005,P11-1040,0,0.114104,"Missing"
N13-1005,D12-1091,0,0.0250646,"Missing"
N13-1005,P06-1061,0,0.305779,"Missing"
N13-1005,P11-1114,1,0.908034,"Missing"
N13-1005,de-marneffe-etal-2006-generating,0,0.0185558,"that describe the reason for a civil unrest event. We identify probable event sentences by extracting all sentences that contain at least one agent term and one purpose phrase. Agents Purpose Phrases protesters, activists, demonstrators, students, groups, crowd, workers, palestinians, supporters, women demanding, to demand, protesting, to protest Table 1: Agent and Purpose Phrases Used for Seeding 3.1.2 Harvesting Event Expressions To constrain the learning process, we require event expressions and purpose phrases to match certain syntactic structures. We apply the Stanford dependency parser (Marneffe et al., 2006) to the probable event sentences to identify verb phrase candidates and to enforce syntactic constraints between the different types of event information. “xcomp” links “took to the streets” with “protesting higher fuel prices”. Figure 3: Syntactic Dependencies between Agents, Event Phrases, and Purpose Phrases Given the syntactic construction shown in Figure 3, with a known agent and purpose phrase, we extract the head verb phrase of the “xcomp” dependency relation as an event phrase candidate. The event phrases that co-occur with at least two unique agent terms and two unique purposes phrase"
N13-1005,N12-1083,0,0.0608543,"Missing"
N13-1005,D07-1075,1,0.894994,"Missing"
N13-1005,N12-1034,0,0.0275632,"1: Bootstrapped Learning of Event Dictionaries 2002) which addresses event-based organization of a stream of news stories. Event recognition is similar to New Event Detection, also called First Story Detection, which is considered the most difficult TDT task (Allan et al., 2000a). Typical approaches reduce documents to a set of features, either as a word vector (Allan et al., 2000b) or a probability distribution (Jin et al., 1999), and compare the incoming stories to stories that appeared in the past by computing similarities between their feature representations. Recently, event paraphrases (Petrovic et al., 2012) have been explored to deal with the diversity of event descriptions. However, the New Event Detection task differs from our event recognition task because we want to find all stories describing a certain type of event, not just new events. 3 Bootstrapped Learning of Event Dictionaries Our bootstrapping approach consists of two stages of learning as shown in Figure 1. The process begins with a few agent seeds, purpose phrase patterns, and unannotated articles selected from a broadcoverage corpus using event keywords. In the first stage, event expressions are harvested from the sentences that h"
N13-1005,P06-2094,0,0.0659968,"Missing"
N13-1005,P05-1047,0,0.186793,"Missing"
N13-1005,P03-1029,0,0.145943,"Missing"
N13-1005,C00-2136,0,0.372244,"Missing"
N18-1013,C12-1163,0,0.0221555,"ng: The PDTB dataset documents its annotations as a list of discourse relations, with each relation associated with its two discourse units. To recover the paragraph context for a discourse relation, we match contents of its two annotated discourse units with all paragraphs in corresponding raw WSJ article. When all the matching was completed, each paragraph was split into a sequence of discourse units, with one discourse relation (implicit or explicit) between each two adFine-tune Discourse Relation Predictions Using a CRF Layer Data analysis and many linguistic studies (Pitler et al., 2008; Asr and Demberg, 2012; Lascarides and Asher, 1993; Hobbs, 1985) have repeatedly shown that discourse relations feature continuity and patterns (e.g., a temporal relation is likely to be followed by another temporal relation). Especially, Pitler et al. (2008) firstly reported that patterns exist between implicit discourse relations and their neighboring explicit discourse relations. Motivated by these observations, we aim to improve implicit discourse relation detection by making use of easily identifiable explicit discourse relations and taking into account global patterns of discourse relation distributions. Spec"
N18-1013,P16-1163,0,0.454261,"llenging task of implicit discourse relation classification when no explicit discourse connective phrase was presented. Early studies (Pitler et al., 2008; Lin et al., 2009, 2014; Rutherford and Xue, 2015) focused on extracting linguistic and semantic features from two discourse units. Recent research (Zhang et al., 2015; Rutherford et al., 2016; Ji and Eisenstein, 2015; Ji et al., 2016) tried to model compositional meanings of two discourse units by exploiting interactions between words in two units with more and more complicated neural network models, including the ones using neural tensor (Chen et al., 2016; Qin et al., 2016; Lei et al., 2017) and attention mechanisms (Liu and Li, 2016; Lan et al., 2017; Zhou et al., 2016). Another trend is to alleviate the shortage of annotated data by leveraging related external data, such as explicit discourse relations in PDTB (Liu et al., 2016; Lan et al., 2017; Qin et al., 2017) and unlabeled data obtained elsewhere (Rutherford and Xue, 2015; Lan et al., 2017), often in a multi-task joint learning framework. However, nearly all the previous works assume that a pair of discourse units is independent from its wider paragraph-level contexts and build their di"
N18-1013,D17-1070,0,0.0470232,"Schmidhuber, 1997) models, have been widely used to encode a paragraph for machine translation (Sutskever et al., 2014), dialogue systems (Serban et al., 2016) and text summarization (Nallapati et al., 2016) because of its ability in modeling long-distance dependencies between words. In addition, among four typical pooling methods (sum, mean, last and max) for calculating sentence representations from RNN-encoded hidden states for individual words, max-pooling along with bidirectional LSTM (Bi-LSTM) (Schuster and Paliwal, 1997) yields the current best universal sentence representation method (Conneau et al., 2017). We adopted a similar neural network architecture for paragraph encoding. 3 model is a sequence of word vectors, one vector per word in the paragraph. In this work, we used the pre-trained 300-dimension Google English word2vec embeddings2 . For each word that is not in the vocabulary of Google word2vec, we will randomly initialize a vector with each dimension sampled from the range [−0.25, 0.25]. In addition, recognizing key entities and discourse connective phrases is important for discourse relation recognition, therefore, we concatenate the raw word embeddings with extra linguistic feature"
N18-1013,D16-1035,0,0.0531312,"17), often in a multi-task joint learning framework. However, nearly all the previous works assume that a pair of discourse units is independent from its wider paragraph-level contexts and build their discourse relation prediction models based on only two relevant discourse units. In contrast, we model inter-dependencies of discourse units in a paragraph when building discourse unit representations; in addition, we model global continuity and patterns in a sequence of discourse relations, including both implicit and explicit relations. Hierarchical neural network models (Liu and Lapata, 2017; Li et al., 2016) have been applied to RST-style discourse parsing (Carlson et al., 2003) mainly for the purpose of generating text-level hierarchical discourse structures. In contrast, we use hierarchical neural network models to build context-aware sentence representations in order to improve implicit discourse relation prediction. ground information (the history of per-share price) introduced in DU1 and DU2. Second, a DU may be involved in multiple discourse relations (e.g., DU4 is connected with both DU3 and DU5 with a “Comparison” relation), therefore the pragmatic meaning representation of a DU should re"
N18-1013,D09-1036,0,0.163389,"Missing"
N18-1013,D17-1133,0,0.0412978,", 2015; Lan et al., 2017), often in a multi-task joint learning framework. However, nearly all the previous works assume that a pair of discourse units is independent from its wider paragraph-level contexts and build their discourse relation prediction models based on only two relevant discourse units. In contrast, we model inter-dependencies of discourse units in a paragraph when building discourse unit representations; in addition, we model global continuity and patterns in a sequence of discourse relations, including both implicit and explicit relations. Hierarchical neural network models (Liu and Lapata, 2017; Li et al., 2016) have been applied to RST-style discourse parsing (Carlson et al., 2003) mainly for the purpose of generating text-level hierarchical discourse structures. In contrast, we use hierarchical neural network models to build context-aware sentence representations in order to improve implicit discourse relation prediction. ground information (the history of per-share price) introduced in DU1 and DU2. Second, a DU may be involved in multiple discourse relations (e.g., DU4 is connected with both DU3 and DU5 with a “Comparison” relation), therefore the pragmatic meaning representation"
N18-1013,Q15-1024,0,0.288758,"8, pages 141–151 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics et al., 2009; Liu et al., 2016; Rutherford and Xue, 2016) have been conducted for predicting discourse relations, primarily focusing on the challenging task of implicit discourse relation classification when no explicit discourse connective phrase was presented. Early studies (Pitler et al., 2008; Lin et al., 2009, 2014; Rutherford and Xue, 2015) focused on extracting linguistic and semantic features from two discourse units. Recent research (Zhang et al., 2015; Rutherford et al., 2016; Ji and Eisenstein, 2015; Ji et al., 2016) tried to model compositional meanings of two discourse units by exploiting interactions between words in two units with more and more complicated neural network models, including the ones using neural tensor (Chen et al., 2016; Qin et al., 2016; Lei et al., 2017) and attention mechanisms (Liu and Li, 2016; Lan et al., 2017; Zhou et al., 2016). Another trend is to alleviate the shortage of annotated data by leveraging related external data, such as explicit discourse relations in PDTB (Liu et al., 2016; Lan et al., 2017; Qin et al., 2017) and unlabeled data obtained elsewhere"
N18-1013,D16-1130,0,0.713071,"ourse connective phrase was presented. Early studies (Pitler et al., 2008; Lin et al., 2009, 2014; Rutherford and Xue, 2015) focused on extracting linguistic and semantic features from two discourse units. Recent research (Zhang et al., 2015; Rutherford et al., 2016; Ji and Eisenstein, 2015; Ji et al., 2016) tried to model compositional meanings of two discourse units by exploiting interactions between words in two units with more and more complicated neural network models, including the ones using neural tensor (Chen et al., 2016; Qin et al., 2016; Lei et al., 2017) and attention mechanisms (Liu and Li, 2016; Lan et al., 2017; Zhou et al., 2016). Another trend is to alleviate the shortage of annotated data by leveraging related external data, such as explicit discourse relations in PDTB (Liu et al., 2016; Lan et al., 2017; Qin et al., 2017) and unlabeled data obtained elsewhere (Rutherford and Xue, 2015; Lan et al., 2017), often in a multi-task joint learning framework. However, nearly all the previous works assume that a pair of discourse units is independent from its wider paragraph-level contexts and build their discourse relation prediction models based on only two relevant discourse units. I"
N18-1013,N16-1037,0,0.0540123,"leans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics et al., 2009; Liu et al., 2016; Rutherford and Xue, 2016) have been conducted for predicting discourse relations, primarily focusing on the challenging task of implicit discourse relation classification when no explicit discourse connective phrase was presented. Early studies (Pitler et al., 2008; Lin et al., 2009, 2014; Rutherford and Xue, 2015) focused on extracting linguistic and semantic features from two discourse units. Recent research (Zhang et al., 2015; Rutherford et al., 2016; Ji and Eisenstein, 2015; Ji et al., 2016) tried to model compositional meanings of two discourse units by exploiting interactions between words in two units with more and more complicated neural network models, including the ones using neural tensor (Chen et al., 2016; Qin et al., 2016; Lei et al., 2017) and attention mechanisms (Liu and Li, 2016; Lan et al., 2017; Zhou et al., 2016). Another trend is to alleviate the shortage of annotated data by leveraging related external data, such as explicit discourse relations in PDTB (Liu et al., 2016; Lan et al., 2017; Qin et al., 2017) and unlabeled data obtained elsewhere (Rutherford and X"
N18-1013,D17-1134,0,0.732524,"hrase was presented. Early studies (Pitler et al., 2008; Lin et al., 2009, 2014; Rutherford and Xue, 2015) focused on extracting linguistic and semantic features from two discourse units. Recent research (Zhang et al., 2015; Rutherford et al., 2016; Ji and Eisenstein, 2015; Ji et al., 2016) tried to model compositional meanings of two discourse units by exploiting interactions between words in two units with more and more complicated neural network models, including the ones using neural tensor (Chen et al., 2016; Qin et al., 2016; Lei et al., 2017) and attention mechanisms (Liu and Li, 2016; Lan et al., 2017; Zhou et al., 2016). Another trend is to alleviate the shortage of annotated data by leveraging related external data, such as explicit discourse relations in PDTB (Liu et al., 2016; Lan et al., 2017; Qin et al., 2017) and unlabeled data obtained elsewhere (Rutherford and Xue, 2015; Lan et al., 2017), often in a multi-task joint learning framework. However, nearly all the previous works assume that a pair of discourse units is independent from its wider paragraph-level contexts and build their discourse relation prediction models based on only two relevant discourse units. In contrast, we mod"
N18-1013,K16-1028,0,0.0661959,"Missing"
N18-1013,P09-1077,0,0.492956,"relation prediction model achieves improved performance on PDTB for both implicit discourse relation classification and explicit discourse relation classification. 2 2.1 2.2 Paragraph Encoding Abstracting latent representations from a long sequence of words, such as a paragraph, is a challenging task. While several novel neural network models (Zhang et al., 2017b,a) have been introduced in recent years for encoding a paragraph, Recurrent Neural Network (RNN)-based Related Work Implicit Discourse Relation Recognition Since the PDTB (Prasad et al., 2008b) corpus was created, a surge of studies (Pitler et al., 2009; Lin 142 methods remain the most effective approaches. RNNs, especially the long-short term memory (LSTM) (Hochreiter and Schmidhuber, 1997) models, have been widely used to encode a paragraph for machine translation (Sutskever et al., 2014), dialogue systems (Serban et al., 2016) and text summarization (Nallapati et al., 2016) because of its ability in modeling long-distance dependencies between words. In addition, among four typical pooling methods (sum, mean, last and max) for calculating sentence representations from RNN-encoded hidden states for individual words, max-pooling along with b"
N18-1013,P09-2004,0,0.0864491,"ction Layer The Softmax Prediction Layer: Finally, we concatenate two adjacent discourse unit representations hDUt−1 and hDUt and predict the discourse relation between them using a softmax function: vealed that recognizing explicit vs. implicit discourse relations requires different strategies. Note that in the PDTB dataset, explicit discourse relations were distinguished from implicit ones, depending on whether a discourse connective exists between two discourse units. Therefore, explicit discourse relation detection can be simplified as a discourse connective phrase disambiguation problem (Pitler and Nenkova, 2009; Lin et al., 2014). On the contrary, predicting an implicit discourse relation should rely on understanding the overall yt−1 = sof tmax(Wy ∗ [hDUt−1 , hDUt ] + by ) (3) 3.2 Untie Parameters in the Softmax Prediction Layer (Implicit vs. Explicit) Previous work (Pitler and Nenkova, 2009; Lin et al., 2014; Rutherford and Xue, 2016) has re144 contents of its two discourse units (Lin et al., 2014; Rutherford and Xue, 2016). Considering the different natures of explicit vs. implicit discourse relation prediction, we decide to untie parameters at the final discourse relation prediction layer and tra"
N18-1013,D15-1266,0,0.361425,"Chen et al., 141 Proceedings of NAACL-HLT 2018, pages 141–151 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics et al., 2009; Liu et al., 2016; Rutherford and Xue, 2016) have been conducted for predicting discourse relations, primarily focusing on the challenging task of implicit discourse relation classification when no explicit discourse connective phrase was presented. Early studies (Pitler et al., 2008; Lin et al., 2009, 2014; Rutherford and Xue, 2015) focused on extracting linguistic and semantic features from two discourse units. Recent research (Zhang et al., 2015; Rutherford et al., 2016; Ji and Eisenstein, 2015; Ji et al., 2016) tried to model compositional meanings of two discourse units by exploiting interactions between words in two units with more and more complicated neural network models, including the ones using neural tensor (Chen et al., 2016; Qin et al., 2016; Lei et al., 2017) and attention mechanisms (Liu and Li, 2016; Lan et al., 2017; Zhou et al., 2016). Another trend is to alleviate the shortage of annotated data by leveraging related external data, such as explicit discourse relations in PDTB (Liu et al., 2016; Lan et al., 2017; Qin e"
N18-1013,C08-2022,0,0.559299,"al., 2009, 2014; Xue et al., 2015). To fill the gap, implicit discourse relation prediction has drawn significant research interest recently and progress has been made (Chen et al., 141 Proceedings of NAACL-HLT 2018, pages 141–151 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics et al., 2009; Liu et al., 2016; Rutherford and Xue, 2016) have been conducted for predicting discourse relations, primarily focusing on the challenging task of implicit discourse relation classification when no explicit discourse connective phrase was presented. Early studies (Pitler et al., 2008; Lin et al., 2009, 2014; Rutherford and Xue, 2015) focused on extracting linguistic and semantic features from two discourse units. Recent research (Zhang et al., 2015; Rutherford et al., 2016; Ji and Eisenstein, 2015; Ji et al., 2016) tried to model compositional meanings of two discourse units by exploiting interactions between words in two units with more and more complicated neural network models, including the ones using neural tensor (Chen et al., 2016; Qin et al., 2016; Lei et al., 2017) and attention mechanisms (Liu and Li, 2016; Lan et al., 2017; Zhou et al., 2016). Another trend is"
N18-1013,prasad-etal-2008-penn,0,0.88564,"tal results show that the intuitive paragraph-level discourse relation prediction model achieves improved performance on PDTB for both implicit discourse relation classification and explicit discourse relation classification. 2 2.1 2.2 Paragraph Encoding Abstracting latent representations from a long sequence of words, such as a paragraph, is a challenging task. While several novel neural network models (Zhang et al., 2017b,a) have been introduced in recent years for encoding a paragraph, Recurrent Neural Network (RNN)-based Related Work Implicit Discourse Relation Recognition Since the PDTB (Prasad et al., 2008b) corpus was created, a surge of studies (Pitler et al., 2009; Lin 142 methods remain the most effective approaches. RNNs, especially the long-short term memory (LSTM) (Hochreiter and Schmidhuber, 1997) models, have been widely used to encode a paragraph for machine translation (Sutskever et al., 2014), dialogue systems (Serban et al., 2016) and text summarization (Nallapati et al., 2016) because of its ability in modeling long-distance dependencies between words. In addition, among four typical pooling methods (sum, mean, last and max) for calculating sentence representations from RNN-encode"
N18-1013,P16-2034,0,0.038874,"d. Early studies (Pitler et al., 2008; Lin et al., 2009, 2014; Rutherford and Xue, 2015) focused on extracting linguistic and semantic features from two discourse units. Recent research (Zhang et al., 2015; Rutherford et al., 2016; Ji and Eisenstein, 2015; Ji et al., 2016) tried to model compositional meanings of two discourse units by exploiting interactions between words in two units with more and more complicated neural network models, including the ones using neural tensor (Chen et al., 2016; Qin et al., 2016; Lei et al., 2017) and attention mechanisms (Liu and Li, 2016; Lan et al., 2017; Zhou et al., 2016). Another trend is to alleviate the shortage of annotated data by leveraging related external data, such as explicit discourse relations in PDTB (Liu et al., 2016; Lan et al., 2017; Qin et al., 2017) and unlabeled data obtained elsewhere (Rutherford and Xue, 2015; Lan et al., 2017), often in a multi-task joint learning framework. However, nearly all the previous works assume that a pair of discourse units is independent from its wider paragraph-level contexts and build their discourse relation prediction models based on only two relevant discourse units. In contrast, we model inter-dependencie"
N18-1013,D16-1246,0,0.226816,"plicit discourse relation classification when no explicit discourse connective phrase was presented. Early studies (Pitler et al., 2008; Lin et al., 2009, 2014; Rutherford and Xue, 2015) focused on extracting linguistic and semantic features from two discourse units. Recent research (Zhang et al., 2015; Rutherford et al., 2016; Ji and Eisenstein, 2015; Ji et al., 2016) tried to model compositional meanings of two discourse units by exploiting interactions between words in two units with more and more complicated neural network models, including the ones using neural tensor (Chen et al., 2016; Qin et al., 2016; Lei et al., 2017) and attention mechanisms (Liu and Li, 2016; Lan et al., 2017; Zhou et al., 2016). Another trend is to alleviate the shortage of annotated data by leveraging related external data, such as explicit discourse relations in PDTB (Liu et al., 2016; Lan et al., 2017; Qin et al., 2017) and unlabeled data obtained elsewhere (Rutherford and Xue, 2015; Lan et al., 2017), often in a multi-task joint learning framework. However, nearly all the previous works assume that a pair of discourse units is independent from its wider paragraph-level contexts and build their discourse relation p"
N18-1013,P17-1093,0,0.642213,"2015; Rutherford et al., 2016; Ji and Eisenstein, 2015; Ji et al., 2016) tried to model compositional meanings of two discourse units by exploiting interactions between words in two units with more and more complicated neural network models, including the ones using neural tensor (Chen et al., 2016; Qin et al., 2016; Lei et al., 2017) and attention mechanisms (Liu and Li, 2016; Lan et al., 2017; Zhou et al., 2016). Another trend is to alleviate the shortage of annotated data by leveraging related external data, such as explicit discourse relations in PDTB (Liu et al., 2016; Lan et al., 2017; Qin et al., 2017) and unlabeled data obtained elsewhere (Rutherford and Xue, 2015; Lan et al., 2017), often in a multi-task joint learning framework. However, nearly all the previous works assume that a pair of discourse units is independent from its wider paragraph-level contexts and build their discourse relation prediction models based on only two relevant discourse units. In contrast, we model inter-dependencies of discourse units in a paragraph when building discourse unit representations; in addition, we model global continuity and patterns in a sequence of discourse relations, including both implicit an"
N18-1013,N15-1081,0,0.552223,"he gap, implicit discourse relation prediction has drawn significant research interest recently and progress has been made (Chen et al., 141 Proceedings of NAACL-HLT 2018, pages 141–151 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics et al., 2009; Liu et al., 2016; Rutherford and Xue, 2016) have been conducted for predicting discourse relations, primarily focusing on the challenging task of implicit discourse relation classification when no explicit discourse connective phrase was presented. Early studies (Pitler et al., 2008; Lin et al., 2009, 2014; Rutherford and Xue, 2015) focused on extracting linguistic and semantic features from two discourse units. Recent research (Zhang et al., 2015; Rutherford et al., 2016; Ji and Eisenstein, 2015; Ji et al., 2016) tried to model compositional meanings of two discourse units by exploiting interactions between words in two units with more and more complicated neural network models, including the ones using neural tensor (Chen et al., 2016; Qin et al., 2016; Lei et al., 2017) and attention mechanisms (Liu and Li, 2016; Lan et al., 2017; Zhou et al., 2016). Another trend is to alleviate the shortage of annotated data by leve"
N18-1013,K16-2007,0,0.2924,"plicit discourse relations were distinguished from implicit ones, depending on whether a discourse connective exists between two discourse units. Therefore, explicit discourse relation detection can be simplified as a discourse connective phrase disambiguation problem (Pitler and Nenkova, 2009; Lin et al., 2014). On the contrary, predicting an implicit discourse relation should rely on understanding the overall yt−1 = sof tmax(Wy ∗ [hDUt−1 , hDUt ] + by ) (3) 3.2 Untie Parameters in the Softmax Prediction Layer (Implicit vs. Explicit) Previous work (Pitler and Nenkova, 2009; Lin et al., 2014; Rutherford and Xue, 2016) has re144 contents of its two discourse units (Lin et al., 2014; Rutherford and Xue, 2016). Considering the different natures of explicit vs. implicit discourse relation prediction, we decide to untie parameters at the final discourse relation prediction layer and train two softmax classifiers, as illustrated in Figure 2. The two classifiers have different sets of parameters, with one classifier for only implicit discourse relations and the other for only explicit discourse relations. yt−1 = ( sof tmax(Wexp [hDUt−1 , hDUt ] + bexp ), sof tmax(Wimp [hDUt−1 , hDUt ] + bimp ), Figure 3: Fine-tun"
N18-2055,W14-2909,0,0.0392972,"nt that connects other foreground and background events. Both the documents and the gold event mentions for each document inherited from the previous RED and KBP annotations were provided to annotators. The annotators were instructed to select only one event as the central event. For 26 documents from the RED corpus and 71 documents from the KBP Related Work Many previous works studied the parameters that determine the overall quality of an individual event, including actualization (Tasaku, 1981), transitivity (Hopper and Thompson, 1980; Tsunoda, 1985) and the broader concept of eventiveness (Monahan and Brunson, 2014). However, these atomic qualities defined for an individual event are inadequate in distinguishing the key foreground event in a document. In concurrent works, Decker (1985); Kay and Aylett (1996) focused on distinguishing foreground events from background events in a sentence and proposed that the most important event within a sentence is usually the event that appears in the main clause, is active voiced, and has a high transitivity. Upadhyay et al. (2016) applied these rules to identifying the trigger event of a news article by identifying the most important event in a 2 The RED corpus cont"
N18-2055,W16-5706,0,0.0957475,"Missing"
N18-2055,P85-1039,0,0.693549,"e, and thus may not appear in the title or in the first sentence of a new article. As illustrated in this example, the trigger event is “protesters leave capitol”, while Introduction According to the grounding principles (Grimes, 1975), a document consists of foreground events that form the skeleton of the story and move the story forward, and background events that add supportive information. Studies have shown that a foreground event tends to be the most important event in a sentence, which is usually the event that appears in the main clause, is active voiced, and has a high transitivity1 (Decker, 1985). But among multiple foreground events, which one is most central to the overall story? We propose a new task of detecting the most dominant event in a news article, which is an event assumed to govern and connect other foreground events and background events. In other words, removal of the central event can break the entirety of a document and 1 High transitivity events have certain properties, are volitional, affirmative, realis etc. 340 Proceedings of NAACL-HLT 2018, pages 340–345 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics human-generated docu"
N18-2055,D14-1162,0,0.0861171,"ons that embed event lemma and parts-of-speech tags and then calculates cosine similarity, absolute and euclidean distances between two event embeddings. Classifier also includes a neural layer component to embed event arguments that are overlapped between the two event mentions. Its output layer takes the calculated cosine similarity, euclidean and absolute distances between event mention embeddings as well as the embedding of the overlapped event arguments as input, and output a confidence score to indicate the similarity of the two event mentions6 . We used 300 dimensional word embeddings (Pennington et al., 2014) and one hot 377 dimensional pos tag embeddings. In addition, we used (Lewis et al., 2015) for semantic role labeling to obtain event arguments. 6.2 Results We evaluated all the systems using the rest 20 documents from the RED corpus and all the 74 docu8 We build an event graph for a document by using undirected edges for coreference relations and directed edges for other relations including set/ member, sub-event, temporal and causal relations. This is mainly meant to retain the symmetrical property of coreference relations. Moreover, since coreference link can easily create cycles in the gra"
N18-2055,W04-1017,0,0.0877991,"Missing"
N18-2055,C04-1057,0,0.043083,"Missing"
N18-2055,N03-1033,0,0.018993,"set of event mentions that are predecessors and successors to Vi respectively. Also, d is a damping vector that is kept 0.85 in our experiments. We initially assign random values to all the event mentions in an event graph and then update scores for all event nodes using equation 1 after each iteration. Computation stops when the sum of differences between the scores computed for all event mentions at two successive iterations reduces below 0.01. 6 We implemented our classifier using the Keras library (Chollet, 2015) 7 Corresponding to the unique 36 POS tags based on the Stanford POS tagger (Toutanova et al., 2003) and an additional ’padding’. 343 Model Rec Prec Richer Event Description (RED) Main event: Headline 45.0 45.0 First event: First sentence 10.0 10.0 Main event: First sentence 40.0 40.0 Random walk: All Relations 40.0 40.0 Random walk: Coref+SM 45.0 45.0 Coreference 75.0 55.55 Coreference + Subevent 75.0 62.5 Coreference + Subevent + Realis 80.0 66.67 Linear Regression 63.33 63.33 SVR 66.67 66.67 Coreference: Predicted 45.0 45.0 KBP 2015 Main event: Headline 45.94 45.94 First event: First sentence 39.19 39.19 Main event: First sentence 39.19 39.19 Coreference 77.03 54.81 Coreference + Subevent"
N18-2055,W16-1001,0,0.0884331,"luding actualization (Tasaku, 1981), transitivity (Hopper and Thompson, 1980; Tsunoda, 1985) and the broader concept of eventiveness (Monahan and Brunson, 2014). However, these atomic qualities defined for an individual event are inadequate in distinguishing the key foreground event in a document. In concurrent works, Decker (1985); Kay and Aylett (1996) focused on distinguishing foreground events from background events in a sentence and proposed that the most important event within a sentence is usually the event that appears in the main clause, is active voiced, and has a high transitivity. Upadhyay et al. (2016) applied these rules to identifying the trigger event of a news article by identifying the most important event in a 2 The RED corpus contains 95 documents in total. However, 65 documents are news summaries, discussion forum posts or web posts. The central event as defined should only be considered for natural coherent texts, therefore, the annotations were only conducted for the 30 news articles in the corpus. 3 The KBP 2015 corpus contains 158 documents, where 81 are news articles and the remaining are discussion forum posts. Then in 7 out of the 81 news articles, annotators unanimously foun"
N18-2055,P96-1054,0,0.835706,". The annotators were instructed to select only one event as the central event. For 26 documents from the RED corpus and 71 documents from the KBP Related Work Many previous works studied the parameters that determine the overall quality of an individual event, including actualization (Tasaku, 1981), transitivity (Hopper and Thompson, 1980; Tsunoda, 1985) and the broader concept of eventiveness (Monahan and Brunson, 2014). However, these atomic qualities defined for an individual event are inadequate in distinguishing the key foreground event in a document. In concurrent works, Decker (1985); Kay and Aylett (1996) focused on distinguishing foreground events from background events in a sentence and proposed that the most important event within a sentence is usually the event that appears in the main clause, is active voiced, and has a high transitivity. Upadhyay et al. (2016) applied these rules to identifying the trigger event of a news article by identifying the most important event in a 2 The RED corpus contains 95 documents in total. However, 65 documents are news summaries, discussion forum posts or web posts. The central event as defined should only be considered for natural coherent texts, theref"
N18-2055,D15-1169,0,0.0472665,"Missing"
N18-2055,W04-3252,0,0.181101,"has been shown important for text summarization. Filatova and Hatzivassiloglou (2004a,b) used normalized frequencies of co-referential event mentions as parameters to prioritize events to be included in a summary and found that this helped in generating better text summaries, despite its being an elementary measure. Our experiments showed that in addition to the number of co-referential event mentions, discourse layout features including both the stretch of an event chain and early presences of event mentions are key factors in identifying the central event of a document. Graph-based methods (Mihalcea and Tarau, 2004) have been widely used to identify keywords and phrases in a document by constructing a word/ phrase graph and applying random walk algorithms (Brin and Page, 2012) on the graph. We implemented random walk based methods for identifying the central event as well, which however did not perform well. Mainly, the random walk based ranking strategy determines the importance of an events based on the importance of its related events in a document graph, which does not effectively capture discourse layout features of coreferential event mentions, which are important for identifying the central event"
N19-1057,D18-1547,0,0.0221623,"n. Each dialogue has an average of eight turns, where each turn contains system utterance transcript, user utterance transcript, turn label and belief state. All the dialogue states and actions are based on a task ontology that supports three different informable slot-types namely price range with 4 values, food with 72 values, area with 7 values, and requests of 7 different types like address and phone. Following the standard settings, we use 600 dialogues for training, 200 for validation and the remaining 400 for testing. We also use dialogues from restaurant domain in MultiWoZ 2.0 dataset (Budzianowski et al., 2018) for secondary evaluation. It banks on a significantly complex ontology covering seven informable slot types with 276 different values (food, price range, restaurant name, area, book time, book day and book people with 97, 6, 105, 8, 43, 8 and 9 values respectively). We use standard training, validation and test splits of 1199, 50 and 61 dialogues respectively. All the models on WoZ 2.0 are evaluated on the two standard metrics introduced in Henderson et al. (2014a). First, Joint Goal Accuracy is the percentage of turns in a dialogue where the user’s informed joint goals are identified correct"
N19-1057,W17-5526,0,0.0677081,"Missing"
N19-1057,P17-1163,0,0.175161,"Missing"
N19-1057,W13-4073,0,0.0248423,". As shown through an example in Figure 1, while exploring different available options, user 576 Proceedings of NAACL-HLT 2019, pages 576–581 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics generic tags to replace specific slot types and values, and handcrafted semantic dictionaries. In practice, it is difficult to scale these models for every slot type and recent state-of-the-art models for DST use deep learning based methods to learn general representations for user and system utterances and previous system actions, and predict the turn state (Henderson et al., 2013, 2014b; Mrkˇsi´c et al., 2015, 2017; Hori et al., 2016; Liu and Lane, 2017; Dernoncourt et al., 2017; Chen et al., 2016). However, these systems are found to perform poorly on rare and unknown slot-value pairs which was recently addressed through local slot-specific encoders (Zhong et al., 2018) and pointer network (Xu and Hu, 2018). A crucial limitation to all these approaches lies in the modeling of appropriate historical context, which is simply ignored in most of the works. Since user’s goal may change back-and-forth between previous values, incorporating relevant historical context is us"
N19-1057,D14-1162,0,0.0855815,"g layer. The models are trained for a maximum of 100 epochs with a batch size of 50. The validation data was used for early stopping and hyperparameter tuning. 4.3 Results Table 1 compares the performance of our proposed models with different baselines, including delexalisation-based model + SD (Wen et al., 2017), DNN and CNN variants of neural belief tracker (Mrkˇsi´c et al., 2017) and the previous state-of-the-art GLAD systems (Zhong et al., 2018) on WoZ 2.0 dataset. We also implement a simplified variant of GLAD, Global BiLSTM Implementation Details We use pretrained GloVe word embeddings (Pennington et al., 2014) concatenated with charac579 Model Global biLSTM based GLE Global biLSTM based GLE + RC + FS GLAD GLAD + RC + FS parameters for encoding the antecedent referential user utterance and the previous system utterance as well as the past utterance and past slot-value scorers. However, we also observe high variance in the joint goal accuracy. Since joint goal is calculated by accumulating turn goals, an error in predicting a turn goal is propagated to all the downstream turns. Approx. # of parameters 1.2 million 6 million 17 million 28 million Table 3: Number of learnable parameters for different mo"
N19-1057,W14-4340,0,0.539297,"he previous turn, as well as previous system actions as evidence. Shown in Figure 2, our model comprises of: Related Work Early work for DST relied on separate Spoken Language Understanding (SLU) module (Henderson et al., 2012) to extract relevant information from user utterances in a pipelined approach. Such systems are prone to error accumulation from a separate SLU module, in absence of necessary dialog context required to interpret the user utterance. Thus, later work on DST moved away from separate SLU modules and inferred the dialog state directly from user utterance and dialog history (Henderson et al., 2014b,c; Zilka and Jurcicek, 2015). These models depend on delexicalization, using Lookup module: retrieves a link to the turn where each of the slots changes. At each step, our system refers to the lookup module that returns the past user utterance (the “antecedent user utterance”) 577 Referential Context Scorer Slot-value Past Utt Encoder Lookup Slot-value Encoder Past Utt Scorer Past slot-value Scorer Sigmoid Weighted Sum Sys Utt Encoder Fusion Scorer User Utt Encoder System Act Encoder System Act Scorer Figure 2: The Architecture of Context Aware Dialogue State Tracker. ent from the previous s"
N19-1057,W17-2626,0,0.0339574,"Missing"
N19-1057,E17-1042,0,0.127735,"Missing"
N19-1057,P18-1134,0,0.107126,"ice, it is difficult to scale these models for every slot type and recent state-of-the-art models for DST use deep learning based methods to learn general representations for user and system utterances and previous system actions, and predict the turn state (Henderson et al., 2013, 2014b; Mrkˇsi´c et al., 2015, 2017; Hori et al., 2016; Liu and Lane, 2017; Dernoncourt et al., 2017; Chen et al., 2016). However, these systems are found to perform poorly on rare and unknown slot-value pairs which was recently addressed through local slot-specific encoders (Zhong et al., 2018) and pointer network (Xu and Hu, 2018). A crucial limitation to all these approaches lies in the modeling of appropriate historical context, which is simply ignored in most of the works. Since user’s goal may change back-and-forth between previous values, incorporating relevant historical context is useful in monitoring implicit goal references. In a recent work, El Asri et al. (2017) discussed on similar limitations of current DST task and introduced a new task of frame tracking that explicitly tracks every slot-values that were introduced during the dialogue. However, that significantly complicates the task by maintaining multip"
N19-1057,P18-1135,0,0.521201,"ndcrafted semantic dictionaries. In practice, it is difficult to scale these models for every slot type and recent state-of-the-art models for DST use deep learning based methods to learn general representations for user and system utterances and previous system actions, and predict the turn state (Henderson et al., 2013, 2014b; Mrkˇsi´c et al., 2015, 2017; Hori et al., 2016; Liu and Lane, 2017; Dernoncourt et al., 2017; Chen et al., 2016). However, these systems are found to perform poorly on rare and unknown slot-value pairs which was recently addressed through local slot-specific encoders (Zhong et al., 2018) and pointer network (Xu and Hu, 2018). A crucial limitation to all these approaches lies in the modeling of appropriate historical context, which is simply ignored in most of the works. Since user’s goal may change back-and-forth between previous values, incorporating relevant historical context is useful in monitoring implicit goal references. In a recent work, El Asri et al. (2017) discussed on similar limitations of current DST task and introduced a new task of frame tracking that explicitly tracks every slot-values that were introduced during the dialogue. However, that significantly comp"
N19-1057,P15-2130,0,0.13185,"Missing"
N19-1179,W17-2711,0,0.280942,"events in the two text units. Third, we model interactions between event causal relations and event coreference relations. For example, coreferent event mentions should have the same causal relations; a causal relation and an identity relation should not co-exist between any two events. We use Integer Linear Programming (ILP) to model these rich causal structures within a document by designing constraints and modifying the objective function to encourage causal relations akin to the observed causal structures and discourage the opposite. Our experimental results on the dataset EventStoryLine (Caselli and Vossen, 2017) show that modeling the global and fine-grained aspects of causal structures within a document greatly improves the performance of causal relation identification, especially in identifying crosssentence causal relations. 2 Related Work In the last decade or so, both unsupervised and supervised causal relation identification approaches have been proposed including linguistic patterns, statistical measures and supervised classifiers, primarily with the goal of acquiring event causality knowledge from a text corpus. The proposed approaches mainly rely on explicit contextual patterns (Girju; Hashi"
N19-1179,P17-2001,0,0.26874,"the point-wise mutual information (PMI) score and the relative textual order between two events. We calculate the PMI score of two event words in EventStoryLine by using co-occurrences of two events in one sentence, and we use the score as a numerical feature. Syntactic Features: We use dependency relations on the dependency path between two events. We use the basic dependencies extracted from StanfordCoreNLP (Manning et al., 2014). For crosssentence event pairs, we consider the dependency path from each event to the root node in its own sentence in extracting dependency relations, following Cheng and Miyao (2017). In addition, we use Part Of Speech tags of two event head words as features. 4.2 Score Replacement We observed that the cross-sentence causal relation classifier is usually not as capable as the intrasentence classifier, probably due to less contextual evidence to rely on. Therefore, for crosssentence event mention pairs that can be converted 4 Specifically, we consider ’nmod’, ’amod’, ’advmod’, ’mark’, ’aux’, ’auxpass’, ’expl’, ’cc’, ’cop’, ’punct’ to be modifiers. to intra-sentence cases through event coreference links, we use a heuristic method to improve causal relation prediction perfor"
N19-1179,D17-1226,1,0.898481,"Missing"
N19-1179,D17-1190,1,0.441775,"Missing"
N19-1179,N18-2055,1,0.814255,"se relations (ΘD ), and event coreference 5 Note that we only conduct the score replacement when a score produced by the intra-sentence classifier is higher than the score produced by the cross-sentence classifier, which indicates that the intra-sentence classifier is more confident. 1811 relations(ΘC ) as well as syntactic structure constraints (ΘS ) for identifying causal relations. Θ = ΘBasic + ΘM + ΘF + ΘD + ΘC + ΘS 5.1 (2) Document Level Main Event Based Constraints Main Event: Main events are central to the story in a document and tend to participate in multiple causal links. Similar to Choubey et al. (2018), we recognize main events based on characteristics of event coreference chains within a document. Specifically, we rank events based on the number of event mentions referring to an event, and choose the top two events as main events6 . Then we add a new objective function (equation 3) and additional constraints to encourage causal links in event mention pairs containing a main event (equation 4) and discourage causal links in the remaining mention pairs (equation 5). ΘM = max hX [km1 m1 (i) + km2 m2 (i)] i∈Λ − X i [kn1 n1 (i) + kn2 n2 (i)] (3) i∈µ−Λ X ∀i ∈ Λ, xij ≥ m1 (i) j∈µ,di =dj X ∀i ∈ Λ,"
N19-1179,D11-1027,0,0.818234,"structures within a document greatly improves the performance of causal relation identification, especially in identifying crosssentence causal relations. 2 Related Work In the last decade or so, both unsupervised and supervised causal relation identification approaches have been proposed including linguistic patterns, statistical measures and supervised classifiers, primarily with the goal of acquiring event causality knowledge from a text corpus. The proposed approaches mainly rely on explicit contextual patterns (Girju; Hashimoto et al., 2014) or other causality cues (Riaz and Girju, 2010; Do et al., 2011), statistical associations between events (Beamer and Girju, 2009; Hu et al., 2017; Hu and Walker, 2017; Do et al., 2011; Hashimoto et al., 2014), and lexical semantics of events (Riaz and Girju, 2013, 2014b,a; Hashimoto et al., 2014). An increasing amount of recent works focused on recognizing event causal relations within a document, but mostly limited to identifying intrasentence causal relations with explicit causal indicators. Mirza et al. (2014) annotated event causal relations in the TempEval-3 corpus and created CausalTimeBank. Mirza and Tonelli (2014) stated that incorporating tempora"
N19-1179,D12-1062,0,0.0304055,"n identification. Caselli and Vossen (2017) showed that only 117 annotated causal relations in this dataset are indicated by explicit causal cue phrases while the others are implicit. We conduct experiments on the EventStoryLine dataset. Distinguished from most of the previous approaches that identify one causal relation each time, we model coarsegrained and fine-grained document-level event causal structures and infer all the causal relations in a document. Integer linear programming (ILP) approaches have been applied to predict a set of temporal relations or an event timeline in a document (Do et al., 2012; Teng et al., 2016; Ning et al., 2017). ILP has been used to improve causal relation identification (Do et al., 2011), but only with finegrained constraints considering discourse relations between two text units. Our approach innovates on modeling other aspects of document-level causal structures, especially heavy involvements of main events in causal relations, that facilitate resolving multiple causal relations. 1809 3 The EventStoryLine Corpus Table 1 shows the statistics of the corpus EventStoryLine v0.91 (Caselli and Vossen, 2017). Item Topics Documents Sentences Event Mentions Intra-sen"
N19-1179,W03-1210,0,0.725494,"Missing"
N19-1179,P14-1093,0,0.391184,"2017) show that modeling the global and fine-grained aspects of causal structures within a document greatly improves the performance of causal relation identification, especially in identifying crosssentence causal relations. 2 Related Work In the last decade or so, both unsupervised and supervised causal relation identification approaches have been proposed including linguistic patterns, statistical measures and supervised classifiers, primarily with the goal of acquiring event causality knowledge from a text corpus. The proposed approaches mainly rely on explicit contextual patterns (Girju; Hashimoto et al., 2014) or other causality cues (Riaz and Girju, 2010; Do et al., 2011), statistical associations between events (Beamer and Girju, 2009; Hu et al., 2017; Hu and Walker, 2017; Do et al., 2011; Hashimoto et al., 2014), and lexical semantics of events (Riaz and Girju, 2013, 2014b,a; Hashimoto et al., 2014). An increasing amount of recent works focused on recognizing event causal relations within a document, but mostly limited to identifying intrasentence causal relations with explicit causal indicators. Mirza et al. (2014) annotated event causal relations in the TempEval-3 corpus and created CausalTime"
N19-1179,W17-2708,0,0.134662,"Missing"
N19-1179,W17-5540,0,0.132678,"ecially in identifying crosssentence causal relations. 2 Related Work In the last decade or so, both unsupervised and supervised causal relation identification approaches have been proposed including linguistic patterns, statistical measures and supervised classifiers, primarily with the goal of acquiring event causality knowledge from a text corpus. The proposed approaches mainly rely on explicit contextual patterns (Girju; Hashimoto et al., 2014) or other causality cues (Riaz and Girju, 2010; Do et al., 2011), statistical associations between events (Beamer and Girju, 2009; Hu et al., 2017; Hu and Walker, 2017; Do et al., 2011; Hashimoto et al., 2014), and lexical semantics of events (Riaz and Girju, 2013, 2014b,a; Hashimoto et al., 2014). An increasing amount of recent works focused on recognizing event causal relations within a document, but mostly limited to identifying intrasentence causal relations with explicit causal indicators. Mirza et al. (2014) annotated event causal relations in the TempEval-3 corpus and created CausalTimeBank. Mirza and Tonelli (2014) stated that incorporating temporal information improved the performance of a causal relation classifier. Mirza and Tonelli (2016) built"
N19-1179,D17-1108,0,0.0256569,"(2017) showed that only 117 annotated causal relations in this dataset are indicated by explicit causal cue phrases while the others are implicit. We conduct experiments on the EventStoryLine dataset. Distinguished from most of the previous approaches that identify one causal relation each time, we model coarsegrained and fine-grained document-level event causal structures and infer all the causal relations in a document. Integer linear programming (ILP) approaches have been applied to predict a set of temporal relations or an event timeline in a document (Do et al., 2012; Teng et al., 2016; Ning et al., 2017). ILP has been used to improve causal relation identification (Do et al., 2011), but only with finegrained constraints considering discourse relations between two text units. Our approach innovates on modeling other aspects of document-level causal structures, especially heavy involvements of main events in causal relations, that facilitate resolving multiple causal relations. 1809 3 The EventStoryLine Corpus Table 1 shows the statistics of the corpus EventStoryLine v0.91 (Caselli and Vossen, 2017). Item Topics Documents Sentences Event Mentions Intra-sentence causal links Cross-sentence causa"
N19-1179,D14-1162,0,0.0953333,"larities Based on Word Embeddings. We apply l2 normalization on event head word embeddings, and then we calculate the Euclidean distance and Cosine distance between Intra- and cross-sentence causal relations are different by nature. For instance, dependency rela1 Statistics are calculated based on latest release https://github.com/tommasoc80/EventStoryLine 2 639 event mentions were excluded in this way. The Common Feature Set 3 http://scikit-learn.org/stable/modules/generated/ sklearn.linear model.LogisticRegression.html 1810 two word embeddings and use them as features. We use Glove Vectors (Pennington et al., 2014) for word embeddings. • Similarities Based on Event Modifiers. We run the dependency parsing tool from the Stanford CoreNLP (Manning et al., 2014) and identify event modifiers as words that have a certain dependency relation4 with an event head word. We measure the similarity between two events using the number of common modifiers and the number of common dependency relations that connect a modifier with an event head word. • Similarities Based on Event Arguments. We consider entities that have a direct dependency relation with an event head word as its event arguments. We use the Stanford Cor"
N19-1179,W13-4004,0,0.16532,"both unsupervised and supervised causal relation identification approaches have been proposed including linguistic patterns, statistical measures and supervised classifiers, primarily with the goal of acquiring event causality knowledge from a text corpus. The proposed approaches mainly rely on explicit contextual patterns (Girju; Hashimoto et al., 2014) or other causality cues (Riaz and Girju, 2010; Do et al., 2011), statistical associations between events (Beamer and Girju, 2009; Hu et al., 2017; Hu and Walker, 2017; Do et al., 2011; Hashimoto et al., 2014), and lexical semantics of events (Riaz and Girju, 2013, 2014b,a; Hashimoto et al., 2014). An increasing amount of recent works focused on recognizing event causal relations within a document, but mostly limited to identifying intrasentence causal relations with explicit causal indicators. Mirza et al. (2014) annotated event causal relations in the TempEval-3 corpus and created CausalTimeBank. Mirza and Tonelli (2014) stated that incorporating temporal information improved the performance of a causal relation classifier. Mirza and Tonelli (2016) built both a rule-based multi-sieve approach and a feature based classifier to recognize causal relatio"
N19-1179,W14-4322,0,0.385049,"Missing"
N19-1179,W14-0707,0,0.594268,"Missing"
N19-1179,P14-5010,0,0.00308341,"istance between Intra- and cross-sentence causal relations are different by nature. For instance, dependency rela1 Statistics are calculated based on latest release https://github.com/tommasoc80/EventStoryLine 2 639 event mentions were excluded in this way. The Common Feature Set 3 http://scikit-learn.org/stable/modules/generated/ sklearn.linear model.LogisticRegression.html 1810 two word embeddings and use them as features. We use Glove Vectors (Pennington et al., 2014) for word embeddings. • Similarities Based on Event Modifiers. We run the dependency parsing tool from the Stanford CoreNLP (Manning et al., 2014) and identify event modifiers as words that have a certain dependency relation4 with an event head word. We measure the similarity between two events using the number of common modifiers and the number of common dependency relations that connect a modifier with an event head word. • Similarities Based on Event Arguments. We consider entities that have a direct dependency relation with an event head word as its event arguments. We use the Stanford CoreNLP to identify entities and their types. We measure the similarity between two events using the number of common event arguments and the number"
N19-1179,W14-0702,0,0.182649,"The proposed approaches mainly rely on explicit contextual patterns (Girju; Hashimoto et al., 2014) or other causality cues (Riaz and Girju, 2010; Do et al., 2011), statistical associations between events (Beamer and Girju, 2009; Hu et al., 2017; Hu and Walker, 2017; Do et al., 2011; Hashimoto et al., 2014), and lexical semantics of events (Riaz and Girju, 2013, 2014b,a; Hashimoto et al., 2014). An increasing amount of recent works focused on recognizing event causal relations within a document, but mostly limited to identifying intrasentence causal relations with explicit causal indicators. Mirza et al. (2014) annotated event causal relations in the TempEval-3 corpus and created CausalTimeBank. Mirza and Tonelli (2014) stated that incorporating temporal information improved the performance of a causal relation classifier. Mirza and Tonelli (2016) built both a rule-based multi-sieve approach and a feature based classifier to recognize causal relations in CausalTimeBank. However, causal relations in CausalTimeBank are few and only explicitly stated intrasentence causal relations were annotated. In addition, Mostafazadeh et al. (2016) annotated both temporal and causal relations in 320 short stories ("
N19-1179,C14-1198,0,0.342244,"her causality cues (Riaz and Girju, 2010; Do et al., 2011), statistical associations between events (Beamer and Girju, 2009; Hu et al., 2017; Hu and Walker, 2017; Do et al., 2011; Hashimoto et al., 2014), and lexical semantics of events (Riaz and Girju, 2013, 2014b,a; Hashimoto et al., 2014). An increasing amount of recent works focused on recognizing event causal relations within a document, but mostly limited to identifying intrasentence causal relations with explicit causal indicators. Mirza et al. (2014) annotated event causal relations in the TempEval-3 corpus and created CausalTimeBank. Mirza and Tonelli (2014) stated that incorporating temporal information improved the performance of a causal relation classifier. Mirza and Tonelli (2016) built both a rule-based multi-sieve approach and a feature based classifier to recognize causal relations in CausalTimeBank. However, causal relations in CausalTimeBank are few and only explicitly stated intrasentence causal relations were annotated. In addition, Mostafazadeh et al. (2016) annotated both temporal and causal relations in 320 short stories (five sentences in each story) taken from the ROCStories Corpus and indicated strong correlations between causal"
N19-1179,C16-1007,0,0.0994006,"al., 2017; Hu and Walker, 2017; Do et al., 2011; Hashimoto et al., 2014), and lexical semantics of events (Riaz and Girju, 2013, 2014b,a; Hashimoto et al., 2014). An increasing amount of recent works focused on recognizing event causal relations within a document, but mostly limited to identifying intrasentence causal relations with explicit causal indicators. Mirza et al. (2014) annotated event causal relations in the TempEval-3 corpus and created CausalTimeBank. Mirza and Tonelli (2014) stated that incorporating temporal information improved the performance of a causal relation classifier. Mirza and Tonelli (2016) built both a rule-based multi-sieve approach and a feature based classifier to recognize causal relations in CausalTimeBank. However, causal relations in CausalTimeBank are few and only explicitly stated intrasentence causal relations were annotated. In addition, Mostafazadeh et al. (2016) annotated both temporal and causal relations in 320 short stories (five sentences in each story) taken from the ROCStories Corpus and indicated strong correlations between causal relations and temporal relations. Lately, Caselli and Vossen (2017) created a corpus called EventStoryLine, which contains 258 do"
N19-1179,W16-1007,0,0.0389439,"dentifying intrasentence causal relations with explicit causal indicators. Mirza et al. (2014) annotated event causal relations in the TempEval-3 corpus and created CausalTimeBank. Mirza and Tonelli (2014) stated that incorporating temporal information improved the performance of a causal relation classifier. Mirza and Tonelli (2016) built both a rule-based multi-sieve approach and a feature based classifier to recognize causal relations in CausalTimeBank. However, causal relations in CausalTimeBank are few and only explicitly stated intrasentence causal relations were annotated. In addition, Mostafazadeh et al. (2016) annotated both temporal and causal relations in 320 short stories (five sentences in each story) taken from the ROCStories Corpus and indicated strong correlations between causal relations and temporal relations. Lately, Caselli and Vossen (2017) created a corpus called EventStoryLine, which contains 258 documents and more than 5,000 causal relations. The EventStoryLine corpus is the largest dataset for causal relation identification till now with comprehensive event causal relations annotated, both intra-sentence and cross-sentence, which presents unique challenges for causal relation identi"
P10-1029,N06-1020,0,0.0178997,"999; Niu et al., 2003)). NER systems, however, do not identify nominal NP instances (e.g., “a software manufacturer” or “the beach”), or handle semantic classes that are not associated with proper named entities (e.g., symptoms).1 ACE The idea of simulataneously learning multiple semantic categories to prevent semantic drift has been explored for other tasks, such as semantic lexicon induction (Thelen and Riloff, 2002; McIntosh and Curran, 2009) and pattern learning (Yangarber, 2003). Our bootstrapping model can be viewed as a form of self-training (e.g., (Ng and Cardie, 2003; Mihalcea, 2004; McClosky et al., 2006)), and cross-category training is similar in spirit to co-training (e.g., (Blum and Mitchell, 1998; Collins and Singer, 1999; Riloff and Jones, 1999; Mueller et al., 2002; Phillips and Riloff, 2002)). But, importantly, our classifiers all use the same feature set so they do not represent independent views of the data. They do, however, offer slightly different perspectives because each is at1 Some NER systems also handle specialized constructs such as dates and monetary amounts. 276 training set. Second, we employ a cross-category bootstrapping process that simultaneously trains a suite of cla"
P10-1029,A97-1029,0,0.0201805,"on from the Web using patterns and statistics, typically for the purpose of knowledge acquisition. Importantly, our goal is to classify instances in context, rather than generate lists of terms. In addition, the goal of our research is to learn specialized terms and jargon that may not be common on the Web, as well as domain-specific usages that may differ from the norm (e.g., “mix” and “lab” are usually ANIMALS in our domain). 2 Related Work Semantic class tagging is most closely related to named entity recognition (NER), mention detection, and semantic lexicon induction. NER systems (e.g., (Bikel et al., 1997; Collins and Singer, 1999; Cucerzan and Yarowsky, 1999; Fleischman and Hovy, 2002) identify proper named entities, such as people, organizations, and locations. Several bootstrapping methods for NER have been previously developed (e.g., (Collins and Singer, 1999; Niu et al., 2003)). NER systems, however, do not identify nominal NP instances (e.g., “a software manufacturer” or “the beach”), or handle semantic classes that are not associated with proper named entities (e.g., symptoms).1 ACE The idea of simulataneously learning multiple semantic categories to prevent semantic drift has been expl"
P10-1029,W09-2201,0,0.0525965,"uce a stand-alone dictionary of words with semantic class labels. These techniques are often designed to learn specialized terminology from unannotated domain-specific texts via bootstrapping. Our work, however, focuses on classification of NP instances in context, so the same phrase may be assigned to different semantic classes in different contexts. Consequently, our classifier can also assign semantic class labels to pronouns. There has also been work on extracting semantically related terms or category members from the Web (e.g., (Pas¸ca, 2004; Etzioni et al., 2005; Kozareva et al., 2008; Carlson et al., 2009)). These techniques harvest broad-coverage semantic information from the Web using patterns and statistics, typically for the purpose of knowledge acquisition. Importantly, our goal is to classify instances in context, rather than generate lists of terms. In addition, the goal of our research is to learn specialized terms and jargon that may not be common on the Web, as well as domain-specific usages that may differ from the norm (e.g., “mix” and “lab” are usually ANIMALS in our domain). 2 Related Work Semantic class tagging is most closely related to named entity recognition (NER), mention de"
P10-1029,W99-0613,0,0.119981,"g patterns and statistics, typically for the purpose of knowledge acquisition. Importantly, our goal is to classify instances in context, rather than generate lists of terms. In addition, the goal of our research is to learn specialized terms and jargon that may not be common on the Web, as well as domain-specific usages that may differ from the norm (e.g., “mix” and “lab” are usually ANIMALS in our domain). 2 Related Work Semantic class tagging is most closely related to named entity recognition (NER), mention detection, and semantic lexicon induction. NER systems (e.g., (Bikel et al., 1997; Collins and Singer, 1999; Cucerzan and Yarowsky, 1999; Fleischman and Hovy, 2002) identify proper named entities, such as people, organizations, and locations. Several bootstrapping methods for NER have been previously developed (e.g., (Collins and Singer, 1999; Niu et al., 2003)). NER systems, however, do not identify nominal NP instances (e.g., “a software manufacturer” or “the beach”), or handle semantic classes that are not associated with proper named entities (e.g., symptoms).1 ACE The idea of simulataneously learning multiple semantic categories to prevent semantic drift has been explored for other tasks, such"
P10-1029,W99-0612,0,0.0601407,"typically for the purpose of knowledge acquisition. Importantly, our goal is to classify instances in context, rather than generate lists of terms. In addition, the goal of our research is to learn specialized terms and jargon that may not be common on the Web, as well as domain-specific usages that may differ from the norm (e.g., “mix” and “lab” are usually ANIMALS in our domain). 2 Related Work Semantic class tagging is most closely related to named entity recognition (NER), mention detection, and semantic lexicon induction. NER systems (e.g., (Bikel et al., 1997; Collins and Singer, 1999; Cucerzan and Yarowsky, 1999; Fleischman and Hovy, 2002) identify proper named entities, such as people, organizations, and locations. Several bootstrapping methods for NER have been previously developed (e.g., (Collins and Singer, 1999; Niu et al., 2003)). NER systems, however, do not identify nominal NP instances (e.g., “a software manufacturer” or “the beach”), or handle semantic classes that are not associated with proper named entities (e.g., symptoms).1 ACE The idea of simulataneously learning multiple semantic categories to prevent semantic drift has been explored for other tasks, such as semantic lexicon inductio"
P10-1029,C02-1130,0,0.0680397,"knowledge acquisition. Importantly, our goal is to classify instances in context, rather than generate lists of terms. In addition, the goal of our research is to learn specialized terms and jargon that may not be common on the Web, as well as domain-specific usages that may differ from the norm (e.g., “mix” and “lab” are usually ANIMALS in our domain). 2 Related Work Semantic class tagging is most closely related to named entity recognition (NER), mention detection, and semantic lexicon induction. NER systems (e.g., (Bikel et al., 1997; Collins and Singer, 1999; Cucerzan and Yarowsky, 1999; Fleischman and Hovy, 2002) identify proper named entities, such as people, organizations, and locations. Several bootstrapping methods for NER have been previously developed (e.g., (Collins and Singer, 1999; Niu et al., 2003)). NER systems, however, do not identify nominal NP instances (e.g., “a software manufacturer” or “the beach”), or handle semantic classes that are not associated with proper named entities (e.g., symptoms).1 ACE The idea of simulataneously learning multiple semantic categories to prevent semantic drift has been explored for other tasks, such as semantic lexicon induction (Thelen and Riloff, 2002;"
P10-1029,P09-1045,0,0.482056,")). has access to contextual features and cannot see the seed. Then we apply the classifier to the corpus to automatically label new instances, and combine these new instances with the seed-based instances. This process expands and diversifies the training set to fuel subsequent bootstrapping. Another challenge is that we want to use a small set of seeds to minimize the amount of human effort, and then use bootstrapping to fully exploit the domain-specific corpus. Iterative self-training, however, often has difficulty sustaining momentum or it succumbs to semantic drift (Komachi et al., 2008; McIntosh and Curran, 2009). To address these issues, we simultaneously induce a suite of classifiers for multiple semantic categories, using the positive instances of one semantic category as negative instances for the others. As bootstrapping progresses, the classifiers gradually improve themselves, and each other, over many iterations. We also explore a onesemantic-class-per-discourse (OSCPD) heuristic that infuses the learning process with fresh training instances, which may be substantially different from the ones seen previously, and we use the labels produced by the classifiers to dynamically create semantic feat"
P10-1029,W04-2405,0,0.0273714,"ns and Singer, 1999; Niu et al., 2003)). NER systems, however, do not identify nominal NP instances (e.g., “a software manufacturer” or “the beach”), or handle semantic classes that are not associated with proper named entities (e.g., symptoms).1 ACE The idea of simulataneously learning multiple semantic categories to prevent semantic drift has been explored for other tasks, such as semantic lexicon induction (Thelen and Riloff, 2002; McIntosh and Curran, 2009) and pattern learning (Yangarber, 2003). Our bootstrapping model can be viewed as a form of self-training (e.g., (Ng and Cardie, 2003; Mihalcea, 2004; McClosky et al., 2006)), and cross-category training is similar in spirit to co-training (e.g., (Blum and Mitchell, 1998; Collins and Singer, 1999; Riloff and Jones, 1999; Mueller et al., 2002; Phillips and Riloff, 2002)). But, importantly, our classifiers all use the same feature set so they do not represent independent views of the data. They do, however, offer slightly different perspectives because each is at1 Some NER systems also handle specialized constructs such as dates and monetary amounts. 276 training set. Second, we employ a cross-category bootstrapping process that simultaneous"
P10-1029,P02-1045,0,0.0305929,"t associated with proper named entities (e.g., symptoms).1 ACE The idea of simulataneously learning multiple semantic categories to prevent semantic drift has been explored for other tasks, such as semantic lexicon induction (Thelen and Riloff, 2002; McIntosh and Curran, 2009) and pattern learning (Yangarber, 2003). Our bootstrapping model can be viewed as a form of self-training (e.g., (Ng and Cardie, 2003; Mihalcea, 2004; McClosky et al., 2006)), and cross-category training is similar in spirit to co-training (e.g., (Blum and Mitchell, 1998; Collins and Singer, 1999; Riloff and Jones, 1999; Mueller et al., 2002; Phillips and Riloff, 2002)). But, importantly, our classifiers all use the same feature set so they do not represent independent views of the data. They do, however, offer slightly different perspectives because each is at1 Some NER systems also handle specialized constructs such as dates and monetary amounts. 276 training set. Second, we employ a cross-category bootstrapping process that simultaneously trains a suite of classifiers for multiple semantic categories, using the positive instances for one semantic class as negative instances for the others. This cross-category training process"
P10-1029,N03-1023,0,0.00985694,"veloped (e.g., (Collins and Singer, 1999; Niu et al., 2003)). NER systems, however, do not identify nominal NP instances (e.g., “a software manufacturer” or “the beach”), or handle semantic classes that are not associated with proper named entities (e.g., symptoms).1 ACE The idea of simulataneously learning multiple semantic categories to prevent semantic drift has been explored for other tasks, such as semantic lexicon induction (Thelen and Riloff, 2002; McIntosh and Curran, 2009) and pattern learning (Yangarber, 2003). Our bootstrapping model can be viewed as a form of self-training (e.g., (Ng and Cardie, 2003; Mihalcea, 2004; McClosky et al., 2006)), and cross-category training is similar in spirit to co-training (e.g., (Blum and Mitchell, 1998; Collins and Singer, 1999; Riloff and Jones, 1999; Mueller et al., 2002; Phillips and Riloff, 2002)). But, importantly, our classifiers all use the same feature set so they do not represent independent views of the data. They do, however, offer slightly different perspectives because each is at1 Some NER systems also handle specialized constructs such as dates and monetary amounts. 276 training set. Second, we employ a cross-category bootstrapping process t"
P10-1029,P07-1068,0,0.0136105,"nt from the ones seen previously, and we use the labels produced by the classifiers to dynamically create semantic features. We evaluate our approach by creating six semantic taggers using a collection of message board posts in the domain of veterinary medicine. Our results show this approach produces high-quality semantic taggers after a sustained bootstrapping cycle that maintains good precision while steadily increasing recall over many iterations. Another line of relevant work is semantic class induction (e.g., (Riloff and Shepherd, 1997; Roark and Charniak, 1998; Thelen and Riloff, 2002; Ng, 2007; McIntosh and Curran, 2009), where the goal is to induce a stand-alone dictionary of words with semantic class labels. These techniques are often designed to learn specialized terminology from unannotated domain-specific texts via bootstrapping. Our work, however, focuses on classification of NP instances in context, so the same phrase may be assigned to different semantic classes in different contexts. Consequently, our classifier can also assign semantic class labels to pronouns. There has also been work on extracting semantically related terms or category members from the Web (e.g., (Pas¸c"
P10-1029,P03-1043,0,0.166801,"not be common on the Web, as well as domain-specific usages that may differ from the norm (e.g., “mix” and “lab” are usually ANIMALS in our domain). 2 Related Work Semantic class tagging is most closely related to named entity recognition (NER), mention detection, and semantic lexicon induction. NER systems (e.g., (Bikel et al., 1997; Collins and Singer, 1999; Cucerzan and Yarowsky, 1999; Fleischman and Hovy, 2002) identify proper named entities, such as people, organizations, and locations. Several bootstrapping methods for NER have been previously developed (e.g., (Collins and Singer, 1999; Niu et al., 2003)). NER systems, however, do not identify nominal NP instances (e.g., “a software manufacturer” or “the beach”), or handle semantic classes that are not associated with proper named entities (e.g., symptoms).1 ACE The idea of simulataneously learning multiple semantic categories to prevent semantic drift has been explored for other tasks, such as semantic lexicon induction (Thelen and Riloff, 2002; McIntosh and Curran, 2009) and pattern learning (Yangarber, 2003). Our bootstrapping model can be viewed as a form of self-training (e.g., (Ng and Cardie, 2003; Mihalcea, 2004; McClosky et al., 2006)"
P10-1029,W02-1017,1,0.891137,"er named entities (e.g., symptoms).1 ACE The idea of simulataneously learning multiple semantic categories to prevent semantic drift has been explored for other tasks, such as semantic lexicon induction (Thelen and Riloff, 2002; McIntosh and Curran, 2009) and pattern learning (Yangarber, 2003). Our bootstrapping model can be viewed as a form of self-training (e.g., (Ng and Cardie, 2003; Mihalcea, 2004; McClosky et al., 2006)), and cross-category training is similar in spirit to co-training (e.g., (Blum and Mitchell, 1998; Collins and Singer, 1999; Riloff and Jones, 1999; Mueller et al., 2002; Phillips and Riloff, 2002)). But, importantly, our classifiers all use the same feature set so they do not represent independent views of the data. They do, however, offer slightly different perspectives because each is at1 Some NER systems also handle specialized constructs such as dates and monetary amounts. 276 training set. Second, we employ a cross-category bootstrapping process that simultaneously trains a suite of classifiers for multiple semantic categories, using the positive instances for one semantic class as negative instances for the others. This cross-category training process gives the learner sustained"
P10-1029,W97-0313,1,0.771743,"ning process with fresh training instances, which may be substantially different from the ones seen previously, and we use the labels produced by the classifiers to dynamically create semantic features. We evaluate our approach by creating six semantic taggers using a collection of message board posts in the domain of veterinary medicine. Our results show this approach produces high-quality semantic taggers after a sustained bootstrapping cycle that maintains good precision while steadily increasing recall over many iterations. Another line of relevant work is semantic class induction (e.g., (Riloff and Shepherd, 1997; Roark and Charniak, 1998; Thelen and Riloff, 2002; Ng, 2007; McIntosh and Curran, 2009), where the goal is to induce a stand-alone dictionary of words with semantic class labels. These techniques are often designed to learn specialized terminology from unannotated domain-specific texts via bootstrapping. Our work, however, focuses on classification of NP instances in context, so the same phrase may be assigned to different semantic classes in different contexts. Consequently, our classifier can also assign semantic class labels to pronouns. There has also been work on extracting semantically"
P10-1029,D08-1106,0,0.0127089,"ouni and Florian, 2009)). has access to contextual features and cannot see the seed. Then we apply the classifier to the corpus to automatically label new instances, and combine these new instances with the seed-based instances. This process expands and diversifies the training set to fuel subsequent bootstrapping. Another challenge is that we want to use a small set of seeds to minimize the amount of human effort, and then use bootstrapping to fully exploit the domain-specific corpus. Iterative self-training, however, often has difficulty sustaining momentum or it succumbs to semantic drift (Komachi et al., 2008; McIntosh and Curran, 2009). To address these issues, we simultaneously induce a suite of classifiers for multiple semantic categories, using the positive instances of one semantic category as negative instances for the others. As bootstrapping progresses, the classifiers gradually improve themselves, and each other, over many iterations. We also explore a onesemantic-class-per-discourse (OSCPD) heuristic that infuses the learning process with fresh training instances, which may be substantially different from the ones seen previously, and we use the labels produced by the classifiers to dyna"
P10-1029,P08-1119,1,0.391517,"here the goal is to induce a stand-alone dictionary of words with semantic class labels. These techniques are often designed to learn specialized terminology from unannotated domain-specific texts via bootstrapping. Our work, however, focuses on classification of NP instances in context, so the same phrase may be assigned to different semantic classes in different contexts. Consequently, our classifier can also assign semantic class labels to pronouns. There has also been work on extracting semantically related terms or category members from the Web (e.g., (Pas¸ca, 2004; Etzioni et al., 2005; Kozareva et al., 2008; Carlson et al., 2009)). These techniques harvest broad-coverage semantic information from the Web using patterns and statistics, typically for the purpose of knowledge acquisition. Importantly, our goal is to classify instances in context, rather than generate lists of terms. In addition, the goal of our research is to learn specialized terms and jargon that may not be common on the Web, as well as domain-specific usages that may differ from the norm (e.g., “mix” and “lab” are usually ANIMALS in our domain). 2 Related Work Semantic class tagging is most closely related to named entity recogn"
P10-1029,W02-1028,1,0.88766,"be substantially different from the ones seen previously, and we use the labels produced by the classifiers to dynamically create semantic features. We evaluate our approach by creating six semantic taggers using a collection of message board posts in the domain of veterinary medicine. Our results show this approach produces high-quality semantic taggers after a sustained bootstrapping cycle that maintains good precision while steadily increasing recall over many iterations. Another line of relevant work is semantic class induction (e.g., (Riloff and Shepherd, 1997; Roark and Charniak, 1998; Thelen and Riloff, 2002; Ng, 2007; McIntosh and Curran, 2009), where the goal is to induce a stand-alone dictionary of words with semantic class labels. These techniques are often designed to learn specialized terminology from unannotated domain-specific texts via bootstrapping. Our work, however, focuses on classification of NP instances in context, so the same phrase may be assigned to different semantic classes in different contexts. Consequently, our classifier can also assign semantic class labels to pronouns. There has also been work on extracting semantically related terms or category members from the Web (e."
P10-1029,N03-1033,0,0.0041834,"1.0, when we end the bootstrapping process. In Section 4, we show that this sliding threshold outperforms fixed threshold values. Animal 612 Dis/Sym 900 Drug 369 Test 404 Human 818 Other 1723 To select seed words, we used the procedure proposed by Roark and Charniak (1998), ranking all of the head nouns in the training corpus by frequency and manually selecting the first 10 nouns that unambiguously belong to each category.7 This process is fast, relatively objective, and guaranteed to yield high-frequency terms, which is important for bootstrapping. We used the Stanford part-ofspeech tagger (Toutanova et al., 2003) to identify nouns, and our own simple rule-based NP chunker. 4 Evaluation 4.2 Baselines 4.1 To assess the difficulty of our data set and task, we evaluated several baselines. The first baseline searches for each head noun in WordNet and labels the noun as category Ck if it has a hypernym synset corresponding to that category. We manually identified the WordNet synsets that, to the best of our ability, seem to most closely correspond Data Our data set consists of message board posts from the Veterinary Information Network (VIN), which is a web site (www.vin.com) for professionals in veterinary"
P10-1029,P03-1044,0,0.103195,"organizations, and locations. Several bootstrapping methods for NER have been previously developed (e.g., (Collins and Singer, 1999; Niu et al., 2003)). NER systems, however, do not identify nominal NP instances (e.g., “a software manufacturer” or “the beach”), or handle semantic classes that are not associated with proper named entities (e.g., symptoms).1 ACE The idea of simulataneously learning multiple semantic categories to prevent semantic drift has been explored for other tasks, such as semantic lexicon induction (Thelen and Riloff, 2002; McIntosh and Curran, 2009) and pattern learning (Yangarber, 2003). Our bootstrapping model can be viewed as a form of self-training (e.g., (Ng and Cardie, 2003; Mihalcea, 2004; McClosky et al., 2006)), and cross-category training is similar in spirit to co-training (e.g., (Blum and Mitchell, 1998; Collins and Singer, 1999; Riloff and Jones, 1999; Mueller et al., 2002; Phillips and Riloff, 2002)). But, importantly, our classifiers all use the same feature set so they do not represent independent views of the data. They do, however, offer slightly different perspectives because each is at1 Some NER systems also handle specialized constructs such as dates and"
P10-1029,P95-1026,0,0.240345,"model incrementally trains semantic class taggers, so we explored the idea of using the labels assigned by the classifiers to create enhanced feature vectors by dynamically adding semantic features. This process allows later stages of bootstrapping to directly benefit from earlier stages. For example, consider the sentence: One Semantic Class Per Discourse We also explored the idea of using a one semantic class per discourse (OSCPD) heuristic to generate additional training instances during bootstrapping. Inspired by Yarowsky’s one sense per discourse heuristic for word sense disambiguation (Yarowsky, 1995), we make the assumption that multiple instances of a word in the same discourse will nearly always correspond to the same semantic class. Since our data set consists of message board posts organized as threads, we consider all posts in the same thread to be a single discourse. After each training step, we apply the classifiers to the unlabeled data to label some new instances. For each newly labeled instance, the OSCPD heuristic collects all instances with the same head noun in the same discourse (thread) and unilaterally labels them with the same semantic class. This heuristic serves as meta"
P10-1029,P98-2182,0,\N,Missing
P10-1029,C98-2177,0,\N,Missing
P11-1114,P07-1073,0,0.0111927,"96; Freitag, 1998b; Ciravegna, 2001; Califf and Mooney, 2003)), weakly supervised learning (e.g., (Riloff, 1996; Riloff and Jones, 1999; Yangarber et al., 2000; Sudo et al., 2003; Stevenson and Greenwood, 2005)), or unsupervised learning (e.g., (Shinyama and Sekine, 2006; Sekine, 2006)). In addition, many classifiers have been created to sequentially label event role fillers in a sentence (e.g., (Freitag, 1998a; Chieu and Ng, 2002; Finn and Kushmerick, 2004; Li et al., 2005; Yu et al., 2005)). Research has also been done on relation extraction (e.g., (Roth and Yih, 2001; Zelenko et al., 2003; Bunescu and Mooney, 2007)), but that task is different from event extraction because it focuses on isolated relations rather than template-based event analysis. Most event extraction systems scan a text and search small context windows using patterns or a classifier. However, recent work has begun to exFigure 1: TIER: A Multi-Layered Architecture for Event Extraction plore more global approaches. (Maslennikov and Chua, 2007) use discourse trees and local syntactic dependencies in a pattern-based framework to incorporate wider context. Ji and Grishman (2008) enforce event role consistency across different documents. (L"
P11-1114,P05-1045,0,0.00556965,"represent four words to the left and four words to the right of the targeted NP, as well as the head noun and modifiers (adjectives and noun modifiers) of the targeted NP itself. Lexico-syntactic patterns: we use the AutoSlog pattern generator (Riloff, 1993) to automatically create lexico-syntactic patterns around each noun phrase in the sentence. These patterns are similar to dependency relations in that they typically represent the syntactic role of the NP with respect to other constituents (e.g., subject-of, object-of, and noun arguments). Semantic features: we use the Stanford NER tagger (Finkel et al., 2005) to determine if the targeted NP is a named entity, and we use the Sundance parser (Riloff and Phillips, 2004) to assign semantic class labels to each NP’s head noun. 4 Event Narrative Document Classification One of our goals was to explore the use of document genre to permit more aggressive strategies for extracting role fillers. In this section, we first present an analysis of the MUC-4 data set which reveals the distribution of event narratives in the corpus, and then explain how we train a classifier to automatically identify event narrative stories. 4.1 Manual Analysis We define an event"
P11-1114,P98-1067,0,0.85422,"given to the role filler extractors. This multi-layered approach creates an event extraction system that can discover role fillers in a variety of different contexts, while maintaining good precision. In the following sections, we position our research with respect to related work, present the details of our multi-layered event extraction model, and show experimental results for five event roles using the MUC-4 data set. 1138 2 Related Work Some event extraction data sets only include documents that describe relevant events (e.g., wellknown data sets for the domains of corporate acquisitions (Freitag, 1998b; Freitag and McCallum, 2000; Finn and Kushmerick, 2004), job postings (Califf and Mooney, 2003; Freitag and McCallum, 2000), and seminar announcements (Freitag, 1998b; Ciravegna, 2001; Chieu and Ng, 2002; Finn and Kushmerick, 2004; Gu and Cercone, 2006). But many IE data sets present a more realistic task where the IE system must determine whether a relevant event is present in the document, and if so, extract its role fillers. Most of the Message Understanding Conference data sets represent this type of event extraction task, containing (roughly) a 50/50 mix of relevant and irrelevant docum"
P11-1114,P06-1061,0,0.428326,"earch with respect to related work, present the details of our multi-layered event extraction model, and show experimental results for five event roles using the MUC-4 data set. 1138 2 Related Work Some event extraction data sets only include documents that describe relevant events (e.g., wellknown data sets for the domains of corporate acquisitions (Freitag, 1998b; Freitag and McCallum, 2000; Finn and Kushmerick, 2004), job postings (Califf and Mooney, 2003; Freitag and McCallum, 2000), and seminar announcements (Freitag, 1998b; Ciravegna, 2001; Chieu and Ng, 2002; Finn and Kushmerick, 2004; Gu and Cercone, 2006). But many IE data sets present a more realistic task where the IE system must determine whether a relevant event is present in the document, and if so, extract its role fillers. Most of the Message Understanding Conference data sets represent this type of event extraction task, containing (roughly) a 50/50 mix of relevant and irrelevant documents (e.g., MUC-3, MUC-4, MUC-6, and MUC-7 (Hirschman, 1998)). Our research focuses on this setting where the event extraction system is not assured of getting only relevant documents to process. Most event extraction models can be characterized as either"
P11-1114,P08-1030,0,0.208204,"traction (e.g., (Roth and Yih, 2001; Zelenko et al., 2003; Bunescu and Mooney, 2007)), but that task is different from event extraction because it focuses on isolated relations rather than template-based event analysis. Most event extraction systems scan a text and search small context windows using patterns or a classifier. However, recent work has begun to exFigure 1: TIER: A Multi-Layered Architecture for Event Extraction plore more global approaches. (Maslennikov and Chua, 2007) use discourse trees and local syntactic dependencies in a pattern-based framework to incorporate wider context. Ji and Grishman (2008) enforce event role consistency across different documents. (Liao and Grishman, 2010) use cross-event inference to help with the extraction of role fillers shared across events. And there have been several recent IE models that explore the idea of identifying relevant sentences to gain a wider contextual view and then extracting role fillers. (Gu and Cercone, 2006) created HMMs to first identify relevant sentences, but their research focused on eliminating redundant extractions and worked with seminar announcements, where the system was only given relevant documents. (Patwardhan and Riloff, 20"
P11-1114,M91-1033,1,0.350906,"ocument, and if so, extract its role fillers. Most of the Message Understanding Conference data sets represent this type of event extraction task, containing (roughly) a 50/50 mix of relevant and irrelevant documents (e.g., MUC-3, MUC-4, MUC-6, and MUC-7 (Hirschman, 1998)). Our research focuses on this setting where the event extraction system is not assured of getting only relevant documents to process. Most event extraction models can be characterized as either pattern-based or classifier-based approaches. Early event extraction systems used handcrafted patterns (e.g., (Appelt et al., 1993; Lehnert et al., 1991)), but more recent systems generate patterns or rules automatically using supervised learning (e.g., (Kim and Moldovan, 1993; Riloff, 1993; Soderland et al., 1995; Huffman, 1996; Freitag, 1998b; Ciravegna, 2001; Califf and Mooney, 2003)), weakly supervised learning (e.g., (Riloff, 1996; Riloff and Jones, 1999; Yangarber et al., 2000; Sudo et al., 2003; Stevenson and Greenwood, 2005)), or unsupervised learning (e.g., (Shinyama and Sekine, 2006; Sekine, 2006)). In addition, many classifiers have been created to sequentially label event role fillers in a sentence (e.g., (Freitag, 1998a; Chieu and"
P11-1114,W05-0610,0,0.268281,"tterns or rules automatically using supervised learning (e.g., (Kim and Moldovan, 1993; Riloff, 1993; Soderland et al., 1995; Huffman, 1996; Freitag, 1998b; Ciravegna, 2001; Califf and Mooney, 2003)), weakly supervised learning (e.g., (Riloff, 1996; Riloff and Jones, 1999; Yangarber et al., 2000; Sudo et al., 2003; Stevenson and Greenwood, 2005)), or unsupervised learning (e.g., (Shinyama and Sekine, 2006; Sekine, 2006)). In addition, many classifiers have been created to sequentially label event role fillers in a sentence (e.g., (Freitag, 1998a; Chieu and Ng, 2002; Finn and Kushmerick, 2004; Li et al., 2005; Yu et al., 2005)). Research has also been done on relation extraction (e.g., (Roth and Yih, 2001; Zelenko et al., 2003; Bunescu and Mooney, 2007)), but that task is different from event extraction because it focuses on isolated relations rather than template-based event analysis. Most event extraction systems scan a text and search small context windows using patterns or a classifier. However, recent work has begun to exFigure 1: TIER: A Multi-Layered Architecture for Event Extraction plore more global approaches. (Maslennikov and Chua, 2007) use discourse trees and local syntactic dependenc"
P11-1114,P10-1081,0,0.254612,")), but that task is different from event extraction because it focuses on isolated relations rather than template-based event analysis. Most event extraction systems scan a text and search small context windows using patterns or a classifier. However, recent work has begun to exFigure 1: TIER: A Multi-Layered Architecture for Event Extraction plore more global approaches. (Maslennikov and Chua, 2007) use discourse trees and local syntactic dependencies in a pattern-based framework to incorporate wider context. Ji and Grishman (2008) enforce event role consistency across different documents. (Liao and Grishman, 2010) use cross-event inference to help with the extraction of role fillers shared across events. And there have been several recent IE models that explore the idea of identifying relevant sentences to gain a wider contextual view and then extracting role fillers. (Gu and Cercone, 2006) created HMMs to first identify relevant sentences, but their research focused on eliminating redundant extractions and worked with seminar announcements, where the system was only given relevant documents. (Patwardhan and Riloff, 2007) developed a system that learns to recognize event sentences and uses patterns tha"
P11-1114,P07-1075,0,0.062688,"reitag, 1998a; Chieu and Ng, 2002; Finn and Kushmerick, 2004; Li et al., 2005; Yu et al., 2005)). Research has also been done on relation extraction (e.g., (Roth and Yih, 2001; Zelenko et al., 2003; Bunescu and Mooney, 2007)), but that task is different from event extraction because it focuses on isolated relations rather than template-based event analysis. Most event extraction systems scan a text and search small context windows using patterns or a classifier. However, recent work has begun to exFigure 1: TIER: A Multi-Layered Architecture for Event Extraction plore more global approaches. (Maslennikov and Chua, 2007) use discourse trees and local syntactic dependencies in a pattern-based framework to incorporate wider context. Ji and Grishman (2008) enforce event role consistency across different documents. (Liao and Grishman, 2010) use cross-event inference to help with the extraction of role fillers shared across events. And there have been several recent IE models that explore the idea of identifying relevant sentences to gain a wider contextual view and then extracting role fillers. (Gu and Cercone, 2006) created HMMs to first identify relevant sentences, but their research focused on eliminating redu"
P11-1114,D07-1075,1,0.933146,"t. Ji and Grishman (2008) enforce event role consistency across different documents. (Liao and Grishman, 2010) use cross-event inference to help with the extraction of role fillers shared across events. And there have been several recent IE models that explore the idea of identifying relevant sentences to gain a wider contextual view and then extracting role fillers. (Gu and Cercone, 2006) created HMMs to first identify relevant sentences, but their research focused on eliminating redundant extractions and worked with seminar announcements, where the system was only given relevant documents. (Patwardhan and Riloff, 2007) developed a system that learns to recognize event sentences and uses patterns that have a semantic affinity for an event role to extract role fillers. GLACIER (Patwardhan and Riloff, 2009) jointly considers sentential evidence and phrasal evidence in a unified probabilistic framework. Our research follows in the same spirit as these approaches by performing multiple levels of text analysis. But our event extraction model includes two novel contributions: (1) we develop a set of role-specific sentence classifiers to learn to recognize secondary contexts associated with each type of event role"
P11-1114,D09-1016,1,0.924209,"cross events. And there have been several recent IE models that explore the idea of identifying relevant sentences to gain a wider contextual view and then extracting role fillers. (Gu and Cercone, 2006) created HMMs to first identify relevant sentences, but their research focused on eliminating redundant extractions and worked with seminar announcements, where the system was only given relevant documents. (Patwardhan and Riloff, 2007) developed a system that learns to recognize event sentences and uses patterns that have a semantic affinity for an event role to extract role fillers. GLACIER (Patwardhan and Riloff, 2009) jointly considers sentential evidence and phrasal evidence in a unified probabilistic framework. Our research follows in the same spirit as these approaches by performing multiple levels of text analysis. But our event extraction model includes two novel contributions: (1) we develop a set of role-specific sentence classifiers to learn to recognize secondary contexts associated with each type of event role , and (2) we exploit text genre to incorporate a third level of analysis that enables the system to aggressively hunt for role fillers in documents that are event narratives. In Section 5,"
P11-1114,P06-2094,0,0.205751,"ed as either pattern-based or classifier-based approaches. Early event extraction systems used handcrafted patterns (e.g., (Appelt et al., 1993; Lehnert et al., 1991)), but more recent systems generate patterns or rules automatically using supervised learning (e.g., (Kim and Moldovan, 1993; Riloff, 1993; Soderland et al., 1995; Huffman, 1996; Freitag, 1998b; Ciravegna, 2001; Califf and Mooney, 2003)), weakly supervised learning (e.g., (Riloff, 1996; Riloff and Jones, 1999; Yangarber et al., 2000; Sudo et al., 2003; Stevenson and Greenwood, 2005)), or unsupervised learning (e.g., (Shinyama and Sekine, 2006; Sekine, 2006)). In addition, many classifiers have been created to sequentially label event role fillers in a sentence (e.g., (Freitag, 1998a; Chieu and Ng, 2002; Finn and Kushmerick, 2004; Li et al., 2005; Yu et al., 2005)). Research has also been done on relation extraction (e.g., (Roth and Yih, 2001; Zelenko et al., 2003; Bunescu and Mooney, 2007)), but that task is different from event extraction because it focuses on isolated relations rather than template-based event analysis. Most event extraction systems scan a text and search small context windows using patterns or a classifier. How"
P11-1114,N06-1039,0,0.0519882,"e characterized as either pattern-based or classifier-based approaches. Early event extraction systems used handcrafted patterns (e.g., (Appelt et al., 1993; Lehnert et al., 1991)), but more recent systems generate patterns or rules automatically using supervised learning (e.g., (Kim and Moldovan, 1993; Riloff, 1993; Soderland et al., 1995; Huffman, 1996; Freitag, 1998b; Ciravegna, 2001; Califf and Mooney, 2003)), weakly supervised learning (e.g., (Riloff, 1996; Riloff and Jones, 1999; Yangarber et al., 2000; Sudo et al., 2003; Stevenson and Greenwood, 2005)), or unsupervised learning (e.g., (Shinyama and Sekine, 2006; Sekine, 2006)). In addition, many classifiers have been created to sequentially label event role fillers in a sentence (e.g., (Freitag, 1998a; Chieu and Ng, 2002; Finn and Kushmerick, 2004; Li et al., 2005; Yu et al., 2005)). Research has also been done on relation extraction (e.g., (Roth and Yih, 2001; Zelenko et al., 2003; Bunescu and Mooney, 2007)), but that task is different from event extraction because it focuses on isolated relations rather than template-based event analysis. Most event extraction systems scan a text and search small context windows using patterns or a classifier. How"
P11-1114,P05-1047,0,0.150983,"relevant documents to process. Most event extraction models can be characterized as either pattern-based or classifier-based approaches. Early event extraction systems used handcrafted patterns (e.g., (Appelt et al., 1993; Lehnert et al., 1991)), but more recent systems generate patterns or rules automatically using supervised learning (e.g., (Kim and Moldovan, 1993; Riloff, 1993; Soderland et al., 1995; Huffman, 1996; Freitag, 1998b; Ciravegna, 2001; Califf and Mooney, 2003)), weakly supervised learning (e.g., (Riloff, 1996; Riloff and Jones, 1999; Yangarber et al., 2000; Sudo et al., 2003; Stevenson and Greenwood, 2005)), or unsupervised learning (e.g., (Shinyama and Sekine, 2006; Sekine, 2006)). In addition, many classifiers have been created to sequentially label event role fillers in a sentence (e.g., (Freitag, 1998a; Chieu and Ng, 2002; Finn and Kushmerick, 2004; Li et al., 2005; Yu et al., 2005)). Research has also been done on relation extraction (e.g., (Roth and Yih, 2001; Zelenko et al., 2003; Bunescu and Mooney, 2007)), but that task is different from event extraction because it focuses on isolated relations rather than template-based event analysis. Most event extraction systems scan a text and sea"
P11-1114,P03-1029,0,0.467129,"red of getting only relevant documents to process. Most event extraction models can be characterized as either pattern-based or classifier-based approaches. Early event extraction systems used handcrafted patterns (e.g., (Appelt et al., 1993; Lehnert et al., 1991)), but more recent systems generate patterns or rules automatically using supervised learning (e.g., (Kim and Moldovan, 1993; Riloff, 1993; Soderland et al., 1995; Huffman, 1996; Freitag, 1998b; Ciravegna, 2001; Califf and Mooney, 2003)), weakly supervised learning (e.g., (Riloff, 1996; Riloff and Jones, 1999; Yangarber et al., 2000; Sudo et al., 2003; Stevenson and Greenwood, 2005)), or unsupervised learning (e.g., (Shinyama and Sekine, 2006; Sekine, 2006)). In addition, many classifiers have been created to sequentially label event role fillers in a sentence (e.g., (Freitag, 1998a; Chieu and Ng, 2002; Finn and Kushmerick, 2004; Li et al., 2005; Yu et al., 2005)). Research has also been done on relation extraction (e.g., (Roth and Yih, 2001; Zelenko et al., 2003; Bunescu and Mooney, 2007)), but that task is different from event extraction because it focuses on isolated relations rather than template-based event analysis. Most event extrac"
P11-1114,C00-2136,0,0.613195,"ction system is not assured of getting only relevant documents to process. Most event extraction models can be characterized as either pattern-based or classifier-based approaches. Early event extraction systems used handcrafted patterns (e.g., (Appelt et al., 1993; Lehnert et al., 1991)), but more recent systems generate patterns or rules automatically using supervised learning (e.g., (Kim and Moldovan, 1993; Riloff, 1993; Soderland et al., 1995; Huffman, 1996; Freitag, 1998b; Ciravegna, 2001; Califf and Mooney, 2003)), weakly supervised learning (e.g., (Riloff, 1996; Riloff and Jones, 1999; Yangarber et al., 2000; Sudo et al., 2003; Stevenson and Greenwood, 2005)), or unsupervised learning (e.g., (Shinyama and Sekine, 2006; Sekine, 2006)). In addition, many classifiers have been created to sequentially label event role fillers in a sentence (e.g., (Freitag, 1998a; Chieu and Ng, 2002; Finn and Kushmerick, 2004; Li et al., 2005; Yu et al., 2005)). Research has also been done on relation extraction (e.g., (Roth and Yih, 2001; Zelenko et al., 2003; Bunescu and Mooney, 2007)), but that task is different from event extraction because it focuses on isolated relations rather than template-based event analysis"
P11-1114,P05-1062,0,0.228025,"utomatically using supervised learning (e.g., (Kim and Moldovan, 1993; Riloff, 1993; Soderland et al., 1995; Huffman, 1996; Freitag, 1998b; Ciravegna, 2001; Califf and Mooney, 2003)), weakly supervised learning (e.g., (Riloff, 1996; Riloff and Jones, 1999; Yangarber et al., 2000; Sudo et al., 2003; Stevenson and Greenwood, 2005)), or unsupervised learning (e.g., (Shinyama and Sekine, 2006; Sekine, 2006)). In addition, many classifiers have been created to sequentially label event role fillers in a sentence (e.g., (Freitag, 1998a; Chieu and Ng, 2002; Finn and Kushmerick, 2004; Li et al., 2005; Yu et al., 2005)). Research has also been done on relation extraction (e.g., (Roth and Yih, 2001; Zelenko et al., 2003; Bunescu and Mooney, 2007)), but that task is different from event extraction because it focuses on isolated relations rather than template-based event analysis. Most event extraction systems scan a text and search small context windows using patterns or a classifier. However, recent work has begun to exFigure 1: TIER: A Multi-Layered Architecture for Event Extraction plore more global approaches. (Maslennikov and Chua, 2007) use discourse trees and local syntactic dependencies in a pattern-b"
P11-1114,C98-1064,0,\N,Missing
P18-1045,W06-0901,0,0.0119317,"ent chains and 486 ILP to discourage coreference links between a specific action event and other event mentions. recover complete chains for the main events, we also encourage associating more coreferent event mentions to a chain that has a large stretch (the number of sentences between the first and the last event mention based on their textual positions). 3 Related Work Compared to entity coreference resolution (Lee et al., 2017; Clark and Manning, 2016a,b; Martschat and Strube, 2015; Lee et al., 2013), far less research was conducted for event coreference resolution. Most existing methods (Ahn, 2006; Chen et al., 2009; Cybulska and Vossen, 2015a,b) heavily rely on surface features, mainly event arguments (i.e., entities such as event participants, time, location, etc.) that were extracted from local contexts of two events, and determine that two events are coreferential if their arguments match. Often, a clustering algorithm, hierarchical Bayesian (Bejan and Harabagiu, 2010, 2014; Yang et al., 2015) or spectral clustering algorithms (Chen and Ji, 2009), is applied on top of a pairwise surface feature based classifier for inducing event clusters. However, identifying potential arguments,"
P18-1045,araki-etal-2014-detecting,0,0.0874479,"nt mentions which, however, are less likely to initiate a new coreference chain. Inspired by this observation, we simply modify the objective function of ILP to encourage more event coreference links in early sections of a document. Subevents: subevents exist mainly to provide details and evidence for the parent event, therefore, the relation between subevents and their parent event presents another aspect of correlations between event relations and hierarchical document topic structures. Subevents may share the same lexical form as the parent event and cause spurious event coreference links (Araki et al., 2014). We observe that subevents referring to specific actions were seldomly referred back in a document and are often singleton events. Following the approach proposed by (Badgett and Huang, 2016), we identify such specific action events and improve event coreference resolution by specifying constraints in 487 ences and the overall topic structures of a document, which is more likely to yield robust and generalizable event coreference resolvers. 4 mulate the baseline objective function that minimizes equation 1. Further we add constraints (equation 2) over each triplets of mentions to enforce tran"
P18-1045,Q16-1028,0,0.0168316,". Note that ξm refers to all the event mentions in sentence m, and xij is the indicator variable which is set to 1 if event mentions defined by index i and j are coreferent. Thus, the above constraint ensures that two topic transition sentences contain at least one coreferent event pair. (equation 5) and the new constraints (equation 6 and 7): ΘG = − X γij (5) i,j∈µ σij = X ¬xki ∧ k&lt;i X ¬xjl ∧ xij (6) j&lt;l σij ∈ {0, 1} Γi = X xki + k,i∈Λ Identifying Topic Transition Sentences Using Sentence Similarities: First, we use the unsupervised method based on weighted word embedding average proposed by Arora et al. (2016) to obtain sentence embeddings. We first compute the weighted average of words’ embeddings in a sentence, where the weight of a word w is given by a/(a + p(w)). Here, p(w) represents the estimated word frequency obtained from English Wikipedia and a is a small constant (1e-5). We then compute the first principal component of averaged word embeddings corresponding to sentences in a document and remove the projection on the first principal component from each averaged word embedding for each sentence. X xij i,j∈Λ M (1 − yij ) ≥ (ϕ[j] − ϕ[i]).σij − d0.75 (|S|)e γij − Γi − Γj ≥ M.yij Γi , Γj , γij"
P18-1045,D16-1088,1,0.91896,"erence links in early sections of a document. Subevents: subevents exist mainly to provide details and evidence for the parent event, therefore, the relation between subevents and their parent event presents another aspect of correlations between event relations and hierarchical document topic structures. Subevents may share the same lexical form as the parent event and cause spurious event coreference links (Araki et al., 2014). We observe that subevents referring to specific actions were seldomly referred back in a document and are often singleton events. Following the approach proposed by (Badgett and Huang, 2016), we identify such specific action events and improve event coreference resolution by specifying constraints in 487 ences and the overall topic structures of a document, which is more likely to yield robust and generalizable event coreference resolvers. 4 mulate the baseline objective function that minimizes equation 1. Further we add constraints (equation 2) over each triplets of mentions to enforce transitivity (Denis et al., 2007; Finkel and Manning, 2008). This guarantees legal clustering by ensuring that xij = xjk = 1 implies xik = 1. Modeling Event Coreference Chain Topic Structure Corre"
P18-1045,cybulska-vossen-2014-using,0,0.0394611,"se relu, tanh and softmax activations in the input, hidden and output layers respectively. We use GloVe vectors (Pennington et al., 2014) for word embeddings and one-hot vectors for pos-tag and dependency relations in each individual model. Postagging, dependency parsing, named entity recognition and entity coreference resolution are performed using Stanford CoreNLP (Manning et al., 2014) Table 2 shows the event mention identification results. We report the F1 score for event mention identification based on the KBP scorer, which considers a mention correct if its span, type and sub4 The ECB+ (Cybulska and Vossen, 2014) corpus is another commonly used dataset for evaluating event coreference resolution performance. But we determined that this corpus is not appropriate for evaluating our ILP model that explicitly focuses on using discourse level topic structures for event coreference resolution. Particularly, the ECB+ corpus was created to facilitate both cross-document and indocument event coreference resolution research. Thus, the documents in the corpus were grouped based on several common topics and in each document, event mentions and coreference relations were only annotated selectively in sentences tha"
P18-1045,W15-0801,0,0.069758,"ourage coreference links between a specific action event and other event mentions. recover complete chains for the main events, we also encourage associating more coreferent event mentions to a chain that has a large stretch (the number of sentences between the first and the last event mention based on their textual positions). 3 Related Work Compared to entity coreference resolution (Lee et al., 2017; Clark and Manning, 2016a,b; Martschat and Strube, 2015; Lee et al., 2013), far less research was conducted for event coreference resolution. Most existing methods (Ahn, 2006; Chen et al., 2009; Cybulska and Vossen, 2015a,b) heavily rely on surface features, mainly event arguments (i.e., entities such as event participants, time, location, etc.) that were extracted from local contexts of two events, and determine that two events are coreferential if their arguments match. Often, a clustering algorithm, hierarchical Bayesian (Bejan and Harabagiu, 2010, 2014; Yang et al., 2015) or spectral clustering algorithms (Chen and Ji, 2009), is applied on top of a pairwise surface feature based classifier for inducing event clusters. However, identifying potential arguments, linking arguments to a proper event mention, a"
P18-1045,P10-1143,0,0.140691,"ted Work Compared to entity coreference resolution (Lee et al., 2017; Clark and Manning, 2016a,b; Martschat and Strube, 2015; Lee et al., 2013), far less research was conducted for event coreference resolution. Most existing methods (Ahn, 2006; Chen et al., 2009; Cybulska and Vossen, 2015a,b) heavily rely on surface features, mainly event arguments (i.e., entities such as event participants, time, location, etc.) that were extracted from local contexts of two events, and determine that two events are coreferential if their arguments match. Often, a clustering algorithm, hierarchical Bayesian (Bejan and Harabagiu, 2010, 2014; Yang et al., 2015) or spectral clustering algorithms (Chen and Ji, 2009), is applied on top of a pairwise surface feature based classifier for inducing event clusters. However, identifying potential arguments, linking arguments to a proper event mention, and recognizing compatibilities between arguments are all error-prone (Lu et al., 2016). Joint event and entity coreference resolution (Lee et al., 2012), joint inferences of event detection and event coreference resolution (Lu and Ng, 2017), and iterative information propagation (Liu et al., 2014; Choubey and Huang, 2017a) have been p"
P18-1045,J14-2004,0,0.347999,"Missing"
P18-1045,N07-1030,0,0.0991437,"Missing"
P18-1045,W09-3208,0,0.375846,", 2016a,b; Martschat and Strube, 2015; Lee et al., 2013), far less research was conducted for event coreference resolution. Most existing methods (Ahn, 2006; Chen et al., 2009; Cybulska and Vossen, 2015a,b) heavily rely on surface features, mainly event arguments (i.e., entities such as event participants, time, location, etc.) that were extracted from local contexts of two events, and determine that two events are coreferential if their arguments match. Often, a clustering algorithm, hierarchical Bayesian (Bejan and Harabagiu, 2010, 2014; Yang et al., 2015) or spectral clustering algorithms (Chen and Ji, 2009), is applied on top of a pairwise surface feature based classifier for inducing event clusters. However, identifying potential arguments, linking arguments to a proper event mention, and recognizing compatibilities between arguments are all error-prone (Lu et al., 2016). Joint event and entity coreference resolution (Lee et al., 2012), joint inferences of event detection and event coreference resolution (Lu and Ng, 2017), and iterative information propagation (Liu et al., 2014; Choubey and Huang, 2017a) have been proposed to mitigate argument mismatch issues. However, such methods are incapabl"
P18-1045,W09-4303,0,0.696778,"and 486 ILP to discourage coreference links between a specific action event and other event mentions. recover complete chains for the main events, we also encourage associating more coreferent event mentions to a chain that has a large stretch (the number of sentences between the first and the last event mention based on their textual positions). 3 Related Work Compared to entity coreference resolution (Lee et al., 2017; Clark and Manning, 2016a,b; Martschat and Strube, 2015; Lee et al., 2013), far less research was conducted for event coreference resolution. Most existing methods (Ahn, 2006; Chen et al., 2009; Cybulska and Vossen, 2015a,b) heavily rely on surface features, mainly event arguments (i.e., entities such as event participants, time, location, etc.) that were extracted from local contexts of two events, and determine that two events are coreferential if their arguments match. Often, a clustering algorithm, hierarchical Bayesian (Bejan and Harabagiu, 2010, 2014; Yang et al., 2015) or spectral clustering algorithms (Chen and Ji, 2009), is applied on top of a pairwise surface feature based classifier for inducing event clusters. However, identifying potential arguments, linking arguments t"
P18-1045,D17-1226,1,0.877792,"Missing"
P18-1045,P08-2012,0,0.0397598,"ferring to specific actions were seldomly referred back in a document and are often singleton events. Following the approach proposed by (Badgett and Huang, 2016), we identify such specific action events and improve event coreference resolution by specifying constraints in 487 ences and the overall topic structures of a document, which is more likely to yield robust and generalizable event coreference resolvers. 4 mulate the baseline objective function that minimizes equation 1. Further we add constraints (equation 2) over each triplets of mentions to enforce transitivity (Denis et al., 2007; Finkel and Manning, 2008). This guarantees legal clustering by ensuring that xij = xjk = 1 implies xik = 1. Modeling Event Coreference Chain Topic Structure Correlations Using Integer Linear Programming ΘB = We model discourse level event-topic correlation structures by formulating the event coreference resolution task as an Integer Linear Programming (ILP) problem. Our baseline ILP system is defined over pairwise scores between event mentions obtained from a pairwise neural network-based coreference resolution classifier. 4.1 X −log(pij )xij − log(1 − pij )(¬xij ) (1) i,j∈Λ s.t. xij ∈ {0, 1} ¬xij + ¬xjk ≥ ¬xik (2) We"
P18-1045,N18-2055,1,0.70489,"entions make the backbone of a document and coreferent mentions of the same event play a key role in achieving a coherent content structure. For example, in figure 1, the events 2 Correlations between Event Coreference Chains and Document Topic Structures We model four aspects of correlations. Correlations between Main Event Chains and Topic Transition Sentences: the main events of a document, e.g., “hearing” and “detention” in this example 1, usually have multiple coreferent event mentions that span over a large portion of the document and align well with the document topic layout structure (Choubey et al., 2018). While fine-grained topic segmentation is a difficult task in its own right, we find that topic transition sentences often overlap in content (for reminding purposes) and can be identified by calculating sentence similarities. For example, sentences S1, S2 and S5 in Figure 1 all mentioned the two main events and the main entity “President Chen”. We, therefore, encourage coreference links between event mentions that appear in topic transition sentences by designing constraints in ILP and modifying the objective function. In addition, to avoid fragmented partial event chains and 486 ILP to disc"
P18-1045,J95-2003,0,0.839907,"Missing"
P18-1045,D16-1245,0,0.0167609,"mentions that appear in topic transition sentences by designing constraints in ILP and modifying the objective function. In addition, to avoid fragmented partial event chains and 486 ILP to discourage coreference links between a specific action event and other event mentions. recover complete chains for the main events, we also encourage associating more coreferent event mentions to a chain that has a large stretch (the number of sentences between the first and the last event mention based on their textual positions). 3 Related Work Compared to entity coreference resolution (Lee et al., 2017; Clark and Manning, 2016a,b; Martschat and Strube, 2015; Lee et al., 2013), far less research was conducted for event coreference resolution. Most existing methods (Ahn, 2006; Chen et al., 2009; Cybulska and Vossen, 2015a,b) heavily rely on surface features, mainly event arguments (i.e., entities such as event participants, time, location, etc.) that were extracted from local contexts of two events, and determine that two events are coreferential if their arguments match. Often, a clustering algorithm, hierarchical Bayesian (Bejan and Harabagiu, 2010, 2014; Yang et al., 2015) or spectral clustering algorithms (Chen a"
P18-1045,P07-1107,0,0.0505687,"event mentions following event mention j respectively. 489 4.2.2 Cross-chain Inferences As illustrated through Figure 1, semantically related events tend to have their mentions co-occur within the same sentence. So, we define the objective function (equation 8) and constraints (9) to favor a sentence with a mention from one event chain to also contain a mention from another event chain, if the two event chains are known to have event mentions co-occur in several other sentences. X ΘC = − Φmn It is also important to understand that positionbased features used in entity coreference resolution (Haghighi and Klein, 2007) are usually defined for an entity pair. However, we model the distributional patterns of an event chain in a document. 4.2.4 Restraining Subevents from Being Included in Coreference Chains Subevents are known to be a major source of false coreference links due to their high surface similarity with their parent events. Therefore, we discourage subevents from being included in coreference chains in our model and modify the global optimization goal by adding a new objective function (equation 11). (8) m,n∈Ω Φmn = X xij i∈ξm ,j∈ξn (9) |ξm |&gt; 1; |ξn |&gt; 1; Φmn ∈ Z; Φmn ≥ 0 To do so, we first define"
P18-1045,P16-1061,0,0.0335794,"mentions that appear in topic transition sentences by designing constraints in ILP and modifying the objective function. In addition, to avoid fragmented partial event chains and 486 ILP to discourage coreference links between a specific action event and other event mentions. recover complete chains for the main events, we also encourage associating more coreferent event mentions to a chain that has a large stretch (the number of sentences between the first and the last event mention based on their textual positions). 3 Related Work Compared to entity coreference resolution (Lee et al., 2017; Clark and Manning, 2016a,b; Martschat and Strube, 2015; Lee et al., 2013), far less research was conducted for event coreference resolution. Most existing methods (Ahn, 2006; Chen et al., 2009; Cybulska and Vossen, 2015a,b) heavily rely on surface features, mainly event arguments (i.e., entities such as event participants, time, location, etc.) that were extracted from local contexts of two events, and determine that two events are coreferential if their arguments match. Often, a clustering algorithm, hierarchical Bayesian (Bejan and Harabagiu, 2010, 2014; Yang et al., 2015) or spectral clustering algorithms (Chen a"
P18-1045,J13-4004,0,0.0179904,"signing constraints in ILP and modifying the objective function. In addition, to avoid fragmented partial event chains and 486 ILP to discourage coreference links between a specific action event and other event mentions. recover complete chains for the main events, we also encourage associating more coreferent event mentions to a chain that has a large stretch (the number of sentences between the first and the last event mention based on their textual positions). 3 Related Work Compared to entity coreference resolution (Lee et al., 2017; Clark and Manning, 2016a,b; Martschat and Strube, 2015; Lee et al., 2013), far less research was conducted for event coreference resolution. Most existing methods (Ahn, 2006; Chen et al., 2009; Cybulska and Vossen, 2015a,b) heavily rely on surface features, mainly event arguments (i.e., entities such as event participants, time, location, etc.) that were extracted from local contexts of two events, and determine that two events are coreferential if their arguments match. Often, a clustering algorithm, hierarchical Bayesian (Bejan and Harabagiu, 2010, 2014; Yang et al., 2015) or spectral clustering algorithms (Chen and Ji, 2009), is applied on top of a pairwise surf"
P18-1045,P17-2003,0,0.0115894,"g more complex and subtle cases, such as partial event coreference with incompatible arguments (Choubey and Huang, 2017a) and cases lacking informative local contexts. Consequently, many event coreference links were missing and the resulted event chains are fragmented. The low performance of event coreference resolution limited its uses in downstream applications. (?) shows that instead of human annotated event coreference relations, using system predicted relations resulted in a significant performance reduction in identifying the central event of a document. Moreover, the recent research by Moosavi and Strube (2017) found that the extensive use of lexical and surface features biases entity coreference resolvers towards seen mentions and do not generalize to unseen domains, and the finding can perfectly apply to event coreference resolution. Therefore, we propose to improve event coreference resolution by modeling correlations between event coreferCorrelations across Semantically Associated Event Chains: semantically associated events often co-occur in the same sentence. For example, mentions of the two main events “hearing” and “detention” co-occur across the document in sentences H, S1, S2 and S5. The c"
P18-1045,D12-1045,0,0.13743,"re extracted from local contexts of two events, and determine that two events are coreferential if their arguments match. Often, a clustering algorithm, hierarchical Bayesian (Bejan and Harabagiu, 2010, 2014; Yang et al., 2015) or spectral clustering algorithms (Chen and Ji, 2009), is applied on top of a pairwise surface feature based classifier for inducing event clusters. However, identifying potential arguments, linking arguments to a proper event mention, and recognizing compatibilities between arguments are all error-prone (Lu et al., 2016). Joint event and entity coreference resolution (Lee et al., 2012), joint inferences of event detection and event coreference resolution (Lu and Ng, 2017), and iterative information propagation (Liu et al., 2014; Choubey and Huang, 2017a) have been proposed to mitigate argument mismatch issues. However, such methods are incapable of handling more complex and subtle cases, such as partial event coreference with incompatible arguments (Choubey and Huang, 2017a) and cases lacking informative local contexts. Consequently, many event coreference links were missing and the resulted event chains are fragmented. The low performance of event coreference resolution li"
P18-1045,D17-1018,0,0.0631296,"nks between event mentions that appear in topic transition sentences by designing constraints in ILP and modifying the objective function. In addition, to avoid fragmented partial event chains and 486 ILP to discourage coreference links between a specific action event and other event mentions. recover complete chains for the main events, we also encourage associating more coreferent event mentions to a chain that has a large stretch (the number of sentences between the first and the last event mention based on their textual positions). 3 Related Work Compared to entity coreference resolution (Lee et al., 2017; Clark and Manning, 2016a,b; Martschat and Strube, 2015; Lee et al., 2013), far less research was conducted for event coreference resolution. Most existing methods (Ahn, 2006; Chen et al., 2009; Cybulska and Vossen, 2015a,b) heavily rely on surface features, mainly event arguments (i.e., entities such as event participants, time, location, etc.) that were extracted from local contexts of two events, and determine that two events are coreferential if their arguments match. Often, a clustering algorithm, hierarchical Bayesian (Bejan and Harabagiu, 2010, 2014; Yang et al., 2015) or spectral clus"
P18-1045,D14-1162,0,0.0895872,"he local context of words. The features include the embedding of word lemma, absolute difference between embeddings of word and its lemma, prefix and suffix of word and pos-tag and dependency relation of its context words, modifiers and governor. We trained 10 classifiers on same feature sets with slightly different neural network architectures and different training parameters including dropout rate, optimizer, learning rate, epochs and network initialization. All the classifiers use relu, tanh and softmax activations in the input, hidden and output layers respectively. We use GloVe vectors (Pennington et al., 2014) for word embeddings and one-hot vectors for pos-tag and dependency relations in each individual model. Postagging, dependency parsing, named entity recognition and entity coreference resolution are performed using Stanford CoreNLP (Manning et al., 2014) Table 2 shows the event mention identification results. We report the F1 score for event mention identification based on the KBP scorer, which considers a mention correct if its span, type and sub4 The ECB+ (Cybulska and Vossen, 2014) corpus is another commonly used dataset for evaluating event coreference resolution performance. But we determ"
P18-1045,liu-etal-2014-supervised,0,0.511414,"rithm, hierarchical Bayesian (Bejan and Harabagiu, 2010, 2014; Yang et al., 2015) or spectral clustering algorithms (Chen and Ji, 2009), is applied on top of a pairwise surface feature based classifier for inducing event clusters. However, identifying potential arguments, linking arguments to a proper event mention, and recognizing compatibilities between arguments are all error-prone (Lu et al., 2016). Joint event and entity coreference resolution (Lee et al., 2012), joint inferences of event detection and event coreference resolution (Lu and Ng, 2017), and iterative information propagation (Liu et al., 2014; Choubey and Huang, 2017a) have been proposed to mitigate argument mismatch issues. However, such methods are incapable of handling more complex and subtle cases, such as partial event coreference with incompatible arguments (Choubey and Huang, 2017a) and cases lacking informative local contexts. Consequently, many event coreference links were missing and the resulted event chains are fragmented. The low performance of event coreference resolution limited its uses in downstream applications. (?) shows that instead of human annotated event coreference relations, using system predicted relation"
P18-1045,P17-1009,0,0.662923,"ential if their arguments match. Often, a clustering algorithm, hierarchical Bayesian (Bejan and Harabagiu, 2010, 2014; Yang et al., 2015) or spectral clustering algorithms (Chen and Ji, 2009), is applied on top of a pairwise surface feature based classifier for inducing event clusters. However, identifying potential arguments, linking arguments to a proper event mention, and recognizing compatibilities between arguments are all error-prone (Lu et al., 2016). Joint event and entity coreference resolution (Lee et al., 2012), joint inferences of event detection and event coreference resolution (Lu and Ng, 2017), and iterative information propagation (Liu et al., 2014; Choubey and Huang, 2017a) have been proposed to mitigate argument mismatch issues. However, such methods are incapable of handling more complex and subtle cases, such as partial event coreference with incompatible arguments (Choubey and Huang, 2017a) and cases lacking informative local contexts. Consequently, many event coreference links were missing and the resulted event chains are fragmented. The low performance of event coreference resolution limited its uses in downstream applications. (?) shows that instead of human annotated eve"
P18-1045,W15-0812,0,0.030976,"Missing"
P18-1045,C16-1308,0,0.388131,"ntities such as event participants, time, location, etc.) that were extracted from local contexts of two events, and determine that two events are coreferential if their arguments match. Often, a clustering algorithm, hierarchical Bayesian (Bejan and Harabagiu, 2010, 2014; Yang et al., 2015) or spectral clustering algorithms (Chen and Ji, 2009), is applied on top of a pairwise surface feature based classifier for inducing event clusters. However, identifying potential arguments, linking arguments to a proper event mention, and recognizing compatibilities between arguments are all error-prone (Lu et al., 2016). Joint event and entity coreference resolution (Lee et al., 2012), joint inferences of event detection and event coreference resolution (Lu and Ng, 2017), and iterative information propagation (Liu et al., 2014; Choubey and Huang, 2017a) have been proposed to mitigate argument mismatch issues. However, such methods are incapable of handling more complex and subtle cases, such as partial event coreference with incompatible arguments (Choubey and Huang, 2017a) and cases lacking informative local contexts. Consequently, many event coreference links were missing and the resulted event chains are"
P18-1045,M95-1005,0,0.708903,"cess each document to define a distinct ILP problem which is solved using the PuLP library (Mitchell et al., 2011). 5 5.1 news articles from the official KBP 2016 and 2017 evaluation corpora7 respectively. For direct comparisons, the results reported for the baselines, including the previous state-of-the-art model, were based on news articles in the test datasets as well. We report the event coreference resolution results based on the version 1.8 of the official KBP 2017 scorer. The scorer employs four coreference scoring measures, namely B 3 (Bagga and Baldwin, 1998), CEAFe (Luo, 2005), MUC (Vilain et al., 1995) and BLANC (Recasens and Hovy, 2011) and the unweighted average of their F1 scores (AV GF 1 ). Evaluation Experimental Setup We trained our ILP system on the KBP 2015 (Ellis et al., 2015) English dataset and evaluated the system on KBP 2016 and KBP 2017 English datasets4 . All the KBP corpora include documents from both discussion forum5 and news articles. But as the goal of this study is to leverage discourse level topic structure in a document for improving event coreference resolution performance, we only evaluate the ILP system using regular documents (news articles) in the KBP corpora. Sp"
P18-1045,H05-1004,0,0.426927,"periments, we process each document to define a distinct ILP problem which is solved using the PuLP library (Mitchell et al., 2011). 5 5.1 news articles from the official KBP 2016 and 2017 evaluation corpora7 respectively. For direct comparisons, the results reported for the baselines, including the previous state-of-the-art model, were based on news articles in the test datasets as well. We report the event coreference resolution results based on the version 1.8 of the official KBP 2017 scorer. The scorer employs four coreference scoring measures, namely B 3 (Bagga and Baldwin, 1998), CEAFe (Luo, 2005), MUC (Vilain et al., 1995) and BLANC (Recasens and Hovy, 2011) and the unweighted average of their F1 scores (AV GF 1 ). Evaluation Experimental Setup We trained our ILP system on the KBP 2015 (Ellis et al., 2015) English dataset and evaluated the system on KBP 2016 and KBP 2017 English datasets4 . All the KBP corpora include documents from both discussion forum5 and news articles. But as the goal of this study is to leverage discourse level topic structure in a document for improving event coreference resolution performance, we only evaluate the ILP system using regular documents (news artic"
P18-1045,P14-5010,0,0.00315824,"10 classifiers on same feature sets with slightly different neural network architectures and different training parameters including dropout rate, optimizer, learning rate, epochs and network initialization. All the classifiers use relu, tanh and softmax activations in the input, hidden and output layers respectively. We use GloVe vectors (Pennington et al., 2014) for word embeddings and one-hot vectors for pos-tag and dependency relations in each individual model. Postagging, dependency parsing, named entity recognition and entity coreference resolution are performed using Stanford CoreNLP (Manning et al., 2014) Table 2 shows the event mention identification results. We report the F1 score for event mention identification based on the KBP scorer, which considers a mention correct if its span, type and sub4 The ECB+ (Cybulska and Vossen, 2014) corpus is another commonly used dataset for evaluating event coreference resolution performance. But we determined that this corpus is not appropriate for evaluating our ILP model that explicitly focuses on using discourse level topic structures for event coreference resolution. Particularly, the ECB+ corpus was created to facilitate both cross-document and indo"
P18-1045,Q15-1037,0,0.117552,"erence resolution (Lee et al., 2017; Clark and Manning, 2016a,b; Martschat and Strube, 2015; Lee et al., 2013), far less research was conducted for event coreference resolution. Most existing methods (Ahn, 2006; Chen et al., 2009; Cybulska and Vossen, 2015a,b) heavily rely on surface features, mainly event arguments (i.e., entities such as event participants, time, location, etc.) that were extracted from local contexts of two events, and determine that two events are coreferential if their arguments match. Often, a clustering algorithm, hierarchical Bayesian (Bejan and Harabagiu, 2010, 2014; Yang et al., 2015) or spectral clustering algorithms (Chen and Ji, 2009), is applied on top of a pairwise surface feature based classifier for inducing event clusters. However, identifying potential arguments, linking arguments to a proper event mention, and recognizing compatibilities between arguments are all error-prone (Lu et al., 2016). Joint event and entity coreference resolution (Lee et al., 2012), joint inferences of event detection and event coreference resolution (Lu and Ng, 2017), and iterative information propagation (Liu et al., 2014; Choubey and Huang, 2017a) have been proposed to mitigate argume"
P18-1045,Q15-1029,0,0.0210402,"c transition sentences by designing constraints in ILP and modifying the objective function. In addition, to avoid fragmented partial event chains and 486 ILP to discourage coreference links between a specific action event and other event mentions. recover complete chains for the main events, we also encourage associating more coreferent event mentions to a chain that has a large stretch (the number of sentences between the first and the last event mention based on their textual positions). 3 Related Work Compared to entity coreference resolution (Lee et al., 2017; Clark and Manning, 2016a,b; Martschat and Strube, 2015; Lee et al., 2013), far less research was conducted for event coreference resolution. Most existing methods (Ahn, 2006; Chen et al., 2009; Cybulska and Vossen, 2015a,b) heavily rely on surface features, mainly event arguments (i.e., entities such as event participants, time, location, etc.) that were extracted from local contexts of two events, and determine that two events are coreferential if their arguments match. Often, a clustering algorithm, hierarchical Bayesian (Bejan and Harabagiu, 2010, 2014; Yang et al., 2015) or spectral clustering algorithms (Chen and Ji, 2009), is applied on top"
P18-1050,J96-1002,0,0.321682,"Missing"
P18-1050,P09-1068,0,0.116975,"Missing"
P18-1050,P08-1090,0,0.100718,"Missing"
P18-1050,W04-3205,0,0.344533,"essed. Beth quickly showered deciding a bath would take too long. She changed into a pair of jeans, a tee shirt, and a sweater. Then, she grabbed her bag and left the cottage. Figure 1: Two narrative examples knowledge is lacking and difficult to obtain. Existing knowledge bases, such as Freebase (Bollacker et al., 2008) or Probase (Wu et al., 2012), often contain rich knowledge about entities, e.g., the birthplace of a person, but contain little event knowledge. Several approaches have been proposed to acquire temporal event knowledge from a text corpus, by either utilizing textual patterns (Chklovski and Pantel, 2004) or building a temporal relation identifier (Yao et al., 2017). However, most of these approaches are limited to identifying temporal relations within one sentence. Inspired by the double temporality characteristic of narrative texts, we propose a novel approach for acquiring rich temporal “before/after” event knowledge across sentences via identifying narrative stories. The double temporality states that a narrative story often describes a sequence of events following the chronological order and therefore, the temporal order of events matches with their textual order (Walsh, 2001; Riedl and Y"
P18-1050,D17-1190,1,0.88999,"Missing"
P18-1050,J13-4004,0,0.0224393,"thing” (Mani, 2012; Dictionary, 2007), we require the headword of the VP to be in the past tense. Furthermore, the subject of the sentence is meant to represent a character. Therefore, we specify 12 grammar rules4 to The Character Rule. A narrative usually has a protagonist character that appears in multiple sentences and ties a sequence of events, therefore, we also specify a rule requiring a narrative paragraph to have a protagonist character. Concretely, inspired by Eisenberg and Finlayson (2017), we applied the named entity recognizer (Finkel et al., 2005) and entity coreference resolver (Lee et al., 2013) from the CoreNLP toolkit (Manning et al., 2014) to identify the longest entity chain in a paragraph that has at least one mention recognized as a Person or Organization, or a gendered pronoun. Then we calculate the normalized length of this entity chain by dividing the number of entity mentions by the number of sentences in the paragraph. We require the normalized length of this longest 3 We manually identified 14 top-level sentence production rules, for example, “S → NP ADVP VP”, “S → PP , NP VP” and “S → S CC S”. Appendix shows all the rules. 4 The example NP rules include “NP → NNP”, “NP →"
P18-1050,D17-1287,0,0.317881,"pheral contents other than events and characters, including time, place, the emotional and psychological states of characters etc., which do not advance the plot but provide essential information to the interpretation of the events (Pentland, 1999). We use rich Linguistic Inquiry and Word Count (LIWC) (Pennebaker et al., 2015) features to capture a variety of textual devices used to describe such contents. et al., 2012) trained a narrative classifier using semantic triplet features on the CSC Islamic Extremist corpus. Our weakly supervised narrative identification method is closely related to Eisenberg and Finlayson (2017), which also explored the two key elements of narratives, the plot and the characters, in designing features with the goal of obtaining a generalizable story detector. But different from this work, our narrative identification method does not require any human annotations and can quickly adapt to new text sources. Temporal event knowledge acquisition is related to script learning (Chambers and Jurafsky, 2008), where a script consists of a sequence of events that are often temporally ordered and represent a typical scenario. However, most of the existing approaches on script learning (Chambers"
P18-1050,P14-5010,0,0.0293966,"quire the headword of the VP to be in the past tense. Furthermore, the subject of the sentence is meant to represent a character. Therefore, we specify 12 grammar rules4 to The Character Rule. A narrative usually has a protagonist character that appears in multiple sentences and ties a sequence of events, therefore, we also specify a rule requiring a narrative paragraph to have a protagonist character. Concretely, inspired by Eisenberg and Finlayson (2017), we applied the named entity recognizer (Finkel et al., 2005) and entity coreference resolver (Lee et al., 2013) from the CoreNLP toolkit (Manning et al., 2014) to identify the longest entity chain in a paragraph that has at least one mention recognized as a Person or Organization, or a gendered pronoun. Then we calculate the normalized length of this entity chain by dividing the number of entity mentions by the number of sentences in the paragraph. We require the normalized length of this longest 3 We manually identified 14 top-level sentence production rules, for example, “S → NP ADVP VP”, “S → PP , NP VP” and “S → S CC S”. Appendix shows all the rules. 4 The example NP rules include “NP → NNP”, “NP → NP CC NP” and “NP → DT NNP”. 540 EAR library9 ("
P18-1050,P05-1045,0,0.0171765,"events in someone’s life or in the development of something” (Mani, 2012; Dictionary, 2007), we require the headword of the VP to be in the past tense. Furthermore, the subject of the sentence is meant to represent a character. Therefore, we specify 12 grammar rules4 to The Character Rule. A narrative usually has a protagonist character that appears in multiple sentences and ties a sequence of events, therefore, we also specify a rule requiring a narrative paragraph to have a protagonist character. Concretely, inspired by Eisenberg and Finlayson (2017), we applied the named entity recognizer (Finkel et al., 2005) and entity coreference resolver (Lee et al., 2013) from the CoreNLP toolkit (Manning et al., 2014) to identify the longest entity chain in a paragraph that has at least one mention recognized as a Person or Organization, or a gendered pronoun. Then we calculate the normalized length of this entity chain by dividing the number of entity mentions by the number of sentences in the paragraph. We require the normalized length of this longest 3 We manually identified 14 top-level sentence production rules, for example, “S → NP ADVP VP”, “S → PP , NP VP” and “S → S CC S”. Appendix shows all the rule"
P18-1050,P09-1045,0,0.0105547,"aragraphs in a document. 7 This value is half of the corresponding thresshold used for identifying seed narrative paragraphs. 8 This value is half of the corresponding thresshold used for identifying seed narrative paragraphs. 9 https://www.csie.ntu.edu.tw/˜cjlin/ liblinear/ 541 News Novels Blogs Sum 0 (Seeds) 20k 75k 6k 101k 1 40k 82k 10k 132k 2 12k 24k 3k 39k 3 5k 6k 1k 12k 4 1k 2k 3k Total 78k 189k 20k 287k the Stanford CoreNLP tools (Manning et al., 2014) to the three text corpora to obtain POS tags, parse trees, named entities, coreference chains, etc. In order to combat semantic drifts (McIntosh and Curran, 2009) in bootstrapping learning, we set the initial selection confidence score produced by the statistical classifier at 0.5 and increase it by 0.05 after each iteration. The bootstrapping system runs for four iterations and learns 287k narrative paragraphs in total. Table 1 shows the number of narratives that were obtained in the seeding stage and in each bootstrapping iteration from each text corpus. Table 1: Number of new narratives generated after each bootstrapping iteration ing paragraph. We use 6 normalized lengths (3 from the target paragraph 10 and 3 from surrounding paragraphs) as feature"
P18-1050,L16-1555,0,0.0328554,"Missing"
P18-1050,W12-3018,0,0.0506178,"Missing"
P18-1050,D13-1036,0,0.0215579,"nd, temporal event 537 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 537–547 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics Then by leveraging the double temporality characteristic of narrative paragraphs, we distill general temporal event knowledge. Specifically, we extract event pairs as well as longer event sequences consisting of strongly associated events that often appear in a particular textual order in narrative paragraphs, by calculating Causal Potential (Beamer and Girju, 2009; Hu et al., 2013) between events. Specifically, we obtained 19k event pairs and 25k event sequences with three to five events from the 287k narrative paragraphs we identified across three genres, news articles, novel books and blogs. Our evaluation shows that both the automatically identified narrative paragraphs and the extracted event knowledge are of high quality. Furthermore, the learned temporal event knowledge is shown to yield additional performance gains when used for temporal relation identification and the Narrative Cloze task. The acquired event temporal knowledge and the knowledge acquisition syste"
P18-1050,W17-5540,0,0.0179171,"s, formally, P (ei ,ej ) PC(ei ) pmi(ei , ej ) = log P (ei )P C(ex ) (ej ) , P (ei ) = C(ei ,ej ) P P , x y C(ex ,ey ) where, x and y refer to all the events in a corpus, C(ei ) is the number of occurrences of ei , C(ei , ej ) is the number of co-occurrences of ei and ej . While each candidate pair of events should have appeared consecutively as a segment in at least one narrative paragraph, when calculating the CP score, we consider event co-occurrences even when two events are not consecutive in a narrative paragraph but have one or two other events in between. Specifically, the same as in (Hu and Walker, 2017), we calculate separate CP scores based on event co-occurrences with zero (consecutive), one or two events in between, and use the weighted average CP score for ranking an event P cp (e ,e ) pair, formally, CP (ei , ej ) = 3d=1 d di j . Then we rank longer event sequences based on CP scores for individual event pairs that are included in an event sequence. However, an event sequence of length n is more than n − 1 event pairs with any two consecutive events as a pair. We prefer event sequences that are coherent overall, where the events that are one or two events away are highly related as well"
P18-1050,P10-1100,0,0.064256,"Missing"
P18-1050,D17-1006,0,0.118079,"Missing"
P18-1050,yao-etal-2017-weakly,1,0.799552,"anged into a pair of jeans, a tee shirt, and a sweater. Then, she grabbed her bag and left the cottage. Figure 1: Two narrative examples knowledge is lacking and difficult to obtain. Existing knowledge bases, such as Freebase (Bollacker et al., 2008) or Probase (Wu et al., 2012), often contain rich knowledge about entities, e.g., the birthplace of a person, but contain little event knowledge. Several approaches have been proposed to acquire temporal event knowledge from a text corpus, by either utilizing textual patterns (Chklovski and Pantel, 2004) or building a temporal relation identifier (Yao et al., 2017). However, most of these approaches are limited to identifying temporal relations within one sentence. Inspired by the double temporality characteristic of narrative texts, we propose a novel approach for acquiring rich temporal “before/after” event knowledge across sentences via identifying narrative stories. The double temporality states that a narrative story often describes a sequence of events following the chronological order and therefore, the temporal order of events matches with their textual order (Walsh, 2001; Riedl and Young, 2010; Grabes, 2013). Therefore, we can easily distill te"
P18-1050,E12-1034,0,\N,Missing
W16-3911,N13-1005,1,0.726294,"while using empirical thresholds to monitor quality of extraction (Riloff et al., 1999). We then proceed to cluster these indicators based on their contextual similarities, which yield some naturally occurring categories of event triggers as seen in Table 2. Our approach is able to outperform naive keyword matching even without having any prior knowledge or hand labeled data, other than a single seed word. Furthermore, our indicators were able to identify tweets that did not contain a single event keyword. 2 Prior Work Our research closely follows the multi-faceted event recognition approach (Huang and Riloff, 2013) for news articles which suggests that event facets, namely agents and purposes, supplement event expressions. However, event indicators in tweets are loosely defined and lack phrasal dependencies, hence posing a more challenging problem. 71 Raw Tweet Stream Seed Phrases Implicit Event Indicators string match Event Relevant Tweets n-grams frequency, specificity check Figure 1: The bootstrapping process iteratively adds event relevant tweets and implicit event indicators. First, in contrast to event facets, event indicators are loosely defined event properties and features which cover a broader"
W16-3911,D14-1162,0,0.0801179,"building a collection of event indicators, we attempt to identify contextual similarities, i.e how each phrase acts as a cue for an event, as discussed in Table 2. For each phrase, we accumulate its contexts using a window of 5 words on either side of each tweet in which the phrase occurs, as shown in Figure 2. Next, we build a feature vector to represent the context words of each indicator using two approaches (1) frequency bag-of-words (2) sum of word embeddings (“tweet-token embedding” to be precise) while clipping stop words and infrequent words. We use the 200-dimensional global vectors (Pennington et al., 2014), pre-trained on 2 billion tweets, covering over 27-billion tokens. We then perform clustering using the affinity-propagation algorithm (Frey and Dueck, 2007). 4 Evaluation We evaluate the quality of tweets identified by using the automatically learned event indicators over the successive iterations of the bootstrapping phase, as described in Section 3. Table 4 shows the number of event indicator phrases and civil unrest tweets that were collected after each iteration of bootstrapping. Table 5 shows examples of correct and incorrect tweets. To measure the relevance of the tweets as identified"
W16-3911,N10-1021,0,0.129337,"Missing"
W16-3911,D11-1141,0,0.0107909,"semantic types of the learned event indicators. “#RiseUpOctober” can be used to accurately identify tweets describing the October New York protest in 2015. However, this approach is heavily biased towards already known trending events that are more likely to be “hashtagged”, and is often not reliable even to capture tweets referring to a single event. Tweets belonging to a particular event domain (eg. civil unrest, disaster, presidential election) can be identified by learning various kinds of event indicators or sensors across contexts. In spite of their highly informal and ambiguous nature (Ritter et al., 2011), tweets often mention multiple event characteristic properties and features that act as implicit event cues. An event indicator is defined as any word or phrase that can act as reliable evidence towards detecting an event mention in text. A strong indicator is one that is almost exclusively relevant to the event under consideration (eg. touchdown is strong evidence of a sports event). A weak indicator tends to occur in more generic textual contexts and is therefore less useful. We also observe that these event indicators can be categorized into sub-classes, depending on how they influence the"
W18-4308,J92-4003,0,0.719566,"Missing"
W18-4308,P14-5010,0,0.00378864,"et al., 2015) dictionary and the feature value is the occurrences of all words in that category. These LIWC features capture presences of certain types of words, such as words denoting 5 We only count news elements that were annotated with exactly the same paragraph boundaries and the same news element type. 6 Note that the bottom level syntactic production rules have the form of POS tag → WORD and contain a lexical word, which made these rules dependent on specific contexts. Therefore, we exclude these bottom level production rules to obtain more general features. 7 We used Stanford CoreNLP (Manning et al., 2014) to generate constituency-based parse trees for each sentence. 64 relativity (e.g., motion, time, space), which were reported effective for detecting narrative stories (Yao and Huang, 2018). Key Event Placement (KEP) Features: Note that only the Inverted Pyramid and Martini Glass structures start with a Standard Lede, which introduces key events directly and may repeat key events and associated event attributes (e.g., character, time and location) that were mentioned in the title as well. Therefore, we design a simple feature representing the number of words in overlap8 between the first parag"
W18-4308,J00-4001,0,0.30141,"es and train a Support Vector Machine (SVM) (Cortes and Vapnik, 1995) classifier to label each news article with one of the proposed news structures. Experimental results show that reasonable performance can be achieved for automatic structure-based news genre classification by using our structure indicative features, even though results on minority classes remain low. 2 Related Work The previous works on automated text categorization have considered various dimensions for categorization, such as topic (Kazawa et al., 2005; Zhou et al., 2009), style (Argamon-Engelson et al., 1998) and author (Stamatatos et al., 2000). Although news structures have been extensively studied in linguistics and journalism (Schokkenbroek, 1999; Van Dijk, 1985; Ytreberg, 2001), there are few studies trying to categorize a news article based on its content organization structure and there is no published dataset for developing such data-driven methods. To the best of our knowledge, we are the first to consider categorizing news articles according to news structures. Our main contributions include defining news elements and news structures, creating the first dataset for news structure identification as well as identifying news s"
W18-4308,P18-1050,1,0.841331,"only count news elements that were annotated with exactly the same paragraph boundaries and the same news element type. 6 Note that the bottom level syntactic production rules have the form of POS tag → WORD and contain a lexical word, which made these rules dependent on specific contexts. Therefore, we exclude these bottom level production rules to obtain more general features. 7 We used Stanford CoreNLP (Manning et al., 2014) to generate constituency-based parse trees for each sentence. 64 relativity (e.g., motion, time, space), which were reported effective for detecting narrative stories (Yao and Huang, 2018). Key Event Placement (KEP) Features: Note that only the Inverted Pyramid and Martini Glass structures start with a Standard Lede, which introduces key events directly and may repeat key events and associated event attributes (e.g., character, time and location) that were mentioned in the title as well. Therefore, we design a simple feature representing the number of words in overlap8 between the first paragraph and news title. 4.2 Experimental Results Feature Sets Unigrams Bigrams Unigrams + Bigrams + Writing Style + KEP Features + Both IP 71.6/85.1/77.8 71.6/87.1/78.6 72.5/87.5/79.3 73.4/85."
yao-etal-2017-online,levy-andrew-2006-tregex,0,\N,Missing
yao-etal-2017-online,J96-1002,0,\N,Missing
yao-etal-2017-online,J92-4003,0,\N,Missing
yao-etal-2017-online,P12-2034,0,\N,Missing
yao-etal-2017-online,N13-1053,0,\N,Missing
yao-etal-2017-online,P14-1147,0,\N,Missing
yao-etal-2017-online,D16-1187,0,\N,Missing
