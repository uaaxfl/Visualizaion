2007.sigdial-1.33,W04-2319,0,\N,Missing
2007.sigdial-1.38,C02-1147,0,0.0270829,"ugen et al., 2004). The recognizer grammar also provides a parse for interpreting the utterance. It is a contextfree grammar enhanced by information from the ontology defining all the objects, tasks and properties about which the user can talk. The parse tree is converted into a semantic representation and added to the current discourse. The semantic representation consists of the speech act and the objects/properties expressed within the user utterance. For dialog management, we use the TAPAS dialog tools (Holzapfel, 2005) based on the language- and domain-independent dialog manager ARIADNE (Denecke, 2002), which uses typed feature structures to represent semantic input and discourse information. If all the information necessary to accomplish a goal is available in discourse, the dialog system calls the corresponding service. Otherwise, clarification questions are generated using a template-based approach. 4 Mixed Initiative Dialog Management Our strategy is to try distinguish between problems due to system errors vs. human inexperience, using different indicators of possible communication problems and a separate problem state model with problem-sensitive response generation, as described next."
2021.eacl-main.253,2020.findings-emnlp.91,0,0.174899,"e (if anywhere). In addition, very often information presented in tables is compact and abbreviated. The associated text can potentially provide rich context that can be used to enhance the representation of the table for more robust question answering. The main focus of this paper is to investigate how to improve question answering on documents that contain both text and tables. While recently there has been a lot of interest in reading comprehension for both text and tables, little research has been done in combining the two sources of information. The only prior study we are aware of is by Chen et al. (2020) who introduced a new dataset for multi-hop QA over tabular and textual data. In their work, the authors heavily rely on the assumption that the questions would be unanswerable if either text or table information is missing. Here we investigate a more realistic scenario of naturally occurring questions, where the answer can be found in either text, tables, both, or none. We evaluate our approach on the Natural Questions corpus (Kwiatkowski et al., 2019) which consists of real anonymized queries issued to the Google search engine and corresponding Wikipedia articles, simulating a real use case"
2021.eacl-main.253,P19-1285,0,0.0155666,"on of the table for QA. Recent work has explored building encoders over large input sequences. Ravula et al. (2020) scaled input sequence length to more than 8,000 tokens for the NQ dataset. However, to make the model efficient, encodings of individual text or table segments communicate through single-vector global memories. Here, we take the approach of using asymmetric attention from table token representations to a small number of relevant text token representations, that are precomputed independently. Our approach is more similar to the handling of prior segment context in Transformer-XL (Dai et al., 2019), but relevant context is selected based on word overlap and not contiguity. The two components of our approach, described next, include the definition of relevant text context for table elements and the mechanism for using contextualized embeddings of the relevant text to enrich the table token representations. 3.2.1 Table-Textual Context Linking Let a table cell that contains a sequence of input tokens be defined as (ut0 . . . utK ), with the corresponding s sub-word units (wordpiecies in BERT or byte-pair encodings in RoBERTa) for the k-th t,k word to be defined as (xt,k 0 . . . xSk ), and"
2021.eacl-main.253,N19-1423,1,0.0923115,"uestions, where the answer can be found in either text, tables, both, or none. We evaluate our approach on the Natural Questions corpus (Kwiatkowski et al., 2019) which consists of real anonymized queries issued to the Google search engine and corresponding Wikipedia articles, simulating a real use case of such a system. Prior work on the Natural Questions dataset has treated text and tables uniformly, linearizing tables and representing them and text segments using the same contextual token representations (for example, starting from pre-trained transformers (Vaswani et al., 2017) like BERT (Devlin et al., 2019)). However, representations developed for text are sub-optimal for tables, since they do not account for the special relationships between table cells, defined by the row and column structure. In this work, we extend the BERT architecture to account for inter-cell relationships in tables. This approach is motivated by Graph Neural Networks with a transformer (Shaw et al., 2018) and is 2895 Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics, pages 2895–2906 April 19 - 23, 2021. ©2021 Association for Computational Linguistics closely relat"
2021.eacl-main.253,2020.acl-main.210,0,0.0221193,"pretraining method are comparable to those in recent and concurrent work. Leveraging tables is a hard problem. However, most studies on table-based QA omit an important additional information source: the text in the article discussing the table. Prior attempts at integrating a KB and text use early fusion of document text and KG information (Sun et al., 2018), where they integrate text and a KG sub-graph in a single graph, from which an entity is selected to answer the question. Structured KGs are often easier to interpret than tables, which have a wide variety of possible schemas. InfoTabS (Gupta et al., 2020) introduced a dataset for the natural language inference task based on premises that are tables, where the authors explore multiple table representations, including a key-value approach and linearized representations with table rows corresponding to ”sentences.” Hypothesis representations are calculated separately. Recently, TaBERT (Yin et al., 2020) introduced a joint table-utterance representation approach, where a table row is concatenated with a short text utterance, such as the query in question answering, and passed as an input to a BERT-based model. Such an approach relies on the initia"
2021.eacl-main.253,2020.acl-main.398,0,0.0446239,"Missing"
2021.eacl-main.253,D17-1160,0,0.0266503,"s. Finally, this is the first model that uses the Natural Questions corpus for question answering on tables, improving the baseline in Alberti et al. (2019) that does not distinguish between tables and text. 2 Related Work Most work on QA with tables prior to BERT involves first converting the table to a Knowledge Graph (KG) where cell entries are entities with row/column relations, then using entity linking to identify spans in the question that match an entity in the knowledge graph, and finally parsing the question to generate a SQL query using some variant of a sequence-to-sequence model (Krishnamurthy et al., 2017). Due to the advances in contextualized word embeddings, more recent work proposed a modification of the BERT transformer architecture to be used for representing tables. Hwang et al. (2019) proposed the usage of additional [SEP] tokens between headers of the table to make a BERT model more suitable for the tables. Recently, Yin et al. (2020) introduced a pretraining procedure for joint representation of tabular data paired with an utterance, where the approach is to linearize the structure of tables to be compatible with a BERT model. Our approach for table encoding is most similar to that of"
2021.eacl-main.253,Q19-1026,1,0.813839,"nsion for both text and tables, little research has been done in combining the two sources of information. The only prior study we are aware of is by Chen et al. (2020) who introduced a new dataset for multi-hop QA over tabular and textual data. In their work, the authors heavily rely on the assumption that the questions would be unanswerable if either text or table information is missing. Here we investigate a more realistic scenario of naturally occurring questions, where the answer can be found in either text, tables, both, or none. We evaluate our approach on the Natural Questions corpus (Kwiatkowski et al., 2019) which consists of real anonymized queries issued to the Google search engine and corresponding Wikipedia articles, simulating a real use case of such a system. Prior work on the Natural Questions dataset has treated text and tables uniformly, linearizing tables and representing them and text segments using the same contextual token representations (for example, starting from pre-trained transformers (Vaswani et al., 2017) like BERT (Devlin et al., 2019)). However, representations developed for text are sub-optimal for tables, since they do not account for the special relationships between tab"
2021.eacl-main.253,2020.acl-main.604,0,0.074858,"select the table rows most relevant to the query. In contrast, we enrich the table representation using an attention mechanism with the representations of the most relevant parts of the context of the article in which the table appears. The Natural Questions is a large corpus that contains real user queries along with their corresponding Wikipedia articles, which may or may not contain an answer anywhere in the article. Alberti et al. (2019) provided a BERT-based baseline that treats both table and text segments like text: a sequence of tokens with word and position embeddings. Re2896 cently, Liu et al. (2020) improved this baseline by using dynamic dual-attention over paragraphs and cascade answer predictor. In another direction, Ravula et al. (2020) used an extended transformer architecture that models extra-long documents with limited propagation of information among different segments. All three approaches did not distinguish between text and table input, treating tables as text while not taking into account table structure. To the best of our knowledge, this is the first work that focuses on table-based QA for the real user queries in the Natural Questions corpus, and shows that table-based an"
2021.eacl-main.253,D19-1603,0,0.0226677,"Missing"
2021.eacl-main.253,P15-1142,0,0.0807757,"Missing"
2021.eacl-main.253,P19-1010,0,0.0345868,"Missing"
2021.eacl-main.253,N18-2074,0,0.386156,"t and tables uniformly, linearizing tables and representing them and text segments using the same contextual token representations (for example, starting from pre-trained transformers (Vaswani et al., 2017) like BERT (Devlin et al., 2019)). However, representations developed for text are sub-optimal for tables, since they do not account for the special relationships between table cells, defined by the row and column structure. In this work, we extend the BERT architecture to account for inter-cell relationships in tables. This approach is motivated by Graph Neural Networks with a transformer (Shaw et al., 2018) and is 2895 Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics, pages 2895–2906 April 19 - 23, 2021. ©2021 Association for Computational Linguistics closely related to the one in M¨ueller et al. (2019). In our work, we pretrain parameters for the new relationships using a large table corpus extracted from Wikipedia (Bhagavatula et al., 2013). In addition, we present a novel approach that refines table representations by attending to related representations of text in the surrounding article. This allows information to propagate from the"
2021.eacl-main.253,D18-1455,0,0.0246856,"table representation learning, TaPaS (Herzig et al., 2020) and GraPPa (Yu et al., 2020), also use pretraining on the Wikitables dataset (Bhagavatula et al., 2013) that we use in our work. Therefore, our table representations based on transformers and our pretraining method are comparable to those in recent and concurrent work. Leveraging tables is a hard problem. However, most studies on table-based QA omit an important additional information source: the text in the article discussing the table. Prior attempts at integrating a KB and text use early fusion of document text and KG information (Sun et al., 2018), where they integrate text and a KG sub-graph in a single graph, from which an entity is selected to answer the question. Structured KGs are often easier to interpret than tables, which have a wide variety of possible schemas. InfoTabS (Gupta et al., 2020) introduced a dataset for the natural language inference task based on premises that are tables, where the authors explore multiple table representations, including a key-value approach and linearized representations with table rows corresponding to ”sentences.” Hypothesis representations are calculated separately. Recently, TaBERT (Yin et a"
2021.eacl-main.253,2020.acl-main.745,0,0.088425,"ties with row/column relations, then using entity linking to identify spans in the question that match an entity in the knowledge graph, and finally parsing the question to generate a SQL query using some variant of a sequence-to-sequence model (Krishnamurthy et al., 2017). Due to the advances in contextualized word embeddings, more recent work proposed a modification of the BERT transformer architecture to be used for representing tables. Hwang et al. (2019) proposed the usage of additional [SEP] tokens between headers of the table to make a BERT model more suitable for the tables. Recently, Yin et al. (2020) introduced a pretraining procedure for joint representation of tabular data paired with an utterance, where the approach is to linearize the structure of tables to be compatible with a BERT model. Our approach for table encoding is most similar to that of M¨ueller et al. (2019), where the authors generalized the BERT architecture similarly to Shaw et al. (2018) with new types of relations to encode table-specific relationships. The main differences between our table representation and M¨ueller et al. (2019) is that in our representation we use 5 types of relations, cell-column, cell-row, in-c"
2021.emnlp-main.140,P17-1171,0,0.0264896,"directly optimizing for knowledge extraction helps resolve complex user queries (Feng et al., 2020) and increases user engagement in social chat (Dinan et al., 2019; Moghe et al., 2018). For long documents, explicit knowledge identification can also be useful for model interpretability and human-in-the-loop assistant scenarios. Following (Feng et al., 2020), we define knowledge identification as the task of locating knowledge in a long document that is relevant to the current user query given the conversation context (Figure 1). Knowledge identification is similar to open question answering (Chen et al., 2017; Min ∗ Equal contribution et al., 2019), the task of answering a factoid question given a large grounding, except that it is not an interactive setting like dialogues. With the assumption of a long grounding document, our task differs from prior work in conversational question answering (Choi et al., 2018; Reddy et al., 2019), which focuses on answering a sequence of factoid questions about a short text snippet. Additionally, real user information needs can involve conversations with diverse forms of user queries and dialogue acts (e.g., asking for user preference, etc) as shown in Figure 1."
2021.emnlp-main.140,2021.naacl-main.85,0,0.016939,"istory turns becomes: Lnext = Lpsg + Lbegin + Lend . Lhist = Lhpsg + Lhbegin + Lhend . Training Objectives Eq. (1-3) show objective functions of knowledge passage Lpsg , begin Lbegin and end Lend span predictions. q(.)t denotes the t-th index of the vector resulting from the softmax ˆ ˆb and eˆ correspond to function. The variables k, the gold passage, begin and end span indices. Lpsg = − log q(Wp Z)kˆ ˙ ˆ Lbegin = − log q(Wb S) (1) b (2) ˙ eˆ Lend = − log q(We S) (3) 1855 3.3 Training and Inference Posterior Regularization During training, we incorporate a posterior regularization mechanism (Cheng et al., 2021) to enhance the model’s robustness to domain shift. Specifically, we add an additional adversarial training loss as below. Div is some f-divergence.2 Let x be the encoded X (defined in Section 3.1.1) after the BERT word embedding layer, D IALKI outputs fpsg (x), fbegin (x) and fend (x) as the next turn passage, begin and end knowledge span logits (the results before the softmax function q(.) in Eq. (1-3)) respectively. X  Ladv = max Div f (x)||f (x+) kk≤a f ∈{fpsg ,fbegin ,fend } The above loss function essentially regularizes the g-based worst-case posterior difference between the clean an"
2021.emnlp-main.140,2020.acl-main.501,0,0.107902,"and dialogue acts (e.g., asking for user preference, etc) as shown in Figure 1. Previous work in knowledge identification encode the grounding document as a single string (Feng et al., 2020), or splitting it into isolated sentences (Dinan et al., 2019; Kim et al., 2020; Zheng et al., 2020) which potentially loses important discourse context. In this paper, we introduce D IALKI to address knowledge identification in conversational systems with long grounding documents. In contrast to previous work, D IALKI extends multi-passage reader models in open question answering (Karpukhin et al., 2020; Cheng et al., 2020) to obtain dense 1852 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 1852–1863 c November 7–11, 2021. 2021 Association for Computational Linguistics encodings of different spans in multiple passages in the grounding document, and it contextualizes them with the dialogue history. Specifically, D I ALKI extracts knowledge given a long document by dividing it into paragraphs or sections and individually contextualizes them with dialogue context. It then extracts knowledge by first selecting the most relevant passage to the dialogue context and then s"
2021.emnlp-main.140,D18-1241,0,0.0217273,"op assistant scenarios. Following (Feng et al., 2020), we define knowledge identification as the task of locating knowledge in a long document that is relevant to the current user query given the conversation context (Figure 1). Knowledge identification is similar to open question answering (Chen et al., 2017; Min ∗ Equal contribution et al., 2019), the task of answering a factoid question given a large grounding, except that it is not an interactive setting like dialogues. With the assumption of a long grounding document, our task differs from prior work in conversational question answering (Choi et al., 2018; Reddy et al., 2019), which focuses on answering a sequence of factoid questions about a short text snippet. Additionally, real user information needs can involve conversations with diverse forms of user queries and dialogue acts (e.g., asking for user preference, etc) as shown in Figure 1. Previous work in knowledge identification encode the grounding document as a single string (Feng et al., 2020), or splitting it into isolated sentences (Dinan et al., 2019; Kim et al., 2020; Zheng et al., 2020) which potentially loses important discourse context. In this paper, we introduce D IALKI to addr"
2021.emnlp-main.140,N19-1423,0,0.281929,"ges D = {p1 , p2 , . . . , p|D |} based on paragraphs or sections. Each passage p consists of a sequence of semantic units p = (s1 , s2 , . . . , sl ), where each semantic unit (SU) can be either a token or a span or a sentence depending on how the document is segmented. For simplicity, we use “span” as the semantic unit in this section to describe our model. Method overview In this section, we introduce D IALKI, a multi-task learning model for knowledge identification as illustrated in Figure 2. We first introduce how we obtain dialogue utterance and knowledge span representations from BERT (Devlin et al., 2019) and a span-level knowledge contextualization mechanism (Section 3.1). These representations are then used for knowledge identification in our multi-task learning framework, which includes the main task of next-turn knowledge identification and an auxiliary task of history knowledge prediction applied during training only (Section 3.2). Finally, we describe our joint training objective and inference details (Section 3.3). 3.1 Encoding Dialog Context and Knowledge the concatenated sequence. More formally, the model input X for a passage p of length l becomes: X = [cls][usr] u1 [agt]u2 · · · [us"
2021.emnlp-main.140,2020.emnlp-main.652,0,0.061473,"Missing"
2021.emnlp-main.140,2020.emnlp-main.550,0,0.0708781,"se forms of user queries and dialogue acts (e.g., asking for user preference, etc) as shown in Figure 1. Previous work in knowledge identification encode the grounding document as a single string (Feng et al., 2020), or splitting it into isolated sentences (Dinan et al., 2019; Kim et al., 2020; Zheng et al., 2020) which potentially loses important discourse context. In this paper, we introduce D IALKI to address knowledge identification in conversational systems with long grounding documents. In contrast to previous work, D IALKI extends multi-passage reader models in open question answering (Karpukhin et al., 2020; Cheng et al., 2020) to obtain dense 1852 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 1852–1863 c November 7–11, 2021. 2021 Association for Computational Linguistics encodings of different spans in multiple passages in the grounding document, and it contextualizes them with the dialogue history. Specifically, D I ALKI extracts knowledge given a long document by dividing it into paragraphs or sections and individually contextualizes them with dialogue context. It then extracts knowledge by first selecting the most relevant passage to the dialog"
2021.emnlp-main.140,2020.acl-main.703,0,0.0282365,"Missing"
2021.emnlp-main.140,P19-1002,0,0.0302065,"Missing"
2021.emnlp-main.140,2020.acl-main.6,0,0.0997061,"Missing"
2021.emnlp-main.140,2021.ccl-1.108,0,0.0434227,"Missing"
2021.emnlp-main.140,D18-1255,0,0.158255,"ded response generation, where knowledge is represented in written documents. Most prior work has explored architectures for knowledge grounding in an end-to-end framework, optimizing a loss function targeting response generation (Ghazvininejad et al., 2018; Zhou et al., 2018; Yavuz et al., 2019). However, the knowledge needed at any one turn in the dialogue is typically localized in the document, and some studies have shown that directly optimizing for knowledge extraction helps resolve complex user queries (Feng et al., 2020) and increases user engagement in social chat (Dinan et al., 2019; Moghe et al., 2018). For long documents, explicit knowledge identification can also be useful for model interpretability and human-in-the-loop assistant scenarios. Following (Feng et al., 2020), we define knowledge identification as the task of locating knowledge in a long document that is relevant to the current user query given the conversation context (Figure 1). Knowledge identification is similar to open question answering (Chen et al., 2017; Min ∗ Equal contribution et al., 2019), the task of answering a factoid question given a large grounding, except that it is not an interactive setting like dialogues."
2021.emnlp-main.140,2020.acl-main.396,0,0.0545627,"Missing"
2021.emnlp-main.140,W18-6319,0,0.0123671,"given the concatenated dialogue context and grounding knowledge (e.g., document or predicted knowledge string) as the input. BART is also used as the baseline for the agent response generation task on Doc2Dial (Feng et al., 2020),8 where the model is given the dialogue history concatenated with the full document to decode the next agent response. We conduct experiments on the same model architecture, with the knowledge input being the predicted knowledge string or passage. Without changing the model at all, using knowledge predicted by D IALKI leads to almost 3 points in the sacrebleu score (Post, 2018), as shown in Table 4. Examples of generated responses are shown in Table 5. Passage Identification Accuracy We map predicted knowledge strings back to the passages and calculate the passage-level accuracy. Table 6 shows that D IALKI outperforms baseline models in locating the passage containing the knowledge string. Notably, our models generalize well in passage prediction to unseen documents or dialogue topics. Similarity Between Global and History Turn Representations The dot product between z (the encoding of the whole input sequence) and each history utterance representation ui (sigmoid n"
2021.emnlp-main.140,Q19-1016,0,0.102423,"ios. Following (Feng et al., 2020), we define knowledge identification as the task of locating knowledge in a long document that is relevant to the current user query given the conversation context (Figure 1). Knowledge identification is similar to open question answering (Chen et al., 2017; Min ∗ Equal contribution et al., 2019), the task of answering a factoid question given a large grounding, except that it is not an interactive setting like dialogues. With the assumption of a long grounding document, our task differs from prior work in conversational question answering (Choi et al., 2018; Reddy et al., 2019), which focuses on answering a sequence of factoid questions about a short text snippet. Additionally, real user information needs can involve conversations with diverse forms of user queries and dialogue acts (e.g., asking for user preference, etc) as shown in Figure 1. Previous work in knowledge identification encode the grounding document as a single string (Feng et al., 2020), or splitting it into isolated sentences (Dinan et al., 2019; Kim et al., 2020; Zheng et al., 2020) which potentially loses important discourse context. In this paper, we introduce D IALKI to address knowledge identif"
2021.emnlp-main.140,D18-1233,0,0.0188621,"nd evaluate on the 2 Related Work knowledge identification task. Feng et al. (2020) Conversational Question Answering. Existing encodes each long document as a single string withconversational question answering tasks (Choi out leveraging document structures, while Dinan et al., 2018; Reddy et al., 2019) are generally de- et al. (2019); Lian et al. (2019); Kim et al. (2020); fined as the task of reading a short text passage and Zheng et al. (2020) separately encode sentences answering a series of interconnected questions in in documents, which may have strong contextual a conversation. ShARC (Saeidi et al., 2018) is a dependencies among each other. Our model leverconversational machine reading dataset to address ages document structures and divides each docuunder-specified questions by requiring agents to ment into multiple passages to process. Similar to 1 our model, Kim et al. (2020); Zheng et al. (2020) We release our source code for experiments at https: //github.com/ellenmellon/DIALKI. incorporate previously used knowledge, but they 1853 use a single vector to sequentially track the state of the used knowledge. Instead, we apply a multi-task learning framework to model relations between grounding"
2021.emnlp-main.140,W19-5917,0,0.0239019,"V in Albany, … • When the inspector visits your location, … Figure 1: In a document-grounded conversation, knowledge identification targets to locate a knowledge string within a long document to assist the agent in addressing the current user query. Introduction Many conversational agent scenarios require knowledge-grounded response generation, where knowledge is represented in written documents. Most prior work has explored architectures for knowledge grounding in an end-to-end framework, optimizing a loss function targeting response generation (Ghazvininejad et al., 2018; Zhou et al., 2018; Yavuz et al., 2019). However, the knowledge needed at any one turn in the dialogue is typically localized in the document, and some studies have shown that directly optimizing for knowledge extraction helps resolve complex user queries (Feng et al., 2020) and increases user engagement in social chat (Dinan et al., 2019; Moghe et al., 2018). For long documents, explicit knowledge identification can also be useful for model interpretability and human-in-the-loop assistant scenarios. Following (Feng et al., 2020), we define knowledge identification as the task of locating knowledge in a long document that is releva"
2021.emnlp-main.140,P18-1205,0,0.0606293,"Missing"
2021.emnlp-main.140,2020.emnlp-main.272,0,0.0575001,"Missing"
2021.emnlp-main.140,2020.findings-emnlp.11,0,0.265178,"a long grounding document, our task differs from prior work in conversational question answering (Choi et al., 2018; Reddy et al., 2019), which focuses on answering a sequence of factoid questions about a short text snippet. Additionally, real user information needs can involve conversations with diverse forms of user queries and dialogue acts (e.g., asking for user preference, etc) as shown in Figure 1. Previous work in knowledge identification encode the grounding document as a single string (Feng et al., 2020), or splitting it into isolated sentences (Dinan et al., 2019; Kim et al., 2020; Zheng et al., 2020) which potentially loses important discourse context. In this paper, we introduce D IALKI to address knowledge identification in conversational systems with long grounding documents. In contrast to previous work, D IALKI extends multi-passage reader models in open question answering (Karpukhin et al., 2020; Cheng et al., 2020) to obtain dense 1852 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 1852–1863 c November 7–11, 2021. 2021 Association for Computational Linguistics encodings of different spans in multiple passages in the grounding document,"
2021.emnlp-main.140,D18-1076,0,0.068441,"reviewed by the DMV in Albany, … • When the inspector visits your location, … Figure 1: In a document-grounded conversation, knowledge identification targets to locate a knowledge string within a long document to assist the agent in addressing the current user query. Introduction Many conversational agent scenarios require knowledge-grounded response generation, where knowledge is represented in written documents. Most prior work has explored architectures for knowledge grounding in an end-to-end framework, optimizing a loss function targeting response generation (Ghazvininejad et al., 2018; Zhou et al., 2018; Yavuz et al., 2019). However, the knowledge needed at any one turn in the dialogue is typically localized in the document, and some studies have shown that directly optimizing for knowledge extraction helps resolve complex user queries (Feng et al., 2020) and increases user engagement in social chat (Dinan et al., 2019; Moghe et al., 2018). For long documents, explicit knowledge identification can also be useful for model interpretability and human-in-the-loop assistant scenarios. Following (Feng et al., 2020), we define knowledge identification as the task of locating knowledge in a long do"
2021.emnlp-main.404,2020.findings-emnlp.196,0,0.0606852,"Missing"
2021.emnlp-main.404,W14-4337,0,0.149198,"ystems communicate DST models, inspired by prompt-based fine-tuning with users through natural language to accomplish a (Radford et al., 2019; Brown et al., 2020a). Specifwide range of tasks, such as food ordering, tech support, restaurant/hotel/travel booking, etc. The back- ically, instead of generating domain and slot symbols in the decoder, we concatenate the dialogue bone module of a typical system is dialogue state context with domain and slot prompts as input to tracking (DST), where the user goal is inferred from the encoder, where prompts are taken directly from the dialogue history (Henderson et al., 2014; Shah the schema. We hypothesize that jointly encoding et al., 2018; Budzianowski et al., 2018). User goals dialogue context and schema-specific textual inforare represented in terms of values of pre-defined mation can further benefit a sequence-to-sequence slots associated with a schema determined by the DST model. This allows task-aware contextualizainformation needed to execute task-specific queries to the backend. In other words, user goals are ex- tion for more effectively guiding the decoder to generate slot values. tracted progressively via slot filling based on the schema throughout t"
2021.emnlp-main.404,2020.tacl-1.28,0,0.0430092,"19) obtain empirical success by using ment of dialogue systems (Zhong et al., 2018; Chao prompts to guide zero shot generation without fineand Lane, 2019). Recently, to further examine the tuning on any prompts. Raffel et al. (2020) uses generalization abilities, large scale cross-domain task-specific prompts in both finetuning and testing datasets have been proposed (Budzianowski et al., phase. Recent studies have also tried to automati2018; Zang et al., 2020; Eric et al., 2019; Rastogi cally discover prompts rather than writing them by et al., 2020b). Classification-based models (Ye humans (Jiang et al., 2020). Our proposed promptet al., 2021; Chen et al., 2020) pick the candidate ing method is largely inspired by this body of work. from the oracle list of possible slot values. The Instead of prompt engineering/generation, we focus assumption of the full access of the schema makes on using available natural language descriptions of them have limited generalization abilities. On the schema categories associated with database docuother hand, generation-based models (Wu et al., mentation as task-specific promptings for DST. 2019; Kim et al., 2020; Lin et al., 2020a) directly generate slot values token"
2021.emnlp-main.404,2020.acl-main.53,0,0.455866,"intentions, which involves filling in values inferred. In classification-based models (Ye et al., of pre-defined slots. Many approaches have 2021; Chen et al., 2020), the prediction of a slot been proposed, often using task-specific architectures with special-purpose classifiers. Revalue is restricted to a fixed set for each slot, and cently, good results have been obtained usnon-categorical slots are constrained to values obing more general architectures based on preserved in the training data. In contrast, generationtrained language models. Here, we introduce based models (Wu et al., 2019; Kim et al., 2020) a new variation of the language modeling apdecode slot values sequentially (token by token) proach that uses schema-driven prompting to based on the dialogue context, with the potential provide task-aware history encoding that is of recovering unseen values. Recently, generationused for both categorical and non-categorical slots. We further improve performance by based DST built on large-scale pretrained neural augmenting the prompting with schema delanguage models (LM) achieve strong results withscriptions, a naturally occurring source of inout relying on domain-specific modules. Among domai"
2021.emnlp-main.404,2020.acl-main.703,0,0.0202169,"slot names as defined in the task-dependent schema. As shown in (b) of Figure 1, given the domain name train and the slot 3.2 Prompt-based DST name day, the specific prompt is in the form of In this section, we formally present the flow of “[domain] train [slot] day”. Different from our prompt-based DST with an encoder-decoder (Lin et al., 2020a; Wu et al., 2019) where the taskarchitecture. Here, we are interested in an encoder- specific information is used in the decoder side, decoder model with a bi-directional encoder (Raffel our symbol-based prompt as additional input to the et al., 2020; Lewis et al., 2020), in contrast with the bi-directional encoder can potentially achieve taskuni-directional encoder used in autoregressive LMs aware contextualizations. Observing that users of(Radford et al., 2019; Brown et al., 2020a). ten revise/repair their earlier requests in dialogues, The input of the prompt-based DST is made we posit that the resulting encoded representations up of a dialogue context ?? and a task-specific can be more effectively used by the decoder for prompt. Here, we use two types of task-specific generating corresponding slot values. prompts, the domain-related prompt ?(?? ), and Nat"
2021.emnlp-main.404,2020.emnlp-main.273,0,0.0757422,"-categorical slots. We further improve performance by based DST built on large-scale pretrained neural augmenting the prompting with schema delanguage models (LM) achieve strong results withscriptions, a naturally occurring source of inout relying on domain-specific modules. Among domain knowledge. Our purely generative systhem, the autoregressive model (Peng et al., 2020a; tem achieves state-of-the-art performance on Hosseini-Asl et al., 2020) uses a uni-directional MultiWOZ 2.2 and achieves competitive perencoder whereas the sequence-to-sequence model formance on two other benchmarks: Multi(Lin et al., 2020a; Heck et al., 2020) represents the WOZ 2.1 and M2M. The data and code will dialogue context using a bi-directional encoder. be available at https://github.com/ chiahsuan156/DST-as-Prompting. In this study, we follow a generation-based DST approach using a pre-trained sequence-to-sequence 1 Introduction model, but with the new strategy of adding taskspecific prompts as input for sequence-to-sequence In task-oriented dialogues, systems communicate DST models, inspired by prompt-based fine-tuning with users through natural language to accomplish a (Radford et al., 2019; Brown et al., 2020a). Sp"
2021.emnlp-main.404,2021.ccl-1.108,0,0.0645303,"Missing"
2021.emnlp-main.404,P19-1373,0,0.0162988,"Large-scale pretrained language models have obtained state-of-the-art performance on diverse generation and understanding tasks including bidirectional encoder style language models (Devlin et al., 2019; Liu et al., 2019), auto-regressive language models (Radford et al., 2019; Brown et al., 2020b) and more flexible sequence-to-sequence language models (Raffel et al., 2020). To adapt to dialogue tasks, variants of systems are finetuned on different dialogue corpora including chit-chat systems (Zhang et al., 2020; Adiwardana et al., 2020; Roller et al., 2020) and task-oriented dialogue systems (Mehri et al., 2019; Wu et al., 2020; Henderson et al., 2020; Peng et al., 2020b). We leave it as future work to leverage domain-adapted language models. 2.3 Prompting Language Models Extending a language model’s knowledge via Task-oriented dialogue datasets (Shah et al., 2018; prompts is an active line of research. Radford Henderson et al., 2014), have spurred the developet al. (2019) obtain empirical success by using ment of dialogue systems (Zhong et al., 2018; Chao prompts to guide zero shot generation without fineand Lane, 2019). Recently, to further examine the tuning on any prompts. Raffel et al. (2020) u"
2021.emnlp-main.404,2020.findings-emnlp.17,0,0.367015,"ially (token by token) proach that uses schema-driven prompting to based on the dialogue context, with the potential provide task-aware history encoding that is of recovering unseen values. Recently, generationused for both categorical and non-categorical slots. We further improve performance by based DST built on large-scale pretrained neural augmenting the prompting with schema delanguage models (LM) achieve strong results withscriptions, a naturally occurring source of inout relying on domain-specific modules. Among domain knowledge. Our purely generative systhem, the autoregressive model (Peng et al., 2020a; tem achieves state-of-the-art performance on Hosseini-Asl et al., 2020) uses a uni-directional MultiWOZ 2.2 and achieves competitive perencoder whereas the sequence-to-sequence model formance on two other benchmarks: Multi(Lin et al., 2020a; Heck et al., 2020) represents the WOZ 2.1 and M2M. The data and code will dialogue context using a bi-directional encoder. be available at https://github.com/ chiahsuan156/DST-as-Prompting. In this study, we follow a generation-based DST approach using a pre-trained sequence-to-sequence 1 Introduction model, but with the new strategy of adding taskspeci"
2021.emnlp-main.404,W18-5045,0,0.0508892,"Missing"
2021.emnlp-main.404,N18-2074,0,0.0636661,"Missing"
2021.emnlp-main.404,2020.emnlp-main.66,0,0.0116876,"ed language models have obtained state-of-the-art performance on diverse generation and understanding tasks including bidirectional encoder style language models (Devlin et al., 2019; Liu et al., 2019), auto-regressive language models (Radford et al., 2019; Brown et al., 2020b) and more flexible sequence-to-sequence language models (Raffel et al., 2020). To adapt to dialogue tasks, variants of systems are finetuned on different dialogue corpora including chit-chat systems (Zhang et al., 2020; Adiwardana et al., 2020; Roller et al., 2020) and task-oriented dialogue systems (Mehri et al., 2019; Wu et al., 2020; Henderson et al., 2020; Peng et al., 2020b). We leave it as future work to leverage domain-adapted language models. 2.3 Prompting Language Models Extending a language model’s knowledge via Task-oriented dialogue datasets (Shah et al., 2018; prompts is an active line of research. Radford Henderson et al., 2014), have spurred the developet al. (2019) obtain empirical success by using ment of dialogue systems (Zhong et al., 2018; Chao prompts to guide zero shot generation without fineand Lane, 2019). Recently, to further examine the tuning on any prompts. Raffel et al. (2020) uses generalizatio"
2021.emnlp-main.404,P19-1078,0,0.396269,"the slot value is intentions, which involves filling in values inferred. In classification-based models (Ye et al., of pre-defined slots. Many approaches have 2021; Chen et al., 2020), the prediction of a slot been proposed, often using task-specific architectures with special-purpose classifiers. Revalue is restricted to a fixed set for each slot, and cently, good results have been obtained usnon-categorical slots are constrained to values obing more general architectures based on preserved in the training data. In contrast, generationtrained language models. Here, we introduce based models (Wu et al., 2019; Kim et al., 2020) a new variation of the language modeling apdecode slot values sequentially (token by token) proach that uses schema-driven prompting to based on the dialogue context, with the potential provide task-aware history encoding that is of recovering unseen values. Recently, generationused for both categorical and non-categorical slots. We further improve performance by based DST built on large-scale pretrained neural augmenting the prompting with schema delanguage models (LM) achieve strong results withscriptions, a naturally occurring source of inout relying on domain-specific m"
2021.emnlp-main.404,2020.nlp4convai-1.13,0,0.145138,"oriented dialogue datasets (Shah et al., 2018; prompts is an active line of research. Radford Henderson et al., 2014), have spurred the developet al. (2019) obtain empirical success by using ment of dialogue systems (Zhong et al., 2018; Chao prompts to guide zero shot generation without fineand Lane, 2019). Recently, to further examine the tuning on any prompts. Raffel et al. (2020) uses generalization abilities, large scale cross-domain task-specific prompts in both finetuning and testing datasets have been proposed (Budzianowski et al., phase. Recent studies have also tried to automati2018; Zang et al., 2020; Eric et al., 2019; Rastogi cally discover prompts rather than writing them by et al., 2020b). Classification-based models (Ye humans (Jiang et al., 2020). Our proposed promptet al., 2021; Chen et al., 2020) pick the candidate ing method is largely inspired by this body of work. from the oracle list of possible slot values. The Instead of prompt engineering/generation, we focus assumption of the full access of the schema makes on using available natural language descriptions of them have limited generalization abilities. On the schema categories associated with database docuother hand, genera"
2021.emnlp-main.404,2020.acl-demos.30,0,0.0237597,"ly the dialogue state labels and does not utilize any external dialogue datasets. 2.2 Language Models Large-scale pretrained language models have obtained state-of-the-art performance on diverse generation and understanding tasks including bidirectional encoder style language models (Devlin et al., 2019; Liu et al., 2019), auto-regressive language models (Radford et al., 2019; Brown et al., 2020b) and more flexible sequence-to-sequence language models (Raffel et al., 2020). To adapt to dialogue tasks, variants of systems are finetuned on different dialogue corpora including chit-chat systems (Zhang et al., 2020; Adiwardana et al., 2020; Roller et al., 2020) and task-oriented dialogue systems (Mehri et al., 2019; Wu et al., 2020; Henderson et al., 2020; Peng et al., 2020b). We leave it as future work to leverage domain-adapted language models. 2.3 Prompting Language Models Extending a language model’s knowledge via Task-oriented dialogue datasets (Shah et al., 2018; prompts is an active line of research. Radford Henderson et al., 2014), have spurred the developet al. (2019) obtain empirical success by using ment of dialogue systems (Zhong et al., 2018; Chao prompts to guide zero shot generation witho"
D15-1085,P05-1045,0,0.00873326,"Missing"
D15-1085,D12-1110,0,0.0827105,"itive 20 15 10 10 5 5 0 0 5 5 10 10 15 15 20 20 20 15 10 5 0 5 10 15 20 (c) MT RNN. 20 15 10 5 0 5 10 15 20 (d) MT RNN using extra Reddit data. Figure 3: t-SNE visualization of sentence embeddings learned from different RNN models. 5 Related Work tion tasks (Kim, 2014; Kalchbrenner et al., 2014; Lai et al., 2015). These models usually build up the sentence representation directly from the lexical surface representation and rely on the pooling layer to capture the dependencies between words. Another popular method for continuous sentence representation is based on the recursive neural network (Socher et al., 2012; Socher et al., 2013; Tai et al., 2015). These models use a tree structure to compose a continuous sentence representation and have the advantages of capturing more fine-grained sentential structure due to the use of parsing trees. Note that the RNN-based sequential modeling used in this paper can be viewed as a linearized tree-structure model. In this paper, we train the neural network model with a multi-task objective reflecting both the probability of the sequence and the probability that the sequence contains names. The general idea of multitask learning dates back to (Caruana, 1997), and"
D15-1085,P14-1062,0,0.00488007,"ls to multi-task model. The external data make the proposed model produce more well-shaped groupings of sentence embeddings. 743 20 20 negative positive 15 10 10 5 5 0 0 5 5 10 15 10 20 15 25 negative positive 15 20 15 10 5 0 5 10 15 20 20 20 15 10 (a) RNN LM. 5 0 5 10 15 20 25 (b) ST RNN. 20 25 negative positive 15 negative positive 20 15 10 10 5 5 0 0 5 5 10 10 15 15 20 20 20 15 10 5 0 5 10 15 20 (c) MT RNN. 20 15 10 5 0 5 10 15 20 (d) MT RNN using extra Reddit data. Figure 3: t-SNE visualization of sentence embeddings learned from different RNN models. 5 Related Work tion tasks (Kim, 2014; Kalchbrenner et al., 2014; Lai et al., 2015). These models usually build up the sentence representation directly from the lexical surface representation and rely on the pooling layer to capture the dependencies between words. Another popular method for continuous sentence representation is based on the recursive neural network (Socher et al., 2012; Socher et al., 2013; Tai et al., 2015). These models use a tree structure to compose a continuous sentence representation and have the advantages of capturing more fine-grained sentential structure due to the use of parsing trees. Note that the RNN-based sequential modeling"
D15-1085,D13-1170,0,0.00193804,"0 0 5 5 10 10 15 15 20 20 20 15 10 5 0 5 10 15 20 (c) MT RNN. 20 15 10 5 0 5 10 15 20 (d) MT RNN using extra Reddit data. Figure 3: t-SNE visualization of sentence embeddings learned from different RNN models. 5 Related Work tion tasks (Kim, 2014; Kalchbrenner et al., 2014; Lai et al., 2015). These models usually build up the sentence representation directly from the lexical surface representation and rely on the pooling layer to capture the dependencies between words. Another popular method for continuous sentence representation is based on the recursive neural network (Socher et al., 2012; Socher et al., 2013; Tai et al., 2015). These models use a tree structure to compose a continuous sentence representation and have the advantages of capturing more fine-grained sentential structure due to the use of parsing trees. Note that the RNN-based sequential modeling used in this paper can be viewed as a linearized tree-structure model. In this paper, we train the neural network model with a multi-task objective reflecting both the probability of the sequence and the probability that the sequence contains names. The general idea of multitask learning dates back to (Caruana, 1997), and is shown to be effec"
D15-1085,D14-1181,0,0.00857228,"e task models to multi-task model. The external data make the proposed model produce more well-shaped groupings of sentence embeddings. 743 20 20 negative positive 15 10 10 5 5 0 0 5 5 10 15 10 20 15 25 negative positive 15 20 15 10 5 0 5 10 15 20 20 20 15 10 (a) RNN LM. 5 0 5 10 15 20 25 (b) ST RNN. 20 25 negative positive 15 negative positive 20 15 10 10 5 5 0 0 5 5 10 10 15 15 20 20 20 15 10 5 0 5 10 15 20 (c) MT RNN. 20 15 10 5 0 5 10 15 20 (d) MT RNN using extra Reddit data. Figure 3: t-SNE visualization of sentence embeddings learned from different RNN models. 5 Related Work tion tasks (Kim, 2014; Kalchbrenner et al., 2014; Lai et al., 2015). These models usually build up the sentence representation directly from the lexical surface representation and rely on the pooling layer to capture the dependencies between words. Another popular method for continuous sentence representation is based on the recursive neural network (Socher et al., 2012; Socher et al., 2013; Tai et al., 2015). These models use a tree structure to compose a continuous sentence representation and have the advantages of capturing more fine-grained sentential structure due to the use of parsing trees. Note that the RN"
D15-1085,P06-1078,0,0.0362163,"ifficult OOV words to cover are names, since they are less likely to be covered by morpheme-like subword fragments and they often result in anomalous recognition output, e.g. REF: what can we get at Litanfeeth HYP: what can we get it leaks on feet While these errors are rare, they create major problems for language processing, since names tend to be important for many applications. Thus, it is of interest to automatically detect such error regions for additional analysis or human correction. Named entity recognition (NER) systems have been applied to speech output (Palmer and Ostendorf, 2005; Sudoh et al., 2006), taking advantage 737 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 737–746, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational Linguistics. network of ASR hypotheses plus ASR word confidence to represent recognizer uncertainty, and word n-gram context to the left and right of the target confusion network slot. These features are combined in a maximum entropy (ME) classifier trained to predict name errors directly. This is the same as the baseline used in (He et al., 2014; Marin, 2015; Marin et al., 2015), but with a di"
D15-1085,N15-1092,0,0.00981607,"o on. As discussed in Section 5, there are other models that have been found useful for obtaining continuous sentence embeddings. It would be of interest to investigate whether other structures are more or less sensitive to data skew and/or useful for incorporating multi-domain training data. processing tasks. Collobert and Weston (2008) propose a unified deep convolutional neural network for different tasks by using a set of taskindependent word embeddings together with a set of task-specific word embeddings. For each task, it uses a unique neural network with its own layers and connections. Liu et al. (2015) propose a different neural network structure for search query classification and document retrieval where lowerlevel layers and connections are all shared but the high-level layers are task-specific. For tasks considered in (Collobert and Weston, 2008) and (Liu et al., 2015), training samples are task-dependent. Thus, both models are trained following the SGD manner by alternating tasks for each training samples with task-dependent training objectives. In this paper, we combine the language modeling task with the sentence-level name prediction task, and each training sample has labels for bot"
D15-1085,P15-1150,0,0.00910046,"Missing"
D15-1085,P14-5010,0,0.00351654,"Missing"
D15-1085,N13-1090,0,0.0500829,"system to resolve errors and ambiguity. Detecting OOV errors requires combining evidence of recognizer uncertainty and anomalous word sequences in a local region. For name errors, lexical cues to names are also useful, e.g. a person’s title, location prepositions, or keywords such as “name”. The baseline system for this work uses structural features extracted from a confusion 3 Multi-task Recurrent Neural Network The RNN is a powerful sequential model and has proven to be useful in many natural language processing tasks, including language modeling (Mikolov et al., 2010) and word similarity (Mikolov et al., 2013c). It also achieves good results for a variety of text classification problems when combined with a convolutional neural network (CNN) (Lai et al., 2015). In this paper, we 738 variables. Thus, the hidden vector ht provides a continuous representation of the word history that emphasizes words that are important for predicting the presence of a name. The vector at time n can be thought of as a sentence embedding. The sentence-level output yt differs from the word-dependent label predictor typically used in named entity detection (or part-of-speech tagging) in that it is providing a sequentiall"
D15-1085,D14-1162,0,\N,Missing
D15-1239,P12-1094,0,0.162244,"n this paper, we look at whether (and how) language use affects the reaction, compared to the relative importance of the author and timing of the post. Early work on factors that appear to influence crowd-based judgments of comments in the Slashdot forum (Lampe and Resnick, 2004) indicate that timing, starting score, length of the comment, and poster anonymity/reputation appear to play a role (where anonymity has a negative effect). Judging by differences in popularity of various discussion forums, topic is clearly important. Evidence that language use also matters is provided by recent work (Danescu-Niculescu-Mizil et al., 2012; Lakkaraju et al., 2013; Althoff et al., 2014; Tan et al., 2014). Teasing these different factors apart, however, is a challenge. The work presented in this paper provides additional insight into this question by controlling for these factors in a different way than previous work and by examining multiple communities of interest. Specifically, using data from Reddit discussion forums, we look at the role of author reputation as measured in terms of a karma k-index, and control for topic and timing by ranking comments in a constrained window within a discussion. The primary contributions of th"
D15-1239,P14-1017,0,0.490505,"ed to the relative importance of the author and timing of the post. Early work on factors that appear to influence crowd-based judgments of comments in the Slashdot forum (Lampe and Resnick, 2004) indicate that timing, starting score, length of the comment, and poster anonymity/reputation appear to play a role (where anonymity has a negative effect). Judging by differences in popularity of various discussion forums, topic is clearly important. Evidence that language use also matters is provided by recent work (Danescu-Niculescu-Mizil et al., 2012; Lakkaraju et al., 2013; Althoff et al., 2014; Tan et al., 2014). Teasing these different factors apart, however, is a challenge. The work presented in this paper provides additional insight into this question by controlling for these factors in a different way than previous work and by examining multiple communities of interest. Specifically, using data from Reddit discussion forums, we look at the role of author reputation as measured in terms of a karma k-index, and control for topic and timing by ranking comments in a constrained window within a discussion. The primary contributions of this work include findings about the role of author reputation and"
D15-1240,W12-2108,0,0.128469,"Missing"
D15-1240,D11-1120,0,0.442199,"Missing"
D15-1240,P12-3005,0,0.129646,"Missing"
D15-1240,W14-1303,0,0.0326645,"Missing"
D16-1108,2014.eamt-1.7,0,0.0133629,"nd content diversity to the previous ones: books, chicago, nyc, seattle, explainlikeimfive, science, running, nfl, and todayilearned. Among these extra subreddits, the smallest in size is nyc (1.5M tokens, 76K comments), and 1031 Table 1: Reddit dataset statistics Models We wish to characterize community language via style and topic. For modeling style, a popular approach has been combining the selected words with part-of-speech (POS) tags to construct models for genre detection (Stamatatos et al., 2000; Feldman et al., 2009; Bergsma et al., 2012) and data selection (Iyer and Ostendorf, 1999; Axelrod, 2014). For topic, a common approach is Latent Dirichlet Allocation (LDA) (Blei et al., 2003). We follow such approaches in our work, acknowledging the challenge of completely separating style/genre and topic factors raised previously (Iyer and Ostendorf, 1999; Sarawgi et al., 2011; Petrenz and Webber, 2011; Axelrod, 2014), which also comes out in our analysis. Generative language models are used for characterizing both style and topic, since they are well suited to handling texts of widely varying lengths. 4.1 Representing Style Replacing words with POS tags reduces the possibility that the style m"
D16-1108,N12-1033,0,0.0451424,"ed others set includes 9 other subreddits that are similar in size and content diversity to the previous ones: books, chicago, nyc, seattle, explainlikeimfive, science, running, nfl, and todayilearned. Among these extra subreddits, the smallest in size is nyc (1.5M tokens, 76K comments), and 1031 Table 1: Reddit dataset statistics Models We wish to characterize community language via style and topic. For modeling style, a popular approach has been combining the selected words with part-of-speech (POS) tags to construct models for genre detection (Stamatatos et al., 2000; Feldman et al., 2009; Bergsma et al., 2012) and data selection (Iyer and Ostendorf, 1999; Axelrod, 2014). For topic, a common approach is Latent Dirichlet Allocation (LDA) (Blei et al., 2003). We follow such approaches in our work, acknowledging the challenge of completely separating style/genre and topic factors raised previously (Iyer and Ostendorf, 1999; Sarawgi et al., 2011; Petrenz and Webber, 2011; Axelrod, 2014), which also comes out in our analysis. Generative language models are used for characterizing both style and topic, since they are well suited to handling texts of widely varying lengths. 4.1 Representing Style Replacing"
D16-1108,W11-0609,0,0.0291249,"lyze community language on the user level and show that more successful users (in terms of positive community reception) tend to be more specialized; in other words, analogous to offline communities, it is rare for a person to be an expert in multiple areas. 2 Related Work It is well known that conversation partners become more linguistically similar to each other as their dialogue evolves, via many aspects such as lexical, syntactic, as well as acoustic characteristics (Niederhoffer and Pennebaker, 2002; Levitan et al., 2011). This pattern is observed even when the conversation is fictional (Danescu-Niculescu-Mizil and Lee, 2011), or happening on social media (Danescu-NiculescuMizil et al., 2011). Regarding the language of online discussions in particular, it has been shown that individual users’ linguistic patterns evolve to match 1030 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1030–1035, c Austin, Texas, November 1-5, 2016. 2016 Association for Computational Linguistics subreddit askmen askscience askwomen atheism changemyview fitness politics worldnews merged others # posts 4.5K 0.9K 3.6K 3.1K 2.3K 2.4K 4.9K 9.9K 28.0K # cmts 1.1M 0.3M 0.8M 1.0M 0.5M 0.9M 2.2M 6.0M"
D16-1108,D15-1239,1,0.789619,"Missing"
D16-1108,P11-2020,0,0.0271607,"back to comments and their style, but interestingly not with their topic. Finally, we analyze community language on the user level and show that more successful users (in terms of positive community reception) tend to be more specialized; in other words, analogous to offline communities, it is rare for a person to be an expert in multiple areas. 2 Related Work It is well known that conversation partners become more linguistically similar to each other as their dialogue evolves, via many aspects such as lexical, syntactic, as well as acoustic characteristics (Niederhoffer and Pennebaker, 2002; Levitan et al., 2011). This pattern is observed even when the conversation is fictional (Danescu-Niculescu-Mizil and Lee, 2011), or happening on social media (Danescu-NiculescuMizil et al., 2011). Regarding the language of online discussions in particular, it has been shown that individual users’ linguistic patterns evolve to match 1030 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1030–1035, c Austin, Texas, November 1-5, 2016. 2016 Association for Computational Linguistics subreddit askmen askscience askwomen atheism changemyview fitness politics worldnews merged o"
D16-1108,P14-5010,0,0.00301331,"community-specific words was designed to capture distinctive community jargon. The general words include punctuation, function words, and words that are common in many subreddits (e.g., sex, culture, see, dumb, simply). The subreddit-specific words seem to reflect both broad topical themes and jargon or style words, as in (themes vs. style/jargon): askmen: wife, single vs. whatever, interested askwomen: mom, husband vs. especially, totally askscience: particle, planet vs. basically, x fitness: exercises, muscles vs. cardio, reps, rack Tokenization and tagging are done using Stanford coreNLP (Manning et al., 2014). Punctuation is separated from the words and treated as a word. All language models are trigrams trained using the SRILM toolkit (Stolcke, 2002); modified KneserNey smoothing is applied to the word_only language model, while Witten-Bell smoothing is applied to the tag_only and both hybrid models. 4.2 Representing Topic We train 100- and 200-dimensional LDA topic modˇ uˇrek els (Blei et al., 2003) using gensim (Reh˚ and Sojka, 2010). We remove all stopwords (250 1032 ID 19 29 32 34 Frequent words -lsb-, -rsb-, -rrb-, -lrb-, **, reddit, comment, confirmed, spanish, fair sex, pilots, child, wome"
D16-1108,W11-0710,0,0.652523,"Missing"
D16-1108,J11-2004,0,0.0310478,"ize community language via style and topic. For modeling style, a popular approach has been combining the selected words with part-of-speech (POS) tags to construct models for genre detection (Stamatatos et al., 2000; Feldman et al., 2009; Bergsma et al., 2012) and data selection (Iyer and Ostendorf, 1999; Axelrod, 2014). For topic, a common approach is Latent Dirichlet Allocation (LDA) (Blei et al., 2003). We follow such approaches in our work, acknowledging the challenge of completely separating style/genre and topic factors raised previously (Iyer and Ostendorf, 1999; Sarawgi et al., 2011; Petrenz and Webber, 2011; Axelrod, 2014), which also comes out in our analysis. Generative language models are used for characterizing both style and topic, since they are well suited to handling texts of widely varying lengths. 4.1 Representing Style Replacing words with POS tags reduces the possibility that the style model is learning topic, but replacing too many words loses useful community jargon. To explore this tradeoff, we compared four trigram language models representing different uses of words vs. POS tags in the vocabulary: • word_only: a regular token-based language model (vocabulary: 156K words) • hyb-1"
D16-1108,W11-0310,0,0.0322705,"s We wish to characterize community language via style and topic. For modeling style, a popular approach has been combining the selected words with part-of-speech (POS) tags to construct models for genre detection (Stamatatos et al., 2000; Feldman et al., 2009; Bergsma et al., 2012) and data selection (Iyer and Ostendorf, 1999; Axelrod, 2014). For topic, a common approach is Latent Dirichlet Allocation (LDA) (Blei et al., 2003). We follow such approaches in our work, acknowledging the challenge of completely separating style/genre and topic factors raised previously (Iyer and Ostendorf, 1999; Sarawgi et al., 2011; Petrenz and Webber, 2011; Axelrod, 2014), which also comes out in our analysis. Generative language models are used for characterizing both style and topic, since they are well suited to handling texts of widely varying lengths. 4.1 Representing Style Replacing words with POS tags reduces the possibility that the style model is learning topic, but replacing too many words loses useful community jargon. To explore this tradeoff, we compared four trigram language models representing different uses of words vs. POS tags in the vocabulary: • word_only: a regular token-based language model (vocab"
D16-1108,C00-2117,0,0.104328,"sis. Statistics are listed in Table 1. The merged others set includes 9 other subreddits that are similar in size and content diversity to the previous ones: books, chicago, nyc, seattle, explainlikeimfive, science, running, nfl, and todayilearned. Among these extra subreddits, the smallest in size is nyc (1.5M tokens, 76K comments), and 1031 Table 1: Reddit dataset statistics Models We wish to characterize community language via style and topic. For modeling style, a popular approach has been combining the selected words with part-of-speech (POS) tags to construct models for genre detection (Stamatatos et al., 2000; Feldman et al., 2009; Bergsma et al., 2012) and data selection (Iyer and Ostendorf, 1999; Axelrod, 2014). For topic, a common approach is Latent Dirichlet Allocation (LDA) (Blei et al., 2003). We follow such approaches in our work, acknowledging the challenge of completely separating style/genre and topic factors raised previously (Iyer and Ostendorf, 1999; Sarawgi et al., 2011; Petrenz and Webber, 2011; Axelrod, 2014), which also comes out in our analysis. Generative language models are used for characterizing both style and topic, since they are well suited to handling texts of widely vary"
D16-1189,P09-1010,0,0.0402293,"al., 2012; Sordoni et al., 2015) inspired significant progress by combining deep learning with reinforcement learning (Mnih et al., 2015; Silver et al., 2016; Lillicrap et al., 2016; Duan et al., 2016). In natural language processing, reinforcement learning has been applied successfully to dialogue systems that generate natural language and converse with a human user (Scheffler and Young, 2002; Singh et al., 1999; Wen et al., 2016). There has also been interest in mapping text instructions to sequences of executable actions and extracting textual knowledge to improve game control performance (Branavan et al., 2009; Branavan et al., 2011). Recently, Narasimhan et al. (2015) studied the task of text-based games with a deep Q-learning framework. He et al. (2016) proposed to use a separate deep network for handling natural language actions and to model Q-values via state-action interaction. Nogueira and Cho (2016) have also proposed a goal-driven web navigation task for languagebased sequential decision making. Narasimhan et al. (2016) applied reinforcement learning for acquiring and incorporating external evidence to improve information extraction accuracy. The study that we present with Reddit popularity"
D16-1189,P11-1028,0,0.0834282,"l., 2015) inspired significant progress by combining deep learning with reinforcement learning (Mnih et al., 2015; Silver et al., 2016; Lillicrap et al., 2016; Duan et al., 2016). In natural language processing, reinforcement learning has been applied successfully to dialogue systems that generate natural language and converse with a human user (Scheffler and Young, 2002; Singh et al., 1999; Wen et al., 2016). There has also been interest in mapping text instructions to sequences of executable actions and extracting textual knowledge to improve game control performance (Branavan et al., 2009; Branavan et al., 2011). Recently, Narasimhan et al. (2015) studied the task of text-based games with a deep Q-learning framework. He et al. (2016) proposed to use a separate deep network for handling natural language actions and to model Q-values via state-action interaction. Nogueira and Cho (2016) have also proposed a goal-driven web navigation task for languagebased sequential decision making. Narasimhan et al. (2016) applied reinforcement learning for acquiring and incorporating external evidence to improve information extraction accuracy. The study that we present with Reddit popularity tracking differs from t"
D16-1189,P16-1153,1,0.917358,"2016; Lillicrap et al., 2016; Duan et al., 2016). In natural language processing, reinforcement learning has been applied successfully to dialogue systems that generate natural language and converse with a human user (Scheffler and Young, 2002; Singh et al., 1999; Wen et al., 2016). There has also been interest in mapping text instructions to sequences of executable actions and extracting textual knowledge to improve game control performance (Branavan et al., 2009; Branavan et al., 2011). Recently, Narasimhan et al. (2015) studied the task of text-based games with a deep Q-learning framework. He et al. (2016) proposed to use a separate deep network for handling natural language actions and to model Q-values via state-action interaction. Nogueira and Cho (2016) have also proposed a goal-driven web navigation task for languagebased sequential decision making. Narasimhan et al. (2016) applied reinforcement learning for acquiring and incorporating external evidence to improve information extraction accuracy. The study that we present with Reddit popularity tracking differs from these other text-based reinforcement learning tasks in that the language in both state and action spaces is unconstrained and"
D16-1189,D15-1239,1,0.645903,"rements) of popularity, including: the volume of comments in response to blog posts (Yano and Smith, 2010) and news articles (Tasgkias et al., 2009; Tatar et al., 2011), the number of Twitter shares of news articles (Bandari et al., 2012), the number of reshares on Facebook (Cheng et al., 2014) and retweets on Twitter (Suh et al., 2010; Hong et al., 2011; Tan et al., 2014; Zhao et al., 2015), the rate of posts related to a source rumor (Lukasik et al., 2015), and the difference in the number of reader up and down votes on posts and comments in Reddit discussion forums (Lakkaraju et al., 2013; Jaech et al., 2015). An advantage of working with the Reddit data is that both positive and negative reactions are accounted for in the karma score. Of the prior work on Reddit, the task explored here is most similar to (Jaech et al., 2015) in that it involves choosing relatively high karma comments (or threads) from a time-limited set rather than directly predicting comment (or post) karma. Prior work on popularity prediction used supervised learning; this is the first work that frames tracking hot topics in social media with deep reinforcement learning. 4 4.1 Characterizing a combinatorial action space Notatio"
D16-1189,P15-2085,0,0.0167351,"hus, our work is more closely related to popularity prediction for social media and online news. These studies have explored a variety of definitions (or measurements) of popularity, including: the volume of comments in response to blog posts (Yano and Smith, 2010) and news articles (Tasgkias et al., 2009; Tatar et al., 2011), the number of Twitter shares of news articles (Bandari et al., 2012), the number of reshares on Facebook (Cheng et al., 2014) and retweets on Twitter (Suh et al., 2010; Hong et al., 2011; Tan et al., 2014; Zhao et al., 2015), the rate of posts related to a source rumor (Lukasik et al., 2015), and the difference in the number of reader up and down votes on posts and comments in Reddit discussion forums (Lakkaraju et al., 2013; Jaech et al., 2015). An advantage of working with the Reddit data is that both positive and negative reactions are accounted for in the karma score. Of the prior work on Reddit, the task explored here is most similar to (Jaech et al., 2015) in that it involves choosing relatively high karma comments (or threads) from a time-limited set rather than directly predicting comment (or post) karma. Prior work on popularity prediction used supervised learning; this"
D16-1189,D15-1001,0,0.0186737,"gress by combining deep learning with reinforcement learning (Mnih et al., 2015; Silver et al., 2016; Lillicrap et al., 2016; Duan et al., 2016). In natural language processing, reinforcement learning has been applied successfully to dialogue systems that generate natural language and converse with a human user (Scheffler and Young, 2002; Singh et al., 1999; Wen et al., 2016). There has also been interest in mapping text instructions to sequences of executable actions and extracting textual knowledge to improve game control performance (Branavan et al., 2009; Branavan et al., 2011). Recently, Narasimhan et al. (2015) studied the task of text-based games with a deep Q-learning framework. He et al. (2016) proposed to use a separate deep network for handling natural language actions and to model Q-values via state-action interaction. Nogueira and Cho (2016) have also proposed a goal-driven web navigation task for languagebased sequential decision making. Narasimhan et al. (2016) applied reinforcement learning for acquiring and incorporating external evidence to improve information extraction accuracy. The study that we present with Reddit popularity tracking differs from these other text-based reinforcement"
D16-1189,D16-1261,0,0.0321309,"t al., 2016). There has also been interest in mapping text instructions to sequences of executable actions and extracting textual knowledge to improve game control performance (Branavan et al., 2009; Branavan et al., 2011). Recently, Narasimhan et al. (2015) studied the task of text-based games with a deep Q-learning framework. He et al. (2016) proposed to use a separate deep network for handling natural language actions and to model Q-values via state-action interaction. Nogueira and Cho (2016) have also proposed a goal-driven web navigation task for languagebased sequential decision making. Narasimhan et al. (2016) applied reinforcement learning for acquiring and incorporating external evidence to improve information extraction accuracy. The study that we present with Reddit popularity tracking differs from these other text-based reinforcement learning tasks in that the language in both state and action spaces is unconstrained and quite rich. Dulac-Arnold et al. (2016) also investigated a problem of large discrete action spaces. A Wolpertinger architecture is proposed to reduce computational complexity of evaluating all actions. While a combinatorial action space can be large and discrete, their method"
D16-1189,N15-1020,1,0.920583,"deep learning with reinforcement learning (Mnih et al., 2015; Silver et al., 2016; Lillicrap et al., 2016; Duan et al., 2016). In natural language processing, reinforcement learning has been applied successfully to dialogue systems that generate natural language and converse with a human user (Scheffler and Young, 2002; Singh et al., 1999; Wen et al., 2016). There has also been interest in mapping text instructions to sequences of executable actions and extracting textual knowledge to improve game control performance (Branavan et al., 2009; Branavan et al., 2011). Recently, Narasimhan et al. (2015) studied the task of text-based games with a deep Q-learning framework. He et al. (2016) proposed to use a separate deep network for handling natural language actions and to model Q-values via state-action interaction. Nogueira and Cho (2016) have also proposed a goal-driven web navigation task for languagebased sequential decision making. Narasimhan et al. (2016) applied reinforcement learning for acquiring and incorporating external evidence to improve information extraction accuracy. The study that we present with Reddit popularity tracking differs from these other text-based reinforcement"
D16-1189,P14-1017,0,0.1357,"l is not to track topics based on frequency, but rather based on reader response. Thus, our work is more closely related to popularity prediction for social media and online news. These studies have explored a variety of definitions (or measurements) of popularity, including: the volume of comments in response to blog posts (Yano and Smith, 2010) and news articles (Tasgkias et al., 2009; Tatar et al., 2011), the number of Twitter shares of news articles (Bandari et al., 2012), the number of reshares on Facebook (Cheng et al., 2014) and retweets on Twitter (Suh et al., 2010; Hong et al., 2011; Tan et al., 2014; Zhao et al., 2015), the rate of posts related to a source rumor (Lukasik et al., 2015), and the difference in the number of reader up and down votes on posts and comments in Reddit discussion forums (Lakkaraju et al., 2013; Jaech et al., 2015). An advantage of working with the Reddit data is that both positive and negative reactions are accounted for in the karma score. Of the prior work on Reddit, the task explored here is most similar to (Jaech et al., 2015) in that it involves choosing relatively high karma comments (or threads) from a time-limited set rather than directly predicting comm"
D17-1243,D16-1238,1,0.846853,"and style which is learned in the mode vectors, and 2) the responses to a comment modeled as the response trigger vectors. Moreover, we augment our model with the attention 2303 mechanism (Bahdanau et al., 2015) to push the model to distill the relevant information from context. The neural attention mechanism has been used for a variety of natural language processing tasks, e.g., machine translation (Bahdanau et al., 2015), question answering (Sukhbaatar et al., 2015), constituency parsing (Vinyals et al., 2015), social media opinion mining (Yang and Eisenstein, 2017). and dependency parsing (Cheng et al., 2016). The attention mechanism developed in this paper for exploiting global modes differs from previous work in that the global modes being attended over are latent rather than explicitly observed, and in that they are learned jointly with the full model. Predicting the community endorsement has been studied by using either hand-crafted features (Jaech et al., 2015) or neural models (Fang et al., 2016; Zayats and Ostendorf, 2017), but all of them focus on supervised learning. Unsupervised learning strategies have been explored for characterizing different factors in language. A hierarchical Dirich"
D17-1243,W16-6209,1,0.9194,"nity endorsement prediction task using a large collection of topic-varying Reddit discussions, the factored embeddings consistently achieve improvement over other text representations. Qualitative analysis shows that the model captures community style and topic, as well as response trigger patterns. 1 Introduction Massive user-generated content on social media has drawn interests in predicting community reactions in the form of virality (Guerini et al., 2011), popularity (Suh et al., 2010; Hong et al., 2011; Lakkaraju et al., 2013; Tan et al., 2014), community endorsement (Jaech et al., 2015; Fang et al., 2016), persuasive impact (Althoff et al., 2014; Tan et al., 2016; Wei et al., 2016), etc. Many of these studies have analyzed content-agnostic factors such as submission timing and author social status, as well as language factors that underlie the textual content, e.g., the topic and idiosyncrasies of the community. In particular, there is an increasing amount of work on online discussion forums such as Reddit that exploits the conversational and community-centric nature of the usergenerated content (Lakkaraju et al., 2013; Althoff et al., 2014; Jaech et al., 2015; Tan et al., 2016; Wei et al., 20"
D17-1243,N15-1184,0,0.0154144,"tion of community reactions. Distributed representations of text, or text embeddings, have achieved great success in many language processing applications, using both supervised and unsupervised methods. Unsupervised learning, in particular, has been successful at different levels, including words (Mikolov et al., 2013b), sentences (Kiros et al., 2015), and documents (Deerwester et al., 1990; Le and Mikolov, 2014). Studies have also shown that the learned embedding captures both syntactic and semantic functions of words (Mikolov et al., 2013a; Pennington et al., 2014; Levy and Goldberg, 2014; Faruqui et al., 2015a). At the same time, em1 karma = #up-votes - #down-votes. goo.gl/TnXgCr. 2296 See https:// Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2296–2306 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics beddings are often viewed as uninterpretable – it is difficult to align embedding dimensions to existing semantic or syntactic classes. This concern has triggered attempts in developing more interpretable embedding models (Faruqui et al., 2015b), which is also a goal of our work. We leverage the fact that the s"
D17-1243,P15-1144,0,0.0172229,"tion of community reactions. Distributed representations of text, or text embeddings, have achieved great success in many language processing applications, using both supervised and unsupervised methods. Unsupervised learning, in particular, has been successful at different levels, including words (Mikolov et al., 2013b), sentences (Kiros et al., 2015), and documents (Deerwester et al., 1990; Le and Mikolov, 2014). Studies have also shown that the learned embedding captures both syntactic and semantic functions of words (Mikolov et al., 2013a; Pennington et al., 2014; Levy and Goldberg, 2014; Faruqui et al., 2015a). At the same time, em1 karma = #up-votes - #down-votes. goo.gl/TnXgCr. 2296 See https:// Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2296–2306 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics beddings are often viewed as uninterpretable – it is difficult to align embedding dimensions to existing semantic or syntactic classes. This concern has triggered attempts in developing more interpretable embedding models (Faruqui et al., 2015b), which is also a goal of our work. We leverage the fact that the s"
D17-1243,D16-1189,1,0.679187,"rsuasive impact (Althoff et al., 2014; Tan et al., 2016; Wei et al., 2016), etc. Many of these studies have analyzed content-agnostic factors such as submission timing and author social status, as well as language factors that underlie the textual content, e.g., the topic and idiosyncrasies of the community. In particular, there is an increasing amount of work on online discussion forums such as Reddit that exploits the conversational and community-centric nature of the usergenerated content (Lakkaraju et al., 2013; Althoff et al., 2014; Jaech et al., 2015; Tan et al., 2016; Wei et al., 2016; He et al., 2016a; Fang et al., 2016), which contrasts with Twitter where the author’s social status seems to play a larger role in popularity. This paper focuses on Reddit, using the karma score1 as a readily available measure of community endorsement. Some of the prior work on Reddit investigates specific linguistic phenomena (e.g. politeness, topic relevance, community style matching) using feature engineering to understand their role in predicting community reactions (Althoff et al., 2014; Jaech et al., 2015). In contrast, this paper explores methods for unsupervised text embedding learning using a model"
D17-1243,D15-1239,1,0.887558,"Evaluated on a community endorsement prediction task using a large collection of topic-varying Reddit discussions, the factored embeddings consistently achieve improvement over other text representations. Qualitative analysis shows that the model captures community style and topic, as well as response trigger patterns. 1 Introduction Massive user-generated content on social media has drawn interests in predicting community reactions in the form of virality (Guerini et al., 2011), popularity (Suh et al., 2010; Hong et al., 2011; Lakkaraju et al., 2013; Tan et al., 2014), community endorsement (Jaech et al., 2015; Fang et al., 2016), persuasive impact (Althoff et al., 2014; Tan et al., 2016; Wei et al., 2016), etc. Many of these studies have analyzed content-agnostic factors such as submission timing and author social status, as well as language factors that underlie the textual content, e.g., the topic and idiosyncrasies of the community. In particular, there is an increasing amount of work on online discussion forums such as Reddit that exploits the conversational and community-centric nature of the usergenerated content (Lakkaraju et al., 2013; Althoff et al., 2014; Jaech et al., 2015; Tan et al.,"
D17-1243,P14-2050,0,0.0366995,"which will improve prediction of community reactions. Distributed representations of text, or text embeddings, have achieved great success in many language processing applications, using both supervised and unsupervised methods. Unsupervised learning, in particular, has been successful at different levels, including words (Mikolov et al., 2013b), sentences (Kiros et al., 2015), and documents (Deerwester et al., 1990; Le and Mikolov, 2014). Studies have also shown that the learned embedding captures both syntactic and semantic functions of words (Mikolov et al., 2013a; Pennington et al., 2014; Levy and Goldberg, 2014; Faruqui et al., 2015a). At the same time, em1 karma = #up-votes - #down-votes. goo.gl/TnXgCr. 2296 See https:// Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2296–2306 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics beddings are often viewed as uninterpretable – it is difficult to align embedding dimensions to existing semantic or syntactic classes. This concern has triggered attempts in developing more interpretable embedding models (Faruqui et al., 2015b), which is also a goal of our work. We levera"
D17-1243,N13-1090,0,0.140571,"we propose a factored neural model with separate mechanisms for representing global context, comment content and response generation. By factoring the model, we hope unsupervised learning will pick up different components of interactive language in the resulting embeddings, which will improve prediction of community reactions. Distributed representations of text, or text embeddings, have achieved great success in many language processing applications, using both supervised and unsupervised methods. Unsupervised learning, in particular, has been successful at different levels, including words (Mikolov et al., 2013b), sentences (Kiros et al., 2015), and documents (Deerwester et al., 1990; Le and Mikolov, 2014). Studies have also shown that the learned embedding captures both syntactic and semantic functions of words (Mikolov et al., 2013a; Pennington et al., 2014; Levy and Goldberg, 2014; Faruqui et al., 2015a). At the same time, em1 karma = #up-votes - #down-votes. goo.gl/TnXgCr. 2296 See https:// Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2296–2306 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics beddings are"
D17-1243,D14-1162,0,0.0846016,"he resulting embeddings, which will improve prediction of community reactions. Distributed representations of text, or text embeddings, have achieved great success in many language processing applications, using both supervised and unsupervised methods. Unsupervised learning, in particular, has been successful at different levels, including words (Mikolov et al., 2013b), sentences (Kiros et al., 2015), and documents (Deerwester et al., 1990; Le and Mikolov, 2014). Studies have also shown that the learned embedding captures both syntactic and semantic functions of words (Mikolov et al., 2013a; Pennington et al., 2014; Levy and Goldberg, 2014; Faruqui et al., 2015a). At the same time, em1 karma = #up-votes - #down-votes. goo.gl/TnXgCr. 2296 See https:// Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2296–2306 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics beddings are often viewed as uninterpretable – it is difficult to align embedding dimensions to existing semantic or syntactic classes. This concern has triggered attempts in developing more interpretable embedding models (Faruqui et al., 2015b), which is also a go"
D17-1243,P14-1017,0,0.0924741,"way to interpret the factored embeddings. Evaluated on a community endorsement prediction task using a large collection of topic-varying Reddit discussions, the factored embeddings consistently achieve improvement over other text representations. Qualitative analysis shows that the model captures community style and topic, as well as response trigger patterns. 1 Introduction Massive user-generated content on social media has drawn interests in predicting community reactions in the form of virality (Guerini et al., 2011), popularity (Suh et al., 2010; Hong et al., 2011; Lakkaraju et al., 2013; Tan et al., 2014), community endorsement (Jaech et al., 2015; Fang et al., 2016), persuasive impact (Althoff et al., 2014; Tan et al., 2016; Wei et al., 2016), etc. Many of these studies have analyzed content-agnostic factors such as submission timing and author social status, as well as language factors that underlie the textual content, e.g., the topic and idiosyncrasies of the community. In particular, there is an increasing amount of work on online discussion forums such as Reddit that exploits the conversational and community-centric nature of the usergenerated content (Lakkaraju et al., 2013; Althoff et"
D17-1243,D16-1108,1,0.744868,"more interpretable embedding models (Faruqui et al., 2015b), which is also a goal of our work. We leverage the fact that the structure of the distributional context impacts what is learned in an unsupervised way and include multiple objectives for separating different types of context. Here, we are interested in linking two types of context with corresponding language factors learned in the embedding space that may impact comment reception. First, conformity to the topic and the language use of the community tends to make the content better accepted (Lakkaraju et al., 2013; Tan et al., 2014; Tran and Ostendorf, 2016). Those global modes typically influence the author’s generation of local content. Second, characteristics of a comment can influence the responses it triggers. Clearly, questions and statements will elicit different responses, and comments directed at a particular discussion participant may prompt that individual to respond. Of more interest here are aspects of comments that might elicit minimal response or responses with different sentiments, which are relevant for eventual endorsement. The primary contribution of this work is the development of a factored neural model to jointly learn these"
D17-1243,Q17-1021,0,0.0243978,"ions: 1) the global context such as community topic and style which is learned in the mode vectors, and 2) the responses to a comment modeled as the response trigger vectors. Moreover, we augment our model with the attention 2303 mechanism (Bahdanau et al., 2015) to push the model to distill the relevant information from context. The neural attention mechanism has been used for a variety of natural language processing tasks, e.g., machine translation (Bahdanau et al., 2015), question answering (Sukhbaatar et al., 2015), constituency parsing (Vinyals et al., 2015), social media opinion mining (Yang and Eisenstein, 2017). and dependency parsing (Cheng et al., 2016). The attention mechanism developed in this paper for exploiting global modes differs from previous work in that the global modes being attended over are latent rather than explicitly observed, and in that they are learned jointly with the full model. Predicting the community endorsement has been studied by using either hand-crafted features (Jaech et al., 2015) or neural models (Fang et al., 2016; Zayats and Ostendorf, 2017), but all of them focus on supervised learning. Unsupervised learning strategies have been explored for characterizing differe"
D17-1279,P11-1051,0,0.0115548,"ropagation and confidence-aware data selection, iii) exploring different alternatives for taking advantage of large, multi-domain unannotated data including both unsupervised embedding initialization and semi-supervised model training. 2 Related Work There has been growing interest in research on automatic methods to help researchers search and extract useful information from scientific literature. Past research has addressed citation sentiment (Athar and Teufel, 2012b,a), citation networks (Kas, 2011; Gabor et al., 2016; Sim et al., 2012; Do et al., 2013; Jaidka et al., 2014), summarization (Abu-Jbara and Radev, 2011) and some analysis of research community (Vogel and Jurafsky, 2012; Anderson et al., 2012; Luan et al., 2012, 2014b; Levow et al., 2014). However, due to scarce hand-annotated data resources, previous work on information extraction (IE) for scientific literature is very limited. Gupta and Manning (2011) first proposed a task that defines scientific terms for 474 abstracts from the ACL anthologhy (Bird et al., 2008) into three aspects: domain, technique and focus and apply templatebased bootstrapping to tackle the problem. Based on this study, Tsai et al. (2013) improve the performance by intro"
D17-1279,U14-1012,0,0.0131253,"on of ULM. Dashed box is the uncertain token which is going to be marginalized over. Arrows and grey nodes are paths to be summed over during training. When all tokens are confident, the score of only one path is calculated. CRF prediction if the node is not connected to a labeled vertex, ensuring the algorithm performs at least as well as standard self-training. Posterior Estimates We develop two strategies to estimate the new posteriors pˆ(yt |x; θ), which can then be used in the CRF training. The first strategy (called G RAPH I NTERP) is the commonly used approach (Subramanya et al., 2010; Aliannejadi et al., 2014) that interpolates the smoothed posterior {q} with CRF marginals p: pˆ(yt |x; θ) = αp(yt |x; θ) + (1 − α)q(y) (3) where α is a mixing coefficient. A second strategy introduced here (called G RAPH F EAT) uses the smoothed posterior {q} as features and learns it with other parameters in the neural network. Given a sentence {x1 , . . . , xn }, let Q = {q1 , . . . , qn } be the predicted label distribution from the graph. We then use Q as a feature input to neural network as P˜ = P + M Q where P is the n × m matrix output by the bidirectional LSTM network as in Eq. 1, and M is m × m matrix and is"
D17-1279,W12-3202,0,0.0383559,"ng advantage of large, multi-domain unannotated data including both unsupervised embedding initialization and semi-supervised model training. 2 Related Work There has been growing interest in research on automatic methods to help researchers search and extract useful information from scientific literature. Past research has addressed citation sentiment (Athar and Teufel, 2012b,a), citation networks (Kas, 2011; Gabor et al., 2016; Sim et al., 2012; Do et al., 2013; Jaidka et al., 2014), summarization (Abu-Jbara and Radev, 2011) and some analysis of research community (Vogel and Jurafsky, 2012; Anderson et al., 2012; Luan et al., 2012, 2014b; Levow et al., 2014). However, due to scarce hand-annotated data resources, previous work on information extraction (IE) for scientific literature is very limited. Gupta and Manning (2011) first proposed a task that defines scientific terms for 474 abstracts from the ACL anthologhy (Bird et al., 2008) into three aspects: domain, technique and focus and apply templatebased bootstrapping to tackle the problem. Based on this study, Tsai et al. (2013) improve the performance by introducing hand-designed features from NER (Collins and Singer, 1999) to the bootstrapping fr"
D17-1279,N12-1073,0,0.0231903,"EVAL Task 10 by extending recent advances in neural tagging models; ii) introducing a semi-supervised learning algorithm that uses graph-based label propagation and confidence-aware data selection, iii) exploring different alternatives for taking advantage of large, multi-domain unannotated data including both unsupervised embedding initialization and semi-supervised model training. 2 Related Work There has been growing interest in research on automatic methods to help researchers search and extract useful information from scientific literature. Past research has addressed citation sentiment (Athar and Teufel, 2012b,a), citation networks (Kas, 2011; Gabor et al., 2016; Sim et al., 2012; Do et al., 2013; Jaidka et al., 2014), summarization (Abu-Jbara and Radev, 2011) and some analysis of research community (Vogel and Jurafsky, 2012; Anderson et al., 2012; Luan et al., 2012, 2014b; Levow et al., 2014). However, due to scarce hand-annotated data resources, previous work on information extraction (IE) for scientific literature is very limited. Gupta and Manning (2011) first proposed a task that defines scientific terms for 474 abstracts from the ACL anthologhy (Bird et al., 2008) into three aspects: domain,"
D17-1279,W12-4303,0,0.0438615,"EVAL Task 10 by extending recent advances in neural tagging models; ii) introducing a semi-supervised learning algorithm that uses graph-based label propagation and confidence-aware data selection, iii) exploring different alternatives for taking advantage of large, multi-domain unannotated data including both unsupervised embedding initialization and semi-supervised model training. 2 Related Work There has been growing interest in research on automatic methods to help researchers search and extract useful information from scientific literature. Past research has addressed citation sentiment (Athar and Teufel, 2012b,a), citation networks (Kas, 2011; Gabor et al., 2016; Sim et al., 2012; Do et al., 2013; Jaidka et al., 2014), summarization (Abu-Jbara and Radev, 2011) and some analysis of research community (Vogel and Jurafsky, 2012; Anderson et al., 2012; Luan et al., 2012, 2014b; Levow et al., 2014). However, due to scarce hand-annotated data resources, previous work on information extraction (IE) for scientific literature is very limited. Gupta and Manning (2011) first proposed a task that defines scientific terms for 474 abstracts from the ACL anthologhy (Bird et al., 2008) into three aspects: domain,"
D17-1279,S17-2091,0,0.250403,"ing (2011) first proposed a task that defines scientific terms for 474 abstracts from the ACL anthologhy (Bird et al., 2008) into three aspects: domain, technique and focus and apply templatebased bootstrapping to tackle the problem. Based on this study, Tsai et al. (2013) improve the performance by introducing hand-designed features from NER (Collins and Singer, 1999) to the bootstrapping framework. QasemiZadeh and Schumann (2012) compile a dataset of scientific terms into 7 fine-grained categories for 171 abstracts of ACL anothology. Similar to our work, very recently Augenstein and Søgaard (2017) also evaluated on ScienceIE dataset, but use multi-task learning to improve the performance of a supervised neural approach. Instead, we introduce a semi-supervised neural tagging approach that leverages unlabeled data. Neural tagging models have been recently introduced to tagging problems such as NER. For example, Collobert et al. (2011) use a CNN over a sequence of word embeddings and apply a CRF layer on top. Huang et al. (2015) use hand-crafted features with LSTMs to improve performance. There is currently great interest in using characterbased embeddings in neural models. (Chiu and Nich"
D17-1279,P17-2054,0,0.232683,"limited. Gupta and Manning (2011) first proposed a task that defines scientific terms for 474 abstracts from the ACL anthologhy (Bird et al., 2008) into three aspects: domain, technique and focus and apply templatebased bootstrapping to tackle the problem. Based on this study, Tsai et al. (2013) improve the performance by introducing hand-designed features from NER (Collins and Singer, 1999) to the bootstrapping framework. QasemiZadeh and Schumann (2012) compile a dataset of scientific terms into 7 fine-grained categories for 171 abstracts of ACL anothology. Similar to our work, very recently Augenstein and Søgaard (2017) also evaluated on ScienceIE dataset, but use multi-task learning to improve the performance of a supervised neural approach. Instead, we introduce a semi-supervised neural tagging approach that leverages unlabeled data. Neural tagging models have been recently introduced to tagging problems such as NER. For example, Collobert et al. (2011) use a CNN over a sequence of word embeddings and apply a CRF layer on top. Huang et al. (2015) use hand-crafted features with LSTMs to improve performance. There is currently great interest in using characterbased embeddings in neural models. (Chiu and Nich"
D17-1279,D15-1041,0,0.0151072,"aset, but use multi-task learning to improve the performance of a supervised neural approach. Instead, we introduce a semi-supervised neural tagging approach that leverages unlabeled data. Neural tagging models have been recently introduced to tagging problems such as NER. For example, Collobert et al. (2011) use a CNN over a sequence of word embeddings and apply a CRF layer on top. Huang et al. (2015) use hand-crafted features with LSTMs to improve performance. There is currently great interest in using characterbased embeddings in neural models. (Chiu and Nichols, 2016; Lample et al., 2016; Ballesteros et al., 2015; Ma and Hovy, 2016). Our approach also takes advantage of neural tagging models and character-based embeddings for IE in scientific articles. Previous work on semi-supervised learning for neural models has mainly focused on transfer learning (Dai and Le, 2015; Luan et al., 2014a; Harsham et al., 2015) or initializing the model with pre-trained word embeddings (Mikolov et al., 2013; Pennington et al., 2014; Levy and Goldberg, 2014; Luan et al., 2016b, 2015, 2016a). In our work, we use pre-training but also use more powerful methods including graph-based semisupervision (Subramanya and Bilmes,"
D17-1279,bird-etal-2008-acl,0,0.109449,"sed citation sentiment (Athar and Teufel, 2012b,a), citation networks (Kas, 2011; Gabor et al., 2016; Sim et al., 2012; Do et al., 2013; Jaidka et al., 2014), summarization (Abu-Jbara and Radev, 2011) and some analysis of research community (Vogel and Jurafsky, 2012; Anderson et al., 2012; Luan et al., 2012, 2014b; Levow et al., 2014). However, due to scarce hand-annotated data resources, previous work on information extraction (IE) for scientific literature is very limited. Gupta and Manning (2011) first proposed a task that defines scientific terms for 474 abstracts from the ACL anthologhy (Bird et al., 2008) into three aspects: domain, technique and focus and apply templatebased bootstrapping to tackle the problem. Based on this study, Tsai et al. (2013) improve the performance by introducing hand-designed features from NER (Collins and Singer, 1999) to the bootstrapping framework. QasemiZadeh and Schumann (2012) compile a dataset of scientific terms into 7 fine-grained categories for 171 abstracts of ACL anothology. Similar to our work, very recently Augenstein and Søgaard (2017) also evaluated on ScienceIE dataset, but use multi-task learning to improve the performance of a supervised neural ap"
D17-1279,Q16-1026,0,0.011056,"øgaard (2017) also evaluated on ScienceIE dataset, but use multi-task learning to improve the performance of a supervised neural approach. Instead, we introduce a semi-supervised neural tagging approach that leverages unlabeled data. Neural tagging models have been recently introduced to tagging problems such as NER. For example, Collobert et al. (2011) use a CNN over a sequence of word embeddings and apply a CRF layer on top. Huang et al. (2015) use hand-crafted features with LSTMs to improve performance. There is currently great interest in using characterbased embeddings in neural models. (Chiu and Nichols, 2016; Lample et al., 2016; Ballesteros et al., 2015; Ma and Hovy, 2016). Our approach also takes advantage of neural tagging models and character-based embeddings for IE in scientific articles. Previous work on semi-supervised learning for neural models has mainly focused on transfer learning (Dai and Le, 2015; Luan et al., 2014a; Harsham et al., 2015) or initializing the model with pre-trained word embeddings (Mikolov et al., 2013; Pennington et al., 2014; Levy and Goldberg, 2014; Luan et al., 2016b, 2015, 2016a). In our work, we use pre-training but also use more powerful methods including graph"
D17-1279,W99-0613,0,0.258013,"(Vogel and Jurafsky, 2012; Anderson et al., 2012; Luan et al., 2012, 2014b; Levow et al., 2014). However, due to scarce hand-annotated data resources, previous work on information extraction (IE) for scientific literature is very limited. Gupta and Manning (2011) first proposed a task that defines scientific terms for 474 abstracts from the ACL anthologhy (Bird et al., 2008) into three aspects: domain, technique and focus and apply templatebased bootstrapping to tackle the problem. Based on this study, Tsai et al. (2013) improve the performance by introducing hand-designed features from NER (Collins and Singer, 1999) to the bootstrapping framework. QasemiZadeh and Schumann (2012) compile a dataset of scientific terms into 7 fine-grained categories for 171 abstracts of ACL anothology. Similar to our work, very recently Augenstein and Søgaard (2017) also evaluated on ScienceIE dataset, but use multi-task learning to improve the performance of a supervised neural approach. Instead, we introduce a semi-supervised neural tagging approach that leverages unlabeled data. Neural tagging models have been recently introduced to tagging problems such as NER. For example, Collobert et al. (2011) use a CNN over a seque"
D17-1279,L16-1586,0,0.0916796,"g models; ii) introducing a semi-supervised learning algorithm that uses graph-based label propagation and confidence-aware data selection, iii) exploring different alternatives for taking advantage of large, multi-domain unannotated data including both unsupervised embedding initialization and semi-supervised model training. 2 Related Work There has been growing interest in research on automatic methods to help researchers search and extract useful information from scientific literature. Past research has addressed citation sentiment (Athar and Teufel, 2012b,a), citation networks (Kas, 2011; Gabor et al., 2016; Sim et al., 2012; Do et al., 2013; Jaidka et al., 2014), summarization (Abu-Jbara and Radev, 2011) and some analysis of research community (Vogel and Jurafsky, 2012; Anderson et al., 2012; Luan et al., 2012, 2014b; Levow et al., 2014). However, due to scarce hand-annotated data resources, previous work on information extraction (IE) for scientific literature is very limited. Gupta and Manning (2011) first proposed a task that defines scientific terms for 474 abstracts from the ACL anthologhy (Bird et al., 2008) into three aspects: domain, technique and focus and apply templatebased bootstrap"
D17-1279,I11-1001,0,0.0743763,"between an [atomic force micorscopy]Material and [metal surface]Material . Figure 1: Annotated ScienceIE examples. Introduction As a research community grows, more and more papers are published each year. As a result there is increasing demand for improved methods for finding relevant papers and automatically understanding the key ideas in those papers. However, due to the large variety of domains and extremely limited annotated resources, there has been relatively little work on scientific information extraction. Previous research has focused on unsupervised approaches such as bootstrapping (Gupta and Manning, 2011; Tsai et al., 2013), where hand-designed templates are used to extract scientific keyphrases, and more templates are added through bootstrapping. Very recently a new challenge on Scientific Information Extraction (ScienceIE) (Augenstein et al., 2017)1 provides a dataset consisting of 500 1 SemEval (Task 10)https://scienceie.github. io/index.html scientific paragraphs with keyphrase annotations for three categories: TASK, P ROCESS, M ATERIAL across three scientific domains, Computer Science (CS), Material Science (MS), and Physics (Phy), as in Figure 1. This dataset enables the use of more adv"
D17-1279,P14-2050,0,0.0133013,"h LSTMs to improve performance. There is currently great interest in using characterbased embeddings in neural models. (Chiu and Nichols, 2016; Lample et al., 2016; Ballesteros et al., 2015; Ma and Hovy, 2016). Our approach also takes advantage of neural tagging models and character-based embeddings for IE in scientific articles. Previous work on semi-supervised learning for neural models has mainly focused on transfer learning (Dai and Le, 2015; Luan et al., 2014a; Harsham et al., 2015) or initializing the model with pre-trained word embeddings (Mikolov et al., 2013; Pennington et al., 2014; Levy and Goldberg, 2014; Luan et al., 2016b, 2015, 2016a). In our work, we use pre-training but also use more powerful methods including graph-based semisupervision (Subramanya and Bilmes, 2011; Liu and Kirchhoff, 2013, 2015, 2016a,b) and a method for leveraging partially labeled data (Kim et al., 2015). We show that the combination of these techniques gives better results than any one alone. 3 Problem Definition and Data The purpose of this work is to extract phrases that can answer questions that researchers usually face when reading a paper: What TASK has the paper addressed? What P ROCESS or method has the paper"
D17-1279,P16-2020,1,0.773433,"mance. There is currently great interest in using characterbased embeddings in neural models. (Chiu and Nichols, 2016; Lample et al., 2016; Ballesteros et al., 2015; Ma and Hovy, 2016). Our approach also takes advantage of neural tagging models and character-based embeddings for IE in scientific articles. Previous work on semi-supervised learning for neural models has mainly focused on transfer learning (Dai and Le, 2015; Luan et al., 2014a; Harsham et al., 2015) or initializing the model with pre-trained word embeddings (Mikolov et al., 2013; Pennington et al., 2014; Levy and Goldberg, 2014; Luan et al., 2016b, 2015, 2016a). In our work, we use pre-training but also use more powerful methods including graph-based semisupervision (Subramanya and Bilmes, 2011; Liu and Kirchhoff, 2013, 2015, 2016a,b) and a method for leveraging partially labeled data (Kim et al., 2015). We show that the combination of these techniques gives better results than any one alone. 3 Problem Definition and Data The purpose of this work is to extract phrases that can answer questions that researchers usually face when reading a paper: What TASK has the paper addressed? What P ROCESS or method has the paper used or compared t"
D17-1279,N15-1009,0,0.0782864,"ed embeddings for IE in scientific articles. Previous work on semi-supervised learning for neural models has mainly focused on transfer learning (Dai and Le, 2015; Luan et al., 2014a; Harsham et al., 2015) or initializing the model with pre-trained word embeddings (Mikolov et al., 2013; Pennington et al., 2014; Levy and Goldberg, 2014; Luan et al., 2016b, 2015, 2016a). In our work, we use pre-training but also use more powerful methods including graph-based semisupervision (Subramanya and Bilmes, 2011; Liu and Kirchhoff, 2013, 2015, 2016a,b) and a method for leveraging partially labeled data (Kim et al., 2015). We show that the combination of these techniques gives better results than any one alone. 3 Problem Definition and Data The purpose of this work is to extract phrases that can answer questions that researchers usually face when reading a paper: What TASK has the paper addressed? What P ROCESS or method has the paper used or compared to? What M ATERIALS has the paper utilized in experiments? While these fundamental concepts are important in a wide variety of scientific disciplines, the terms that are used in specific disciplines can be substantially differ2642 ent. For example, M ATERIALS in"
D17-1279,N16-1030,0,0.56146,"n et al., 2017)1 provides a dataset consisting of 500 1 SemEval (Task 10)https://scienceie.github. io/index.html scientific paragraphs with keyphrase annotations for three categories: TASK, P ROCESS, M ATERIAL across three scientific domains, Computer Science (CS), Material Science (MS), and Physics (Phy), as in Figure 1. This dataset enables the use of more advanced approaches such as neural network (NN) models. To that end, we cast the keyphrase extraction task as a sequence tagging problem, and build on recent progress in another information extraction task: Named Entity Recognition (NER) (Lample et al., 2016; Peng and Dredze, 2015). Like named entities, keyphrases can be identified by their linguistic context, e.g. researchers ”use” methods. In addition, keyphrases can be associated with different categories in different contexts. For example, ‘semantic parsing’ can be labeled as a TASK in one article and as a PROCESS in another. Scientific keyphrases differ in that they can include both noun phrases and verb phrases and in that non-standard “words” (equations, chemical compounds, references) can provide important cues. Since the scale of the data is still small for supervised training of neural"
D17-1279,P16-1101,0,0.0122727,"earning to improve the performance of a supervised neural approach. Instead, we introduce a semi-supervised neural tagging approach that leverages unlabeled data. Neural tagging models have been recently introduced to tagging problems such as NER. For example, Collobert et al. (2011) use a CNN over a sequence of word embeddings and apply a CRF layer on top. Huang et al. (2015) use hand-crafted features with LSTMs to improve performance. There is currently great interest in using characterbased embeddings in neural models. (Chiu and Nichols, 2016; Lample et al., 2016; Ballesteros et al., 2015; Ma and Hovy, 2016). Our approach also takes advantage of neural tagging models and character-based embeddings for IE in scientific articles. Previous work on semi-supervised learning for neural models has mainly focused on transfer learning (Dai and Le, 2015; Luan et al., 2014a; Harsham et al., 2015) or initializing the model with pre-trained word embeddings (Mikolov et al., 2013; Pennington et al., 2014; Levy and Goldberg, 2014; Luan et al., 2016b, 2015, 2016a). In our work, we use pre-training but also use more powerful methods including graph-based semisupervision (Subramanya and Bilmes, 2011; Liu and Kirchh"
D17-1279,P14-5010,0,0.0251778,"each domain. Two special tokens BOS and EOS are added when pre-training, indicating the begin and end of a sentence. The number of the graph vertices is 2M in tranductive setting and 1.4M in inductive setting. The ULM parameter η in Eq. 4 is tuned from 0.1 to 0.9, the best η is 0.4. The best parameters of label propagation are µ = 10−6 and ν = 10−5 . The interpolation parameter α in Eq. 3 is tuned from 0.1 to 0.9, the best α is 0.3. We do iteration of semi-supervised learning until we obtain the best result on the dev set, which is mostly achieved in the second round. We use Stanford CoreNLP (Manning et al., 2014) tokenizer to tokenize words. The tokenizer is augmented with a few hand-designed rules to handle equations (e.g. “fs(B,t)=Spel(t)S” is a single token) and other non-standard word phenomena (Cu40Zn, 20MW/m2) in scientific literature. We use Approximate Nearest Neighbor Searching (ANN)4 to calculate the k-nearest neighbors. For all experiments in this paper, k = 10. Setup We evaluate our system in both inductive and transductive settings. The systems with a ∗ superscript in the table are transductive. The inductive setting uses 400 full articles in ScienceIE training and dev sets, while the tra"
D17-1279,D15-1064,0,0.0316089,"ides a dataset consisting of 500 1 SemEval (Task 10)https://scienceie.github. io/index.html scientific paragraphs with keyphrase annotations for three categories: TASK, P ROCESS, M ATERIAL across three scientific domains, Computer Science (CS), Material Science (MS), and Physics (Phy), as in Figure 1. This dataset enables the use of more advanced approaches such as neural network (NN) models. To that end, we cast the keyphrase extraction task as a sequence tagging problem, and build on recent progress in another information extraction task: Named Entity Recognition (NER) (Lample et al., 2016; Peng and Dredze, 2015). Like named entities, keyphrases can be identified by their linguistic context, e.g. researchers ”use” methods. In addition, keyphrases can be associated with different categories in different contexts. For example, ‘semantic parsing’ can be labeled as a TASK in one article and as a PROCESS in another. Scientific keyphrases differ in that they can include both noun phrases and verb phrases and in that non-standard “words” (equations, chemical compounds, references) can provide important cues. Since the scale of the data is still small for supervised training of neural systems, we introduce se"
D17-1279,D14-1162,0,0.0809487,"hand-crafted features with LSTMs to improve performance. There is currently great interest in using characterbased embeddings in neural models. (Chiu and Nichols, 2016; Lample et al., 2016; Ballesteros et al., 2015; Ma and Hovy, 2016). Our approach also takes advantage of neural tagging models and character-based embeddings for IE in scientific articles. Previous work on semi-supervised learning for neural models has mainly focused on transfer learning (Dai and Le, 2015; Luan et al., 2014a; Harsham et al., 2015) or initializing the model with pre-trained word embeddings (Mikolov et al., 2013; Pennington et al., 2014; Levy and Goldberg, 2014; Luan et al., 2016b, 2015, 2016a). In our work, we use pre-training but also use more powerful methods including graph-based semisupervision (Subramanya and Bilmes, 2011; Liu and Kirchhoff, 2013, 2015, 2016a,b) and a method for leveraging partially labeled data (Kim et al., 2015). We show that the combination of these techniques gives better results than any one alone. 3 Problem Definition and Data The purpose of this work is to extract phrases that can answer questions that researchers usually face when reading a paper: What TASK has the paper addressed? What P ROCES"
D17-1279,W12-3203,0,0.159301,"ucing a semi-supervised learning algorithm that uses graph-based label propagation and confidence-aware data selection, iii) exploring different alternatives for taking advantage of large, multi-domain unannotated data including both unsupervised embedding initialization and semi-supervised model training. 2 Related Work There has been growing interest in research on automatic methods to help researchers search and extract useful information from scientific literature. Past research has addressed citation sentiment (Athar and Teufel, 2012b,a), citation networks (Kas, 2011; Gabor et al., 2016; Sim et al., 2012; Do et al., 2013; Jaidka et al., 2014), summarization (Abu-Jbara and Radev, 2011) and some analysis of research community (Vogel and Jurafsky, 2012; Anderson et al., 2012; Luan et al., 2012, 2014b; Levow et al., 2014). However, due to scarce hand-annotated data resources, previous work on information extraction (IE) for scientific literature is very limited. Gupta and Manning (2011) first proposed a task that defines scientific terms for 474 abstracts from the ACL anthologhy (Bird et al., 2008) into three aspects: domain, technique and focus and apply templatebased bootstrapping to tackle the"
D17-1279,D10-1017,0,0.0224238,"Missing"
D17-1279,W12-3204,0,0.0712801,"rent alternatives for taking advantage of large, multi-domain unannotated data including both unsupervised embedding initialization and semi-supervised model training. 2 Related Work There has been growing interest in research on automatic methods to help researchers search and extract useful information from scientific literature. Past research has addressed citation sentiment (Athar and Teufel, 2012b,a), citation networks (Kas, 2011; Gabor et al., 2016; Sim et al., 2012; Do et al., 2013; Jaidka et al., 2014), summarization (Abu-Jbara and Radev, 2011) and some analysis of research community (Vogel and Jurafsky, 2012; Anderson et al., 2012; Luan et al., 2012, 2014b; Levow et al., 2014). However, due to scarce hand-annotated data resources, previous work on information extraction (IE) for scientific literature is very limited. Gupta and Manning (2011) first proposed a task that defines scientific terms for 474 abstracts from the ACL anthologhy (Bird et al., 2008) into three aspects: domain, technique and focus and apply templatebased bootstrapping to tackle the problem. Based on this study, Tsai et al. (2013) improve the performance by introducing hand-designed features from NER (Collins and Singer, 1999)"
D17-1279,L16-1294,0,\N,Missing
D18-1360,P11-1051,0,0.034294,"Missing"
D18-1360,D17-1181,0,0.0731328,"Missing"
D18-1360,N18-3011,0,0.139142,"Missing"
D18-1360,S17-2097,0,0.0608893,"2017 and 2018 have been introduced, which facilitate research on supervised and semi-supervised learning for scientific information extraction. SemEval 17 (Augenstein et al., 2017) includes 500 paragraphs from articles in the domains of computer science, physics, and material science. It includes three types of entities (called keyphrases): Tasks, Methods, and Materials and two relation types: hyponym-of and synonym-of. SemEval 18 (G´abor et al., 2018) is focused on predicting relations between entities within a sentence. It consists of six relation types. Using these datasets, neural models (Ammar et al., 2017, 2018; Luan et al., 2017b; Augenstein and Søgaard, 2017) are introduced for extracting scientific information. We extend these datasets by increasing relation coverage, adding cross-sentence coreference linking, and removing some annotation constraints. Different from most previous IE systems for scientific literature and general domains (Miwa and Bansal, 2016; Xu et al., 2016; Peng et al., 2017; Quirk and Poon, 2017; Luan et al., 2018; Adel and Sch¨utze, 2017), which use preprocessed syntactic, discourse or coreference features as input, our unified framework does not rely on any pipeline pr"
D18-1360,W12-3202,0,0.0187928,"oreference clusters without hand-engineered features. We use our unified framework to build a scientific knowledge graph from a large collection of documents and analyze information in scientific literature. 2 Related Work There has been growing interest in research on automatic methods for information extraction from scientific articles. Past research in scientific IE addressed analyzing citations (Athar and Teufel, 2012b,a; Kas, 2011; Gabor et al., 2016; Sim et al., 2012; Do et al., 2013; Jaidka et al., 2014; AbuJbara and Radev, 2011), analyzing research community (Vogel and Jurafsky, 2012; Anderson et al., 2012), and unsupervised methods for extracting scientific entities and relations (Gupta and Manning, 2011; Tsai et al., 2013; G´abor et al., 2016). More recently, two datasets in SemEval 2017 and 2018 have been introduced, which facilitate research on supervised and semi-supervised learning for scientific information extraction. SemEval 17 (Augenstein et al., 2017) includes 500 paragraphs from articles in the domains of computer science, physics, and material science. It includes three types of entities (called keyphrases): Tasks, Methods, and Materials and two relation types: hyponym-of and synony"
D18-1360,S17-2091,0,0.38619,"ny pipeline processing and is able to model overlapping spans. While Singh et al. (2013) show improvements by jointly modeling entities, relations, and coreference links, most recent neural models for these tasks focus on single tasks (Clark and Manning, 2016; Wiseman et al., 2016; Lee et al., 2017; Lample et al., 2016; Peng et al., 2017) or joint entity and relation extraction (Katiyar and Cardie, 2017; Zhang et al., 2017; Adel and Sch¨utze, 2017; Zheng et al., 2017). Among those studies, many papers assume the entity boundaries are given, such as (Clark and Manning, 2016), Adel and Sch¨utze (2017) and Peng et al. (2017). Our work relaxes this constraint and predicts entity boundaries by optimizing over all possible spans. Our model draws from recent end-to-end span-based models for coreference resolution (Lee et al., 2017, 2018) and semantic role labeling (He et al., 2018) and extends them for the multi-task framework involving the three tasks of identification of entity, relation and coreference. Neural multi-task learning has been applied to a range of NLP tasks. Most of these models share word-level representations (Collobert and Weston, 2008; Klerke et al., 2016; Luan et al., 2016,"
D18-1360,P17-2054,0,0.0351087,"litate research on supervised and semi-supervised learning for scientific information extraction. SemEval 17 (Augenstein et al., 2017) includes 500 paragraphs from articles in the domains of computer science, physics, and material science. It includes three types of entities (called keyphrases): Tasks, Methods, and Materials and two relation types: hyponym-of and synonym-of. SemEval 18 (G´abor et al., 2018) is focused on predicting relations between entities within a sentence. It consists of six relation types. Using these datasets, neural models (Ammar et al., 2017, 2018; Luan et al., 2017b; Augenstein and Søgaard, 2017) are introduced for extracting scientific information. We extend these datasets by increasing relation coverage, adding cross-sentence coreference linking, and removing some annotation constraints. Different from most previous IE systems for scientific literature and general domains (Miwa and Bansal, 2016; Xu et al., 2016; Peng et al., 2017; Quirk and Poon, 2017; Luan et al., 2018; Adel and Sch¨utze, 2017), which use preprocessed syntactic, discourse or coreference features as input, our unified framework does not rely on any pipeline processing and is able to model overlapping spans. While Si"
D18-1360,P16-1061,0,0.0307772,"ing some annotation constraints. Different from most previous IE systems for scientific literature and general domains (Miwa and Bansal, 2016; Xu et al., 2016; Peng et al., 2017; Quirk and Poon, 2017; Luan et al., 2018; Adel and Sch¨utze, 2017), which use preprocessed syntactic, discourse or coreference features as input, our unified framework does not rely on any pipeline processing and is able to model overlapping spans. While Singh et al. (2013) show improvements by jointly modeling entities, relations, and coreference links, most recent neural models for these tasks focus on single tasks (Clark and Manning, 2016; Wiseman et al., 2016; Lee et al., 2017; Lample et al., 2016; Peng et al., 2017) or joint entity and relation extraction (Katiyar and Cardie, 2017; Zhang et al., 2017; Adel and Sch¨utze, 2017; Zheng et al., 2017). Among those studies, many papers assume the entity boundaries are given, such as (Clark and Manning, 2016), Adel and Sch¨utze (2017) and Peng et al. (2017). Our work relaxes this constraint and predicts entity boundaries by optimizing over all possible spans. Our model draws from recent end-to-end span-based models for coreference resolution (Lee et al., 2017, 2018) and semantic rol"
D18-1360,L16-1586,0,0.0363768,"Extending a previous end-to-end coreference resolution system, we develop a multi-task learning framework that can detect scientific entities, relations, and coreference clusters without hand-engineered features. We use our unified framework to build a scientific knowledge graph from a large collection of documents and analyze information in scientific literature. 2 Related Work There has been growing interest in research on automatic methods for information extraction from scientific articles. Past research in scientific IE addressed analyzing citations (Athar and Teufel, 2012b,a; Kas, 2011; Gabor et al., 2016; Sim et al., 2012; Do et al., 2013; Jaidka et al., 2014; AbuJbara and Radev, 2011), analyzing research community (Vogel and Jurafsky, 2012; Anderson et al., 2012), and unsupervised methods for extracting scientific entities and relations (Gupta and Manning, 2011; Tsai et al., 2013; G´abor et al., 2016). More recently, two datasets in SemEval 2017 and 2018 have been introduced, which facilitate research on supervised and semi-supervised learning for scientific information extraction. SemEval 17 (Augenstein et al., 2017) includes 500 paragraphs from articles in the domains of computer science,"
D18-1360,I11-1001,0,0.608513,"that refer to the same scientific concept, including generic terms (such as the pronoun it, or phrases like our method) that are not informative by themselves. With co-reference, context-free grammar can be connected to MORPA through the intermediate co-referred pronoun it. Applying existing IE systems to this data, without co-reference, will result in much lower relation coverage (and a sparse knowledge base). In this paper, we develop a unified learning model for extracting scientific entities, relations, and coreference resolution. This is different from previous work (Luan et al., 2017b; Gupta and Manning, 2011; Tsai et al., 2013; G´abor et al., 2018) which often addresses these tasks as independent 3219 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 3219–3232 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics components of a pipeline. Our unified model is a multi-task setup that shares parameters across low-level tasks, making predictions by leveraging context across the document through coreference links. Specifically, we extend prior work for learning span representations and coreference resolution (Lee"
D18-1360,P18-2058,1,0.910122,"2013; G´abor et al., 2018) which often addresses these tasks as independent 3219 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 3219–3232 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics components of a pipeline. Our unified model is a multi-task setup that shares parameters across low-level tasks, making predictions by leveraging context across the document through coreference links. Specifically, we extend prior work for learning span representations and coreference resolution (Lee et al., 2017; He et al., 2018). Different from a standard tagging system, our system enumerates all possible spans during decoding and can effectively detect overlapped spans. It avoids cascading errors between tasks by jointly modeling all spans and span-span relations. To explore this problem, we create a dataset S CI ERC for scientific information extraction, which includes annotations of scientific terms, relation categories and co-reference links. Our experiments show that the unified model is better at predicting span boundaries, and it outperforms previous state-of-the-art scientific IE systems on entity and relatio"
D18-1360,P17-1085,0,0.0391968,"et al., 2016; Peng et al., 2017; Quirk and Poon, 2017; Luan et al., 2018; Adel and Sch¨utze, 2017), which use preprocessed syntactic, discourse or coreference features as input, our unified framework does not rely on any pipeline processing and is able to model overlapping spans. While Singh et al. (2013) show improvements by jointly modeling entities, relations, and coreference links, most recent neural models for these tasks focus on single tasks (Clark and Manning, 2016; Wiseman et al., 2016; Lee et al., 2017; Lample et al., 2016; Peng et al., 2017) or joint entity and relation extraction (Katiyar and Cardie, 2017; Zhang et al., 2017; Adel and Sch¨utze, 2017; Zheng et al., 2017). Among those studies, many papers assume the entity boundaries are given, such as (Clark and Manning, 2016), Adel and Sch¨utze (2017) and Peng et al. (2017). Our work relaxes this constraint and predicts entity boundaries by optimizing over all possible spans. Our model draws from recent end-to-end span-based models for coreference resolution (Lee et al., 2017, 2018) and semantic role labeling (He et al., 2018) and extends them for the multi-task framework involving the three tasks of identification of entity, relation and core"
D18-1360,N16-1179,0,0.0276611,"anning, 2016), Adel and Sch¨utze (2017) and Peng et al. (2017). Our work relaxes this constraint and predicts entity boundaries by optimizing over all possible spans. Our model draws from recent end-to-end span-based models for coreference resolution (Lee et al., 2017, 2018) and semantic role labeling (He et al., 2018) and extends them for the multi-task framework involving the three tasks of identification of entity, relation and coreference. Neural multi-task learning has been applied to a range of NLP tasks. Most of these models share word-level representations (Collobert and Weston, 2008; Klerke et al., 2016; Luan et al., 2016, 2017a; Rei, 2017), while Peng et al. (2017) uses high-order cross-task factors. Our model instead propagates 3220 cross-task information via span representations, which is related to Swayamdipta et al. (2017). 3 Statistics #Entities #Relations #Relations/Doc #Coref links #Coref clusters Dataset Our dataset (called S CI ERC) includes annotations for scientific entities, their relations, and coreference clusters for 500 scientific abstracts. These abstracts are taken from 12 AI conference/workshop proceedings in four AI communities from the Semantic Scholar Corpus2 . S CI ER"
D18-1360,N16-1030,0,0.246994,"systems for scientific literature and general domains (Miwa and Bansal, 2016; Xu et al., 2016; Peng et al., 2017; Quirk and Poon, 2017; Luan et al., 2018; Adel and Sch¨utze, 2017), which use preprocessed syntactic, discourse or coreference features as input, our unified framework does not rely on any pipeline processing and is able to model overlapping spans. While Singh et al. (2013) show improvements by jointly modeling entities, relations, and coreference links, most recent neural models for these tasks focus on single tasks (Clark and Manning, 2016; Wiseman et al., 2016; Lee et al., 2017; Lample et al., 2016; Peng et al., 2017) or joint entity and relation extraction (Katiyar and Cardie, 2017; Zhang et al., 2017; Adel and Sch¨utze, 2017; Zheng et al., 2017). Among those studies, many papers assume the entity boundaries are given, such as (Clark and Manning, 2016), Adel and Sch¨utze (2017) and Peng et al. (2017). Our work relaxes this constraint and predicts entity boundaries by optimizing over all possible spans. Our model draws from recent end-to-end span-based models for coreference resolution (Lee et al., 2017, 2018) and semantic role labeling (He et al., 2018) and extends them for the multi-t"
D18-1360,D17-1018,1,0.224528,"011; Tsai et al., 2013; G´abor et al., 2018) which often addresses these tasks as independent 3219 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 3219–3232 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics components of a pipeline. Our unified model is a multi-task setup that shares parameters across low-level tasks, making predictions by leveraging context across the document through coreference links. Specifically, we extend prior work for learning span representations and coreference resolution (Lee et al., 2017; He et al., 2018). Different from a standard tagging system, our system enumerates all possible spans during decoding and can effectively detect overlapped spans. It avoids cascading errors between tasks by jointly modeling all spans and span-span relations. To explore this problem, we create a dataset S CI ERC for scientific information extraction, which includes annotations of scientific terms, relation categories and co-reference links. Our experiments show that the unified model is better at predicting span boundaries, and it outperforms previous state-of-the-art scientific IE systems on"
D18-1360,N18-2108,1,0.89945,"Missing"
D18-1360,I17-1061,1,0.790259,"onnected by entities that refer to the same scientific concept, including generic terms (such as the pronoun it, or phrases like our method) that are not informative by themselves. With co-reference, context-free grammar can be connected to MORPA through the intermediate co-referred pronoun it. Applying existing IE systems to this data, without co-reference, will result in much lower relation coverage (and a sparse knowledge base). In this paper, we develop a unified learning model for extracting scientific entities, relations, and coreference resolution. This is different from previous work (Luan et al., 2017b; Gupta and Manning, 2011; Tsai et al., 2013; G´abor et al., 2018) which often addresses these tasks as independent 3219 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 3219–3232 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics components of a pipeline. Our unified model is a multi-task setup that shares parameters across low-level tasks, making predictions by leveraging context across the document through coreference links. Specifically, we extend prior work for learning span representations and c"
D18-1360,P16-2020,1,0.809336,"nd Sch¨utze (2017) and Peng et al. (2017). Our work relaxes this constraint and predicts entity boundaries by optimizing over all possible spans. Our model draws from recent end-to-end span-based models for coreference resolution (Lee et al., 2017, 2018) and semantic role labeling (He et al., 2018) and extends them for the multi-task framework involving the three tasks of identification of entity, relation and coreference. Neural multi-task learning has been applied to a range of NLP tasks. Most of these models share word-level representations (Collobert and Weston, 2008; Klerke et al., 2016; Luan et al., 2016, 2017a; Rei, 2017), while Peng et al. (2017) uses high-order cross-task factors. Our model instead propagates 3220 cross-task information via span representations, which is related to Swayamdipta et al. (2017). 3 Statistics #Entities #Relations #Relations/Doc #Coref links #Coref clusters Dataset Our dataset (called S CI ERC) includes annotations for scientific entities, their relations, and coreference clusters for 500 scientific abstracts. These abstracts are taken from 12 AI conference/workshop proceedings in four AI communities from the Semantic Scholar Corpus2 . S CI ERC extends previous"
D18-1360,D17-1279,1,0.790305,"onnected by entities that refer to the same scientific concept, including generic terms (such as the pronoun it, or phrases like our method) that are not informative by themselves. With co-reference, context-free grammar can be connected to MORPA through the intermediate co-referred pronoun it. Applying existing IE systems to this data, without co-reference, will result in much lower relation coverage (and a sparse knowledge base). In this paper, we develop a unified learning model for extracting scientific entities, relations, and coreference resolution. This is different from previous work (Luan et al., 2017b; Gupta and Manning, 2011; Tsai et al., 2013; G´abor et al., 2018) which often addresses these tasks as independent 3219 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 3219–3232 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics components of a pipeline. Our unified model is a multi-task setup that shares parameters across low-level tasks, making predictions by leveraging context across the document through coreference links. Specifically, we extend prior work for learning span representations and c"
D18-1360,S18-1125,1,0.840133,"r et al., 2018) is focused on predicting relations between entities within a sentence. It consists of six relation types. Using these datasets, neural models (Ammar et al., 2017, 2018; Luan et al., 2017b; Augenstein and Søgaard, 2017) are introduced for extracting scientific information. We extend these datasets by increasing relation coverage, adding cross-sentence coreference linking, and removing some annotation constraints. Different from most previous IE systems for scientific literature and general domains (Miwa and Bansal, 2016; Xu et al., 2016; Peng et al., 2017; Quirk and Poon, 2017; Luan et al., 2018; Adel and Sch¨utze, 2017), which use preprocessed syntactic, discourse or coreference features as input, our unified framework does not rely on any pipeline processing and is able to model overlapping spans. While Singh et al. (2013) show improvements by jointly modeling entities, relations, and coreference links, most recent neural models for these tasks focus on single tasks (Clark and Manning, 2016; Wiseman et al., 2016; Lee et al., 2017; Lample et al., 2016; Peng et al., 2017) or joint entity and relation extraction (Katiyar and Cardie, 2017; Zhang et al., 2017; Adel and Sch¨utze, 2017; Z"
D18-1360,P16-1105,0,0.311677,"nd Materials and two relation types: hyponym-of and synonym-of. SemEval 18 (G´abor et al., 2018) is focused on predicting relations between entities within a sentence. It consists of six relation types. Using these datasets, neural models (Ammar et al., 2017, 2018; Luan et al., 2017b; Augenstein and Søgaard, 2017) are introduced for extracting scientific information. We extend these datasets by increasing relation coverage, adding cross-sentence coreference linking, and removing some annotation constraints. Different from most previous IE systems for scientific literature and general domains (Miwa and Bansal, 2016; Xu et al., 2016; Peng et al., 2017; Quirk and Poon, 2017; Luan et al., 2018; Adel and Sch¨utze, 2017), which use preprocessed syntactic, discourse or coreference features as input, our unified framework does not rely on any pipeline processing and is able to model overlapping spans. While Singh et al. (2013) show improvements by jointly modeling entities, relations, and coreference links, most recent neural models for these tasks focus on single tasks (Clark and Manning, 2016; Wiseman et al., 2016; Lee et al., 2017; Lample et al., 2016; Peng et al., 2017) or joint entity and relation extract"
D18-1360,Q17-1008,0,0.197674,"onym-of and synonym-of. SemEval 18 (G´abor et al., 2018) is focused on predicting relations between entities within a sentence. It consists of six relation types. Using these datasets, neural models (Ammar et al., 2017, 2018; Luan et al., 2017b; Augenstein and Søgaard, 2017) are introduced for extracting scientific information. We extend these datasets by increasing relation coverage, adding cross-sentence coreference linking, and removing some annotation constraints. Different from most previous IE systems for scientific literature and general domains (Miwa and Bansal, 2016; Xu et al., 2016; Peng et al., 2017; Quirk and Poon, 2017; Luan et al., 2018; Adel and Sch¨utze, 2017), which use preprocessed syntactic, discourse or coreference features as input, our unified framework does not rely on any pipeline processing and is able to model overlapping spans. While Singh et al. (2013) show improvements by jointly modeling entities, relations, and coreference links, most recent neural models for these tasks focus on single tasks (Clark and Manning, 2016; Wiseman et al., 2016; Lee et al., 2017; Lample et al., 2016; Peng et al., 2017) or joint entity and relation extraction (Katiyar and Cardie, 2017; Zhang"
D18-1360,P17-1161,0,0.031299,"on Triples Figure 4: A part of an automatically constructed Experimental Setup We evaluate our unified framework S CI IE on S CI ERC and SemEval 17. The knowledge graph for 3224 • LSTM+CRF The state-of-the-art NER system (Lample et al., 2016), which applies CRF on top of LSTM for named entity tagging, the approach has also been used in scientific term extraction (Luan et al., 2017b). • LSTM+CRF+ELMo LSTM+CRF ELM O as an additional input feature. with • E2E Rel State-of-the-art joint entity and relation extraction system (Miwa and Bansal, 2016) that has also been used in scientific literature (Peters et al., 2017; Augenstein et al., 2017). This system uses syntactic features such as part-of-speech tagging and dependency parsing. • E2E Rel(Pipeline) Pipeline setting of E2E Rel. Extract entities first and use entity results as input to relation extraction task. Dev Model LSTM+CRF LSTM+CRF+ELMo E2E Rel(Pipeline) E2E Rel E2E Rel+ELM O S CI IE • E2E Rel+ELMo E2E Rel with ELM O as an additional input feature. • E2E Coref State-of-the-art coreference system Lee et al. (2017) combined with ELM O. Our system S CI IE extends E2E Coref with multi-task learning. Test P R F1 P R F1 67.2 68.1 66.7 64.3 67.5 70.0 65"
D18-1360,N18-1202,0,0.0333363,"es to compute the different Φ: ΦE (e, si ) = φe (si ) (4) ΦR (r, si , sj ) = φmr (si ) + φmr (sj ) + φr (si , sj ) ΦC (si , sj ) = φmc (si ) + φmc (sj ) + φc (si , sj ) The scores in Equation (4) are defined for entity types, relations, and antecedents that are not the null-type . Scores involving the null label are set to a constant 0: ΦE (, si ) = ΦR (, si , sj ) = ΦC (si , ) = 0. We use the same span representations g from (Lee et al., 2017) and share them across the three tasks. We start by building bi-directional LSTMs (Hochreiter and Schmidhuber, 1997) from word, character and ELMo (Peters et al., 2018) embeddings. For a span si , its vector representation gi is constructed by concatenating si ’s left and right end points from the BiLSTM outputs, an attentionbased soft “headword,” and embedded span width features. Hyperparameters and other implementation details will be described in Section 6. 4.4 Inference and Pruning Following previous work, we use beam pruning to reduce the number of pairwise span factors from O(n4 ) to O(n2 ) at both training and test time, where n is the number of words in the document. We define two separate beams: BC to prune spans for the coreference resolution task,"
D18-1360,L16-1294,0,0.0979017,"Missing"
D18-1360,E17-1110,0,0.0299337,"-of. SemEval 18 (G´abor et al., 2018) is focused on predicting relations between entities within a sentence. It consists of six relation types. Using these datasets, neural models (Ammar et al., 2017, 2018; Luan et al., 2017b; Augenstein and Søgaard, 2017) are introduced for extracting scientific information. We extend these datasets by increasing relation coverage, adding cross-sentence coreference linking, and removing some annotation constraints. Different from most previous IE systems for scientific literature and general domains (Miwa and Bansal, 2016; Xu et al., 2016; Peng et al., 2017; Quirk and Poon, 2017; Luan et al., 2018; Adel and Sch¨utze, 2017), which use preprocessed syntactic, discourse or coreference features as input, our unified framework does not rely on any pipeline processing and is able to model overlapping spans. While Singh et al. (2013) show improvements by jointly modeling entities, relations, and coreference links, most recent neural models for these tasks focus on single tasks (Clark and Manning, 2016; Wiseman et al., 2016; Lee et al., 2017; Lample et al., 2016; Peng et al., 2017) or joint entity and relation extraction (Katiyar and Cardie, 2017; Zhang et al., 2017; Adel an"
D18-1360,J86-2003,0,0.048649,"Missing"
D18-1360,N16-1114,0,0.0782222,"Missing"
D18-1360,C16-1138,0,0.0255081,"lation types: hyponym-of and synonym-of. SemEval 18 (G´abor et al., 2018) is focused on predicting relations between entities within a sentence. It consists of six relation types. Using these datasets, neural models (Ammar et al., 2017, 2018; Luan et al., 2017b; Augenstein and Søgaard, 2017) are introduced for extracting scientific information. We extend these datasets by increasing relation coverage, adding cross-sentence coreference linking, and removing some annotation constraints. Different from most previous IE systems for scientific literature and general domains (Miwa and Bansal, 2016; Xu et al., 2016; Peng et al., 2017; Quirk and Poon, 2017; Luan et al., 2018; Adel and Sch¨utze, 2017), which use preprocessed syntactic, discourse or coreference features as input, our unified framework does not rely on any pipeline processing and is able to model overlapping spans. While Singh et al. (2013) show improvements by jointly modeling entities, relations, and coreference links, most recent neural models for these tasks focus on single tasks (Clark and Manning, 2016; Wiseman et al., 2016; Lee et al., 2017; Lample et al., 2016; Peng et al., 2017) or joint entity and relation extraction (Katiyar and"
D18-1360,Q15-1009,0,0.0489779,"Missing"
D18-1360,D17-1182,0,0.0830998,"Missing"
D18-1360,P17-1113,0,0.0653047,"8; Adel and Sch¨utze, 2017), which use preprocessed syntactic, discourse or coreference features as input, our unified framework does not rely on any pipeline processing and is able to model overlapping spans. While Singh et al. (2013) show improvements by jointly modeling entities, relations, and coreference links, most recent neural models for these tasks focus on single tasks (Clark and Manning, 2016; Wiseman et al., 2016; Lee et al., 2017; Lample et al., 2016; Peng et al., 2017) or joint entity and relation extraction (Katiyar and Cardie, 2017; Zhang et al., 2017; Adel and Sch¨utze, 2017; Zheng et al., 2017). Among those studies, many papers assume the entity boundaries are given, such as (Clark and Manning, 2016), Adel and Sch¨utze (2017) and Peng et al. (2017). Our work relaxes this constraint and predicts entity boundaries by optimizing over all possible spans. Our model draws from recent end-to-end span-based models for coreference resolution (Lee et al., 2017, 2018) and semantic role labeling (He et al., 2018) and extends them for the multi-task framework involving the three tasks of identification of entity, relation and coreference. Neural multi-task learning has been applied to a range of"
D18-1360,P17-1194,0,0.0213066,"g et al. (2017). Our work relaxes this constraint and predicts entity boundaries by optimizing over all possible spans. Our model draws from recent end-to-end span-based models for coreference resolution (Lee et al., 2017, 2018) and semantic role labeling (He et al., 2018) and extends them for the multi-task framework involving the three tasks of identification of entity, relation and coreference. Neural multi-task learning has been applied to a range of NLP tasks. Most of these models share word-level representations (Collobert and Weston, 2008; Klerke et al., 2016; Luan et al., 2016, 2017a; Rei, 2017), while Peng et al. (2017) uses high-order cross-task factors. Our model instead propagates 3220 cross-task information via span representations, which is related to Swayamdipta et al. (2017). 3 Statistics #Entities #Relations #Relations/Doc #Coref links #Coref clusters Dataset Our dataset (called S CI ERC) includes annotations for scientific entities, their relations, and coreference clusters for 500 scientific abstracts. These abstracts are taken from 12 AI conference/workshop proceedings in four AI communities from the Semantic Scholar Corpus2 . S CI ERC extends previous datasets in scienti"
D18-1360,W12-3203,0,0.0547651,"end-to-end coreference resolution system, we develop a multi-task learning framework that can detect scientific entities, relations, and coreference clusters without hand-engineered features. We use our unified framework to build a scientific knowledge graph from a large collection of documents and analyze information in scientific literature. 2 Related Work There has been growing interest in research on automatic methods for information extraction from scientific articles. Past research in scientific IE addressed analyzing citations (Athar and Teufel, 2012b,a; Kas, 2011; Gabor et al., 2016; Sim et al., 2012; Do et al., 2013; Jaidka et al., 2014; AbuJbara and Radev, 2011), analyzing research community (Vogel and Jurafsky, 2012; Anderson et al., 2012), and unsupervised methods for extracting scientific entities and relations (Gupta and Manning, 2011; Tsai et al., 2013; G´abor et al., 2016). More recently, two datasets in SemEval 2017 and 2018 have been introduced, which facilitate research on supervised and semi-supervised learning for scientific information extraction. SemEval 17 (Augenstein et al., 2017) includes 500 paragraphs from articles in the domains of computer science, physics, and mater"
D18-1360,W16-1300,0,0.129169,"Missing"
D18-1360,E12-2021,0,0.0929946,"Missing"
D18-1360,W12-3204,0,0.0604475,"entities, relations, and coreference clusters without hand-engineered features. We use our unified framework to build a scientific knowledge graph from a large collection of documents and analyze information in scientific literature. 2 Related Work There has been growing interest in research on automatic methods for information extraction from scientific articles. Past research in scientific IE addressed analyzing citations (Athar and Teufel, 2012b,a; Kas, 2011; Gabor et al., 2016; Sim et al., 2012; Do et al., 2013; Jaidka et al., 2014; AbuJbara and Radev, 2011), analyzing research community (Vogel and Jurafsky, 2012; Anderson et al., 2012), and unsupervised methods for extracting scientific entities and relations (Gupta and Manning, 2011; Tsai et al., 2013; G´abor et al., 2016). More recently, two datasets in SemEval 2017 and 2018 have been introduced, which facilitate research on supervised and semi-supervised learning for scientific information extraction. SemEval 17 (Augenstein et al., 2017) includes 500 paragraphs from articles in the domains of computer science, physics, and material science. It includes three types of entities (called keyphrases): Tasks, Methods, and Materials and two relation type"
D18-1360,W12-4303,0,\N,Missing
H05-1030,W02-1007,1,0.908554,"Missing"
H05-1030,graff-bird-2000-many,0,0.311885,"et of opportunities and challenges. While new obstacles arise from the presence of speech repairs, the possibility of word errors, and the absence of punctuation and sentence boundaries, speech also presents a tremendous opportunity to leverage multi-modal input, in the form of acoustic or even visual cues. As a step in this direction, this paper identifies a set of useful prosodic features and describes how they can be effectively incorporated into a statistical parsing model, ignoring for now the problem of word errors. Evaluated on the Switchboard corpus of conversational telephone speech (Graff and Bird, 2000), our prosody-aware parser out-performs a state-of-the-art system that uses lexical and syntactic features only. While we are not the first to employ prosodic cues in a statistical parsing model, previous efforts (Gregory et al., 2004; Kahn et al., 2004) incorporated these features as word tokens and thereby suffered from the side-effect of displacing words in the n-gram models by the parser. To avoid this problem, we generate a set of candidate parses using an off-the-shelf, k-best parser, and use prosodic (and other) features to rescore the candidate parses. Our system architecture combines"
H05-1030,N04-1011,1,0.863186,"leverage multi-modal input, in the form of acoustic or even visual cues. As a step in this direction, this paper identifies a set of useful prosodic features and describes how they can be effectively incorporated into a statistical parsing model, ignoring for now the problem of word errors. Evaluated on the Switchboard corpus of conversational telephone speech (Graff and Bird, 2000), our prosody-aware parser out-performs a state-of-the-art system that uses lexical and syntactic features only. While we are not the first to employ prosodic cues in a statistical parsing model, previous efforts (Gregory et al., 2004; Kahn et al., 2004) incorporated these features as word tokens and thereby suffered from the side-effect of displacing words in the n-gram models by the parser. To avoid this problem, we generate a set of candidate parses using an off-the-shelf, k-best parser, and use prosodic (and other) features to rescore the candidate parses. Our system architecture combines earlier models proposed for parse reranking (Collins, 2000) and filtering out edit regions (Charniak and Johnson, 2001). Detecting and removing edits prior to parsing is motivated by the claim that probabilistic contextfree grammars ("
H05-1030,P83-1019,0,0.0846047,"Missing"
H05-1030,N04-4032,1,0.919297,"input, in the form of acoustic or even visual cues. As a step in this direction, this paper identifies a set of useful prosodic features and describes how they can be effectively incorporated into a statistical parsing model, ignoring for now the problem of word errors. Evaluated on the Switchboard corpus of conversational telephone speech (Graff and Bird, 2000), our prosody-aware parser out-performs a state-of-the-art system that uses lexical and syntactic features only. While we are not the first to employ prosodic cues in a statistical parsing model, previous efforts (Gregory et al., 2004; Kahn et al., 2004) incorporated these features as word tokens and thereby suffered from the side-effect of displacing words in the n-gram models by the parser. To avoid this problem, we generate a set of candidate parses using an off-the-shelf, k-best parser, and use prosodic (and other) features to rescore the candidate parses. Our system architecture combines earlier models proposed for parse reranking (Collins, 2000) and filtering out edit regions (Charniak and Johnson, 2001). Detecting and removing edits prior to parsing is motivated by the claim that probabilistic contextfree grammars (PCFGs) perform poorl"
H05-1030,P90-1003,0,0.0944145,"is also perceptual evidence that prosody provides cues to human listeners that aid in syntactic disambiguation (Price et al., 1991), and the most important of these cues seems to be the prosodic phrases (perceived groupings of words) or the boundary events marking them. However, the utility of sentence-internal prosody in parsing conversational speech is not well established. Most early work on integrating prosody in parsing was in the context of human-computer dialog systems, where parsers typically operated on isolated utterances. The primary use of prosody was to rule out candidate parses (Bear and Price, 1990; Batliner et al., 1996). Since then, parsing has advanced considerably, and the use of statistical parsers makes the candidate pruning benefits of prosody less important. This raises the question of whether prosody is useful for improving parsing accuracy for conversational speech, apart from its use in sentence a new part-of-speech tag EW. Consecutive sequences of edit words are inserted as single, flat EDITED constituents. 4. Features (syntactic and/or prosodic) are extracted for each candidate, i.e. candidates are converted to feature vector representation. 5. The candidates are rescored b"
H05-1030,N01-1016,1,0.85709,"actic features only. While we are not the first to employ prosodic cues in a statistical parsing model, previous efforts (Gregory et al., 2004; Kahn et al., 2004) incorporated these features as word tokens and thereby suffered from the side-effect of displacing words in the n-gram models by the parser. To avoid this problem, we generate a set of candidate parses using an off-the-shelf, k-best parser, and use prosodic (and other) features to rescore the candidate parses. Our system architecture combines earlier models proposed for parse reranking (Collins, 2000) and filtering out edit regions (Charniak and Johnson, 2001). Detecting and removing edits prior to parsing is motivated by the claim that probabilistic contextfree grammars (PCFGs) perform poorly at detecting edit regions. We validate this claim empirically: two state-of-the-art PCFGs (Bikel, 2004; Charniak and Johnson, 2005) are both shown to perform significantly below a state-of-the-art edit detection system (Johnson et al., 2004). 2 Previous Work As mentioned earlier, conversational speech presents a different set of challenges and opportunities than encountered in parsing text. This paper focuses on the challenges associated with disfluencies (Se"
H05-1030,P05-1022,1,0.900216,"porated into a statistical parsing model. On the Switchboard corpus of conversational speech, the system achieves improved parse accuracy over a state-of-the-art system which uses only lexical and syntactic features. Since removal of edit regions is known to improve downstream parse accuracy, we explore alternatives for edit detection and show that PCFGs are not competitive with more specialized techniques. 1 Introduction For more than a decade, the Penn Treebank’s Wall Street Journal corpus has served as a benchmark for developing and evaluating statistical parsing techniques (Collins, 2000; Charniak and Johnson, 2005). While this common benchmark has served as a valuable shared task for focusing community effort, it has unfortunately led to the relative neglect of other genres, particularly speech. Parsed speech stands to benefit from practically every application envisioned for parsed text, including machine translation, information extraction, and language modeling. In contrast to text, however, speech (in particular, conversational speech) presents a distinct set of opportunities and challenges. While new obstacles arise from the presence of speech repairs, the possibility of word errors, and the absenc"
H05-1030,P99-1053,0,0.322839,"Missing"
H05-1030,H91-1073,1,0.912342,"Missing"
H05-1030,J99-4003,0,\N,Missing
H91-1073,H90-1006,0,0.0614255,"re frequently misidentifled; these sentences had no boundary tone, but did have a break 375 baseline and range in a parenthetical phrase, relative to the rest of the sentence. This pitch range change was not always perceived for appositives. In examining the associated fundamental frequency (F0) contours, we observed a region of reduced F0 excursion during the period of perceived range change. Though intonation is an important cue, duration and pauses alone provide enough information to automatically label break indices with a high correlation (greater than 0.86) to hand-labeled break indices [15]. such effects by asking the speakers to reread sentences in which overt segmental cues were produced, i.e., where the gross phonetic transcription of the two versions of the sentence would differ. In the results presented here, segment duration normalization is determined automatically using an HMM-based speech recognition system, the SRI Decipher system, which uses phonological rules to generate bushy pronunciation networks that should enable more accurate phonetic transcription and alignment than single pronunciation speech recognizers [22]. Each phone duration was normalized according to s"
H91-1073,H91-1073,1,0.0512347,"Missing"
H91-1073,J90-3003,0,0.180081,"Missing"
H92-1038,H91-1048,0,0.0170862,"ell as other approaches. To simplify initial experiments, we have made the assumption that phoneme segments are generated independently. In this case (1) is rewritten as A* = argmax H p(ai I X(sl), si)p(si, X(si)) A,S i where ai is one label of the sequence, si is a single segment of the segmentation 1, and X(sl) is the portion of the observation sequence corresponding to si. Segmental features are incorporated by constraining p(a~ IX(s0, s~) to be of the form p(a~ If(X(sl)), s0, as mentioned above. There are a number of segment-based systems that take a classification approach to recognition [1, 2, 3]. With the exception of [2], however, these do not include an explicit computation of the segmentation probability. Our 1 If si is defined as the start and end times of the segment, clearly consecutivesi are not independent. To avoid this problem, we think of si as correspondingto the length of the segment. 198 Component The formulation described above is quite general, allowing the use of a number of different classification and segmentation components. The particular classifier used in the experiments described below is based on the Stochastic Segment Model (SSM) [4], an approach that uses s"
H92-1038,H91-1049,0,0.131425,"this problem, we think of si as correspondingto the length of the segment. 198 Component The formulation described above is quite general, allowing the use of a number of different classification and segmentation components. The particular classifier used in the experiments described below is based on the Stochastic Segment Model (SSM) [4], an approach that uses segmental measurements in a statistical framework. This model represents the probability of a phoneme based on the joint statistics of an entire segment of speech. Several variants of the SSM have been developed since its introduction [5, 6], and recent work has shown this model to be comparable in performance to hidden-Markov model systems for the task of word recognition [7]. The use of the SSM for classification in the CIR formalism is described next. Using the formalism of [4], p(X(8i)[8i, ai) is characterized as p(f(X(si))[si,ai), where f(.) is a linear time warping transformation that maps variable length X(sl) to a fixed length sequence of vectors Y = f(X(si)). The specific model for Y is multi-variate Gaussian, generally subject to some assumptions about the covariance structure to reduce the number of free parameters in"
H92-1038,H91-1012,0,0.0225689,"Missing"
H92-1038,H91-1013,0,0.0239047,"Missing"
H92-1038,H92-1093,1,0.856691,"Missing"
H92-1093,H91-1013,0,0.31911,"Missing"
H92-1093,H90-1003,0,0.0686161,"Missing"
H93-1020,H91-1012,0,0.159941,"he tied mixture model have been explored. First, different assumptions can be made about feature correlation within individual mixture components. Separate sets of tied mixtures have been used for various input features including cepstra, derivatives of cepstra, and power and its derivative, where each of these feature sets have been treated as independent observation streams. Within an observation stream, different assumptions about feature correlation have been explored, with some researchers currently favoring diagonal covariance matrices [4, 5] and others adopting full covariance matrices [6, 7]. Second, the issue of parameter initialization can be important, since the training algorithm is an iterative hillclimbing technique that guarantees convergence only to a local optimum. Many researchers initialize their systems with parameters estimated from data subsets determined by K-means clustering, e.g. [6], although Paul describes a different, bootstrapping initialization [4]. Often a large number of mixture components are used and, since the parameters can be overtrained, contradictory results are reported on the benefits of parameter re-estimation. For example, while many researchers"
H93-1020,H92-1079,0,0.0304434,"in the CMU senone models [8], involves tying mixture weights over classes of context-dependent models. Their approach to finding regions of mixture weight tying involves clustering discrete observation distributions and mapping these clustered distributions to the mixture weights for the associated triphone contexts. In addition to the work described above, there are related methods that have informed the research concerning tied mixtures. First, mixture modeling does not require the use of Gaussian distributions. Good results have also been obtained using mixtures of Laplacian distributions [9, 10], and presumably other component densities would perform well too. Ney [11] has found strong similarities between radial basis functions and mixture densities using Gaussians with diagonal covariances. Recent work at BBN has explored the use of elliptical basis functions which share many properties with tied mixtures of full-covariance Gaussians [12]. Second, the positive results achieved by several researchers using non-tied mixture systems [13] raise the question of whether tiedmixtures have significant performance advantages over untied mixtures when there is adequate training data. It is p"
H93-1020,H91-1015,0,\N,Missing
H94-1014,H94-1013,0,0.540304,"other important issue in mixture language modeling, because the process of partitioning the d,tA into topic-dependent subsets reduces the amount of training available to estimate each component language model. These two issues, automatic clnstering for topic initialization and robust parameter estimation, are described further in the next two subsections. T Ak k=l n i=1 (2) Our approach has the advantage that it can be used either as a static or a dynamic model, and can easily leverage the techniques that have been developed for e~ptive language modeling, particularly cache [1, 9] and trigger [2, 3] models. One might raise the issue of recognition search cost for a model of mixtures at the sentence level, but in the N-best rescoring framework [10] the additional cost of the mixture language model is minimal. The general framework and mechanism for designing the mixture language model will be described in the next section, including descriptions of automatic topic clustering and robust.estimation techniques. Following this discussion, we will present some experimental results on mixture language modeling obtained using the BU recognition system. Finally, the paper will conclude with a dis"
H94-1014,H91-1013,0,0.0482601,"ing available to estimate each component language model. These two issues, automatic clnstering for topic initialization and robust parameter estimation, are described further in the next two subsections. T Ak k=l n i=1 (2) Our approach has the advantage that it can be used either as a static or a dynamic model, and can easily leverage the techniques that have been developed for e~ptive language modeling, particularly cache [1, 9] and trigger [2, 3] models. One might raise the issue of recognition search cost for a model of mixtures at the sentence level, but in the N-best rescoring framework [10] the additional cost of the mixture language model is minimal. The general framework and mechanism for designing the mixture language model will be described in the next section, including descriptions of automatic topic clustering and robust.estimation techniques. Following this discussion, we will present some experimental results on mixture language modeling obtained using the BU recognition system. Finally, the paper will conclude with a discussion of the possible extensions of mixture language models, to dynamic language modeling and to applications other than speech transcription. 2. M I"
H94-1014,H94-1016,0,0.054923,"Missing"
H94-1014,H93-1003,0,0.0510994,"Missing"
H94-1014,H94-1011,0,\N,Missing
H94-1014,H91-1057,0,\N,Missing
N03-2003,H01-1051,1,0.794362,"Missing"
N03-2003,W96-0213,0,\N,Missing
N03-2012,H01-1051,1,0.354974,"Missing"
N04-1018,P92-1008,0,0.490741,"Missing"
N04-1018,N01-1006,0,0.0166245,"the training data in preparation for the next iteration. The standard stopping criterion for rule learning is to stop when the score of the best rule falls below a threshold value; statistical significance measures have also been used (Mangu and Padmanabhan, 2001). To tag new data, the rules are applied in the order in which they were learned. This allows rules which are learned later in the process to fine tune the effects of the earlier rules. TBL produces concise, comprehensible rules, and uses the entire corpus to train all of the rules. We used Florian and Ngai’s Fast TBL system (fnTBL) (Ngai and Florian, 2001) to train rules using disfluency annotated conversational speech data. The input to our TBL system consists of text divided into utterances, with IPs and SUs inserted as if they were extra words. (For simplicity, these special words are also assigned “IP” and “SU” as part of speech tags.) Our TBL system used the following types of features: • Identity of the word. • Part of speech (POS) and grouped part of speech (GPOS) of the word (same as the decision tree). • Is the word commonly used as: filled pause (FP), backchannel (BC), explicit editing term (EET), discourse marker (DM)? • Does this wo"
N04-1018,J95-4004,0,0.209741,"iable in STT output. In the linearly interpolated and joint HMM approaches, the relative weighting of the two knowledge sources is estimated on the development test set for STT output, so it is possible for prosodic cues to be given a higher weight. 4.3 Edit and Filler Detection After SUs and IPs have been marked, we use transformation-based learning (TBL) to learn rules to detect edit disfluencies and conversational fillers. TBL is an automatic rule learning technique that has been successfully applied to a variety of problems in natural language processing, including part-of-speech tagging (Brill, 1995), spelling correction (Mangu and Brill, 1997), error correction in automatic speech recognition (Mangu and Padmanabhan, 2001), and named entity detection (Kim and Woodland, 2000). We selected TBL for our tagging-like metadata detection task since it has been used successfully for these other tagging tasks. TBL is an iterative technique for inducing rules from training data. A TBL system consists of a baseline predictor, a set of rule templates, and an objective function for scoring potential rules. After tagging the training data using the baseline predictor, the system learns a list of rules"
N04-1018,N01-1016,0,0.233158,"ov Model (HMM) algorithms to an n-gram LM paradigm to represent non-lexical events such as IPs and sentence boundaries as hidden states. Liu et al. (2003) built on this framework and extended prosodic features and the hidden event LM to predict edit IPs on both human transcripts and STT system output. Their system also detected the onset of the reparandum by employing rule-based pattern matching once edit IPs have been detected. Edit disfluency detection systems that rely exclusively on word-based information have been presented by Heeman et al. (Heeman et al., 1996) and Charniak and Johnson (Charniak and Johnson, 2001). Common to both of these approaches is a focus on repeated or similar sequences of words and information about the words themselves and the length and similarity of the sequences. Our approach is most similar to (Liu et al., 2003), since we also detect boundary events such as IPs first and use them as “signals” when identifying the reparandum in a later stage. The motivation to detect IPs first is that Speech Prosodic and Lexical Feature Extraction Word Boundary Event Prediction (DT/HE-LM) IP/SU Prediction Filler/Edit Word Detection (TBL) Output Figure 1: System Diagram speech before an IP is"
N04-1018,H90-1021,0,0.0873707,"Missing"
N04-1018,P83-1019,0,0.182278,"es, and alterations are in boldface. Annotation of complex edit disfluencies, where a disfluency occurs within an alteration, can be difficult. The data used here is annotated with a flattened structure that treats these cases as simple disfluencies with multiple IPs (Strassel, 2003). IPs within a complex disfluency are detected separately, and contiguous sequences of edit words associated with these IPs are referred to as a deletable region. 3 Previous Work In an early study on automatic disfluency detection a deterministic parser and correction rules were used to clean up edit disfluencies (Hindle, 1983). However theirs was not a truly automatic system as it relied on handannotated “edit signals” to locate IPs. Bear et al. (1992) explored pattern matching, parsing and acoustic cues and concluded that multiple sources of information would be needed to detect edit disfluencies. A decision-tree-based system that took advantage of various acoustic and lexical features to detect IPs was developed in (Nakatani and Hirschberg, 1994). Shriberg et al. (1997) applied machine prediction of IPs with decision trees to the broader Switchboard corpus by generating decision trees with a variety of prosodic f"
N04-4032,N04-1018,1,\N,Missing
N04-4032,N01-1016,0,\N,Missing
N06-2014,P04-1085,0,0.268279,"03), which include the number of words in a spurt, the number of keywords associated with the positive and negative classes, and classification based on keywords. We also obtain word and class-based bigram language models for each class from the training data, and compute such language model features as the perplexity of a spurt, probability of the spurt, and the probability of the first two words in a spurt, using each language model. We also include the most likely class by the language models as features. 3.2 Results First, we performed the same experiment as in (Hillard et al., 2003) and (Galley et al., 2004), using the contrast classifier (CC) method . Among the four meetings, the data from one meeting was set aside for testing. Table 1 compares the 3-class accuracy of the contrast classifier with previous results, merging positive and backchannel class together into one class as in the other work. When only lexical features are used (the first three entries), the SVMbased contrast classifier using meta-classifiers gives the best performance, outperforming the decision tree in (Hillard et al., 2003) and the maximum en55 Table 1: Comparison of 3-way classification accuracy on lexical (lex) vs. exp"
N06-2014,N03-2012,1,0.601889,"Missing"
N06-2014,W01-0501,0,0.0528507,"Missing"
N06-2014,P95-1026,0,0.0820288,"Missing"
N09-2044,J93-2001,0,0.0743674,"s are explored, with performance assessed on matched data. In this paper, we assess classification results of texts that come from new genres, as well as those matching the training set. In addressing new genres, we have two main contributions: new features and factored coding. Proceedings of NAACL HLT 2009: Short Papers, pages 173–176, c Boulder, Colorado, June 2009. 2009 Association for Computational Linguistics The standard features for genre classification models include words, part-of-speech (POS) tags, and punctuation (Kessler et al., 1997; Stamatatos et al., 2000; Lee and Myaeng, 2002; Biber, 1993), but constituent-based syntactic categories have also been explored (Karlgren and Cutting, 1994). (Feldman et al., 2009) used mixed word and POS histogram mean and variance as features for genre classification. In this work, we augment those histogram statistics with higher-order ones, as well as add new word features aimed at capturing online genres. Further, we propose a factored genre model, and demonstrate its effect on genre classification of out-of-domain documents. 2 2.1 Methods Corpora To train our algorithm, we use eight different genres: broadcast news (bn, 671 docs), broadcast conv"
N09-2044,C94-2174,0,0.246083,"s classification results of texts that come from new genres, as well as those matching the training set. In addressing new genres, we have two main contributions: new features and factored coding. Proceedings of NAACL HLT 2009: Short Papers, pages 173–176, c Boulder, Colorado, June 2009. 2009 Association for Computational Linguistics The standard features for genre classification models include words, part-of-speech (POS) tags, and punctuation (Kessler et al., 1997; Stamatatos et al., 2000; Lee and Myaeng, 2002; Biber, 1993), but constituent-based syntactic categories have also been explored (Karlgren and Cutting, 1994). (Feldman et al., 2009) used mixed word and POS histogram mean and variance as features for genre classification. In this work, we augment those histogram statistics with higher-order ones, as well as add new word features aimed at capturing online genres. Further, we propose a factored genre model, and demonstrate its effect on genre classification of out-of-domain documents. 2 2.1 Methods Corpora To train our algorithm, we use eight different genres: broadcast news (bn, 671 docs), broadcast conversations (bc, 698 docs), meetings (mt, 493 docs), newswire (nw, 471 docs), conversational teleph"
N09-2044,P97-1005,0,0.622323,"Missing"
N09-2044,J93-2004,0,0.0365119,"Missing"
N09-2044,C00-2117,0,0.457893,"that only a small, fixed set of different genres are explored, with performance assessed on matched data. In this paper, we assess classification results of texts that come from new genres, as well as those matching the training set. In addressing new genres, we have two main contributions: new features and factored coding. Proceedings of NAACL HLT 2009: Short Papers, pages 173–176, c Boulder, Colorado, June 2009. 2009 Association for Computational Linguistics The standard features for genre classification models include words, part-of-speech (POS) tags, and punctuation (Kessler et al., 1997; Stamatatos et al., 2000; Lee and Myaeng, 2002; Biber, 1993), but constituent-based syntactic categories have also been explored (Karlgren and Cutting, 1994). (Feldman et al., 2009) used mixed word and POS histogram mean and variance as features for genre classification. In this work, we augment those histogram statistics with higher-order ones, as well as add new word features aimed at capturing online genres. Further, we propose a factored genre model, and demonstrate its effect on genre classification of out-of-domain documents. 2 2.1 Methods Corpora To train our algorithm, we use eight different genres: broadcast"
N10-1101,W03-1028,0,0.0107781,"rom each individual user’s Twitter messages as his/her tags. Due to the lack of human generated annotations, we employ an unsupervised strategy. Related Work Research work related to Twitter message analysis includes a user sentiment study (Jansen et al., 2009) and information retrieval indexing. To our knowledge, no previously published research has yet addressed problems on tagging user’s personal interests from Twitter messages via keyword extraction, though several studies have looked at keyword extraction using other genres. For supervised keyword extraction, (Turney, 2000; Turney, 2003; Hulth, 2003; Yih et al., 2006; Liu et al., 2008) employed TFIDF or its variants with Part-of-Speech (POS), capitalization, phrase and sentence length, etc., as features to train keyword extraction models, and discriminative training is usually adopted. Yih et al. (2006) use logistic regression to extract keywords from web pages for content-targeted advertising, which has the most similar application to our work. However, due to the lack of human annotation on Twitter messages, we have to adopt an unsupervised strategy. For unsupervised keyword extraction, TFIDF ranking is a popular method, and its effect"
N10-1101,N09-1070,0,0.015018,"e length, etc., as features to train keyword extraction models, and discriminative training is usually adopted. Yih et al. (2006) use logistic regression to extract keywords from web pages for content-targeted advertising, which has the most similar application to our work. However, due to the lack of human annotation on Twitter messages, we have to adopt an unsupervised strategy. For unsupervised keyword extraction, TFIDF ranking is a popular method, and its effectiveness has been shown in (Hulth, 2003; Yih et al., 2006). TextRank and its variants (Mihalcea and Tarau, 2004; Wan et al., 2007; Liu et al., 2009) are graph-based text ranking models, which are derived from Google’s PageRank algorithm (Brin and Page, 1998). It outperforms TFIDF ranking on traditional keyword extraction tasks. However, previous work on both TFIDF ranking and TextRank has been done mainly on academic papers, spoken documents or 689 Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 689–692, c Los Angeles, California, June 2010. 2010 Association for Computational Linguistics web pages, whose language style is more formal (or, less “conversational”) than that of Twitter m"
N10-1101,W00-1308,0,0.070287,"id candidate words for user tags. The second set includes 67 abbreviations of function words that usually form grammatical parts in a sentence, such as “im” (i’m), “abt” (about). Simply removing them will affect the POS tagging. Thus, the abbreviations in both these sets are replaced with the corresponding complete words or phrases. The third set includes 4576 phrase abbreviations that are usually separable parts of a sentence that do not directly indicate discussion topics, such as “lol” (laugh out loud), “clm” (cool like me), which are removed in this step. We apply the Stanford POS tagger (Toutanova and Manning, 2000) on Twitter messages, and only select nouns and adjectives as valid candidates for user tags. At the end of the preprocessing pipeline, the candidate words are processed with the rulebased Porter stemmer2 and stopwords are filtered using a publicly available list.3 1 www.noslang.com/dictionary tartarus.org/ martin/PorterStemmer/ 3 armandbrahaj.blog.al/2009/04/14/ list-of-english-stop-words/ 2 3.2 3.2.1 TFIDF ranking In the TFIDF ranking algorithm, messages from user u are put together as one document. The TFIDF value of word i from this user’s messages is computed as tf idfi,u ni,u U =P log( )"
N10-1101,P07-1070,0,0.00787739,"phrase and sentence length, etc., as features to train keyword extraction models, and discriminative training is usually adopted. Yih et al. (2006) use logistic regression to extract keywords from web pages for content-targeted advertising, which has the most similar application to our work. However, due to the lack of human annotation on Twitter messages, we have to adopt an unsupervised strategy. For unsupervised keyword extraction, TFIDF ranking is a popular method, and its effectiveness has been shown in (Hulth, 2003; Yih et al., 2006). TextRank and its variants (Mihalcea and Tarau, 2004; Wan et al., 2007; Liu et al., 2009) are graph-based text ranking models, which are derived from Google’s PageRank algorithm (Brin and Page, 1998). It outperforms TFIDF ranking on traditional keyword extraction tasks. However, previous work on both TFIDF ranking and TextRank has been done mainly on academic papers, spoken documents or 689 Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 689–692, c Los Angeles, California, June 2010. 2010 Association for Computational Linguistics web pages, whose language style is more formal (or, less “conversational”) tha"
N10-1101,W04-3252,0,\N,Missing
N10-1108,P07-1011,0,0.0499809,"Missing"
N10-1108,N06-2021,0,\N,Missing
N13-1085,W11-1406,0,0.164755,"priate locations. The results have implications for automatic assessment of text difficulty in that locations of atypical prosodic lengthening are indicative of difficult lexical items and syntactic constructions. 1 Introduction Fluent reading is known to be a good indicator of reading comprehension, especially for early readers (Rasinski, 2006), so oral reading is often used to evaluate a student’s reading level. One method that can be automated with speech recognition technology is the number of words that a student can read correctly of a normed passage, or Words Correct Per Minute (WCPM) (Downey et al., 2011). Since WCPM depends on speaking rate as well as literacy, we are interested in identifying new measures that can be automatically computed for use in combination with WCPM to provide a better assessment of reading level. In particular, we investigate finegrained measures that, if useful in identifying points of difficulty for readers, can lead to new approaches for assessing text difficulty. The WCPM is reduced when a person repeats or incorrectly reads a word, but also when they introduce pauses and articulate words more slowly. Pauses and lengthened articulation can be an indicator of uncer"
N15-1022,W03-1004,0,0.0768064,"Missing"
N15-1022,W03-0310,0,0.100888,"Missing"
N15-1022,P11-2117,0,0.57538,"Missing"
N15-1022,de-marneffe-etal-2006-generating,0,0.0513912,"Missing"
N15-1022,W04-3208,0,0.0543309,"Missing"
N15-1022,N13-1092,0,0.148358,"Missing"
N15-1022,D13-1029,1,0.821758,"Missing"
N15-1022,D14-1058,1,0.494388,"Missing"
N15-1022,P13-1151,0,0.0751078,"Missing"
N15-1022,D14-1043,1,0.798061,"Missing"
N15-1022,P03-1054,0,0.0453331,"Missing"
N15-1022,P14-5010,0,0.0053723,"Missing"
N15-1022,E09-1065,0,0.0331283,"Missing"
N15-1022,J05-4003,0,0.154675,"Missing"
N15-1022,E06-1021,0,0.127307,"Missing"
N15-1022,N10-1063,0,0.0263911,"Missing"
N15-1022,P12-1107,0,0.559583,"Missing"
N15-1022,N10-1056,0,\N,Missing
N15-1022,C10-1152,0,\N,Missing
N15-1022,P12-1091,0,\N,Missing
N15-1022,P94-1019,0,\N,Missing
N15-1022,W10-0406,0,\N,Missing
N15-1161,N01-1016,0,0.185611,"Missing"
N15-1161,W10-4343,0,0.0559791,"Missing"
N15-1161,N09-2028,0,0.0683417,"Missing"
N15-1161,P06-1021,0,0.0478851,"Missing"
N15-1161,Q14-1011,0,0.0242004,"Missing"
N15-1161,P04-1005,0,0.0984326,"Missing"
N15-1161,H05-1030,1,0.817171,"Missing"
N15-1161,P09-2070,0,0.0325579,"Missing"
N15-1161,N09-1074,0,0.0352402,"Missing"
N15-1161,N13-1102,0,0.0263867,"Missing"
N15-1161,D13-1013,0,0.0240239,"Missing"
N15-1161,D10-1017,0,0.0265576,"Missing"
N15-1161,C14-1138,0,0.0305281,"Missing"
N15-1161,C10-1154,0,0.0561419,"Missing"
N16-1079,N09-1035,0,0.0260491,"n is generated automatically using the LOGIOS lexicon tool.1 The same is not true for the targets, since they are unknown beforehand. Thus, when the lexicon is used to map puns to phonemes the vocabulary size is essentially unlimited. But, when it is used to map the phoneme lattice into a word lattice of potential targets then the fixed vocabulary from the language model is used. The CMU dictionary includes multiple pronunciations for some words. All pronunciations are used with unweighted parallel paths. The version of the dictionary used here includes stress markers and syllable boundaries (Bartlett et al., 2009). In the simplest version of our model, this information is ig1 http://www.speech.cs.cmu.edu/tools/lextool.html 657 nored in order to reduce the number of learned parameters in the PEM. After composing with the phonetic edit model and the lexicon, we do a conservative pruning of the WFST to remove highly improbable word sequences based on the phonetic score and run epsilon removal on the resulting lattice. This reduces the memory footprint and allows use of a larger language model. 3.4 Language Model A 230 million word corpus was formed from comments obtained from Reddit, an online discussion"
N16-1079,A00-2038,0,0.0621103,"sallow the possibility that the pun is hypothesized as a target, i.e. homographic puns, in order to focus on the class of puns whose relationship with their targets is primarily phonological. The OpenFst library is used to perform all of the WFST operations (Allauzen et al., 2007). 3.2 Phonetic Edit Model The purpose of the phonetic edit model is to estimate the probability of the pun phoneme sequence given a candidate target phoneme sequence. We prefer to learn a model from the data rather than adopt an existing model that relies on phonetic features and edit costs that were derived by hand (Kondrak, 2000). As shown by Ristad and Yianilos (1998), a memoryless WFST model can learn a probability distribution over edit operations with a principled objective, namely, to maximize the likelihood of the source/target sequences in the training data. In our case, the training data is pun/target phoneme sequences. Memoryless, in this context, refers to the fact that the model is not conditioning on previous symbols, i.e. there is only a single state in the WFST. The model assumes that the phoneme sequence of the pun is generated through the stochastic application of insertions, deletions, and substitutio"
N16-1079,P15-1070,0,0.0503176,"inary phonetic edit cost table to be one part of a scoring system. The model is based on the phoneme edit counts from Sobkowiak (1991) with an ad-hoc formula for transforming the counts into substitution costs. However, Hempelmann makes no effort to empirically test his model at the recovery task. The model uses a subset of 1,182 puns from the 3,850 identified by Sobkowiak. This subset is the data used for training our phonetic edit models and we use Hempelmann’s cost function as a baseline. The task of automatic target recovery of paronomasic puns has not been previously attempted. Recently, Miller and Gurevych (2015) studied methods for automatic understanding of homographic puns using methods from word-sense disambiguation. Paronomasia is intentionally excluded from their data. 3 Target Recovery Following the convention of Miller and Gurevych (2015), we assume that the position of the pun in the input sentence is known. The target recovery task is to identify the pun target given the pun and its left and right word contexts. 3.1 Model The model has three parts: a phonetic edit model, a phonetic lexicon and a language model. The recovered target T ∗ is the word (or words) with the maximum probability give"
N16-1079,N12-2012,0,0.0273886,"and Future Work The quality of our phonetic edit model is evident from its performance at the target recovery task, as well as the fact that it captures known linguistic phenomena such as vowel reduction and dialectal features. Furthermore, by collecting human ratings we are able to empirically verify the previously untested assumption that lower phonetic edit costs in puns correlate with pun goodness. The strength of the model can be leveraged to improve the quality of pun generation and humor classification systems that have used weaker phonetic edit models (Binsted, 1996; Valitutti, 2011; Raz, 2012). Some pun generation systems are limited to exact homophones. In this work, we did not consider homographic puns. In principle, our algorithm can handle these by introducing an LM weight to control the balance of PEM/LM scores. Pun generation is much more complicated than target recovery as reflected in the complexity of proposed systems for humor generation. However, improved understanding of puns by way of progress in the target recovery task should also lead to corresponding improvements in the task of pun generation. Our syllable extension to the PEM gave the best performance, but only by"
N16-1079,W05-1614,0,0.00892195,"ed.” Here, “clothed” is the pun and “closed” is the target. Paronomasic puns are distinguished from homographic puns such as “Two silkworms had a race. They ended up in a tie.” which puns on the two definitions of the word “tie”. When the pun and target are homophonic this is called a perfect pun, and when nearly homophonic an imperfect pun (Zwicky and Zwicky, 1986) (or a heterophonic pun (Hempelmann, 2003)). The focus of this work is to propose and evaluate a model for target recovery of both perfect and imperfect paronomasic puns, assuming that the location of the pun word or word sequence. Ritchie (2005) classifies puns in terms of whether they are self-contained, i.e., based on general knowledge and humorous in a variety of circumstances, or contextually integrated, i.e. relying on a specific context such as a visual context, knowledge of a recent event or discussion. Many puns of this type are associated with cartoons or images, e.g. a cartoon with pies and cakes in the street having the caption “The streets were oddly desserted” (desserted/deserted). Contextually integrated puns lose their humor out of context because the pun is difficult to detect. However, target recovery is often still"
N16-1079,P00-1073,0,0.127909,"k or other punctuation. The vocabulary is set by intersecting the vocabulary from the CMU pronunciation dictionary with the set of tokens that occur at least 30 times in the language model training data. This gives us a 36,175 word vocabulary. As Sobkowiak (1991) observed, the target tends to have a much higher unigram probability than the pun. This means that the vocabulary size need not be too large to cover most of the targets. The language model is a trigram model with modified Kneser-Ney smoothing (Chen and Goodman, 1999). Entropy pruning is used to reduce the size of the language model (Stolcke, 2000). It is important to perform the determinization and minimization operations on the LM after converting it into the FST representation, in order to reduce the size of the model (Mohri et al., 2008). Because we are using a trigram model, only two words of context are needed on each side of the pun. 3.5 Extending the Phoneme Edit Model with Syllable Structure and Stress data used by the language model and the phonetic edit model. (It would be nice to have used some of the 1,182 puns from Sobkowiak for test data but only the isolated pun/target pairs were provided without the necessary word conte"
N16-1079,D15-1284,0,0.121598,"kes a good pun. The model is evaluated on a small corpus where it is able to automatically recover a large fraction of the pun targets. 1 Introduction From the high culture of Shakespeare’s plays (Tanaka, 1992), to the depths of the YouTube comments section, from advertising slogans (Keller, 2009) to conversations with nerdy parents, puns are a versatile rhetorical device and their understanding is essential to any comprehensive approach to computational humor. Humor has been described as “one of the most interesting and puzzling research areas in the field of natural language understanding” (Yang et al., 2015). Puns, in particular, offer an interesting subject for study since their humor derives from wordplay and double-meaning. An important class of puns, known as paronomasic puns, are those where one entity, the pun, is phonologically similar to another, the target (Joseph, 2008). Consider an example from Crosbie (1977): “Sign by gate to nudist colony: Come in. We are Never Clothed.” Here, “clothed” is the pun and “closed” is the target. Paronomasic puns are distinguished from homographic puns such as “Two silkworms had a race. They ended up in a tie.” which puns on the two definitions of the wor"
N18-1007,N16-1024,0,0.0388215,"ay have benefits for parsers operating on alternative ASR hypotheses. The results we compare to in Section 4 are relatively old. More recent parsing results on spontaneous speech involve dependency parsers using only text (Rasooli and Tetreault, 2013; Honnibal and Johnson, 2014; Yoshikawa et al., 2016), with the exception of a study on unsupervised dependency parsing (Pate and Goldwater, 2013). With the recent success of transition-based neural approaches in dependency parsing, researchers have adapted transition-based ideas to constituent parsing (Zhu et al., 2013; Watanabe and Sumita, 2015; Dyer et al., 2016). These approaches have not yet been used with speech, to our knowledge, but we expect it to be straightforward to extend our prosody integration framework to these systems, both for dependency and constituency parsing. 7 stituent parsing, since prosodic cues tend to align with constituent boundaries. However, it remains an open question as to whether dependency, constituency or other parsing frameworks are better suited to leveraging prosody. Our study builds on a parser that uses reverse order text processing, since it provides a stronger text-only baseline. However, the prosody modeling com"
N18-1007,1993.eamt-1.1,0,0.608238,"Missing"
N18-1007,Q13-1006,0,0.0167593,"an improve ASR performance (Kahn and Ostendorf, 2012; Yoshikawa et al., 2016). Our analysis of performance degradation for the system with prosody when the gold transcript and associated parse are in error suggests that prosody may have benefits for parsers operating on alternative ASR hypotheses. The results we compare to in Section 4 are relatively old. More recent parsing results on spontaneous speech involve dependency parsers using only text (Rasooli and Tetreault, 2013; Honnibal and Johnson, 2014; Yoshikawa et al., 2016), with the exception of a study on unsupervised dependency parsing (Pate and Goldwater, 2013). With the recent success of transition-based neural approaches in dependency parsing, researchers have adapted transition-based ideas to constituent parsing (Zhu et al., 2013; Watanabe and Sumita, 2015; Dyer et al., 2016). These approaches have not yet been used with speech, to our knowledge, but we expect it to be straightforward to extend our prosody integration framework to these systems, both for dependency and constituency parsing. 7 stituent parsing, since prosodic cues tend to align with constituent boundaries. However, it remains an open question as to whether dependency, constituency"
N18-1007,P06-1055,0,0.121059,"Missing"
N18-1007,P06-1021,0,0.716489,"r, handling disfluencies as constituents via a neural attention mechanism. A challenge of incorporating prosody in parsing is that multiple acoustic cues interact to signal prosodic structure, including pauses, lengthening, fundamental frequency modulation, and spectral shape. These cues also vary with the phonetic segment, emphasis, emotion and speaker, so feature extraction typically involves multiple time windows and normalization techniques. The most successful constituent parsers have mapped these features to prosodic boundary posteriors by using labeled training data (Kahn et al., 2005; Hale et al., 2006; Dreyer and Shafran, 2007). The approach proposed here takes advantage of advances in neural networks to automatically learn a good feature representation without the need to explicitly represent prosodic constituents. To narrow the scope of this work and facilitate error analysis, our experiments use known transcripts and sentence segmentation. Our work offers the following contributions. We introduce a framework for directly integrating acoustic-prosodic features with text in a neural encoder-decoder parser that does not require handannotated prosodic structure. We demonstrate improvements"
N18-1007,H91-1073,1,0.371999,"rical Engineering, University of Washington 2 Toyota Technological Institute at Chicago 3 Department of Computer Science, UNC Chapel Hill {ttmt001, ostendor}@uw.edu, mbansal@cs.unc.edu, {shtoshni, kgimpel, klivescu}@ttic.edu Abstract Despite these challenges, speech carries helpful extra information – beyond the words – associated with the prosodic structure of an utterance and encoded via variation in timing and intonation. Speakers pause in locations that are correlated with syntactic structure (Grosjean et al., 1979), and listeners use prosodic structure in resolving syntactic ambiguities (Price et al., 1991). Prosodic cues also signal disfluencies by marking the interruption point (Shriberg, 1994). However, most speech parsing systems in practice take little advantage of these cues. Our study focuses on this last challenge, aiming to incorporate prosodic cues in a neural parser, handling disfluencies as constituents via a neural attention mechanism. A challenge of incorporating prosody in parsing is that multiple acoustic cues interact to signal prosodic structure, including pauses, lengthening, fundamental frequency modulation, and spectral shape. These cues also vary with the phonetic segment,"
N18-1007,D13-1013,0,0.587645,"Speech poses challenges for parsing: transcripts may contain errors and lack punctuation; even perfect transcripts can be difficult to handle because of disfluencies (restarts, repetitions, and self-corrections), filled pauses (“um”, “uh”), interjections (“like”), parentheticals (“you know”, “I mean”), and sentence fragments. Some of these phenomena can be handled in standard grammars, but disfluencies typically require extensions of the model. Different approaches have been explored in both constituency parsing (Charniak and Johnson, 2001; Johnson and Charniak, 2004) and dependency parsing (Rasooli and Tetreault, 2013; Honnibal and Johnson, 2014). ∗ Equal Contribution. 69 Proceedings of NAACL-HLT 2018, pages 69–81 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics speech over a high-quality text-only parser and provide analyses showing where prosodic features help and that assessment of their utility is affected by human transcription errors. 2 decoder hidden state at time step t, which captures the previous output sequence context y<t . uit = v > tanh(W 1 hi + W 2 dt + ba ) Task and Model Description αt = softmax(ut ) Our model maps a sequence of word-level input fe"
N18-1007,Q14-1011,0,0.616409,"parsing: transcripts may contain errors and lack punctuation; even perfect transcripts can be difficult to handle because of disfluencies (restarts, repetitions, and self-corrections), filled pauses (“um”, “uh”), interjections (“like”), parentheticals (“you know”, “I mean”), and sentence fragments. Some of these phenomena can be handled in standard grammars, but disfluencies typically require extensions of the model. Different approaches have been explored in both constituency parsing (Charniak and Johnson, 2001; Johnson and Charniak, 2004) and dependency parsing (Rasooli and Tetreault, 2013; Honnibal and Johnson, 2014). ∗ Equal Contribution. 69 Proceedings of NAACL-HLT 2018, pages 69–81 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics speech over a high-quality text-only parser and provide analyses showing where prosodic features help and that assessment of their utility is affected by human transcription errors. 2 decoder hidden state at time step t, which captures the previous output sequence context y<t . uit = v > tanh(W 1 hi + W 2 dt + ba ) Task and Model Description αt = softmax(ut ) Our model maps a sequence of word-level input features to a linearized parse"
N18-1007,H05-1030,1,0.848641,"s in a neural parser, handling disfluencies as constituents via a neural attention mechanism. A challenge of incorporating prosody in parsing is that multiple acoustic cues interact to signal prosodic structure, including pauses, lengthening, fundamental frequency modulation, and spectral shape. These cues also vary with the phonetic segment, emphasis, emotion and speaker, so feature extraction typically involves multiple time windows and normalization techniques. The most successful constituent parsers have mapped these features to prosodic boundary posteriors by using labeled training data (Kahn et al., 2005; Hale et al., 2006; Dreyer and Shafran, 2007). The approach proposed here takes advantage of advances in neural networks to automatically learn a good feature representation without the need to explicitly represent prosodic constituents. To narrow the scope of this work and facilitate error analysis, our experiments use known transcripts and sentence segmentation. Our work offers the following contributions. We introduce a framework for directly integrating acoustic-prosodic features with text in a neural encoder-decoder parser that does not require handannotated prosodic structure. We demons"
N18-1007,D17-1296,0,0.24349,"dule; we hypothesized that the location-sensitive attention model helps handle these differences based on analysis of the text-only results (Table 1). However, more explicit modeling of disfluency pattern match characteristics in a dependency parser (Honnibal and Johnson, 2014) leads to better disfluency detection performance (F = 84.1 vs. 76.7 for our text only model). Pattern match features also benefit a neural model for disfluency detection alone (F = 87.0) (Zayats et al., 2016), and similar gains are observed by formulating disfluency detection in a transition-based framework (F = 87.5) (Wang et al., 2017). Experiments with oracle disfluencies as features improve the CL-attn text-only parsing performance from 87.85 to 89.38 on the test set, showing that more accurate disfluency modeling is a potential area of improvement. a sudden you be> all alone it ’d be nice to go someplace with people similar to you to have friends S2: uh uh <i have had> my wife ’s picked up a couple of things saying uh boy if we could refinish that ’d be a beautiful piece of furniture Multi-syllable errors are especially problematic, leading to serious inconsistencies between the text and the acoustic signal. Further, the"
N18-1007,P15-1113,0,0.0224504,"ror suggests that prosody may have benefits for parsers operating on alternative ASR hypotheses. The results we compare to in Section 4 are relatively old. More recent parsing results on spontaneous speech involve dependency parsers using only text (Rasooli and Tetreault, 2013; Honnibal and Johnson, 2014; Yoshikawa et al., 2016), with the exception of a study on unsupervised dependency parsing (Pate and Goldwater, 2013). With the recent success of transition-based neural approaches in dependency parsing, researchers have adapted transition-based ideas to constituent parsing (Zhu et al., 2013; Watanabe and Sumita, 2015; Dyer et al., 2016). These approaches have not yet been used with speech, to our knowledge, but we expect it to be straightforward to extend our prosody integration framework to these systems, both for dependency and constituency parsing. 7 stituent parsing, since prosodic cues tend to align with constituent boundaries. However, it remains an open question as to whether dependency, constituency or other parsing frameworks are better suited to leveraging prosody. Our study builds on a parser that uses reverse order text processing, since it provides a stronger text-only baseline. However, the"
N18-1007,N04-4032,1,0.423253,"act the prosodic features, there is a statistically significant (p-value < 0.05) negative effect on parsing with prosody. Not included in this analysis are sentence boundary errors, which also change the “gold” parse. Thus, prosody may be more useful than results here indicate. 6 It is well known that prosodic features play a role in human resolution of syntactic ambiguities, with more than two decades of studies seeking to incorporate prosodic features in parsing. A series of studies looked at constituent parsing informed by the presence (or likelihood) of prosodic breaks at word boundaries (Kahn et al., 2004, 2005; Hale et al., 2006; Dreyer and Shafran, 2007). Our approach improves over performance of these systems using raw acoustic features, without the need for handlabeling prosodic breaks. The gain is in part due to the improved text-based parser, but the incremental benefit of prosody here is similar to that in these prior studies. (In prior work using acoustic feature directly (Gregory et al., 2004), prosody actually degraded performance.) Our analyses of the impact of prosody also extends prior work. Related Work Related work on parsing conversational speech has mainly addressed four probl"
N18-1007,D16-1109,0,0.187986,"n interruption point. Repairs often involve parallel grammatical Prosody is also known to provide useful cues to sentence boundaries (Liu et al., 2006), and automatic sentence segmentation performance has been shown to have a significant impact on parsing performance (Kahn and Ostendorf, 2012). In our study, sentence boundaries are given so as to focus on the role of prosody in resolving sentenceinternal parse ambiguity, for which prior work had 76 obtained smaller gains. Studies have also shown that parsing lattices or confusion networks can improve ASR performance (Kahn and Ostendorf, 2012; Yoshikawa et al., 2016). Our analysis of performance degradation for the system with prosody when the gold transcript and associated parse are in error suggests that prosody may have benefits for parsers operating on alternative ASR hypotheses. The results we compare to in Section 4 are relatively old. More recent parsing results on spontaneous speech involve dependency parsers using only text (Rasooli and Tetreault, 2013; Honnibal and Johnson, 2014; Yoshikawa et al., 2016), with the exception of a study on unsupervised dependency parsing (Pate and Goldwater, 2013). With the recent success of transition-based neural"
N18-1007,D12-1096,0,0.0498214,"r benefit from prosody in the fluent set. tachment errors. Adding the remaining acousticprosodic features helps to correct more types of attachment errors, especially VP and NP attachment. Figure 3 demonstrates one case where the pause feature helps in correcting a PP attachment error made by a text-only parser. Other interesting examples (see Appendix A.2) suggest that the learned f0/E features help reduce NP attachment errors where the audio reveals a prominent word at the constituent boundary, even though there is no pause at that word. Types of errors. We use the Berkeley Parser Analyzer (Kummerfeld et al., 2012) to compare the types of errors made by the different parsers.10 Table 7 presents the relative error reductions over the text-only baseline achieved by the text + p model and our best model for disfluent sentences. The two models differ in the types of error reductions they provide. Including pause information gives largest improvements on PP attachment and Modifier atEffect of transcription errors. The results and analyses so far have assumed that we have reliable transcripts. In fact, the original transcripts contained errors, and the Treebank annotators used these without reference to audio"
N18-2094,P16-2003,0,0.0292206,"ore. Again, a commonality across tasks is the frequent use of unsupervised representations of textual features. In representing text, a common assumption is that community language reflects topical interests, so representations aimed at topic modeling have been used, including LDA (Pennacchiotti and Popescu, 2011) and tf-idf weighted word2vec embeddings (Boom et al., 2016; Wijeratne et al., 2016). Yu et al. (2016) compute a user embedding by averaging tweet embeddings. Other work investigates methods for learning embeddings that integrate text and social network (graph or textbased) features (Benton et al., 2016). The work closest to ours is by Fani et al. (2017), which learns embeddings that are close for likeminded users, where like-minded pairs are identified by a deterministic algorithm that leverages timing of related posts. Our approach requires no additional heuristics for defining user similarity, but instead relies on an objective that maximizes self-similarity and minimizes similarity to other users randomly sampled from a large general pool. Our person re-identification proxy task makes use of the triplet loss used to learn person embeddings for face recognition (Schroff et al., 2015). In i"
N18-2094,D16-1108,1,0.831818,"e goal is for two embeddings from the same user to be closer to each other than to the embedding of a random user. The hypothesis is that a representation useful for detecting similarities between posts from the same person made at different times will also do well at identifying similarities between people in the same community. This hypothesis stems from observations that people with shared interests often talk about topics related to these interests, and that they tend to have shared jargon and other similarities in language use (Nguyen and Ros´e, 2011; Danescu-NiculescuMizil et al., 2013; Tran and Ostendorf, 2016). In this paper, we demonstrate experimentally that the re-identification proxy task is useful with simple models that are suited to the retrieval scenario, and present analyses showing that the approach learns to emphasize words associated with individual interests and polarizing issues. This paper addresses the problem of community membership detection using only text features in a scenario where a small number of positive labeled examples defines the community. The solution introduces an unsupervised proxy task for learning user embeddings: user re-identification. Experiments with 16 differ"
N18-2094,D14-1181,0,0.0114627,"Missing"
N18-2094,P16-2073,0,0.170852,"onomist, hedge fund manager, and ultramarathon runner communities. For the most part, the top ranked users from the general population tended to be people from related communities. For example, the top false ultimate frisbee users contained people who wrote about their participation in tournaments for other sports such as soccer. 4.3 5 Related Work One notion of community detection involves discovering different communities within a collection of users (Chen et al., 2009; Di, 2011; Fani et al., 2017). A related task is making recommendations of friends or people to follow (Gupta et al., 2013; Yu et al., 2016). In contrast, our task involves identifying other members of a community, which is specified in terms of a set of example users. These tasks use different learning frameworks (our work uses supervised learning), but the features (social network and/or text cues) Analysis The finding that the W2V-initialized RE-ID model is significantly better than W2V raises the question: how do the embeddings learned by the reidentification task differ from the ones learned by 4 People who write news articles about the Supreme Court of the United States. 5 The Stranger is a small weekly newspaper. 598 are re"
N18-2094,W11-0710,0,0.116062,"Missing"
N18-2094,P15-1169,0,0.0734532,"Missing"
N18-5020,P17-3020,0,0.0264314,"ational AI that has challenged researchers since 1990. Recent work has addressed tasks where passing the Turing test is not a concern. Goaloriented conversational systems facilitate natural user interaction with devices via text and spoken language. These AI assistants typically focus on short interactions, as in commercial products such as Amazon Alexa, Microsoft Cortana, Google Assistant, and Apple Siri. General conversational systems, called chatbots, have constrained social interaction capabilities but have difficulty generating conversations with long-term coherence (Serban et al., 2017; Sato et al., 2017; Shao et al., 2017; Tian et al., 2017; Ghazvininejad et al., 2018). The Alexa Prize sets forth a new challenge: creating a system that can hold a coherent and engaging conversation on current events and popular topics such as sports, politics, entertainment, fashion and technology (Ram et al., 2017). Our system, Sound1 Figure 1: A sample dialog. Suspected speech recognition errors in the user utterances are underlined. ing Board,2 demonstrates that it is feasible to build an agent that can engage in long-term conversation when backed by rich content and knowledge of the user obtained through"
N18-5020,D17-1235,0,0.0251686,"challenged researchers since 1990. Recent work has addressed tasks where passing the Turing test is not a concern. Goaloriented conversational systems facilitate natural user interaction with devices via text and spoken language. These AI assistants typically focus on short interactions, as in commercial products such as Amazon Alexa, Microsoft Cortana, Google Assistant, and Apple Siri. General conversational systems, called chatbots, have constrained social interaction capabilities but have difficulty generating conversations with long-term coherence (Serban et al., 2017; Sato et al., 2017; Shao et al., 2017; Tian et al., 2017; Ghazvininejad et al., 2018). The Alexa Prize sets forth a new challenge: creating a system that can hold a coherent and engaging conversation on current events and popular topics such as sports, politics, entertainment, fashion and technology (Ram et al., 2017). Our system, Sound1 Figure 1: A sample dialog. Suspected speech recognition errors in the user utterances are underlined. ing Board,2 demonstrates that it is feasible to build an agent that can engage in long-term conversation when backed by rich content and knowledge of the user obtained through interaction. Soundi"
N18-5020,P17-2036,0,0.0298266,"hers since 1990. Recent work has addressed tasks where passing the Turing test is not a concern. Goaloriented conversational systems facilitate natural user interaction with devices via text and spoken language. These AI assistants typically focus on short interactions, as in commercial products such as Amazon Alexa, Microsoft Cortana, Google Assistant, and Apple Siri. General conversational systems, called chatbots, have constrained social interaction capabilities but have difficulty generating conversations with long-term coherence (Serban et al., 2017; Sato et al., 2017; Shao et al., 2017; Tian et al., 2017; Ghazvininejad et al., 2018). The Alexa Prize sets forth a new challenge: creating a system that can hold a coherent and engaging conversation on current events and popular topics such as sports, politics, entertainment, fashion and technology (Ram et al., 2017). Our system, Sound1 Figure 1: A sample dialog. Suspected speech recognition errors in the user utterances are underlined. ing Board,2 demonstrates that it is feasible to build an agent that can engage in long-term conversation when backed by rich content and knowledge of the user obtained through interaction. Sounding Board won the in"
N19-1008,P14-2050,0,0.0454407,"nt categories are given in Table 1. Conditioning on the different contexts, we analyze errors in the development set made by the 88 (a) Prosody prediction model (b) Late fusion model Figure 1: Prosody prediction (left) and late fusion (right) models. xi is a contcatenation of token, POS and identity features embeddings at time i; ri , j is a concatenation of stress and phone embeddings for phone j in token i; pei is a vector of prosodic cues; gi and hi are hidden states of token level and phone level LSTMs, correspondingly. another on the phone level. First, we use pretrained word embeddings (Levy and Goldberg, 2014), part-of-speech tags embeddings, and identity features (whether the word is a filled pause, discourse marker, or incomplete) as inputs to a word-level bidirectional LSTM. Then, for each phone in a word we concatenate the phone embedding, its stress embedding, and the hidden state of the word-level LSTM for the corresponding token. The resulting phone feature vector is used as input to the second bidirectional LSTM. The last hidden state hi of this second LSTM for token i summarizes the phone, stress and context information of that token, which we use to predict word-level prosodic cues. We us"
N19-1008,D18-1490,0,0.583356,"et al., 2015; Suzuki et al., 2016), often targeting unsupervised translation from one modality to the other. In our work we use innovations as a novel representation learning approach, where our emphasis is on looking into complementary cues rather than similarity between multiple modalities. Most work on disfluency detection falls into three main categories: sequence tagging, noisy-channel and parsing-based approaches. Sequence tagging approaches rely on BIO tagging with recurrent neural networks (Hough and Schlangen, 2015; Zayats et al., 2016; Wang et al., 2016; Zayats and Ostendorf, 2018; Lou et al., 2018). Noisy channel models operate on a relationship between the reparandum and repair for identifying disfluencies (Charniak and Johnson, 2001; Zwarts et al., 2010). Lou and Johnson (2017) used a neural language model to rerank sentences using the noisy channel model. Another line of work combined parsing and disfluency removal tasks (Rasooli and Tetreault, 2013; Honnibal and Johnson, 2014; Tran et al., 2018). Recently a transitionbased neural model architecture was proposed for disfluency detection (Wang et al., 2017). The current state of the art in disfluency detection (Wang et al., 2018) uses"
N19-1008,P17-2087,0,0.763973,"oach, where our emphasis is on looking into complementary cues rather than similarity between multiple modalities. Most work on disfluency detection falls into three main categories: sequence tagging, noisy-channel and parsing-based approaches. Sequence tagging approaches rely on BIO tagging with recurrent neural networks (Hough and Schlangen, 2015; Zayats et al., 2016; Wang et al., 2016; Zayats and Ostendorf, 2018; Lou et al., 2018). Noisy channel models operate on a relationship between the reparandum and repair for identifying disfluencies (Charniak and Johnson, 2001; Zwarts et al., 2010). Lou and Johnson (2017) used a neural language model to rerank sentences using the noisy channel model. Another line of work combined parsing and disfluency removal tasks (Rasooli and Tetreault, 2013; Honnibal and Johnson, 2014; Tran et al., 2018). Recently a transitionbased neural model architecture was proposed for disfluency detection (Wang et al., 2017). The current state of the art in disfluency detection (Wang et al., 2018) uses a neural machine translation framework with a transformer architecture and additional simulated data. All of the models mentioned above rely heavily on pattern match features, hand-cra"
N19-1008,N01-1016,0,0.578252,": Using Prosody Innovations in Disfluency Detection Vicky Zayats and Mari Ostendorf Electrical Engineering Department University of Washington [vzayats, ostendor]@uw.edu Abstract The interruption point is associated with a disruption in the realization of a prosodic phrase, which could involve cutting words off or elongation associated with hesitation, followed by a prosodic reset at the start of the repair. There may also be emphasis in the repair to highlight the correction. Researchers have been working on automatic disfluency detection for many years (Lickley, 1994; Shriberg et al., 1997; Charniak and Johnson, 2001; Johnson and Charniak, 2004; Lease et al., 2006; Qian and Liu, 2013; Zayats et al., 2016), motivated in part by early work on parsing speech that assumed reliable detection of the interruption point (Nakatani and Hirschberg, 1994; Shriberg and Stolcke, 1997; Liu et al., 2006). The first efforts to integrate prosody with word cues for disfluency detection (Baron et al., 2002; Snover et al., 2004) found gains from using prosody, but word cues played the primary role. In subsequent work (Qian and Liu, 2013; Honnibal and Johnson, 2014; Wang et al., 2017), more effective models of word transcripts"
N19-1008,N13-1102,0,0.0734649,"tendorf Electrical Engineering Department University of Washington [vzayats, ostendor]@uw.edu Abstract The interruption point is associated with a disruption in the realization of a prosodic phrase, which could involve cutting words off or elongation associated with hesitation, followed by a prosodic reset at the start of the repair. There may also be emphasis in the repair to highlight the correction. Researchers have been working on automatic disfluency detection for many years (Lickley, 1994; Shriberg et al., 1997; Charniak and Johnson, 2001; Johnson and Charniak, 2004; Lease et al., 2006; Qian and Liu, 2013; Zayats et al., 2016), motivated in part by early work on parsing speech that assumed reliable detection of the interruption point (Nakatani and Hirschberg, 1994; Shriberg and Stolcke, 1997; Liu et al., 2006). The first efforts to integrate prosody with word cues for disfluency detection (Baron et al., 2002; Snover et al., 2004) found gains from using prosody, but word cues played the primary role. In subsequent work (Qian and Liu, 2013; Honnibal and Johnson, 2014; Wang et al., 2017), more effective models of word transcripts have been the main source of performance gains. The success of rece"
N19-1008,N15-1029,0,0.825319,"ng et al., 2018) uses a neural machine translation framework with a transformer architecture and additional simulated data. All of the models mentioned above rely heavily on pattern match features, hand-crafted or automatically extracted, that help to identify repetitions and disfluencies with parallel syntactic structure. While prosodic features are useful for detecting interruption points (Nakatani and Hirschberg, 1994; Shriberg and Stolcke, 1997; Shriberg, 1999; Liu et al., 2006), recent methods on disfluency detection predominantly rely on lexical information exclusively. An exception is (Ferguson et al., 2015), which showed some gains using a simple concatenation of pause and word duration features. Similar to disfluency detection, parsing has seen little use of prosody in recent studies. However, Tran et al. (2018) recently demonstrated that that a neural model using pause, word and rhyme duration, f0 and energy helps in spoken language parsing, specifically in the regions that contain disfluencies. Early fusion and late fusion are the two most popular types of modality fusion techniques. In recent years, more interesting modality fusion approaches were introduced, most of them where the fusion ha"
N19-1008,D13-1013,0,0.0650468,"categories: sequence tagging, noisy-channel and parsing-based approaches. Sequence tagging approaches rely on BIO tagging with recurrent neural networks (Hough and Schlangen, 2015; Zayats et al., 2016; Wang et al., 2016; Zayats and Ostendorf, 2018; Lou et al., 2018). Noisy channel models operate on a relationship between the reparandum and repair for identifying disfluencies (Charniak and Johnson, 2001; Zwarts et al., 2010). Lou and Johnson (2017) used a neural language model to rerank sentences using the noisy channel model. Another line of work combined parsing and disfluency removal tasks (Rasooli and Tetreault, 2013; Honnibal and Johnson, 2014; Tran et al., 2018). Recently a transitionbased neural model architecture was proposed for disfluency detection (Wang et al., 2017). The current state of the art in disfluency detection (Wang et al., 2018) uses a neural machine translation framework with a transformer architecture and additional simulated data. All of the models mentioned above rely heavily on pattern match features, hand-crafted or automatically extracted, that help to identify repetitions and disfluencies with parallel syntactic structure. While prosodic features are useful for detecting interrup"
N19-1008,Q14-1011,0,0.640764,"etection for many years (Lickley, 1994; Shriberg et al., 1997; Charniak and Johnson, 2001; Johnson and Charniak, 2004; Lease et al., 2006; Qian and Liu, 2013; Zayats et al., 2016), motivated in part by early work on parsing speech that assumed reliable detection of the interruption point (Nakatani and Hirschberg, 1994; Shriberg and Stolcke, 1997; Liu et al., 2006). The first efforts to integrate prosody with word cues for disfluency detection (Baron et al., 2002; Snover et al., 2004) found gains from using prosody, but word cues played the primary role. In subsequent work (Qian and Liu, 2013; Honnibal and Johnson, 2014; Wang et al., 2017), more effective models of word transcripts have been the main source of performance gains. The success of recent neural network systems raises the question of what the role is for prosody in future work. In the next section, we hypothesize where prosody might help and look at the relative frequency of these cases and the performance of a high accuracy disfluency detection algorithm in these contexts. With the premise that there is a potential for prosody to benefit disfluency detection, we then propose a new approach to extracting prosodic features. A major challenge for a"
N19-1008,P04-1005,0,0.43941,"in Disfluency Detection Vicky Zayats and Mari Ostendorf Electrical Engineering Department University of Washington [vzayats, ostendor]@uw.edu Abstract The interruption point is associated with a disruption in the realization of a prosodic phrase, which could involve cutting words off or elongation associated with hesitation, followed by a prosodic reset at the start of the repair. There may also be emphasis in the repair to highlight the correction. Researchers have been working on automatic disfluency detection for many years (Lickley, 1994; Shriberg et al., 1997; Charniak and Johnson, 2001; Johnson and Charniak, 2004; Lease et al., 2006; Qian and Liu, 2013; Zayats et al., 2016), motivated in part by early work on parsing speech that assumed reliable detection of the interruption point (Nakatani and Hirschberg, 1994; Shriberg and Stolcke, 1997; Liu et al., 2006). The first efforts to integrate prosody with word cues for disfluency detection (Baron et al., 2002; Snover et al., 2004) found gains from using prosody, but word cues played the primary role. In subsequent work (Qian and Liu, 2013; Honnibal and Johnson, 2014; Wang et al., 2017), more effective models of word transcripts have been the main source o"
N19-1008,C10-1154,0,0.350708,"Missing"
N19-1008,N04-4040,0,0.0274912,"epair. There may also be emphasis in the repair to highlight the correction. Researchers have been working on automatic disfluency detection for many years (Lickley, 1994; Shriberg et al., 1997; Charniak and Johnson, 2001; Johnson and Charniak, 2004; Lease et al., 2006; Qian and Liu, 2013; Zayats et al., 2016), motivated in part by early work on parsing speech that assumed reliable detection of the interruption point (Nakatani and Hirschberg, 1994; Shriberg and Stolcke, 1997; Liu et al., 2006). The first efforts to integrate prosody with word cues for disfluency detection (Baron et al., 2002; Snover et al., 2004) found gains from using prosody, but word cues played the primary role. In subsequent work (Qian and Liu, 2013; Honnibal and Johnson, 2014; Wang et al., 2017), more effective models of word transcripts have been the main source of performance gains. The success of recent neural network systems raises the question of what the role is for prosody in future work. In the next section, we hypothesize where prosody might help and look at the relative frequency of these cases and the performance of a high accuracy disfluency detection algorithm in these contexts. With the premise that there is a pote"
N19-1008,N18-1007,1,0.924352,"bution less skewed. Word Duration. Similar to pause information, we extract word duration information using MsState time alignments. We do not need to do the standard word-based duration normalization, since the idea behind the innovation model is to normalize Text Encoding for Prosody Prediction We use both context around a word as well as subword information in text encoding for prosody prediction. Our text encoding consists of two bidirectional LSTMs: one on the token level and 89 prosodic features using a richer context representation. Fundamental frequency (F0) and Energy (E). Similar to Tran et al. (2018), we use three F0 features and three energy features. The three F0 features include normalized cross correlation function (NCCF), log-pitch weighted by probability of voicing (POV), and the estimated delta of log pitch. The three energy features include the log of total energy, the log of total energy from lower 20 mel-frequency bands and the log of total energy from higher 20 mel-frequency bands. The contour features are extracted from 25-ms frames with 10-ms hops using Kaldi (Povey et al., 2011). Our model is trained to predict the mean of these features across the frames in a word. MFCCs. I"
N19-1008,C18-1299,0,0.702462,"2018; Lou et al., 2018). Noisy channel models operate on a relationship between the reparandum and repair for identifying disfluencies (Charniak and Johnson, 2001; Zwarts et al., 2010). Lou and Johnson (2017) used a neural language model to rerank sentences using the noisy channel model. Another line of work combined parsing and disfluency removal tasks (Rasooli and Tetreault, 2013; Honnibal and Johnson, 2014; Tran et al., 2018). Recently a transitionbased neural model architecture was proposed for disfluency detection (Wang et al., 2017). The current state of the art in disfluency detection (Wang et al., 2018) uses a neural machine translation framework with a transformer architecture and additional simulated data. All of the models mentioned above rely heavily on pattern match features, hand-crafted or automatically extracted, that help to identify repetitions and disfluencies with parallel syntactic structure. While prosodic features are useful for detecting interruption points (Nakatani and Hirschberg, 1994; Shriberg and Stolcke, 1997; Shriberg, 1999; Liu et al., 2006), recent methods on disfluency detection predominantly rely on lexical information exclusively. An exception is (Ferguson et al.,"
N19-1008,C16-1027,0,0.573678,"ties (Andrew et al., 2013; Ryan Kiros, 2014; Xu et al., 2015; Suzuki et al., 2016), often targeting unsupervised translation from one modality to the other. In our work we use innovations as a novel representation learning approach, where our emphasis is on looking into complementary cues rather than similarity between multiple modalities. Most work on disfluency detection falls into three main categories: sequence tagging, noisy-channel and parsing-based approaches. Sequence tagging approaches rely on BIO tagging with recurrent neural networks (Hough and Schlangen, 2015; Zayats et al., 2016; Wang et al., 2016; Zayats and Ostendorf, 2018; Lou et al., 2018). Noisy channel models operate on a relationship between the reparandum and repair for identifying disfluencies (Charniak and Johnson, 2001; Zwarts et al., 2010). Lou and Johnson (2017) used a neural language model to rerank sentences using the noisy channel model. Another line of work combined parsing and disfluency removal tasks (Rasooli and Tetreault, 2013; Honnibal and Johnson, 2014; Tran et al., 2018). Recently a transitionbased neural model architecture was proposed for disfluency detection (Wang et al., 2017). The current state of the art i"
N19-1008,D17-1296,0,0.622486,"kley, 1994; Shriberg et al., 1997; Charniak and Johnson, 2001; Johnson and Charniak, 2004; Lease et al., 2006; Qian and Liu, 2013; Zayats et al., 2016), motivated in part by early work on parsing speech that assumed reliable detection of the interruption point (Nakatani and Hirschberg, 1994; Shriberg and Stolcke, 1997; Liu et al., 2006). The first efforts to integrate prosody with word cues for disfluency detection (Baron et al., 2002; Snover et al., 2004) found gains from using prosody, but word cues played the primary role. In subsequent work (Qian and Liu, 2013; Honnibal and Johnson, 2014; Wang et al., 2017), more effective models of word transcripts have been the main source of performance gains. The success of recent neural network systems raises the question of what the role is for prosody in future work. In the next section, we hypothesize where prosody might help and look at the relative frequency of these cases and the performance of a high accuracy disfluency detection algorithm in these contexts. With the premise that there is a potential for prosody to benefit disfluency detection, we then propose a new approach to extracting prosodic features. A major challenge for all efforts to incorp"
N19-1008,D17-1115,0,\N,Missing
N19-1284,N19-1423,0,0.0989801,"ter (socialbot) and human-human conversations. 1 The implementation of code is available https://github.com/hao-cheng/dynamic_ speaker_model.git at While many studies rely only on discrete metadata and/or demographic information, such information is not always available. Thus, it is of interest to learn about the speaker from the language directly, as it relates to the person’s interests and speaking style. Motivated by the success of unsupervised contextualized representation learning for words and documents (Mikolov et al., 2013; Kiros et al., 2015; McCann et al., 2017; Peters et al., 2018; Devlin et al., 2019), our approach is to use unsupervised learning with a neural model of a speaker’s dialog history. The model uses latent speaker mode vectors for representing a speaker turn as in (Cheng et al., 2017), which provides a framework for analysis of what the model learns about speaking style. Further, the model is structured to allow a dynamic update of the speaker vector at each turn in a dialog, in order to capture changes over time and improve the speaker representation with added data. The speaker embeddings can be used as context in conversational language understanding tasks, e.g., as an addit"
N19-1284,N16-1037,0,0.0581815,"Missing"
N19-1284,K15-1011,0,0.0144597,"nuous embeddings for neural conversation models is studied in (Li 2779 et al., 2016), where the model directly learns a dictionary of speaker embeddings. Our unsupervised dynamic speaker model differs from previous work in that we build speaker embeddings as a weighted combination of latent modes with weights computed based on the utterance. Thus, the model can construct embeddings for any new users and dynamically update the embeddings as the conversation evolves. Speaker language variances have been analyzed by previous work and incorporated in NLP models. Preot¸iuc-Pietro et al. (2016) and Johannsen et al. (2015) find that speaker-level language variance affects lexical choices and even syntactic structure based on psycholinguistic hypotheses. Speaker demographics are used to improve both low-level tasks such as part-of-speech tagging (Hovy and Søgaard, 2015) and high-level applications such as sentiment analysis (Volkova et al., 2013) and machine translation (Mirkin et al., 2015). Lynn et al. (2017) introduce a continuous adaptation method to include user age, gender, personality traits and language features for personalizing several supervised NLP models. Different from previous work, we study the u"
N19-1284,W13-3214,0,0.051659,"Missing"
N19-1284,N18-5020,1,0.834027,"e randomly initialized based on N (0, 0.01). The mode vector dimension c is set to 64. We tune the number of global mode vectors K from {16, 32} and the hidden layer size m from {128, 160}. The final model is selected based on the log-likelihood on the development set. 3 User Topic Decision Prediction We first study a prediction task that estimates whether the user engaged in a socialbot conversation would accept a suggested topic. Specifically, we use a corpus of human-socialbot conversations collected during the 2017 Alexa Prize competition (Ram et al., 2017) from the Sounding Board system (Fang et al., 2018; Fang, 2019). Due to privacy concerns, the socialbot does not have access to any identity information about users. Also, since each device may be used by multiple users, the device address is not a reliable indicator of the user ID. Therefore, the ability to profile the user through one conversational interaction is desirable for guiding the socialbot’s dialog policy. 2774 # conversations # topic decisions train 19,076 85,340 dev 6,321 28,060 test 6,465 29,561 Table 1: Data statistics of the topic decision dataset. 3.1 Data Each conversation begins with a greeting and ends when the user makes"
N19-1284,N16-1062,0,0.0595661,"Missing"
N19-1284,N16-3018,0,0.021816,"nd Litman (2001), user modeling for conversational systems has a long history. The research can be tracked back to the GRUNDY system (Rich, 1979) which categorizes users in terms of hand-crafted sets of user properties for book recommendation. Other systems have focused on different aspects of users, e.g., the expertise level of the user in a specific domain (Chin, 1986; Sleeman, 1985; Paris, 1987; Hovy, 1987), the user’s intent and plan (Allen and Perrault, 1980; Carberry, 1983; Litman, 1986; Moore and Paris, 1992), and the user’s personality (Mairesse and Walker, 2006; DeVault et al., 2014; Fung et al., 2016; Fang et al., 2017). User modeling has also been employed for personalized topic suggestion in recent Alexa Prize socialbots, using a pre-defined mapping between personality types and topics (Fang et al., 2017), or a conditional random field sequence model with hand-crafted user and context features (Ahmadvand et al., 2018). Modeling speakers with continuous embeddings for neural conversation models is studied in (Li 2779 et al., 2016), where the model directly learns a dictionary of speaker embeddings. Our unsupervised dynamic speaker model differs from previous work in that we build speaker"
N19-1284,P82-1020,0,0.791301,"Missing"
N19-1284,P15-2079,0,0.0447632,"ntation of speakers is shown to be useful for content ranking in a socialbot and dialog act prediction in human-human conversations.1 1 Introduction Representing language in context is key to improving natural language processing (NLP). There are a variety of useful contexts, including word history, related documents, author/speaker information, social context, knowledge graphs, visual or situational grounding, etc. This paper addresses the problem of modeling the speaker. Accounting for author/speaker variations has been shown to be useful in many NLP tasks, including language understanding (Hovy and Søgaard, 2015; Volkova et al., 2013), language generation (Mirkin et al., 2015; Li et al., 2016), human-computer dialog policy (Bowden et al., 2018), query completion (Jaech and Ostendorf, 2018; Shokouhi, 2013), comment recommendation (Agarwal et al., 2011) and more. In this work, we specifically focus on dialogs, including both human-computer (socialbot) and human-human conversations. 1 The implementation of code is available https://github.com/hao-cheng/dynamic_ speaker_model.git at While many studies rely only on discrete metadata and/or demographic information, such information is not always available."
N19-1284,P16-1094,0,0.0162561,"ct prediction in human-human conversations.1 1 Introduction Representing language in context is key to improving natural language processing (NLP). There are a variety of useful contexts, including word history, related documents, author/speaker information, social context, knowledge graphs, visual or situational grounding, etc. This paper addresses the problem of modeling the speaker. Accounting for author/speaker variations has been shown to be useful in many NLP tasks, including language understanding (Hovy and Søgaard, 2015; Volkova et al., 2013), language generation (Mirkin et al., 2015; Li et al., 2016), human-computer dialog policy (Bowden et al., 2018), query completion (Jaech and Ostendorf, 2018; Shokouhi, 2013), comment recommendation (Agarwal et al., 2011) and more. In this work, we specifically focus on dialogs, including both human-computer (socialbot) and human-human conversations. 1 The implementation of code is available https://github.com/hao-cheng/dynamic_ speaker_model.git at While many studies rely only on discrete metadata and/or demographic information, such information is not always available. Thus, it is of interest to learn about the speaker from the language directly, as"
N19-1284,P86-1033,0,0.596345,"eaker gender language variations are indeed captured by the learned modes. 5 Related Work As reviewed by Zukerman and Litman (2001), user modeling for conversational systems has a long history. The research can be tracked back to the GRUNDY system (Rich, 1979) which categorizes users in terms of hand-crafted sets of user properties for book recommendation. Other systems have focused on different aspects of users, e.g., the expertise level of the user in a specific domain (Chin, 1986; Sleeman, 1985; Paris, 1987; Hovy, 1987), the user’s intent and plan (Allen and Perrault, 1980; Carberry, 1983; Litman, 1986; Moore and Paris, 1992), and the user’s personality (Mairesse and Walker, 2006; DeVault et al., 2014; Fung et al., 2016; Fang et al., 2017). User modeling has also been employed for personalized topic suggestion in recent Alexa Prize socialbots, using a pre-defined mapping between personality types and topics (Fang et al., 2017), or a conditional random field sequence model with hand-crafted user and context features (Ahmadvand et al., 2018). Modeling speakers with continuous embeddings for neural conversation models is studied in (Li 2779 et al., 2016), where the model directly learns a dict"
N19-1284,D17-1231,0,0.0592436,"Missing"
N19-1284,D17-1119,0,0.0139415,"rs and dynamically update the embeddings as the conversation evolves. Speaker language variances have been analyzed by previous work and incorporated in NLP models. Preot¸iuc-Pietro et al. (2016) and Johannsen et al. (2015) find that speaker-level language variance affects lexical choices and even syntactic structure based on psycholinguistic hypotheses. Speaker demographics are used to improve both low-level tasks such as part-of-speech tagging (Hovy and Søgaard, 2015) and high-level applications such as sentiment analysis (Volkova et al., 2013) and machine translation (Mirkin et al., 2015). Lynn et al. (2017) introduce a continuous adaptation method to include user age, gender, personality traits and language features for personalizing several supervised NLP models. Different from previous work, we study the use of speaker embeddings learned from utterances in an unsupervised fashion and analyze the possible interpretability of the latent modes. 6 Conclusion In this paper, we address the problem of modeling speakers from their language using an unsupervised approach. A dynamic speaker model is proposed to learn speaker embeddings that are updated as the conversation evolves. The model achieves pro"
N19-1284,N06-2022,0,0.0592433,"d modes. 5 Related Work As reviewed by Zukerman and Litman (2001), user modeling for conversational systems has a long history. The research can be tracked back to the GRUNDY system (Rich, 1979) which categorizes users in terms of hand-crafted sets of user properties for book recommendation. Other systems have focused on different aspects of users, e.g., the expertise level of the user in a specific domain (Chin, 1986; Sleeman, 1985; Paris, 1987; Hovy, 1987), the user’s intent and plan (Allen and Perrault, 1980; Carberry, 1983; Litman, 1986; Moore and Paris, 1992), and the user’s personality (Mairesse and Walker, 2006; DeVault et al., 2014; Fung et al., 2016; Fang et al., 2017). User modeling has also been employed for personalized topic suggestion in recent Alexa Prize socialbots, using a pre-defined mapping between personality types and topics (Fang et al., 2017), or a conditional random field sequence model with hand-crafted user and context features (Ahmadvand et al., 2018). Modeling speakers with continuous embeddings for neural conversation models is studied in (Li 2779 et al., 2016), where the model directly learns a dictionary of speaker embeddings. Our unsupervised dynamic speaker model differs fr"
N19-1284,D15-1130,0,0.0874091,"ocialbot and dialog act prediction in human-human conversations.1 1 Introduction Representing language in context is key to improving natural language processing (NLP). There are a variety of useful contexts, including word history, related documents, author/speaker information, social context, knowledge graphs, visual or situational grounding, etc. This paper addresses the problem of modeling the speaker. Accounting for author/speaker variations has been shown to be useful in many NLP tasks, including language understanding (Hovy and Søgaard, 2015; Volkova et al., 2013), language generation (Mirkin et al., 2015; Li et al., 2016), human-computer dialog policy (Bowden et al., 2018), query completion (Jaech and Ostendorf, 2018; Shokouhi, 2013), comment recommendation (Agarwal et al., 2011) and more. In this work, we specifically focus on dialogs, including both human-computer (socialbot) and human-human conversations. 1 The implementation of code is available https://github.com/hao-cheng/dynamic_ speaker_model.git at While many studies rely only on discrete metadata and/or demographic information, such information is not always available. Thus, it is of interest to learn about the speaker from the lang"
N19-1284,N18-1202,0,0.122765,"ding both human-computer (socialbot) and human-human conversations. 1 The implementation of code is available https://github.com/hao-cheng/dynamic_ speaker_model.git at While many studies rely only on discrete metadata and/or demographic information, such information is not always available. Thus, it is of interest to learn about the speaker from the language directly, as it relates to the person’s interests and speaking style. Motivated by the success of unsupervised contextualized representation learning for words and documents (Mikolov et al., 2013; Kiros et al., 2015; McCann et al., 2017; Peters et al., 2018; Devlin et al., 2019), our approach is to use unsupervised learning with a neural model of a speaker’s dialog history. The model uses latent speaker mode vectors for representing a speaker turn as in (Cheng et al., 2017), which provides a framework for analysis of what the model learns about speaking style. Further, the model is structured to allow a dynamic update of the speaker vector at each turn in a dialog, in order to capture changes over time and improve the speaker representation with added data. The speaker embeddings can be used as context in conversational language understanding ta"
N19-1284,J00-3003,0,0.681342,"Missing"
N19-1284,P17-2083,0,0.0366541,"Missing"
N19-1284,E17-1041,0,0.036906,"Missing"
N19-1284,D17-1229,0,0.027871,"Missing"
N19-1284,D13-1187,0,0.213857,"hown to be useful for content ranking in a socialbot and dialog act prediction in human-human conversations.1 1 Introduction Representing language in context is key to improving natural language processing (NLP). There are a variety of useful contexts, including word history, related documents, author/speaker information, social context, knowledge graphs, visual or situational grounding, etc. This paper addresses the problem of modeling the speaker. Accounting for author/speaker variations has been shown to be useful in many NLP tasks, including language understanding (Hovy and Søgaard, 2015; Volkova et al., 2013), language generation (Mirkin et al., 2015; Li et al., 2016), human-computer dialog policy (Bowden et al., 2018), query completion (Jaech and Ostendorf, 2018; Shokouhi, 2013), comment recommendation (Agarwal et al., 2011) and more. In this work, we specifically focus on dialogs, including both human-computer (socialbot) and human-human conversations. 1 The implementation of code is available https://github.com/hao-cheng/dynamic_ speaker_model.git at While many studies rely only on discrete metadata and/or demographic information, such information is not always available. Thus, it is of interes"
N19-1284,W18-5030,0,0.0311638,"stem turn t0 . The topic embedding xt0 ’s are looked up from the embedding dictionary learned by the FFNN. They are initialized by averaging the embeddings of their component words using the public pretrained 300-dimensional word embeddings (Bojanowski et al., 2017). For the user embedding vector, we explore two settings that use different numbers of user turns as context. In both settings, topic decisions occurring in the first 5 user turns are not used for evaluations. Static User Embeddings: Motivated by the findings that most user characteristics can be inferred from initial interactions (Ravichander and Black, 2018), we derive a static user embedding vector for a conversation using the first 5 user turns and apply it for predicting topic decisions afterwards. Dynamic User Embeddings: Alternatively, we build a user embedding vector for user turn t using all previous user turns. Here, a topic decision for system turn t0 is aligned with its preceding user turn t. In our experiments, we compare different unsupervised models with our proposed dynamic speaker model. For both settings, all unsupervised models are pre-trained on all user turns in training conversations. They are fixed when training the FFNN clas"
N19-1308,D18-1307,0,0.102224,"Missing"
N19-1308,P18-2014,0,0.0234032,"g systems. Our framework offers an alternative approach, forgoing sequence labeling entirely and simply considering all possible spans as candidate entities. Neural graph-based models have achieved significant improvements over traditional featurebased approaches on several graph modeling tasks. Knowledge graph completion (Yang et al., 2015; Bordes et al., 2013) is one prominent example. For relation extraction tasks, graphs have been used primarily as a means to incorporate pipelined features such as syntactic or discourse relations (Peng et al., 2017; Song et al., 2018; Zhang et al., 2018). Christopoulou et al. (2018) models all possible paths between entities as a graph, and refines pair-wise embeddings by performing a walk on the graph structure. All these previous works assume that the nodes of the graph (i.e. the entity candidates to be considered during relation extraction) are predefined and fixed throughout the learning process. On the other hand, our framework does not require a fixed set of entity boundaries as an input for graph construction. Motivated by state-ofthe-art span-based approaches to coreference resolution (Lee et al., 2017, 2018) and semantic role labeling (He et al., 2018), the mode"
N19-1308,Q14-1037,0,0.0405465,"ght benefit another. Most previous work in IE (e.g., (Nadeau and Sekine, 2007; Chan and Roth, 2011)) employs a pipeline approach, first detecting entities and then using the detected entity spans for relation extraction and coreference resolution. To avoid cascading 1 Code and pre-trained models are publicly available at https://github.com/luanyi/DyGIE. errors introduced by pipeline-style systems, recent work has focused on coupling different IE tasks as in joint modeling of entities and relations (Miwa and Bansal, 2016; Zhang et al., 2017), entities and coreferences (Hajishirzi et al., 2013; Durrett and Klein, 2014), joint inference (Singh et al., 2013) or multi-task (entity/relation/coreference) learning (Luan et al., 2018a). These models mostly rely on the first layer LSTM to share span representations between different tasks and are usually designed for specific domains. In this paper, we introduce a general framework Dynamic Graph IE (DY GIE) for coupling multiple information extraction tasks through shared span representations which are refined leveraging contextualized information from relations and coreferences. Our framework is effective in several domains, demonstrating a benefit from incorporat"
N19-1308,D13-1029,1,0.80646,"learned from one task might benefit another. Most previous work in IE (e.g., (Nadeau and Sekine, 2007; Chan and Roth, 2011)) employs a pipeline approach, first detecting entities and then using the detected entity spans for relation extraction and coreference resolution. To avoid cascading 1 Code and pre-trained models are publicly available at https://github.com/luanyi/DyGIE. errors introduced by pipeline-style systems, recent work has focused on coupling different IE tasks as in joint modeling of entities and relations (Miwa and Bansal, 2016; Zhang et al., 2017), entities and coreferences (Hajishirzi et al., 2013; Durrett and Klein, 2014), joint inference (Singh et al., 2013) or multi-task (entity/relation/coreference) learning (Luan et al., 2018a). These models mostly rely on the first layer LSTM to share span representations between different tasks and are usually designed for specific domains. In this paper, we introduce a general framework Dynamic Graph IE (DY GIE) for coupling multiple information extraction tasks through shared span representations which are refined leveraging contextualized information from relations and coreferences. Our framework is effective in several domains, demonstrating"
N19-1308,P18-2058,1,0.867573,"hristopoulou et al. (2018) models all possible paths between entities as a graph, and refines pair-wise embeddings by performing a walk on the graph structure. All these previous works assume that the nodes of the graph (i.e. the entity candidates to be considered during relation extraction) are predefined and fixed throughout the learning process. On the other hand, our framework does not require a fixed set of entity boundaries as an input for graph construction. Motivated by state-ofthe-art span-based approaches to coreference resolution (Lee et al., 2017, 2018) and semantic role labeling (He et al., 2018), the model uses a beam pruning strategy to dynamically select high-quality spans, and constructs a graph using the selected spans as nodes. 3037 Many state-of-the-art RE models rely upon domain-specific external syntactic tools to construct dependency paths between the entities in a sentence (Li and Ji, 2014; Xu et al., 2015; Miwa and Bansal, 2016; Zhang et al., 2017). These systems suffer from cascading errors from these tools and are hard to generalize to different domains. To make the model more general, we combine the multitask learning framework with ELMo embeddings (Peters et al., 2018)"
N19-1308,N18-1079,0,0.0616091,"ns that only incorporate broader context indirectly via the gradients passed back to the LSTM layer. In contrast, DY GIE uses dynamic graph propagation to explicitly incorporate rich contextual information into the span representations. Entity recognition has commonly been cast as a sequence labeling problem, and has benefited substantially from the use of neural architectures (Collobert et al., 2011; Lample et al., 2016; Ma and Hovy, 2016; Luan et al., 2017b, 2018b). However, most systems based on sequence labeling suffer from an inability to extract entities with overlapping spans. Recently Katiyar and Cardie (2018) and Wang and Lu (2018) have presented methods enabling neural models to extract overlapping entities, applying hypergraph-based representations on top of sequence labeling systems. Our framework offers an alternative approach, forgoing sequence labeling entirely and simply considering all possible spans as candidate entities. Neural graph-based models have achieved significant improvements over traditional featurebased approaches on several graph modeling tasks. Knowledge graph completion (Yang et al., 2015; Bordes et al., 2013) is one prominent example. For relation extraction tasks, graphs"
N19-1308,N18-2016,0,0.25756,"Missing"
N19-1308,N16-1030,0,0.0666976,"r to ours is the work in Luan et al. (2018a) that takes a multi-task learning approach to entity, relation, and coreference extraction. In this model, the different tasks share span representations that only incorporate broader context indirectly via the gradients passed back to the LSTM layer. In contrast, DY GIE uses dynamic graph propagation to explicitly incorporate rich contextual information into the span representations. Entity recognition has commonly been cast as a sequence labeling problem, and has benefited substantially from the use of neural architectures (Collobert et al., 2011; Lample et al., 2016; Ma and Hovy, 2016; Luan et al., 2017b, 2018b). However, most systems based on sequence labeling suffer from an inability to extract entities with overlapping spans. Recently Katiyar and Cardie (2018) and Wang and Lu (2018) have presented methods enabling neural models to extract overlapping entities, applying hypergraph-based representations on top of sequence labeling systems. Our framework offers an alternative approach, forgoing sequence labeling entirely and simply considering all possible spans as candidate entities. Neural graph-based models have achieved significant improvements over"
N19-1308,D17-1018,1,0.941791,"al., 2017; Song et al., 2018; Zhang et al., 2018). Christopoulou et al. (2018) models all possible paths between entities as a graph, and refines pair-wise embeddings by performing a walk on the graph structure. All these previous works assume that the nodes of the graph (i.e. the entity candidates to be considered during relation extraction) are predefined and fixed throughout the learning process. On the other hand, our framework does not require a fixed set of entity boundaries as an input for graph construction. Motivated by state-ofthe-art span-based approaches to coreference resolution (Lee et al., 2017, 2018) and semantic role labeling (He et al., 2018), the model uses a beam pruning strategy to dynamically select high-quality spans, and constructs a graph using the selected spans as nodes. 3037 Many state-of-the-art RE models rely upon domain-specific external syntactic tools to construct dependency paths between the entities in a sentence (Li and Ji, 2014; Xu et al., 2015; Miwa and Bansal, 2016; Zhang et al., 2017). These systems suffer from cascading errors from these tools and are hard to generalize to different domains. To make the model more general, we combine the multitask learning"
N19-1308,N18-2108,1,0.938635,"end points of si , an attention-based soft “headword,” and an embedded span width feature, following Lee et al. (2017). Coreference Propagation Layer The propagation process starts from the span representations gi0 . At each iteration t, we first compute an update vector utC for each span si . Then we use utC to update the current representation git , producing the next span representation git+1 . By repeating this process N times, the final span representations giN share contextual information across spans that are likely to be antecedents in the coreference graph, similar to the process in (Lee et al., 2018). Relation Propagation Layer The outputs giN from the coreference propagation layer are passed as inputs to the relation propagation layer. Similar to the coreference propagation process, at each iteration t, we first compute the update vectors utR for each span si , then use it to compute git+1 . Information can be integrated from multiple relation paths by repeating this process M times. Final Prediction Layer We use the outputs of the relation graph layer giN +M to predict the entity labels E and relation labels R. For entities, we pass giN +M to a feed-forward network (FFNN) to 3038 Final"
N19-1308,P14-1038,0,0.10148,"fixed throughout the learning process. On the other hand, our framework does not require a fixed set of entity boundaries as an input for graph construction. Motivated by state-ofthe-art span-based approaches to coreference resolution (Lee et al., 2017, 2018) and semantic role labeling (He et al., 2018), the model uses a beam pruning strategy to dynamically select high-quality spans, and constructs a graph using the selected spans as nodes. 3037 Many state-of-the-art RE models rely upon domain-specific external syntactic tools to construct dependency paths between the entities in a sentence (Li and Ji, 2014; Xu et al., 2015; Miwa and Bansal, 2016; Zhang et al., 2017). These systems suffer from cascading errors from these tools and are hard to generalize to different domains. To make the model more general, we combine the multitask learning framework with ELMo embeddings (Peters et al., 2018) without relying on external syntactic tools and risking the cascading errors that accompany them, and improve the interaction between tasks through dynamic graph propagation. While the performance of DyGIE benefits from ELMo, it advances over some systems (Luan et al., 2018a; Sanh et al., 2019) that also inc"
N19-1308,I17-1061,1,0.845416,"18a) that takes a multi-task learning approach to entity, relation, and coreference extraction. In this model, the different tasks share span representations that only incorporate broader context indirectly via the gradients passed back to the LSTM layer. In contrast, DY GIE uses dynamic graph propagation to explicitly incorporate rich contextual information into the span representations. Entity recognition has commonly been cast as a sequence labeling problem, and has benefited substantially from the use of neural architectures (Collobert et al., 2011; Lample et al., 2016; Ma and Hovy, 2016; Luan et al., 2017b, 2018b). However, most systems based on sequence labeling suffer from an inability to extract entities with overlapping spans. Recently Katiyar and Cardie (2018) and Wang and Lu (2018) have presented methods enabling neural models to extract overlapping entities, applying hypergraph-based representations on top of sequence labeling systems. Our framework offers an alternative approach, forgoing sequence labeling entirely and simply considering all possible spans as candidate entities. Neural graph-based models have achieved significant improvements over traditional featurebased approaches on"
N19-1308,D18-1360,1,0.148181,"ne approach, first detecting entities and then using the detected entity spans for relation extraction and coreference resolution. To avoid cascading 1 Code and pre-trained models are publicly available at https://github.com/luanyi/DyGIE. errors introduced by pipeline-style systems, recent work has focused on coupling different IE tasks as in joint modeling of entities and relations (Miwa and Bansal, 2016; Zhang et al., 2017), entities and coreferences (Hajishirzi et al., 2013; Durrett and Klein, 2014), joint inference (Singh et al., 2013) or multi-task (entity/relation/coreference) learning (Luan et al., 2018a). These models mostly rely on the first layer LSTM to share span representations between different tasks and are usually designed for specific domains. In this paper, we introduce a general framework Dynamic Graph IE (DY GIE) for coupling multiple information extraction tasks through shared span representations which are refined leveraging contextualized information from relations and coreferences. Our framework is effective in several domains, demonstrating a benefit from incorporating broader context learned from relation and coreference annotations. Figure 1 shows an example illustrating"
N19-1308,D17-1279,1,0.868329,"18a) that takes a multi-task learning approach to entity, relation, and coreference extraction. In this model, the different tasks share span representations that only incorporate broader context indirectly via the gradients passed back to the LSTM layer. In contrast, DY GIE uses dynamic graph propagation to explicitly incorporate rich contextual information into the span representations. Entity recognition has commonly been cast as a sequence labeling problem, and has benefited substantially from the use of neural architectures (Collobert et al., 2011; Lample et al., 2016; Ma and Hovy, 2016; Luan et al., 2017b, 2018b). However, most systems based on sequence labeling suffer from an inability to extract entities with overlapping spans. Recently Katiyar and Cardie (2018) and Wang and Lu (2018) have presented methods enabling neural models to extract overlapping entities, applying hypergraph-based representations on top of sequence labeling systems. Our framework offers an alternative approach, forgoing sequence labeling entirely and simply considering all possible spans as candidate entities. Neural graph-based models have achieved significant improvements over traditional featurebased approaches on"
N19-1308,S18-1125,1,0.146415,"ne approach, first detecting entities and then using the detected entity spans for relation extraction and coreference resolution. To avoid cascading 1 Code and pre-trained models are publicly available at https://github.com/luanyi/DyGIE. errors introduced by pipeline-style systems, recent work has focused on coupling different IE tasks as in joint modeling of entities and relations (Miwa and Bansal, 2016; Zhang et al., 2017), entities and coreferences (Hajishirzi et al., 2013; Durrett and Klein, 2014), joint inference (Singh et al., 2013) or multi-task (entity/relation/coreference) learning (Luan et al., 2018a). These models mostly rely on the first layer LSTM to share span representations between different tasks and are usually designed for specific domains. In this paper, we introduce a general framework Dynamic Graph IE (DY GIE) for coupling multiple information extraction tasks through shared span representations which are refined leveraging contextualized information from relations and coreferences. Our framework is effective in several domains, demonstrating a benefit from incorporating broader context learned from relation and coreference annotations. Figure 1 shows an example illustrating"
N19-1308,P16-1101,0,0.073951,"in Luan et al. (2018a) that takes a multi-task learning approach to entity, relation, and coreference extraction. In this model, the different tasks share span representations that only incorporate broader context indirectly via the gradients passed back to the LSTM layer. In contrast, DY GIE uses dynamic graph propagation to explicitly incorporate rich contextual information into the span representations. Entity recognition has commonly been cast as a sequence labeling problem, and has benefited substantially from the use of neural architectures (Collobert et al., 2011; Lample et al., 2016; Ma and Hovy, 2016; Luan et al., 2017b, 2018b). However, most systems based on sequence labeling suffer from an inability to extract entities with overlapping spans. Recently Katiyar and Cardie (2018) and Wang and Lu (2018) have presented methods enabling neural models to extract overlapping entities, applying hypergraph-based representations on top of sequence labeling systems. Our framework offers an alternative approach, forgoing sequence labeling entirely and simply considering all possible spans as candidate entities. Neural graph-based models have achieved significant improvements over traditional feature"
N19-1308,P16-1105,0,0.500436,"the same entity into one cluster. Thus, we might expect that knowledge learned from one task might benefit another. Most previous work in IE (e.g., (Nadeau and Sekine, 2007; Chan and Roth, 2011)) employs a pipeline approach, first detecting entities and then using the detected entity spans for relation extraction and coreference resolution. To avoid cascading 1 Code and pre-trained models are publicly available at https://github.com/luanyi/DyGIE. errors introduced by pipeline-style systems, recent work has focused on coupling different IE tasks as in joint modeling of entities and relations (Miwa and Bansal, 2016; Zhang et al., 2017), entities and coreferences (Hajishirzi et al., 2013; Durrett and Klein, 2014), joint inference (Singh et al., 2013) or multi-task (entity/relation/coreference) learning (Luan et al., 2018a). These models mostly rely on the first layer LSTM to share span representations between different tasks and are usually designed for specific domains. In this paper, we introduce a general framework Dynamic Graph IE (DY GIE) for coupling multiple information extraction tasks through shared span representations which are refined leveraging contextualized information from relations and c"
N19-1308,D15-1064,0,0.0263659,"demonstrate that our framework significantly outperforms the state-of-the-art on joint entity and relation detection tasks across four datasets: ACE 2004, ACE 2005, SciERC and the Wet Lab Protocol Corpus. 3) We further show that our approach excels at detecting entities with overlapping spans, achieving an improvement of up to 8 F1 points on three benchmarks annotated with overlapped spans: ACE 2004, ACE 2005 and GENIA. 2 Related Work Previous studies have explored joint modeling (Miwa and Bansal, 2016; Zhang et al., 2017; Singh et al., 2013; Yang and Mitchell, 2016)) and multi-task learning (Peng and Dredze, 2015; Peng et al., 2017; Luan et al., 2018a, 2017a) as methods to share representational strength across related information extraction tasks. The most similar to ours is the work in Luan et al. (2018a) that takes a multi-task learning approach to entity, relation, and coreference extraction. In this model, the different tasks share span representations that only incorporate broader context indirectly via the gradients passed back to the LSTM layer. In contrast, DY GIE uses dynamic graph propagation to explicitly incorporate rich contextual information into the span representations. Entity recogni"
N19-1308,Q17-1008,0,0.180357,"amework significantly outperforms the state-of-the-art on joint entity and relation detection tasks across four datasets: ACE 2004, ACE 2005, SciERC and the Wet Lab Protocol Corpus. 3) We further show that our approach excels at detecting entities with overlapping spans, achieving an improvement of up to 8 F1 points on three benchmarks annotated with overlapped spans: ACE 2004, ACE 2005 and GENIA. 2 Related Work Previous studies have explored joint modeling (Miwa and Bansal, 2016; Zhang et al., 2017; Singh et al., 2013; Yang and Mitchell, 2016)) and multi-task learning (Peng and Dredze, 2015; Peng et al., 2017; Luan et al., 2018a, 2017a) as methods to share representational strength across related information extraction tasks. The most similar to ours is the work in Luan et al. (2018a) that takes a multi-task learning approach to entity, relation, and coreference extraction. In this model, the different tasks share span representations that only incorporate broader context indirectly via the gradients passed back to the LSTM layer. In contrast, DY GIE uses dynamic graph propagation to explicitly incorporate rich contextual information into the span representations. Entity recognition has commonly b"
N19-1308,D14-1162,0,0.0860569,"from neighboring relation types and co-referred entities. These refined span representations are used in a multi-task framework to predict entity types, relation types, and coreference links. 3.1 Model Architecture In this section, we give an overview of the main components and layers of the DY GIE framework, as illustrated in Figure 2. Details of the graph construction and refinement process will be presented in the next section. Token Representation Layer We apply a bidirectional LSTM over the input tokens. The input for each token is a concatenation of the character reprensetation, GLoVe (Pennington et al., 2014) word embeddings, and ELMo embeddings (Peters et al., 2018). The output token representations are obtained by stacking the forward and backward LSTM hidden states. Span Representation Layer For each span si , its initial vector representation gi0 is obtained by concatenating BiLSTM outputs at the left and right end points of si , an attention-based soft “headword,” and an embedded span width feature, following Lee et al. (2017). Coreference Propagation Layer The propagation process starts from the span representations gi0 . At each iteration t, we first compute an update vector utC for each sp"
N19-1308,N18-1202,0,0.400911,"ng (He et al., 2018), the model uses a beam pruning strategy to dynamically select high-quality spans, and constructs a graph using the selected spans as nodes. 3037 Many state-of-the-art RE models rely upon domain-specific external syntactic tools to construct dependency paths between the entities in a sentence (Li and Ji, 2014; Xu et al., 2015; Miwa and Bansal, 2016; Zhang et al., 2017). These systems suffer from cascading errors from these tools and are hard to generalize to different domains. To make the model more general, we combine the multitask learning framework with ELMo embeddings (Peters et al., 2018) without relying on external syntactic tools and risking the cascading errors that accompany them, and improve the interaction between tasks through dynamic graph propagation. While the performance of DyGIE benefits from ELMo, it advances over some systems (Luan et al., 2018a; Sanh et al., 2019) that also incorporate ELMo. The analyses presented here give insights into the benefits of joint modeling. 3 Model Problem Definition The input is a document represented as a sequence of words D, from which we derive S = {s1 , . . . , sT }, the set of all possible within-sentence word sequence spans (u"
N19-1308,W12-4501,0,0.182999,"Missing"
N19-1308,W16-1300,0,0.149812,"Missing"
N19-1308,D18-1246,0,0.018417,"presentations on top of sequence labeling systems. Our framework offers an alternative approach, forgoing sequence labeling entirely and simply considering all possible spans as candidate entities. Neural graph-based models have achieved significant improvements over traditional featurebased approaches on several graph modeling tasks. Knowledge graph completion (Yang et al., 2015; Bordes et al., 2013) is one prominent example. For relation extraction tasks, graphs have been used primarily as a means to incorporate pipelined features such as syntactic or discourse relations (Peng et al., 2017; Song et al., 2018; Zhang et al., 2018). Christopoulou et al. (2018) models all possible paths between entities as a graph, and refines pair-wise embeddings by performing a walk on the graph structure. All these previous works assume that the nodes of the graph (i.e. the entity candidates to be considered during relation extraction) are predefined and fixed throughout the learning process. On the other hand, our framework does not require a fixed set of entity boundaries as an input for graph construction. Motivated by state-ofthe-art span-based approaches to coreference resolution (Lee et al., 2017, 2018) and"
N19-1308,D18-1019,0,0.141103,"er context indirectly via the gradients passed back to the LSTM layer. In contrast, DY GIE uses dynamic graph propagation to explicitly incorporate rich contextual information into the span representations. Entity recognition has commonly been cast as a sequence labeling problem, and has benefited substantially from the use of neural architectures (Collobert et al., 2011; Lample et al., 2016; Ma and Hovy, 2016; Luan et al., 2017b, 2018b). However, most systems based on sequence labeling suffer from an inability to extract entities with overlapping spans. Recently Katiyar and Cardie (2018) and Wang and Lu (2018) have presented methods enabling neural models to extract overlapping entities, applying hypergraph-based representations on top of sequence labeling systems. Our framework offers an alternative approach, forgoing sequence labeling entirely and simply considering all possible spans as candidate entities. Neural graph-based models have achieved significant improvements over traditional featurebased approaches on several graph modeling tasks. Knowledge graph completion (Yang et al., 2015; Bordes et al., 2013) is one prominent example. For relation extraction tasks, graphs have been used primaril"
N19-1308,D15-1062,0,0.0312111,"the learning process. On the other hand, our framework does not require a fixed set of entity boundaries as an input for graph construction. Motivated by state-ofthe-art span-based approaches to coreference resolution (Lee et al., 2017, 2018) and semantic role labeling (He et al., 2018), the model uses a beam pruning strategy to dynamically select high-quality spans, and constructs a graph using the selected spans as nodes. 3037 Many state-of-the-art RE models rely upon domain-specific external syntactic tools to construct dependency paths between the entities in a sentence (Li and Ji, 2014; Xu et al., 2015; Miwa and Bansal, 2016; Zhang et al., 2017). These systems suffer from cascading errors from these tools and are hard to generalize to different domains. To make the model more general, we combine the multitask learning framework with ELMo embeddings (Peters et al., 2018) without relying on external syntactic tools and risking the cascading errors that accompany them, and improve the interaction between tasks through dynamic graph propagation. While the performance of DyGIE benefits from ELMo, it advances over some systems (Luan et al., 2018a; Sanh et al., 2019) that also incorporate ELMo. Th"
N19-1308,N16-1033,0,0.0664368,"rmation, making the code publicly available. 2) We demonstrate that our framework significantly outperforms the state-of-the-art on joint entity and relation detection tasks across four datasets: ACE 2004, ACE 2005, SciERC and the Wet Lab Protocol Corpus. 3) We further show that our approach excels at detecting entities with overlapping spans, achieving an improvement of up to 8 F1 points on three benchmarks annotated with overlapped spans: ACE 2004, ACE 2005 and GENIA. 2 Related Work Previous studies have explored joint modeling (Miwa and Bansal, 2016; Zhang et al., 2017; Singh et al., 2013; Yang and Mitchell, 2016)) and multi-task learning (Peng and Dredze, 2015; Peng et al., 2017; Luan et al., 2018a, 2017a) as methods to share representational strength across related information extraction tasks. The most similar to ours is the work in Luan et al. (2018a) that takes a multi-task learning approach to entity, relation, and coreference extraction. In this model, the different tasks share span representations that only incorporate broader context indirectly via the gradients passed back to the LSTM layer. In contrast, DY GIE uses dynamic graph propagation to explicitly incorporate rich contextual informati"
N19-1308,D17-1182,0,0.0957664,"Missing"
N19-1308,D18-1244,0,0.0315507,"p of sequence labeling systems. Our framework offers an alternative approach, forgoing sequence labeling entirely and simply considering all possible spans as candidate entities. Neural graph-based models have achieved significant improvements over traditional featurebased approaches on several graph modeling tasks. Knowledge graph completion (Yang et al., 2015; Bordes et al., 2013) is one prominent example. For relation extraction tasks, graphs have been used primarily as a means to incorporate pipelined features such as syntactic or discourse relations (Peng et al., 2017; Song et al., 2018; Zhang et al., 2018). Christopoulou et al. (2018) models all possible paths between entities as a graph, and refines pair-wise embeddings by performing a walk on the graph structure. All these previous works assume that the nodes of the graph (i.e. the entity candidates to be considered during relation extraction) are predefined and fixed throughout the learning process. On the other hand, our framework does not require a fixed set of entity boundaries as an input for graph construction. Motivated by state-ofthe-art span-based approaches to coreference resolution (Lee et al., 2017, 2018) and semantic role labelin"
P05-1054,cieri-etal-2004-fisher,0,0.0566804,", it will increase our understanding of language production. From the engineering perspective, it can help improve the performance of a number of natural language processing tasks, such as text classification, machine translation or In this work, we focus on lexical differences between genders on telephone conversations and use machine learning techniques applied on text categorization and feature selection to characterize these differences. Therefore our conclusions are entirely data-driven. We use a very large corpus created for automatic speech recognition - the Fisher corpus described in (Cieri et al., 2004). The Fisher corpus is annotated with the gender of each speaker making it an ideal resource to study not only the characteristics of individual genders but also of gender pairs in spontaneous, conversational speech. The size and 435 Proceedings of the 43rd Annual Meeting of the ACL, pages 435–442, c Ann Arbor, June 2005. 2005 Association for Computational Linguistics scope of the Fisher corpus is such that robust results can be derived for American English. The computational methods we apply can assist us in answering questions, such as “To which degree are genderdiscriminative words content-"
P05-1054,J00-4001,0,0.0237761,"Missing"
P05-1065,W03-1004,0,0.0464885,"ords 71.5k 444k 927k 1M Table 1: Distribution of articles and words in the Weekly Reader corpus. Corpus Britannica B. Elementary CNN CNN Abridged Num Articles 115 115 111 111 Num Words 277k 74k 51k 37k Table 2: Distribution of articles and words in the Britannica and CNN corpora. tions of the newspaper. We design classifiers to distinguish each of these four categories. This corpus contains just under 2400 articles, distributed as shown in Table 1. Additionally, we have two corpora consisting of articles for adults and corresponding simplified versions for children or other language learners. Barzilay and Elhadad (2003) have allowed us to use their corpus from Encyclopedia Britannica, which contains articles from the full version of the encyclopedia and corresponding articles from Britannica Elementary, a new version targeted at children. The Western/Pacific Literacy Network’s (2004) web site has an archive of CNN news stories and abridged versions which we have also received permission to use. Although these corpora do not provide an explicit grade-level ranking for each article, broad categories are distinguished. We use these data as a supplement to the Weekly Reader corpus for learning models to distingu"
P05-1065,A00-2018,0,0.0468918,"d reading level classes than can serve to provide features for more detailed classification. Table 2 shows the size of the supplemental corpora. 4 Approach Existing reading level measures are inadequate due to their reliance on vocabulary lists and/or a superficial representation of syntax. Our approach uses ngram language models as a low-cost automatic ap525 proximation of both syntactic and semantic analysis. Statistical language models (LMs) are used successfully in this way in other areas of NLP such as speech recognition and machine translation. We also use a standard statistical parser (Charniak, 2000) to provide syntactic analysis. In practice, a teacher is likely to be looking for texts at a particular level rather than classifying a group of texts into a variety of categories. Thus we construct one classifier per category which decides whether a document belongs in that category or not, rather than constructing a classifier which ranks documents into different categories relative to each other. 4.1 Statistical Language Models Statistical LMs predict the probability that a particular word sequence will occur. The most commonly used statistical language model is the n-gram model, which ass"
P05-1065,N04-1025,0,0.773016,"Section 6 provides a summary and description of future work. 2 Reading Level Assessment This section highlights examples and features of some commonly used measures of reading level and discusses current research on the topic of reading level assessment using NLP techniques. Many traditional methods of reading level assessment focus on simple approximations of syntactic complexity such as sentence length. The widelyused Flesch-Kincaid Grade Level index is based on the average number of syllables per word and the average sentence length in a passage of text (Kincaid et al., 1975) (as cited in (Collins-Thompson and Callan, 2004)). Similarly, the Gunning Fog index is based on the average number of words per sentence and the percentage of words with three or more syllables (Gunning, 1952). These methods are quick and easy to calculate but have drawbacks: sentence length is not an accurate measure of syntactic complexity, and syllable count does not necessarily indicate the difficulty of a word. Additionally, a student may be familiar with a few complex words (e.g. dinosaur names) but unable to understand complex syntactic constructions. Other measures of readability focus on semantics, which is usually approximated by"
P05-1065,J01-2004,0,0.00538979,"de level, which is indeed what happened for 10 randomly chosen articles from standard edition of The Washington Post. 6 Conclusions and Future Work Statistical LMs were used to classify texts based on reading level, with trigram models being noticeably more accurate than bigrams and unigrams. Combining information from statistical LMs with other features using support vector machines provided the best results. Future work includes testing additional classifier features, e.g. parser likelihood scores and features obtained using a syntax-based language model such as Chelba and Jelinek (2000) or Roark (2001). Further experiments are planned on the generalizability of our classifier to text from other sources (e.g. newspaper articles, web pages); to accomplish this we will add higher level text as negative training data. We also plan to test these techniques on languages other than English, and incorporate them with an information retrieval system to create a tool that may be used by teachers to help select reading material for their students. Acknowledgments This material is based upon work supported by the National Science Foundation under Grant No. IIS-0326276. Any opinions, findings, and concl"
P05-1065,W96-0213,0,\N,Missing
P11-2021,W06-1615,0,0.420236,"This differs from punctuation annotation or segmentation, which is usually seen as a sequence tagging or classification task at word boundaries, and uses mostly local features. Our focus also allows us to clearly analyze the performance on different question types, in isolation from segmentation issues. We compare performance of textualand speech-trained lexical models, and examine the detection accuracy of each question type. Finally, 119 we compare two domain adaptation approaches to utilize unlabeled speech data: bootstrapping, and Blitzer et al.’s Structural Correspondence Learning (SCL) (Blitzer et al., 2006). SCL is a featurelearning method that uses unlabeled data from both domains. Although it has been applied to several NLP tasks, to our knowledge we are the first to apply SCL to both lexical and prosodic features in order to adapt from text to speech. 3 Experiments 3.1 Data The Wiki talk pages consist of threaded posts by different authors about a particular Wikipedia entry. While these lack certain properties of spontaneous speech (such as backchannels, disfluencies, and interruptions), they are more conversational than news articles, containing utterances such as: “Are you serious?” or “Hey"
P11-2021,D09-1130,0,0.0423926,"Missing"
P11-2021,W10-2607,1,0.739985,"and Allen (1997)). Previous work has investigated the utility of various feature types; Boakye et al. (2009), Shriberg et al. (1998) and Stolcke et al. (2000) showed that prosodic features were useful for question detection in English conversational speech, but (at least in the absence of recognition errors) most of the performance was achieved with words alone. There has been some previous investigation of domain adaptation for dialogue act classification, including adaptation between: different speech corpora (MRDA and Switchboard) (Guz et al., 2010), speech corpora in different languages (Margolis et al., 2010), and from a speech domain (MRDA/Switchboard) to text domains (emails and forums) (Jeong et al., 2009). These works did not use prosodic features, although Venkataraman Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 118–124, c Portland, Oregon, June 19-24, 2011. 2011 Association for Computational Linguistics et al. (2003) included prosodic features in a semisupervised learning approach for dialogue act labeling within a single spoken domain. Also relevant is the work of Moniz et al. (2011), who compared question types in different Por"
P11-2021,A97-1004,0,0.262995,"Missing"
P11-2021,W04-2319,0,0.0198822,"sodic features (extracted from the acoustic signal) and lexical features (extracted from the word sequence) have been shown to be useful for these tasks (Shriberg et al., 1998; Kim and Woodland, 2003; Ang et al., 2005). However, access to labeled speech training data is generally required in order to use prosodic features. On the other hand, the Internet contains large quantities of textual data that is already labeled with punctuation, and which can be used to train a system using lexical features. In this work, we focus on question detection in the Meeting Recorder Dialog Act corpus (MRDA) (Shriberg et al., 2004), using text sentences with question marks in Wikipedia “talk” 118 pages. We compare the performance of a question detector trained on the text domain using lexical features with one trained on MRDA using lexical features and/or prosodic features. In addition, we experiment with two unsupervised domain adaptation methods to incorporate unlabeled MRDA utterances into the text-based question detector. The goal is to use the unlabeled domain-matched data to bridge stylistic differences as well as to incorporate the prosodic features, which are unavailable in the labeled text data. 2 Related Work"
P11-2021,J00-3003,0,0.195344,"Missing"
P16-1153,P09-1010,0,0.0211587,"There has been increasing interest in applying deep reinforcement learning to a variety problems, but only a few studies address problems with natural language state or action spaces. In language processing, reinforcement learning has been applied to a dialogue management system that converses with a human user by taking actions that generate natural language (Scheffler and Young, 2002; Young et al., 2013). There has also been interest in extracting textual knowledge to improve game control performance (Branavan et al., 2011), and mapping text instructions to sequences of executable actions (Branavan et al., 2009). In some applications, it is possible to manually design features for state-action pairs, which are then used in reinforcement learning to learn a near-optimal policy (Li et al., 2009). Designing such features, however, require substantial domain knowledge. The work most closely related to our study inolves application of deep reinforcement to learning decision policies for parser-based text games. Narasimhan et al. (2015) applied a Long ShortTerm Memory DQN framework, which achieves higher average reward than the random and Bagof-Words DQN baselines. In this work, actions are constrained to"
P16-1153,P11-1028,0,0.0158818,"N can generalize well to “unseen” natural language descriptions of actions. 4 Related Work There has been increasing interest in applying deep reinforcement learning to a variety problems, but only a few studies address problems with natural language state or action spaces. In language processing, reinforcement learning has been applied to a dialogue management system that converses with a human user by taking actions that generate natural language (Scheffler and Young, 2002; Young et al., 2013). There has also been interest in extracting textual knowledge to improve game control performance (Branavan et al., 2011), and mapping text instructions to sequences of executable actions (Branavan et al., 2009). In some applications, it is possible to manually design features for state-action pairs, which are then used in reinforcement learning to learn a near-optimal policy (Li et al., 2009). Designing such features, however, require substantial domain knowledge. The work most closely related to our study inolves application of deep reinforcement to learning decision policies for parser-based text games. Narasimhan et al. (2015) applied a Long ShortTerm Memory DQN framework, which achieves higher average rewar"
P16-1153,D15-1001,0,0.52424,"Missing"
P16-1153,D15-1166,0,0.00887504,"Missing"
P18-2111,P14-2100,0,0.0253177,"signals, i.e. personalization. Personalization relies on the fact that query likelihoods are drastically different among different people depending on their needs and interests. Recently, Park and Chiba (2017) suggested a significantly different approach to QAC. In their 700 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 700–705 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics introduced by Mikolov and Zweig (2012) and has been used multiple times for LM personalization (Wen et al., 2013; Huang et al., 2014; Li et al., 2016). It works by concatenating a user embedding to the character embedding at every step of the input to the recurrent layer. Jaech and Ostendorf (2017) refer to this model as the ConcatCell and show that it is equivalent to adding a term Vu to adjust the bias of the recurrent layer. The hidden state of a ConcatCell with embedding size e and hidden state size h is given in Equation 1 where σ is the activation function, wt is the character embedding, ht−1 is the previous hidden state, and W ∈ Re+h×h and b ∈ Rh are the recurrent layer weight matrix and bias vector. the trained Fac"
P18-2111,P16-1094,0,0.0653354,"nalization. Personalization relies on the fact that query likelihoods are drastically different among different people depending on their needs and interests. Recently, Park and Chiba (2017) suggested a significantly different approach to QAC. In their 700 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 700–705 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics introduced by Mikolov and Zweig (2012) and has been used multiple times for LM personalization (Wen et al., 2013; Huang et al., 2014; Li et al., 2016). It works by concatenating a user embedding to the character embedding at every step of the input to the recurrent layer. Jaech and Ostendorf (2017) refer to this model as the ConcatCell and show that it is equivalent to adding a term Vu to adjust the bias of the recurrent layer. The hidden state of a ConcatCell with embedding size e and hidden state size h is given in Equation 1 where σ is the activation function, wt is the character embedding, ht−1 is the previous hidden state, and W ∈ Re+h×h and b ∈ Rh are the recurrent layer weight matrix and bias vector. the trained FactorCell model to d"
Q18-1009,W16-6209,1,0.126295,"Distributed under a CC-BY 4.0 license. (a) Forward hierarchical and timing structure (b) Backward hierarchical and timing structure Figure 2: An example of model propagation in a graph-structured LSTM. Here, the node name are shown in a chronological order, e.g. comment t1 was made earlier than t2. 2(a) Propagation of graph-structured LSTM in the forward direction. Blue arrows represent hierarchical propagation, green arrows represent timing propagation.2(b) Backward hierarchical (blue) and timing (green) propagation of graph-LSTM. popularity, which has also been called community endorsement (Fang et al., 2016), is the task of interest in our work on tree-structured modeling of discussions. Previous studies found that the time when the comment/post was published has a big impact on its popularity (Lakkaraju et al., 2013). In addition, the number of immediate responses can be predictive of the popularity, but some comments with a high number of replies can be either controversial or have a highly negative score. Language should be extremely important for distinguishing these cases. Indeed, community style matching is shown to be correlated to comment popularity in Reddit (Tran and Ostendorf, 2016). H"
Q18-1009,D16-1189,1,0.946035,"Fang et al., 2016), the LSTM units include both hierachical and temporal components to the update, which distinguishes this work from prior treestructured LSTM models. We assess the utility of the model in experiments on popularity prediction with Reddit discussions, comparing to a neural network baseline that treats comments independently but leverages information about the graph context and timing of the comment. We analyze the results to show that the graph LSTM provides a useful summary representation of the language context of the comment. As in Fang et al. (2016), but unlike other work (He et al., 2016), our model makes use of the full discussion thread in predicting popularity. While knowledge of the full discussion is only useful for posthoc analysis of past discussions, it is reasonable to consider initial responses to a comment, particularly given that many responses occur within minutes of someone posting a comment. Comments are often popular because of witty analogies made, which requires knowledge of the world beyond what is captured in current models. Responses to these comments, as well as to controversial comments, can improve popularity prediction. Responses of others clearly infl"
Q18-1009,D15-1239,1,0.943582,"arity, but some comments with a high number of replies can be either controversial or have a highly negative score. Language should be extremely important for distinguishing these cases. Indeed, community style matching is shown to be correlated to comment popularity in Reddit (Tran and Ostendorf, 2016). However, learning useful language cues can be difficult due to the low frequency of these events and the dominance of time, topic and other factors. Thus, in several prior studies, authors constrained the problem to reduce the effect of those factors (Lakkaraju et al., 2013; Tan et al., 2014; Jaech et al., 2015). In this study, we have no such constraints, but attempt to use the tree structure to capture the flow of information in order to better model the context in which a comment is submitted, including both the history it responds to as well as the subsequent response to that comment. To capture discussion dynamics, we introduce a novel approach to modeling the discussion using a bidirectional graph-structured LSTM, where each comment in the tree corresponds to a single LSTM 122 unit. In one direction, we capture the prior history of contributions leading up to a node, and in the other, we charac"
Q18-1009,S15-1002,0,0.0625505,"model uses a simple bag-of-words approach, and ii) they learn latent submission context models to determine the relative importance of textual cues, while our approach uses a submission context SVM to prune low karma comments (ignoring their text). Allowing for differences in baselines, we note that the absolute gain in performance from using text features is larger for our model, which represents language context. Tree LSTMs are a modification of sequential LSTMs that have been proposed for a variety of sentence-level NLP tasks (Tai et al., 2015; Zhu et al., 2015; Zhang et al., 2016; Le and Zuidema, 2015). The architecture of tree LSTMs varies depending on the task. Some options include summarizing over the children, adding a separate forget gate for each Ex 1 2 0 0 karma 7 7 7 4 0 0 3 7 0 0 7 4 5 7 7 3 3 7 7 7 7 6 7 0 5 6 7 6 0 0 0 8 6 0 0 0 Comment Republicans are fundamentally dishonest. (politics, id:1x9pcx) That is rape. She was drunk and could not consent. Period. Any of the supposed evidence otherwise is nothing but victim blaming. (askwomen, id:2h8pyh) The liberals keep saying the titanic is sinking but my side is 500 feet in the air. (politics, id:1upfgl) I miss your show, Stephen Col"
Q18-1009,P15-1150,0,0.448649,"g et al. (2016) use an LSTM to characterize comments, while our model uses a simple bag-of-words approach, and ii) they learn latent submission context models to determine the relative importance of textual cues, while our approach uses a submission context SVM to prune low karma comments (ignoring their text). Allowing for differences in baselines, we note that the absolute gain in performance from using text features is larger for our model, which represents language context. Tree LSTMs are a modification of sequential LSTMs that have been proposed for a variety of sentence-level NLP tasks (Tai et al., 2015; Zhu et al., 2015; Zhang et al., 2016; Le and Zuidema, 2015). The architecture of tree LSTMs varies depending on the task. Some options include summarizing over the children, adding a separate forget gate for each Ex 1 2 0 0 karma 7 7 7 4 0 0 3 7 0 0 7 4 5 7 7 3 3 7 7 7 7 6 7 0 5 6 7 6 0 0 0 8 6 0 0 0 Comment Republicans are fundamentally dishonest. (politics, id:1x9pcx) That is rape. She was drunk and could not consent. Period. Any of the supposed evidence otherwise is nothing but victim blaming. (askwomen, id:2h8pyh) The liberals keep saying the titanic is sinking but my side is 500 feet in"
Q18-1009,P14-1017,0,0.0395761,"ctive of the popularity, but some comments with a high number of replies can be either controversial or have a highly negative score. Language should be extremely important for distinguishing these cases. Indeed, community style matching is shown to be correlated to comment popularity in Reddit (Tran and Ostendorf, 2016). However, learning useful language cues can be difficult due to the low frequency of these events and the dominance of time, topic and other factors. Thus, in several prior studies, authors constrained the problem to reduce the effect of those factors (Lakkaraju et al., 2013; Tan et al., 2014; Jaech et al., 2015). In this study, we have no such constraints, but attempt to use the tree structure to capture the flow of information in order to better model the context in which a comment is submitted, including both the history it responds to as well as the subsequent response to that comment. To capture discussion dynamics, we introduce a novel approach to modeling the discussion using a bidirectional graph-structured LSTM, where each comment in the tree corresponds to a single LSTM 122 unit. In one direction, we capture the prior history of contributions leading up to a node, and in"
Q18-1009,D16-1108,1,0.331385,"orsement (Fang et al., 2016), is the task of interest in our work on tree-structured modeling of discussions. Previous studies found that the time when the comment/post was published has a big impact on its popularity (Lakkaraju et al., 2013). In addition, the number of immediate responses can be predictive of the popularity, but some comments with a high number of replies can be either controversial or have a highly negative score. Language should be extremely important for distinguishing these cases. Indeed, community style matching is shown to be correlated to comment popularity in Reddit (Tran and Ostendorf, 2016). However, learning useful language cues can be difficult due to the low frequency of these events and the dominance of time, topic and other factors. Thus, in several prior studies, authors constrained the problem to reduce the effect of those factors (Lakkaraju et al., 2013; Tan et al., 2014; Jaech et al., 2015). In this study, we have no such constraints, but attempt to use the tree structure to capture the flow of information in order to better model the context in which a comment is submitted, including both the history it responds to as well as the subsequent response to that comment. To"
Q18-1009,N16-1035,0,0.0385668,"cterize comments, while our model uses a simple bag-of-words approach, and ii) they learn latent submission context models to determine the relative importance of textual cues, while our approach uses a submission context SVM to prune low karma comments (ignoring their text). Allowing for differences in baselines, we note that the absolute gain in performance from using text features is larger for our model, which represents language context. Tree LSTMs are a modification of sequential LSTMs that have been proposed for a variety of sentence-level NLP tasks (Tai et al., 2015; Zhu et al., 2015; Zhang et al., 2016; Le and Zuidema, 2015). The architecture of tree LSTMs varies depending on the task. Some options include summarizing over the children, adding a separate forget gate for each Ex 1 2 0 0 karma 7 7 7 4 0 0 3 7 0 0 7 4 5 7 7 3 3 7 7 7 7 6 7 0 5 6 7 6 0 0 0 8 6 0 0 0 Comment Republicans are fundamentally dishonest. (politics, id:1x9pcx) That is rape. She was drunk and could not consent. Period. Any of the supposed evidence otherwise is nothing but victim blaming. (askwomen, id:2h8pyh) The liberals keep saying the titanic is sinking but my side is 500 feet in the air. (politics, id:1upfgl) I miss"
Q18-1035,P16-1160,0,0.0328461,"the one-hot context vector. Both strategies are explored, depending on the nature of the original context space. As noted in Section 5, adaptation of the softmax bias has been used in other studies. As we show in the experimental work, it is useful for representing phenomena where unigram statistics are important. 3 Data Our experiments make use of six datasets: four targeting word-level sequences, and two targeting character sequences. The character studies are motivated by the growing interest in character-level models in both speech recognition and machine translation (Hannun et al., 2014; Chung et al., 2016). By using multiple datasets with different types of context, we hope to learn more about what makes a dataset amenable to adaptation. The datasets range in size from over 100 million words of training data to 5 million characters of training data for the smallest one. When using a word-based vocabulary, we preprocess the data by lowercasing, tokenizing and removing most punctuation. We also truncate sentences to be shorter than a maximum length of 60 words for AGNews and DBPedia and 150 to 200 tokens for the remaining datasets. Summary information is provided in Table 1, including the trainin"
Q18-1035,W17-4912,0,0.336981,"s not being designed explicitly for text classification. Further, it allows us to be able to directly compare our model perfor1 Data was accessed from http://followthehashtag.com. mance against previously published text classification benchmarks. Note that the use of classification accuracy for evaluation here involves counting errors associated with applying the generative model to independent test samples. This differs from the accuracy criterion used for evaluating context-sensitive language models for text generation based on a separate discriminative classifier trained on generated text (Ficler and Goldberg, 2017; Hu et al., 2017). We discuss this further in Section 5. The experiments compare the FactorCell model (equations 4 and 6) to two popular alternatives, which we refer to as ConcatCell (equations 2 and 6) and SoftmaxBias (equation 6). As noted earlier, the SoftmaxBias method is a simplification of the ConcatCell model, which is in turn a simplification of the FactorCell model. The SoftmaxBias method impacts only the output layer and thus only unigram statistics. Since bag-of-word models provide strong baselines in many text classification tasks, we hypothesize that the SoftmaxBias model will ca"
Q18-1035,N16-1149,0,0.0684345,"Missing"
Q18-1035,P18-2111,1,0.628491,"ontrast, in many prior studies of language model adaptation, context is specified in terms of text samples, such as prior user queries, prior sentences in a dialog, other documents related in terms of topic or style, etc. The FactorCell framework introduced here is also applicable to this type of context, but the best encoding of the text into an embedding (e.g. using bag of words, sequence models, etc.) is likely to vary with the application. The FactorCell can also be used with online learning of context vectors, e.g. to take advantage of previous text from a particular author (or speaker) (Jaech and Ostendorf, 2018). The models evaluated here were tuned to minimize perplexity, as is typical for language modeling. In analyses of performance with different hyperparameter settings, we find that perplexity is not always positively correlated with accuracy, but the criteria are more often correlated for approaches that adapt the recurrent layer. While not surprising, the results raise concerns about using perplexity as the sole evaluation metric for context-aware language models. More work is needed to understand the relative utility of these objectives for language model design. Acknowledgments This research"
Q18-1035,W16-6212,1,0.843038,"ost are in English. There are 4,333 different hotels but we group all the ones that do not occur at least 50 times in the training data into a single entity, leaving us with around 3,500. These four datasets use wordbased vocabularies. We also experiment on two Twitter datasets: EuroTwitter and GeoTwitter. EuroTwitter consists of 80 thousand tweets labeled with one of nine languages: (English, Spanish, Galician, Catalan, Basque, Portuguese, French, German, and Italian). The corpus was created by combining portions of multiple published datasets for language identification including Twitter70 (Jaech et al., 2016), TweetLID (Zubiaga et al., 2014), and the monolingual portion of tweets from a code-switching detection 500 workshop (Molina et al., 2016). The GeoTwitter data contains tweets with latitude and longitude information from England, Spain, and the United States.1 The latitude and longitude coordinates are given as numerical inputs. This is different from the other five datasets that all use categorical context variables. 4 Experiments with Different Contexts The goal of our experiments is to show that the FactorCell model can deliver improved performance over current approaches for multiple lang"
Q18-1035,P16-1094,0,0.107407,"neural network approaches due to their flexibility for representing diverse types of contexts. Specifically, we focus on recurrent neural networks (RNNs) due to their widespread use. The standard approach to adapt an RNN language model is to concatenate the context representation with the word embedding at the input to the RNN (Mikolov and Zweig, 2012). Optionally, the context embedding is also concatenated with the output from the recurrent layer to adapt the softmax layer. This basic strategy has been adopted for various types of adaptation such as for LM personalization (Wen et al., 2013; Li et al., 2016), adapting to television show genres (Chen et al., 2015), and adapting to long range dependencies in a document (Ji et al., 2016), etc. We propose a more powerful mechanism for using a context vector, which we call the FactorCell. Rather than simply using context as an additional input, it is used to control a factored (low-rank) transformation of the recurrent layer weight matrix. The motivation is that allowing a greater fraction of the model parameters to be adjusted in response to the input context will produce a model that is more adaptable and responsive to that context. We evaluate the"
Q18-1035,W16-5805,0,0.0371506,"to a single entity, leaving us with around 3,500. These four datasets use wordbased vocabularies. We also experiment on two Twitter datasets: EuroTwitter and GeoTwitter. EuroTwitter consists of 80 thousand tweets labeled with one of nine languages: (English, Spanish, Galician, Catalan, Basque, Portuguese, French, German, and Italian). The corpus was created by combining portions of multiple published datasets for language identification including Twitter70 (Jaech et al., 2016), TweetLID (Zubiaga et al., 2014), and the monolingual portion of tweets from a code-switching detection 500 workshop (Molina et al., 2016). The GeoTwitter data contains tweets with latitude and longitude information from England, Spain, and the United States.1 The latitude and longitude coordinates are given as numerical inputs. This is different from the other five datasets that all use categorical context variables. 4 Experiments with Different Contexts The goal of our experiments is to show that the FactorCell model can deliver improved performance over current approaches for multiple language model applications and a variety of types of contexts. Specifically, results are reported for contextconditioned perplexity and genera"
Q18-1035,E17-2025,0,0.0357422,"computational cost. 2.3 Adapting the Softmax Bias The last layer of the model predicts the probability of the next symbol in the sequence using the output from the recurrent layer using the softmax function to create a normalized probability distribution. The output probabilities are given by yt = softmax(ELht + bout ), (5) where E ∈ R|V |×e is the matrix of word embeddings, L ∈ Re×d is a linear projection to match the dimension of the recurrent layer (when e 6= d), and 499 bout ∈ R|V |is the softmax bias vector. We tie the word embeddings in the input layer with the ones in the output layer (Press and Wolf, 2017; Inan et al., 2017). If sj is the indicator row vector for the jth word in the vocabulary P then p(wt |w1:t−1 ) = st yt and log p(w1:T ) = t log swt yt . Adapting the softmax bias alters the unigram distribution. There are two ways to accomplish this. When the values that the context can take are categorical with low cardinality then context-dependent softmax bias vectors can be learned directly. This is equivalent to replacing c with a one-hot vector. Otherwise, a projection of the context embedding, Qc where Q ∈ R|V |×k , can be used to adapt the bias vector as in yt = softmax(ELht + Qc + b"
Q18-1035,D17-1035,0,0.01713,"ly reported classification accuracies for generative models (Yogatama et al., 2017). However, it is not competitive with state-ofthe-art discriminative models on these tasks with the full training set. With less training data, it probably would be, based on the results in (Yogatama et al., 2017). The numbers in Table 3 do not adequately convey the fact that there are hyperparameters whose effect on perplexity is greater than the sometimes small relative differences between models. Even the seed for the random weight initialization can have a “major impact” on the final performance of an LSTM (Reimers and Gurevych, 2017). We use Figure 1 to show how the three classes of models perform across a range of hyperparameters. The figure compares 502 Figure 1: Accuracy vs. perplexity for different classes of models on the four word-based datasets. Within the same model class but across different hyperparameter settings, there is much more variation in perplexity than in accuracy. The LSTM cell size is mainly responsible for this; it has a much bigger impact on perplexity than on accuracy. It is also apparent that the models with the lowest perplexity are not always the ones with the highest accuracy. See Section 4.4"
Q18-1035,D15-1199,0,0.0347163,"orating document or topic information (Mikolov and Zweig, 2012; Ji et al., 2016; Ghosh et al., 2016; Dieng et al., 2017), where context is defined in terms of word or n-gram statistics. Our work differs from these studies in that the context is defined by a variety of sources, including discrete and/or continuous metadata, which is mapped to a context vector in end-to-end training. Context-sensitive language models for text generation tend to involve other forms of context similar to the objective of our work, including speaker characteristics (Luan et al., 2016; Li et al., 2016), dialog act (Wen et al., 2015), sentiment and other factors (Tang et al., 2016; Hu et al., 2017), and style (Ficler and Goldberg, 2017). As noted earlier, some of this work has used discriminative text classification to evaluate generation. In preliminary experiments with the Yelp data set, we found that the generative classifier accuracy of our model is highly correlated with discriminative classfier accuracy (r ≈ 0.95). Thus, by this measure, we anticipate that the model would be useful for generation applications. Anecdotally, we find that the model gives more coherent generation results for DBPedia data, but further va"
roark-etal-2006-sparseval,A00-2018,1,\N,Missing
roark-etal-2006-sparseval,J93-2004,0,\N,Missing
roark-etal-2006-sparseval,P97-1003,0,\N,Missing
roark-etal-2006-sparseval,N01-1016,1,\N,Missing
roark-etal-2006-sparseval,N04-4032,1,\N,Missing
roark-etal-2006-sparseval,J01-2004,1,\N,Missing
S18-1125,P11-1051,0,0.192322,"Missing"
S18-1125,W12-3202,0,0.0355986,"r and Teufel, 2012b,a), citation networks (Kas, 2011; Gabor et al., 2016; Sim et al., 2012; Do et al., 2013; Jaidka et al., 2014), summarization (Abu-Jbara and Model T1.1 T2-E T2-C Our system (Micro) Our system (Macro) 78.9 78.4 50.0 49.3 39.1 37.0 Team-1 Team-2 81.7 76.7 48.8 37.4 49.3 33.6 Table 2: Competition result for the top 3 teams. The official evaluation metric is macro F1 score. T1.1 means Subtask 1.1, T2-E means Subtask 2 senerio 1 (extraction task), T2-C means Subtask 2 senerio 2 (classification task). Radev, 2011) and some analysis of research community (Vogel and Jurafsky, 2012; Anderson et al., 2012). However, due to scarce hand-annotated data resources, previous work on information extraction (IE) for scientific literature is very limited. Most previous work focuses on unsupervised methods for extracting scientific terms such as bootstrapping Gupta and Manning (2011); Tsai et al. (2013), or extracting relations (G´abor et al., 2016). Luan et al. (2017); Augenstein and Søgaard (2017) applied semi-supervised learning and multi-task learning to neural based models to leverage large unannotated scholarly datasets for a scientific term extraction task (Augenstein and Søgaard, 2017). Although"
S18-1125,N12-1073,0,0.0386558,"Missing"
S18-1125,W12-4303,0,0.181219,"Missing"
S18-1125,P17-2054,0,0.0362696,"score. T1.1 means Subtask 1.1, T2-E means Subtask 2 senerio 1 (extraction task), T2-C means Subtask 2 senerio 2 (classification task). Radev, 2011) and some analysis of research community (Vogel and Jurafsky, 2012; Anderson et al., 2012). However, due to scarce hand-annotated data resources, previous work on information extraction (IE) for scientific literature is very limited. Most previous work focuses on unsupervised methods for extracting scientific terms such as bootstrapping Gupta and Manning (2011); Tsai et al. (2013), or extracting relations (G´abor et al., 2016). Luan et al. (2017); Augenstein and Søgaard (2017) applied semi-supervised learning and multi-task learning to neural based models to leverage large unannotated scholarly datasets for a scientific term extraction task (Augenstein and Søgaard, 2017). Although not much supervised relation extraction work has been done on scientific literature, neural network techniqueshave obtained the state of the art for general domain relation extraction. Both convolutional (Santos et al., 2015) and RNNbased architectures (Xu et al., 2016; Miwa and Bansal, 2016; Peng et al., 2017; Quirk and Poon, 2017) have been successfully applied to the task and significa"
S18-1125,bird-etal-2008-acl,0,0.0822868,"enate the distance embedding with all the other features. The concatenated features are then projected down to a lower dimension through tanh function and make the final prediction through a sof tmax function. 3 Experimental Setup External Data We use two external resources for pretraining word embeddings: i) the Semantic Scholar Corpus,2 a collection of over 20 million research papers from which we extract a subset of 110k abstracts of publications in the artificial intelligence area; and ii) the ACL Anothology Reference Corpus, which contains 22k full papers published in the ACL Anothology (Bird et al., 2008). Baseline We compare our model with a baseline that removes the Concept Selection Layer and replaces it with a weighted sum (using attention) of hidden states (from the Sequential LSTM Layer) for all words in a concept. For all experiments, we explore tuning with two different evaluation metrics: macro-F1 score and micro-F1 score.3 We keep the pre-trained concept embedding fixed as additional input feature. The word embedding dimension is 250; the LSTM hidden dimension is 100 (for both sequential and dependency layer); the character-level hidden dimension is 25; and the optimization algorithm"
S18-1125,L16-1586,0,0.166587,"Missing"
S18-1125,D17-1279,1,0.943033,"unt when relevant for the classification task (5 out of the 6 semantic relations are asymmetrical). We will use this example throughout the paper to illustrate various parts of our system. The SemEval 2018 Task 7 dataset contains 350 abstracts from the ACL Anthology for training and validation, and 150 abstracts for testing each subtask. Since the scale of the data is small for supervised training of neural systems, we introduce several strategies to leverage a large quantity of unlabeled scientific articles. In addition to initializing a neural system with pre-trained word embeddings, as in (Luan et al., 2017), we also try to incorporate embeddings of concepts that span multiple words. In neural models such as (Miwa and Bansal, 2016), phrases are often represented by an average (or weighted average) of the token’s sequential LSTM representation. The intuition behind explicit modeling of multi-word concept embeddings is that the concept use may be different from that of its individual words. Due to the size of the dataset and the nature of scientific literature, a large number of the scientific terms in the test set have never appeared in the training set, so supervised learning of the phrase embedd"
S18-1125,P16-1105,0,0.397933,"le throughout the paper to illustrate various parts of our system. The SemEval 2018 Task 7 dataset contains 350 abstracts from the ACL Anthology for training and validation, and 150 abstracts for testing each subtask. Since the scale of the data is small for supervised training of neural systems, we introduce several strategies to leverage a large quantity of unlabeled scientific articles. In addition to initializing a neural system with pre-trained word embeddings, as in (Luan et al., 2017), we also try to incorporate embeddings of concepts that span multiple words. In neural models such as (Miwa and Bansal, 2016), phrases are often represented by an average (or weighted average) of the token’s sequential LSTM representation. The intuition behind explicit modeling of multi-word concept embeddings is that the concept use may be different from that of its individual words. Due to the size of the dataset and the nature of scientific literature, a large number of the scientific terms in the test set have never appeared in the training set, so supervised learning of the phrase embeddings is not feasible. Therefore, we pre-trained scientific term embeddings on a large scientific corpus and provide a strategy"
S18-1125,Q17-1008,0,0.0577441,"or extracting relations (G´abor et al., 2016). Luan et al. (2017); Augenstein and Søgaard (2017) applied semi-supervised learning and multi-task learning to neural based models to leverage large unannotated scholarly datasets for a scientific term extraction task (Augenstein and Søgaard, 2017). Although not much supervised relation extraction work has been done on scientific literature, neural network techniqueshave obtained the state of the art for general domain relation extraction. Both convolutional (Santos et al., 2015) and RNNbased architectures (Xu et al., 2016; Miwa and Bansal, 2016; Peng et al., 2017; Quirk and Poon, 2017) have been successfully applied to the task and significantly improve performance. 6 Conclusion This paper describes the system of the UWNLP team submitted to SemEval 2018 Task 7. We extend state-of-the-art neural models for information extraction by proposing a Concept Selection module which can leverage the semantic information of concepts pre-trained from a large scholarly dataset. Our system ranked second in the relation classification task (subtask 1.1 and subtask 2 senerio 2), and first in the relation extraction task (subtask 2 scenario 1). Acknowledgments This re"
S18-1125,E17-1110,0,0.0318402,"tions (G´abor et al., 2016). Luan et al. (2017); Augenstein and Søgaard (2017) applied semi-supervised learning and multi-task learning to neural based models to leverage large unannotated scholarly datasets for a scientific term extraction task (Augenstein and Søgaard, 2017). Although not much supervised relation extraction work has been done on scientific literature, neural network techniqueshave obtained the state of the art for general domain relation extraction. Both convolutional (Santos et al., 2015) and RNNbased architectures (Xu et al., 2016; Miwa and Bansal, 2016; Peng et al., 2017; Quirk and Poon, 2017) have been successfully applied to the task and significantly improve performance. 6 Conclusion This paper describes the system of the UWNLP team submitted to SemEval 2018 Task 7. We extend state-of-the-art neural models for information extraction by proposing a Concept Selection module which can leverage the semantic information of concepts pre-trained from a large scholarly dataset. Our system ranked second in the relation classification task (subtask 1.1 and subtask 2 senerio 2), and first in the relation extraction task (subtask 2 scenario 1). Acknowledgments This research was supported by"
S18-1125,P15-1061,0,0.0642462,"ting scientific terms such as bootstrapping Gupta and Manning (2011); Tsai et al. (2013), or extracting relations (G´abor et al., 2016). Luan et al. (2017); Augenstein and Søgaard (2017) applied semi-supervised learning and multi-task learning to neural based models to leverage large unannotated scholarly datasets for a scientific term extraction task (Augenstein and Søgaard, 2017). Although not much supervised relation extraction work has been done on scientific literature, neural network techniqueshave obtained the state of the art for general domain relation extraction. Both convolutional (Santos et al., 2015) and RNNbased architectures (Xu et al., 2016; Miwa and Bansal, 2016; Peng et al., 2017; Quirk and Poon, 2017) have been successfully applied to the task and significantly improve performance. 6 Conclusion This paper describes the system of the UWNLP team submitted to SemEval 2018 Task 7. We extend state-of-the-art neural models for information extraction by proposing a Concept Selection module which can leverage the semantic information of concepts pre-trained from a large scholarly dataset. Our system ranked second in the relation classification task (subtask 1.1 and subtask 2 senerio 2), and"
S18-1125,W12-3203,0,0.136778,"Missing"
S18-1125,W12-3204,0,0.0699626,"d citation sentiment (Athar and Teufel, 2012b,a), citation networks (Kas, 2011; Gabor et al., 2016; Sim et al., 2012; Do et al., 2013; Jaidka et al., 2014), summarization (Abu-Jbara and Model T1.1 T2-E T2-C Our system (Micro) Our system (Macro) 78.9 78.4 50.0 49.3 39.1 37.0 Team-1 Team-2 81.7 76.7 48.8 37.4 49.3 33.6 Table 2: Competition result for the top 3 teams. The official evaluation metric is macro F1 score. T1.1 means Subtask 1.1, T2-E means Subtask 2 senerio 1 (extraction task), T2-C means Subtask 2 senerio 2 (classification task). Radev, 2011) and some analysis of research community (Vogel and Jurafsky, 2012; Anderson et al., 2012). However, due to scarce hand-annotated data resources, previous work on information extraction (IE) for scientific literature is very limited. Most previous work focuses on unsupervised methods for extracting scientific terms such as bootstrapping Gupta and Manning (2011); Tsai et al. (2013), or extracting relations (G´abor et al., 2016). Luan et al. (2017); Augenstein and Søgaard (2017) applied semi-supervised learning and multi-task learning to neural based models to leverage large unannotated scholarly datasets for a scientific term extraction task (Augenstein and S"
S18-1125,C16-1138,0,0.0265432,"and Manning (2011); Tsai et al. (2013), or extracting relations (G´abor et al., 2016). Luan et al. (2017); Augenstein and Søgaard (2017) applied semi-supervised learning and multi-task learning to neural based models to leverage large unannotated scholarly datasets for a scientific term extraction task (Augenstein and Søgaard, 2017). Although not much supervised relation extraction work has been done on scientific literature, neural network techniqueshave obtained the state of the art for general domain relation extraction. Both convolutional (Santos et al., 2015) and RNNbased architectures (Xu et al., 2016; Miwa and Bansal, 2016; Peng et al., 2017; Quirk and Poon, 2017) have been successfully applied to the task and significantly improve performance. 6 Conclusion This paper describes the system of the UWNLP team submitted to SemEval 2018 Task 7. We extend state-of-the-art neural models for information extraction by proposing a Concept Selection module which can leverage the semantic information of concepts pre-trained from a large scholarly dataset. Our system ranked second in the relation classification task (subtask 1.1 and subtask 2 senerio 2), and first in the relation extraction task (subt"
W03-0703,H01-1051,0,0.0719664,"Missing"
W03-0703,P00-1013,0,0.0155944,"aving multiple concurrent subdialogs directly affects the dialog planning component. Different user queries and dialog states might need to be tracked simultaneously, and formal models of this type of interaction need to be established. Recently, probabilistic models of human-computer dialogs have become increasingly popular. The most commonly used paradigm is that of Markov decision processes and partially observable Markov decision processes, where the entire dialog is modelled as a sequence of states and associated actions, each of which has a certain value (or reward) (Singh et al., 1999; Roy et al., 2000). The goal is to to choose that sequence of actions which maximizes the overall reward in response to the user’s query. States can be thought of as representing the underlying intentions of the user. These are typically not entirely transparent but only indirectly (or partially) observable through the speech input. Multi-party dialogs might require extensions to this and other modeling frameworks. For instance, it is unclear whether multiple parallel subdialogs can be modelled by a single state sequence (i.e. a single decision process), or whether multiple, partially independent decision proce"
W03-0703,damianos-etal-2000-evaluating,0,\N,Missing
W08-0124,2007.sigdial-1.33,1,0.556577,"Missing"
W08-0124,W04-2319,0,0.044048,"ve divided these meetings into: ICSI T RAIN S ET, consisting of the 33 meetings for which d mod 4 ∈ {1, 2}; ICSI D EV S ET, consisting of the 18 meetings for which d mod 4 ≡ 3; and ICSI E VAL S ET, consisting of the 16 meetings for which d mod 4 ≡ 0. These three sets are not disjoint in participants, and the number of instrumented participants K varies from meeting to meeting, between 3 and 9. The corpus is accompanied by metadata specifying the gender, age, and education level of each participant. We use only the forced alignments of these meetings, available in the accompanying MRDA Corpus (Shriberg et al, 2004). 4 Features Our observation space is the complete K-participant vocal interaction on-off pattern description for a meeting C, a discretized version of which we denote as qt ∈ {0, 1}K for 1≤t≤T , where T is the duration of C in terms of the number of 100 ms frames. Details regarding the discretization (and subsequent feature computation) can be found in (Laskowski, Ostendorf and Schultz, 2007). We compute from qt the following features4 which are the elements of F: fkV I , the probabil4 Feature type superscripts indicate talkspurt initiation (I) or continuation (C), for either single-participa"
W10-2607,W06-1615,0,0.81794,"daptation in the literature. We refer to the labeled training domain as the source domain. We compare two adaptation approaches: a simple one based on forcing the classifier to learn only on “shared” features that appear in both domains, and a more complex one based on Structural Correspondence Learning (SCL) from Blitzer et al. (2007). The shared feature approach has been investigated for adaptation in other tasks, e.g. Aue and Gamon (2005) for sentiment classification and Dredze et al. (2007) for parsing. SCL has been used successfully for sentiment classification and part-ofspeech tagging (Blitzer et al., 2006); here we investigate its applicability to the DA classification task, using a multi-view learning implementation as suggested by Blitzer et al. (2009). In addition to analyzing these two methods on a novel task, we show an interesting comparison between them: in this setting, both methods turn out to have a similar effect caused by correlating cues for a particular DA class (Backchannel) with length. Abstract We investigate the classification of utterances into high-level dialog act categories using word-based features, under conditions where the train and test data differ by genre and/or lan"
W10-2607,P07-1056,0,0.863753,"2 TTI-Chicago, Chicago, IL, USA. amargoli@ee.washington.edu, klivescu@ttic.edu, mo@ee.washington.edu This work attempts to use unlabeled target domain data in order to improve cross-domain training performance, an approach referred to as both unsupervised and semi-supervised domain adaptation in the literature. We refer to the labeled training domain as the source domain. We compare two adaptation approaches: a simple one based on forcing the classifier to learn only on “shared” features that appear in both domains, and a more complex one based on Structural Correspondence Learning (SCL) from Blitzer et al. (2007). The shared feature approach has been investigated for adaptation in other tasks, e.g. Aue and Gamon (2005) for sentiment classification and Dredze et al. (2007) for parsing. SCL has been used successfully for sentiment classification and part-ofspeech tagging (Blitzer et al., 2006); here we investigate its applicability to the DA classification task, using a multi-view learning implementation as suggested by Blitzer et al. (2009). In addition to analyzing these two methods on a novel task, we show an interesting comparison between them: in this setting, both methods turn out to have a simila"
W10-2607,D09-1130,0,0.171914,"Missing"
W10-2607,C08-1123,0,0.0783649,"15 July 2010. 2010 Association for Computational Linguistics apply machine translation to the target domain to convert it to the language of the source domain. value decomposition (SVD) is performed on the collection of learned linear predictors corresponding to different pivot features. Features that tend to get similar weights in predicting pivot features will be tied together in the SVD. By learning on the SVD dimensions, the source-trained classifier can put weight on target-only features. 2 Related Work Automatic DA tagging across domain has been investigated by a handful of researchers. Webb and Liu (2008) investigated cross-corpus training between Swbd and another corpus consisting of task-oriented calls, although no adaptation was attempted. Similarly, Rosset et al. (2008) reported on recognition of task-oriented DA tags across domain and language (French to English) by using utterances that had been pre-processed to extract entities. Tur (2005) applied supervised model adaptation to intent classification across customer dialog systems, and Guz et al. (2010) applied supervised model adaptation methods for DA segmentation and classification on MRDA using labeled data from both MRDA and Swbd. M"
W10-2607,N03-1027,0,0.0351638,"d target domain examples into training. Success has also been reported for self-training approaches on same-domain semisupervised learning (Venkataraman et al., 2003; Tur et al., 2005). We are not aware of prior work on cross-lingual DA tagging via machine translation, although a translation approach has been employed for cross-lingual text classification and information retrieval, e.g. Bel et al. (2003). In recent years there has been increasing interest in domain adaptation methods based on unlabeled target domain data. Several kinds of approaches have been proposed, including selftraining (Roark and Bacchiani, 2003), instance weighting (Huang et al., 2007), change of feature representation (Pan et al., 2008), and clustering methods (Xing et al., 2007). SCL (Blitzer et al., 2006) is one feature representation approach that has been effective on certain high-dimensional NLP problems, including part-of-speech tagging and sentiment classification. SCL uses unlabeled data to learn feature projections that tie together source and target features via their correlations with features shared between domains. It first selects “pivot features” that are common in both domains; next, linear predictors for those featu"
W10-2607,W04-2319,0,0.0927474,"mains can differ linguistically, and the same DA categories might be characterized by different cues. The domain characteristics (face-to-face vs. telephone, two-party vs. multi-party, informal vs. agendadriven, familiar vs. stranger) can influence both the distribution of tags and word choice. We classify pre-segmented utterances based on their transcripts, and we consider only four highlevel classes: Statement, Question, Backchannel, and Incomplete. Experiments are performed using all train/test pairs among three conversational speech corpora : the Meeting Recorder Dialog Act corpus (MRDA) (Shriberg et al., 2004), Switchboard DAMSL (Swbd) (Jurafsky et al., 1997), and the Spanish Callhome dialog act corpus (SpCH) (Levin et al., 1998). The first is multi-party, face-to-face meeting speech; the second is topicprompted telephone speech between strangers; and the third is informal telephone speech between friends and family members. The first two are in English, while the third is in Spanish. When the source and target domains differ in language, we 45 Proceedings of the 2010 Workshop on Domain Adaptation for Natural Language Processing, ACL 2010, pages 45–52, c Uppsala, Sweden, 15 July 2010. 2010 Associat"
W10-2607,J00-3003,0,0.279064,"Missing"
W10-2607,D07-1112,0,\N,Missing
W11-0706,W11-0707,1,0.896375,"ity bids may involve multiple turns when the participant is challenged. Similarly, discussion participants may align with or distance themselves from other participants with a single statement, or someone could agree with one person at a particular point in the conversation and disagree with them at a different point. Such localized phenomena are also important for understanding the broader context of that participant’s influence or role in the conversation (Bunderson, 2003). In this paper, we focus on a particular type of authority claim, namely forum claims, as defined in a companion paper (Bender et al., 2011). Forum claims are based on policy, norms, or contextual rules of behavior in the interaction. In our experiments, we explore the phenomenon using Wikipedia discussion (“talk”) pages, which are discussions associated with a Wikipedia article in which changes to the article are debated by the editors in a series of discussion threads. Examples of such forum claims are: • I do think my understanding of Wikipedia and policy is better than yours. • So it has all those things going for it, and I do think it complies with [[WP:V]] and 39 Proceedings of the Workshop on Language in Social Media (LSM 2"
W11-0706,P06-2063,0,0.0316541,"at the discussion participant level, with evidence marked at the turn level without distinguishing the different types of claims as in (Bender et al., 2011). Treating the problem of detecting forum claims as a sentence-level classification problem is similar to other natural language processing tasks, such as sentiment classification. Early work in sentiment analysis used unigram features (Pang and Lee, 2004; Pang and Lee, 2005). However, error analyses suggested that highly accurate sentiment classification requires deeper understanding of the text, or at least higher order n-gram features. Kim and Hovy (2006) used unigrams, bigrams, and trigrams for extracting the polarity of online reviews. Gilbert et al. (2009) employed weighted n-grams together with additional features to classify blog comments based on agreement polarity. We conjecture that authority claim detection will also benefit from moving beyond unigram features. The focus of the paper is on two questions in feature extraction: • Can we exploit domain knowledge to address overtraining issues in sparse data conditions? • Is parse context more effective than n-gram context? Our experiments compare the performance obtained using multiple m"
W11-0706,W08-0124,1,0.83814,"n and opinions. Automatically extracting this type of social information in language from discussions is useful for understanding group interactions and relationships. The aspect of social communication most explored so far is the detection of participant role, particularly in spoken genres such as broadcast news, broadcast conversations, and meetings. Several studies have explored different types of features (lexical, prosodic, and turn-taking) in a variety of statistical modeling frameworks (Barzilay et al., 2000; Maskey and Hirschberg, 2006; Liu, 2006; Liu and Liu, 2007; Vinciarelli, 2007; Laskowski et al., 2008; Hutchinson et al., 2010). Typically, these studies assume that a speaker inhabits a role for the duration of the discussion, so multiple turns contribute to the decision. Participant status is similar although the language of others is often more relevant than that of the participant in question. Communication of other types of social information can be more localized. For example, an attempt to establish authority frequently occurs within a single sentence or turn when entering a discussion, though authority bids may involve multiple turns when the participant is challenged. Similarly, disc"
W11-0706,N06-2021,0,0.0311191,"th others in addition to communicating information and opinions. Automatically extracting this type of social information in language from discussions is useful for understanding group interactions and relationships. The aspect of social communication most explored so far is the detection of participant role, particularly in spoken genres such as broadcast news, broadcast conversations, and meetings. Several studies have explored different types of features (lexical, prosodic, and turn-taking) in a variety of statistical modeling frameworks (Barzilay et al., 2000; Maskey and Hirschberg, 2006; Liu, 2006; Liu and Liu, 2007; Vinciarelli, 2007; Laskowski et al., 2008; Hutchinson et al., 2010). Typically, these studies assume that a speaker inhabits a role for the duration of the discussion, so multiple turns contribute to the decision. Participant status is similar although the language of others is often more relevant than that of the participant in question. Communication of other types of social information can be more localized. For example, an attempt to establish authority frequently occurs within a single sentence or turn when entering a discussion, though authority bids may involve mult"
W11-0706,P04-1035,0,0.004362,"empts to establish topic expertise in Wikipedia discussions (Marin et al., 2010). Their work used a different annotation process than that which we build on here. In particular, the annotation was performed at the discussion participant level, with evidence marked at the turn level without distinguishing the different types of claims as in (Bender et al., 2011). Treating the problem of detecting forum claims as a sentence-level classification problem is similar to other natural language processing tasks, such as sentiment classification. Early work in sentiment analysis used unigram features (Pang and Lee, 2004; Pang and Lee, 2005). However, error analyses suggested that highly accurate sentiment classification requires deeper understanding of the text, or at least higher order n-gram features. Kim and Hovy (2006) used unigrams, bigrams, and trigrams for extracting the polarity of online reviews. Gilbert et al. (2009) employed weighted n-grams together with additional features to classify blog comments based on agreement polarity. We conjecture that authority claim detection will also benefit from moving beyond unigram features. The focus of the paper is on two questions in feature extraction: • Can"
W11-0706,P05-1015,0,0.0101758,"opic expertise in Wikipedia discussions (Marin et al., 2010). Their work used a different annotation process than that which we build on here. In particular, the annotation was performed at the discussion participant level, with evidence marked at the turn level without distinguishing the different types of claims as in (Bender et al., 2011). Treating the problem of detecting forum claims as a sentence-level classification problem is similar to other natural language processing tasks, such as sentiment classification. Early work in sentiment analysis used unigram features (Pang and Lee, 2004; Pang and Lee, 2005). However, error analyses suggested that highly accurate sentiment classification requires deeper understanding of the text, or at least higher order n-gram features. Kim and Hovy (2006) used unigrams, bigrams, and trigrams for extracting the polarity of online reviews. Gilbert et al. (2009) employed weighted n-grams together with additional features to classify blog comments based on agreement polarity. We conjecture that authority claim detection will also benefit from moving beyond unigram features. The focus of the paper is on two questions in feature extraction: • Can we exploit domain kn"
W11-0706,P06-1055,0,0.00907686,"ams as lexical context, we propose using syntactic context, represented by information about the parse tree of each sentence in the data. Given the low amount of available training data, learning n-gram features we believe is likely to overtrain, due to the combinatorial explosion in the feature space. On the other hand, adding parse tree context information to each feature results in a much smaller increase in feature space, due to the smaller number of non-terminal tokens as compared to the vocabulary size. To extract such features, the data was run through a version of the Berkeley parser (Petrov et al., 2006) trained on the Wall Street Journal portion of the Penn Treebank. For each sentence, the one-best parse was used to extract the list of non-terminals above each word in the sequence. The list was then filtered to a shorter subset of non-terminal tags. The words augmented with non-terminal parse tree tags were treated as individual features and used in the usual way. We used a context of at most three non-terminal tags (i.e. the POS tag and two additional levels if present.) For simplicity, multi-word phrases from the knowledge-driven word list were either removed en42 Counts 38,384 5,935 504 9"
W11-0707,D10-1100,0,0.0289409,"arsing and semantic role labeling. Such tasks involve recognizing information that is implicit in the linguistic signal but nonetheless part of its structure. Tasks such as named-entity recognition and word sense disambiguation are also close to the linguistic structure of the signal. Authority claims and alignment moves, on the other hand, are examples of communicative moves aimed at social positioning of a discussant within a group of participants, which may be specialized dialog acts but are referred to here as “social acts.” We distinguish social acts from “social events” as described in (Agarwal and Rambow, 2010): social events correspond to types of interactions among people, whereas a social act is associated with a fine-grained social goal and reflected in the specific choices of words and orthographic or prosodic cues at the level of a turn. The primary value of this new data set is in facilitating computational modeling of a new task type, i.e. the identification of fine-grained social acts in linguistic interaction. While there has been some prior work on detecting agreements and disagree48 Proceedings of the Workshop on Language in Social Media (LSM 2011), pages 48–57, c Portland, Oregon, 23 Ju"
W11-0707,J08-4004,0,0.0817579,"tor files as well as the files output by the reconciliation process. 3.5 Annotation Quality In complicated annotation tasks, such as those conducted in this work, establishing reliable ground truth is a fundamental challenge. The most popular approach to measuring annotation quality is via the surrogate of annotation consistency. This assumes that when annotators working independently arrive at the same decisions they have correctly carried out the task specified by the annotation guidelines. Several quantitative measures of annotator consistency have been proposed and debated over the years (Artstein and Poesio, 2008). We use the well-known Cohen’s kappa coefficient κ, which accounts for uneven class priors, so one may obtain a low agreement score even when a high percentage of tokens have the same label. We also report the percentage of instances on which the annotators agreed, A, which includes agreement on the absence of a particular label. When a set of instances have been labeled by more than two annotators, we compute the average of pairwise agreement. Scores for authority claim and alignment move agreement are presented in Tables 2 and 3.2 For 2 Institutional claims are exceedingly rare in our data,"
W11-0707,P04-1085,0,0.354248,"e-grained social goal and reflected in the specific choices of words and orthographic or prosodic cues at the level of a turn. The primary value of this new data set is in facilitating computational modeling of a new task type, i.e. the identification of fine-grained social acts in linguistic interaction. While there has been some prior work on detecting agreements and disagree48 Proceedings of the Workshop on Language in Social Media (LSM 2011), pages 48–57, c Portland, Oregon, 23 June 2011. 2011 Association for Computational Linguistics ments in multiparty discussions (Hillard et al., 2003; Galley et al., 2004), which is related to detecting positive/negative alignment moves, most previous work on authority bids has involved descriptive studies, e.g. (Galegher et al., 1998). Computational modeling of these phenomena and automatic detection will help with understanding effective argumentation strategies in online discussions and automatic identification of divisive or controversial discussions and online trolls. We believe that these tasks also provide an interesting arena in which to study linguistic feature engineering and feature selection. As with tasks such as sentiment analysis, a simple “bagof"
W11-0707,N03-2012,1,0.489074,"associated with a fine-grained social goal and reflected in the specific choices of words and orthographic or prosodic cues at the level of a turn. The primary value of this new data set is in facilitating computational modeling of a new task type, i.e. the identification of fine-grained social acts in linguistic interaction. While there has been some prior work on detecting agreements and disagree48 Proceedings of the Workshop on Language in Social Media (LSM 2011), pages 48–57, c Portland, Oregon, 23 June 2011. 2011 Association for Computational Linguistics ments in multiparty discussions (Hillard et al., 2003; Galley et al., 2004), which is related to detecting positive/negative alignment moves, most previous work on authority bids has involved descriptive studies, e.g. (Galegher et al., 1998). Computational modeling of these phenomena and automatic detection will help with understanding effective argumentation strategies in online discussions and automatic identification of divisive or controversial discussions and online trolls. We believe that these tasks also provide an interesting arena in which to study linguistic feature engineering and feature selection. As with tasks such as sentiment ana"
W11-0707,W11-0706,1,0.807499,"with authority claims and external variables such as user status and v-index, on the one hand, and the interaction between authority claims and alignment moves on the other. As an example of a social medium, Wikipedia is characterized by its task-orientation and by the fact that all of the interactants’ “identity work” with respect to their identity in the medium is captured in the database. This, in turn, causes the data set to be rich in the type of social acts we are investigating. The dataset was used for research in automatic detection of forum claims, as presented in a companion paper (Marin et al., 2011). That work focused on using lexical features, filtered through word lists obtained from domain experts and through datadriven methods, and extended with parse tree information. Automatic detection of other types of authority claims and of alignment moves is left for future research. We believe that, as social acts, authority claims and alignment moves are broadly recognized communication behaviors that play an important role in human interaction across a variety of contexts. However, because Wikipedia discussions are shaped by a set of well-defined, local communication norms which are closely"
W15-3003,W11-2123,0,0.0158756,"n the WIT3 TED Chinese-English corpus (Cettolo et al., 2012), a good example of a subdomain with little available training data. We used the IWSLT dev2010 and test2010 sets (also from WIT3 ) for tuning and evaluation. The larger pool from which we selected data was constructed from an aggregation of 47 LDC Chinese-English parallel datasets.1 Table 1 contains the corpus statistics for the task and pool bilingual corpora. Corpus TED (task) LDC (pool) Sentences 145,901 6,025,295 Vocab (En) 49,323 458,570 Vocab (Zh) 64,616 714,628 Table 1: Chinese-English Parallel Data. We used the KenLM toolkit (Heafield, 2011) to build all language models used in this work (i.e., both for data selection and for the MT systems used for extrinsic evaluation). In all cases the models were 4-gram LMs. We used the Stanford part-of-speech tagger (Toutanova et al., 2003) when constructing our hybrid representations, to generate the POS tags for each of the English and Chinese sides of the corpora.2 We consider three ways of applying data selection using the standard (fully lexicalized) corpus representation and our hybrid representation. The first two use the monolingual MooreLewis method (Equation 1) to respectively comp"
W15-3003,2014.eamt-1.7,0,0.0214638,"LDC2013E125 LDC2013E132 LDC2013E83 LDC2013T03 LDC2013T05 LDC2013T07 LDC2013T11 LDC2013T16 LDC2014E08 LDC2014E111 LDC2014E50 LDC2014E69 LDC2014E99 LDC2014T04 LDC2014T11. 2 The Stanford NLP tools use the Penn tagsets, which comprise 43 tags for English and 35 for Chinese. 60 TED vocab LDC vocab Joint vocab LDC minus singletons Baseline selection vocab English 49,323 458,570 470,154 243,882 257,744 Chinese 64,616 714,628 729,283 373,381 388,927 Joint vocab Vocab with count ≥ 10 POS tags Hybrid vocab previously found that setting the threshold to 10 is slightly better than a minimum count of 20 (Axelrod, 2014), and varying the threshold further is a topic for future work; see Section 5. (most domain-like) to highest score (least domainlike). For each of those ranked pools, we consider increasingly larger subsets of the data: the best n = 50K, the best n = 100K, and so on. The largest subset we consider consists of the best n = 4M sentence pairs out of the 6M available. 4.2 4.2.1 Results Intrinsic Evaluation As noted, each of the bilingual Moore-Lewis method and our hybrid word/POS variation produces a version of the additional training pool in which sentences are ranked by relevance. We then select"
W15-3003,W13-2233,0,0.0717906,"Missing"
W15-3003,D11-1033,1,0.879115,"the undue effects of infrequent words. The proposal can be realized straightforwardly: after part-of-speech tagging the in-domain and pool corpora, we identify all words that appear infrequently in either one of the two corpora, and replace each of their word tokens with its POS tag. (1) where Hm (s) is the per-word cross entropy of s according to language model m. Lower scores for cross-entropy difference indicate more relevant sentences, i.e. those that are most like the target domain and unlike the full pool average. In bilingual settings, the bilingual Moore-Lewis criterion, introduced by Axelrod et al. (2011), combines the 59 cal machine translation as a downstream task. Relevance computation, sentence ranking and subset selection then proceed as usual according to the Moore-Lewis or bilingual Moore-Lewis criterion. As an example, consider again the phrases “an earthquake in Port-au-Prince” and “an earthquake in Kodari”, and suppose that the words an, in, and earthquake are above-threshold in frequency. Our hybrid word/POS representation for both sentences would be the same: “an earthquake in NNP”. Our approach differs from the standard data selection method most significantly in its handling of r"
W15-3003,N03-2003,1,0.666232,"Missing"
W15-3003,P10-2041,0,0.202258,"system. The catch, of course, is that any large data pool can be expected to contain sentences that are at best irrelevant to the domain, and at worst detrimental: the goals of fidelity (matching in-domain data as closely as possible) and broad coverage are often at odds (Gasc´o et al., 2012). As a result, much work has focused on fidelity. Mirkin and Besacier (2014) survey the difficulties of increasing coverage while minimizing impact on model performance. We build on the standard approach for data selection in language modeling, which uses crossentropy difference as the similarity metric (Moore and Lewis, 2010). The Moore-Lewis procedure first trains an in-domain language model (LM) on the in-domain data, and another LM on the full pool of general data. It assigns to each full-pool sentence s a cross-entropy difference score, HLMIN (s) − HLMP OOL (s), (2) 3 Our Approach: Abstracting Away Words in the Long Tail Our approach is motivated by the observation that domain mismatches can have a strong register component, and this comprises both lexical and syntactic differences. We are inspired, as well, by work in stylometry, observing that attempts to quantify differences between text datasets try to lea"
W15-3003,P02-1040,0,0.0918678,"e, particularly for Chinese. Similarly, Figure 1 shows that our hybrid method also increases more rapidly to asymptotically approach full in-domain vocabulary coverage as well. 4.2.2 Extrinsic Evaluation Improved vocabulary coverage is a positive result, but we are also interested in downstream application performance. Accordingly, we trained SMT systems using cdec (Dyer et al., 2010) on subsets of selected data. All SMT systems were tuned using MIRA (Chiang et al., 2008) on the dev2010 data from IWSLT (Federico et al., 2011), and then evaluated on the test2010 IWSLT test set using both BLEU (Papineni et al., 2002) and TER (Snover et al., 2006). To isolate the impact of the data selection method, we present results just using the selected data, without the combining with the in-domain data into a multi-model sys4.2.3 Selection Model Size The resulting translation system sizes conform with prior work: selecting smaller subsets yields smaller downstream MT systems. For example, an MT system trained on 1M selected sentences is ∼2.3GB in size, a factor of 5 smaller than the 11.3GB baseline MT system trained on all 6M sentences. In addition, we observe a healthy re62 Figure 1: Percentage of TED vocabulary co"
W15-3003,D08-1024,1,0.706303,"inearly with the amount of selection data. By contrast, our proposed method appears to asymptotically approach full in-domain vocabulary coverage, particularly for Chinese. Similarly, Figure 1 shows that our hybrid method also increases more rapidly to asymptotically approach full in-domain vocabulary coverage as well. 4.2.2 Extrinsic Evaluation Improved vocabulary coverage is a positive result, but we are also interested in downstream application performance. Accordingly, we trained SMT systems using cdec (Dyer et al., 2010) on subsets of selected data. All SMT systems were tuned using MIRA (Chiang et al., 2008) on the dev2010 data from IWSLT (Federico et al., 2011), and then evaluated on the test2010 IWSLT test set using both BLEU (Papineni et al., 2002) and TER (Snover et al., 2006). To isolate the impact of the data selection method, we present results just using the selected data, without the combining with the in-domain data into a multi-model sys4.2.3 Selection Model Size The resulting translation system sizes conform with prior work: selecting smaller subsets yields smaller downstream MT systems. For example, an MT system trained on 1M selected sentences is ∼2.3GB in size, a factor of 5 smalle"
W15-3003,2006.amta-papers.25,0,0.0215726,"ilarly, Figure 1 shows that our hybrid method also increases more rapidly to asymptotically approach full in-domain vocabulary coverage as well. 4.2.2 Extrinsic Evaluation Improved vocabulary coverage is a positive result, but we are also interested in downstream application performance. Accordingly, we trained SMT systems using cdec (Dyer et al., 2010) on subsets of selected data. All SMT systems were tuned using MIRA (Chiang et al., 2008) on the dev2010 data from IWSLT (Federico et al., 2011), and then evaluated on the test2010 IWSLT test set using both BLEU (Papineni et al., 2002) and TER (Snover et al., 2006). To isolate the impact of the data selection method, we present results just using the selected data, without the combining with the in-domain data into a multi-model sys4.2.3 Selection Model Size The resulting translation system sizes conform with prior work: selecting smaller subsets yields smaller downstream MT systems. For example, an MT system trained on 1M selected sentences is ∼2.3GB in size, a factor of 5 smaller than the 11.3GB baseline MT system trained on all 6M sentences. In addition, we observe a healthy re62 Figure 1: Percentage of TED vocabulary covered vs. number of selected s"
W15-3003,W13-2803,0,0.0193944,"cally a range of values for n is considered, selecting the n that performs best on held-out indomain data. While shown to be effective, however, wordbased scores may not capture all facets of relevance. The strategy of a hybrid word/POS representation was first explored by Bulyko et al. (2003), who used class-dependent weights for mixing multi-source language models. The classes were a combination of the 100 most frequent words and POS tags. Bisazza and Federico (2012) target in-domain coverage by using a hybrid word/POS representation to train an additional LM for decoding in an MT pipeline. Toral (2013) uses a hybrid word/class representation for data selection for language modeling; he replaces all named entities with their type (e.g. ’person’, ’organization’), and experiments with also lemmatizing the remaining words. Related Work Data selection is a widely-used variant of domain adaptation that requires quantifying the relevance to the domain of the sentences in a pooled corpus of additional data. The pool is sorted by relevance score, the highest ranked portion is kept, and the rest discarded.This process – also known as “rank-and-select” in language modeling (Sethy et al., 2009) – ident"
W15-3003,N03-1033,0,0.0161681,"from which we selected data was constructed from an aggregation of 47 LDC Chinese-English parallel datasets.1 Table 1 contains the corpus statistics for the task and pool bilingual corpora. Corpus TED (task) LDC (pool) Sentences 145,901 6,025,295 Vocab (En) 49,323 458,570 Vocab (Zh) 64,616 714,628 Table 1: Chinese-English Parallel Data. We used the KenLM toolkit (Heafield, 2011) to build all language models used in this work (i.e., both for data selection and for the MT systems used for extrinsic evaluation). In all cases the models were 4-gram LMs. We used the Stanford part-of-speech tagger (Toutanova et al., 2003) when constructing our hybrid representations, to generate the POS tags for each of the English and Chinese sides of the corpora.2 We consider three ways of applying data selection using the standard (fully lexicalized) corpus representation and our hybrid representation. The first two use the monolingual MooreLewis method (Equation 1) to respectively compute relevance scores using the English (output) side and the Chinese (input) side of the parallel corpora. The third uses bilingual Moore-Lewis (Equation 2) to compute the bilingual score over both sides. Each of these three variants produces"
W15-3003,E12-1045,0,\N,Missing
W15-3003,E12-1016,0,\N,Missing
W15-3003,P10-4002,1,\N,Missing
W15-3003,2014.amta-researchers.23,0,\N,Missing
W15-3003,2011.iwslt-evaluation.1,0,\N,Missing
W15-3003,2012.eamt-1.60,0,\N,Missing
W16-5807,D15-1041,1,0.856653,"n studied by Mandal et al. (2015) in the context of question answering and by King and Abney (2013). Both used primarily character n-gram features. Several other studies have investigated the use of character sequence models in language processing. These techniques were first applied only to create word embeddings (dos Santos and Zadrozny, 2015; dos Santos and Guimaraes, 2015) and then later extended to have the word embeddings feed directly into a word-level RNN. Applications include partof-speech (POS) tagging (Ling et al., 2015b), language modeling (Ling et al., 2015a), dependency parsing (Ballesteros et al., 2015), translation (Ling et al., 2015b), and slot filling text analysis (Jaech et al., 2016a). A more extensive discussion of related work on language ID and character sequence models can be found in Jaech et al. (2016b). 6 Conclusion We present C2V2L, a hierarchical neural model for language ID that preforms competitively on challenging word-level language ID tasks. Without feature engineering, we achieved the best performance in two common categories and good results in two others. Future work could include adapting C2V2L for other sequence labeling tasks, having shown that the current architectu"
W16-5807,W15-3904,0,0.0252191,"results.html 63 English-Spanish and English-Nepali in the EMNLP 2014 Language Identitication in Code-Switched Data (Solorio et al., 2014), using an RNN with skipgram word embeddings and character n-gram features. Word-level language ID has also been studied by Mandal et al. (2015) in the context of question answering and by King and Abney (2013). Both used primarily character n-gram features. Several other studies have investigated the use of character sequence models in language processing. These techniques were first applied only to create word embeddings (dos Santos and Zadrozny, 2015; dos Santos and Guimaraes, 2015) and then later extended to have the word embeddings feed directly into a word-level RNN. Applications include partof-speech (POS) tagging (Ling et al., 2015b), language modeling (Ling et al., 2015a), dependency parsing (Ballesteros et al., 2015), translation (Ling et al., 2015b), and slot filling text analysis (Jaech et al., 2016a). A more extensive discussion of related work on language ID and character sequence models can be found in Jaech et al. (2016b). 6 Conclusion We present C2V2L, a hierarchical neural model for language ID that preforms competitively on challenging word-level language"
W16-5807,W16-6212,1,0.914402,"in social media text where informal styles, closely related language pairs, and code-switching are common. Progress on language ID is needed especially since downstream tasks, like translation or semantic parsing, depend on it. Continuous representations for language data, which have produced new states of the art for language modeling (Mikolov et al., 2010), machine translation (Bahdanau et al., 2015), and other tasks, can be useful for language ID. For the Language Identification in Code-Switched Data shared task (LICS 2016), we submitted a hierarchical characterword model closely following Jaech et al. (2016b), focusing on word-level language ID. Our discussion of the model closely follows that paper. This model, which we call C2V2L (“character to vector to language”) is hierarchical in the sense that it explicitly builds a continuous representation for each word from its character sequence, capturing orthographic and morphology-related patterns, Model Our model has two main components, though they are trained together, end-to-end. The first, “char2vec,” applies a convolutional neural network (CNN) to a whitespace-delimited word’s Unicode character sequence, providing a word vector. The second is"
W16-5807,N13-1131,0,0.0417997,"ious work on the text domain mostly uses word or character ngram features combined with linear classifiers (Hurtado et al., 2014; Gamallo et al., 2014). Chang and Lin (2014) outperformed the top results for 2 Full results can be found at http://care4lang1. seas.gwu.edu/cs2/results.html 63 English-Spanish and English-Nepali in the EMNLP 2014 Language Identitication in Code-Switched Data (Solorio et al., 2014), using an RNN with skipgram word embeddings and character n-gram features. Word-level language ID has also been studied by Mandal et al. (2015) in the context of question answering and by King and Abney (2013). Both used primarily character n-gram features. Several other studies have investigated the use of character sequence models in language processing. These techniques were first applied only to create word embeddings (dos Santos and Zadrozny, 2015; dos Santos and Guimaraes, 2015) and then later extended to have the word embeddings feed directly into a word-level RNN. Applications include partof-speech (POS) tagging (Ling et al., 2015b), language modeling (Ling et al., 2015a), dependency parsing (Ballesteros et al., 2015), translation (Ling et al., 2015b), and slot filling text analysis (Jaech"
W16-5807,D15-1176,0,0.165722,"nsion of y is 3n2 , corresponding to the number of filters used. Similar to Kim et al. (2016) who use a highway network after the max-pooling layer, we apply a residual network layer. The residual network uses a matrix W ∈ R3n2 ×3n2 and bias vector b3 to create the vector z = y + fR (y) where fR (y) = ReLU(Wy + b3 ). The resulting vector z is used as a word embedding vector in the word-level LSTM portion of the model. There are three differences between our version of the model and the one described by Kim et al. (2016). First, we use two layers of convolution instead of just one, inspired by Ling et al. (2015a) which uses a 2-layer LSTM for character modeling. Second, we use the ReLU function as a nonlinearity as opposed to the tanh function. ReLU has been highly successful in computer vision in conjunction with convolutional layers (Jarrett et al., 2009). Finally, we use a residual network layer instead of a highway network layer after the max-pooling step, to reduce the model size. 2.2 Sentence-level Context The sequence of word embedding vectors is processed by a bi-LSTM, which outputs a sequence of 61 Figure 1: C2V2L model architecture. The model takes the word “esfuezo,” a misspelling of the"
W16-6209,D15-1085,1,0.845953,"∈ RC . is obtained as c The computation of basis coefficients is similar to the attention mechanism that has been used in the context of machine translation (Bahdanau et al., 2015), constituency parsing (Vinyals et al., 2015), question answering and language modeling (Weston et al., 2015; Sukhbaatar et al., 2015). To the best of our knowledge, this is the first attempt to use the attention mechanism for latent basis learning. Text embeddings: Recurrent neural networks (RNNs) have been widely used to obtain sequence embeddings for different applications in recent years (Sutskever et al., 2014; Cheng et al., 2015; Palangi et al., 2016). In this paper, we use a bidirectional RNN to encode each sentence, and concatenate the hidden layers at the last time step of each direction as the sentence embedding. For comments with multiple sentences, we average the sentence embeddings into a single vector as the textual content embedding d ∈ RD . For the t-th token in a sentence, the hidden layers of the bi-directional RNN are computed as (l) (l) ht = GRU(zt , ht−1 ), (r) ht (r) = GRU(zt , ht+1 ), (l) where zt ∈ RD is the token input vector, ht ∈ RD (r) and ht ∈ RD are the hidden layers for the leftto-right and r"
W16-6209,D15-1239,1,0.848649,"eaders to social media and online content has been studied using a variety of measurements, including: the volume of comments in response to blog posts (Yano and Smith, 2010) and news articles (Tasgkias et al., 2009; Tatar et al., 2011), the number of Twitter shares of news articles (Bandari et al., 2012), the number of reshares on Facebook (Cheng et al., 2014) and retweets on Twitter (Suh et al., 2010; Hong et al., 2011; Tan et al., 2014; Zhao et al., 2015), and the difference in the number of reader up and down votes on posts and comments in Reddit discussion forums (Lakkaraju et al., 2013; Jaech et al., 2015). An advantage of working with the Reddit data is that both positive and negative reactions are accounted for, so the total (karma in Reddit) is a reasonable proxy for community endorsement. For all the different types of measures, a challenge in predicting the cumulative reaction is that the cases of most interest are at the tails of a Zipfian distribution. Various prediction tasks have been proposed with this in mind, including regression on a log score (Bandari et al., 2012), classification into 3-4 groups (e.g. none, low, high) (Tasgkias et al., 2009; Hong et al., 2011; Yano and Smith, 201"
W16-6209,P14-5010,0,0.0032927,"proposed by Cho et al. (2014) as a simpler alternative to the long short-term memory unit (Hochreiter and Schmidhuber, 1997) for addressing the vanishing gradient issue in RNNs. For consistency of the model and consideration of computation speed, we replace the hyperbolic tangent function in the GRU with the LReL function. Although not shown in Fig. 3, weight matrices in the bi-directional RNN are jointly learned with all other parameters. To generate the token input vector to the RNN, we utilize the lemma and part-of-speech (POS) tag of each token (obtained with the Stanford CoreNLP toolkit (Manning et al., 2014)), in addition to its word form. A token embedding zt ∈ RD for the t-th token in a sentence is computed as zt = Eword eword + Epos epos + Elemma elemma , t t t where et ’s are one-hot encoding vectors for the token, and E’s are parameters to be learned. The dimensions of these one-hot encoding vectors are determined by the size of the corresponding vocabularies, which include all observed types except singletons. Thus, these embedding matrices E’s have the same first dimension D but different second dimensions. This type of additive token embedding has been used in (Botha and Blunsom, 2014; Fa"
W16-6209,P14-1017,0,0.332537,"e full discussion thread vs. a limited time window puts a focus on participant interaction in understanding community endorsement. 2 Related Work The cumulative response of readers to social media and online content has been studied using a variety of measurements, including: the volume of comments in response to blog posts (Yano and Smith, 2010) and news articles (Tasgkias et al., 2009; Tatar et al., 2011), the number of Twitter shares of news articles (Bandari et al., 2012), the number of reshares on Facebook (Cheng et al., 2014) and retweets on Twitter (Suh et al., 2010; Hong et al., 2011; Tan et al., 2014; Zhao et al., 2015), and the difference in the number of reader up and down votes on posts and comments in Reddit discussion forums (Lakkaraju et al., 2013; Jaech et al., 2015). An advantage of working with the Reddit data is that both positive and negative reactions are accounted for, so the total (karma in Reddit) is a reasonable proxy for community endorsement. For all the different types of measures, a challenge in predicting the cumulative reaction is that the cases of most interest are at the tails of a Zipfian distribution. Various prediction tasks have been proposed with this in mind,"
W16-6212,N10-1027,0,0.0248549,"pose a challenge to language identification. We introduce a hierarchical model that learns character and contextualized word-level representations for language identification. Our method performs well against strong baselines, and can also reveal code-switching. 1 Introduction Language identification (language ID), despite being described as a solved problem more than ten years ago (McNamee, 2005), remains a difficult problem. Particularly when working with short texts, informal styles, or closely related language pairs, it is an active area of research (Gella et al., 2014; Wang et al., 2015; Baldwin and Lui, 2010). These difficult cases are often found in social media content. Progress on language ID is needed especially since downstream tasks, like translation and semantic parsing, depend on correct language ID. This paper brings continuous representations for language data, which have produced new states of the art for language modeling (Mikolov et al., 2010), machine translation (Bahdanau et al., 2015), and other tasks, to language ID. We adapt a hierarchical character-word neural architecture from Kim et al. (2016), demonstrating that it works well for language ID. Our model, which we call C2V2L (“"
W16-6212,D15-1041,1,0.824864,"rs, published datasets quickly go out of date when the tweets in question are no longer available online, making it difficult to compare against prior work. Several other studies have investigated the use of character sequence models in language processing. These techniques were first used only to create word embeddings (dos Santos and Zadrozny, 2015; dos Santos and Guimaraes, 2015) and then later extended to have the word embeddings feed directly into a word-level RNN. Applications include partof-speech tagging (Ling et al., 2015b), language modeling (Ling et al., 2015a), dependency parsing (Ballesteros et al., 2015), translation (Ling et al., 2015b), and slot filling text analysis (Jaech et al., 2016a). The work is divided in terms of whether the character sequence is modeled with an LSTM or CNN, though virtually all now leverage the resulting word vectors in a word-level RNN. We are not aware of prior results comparing LSTMs and CNNs on a specific task, but the reduction in model size com91 Chang and Lin (2014) outperformed the top results for English-Spanish and English-Nepali in the EMNLP 2014 Language Identification in CodeSwitched Data (Solorio et al., 2014), using an RNN with skipgram word embeddin"
W16-6212,W12-2108,0,0.193237,"ds from the training data and their cosine similarities for inputs “couldn’t”, “@maria_sanchez”, and “noite”. Model langid.py 5-gram LM C2V2L (ours) F1 87.9 93.8 91.2 pared to word-only systems is reported to be much higher for LSTM architectures. All analyses report that the greatest improvements in performance from character sequence models are for infrequent and previously unseen words, as expected. Table 6: F1 scores on the Twitter70 dataset. with information from the Twitter social graph improves language ID on TweetLID from 74.7 to 76.6 F1 , only slightly better than our result of 76.2. Bergsma et al. (2012) created their own multilingual Twitter dataset and tested both a discriminative model based on n-grams plus hand-crafted features and a compression-based classifier. Since the Twitter API requires researchers to re-download tweets based on their identifiers, published datasets quickly go out of date when the tweets in question are no longer available online, making it difficult to compare against prior work. Several other studies have investigated the use of character sequence models in language processing. These techniques were first used only to create word embeddings (dos Santos and Zadroz"
W16-6212,W15-3904,0,0.0325553,"own multilingual Twitter dataset and tested both a discriminative model based on n-grams plus hand-crafted features and a compression-based classifier. Since the Twitter API requires researchers to re-download tweets based on their identifiers, published datasets quickly go out of date when the tweets in question are no longer available online, making it difficult to compare against prior work. Several other studies have investigated the use of character sequence models in language processing. These techniques were first used only to create word embeddings (dos Santos and Zadrozny, 2015; dos Santos and Guimaraes, 2015) and then later extended to have the word embeddings feed directly into a word-level RNN. Applications include partof-speech tagging (Ling et al., 2015b), language modeling (Ling et al., 2015a), dependency parsing (Ballesteros et al., 2015), translation (Ling et al., 2015b), and slot filling text analysis (Jaech et al., 2016a). The work is divided in terms of whether the character sequence is modeled with an LSTM or CNN, though virtually all now leverage the resulting word vectors in a word-level RNN. We are not aware of prior results comparing LSTMs and CNNs on a specific task, but the reduct"
W16-6212,W14-5151,0,0.015758,"s’ brevity and unconventional spelling pose a challenge to language identification. We introduce a hierarchical model that learns character and contextualized word-level representations for language identification. Our method performs well against strong baselines, and can also reveal code-switching. 1 Introduction Language identification (language ID), despite being described as a solved problem more than ten years ago (McNamee, 2005), remains a difficult problem. Particularly when working with short texts, informal styles, or closely related language pairs, it is an active area of research (Gella et al., 2014; Wang et al., 2015; Baldwin and Lui, 2010). These difficult cases are often found in social media content. Progress on language ID is needed especially since downstream tasks, like translation and semantic parsing, depend on correct language ID. This paper brings continuous representations for language data, which have produced new states of the art for language modeling (Mikolov et al., 2010), machine translation (Bahdanau et al., 2015), and other tasks, to language ID. We adapt a hierarchical character-word neural architecture from Kim et al. (2016), demonstrating that it works well for lan"
W16-6212,N16-1155,0,0.0356935,"Missing"
W16-6212,D15-1240,1,0.849284,"d in the Wikipedia data, is ignored, then there is a net performance gain. In Table 5, we show the top seven neighbors to selected input words based on cosine similarity. In the left column we see that words with similar features, such as the presence of the “n’t” contraction, can be grouped together by char2vec. In the middle column, an out-of-vocabulary username is supplied and similar usernames are retrieved. When working with n-gram features, removing usernames is common, but some previous work demonstrates that they still carry useful information for predicting the language of the tweet (Jaech and Ostendorf, 2015). The third example,“noite” (Portuguese for “night”), shows that the word embeddings are largely invariant to changes in punctuation and capitalization. 5.3 Twitter70 We compare C2V2L to langid.py and the 5gram language model on the Twitter70 dataset; see Table 6. Although the 5-gram model achieves the best performance, the results are virtually identical to those for C2V2L except for the closely-related Bosnian-Croatian language pair. The lowest performance for all the models is on closely related language pairs. For example, using the C2V2L model, the F1 score for Danish is only 62.7 due to"
W16-6212,W16-5807,1,0.837176,"able online, making it difficult to compare against prior work. Several other studies have investigated the use of character sequence models in language processing. These techniques were first used only to create word embeddings (dos Santos and Zadrozny, 2015; dos Santos and Guimaraes, 2015) and then later extended to have the word embeddings feed directly into a word-level RNN. Applications include partof-speech tagging (Ling et al., 2015b), language modeling (Ling et al., 2015a), dependency parsing (Ballesteros et al., 2015), translation (Ling et al., 2015b), and slot filling text analysis (Jaech et al., 2016a). The work is divided in terms of whether the character sequence is modeled with an LSTM or CNN, though virtually all now leverage the resulting word vectors in a word-level RNN. We are not aware of prior results comparing LSTMs and CNNs on a specific task, but the reduction in model size com91 Chang and Lin (2014) outperformed the top results for English-Spanish and English-Nepali in the EMNLP 2014 Language Identification in CodeSwitched Data (Solorio et al., 2014), using an RNN with skipgram word embeddings and character ngram features. Word-level language ID has also been studied by Manda"
W16-6212,N13-1131,0,0.0833342,"quence is modeled with an LSTM or CNN, though virtually all now leverage the resulting word vectors in a word-level RNN. We are not aware of prior results comparing LSTMs and CNNs on a specific task, but the reduction in model size com91 Chang and Lin (2014) outperformed the top results for English-Spanish and English-Nepali in the EMNLP 2014 Language Identification in CodeSwitched Data (Solorio et al., 2014), using an RNN with skipgram word embeddings and character ngram features. Word-level language ID has also been studied by Mandal et al. (2015) in the context of question answering and by King and Abney (2013). Both used primarily character n-gram features, which are well motivated for code-switching tasks since the presence of multiple languages increases the odds of encountering a previously unseen word. 7 Conclusion We present C2V2L, a hierarchical neural model for language ID that outperforms previous work on the challenging TweetLID task. We also find that smoothed character n-gram language models can work well as classifiers for language ID for short texts. Without feature engineering, our n-gram baseline beat eleven out of the twelve submissions in the TweetLID shared task, and gives the bes"
W16-6212,D15-1176,0,0.126313,"etwork layers allow values from the previous layer to pass through unchanged but the residual layer is preferred in our case because it uses half as many parameters (He et al., 2015). The residual network uses a matrix W ∈ R3n2 ×3n2 and bias vector b3 to create the vector z = y + fR (y) where fR (y) = ReLU(Wy + b3 ). The resulting vector z is used as a word embedding vector in the word-level LSTM portion of the model. There are three differences between our version of the model and the one described by Kim et al. (2016). First, we use two layers of convolution instead of just one, inspired by Ling et al. (2015a) 85 who used a 2-layer LSTM for character modeling. Second, we use the ReLU function as a nonlinearity as opposed to the tanh function. ReLU has been highly successful in computer vision applications in conjunction with convolutional layers (Jarrett et al., 2009). Finally, we use a residual network layer instead of a highway network layer after the maxpooling step, to reduce the model size. Figure 1: C2V2L model architecture. The model takes the (misspelled) word “esfuezo,” and produces a word vector via the two CNN layers and the residual layer. The word vectors are then combined via the LS"
W16-6212,P12-3005,0,0.113067,"ayers, the size of the word-level LSTM vector, and the dropout rate. The selected values are listed in Table 2. 5 Experiments For all the studies below on language identification, we compare to two baselines: i) langid.py, a popular open-source language ID package, and ii) a classifier using n-gram character language models. For the TweetLID dataset, additional comparisons are included as described next. In addition, we test our model’s word-level performance on a codeswitching dataset. The first baseline, based on the langid.py package, uses a naïve Bayes classifier over byte ngram features (Lui and Baldwin, 2012). The pretrained model distributed with the package is designed to perform well on a wide range of domains, and achieved high performance on “microblog messages” (tweets) in the original paper. langid.py uses feature selection for domain adaptation and to reduce the model size; thus, retraining it on indomain data as we do in this paper does not provide an entirely fair comparison. However, we include it for its popularity and importance. The second baseline is built from character ngram language models. It assigns each tweet according to language `∗ = arg max` p(tweet |`), i.e., applying Baye"
W16-6212,W15-1703,0,0.0134668,"ventional spelling pose a challenge to language identification. We introduce a hierarchical model that learns character and contextualized word-level representations for language identification. Our method performs well against strong baselines, and can also reveal code-switching. 1 Introduction Language identification (language ID), despite being described as a solved problem more than ten years ago (McNamee, 2005), remains a difficult problem. Particularly when working with short texts, informal styles, or closely related language pairs, it is an active area of research (Gella et al., 2014; Wang et al., 2015; Baldwin and Lui, 2010). These difficult cases are often found in social media content. Progress on language ID is needed especially since downstream tasks, like translation and semantic parsing, depend on correct language ID. This paper brings continuous representations for language data, which have produced new states of the art for language modeling (Mikolov et al., 2010), machine translation (Bahdanau et al., 2015), and other tasks, to language ID. We adapt a hierarchical character-word neural architecture from Kim et al. (2016), demonstrating that it works well for language ID. Our model"
W16-6212,N15-1109,0,\N,Missing
W17-5036,D14-1181,0,0.00452694,"Missing"
W17-5036,D16-1076,0,0.036147,"Missing"
W18-0505,W02-0109,0,0.443364,"Missing"
W18-0505,D15-1166,0,0.0363645,", 2016). Attention mechanisms were introduced to improve neural machine translation tasks (Bahdanau et al., 2014), and have also been shown to imhttp://www.corestandards.org/ 46 prove the performance of text classification (Yang et al., 2016). In machine translation, attention is computed over the source sequence when predicting the words in the target sequence. This “context” attention is based on a score computed between the target hidden state ht and a subset of the source hidden states hs . The score can be computed in several ways, of which a general form is score(ht , hs ) = hTt Wα hTs (Luong et al., 2015). Attention has also been used for a variety of other language processing tasks. In particular, for text classification, attention weights are learned that target the final classification decision. This approach is referred to as “self attention” in (Lin et al., 2017), but will be referred to here as “task attention.” The hierarchical RNN in (Yang et al., 2016) uses task attention mechanisms at both word and sentence levels. Since our work builds on this model, it is described in further detail in section 4. In addition, we propose extensions of the hierarchical RNN that leverage attention in"
W18-0505,N04-1025,0,0.178713,"Missing"
W18-0505,C10-2032,0,0.0220009,"ated text analysis systems has made it possible to leverage additional linguistic features, as well as conventional reading metrics, to estimate text complexity quantified as reading level. NLP tools can be used to extract a variety of lexical, syntactic and discourse features from text, which can then be used with traditional features as input to models for predicting reading level. Some of the models include statistical language models (Collins-Thompson and Callan, 2004), support vector machine classifiers (Schwarm and Ostendorf, 2005; Petersen and Ostendorf, 2009), and logistic regression (Feng et al., 2010). Text coherence has also been explored as a predictor of difficulty level in (Graesser et al., 2004), with an extended feature set that includes syntactic complexity and discourse in addition to coherence (Graesser et al., 2011). A study conducted in (Nelson et al., 2012) indicates that metrics that incorporate a large set of linguistic features perform better at predicting text difficulty level; the metrics were specifically tested on the Common Core Standards (CCS) texts.1 Features from second language acquisition complexity measures were used in (Vajjala and Meurers, 2012) to improve reada"
W18-0505,D14-1162,0,0.0796016,"Missing"
W18-0505,P05-1065,1,0.814949,"n the training data. 1 Introduction A typical classroom presents a diverse set of students in terms of their reading comprehension skills, particularly in the case of English language learners (ELLs). Supporting these students often requires educators to estimate accessibility of instructional texts. To address this need, several automated systems have been developed to estimate text difficulty, including readability metrics like Lexile (Stenner et al., 1988), the end-toend system TextEvaluator (Sheehan et al., 2013), and linear models (Vajjala and Meurers, 2014; Petersen and Ostendorf, 2009; Schwarm and Ostendorf, 2005). These systems leverage knowledgebased features to train regression or classification models. Most systems are trained on literary and generic texts, since analysis of text difficulty is usually tied to language teaching. Existing approaches for automated text complexity analysis pose two issues: 1) systems using knowledge based features typically work better for longer texts (Vajjala and Meurers, 2014), and 2) complex45 Proceedings of the Thirteenth Workshop on Innovative Use of NLP for Building Educational Applications, pages 45–55 c New Orleans, Louisiana, June 5, 2018. 2018 Association fo"
W18-0505,W13-1506,0,0.20975,"ge-based linear models for short texts, and have the capacity to generalize to genres not present in the training data. 1 Introduction A typical classroom presents a diverse set of students in terms of their reading comprehension skills, particularly in the case of English language learners (ELLs). Supporting these students often requires educators to estimate accessibility of instructional texts. To address this need, several automated systems have been developed to estimate text difficulty, including readability metrics like Lexile (Stenner et al., 1988), the end-toend system TextEvaluator (Sheehan et al., 2013), and linear models (Vajjala and Meurers, 2014; Petersen and Ostendorf, 2009; Schwarm and Ostendorf, 2005). These systems leverage knowledgebased features to train regression or classification models. Most systems are trained on literary and generic texts, since analysis of text difficulty is usually tied to language teaching. Existing approaches for automated text complexity analysis pose two issues: 1) systems using knowledge based features typically work better for longer texts (Vajjala and Meurers, 2014), and 2) complex45 Proceedings of the Thirteenth Workshop on Innovative Use of NLP for"
W18-0505,P15-1150,0,0.0114671,"perform better at predicting text difficulty level; the metrics were specifically tested on the Common Core Standards (CCS) texts.1 Features from second language acquisition complexity measures were used in (Vajjala and Meurers, 2012) to improve readability assessment. This 1 2.2 Text Classification with RNNs Recurrent neural networks (RNNs) are adept at learning text representations, as demonstrated by language modeling (Mikolov et al., 2010) and text classification tasks (Yogatama et al., 2017). Additional RNN structures have been proposed for improved representation, including tree LSTMs (Tai et al., 2015) and a hierarchical RNN (Yang et al., 2016). In addition, hierarchical models have been proposed to better represent document structure (Yang et al., 2016). Attention mechanisms were introduced to improve neural machine translation tasks (Bahdanau et al., 2014), and have also been shown to imhttp://www.corestandards.org/ 46 prove the performance of text classification (Yang et al., 2016). In machine translation, attention is computed over the source sequence when predicting the words in the target sequence. This “context” attention is based on a score computed between the target hidden state h"
W18-0505,W12-2019,0,0.12236,"and logistic regression (Feng et al., 2010). Text coherence has also been explored as a predictor of difficulty level in (Graesser et al., 2004), with an extended feature set that includes syntactic complexity and discourse in addition to coherence (Graesser et al., 2011). A study conducted in (Nelson et al., 2012) indicates that metrics that incorporate a large set of linguistic features perform better at predicting text difficulty level; the metrics were specifically tested on the Common Core Standards (CCS) texts.1 Features from second language acquisition complexity measures were used in (Vajjala and Meurers, 2012) to improve readability assessment. This 1 2.2 Text Classification with RNNs Recurrent neural networks (RNNs) are adept at learning text representations, as demonstrated by language modeling (Mikolov et al., 2010) and text classification tasks (Yogatama et al., 2017). Additional RNN structures have been proposed for improved representation, including tree LSTMs (Tai et al., 2015) and a hierarchical RNN (Yang et al., 2016). In addition, hierarchical models have been proposed to better represent document structure (Yang et al., 2016). Attention mechanisms were introduced to improve neural machin"
W18-0505,N16-1174,0,0.43543,"lty level; the metrics were specifically tested on the Common Core Standards (CCS) texts.1 Features from second language acquisition complexity measures were used in (Vajjala and Meurers, 2012) to improve readability assessment. This 1 2.2 Text Classification with RNNs Recurrent neural networks (RNNs) are adept at learning text representations, as demonstrated by language modeling (Mikolov et al., 2010) and text classification tasks (Yogatama et al., 2017). Additional RNN structures have been proposed for improved representation, including tree LSTMs (Tai et al., 2015) and a hierarchical RNN (Yang et al., 2016). In addition, hierarchical models have been proposed to better represent document structure (Yang et al., 2016). Attention mechanisms were introduced to improve neural machine translation tasks (Bahdanau et al., 2014), and have also been shown to imhttp://www.corestandards.org/ 46 prove the performance of text classification (Yang et al., 2016). In machine translation, attention is computed over the source sequence when predicting the words in the target sequence. This “context” attention is based on a score computed between the target hidden state ht and a subset of the source hidden states"
W19-4450,D15-1075,0,0.0373113,"netuning the model and the last layer is likely to be more tuned to the original BERT training tasks. These embeddings are then used as input to the BCA model (BERT-BCA), which is then trained on the essay scoring task. 3 Data set 1 2 Low 1,202 129 655 222 Essays 1783 1800 Avg. len 350 350 Score range 2-12 1-6 shown in Table 2. Since only the training samples are available for both sets, we report results for 5-fold cross-validation using the same splits as (Taghipour and Ng, 2016). Pretraining tasks use two data sets. The NLI task uses the Stanford natural language inference (SNLI) data set (Bowman et al., 2015). We cast our NLI task as a four-way classification task, because a subset of the data does not have gold labels. Unlabeled examples were used with an “X” label. While tuning on the main task, we found that including the fourth NLI label gave better performance on the essay scoring than not using it. The DM task is based on a collection of over 13K free books from www.smashwords.com – an online book distribution platform.4 Labeled discourse marker data was created by identifying sentence pairs that had a discourse marker at the start of the second sentence. We used 87 discourse markers, which"
W19-4450,P18-1189,0,0.0545451,"Missing"
W19-4450,N18-1202,0,0.0858161,"Missing"
W19-4450,N18-1024,0,0.170625,"ang Liu2 , and Mari Ostendorf1 1 Department of Electrical and Computer Engineering, University of Washington {farahn, ostendor}@uw.edu 2 LAIX Inc. {huy.nguyen, yang.liu}@liulishuo.com Abstract These problems (and the success of deep learning in other areas of language processing) have led to the development of neural methods for automatic essay scoring, moving away from feature engineering. A variety of studies (mostly LSTM-based) have reported AES performance comparable to or better than feature-based models (Taghipour and Ng, 2016; Cummins and Rei, 2018; Wang et al., 2018; Jin et al., 2018; Farag et al., 2018; Zhang and Litman, 2018). However, the current state-of-the-art models still use a combination of neural models and hand-crafted features (Liu et al., 2019). While vanilla RNNs, particularly LSTMs, are good at representing text sequences, essays are longer structured documents and less well suited to an RNN representation. Thus, our work looks at advancing AES by exploring other architectures that incorporate document structure for longer documents. Discourse structure and coherence are important aspects of essay writing and are often explicitly a part of grading rubrics. We explore methods t"
W19-4450,W19-4302,0,0.0466056,"Missing"
W19-4450,P09-1077,0,0.0150971,"mining features (Nguyen and Litman, 2018). In our implementation, we remove one set of basic features, e.g., word counts, spelling errors etc., since they are present in both models and keep the set from (Vajjala and Meurers, 2014). Given the extracted features, a gradient boosting algorithm is used to learn a regression model. Predicted scores are scaled and rounded to calculate Quadratic Weighted Kappa (QWK) against the true scores. These two feature sets are chosen because they incorporate discourse features in AES. In (Vajjala and Meurers, 2014), the authors used the addDiscourse toolkit (Pitler et al., 2009), which takes as input the syntactic tree of the sentence, and tags the discourse connectives, e.g., therefore, however, and their senses, e.g., CONTINGENCY.Cause, COMPARISON.Contrast. These automated annotations are then used to calculate connective based features, at 487 Model Arg (Klebanov16) Length (Klebanov16) Arg + Len (Klebanov16) Nguyen18 Feature baseline USE-HAN BERT-HAN HAN NLI-HAN DM-HAN NLI-DM-HAN BCA NLI-BCA DM-BCA NLI-DM-BCA BERT-BCA e.g., number of discourse connectives per sentence, number of each sense. In (Nguyen and Litman, 2018), an end-to-end pipeline system was built to p"
W19-4450,P17-1092,0,0.0200298,"u et al., 2019) are achieved with a model that learns the combination of hand-crafted features and the neural document representation. Thus, for tasks with limited labeled data, there is still a place for hand-crafted features. Like other neural models, our approach suffers from a lack of interpretability. While our analysis of sentence similarity with the DM-BCA model provides some useful insights into differences between high and low scoring TOEFL essays, the best scoring model did not have the same behavior. This remains an open problem. could also be explored, e.g., (Le and Mikolov, 2014; Ji and Smith, 2017; Card et al., 2018). Numerous previous studies have looked at using external data to improve performance of neural classifiers. One study that influenced our work is (Jernite et al., 2017), which showed that discoursebased tasks such as sentence order and conjunction prediction can improve neural sentence representations for several NLP tasks. This study used the Book Corpus data (Zhu et al., 2015) and the Gutenberg data (Stroube, 2003) for discoursebased tasks. Our task is similar, but we use a larger set of discourse markers. Representations from pretrained models including (Devlin et al.,"
W19-4450,P18-1100,0,0.172118,"Missing"
W19-4450,W16-2808,0,0.324532,"e systems and on validity studies (Shermis, 2014). The ability to evaluate student writing has always been important for language teaching and learning; now it also extends to science, since the focus is shifting towards assessments that can more accurately gauge construct knowledge as compared to multiple choice questions (Shermis, 2014). Most existing systems for automatic essay scoring leverage hand crafted features, ranging from wordcounts to argumentation structure and coherence, in linear regression and logistic regression models (Chodorow and Burstein, 2004; Shermis and Burstein, 2013; Klebanov et al., 2016; Nguyen and Litman, 2018). Improving feature-based models requires extensive redesigning of features (Taghipour and Ng, 2016). Due to high variability in types of student essays, feature-based systems are often individually designed for specific prompts (Burstein et al., 2013). This poses a challenge for building essay scoring systems. 484 Proceedings of the Fourteenth Workshop on Innovative Use of NLP for Building Educational Applications, pages 484–493 c Florence, Italy, August 2, 2019. 2019 Association for Computational Linguistics • Natural language inference (NLI): given a pair of senten"
W19-4450,D16-1193,0,0.235558,"ted Essay Scoring with Discourse-Aware Neural Models Farah Nadeem1 , Huy Nguyen2 , Yang Liu2 , and Mari Ostendorf1 1 Department of Electrical and Computer Engineering, University of Washington {farahn, ostendor}@uw.edu 2 LAIX Inc. {huy.nguyen, yang.liu}@liulishuo.com Abstract These problems (and the success of deep learning in other areas of language processing) have led to the development of neural methods for automatic essay scoring, moving away from feature engineering. A variety of studies (mostly LSTM-based) have reported AES performance comparable to or better than feature-based models (Taghipour and Ng, 2016; Cummins and Rei, 2018; Wang et al., 2018; Jin et al., 2018; Farag et al., 2018; Zhang and Litman, 2018). However, the current state-of-the-art models still use a combination of neural models and hand-crafted features (Liu et al., 2019). While vanilla RNNs, particularly LSTMs, are good at representing text sequences, essays are longer structured documents and less well suited to an RNN representation. Thus, our work looks at advancing AES by exploring other architectures that incorporate document structure for longer documents. Discourse structure and coherence are important aspects of essay"
W19-4450,W18-0505,1,0.924218,"d BCA models. The use of contextualized embeddings can also be thought of as pre-training with an auxiliary task of language modeling (or masked language modeling). In this work, we chose the bidirectional transformer architecture (BERT) embeddings (Devlin et al., 2018), which uses a transformer architecture trained on two tasks, masked language model and next sentence prediction. We hypothesized that the next sentence prediction would capture aspects of discourse coherence. • Hierarchical recurrent network with attention (HAN) (Yang et al., 2016) • Bidirectional context with attention (BCA) (Nadeem and Ostendorf, 2018) Both models are LSTM based. HAN captures the hierarchical structure within a document, by using two stacked layers of LSTMs. The first layer takes word embeddings as input and outputs contextualized word representations. Self attention is used to compute a sentence vector as a weighted average of the contextualized word vectors. The second LSTM takes sentence vectors as input and outputs a document vector based on averaging using self attention at the sentence level. BCA extends HAN to account for cross sentence dependencies. For each word, using the contextualized word vectors output from th"
W19-4450,D18-1090,0,0.290813,"odels Farah Nadeem1 , Huy Nguyen2 , Yang Liu2 , and Mari Ostendorf1 1 Department of Electrical and Computer Engineering, University of Washington {farahn, ostendor}@uw.edu 2 LAIX Inc. {huy.nguyen, yang.liu}@liulishuo.com Abstract These problems (and the success of deep learning in other areas of language processing) have led to the development of neural methods for automatic essay scoring, moving away from feature engineering. A variety of studies (mostly LSTM-based) have reported AES performance comparable to or better than feature-based models (Taghipour and Ng, 2016; Cummins and Rei, 2018; Wang et al., 2018; Jin et al., 2018; Farag et al., 2018; Zhang and Litman, 2018). However, the current state-of-the-art models still use a combination of neural models and hand-crafted features (Liu et al., 2019). While vanilla RNNs, particularly LSTMs, are good at representing text sequences, essays are longer structured documents and less well suited to an RNN representation. Thus, our work looks at advancing AES by exploring other architectures that incorporate document structure for longer documents. Discourse structure and coherence are important aspects of essay writing and are often explicitly a part of"
W19-4450,D14-1162,0,0.0876831,"m the first LSTM, a look-back and look-ahead context vector is computed based on the similarity with words in the previous and following sentence, respectively. The final word representation is then created as a concatenation of the LSTM output, the look-back and look-ahead context vectors, and then used to create a sentence vector using attention weights, which feeds into the second LSTM. The representation of cross-sentence dependencies makes this model discourse aware. 2.2 2.3 Training Methods All HAN models and a subset of BCA models are initialized with pretrained Glove word embeddings1 (Pennington et al., 2014). All models are trained with the essay training data. For models that are pretrained, the word-level LSTM and bidirectional context with attention (for BCA), are common for all tasks used in training. Given the word-level representations, the model computes attention weights over words for the target task (DM, NLI or essay scoring). The sentence representation is then computed by averaging the word representations using task-specific attention weights. For the pretraining tasks, the sentence representations the two sentences in the pair are concatenated, passed through a feedforward neural ne"
W19-4450,N16-1174,0,0.29595,"sentence pairs, so they impact the first-level LSTM of the HAN and BCA models. The use of contextualized embeddings can also be thought of as pre-training with an auxiliary task of language modeling (or masked language modeling). In this work, we chose the bidirectional transformer architecture (BERT) embeddings (Devlin et al., 2018), which uses a transformer architecture trained on two tasks, masked language model and next sentence prediction. We hypothesized that the next sentence prediction would capture aspects of discourse coherence. • Hierarchical recurrent network with attention (HAN) (Yang et al., 2016) • Bidirectional context with attention (BCA) (Nadeem and Ostendorf, 2018) Both models are LSTM based. HAN captures the hierarchical structure within a document, by using two stacked layers of LSTMs. The first layer takes word embeddings as input and outputs contextualized word representations. Self attention is used to compute a sentence vector as a weighted average of the contextualized word vectors. The second LSTM takes sentence vectors as input and outputs a document vector based on averaging using self attention at the sentence level. BCA extends HAN to account for cross sentence depende"
W19-4450,N10-1000,0,0.0899953,"Missing"
