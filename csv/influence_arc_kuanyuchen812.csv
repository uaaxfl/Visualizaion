2018.ijclclp-2.5,D14-1162,0,0.0845383,"Missing"
2018.ijclclp-2.5,D16-1264,0,0.0285667,"Missing"
2018.ijclclp-2.5,P18-1178,0,0.0216066,"Missing"
2019.ijclclp-2.1,Q17-1010,0,0.088614,"Missing"
2019.ijclclp-2.1,D18-1241,0,0.0559296,"Missing"
2019.ijclclp-2.1,D17-1082,0,0.0225182,"Missing"
2019.ijclclp-2.1,D14-1162,0,0.0834849,"Missing"
2019.ijclclp-2.2,P02-1058,0,0.302047,"Missing"
2019.ijclclp-2.2,W04-3252,0,0.0632045,"Missing"
2019.ijclclp-2.2,D15-1226,0,0.0498509,"Missing"
2019.ijclclp-2.2,D14-1162,0,0.0896914,"Missing"
2019.ijclclp-2.2,N18-1202,0,0.102442,"Missing"
2019.ijclclp-2.2,D08-1079,0,0.0963336,"Missing"
2019.ijclclp-2.2,K19-1074,0,0.0370284,"Missing"
2019.ijclclp-2.2,P19-1628,0,0.0293389,"Missing"
2019.rocling-1.2,K16-1028,0,0.0444707,"Missing"
2019.rocling-1.2,N16-1012,0,0.0518162,"Missing"
2019.rocling-1.2,D15-1044,0,0.125513,"Missing"
2019.rocling-1.2,P19-1628,0,0.0380618,"Missing"
2019.rocling-1.4,P18-1063,0,0.0462206,"Missing"
2019.rocling-1.4,W04-1013,0,0.0654605,"Missing"
2019.rocling-1.4,N18-1049,0,0.0510501,"Missing"
2019.rocling-1.4,N18-1158,0,0.0443353,"Missing"
2019.rocling-1.4,D18-1088,0,0.0311473,"Missing"
2020.rocling-1.16,D18-1398,0,0.0643075,"Missing"
2020.rocling-1.8,D18-1398,0,0.0578087,"Missing"
2021.rocling-1.22,D19-1410,0,0.0647684,"Missing"
2021.rocling-1.22,N19-1423,0,0.0767816,"Missing"
2021.rocling-1.22,D19-1352,0,0.0358579,"Missing"
2021.rocling-1.22,P18-5007,0,0.0430211,"Missing"
2021.rocling-1.46,2020.findings-emnlp.58,0,0.0610852,"Missing"
2021.rocling-1.46,N16-1066,0,0.0614855,"Missing"
2021.rocling-1.46,W10-0208,0,0.0499802,"成果報告。 為了分析中文文本的情緒效價(Valence) 與喚起程度(Arousal)，本研究基於當前 流行的預訓練語言模型 BERT 與近期基 於全詞遮蔽(Whole Word Masking)進行 預訓練的 MacBERT，觀察模型在不同 設定下的預測成果，並比較 BERT 與 MacBERT 在中文文本情緒預測效能的 差 異 。 我 們 發 現 ， 相 較 於 BERT， MacBERT 可以在驗證集上獲得些許的 效能提升。因此，我們將數個使用不 同訓練方法所得的預測模型進行預測 結果平均，作為最終的輸出。 關鍵字：情感分析、預訓練語言模型、BERT、 MacBERT Keywords: Sentiment Analysis, Pre-trained Language Model, BERT, MacBERT 1 緒論 (Introduction) 情緒分析已經是自然語言處理中備受矚目的 任務之一，屬於文本分類的子任務，目標在 於面對不同的文本時，能夠辨識出文本所欲 表達的各類情緒量值，比如：正面、負面、 情 緒 高 漲 、 情 緒 低 落 等(Wei et al., 2011; Malandrakis et al., 2013; Wang et al., 2016; Du and Zhang, 2016; Wu et la., 2017; Yu et al., 2020, Kim et al., 2010; Paltoglou et al, 2013; Goel et la., 2017; Zhu et al., 2019; Wang et al., 2019; 2020)。情緒 辨識可以廣泛地應用在我們的生活中，比如： 分析網路上的社群評論、售後產品的相關回 饋、客服機器人的應答等。 此次的共同任務：教育文本的維度式情 感分析，其目標在於分析出中文教育文本中 的 喚 起 程 度(Arousal)以 及 效 價 程 度(Valence) (Russell, 1980)，其中喚起程度的高低意味著 語者是興奮或是平靜，而效價程度則是代表 Abstract This technical report aims at the ROCLING 2021 Shared Task: Dimensional Sentiment Analysis for Educational Texts. In order to predict the affective states of Chinese educational texts, we present a practical framework by employing pre-trained language models, such as BERT and MacBERT. Several valuable observations and a"
2021.rocling-1.46,W17-5207,0,0.06441,"Missing"
2021.rocling-1.46,P19-1045,0,0.0204856,"模型 BERT 與近期基 於全詞遮蔽(Whole Word Masking)進行 預訓練的 MacBERT，觀察模型在不同 設定下的預測成果，並比較 BERT 與 MacBERT 在中文文本情緒預測效能的 差 異 。 我 們 發 現 ， 相 較 於 BERT， MacBERT 可以在驗證集上獲得些許的 效能提升。因此，我們將數個使用不 同訓練方法所得的預測模型進行預測 結果平均，作為最終的輸出。 關鍵字：情感分析、預訓練語言模型、BERT、 MacBERT Keywords: Sentiment Analysis, Pre-trained Language Model, BERT, MacBERT 1 緒論 (Introduction) 情緒分析已經是自然語言處理中備受矚目的 任務之一，屬於文本分類的子任務，目標在 於面對不同的文本時，能夠辨識出文本所欲 表達的各類情緒量值，比如：正面、負面、 情 緒 高 漲 、 情 緒 低 落 等(Wei et al., 2011; Malandrakis et al., 2013; Wang et al., 2016; Du and Zhang, 2016; Wu et la., 2017; Yu et al., 2020, Kim et al., 2010; Paltoglou et al, 2013; Goel et la., 2017; Zhu et al., 2019; Wang et al., 2019; 2020)。情緒 辨識可以廣泛地應用在我們的生活中，比如： 分析網路上的社群評論、售後產品的相關回 饋、客服機器人的應答等。 此次的共同任務：教育文本的維度式情 感分析，其目標在於分析出中文教育文本中 的 喚 起 程 度(Arousal)以 及 效 價 程 度(Valence) (Russell, 1980)，其中喚起程度的高低意味著 語者是興奮或是平靜，而效價程度則是代表 Abstract This technical report aims at the ROCLING 2021 Shared Task: Dimensional Sentiment Analysis for Educational Texts. In order to predict the affective states of Chinese educational texts, we present a practical framework by employing pre-trained language models, such as BERT and MacBERT. Several valuable observations and analyses can be drawn from a series of experiments. From the"
2021.rocling-1.5,N19-1423,0,0.0523954,"Missing"
2021.rocling-1.5,D19-1170,0,0.0393739,"Missing"
2021.rocling-1.5,D19-1630,0,0.0309004,"Missing"
2021.rocling-1.5,O11-1010,0,0.0572486,"Missing"
2021.rocling-1.5,P19-1334,0,0.0473568,"Missing"
2021.rocling-1.5,W13-3819,0,0.0645537,"Missing"
2021.rocling-1.5,D19-1258,0,0.0248831,"Missing"
2021.rocling-1.5,S18-2023,0,0.0652759,"Missing"
2021.rocling-1.5,D16-1264,0,0.116042,"Missing"
C16-1035,P07-1056,0,0.0477674,"7 0.819 0.852 0.835 0.833 0.852 0.862 Kitchen 0.824 0.858 0.860 0.857 0.884 0.860 0.871 0.884 0.890 Average 0.790 0.826 0.824 0.813 0.842 0.826 0.831 0.842 0.853 Table 1: Experimental results on sentiment analysis achieved by the proposed EV model and other baseline features, including unigrams, bigrams, PCA, and the combinations. 4 4.1 Experimental Setup & Results Experiments on the EV Model for Sentiment Analysis At the outset, we evaluate the proposed EV model on the sentiment polarity classification task. Four widely-used benchmark multi-domain sentiment datasets are used in this study 1 (Blitzer et al., 2007). They are product reviews taken from Amazon.com in four different domains: Books, DVD, Electronics, and Kitchen. Each of the reviews, ranging from Star-1 to Star-5, were rated by a customer. The reviews with Star-1 and Star-2 were labelled as Negative, and those with Star-4 and Star-5 were labeled as Positive. Each of the four datasets contains 1,000 positive reviews, 1,000 negative reviews, and a number of unlabeled reviews. Labeled reviews in each domain are randomly split up into ten folds (with nine folds serving as the training set and the remaining one as the test set). All of the follo"
C16-1035,P15-2136,0,0.026887,"r, we leverage a density peaks clustering summarization method (Rodriguez and Laio, 2014; Zhang et al., 2015), which can take both relevance and redundancy information into account at the same time. That is, a concise summary for a given document set can be automatically generated through a one-pass process instead of an iterative process. Recently, the summarization method has proven its empirical effectiveness (Zhang et al., 2015). For evaluation, we adopt the widely-used automatic evaluation metric ROUGE (Lin, 2003), and take ROUGE-1 and ROUGE-2 (in F-scores) as the main measures following Cao et al., (2015). We compare the proposed EV model with two baseline systems (the vector space model (VSM) (Gong and Liu, 2001) and the LexRank (Erkan and Radev, 2004) method), the best peer systems (including Peer T, Peer 26, and Peer 65) participating DUC evaluations, and the recently elaborated DNN-based systems (including CNN and PriorSum) (Cao et al., 2015). Owing to the space limitation, we omit the detailed introduction to these summarization methods; interested readers may refer to Penn and Zhu (2008), Liu and Hakkani-Tur (2011), Nenkova and McKeown (2011), and Cao et al., (2015) for more in-depth ela"
C16-1035,W14-1504,0,0.0251256,"ph (or sentence and document) by simply taking an average over the word embeddings corresponding to the words occurring in the paragraph. By doing so, this thread of methods has recently This work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details: http://creativecommons.org/licenses/by/4.0/ 358 Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 358–368, Osaka, Japan, December 11-17 2016. enjoyed substantial success in many NLP-related tasks (Collobert and Weston, 2008; Tang et al., 2014; Kageback et al., 2014). Although the empirical effectiveness of word embedding methods has been proven recently, the composite representation for a paragraph (or sentence and document) is a bit queer. Theoretically, paragraph-based representation learning is expected to be more suitable for such tasks as information retrieval, sentiment analysis and document summarization (Huang et al., 2013; Le and Mikolov, 2014; Palangi et al., 2015), to name but a few. However, to the best of our knowledge, unsupervised paragraph embedding has been largely under-explored on these tasks. Classic paragraph embedding methods infer"
C16-1035,N10-1134,0,0.0322079,"rpus/MATBN-corpus.htm https://catalog.ldc.upenn.edu/LDC2011T13 365 deficiency of the EV model in spoken document summarization; we thus believe that it is more suitable for use in spoken content processing. In the last set of experiments, we compare the results mentioned above with that of several wellpracticed, state-of-the-art unsupervised summarization methods, including the graph-based methods (i.e., the Markov random walk (MRW) method (Wan and Yang, 2008) and the LexRank method (Erkan and Radev, 2004)) and the combinatorial optimization methods (i.e., the submodularity-based (SM) method (Lin and Bilmes, 2010) and the integer linear programming (ILP) method (Riedhammer et al., 2010)). Among them, the ability of reducing redundant information has been aptly incorporated into the submodular-based method and the ILP method. Interested readers may refer to Penn and Zhu (2008), Liu and Hakkani-Tur (2011), and Nenkova and McKeown (2011) for comprehensive reviews and new insights into the major methods that have been developed and applied with good success to a wide range of spoken document summarization tasks. The results are also listed in Table 3. Several noteworthy observations can be drawn from the r"
C16-1035,P08-1054,0,0.142534,"on metric ROUGE (Lin, 2003), and take ROUGE-1 and ROUGE-2 (in F-scores) as the main measures following Cao et al., (2015). We compare the proposed EV model with two baseline systems (the vector space model (VSM) (Gong and Liu, 2001) and the LexRank (Erkan and Radev, 2004) method), the best peer systems (including Peer T, Peer 26, and Peer 65) participating DUC evaluations, and the recently elaborated DNN-based systems (including CNN and PriorSum) (Cao et al., 2015). Owing to the space limitation, we omit the detailed introduction to these summarization methods; interested readers may refer to Penn and Zhu (2008), Liu and Hakkani-Tur (2011), Nenkova and McKeown (2011), and Cao et al., (2015) for more in-depth elaboration. It is worthy to note that the proposed EV model, the two baseline systems, and the best peer systems are unsupervised methods, while the DNN-based systems are supervised ones. The experimental results are listed in Table 2. Several interesting observations can be concluded from the results. First, the proposed EV model outperforms VSM by a large margin in all cases, and performs comparably to other well-designed unsupervised summarization methods. Second, both LexRank and EV (with th"
C16-1035,P14-1146,0,0.0395017,"ent a given paragraph (or sentence and document) by simply taking an average over the word embeddings corresponding to the words occurring in the paragraph. By doing so, this thread of methods has recently This work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details: http://creativecommons.org/licenses/by/4.0/ 358 Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 358–368, Osaka, Japan, December 11-17 2016. enjoyed substantial success in many NLP-related tasks (Collobert and Weston, 2008; Tang et al., 2014; Kageback et al., 2014). Although the empirical effectiveness of word embedding methods has been proven recently, the composite representation for a paragraph (or sentence and document) is a bit queer. Theoretically, paragraph-based representation learning is expected to be more suitable for such tasks as information retrieval, sentiment analysis and document summarization (Huang et al., 2013; Le and Mikolov, 2014; Palangi et al., 2015), to name but a few. However, to the best of our knowledge, unsupervised paragraph embedding has been largely under-explored on these tasks. Classic paragraph"
C16-1035,O05-3004,1,0.749163,"Missing"
C16-1035,N15-1136,0,0.0213232,"and were grouped into various thematic clusters. The summary length was limited to 100 words for both DUC 2001 and DUC 2002, and 665 bytes for DUC 2004. The general background information was inferred from the LDC Gigaword corpus 3 (including Associated Press Worldstream (AP), New York Times Newswire Service (NYT), and Xinhua News Agency (XIN)). The most common belief in the document summarization community is that relevance and redundancy are two key factors for generating a concise summary. In this paper, we leverage a density peaks clustering summarization method (Rodriguez and Laio, 2014; Zhang et al., 2015), which can take both relevance and redundancy information into account at the same time. That is, a concise summary for a given document set can be automatically generated through a one-pass process instead of an iterative process. Recently, the summarization method has proven its empirical effectiveness (Zhang et al., 2015). For evaluation, we adopt the widely-used automatic evaluation metric ROUGE (Lin, 2003), and take ROUGE-1 and ROUGE-2 (in F-scores) as the main measures following Cao et al., (2015). We compare the proposed EV model with two baseline systems (the vector space model (VSM)"
D14-1156,H91-1057,0,0.152474,"Missing"
D14-1156,P07-1085,0,0.0519061,"Missing"
D14-1156,P11-5003,0,0.0325849,"aneous (or in-domain) corpus. Finally, the enhanced query model (that is P(w|H) in speech recognition) can be estimated by RM, SMM, RSMM or QMM, and further combined with the background n-gram (e.g., trigram) language model to form an adaptive language model to guide the speech recognition process. 4.2 Speech Summarization On the other hand, extractive speech summarization aims at producing a concise summary by selecting salient sentences or paragraphs from the original spoken document according to a predefined target summarization ratio (Carbonell and Goldstein, 1998; Mani and Maybury, 1999; Nenkova and McKeown, 2011; Liu and Hakkani-Tur, 2011). Intuitively, this task could be framed as an ad-hoc IR problem, where the spoken document is treated as an information need and each sentence of the document is regarded as a candidate information unit to be retrieved according to its relevance to the information need. Therefore, KLM can be used to quantify how close the document D and one of its sentences S are: the closer the sentence model P(w|S) to the document model P(w|D), the more PRM ( w |Q )   D r DTop P( w |Dr )  DrDTop P(Q |Dr ) P( Dr )   L    P ( w |D r ) P ( Dr |Q )    wV  Dr D"
D14-1156,W01-0100,0,\N,Missing
I13-1158,P11-1015,0,0.0545216,"d   P   Pw | nw,d d , (7) wd where Pw |     exp  T w  bw , T  exp  w'  bw' w'V   wd (9) The integral over  d in Eq. (9) is intractable. To simplify the estimation, we assume that the posterior distribution is highly peaked around the MAP estimate of  d . By adding regularization terms for R and  d and taking the logarithm, the parameters of LBDM are approximately estimated by Rˆ , bˆ  arg max    nw, d log P w |ˆd ; R, b d  D wd  R, b 2 2   R   ˆd ,  Log-Bilinear Document Modeling Log-Bilinear document modeling (LBDM) (Maas and Ng, 2010; Maas et al., 2011) can be considered as a relaxed version of LTM. LBDM attempts to learn the word representation with a semantic space and use training documents to constrain those semantically similar words to be represented in near vicinity. The major difference of LBDM and LTM is that LBDM aims to directly parameterize the model for capturing word representations, while LTM focuses on estimating a set of latent topics (Maas and Ng, 2010). d D  (10) where ˆd denotes the MAP estimate of  d for each document d  D . Since the objective function in Eq. (10) is not convex, a coordinate ascent process is perfo"
O09-1014,J92-4003,0,0.29066,"Missing"
O09-1014,W97-0309,0,0.0819624,"Missing"
O11-1001,J05-1003,0,0.0675241,"Missing"
O11-1001,O10-1003,1,0.552822,"Missing"
O11-1001,W02-1001,0,0.23192,"Missing"
O11-1001,N03-1028,0,0.168417,"Missing"
O11-1001,P03-1021,0,0.01456,"Missing"
O11-1001,O10-1005,1,0.807216,"Missing"
O11-1001,O05-3004,1,0.818006,"Missing"
O11-1001,N04-1023,0,0.0756256,"Missing"
O13-1001,W97-0703,0,0.473494,"Missing"
O13-1001,P04-1085,0,0.0956261,"Missing"
O13-1001,P08-1054,0,0.0388536,"Missing"
O13-1001,N07-2054,0,0.0562749,"Missing"
O14-1002,N10-1134,0,0.08846,"Missing"
O14-1002,P11-5003,0,0.0392463,"Missing"
O14-1002,P08-1054,0,0.0612,"Missing"
O14-1002,C10-1111,0,0.0751714,"Missing"
O14-1002,N07-2054,0,0.0711078,"Missing"
O15-1001,O05-3004,1,0.74818,"Missing"
O15-1002,C88-1071,0,0.111829,"Missing"
O15-1002,O05-3004,1,0.783531,"Missing"
O15-1008,C10-2032,0,0.332579,"Missing"
O15-1008,Q13-1028,0,0.05379,"Missing"
O15-3004,mochizuki-okumura-2000-comparison,0,0.184053,"Missing"
O15-3004,W04-1013,0,0.0235877,"Missing"
O15-3004,W97-0707,0,0.294225,"Missing"
O15-3004,O05-3004,1,0.78245,"Missing"
O15-3005,O11-2001,1,0.521578,"Missing"
O15-3005,O09-1004,0,0.0128541,"Missing"
O16-1012,P11-5003,0,0.123639,"Missing"
O16-1012,D15-1229,0,0.0740231,"Missing"
O16-1012,D15-1044,0,0.0889799,"Missing"
O16-1012,K16-1028,0,0.0360188,"Missing"
O16-1012,D14-1179,0,0.0116194,"Missing"
O16-1012,D15-1166,0,0.0695141,"Missing"
O16-3004,I08-2116,0,0.0837004,"Missing"
O16-3006,O05-3004,1,0.749315,"Missing"
O17-2001,P04-1085,0,0.107934,"Missing"
O17-2001,N10-1134,0,0.105237,"Missing"
W13-4414,O11-1001,1,0.842514,"n such a document are assumed to be independent of each other (the so-called “bag-of-words” assumption). When we calculate the conditional probability P( wL |W1L 1 ) , we can linearly combine the associated WTM models of the words occurring in W1L 1 to form a composite WTM model for predicting wL : PWTM( wL |W1L 1 ) In addition to topic models, many other language modeling techniques have been proposed to complement the n-gram model in different ways, such as recurrent neural network language modeling (RNNLM) (Tomáš et al., 2010), discriminative language modeling (DLM) (Roark et al., 2007; Lai et al., 2011; Chen et al., 2012), and relevance modeling (RM) (Lavrenko and Croft, 2001; Chen and Chen, 2011; Chen and Chen, 2013). RNNLM tries to project W1L 1 and wL into a continuous space, and estimate the conditional probability in a recursive way by incorporating the full information about W1L 1 . DLM takes an objective function corresponding to minimizing the word error rate for speech recognition or maximizing the ROUGE score for summarization as a holy grail and updates the language model parameters to achieve the goal. RM assumes that each word sequence W1L is associated with a relevance class"
W13-4414,P10-1009,0,0.0130967,"fly review the n-gram and topic language models. Section 3 details our proposed CSC system. A series of experiments are presented in Section 4. Finally, conclusions and future work are given in Section 5. 2 2.1 Language Modeling N-gram Language Modeling From the early 20th century, statistical language modeling has been successfully applied to various applications related to natural language processing (NLP), such as speech recognition (Chen and Goodman, 1999; Chen and Chen, 2011), information retrieval (Ponte and Croft, 1998; Lavrenko and Croft, 2001; Lavrenko, 2009), document summarization (Lin and Chen, 2010), and spelling correction (Chen et al., 2009; Liu et al., 2011; Wu et al., 2010). The most widely-used and well-practiced language model, by far, is the n-gram language model (Jelinek, 1999), because of its simplicity and fair predictive power. Quantifying the quality of a word string in a natural language is the most commonly executed task. Take the tri-gram model for example, when given a word string W1L  w1 , w2 ,, wL , the probability of the word string is approximated by the Since most Chinese characters have other characters similar to them in either shape or pronunciation, an intuitiv"
W13-4414,W10-4107,0,0.69511,"ing the long-span semantic information for language modeling for CSC. Moreover, we make a step forward to incorporate a search engine to provide extra information from the Web resources to make a more robust system. Introduction Chinese is a tonal syllabic and character (symbol) language, in which each character is pronounced as a tonal syllable. A Chinese “word” usually comprises two or more characters. The difficulty of Chinese processing is that many Chinese characters have similar shapes or similar (or same) pronunciations. Some characters are even similar in both shape and pronunciation (Wu et al., 2010; Liu et al., 2011). However, the meanings of these characters (or words composed of the characters) may be widely divergent. Due to this reason, all the students in elementary school in Taiwan or the foreign Chinese learners need to practice to identify and correct “erroneous words” in a Chinese sentence, which is called the Incorrect Character Correction (ICC) test. In fact, the ICC test is not a simple task even for some adult native speakers in Taiwan. The rest of this paper is organized as follows. In Section 2, we briefly review the n-gram and topic language models. Section 3 details our"
W13-4414,W03-1726,0,0.102448,"Missing"
W13-4414,O09-2007,0,0.0175181,"s. Section 3 details our proposed CSC system. A series of experiments are presented in Section 4. Finally, conclusions and future work are given in Section 5. 2 2.1 Language Modeling N-gram Language Modeling From the early 20th century, statistical language modeling has been successfully applied to various applications related to natural language processing (NLP), such as speech recognition (Chen and Goodman, 1999; Chen and Chen, 2011), information retrieval (Ponte and Croft, 1998; Lavrenko and Croft, 2001; Lavrenko, 2009), document summarization (Lin and Chen, 2010), and spelling correction (Chen et al., 2009; Liu et al., 2011; Wu et al., 2010). The most widely-used and well-practiced language model, by far, is the n-gram language model (Jelinek, 1999), because of its simplicity and fair predictive power. Quantifying the quality of a word string in a natural language is the most commonly executed task. Take the tri-gram model for example, when given a word string W1L  w1 , w2 ,, wL , the probability of the word string is approximated by the Since most Chinese characters have other characters similar to them in either shape or pronunciation, an intuitive idea for CSC is to construct a confusion s"
