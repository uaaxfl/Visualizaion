2008.amta-papers.13,J07-2003,0,0.0414222,"oes not depend heavily on such overlap? Answering these questions will make it possible to characterize the utility of paraphrase-based optimization in real-world scenarios, and and how best to leverage it in those scenarios where it does prove useful. 3 Research Questions Paraphrasing Model We generate sentence-level paraphrases via Englishto-English translation using phrase table pivoting, following (Madnani et al., 2007). The translation system we use (for both paraphrase generation and translation) is based on a state-of-the-art hierarchical phrase-based translation model as described in (Chiang, 2007). English-to-English hierarchical phrases are induced using the pivot-based technique proposed in (Bannard and Callison-Burch, 2005) with primary features similar to those used by (Madnani et al., 2007): the joint probability p(e¯1 , e¯2 ), the two conditionals p(e¯1 |e¯2 ) & p(e¯2 |e¯1 ) and the target length. To limit noise during pivoting, we only keep the top 20 paraphrase pairs resulting from each pivot, as determined by the induced fractional counts. Furthermore, we pre-process the source to identify all named entities using BBN IdentiFinder (Bikel et al., 1999) and strongly bias our dec"
2008.amta-papers.13,C04-1051,0,0.191571,"Missing"
2008.amta-papers.13,W04-3250,0,0.0429334,"he paraphraser only on a subset—1 million sentences—instead of the full set. • We use a 1-3 split of the 4 reference translations from the NIST MT02 test set to tune the feature weights for the paraphraser similar to Madnani et al. (2007). • No changes are made to the number of references in any validation set. Only the tuning sets differed in the number of references across different experiments. • BLEU and TER are calculated on lowercased translation output. Brevity penalties for BLEU are indicated if not equal to 1. • For each experiment, BLEU scores shown in bold are significantly better (Koehn, 2004) than the appropriate baselines for that experiment (p &lt; 0.05). 4.1 Table 1: BLEU and TER scores are shown for MT04+05. 1H=Tuning with 1 human reference, 1H+1P=Tuning with the human reference and its paraphrase. Lower TER scores are better. BLEU TER 1H 37.65 56.39 1H+1P 39.32 54.39 Single Reference Datasets In this section, we attempt to gauge the utility of the paraphrase approach in a realistic scenario where only a single reference translation is available for the tuning set. We use the NIST MT03 data, which has four references per development item, to simulate a tuning set in which only a"
2008.amta-papers.13,A00-2023,0,0.0610845,"Missing"
2008.amta-papers.13,W07-0716,1,0.921324,"a for MT research, requires undertaking an elaborate process that involves translation agencies, detailed translation guidelines, and quality control processes (Strassel et al., 2006). Most state-of-the-art statistical machine translation systems use log-linear models, which are defined in terms of hypothesis features and weights for those features. It is standard to tune the feature weights in order to maximize a translation quality metric, using heldout test sentences and their corresponding reference translations. However, obtaining reference translations is expensive. In our earlier work (Madnani et al., 2007), we introduced a new full-sentence paraphrase technique, based on English-to-English decoding with an MT system, and demonstrated that the resulting paraphrases can be used to cut the number of human reference translations needed in half. In this paper, we take the idea a step further, asking how far it is possible to get with just a single good reference translation for each item in the development set. Our analysis suggests that it is necessary to invest in four or more human translations in order to significantly improve on a single translation augmented by monolingual paraphrases. 1 In ou"
2008.amta-papers.13,P08-1023,0,0.0319501,"Missing"
2008.amta-papers.13,P03-1021,0,0.0175466,"n Parameter Optimization Nitin Madnani§ , Philip Resnik§ , Bonnie J. Dorr§ & Richard Schwartz† § Laboratory for Computational Linguistics and Information Processing § Institute for Advanced Computer Studies § University of Maryland, College Park † BBN Technologies {nmadnani,resnik,bonnie}@umiacs.umd.edu Abstract It is standard practice to tune the feature weights in models of this kind in order to maximize a translation quality metric such as BLEU (Papineni et al., 2002) or TER (Snover et al., 2006), using heldout “development” sentences paired with their corresponding reference translations. Och (2003) showed that system achieves its best performance when the model parameters are tuned using the same objective function being used for evaluating the system. However, this reliance on multiple reference translations creates a problem, because reference translations are labor intensive and expensive to obtain. For example, producing reference translations at the Linguistic Data Consortium, a common source of translated data for MT research, requires undertaking an elaborate process that involves translation agencies, detailed translation guidelines, and quality control processes (Strassel et al"
2008.amta-papers.13,P02-1040,0,0.0870838,"ference, Hawaii, 21-25 October 2008] Are Multiple Reference Translations Necessary? Investigating the Value of Paraphrased Reference Translations in Parameter Optimization Nitin Madnani§ , Philip Resnik§ , Bonnie J. Dorr§ & Richard Schwartz† § Laboratory for Computational Linguistics and Information Processing § Institute for Advanced Computer Studies § University of Maryland, College Park † BBN Technologies {nmadnani,resnik,bonnie}@umiacs.umd.edu Abstract It is standard practice to tune the feature weights in models of this kind in order to maximize a translation quality metric such as BLEU (Papineni et al., 2002) or TER (Snover et al., 2006), using heldout “development” sentences paired with their corresponding reference translations. Och (2003) showed that system achieves its best performance when the model parameters are tuned using the same objective function being used for evaluating the system. However, this reliance on multiple reference translations creates a problem, because reference translations are labor intensive and expensive to obtain. For example, producing reference translations at the Linguistic Data Consortium, a common source of translated data for MT research, requires undertaking"
2008.amta-papers.13,2006.amta-papers.25,1,0.830776,"2008] Are Multiple Reference Translations Necessary? Investigating the Value of Paraphrased Reference Translations in Parameter Optimization Nitin Madnani§ , Philip Resnik§ , Bonnie J. Dorr§ & Richard Schwartz† § Laboratory for Computational Linguistics and Information Processing § Institute for Advanced Computer Studies § University of Maryland, College Park † BBN Technologies {nmadnani,resnik,bonnie}@umiacs.umd.edu Abstract It is standard practice to tune the feature weights in models of this kind in order to maximize a translation quality metric such as BLEU (Papineni et al., 2002) or TER (Snover et al., 2006), using heldout “development” sentences paired with their corresponding reference translations. Och (2003) showed that system achieves its best performance when the model parameters are tuned using the same objective function being used for evaluating the system. However, this reliance on multiple reference translations creates a problem, because reference translations are labor intensive and expensive to obtain. For example, producing reference translations at the Linguistic Data Consortium, a common source of translated data for MT research, requires undertaking an elaborate process that inv"
2008.amta-papers.13,strassel-etal-2006-integrated,0,0.0152588,"ns. Och (2003) showed that system achieves its best performance when the model parameters are tuned using the same objective function being used for evaluating the system. However, this reliance on multiple reference translations creates a problem, because reference translations are labor intensive and expensive to obtain. For example, producing reference translations at the Linguistic Data Consortium, a common source of translated data for MT research, requires undertaking an elaborate process that involves translation agencies, detailed translation guidelines, and quality control processes (Strassel et al., 2006). Most state-of-the-art statistical machine translation systems use log-linear models, which are defined in terms of hypothesis features and weights for those features. It is standard to tune the feature weights in order to maximize a translation quality metric, using heldout test sentences and their corresponding reference translations. However, obtaining reference translations is expensive. In our earlier work (Madnani et al., 2007), we introduced a new full-sentence paraphrase technique, based on English-to-English decoding with an MT system, and demonstrated that the resulting paraphrases"
2011.mtsummit-papers.41,N06-1013,0,0.0167461,"and target words in the parallel data with their class names Approved for Public Release, Distribution Unlimited. – June, 2011 356 (michael) (mike) (daniel) (mike) 3) Train class alignments (in the same way as training word alignments in Step 1) 4) Replace the class names with the corresponding source and target words to get a new set of word alignments (denoted as “class-derived” word alignments) 5) Use both the regular and the classderived word alignments in MT training and decoding In Step 5), there are different ways to combine the two sets of alignments, such as the entropy approach in (Ayan and Dorr, 2006) and probability interpolating approach in (Wang et al., 2006). We concatenated the two sets of alignments together for the rule extraction – the same way as used in (Xu and Rosti, 2010). GIZA++ also uses word classes inside its training by introducing word class dependencies in the Model4, Model5 and HMM training. Following (Och and Ney, 2003), we used the GIZA++ word clustering tool – “_mkcls” – to generate 50 source and 50 target word classes for the word alignment training. As described above, the way that we use word classes is outside the GIZA++ training. We verified that the gains from"
2011.mtsummit-papers.41,P95-1037,0,0.0277522,"elf yourself themgawker.com selves myself itself Table 2. Comparison of 9 of the 50K English word classes generated by the hirarchical clustering and the WSB clustering algorithms Approved for Public Release, Distribution Unlimited. – June, 2011 355 To compare with other clustering algorithms, we tried the GIZA++ word clustering tool, but it could not handle the 5B word corpus (mainly due to speed). Another state-of-the-art algorithm is the hierarchical word clustering algorithm reported in (Miller et al., 2004), which is an integration of the clustering methods from (Brown et al., 1990) and (Magerman, 1995). Miller et al. (2004) showed promising results when they used it in the name tagging application. We denote this algorithm as “hierarchical clustering”. We also ran this clustering on the 5B corpus to generate different numbers of classes. We judged that the WSB clustering produced better classes, especially in terms of semantics. Table 2 shows nine classes from the “hierarchical” and “WSB” clustering that include words, “obama”, “michael”, “massachusetts”, “lawyer”, “christianity”, “announce”, “transfer”, “sick” and “himself”. As can be seen, the WSB clustering performs better in clustering"
2011.mtsummit-papers.41,C04-1036,0,0.0244621,"ocus in the future. 3 A novel word clustering algorithm For MT, source and target words that have the same meaning should be aligned together. So it is better to cluster words semantically if word class alignments are used to improve word alignments. Ker and Zhang (1997) used manmade thesauri in their work to help aligning words. Thesauri are often unavailable in lowresource situations. Hence, to achieve our goal we needed a semantic clustering algorithm, which clusters synonyms together. A number of semantic clustering algorithms have been reported, such as those in (Bellegarda et al., 1996, Geffet and Dagan, 2004, Ichioka and Fukmoto, 2008, Lee, 1999, Lin, 1998, Muller et al., 2006, Sinha and Mihalcea, 2007, Weeds and Weir, 2005). But we found the word similarity measure reported in (Lin, 1998) was more attractive, because we could easily generalize it to develop a word clustering algorithm that can be used on any language. In the paper the mutual information between two words w1 and w2 is defined as, I ( w1 , r , w2 ) = log Cnt ( w1 , r , w2 ) ⋅ Cnt (∗, r ,∗) Cnt ( w1 , r ,∗) ⋅ Cnt (∗, r , w2 ) where “r” represents the grammatical relationship of w1 and w2 , such as “is subject of”, “is object of” an"
2011.mtsummit-papers.41,2009.mtsummit-papers.4,0,0.0195695,"n to begin with such a situation was that we would like to exRelated work Maxwell and Hughes (2006) proposed two strategies for low-resource languages: (I) transfer relevant linguistic information from existing tools and resources from resource-rich languages to a lower-resource language, and (II) develop methods that require less data. Strategy (I) is often used between two closely related languages. For example, the bootstrapping of an UrduEnglish MT with a Hindi-English system in (Sinha, 2009) and generating rules for the Yiddish-English MT from the German-English and Hebrew-English MT in (Genzel et al., 2009). There are more efforts reported on exploring strategy (II), such as the work in (Al-Onaizan et al., 2000, Nießen and Ney, 2004, Roy and Popwich, 2010, Wang et al., 2006, Homola and Kubon, 2005, Xiang et al., 2010). All those methods are language-specific, because they either need to find the relevant resource-rich language or use language-specific features. Baker et al. (2010) showed more general ways to incorporate syntactic and semantic knowledge into the UrduEnglish MT systems, but the knowledge was obtained through parsing both the source and target languages. Parsing tools are often una"
2011.mtsummit-papers.41,J97-2004,0,0.54343,"cal evidence. With improper word alignments unreliable translation rules are extracted. In this paper we focus on improving word alignment quality. The statistical evidence for word classes, in general are more adequate, so alignments estimated for word classes would be relatively more reliable. Our strategy was to use word class alignments to improve word alignments. Some work has been reported that improved MT with word classes, such as generating syntactic and semantic features for the SMT decoding in (Baker et al. 2010), broadening the coverage of word alignments with class alignments in (Ker and Zhang, 1997) and improving word alignments with cross-lingual word similarity in (Wang and Chou, 2004). All those methods are more or less language-specific, and could not easily be applied to other languages. Our major contribution here is the development of a novel semantic word clustering algorithm that can be easily applied to any language. Another contribution is the design of a new procedure for using word classes to improve word alignment quality. Our results showed that the new algorithm outperforms a start-of-the-art hierarchical clustering approach when used to improve word alignment quality. We"
2011.mtsummit-papers.41,P99-1004,0,0.0981579,"ithm For MT, source and target words that have the same meaning should be aligned together. So it is better to cluster words semantically if word class alignments are used to improve word alignments. Ker and Zhang (1997) used manmade thesauri in their work to help aligning words. Thesauri are often unavailable in lowresource situations. Hence, to achieve our goal we needed a semantic clustering algorithm, which clusters synonyms together. A number of semantic clustering algorithms have been reported, such as those in (Bellegarda et al., 1996, Geffet and Dagan, 2004, Ichioka and Fukmoto, 2008, Lee, 1999, Lin, 1998, Muller et al., 2006, Sinha and Mihalcea, 2007, Weeds and Weir, 2005). But we found the word similarity measure reported in (Lin, 1998) was more attractive, because we could easily generalize it to develop a word clustering algorithm that can be used on any language. In the paper the mutual information between two words w1 and w2 is defined as, I ( w1 , r , w2 ) = log Cnt ( w1 , r , w2 ) ⋅ Cnt (∗, r ,∗) Cnt ( w1 , r ,∗) ⋅ Cnt (∗, r , w2 ) where “r” represents the grammatical relationship of w1 and w2 , such as “is subject of”, “is object of” and Cnt (⋅) denotes the count. The simil"
2011.mtsummit-papers.41,P98-2127,0,0.205614,", source and target words that have the same meaning should be aligned together. So it is better to cluster words semantically if word class alignments are used to improve word alignments. Ker and Zhang (1997) used manmade thesauri in their work to help aligning words. Thesauri are often unavailable in lowresource situations. Hence, to achieve our goal we needed a semantic clustering algorithm, which clusters synonyms together. A number of semantic clustering algorithms have been reported, such as those in (Bellegarda et al., 1996, Geffet and Dagan, 2004, Ichioka and Fukmoto, 2008, Lee, 1999, Lin, 1998, Muller et al., 2006, Sinha and Mihalcea, 2007, Weeds and Weir, 2005). But we found the word similarity measure reported in (Lin, 1998) was more attractive, because we could easily generalize it to develop a word clustering algorithm that can be used on any language. In the paper the mutual information between two words w1 and w2 is defined as, I ( w1 , r , w2 ) = log Cnt ( w1 , r , w2 ) ⋅ Cnt (∗, r ,∗) Cnt ( w1 , r ,∗) ⋅ Cnt (∗, r , w2 ) where “r” represents the grammatical relationship of w1 and w2 , such as “is subject of”, “is object of” and Cnt (⋅) denotes the count. The similarity betwe"
2011.mtsummit-papers.41,W06-0605,0,0.0122528,"el text, translation dictionaries, syntactic and semantic parsing tools, which in the literature are often referred to as “low-density” or “low-resource”. Nowadays it becomes much easier to obtain monolingual text data through the internet, but it still remains difficult to obtain parallel text. In this paper we deal with a low-resource situation where only a limited amount of bilingual text is available but large amounts of monolingual text are available for both the source and target languages. One more important reason to begin with such a situation was that we would like to exRelated work Maxwell and Hughes (2006) proposed two strategies for low-resource languages: (I) transfer relevant linguistic information from existing tools and resources from resource-rich languages to a lower-resource language, and (II) develop methods that require less data. Strategy (I) is often used between two closely related languages. For example, the bootstrapping of an UrduEnglish MT with a Hindi-English system in (Sinha, 2009) and generating rules for the Yiddish-English MT from the German-English and Hebrew-English MT in (Genzel et al., 2009). There are more efforts reported on exploring strategy (II), such as the work"
2011.mtsummit-papers.41,N04-1043,0,0.0365,"ected ful unbelted diagnosed himself herself unabatedly himself ourselves herbullhorns mcvety records self yourself themgawker.com selves myself itself Table 2. Comparison of 9 of the 50K English word classes generated by the hirarchical clustering and the WSB clustering algorithms Approved for Public Release, Distribution Unlimited. – June, 2011 355 To compare with other clustering algorithms, we tried the GIZA++ word clustering tool, but it could not handle the 5B word corpus (mainly due to speed). Another state-of-the-art algorithm is the hierarchical word clustering algorithm reported in (Miller et al., 2004), which is an integration of the clustering methods from (Brown et al., 1990) and (Magerman, 1995). Miller et al. (2004) showed promising results when they used it in the name tagging application. We denote this algorithm as “hierarchical clustering”. We also ran this clustering on the 5B corpus to generate different numbers of classes. We judged that the WSB clustering produced better classes, especially in terms of semantics. Table 2 shows nine classes from the “hierarchical” and “WSB” clustering that include words, “obama”, “michael”, “massachusetts”, “lawyer”, “christianity”, “announce”, “"
2011.mtsummit-papers.41,W06-3811,0,0.0149935,"d target words that have the same meaning should be aligned together. So it is better to cluster words semantically if word class alignments are used to improve word alignments. Ker and Zhang (1997) used manmade thesauri in their work to help aligning words. Thesauri are often unavailable in lowresource situations. Hence, to achieve our goal we needed a semantic clustering algorithm, which clusters synonyms together. A number of semantic clustering algorithms have been reported, such as those in (Bellegarda et al., 1996, Geffet and Dagan, 2004, Ichioka and Fukmoto, 2008, Lee, 1999, Lin, 1998, Muller et al., 2006, Sinha and Mihalcea, 2007, Weeds and Weir, 2005). But we found the word similarity measure reported in (Lin, 1998) was more attractive, because we could easily generalize it to develop a word clustering algorithm that can be used on any language. In the paper the mutual information between two words w1 and w2 is defined as, I ( w1 , r , w2 ) = log Cnt ( w1 , r , w2 ) ⋅ Cnt (∗, r ,∗) Cnt ( w1 , r ,∗) ⋅ Cnt (∗, r , w2 ) where “r” represents the grammatical relationship of w1 and w2 , such as “is subject of”, “is object of” and Cnt (⋅) denotes the count. The similarity between two words w1 and w"
2011.mtsummit-papers.41,J03-1002,0,0.044788,"ferent languages. We aimed at exploring approaches that can be easily applied to any languages. 1.2 Our strategy and contribution The performance of SMT normally degrades greatly when training data becomes insufficient. The degradation mainly results from two factors, less reliable translation rules and poorer rule coverage. In this paper we focus only on the first Approved for Public Release, Distribution Unlimited. – June, 2011 352 factor. Word alignment quality is one of the biggest factors that affect the extraction of reliable translation rules. Statistical word aligners, such as GIZA++ (Och and Ney, 2003), are generally weak at aligning infrequent words properly because of the insufficient statistical evidence. With improper word alignments unreliable translation rules are extracted. In this paper we focus on improving word alignment quality. The statistical evidence for word classes, in general are more adequate, so alignments estimated for word classes would be relatively more reliable. Our strategy was to use word class alignments to improve word alignments. Some work has been reported that improved MT with word classes, such as generating syntactic and semantic features for the SMT decodin"
2011.mtsummit-papers.41,P08-1066,0,0.0192289,"For these sentences we kept only one reference translation (chosen randomly). For measuring the MT performance we randomly selected 3,000 (3K) sentences from the same newswire portion without overlaps with the 1K tuning set. Many of the 3K sentences have multiple reference translations, which we thought would be fine to be kept for better score measuring. We used the IBM BLEU (Papineni et al., 2002) metric to measure the MT performance. All the scores reported in this paper were measured on the 3K test set. 2.3 The baseline performance We built our SMT systems based on the model described in (Shen et al., 2008). We used GIZA++ to train word alignments. The decoding parameters were tuned using the minimum error rate method (Och, 2003) to maximize the BLEU score. The BLEU scores for the MT models trained with the 200M and the 165K training corpuses are shown in Table 1. As can be seen, there is a big performance loss (17.3 = 35.4-18.1) when the amount of training is reduced from 200M to 165K (column “200M” vs. column “165K”). The “165K” system serves as the baseline for the work reported in this paper. Train data BLEU 200M 35.37 165K 18.06 165K-best 20.36 Table 1. MT performance (BLEU scores) of syste"
2011.mtsummit-papers.41,2009.mtsummit-caasl.12,0,0.0249415,"ge amounts of monolingual text are available for both the source and target languages. One more important reason to begin with such a situation was that we would like to exRelated work Maxwell and Hughes (2006) proposed two strategies for low-resource languages: (I) transfer relevant linguistic information from existing tools and resources from resource-rich languages to a lower-resource language, and (II) develop methods that require less data. Strategy (I) is often used between two closely related languages. For example, the bootstrapping of an UrduEnglish MT with a Hindi-English system in (Sinha, 2009) and generating rules for the Yiddish-English MT from the German-English and Hebrew-English MT in (Genzel et al., 2009). There are more efforts reported on exploring strategy (II), such as the work in (Al-Onaizan et al., 2000, Nießen and Ney, 2004, Roy and Popwich, 2010, Wang et al., 2006, Homola and Kubon, 2005, Xiang et al., 2010). All those methods are language-specific, because they either need to find the relevant resource-rich language or use language-specific features. Baker et al. (2010) showed more general ways to incorporate syntactic and semantic knowledge into the UrduEnglish MT sy"
2011.mtsummit-papers.41,P06-2112,0,0.0118756,"n dictionaries, syntactic and semantic parsing tools, which in the literature are often referred to as “low-density” or “low-resource”. Nowadays it becomes much easier to obtain monolingual text data through the internet, but it still remains difficult to obtain parallel text. In this paper we deal with a low-resource situation where only a limited amount of bilingual text is available but large amounts of monolingual text are available for both the source and target languages. One more important reason to begin with such a situation was that we would like to exRelated work Maxwell and Hughes (2006) proposed two strategies for low-resource languages: (I) transfer relevant linguistic information from existing tools and resources from resource-rich languages to a lower-resource language, and (II) develop methods that require less data. Strategy (I) is often used between two closely related languages. For example, the bootstrapping of an UrduEnglish MT with a Hindi-English system in (Sinha, 2009) and generating rules for the Yiddish-English MT from the German-English and Hebrew-English MT in (Genzel et al., 2009). There are more efforts reported on exploring strategy (II), such as the work"
2011.mtsummit-papers.41,W04-3226,0,0.0591061,"Missing"
2011.mtsummit-papers.41,J05-4002,0,0.011564,"d be aligned together. So it is better to cluster words semantically if word class alignments are used to improve word alignments. Ker and Zhang (1997) used manmade thesauri in their work to help aligning words. Thesauri are often unavailable in lowresource situations. Hence, to achieve our goal we needed a semantic clustering algorithm, which clusters synonyms together. A number of semantic clustering algorithms have been reported, such as those in (Bellegarda et al., 1996, Geffet and Dagan, 2004, Ichioka and Fukmoto, 2008, Lee, 1999, Lin, 1998, Muller et al., 2006, Sinha and Mihalcea, 2007, Weeds and Weir, 2005). But we found the word similarity measure reported in (Lin, 1998) was more attractive, because we could easily generalize it to develop a word clustering algorithm that can be used on any language. In the paper the mutual information between two words w1 and w2 is defined as, I ( w1 , r , w2 ) = log Cnt ( w1 , r , w2 ) ⋅ Cnt (∗, r ,∗) Cnt ( w1 , r ,∗) ⋅ Cnt (∗, r , w2 ) where “r” represents the grammatical relationship of w1 and w2 , such as “is subject of”, “is object of” and Cnt (⋅) denotes the count. The similarity between two words w1 and w2 is then computed based on the mutual informatio"
2011.mtsummit-papers.41,D10-1065,0,0.0607331,"nts (in the same way as training word alignments in Step 1) 4) Replace the class names with the corresponding source and target words to get a new set of word alignments (denoted as “class-derived” word alignments) 5) Use both the regular and the classderived word alignments in MT training and decoding In Step 5), there are different ways to combine the two sets of alignments, such as the entropy approach in (Ayan and Dorr, 2006) and probability interpolating approach in (Wang et al., 2006). We concatenated the two sets of alignments together for the rule extraction – the same way as used in (Xu and Rosti, 2010). GIZA++ also uses word classes inside its training by introducing word class dependencies in the Model4, Model5 and HMM training. Following (Och and Ney, 2003), we used the GIZA++ word clustering tool – “_mkcls” – to generate 50 source and 50 target word classes for the word alignment training. As described above, the way that we use word classes is outside the GIZA++ training. We verified that the gains from our method are additive to the gains from the use of the word classes inside GIZA++, so we used the GIZA++ word classes in all our alignment training experiments, unless specified. 4.2 E"
2020.clssts-1.7,J03-1002,0,0.0178101,"we train the learner is small, we did not manage to obtain results that generalized better. • The QST-transformed score scoreqst (q, d) w∈q Text Tune 482 449 460 Table 1: Size of various datasets (in terms of number of documents). • Original retrieval score(q, d) w∈q Train 338 316 291 4.2. The CLIR System We give a brief description of the CLIR system that is used to generate the original retrieval scores. A more detailed description appears in (Zbib et al., 2019). It uses a probabilistic bilingual dictionary, trained on a set of parallel sentences and lexicons that were aligned with GIZA++ (Och and Ney, 2003). For each language pair (Somali-English, Swahili-English and Tagalog-English) the bilingual dictionary provides a translation probability P (e|f ) between a source word f and a target word e. Queries consist of one or more words in the target language (English), and a document is deemed relevant to a query if it contains at least one occurrence of each of the terms of the query.2 αi · fi i=1 and new decisions decision(q, d) = 1[scoremodel (q, d) ≥ t∗ ]. During training, AQWV performance is also measured on a “tuning” set for early stopping. L2 regularization (which forces the trained weights"
2020.clssts-1.8,W19-6721,0,0.0609898,"Missing"
2020.clssts-1.8,kamholz-etal-2014-panlex,0,0.0601285,"Missing"
2020.clssts-1.8,P07-2045,0,0.00893272,"sed SMT: a phrase-based statistical MT system trained over the Lithuanian or Bulgarian bilingual data. • Post-Processing: Filter out any hyponyms that have a vector cosine distance relative to the original EXAMPLE OF phrase greater than 0.35. As above, we use the word embeddings from (Bojanowski et al., 2017). All MT models produce N-best (N=20) hypotheses as output for downstream summarization processing. We used the tensor2tensor toolkit (Vaswani et al., 2018) for the transformer implementation and the fairseq toolkit (Ott et al., 2019) for the dynamic convolution model. We also used Moses (Koehn et al., 2007) for training the phrase-based model. Our own tokenizer was used instead of the tokenizer from Moses to match the tokenization scheme used by other system components. Subword tokenization was done using the sentencepiece toolkit (Kudo and Richardson, 2018), an unsupervised text tokenization method that is independent of the language being processed. 5. For instance, the expansions for the query EXAMPLE OF(footwear) include: “baby shoe”, “bowling shoe”, “sneaker”, “wooden shoe”, “rubber boot”, “congress shoe”, “ghillie”, “combat boot”, “footgear”, “huarache”, etc. 5.2. Indexing We construct inv"
2020.clssts-1.8,D18-2012,0,0.0310714,"ve, we use the word embeddings from (Bojanowski et al., 2017). All MT models produce N-best (N=20) hypotheses as output for downstream summarization processing. We used the tensor2tensor toolkit (Vaswani et al., 2018) for the transformer implementation and the fairseq toolkit (Ott et al., 2019) for the dynamic convolution model. We also used Moses (Koehn et al., 2007) for training the phrase-based model. Our own tokenizer was used instead of the tokenizer from Moses to match the tokenization scheme used by other system components. Subword tokenization was done using the sentencepiece toolkit (Kudo and Richardson, 2018), an unsupervised text tokenization method that is independent of the language being processed. 5. For instance, the expansions for the query EXAMPLE OF(footwear) include: “baby shoe”, “bowling shoe”, “sneaker”, “wooden shoe”, “rubber boot”, “congress shoe”, “ghillie”, “combat boot”, “footgear”, “huarache”, etc. 5.2. Indexing We construct inverted indexes for both the source language and the target language. For text documents, we index words and n-grams. For speech documents, we index both the 1-best output (which is treated as regular text) and the confusion network, saving the ASR posterior"
2020.clssts-1.8,P11-1052,0,0.0715547,"Missing"
2020.clssts-1.8,Q17-1010,0,0.0435206,"tion (Section 6). Because there is no casing information in the ASR transcript, we augmented the MT training data with the lower-cased version of the source data with punctuation marks removed to mimic the condition of ASR output. The neural MT models were trained on both versions of the data together, in a single “multi-style” fashion, to handle both text and ASR transcript as input. This was however not done for the phrase-based model described below. 2 46 http://commoncrawl.org 4.2. MT Models are expanded using nearest-neighbor words of English pretrained Wikipedia-derived word embeddings (Bojanowski et al., 2017) (with a minimum cosine similarity cosmin , typically between 0.3-0.4). The weight of each expansion is an exponential function of the cosine similarity, as follows The machine translation component consists of two multilingual neural MT models and one phrase-based statistical MT (SMT) model: 1. Transformer NMT: a 6-layer transfomer-based model (Vaswani et al., 2017) jointly trained over Lithuanian, Bulgarian, Russian and Ukrainian data. We applied data oversampling and used 21k subword units in the vocabulary. We trained the model over the training data using 600k training steps with a batch"
2020.clssts-1.8,P19-3004,0,0.278746,"rier and to make domain information accessible to all users irrespective of language and region. The IARPA MATERIAL1 program presents us with the challenge of developing high-performance CLIR, machine translation, automatic speech recognition (ASR), and summarization for a new language in a few weeks, given limited training resources. In this paper, we describe our CLIR system entry to the MATERIAL evaluation of October, 2019. We were to process evaluation data for both Lithuanian and Bulgarian and to submit system output in 10 days. Our CLIR system achieves the same goal as the SARAL system (Boschee et al., 2019a). While both systems feature a neural network (NN) architecture, the main difference lies in the way an NN model is used. The SARAL system uses a neural network attention model (dot-product) to compute query-document relevance from a shared embedding space, while our system utilizes neural network (multilayer perceptron) as part of the Neural Network Lexical Translation Model (Zbib et al., 2019) to produce probability of translation needed by a probabilistic CLIR model. The rest of this paper is organized as follows: we introduce the task and data in section 2, including a high level overvie"
2020.clssts-1.8,N19-4009,0,0.0246244,"tree and record all hyponyms found during the process. 3. Moses Phrase-based SMT: a phrase-based statistical MT system trained over the Lithuanian or Bulgarian bilingual data. • Post-Processing: Filter out any hyponyms that have a vector cosine distance relative to the original EXAMPLE OF phrase greater than 0.35. As above, we use the word embeddings from (Bojanowski et al., 2017). All MT models produce N-best (N=20) hypotheses as output for downstream summarization processing. We used the tensor2tensor toolkit (Vaswani et al., 2018) for the transformer implementation and the fairseq toolkit (Ott et al., 2019) for the dynamic convolution model. We also used Moses (Koehn et al., 2007) for training the phrase-based model. Our own tokenizer was used instead of the tokenizer from Moses to match the tokenization scheme used by other system components. Subword tokenization was done using the sentencepiece toolkit (Kudo and Richardson, 2018), an unsupervised text tokenization method that is independent of the language being processed. 5. For instance, the expansions for the query EXAMPLE OF(footwear) include: “baby shoe”, “bowling shoe”, “sneaker”, “wooden shoe”, “rubber boot”, “congress shoe”, “ghillie”,"
2020.clssts-1.8,P16-1009,0,0.0911142,"Missing"
2020.clssts-1.8,tiedemann-2012-parallel,0,0.0605221,"Missing"
2020.clssts-1.8,W18-1819,0,0.0298316,"s. • WordNet Hyponym Traversal: For each Synset, recursively traverse its hyponym tree and record all hyponyms found during the process. 3. Moses Phrase-based SMT: a phrase-based statistical MT system trained over the Lithuanian or Bulgarian bilingual data. • Post-Processing: Filter out any hyponyms that have a vector cosine distance relative to the original EXAMPLE OF phrase greater than 0.35. As above, we use the word embeddings from (Bojanowski et al., 2017). All MT models produce N-best (N=20) hypotheses as output for downstream summarization processing. We used the tensor2tensor toolkit (Vaswani et al., 2018) for the transformer implementation and the fairseq toolkit (Ott et al., 2019) for the dynamic convolution model. We also used Moses (Koehn et al., 2007) for training the phrase-based model. Our own tokenizer was used instead of the tokenizer from Moses to match the tokenization scheme used by other system components. Subword tokenization was done using the sentencepiece toolkit (Kudo and Richardson, 2018), an unsupervised text tokenization method that is independent of the language being processed. 5. For instance, the expansions for the query EXAMPLE OF(footwear) include: “baby shoe”, “bowli"
A00-1044,A97-1029,1,0.720136,"ibute to performance? (Section 7) 2 2.1 Algorithms and Data normal punctuation. Printing the on-line text, rather than using the original newsprint, produced the images for OCR, which were all scanned at 600 DPI. Task Definition and Data The named entity (NE) task used for this evaluation requires the system to identify all named locations, named persons, named organizations, dates, times, monetary amounts, and percentages. The task definition is given in Chinchor, et al, (1998). 2.2 Algorithms The information extraction system tested is IdentiFinder(TM), which has previously been detailed in Bikel et al. (1997, 1999). In that system, an HMM labels each word either with one of the desired classes (e.g., person, organization, etc.) or with the label NOT-ANAME (to represent &quot;none of the desired classes&quot;). The states of the HMM fall into regions, one region for each desired class plus one for NOT-A-NAME. (See Figure 2-1.) The HMM thus has a model of each desired class and of the other text. Note that the implementation is not confined to the seven name classes used in the NE task; the particular classes to be recognized can be easily changed via a parameter. For speech recognition, roughly 175 hours of"
A00-1044,M98-1028,0,0.0342635,"Missing"
A97-1029,J93-2006,1,\N,Missing
A97-1029,M95-1012,0,\N,Missing
A97-1029,H94-1053,1,\N,Missing
A97-1029,M92-1024,1,\N,Missing
boisen-etal-2000-annotating,A97-1028,0,\N,Missing
boisen-etal-2000-annotating,A00-1044,1,\N,Missing
boisen-etal-2000-annotating,A97-1030,0,\N,Missing
boisen-etal-2000-annotating,W99-0612,0,\N,Missing
boisen-etal-2000-annotating,A97-1029,1,\N,Missing
D08-1090,P05-1033,0,0.0322789,"lation model produces the English text to which the 860 language model has been adapted. Bias text that is used by one adaptation but not the other will receive no special treatment by the other model. This could result in new translation rules that produce text to which the language assigns low probability, or it could result in the language model being able to assign a high probability to a good English translation that cannot be produced by the translation model due to a lack of necessary translation rules. While both adaptation methods are integrated into a hierarchical translation model (Chiang, 2005), they are largely implementation independent. Language model adaptation could be integrated into any statistical machine translation that uses a language model over words, while translation model adaptation could be added to any statistical machine translation that can utilize phrasal translation rules. 3.1 Language Model Adaptation For every source document, we estimate a new language model, the bias language model, from the corresponding bias text. Since this bias text is short, the corresponding bias language model is small and specific, giving high probabilities to those phrases that occu"
D08-1090,P98-1069,0,0.0756805,"adapt the MT system to increase the probability of generating texts that resemble the comparable document. Experimental results obtained by adapting both the language and translation models show substantial gains over the baseline system. 1 Introduction While the amount of parallel data available to train a statistical machine translation system is sharply limited, vast amounts of monolingual data are generally available, especially when translating to languages such as English. Yet monolingual data are generally only used to train the language model of the translation system. Previous work (Fung and Yee, 1998; Richard Schwartz BBN Technologies 10 Moulton Street Cambridge, MA 02138, USA schwartz@bbn.com Rapp, 1999) has sought to learn new translations for words by looking at comparable, but not parallel, corpora in multiple languages and analyzing the cooccurrence of words, resulting in the generation of new word-to-word translations. More recently, Resnik and Smith (2003) and Munteanu and Marcu (2005) have exploited monolingual data in both the source and target languages to find document or sentence pairs that appear to be parallel. This newly discovered bilingual data can then be used as additio"
D08-1090,2005.eamt-1.19,0,0.41674,"as increasing the probability of existing translation rules. Translation adaptation using the translation system’s own output, known as Self-Training (Ueffing, 2006) has previously shown gains by augmenting the translation model with additional translation rules. In that approach however, the translation model was augmented using parallel data, rather than comparable data, by interpolating a translation model trained using the system output with the original translation model. Translation model adaptation using comparable out-of-domain parallel data, rather than monolingual data was shown by Hildebrand et al. (2005) to yield significant gains over a baseline system. The translation model was adapted by selecting comparable sentences from parallel corpora for each of the sentences to be translated. In addition to selecting outof-domain data to adapt the translation model, comparable data selection techniques have been used to select and weight portions of the existing training data for the translation model to improve translation performance (Lu et al., 2007). The research presented in this paper utilizes a different approach to translation model adaptation using comparable monolingual text rather than pa"
D08-1090,W03-1003,0,0.0205357,"isit to India. Many phrases and words are shared between the two, including: the name of the movie, the name and relationship of the actress’ character, the name and age of her son and many others. Such a pairing is extremely comparable, although even less related document pairs could easily be considered comparable. We seek to take advantage of these comparable documents to inform the translation of the source document. This can be done by augmenting the major components of the statistical translation system: the Language Model and the Translation Model. This work is in the same tradition as Kim and Khudanpur (2003), Zhao et al. (2004), and Kim (2005). Kim (2005) used large amounts of comparable data to adapt language models on a documentby-document basis, while Zhao et al. (2004) used comparable data to perform sentence level adaptation of the language model. These adapted language models were shown to improve performance 1 This is an actual source document from the tuning set used in our experiments, and the first of a number of similar passages found by the comparable text selection system described in section 2. 858 for both automatic speech recognition as well as machine translation. In addition to"
D08-1090,D07-1036,0,0.0107067,"inal translation model. Translation model adaptation using comparable out-of-domain parallel data, rather than monolingual data was shown by Hildebrand et al. (2005) to yield significant gains over a baseline system. The translation model was adapted by selecting comparable sentences from parallel corpora for each of the sentences to be translated. In addition to selecting outof-domain data to adapt the translation model, comparable data selection techniques have been used to select and weight portions of the existing training data for the translation model to improve translation performance (Lu et al., 2007). The research presented in this paper utilizes a different approach to translation model adaptation using comparable monolingual text rather than parallel text, exploiting data that would otherwise be unused for estimating the translation model. In addition, this data also informs the translation system by interpolating the original language model with a new language model trained from the same comparable documents. We discuss the selection of comparable text for model adaptation in section 2. In sections 3.1 and 3.2, we describe the model adaptation for the language model and translation mod"
D08-1090,J05-4003,0,0.0718158,"ual data are generally available, especially when translating to languages such as English. Yet monolingual data are generally only used to train the language model of the translation system. Previous work (Fung and Yee, 1998; Richard Schwartz BBN Technologies 10 Moulton Street Cambridge, MA 02138, USA schwartz@bbn.com Rapp, 1999) has sought to learn new translations for words by looking at comparable, but not parallel, corpora in multiple languages and analyzing the cooccurrence of words, resulting in the generation of new word-to-word translations. More recently, Resnik and Smith (2003) and Munteanu and Marcu (2005) have exploited monolingual data in both the source and target languages to find document or sentence pairs that appear to be parallel. This newly discovered bilingual data can then be used as additional training data for the translation system. Such methods generally have a very low yield leaving vast amounts of data that is only used for language modeling. These methods rely upon comparable corpora, that is, multiple corpora that are of the same general genre. In addition to this, documents can be comparable—two documents that are both on the same event or topic. Comparable documents occur b"
D08-1090,P00-1056,0,0.0454094,"based on a very small number of occurrences in the training data. In other cases, translations may be known for individual words in the source document, but not for longer phrases. Translation model adaptation seeks to generate new phrasal translation rules for these source words and phrases. The bias text for a source document may, if comparable, contain a number of English words and phrases that are the English side of these desired rules. Because the source data and the bias text are not translations of each other and are not sentence aligned, conventional alignment tools, such as GIZA++ (Och and Ney, 2000), cannot be used to align the source and bias text. Because the passages in the bias text are not translations of the source document, it will always be the case that portions of the source document have no translation in the bias text, 861 and portions of the bias text have no translation in the source document. In addition a phrase in one of these texts might have multiple, differing translations in the other text. Unlike language model adaptation, the entirety of the bias text is not used for translation adaptation. We extract those phrases that occur in at least M of the passages in the bi"
D08-1090,P02-1040,0,0.103551,"m, although in practice this is a small expenditure. The most intensive portion is the initial indexing of the monolingual corpus, but this is only required once and can be reused for any subsequent test set that is evaluated. This index can then be quickly searched for comparable passages. When considering research environments, test sets are used repeatedly and bias texts only need to be built once per set, making the building cost negligible. Otherwise, the time required to build the bias text is still small compared to the actual translation time. All conditions were optimized using BLEU (Papineni et al., 2002) and evaluated using both BLEU and Translation Edit Rate (TER) (Snover et al., 2006). BLEU is an accuracy measure, so higher values indicate better performance, while TER is an error metric, so lower values indicate better performance. Optimization was performed on a tuning set of newswire data, comprised of portions of MTEval 2004, MTEval 2005, and GALE 2007 newswire development data, a total of 48921 words of English in 1385 segments and 173 documents. Results were measured on the NIST MTEval 2006 Arabic Evaluation set, which was 55578 words of English in 1797 segments and 104 documents. Fou"
D08-1090,P99-1067,0,0.0328174,"mental results obtained by adapting both the language and translation models show substantial gains over the baseline system. 1 Introduction While the amount of parallel data available to train a statistical machine translation system is sharply limited, vast amounts of monolingual data are generally available, especially when translating to languages such as English. Yet monolingual data are generally only used to train the language model of the translation system. Previous work (Fung and Yee, 1998; Richard Schwartz BBN Technologies 10 Moulton Street Cambridge, MA 02138, USA schwartz@bbn.com Rapp, 1999) has sought to learn new translations for words by looking at comparable, but not parallel, corpora in multiple languages and analyzing the cooccurrence of words, resulting in the generation of new word-to-word translations. More recently, Resnik and Smith (2003) and Munteanu and Marcu (2005) have exploited monolingual data in both the source and target languages to find document or sentence pairs that appear to be parallel. This newly discovered bilingual data can then be used as additional training data for the translation system. Such methods generally have a very low yield leaving vast amo"
D08-1090,P08-1066,0,0.012193,"t in need of additional translation rules. So, it is under such a condition we would expect the translation model adaptation to be the most beneficial. We evaluate the system’s performance under this condition in section 4.2. The effectiveness of this technique on state-of-the-art systems, and its efficiency when used with a well trained generic translation model is presented in section 4.3. 4.1 Implementation Details Both language-model and translation-model adaptation are implemented on top of a hierarchical Arabic-to-English translation system with string-todependency rules as described in Shen et al. (2008). While generalized rules are generated from the parallel data, rules generated by the translation model adaptation are not generalized and are used only as phrasal rules. A trigram language model was used during decoding, and a 5-gram language model was used to re-score the n-best list after decoding. In addition to the features described in Shen et al. (2008), a new feature is added to the model for the bias rule weight, allowing the translation system to effectively tune the probability of the rules added by translation model adaptation in order to improve performance on the tuning set. Bia"
D08-1090,2006.amta-papers.25,1,0.77697,"initial indexing of the monolingual corpus, but this is only required once and can be reused for any subsequent test set that is evaluated. This index can then be quickly searched for comparable passages. When considering research environments, test sets are used repeatedly and bias texts only need to be built once per set, making the building cost negligible. Otherwise, the time required to build the bias text is still small compared to the actual translation time. All conditions were optimized using BLEU (Papineni et al., 2002) and evaluated using both BLEU and Translation Edit Rate (TER) (Snover et al., 2006). BLEU is an accuracy measure, so higher values indicate better performance, while TER is an error metric, so lower values indicate better performance. Optimization was performed on a tuning set of newswire data, comprised of portions of MTEval 2004, MTEval 2005, and GALE 2007 newswire development data, a total of 48921 words of English in 1385 segments and 173 documents. Results were measured on the NIST MTEval 2006 Arabic Evaluation set, which was 55578 words of English in 1797 segments and 104 documents. Four reference translations were used for scoring each translation. Parameter optimizat"
D08-1090,2006.iwslt-papers.3,0,0.0257977,"m the tuning set used in our experiments, and the first of a number of similar passages found by the comparable text selection system described in section 2. 858 for both automatic speech recognition as well as machine translation. In addition to language model adaptation we also modify the translation model, adding additional translation rules that enable the translation of new words and phrases in both the source and target languages, as well as increasing the probability of existing translation rules. Translation adaptation using the translation system’s own output, known as Self-Training (Ueffing, 2006) has previously shown gains by augmenting the translation model with additional translation rules. In that approach however, the translation model was augmented using parallel data, rather than comparable data, by interpolating a translation model trained using the system output with the original translation model. Translation model adaptation using comparable out-of-domain parallel data, rather than monolingual data was shown by Hildebrand et al. (2005) to yield significant gains over a baseline system. The translation model was adapted by selecting comparable sentences from parallel corpora"
D08-1090,C04-1059,0,0.223215,"are shared between the two, including: the name of the movie, the name and relationship of the actress’ character, the name and age of her son and many others. Such a pairing is extremely comparable, although even less related document pairs could easily be considered comparable. We seek to take advantage of these comparable documents to inform the translation of the source document. This can be done by augmenting the major components of the statistical translation system: the Language Model and the Translation Model. This work is in the same tradition as Kim and Khudanpur (2003), Zhao et al. (2004), and Kim (2005). Kim (2005) used large amounts of comparable data to adapt language models on a documentby-document basis, while Zhao et al. (2004) used comparable data to perform sentence level adaptation of the language model. These adapted language models were shown to improve performance 1 This is an actual source document from the tuning set used in our experiments, and the first of a number of similar passages found by the comparable text selection system described in section 2. 858 for both automatic speech recognition as well as machine translation. In addition to language model adapt"
D08-1090,C98-1066,0,\N,Missing
D08-1090,J03-3002,0,\N,Missing
D14-1095,D13-1174,0,0.0124657,"e 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 880–885, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics Feature morpheme unigrams morpheme bigram phonetic seq. unigrams phonetic seq. bigram number of morphemes morpheme lengths phologically rich languages. This results in improved language model perplexity, better word alignments and higher BLEU scores. Recent work has demonstrated that even morphological analyzers that use little or no supervision can help improve performance in language modeling and machine translation (Chahuneau et al., 2013b; Stallard et al., 2012). It has also been shown that segmentation lattices improve the quality of machine translation systems (Dyer, 2009). In this work, we leverage morphological segmentation to reduce OOV rates in KWS. We investigate segmentations produced by a range of models, including acoustic sub-word units. We incorporate these subword units into a lattice framework within the KWS system. We also demonstrate the value of using alternative segmentations instead of or in combination with morphemes. In addition to improving the performance of KWS systems, this finding may also benefit ot"
D14-1095,kruengkrai-etal-2006-conditional,0,0.0269761,"egmentations. Hence, we choose the top two segmentations per word from the output of the ranker to use in our KWS system. In both filters, we use several features like morpheme unigrams, bigrams, lengths, number of morphemes, and phone sequences corresponding to the morphemes. In our supervised systems, we can encode features that go beyond individual boundaries, like the total number of morphemes in the segmentation. This global view distinguishes our classifier/ranker from traditional approaches that model segmentation as a sequence tagging task (Ruokolainen et al., 2013; Kudo et al., 2004; Kruengkrai et al., 2006). Another departure of our approach is the use of phonetic information, in the form of phonetic sequences corresponding to the morpheme unigrams and bigrams. The hypothesis is that syllabic boundaries are correlated with morpheme boundaries to some extent. The phonetic sequences for words are obtained using a publicly available Text-to-Phone (T2P) system (Lenzo, 1998). Unsupervised Morphological Segmentation We employ a widely-used unsupervised system Morfessor (Creutz and Lagus, 2005) which achieves state-of-the-art unsupervised performance in the MorphoChallenge evaluation. Morfessor uses pr"
D14-1095,N13-1140,0,0.0288465,"e 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 880–885, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics Feature morpheme unigrams morpheme bigram phonetic seq. unigrams phonetic seq. bigram number of morphemes morpheme lengths phologically rich languages. This results in improved language model perplexity, better word alignments and higher BLEU scores. Recent work has demonstrated that even morphological analyzers that use little or no supervision can help improve performance in language modeling and machine translation (Chahuneau et al., 2013b; Stallard et al., 2012). It has also been shown that segmentation lattices improve the quality of machine translation systems (Dyer, 2009). In this work, we leverage morphological segmentation to reduce OOV rates in KWS. We investigate segmentations produced by a range of models, including acoustic sub-word units. We incorporate these subword units into a lattice framework within the KWS system. We also demonstrate the value of using alternative segmentations instead of or in combination with morphemes. In addition to improving the performance of KWS systems, this finding may also benefit ot"
D14-1095,W04-3230,0,0.063284,"average 2.5 gold segmentations. Hence, we choose the top two segmentations per word from the output of the ranker to use in our KWS system. In both filters, we use several features like morpheme unigrams, bigrams, lengths, number of morphemes, and phone sequences corresponding to the morphemes. In our supervised systems, we can encode features that go beyond individual boundaries, like the total number of morphemes in the segmentation. This global view distinguishes our classifier/ranker from traditional approaches that model segmentation as a sequence tagging task (Ruokolainen et al., 2013; Kudo et al., 2004; Kruengkrai et al., 2006). Another departure of our approach is the use of phonetic information, in the form of phonetic sequences corresponding to the morpheme unigrams and bigrams. The hypothesis is that syllabic boundaries are correlated with morpheme boundaries to some extent. The phonetic sequences for words are obtained using a publicly available Text-to-Phone (T2P) system (Lenzo, 1998). Unsupervised Morphological Segmentation We employ a widely-used unsupervised system Morfessor (Creutz and Lagus, 2005) which achieves state-of-the-art unsupervised performance in the MorphoChallenge eva"
D14-1095,coltekin-2010-freely,0,0.0202908,"Missing"
D14-1095,W13-3504,0,0.199738,"g corpus, each word has on average 2.5 gold segmentations. Hence, we choose the top two segmentations per word from the output of the ranker to use in our KWS system. In both filters, we use several features like morpheme unigrams, bigrams, lengths, number of morphemes, and phone sequences corresponding to the morphemes. In our supervised systems, we can encode features that go beyond individual boundaries, like the total number of morphemes in the segmentation. This global view distinguishes our classifier/ranker from traditional approaches that model segmentation as a sequence tagging task (Ruokolainen et al., 2013; Kudo et al., 2004; Kruengkrai et al., 2006). Another departure of our approach is the use of phonetic information, in the form of phonetic sequences corresponding to the morpheme unigrams and bigrams. The hypothesis is that syllabic boundaries are correlated with morpheme boundaries to some extent. The phonetic sequences for words are obtained using a publicly available Text-to-Phone (T2P) system (Lenzo, 1998). Unsupervised Morphological Segmentation We employ a widely-used unsupervised system Morfessor (Creutz and Lagus, 2005) which achieves state-of-the-art unsupervised performance in the"
D14-1095,N09-1046,0,0.0111373,"Computational Linguistics Feature morpheme unigrams morpheme bigram phonetic seq. unigrams phonetic seq. bigram number of morphemes morpheme lengths phologically rich languages. This results in improved language model perplexity, better word alignments and higher BLEU scores. Recent work has demonstrated that even morphological analyzers that use little or no supervision can help improve performance in language modeling and machine translation (Chahuneau et al., 2013b; Stallard et al., 2012). It has also been shown that segmentation lattices improve the quality of machine translation systems (Dyer, 2009). In this work, we leverage morphological segmentation to reduce OOV rates in KWS. We investigate segmentations produced by a range of models, including acoustic sub-word units. We incorporate these subword units into a lattice framework within the KWS system. We also demonstrate the value of using alternative segmentations instead of or in combination with morphemes. In addition to improving the performance of KWS systems, this finding may also benefit other applications that currently use morphological segmentation for OOV reduction. 3 Table 1: Example of features used in the supervised filt"
D14-1095,P12-2063,1,0.789403,"rical Methods in Natural Language Processing (EMNLP), pages 880–885, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics Feature morpheme unigrams morpheme bigram phonetic seq. unigrams phonetic seq. bigram number of morphemes morpheme lengths phologically rich languages. This results in improved language model perplexity, better word alignments and higher BLEU scores. Recent work has demonstrated that even morphological analyzers that use little or no supervision can help improve performance in language modeling and machine translation (Chahuneau et al., 2013b; Stallard et al., 2012). It has also been shown that segmentation lattices improve the quality of machine translation systems (Dyer, 2009). In this work, we leverage morphological segmentation to reduce OOV rates in KWS. We investigate segmentations produced by a range of models, including acoustic sub-word units. We incorporate these subword units into a lattice framework within the KWS system. We also demonstrate the value of using alternative segmentations instead of or in combination with morphemes. In addition to improving the performance of KWS systems, this finding may also benefit other applications that cur"
D14-1095,P08-2015,0,0.0136628,"Missing"
H89-2033,H89-2027,1,0.875671,"Missing"
H89-2033,H89-2020,1,\N,Missing
H89-2033,H89-1016,0,\N,Missing
H89-2033,H89-1024,0,\N,Missing
H89-2034,H89-1011,1,0.779924,"Missing"
H90-1003,H90-1016,1,0.840367,"Missing"
H90-1003,H89-2027,1,0.392992,"by a factor of 40 with no additional search errors. 1. Introduction In a Spoken Language System (SLS) we must use all available knowledge sources (KSs) to decide on the spoken sentence. While there are many knowledge sources, they are often grouped together into speech models, statistical language model, and natural language understanding models. To optimize accuracy we must choose the sentence that has the highest score (probability) given all of the KSs. This potentially requires a very large search space. The N-Best paradigm for integrating several diverse KSs has been described previously [2, 10]. First, we use a subset of the KSs to choose a small number of likely sentences. Then these sentences are scored using the remainder of the KSs. In Chow et. al., we also presented an efficient speech recognition search algorithm that was capable of computing the N most likely sentence hypotheses for an utterance, given the speech models and statistical language models. However, this algorithm greatly increases the needed computation over that needed for finding the best single sentence. In this paper we introduce two techniques that dramatically decrease the computation needed for the N-Best"
H90-1003,H89-2020,1,0.343173,"ories within the word, we correctly identify the second best path. While the computation needed is greater than for the lattice algorithm it is less than for the sentence-dependent algorithm, since the number of theories only needs to account for number of possible previous words - not all possible preceding sequences. Therefore the number n, of theories kept locally only needs to be 3 to 6 instead of 20 to 100. Comparison of N-Best Algorithms 3. Forward-Backward Search We performed experimentsto compare the behavior of the three N-Best algorithms. In all three cases we used the Class Grammar [3], a first-order statistical grammar based on 100 word classes. All words within a class are assumed equally likely. The test set perplexity is approximately 100. The test set used was the June '88 speaker-dependent test set of 300 sentences. To enable direct comparison with previous results we did not use models of triphones across word boundaries, and the models were not smoothed. We expect all three algorithms to improve significantly when the latest modeling methods are used. The time-synchronous beam search follows a large number of theories on the off chance that they will get better duri"
H90-1016,H89-2027,1,0.795053,"Missing"
H90-1016,H89-2020,1,0.778174,"will almost always violate the constraints of the language model. Thus, a Word-Pair type language model will have a fixed high error rate. The group at IBM has long been an advocate of statistical language models that can reduce the entropy or perplexity of the language while still allowing all possible word sequences with some probability. For most SLS domains where there is not a large amount of training data available, it is most practical to use a statistical model of word classes rather than individual words. We have circulated a so called Class Grammar for the Resource Management Domain [3]. The language model was simply constructed, having only first-order statistics and not distinguishing the probability of different words within a class. The measured test set perplexity of this language model is about 100. While more powerful ""fair"" models could be constructed, we felt that this model would predict the difficulty of a somewhat larger task domain. The word error rate is typically twice that of the Word-Pair (WP) grammar. One problem with this type of grammar is that the computation is quite a bit larger than for the WP grammar, since all 1000 words can follow each word (rather"
H90-1016,H90-1003,1,0.786511,"lity, and better hardware and software support. We would not have to divert attention from speech-recognition research and our results wouM be more easily shared with the community. We felt that these hopes were not unrealistic because, for our algorithms in particular, which are research-oriented and seldom have a stable, regular StlUCture, the potential gains from custom hardware wouM not be that great. Our strategy in developing a real-time spoken language system has been to find appropriate commercial hardware Two of the new algorithms used in this effort are described in a separate paper [8]. Specifically, the Word-Dependent NBest search and the Forward-Backward Search. In Section 2 we describe the class of hardware that we have chosen for this task. Section 3 reviews the reasons for using a statistical grammar and presents a new more efficient algorithm for time-synchronous decoding with a statistical grammar. In Section 4 we compare the accuracy of the standard Viterbi algorithm with the time-synchronous forward search algorithm that we use. And in Section 5 we give the latest status of the speed and accuracy of our system. 2. Hardware It is already quite straightforward to per"
H90-1060,H89-1016,0,0.0624828,"Missing"
H91-1065,H90-1078,1,0.773244,"Missing"
H91-1065,H90-1069,1,0.904241,"Missing"
H91-1065,A88-1019,0,0.193383,"2, P a r l a n c e ruz, and Delphi 4, use these techniques quite successfully. However, as we move from the problem of understanding queries in fixed domains to processing open text for applications such as data extraction, we have found rule-based techniques too brittle, and the amount of work necessary to build them intractable, especially when attempting to use the same system on multiple domains. We report in this paper on one application of probabilistic models to language processing, the assignment of part of speech to words in open text. The effectiveness of such models is well known (Church 1988) and they are currently in use in parsers (e.g. de Marcken 1990). Our work is an incremental improvement on these models in two ways: (1) We have run experiments regarding the amount of training data needed in moving to a new domain; (2) we have added probabilistic models of word features to handle unknown words effectively. We describe POST and its algorithms and then we describe our extensions, showing the results of our experiments. 1 The work reported here was supported by the Advanced Research Projects Agency and was monitored by the Rome Air Development Center under Contract No. F30602-8"
H91-1065,P90-1031,0,0.35487,"Missing"
H91-1065,H89-2011,0,0.0669481,"Missing"
H91-1065,H89-2006,0,0.0264641,"en we describe our extensions, showing the results of our experiments. 1 The work reported here was supported by the Advanced Research Projects Agency and was monitored by the Rome Air Development Center under Contract No. F30602-87-D-0093. The views and conclusions contained in this document are those of the authors and should not be interpreted as necessarily representing the official policies, whether expressed or implied, of the Defense Advanced Research Projects Agency or the United States Government. 2 Weischedel, et al. 1989. 3 Parlance is a trademark of BBN Systems and Technologies. 4 Stallard, 1989. 331 2. POST: USING PROBABILITIES TO TAG PART OF SPEECH Predicting the part of speech o f a word is one straightforward way to use probabilities. Many words are several ways ambiguous, such as the following: a round table: adjective a round of cheese: noun to round out your interests: verb to work the year round: adverb Even in context, part of speech can be ambiguous, as in the famous example: &quot;Time flies.&quot; where both words are two ways a m b i g u o u s , r e s u l t i n g in two g r a m m a t i c a l interpretations as sentences. Models predicting part of speech can serve to cut down the s"
H91-1065,H89-1013,1,0.766068,"atures to handle unknown words effectively. We describe POST and its algorithms and then we describe our extensions, showing the results of our experiments. 1 The work reported here was supported by the Advanced Research Projects Agency and was monitored by the Rome Air Development Center under Contract No. F30602-87-D-0093. The views and conclusions contained in this document are those of the authors and should not be interpreted as necessarily representing the official policies, whether expressed or implied, of the Defense Advanced Research Projects Agency or the United States Government. 2 Weischedel, et al. 1989. 3 Parlance is a trademark of BBN Systems and Technologies. 4 Stallard, 1989. 331 2. POST: USING PROBABILITIES TO TAG PART OF SPEECH Predicting the part of speech o f a word is one straightforward way to use probabilities. Many words are several ways ambiguous, such as the following: a round table: adjective a round of cheese: noun to round out your interests: verb to work the year round: adverb Even in context, part of speech can be ambiguous, as in the famous example: &quot;Time flies.&quot; where both words are two ways a m b i g u o u s , r e s u l t i n g in two g r a m m a t i c a l interpretatio"
H91-1080,H90-1006,0,\N,Missing
H92-1014,H91-1042,1,0.887996,"s optimal, and that the two-pass processing strategy was slightly better than either of the others. This was the configuration we used on the February '92 evaluation data. SPOKEN LANGUAGE UNDERSTANDING - HARC, BBN's spoken language system, utilizes BYBLOS as its speech recognition component, and DELPHI as its natural language understanding component. DELPHI uses a definite clause grammar formalism, augmented by the use of constraint nodes [9] and a labelled argument formalism [3]. The parsing algorithm uses a statistically trained agenda to produce the single best parse for an input utterance [1]. In Table 5 we show our Weighted Error on the February '92 evaluation data for Combined Class A+D, and Classes A and D separately, as calculated by NIST. During the test run, we had neglected to include the date information provided for individual scenarios. We include the results of a re-run with the same system as ran the February '92 test set, with We experimented with several conditions to optimize the connection of BYBLOS with DELPHI. The basic interface 76 Condition N WE Text FaUback on &quot; &quot; Fallback off &quot; &quot; (1) 1 5 20 1 5 20 47.9 64.6 58.0 60.1 64.2 56.9 59.0 5 56.6 Two Pass Finally, we"
H92-1014,H92-1061,1,0.872639,"Missing"
H92-1014,H92-1062,1,0.888034,"e October '91 dry-run corpus as development test in Table 4. The results of our experiments indicated that an N of 5 was optimal, and that the two-pass processing strategy was slightly better than either of the others. This was the configuration we used on the February '92 evaluation data. SPOKEN LANGUAGE UNDERSTANDING - HARC, BBN's spoken language system, utilizes BYBLOS as its speech recognition component, and DELPHI as its natural language understanding component. DELPHI uses a definite clause grammar formalism, augmented by the use of constraint nodes [9] and a labelled argument formalism [3]. The parsing algorithm uses a statistically trained agenda to produce the single best parse for an input utterance [1]. In Table 5 we show our Weighted Error on the February '92 evaluation data for Combined Class A+D, and Classes A and D separately, as calculated by NIST. During the test run, we had neglected to include the date information provided for individual scenarios. We include the results of a re-run with the same system as ran the February '92 test set, with We experimented with several conditions to optimize the connection of BYBLOS with DELPHI. The basic interface 76 Condition N W"
H92-1014,H89-2027,1,0.880499,"10411 spontaneous utterances from 286 subjects. The data originated from 5 collection sites using a variety of strategies for eliciting and capturing spontaneous queries from the subjects [7]. The training data was not balanced across the five sites, however. MIT was represented by 3-4 times as much data as any other site. Overall, MIT data accounted for nearly half of the ATIS2 subcorpus (4600 utterances). RECOGNITION The BYBLOS speech recognition system produces an ordered list of the N top-scoring hypotheses which is then reordered by several detailed knowledge sources. The Nbest strategy [4,8] permits the use of computationally prohibitive models by greatly reducing the search space to a few dozen word sequences. It has enabled us to use crossword-boundary triphone models and trigram language models with ease. The N-best list is also a robust interface between speech and natural language that provides a way to recover from speech errors in the top choice word sequence. The evaluation test data was drawn from this same pool of data so we decided to ignore the earlier batches of ATIS data that were collected under still different circumstances (most of it was read speech) and would n"
H92-1014,H91-1012,0,0.032865,"uage models with ease. The N-best list is also a robust interface between speech and natural language that provides a way to recover from speech errors in the top choice word sequence. The evaluation test data was drawn from this same pool of data so we decided to ignore the earlier batches of ATIS data that were collected under still different circumstances (most of it was read speech) and would not be represented in the new test (dialects were predominantly southern in the ATIS0 subcorpus). The overall system architecture for this evaluation is similar to that used in the February '91 tests [6]. Specifically, we use a 4-pass approach to produce the N-best lists for natural language processing. We filtered the training data for quality in several ways. All utterances that were marked as truncated in the SRO (speech recognition output) transcription were ignored. Similarly, we omitted from the training all utterances that contained a word fragment, We also ignored any utterances 1. A forward pass with a bigram grammar and discrete 72 that contained rare nonspeech events. Finally, our forwardbackward training program rejected any input that failed to align properly. These steps removed"
H92-1014,H92-1003,0,0.105162,"the N-best interface. Results are presented for speech recognition alone and for the overall spoken language system. A detailed discussion of DELPHI is presented in [2,3] elsewhere in these proceedings. 2. B Y B L O S - S P E E C H 2.1 Training and D e v e l o p m e n t Test Data We used speech data from the ATIS2 subcorpus exclusively to train the parameters of the acoustic model. This subcorpus consists of 10411 spontaneous utterances from 286 subjects. The data originated from 5 collection sites using a variety of strategies for eliciting and capturing spontaneous queries from the subjects [7]. The training data was not balanced across the five sites, however. MIT was represented by 3-4 times as much data as any other site. Overall, MIT data accounted for nearly half of the ATIS2 subcorpus (4600 utterances). RECOGNITION The BYBLOS speech recognition system produces an ordered list of the N top-scoring hypotheses which is then reordered by several detailed knowledge sources. The Nbest strategy [4,8] permits the use of computationally prohibitive models by greatly reducing the search space to a few dozen word sequences. It has enabled us to use crossword-boundary triphone models and"
H92-1014,H90-1003,1,0.903119,"10411 spontaneous utterances from 286 subjects. The data originated from 5 collection sites using a variety of strategies for eliciting and capturing spontaneous queries from the subjects [7]. The training data was not balanced across the five sites, however. MIT was represented by 3-4 times as much data as any other site. Overall, MIT data accounted for nearly half of the ATIS2 subcorpus (4600 utterances). RECOGNITION The BYBLOS speech recognition system produces an ordered list of the N top-scoring hypotheses which is then reordered by several detailed knowledge sources. The Nbest strategy [4,8] permits the use of computationally prohibitive models by greatly reducing the search space to a few dozen word sequences. It has enabled us to use crossword-boundary triphone models and trigram language models with ease. The N-best list is also a robust interface between speech and natural language that provides a way to recover from speech errors in the top choice word sequence. The evaluation test data was drawn from this same pool of data so we decided to ignore the earlier batches of ATIS data that were collected under still different circumstances (most of it was read speech) and would n"
H92-1014,H89-2006,1,0.902556,"s of one such experiment, utilizing the October '91 dry-run corpus as development test in Table 4. The results of our experiments indicated that an N of 5 was optimal, and that the two-pass processing strategy was slightly better than either of the others. This was the configuration we used on the February '92 evaluation data. SPOKEN LANGUAGE UNDERSTANDING - HARC, BBN's spoken language system, utilizes BYBLOS as its speech recognition component, and DELPHI as its natural language understanding component. DELPHI uses a definite clause grammar formalism, augmented by the use of constraint nodes [9] and a labelled argument formalism [3]. The parsing algorithm uses a statistically trained agenda to produce the single best parse for an input utterance [1]. In Table 5 we show our Weighted Error on the February '92 evaluation data for Combined Class A+D, and Classes A and D separately, as calculated by NIST. During the test run, we had neglected to include the date information provided for individual scenarios. We include the results of a re-run with the same system as ran the February '92 test set, with We experimented with several conditions to optimize the connection of BYBLOS with DELPHI"
H92-1049,H90-1016,1,0.886025,"ech Recognition Demonstrations Steve Austin, Rusty Bobrow, Dan Ellard, Robert Ingria, John Makhoul, Long Nguyen, Pat Peterson, Paul Placeway, Richard Schwartz BBN Systems and Technologies Cambridge MA 02138 Typically, real-time speech recognition - if achieved at all - is accomplished either by greatly simplifying the processing to be done, or by the use of special-purpose hardware. Each of these approaches has obvious problems. The former results in a substantial loss in accuracy, while the latter often results in obsolete hardware being developed at great expense and delay. Starting in 1990 [1] [2] we have taken a different approach based on modifying the algorithms to provide increased speed without loss in accuracy. Our goal has been to use commercially available off-the-shelf (COTS) hardware to perform speech recognition. Initially, this meant using workstations with powerful but standard signal processing boards acting as accelerators. However, even these signal processing boards have two significant disadvantages: 1. They often cost as much as the workstation they are plugged into. . The interface between each board and workstation is complicated, and always different for each"
H92-1049,H90-1003,1,0.810065,"Recognition Demonstrations Steve Austin, Rusty Bobrow, Dan Ellard, Robert Ingria, John Makhoul, Long Nguyen, Pat Peterson, Paul Placeway, Richard Schwartz BBN Systems and Technologies Cambridge MA 02138 Typically, real-time speech recognition - if achieved at all - is accomplished either by greatly simplifying the processing to be done, or by the use of special-purpose hardware. Each of these approaches has obvious problems. The former results in a substantial loss in accuracy, while the latter often results in obsolete hardware being developed at great expense and delay. Starting in 1990 [1] [2] we have taken a different approach based on modifying the algorithms to provide increased speed without loss in accuracy. Our goal has been to use commercially available off-the-shelf (COTS) hardware to perform speech recognition. Initially, this meant using workstations with powerful but standard signal processing boards acting as accelerators. However, even these signal processing boards have two significant disadvantages: 1. They often cost as much as the workstation they are plugged into. . The interface between each board and workstation is complicated, and always different for each comb"
H93-1015,H93-1003,0,0.0359805,"lgorithms that worked best on smaller vocabularies would not be the same ones that work best on larger vocabularies. We found that, while the required computation certainly increased, the programs that we had developed on the smaller problems still worked efficiently enough on the larger problems. However, while the BYBLOS system achieved the lowest word error rate obtained by any site for recognition of ATIS speech, the error rates for the WSJ tests were the second lowest of the six sites that tested their systems on this corpus. The reader will find more details on the evaluation results in [1]. In the sections that follow, we will describe the BBN BYBLOS system briefly. Then we enumerate several modifications to the BBN BYBLOS system. Following this we will describe four different experiments that we performed and the results obtained. 2. 3. Each of the N hypotheses is rescored with cross-wordboundary triphones and semi-continuous density HMMs. 4. The N-best list can be rescored with a trigram grammar (or any other language model). Each utterance is decoded with each gender-dependent model. For each utterance, the N-best list with the highest top-1 hypothesis score is chosen. The t"
H93-1015,H89-2027,1,0.863151,"ifications to the BBN BYBLOS system. Following this we will describe four different experiments that we performed and the results obtained. 2. 3. Each of the N hypotheses is rescored with cross-wordboundary triphones and semi-continuous density HMMs. 4. The N-best list can be rescored with a trigram grammar (or any other language model). Each utterance is decoded with each gender-dependent model. For each utterance, the N-best list with the highest top-1 hypothesis score is chosen. The top choice in the final list constitutes the speech recognition results reported below. This N-best strategy [3, 4] permits the use of otherwise computationally prohibitive models by greatly reducing the search space to a few (N=20-100) word sequences. It has enabled us to use cross-word-bonndary triphone models and trigram language models with ease. During most of the development of the system we used the 1000-Word RM cospus [8] for testing. More recently, the system has been used for recognizing spontaneous speech from the ATIS corpus, which contains many spontaneous speech effects, such as partial words, nonspeech sounds, extraneous.noises, false starts, etc. The vocabulary of the ATIS domain was about"
H93-1015,H93-1014,0,\N,Missing
H93-1018,H90-1003,1,0.845688,"ly. If we multiply the forward score for a path by the backward score of another path ending at the same frame, we have an estimate of the total score for the combined path, given the entire utterance. In a sense, the forward search provides the ideal fast match for the backward pass, in that it gives a good estimate of the score for each of the words that can follow in the backward direction, including the effect of all of the remaining speech. Since the development of the exact algorithm, there have been several approximations developed that are much faster, with varying degrees of accuracy [2, 3, 7, 8]. The most recent algorithm [9] empirically retains the accuracy of the exact algorithm, while requiring little more computation than that of a simple 1-best search. When we first introduced the FBS to speed up the N-best search algorithm, the model used in the forward and backward directions were identical. So the estimate of the backward scores provided by the forward pass were exact. This method has also been used in a best-first stack search [8], in which it is very effective, since the forward-backward score for any theory covers the whole utterance. The forwardbackward score solves the p"
H93-1018,H90-1004,0,0.023083,"ly. If we multiply the forward score for a path by the backward score of another path ending at the same frame, we have an estimate of the total score for the combined path, given the entire utterance. In a sense, the forward search provides the ideal fast match for the backward pass, in that it gives a good estimate of the score for each of the words that can follow in the backward direction, including the effect of all of the remaining speech. Since the development of the exact algorithm, there have been several approximations developed that are much faster, with varying degrees of accuracy [2, 3, 7, 8]. The most recent algorithm [9] empirically retains the accuracy of the exact algorithm, while requiring little more computation than that of a simple 1-best search. When we first introduced the FBS to speed up the N-best search algorithm, the model used in the forward and backward directions were identical. So the estimate of the backward scores provided by the forward pass were exact. This method has also been used in a best-first stack search [8], in which it is very effective, since the forward-backward score for any theory covers the whole utterance. The forwardbackward score solves the p"
H93-1018,H89-2027,1,\N,Missing
H93-1018,H93-1017,0,\N,Missing
H93-1018,H93-1016,0,\N,Missing
H94-1053,H91-1020,0,0.220199,"Missing"
H94-1053,J92-1004,0,\N,Missing
H94-1065,H93-1015,1,0.796216,"age, a rather expensive approach for real applications, or by training on a large number of microphones in the hope that the system will obtain the necessary robustness. • Use robust signal processing algorithms. • • Develop a feature transformation that maps the alternate microphone data to training microphone data. • Use statistical methods in order to adapt the parameters of the acoustic models. In previous work we had discussed the use of Cepstmm Mean Subtraction and the RASTA algorithm as two simple signal processing algorithms to compensate the degradation caused by an alternate channel [7]. In this pape r, we present an approach towards feature mapping by modeling the difference between the test and the training microphone, prior to recotion. We have developed the Tied-Mixture Normalization Algorithm, a technique for adaptation to a new microphone based on modifying the continuous densities in a tied-mixture I-IMM system, using a relatively small amount of stereo training speech. This method is presented in detail in Section 2. In Section 3 we describe several experiments on a known microphone task and the effect of the adaptation method in the performance of the recognition sy"
H94-1081,H91-1013,0,0.0337148,"Missing"
H94-1081,H89-2027,1,0.779598,"Dead? Long Nguyen, Richard Schwartz, Ying Zhao, George Zavaliagkost BBN Systems and Technologies Cambridge, MA 02138 tNortheastern University ABSTRACT We developed a faster search algorithm that avoids the use of the N-Best paradigm until after more powerful knowledge sources have been used. We found, however, that there was little or no decrease in word errors. We then showed that the use of the N-Best paradigm is still essential for the use of still more powerful knowledge sources, and for several other purposes that are outlined in the paper. 1. I N T R O D U C T I O N The N-Best Paradigm [1] was introduced originally as a means for integrating the speech recognition and language understanding components of a spoken language system. Since then, we have generalized its use for integrating into the recognition search other expensive knowledge sources (such as higher-order n-gram language models, betweenword co-articulation models, and segmental models) without increasing the search space [2]. The basic idea is that we use inexpensive knowledge sources to find N alternative sentence hypotheses. Then we rescore each of these hypotheses with the more expensive and more accurate knowled"
H94-1081,P93-1008,0,0.0664316,"radigm results in substantial search errors. If it does, then its use for the other purposes mentioned above would also be questionable. First we describe briefly how we used the N-Best paradigm in previous versions of BYBLOS. Then, we descfibe our attempts to avoid the errors that might be a result of using the N-Best paradigm. Finally, we argue that there will always be cases where the N-Best paradigm will make it possible to use some knowledge sources that would likely never be used otherwise. 2. 3 - P A S S N - B E S T SEARCH STRATEGY The BYBLOS system has been described previously (e.g., [3]). We reiterate here the use of the N-Best Paradigm in that system. The decoder used a 3-pass search strategy. The strategy used a forward pass followed by a backward Word-Dependent NBest search algorithm [4] using a bigram language model, within-word triphone models, and top-1 (discrete VQ) densities. The N-Best hypotheses were then rescored using crossword triphone context models, top-5 mixture densities, and trigram language model. Typically, the backward Word-Dependent N-Best pass requires about half the time required by the forward pass. Rescoring each alternative sentence hypothesis indi"
H94-1081,H93-1017,0,0.0612456,"test. This was because there were many sentences for which the correct answer was not in the N-Best hypotheses although it bad a higher total score (when including the trigram language model and cross-word triphones) than any sentence hypothesis in the N-Best list. We felt that this was due to the higher word error rate that resulted from recognition with a large vocabulary of 20,000 words, and the long utterances found in the Wall Street Journal (WSJ) corpus. Therefore, this year we implemented a more complicated search strategy similar to the Progressive-Search strategy suggested by Murveit [6] in which we use the initial passes to create a lattice of alternative hypotheses, which can then be rescored. The advantage of this approach is that a lattice with a small number of alternatives at each point can represent a very large number of alternative sentence hypotheses. In addition, rescoring the lattice of alternatives is computationally less expensive than rescoring a large explicit list of sentence alternatives. This also avoids the rather large intermediate storage required to store the N-Best hypotheses. 3.1. 4-Pass Lattice Search Algorithm In this section we describe a 4-Pass La"
H94-1081,H93-1018,1,\N,Missing
H94-1086,P93-1008,0,0.0328063,"tters (bilets) are modeled as well as three letter (trilet) contexts. In a given set of sentences there may be many tfilets, up to the number of symbols cubed. However, in English only a subset of these are allowed. In the ATIS task there are 3639 different trilets in the training sentences. LF_~__~R~ognition__~ Mat Likely Speech _[ F u t u r e Input r I Extraction ] Vectea's I Search Senten~ Figure 1: BYBLOS speech system. 3. A I R L I N E T R A V E L I N F O R M A T I O N SERVICE: AN INITIAL 3050 WORD, 52 SYMBOL TASK In the initial system, the BBN BYBLOS Continuous Speech Recognition system [4, 5, 6] (see Figure I) was used without modification on an on-line cursive handwriting corpus created from prompts from the ARPA Airline Travel Information Service (ATIS) corpus [7]. These full sentence prompts (approximately 10 words per sentence) were written by a single subject. These sentences were then reviewed (verified) to make sure that the prompts were transcribed correctly. After verification, these sentences were separated into a set of 381 training sentences and a mutually exclusive set of 94 test sentences. The lexicon for this task consisted of 3050 words, where lowercase and capitalize"
H94-1086,H92-1003,0,0.0252218,"only a subset of these are allowed. In the ATIS task there are 3639 different trilets in the training sentences. LF_~__~R~ognition__~ Mat Likely Speech _[ F u t u r e Input r I Extraction ] Vectea's I Search Senten~ Figure 1: BYBLOS speech system. 3. A I R L I N E T R A V E L I N F O R M A T I O N SERVICE: AN INITIAL 3050 WORD, 52 SYMBOL TASK In the initial system, the BBN BYBLOS Continuous Speech Recognition system [4, 5, 6] (see Figure I) was used without modification on an on-line cursive handwriting corpus created from prompts from the ARPA Airline Travel Information Service (ATIS) corpus [7]. These full sentence prompts (approximately 10 words per sentence) were written by a single subject. These sentences were then reviewed (verified) to make sure that the prompts were transcribed correctly. After verification, these sentences were separated into a set of 381 training sentences and a mutually exclusive set of 94 test sentences. The lexicon for this task consisted of 3050 words, where lowercase and capitalized versions of a word are considered distinct. For this initial system there were 54 characters: 52 lower and upper case alphabetic, a space character, and a ""backspace"" chara"
H94-1086,H92-1073,0,0.0416167,"tion was suspended at this point since so few errors did not allow any further analysis of the problems in our methods. The above experiments demonstrated the potential utility of speech recognition methods, especially the use of HMMs and grammars, to the problem of on-line cursive handwriting recognition. Based on these good preliminary results, we embarked on a more ambitious task with a larger vocabulary and more writers. 4. WALL STREET JOURNAL: A 25,000 WORD, 86 SYMBOL TASK During the past year, we have collected cursive written data using text from the ARPA Wall Street Journal task (WSJ) [10], including numerals, punctuation, and other symbols, for a total of 88 symbols (62 alphanumeric, 24 punctuation and special symbols, space, and backspace). The prompts from the Wall Street Journal consist mainly of full sentences with scattered article headings and stock listings (all are referred to as sentences for convenience). We have thus far collected over 7000 sentences (175,000 words total or about 25 words/sentence) from 21 writers on two GRiD Convertible pentops. See Figure 5 for an example of the data collected. The writers were gathered from the Cambridge, Massachusetts area and w"
J93-2006,P87-1005,1,0.704984,"Missing"
J93-2006,H89-1008,0,0.0358924,"Missing"
J93-2006,H90-1053,0,0.0115364,"(left-hand side) of the grammar, there is a set of context-free rules LHS LHS ,-*-- RHS1 RHS2 LHS ~ RHSn. For each rule, one estimates the probability of the right-hand side given the lefthand side, p(RHSj I LHS). With supervised training, where a set of correct parse trees is provided as training, one estimates p(RHSj I LHS) by the number of times rule LHS *---RHSj appears in the training set divided by the number of times LHS appears in the trees. The probability of a syntactic structure S, given the input string W, is then modeled by the product of the probabilities of the rules used in S. Chitrao and Grishman (1990) used a similar context-free model. Using this model, we explored the following issues: What method of training the rule probabilities should be employed? Our results were much more effective with supervised training, which explains w h y the model performed better in our experiments than Chitrao and Grishman found with unsupervised training. 370 Ralph Weischedel et at. Coping with Ambiguity and Unknown Words START I S /// / NP / / / / PRO V DET it is the  vP ORD N-BAR ORDDIGIT third / ~ N DET bid the .L ~ V P N V V companyhas gotten DET [ ] this N year Rules Used START --) S p(TREE[W) = !-~"
J93-2006,A88-1019,0,0.424873,"ould be diluted. Probability models could be employed where less knowledge was available. 3. Given the vocabulary size, we could not expect to give full syntactic or semantic features. The labor for handcrafted definitions would not be warranted. Statistical language models have a learning component that might supplement handcrafted knowledge. 4. Purely rule-based techniques seemed too brittle for dealing with the variety of constructions, the long sentences (averaging 29 words per sentence), and the degree of unexpected input. Statistical models based on local information (e.g., DeRose 1988; Church 1988) might operate effectively in spite of sentence length and unexpected input. To see whether our four hypotheses (in italics above) effectively addressed the four concerns above, we chose to test the hypotheses on two well-known problems: ambiguity (both at the structural level and at the part-of-speech level) and inferring syntactic and semantic information about unknown words. Guided by the past success of probabilistic models in speech processing, we have integrated probabilistic models into our language processing systems. Early speech research used purely knowledge-based approaches, analog"
J93-2006,P90-1031,0,0.121148,"Missing"
J93-2006,J88-1003,0,0.373827,"ted domains would be diluted. Probability models could be employed where less knowledge was available. 3. Given the vocabulary size, we could not expect to give full syntactic or semantic features. The labor for handcrafted definitions would not be warranted. Statistical language models have a learning component that might supplement handcrafted knowledge. 4. Purely rule-based techniques seemed too brittle for dealing with the variety of constructions, the long sentences (averaging 29 words per sentence), and the degree of unexpected input. Statistical models based on local information (e.g., DeRose 1988; Church 1988) might operate effectively in spite of sentence length and unexpected input. To see whether our four hypotheses (in italics above) effectively addressed the four concerns above, we chose to test the hypotheses on two well-known problems: ambiguity (both at the structural level and at the part-of-speech level) and inferring syntactic and semantic information about unknown words. Guided by the past success of probabilistic models in speech processing, we have integrated probabilistic models into our language processing systems. Early speech research used purely knowledge-based appr"
J93-2006,H89-2011,0,0.0290891,"Missing"
J93-2006,J86-3002,0,0.0502357,"Missing"
J93-2006,H90-1052,0,0.016576,"Missing"
J93-2006,H89-2014,0,0.0150366,"e associated with each algorithm. For example, in morphological processing in English (Section 2), the events are the use of a word with a particular part of speech in a string of words. At the level of syntax (Section 3), an event is the use of a particular structure; the model predicts what the most likely rule is given a particular situation. One can similarly use probabilities for assigning semantic structure (Section 4). We report in Section 2 on our experiments on the assignment of part of speech to words in text. The effectiveness of such models is well known (DeRose 1988; Church 1988; Kupiec 1989; Jelinek 1985), and they are currently in use in parsers (e.g. de Marcken 1990). Our work is an incremental improvement on these models in three ways: (1) Much less training data than theoretically required proved adequate; (2) we integrated a probabilistic model of word features to handle unknown words uniformly within the probabilistic model and measured its contribution; and (3) we have applied the forward-backward algorithm to accurately compute the most likely tag set. In Section 3, we demonstrate that probability models can improve the performance of knowledge-based syntactic and semant"
J93-2006,H89-2006,0,0.0214676,"corpora can effectively reduce the ambiguity in processing text and can be used to acquire lexical information from a corpus, by supplementing knowledge-based techniques. Based on the results of those experiments, we have constructed a new natural language system (PLUM)for extracting data from text, e.g., newswire text. 1. Introduction Natural language processing, and AI in general, have focused mainly on building rule-based systems with carefully handcrafted rules and domain knowledge. Our own natural language database query systems, JANUS (Weischedel et al. 1989), ParlanceTM, 1 and Delphi (Stallard 1989), have used these techniques quite successfully. However, as we move from the application of understanding database queries in limited domains to applications of processing open-ended text, we found challenges that questioned our previous assumptions and suggested probabilistic models instead. 1. We could no longer assume a limited vocabulary. Rather in the domain of terrorist incidents of the Third Message Understanding conference (MUC-3) (Sundheim 1991), roughly 20,000 vocabulary items appear in a corpus 430,000 words long. Additional text from that domain would undoubtedly contain new words"
J93-2006,M91-1001,0,0.0509857,"afted rules and domain knowledge. Our own natural language database query systems, JANUS (Weischedel et al. 1989), ParlanceTM, 1 and Delphi (Stallard 1989), have used these techniques quite successfully. However, as we move from the application of understanding database queries in limited domains to applications of processing open-ended text, we found challenges that questioned our previous assumptions and suggested probabilistic models instead. 1. We could no longer assume a limited vocabulary. Rather in the domain of terrorist incidents of the Third Message Understanding conference (MUC-3) (Sundheim 1991), roughly 20,000 vocabulary items appear in a corpus 430,000 words long. Additional text from that domain would undoubtedly contain new words. Probabilistic models offer a mathematically grounded, empirically based means of predicting the most likely interpretation. * BBN Systemsand Technologies,70 FawcettStreet, CambridgeMA 02138. t Sage Lab, RensselaerPolytechnicInstitute,TroyNY 12180. :~ComputerScienceDepartment, BowdoinCollege,BrunswickME 04011. 1 Parlance is a trademarkof BBN Systemsand Technologies. (~) 1993 Associationfor ComputationalLinguistics Computational Linguistics 2. Volume 19,"
J93-2006,H89-1013,1,0.818529,"vinced that probabilistic models based on annotated corpora can effectively reduce the ambiguity in processing text and can be used to acquire lexical information from a corpus, by supplementing knowledge-based techniques. Based on the results of those experiments, we have constructed a new natural language system (PLUM)for extracting data from text, e.g., newswire text. 1. Introduction Natural language processing, and AI in general, have focused mainly on building rule-based systems with carefully handcrafted rules and domain knowledge. Our own natural language database query systems, JANUS (Weischedel et al. 1989), ParlanceTM, 1 and Delphi (Stallard 1989), have used these techniques quite successfully. However, as we move from the application of understanding database queries in limited domains to applications of processing open-ended text, we found challenges that questioned our previous assumptions and suggested probabilistic models instead. 1. We could no longer assume a limited vocabulary. Rather in the domain of terrorist incidents of the Third Message Understanding conference (MUC-3) (Sundheim 1991), roughly 20,000 vocabulary items appear in a corpus 430,000 words long. Additional text from that"
J93-2006,J93-1005,0,\N,Missing
M98-1009,A97-1029,1,0.686516,"Missing"
M98-1009,P96-1025,0,0.020241,"win Lewis.” 5) Whenever a relation involves an entity that is not a direct descendant of that relation in the parse tree, semantic pointer labels are attached to all of the intermediate nodes. These labels serve to form a continuous chain between the relation and its argument. Figure 3 shows an augmented parse tree corresponding to the semantic annotation in Figure 2. Note that nodes with semantic labels ending in “-r” are MUC reportable names and descriptors. Statistical Model In SIFT’s statistical model, augmented parse trees are generated according to a process similar to that described in Collins (1996, 1997). For each constituent, the head is generated first, followed by the modifiers, which are generated from the head outward. Head words, along with their part-of-speech tags and features, are generated for each modifier as soon as the modifier is created. Word features are introduced primarily to help with unknown words, as in Weischedel et al. (1993). We illustrate the generation process by walking through a few of the steps of the parse shown in Figure 3. At each step in the process, a choice is made from a statistical distribution, with the probability of each possible selection depend"
M98-1009,P97-1003,0,0.031701,"Missing"
M98-1009,J93-2004,0,0.0384257,"aining program estimates the parameters of a unified statistical model that accounts for both syntax and semantics. Later, when presented with a new sentence, the search program explores the statistical model to find the most likely combined semantic and syntactic interpretation. syntactic annotations (Penn Treebank) semantic annotations training program training statistical model sentences decoding search program combined semanticsyntactic interpretations Figure 1: Block diagram of sentence-level model. Training Data Our source for syntactically annotated training data was the Penn Treebank (Marcus et al., 1993). Significantly, we do not require that syntactic annotations be from the same source, or cover the same domain, as the target task. For example, while the Penn Treebank consists of Wall Street Journal text, the target source for this evaluation was New York Times newswire. Similarly, although the Penn Treebank domain covers general and financial news, the target domain for this evaluation was space technology. The ability to use syntactic training from a different source and domain than the target is an important feature of our model. Since the Penn Treebank serves as our syntactically annota"
M98-1009,W97-0302,0,0.0350367,"Missing"
N04-4010,W99-0612,0,0.0357195,"Missing"
N04-4010,W03-1026,0,0.0267296,"Missing"
N04-4010,O03-5001,0,0.0577236,"Missing"
N04-4010,W02-2024,0,0.0232548,"Missing"
N04-4010,W03-0419,0,0.0212412,"Missing"
N04-4010,W03-0433,1,0.873511,"Missing"
N04-4010,C02-1012,0,\N,Missing
N04-4040,J95-4004,0,0.232512,"Missing"
N04-4040,N01-1016,0,0.135179,"of less frequently disfluent discourse markers by taking speaker style into account. 1 Introduction Disfluencies in human speech are widespread and cause problems for both downstream processing and human readability of speech transcripts. Recent human studies (Jones et al., 2003) have examined the effect of disfluencies on the readability of speech transcripts. These results suggest that the “cleaning” of text by removing disfluent words can increase the speed at which readers can process text. Recent work on detecting edits for use in parsing of speech transcripts (Core and Schubert, 1999), (Charniak and Johnson, 2001) has shown an improvement in the parser error rate by modeling disfluencies. Many researchers investigating disfluency detection have focused on the use of prosodic cues, as opposed to lexical features (Nakatani and Hirschberg, 1994). There are different approaches to detecting disfluencies. In one approach, one can first try to locate evidence of a general disfluency, e.g., using prosodic features or language model discontinuations. These locations are called interruption points (IPs). Following this, it is generally sufficient to look in the nearby vicinity of the IP to find the dis1 This wo"
N04-4040,J99-4003,0,0.0446307,"wartz@bbn.com fluent words. The most successful approaches so far combine the detection of IPs using prosodic features and language modeling techniques (Liu et al., 2003), (Shriberg et al., 2001), (Stolcke et al., 1998). Our work is based on the premise that the vast majority of disfluencies can be detected using primarily lexical features—specifically the words themselves and part-of-speech (POS) labels—without the use of extensive prosodic cues. Lexical modeling of disfluencies with only minimal acoustic cues has been shown to be successful in the past using strongly statistical techniques (Heeman and Allen, 1999). We shall discuss our algorithm and compare it to two other algorithms that make extensive use of acoustic features. Our algorithm performs comparably on most of the tasks assigned and in some cases outperforms systems that used both prosodic and lexical features. We discuss the task definition in Section 2. In Section 3 we describe our Transformation-Based Learning (TBL) algorithm and its associated features. Section 4 presents results for our system and two other systems that make heavy use of prosodic features to detect disfluencies. We then discuss the errors made by our system, in Sectio"
N04-4040,P83-1019,0,0.175893,"Missing"
N04-4040,W96-0213,0,0.0152166,"are our System A to two other systems that were designed for the same task, System B and System C. System C was only applied to conversational speech, so there are no results for it on broadcast news transcripts. Our system was also given the same speech recognition output as System C for the conversational speech condition, whereas System B used transcripts produced by a different speech recognition system. The rules learned by the system are conditioned on several features of each of the words including the lexeme (the word itself), a POS tag for the word, whether the 2 We use a POS tagger (Ratnaparkhi, 1996) trained on switchboard data with the additional tags of FP (filled pause) and FRAG (word fragment). System B used both prosodic cues and lexical information to detect disfluencies. The prosodic cues were modeled by a decision tree classifier, whereas the lexical information was modeled using a 4-gram language model, separately trained for both CTS and BNEWS. System C first inserts IPs into the text using a decisiontree classifier based on both prosodic and lexical features and then uses TBL. In addition to POS, System C’s feature set also includes whether the word is commonly used as a filler"
N07-1029,P05-1033,0,0.137728,"through this network. The alignment of speech recognition outputs is fairly straightforward due to the strict constraint in word order. However, machine translation outputs do not have this constraint as the word order may be different between the source and target languages. MT systems employ various re-ordering (distortion) models to take this into account. In recent years, machine translation systems based on new paradigms have emerged. These systems employ more than just the surface-level information used by the state-of-the-art phrase-based translation systems. For example, hierarchical (Chiang, 2005) and syntax-based (Galley et al., 2006) systems have recently improved in both accuracy and scalability. Three MT system combination methods are presented in this paper. They operate on the sentence, phrase and word level. The sentence-level combination is based on selecting the best hypothesis out of the merged N-best lists. This method does not generate new hypotheses – unlike the phrase and word-level methods. The phrase-level combination  228 Proceedings of NAACL HLT 2007, pages 228–235, c Rochester, NY, April 2007. 2007 Association for Computational Linguistics is based on extracting sen"
N07-1029,A94-1016,0,0.891074,". The sentence-level combination is based on selecting the best hypothesis out of the merged N-best lists. This method does not generate new hypotheses – unlike the phrase and word-level methods. The phrase-level combination  228 Proceedings of NAACL HLT 2007, pages 228–235, c Rochester, NY, April 2007. 2007 Association for Computational Linguistics is based on extracting sentence-specific phrase translation tables from system outputs with alignments to source and running a phrasal decoder with this new translation table. This approach is similar to the multi-engine MT framework proposed in (Frederking and Nirenburg, 1994) which is not capable of re-ordering. The word-level combination is based on consensus network decoding. Translation edit rate (TER) (Snover et al., 2006) is used to align the hypotheses and minimum Bayes risk decoding under TER (Sim et al., 2007) is used to select the alignment hypothesis. All combination methods use weights which may be tuned using Powell’s method (Brent, 1973) on -best lists. Both sentence and phrase-level combination methods can generate best lists which may also be used as new system outputs in the word-level combination. Experiments on combining six machine translation s"
N07-1029,P05-3026,0,0.607609,"ion Systems  Antti-Veikko I. Rosti and Necip Fazil Ayan and Bing Xiang and Spyros Matsoukas and Richard Schwartz and Bonnie J. Dorr BBN Technologies, 10 Moulton Street, Cambridge, MA 02138   arosti,bxiang,smatsouk,schwartz @bbn.com   Institute for Advanced Computer Studies, University of Maryland, College Park, MD 20742  nfa,bonnie @umiacs.umd.edu  Abstract Combined with the latest advances in phrase-based translation systems, it has become more attractive to take advantage of the various outputs in forming consensus translations (Frederking and Nirenburg, 1994; Bangalore et al., 2001; Jayaraman and Lavie, 2005; Matusov et al., 2006). Currently there are several approaches to machine translation (MT) based on different paradigms; e.g., phrasal, hierarchical and syntax-based. These three approaches yield similar translation accuracy despite using fairly different levels of linguistic knowledge. The availability of such a variety of systems has led to a growing interest toward finding better translations by combining outputs from multiple systems. This paper describes three different approaches to MT system combination. These combination methods operate on sentence, phrase and word level exploiting in"
N07-1029,koen-2004-pharaoh,0,0.0363196,"imilarity score weights. The parameters  through L interpolate between the sum, average and maximum of the similarity scores. These interpolation weights and the system weights  are constrained to sum to one. The number of tunable combination weights, in ad  + dition to normal decoder weights, is  where  is the number of systems and is the + number of similarity levels; i.e.,  free system similarity score weights and two free inweights, terpolation weights.       4.2 Phrase-Based Decoding The phrasal decoder used in the phrase-level combination is based on standard beam search (Koehn, 2004). The decoder features are: a trigram language model score, number of target phrases, number of target words, phrase distortion, phrase distortion computed over the original translations and phrase translation confidences estimated in Section 4.1. The total score for a hypothesis is computed as a log-linear combination of these features. The feature weights and combination weights (system and similarity) may be tuned using Powell’s method on -best lists as described in Section 2. The phrase-level combination tuning can be summarized as follows:  1. Estimate sentence posteriors given the total"
N07-1029,E06-1005,0,0.801984,"I. Rosti and Necip Fazil Ayan and Bing Xiang and Spyros Matsoukas and Richard Schwartz and Bonnie J. Dorr BBN Technologies, 10 Moulton Street, Cambridge, MA 02138   arosti,bxiang,smatsouk,schwartz @bbn.com   Institute for Advanced Computer Studies, University of Maryland, College Park, MD 20742  nfa,bonnie @umiacs.umd.edu  Abstract Combined with the latest advances in phrase-based translation systems, it has become more attractive to take advantage of the various outputs in forming consensus translations (Frederking and Nirenburg, 1994; Bangalore et al., 2001; Jayaraman and Lavie, 2005; Matusov et al., 2006). Currently there are several approaches to machine translation (MT) based on different paradigms; e.g., phrasal, hierarchical and syntax-based. These three approaches yield similar translation accuracy despite using fairly different levels of linguistic knowledge. The availability of such a variety of systems has led to a growing interest toward finding better translations by combining outputs from multiple systems. This paper describes three different approaches to MT system combination. These combination methods operate on sentence, phrase and word level exploiting information from -best li"
N07-1029,J03-1002,0,0.00618995,"mber of features; i.e., $  system score scaling factors (  ), three free interpolation weights (Equation 4) for the scaling factor estimation,   GLM weights (  ), three free interpolation weights (Equation 4) for the hypothesis confidence estimation and two free LM re-scoring weights (Equation 6). All parameters may be tuned using Powell’s method on -best lists as described in Section 2. The tuning of the sentence-level combination method may be summarized as follows:     individual systems. If the alignments are not available, they can be automatically generated; e.g., using GIZA++ (Och and Ney, 2003). The phrase translation table is generated for each source sentence using confidence scores derived from sentence posteriors with system-specific total score scaling factors and similarity scores based on the agreement among the phrases from all systems. 4.1 Phrase Confidence Estimation   Each phrase has an initial confidence based on the sentence posterior  estimated from an -best list in the same fashion as in Section 3.2. The confidence of the phrase table entry is increased if several systems agree on the target words. The agreement is measured by four levels of similarity:  1. Same"
N07-1029,P03-1021,0,0.0903169,"een a system output and a targeted reference which preserves the meaning and fluency of the sentence (Snover et al., 2006). The targeted reference is generated by human posteditors who make edits to a reference translation so as to minimize the TER between the reference and 229  the vectors is gradually moved toward a larger positive change in the evaluation metric. To improve the chances of finding a global optimum, the algorithm is repeated with varying initial values. The modified Powell’s method has been previously used in optimizing the weights of a standard feature-based MT decoder in (Och, 2003) where a more efficient algorithm for log-linear models was proposed. However, this is specific to log-linear models and cannot be easily extended for more complicated functions. scaling factors were estimated from the tuning data , + to limit the feature values between   . The same scaling factors have to be applied to the features obtained from the test data. The total confidence score of hypothesis  is obtained from the system confidences  as + @   8      L      BED       &quot;! + @    BED  (4) where  is the number of systems generating the hypo"
N07-1029,P02-1040,0,0.112333,"Missing"
N07-1029,2006.amta-papers.25,1,0.904798,"phrase and word-level methods. The phrase-level combination  228 Proceedings of NAACL HLT 2007, pages 228–235, c Rochester, NY, April 2007. 2007 Association for Computational Linguistics is based on extracting sentence-specific phrase translation tables from system outputs with alignments to source and running a phrasal decoder with this new translation table. This approach is similar to the multi-engine MT framework proposed in (Frederking and Nirenburg, 1994) which is not capable of re-ordering. The word-level combination is based on consensus network decoding. Translation edit rate (TER) (Snover et al., 2006) is used to align the hypotheses and minimum Bayes risk decoding under TER (Sim et al., 2007) is used to select the alignment hypothesis. All combination methods use weights which may be tuned using Powell’s method (Brent, 1973) on -best lists. Both sentence and phrase-level combination methods can generate best lists which may also be used as new system outputs in the word-level combination. Experiments on combining six machine translation system outputs were performed. Three systems were phrasal, two hierarchical and one syntaxbased. The systems were evaluated on NIST MT05 and the newsgroup"
N07-1029,P06-1121,0,\N,Missing
N12-1006,W05-0909,0,0.0349848,"Missing"
N12-1006,W10-0701,1,0.675469,"Missing"
N12-1006,E06-1047,0,0.780602,"Missing"
N12-1006,N09-1025,0,0.0340312,"tuning, and we only varied the corpora used for training, tuning and testing. The MT system we used is based on a phrase-based hierarchical model similar to that of Shen et al. (2008). We used GIZA++ (Och and Ney, 2003) to align sentences and extract hierarchical rules. The decoder used a log-linear model that combines the scores of multiple feature scores, including translation probabilities, smoothed lexical probabilities, a dependency tree language model, in addition to a trigram English language model. Additionally, we used 50,000 sparse, binary-valued source and target features based on Chiang et al. (2009). The English language model was trained on 7 billion words from the Gigaword and from a web crawl. The feature weights were tuned to maximize the BLEU score on a tuning set using the ExpectedBLEU optimization procedure (Devlin, 2009). The Dialectal Arabic side of our corpus consisted of 1.5M words (1.1M Levantine and 380k Egyptian). Table 2 gives statistics about the various train/tune/test splits we used in our experiments. Since the Egyptian set was so small, we split it only to training/test sets, opting not to have a tuning set. The MSA training data we used consisted of ArabicEnglish cor"
N12-1006,P05-1071,0,0.186427,"e, 2004; Habash and Sadat, 2006) by decreasing the size of the source vocabulary, and improving the quality of word alignments. The morphological analyzers that underlie most segmenters were developed for MSA, but the different dialects of Arabic share many of the morphological affixes of MSA, and it is therefore not unreasonable to expect MSA segmentation to also improve Dialect Arabic to English MT. To test this, 3 We also computed TER (Snover et al., 2006) and METEOR scores, but omit them because they demonstrated similar trends. 53 we ran experiments using the MADA morphological analyzer (Habash and Rambow, 2005). Table 3 shows the effect of applying segmentation to the text, for both MSA and Dialectal Arabic. The BLEU score improves uniformly, although the improvements are most dramatic for smaller datasets, which is consistent with previous work (Habash and Sadat, 2006). Morphological segmentation gives a smaller gain on dialectal input, which could be due to two factors: the segmentation accuracy likely decreases since we are using an unmodified MSA segmenter, and there is higher variability in the written form of dialect compared to MSA. Given the significant, albeit smaller gain on dialectal inpu"
N12-1006,P06-1086,0,0.552091,"Missing"
N12-1006,N06-2013,0,0.0454305,"achieved using the full set of dialectal data (which combines Levantine and Egyptian), since the Egyptian alone is sparse. For Levantine, adding Egyptian has no effect. In both cases, adding MSA to the dialectal data results in marginally worse translations. score.3 In addition, we also report the OOV rate of the test set relative to the training corpus in each experimental setups. 4.1 Morphological Decomposition Arabic has a complex morphology compared to English. Preprocessing the Arabic source by morphological segmentation has been shown to improve the performance of Arabic MT (Lee, 2004; Habash and Sadat, 2006) by decreasing the size of the source vocabulary, and improving the quality of word alignments. The morphological analyzers that underlie most segmenters were developed for MSA, but the different dialects of Arabic share many of the morphological affixes of MSA, and it is therefore not unreasonable to expect MSA segmentation to also improve Dialect Arabic to English MT. To test this, 3 We also computed TER (Snover et al., 2006) and METEOR scores, but omit them because they demonstrated similar trends. 53 we ran experiments using the MADA morphological analyzer (Habash and Rambow, 2005). Table"
N12-1006,N04-4015,0,0.0397882,"scores are achieved using the full set of dialectal data (which combines Levantine and Egyptian), since the Egyptian alone is sparse. For Levantine, adding Egyptian has no effect. In both cases, adding MSA to the dialectal data results in marginally worse translations. score.3 In addition, we also report the OOV rate of the test set relative to the training corpus in each experimental setups. 4.1 Morphological Decomposition Arabic has a complex morphology compared to English. Preprocessing the Arabic source by morphological segmentation has been shown to improve the performance of Arabic MT (Lee, 2004; Habash and Sadat, 2006) by decreasing the size of the source vocabulary, and improving the quality of word alignments. The morphological analyzers that underlie most segmenters were developed for MSA, but the different dialects of Arabic share many of the morphological affixes of MSA, and it is therefore not unreasonable to expect MSA segmentation to also improve Dialect Arabic to English MT. To test this, 3 We also computed TER (Snover et al., 2006) and METEOR scores, but omit them because they demonstrated similar trends. 53 we ran experiments using the MADA morphological analyzer (Habash"
N12-1006,J03-1002,0,0.00335263,"tal cost was $44k, or $0.03/word – an order of magnitude cheaper than professional translation. 4 Experiments in Dialectal Arabic-English Machine Translation We performed a set of experiments to contrast systems trained using our dialectal parallel corpus with systems trained on a (much larger) MSA-English parallel corpus. All experiments use the same methods for training, decoding and parameter tuning, and we only varied the corpora used for training, tuning and testing. The MT system we used is based on a phrase-based hierarchical model similar to that of Shen et al. (2008). We used GIZA++ (Och and Ney, 2003) to align sentences and extract hierarchical rules. The decoder used a log-linear model that combines the scores of multiple feature scores, including translation probabilities, smoothed lexical probabilities, a dependency tree language model, in addition to a trigram English language model. Additionally, we used 50,000 sparse, binary-valued source and target features based on Chiang et al. (2009). The English language model was trained on 7 billion words from the Gigaword and from a web crawl. The feature weights were tuned to maximize the BLEU score on a tuning set using the ExpectedBLEU opt"
N12-1006,P02-1040,0,0.104675,"Missing"
N12-1006,2006.amta-papers.21,0,0.538693,"Missing"
N12-1006,W11-2602,0,0.322886,"le 6). 4.5 Mapping from Dialectal Arabic to MSA Before Translating to English Given the large amount of linguistic resources that have been developed for MSA over the past years, and the extensive research that was conducted on machine translation from MSA to English and other languages, an obvious research question is whether Dialectal Arabic is best translated to English by first pivoting through MSA, rather than directly. The proximity of Dialectal Arabic to MSA makes the mapping in principle easier than general machine translation, and a number of researchers have explored this direction (Salloum and Habash, 2011). In this scenario, the dialectal source would first be automatically transformed to MSA, using either a rule-based or statistical mapping module. The Dialectal Arabic-English parallel corpus we created presents a unique opportunity to compare the MSA-pivoting approach against direct translation. First, we collected equivalent MSA data for the Levantine Web test and tuning sets, by asking Turkers to transform dialectal passages to valid and fluent MSA. Turkers were shown example transformations, and we encouraged fewer changes where applicable (e.g. morphological rather than lexical mapping),"
N12-1006,2010.amta-papers.5,0,0.238646,"Missing"
N12-1006,P08-1066,0,0.0165598,"creating our parallel corpus. The total cost was $44k, or $0.03/word – an order of magnitude cheaper than professional translation. 4 Experiments in Dialectal Arabic-English Machine Translation We performed a set of experiments to contrast systems trained using our dialectal parallel corpus with systems trained on a (much larger) MSA-English parallel corpus. All experiments use the same methods for training, decoding and parameter tuning, and we only varied the corpora used for training, tuning and testing. The MT system we used is based on a phrase-based hierarchical model similar to that of Shen et al. (2008). We used GIZA++ (Och and Ney, 2003) to align sentences and extract hierarchical rules. The decoder used a log-linear model that combines the scores of multiple feature scores, including translation probabilities, smoothed lexical probabilities, a dependency tree language model, in addition to a trigram English language model. Additionally, we used 50,000 sparse, binary-valued source and target features based on Chiang et al. (2009). The English language model was trained on 7 billion words from the Gigaword and from a web crawl. The feature weights were tuned to maximize the BLEU score on a t"
N12-1006,2006.amta-papers.25,1,0.697297,"plex morphology compared to English. Preprocessing the Arabic source by morphological segmentation has been shown to improve the performance of Arabic MT (Lee, 2004; Habash and Sadat, 2006) by decreasing the size of the source vocabulary, and improving the quality of word alignments. The morphological analyzers that underlie most segmenters were developed for MSA, but the different dialects of Arabic share many of the morphological affixes of MSA, and it is therefore not unreasonable to expect MSA segmentation to also improve Dialect Arabic to English MT. To test this, 3 We also computed TER (Snover et al., 2006) and METEOR scores, but omit them because they demonstrated similar trends. 53 we ran experiments using the MADA morphological analyzer (Habash and Rambow, 2005). Table 3 shows the effect of applying segmentation to the text, for both MSA and Dialectal Arabic. The BLEU score improves uniformly, although the improvements are most dramatic for smaller datasets, which is consistent with previous work (Habash and Sadat, 2006). Morphological segmentation gives a smaller gain on dialectal input, which could be due to two factors: the segmentation accuracy likely decreases since we are using an unmod"
N12-1006,P11-2007,1,0.80814,"nizing Arabic text with a focus on Dialectal Arabic. For example, MAGEAD (Habash and Rambow, 2006) is a morphological analyzer and generator that can analyze the surface form of MSA and dialect words into 50 their root/pattern and affixed morphemes, or generate the surface form in the opposite direction. Amazon’s Mechanical Turk (MTurk) is becoming an essential tool for creating annotated resources for computational linguistics. Callison-Burch and Dredze (2010) provide an overview of various tasks for which MTurk has been used, and offer a set of best practices for ensuring high-quality data. Zaidan and Callison-Burch (2011a) studied the quality of crowdsourced translations, by quantifying the quality of non-professional English translations of 2,000 Urdu sentences that were originally translated by the LDC. They demonstrated a variety of mechanisms that increase the translation quality of crowdsourced translations to near professional levels, with a total cost that is less than one tenth the cost of professional translation. Zaidan and Callison-Burch (2011b) created the Arabic Online Commentary (AOC) dataset, a 52Mword monolingual dataset rich in dialectal content. Over 100k sentences from the AOC were annotate"
N12-1006,P11-1122,1,0.553138,"Missing"
N13-1069,W10-0701,0,0.261316,"Missing"
N13-1069,N09-1025,0,0.0511387,"Missing"
N13-1069,N03-1017,0,0.0201457,"Missing"
N13-1069,W07-0716,0,0.0671596,"ced translations for training or parameter tuning. Zbib et al. (2012) trained a Dialectal Arabic to English MT system using Mechanical Turk translations. But the data they translated on MTurk does not have professional translations to conduct the systematic comparison we do in this paper. It is well known that scoring MT output against multiple references improves MT scores such as BLEU significantly, since it increases the chance of matching n-grams between the MT output and the references. Tuning system parameter with multiple references also improves machine translation for the same reason Madnani et al. (2007) and Madnani et al. (2008) showed that tuning on additional references obtained by automatic paraphrasing helps when only few tuning references are available. 3 Data Translation The data we used are Arabic-English parallel corpora released by the LDC for the DARPA BOLT Phase 1 program1 . The data was collected from Egyptian online discussion forums, and consists of separate discussion threads, each composed of an initial user posting and multiple reply postings. The data tends to be bimodal: the first posting in the thread is often formal and expressed in Modern Standard Arabic, while the subs"
N13-1069,2008.amta-papers.13,1,0.799125,"ing or parameter tuning. Zbib et al. (2012) trained a Dialectal Arabic to English MT system using Mechanical Turk translations. But the data they translated on MTurk does not have professional translations to conduct the systematic comparison we do in this paper. It is well known that scoring MT output against multiple references improves MT scores such as BLEU significantly, since it increases the chance of matching n-grams between the MT output and the references. Tuning system parameter with multiple references also improves machine translation for the same reason Madnani et al. (2007) and Madnani et al. (2008) showed that tuning on additional references obtained by automatic paraphrasing helps when only few tuning references are available. 3 Data Translation The data we used are Arabic-English parallel corpora released by the LDC for the DARPA BOLT Phase 1 program1 . The data was collected from Egyptian online discussion forums, and consists of separate discussion threads, each composed of an initial user posting and multiple reply postings. The data tends to be bimodal: the first posting in the thread is often formal and expressed in Modern Standard Arabic, while the subsequent threads use a less"
N13-1069,J03-1002,0,0.0109205,"Missing"
N13-1069,P02-1040,0,0.0993694,"Web-forum 0KW 100KW 200KW 400KW 22.82 24.05 24.85 25.19 22.82 23.79 24.20 24.51 22.82 24.26 25.19 25.38 22.82 24.31 25.16 Table 1: Comparison of the effect of web forum training data when using professional and MTurk reference translations. All results use professional references for the tuning and test sets. two versions of each set: one with the professional reference translations for the target, and the other with the same source data, but the MTurk translations. We defined two versions of the test and tuning sets similarly. We report translation results in terms of lower-case BLEU scores (Papineni et al., 2002). 4.1 Training Data References We first study the effect of training data references, varying the amount of training data and type of translations, while using the same professional translation references for tuning and scoring. The first set of baseline experiments were trained on web forum data only, using professional translations. The first line of Table 1 shows that doubling of the training data adds 2.5 then 2.3 BLEU points. We repeated the experiments, but with MTurk training references, and saw that the scores are lower by 1.32.5 BLEU points, depending on the size of training data, and"
N13-1069,P08-1066,0,0.0448893,"Missing"
N13-1069,P11-2007,0,0.0144577,"ference translations of four Arabic-English parallel corpora previously released by the Linguistic Data Consortium (LDC) • A second translation of the development set obtained via MTurk improves parameter tuning and output evaluation. 2 Previous Work There have been several publications on crowdsourcing data annotation for NLP. Callison-Burch and Dredze (2010) give an overview of the NAACL2010 Workshop on using Mechanical Turk for data annotation. They describe tasks for which MTurk can be used, and summarize a set of best practices. They also include references to the workshop contributions. Zaidan and Callison-Burch (2011a) created a monolingual Arabic data set rich in dialectal content from user commentaries on newspaper websites. They hired native Arabic speakers on MTurk 612 Proceedings of NAACL-HLT 2013, pages 612–616, c Atlanta, Georgia, 9–14 June 2013. 2013 Association for Computational Linguistics to identify the dialect level and used the collected labels to train automatic dialect identification systems. They did not translate the collected data, however. Zaidan and Callison-Burch (2011b) obtained multiple translations of the NIST 2009 Urdu-English evaluation set using MTurk. They trained a statistica"
N13-1069,P11-1122,0,0.180064,"Missing"
N13-1069,N12-1006,1,0.834416,"Zaidan and Callison-Burch (2011b) obtained multiple translations of the NIST 2009 Urdu-English evaluation set using MTurk. They trained a statistical model on a set of features to select among the multiple translations. They showed that the MTurk translations selected by their model approached the range of quality of professional translations, and that the selected MTurk translations can be used reliably to score the outputs of different MT systems submitted to the NIST evaluation. Unlike our work, they did not investigate the use of crowdsourced translations for training or parameter tuning. Zbib et al. (2012) trained a Dialectal Arabic to English MT system using Mechanical Turk translations. But the data they translated on MTurk does not have professional translations to conduct the systematic comparison we do in this paper. It is well known that scoring MT output against multiple references improves MT scores such as BLEU significantly, since it increases the chance of matching n-grams between the MT output and the references. Tuning system parameter with multiple references also improves machine translation for the same reason Madnani et al. (2007) and Madnani et al. (2008) showed that tuning on"
P07-1040,koen-2004-pharaoh,0,\N,Missing
P07-1040,H91-1013,0,\N,Missing
P07-1040,A94-1016,0,\N,Missing
P07-1040,E06-1005,0,\N,Missing
P07-1040,P02-1040,0,\N,Missing
P07-1040,W05-0909,0,\N,Missing
P07-1040,P05-1033,0,\N,Missing
P07-1040,J03-1002,0,\N,Missing
P07-1040,N07-1029,1,\N,Missing
P07-1040,P06-1121,0,\N,Missing
P07-1040,P03-1021,0,\N,Missing
P14-1129,D13-1106,0,0.772389,"Missing"
P14-1129,N09-1025,0,0.363403,"Missing"
P14-1129,J07-2003,0,0.0252724,"ior of our baseline features, as we show in the next section. 6.2 “Simple Hierarchical” NIST Results The baseline used in the last section is a highlyengineered research system, which uses a wide array of features that were refined over a number of years, and some of which require linguistic resources. Because of this, the baseline BLEU scores are much higher than a typical MT system – especially a real-time, production engine which must support many language pairs. Therefore, we also present results using a simpler version of our decoder which emulates Chiang’s original Hiero implementation (Chiang, 2007). Specifically, this means that we don’t use dependency-based rule extraction, and our decoder only contains the following MT features: (1) rule probabilities, (2) n-gram Kneser-Ney LM, (3) lexical smoothing, (4) target word count, (5) concat rule penalty. Results are shown in the third section of Table 3. The “Simple Hierarchical” Arabic-English system is -6.4 BLEU worse than our strong baseline, and would have ranked 10th place out of 11 systems in the evaluation. When the NNJM features are added to this system, we see an improvement of +6.3 BLEU, which would have ranked 1st place in the eva"
P14-1129,N12-1059,1,0.664043,"Missing"
P14-1129,N13-1044,0,0.0372017,"esc) 52.2 + T2S/L2R NNJM (Resc) 52.3 + T2S/R2L NNJM (Resc) 52.8 “Simple Hier.” Baseline 43.4 + S2T/L2R NNJM (Dec) 47.2 + S2T NNLTM (Dec) 48.5 + Other NNJMs (Resc) 49.7 We also perform 1000-best rescoring with the following features: • 5-gram Kneser-Ney LM • Recurrent neural network language model (RNNLM) (Mikolov et al., 2010) Although we consider the RNNLM to be part of our baseline, we give it special treatment in the results section because we would expect it to have the highest overlap with our NNJM. 5.2 Training and Optimization For Arabic word tokenization, we use the MADAARZ tokenizer (Habash et al., 2013) for the BOLT condition, and the Sakhr9 tokenizer for the NIST condition. For Chinese tokenization, we use a simple longest-match-first lexicon-based approach. For word alignment, we align all of the training data with both GIZA++ (Och and Ney, 2003) and NILE (Riesa et al., 2011), and concatenate the corpora together for rule extraction. For MT feature weight optimization, we use iterative k-best optimization with an ExpectedBLEU objective function (Rosti et al., 2010). 6 Experimental Results We present MT primary results on Arabic-English and Chinese-English for the NIST OpenMT12 and DARPA BO"
P14-1129,D13-1053,1,0.625933,"Missing"
P14-1129,D13-1176,0,0.682331,"Missing"
P14-1129,N12-1005,0,0.506559,"ls were primarily used to create n-gram neural network language models (NNLMs) for speech recognition and machine translation (Bengio et al., 2003; Schwenk, 2010). They have since been extended to translation modeling, parsing, and many other NLP tasks. In this paper we use a basic neural network architecture and a lexicalized probability model to create a powerful MT decoding feature. Specifically, we introduce a novel formulation for a neural network joint model (NNJM), which augments an n-gram target language model with an m-word source window. Unlike previous approaches to joint modeling (Le et al., 2012), our feature can be easily integrated into any statistical machine translation (SMT) decoder, which leads to substantially larger improvements than k-best rescoring only. Additionally, we present several variations of this model which provide significant additive BLEU gains. We also present a novel technique for training the neural network to be self-normalized, which avoids the costly step of posteriorizing over the entire vocabulary in decoding. When used in conjunction with a pre-computed hidden layer, these techniques speed up NNJM computation by a factor of 10,000x, with only a small red"
P14-1129,J06-4004,0,0.516283,"Missing"
P14-1129,N13-1090,0,0.0262738,"Missing"
P14-1129,J03-1002,0,0.0112043,"LM • Recurrent neural network language model (RNNLM) (Mikolov et al., 2010) Although we consider the RNNLM to be part of our baseline, we give it special treatment in the results section because we would expect it to have the highest overlap with our NNJM. 5.2 Training and Optimization For Arabic word tokenization, we use the MADAARZ tokenizer (Habash et al., 2013) for the BOLT condition, and the Sakhr9 tokenizer for the NIST condition. For Chinese tokenization, we use a simple longest-match-first lexicon-based approach. For word alignment, we align all of the training data with both GIZA++ (Och and Ney, 2003) and NILE (Riesa et al., 2011), and concatenate the corpora together for rule extraction. For MT feature weight optimization, we use iterative k-best optimization with an ExpectedBLEU objective function (Rosti et al., 2010). 6 Experimental Results We present MT primary results on Arabic-English and Chinese-English for the NIST OpenMT12 and DARPA BOLT conditions. We also present a set of auxiliary results in order to further analyze our features. 6.1 9 Table 3: Primary results on Arabic-English and Chinese-English NIST MT12 Test Set. The first section corresponds to the top and bottom ranked sy"
P14-1129,D11-1046,0,0.0133144,"k language model (RNNLM) (Mikolov et al., 2010) Although we consider the RNNLM to be part of our baseline, we give it special treatment in the results section because we would expect it to have the highest overlap with our NNJM. 5.2 Training and Optimization For Arabic word tokenization, we use the MADAARZ tokenizer (Habash et al., 2013) for the BOLT condition, and the Sakhr9 tokenizer for the NIST condition. For Chinese tokenization, we use a simple longest-match-first lexicon-based approach. For word alignment, we align all of the training data with both GIZA++ (Och and Ney, 2003) and NILE (Riesa et al., 2011), and concatenate the corpora together for rule extraction. For MT feature weight optimization, we use iterative k-best optimization with an ExpectedBLEU objective function (Rosti et al., 2010). 6 Experimental Results We present MT primary results on Arabic-English and Chinese-English for the NIST OpenMT12 and DARPA BOLT conditions. We also present a set of auxiliary results in order to further analyze our features. 6.1 9 Table 3: Primary results on Arabic-English and Chinese-English NIST MT12 Test Set. The first section corresponds to the top and bottom ranked systems from the evaluation, and"
P14-1129,W10-1748,1,0.480231,"the highest overlap with our NNJM. 5.2 Training and Optimization For Arabic word tokenization, we use the MADAARZ tokenizer (Habash et al., 2013) for the BOLT condition, and the Sakhr9 tokenizer for the NIST condition. For Chinese tokenization, we use a simple longest-match-first lexicon-based approach. For word alignment, we align all of the training data with both GIZA++ (Och and Ney, 2003) and NILE (Riesa et al., 2011), and concatenate the corpora together for rule extraction. For MT feature weight optimization, we use iterative k-best optimization with an ExpectedBLEU objective function (Rosti et al., 2010). 6 Experimental Results We present MT primary results on Arabic-English and Chinese-English for the NIST OpenMT12 and DARPA BOLT conditions. We also present a set of auxiliary results in order to further analyze our features. 6.1 9 Table 3: Primary results on Arabic-English and Chinese-English NIST MT12 Test Set. The first section corresponds to the top and bottom ranked systems from the evaluation, and are taken from the NIST website. The second section corresponds to results on top of our strongest baseline. The third section corresponds to results on top of a simpler baseline. Within each"
P14-1129,W10-3815,0,0.0589328,"T program (Approved for Public Release, Distribution Unlimited). The views, opinions, and/or findings contained in this article are those of the author and should not be interpreted as representing the official views or policies, either expressed or implied, of the Defense Advanced Research Projects Agency or the Department of Defense. Introduction In recent years, neural network models have become increasingly popular in NLP. Initially, these models were primarily used to create n-gram neural network language models (NNLMs) for speech recognition and machine translation (Bengio et al., 2003; Schwenk, 2010). They have since been extended to translation modeling, parsing, and many other NLP tasks. In this paper we use a basic neural network architecture and a lexicalized probability model to create a powerful MT decoding feature. Specifically, we introduce a novel formulation for a neural network joint model (NNJM), which augments an n-gram target language model with an m-word source window. Unlike previous approaches to joint modeling (Le et al., 2012), our feature can be easily integrated into any statistical machine translation (SMT) decoder, which leads to substantially larger improvements th"
P14-1129,C12-2104,0,0.367884,"Missing"
P14-1129,J10-4005,0,0.0126491,"as a single concatenated token. Formally, the probability model is: |S| Πi=1 P (tsi |si , si−1 , si+1 , · · · ) This model is trained and evaluated like our NNJM. It is easy and computationally inexpensive to use this model in decoding, since only one neural network computation must be made for each source word. In rescoring, we also use a T2S NNLTM model computed over every target word: |T | Πi=1 P (sti |ti , ti−1 , ti+1 , · · · ) 5 MT System In this section, we describe the MT system used in our experiments. 5.1 MT Decoder We use a state-of-the-art string-to-dependency hierarchical decoder (Shen et al., 2010). Our baseline decoder contains a large and powerful set of features, which include: T2S/L2R , ta0i , ta0i −1 , ta0i +1 , · · · ) |S| Πi=1 P (si |si−1 , si−2 , · · · T2S/R2L |S| Πi=1 P (si |si+1 , si+2 , · · · , ta0i , ta0i −1 , ta0i +1 , · · · ) where a0i is the target-to-source affiliation, defined analogously to ai . The T2S variations cannot be used in decoding due to the large target context required, and are thus only used in k-best rescoring. The S2T/R2L 1374 • • • • • • • • Forward and backward rule probabilities 4-gram Kneser-Ney LM Dependency LM (Shen et al., 2010) Contextual lexical"
P14-1129,D08-1090,1,0.567711,"Missing"
P14-1129,D13-1023,0,0.00907149,"e 2: Speed of the neural network computation on a single CPU thread. “lookups/sec” is the number of unique n-gram probabilities that can be computed per second. “sec/word” is the amortized cost of unique NNJM lookups in decoding, per source word. Table 2 shows the speed of self-normalization and pre-computation for the NNJM. The decoding cost is based on a measurement of ∼1200 unique NNJM lookups per source word for our ArabicEnglish system.8 By combining self-normalization and precomputation, we can achieve a speed of 1.4M lookups/second, which is on par with fast backoff LM implementations (Tanaka et al., 2013). We demonstrate in Section 6.6 that using the selfnormalized/pre-computed NNJM results in only a very small BLEU degradation compared to the standard NNJM. 3 Decoding with the NNJM Because our NNJM is fundamentally an n-gram NNLM with additional source context, it can easily be integrated into any SMT decoder. In this section, we describe the considerations that must be taken when integrating the NNJM into a hierarchical decoder. 6 tanh() is implemented using a lookup table. 3500 ≈ 5 × 512 + 2 × 513; 2.8M ≈ 2 × 2689 × 512 + 2 × 513; 35M ≈ 2 × 2689 × 512 + 2 × 513 × 32000. For the sake of a fa"
P14-1129,D13-1140,0,0.825958,"r training objective function: X  L = log(P (xi )) − α(log(Z(xi )) − 0)2 i X  = log(P (xi )) − α log2 (Z(xi )) i In this case, the output layer bias weights are initialized to log(1/|V |), so that the initial network is self-normalized. At decode time, we simply use Ur (x) as the feature score, rather than log(P (x)). For our NNJM architecture, selfnormalization increases the lookup speed during decoding by a factor of ∼15x. Table 1 shows the neural network training results with various values of the free parameter α. In all subsequent MT experiments, we use α = 10−1 . We should note that Vaswani et al. (2013) implements a method called Noise Contrastive Estimation (NCE) that is also used to train selfnormalized NNLMs. Although NCE results in faster training time, it has the downside that there 1372 α 0 10−2 10−1 1 Arabic BOLT Val log(P (x)) |log(Z(x))| −1.82 5.02 −1.81 1.35 −1.83 0.68 −1.91 0.28 Table 1: Comparison of neural network likelihood for various α values. log(P (x)) is the average log-likelihood on a held-out set. |log(Z(x)) |is the mean error in log-likelihood when using Ur (x) directly instead of the true softmax probability log(P (x)). Note that α = 0 is equivalent to the standard neu"
P14-1129,D13-1141,0,0.248696,"Missing"
P15-1004,D13-1106,0,0.147527,"ntly. Each of our proposed methods results in significant improvements that are complementary. The overall improvement is +2.7 and +1.8 BLEU points for Arabic-English and ChineseEnglish translation over a state-of-the-art system that already includes neural network features. 1 Introduction Recent advances in applying Neural Networks to Statistical Machine Translation (SMT) have generally taken one of two approaches. They either develop neural network-based features that are used to score hypotheses generated from traditional translation grammars (Sundermeyer et al., 2014; Devlin et al., 2014; Auli et al., 2013; Le et al., 2012; Schwenk, 2012), or they implement the whole translation process as a single neural network (Bahdanau et al., 2014; Sutskever et al., 2014). The latter approach, sometimes referred to as Neural Machine Translation, attempts to overhaul SMT, while the former capitalizes on the strength of the current SMT paradigm and leverages the modeling power of neural networks to improve the scoring of hypotheses generated ∗ * Research conducted when the author was at BBN. 31 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International J"
P15-1004,J93-2003,0,0.0288832,"0.8 31.5 31.3 31.5 31.9 31.8 32.0 Table 3: Experimental results to investigate the effects of the new features, DTN and MTL. The top part shows the BOLT results, while the bottom part shows the NIST results. The best results for each conditions and each language-pair are in bold. The baselines are in italics. . AR-EN mixed-case ZH-EN mixed-case Base. 53.7 51.8 36.6 34.4 Feat 55.4 53.1 37.8 35.5 Tensor 55.9 53.7 38.2 35.9 MTL 56.4 54.1 38.5 36.1 The features we propose in this paper address the major aspects of SMT modeling that have informed much of the research since the original IBM models (Brown et al., 1993): lexical translation, reordering, word fertility, and language models. Of particular relevance to our work are approaches that incorporate context-sensitivity into the models (Carpuat and Wu, 2007), formulate reordering as orientation prediction task (Tillman, 2004) and that use neural network language models (Bengio et al., 2003; Schwenk, 2010; Schwenk, 2012), and incorporate source-side context into them (Devlin et al., 2014; Auli et al., 2013; Le et al., 2012; Schwenk, 2012). Approaches to incorporating source context into a neural network model differ mainly in how they represent the sour"
P15-1004,D07-1007,0,0.0235059,"ST results. The best results for each conditions and each language-pair are in bold. The baselines are in italics. . AR-EN mixed-case ZH-EN mixed-case Base. 53.7 51.8 36.6 34.4 Feat 55.4 53.1 37.8 35.5 Tensor 55.9 53.7 38.2 35.9 MTL 56.4 54.1 38.5 36.1 The features we propose in this paper address the major aspects of SMT modeling that have informed much of the research since the original IBM models (Brown et al., 1993): lexical translation, reordering, word fertility, and language models. Of particular relevance to our work are approaches that incorporate context-sensitivity into the models (Carpuat and Wu, 2007), formulate reordering as orientation prediction task (Tillman, 2004) and that use neural network language models (Bengio et al., 2003; Schwenk, 2010; Schwenk, 2012), and incorporate source-side context into them (Devlin et al., 2014; Auli et al., 2013; Le et al., 2012; Schwenk, 2012). Approaches to incorporating source context into a neural network model differ mainly in how they represent the source sentence and in how long is the history they keep. In terms of representation of the source sentence, we follow (Devlin et al., 2014) in using a window around the affiliated source word. To name"
P15-1004,P14-1062,0,0.0563792,"ture and model learning. 38 task learning to integrate the SMT features more tightly. Empirically, all our proposals successfully produce an improvement over state-of-the-art machine translation system for Arabic-to-English and Chinese-to-English and for both BOLT web forum and NIST conditions. Building on the success of this paper, we plan to develop other neuralnetwork-based features, and to also relax the limiteation of current rule extraction heuristics by generating translations word-by-word. works, other network architectures that have been applied to SMT include convolutional networks (Kalchbrenner et al., 2014) and recursive networks (Socher et al., 2011). The simplicity of feedforward networks works to our advantage. More specifically, due to the absence of a feedback loop, the feedforward architecture allows us to treat individual decisions independently, which makes parallelization of the training easy and the querying the network at decoding time straightforward. The use of tensors in the hidden layers strengthens the neural network model, allowing us to model more complex feature interactions like collocation, which has been long recognized as important information for many NLP tasks (e.g. word"
P15-1004,N09-1025,0,0.017756,"or the DARPA BOLT Web Forum and the NIST OpenMT12 conditions. 5.1 Baseline MT System We run our experiments using a state-of-the-art string-to-dependency hierarchical decoder (Shen et al., 2010). The baseline we use includes a set of powerful features as follow: • Forward and backward rule probabilities • Contextual lexical smoothing (Devlin, 2009) • 5-gram Kneser-Ney LM • Dependency LM (Shen et al., 2010) • Length distribution (Shen et al., 2010) • Trait features (Devlin and Matsoukas, 2012) • Factored source syntax (Huang et al., 2013) • Discriminative sparse feature, totaling 50k features (Chiang et al., 2009) • Neural Network Joint Model (NNJM) and Neural Network Lexical Translation Model 1 In this and in the other parts of the paper, we add the normalization regularization term described in (Devlin et al., 2014) to the loss function to avoid computing the normalization constant at model query/decoding time. 35 around 0.9 BLEU point improvement in AR-EN and around 0.3 BLEU in ZH-EN compared to the baseline. The JMO consistently provides better improvement compared to a larger JM context (row S2 ), validating our hypothesis that using offset source context captures important non-local context. Rows"
P15-1004,N12-1005,0,0.150227,"roposed methods results in significant improvements that are complementary. The overall improvement is +2.7 and +1.8 BLEU points for Arabic-English and ChineseEnglish translation over a state-of-the-art system that already includes neural network features. 1 Introduction Recent advances in applying Neural Networks to Statistical Machine Translation (SMT) have generally taken one of two approaches. They either develop neural network-based features that are used to score hypotheses generated from traditional translation grammars (Sundermeyer et al., 2014; Devlin et al., 2014; Auli et al., 2013; Le et al., 2012; Schwenk, 2012), or they implement the whole translation process as a single neural network (Bahdanau et al., 2014; Sutskever et al., 2014). The latter approach, sometimes referred to as Neural Machine Translation, attempts to overhaul SMT, while the former capitalizes on the strength of the current SMT paradigm and leverages the modeling power of neural networks to improve the scoring of hypotheses generated ∗ * Research conducted when the author was at BBN. 31 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference o"
P15-1004,N12-1059,1,0.860591,"xperiments We demonstrate the impact of our work with extensive MT experiments on Arabic-English and Chinese-English translation for the DARPA BOLT Web Forum and the NIST OpenMT12 conditions. 5.1 Baseline MT System We run our experiments using a state-of-the-art string-to-dependency hierarchical decoder (Shen et al., 2010). The baseline we use includes a set of powerful features as follow: • Forward and backward rule probabilities • Contextual lexical smoothing (Devlin, 2009) • 5-gram Kneser-Ney LM • Dependency LM (Shen et al., 2010) • Length distribution (Shen et al., 2010) • Trait features (Devlin and Matsoukas, 2012) • Factored source syntax (Huang et al., 2013) • Discriminative sparse feature, totaling 50k features (Chiang et al., 2009) • Neural Network Joint Model (NNJM) and Neural Network Lexical Translation Model 1 In this and in the other parts of the paper, we add the normalization regularization term described in (Devlin et al., 2014) to the loss function to avoid computing the normalization constant at model query/decoding time. 35 around 0.9 BLEU point improvement in AR-EN and around 0.3 BLEU in ZH-EN compared to the baseline. The JMO consistently provides better improvement compared to a larger"
P15-1004,W02-1006,0,0.0792732,"(Socher et al., 2011). The simplicity of feedforward networks works to our advantage. More specifically, due to the absence of a feedback loop, the feedforward architecture allows us to treat individual decisions independently, which makes parallelization of the training easy and the querying the network at decoding time straightforward. The use of tensors in the hidden layers strengthens the neural network model, allowing us to model more complex feature interactions like collocation, which has been long recognized as important information for many NLP tasks (e.g. word sense disambiguation (Lee and Ng, 2002)). The tensor formulation we use is similar to that of (Yu et al., 2012; Hutchinson et al., 2013). Tensor Neural Networks have a wide application in other field, but have only been recently applied in NLP (Socher et al., 2013; Pei et al., 2014). To our knowledge, our work is the first to use tensor networks in SMT. Our approach to multitask learning is related to work that is often labeled joint training or transfer learning. To name a few of these works, Finkel and Manning (2009) successfully train name entity recognizers and syntactic parsers jointly, and Singh et al. (2013) train models for"
P15-1004,P14-1129,1,0.74947,"Missing"
P15-1004,J03-1002,0,0.00979299,"Missing"
P15-1004,P14-1028,0,0.0536156,"allelization of the training easy and the querying the network at decoding time straightforward. The use of tensors in the hidden layers strengthens the neural network model, allowing us to model more complex feature interactions like collocation, which has been long recognized as important information for many NLP tasks (e.g. word sense disambiguation (Lee and Ng, 2002)). The tensor formulation we use is similar to that of (Yu et al., 2012; Hutchinson et al., 2013). Tensor Neural Networks have a wide application in other field, but have only been recently applied in NLP (Socher et al., 2013; Pei et al., 2014). To our knowledge, our work is the first to use tensor networks in SMT. Our approach to multitask learning is related to work that is often labeled joint training or transfer learning. To name a few of these works, Finkel and Manning (2009) successfully train name entity recognizers and syntactic parsers jointly, and Singh et al. (2013) train models for coreference resolution, named entity recognition and relation extraction jointly. Both efforts are motivated by the minimization of cascading errors. Our work is most closely related to Collobert and Weston (2008; Collobert et al. (2011), who"
P15-1004,N09-1037,0,0.0705049,"Missing"
P15-1004,W10-1748,1,0.831052,"LEU points in ZH-EN. (NNLTM) (Devlin et al., 2014) As shown, our baseline system already includes neural network-based features. NNJM, NNLTM and use two hidden layers with 500 units and use embedding of size 200 for each input. We use the MADA-ARZ tokenizer (Habash et al., 2013) for Arabic word tokenization. For Chinese tokenization, we use a simple longest-matchfirst lexicon-based approach. We align the training data using GIZA++ (Och and Ney, 2003). For tuning the weights of MT features including the new features, we use iterative k-best optimization with an ExpectedBLEU objective function (Rosti et al., 2010), and decode the test sets after 5 tuning iteration. We report the lower-cased BLEU and TER scores. 5.2 BOLT Discussion Forum The bulk of our experiments is on the BOLT Web Discussion Forum domain, which uses data collected by the LDC. The parallel training data consists of all of the high-quality NIST training corpora, plus an additional 3 million words of translated forum data. The tuning and test sets consist of roughly 5000 segments each, with 2 independent references for Arabic and 3 for Chinese. 5.2.1 Effects of New Features We first look at the effects of the proposed features compared"
P15-1004,W10-3815,0,0.0299312,"53.7 51.8 36.6 34.4 Feat 55.4 53.1 37.8 35.5 Tensor 55.9 53.7 38.2 35.9 MTL 56.4 54.1 38.5 36.1 The features we propose in this paper address the major aspects of SMT modeling that have informed much of the research since the original IBM models (Brown et al., 1993): lexical translation, reordering, word fertility, and language models. Of particular relevance to our work are approaches that incorporate context-sensitivity into the models (Carpuat and Wu, 2007), formulate reordering as orientation prediction task (Tillman, 2004) and that use neural network language models (Bengio et al., 2003; Schwenk, 2010; Schwenk, 2012), and incorporate source-side context into them (Devlin et al., 2014; Auli et al., 2013; Le et al., 2012; Schwenk, 2012). Approaches to incorporating source context into a neural network model differ mainly in how they represent the source sentence and in how long is the history they keep. In terms of representation of the source sentence, we follow (Devlin et al., 2014) in using a window around the affiliated source word. To name some other approaches, Auli et al. (2013) uses latent semantic analysis and source sentence embeddings learned from the recurrent neural network; Sun"
P15-1004,N13-1044,0,0.0222499,"features and the set of target-enumerating features produce around 1.1 to 1.2 points BLEU gain in AR-EN and 0.3 to 0.5 points BLEU gain in ZHEN. The combination of the two sets produces a complementary gain in addition to the gains of the individual models as Row (S10 ) shows. The combined gain improves to 1.5 BLEU points in AREN and 0.7 BLEU points in ZH-EN. (NNLTM) (Devlin et al., 2014) As shown, our baseline system already includes neural network-based features. NNJM, NNLTM and use two hidden layers with 500 units and use embedding of size 200 for each input. We use the MADA-ARZ tokenizer (Habash et al., 2013) for Arabic word tokenization. For Chinese tokenization, we use a simple longest-matchfirst lexicon-based approach. We align the training data using GIZA++ (Och and Ney, 2003). For tuning the weights of MT features including the new features, we use iterative k-best optimization with an ExpectedBLEU objective function (Rosti et al., 2010), and decode the test sets after 5 tuning iteration. We report the lower-cased BLEU and TER scores. 5.2 BOLT Discussion Forum The bulk of our experiments is on the BOLT Web Discussion Forum domain, which uses data collected by the LDC. The parallel training da"
P15-1004,C12-2104,0,0.234404,"esults in significant improvements that are complementary. The overall improvement is +2.7 and +1.8 BLEU points for Arabic-English and ChineseEnglish translation over a state-of-the-art system that already includes neural network features. 1 Introduction Recent advances in applying Neural Networks to Statistical Machine Translation (SMT) have generally taken one of two approaches. They either develop neural network-based features that are used to score hypotheses generated from traditional translation grammars (Sundermeyer et al., 2014; Devlin et al., 2014; Auli et al., 2013; Le et al., 2012; Schwenk, 2012), or they implement the whole translation process as a single neural network (Bahdanau et al., 2014; Sutskever et al., 2014). The latter approach, sometimes referred to as Neural Machine Translation, attempts to overhaul SMT, while the former capitalizes on the strength of the current SMT paradigm and leverages the modeling power of neural networks to improve the scoring of hypotheses generated ∗ * Research conducted when the author was at BBN. 31 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Langua"
P15-1004,D13-1053,1,0.854027,"extensive MT experiments on Arabic-English and Chinese-English translation for the DARPA BOLT Web Forum and the NIST OpenMT12 conditions. 5.1 Baseline MT System We run our experiments using a state-of-the-art string-to-dependency hierarchical decoder (Shen et al., 2010). The baseline we use includes a set of powerful features as follow: • Forward and backward rule probabilities • Contextual lexical smoothing (Devlin, 2009) • 5-gram Kneser-Ney LM • Dependency LM (Shen et al., 2010) • Length distribution (Shen et al., 2010) • Trait features (Devlin and Matsoukas, 2012) • Factored source syntax (Huang et al., 2013) • Discriminative sparse feature, totaling 50k features (Chiang et al., 2009) • Neural Network Joint Model (NNJM) and Neural Network Lexical Translation Model 1 In this and in the other parts of the paper, we add the normalization regularization term described in (Devlin et al., 2014) to the loss function to avoid computing the normalization constant at model query/decoding time. 35 around 0.9 BLEU point improvement in AR-EN and around 0.3 BLEU in ZH-EN compared to the baseline. The JMO consistently provides better improvement compared to a larger JM context (row S2 ), validating our hypothesi"
P15-1004,P13-1124,1,0.852162,"=1 estimates P (e5 |e4 , e3 , Ca4 = {f8 , f9 , f10 }). . . . f5 z C7 = } |Ca5 { f6 f7 f8 . . . e3 e4 e5 e6 f9 e7 get word l(fj ) = ebj given a source context Cj , bj ∈ B is the source-to-target word affiliation as defined in (Devlin et al., 2014). When fj is translated to more than one word, we arbitrarily keep the left-most one. The target word vocabulary V is extended with a N U LL token to accommodate unaligned source words. Orientation model (ORI) describes the probability of orientation of the translation of phrases surrounding a source word fj relative to its own translation. We follow (Setiawan et al., 2013) in modeling the orientation of the left and right phrases of fj with maximal orientation span (the longest neighboring phrase consistent with alignment), which we denote by Lj and Rj respectively. Thus, o(fj ) = hoLj (fj ), oRj (fj )i, where oLj and oRj refer to the orientation of Lj and Rj respectively. For unaligned fj , we set o(fj ) = oLj (Rj ), the orientation of Rj with respect to Lj . Fertility model (FM) models the probability that a source word fj generates φ(fj ) words in the hypothesis. Our implemented model only distinguishes between aligned and unaligned source words (i.e., φ(fj"
P15-1004,D13-1176,0,0.0339894,"; Le et al., 2012; Schwenk, 2012). Approaches to incorporating source context into a neural network model differ mainly in how they represent the source sentence and in how long is the history they keep. In terms of representation of the source sentence, we follow (Devlin et al., 2014) in using a window around the affiliated source word. To name some other approaches, Auli et al. (2013) uses latent semantic analysis and source sentence embeddings learned from the recurrent neural network; Sundermeyer et al. (2014) take the representation from a bidirectional LSTM recurrent neural network; and Kalchbrenner and Blunsom (2013) employ a convolutional sentence model. For target context, recent work has tried to look beyond the classical n-gram history. (Auli et al., 2013; Sundermeyer et al., 2014) consider an unbounded history, at the expense of making their model only applicable for N-best rescoring. Another recent line of research (Bahdanau et al., 2014; Sutskever et al., 2014) departs more radically from conventional feature-based SMT and implements the MT system as a single neural network. These models use a representation of the whole input sentence. We use a feedforward neural network in this work. Besides feed"
P15-1004,J10-4005,0,0.0232201,"the sum of log likelihoods since we assume that the features are independent. Fig. 3(c) illustrates MTL between M models sharing the input embedding layer and the first hidden layer (t = 1) compared to the separatelytrained conventional feedforward neural network and tensor neural network. 5 Experiments We demonstrate the impact of our work with extensive MT experiments on Arabic-English and Chinese-English translation for the DARPA BOLT Web Forum and the NIST OpenMT12 conditions. 5.1 Baseline MT System We run our experiments using a state-of-the-art string-to-dependency hierarchical decoder (Shen et al., 2010). The baseline we use includes a set of powerful features as follow: • Forward and backward rule probabilities • Contextual lexical smoothing (Devlin, 2009) • 5-gram Kneser-Ney LM • Dependency LM (Shen et al., 2010) • Length distribution (Shen et al., 2010) • Trait features (Devlin and Matsoukas, 2012) • Factored source syntax (Huang et al., 2013) • Discriminative sparse feature, totaling 50k features (Chiang et al., 2009) • Neural Network Joint Model (NNJM) and Neural Network Lexical Translation Model 1 In this and in the other parts of the paper, we add the normalization regularization term"
P15-1004,D08-1090,1,0.790114,"Missing"
P15-1004,D14-1003,0,0.0876777,"g to estimate the neural network parameters jointly. Each of our proposed methods results in significant improvements that are complementary. The overall improvement is +2.7 and +1.8 BLEU points for Arabic-English and ChineseEnglish translation over a state-of-the-art system that already includes neural network features. 1 Introduction Recent advances in applying Neural Networks to Statistical Machine Translation (SMT) have generally taken one of two approaches. They either develop neural network-based features that are used to score hypotheses generated from traditional translation grammars (Sundermeyer et al., 2014; Devlin et al., 2014; Auli et al., 2013; Le et al., 2012; Schwenk, 2012), or they implement the whole translation process as a single neural network (Bahdanau et al., 2014; Sutskever et al., 2014). The latter approach, sometimes referred to as Neural Machine Translation, attempts to overhaul SMT, while the former capitalizes on the strength of the current SMT paradigm and leverages the modeling power of neural networks to improve the scoring of hypotheses generated ∗ * Research conducted when the author was at BBN. 31 Proceedings of the 53rd Annual Meeting of the Association for Computational"
P15-1004,N04-4026,0,0.122585,"Missing"
P94-1004,J92-1004,0,0.271899,"Missing"
P94-1004,P92-1017,0,0.0140784,"Missing"
P94-1004,H91-1020,0,\N,Missing
P94-1004,H90-1047,1,\N,Missing
P94-1004,H90-1053,0,\N,Missing
P96-1008,H94-1011,0,0.0271826,"Missing"
P96-1008,J92-1004,0,0.0101679,"emantic interpretation model), combined with the full space of possible postdiscourse meanings Mo, is searched for the single candidate that maximizes P( M o I H, M s) P( M s,T) P(W I T ) , conditioned on the current history H. The discourse history is then updated and the post-discourse meaning is returned. process: semantic labels identify the basic units of meaning, while syntactic structures help identify relationships between those units. 3.1 Statistical Parsing Model The parsing model is a probabilistic recursive transition network similar to those described in (Miller et ai. 1994) and (Seneff 1992). The probability of a parse tree T given a word string Wis rewritten using Bayes role as: We now proceed to a detailed discussion of each of these three stages, beginning with parsing. P(TIW) = 3. Parsing Since P(W) is constant for any given word string, candidate parses can be ranked by considering only the product P(T) P(W I 7&quot;). The probability P(T) is modeled by state transition probabilities in the recursive transition network, and P(W I T) is modeled by word transition probabilities. Our parse representation is essentially syntactic in form, patterned on a simplified head-centered theor"
P96-1008,J95-2002,0,0.0313972,"to unigram models. P( M s , T ) = P( FT, S , T ) = P( FT) P ( T I FT) P(S I FT, T). 3.3 Searching the Parsing Model Obviously, the prior probabilities P(FT) can be obtained directly from the training data. To compute P(T I FT), each of the state transitions from the previous parsing model are simply rescored conditioned on the frame type. The new state transition probabilities are: In order to explore the space of possible parses efficiently, the parsing model is searched using a decoder based on an adaptation of the Earley parsing algorithm (Earley 1970). This adaptation, related to that of (Stolcke 1995), involves reformulating the Earley algorithm to work with probabilistic recursive transition networks rather than with deterministic production rules. For details of the decoder, see (Miller 1996). P(staten I staten_ t, stateup, FT) . To compute P(S I FT, T) , we make the independence assumption that slot filling operations depend only on the frame type, the slot operations already performed, and on the local parse structure around the operation. This local neighborhood consists of the parse node itself, its two left siblings, its two right siblings, and its four immediate ancestors. Further,"
P96-1008,M91-1028,0,\N,Missing
P96-1008,H90-1022,0,\N,Missing
P96-1008,H90-1020,0,\N,Missing
P96-1008,P94-1004,1,\N,Missing
P96-1008,M92-1024,0,\N,Missing
P96-1008,P95-1037,0,\N,Missing
W03-0501,P00-1041,0,0.642724,"structured templates (Paice and Jones, 1993), sentence compression (Hori et al., 2002; Knight and Marcu, 2001; Grefenstette, 1998, Luhn, 1958), and generation of abstracts from multiple sources (Radev and McKeown, 1998). We focus instead on the construction of headline-style abstracts from a single story. Headline generation can be viewed as analogous to statistical machine translation, where a concise document is generated from a verbose one using a Noisy Channel Model and the Viterbi search to select the most likely summarization. This approach has been explored in (Zajic et al., 2002) and (Banko et al., 2000). The approach we use in Hedge is most similar to that of (Knight and Marcu, 2001), where a single sentence is shortened using statistical compression. As in this work, we select headline words from story words in the order that they appear in the story—in particular, the first sentence of the story. However, we use linguistically motivated heuristics for shortening the sentence; there is no statistical model, which means we do not require any prior training on a large corpus of story/headline pairs. Linguistically motivated heuristics have been used by (McKeown et al, 2002) to distinguish con"
W03-0501,P97-1003,0,0.00838243,"eadlines, which are often intended only to catch the eye, our approach produces informative abstracts describing the main theme or event of the newspaper article. We claim that the construction of informative abstracts requires access to deeper linguistic knowledge, in order to make substantial improvements over purely statistical approaches. In this paper, we present our technique for producing headlines using a parse-and-trim approach based on the BBN Parser. As described in Miller et al. (1998), the BBN parser builds augmented parse trees according to a process similar to that described in Collins (1997). The BBN parser has been used successfully for the task Richard Schwartz BBN schwartz@bbn.com of information extraction in the SIFT system (Miller et al., 2000). The next section presents previous work in the area of automatic generation of abstracts. Following this, we present feasibility tests used to establish the validity of an approach that constructs headlines from words in a story, taken in order and focusing on the earlier part of the story. Next, we describe the application of the parse-and-trim approach to the problem of headline generation. We discuss the linguistically-motivated h"
W03-0501,W97-0710,0,0.0392103,"lier work on headline generation, a probabilistic model for automatic headline generation (Zajic et al, 2002). In this paper we will refer to this statistical system as HMM Hedge We demonstrate the effectiveness of our linguistically-motivated approach, Hedge Trimmer, over the probabilistic model, HMM Hedge, using both human evaluation and automatic metrics. 2 Previous Work Other researchers have investigated the topic of automatic generation of abstracts, but the focus has been different, e.g., sentence extraction (Edmundson, 1969; Johnson et al, 1993; Kupiec et al., 1995; Mann et al., 1992; Teufel and Moens, 1997; Zechner, 1995), processing of structured templates (Paice and Jones, 1993), sentence compression (Hori et al., 2002; Knight and Marcu, 2001; Grefenstette, 1998, Luhn, 1958), and generation of abstracts from multiple sources (Radev and McKeown, 1998). We focus instead on the construction of headline-style abstracts from a single story. Headline generation can be viewed as analogous to statistical machine translation, where a concise document is generated from a verbose one using a Noisy Channel Model and the Viterbi search to select the most likely summarization. This approach has been explor"
W03-0501,A00-2030,0,0.00711002,"rticle. We claim that the construction of informative abstracts requires access to deeper linguistic knowledge, in order to make substantial improvements over purely statistical approaches. In this paper, we present our technique for producing headlines using a parse-and-trim approach based on the BBN Parser. As described in Miller et al. (1998), the BBN parser builds augmented parse trees according to a process similar to that described in Collins (1997). The BBN parser has been used successfully for the task Richard Schwartz BBN schwartz@bbn.com of information extraction in the SIFT system (Miller et al., 2000). The next section presents previous work in the area of automatic generation of abstracts. Following this, we present feasibility tests used to establish the validity of an approach that constructs headlines from words in a story, taken in order and focusing on the earlier part of the story. Next, we describe the application of the parse-and-trim approach to the problem of headline generation. We discuss the linguistically-motivated heuristics we use to produce results that are headlinelike. Finally, we evaluate Hedge Trimmer by comparing it to our earlier work on headline generation, a proba"
W03-0501,J98-3005,0,\N,Missing
W03-0501,X98-1014,1,\N,Missing
W03-0501,P02-1040,0,\N,Missing
W05-0901,J96-2004,0,0.0669426,"rd. Unfortunately, a consistent gold standard has not yet been reported. For example, in two previous studies (Mani, 2001; Tombros and Sanderson, 1998), users’ judgments were compared to “gold standard judgments” produced by members of the University of Pennsylvania’s Linguistic Data Consortium. Although these judgments were supposed to represent the correct relevance judgments for each of the documents associated with an event, both studies reported that annotators’ judgments varied greatly and that this was a significant issue for the evaluations. In the SUMMAC experiments, the Kappa score (Carletta, 1996; Eugenio and Glass, 2004) for interannotator agreement was reported to be 0.38 (Mani et al., 2002). In fact, large variations have been found in the initial summary scoring of an individual participant and a subsequent scoring that occurs a few weeks later (Mani, 2001; van Halteren and Teufel, 2003). This paper attempts to overcome the problem of interannotator inconsistency by measuring summary effectiveness in an extrinsic task using a much more consistent form of user judgment instead of a gold standard. Using Relevance-Prediction increases the confidence in our results and strengthens the"
W05-0901,J04-1005,0,0.0262553,"Missing"
W05-0901,N03-1020,0,0.0884371,"-based usefulness. Section 3 presents a novel extrinsic measure called Relevance-Prediction. Section 4 demonstrates that this is a more reliable measure than that of previous gold standard methods, e.g., the LDC-Agreement method used for SUMMAC-style evaluations, and that this reliability allows us to make stronger statistical statements about the benefits of summarization. We expect these findings to be important for future summarization evaluations. Section 5 presents the results of correlation between task usefulness and the Recall Oriented Understudy for Gisting Evaluation (ROUGE) metric (Lin and Hovy, 2003).1 While we show that ROUGE correlates with task usefulness (using our Relevance-Prediction measure), we detect a slight difference between informative, extractive headlines (containing words from the full document) and less informative, non-extractive “eye-catchers” (containing words that might not appear in the full document, and intended to entice a reader to read the entire document). Section 6 further highlights the importance of this point and discusses the implications for automatic evaluation of non-extractive summaries. To evaluate nonextractive summaries reliably, an automatic measur"
W05-0901,C04-1072,0,0.0210096,"t whether ROUGE correlates more highly with Relevance-Prediction than with LDC-Agreement, we calculated the correlation for the results of both techniques using Pearson’s r (Siegel and Castellan, 1988): Pn (ri − r¯)(si − s¯) pPn i=1 pPn (r − r¯)2 ¯)2 i=1 i i=1 (si − s where ri is the ROUGE score of surrogate i, r¯ is the average ROUGE score of all data points, si is the agreement score of summary i (using Relevance-Prediction or LDC-Agreement), and s¯ is the average agreement score. Pearson’s statistics is commonly used in summarization and machine translation evaluation, see e.g. (Lin, 2004; Lin and Och, 2004). As one might expect, there is some variability in the correlation between ROUGE and human judgments for 9 We also computed ROUGE 2-gram, ROUGE L and ROUGE W, but the trend for these did not differ from ROUGE1. 6 Figure 1: Distribution of the Correlation Variation for Relevance-Prediction on HEAD and HUM the different partitions. However, the boxplots for both HEAD and HUM indicate that the first and third quartile were relatively close to the median (see Figure 1). Table 5 shows the Pearson Correlations with ROUGE1 using Relevance-Prediction and LDC-Agreement. For Relevance-Prediction, we ob"
W05-0901,W04-1013,0,0.0100221,"tion To test whether ROUGE correlates more highly with Relevance-Prediction than with LDC-Agreement, we calculated the correlation for the results of both techniques using Pearson’s r (Siegel and Castellan, 1988): Pn (ri − r¯)(si − s¯) pPn i=1 pPn (r − r¯)2 ¯)2 i=1 i i=1 (si − s where ri is the ROUGE score of surrogate i, r¯ is the average ROUGE score of all data points, si is the agreement score of summary i (using Relevance-Prediction or LDC-Agreement), and s¯ is the average agreement score. Pearson’s statistics is commonly used in summarization and machine translation evaluation, see e.g. (Lin, 2004; Lin and Och, 2004). As one might expect, there is some variability in the correlation between ROUGE and human judgments for 9 We also computed ROUGE 2-gram, ROUGE L and ROUGE W, but the trend for these did not differ from ROUGE1. 6 Figure 1: Distribution of the Correlation Variation for Relevance-Prediction on HEAD and HUM the different partitions. However, the boxplots for both HEAD and HUM indicate that the first and third quartile were relatively close to the median (see Figure 1). Table 5 shows the Pearson Correlations with ROUGE1 using Relevance-Prediction and LDC-Agreement. For Relevan"
W05-0901,N04-1019,0,0.0504141,"her highlights the importance of this point and discusses the implications for automatic evaluation of non-extractive summaries. To evaluate nonextractive summaries reliably, an automatic measure may require knowledge of sophisticated meaning units.2 It is our hope that the conclusions drawn herein will prompt investigation into more sophisticated automatic metrics as researchers shift their focus to non-extractive summaries. 1 ROUGE has been previously used as the primary automatic evaluation metric by NIST in the 2003 and 2004 DUC Evaluations. 2 The content units proposed in recent methods (Nenkova and Passonneau, 2004) are a first step in this direction. 1 Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation c and/or Summarization, pages 1–8, Ann Arbor, June 2005. 2005 Association for Computational Linguistics 2 Background In the past, assessments of usefulness involved a wide range of both intrinsic and extrinsic (task-based) measures (Sparck-Jones and Gallier, 1996). Intrinsic evaluations focus on coherence and informativeness (Jing et al., 1998) and often involve quality comparisons between automatic summaries and reference summaries that are pre-determin"
W05-0901,W00-0407,0,0.0284813,". Using Relevance-Prediction increases the confidence in our results and strengthens the statistical statements we can make about the benefits of summarization. The next section describes an alternative approach to measuring task-based usefulness, where the usage of external judgments as a gold standard is replaced by the 3 A topic is an event or activity, along with all directly related events and activities. An event is something that happens at some specific time and place, and the unavoidable consequences. 2 user’s own decisions on the full text. Following the lead of earlier evaluations (Oka and Ueda, 2000; Mani et al., 2002; Sakai and Sparck-Jones, 2001), we focus on relevance assessment as our extrinsic task. 3 Evaluation of Usefulness of Summaries We define a new extrinsic measure of task-based usefulness called Relevance-Prediction, where we compare a summary-based decision to the subject’s own full-text decision rather than to a different subject’s decision. Our findings differ from that of the SUMMAC results (Mani et al., 2002) in that using Relevance-Prediction as an alternative to comparision to a gold standard is a more realistic agreement measure for assessing usefulness in a relevanc"
W05-0901,J98-3005,0,0.0406616,"used. Definitive conclusions about the usefulness of summaries would provide justification for continued research and development of new summarization methods. To investigate the question of whether text summarization is useful in an extrinsic task, we examined human performance in a relevance assessment task using a human text surrogate (i.e. text intended to stand in the place of a document). We use single-document English summaries as these are sufficient for investigating task-based usefulness, although more elaborate surrogates are possible, e.g., those that span more than one document (Radev and McKeown, 1998; Mani and Bloedorn, 1998). The next section motivates the need for developing a new framework for measuring task-based usefulness. Section 3 presents a novel extrinsic measure called Relevance-Prediction. Section 4 demonstrates that this is a more reliable measure than that of previous gold standard methods, e.g., the LDC-Agreement method used for SUMMAC-style evaluations, and that this reliability allows us to make stronger statistical statements about the benefits of summarization. We expect these findings to be important for future summarization evaluations. Section 5 presents the results"
W05-0901,W03-0508,0,0.0592052,"Missing"
W08-0329,E06-1005,0,\N,Missing
W08-0329,P02-1040,0,\N,Missing
W08-0329,P07-1040,1,\N,Missing
W08-0329,W07-0734,0,\N,Missing
W09-0409,E06-1005,0,\N,Missing
W09-0409,W09-0441,1,\N,Missing
W09-0409,C08-1005,0,\N,Missing
W09-0409,P02-1040,0,\N,Missing
W09-0409,P08-2021,0,\N,Missing
W09-0409,W08-0329,1,\N,Missing
W09-0409,P07-1040,1,\N,Missing
W09-0409,D08-1011,0,\N,Missing
W09-0441,W05-0909,0,0.098914,"frequently measured together on a discrete 5 or 7 point scale, with their average being used as a single score of translation quality. HTER is a more complex and semi-automatic measure in which humans do not score translations directly, but rather generate a new reference translation that is closer to the MT output but retains the fluency and meaning of the original reference. This new targeted reference is then used as the reference translation when scoring the MT output using Translation Edit Rate (TER) (Snover et al., 2006) or when used with other automatic metrics such as BLEU or METEOR (Banerjee and Lavie, 2005). One of the difficulties in the creation of targeted references is a further requirement that the annotator attempt to minimize the number of edits, as measured by TER, between the MT output and the targeted reference, creating the reference that is as close as possible to the MT output while still being adequate and fluent. In this way, only true errors in the MT output are counted. While HTER has been shown to be more consistent and finer grained than individual human annotators of Fluency and Adequacy, it is much more time consuming and taxing on human annotators than other types of human"
W09-0441,P05-1074,0,0.0378241,"ons, its average rank would be 1), although it performed well on average. In particular, TERp did significantly better than the TER metric, indicating the benefit of the enhancements made to TER. 4 Paraphrases TERp uses probabilistic phrasal substitutions to align phrases in the hypothesis with phrases in the reference. It does so by looking up—in a precomputed phrase table—paraphrases of phrases in the reference and using its associated edit cost as the cost of performing a match against the hypothesis. The paraphrases used in TERp were extracted using the pivot-based method as described in (Bannard and Callison-Burch, 2005) with several additional filtering mechanisms to increase the precision. The pivot-based method utilizes the inherent monolingual semantic knowledge from bilingual corpora: we first identify English-to-F phrasal correspondences, then map from English to English by following translation units from English to F and back. For example, if the two English phrases e1 and e2 both correspond to the same foreign phrase f, then they may be considered to be paraphrases of each other with the following probability: p(e1|e2) ≈ p(e1|f ) ∗ p(f |e2) 3 System description of metrics are also distributed by AMTA"
W09-0441,D08-1021,0,0.0160003,"s set included pairs that only shared partial semantic content. Most paraphrases extracted by the pivot method are expected to be of this nature. These pairs are not directly beneficial to TERp since they cannot be substituted for each other in all contexts. However, the fact that they share at least some semantic content does suggest that they may not be entirely useless either. Examples include: Varying Paraphrase Pivot Corpora To determine the effect that the pivot language might have on the quality and utility of the extracted paraphrases in TERp, we used paraphrase pairsmade available by Callison-Burch (2008). These paraphrase pairs were extracted from Europarl data using each of 10 European languages (German, Italian, French etc.) as a pivot language separately and then combining the extracted paraphrase pairs. Callison-Burch (2008) also extracted and made available syntactically constrained paraphrase pairs from the same data that are more likely to be semantically related. We used both sets of paraphrases in TERp as alternatives to the paraphrase pairs that we extracted from the Arabic newswire bitext. The results are shown in the last four rows of Table 5 and show that using a pivot language o"
W09-0441,N06-1058,0,0.0246276,"culating the TER score. For the denominator, TER uses the average number of words across all the references. 2.2 TER-Plus TER-Plus (TERp) is an extension of TER that aligns words in the hypothesis and reference not only when they are exact matches but also when the words share a stem or are synonyms. In addition, it uses probabilistic phrasal substitutions to align phrases in the hypothesis and reference. These phrases are generated by considering possible paraphrases of the reference words. Matching using stems and synonyms (Banerjee and Lavie, 2005) and using paraphrases (Zhou et al., 2006; Kauchak and Barzilay, 2006) have previously been shown to be beneficial for automatic MT evaluation. Paraphrases have also been shown to be useful in expanding the number of references used for parameter tuning (Madnani et al., 2007; Madnani et al., 2008) although they are not used directly in this fashion within TERp. While all edit costs in TER are constant, all edit costs in TERp are optimized to maximize correlation with human judgments. This is because while a set of constant weights might prove adequate for the purpose of measuring translation quality—as evidenced by correlation with human judgments both for TER a"
W09-0441,E06-1031,0,0.0692041,"cost of a phrase substitution between the reference phrase, p1 and the hypothesis phrase p2 is: and Tomkins, 1997), TER addresses this by using a greedy search to select the words to be shifted, as well as further constraints on the words to be shifted. These constraints are intended to simulate the way in which a human editor might choose the words to shift. For exact details on these constraints, see Snover et al. (2006). There are other automatic metrics that follow the general formulation as TER but address the complexity of shifting in different ways, such as the CDER evaluation metric (Leusch et al., 2006). When TER is used with multiple references, it does not combine the references. Instead, it scores the hypothesis against each reference individually. The reference against which the hypothesis has the fewest number of edits is deemed the closet reference, and that number of edits is used as the numerator for calculating the TER score. For the denominator, TER uses the average number of words across all the references. 2.2 TER-Plus TER-Plus (TERp) is an extension of TER that aligns words in the hypothesis and reference not only when they are exact matches but also when the words share a stem"
W09-0441,W07-0716,1,0.299608,"ot only when they are exact matches but also when the words share a stem or are synonyms. In addition, it uses probabilistic phrasal substitutions to align phrases in the hypothesis and reference. These phrases are generated by considering possible paraphrases of the reference words. Matching using stems and synonyms (Banerjee and Lavie, 2005) and using paraphrases (Zhou et al., 2006; Kauchak and Barzilay, 2006) have previously been shown to be beneficial for automatic MT evaluation. Paraphrases have also been shown to be useful in expanding the number of references used for parameter tuning (Madnani et al., 2007; Madnani et al., 2008) although they are not used directly in this fashion within TERp. While all edit costs in TER are constant, all edit costs in TERp are optimized to maximize correlation with human judgments. This is because while a set of constant weights might prove adequate for the purpose of measuring translation quality—as evidenced by correlation with human judgments both for TER and HTER—they may not be ideal for maximizing correlation. TERp uses all the edit operations of TER— Matches, Insertions, Deletions, Substitutions and Shifts—as well as three new edit operations: Stem Match"
W09-0441,2008.amta-papers.13,1,0.681799,"exact matches but also when the words share a stem or are synonyms. In addition, it uses probabilistic phrasal substitutions to align phrases in the hypothesis and reference. These phrases are generated by considering possible paraphrases of the reference words. Matching using stems and synonyms (Banerjee and Lavie, 2005) and using paraphrases (Zhou et al., 2006; Kauchak and Barzilay, 2006) have previously been shown to be beneficial for automatic MT evaluation. Paraphrases have also been shown to be useful in expanding the number of references used for parameter tuning (Madnani et al., 2007; Madnani et al., 2008) although they are not used directly in this fashion within TERp. While all edit costs in TER are constant, all edit costs in TERp are optimized to maximize correlation with human judgments. This is because while a set of constant weights might prove adequate for the purpose of measuring translation quality—as evidenced by correlation with human judgments both for TER and HTER—they may not be ideal for maximizing correlation. TERp uses all the edit operations of TER— Matches, Insertions, Deletions, Substitutions and Shifts—as well as three new edit operations: Stem Matches, Synonym Matches and"
W09-0441,niessen-etal-2000-evaluation,0,0.0236852,"ce. The handicap of using a single reference can be addressed by the construction of a lattice of reference translations. Such a technique has been used with TER to combine the output of multiple translation systems (Rosti et al., 2007). TERp does not utilize this methodology2 and instead focuses on addressing the exact matching flaw of TER. A brief description of TER is presented in Section 2.1, followed by a discussion of how TERp differs from TER in Section 2.2. 2.1 TER One of the first automatic metrics used to evaluate automatic machine translation (MT) systems was Word Error Rate (WER) (Niessen et al., 2000), which is the standard evaluation metric for Automatic Speech Recognition. WER is computed as the Levenshtein (Levenshtein, 1966) distance between the words of the system output and the words of the reference translation divided by the length of the reference translation. Unlike speech recognition, there are many correct translations for any given foreign sentence. These correct translations differ not only in their word choice but also in the order in which the words occur. WER is generally seen as inadequate for evaluation for machine translation as it fails to combine knowledge from multip"
W09-0441,P02-1040,0,0.102317,"n Edit Rate evaluation metric with tunable parameters and the incorporation of morphology, synonymy and paraphrases. TER-Plus was shown to be one of the top metrics in NIST’s Metrics MATR 2008 Challenge, having the highest average rank in terms of Pearson and Spearman correlation. Optimizing TER-Plus to different types of human judgments yields significantly improved correlations and meaningful changes in the weight of different types of edits, demonstrating significant differences between the types of human judgments. 1 schwartz@bbn.com Introduction Since the introduction of the BLEU metric (Papineni et al., 2002), statistical MT systems have moved away from human evaluation of their performance and towards rapid evaluation using automatic metrics. These automatic metrics are themselves evaluated by their ability to generate scores for MT output that correlate well with human judgments of translation quality. Numerous methods of judging MT output by humans Proceedings of the Fourth Workshop on Statistical Machine Translation , pages 259–268, c Athens, Greece, 30 March – 31 March 2009. 2009 Association for Computational Linguistics 259 or a missing determinator). Different types of translation errors va"
W09-0441,P07-1040,1,0.824501,"an alignment between the hypothesis and the reference, enabling it to be useful beyond general translation evaluation. While TER has been shown to correlate well with human judgments of translation quality, it has several flaws, including the use of only a single reference translation and the measuring of similarity only by exact word matches between the hypothesis and the reference. The handicap of using a single reference can be addressed by the construction of a lattice of reference translations. Such a technique has been used with TER to combine the output of multiple translation systems (Rosti et al., 2007). TERp does not utilize this methodology2 and instead focuses on addressing the exact matching flaw of TER. A brief description of TER is presented in Section 2.1, followed by a discussion of how TERp differs from TER in Section 2.2. 2.1 TER One of the first automatic metrics used to evaluate automatic machine translation (MT) systems was Word Error Rate (WER) (Niessen et al., 2000), which is the standard evaluation metric for Automatic Speech Recognition. WER is computed as the Levenshtein (Levenshtein, 1966) distance between the words of the system output and the words of the reference trans"
W09-0441,2006.amta-papers.25,1,0.965034,"correct meaning, even if the translation is not fully fluent. Fluency and Adequacy are frequently measured together on a discrete 5 or 7 point scale, with their average being used as a single score of translation quality. HTER is a more complex and semi-automatic measure in which humans do not score translations directly, but rather generate a new reference translation that is closer to the MT output but retains the fluency and meaning of the original reference. This new targeted reference is then used as the reference translation when scoring the MT output using Translation Edit Rate (TER) (Snover et al., 2006) or when used with other automatic metrics such as BLEU or METEOR (Banerjee and Lavie, 2005). One of the difficulties in the creation of targeted references is a further requirement that the annotator attempt to minimize the number of edits, as measured by TER, between the MT output and the targeted reference, creating the reference that is as close as possible to the MT output while still being adequate and fluent. In this way, only true errors in the MT output are counted. While HTER has been shown to be more consistent and finer grained than individual human annotators of Fluency and Adequa"
W09-0441,W06-1610,0,0.0609664,"align the two phrases according to TERp. In effect, the probability of the paraphrase is used to determine how much to discount the alignment of the two phrases. Specifically, the cost of a phrase substitution between the reference phrase, p1 and the hypothesis phrase p2 is: and Tomkins, 1997), TER addresses this by using a greedy search to select the words to be shifted, as well as further constraints on the words to be shifted. These constraints are intended to simulate the way in which a human editor might choose the words to shift. For exact details on these constraints, see Snover et al. (2006). There are other automatic metrics that follow the general formulation as TER but address the complexity of shifting in different ways, such as the CDER evaluation metric (Leusch et al., 2006). When TER is used with multiple references, it does not combine the references. Instead, it scores the hypothesis against each reference individually. The reference against which the hypothesis has the fewest number of edits is deemed the closet reference, and that number of edits is used as the numerator for calculating the TER score. For the denominator, TER uses the average number of words across all"
W10-1748,W09-0409,1,0.916031,"ptimization of the combination weights for a 44 system multi-source language combination (All-English). The system combination gained around 0.42.0 BLEU points over the best individual systems on the single source conditions. On the multi-source condition, the system combination gained 6.6 BLEU points. 1 Introduction The BBN submissions to the WMT10 system combination task were based on confusion network decoding. The confusion networks were built using the incremental hypothesis alignment algorithm with flexible matching introduced in the BBN submission for the WMT09 system combination task (Rosti et al., 2009). This year, the system combination weights were tuned to maximize the BLEU score (Papineni et al., 2002) of the 1-best decoding output (lattice based BLEU tuning) using downhill simplex method (Press et al., 2007). A 44 system multi-source combination was also submitted. Since the gradient-free optimization algorithms do not seem to be able to handle more than 20-30 weights, a gradient ascent to maximize an approximate expected BLEU ob321 Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 321–326, c Uppsala, Sweden, 15-16 July 2010. 2010 Associatio"
W10-1748,2006.amta-papers.25,0,0.0222822,"outputs and all 44 English outputs are detailed in Section 4. Finally, Section 5 concludes this paper with some ideas for future work. 1 sat(1) 2 mat(1) 3 (a) Skeleton hypothesis. 0 cat(1,1) 1 sat(1,1) on(0,1) NULL(1,0) 2 mat(1,1) 3 4 (b) Two hypotheses (insertion). 0 2 cat(1) 0 Hypothesis Alignment The confusion networks were built by using the incremental hypothesis alignment algorithm with flexible matching introduced in Rosti et al. (2009). The algorithm is reviewed in more detail here. It is loosely related to the alignment performed in the calculation of the translation edit rate (TER) (Snover et al., 2006) which estimates the edit distance between two strings allowing shifts of blocks of words in addition to insertions, deletions, and substitutions. Calculating an exact TER for strings longer than a few tokens1 is not computationally feasible, so the tercom2 software uses heuristic shift constraints and pruning to find an upper bound of TER. In this work, the hypotheses were aligned incrementally with the confusion network, thus using tokens from all previously aligned hypotheses in computing the edit distance. Lower substitution costs were assigned to tokens considered equivalent and the heuri"
W10-1748,W09-0441,1,0.725529,"on mat”, “sat mat”, and “cat sat hat”, an initial network in Figure 1(a) is generated. The following two hypotheses have a distance of one edit from the initial network, so the second can be aligned next. Figure 1(b) shows the additional node created and the two new arcs for ‘on’ and ‘NULL’ tokens. The third hypothesis has deleted token ‘cat’ and matches the 1 Hypotheses are tokenized and lower-cased prior to alignment. Tokens generally refer to words and punctuation. 2 http://www.cs.umd.edu/˜snover/tercom/ current version 0.7.25. 3 This algorithm is not equivalent to an incremental TERPlus (Snover et al., 2009) due to different shift constraints and the lack of paraphrase matching 322 3 ‘NULL’ token between nodes 2 and 3 as seen in Figure 1(c). The fourth hypothesis matches all but the final token ‘hat’ which becomes a substitution for ‘mat’ in Figure 1(d). The binary vectors in the parentheses following each token show which system generated the token aligned to that arc. If the systems generated N -best hypotheses, a fractional increment could be added to these vectors as in (Rosti et al., 2007). Given these system specific scores are normalized to sum to one over all arcs connecting two consecuti"
W10-1748,P03-1021,0,0.0832836,"ient of the objective function. Due to the size of the lattices, the objective function evaluation may have to be distributed to multiple servers. The optimizer client accumulates the BLEU statistics of the 1-best hypotheses from the servers for given search weights, computes the final BLEU score, and passes it to the optimization algorithm which returns a new set of search weights. The lattice based tuning explores the entire search space and does not require multiple decoding iterations with N -best list merging to approximate the search space as in the standard minimum error rate training (Och, 2003). This allows much faster turnaround in weight tuning. Differentiable approximations of BLEU have been proposed for consensus decoding. Tromble et al. (2008) used a linear approximation and Pauls et al. (2009) used a closer approximation called CoBLEU. CoBLEU is based on the BLEU formula but the n-gram counts are replaced by expected counts over a translation forest. Due to the min-functions required in converting the n-gram counts to matches and a non-differentiable brevity penalty, a sub-gradient ascent must be used. In this work, an approximate expected BLEU (ExpBLEU) defined over N -best l"
W10-1748,P02-1040,0,0.1079,". The system combination gained around 0.42.0 BLEU points over the best individual systems on the single source conditions. On the multi-source condition, the system combination gained 6.6 BLEU points. 1 Introduction The BBN submissions to the WMT10 system combination task were based on confusion network decoding. The confusion networks were built using the incremental hypothesis alignment algorithm with flexible matching introduced in the BBN submission for the WMT09 system combination task (Rosti et al., 2009). This year, the system combination weights were tuned to maximize the BLEU score (Papineni et al., 2002) of the 1-best decoding output (lattice based BLEU tuning) using downhill simplex method (Press et al., 2007). A 44 system multi-source combination was also submitted. Since the gradient-free optimization algorithms do not seem to be able to handle more than 20-30 weights, a gradient ascent to maximize an approximate expected BLEU ob321 Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 321–326, c Uppsala, Sweden, 15-16 July 2010. 2010 Association for Computational Linguistics This paper is organized as follows. Section 2 reviews the incremental hyp"
W10-1748,D09-1147,0,0.199838,"of the 1-best hypotheses from the servers for given search weights, computes the final BLEU score, and passes it to the optimization algorithm which returns a new set of search weights. The lattice based tuning explores the entire search space and does not require multiple decoding iterations with N -best list merging to approximate the search space as in the standard minimum error rate training (Och, 2003). This allows much faster turnaround in weight tuning. Differentiable approximations of BLEU have been proposed for consensus decoding. Tromble et al. (2008) used a linear approximation and Pauls et al. (2009) used a closer approximation called CoBLEU. CoBLEU is based on the BLEU formula but the n-gram counts are replaced by expected counts over a translation forest. Due to the min-functions required in converting the n-gram counts to matches and a non-differentiable brevity penalty, a sub-gradient ascent must be used. In this work, an approximate expected BLEU (ExpBLEU) defined over N -best lists was used as a differentiable objective function. ExpBLEU uses expected BLEU statistics where the min-function is not needed as the statistics are computed offline and the brevity penalty is replaced by a"
W10-1748,P07-1040,1,0.881499,"/˜snover/tercom/ current version 0.7.25. 3 This algorithm is not equivalent to an incremental TERPlus (Snover et al., 2009) due to different shift constraints and the lack of paraphrase matching 322 3 ‘NULL’ token between nodes 2 and 3 as seen in Figure 1(c). The fourth hypothesis matches all but the final token ‘hat’ which becomes a substitution for ‘mat’ in Figure 1(d). The binary vectors in the parentheses following each token show which system generated the token aligned to that arc. If the systems generated N -best hypotheses, a fractional increment could be added to these vectors as in (Rosti et al., 2007). Given these system specific scores are normalized to sum to one over all arcs connecting two consecutive nodes, they may be viewed as system specific word arc posterior estimates. Note, for 1-best hypotheses the scores sum to one without normalization. Standard search algorithms may be used to find N best hypotheses from the final lattice. The score for arc l is computed as: sl = log  σn snl + λL(wl |wP (l) ) + ωS(wl ) (2) where σn are the system weights constrained to sum to one, snl are the system specific arc posteriors, λ is a language model (LM) scaling factor, L(wl |wP (l) ) is the bi"
W10-1748,D08-1065,0,\N,Missing
W10-1763,N07-2014,0,0.209499,"tional level of lexical ambiguity. Readers can usually guess the correct pronunciation of words in non-diacritized text from the sentence and discourse context. Grammatical case on nouns and adjectives are also marked using diacritics at the end of words. Arabic MT systems use undiacritized text, since most available Arabic data is undiacritized. There has been work done on using diacritics in Automatic Speech Recognition, e.g. (Vergyri and Kirchho, 2004). However, the only previous work on using diacritization for MT is (Diab et al., 2007), which used the diacritization system described in (Habash and Rambow, 2007). It investigated the eect of using full diacritization as well as partial diacritization on MT results. The authors found that using full diacritics deteriorates MT performance. They used partial diacritization schemes, such as diacritizing only passive verbs, keeping the case endings diacritics, or only gemination diacritics. They also saw no gain in most congurations. The authors argued that the deterioration in performance is caused by the increase in the size of the vocabulary, which in turn makes the translation model sparser; as well as by errors during the automatic diacritization pr"
W10-1763,N03-1017,0,0.00933618,"porate context more directly by using POS tags on the target side to model word context. They augmented the target words with POS tags of the word itself and its surrounding words, and used the augmented words in decoding and for language model rescoring. They reported gains on IraqiArabic-to-English translation. Finally, using word-to-word context-free lexical translation probabilities has been shown to improve the performance of machine translation systems, even those using much more sophisticated models. This feature, usually called lexical smoothing, has been used in phrase-based systems (Koehn et al., 2003). Och et al. (2004) also found that including 2.2 Decision Trees Decision trees have been used extensively in various areas of machine learning, typically as a way to cluster patterns in order to improve classication (Duda et al., 2000). They have, for instance, been long used successfully in speech recognition to cluster contextdependent phoneme model states (Young et al., 1994). Decision trees have also been used in machine translation, although to a lesser extent. In this respect, our work is most similar to (Brunning et al., 2009), where the authors extended word alignment models for IBM"
W10-1763,N03-1000,0,0.0391861,"Missing"
W10-1763,W05-0711,0,0.498154,"Missing"
W10-1763,J03-1002,0,0.00523203,"Missing"
W10-1763,N04-1021,0,0.0309876,"irectly by using POS tags on the target side to model word context. They augmented the target words with POS tags of the word itself and its surrounding words, and used the augmented words in decoding and for language model rescoring. They reported gains on IraqiArabic-to-English translation. Finally, using word-to-word context-free lexical translation probabilities has been shown to improve the performance of machine translation systems, even those using much more sophisticated models. This feature, usually called lexical smoothing, has been used in phrase-based systems (Koehn et al., 2003). Och et al. (2004) also found that including 2.2 Decision Trees Decision trees have been used extensively in various areas of machine learning, typically as a way to cluster patterns in order to improve classication (Duda et al., 2000). They have, for instance, been long used successfully in speech recognition to cluster contextdependent phoneme model states (Young et al., 1994). Decision trees have also been used in machine translation, although to a lesser extent. In this respect, our work is most similar to (Brunning et al., 2009), where the authors extended word alignment models for IBM Model 1 and Hidden"
W10-1763,P03-1021,0,0.0222495,"Missing"
W10-1763,P02-1040,0,0.0784444,"Missing"
W10-1763,P07-1000,0,0.309138,"Missing"
W10-1763,P08-1066,0,0.0163869,"trees to deal with the sparsity side-eect. The decision trees cluster attribute-dependent source words by reducing the entropy of the lexical translation probabilities. We also present another method where, instead of clustering the attribute-dependent source words, the decision trees are used to interpolate attributedependent lexical translation probability models, and use those probabilities to compute a feature in the decoder log-linear model. The experiments we present in this paper were conducted on the translation of Arabicto-English newswire data using a hierarchical system based on (Shen et al., 2008), and using Arabic diacritics (see section 2.3) and part-ofspeech (POS) as source word attributes. Previous work that attempts to use Arabic diacritics in machine translation runs against the sparsity problem, and appears to lose most of the useful information contained in the diacritics when using partial diacritization (Diab et al., 2007). Using the methods proposed in this paper, we manage to obtain consistent improvements from diacritics against a strong baseline. The methods we propose, though, are not restrictive to Arabic-to-English translation. The same techniques can also be used with"
W10-1763,2007.mtsummit-papers.20,0,0.945259,"nslation probability models, and use those probabilities to compute a feature in the decoder log-linear model. The experiments we present in this paper were conducted on the translation of Arabicto-English newswire data using a hierarchical system based on (Shen et al., 2008), and using Arabic diacritics (see section 2.3) and part-ofspeech (POS) as source word attributes. Previous work that attempts to use Arabic diacritics in machine translation runs against the sparsity problem, and appears to lose most of the useful information contained in the diacritics when using partial diacritization (Diab et al., 2007). Using the methods proposed in this paper, we manage to obtain consistent improvements from diacritics against a strong baseline. The methods we propose, though, are not restrictive to Arabic-to-English translation. The same techniques can also be used with other language pairs and arbitrary word attribute types. The attributes we use in the described experiments are local; but long distance features can also be used. In the next section, we review relevant previous work in three areas: Lexical smoothing and lexical disambiguation techniques in machine translation; using decision trees in nat"
W10-1763,2006.amta-papers.25,1,0.797346,"Missing"
W10-1763,2007.tmi-papers.28,0,0.0422183,"Missing"
W10-1763,W04-1612,0,0.539465,"Missing"
W10-1763,H05-1097,0,0.0230829,"s, as a lexical smoothing feature in the decoder model. We present experiments using Arabic-to-English newswire data, and using Arabic diacritics and part-ofspeech as source word attributes, and show that the proposed method improves on a state-of-the-art translation system. 1 Introduction Modern statistical machine translation (SMT) models, such as phrase-based SMT or hierarchical SMT, implicitly incorporate source language context. It has been shown, however, that such systems can still benet from the explicit addition of lexical, syntactic or other kinds of context-informed word features (Vickrey et al., 2005; Gimpel and Smith, 2008; Brunning et al., 2009; Devlin, 2009). But the benet obtained from the addition of attribute information is in general countered by the increase in the model complexity, which in turn results in a sparser translation model when estimated from the same corpus of data. The increase in model sparsity usually results in a deterioration of translation quality. 428 Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 428–437, c Uppsala, Sweden, 15-16 July 2010. 2010 Association for Computational Linguistics IBM Model 1 (Brown et al"
W10-1763,H94-1062,0,0.0589736,"lities has been shown to improve the performance of machine translation systems, even those using much more sophisticated models. This feature, usually called lexical smoothing, has been used in phrase-based systems (Koehn et al., 2003). Och et al. (2004) also found that including 2.2 Decision Trees Decision trees have been used extensively in various areas of machine learning, typically as a way to cluster patterns in order to improve classication (Duda et al., 2000). They have, for instance, been long used successfully in speech recognition to cluster contextdependent phoneme model states (Young et al., 1994). Decision trees have also been used in machine translation, although to a lesser extent. In this respect, our work is most similar to (Brunning et al., 2009), where the authors extended word alignment models for IBM Model 1 and Hidden Markov Model (HMM) alignments. They used decision trees to cluster the context-dependent source words. Contexts belonging to the same cluster were grouped together during Expectation Maximization (EM) training, thus providing a more robust probability estimate. While Brunning et al. (2009) used the source context clusters for word alignments, we use the attribut"
W10-1763,P06-1073,0,0.313466,"Missing"
W10-1763,W08-0302,0,\N,Missing
W10-1763,J93-2003,0,\N,Missing
W10-1763,N09-1013,0,\N,Missing
W10-1763,D07-1007,0,\N,Missing
W10-1763,P07-1005,0,\N,Missing
W11-2119,D09-1005,0,0.302317,"during the backward pass noting that: gn Pij X  cijgn gn j∈Ji Pij X 1 1 1 1 s1 ⊕ s2 = hp1 + p2 , r1h + r2h , r1m + r2m i 1 1 1 1 s1 ⊗ s2 = hp1 p2 , p1 r2h + p2 r1h , p1 r2m + p2 r1m i gn X  X X ∂sil X l∈Li Pij j:l∈Ji ∂sil pil ∂λ Pij j∈Ji X ∂sil X l∈Li hnik ∂sil pil ∂λ cijgn X gn where is the derivative of µ(x, c) with respect to x, and the parentheses in the equations for mnik and hnik signify that the second terms do not depend on the edge l. Forward-backward algorithm under expectation semiring The sufficient statistics for graph expected BLEU can be computed using expectation semirings (Li and Eisner, 2009). Instead of computing single forward/backward or inside/outside scores, additional n-gram elements are tracked for matches and counts. For example in a bi-gram graph, the elements for edge l are represented by a 5-tuple7 sl = 1 , r 2 , r 1 , r 2 i where p = eγsil and: hpl , rlh l lh lm lm n rlh = X δ(cnil , g n )eγsil (16) µ0ign eγsil (17) gn n rlm = X gn Assuming the lattice is topologically sorted, the forward algorithm8 under expectation semiring for a 37 8 The sentence index i is dropped for brevity. For inside-outside algorithm, see (Li and Eisner, 2009). 162 µ0ign cijgn = gn j∈Ji µ0 (x,"
W11-2119,P03-1021,0,0.745958,"ration used to compute the clipped counts (matches) in the BLEU score (Papineni et al., 2002) was replaced by a differentiable function, so there was no need to use sub-gradient ascent as in CoBLEU. The expected BLEU (xBLEU) naturally generalizes to hypergraphs by simply replacing the forwardbackward algorithm with inside-outside algorithm when computing the expected n-gram counts and sufficient statistics for the gradient. The gradient ascent optimization of the xBLEU appears to be more stable than the gradient-free direct 1-best BLEU tuning or N -best list based minimum error rate training (Och, 2003), especially when tuning a large number of weights. On the official WMT11 language pairs with up to 30 weights, there was no significant benefit from maximizing xBLEU. However, on a 39 system multi-source combination (43 weights total), it yielded a significant gain over gradient-free BLEU tuning and N best list based expected BLEU tuning. 2 Hypothesis Alignment and Features The incremental hypothesis alignment with flexible matching (Rosti et al., 2009) produces a confusion network for each system output acting as a skeleton hypothesis for the ith source sentence. A confusion network is a gra"
W11-2119,P02-1040,0,0.0803035,"iews or policies, either expressed or implied, of the Defense Advanced Research Projects Agency or the Department of Defense. with flexible matching (Rosti et al., 2009). A novel bi-gram count feature was used in addition to the standard decoder features. The N-best list based expected BLEU tuning (Rosti et al., 2010), similar to the one proposed by Smith and Eisner (2006), was extended to operate on word lattices. This method is closely related to the consensus BLEU (CoBLEU) proposed by Pauls et al. (2009). The minimum operation used to compute the clipped counts (matches) in the BLEU score (Papineni et al., 2002) was replaced by a differentiable function, so there was no need to use sub-gradient ascent as in CoBLEU. The expected BLEU (xBLEU) naturally generalizes to hypergraphs by simply replacing the forwardbackward algorithm with inside-outside algorithm when computing the expected n-gram counts and sufficient statistics for the gradient. The gradient ascent optimization of the xBLEU appears to be more stable than the gradient-free direct 1-best BLEU tuning or N -best list based minimum error rate training (Och, 2003), especially when tuning a large number of weights. On the official WMT11 language"
W11-2119,D09-1147,0,0.300102,"presentation are those of the author/presenter and should not be interpreted as representing the official views or policies, either expressed or implied, of the Defense Advanced Research Projects Agency or the Department of Defense. with flexible matching (Rosti et al., 2009). A novel bi-gram count feature was used in addition to the standard decoder features. The N-best list based expected BLEU tuning (Rosti et al., 2010), similar to the one proposed by Smith and Eisner (2006), was extended to operate on word lattices. This method is closely related to the consensus BLEU (CoBLEU) proposed by Pauls et al. (2009). The minimum operation used to compute the clipped counts (matches) in the BLEU score (Papineni et al., 2002) was replaced by a differentiable function, so there was no need to use sub-gradient ascent as in CoBLEU. The expected BLEU (xBLEU) naturally generalizes to hypergraphs by simply replacing the forwardbackward algorithm with inside-outside algorithm when computing the expected n-gram counts and sufficient statistics for the gradient. The gradient ascent optimization of the xBLEU appears to be more stable than the gradient-free direct 1-best BLEU tuning or N -best list based minimum erro"
W11-2119,P07-1040,1,0.930481,"dinburgh, Scotland, UK, July 30–31, 2011. 2011 Association for Computational Linguistics vertices. Consecutive vertices are connected by one or more edges representing alternatives. Each edge l is associated with a token and a set of scores. A token may be a word, punctuation symbol, or special NULL token indicating a deletion in the alignment. The set of scores includes a vector of Ns system specific confidences, siln , indicating whether the token was aligned from the output of the system n.1 Other scores may include a language model (LM) score as well as non-NULL and NULL token indicators (Rosti et al., 2007). As Rosti et al. (2010) described, the networks for all skeletons are connected to a start and end vertex with NULL tokens in order to form a joint lattice with multiple parallel networks. The edges connecting the start vertex to the initial vertices in each network have a heuristic prior estimated from the alignment statistics at the confidence corresponding to the skeleton system. The edges connecting the final vertices of each network to the end vertex have all system confidences set to one, so the final edge does not change the score of any path. A single word confidence is produced from"
W11-2119,W09-0409,1,0.923987,"on networks for the BBN submissions to the WMT11 system combination task were built using incremental hypothesis alignment algorithm ∗ This work was supported by DARPA/I2O Contract No. HR0011-06-C-0022 under the GALE program (Approved for Public Release, Distribution Unlimited). The views, opinions, and/or findings contained in this article/presentation are those of the author/presenter and should not be interpreted as representing the official views or policies, either expressed or implied, of the Defense Advanced Research Projects Agency or the Department of Defense. with flexible matching (Rosti et al., 2009). A novel bi-gram count feature was used in addition to the standard decoder features. The N-best list based expected BLEU tuning (Rosti et al., 2010), similar to the one proposed by Smith and Eisner (2006), was extended to operate on word lattices. This method is closely related to the consensus BLEU (CoBLEU) proposed by Pauls et al. (2009). The minimum operation used to compute the clipped counts (matches) in the BLEU score (Papineni et al., 2002) was replaced by a differentiable function, so there was no need to use sub-gradient ascent as in CoBLEU. The expected BLEU (xBLEU) naturally gener"
W11-2119,W10-1748,1,0.552822,"upported by DARPA/I2O Contract No. HR0011-06-C-0022 under the GALE program (Approved for Public Release, Distribution Unlimited). The views, opinions, and/or findings contained in this article/presentation are those of the author/presenter and should not be interpreted as representing the official views or policies, either expressed or implied, of the Defense Advanced Research Projects Agency or the Department of Defense. with flexible matching (Rosti et al., 2009). A novel bi-gram count feature was used in addition to the standard decoder features. The N-best list based expected BLEU tuning (Rosti et al., 2010), similar to the one proposed by Smith and Eisner (2006), was extended to operate on word lattices. This method is closely related to the consensus BLEU (CoBLEU) proposed by Pauls et al. (2009). The minimum operation used to compute the clipped counts (matches) in the BLEU score (Papineni et al., 2002) was replaced by a differentiable function, so there was no need to use sub-gradient ascent as in CoBLEU. The expected BLEU (xBLEU) naturally generalizes to hypergraphs by simply replacing the forwardbackward algorithm with inside-outside algorithm when computing the expected n-gram counts and su"
W11-2119,P06-2101,0,0.568307,"under the GALE program (Approved for Public Release, Distribution Unlimited). The views, opinions, and/or findings contained in this article/presentation are those of the author/presenter and should not be interpreted as representing the official views or policies, either expressed or implied, of the Defense Advanced Research Projects Agency or the Department of Defense. with flexible matching (Rosti et al., 2009). A novel bi-gram count feature was used in addition to the standard decoder features. The N-best list based expected BLEU tuning (Rosti et al., 2010), similar to the one proposed by Smith and Eisner (2006), was extended to operate on word lattices. This method is closely related to the consensus BLEU (CoBLEU) proposed by Pauls et al. (2009). The minimum operation used to compute the clipped counts (matches) in the BLEU score (Papineni et al., 2002) was replaced by a differentiable function, so there was no need to use sub-gradient ascent as in CoBLEU. The expected BLEU (xBLEU) naturally generalizes to hypergraphs by simply replacing the forwardbackward algorithm with inside-outside algorithm when computing the expected n-gram counts and sufficient statistics for the gradient. The gradient ascen"
W11-2119,2006.amta-papers.25,0,0.0460353,"Missing"
X98-1014,A97-1029,1,0.810672,"Missing"
X98-1014,P96-1025,0,0.0206753,"c pointer labels are attached to all of the intermediate nodes. These labels serve to form a continuous chain between the relation and its argument. Figure 3 shows an augmented parse tree corresponding to the semantic annotation in Figure 2. Note that nodes with semantic labels ending in &quot;-r&quot; mark MUC reportable names and descriptors. Once a constrained parse is found, it must be augmented to reflect the semantic structure. Augmentation is a five step process. Statistical Model In SIFT's statistical model, augmented parse trees are generated according to a process similar to that described in Collins (1996, 1997). For each constituent, the head is generated first, followed by the modifiers, which are generated from the head outward. Head words, along with their part-of-speech tags and features, are generated for each modifier as soon as the modifier is created. Word features are introduced primarily to help with unknown words, as in Weischedel et al. (1993). 1) Nodes are inserted into the parse tree to distinguish names and descriptors that are not bracketed in the parse. For example, the parser produces a single noun phrase with no internal structure for &quot;Lt. Cmdr. David Edwin Lewis&quot;. Addition"
X98-1014,W97-0302,0,0.012406,"p,Cm_l,Wp)= 21 P ( c m I Cp,Chp,Cm_l,Wp) -I-~, 2 P ( c m ICp,Chp,Cm-l) For part-of-speech tags, the mixture components Pruning: Given multiple constituents that cover identical spans in the chart, only those constituents with probabilities within a threshold of the highest scoring constituent are maintained; all others are pruned. For purposes of pruning, and only for purposes of pruning, the prior probability of each constituent category is multiplied by the generative probability of that are: P'(t m I Cm, t h, w h) = 21 P(t m I cm, w h ) + 2 2 P(t m ]Cm,th) +2 3 P(t m I c m) 80 constituent (Goodman, 1997). We can think of this prior probability as an estimate of the probability of generating a subtree with the constituent category, starting at the topmost node. Thus, the scores used in pruning can be considered as the product of: 1. The probability of generating a constituent of the specified category, starting at the topmost node. 2. The probability of generating the structure beneath that constituent, having already generated a constituent of that category. trained on 200 articles annotated with full MUC answer keys, so that even non-local relations were marked. (That level of semantic annot"
X98-1014,J93-2004,0,\N,Missing
X98-1014,J93-2006,1,\N,Missing
X98-1014,P97-1003,0,\N,Missing
