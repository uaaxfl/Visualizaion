1987.mtsummit-1.23,C86-1155,1,0.396745,"Missing"
1987.mtsummit-1.31,J85-2001,1,0.884854,"ch, 1986. The main objective of the project was to develop a proto-type system which would demonstrate the feasibility of MT systems for certain restricted subject domains and document types. We considered that this initial objective of the project was well fulfilled so that we started another four year project (the MU-II project) from April, 1986. The new project aims to develop & system which will be used for daily translation services in JICST (Japan Information Center for Science and Technology) from 1990. Because the technical details of the MU-project have been given in the other papers (1) (2) (3) (4) (5), we will report overall results of the Mu Project and brief introduction of the outline of the new project. 2 Outline of the Mu Systems The following gives the rough sketch of the current MU systems. 1. Basic Approach: Transfer Approach 2. Design Principle: lexicon Driven Processing Neutral Dictionary Heuristically Guided Processing 3. Language Pairs: Japanese to English, English to Japanese Two systems have been developed, both of which uses same software systems 4. Subject Fields: Electrical Engineering The subject fields will be extended in the other scientific and technolo"
1987.mtsummit-1.31,P84-1069,1,0.892706,"e main objective of the project was to develop a proto-type system which would demonstrate the feasibility of MT systems for certain restricted subject domains and document types. We considered that this initial objective of the project was well fulfilled so that we started another four year project (the MU-II project) from April, 1986. The new project aims to develop & system which will be used for daily translation services in JICST (Japan Information Center for Science and Technology) from 1990. Because the technical details of the MU-project have been given in the other papers (1) (2) (3) (4) (5), we will report overall results of the Mu Project and brief introduction of the outline of the new project. 2 Outline of the Mu Systems The following gives the rough sketch of the current MU systems. 1. Basic Approach: Transfer Approach 2. Design Principle: lexicon Driven Processing Neutral Dictionary Heuristically Guided Processing 3. Language Pairs: Japanese to English, English to Japanese Two systems have been developed, both of which uses same software systems 4. Subject Fields: Electrical Engineering The subject fields will be extended in the other scientific and technological fields"
1987.mtsummit-1.31,C86-1021,1,0.862899,"in objective of the project was to develop a proto-type system which would demonstrate the feasibility of MT systems for certain restricted subject domains and document types. We considered that this initial objective of the project was well fulfilled so that we started another four year project (the MU-II project) from April, 1986. The new project aims to develop & system which will be used for daily translation services in JICST (Japan Information Center for Science and Technology) from 1990. Because the technical details of the MU-project have been given in the other papers (1) (2) (3) (4) (5), we will report overall results of the Mu Project and brief introduction of the outline of the new project. 2 Outline of the Mu Systems The following gives the rough sketch of the current MU systems. 1. Basic Approach: Transfer Approach 2. Design Principle: lexicon Driven Processing Neutral Dictionary Heuristically Guided Processing 3. Language Pairs: Japanese to English, English to Japanese Two systems have been developed, both of which uses same software systems 4. Subject Fields: Electrical Engineering The subject fields will be extended in the other scientific and technological fields in"
1987.mtsummit-1.31,C86-1155,1,0.772754,"Missing"
1990.tc-1.1,C86-1021,1,0.819266,"Missing"
1990.tc-1.1,C88-2142,1,0.923113,"Missing"
1990.tc-1.1,1987.mtsummit-1.10,0,0.0621419,"Missing"
1990.tc-1.1,J85-2003,0,0.0469891,"Missing"
1993.tmi-1.11,P91-1021,0,0.112431,"ontext: One can check whether specified terms exist in LF of the source text or the target text. The conditions 'in-source' and 'in-target' ensure respectively that the transfer rule applies only when LF of the source or the target contains the specified term (or set of terms). For example, ig([aspect(E,imperfective), cond([in(tense(E,simul))])], [aspect(E,imperfective)]) This rule establishes a translation pair of terms representing the aspects in the S-LF and T-LF (both imperfective)6, under the condition that LF of the source text contains a term with a specific tense value (simultaneous). [2] Shift: Changes in Indices: A shift condition holds between S-LF and T-LF when an index of the source is substituted in all its instances by another index in the target. This handles cases of complex transfer like head switching [Lindop 1991], and can also be used recursively in case of combined syntactic changes [Sun 1992]. For example, ig([prendere(E), decisione(D), obj(E,D), cond([shift(2,D,E)])], [(apofasizo(E)]). 4 Though we omit the details, grammatical properties such as Number, Voice, etc. are also represented as properties of indices. 5 Indices play a dual role in our framework as lin"
1993.tmi-1.11,C86-1081,0,0.0251476,"interpretation 6 Though the terms in S-LF and T-LF are the same in this example, they have completely independent semantics. That is, the term in S-LF only means that an event E is described by a sentence with imperfective aspect in Italian. -139- In this rule, the translation pair between the support verb construction prendere una decisione(to make a decision) and the verb apofasizo (to decide) is defined, and the shift condition shows that the index for the noun decisione in the Italian LF is stated to be lexically included in the index for the verb apofasizo and disappear in the Greek LF. [3] Conditions which Invoke Knowledge-based Processing: Conditions can be a set of terms which have to be proved by the knowledge base. Reasoning is performed on the basis of the content of the knowledge-base and LF of the source. A special functor 'pred' is used which indicates that the translation pair is valid only if the formulae (or set of formula) specified in the condition is proved to be true. ig([aspect(E,perfective), cond([pred(mult-occurrence(E)), shift(1,E1,E)])], [set(E,E1), aspect(E1,imperfective)]) This rule establishes equivalence between perfective aspect in Italian and imperfect"
1993.tmi-1.11,C88-2144,0,0.0673898,"Missing"
1993.tmi-1.11,C92-2098,0,0.0612139,"Missing"
1993.tmi-1.11,C82-1057,0,0.0557255,"Missing"
1993.tmi-1.11,J82-2005,0,0.0670117,"Missing"
1993.tmi-1.11,E85-1003,0,0.0201865,"Missing"
1993.tmi-1.12,P91-1021,0,0.138895,"ontext: One can check whether specified terms exist in LF of the source text or the target text. The conditions 'in-source' and 'in-target' ensure respectively that the transfer rule applies only when LF of the source or the target contains the specified term (or set of terms). For example, ig([aspect(E,imperfective), cond([in(tense(E,simul))])], [aspect(E,imperfective)]) This rule establishes a translation pair of terms representing the aspects in the S-LF and T-LF (both imperfective)6, under the condition that LF of the source text contains a term with a specific tense value (simultaneous). [2] Shift: Changes in Indices: A shift condition holds between S-LF and T-LF when an index of the source is substituted in all its instances by another index in the target. This handles cases of complex transfer like head switching [Lindop 1991], and can also be used recursively in case of combined syntactic changes [Sun 1992]. For example, ig([prendere(E), decisione(D), obj(E,D), cond([shift(2,D,E)])], [(apofasizo(E)]). 4 Though we omit the details, grammatical properties such as Number, Voice, etc. are also represented as properties of indices. 5 Indices play a dual role in our framework as lin"
1993.tmi-1.12,C86-1081,0,0.0309569,"interpretation 6 Though the terms in S-LF and T-LF are the same in this example, they have completely independent semantics. That is, the term in S-LF only means that an event E is described by a sentence with imperfective aspect in Italian. -139- In this rule, the translation pair between the support verb construction prendere una decisione(to make a decision) and the verb apofasizo (to decide) is defined, and the shift condition shows that the index for the noun decisione in the Italian LF is stated to be lexically included in the index for the verb apofasizo and disappear in the Greek LF. [3] Conditions which Invoke Knowledge-based Processing: Conditions can be a set of terms which have to be proved by the knowledge base. Reasoning is performed on the basis of the content of the knowledge-base and LF of the source. A special functor 'pred' is used which indicates that the translation pair is valid only if the formulae (or set of formula) specified in the condition is proved to be true. ig([aspect(E,perfective), cond([pred(mult-occurrence(E)), shift(1,E1,E)])], [set(E,E1), aspect(E1,imperfective)]) This rule establishes equivalence between perfective aspect in Italian and imperfect"
1993.tmi-1.12,C88-2144,0,0.0678947,"Missing"
1993.tmi-1.12,C92-2098,0,0.0612357,"Missing"
1993.tmi-1.12,C82-1057,0,0.0566445,"Missing"
1993.tmi-1.12,J82-2005,0,0.0450245,"Missing"
1993.tmi-1.12,E85-1003,0,0.0270691,"Missing"
2004.iwslt-evaluation.1,2004.iwslt-evaluation.5,0,0.0434217,"Missing"
2004.iwslt-evaluation.1,2004.iwslt-evaluation.6,0,0.0792214,"Missing"
2004.iwslt-evaluation.1,2004.iwslt-evaluation.8,1,0.857983,"Missing"
2004.iwslt-evaluation.1,2004.iwslt-evaluation.11,0,0.00856795,"Missing"
2004.iwslt-evaluation.1,2004.iwslt-evaluation.13,0,0.00858588,"Missing"
2004.iwslt-evaluation.1,2004.iwslt-evaluation.14,0,0.0294958,"Missing"
2004.iwslt-evaluation.1,2004.iwslt-evaluation.7,0,0.0434735,"Missing"
2004.iwslt-evaluation.1,2004.iwslt-evaluation.10,0,0.0223145,"Missing"
2004.iwslt-evaluation.1,2004.iwslt-evaluation.12,0,0.0295568,"Missing"
2004.iwslt-evaluation.1,2004.iwslt-evaluation.15,0,0.0273582,"Missing"
2004.iwslt-evaluation.1,niessen-etal-2000-evaluation,0,\N,Missing
2004.iwslt-evaluation.1,2004.iwslt-evaluation.2,1,\N,Missing
2004.iwslt-evaluation.1,P02-1040,0,\N,Missing
2004.iwslt-evaluation.1,W05-0909,0,\N,Missing
2004.iwslt-evaluation.1,2005.iwslt-1.1,0,\N,Missing
2004.iwslt-evaluation.1,2005.iwslt-1.6,0,\N,Missing
2004.iwslt-evaluation.1,2004.iwslt-evaluation.4,0,\N,Missing
2004.iwslt-evaluation.1,zhang-etal-2004-interpreting,0,\N,Missing
2004.iwslt-evaluation.1,P03-1021,0,\N,Missing
2004.iwslt-evaluation.1,2004.iwslt-evaluation.9,0,\N,Missing
2008.amta-papers.19,J93-2003,0,0.0116609,"e ME model for estimating possible domains of a given source term. Section 4 presents an algorithm to mine domain specific MPTPs from a bilingual lexicon. In Section 5, we describe the decoding algorithm for the translation model. Section 6 reports our experimental setup and results. After reviewing the related work briefly in Section 7, we conclude this paper in Section 8. 3 Cascaded translation model Our cascaded translation model is a direct translation model using a LM for ranking. It can be regarded as a reversal of the order of the source and target languages in the noisy-channel model (Brown et al., 1993). Let E and C denote terms in the source and target languages. The translation model produces the optimal term C ∗ given E, C ∗ = arg max p(C|E)p(C). C (1) In order to choose a target term C by making use of the domain information of the source term E, we integrate the domain D in the channel model, X p(C|E) = p(C|E, D)p(D|E). (2) D We estimate D of E using a log-linear ME model: ( ) X 1 exp λk hk (D, E) , (3) pΛ (D|E) = ZΛ (E) k ( ) X X ZΛ (E) = exp λk hk (D, E) . D k Here, pΛ (D|E) is computed by the feature vector {hk }, and the feature weight vector Λ = {λk }. We use the words and the morp"
2008.amta-papers.19,2007.mtsummit-papers.9,0,0.245396,"Missing"
2008.amta-papers.19,J07-2003,0,0.101549,"Missing"
2008.amta-papers.19,H05-1085,0,0.0742789,"Missing"
2008.amta-papers.19,E03-1076,0,0.156072,"24 |20 |14 transliteration 17 |19 |16 １ － 羟基 － ２ － 丙酮/１ － 羟基 － ２ － 丙酮 减压 舱/减压 室; rose/玫瑰/蔷薇 current - mode logic/电流 型 逻辑/电流 － 型 逻辑 light antiaircraft artillery/小高 炮/轻 高射 炮兵 specific gravity cell /比 重 电池/比 重 细胞 autohemorrhage/自出血/autohemorrhage grc/盖尔研究公司/grc kallman syndrome/卡尔曼 综合征/kallman 综合征 1 - hydroxy - 2 - propanone/ decompression chamber/ Table 7: Error analysis on translating the MD test set. M and P denote Moses 4 and Pharaoh respectively. main specific n-gram character/word LMs. In addition, integrating abbreviation lexicons and transliteration models are also needed. 7 Related work Koehn and Knight (2003a) investigated several empirical methods for splitting German compounds. Even though they did not apply morphology analysis to German compounds, their work integrated the idea of word-splitting into a phrase-based translation system. They reported significant results in translating German base noun phrases into English. We built our cascaded translation model inspired by their work. The differences are that we further employed morphological analysis to technical terms, grouped the morpheme correspondences, and proposed using domain information for morphemelevel translation disambiguation. Vir"
2008.amta-papers.19,P03-1040,0,0.123589,"24 |20 |14 transliteration 17 |19 |16 １ － 羟基 － ２ － 丙酮/１ － 羟基 － ２ － 丙酮 减压 舱/减压 室; rose/玫瑰/蔷薇 current - mode logic/电流 型 逻辑/电流 － 型 逻辑 light antiaircraft artillery/小高 炮/轻 高射 炮兵 specific gravity cell /比 重 电池/比 重 细胞 autohemorrhage/自出血/autohemorrhage grc/盖尔研究公司/grc kallman syndrome/卡尔曼 综合征/kallman 综合征 1 - hydroxy - 2 - propanone/ decompression chamber/ Table 7: Error analysis on translating the MD test set. M and P denote Moses 4 and Pharaoh respectively. main specific n-gram character/word LMs. In addition, integrating abbreviation lexicons and transliteration models are also needed. 7 Related work Koehn and Knight (2003a) investigated several empirical methods for splitting German compounds. Even though they did not apply morphology analysis to German compounds, their work integrated the idea of word-splitting into a phrase-based translation system. They reported significant results in translating German base noun phrases into English. We built our cascaded translation model inspired by their work. The differences are that we further employed morphological analysis to technical terms, grouped the morpheme correspondences, and proposed using domain information for morphemelevel translation disambiguation. Vir"
2008.amta-papers.19,koen-2004-pharaoh,0,0.0553447,"Missing"
2008.amta-papers.19,2005.mtsummit-papers.11,0,0.0154416,"der to find correspondences of morpheme phrases between the source and target languages, we propose an algorithm to mine morpheme phrase translation pairs from a bilingual lexicon. We also build a cascaded translation model that dynamically shifts translation units from phrase level to word and morpheme phrase levels. The experimental results show the significant improvements over the current phrase-based SMT systems. 1 Introduction Statistical machine translation (SMT) provides an impressive framework in which a machine translation system can be built, only if a parallel corpus is available. Koehn (2005) collected a corpus of parallel text in 11 languages and trained SMT systems for 110 language pairs within three weeks. However, the experimental results also revealed several language-specific issues: morphologically-rich languages (e.g., German) were more difficult to translate into than from, and two distant languages (e.g., 202 Finnish and English) that have discrepant morphologies (word structures) were difficult to deal with. These issues arise because SMT systems usually employ words as the minimum units of translation, even when some elements represented by individual words in one lang"
2008.amta-papers.19,P08-1113,0,0.103495,"Missing"
2008.amta-papers.19,J04-4002,0,0.0711605,"Missing"
2008.amta-papers.19,W07-0704,0,0.402782,"., German) were more difficult to translate into than from, and two distant languages (e.g., 202 Finnish and English) that have discrepant morphologies (word structures) were difficult to deal with. These issues arise because SMT systems usually employ words as the minimum units of translation, even when some elements represented by individual words in one language are included in the morphology of another language. Numerous researchers have proposed a variety of approaches that make use of morphological information in machine translation (Popovi´c and Ney, 2004; Goldwater and McClosky, 2005; Oflazer and El-Kahlout, 2007; Virpioja et al., 2007; Oflazer, 2008). Most studies assume that the input language (e.g., Arabic, Catalan, Czech, and Spanish) is morphologically richer than the output language (English) because translating from an information-rich language into an information-poor language is easier than the other way around (Koehn 2005). Some recent studies (Oflazer and El-Kahlout, 2007; Oflazer, 2008) explored the opposite direction (e.g., English to Turkish), but more case studies are necessary. Moreover, a number of new technical terms are emerging daily in English, which is the dominant international"
2008.amta-papers.19,P02-1040,0,0.074818,"Missing"
2008.amta-papers.19,popovic-ney-2004-towards,0,0.155206,"Missing"
2008.amta-papers.19,2007.mtsummit-papers.65,0,0.0855577,"Missing"
2009.iwslt-evaluation.15,P08-1023,0,0.0260161,"a2 are word alignments between a source language phrase f1m and a target language phrase en1 ; p(·) and l(·) are phrasal translation probabilities and lexical weights, respectively; wp stands for the word penalty. In order to be used in CKY-style decoding [10], a rule in the form of (1) can be easily transformed into an end-to-end Hiero-style [10] translation rule: (2) And the corresponding synchronous PCFG production takes the form of: X →∑i wi ∗log(pi ) f1m , en1 . (3) It has been proved that the end-to-end phrase table significantly influence the translation result of syntax-based systems [11, 12]. Note that the phrase table mentioned here 2 http://www.statmt.org/moses/ is not constrained to be linguistic phrases, i.e., a phrase inside the table is not necessarily covered by a subtree. This makes the phrase table more flexible to be used than in the tree/forest-to-string systems [11, 12] where additional approaches have to be employed in order to make use of nonlinguistic phrases [13, 14]. Return back to Figure 1, HPSG trees attached with PASs are generated by parsing the target language sentences using Enju. Then, we extract HPSG and PAS based translation rules from the word-aligned a"
2009.iwslt-evaluation.15,P07-1089,0,0.0155645,"hronous PCFG production takes the form of: X →∑i wi ∗log(pi ) f1m , en1 . (3) It has been proved that the end-to-end phrase table significantly influence the translation result of syntax-based systems [11, 12]. Note that the phrase table mentioned here 2 http://www.statmt.org/moses/ is not constrained to be linguistic phrases, i.e., a phrase inside the table is not necessarily covered by a subtree. This makes the phrase table more flexible to be used than in the tree/forest-to-string systems [11, 12] where additional approaches have to be employed in order to make use of nonlinguistic phrases [13, 14]. Return back to Figure 1, HPSG trees attached with PASs are generated by parsing the target language sentences using Enju. Then, we extract HPSG and PAS based translation rules from the word-aligned and target-language-parsed parallel corpora. The HPSG-based xRs (tree-to-string, [15]) translation rules (binarized inversely [16]) are extracted using the GHKM minimal-rule extraction algorithm of [1]. In order to trace the lexical level translation information (similar to PASbased rules (Section 3 and Figure 2)), we remember the endto-end alignments in an HPSG-based translation rule. The ideas a"
2009.iwslt-evaluation.15,N04-1035,0,0.47118,"by Head-driven Phrase Structure Grammar and Predicate-Argument Structures. We report the results of our system on both the development and test sets. 1. Introduction How can we integrate deep syntactic information into current statistical machine translation (SMT) systems to further improve the accuracy and fluency? How to deal with global reordering problem for a language pair that do not share isomorphic syntactic structures? These remain to be essential issues faced by current SMT research community. In this paper, we manage to answer these questions in terms of string-to-tree translation [1, 2, 3]. English, the target language in our case study, is the most popularly researched language with plenty resources and syntactic parsers. In contrast to commit to Probabilistic Context-Free Grammar (PCFG) which only generates shallow trees of English [1, 2, 3], we propose the use of deep parse trees and semantic dependencies described respectively by Head-driven Phrase Structure Grammar (HPSG) [4, 5] and PredicateArgument Structures (PASs). We illustrate two major characteristics that an HPSG tree (used by us) differs from a PCFG tree. First, a node in an HPSG tree is represented by a typed fea"
2009.iwslt-evaluation.15,P06-1121,0,0.0665645,"by Head-driven Phrase Structure Grammar and Predicate-Argument Structures. We report the results of our system on both the development and test sets. 1. Introduction How can we integrate deep syntactic information into current statistical machine translation (SMT) systems to further improve the accuracy and fluency? How to deal with global reordering problem for a language pair that do not share isomorphic syntactic structures? These remain to be essential issues faced by current SMT research community. In this paper, we manage to answer these questions in terms of string-to-tree translation [1, 2, 3]. English, the target language in our case study, is the most popularly researched language with plenty resources and syntactic parsers. In contrast to commit to Probabilistic Context-Free Grammar (PCFG) which only generates shallow trees of English [1, 2, 3], we propose the use of deep parse trees and semantic dependencies described respectively by Head-driven Phrase Structure Grammar (HPSG) [4, 5] and PredicateArgument Structures (PASs). We illustrate two major characteristics that an HPSG tree (used by us) differs from a PCFG tree. First, a node in an HPSG tree is represented by a typed fea"
2009.iwslt-evaluation.15,N09-1025,0,0.211268,"by Head-driven Phrase Structure Grammar and Predicate-Argument Structures. We report the results of our system on both the development and test sets. 1. Introduction How can we integrate deep syntactic information into current statistical machine translation (SMT) systems to further improve the accuracy and fluency? How to deal with global reordering problem for a language pair that do not share isomorphic syntactic structures? These remain to be essential issues faced by current SMT research community. In this paper, we manage to answer these questions in terms of string-to-tree translation [1, 2, 3]. English, the target language in our case study, is the most popularly researched language with plenty resources and syntactic parsers. In contrast to commit to Probabilistic Context-Free Grammar (PCFG) which only generates shallow trees of English [1, 2, 3], we propose the use of deep parse trees and semantic dependencies described respectively by Head-driven Phrase Structure Grammar (HPSG) [4, 5] and PredicateArgument Structures (PASs). We illustrate two major characteristics that an HPSG tree (used by us) differs from a PCFG tree. First, a node in an HPSG tree is represented by a typed fea"
2009.iwslt-evaluation.15,J08-1002,1,0.797575,"e pointer to semantic arguments Table 1: Examples of syntactic/semantic features extracted from HPSG signs that are included in the output of Enju (top and bottom stands for features of phrasal and lexical nodes, respectively). ing translation. The idea proposed in this paper can be considered as a natural integration of syntactic information and semantic dependency information for assisting string-to-tree translation. We call the integration natural here, because the HPSG tree and PAS of an English sentence are generated synchronously by using a state-of-the-art HPSG parser on English, Enju1 [6]. Note that the information available in the output of Enju is a fairly crude approximation of the TFS used in the full HPSG grammar [4, 5] due to practicable considerations [6]. Although the information taken from Enju’s output is much more than the commonly used PCFG parser, the HPSG-based translation rule is still extracted from an approximation of the full HPSG grammar. 2. System Outline 2.1. Parameter Estimation The diagram of parameter estimation in our system is shown in Figure 1, which is similar to most syntax-based SMT sys1 http://www-tsujii.is.s.u-tokyo.ac.jp/enju/index.html Proceed"
2009.iwslt-evaluation.15,J03-1002,0,0.00225715,"ction and estimation, GHKM [1] Phrase-training (Moses [9]) PAS-based translation rules Phrase translation table HPSG-based translation rules Binarizing (Section 4) To Hiero-style (Section 2.1) α1 SRILM [17] 5-gram LM α2 α3 Binarizing (Section 4) α4 Minimum Error Rate Training on the development sets (Z-mert [18]) Figure 1: The parameter estimation and rule combination diagram of our system. tems [1, 7]. Given bilingual parallel corpora, we first tokenize the source and target sentences (e.g., word segmentation of Chinese; punctuation segmentation and lowercase of English). Then, we use GIZA++ [8] and grow-diag-finaland balancing strategy (dealing with unaligned source/target words) [9] on the tokenized parallel corpora to obtain a phrase-aligned parallel corpora. A phrase translation table (PTT) is estimated from the phrase-aligned parallel corpora. We implement the step of phrase table extraction employing the Moses toolkit2 [9]. Recall that the Moses-style phrase translation rule takes the following form: Here, a1 and a2 are word alignments between a source language phrase f1m and a target language phrase en1 ; p(·) and l(·) are phrasal translation probabilities and lexical weights,"
2009.iwslt-evaluation.15,P07-2045,0,0.00823132,"The diagram of parameter estimation in our system is shown in Figure 1, which is similar to most syntax-based SMT sys1 http://www-tsujii.is.s.u-tokyo.ac.jp/enju/index.html Proceedings of IWSLT 2009, Tokyo - Japan Original parallel corpora Target language parsing (Enju [6]) Lexical analyzing Phrase-aligned and target-language-parsed parallel corpora Tokenized parallel corpora Predicate-Argument Structure based translation rule extraction and estimation (Section 3) GIZA++ & balancing Phrase-aligned parallel corpora HPSG translation rule extraction and estimation, GHKM [1] Phrase-training (Moses [9]) PAS-based translation rules Phrase translation table HPSG-based translation rules Binarizing (Section 4) To Hiero-style (Section 2.1) α1 SRILM [17] 5-gram LM α2 α3 Binarizing (Section 4) α4 Minimum Error Rate Training on the development sets (Z-mert [18]) Figure 1: The parameter estimation and rule combination diagram of our system. tems [1, 7]. Given bilingual parallel corpora, we first tokenize the source and target sentences (e.g., word segmentation of Chinese; punctuation segmentation and lowercase of English). Then, we use GIZA++ [8] and grow-diag-finaland balancing strategy (dealing wi"
2009.iwslt-evaluation.15,J07-2003,0,0.634756,"9] on the tokenized parallel corpora to obtain a phrase-aligned parallel corpora. A phrase translation table (PTT) is estimated from the phrase-aligned parallel corpora. We implement the step of phrase table extraction employing the Moses toolkit2 [9]. Recall that the Moses-style phrase translation rule takes the following form: Here, a1 and a2 are word alignments between a source language phrase f1m and a target language phrase en1 ; p(·) and l(·) are phrasal translation probabilities and lexical weights, respectively; wp stands for the word penalty. In order to be used in CKY-style decoding [10], a rule in the form of (1) can be easily transformed into an end-to-end Hiero-style [10] translation rule: (2) And the corresponding synchronous PCFG production takes the form of: X →∑i wi ∗log(pi ) f1m , en1 . (3) It has been proved that the end-to-end phrase table significantly influence the translation result of syntax-based systems [11, 12]. Note that the phrase table mentioned here 2 http://www.statmt.org/moses/ is not constrained to be linguistic phrases, i.e., a phrase inside the table is not necessarily covered by a subtree. This makes the phrase table more flexible to be used than in"
2009.iwslt-evaluation.15,P09-1020,0,0.0220831,"hronous PCFG production takes the form of: X →∑i wi ∗log(pi ) f1m , en1 . (3) It has been proved that the end-to-end phrase table significantly influence the translation result of syntax-based systems [11, 12]. Note that the phrase table mentioned here 2 http://www.statmt.org/moses/ is not constrained to be linguistic phrases, i.e., a phrase inside the table is not necessarily covered by a subtree. This makes the phrase table more flexible to be used than in the tree/forest-to-string systems [11, 12] where additional approaches have to be employed in order to make use of nonlinguistic phrases [13, 14]. Return back to Figure 1, HPSG trees attached with PASs are generated by parsing the target language sentences using Enju. Then, we extract HPSG and PAS based translation rules from the word-aligned and target-language-parsed parallel corpora. The HPSG-based xRs (tree-to-string, [15]) translation rules (binarized inversely [16]) are extracted using the GHKM minimal-rule extraction algorithm of [1]. In order to trace the lexical level translation information (similar to PASbased rules (Section 3 and Figure 2)), we remember the endto-end alignments in an HPSG-based translation rule. The ideas a"
2009.iwslt-evaluation.15,N04-1014,0,0.0309107,"t constrained to be linguistic phrases, i.e., a phrase inside the table is not necessarily covered by a subtree. This makes the phrase table more flexible to be used than in the tree/forest-to-string systems [11, 12] where additional approaches have to be employed in order to make use of nonlinguistic phrases [13, 14]. Return back to Figure 1, HPSG trees attached with PASs are generated by parsing the target language sentences using Enju. Then, we extract HPSG and PAS based translation rules from the word-aligned and target-language-parsed parallel corpora. The HPSG-based xRs (tree-to-string, [15]) translation rules (binarized inversely [16]) are extracted using the GHKM minimal-rule extraction algorithm of [1]. In order to trace the lexical level translation information (similar to PASbased rules (Section 3 and Figure 2)), we remember the endto-end alignments in an HPSG-based translation rule. The ideas are described in [1, 11] in detail. In order to use the dependency structure, we describe a linear-time algorithm based on minimum covering trees to extract PAS-based translation rules (Section 3). SRI Language Modeling (SRILM) toolkit3 [17] is employed to train a 5-gram language model"
2009.iwslt-evaluation.15,N06-1033,0,0.449598,"a phrase inside the table is not necessarily covered by a subtree. This makes the phrase table more flexible to be used than in the tree/forest-to-string systems [11, 12] where additional approaches have to be employed in order to make use of nonlinguistic phrases [13, 14]. Return back to Figure 1, HPSG trees attached with PASs are generated by parsing the target language sentences using Enju. Then, we extract HPSG and PAS based translation rules from the word-aligned and target-language-parsed parallel corpora. The HPSG-based xRs (tree-to-string, [15]) translation rules (binarized inversely [16]) are extracted using the GHKM minimal-rule extraction algorithm of [1]. In order to trace the lexical level translation information (similar to PASbased rules (Section 3 and Figure 2)), we remember the endto-end alignments in an HPSG-based translation rule. The ideas are described in [1, 11] in detail. In order to use the dependency structure, we describe a linear-time algorithm based on minimum covering trees to extract PAS-based translation rules (Section 3). SRI Language Modeling (SRILM) toolkit3 [17] is employed to train a 5-gram language model (LM) on the tokenized target language side w"
2009.iwslt-evaluation.15,P03-1021,0,0.00667254,"lexical level translation information (similar to PASbased rules (Section 3 and Figure 2)), we remember the endto-end alignments in an HPSG-based translation rule. The ideas are described in [1, 11] in detail. In order to use the dependency structure, we describe a linear-time algorithm based on minimum covering trees to extract PAS-based translation rules (Section 3). SRI Language Modeling (SRILM) toolkit3 [17] is employed to train a 5-gram language model (LM) on the tokenized target language side with Kneser-Ney smoothing. Toolkit Z-mert4 [18] is used for Minimum-Error Rate Training (MERT) [19]. 3 http://www.speech.sri.com/projects/srilm/ 4 http://www.cs.jhu.edu/ - 100 - ozaidan/zmert/ Proceedings of IWSLT 2009, Tokyo - Japan 2.2. Rule Combination Since the PTT, the HPSG-based rules, and the PAS-based rules are independently extracted and estimated, the distribution overlapping among them is inevitable. As shown in Figure 1, we use Z-mert to tune their weights on the development sets. The optimal derivation is computed by:   3 ∑  ∑ αi wj 4 d∗ = arg max log pji i (d) + log pα . LM (d)  d∈D  i=1 ji Here, pj1 , pj2 , and pj3 represent the feature subsets from PPT, binarized PAS-b"
2009.iwslt-evaluation.15,W05-1506,0,0.0324405,"3], we define a PASR to be an xRs rule and binarize it in an inverse way [16]. 3.2.1. Definition of PASR 2.3. Decoding We use a CKY-style algorithm with beam-pruning and cubepruning [10] to decode Chinese sentences. For efficient decoding with integrated N-gram LMs, we binarize all translation rules into rules that contain at most two variables and can be incrementally scored by LM [16]. For each source language sentence f , the output of the chart-parsing algorithm is expressed as a hyper-graph representing a set of derivations. Given a hyper-graph for f , we use the Algorithm 3 described in [20] to extract its k-best derivations. Since different derivations may lead to the same target language string e, we further adopt Algorithm 3’s modification (i.e., keep a hash-table to maintain the unique target sentences [21]) to efficiently generate the unique k-best translations. 3. PAS-based Translation Rule Extraction In this section, we first express an example of an HPSG tree attached with PASs, and then describe the data structure and an extraction algorithm of PAS-based translation rules (short as PASR, hereafter). A PASR is a 4-tuple ⟨S, T, A, n⟩, which describes the alignment A betwee"
2009.iwslt-evaluation.15,2006.amta-papers.8,0,0.0203907,"e included in the three subsets. In addition, number of phrases and words are contained in pj1 , and number of rules are included in pj2 and pj3 . three elements in the source and target language sides. In particular, we observe that the “head” of this rule is ignored whose arguments can be generalized into variables. Note that PASs are not only attached to verbs in a sentence, but also to all other words in the sentence. The corresponding PASRs are illustrated in Figure 2 as well. Even apparently similar in data structure, we argue our PASRs are still different from the traditional xRs rules [1, 11, 21], since the knowledge of semantic dependencies are further explicitly employed. We give the formal definitions and a lineartime rule extraction algorithm in the following subsections. 3.2. Definitions Using a strategy similar to most string-to-tree systems [3], we define a PASR to be an xRs rule and binarize it in an inverse way [16]. 3.2.1. Definition of PASR 2.3. Decoding We use a CKY-style algorithm with beam-pruning and cubepruning [10] to decode Chinese sentences. For efficient decoding with integrated N-gram LMs, we binarize all translation rules into rules that contain at most two varia"
2009.iwslt-evaluation.15,P02-1040,0,0.0763328,"Missing"
2009.mtsummit-posters.26,C02-2020,0,0.338268,"), but also for other natural language processing applications such as cross-language information retrieval (Grefenstette, 1998). At first, researchers constructed bilingual dictionary from parallel corpora. For example, Wu (1994) extracted English-Chinese translation lexicon through statistical training on a large parallel corpus. But for some languages, collecting parallel corpora is not easy. Thus, utilizing comparable corpora, in which texts are not translation of each other but share similar concepts, to extract bilingual dictionary has drawn more and more attention recently (Fung, 2000; Chiao and Zweigenbaum, 2002; Daille and Morin, 2005; Robitaille et al., 2006; Morin et al., 2007; Otero, 2008; Saralegi et al., 2008). There are two popular strategies for constructing bilingual dictionary from comparable corpora: context-based strategy and syntax-based strategy. Context-based strategy is based on the observation that a term and its translation appear in similar lexical contexts (Daille and Morin, 2008). This strategy has shown its effectiveness in terminology extraction (Fung, 2000; Chiao and Zweigenbaum, 2002; Daille and Morin, 2005; Robitaille et al., 2006; Morin et al., 2007; Daille and Morin, 2008;"
2009.mtsummit-posters.26,I05-1062,0,0.291006,"language processing applications such as cross-language information retrieval (Grefenstette, 1998). At first, researchers constructed bilingual dictionary from parallel corpora. For example, Wu (1994) extracted English-Chinese translation lexicon through statistical training on a large parallel corpus. But for some languages, collecting parallel corpora is not easy. Thus, utilizing comparable corpora, in which texts are not translation of each other but share similar concepts, to extract bilingual dictionary has drawn more and more attention recently (Fung, 2000; Chiao and Zweigenbaum, 2002; Daille and Morin, 2005; Robitaille et al., 2006; Morin et al., 2007; Otero, 2008; Saralegi et al., 2008). There are two popular strategies for constructing bilingual dictionary from comparable corpora: context-based strategy and syntax-based strategy. Context-based strategy is based on the observation that a term and its translation appear in similar lexical contexts (Daille and Morin, 2008). This strategy has shown its effectiveness in terminology extraction (Fung, 2000; Chiao and Zweigenbaum, 2002; Daille and Morin, 2005; Robitaille et al., 2006; Morin et al., 2007; Daille and Morin, 2008; Saralegi et al., 2008)."
2009.mtsummit-posters.26,I08-1013,0,0.0814035,"hus, utilizing comparable corpora, in which texts are not translation of each other but share similar concepts, to extract bilingual dictionary has drawn more and more attention recently (Fung, 2000; Chiao and Zweigenbaum, 2002; Daille and Morin, 2005; Robitaille et al., 2006; Morin et al., 2007; Otero, 2008; Saralegi et al., 2008). There are two popular strategies for constructing bilingual dictionary from comparable corpora: context-based strategy and syntax-based strategy. Context-based strategy is based on the observation that a term and its translation appear in similar lexical contexts (Daille and Morin, 2008). This strategy has shown its effectiveness in terminology extraction (Fung, 2000; Chiao and Zweigenbaum, 2002; Daille and Morin, 2005; Robitaille et al., 2006; Morin et al., 2007; Daille and Morin, 2008; Saralegi et al., 2008). But there exists one problem that some words coming from the same domain may appear in similar contexts even if they are not translation of each other (Yu and Tsujii, 2009). Besides of using window-based contexts, there were also some works utilizing syntax for bilingual dictionary extraction (Tanaka, 2002; Otero, 2007; Otero, 2008; Yu and Tsujii, 2009). In these works"
2009.mtsummit-posters.26,W95-0114,0,0.544009,"pora with 124,316 article pairs, in which there exist 1,132,492 English sentences and 665,789 Chinese sentences. It is clear that the large scale of Wikipedia ensures the quantity of collected comparable corpora. Besides, because the inter-language links are created by article authors, the quality of collected corpora can also be guaranteed. Figure 1. Part of the articles describing ‘computer’ in both English and Chinese Wikipedia. 3 Extracting Bilingual Dictionary with Context Heterogeneity and Dependency Heterogeneity 3.1 Comparison between Context Heterogeneity and Dependency Heterogeneity Fung (1995) proposed using context heterogeneity similarity for bilingual dictionary extraction from comparable corpora in a specific domain. It is based on the assumption that the context heterogeneity of a given domain-specific word is more similar to that of its translation in another language than that of an unrelated word in the other language (Fung, 1995). The author demonstrated that this feature is more salient than the feature that concerned the occurrence frequencies of words (Fung, 1995). Through the investigation that some words from the same domain may appear in similar context even if they"
2009.mtsummit-posters.26,1998.amta-tutorials.5,0,0.0409948,"ich combines context heterogeneity similarity and dependency heterogeneity similarity, to extract bilingual dictionary from the collected comparable corpora. Experimental results show that because of combining the advantages of context heterogeneity similarity and dependency heterogeneity similarity appropriately, the proposed approach outperforms both the two individual approaches. 1 Introduction Bilingual dictionary is a crucial part not only for machine translation (Och and Ney, 2003), but also for other natural language processing applications such as cross-language information retrieval (Grefenstette, 1998). At first, researchers constructed bilingual dictionary from parallel corpora. For example, Wu (1994) extracted English-Chinese translation lexicon through statistical training on a large parallel corpus. But for some languages, collecting parallel corpora is not easy. Thus, utilizing comparable corpora, in which texts are not translation of each other but share similar concepts, to extract bilingual dictionary has drawn more and more attention recently (Fung, 2000; Chiao and Zweigenbaum, 2002; Daille and Morin, 2005; Robitaille et al., 2006; Morin et al., 2007; Otero, 2008; Saralegi et al.,"
2009.mtsummit-posters.26,N07-1025,0,0.0273336,"for bilingual dictionary extraction in detail; Experimental results and discussion are listed in Section 4; Section 5 compared the proposed work with related works; finally, Section 6 draws a brief conclusion and gives the direction of future work. 2 Mining Comparable Wikipedia Corpora from As a rich and free resource, Wikipedia contains very large amount of articles written in different languages and various types of link information showing the relations between articles. It has been used as external resource in many natural language processing tasks successfully (Buscaldi and Rosso, 2006; Mihalcea, 2007; Nakayama et al., 2007). Among the link information in Wikipedia, the inter-language link, which is created by article authors, connects large amount of articles that describe the same term but are written in different languages. For example, Erdmann et al. (2008) showed that in the English and Japanese Wikipedia database dump data 1 from November/December 2006 with 3,068,118 English articles and 455,524 Japanese articles, there are 103,374 inter-language links from English to Japanese and 108,086 interlanguage links from Japanese to English. It has been demonstrated that these inter-language"
2009.mtsummit-posters.26,P07-1084,0,0.153784,"anguage information retrieval (Grefenstette, 1998). At first, researchers constructed bilingual dictionary from parallel corpora. For example, Wu (1994) extracted English-Chinese translation lexicon through statistical training on a large parallel corpus. But for some languages, collecting parallel corpora is not easy. Thus, utilizing comparable corpora, in which texts are not translation of each other but share similar concepts, to extract bilingual dictionary has drawn more and more attention recently (Fung, 2000; Chiao and Zweigenbaum, 2002; Daille and Morin, 2005; Robitaille et al., 2006; Morin et al., 2007; Otero, 2008; Saralegi et al., 2008). There are two popular strategies for constructing bilingual dictionary from comparable corpora: context-based strategy and syntax-based strategy. Context-based strategy is based on the observation that a term and its translation appear in similar lexical contexts (Daille and Morin, 2008). This strategy has shown its effectiveness in terminology extraction (Fung, 2000; Chiao and Zweigenbaum, 2002; Daille and Morin, 2005; Robitaille et al., 2006; Morin et al., 2007; Daille and Morin, 2008; Saralegi et al., 2008). But there exists one problem that some words"
2009.mtsummit-posters.26,P07-2055,0,0.0255547,"es: (1) a stemmer2 is used to do stemming for the English corpus. To avoid excessive stemming, we use stems only for translation candidates because we only consider about dictionary extraction for nouns currently. (2) stop words are removed. For English, we use the stop word list from (Fung, 1995). For Chinese, we remove ‘ (of)’ as stop word. (3) we remove the sentences with more than k (set as 30 empirically) words from both English corpus and Chinese corpus, in order to reduce the effect of parsing error on dictionary extraction. After preprocessing, we use a Chinese morphological analyzer (Nakagawa and Uchimoto, 2007) and an English pos-tagger (Tsuruoka et al., 2005) to analyze the raw corpora. Then, a syntactic analyzer 的 2 http://search.cpan.org/~snowhare/Lingua-Stem-0.83/ Experimental Setting • exp1: this experiment uses 500 ChineseEnglish single-noun pairs that are randomly selected from the aligned titles of the collected pages. We divide them into 10 folders. 5 folders are for testing and the other 5 folders are for development. • exp2: because the data from Wikipedia page titles used in exp1 could be more likely to have a translation in the corresponding article than the non-title words, only using"
2009.mtsummit-posters.26,J03-1002,0,0.00311793,"ed by article author, the quality of collected corpora can also be guaranteed. After that, this paper presents an approach, which combines context heterogeneity similarity and dependency heterogeneity similarity, to extract bilingual dictionary from the collected comparable corpora. Experimental results show that because of combining the advantages of context heterogeneity similarity and dependency heterogeneity similarity appropriately, the proposed approach outperforms both the two individual approaches. 1 Introduction Bilingual dictionary is a crucial part not only for machine translation (Och and Ney, 2003), but also for other natural language processing applications such as cross-language information retrieval (Grefenstette, 1998). At first, researchers constructed bilingual dictionary from parallel corpora. For example, Wu (1994) extracted English-Chinese translation lexicon through statistical training on a large parallel corpus. But for some languages, collecting parallel corpora is not easy. Thus, utilizing comparable corpora, in which texts are not translation of each other but share similar concepts, to extract bilingual dictionary has drawn more and more attention recently (Fung, 2000; C"
2009.mtsummit-posters.26,2007.mtsummit-papers.26,0,0.80741,"on appear in similar lexical contexts (Daille and Morin, 2008). This strategy has shown its effectiveness in terminology extraction (Fung, 2000; Chiao and Zweigenbaum, 2002; Daille and Morin, 2005; Robitaille et al., 2006; Morin et al., 2007; Daille and Morin, 2008; Saralegi et al., 2008). But there exists one problem that some words coming from the same domain may appear in similar contexts even if they are not translation of each other (Yu and Tsujii, 2009). Besides of using window-based contexts, there were also some works utilizing syntax for bilingual dictionary extraction (Tanaka, 2002; Otero, 2007; Otero, 2008; Yu and Tsujii, 2009). In these works, syntactic contexts of words were acquired through hand-made templates or automatic analyzers. This strategy enlarges the lexical information used for word similarity calculation from a restricted window to the entire sentence. In addition, the usage of syntactic contexts brings richer information to dictionary extraction than using window-based contexts. While, this strategy requires larger corpora for correct dictionary extraction compared with the context-based strategy. Besides, no matter of using which strategy, a large comparable corpus"
2009.mtsummit-posters.26,C02-1065,0,0.19292,"its translation appear in similar lexical contexts (Daille and Morin, 2008). This strategy has shown its effectiveness in terminology extraction (Fung, 2000; Chiao and Zweigenbaum, 2002; Daille and Morin, 2005; Robitaille et al., 2006; Morin et al., 2007; Daille and Morin, 2008; Saralegi et al., 2008). But there exists one problem that some words coming from the same domain may appear in similar contexts even if they are not translation of each other (Yu and Tsujii, 2009). Besides of using window-based contexts, there were also some works utilizing syntax for bilingual dictionary extraction (Tanaka, 2002; Otero, 2007; Otero, 2008; Yu and Tsujii, 2009). In these works, syntactic contexts of words were acquired through hand-made templates or automatic analyzers. This strategy enlarges the lexical information used for word similarity calculation from a restricted window to the entire sentence. In addition, the usage of syntactic contexts brings richer information to dictionary extraction than using window-based contexts. While, this strategy requires larger corpora for correct dictionary extraction compared with the context-based strategy. Besides, no matter of using which strategy, a large comp"
2009.mtsummit-posters.26,1994.amta-1.26,0,0.0643988,"ionary from the collected comparable corpora. Experimental results show that because of combining the advantages of context heterogeneity similarity and dependency heterogeneity similarity appropriately, the proposed approach outperforms both the two individual approaches. 1 Introduction Bilingual dictionary is a crucial part not only for machine translation (Och and Ney, 2003), but also for other natural language processing applications such as cross-language information retrieval (Grefenstette, 1998). At first, researchers constructed bilingual dictionary from parallel corpora. For example, Wu (1994) extracted English-Chinese translation lexicon through statistical training on a large parallel corpus. But for some languages, collecting parallel corpora is not easy. Thus, utilizing comparable corpora, in which texts are not translation of each other but share similar concepts, to extract bilingual dictionary has drawn more and more attention recently (Fung, 2000; Chiao and Zweigenbaum, 2002; Daille and Morin, 2005; Robitaille et al., 2006; Morin et al., 2007; Otero, 2008; Saralegi et al., 2008). There are two popular strategies for constructing bilingual dictionary from comparable corpora:"
2009.mtsummit-posters.26,N09-2031,1,0.572313,"y from comparable corpora: context-based strategy and syntax-based strategy. Context-based strategy is based on the observation that a term and its translation appear in similar lexical contexts (Daille and Morin, 2008). This strategy has shown its effectiveness in terminology extraction (Fung, 2000; Chiao and Zweigenbaum, 2002; Daille and Morin, 2005; Robitaille et al., 2006; Morin et al., 2007; Daille and Morin, 2008; Saralegi et al., 2008). But there exists one problem that some words coming from the same domain may appear in similar contexts even if they are not translation of each other (Yu and Tsujii, 2009). Besides of using window-based contexts, there were also some works utilizing syntax for bilingual dictionary extraction (Tanaka, 2002; Otero, 2007; Otero, 2008; Yu and Tsujii, 2009). In these works, syntactic contexts of words were acquired through hand-made templates or automatic analyzers. This strategy enlarges the lexical information used for word similarity calculation from a restricted window to the entire sentence. In addition, the usage of syntactic contexts brings richer information to dictionary extraction than using window-based contexts. While, this strategy requires larger corpo"
2020.coling-main.609,N19-1423,0,0.04791,"ractice. For these reasons, we propose CharacterBERT, a new variant of BERT that drops the wordpiece system altogether and uses a Character-CNN module instead to represent entire words by consulting their characters. We show that this new model improves the performance of BERT on a variety of medical domain tasks while at the same time producing robust, word-level, and open-vocabulary representations. 1 Introduction Pre-trained language representations from Transformers (Vaswani et al., 2017) have become arguably the most popular choice for building NLP systems1 . Among all such models, BERT (Devlin et al., 2019) has probably been the most successful, spawning a large number of new improved variants (Liu et al., 2019; Lan et al., 2019; Sun et al., 2019; Zhang et al., 2019; Clark et al., 2020). As a result, many of the recent language representation models inherited BERT’s subword tokenization system which relies on a predefined set of wordpieces (Wu et al., 2016), supposedly striking a good balance between the flexibility of characters and the efficiency of full words. While current research mostly focuses on improving language representations for the default “generaldomain”, there seems to be a growi"
2020.coling-main.609,P19-1266,0,0.031135,"Missing"
2020.coling-main.609,P19-2041,1,0.894488,"Missing"
2020.coling-main.609,P16-1100,0,0.0205295,"eptually simpler word-level models. This new variant does not rely on wordpieces but instead consults the characters of This work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details: http:// creativecommons.org/licenses/by/4.0/. 1 See the leaderboard of the GLUE benchmark. 2 See the baselines from the BLUE benchmark. 6903 Proceedings of the 28th International Conference on Computational Linguistics, pages 6903–6915 Barcelona, Spain (Online), December 8-13, 2020 each token to build representations similarly to previous word-level open-vocabulary systems (Luong and Manning, 2016; Kim et al., 2016; Jozefowicz et al., 2016). In practice, we replace BERT’s wordpiece embedding layer with ELMo’s (Peters et al., 2018) Character-CNN module while keeping the rest of the architecture untouched. As a result, CharacterBERT is able to produce word-level contextualized representations and does not require a wordpiece vocabulary. Furthermore, this new model seems better suited than vanilla BERT for training specialized models, as evidenced by an evaluation on multiple tasks from the medical domain. Finally, as expected from a character-based system, CharacterBERT is also seemingly"
2020.coling-main.609,W19-5006,0,0.0200639,"between the flexibility of characters and the efficiency of full words. While current research mostly focuses on improving language representations for the default “generaldomain”, there seems to be a growing interest in building suitable word embeddings for more specialized domains (El Boukkouri et al., 2019; Si et al., 2019; Elwany et al., 2019). However, with the growing complexity of recent representation models, the default trend seems to favor re-training general-domain models on specialized corpora rather than building models from scratch with a specialized vocabulary (e.g., BlueBERT (Peng et al., 2019) and BioBERT (Lee et al., 2020)). While these methods undeniably produce good models 2 , a few questions remain: How suitable are the predefined general-domain vocabularies when used in the context of specialized domains (e.g., the medical domain)? Is it better to train specialized models with specialized subword units? Do we induce any biases by training specialized models with general-domain wordpieces? In this paper, we propose CharacterBERT, a possible solution for avoiding any biases that may come from the use of a predefined wordpiece vocabulary, and an effort to revert back to conceptua"
2020.coling-main.609,N18-1202,0,0.048799,"ed under a Creative Commons Attribution 4.0 International Licence. Licence details: http:// creativecommons.org/licenses/by/4.0/. 1 See the leaderboard of the GLUE benchmark. 2 See the baselines from the BLUE benchmark. 6903 Proceedings of the 28th International Conference on Computational Linguistics, pages 6903–6915 Barcelona, Spain (Online), December 8-13, 2020 each token to build representations similarly to previous word-level open-vocabulary systems (Luong and Manning, 2016; Kim et al., 2016; Jozefowicz et al., 2016). In practice, we replace BERT’s wordpiece embedding layer with ELMo’s (Peters et al., 2018) Character-CNN module while keeping the rest of the architecture untouched. As a result, CharacterBERT is able to produce word-level contextualized representations and does not require a wordpiece vocabulary. Furthermore, this new model seems better suited than vanilla BERT for training specialized models, as evidenced by an evaluation on multiple tasks from the medical domain. Finally, as expected from a character-based system, CharacterBERT is also seemingly more robust to noise and misspellings. To the best of our knowledge, this is the first work that replaces BERT’s wordpiece system with"
2020.coling-main.609,P19-1561,0,0.0612103,"Missing"
2020.coling-main.609,D18-1187,0,0.0534015,"Missing"
2020.coling-main.609,P19-1139,0,0.0221757,"present entire words by consulting their characters. We show that this new model improves the performance of BERT on a variety of medical domain tasks while at the same time producing robust, word-level, and open-vocabulary representations. 1 Introduction Pre-trained language representations from Transformers (Vaswani et al., 2017) have become arguably the most popular choice for building NLP systems1 . Among all such models, BERT (Devlin et al., 2019) has probably been the most successful, spawning a large number of new improved variants (Liu et al., 2019; Lan et al., 2019; Sun et al., 2019; Zhang et al., 2019; Clark et al., 2020). As a result, many of the recent language representation models inherited BERT’s subword tokenization system which relies on a predefined set of wordpieces (Wu et al., 2016), supposedly striking a good balance between the flexibility of characters and the efficiency of full words. While current research mostly focuses on improving language representations for the default “generaldomain”, there seems to be a growing interest in building suitable word embeddings for more specialized domains (El Boukkouri et al., 2019; Si et al., 2019; Elwany et al., 2019). However, with the"
2020.emnlp-main.125,D17-1126,0,0.0655004,"Missing"
2020.emnlp-main.125,D16-1002,0,0.054823,"Missing"
2020.emnlp-main.125,W16-2340,0,0.0202257,"Missing"
2020.emnlp-main.125,P09-2073,0,0.0457953,"ordered tree mapping. Second, we propose a phrase representation model that allows non-compositional global alignments. 2 2.1 Related Work Tree Mapping and Phrase Alignment 3 Ordered tree mapping has been employed to estimate the similarity of a pair of sentences for its ability to align syntactic trees (Punyakanok et al., 2004; Alabbas and Ramsay, 2013; Yao et al., 2013; McCaffery and Nederhof, 2016). However, it is too restrictive in that the order of the aligned phrases in the sentences must be the same. Previous studies extended the algorithm to adapt the edit costs (Bernard et al., 2008; Mehdad, 2009; Alabbas and Ramsay, 2013) and edit operations (Heilman and Smith, 2010; Wang and Manning, 2010) to specific tasks. In contrast, the unordered tree mapping that we employ in this study is sufficiently flexible to assure identification of optimal compositional phrase alignments. Parallel parsing also involves phrase alignment in its parsing process. As the tree isomorphism assumption is too restrictive, previous studies have employed various relaxation techniques that prefer but do not force synchronisation. Burkett et al. (2010) used weakly synchronised grammar, and Das and Smith (2009) used"
2020.emnlp-main.125,D16-1244,0,0.107955,"Missing"
2020.emnlp-main.125,N18-1202,0,0.0457151,"epresentation, we compared it with the representation of the [CLS] symbol (denoted as BERT+[CLS]). BERT defines its input to begin with the special symbol [CLS], whose representation has been commonly used as a representation of sentence pair (Devlin et al., 2019). The assumption here is that BERT may learn to embed information of similarity distribution into [CLS] representation. As a pre-trained model for generating phrase representations, we compared the fine-tuning approach with the feature-based approach, i.e. FastText (Bojanowski et al., 2017) and embeddings from language models (ELMo) (Peters et al., 2018). For all pre-trained models, we used mean-pooling to generate a basic phrase representation, which consistently outperformed max-pooling in our preliminary experiments. 5.4 Model Settings We used the following public pre-trained models: ‘crawl-300d-2M-subword’6 as FastText, ‘Original (5.5B)’7 as ELMo, and ‘BERT-Base, Uncased’8 as BERT. We implemented our method and its variations using PyTorch9 with libraries Transformers,10 6 https://dl.fbaipublicfiles.com/fasttext/ vectors-english/crawl-300d-2M-subword.zip 7 https://allennlp.org/elmo 8 https://huggingface.co/bert-base-uncased 9 https://pyto"
2020.emnlp-main.125,W06-3104,0,0.0443029,"and edit operations (Heilman and Smith, 2010; Wang and Manning, 2010) to specific tasks. In contrast, the unordered tree mapping that we employ in this study is sufficiently flexible to assure identification of optimal compositional phrase alignments. Parallel parsing also involves phrase alignment in its parsing process. As the tree isomorphism assumption is too restrictive, previous studies have employed various relaxation techniques that prefer but do not force synchronisation. Burkett et al. (2010) used weakly synchronised grammar, and Das and Smith (2009) used quasi-synchronous grammars (Smith and Eisner, 2006). Choe and McClosky (2015) used dual decomposition to encourage agreement between two parse trees. All of these methods allow excess flexibility beyond compositionality in alignment. Rule extraction for tree transducers also involves phrase alignments (Mart´ınez-G´omez and Miyao, 2016) but disregards phrase boundaries to maximise the coverage of extracted rules. In contrast, the phrase alignment problem addressed in our study adheres to syntactic structures. 2.2 resentations. In this study, we propose dedicated phrase representations for the alignment problem. Before contextualised representat"
2020.emnlp-main.125,C10-1131,0,0.185864,"rmance competitive with that of experienced human annotators. 1 ??2?? Relying on team spirit , the research group developed antivirus vaccines ??3?? ??2?? lca(??1?? , ??2?? ) ??1?? The scientific team created vaccines against the virus through teamwork Figure 1: Phrase alignments by the proposed method (phrases of the same colour are paraphrases) Introduction Phrase alignment is a fundamental problem in modelling the interactions between a pair of sentences, such as paraphrase identification, textual entailment recognition, and question answering (Das and Smith, 2009; Heilman and Smith, 2010; Wang and Manning, 2010). Phrase alignment generally adheres to compositionality, in which a phrase pair is aligned based on the alignments of their child phrases. Nonetheless, non-compositional alignments involving long-distance phrase reordering are prevalent in practice (Burkett et al., 2010; Heilman and Smith, 2010; Arase and Tsujii, 2017). Figure 1 shows an example of phrase alignment in which phrases of the same colours are alignable, i.e. they are phrasal paraphrases. The alignment of ‘antivirus vaccines’ and ‘vaccines against the virus’ is compositional, as supported by alignments of their child nodes althoug"
2020.emnlp-main.125,E14-1021,0,0.0722231,"Missing"
2020.emnlp-main.125,N13-1106,0,0.0415288,"Missing"
2020.emnlp-main.125,2020.acl-main.341,0,0.0255862,"es considered word alignment distributions for modelling semantic interactions between a pair of sentences (He and Lin, 2016; Parikh et al., 2016; Chen et al., 2017). We agree with their intuition that the pairwise similarities alone are not good enough to define the cost of alignment. In case there are other similar phrases, their pairwise similarities have to be properly adjusted. This adjustment is crucial for treating non-compositional global alignment. Phrase Representation Generation Researchers have proposed specialised phrase representations for specific tasks (Arase and Tsujii, 2019; Yin et al., 2020) on top of contextualised rep3.1 Phrase Alignment Method Preliminaries and Notation We refer to one of the paraphrasal sentences as the source, s, and the other as the target, t. Superscripts s and t represent source and target, respectively. The syntactic trees of the source and target, T s = {τis }i and T t = {τjt }j , determine the phrase structures; τis and τjt are the source and target phrases. The alignments of their phrases are H = {hi = hτis , τit i}i . We interchangeably use the subscript of a node as the index of the alignment or the index of the node in a tree whenever the meaning i"
A92-1014,J90-1003,0,0.0889935,"Missing"
A92-1014,C90-1005,0,0.185946,"Missing"
A92-1014,C90-3010,0,0.0926168,"Missing"
A92-1014,J90-2002,0,0.0621388,"Missing"
A92-1014,E85-1013,0,0.0522061,"Missing"
A92-1014,J93-1005,0,\N,Missing
A92-1014,E85-1024,0,\N,Missing
A92-1014,C92-2085,1,\N,Missing
A94-1012,W89-0209,0,0.0327278,"main is vital for the development of practical systems. In the currently working systems, such customization has been carried out manually by linguists or lexicographers with time-consuming effort. We have already proposed a mechanism which acquires sublanguage-specific linguistic knowledge from parsing failures and which can be used as a tool for linguistic knowledge customization (Kiyono and Tsujii, 1993; Kiyono and Tsujii, 1994). Our approach is characterized by a mixture of symbolic and statistical approaches to grammatical knowledge acquisition. Unlike probabilistic parsing, proposed by (Fujisaki et al., 1989; Briscoe and Carroll, 1993), 2 2.1 The System Organization Hypothesis Generation Figure 1 shows the framework of our system. When the parser fails to analyse a sentence, the Hypothesis Generator (HG) produces hypotheses of missing knowledge each of which could rectify the defects of the current grammar. As the parser is a sort of Chart Parser and maintains partial parsing results in the form of inactive and active edges, a parsing failure means that no inactive edge of category S spanning the whole sentence exists. The HG tries to introduce an inactive edge of S by making hypotheses of missin"
A94-1012,E93-1027,1,0.822887,"hnologies in natural language processing are not so mature as to make general purpose systems applicable to any domains; therefore rapid customization of linguistic knowledge to the sublanguage of an application domain is vital for the development of practical systems. In the currently working systems, such customization has been carried out manually by linguists or lexicographers with time-consuming effort. We have already proposed a mechanism which acquires sublanguage-specific linguistic knowledge from parsing failures and which can be used as a tool for linguistic knowledge customization (Kiyono and Tsujii, 1993; Kiyono and Tsujii, 1994). Our approach is characterized by a mixture of symbolic and statistical approaches to grammatical knowledge acquisition. Unlike probabilistic parsing, proposed by (Fujisaki et al., 1989; Briscoe and Carroll, 1993), 2 2.1 The System Organization Hypothesis Generation Figure 1 shows the framework of our system. When the parser fails to analyse a sentence, the Hypothesis Generator (HG) produces hypotheses of missing knowledge each of which could rectify the defects of the current grammar. As the parser is a sort of Chart Parser and maintains partial parsing results in t"
A94-1012,C94-2134,1,0.683296,"uage processing are not so mature as to make general purpose systems applicable to any domains; therefore rapid customization of linguistic knowledge to the sublanguage of an application domain is vital for the development of practical systems. In the currently working systems, such customization has been carried out manually by linguists or lexicographers with time-consuming effort. We have already proposed a mechanism which acquires sublanguage-specific linguistic knowledge from parsing failures and which can be used as a tool for linguistic knowledge customization (Kiyono and Tsujii, 1993; Kiyono and Tsujii, 1994). Our approach is characterized by a mixture of symbolic and statistical approaches to grammatical knowledge acquisition. Unlike probabilistic parsing, proposed by (Fujisaki et al., 1989; Briscoe and Carroll, 1993), 2 2.1 The System Organization Hypothesis Generation Figure 1 shows the framework of our system. When the parser fails to analyse a sentence, the Hypothesis Generator (HG) produces hypotheses of missing knowledge each of which could rectify the defects of the current grammar. As the parser is a sort of Chart Parser and maintains partial parsing results in the form of inactive and ac"
A94-1012,H91-1067,0,\N,Missing
A94-1012,J93-2003,0,\N,Missing
A94-1012,P91-1027,0,\N,Missing
A94-1012,J93-1002,0,\N,Missing
C00-1030,W98-1118,0,0.0156778,"For such cases we will need to add postprocessing rules. There are of course many NE models that are not based on HMMs that have had success in the NE task at the MUC conferences. Our main requirement in implementing a model for the domain of molecular-biology has been ease of development, accuracy and portability to other sub-domains since molecular-biology itself is a wide eld. HMMs seemed to be the most favourable option at this time. Alternatives that have also had considerable success are decision trees, e.g. (Nobata et al., 1999) and maximum-entropy. The maximum entropy model shown in (Borthwick et al., 1998) in particular seems a promising approach because of its ability to handle overlapping and large feature sets within a well founded mathematical framework. However this implementation of the method seems to incorporate a number of handcoded domain speci c lexical features and dictionary lists that reduce portability. Undoubtedly we could incorporate richer features into our model and based on the evidence of others we would like to add head nouns as one type of feature in the future. Acknowledgements We would like to express our gratitude to Yuka Tateishi and Tomoko Ohta of the Tsujii laborato"
C00-1030,P96-1041,0,0.0234265,"s taken from a sub-domain of MEDLINE and the results of our experiments on this corpus. 2 Background Recent studies into the use of supervised learning-based models for the named entity task in the micro-biology domain have shown that models based on HMMs and decision trees such as (Nobata et al., 1999) are much more generalisable and adaptable to new classes of words than systems based on traditional hand-built patterns and domain speci c heuristic rules such as (Fukuda et al., 1998), overcoming the problems associated with data sparseness with the help of sophisticated smoothing algorithms (Chen and Goodman, 1996). HMMs can be considered to be stochastic nite state machines and have enjoyed success in a number of elds including speech recognition and part-of-speech tagging (Kupiec, 1992). It has been natural therefore that these models have been adapted for use in other wordclass prediction tasks such as the named-entity task in IE. Such models are often based on ngrams. Although the assumption that a word's part-of-speech or name class can be predicted by the previous n-1 words and their classes is counter-intuitive to our understanding of linguistic structures and long distance dependencies, this sim"
C00-1030,E99-1043,1,0.641132,"ation of technical expressions in these texts. This task can be considered to be similar to the named entity task in the MUC evaluation exercises (MUC, 1995). In our current work we are using abstracts available from PubMed's MEDLINE (MEDLINE, 1999). The MEDLINE database is an online collection of abstracts for published journal articles in biology and medicine and contains more than nine million articles. With the rapid growth in the number of published papers in the eld of molecular-biology there has been growing interest in the application of information extraction, (Sekimizu et al., 1998)(Collier et al., 1999)(Thomas et al., 1999)(Craven and Kumlien, 1999), to help solve some of the problems that are associated with information overload. In the remainder of this paper we will rst of all outline the background to the task and then describe the basics of HMMs and the formal model we are using. The following sections give an outline of a new tagged corpus (Ohta et al., 1999) that our team has developed using abstracts taken from a sub-domain of MEDLINE and the results of our experiments on this corpus. 2 Background Recent studies into the use of supervised learning-based models for the named entity ta"
C00-1030,M93-1007,0,\N,Missing
C00-1030,A97-1029,0,\N,Missing
C00-1047,J90-1003,0,\N,Missing
C00-1047,C94-1084,0,\N,Missing
C00-1047,W99-0609,0,\N,Missing
C00-1060,J96-1002,0,0.00667717,"Missing"
C00-1060,J93-1002,0,0.0347235,"setsus which follow bunsetsu i as in (7). They report that this kind of contextual information improves accuracy. However, the model has to assume the independency of all the random variables, which may cause some errors. Y def P (i → j) = P (bynd |Φi , Ψk , ∆i,k ) i&lt;k&lt;j ×P (dpnd |Φi , Ψj , ∆i,j ) × Y P (btwn |Φi , Ψk , ∆i,k )(7) k&gt;j The difference between our model and these previous models are discussed in Section 3. 2.2 Statistical Approaches with a grammar There have been many proposals for statistical frameworks particularly designed for parsers with hand-crafted grammars (Schabes, 1992; Briscoe and Carroll, 1993; Abney, 1996; Inui et al., 1997). The main issue in this type of research is how to assign likelihoods to a single linguistic structure generated by a grammar. Some of them (Briscoe and Carroll, 1993; Inui et al., 1997) treat information on contexts, but the contextual information is derived only from a structure to which the parser is trying to assign a likelihood value. Then, the major difference between their method and ours is that we consider the attributes of alternative linguistic structures generated by the grammar in order to determine the likelihood for linguistic structures. 2.3 SL"
C00-1060,W98-1114,0,0.0220178,"improved to overcome these problems and compared with other works directly. 4.4 Discussion and Future Work The following are some observations about the speed of our parser. Existing statistical parsers are quite efficient compared to grammar-based systems. Particularly, our system used an HPSG-based grammar, whose speed is said to be slow. However, recent advances in HPSG parsing (Torisawa et al., 2000) enabled us to obtain a unique parse tree with our system in 0.5 sec. in average for sentences in the EDR corpus. Future work shall extend SLUNG so that semantic representations are produced. Carroll et al. (1998) discussed the precision of argument structures. We believe that the focus of our study will shift from a shallow level to such a deeper level for our final aim, realization of intelligent natural language processing systems. 5 Conclusion We presented a hybrid parsing scheme that uses a hand-crafted grammar and a statistical technique. As other hybrid parsing methods, the statistical technique is used for picking up the most preferable parse tree from the parse forest generated by the grammar. The difference from other works is that the precise contextual information needed to estimate the lik"
C00-1060,W98-1511,0,0.0623567,"sed. We evaluate the accuracy of bunsetsu-dependencies as they do, thus here we introduce them for comparison. All models introduced below are based on the likelihood value of the dependency between two bunsetsus. But they differ from each other in the attributes or outputs which are considered when a likelihood value is calculated. There are some models which calculate the likelihood values of a dependency between bunsetsu i and j as in (6), such as a decision tree model (Haruno et al., 1998), a maximum entropy model (Uchimoto et al., 1999), a model based on distance and lexical information (Fujio and Matsumoto, 1998). Attributes Φi and Ψj consist of a part-of-speech (POS), a lexical item, presence of a comma, and so on. And ∆i,j is the number of intervening bunsetsus between i and j. def (6) P (i → j) = P (T |Φi , Ψj , ∆i,j ) However, these models fail to reflect contextual information because attributes of the surrounding bunsetsus are not considered. Uchimoto et al. (2000) proposed a model using posterior context. The model utilizes not only attributes about bunsetsus i, j but also attributes about all bunsetsus (including j) which follow bunsetsu i. That is, instead of learning two output values “T(tru"
C00-1060,P98-1083,0,0.107236,"se Several statistical models for Japanese dependency analysis which do not utilize a hand-crafted grammar have been proposed. We evaluate the accuracy of bunsetsu-dependencies as they do, thus here we introduce them for comparison. All models introduced below are based on the likelihood value of the dependency between two bunsetsus. But they differ from each other in the attributes or outputs which are considered when a likelihood value is calculated. There are some models which calculate the likelihood values of a dependency between bunsetsu i and j as in (6), such as a decision tree model (Haruno et al., 1998), a maximum entropy model (Uchimoto et al., 1999), a model based on distance and lexical information (Fujio and Matsumoto, 1998). Attributes Φi and Ψj consist of a part-of-speech (POS), a lexical item, presence of a comma, and so on. And ∆i,j is the number of intervening bunsetsus between i and j. def (6) P (i → j) = P (T |Φi , Ψj , ∆i,j ) However, these models fail to reflect contextual information because attributes of the surrounding bunsetsus are not considered. Uchimoto et al. (2000) proposed a model using posterior context. The model utilizes not only attributes about bunsetsus i, j but"
C00-1060,P98-2144,1,0.828236,"s to five, but in his case, without a grammar. 3.2 The Triplet/Quadruplet Model The Triplet/Quadruplet Model calculates the likelihood of the dependency between bunsetsu i and bunsetsu cn ; P (i → cn ) with the formulas (8) and (9), where cn denotes the nth candidate among bunsetsu i’s candidates; Φi denotes some attributes of i; and Ψcn denotes attributes of cn (including attributes between i and cn ). def P (i → cn ) = P (n |Φi , Ψc1 , Ψc2 ) (n = 1, 2) (8) def P (i → cn ) = P (n |Φi , Ψc1 , Ψc2 , Ψcl ) (n = 1, 2, l)(9) 2 This heuristics is a Japanese version of a left-association rule: see (Mitsuishi et al., 1998) for detail. As (8) and (9) suggest, the model considers attributes of the modifier bunsetsu and attributes of all modification candidates simultaneously in the conditional parts of the probabilities. Moreover, what is calculated is not the probability of “whether the dependency is correct (T, see Formula(6))”, but the probability of “which of the given candidates is chosen as the modifiee (n =1, 2, or l)”. These characteristics imply the following two advantages. Advantage 1 A new distance metric. The correct modifiee can be chosen by considering relative position among grammatically licensed"
C00-1060,W97-0301,0,0.0181276,"rafted grammars can contribute to parsing accuracy on a shallow level. 1 Introduction There have been many attempts to combine handcrafted high-level grammars, such as FB-LTAG, HPSG and LFG, and statistical disambiguation techniques to obtain precise linguistic structures (Schabes, 1992; Abney, 1996; Carroll et al., 1998). One evident advantage of this approach over purely statistical parsing techniques is that grammars can provide precise semantic representations. However, considering that remarkable parsing accuracy in a shallow level has been achieved by purely statistical techniques (e.g. Ratnaparkhi (1997)), it may be thought more reasonable to use high-level grammars just for postprocessing which maps results of shallow syntactical analyses onto deep analyses. This work was conducted while the first author was a graduate student at Univ. of Tokyo. NH n M H h Figure 1: A tree M with a non-head daughter NH and a head daughter H. In this work we propose that hand-crafted highlevel grammars can be useful in shallow-level analyses and statistical models. In our framework, grammars are used to obtain precise features for probability estimation, which are difficult to obtain without a grammar, and we"
C00-1060,C92-2066,0,0.0342602,"ies for all bunsetsus which follow bunsetsu i as in (7). They report that this kind of contextual information improves accuracy. However, the model has to assume the independency of all the random variables, which may cause some errors. Y def P (i → j) = P (bynd |Φi , Ψk , ∆i,k ) i&lt;k&lt;j ×P (dpnd |Φi , Ψj , ∆i,j ) × Y P (btwn |Φi , Ψk , ∆i,k )(7) k&gt;j The difference between our model and these previous models are discussed in Section 3. 2.2 Statistical Approaches with a grammar There have been many proposals for statistical frameworks particularly designed for parsers with hand-crafted grammars (Schabes, 1992; Briscoe and Carroll, 1993; Abney, 1996; Inui et al., 1997). The main issue in this type of research is how to assign likelihoods to a single linguistic structure generated by a grammar. Some of them (Briscoe and Carroll, 1993; Inui et al., 1997) treat information on contexts, but the contextual information is derived only from a structure to which the parser is trying to assign a likelihood value. Then, the major difference between their method and ours is that we consider the attributes of alternative linguistic structures generated by the grammar in order to determine the likelihood for li"
C00-1060,C00-2110,0,0.0191864,"Missing"
C00-1060,E99-1026,0,0.412941,"ndency analysis which do not utilize a hand-crafted grammar have been proposed. We evaluate the accuracy of bunsetsu-dependencies as they do, thus here we introduce them for comparison. All models introduced below are based on the likelihood value of the dependency between two bunsetsus. But they differ from each other in the attributes or outputs which are considered when a likelihood value is calculated. There are some models which calculate the likelihood values of a dependency between bunsetsu i and j as in (6), such as a decision tree model (Haruno et al., 1998), a maximum entropy model (Uchimoto et al., 1999), a model based on distance and lexical information (Fujio and Matsumoto, 1998). Attributes Φi and Ψj consist of a part-of-speech (POS), a lexical item, presence of a comma, and so on. And ∆i,j is the number of intervening bunsetsus between i and j. def (6) P (i → j) = P (T |Φi , Ψj , ∆i,j ) However, these models fail to reflect contextual information because attributes of the surrounding bunsetsus are not considered. Uchimoto et al. (2000) proposed a model using posterior context. The model utilizes not only attributes about bunsetsus i, j but also attributes about all bunsetsus (including j)"
C00-1060,2000.iwpt-1.43,0,0.0481985,"ate the likelihood values of a dependency between bunsetsu i and j as in (6), such as a decision tree model (Haruno et al., 1998), a maximum entropy model (Uchimoto et al., 1999), a model based on distance and lexical information (Fujio and Matsumoto, 1998). Attributes Φi and Ψj consist of a part-of-speech (POS), a lexical item, presence of a comma, and so on. And ∆i,j is the number of intervening bunsetsus between i and j. def (6) P (i → j) = P (T |Φi , Ψj , ∆i,j ) However, these models fail to reflect contextual information because attributes of the surrounding bunsetsus are not considered. Uchimoto et al. (2000) proposed a model using posterior context. The model utilizes not only attributes about bunsetsus i, j but also attributes about all bunsetsus (including j) which follow bunsetsu i. That is, instead of learning two output values “T(true)” or “F(false)” for the dependency between two bunsetsus, three output values are used for learning: the bunsetsu i is “bynd (dependent on a bunsetsu beyond j)”, “dpnd (dependent on the bunsetsu j)” or “btwn (dependent on a bunsetsu between i and j)”. The probability is calculated by multiplying probabilities for all bunsetsus which follow bunsetsu i as in (7)."
C00-1060,J97-4005,0,\N,Missing
C00-1060,C98-1080,0,\N,Missing
C00-1060,C98-2139,1,\N,Missing
C00-1070,W99-0615,1,\N,Missing
C00-1070,W96-0213,0,\N,Missing
C02-1083,nenadic-etal-2002-automatic,1,0.734504,"nce existing term dictionaries cannot cover the needs of specialists, automatic term extraction tools are important for consistent term discovery. ATRACT (Mima et al., 2001a) is a terminology management workbench that integrates ATR and ATC. Its main aim is to help biologists to gather and manage terminology in the domain. The module retrieves and classifies terms on the fly and sends the results as XML tag information to TIMS. The ATR method is based on the C/NC-value method (Frantzi et al., 2000). The original method has been augmented with acronym acquisition and term variation management (Nenadic et al. 2002), in order to link different terms that denote the same concept. Term variation management is based on term normalisation as an integral part of the ATR process. All orthographic, morphological and syntactic term variations and acronym variants (if any) are conflated prior to the statistical analysis, so that term candidates comprise all variants that appear in a corpus. Besides term recognition, term clustering is an indispensable component in a knowledge management process (see figure 2). Since terminological opacity and polysemy are very common in molecular biology, term clustering is essen"
C02-1083,C96-2212,0,0.117078,"ariants (if any) are conflated prior to the statistical analysis, so that term candidates comprise all variants that appear in a corpus. Besides term recognition, term clustering is an indispensable component in a knowledge management process (see figure 2). Since terminological opacity and polysemy are very common in molecular biology, term clustering is essential for the semantic integration of terms, the construction of domain ontology and for choosing the appropriate semantic information. The ATC method is based on Ushioda’s AMI (Average Mutual Information)-hierarchical clustering method (Ushioda, 1996). Our implementation uses parallel symmetric processing for high speed clustering and is built on the C/NC-value results. As input, we use co-occurrences of automatically recognised terms and their contexts, and the output is a dendrogram of hierarchical term clusters (like a thesaurus). The calculated term cluster information is stored in LiLFeS (see below) and combined with a predefined ontology according to the term classes automatically assigned. 1.3 LiLFeS LiLFeS (Miyao et al., 2000) is a Prolog-like programming language and language processor used for defining definite clause programs wi"
C02-1100,P90-1021,0,0.144802,"nciple consists of only one element. When we lose just one element in the head feature principle, a large amount of information in the daughter’s substructure is not propagated to its mother. As Copestake (1993) mentioned, another problem in Carpenter’s default unification is that the time complexity for finding the optimal answer of default unification is exponential because we have to verify the unifiability of the power set of constraints in a default feature structure. Here, we propose ideal lenient default unifica2 Background Default unification has been investigated by many researchers (Bouma, 1990; Russell et al., 1991; Copestake, 1993; Carpenter, 1993; Lascarides and Copestake, 1999) in the context of developing lexical semantics. Here, we first explain the definition given by Carpenter (1993) because his definition is both concise and comprehensive. 2.1 Carpenter’s Default Unification Carpenter proposed two types of default unification, credulous default unification and skeptical default unification. (Credulousn Default¯ Unification) o &lt; ¯ 0 is maximal such that F tc G = F t G0 ¯ G v G 0 F t G is defined (Skeptical Default Unification) &lt; &lt; F ts G = (F tc G) F F is called a strict fea"
C02-1100,P00-1058,0,0.0182108,"n, i.e., the extracted rules are not frequently triggered because they can be applied to feature structures that are exactly equivalent to their daughter’s part. By collecting a number of such rules,3 a grammar becomes wide-coverage with some overgeneration. They can be regarded as exceptions in a grammar, which are difficult to be captured only by propagating information from daughters to a mother. This approach can be regarded as a kind of explanation-based learning (Samuelsson and Rayner, 1991). The explanation-based learning method is recently attracting researcher’s attention (Xia, 1999; Chiang, 2000) because their parsers are comparative to the state-of-the-art parsers in terms of precision and recall. In the context of unificationbased grammars, Neumann (1994) has developed a parser running with an HPSG grammar learned by explanation-based learning. It should be also noted that Kiyono and Tsujii (1993) exemplified the grammar extraction approach using offline parsing in the 3 Although the size of the grammar becomes very large, the extracted rules can be found by a hash algorithm very efficiently. This tractability helps to use this approach in practical applications. # of sentences Avg."
C02-1100,C92-2072,0,0.0428875,"cessing, thus, efficient and wide coverage parsing has been extensively pursued in natural language literature. This study aims at robust processing within the Head-driven Phrase Structure Grammar (HPSG) to extend the coverage of manually-developed HPSG grammars. The meaning of ‘robust processing’ is not limited to robust processing for ill-formed sentences found in a spoken language, but includes robust processing for sentences which are well-formed but beyond the grammar writer’s expectation. Studies of robust parsing within unification-based grammars have been explored by many researchers (Douglas and Dale, 1992; Imaichi and Matsumoto, 1995). They classified the errors found in analyzing ill-formed sentences into several categories to make them tractable, e.g., constraint violation, missing or extra elements, etc. In this paper, we focus on recovery from the constraint violation errors, which is a violation of feature values. All errors in agreement fall into this category. Since many of the grammatical components in HPSG are written as constraints represented by feature structures, many of the errors are expected to be recovered by the recovery of constraint violation errors. This paper proposes two"
C02-1100,E93-1027,1,0.777179,"grammar, which are difficult to be captured only by propagating information from daughters to a mother. This approach can be regarded as a kind of explanation-based learning (Samuelsson and Rayner, 1991). The explanation-based learning method is recently attracting researcher’s attention (Xia, 1999; Chiang, 2000) because their parsers are comparative to the state-of-the-art parsers in terms of precision and recall. In the context of unificationbased grammars, Neumann (1994) has developed a parser running with an HPSG grammar learned by explanation-based learning. It should be also noted that Kiyono and Tsujii (1993) exemplified the grammar extraction approach using offline parsing in the 3 Although the size of the grammar becomes very large, the extracted rules can be found by a hash algorithm very efficiently. This tractability helps to use this approach in practical applications. # of sentences Avg. length of sentences Training Corpus 5,903 23.59 Test Set A 1,480 23.93 Test Set B 100 6.63    Table 1: Corpus size and average length of sentences                                            Figure 4: The average"
C02-1100,J99-1002,0,0.181571,"the head feature principle, a large amount of information in the daughter’s substructure is not propagated to its mother. As Copestake (1993) mentioned, another problem in Carpenter’s default unification is that the time complexity for finding the optimal answer of default unification is exponential because we have to verify the unifiability of the power set of constraints in a default feature structure. Here, we propose ideal lenient default unifica2 Background Default unification has been investigated by many researchers (Bouma, 1990; Russell et al., 1991; Copestake, 1993; Carpenter, 1993; Lascarides and Copestake, 1999) in the context of developing lexical semantics. Here, we first explain the definition given by Carpenter (1993) because his definition is both concise and comprehensive. 2.1 Carpenter’s Default Unification Carpenter proposed two types of default unification, credulous default unification and skeptical default unification. (Credulousn Default¯ Unification) o &lt; ¯ 0 is maximal such that F tc G = F t G0 ¯ G v G 0 F t G is defined (Skeptical Default Unification) &lt; &lt; F ts G = (F tc G) F F is called a strict feature structure, whose information must not be lost, and G is called a default feature str"
C02-1100,J93-2004,0,0.0240333,"Missing"
C02-1100,P91-1028,0,0.0775035,"Missing"
C02-1100,W98-0141,1,0.8832,"Missing"
C02-2024,P99-1061,0,0.0812034,"and unifiable for all π ∈ (PathF ∪ PathG ). The Quick Check algorithm described in (Torisawa and Tsujii, 1995; Malouf et al., 2000) also uses this condition for the efficient checking of unifiability between two TFSs. Given two TFSs and statically determined paths, the Quick Check algorithm can efficiently determine whether these two TFSs are non-unifiable or there is some uncertainty about their unifiability by checking the path values. It is worth noting that this algorithm is used in many modern unification grammar-based systems, e.g., the LKB system (Copestake, 1999) and the PAGE system (Kiefer et al., 1999). Unlike the Quick Check algorithm, which checks unifiability between two TFSs, our ISTFS checks unifiability between one TFS and n TFSs. The ISTFS checks unifiability by using dynamically determined paths, not statically determined paths. In our case, using only statically determined paths might extremely degrades the system performance. Suppose that any statically determined paths are not defined in the query TFS. Because there is no path to be used for checking unifiability, it is required to unify a query with every element of the data set. It should also be noted that using all paths defi"
C02-2024,J93-2004,0,0.0279692,"Missing"
C02-2024,W98-0141,1,0.889983,"Missing"
C04-1204,J97-4005,0,0.0510543,"ment labels in a predicate-argument structure are basically defined in a left-to-right order of syntactic realizations, while if we had a cue for a movement in the Penn Treebank, arguments are put in its canonical position in a predicate-argument structure. 3.2 Disambiguation model By grammar extraction, we are able to obtain a large lexicon together with complete derivation trees of HPSG, i.e, an HPSG treebank. The HPSG treebank can then be used as training data for the machine learning of the disambiguation model. Following recent research about disambiguation models on linguistic grammars (Abney, 1997; Johnson et al., 1999; Riezler et al., 2002; Clark and Curran, 2003; Miyao et al., 2003; Malouf and van Noord, 2004), we apply a log-linear model or maximum entropy model (Berger et al., 1996) on HPSG derivations. We represent an HPSG sign as a tu, where is a lexical sign of the ple head word, is a part-of-speech, and is a symbol representing the structure of the sign (mostly corresponding to nonterminal symbols of the Penn Treebank). Given an HPSG schema and the distance between the head words of the head/nonhead daughter constituents, each (binary) branching of an HPSG derivation is represe"
C04-1204,P98-1013,0,0.0442569,"Missing"
C04-1204,J96-1002,0,0.00497721,"Missing"
C04-1204,A00-2018,0,0.420965,"with existing studies on the task of identifying PropBank annotations. 1 Introduction Recently, deep linguistic analysis has successfully been applied to real-world texts. Several parsers have been implemented in various grammar formalisms and empirical evaluation has been reported: LFG (Riezler et al., 2002; Cahill et al., 2002; Burke et al., 2004), LTAG (Chiang, 2000), CCG (Hockenmaier and Steedman, 2002b; Clark et al., 2002; Hockenmaier, 2003), and HPSG (Miyao et al., 2003; Malouf and van Noord, 2004). However, their accuracy was still below the state-of-theart PCFG parsers (Collins, 1999; Charniak, 2000) in terms of the PARSEVAL score. Since deep parsers can output deeper representation of the structure of a sentence, such as predicate argument structures, several studies reported the accuracy of predicateargument relations using a treebank developed for each formalism. However, resources used for the evaluation were not available for other formalisms, and the results cannot be compared with each other. In this paper, we employ PropBank (Kingsbury and Palmer, 2002) for the evaluation of the accuracy of HPSG parsing. In the PropBank, semantic arguments of a predicate and their semantic roles a"
C04-1204,W03-1006,0,0.277134,"of HPSG parsing. In the PropBank, semantic arguments of a predicate and their semantic roles are manually annotated. Since the PropBank has been developed independently of any grammar formalisms, the results are comparable with other published results using the same test data. Interestingly, several studies suggested that the identification of PropBank annotations would require linguistically-motivated features that can be Jun’ichi Tsujii Department of Computer Science University of Tokyo CREST, JST tsujii@is.s.u-tokyo.ac.jp obtained by deep linguistic analysis (Gildea and Hockenmaier, 2003; Chen and Rambow, 2003). They employed a CCG (Steedman, 2000) or LTAG (Schabes et al., 1988) parser to acquire syntactic/semantic structures, which would be passed to statistical classifier as features. That is, they used deep analysis as a preprocessor to obtain useful features for training a probabilistic model or statistical classifier of a semantic argument identifier. These results imply the superiority of deep linguistic analysis for this task. Although the statistical approach seems a reasonable way for developing an accurate identifier of PropBank annotations, this study aims at establishing a method of dire"
C04-1204,2000.iwpt-1.9,0,0.0774359,"linguistic analysis of real-world text was impossible. Their success owed much to a consistent effort to maintain a wide-coverage LFG grammar, as well as varS ARG0-choose NP-1 they VP VP did n’t VP have S NP ARG0-choose VP *-1 to VP choose REL-choose ARG1-choose NP this particular moment Figure 1: Annotation of the PropBank ious techniques for robust parsing. However, the manual development of widecoverage linguistic grammars is still a difficult task. Recent progress in deep linguistic analysis has mainly depended on the acquisition of lexicalized grammars from annotated corpora (Xia, 1999; Chen and Vijay-Shanker, 2000; Chiang, 2000; Hockenmaier and Steedman, 2002a; Cahill et al., 2002; Frank et al., 2003; Miyao et al., 2004). This approach not only allows for the low-cost development of wide-coverage grammars, but also provides the training data for statistical modeling as a byproduct. Thus, we now have a basis for integrating statistical language modeling with deep linguistic analysis. To date, accurate parsers have been developed for LTAG (Chiang, 2000), CCG (Hockenmaier and Steedman, 2002b; Clark et al., 2002; Hockenmaier, 2003), and LFG (Cahill et al., 2002; Burke et al., 2004). Those studies have open"
C04-1204,P00-1058,0,0.0827633,"th PropBank annotations, by assuming a unique mapping from HPSG semantic representation into PropBank annotation. Even though PropBank was not used for the training of a disambiguation model, an HPSG parser achieved the accuracy competitive with existing studies on the task of identifying PropBank annotations. 1 Introduction Recently, deep linguistic analysis has successfully been applied to real-world texts. Several parsers have been implemented in various grammar formalisms and empirical evaluation has been reported: LFG (Riezler et al., 2002; Cahill et al., 2002; Burke et al., 2004), LTAG (Chiang, 2000), CCG (Hockenmaier and Steedman, 2002b; Clark et al., 2002; Hockenmaier, 2003), and HPSG (Miyao et al., 2003; Malouf and van Noord, 2004). However, their accuracy was still below the state-of-theart PCFG parsers (Collins, 1999; Charniak, 2000) in terms of the PARSEVAL score. Since deep parsers can output deeper representation of the structure of a sentence, such as predicate argument structures, several studies reported the accuracy of predicateargument relations using a treebank developed for each formalism. However, resources used for the evaluation were not available for other formalisms, a"
C04-1204,W03-1013,0,0.0325772,"ly defined in a left-to-right order of syntactic realizations, while if we had a cue for a movement in the Penn Treebank, arguments are put in its canonical position in a predicate-argument structure. 3.2 Disambiguation model By grammar extraction, we are able to obtain a large lexicon together with complete derivation trees of HPSG, i.e, an HPSG treebank. The HPSG treebank can then be used as training data for the machine learning of the disambiguation model. Following recent research about disambiguation models on linguistic grammars (Abney, 1997; Johnson et al., 1999; Riezler et al., 2002; Clark and Curran, 2003; Miyao et al., 2003; Malouf and van Noord, 2004), we apply a log-linear model or maximum entropy model (Berger et al., 1996) on HPSG derivations. We represent an HPSG sign as a tu, where is a lexical sign of the ple head word, is a part-of-speech, and is a symbol representing the structure of the sign (mostly corresponding to nonterminal symbols of the Penn Treebank). Given an HPSG schema and the distance between the head words of the head/nonhead daughter constituents, each (binary) branching of an HPSG derivation is represented as a tuple , where"
C04-1204,P02-1042,0,0.109748,"from HPSG semantic representation into PropBank annotation. Even though PropBank was not used for the training of a disambiguation model, an HPSG parser achieved the accuracy competitive with existing studies on the task of identifying PropBank annotations. 1 Introduction Recently, deep linguistic analysis has successfully been applied to real-world texts. Several parsers have been implemented in various grammar formalisms and empirical evaluation has been reported: LFG (Riezler et al., 2002; Cahill et al., 2002; Burke et al., 2004), LTAG (Chiang, 2000), CCG (Hockenmaier and Steedman, 2002b; Clark et al., 2002; Hockenmaier, 2003), and HPSG (Miyao et al., 2003; Malouf and van Noord, 2004). However, their accuracy was still below the state-of-theart PCFG parsers (Collins, 1999; Charniak, 2000) in terms of the PARSEVAL score. Since deep parsers can output deeper representation of the structure of a sentence, such as predicate argument structures, several studies reported the accuracy of predicateargument relations using a treebank developed for each formalism. However, resources used for the evaluation were not available for other formalisms, and the results cannot be compared with each other. In this"
C04-1204,P02-1036,0,0.0638059,"Missing"
C04-1204,W03-1008,0,0.272538,"the evaluation of the accuracy of HPSG parsing. In the PropBank, semantic arguments of a predicate and their semantic roles are manually annotated. Since the PropBank has been developed independently of any grammar formalisms, the results are comparable with other published results using the same test data. Interestingly, several studies suggested that the identification of PropBank annotations would require linguistically-motivated features that can be Jun’ichi Tsujii Department of Computer Science University of Tokyo CREST, JST tsujii@is.s.u-tokyo.ac.jp obtained by deep linguistic analysis (Gildea and Hockenmaier, 2003; Chen and Rambow, 2003). They employed a CCG (Steedman, 2000) or LTAG (Schabes et al., 1988) parser to acquire syntactic/semantic structures, which would be passed to statistical classifier as features. That is, they used deep analysis as a preprocessor to obtain useful features for training a probabilistic model or statistical classifier of a semantic argument identifier. These results imply the superiority of deep linguistic analysis for this task. Although the statistical approach seems a reasonable way for developing an accurate identifier of PropBank annotations, this study aims at estab"
C04-1204,J02-3001,0,0.0540139,"argument (i.e., subject) and “this particular moment” for the 1st argument (i.e., object). Existing studies applied statistical classifiers to the identification of the PropBank or FrameNet annotations. Similar to many methods of applying machine learning to NLP tasks, they first formulated the task as identifying in a sentence each argument of a given predicate. Then, parameters of the identifier were learned from the annotated corpus. Features of a statistical model were defined as a pattern on a partial structure of the syntactic tree output by an automatic parser (Gildea and Palmer, 2002; Gildea and Jurafsky, 2002). Several studies proposed the use of deep linguistic features, such as predicate-argument relations output by a CCG parser (Gildea and Hockenmaier, 2003) and derivation trees output by an LTAG parser (Chen and Rambow, 2003). Both studies reported that the identification accuracy improved by introducing such deep linguistic features. Although deep analysis has not outperformed PCFG parsers in terms of the accuracy of surface structure, these results are implicitly supporting the necessity of deep linguistic analysis for the recognition of semantic relations. However, these results do not direc"
C04-1204,P02-1031,0,0.0161748,"ents: “they” for the 0th argument (i.e., subject) and “this particular moment” for the 1st argument (i.e., object). Existing studies applied statistical classifiers to the identification of the PropBank or FrameNet annotations. Similar to many methods of applying machine learning to NLP tasks, they first formulated the task as identifying in a sentence each argument of a given predicate. Then, parameters of the identifier were learned from the annotated corpus. Features of a statistical model were defined as a pattern on a partial structure of the syntactic tree output by an automatic parser (Gildea and Palmer, 2002; Gildea and Jurafsky, 2002). Several studies proposed the use of deep linguistic features, such as predicate-argument relations output by a CCG parser (Gildea and Hockenmaier, 2003) and derivation trees output by an LTAG parser (Chen and Rambow, 2003). Both studies reported that the identification accuracy improved by introducing such deep linguistic features. Although deep analysis has not outperformed PCFG parsers in terms of the accuracy of surface structure, these results are implicitly supporting the necessity of deep linguistic analysis for the recognition of semantic relations. However"
C04-1204,hockenmaier-steedman-2002-acquiring,0,0.127828,"ons, by assuming a unique mapping from HPSG semantic representation into PropBank annotation. Even though PropBank was not used for the training of a disambiguation model, an HPSG parser achieved the accuracy competitive with existing studies on the task of identifying PropBank annotations. 1 Introduction Recently, deep linguistic analysis has successfully been applied to real-world texts. Several parsers have been implemented in various grammar formalisms and empirical evaluation has been reported: LFG (Riezler et al., 2002; Cahill et al., 2002; Burke et al., 2004), LTAG (Chiang, 2000), CCG (Hockenmaier and Steedman, 2002b; Clark et al., 2002; Hockenmaier, 2003), and HPSG (Miyao et al., 2003; Malouf and van Noord, 2004). However, their accuracy was still below the state-of-theart PCFG parsers (Collins, 1999; Charniak, 2000) in terms of the PARSEVAL score. Since deep parsers can output deeper representation of the structure of a sentence, such as predicate argument structures, several studies reported the accuracy of predicateargument relations using a treebank developed for each formalism. However, resources used for the evaluation were not available for other formalisms, and the results cannot be compared wit"
C04-1204,P02-1043,0,0.194943,"ons, by assuming a unique mapping from HPSG semantic representation into PropBank annotation. Even though PropBank was not used for the training of a disambiguation model, an HPSG parser achieved the accuracy competitive with existing studies on the task of identifying PropBank annotations. 1 Introduction Recently, deep linguistic analysis has successfully been applied to real-world texts. Several parsers have been implemented in various grammar formalisms and empirical evaluation has been reported: LFG (Riezler et al., 2002; Cahill et al., 2002; Burke et al., 2004), LTAG (Chiang, 2000), CCG (Hockenmaier and Steedman, 2002b; Clark et al., 2002; Hockenmaier, 2003), and HPSG (Miyao et al., 2003; Malouf and van Noord, 2004). However, their accuracy was still below the state-of-theart PCFG parsers (Collins, 1999; Charniak, 2000) in terms of the PARSEVAL score. Since deep parsers can output deeper representation of the structure of a sentence, such as predicate argument structures, several studies reported the accuracy of predicateargument relations using a treebank developed for each formalism. However, resources used for the evaluation were not available for other formalisms, and the results cannot be compared wit"
C04-1204,P03-1046,0,0.437634,"representation into PropBank annotation. Even though PropBank was not used for the training of a disambiguation model, an HPSG parser achieved the accuracy competitive with existing studies on the task of identifying PropBank annotations. 1 Introduction Recently, deep linguistic analysis has successfully been applied to real-world texts. Several parsers have been implemented in various grammar formalisms and empirical evaluation has been reported: LFG (Riezler et al., 2002; Cahill et al., 2002; Burke et al., 2004), LTAG (Chiang, 2000), CCG (Hockenmaier and Steedman, 2002b; Clark et al., 2002; Hockenmaier, 2003), and HPSG (Miyao et al., 2003; Malouf and van Noord, 2004). However, their accuracy was still below the state-of-theart PCFG parsers (Collins, 1999; Charniak, 2000) in terms of the PARSEVAL score. Since deep parsers can output deeper representation of the structure of a sentence, such as predicate argument structures, several studies reported the accuracy of predicateargument relations using a treebank developed for each formalism. However, resources used for the evaluation were not available for other formalisms, and the results cannot be compared with each other. In this paper, we employ Pr"
C04-1204,P99-1069,0,0.0303224,"n a predicate-argument structure are basically defined in a left-to-right order of syntactic realizations, while if we had a cue for a movement in the Penn Treebank, arguments are put in its canonical position in a predicate-argument structure. 3.2 Disambiguation model By grammar extraction, we are able to obtain a large lexicon together with complete derivation trees of HPSG, i.e, an HPSG treebank. The HPSG treebank can then be used as training data for the machine learning of the disambiguation model. Following recent research about disambiguation models on linguistic grammars (Abney, 1997; Johnson et al., 1999; Riezler et al., 2002; Clark and Curran, 2003; Miyao et al., 2003; Malouf and van Noord, 2004), we apply a log-linear model or maximum entropy model (Berger et al., 1996) on HPSG derivations. We represent an HPSG sign as a tu, where is a lexical sign of the ple head word, is a part-of-speech, and is a symbol representing the structure of the sign (mostly corresponding to nonterminal symbols of the Penn Treebank). Given an HPSG schema and the distance between the head words of the head/nonhead daughter constituents, each (binary) branching of an HPSG derivation is represented as a tuple , wher"
C04-1204,kingsbury-palmer-2002-treebank,0,0.512225,"SG (Miyao et al., 2003; Malouf and van Noord, 2004). However, their accuracy was still below the state-of-theart PCFG parsers (Collins, 1999; Charniak, 2000) in terms of the PARSEVAL score. Since deep parsers can output deeper representation of the structure of a sentence, such as predicate argument structures, several studies reported the accuracy of predicateargument relations using a treebank developed for each formalism. However, resources used for the evaluation were not available for other formalisms, and the results cannot be compared with each other. In this paper, we employ PropBank (Kingsbury and Palmer, 2002) for the evaluation of the accuracy of HPSG parsing. In the PropBank, semantic arguments of a predicate and their semantic roles are manually annotated. Since the PropBank has been developed independently of any grammar formalisms, the results are comparable with other published results using the same test data. Interestingly, several studies suggested that the identification of PropBank annotations would require linguistically-motivated features that can be Jun’ichi Tsujii Department of Computer Science University of Tokyo CREST, JST tsujii@is.s.u-tokyo.ac.jp obtained by deep linguistic analy"
C04-1204,H94-1020,0,0.0617959,"ances in deep linguistic analysis and the development of semantically annotated corpora. Section 3 describes the details of the implementation of an HPSG parser evaluated in this study. Section 4 discusses a problem in adopting PropBank for the performance evaluation of deep linguistic parsers and proposes its solution. Section 5 reports empirical evaluation of the accuracy of the HPSG parser. 2 Deep linguistic analysis and semantically annotated corpora Riezler et al. (2002) reported the successful application of a hand-crafted LFG (Bresnan, 1982) grammar to the parsing of the Penn Treebank (Marcus et al., 1994) by exploiting various techniques for robust parsing. The study was impressive because most researchers had believed that deep linguistic analysis of real-world text was impossible. Their success owed much to a consistent effort to maintain a wide-coverage LFG grammar, as well as varS ARG0-choose NP-1 they VP VP did n’t VP have S NP ARG0-choose VP *-1 to VP choose REL-choose ARG1-choose NP this particular moment Figure 1: Annotation of the PropBank ious techniques for robust parsing. However, the manual development of widecoverage linguistic grammars is still a difficult task. Recent progress"
C04-1204,P02-1035,0,0.468797,"ent relations. We could directly compare the output of HPSG parsing with PropBank annotations, by assuming a unique mapping from HPSG semantic representation into PropBank annotation. Even though PropBank was not used for the training of a disambiguation model, an HPSG parser achieved the accuracy competitive with existing studies on the task of identifying PropBank annotations. 1 Introduction Recently, deep linguistic analysis has successfully been applied to real-world texts. Several parsers have been implemented in various grammar formalisms and empirical evaluation has been reported: LFG (Riezler et al., 2002; Cahill et al., 2002; Burke et al., 2004), LTAG (Chiang, 2000), CCG (Hockenmaier and Steedman, 2002b; Clark et al., 2002; Hockenmaier, 2003), and HPSG (Miyao et al., 2003; Malouf and van Noord, 2004). However, their accuracy was still below the state-of-theart PCFG parsers (Collins, 1999; Charniak, 2000) in terms of the PARSEVAL score. Since deep parsers can output deeper representation of the structure of a sentence, such as predicate argument structures, several studies reported the accuracy of predicateargument relations using a treebank developed for each formalism. However, resources use"
C04-1204,C88-2121,0,0.0261415,"Missing"
C04-1204,J03-4003,0,\N,Missing
C04-1204,C98-1013,0,\N,Missing
C08-1069,P05-1022,0,0.0138055,"ncludes a minimum number of back-off rules. 4 Experiments 4.1 Experiment Setting We compared the performance of an HPSG parser with several CFG parsers. The HPSG parser is the Enju parser (Ninomiya et al., 2007), which has been developed for parsing with the Enju HPSG grammar. A disambiguation module based on a discriminative maximum-entropy model is used in the Enju parser. We compared the Enju parser with four CFG parsers: Stanford’s lexicalized parser (Klein and Manning, 2003), Collins’ parser (Collins, 1999), Charniak’s parser (Charniak, 2000), and Charniak and Johnson’s reranking parser (Charniak and Johnson, 2005). The first three parsers are based on treebank PCFGs derived from PTB. The last parser is a combination of Charniak’s parser and a reranking module based on a maximum-entropy model. The Enju parser and Collins’ parser require POS-tagged sentences as the input. A POS tagger distributed with the Enju parser was used for the POS-tagging. We used a standard split of PTB for the training/development/test data: sections 02-21 for the extraction of the synchronous grammar, section 22 for the development, and section 23 for the evaluation of the parsers. Some of the trees in PTB are missing in the HP"
C08-1069,A00-2018,0,0.0606592,"be sufficiently small so that the highest-scored derivation includes a minimum number of back-off rules. 4 Experiments 4.1 Experiment Setting We compared the performance of an HPSG parser with several CFG parsers. The HPSG parser is the Enju parser (Ninomiya et al., 2007), which has been developed for parsing with the Enju HPSG grammar. A disambiguation module based on a discriminative maximum-entropy model is used in the Enju parser. We compared the Enju parser with four CFG parsers: Stanford’s lexicalized parser (Klein and Manning, 2003), Collins’ parser (Collins, 1999), Charniak’s parser (Charniak, 2000), and Charniak and Johnson’s reranking parser (Charniak and Johnson, 2005). The first three parsers are based on treebank PCFGs derived from PTB. The last parser is a combination of Charniak’s parser and a reranking module based on a maximum-entropy model. The Enju parser and Collins’ parser require POS-tagged sentences as the input. A POS tagger distributed with the Enju parser was used for the POS-tagging. We used a standard split of PTB for the training/development/test data: sections 02-21 for the extraction of the synchronous grammar, section 22 for the development, and section 23 for the"
C08-1069,J07-2003,0,0.0118602,"∈ T2 , i = 1, . . . k}, which is a set of pairs of non-terminal nodes that dominate the same span of s, and then !   ()  (  2. split (T1 , T2 ) at each (Ni1 , Ni2 ) for i = 1, . . . , k. *+ *1 *+ *12 ++3 *1 ++ 4564     ()  ( -/./0  78 . 7 -/.#0 ++3 *1 *9+2 ++ *12 78 . 7 *9+!2 4/564 Figure 3: An example of synchronous TSG: synchronous productions (top) and a synchronous derivation (bottom). Stochastic synchronous grammars have been used in several machine-translation systems to serve as a model of tree-to-tree translation (e.g., (Eisner, 2003; Chiang, 2007)). Our objective of automatic conversion between syntactic analyses is similar to the tree-to-tree machine translation. An important difference is that, for our purpose, the generated tree pair should have the same yields since they are two analyses of the same sentence. We also want the synchronous grammar to be able to generate a pair of trees wherein some constituents in one tree cross with the constituents in the other tree; for example, such a treetransformation is necessary to change the articleattachment levels. We show below that, by means of a simple algorithm, we can obtain a synchro"
C08-1069,C04-1041,0,0.0663942,"Missing"
C08-1069,P07-1032,0,0.0747532,"Missing"
C08-1069,P03-2041,0,0.142299,"vel of the articles: HPSG analysis (left) and PTB-CFG analysis (right).                     Figure 2: An example of synchronous CFG 2.3 Stochastic Synchronous Tree-Substitution Grammar for Tree Conversion For the purpose of the inverted transformation of simplified HPSG trees to PTB-CFG trees, we use a statistical approach based on the stochastic synchronous grammars. Stochastic synchronous grammars are a family of probabilistic models that generate a pair of trees by recursively applying synchronous productions, starting with a pair of initial symbols. See e.g., Eisner (2003) for a more formal definition. Figure 2 shows an example of synchronous CFG, which generates the pairs of strings of the form (abm c, cbm a). Each nonterminal symbol on the yields of the synchronous production is linked to a non-terminal symbol on the other rule’s yield. In the figure, the links are represented by subscripts. A linked pair of the nonterminal symbols is simultaneously expanded by another synchronous production. The probability of a derivation D of a tree pair hS, T i is defined as the product of the probability of the pair of initial symbols (i.e., the root nodes of S and T ),"
C08-1069,J07-3004,0,0.0617922,"Missing"
C08-1069,N03-1016,0,0.0191937,"ous productions, and the scores of the backoff rules. We set the value of  to be sufficiently small so that the highest-scored derivation includes a minimum number of back-off rules. 4 Experiments 4.1 Experiment Setting We compared the performance of an HPSG parser with several CFG parsers. The HPSG parser is the Enju parser (Ninomiya et al., 2007), which has been developed for parsing with the Enju HPSG grammar. A disambiguation module based on a discriminative maximum-entropy model is used in the Enju parser. We compared the Enju parser with four CFG parsers: Stanford’s lexicalized parser (Klein and Manning, 2003), Collins’ parser (Collins, 1999), Charniak’s parser (Charniak, 2000), and Charniak and Johnson’s reranking parser (Charniak and Johnson, 2005). The first three parsers are based on treebank PCFGs derived from PTB. The last parser is a combination of Charniak’s parser and a reranking module based on a maximum-entropy model. The Enju parser and Collins’ parser require POS-tagged sentences as the input. A POS tagger distributed with the Enju parser was used for the POS-tagging. We used a standard split of PTB for the training/development/test data: sections 02-21 for the extraction of the synchr"
C08-1069,W05-1511,1,0.893231,"Missing"
C08-1069,W07-2208,1,0.872039,"sed on synchronous grammars. The use of such a shallow representation as a common format has the advantage of reduced noise introduced by the conversion in comparison with the noise produced by the conversion to deeper representations. We compared an HPSG parser with several CFG parsers in our experiment and found that meaningful differences among the parsers’ performance can still be observed by such a shallow representation. 1 Introduction Recently, there have been advancement made in the parsing techniques for large-scale lexicalized grammars (Clark and Curran, 2004; Ninomiya et al., 2005; Ninomiya et al., 2007), and it have presumably been accelerated by the development of the semi-automatic acquisition techniques of large-scale lexicalized grammars from parsed corpora (Hockenmaier and Steedman, 2007; Miyao c 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. et al., 2005). In many of the studies on lexicalized grammar parsing, the accuracy of the parsing results is evaluated in terms of the accuracy of the semantic representations output by the parsers. Since the formalisms f"
C08-1069,J93-2004,0,\N,Missing
C08-1069,J03-4003,0,\N,Missing
C08-1083,J96-1002,0,0.130705,"t to design effective features for abbreviation recognition and to reuse the knowledge obtained from the training processes. In this paper, we formalize the task of abbreviation recognition as a sequential alignment problem, which finds the optimal alignment (origins of abbreviation letters) between two strings (abbreviation and full form). We design a large amount of features that directly express the events where letters produce or do not produce abbreviations. Preparing an aligned abbreviation corpus, we obtain the optimal combination of the features by using the maximum entropy framework (Berger et al., 1996). We report the remarkable improvements and conclude this paper. 2 2.1 Proposed method Abbreviation alignment model We express a sentence x as a sequence of letters (x1 , ..., xL ), and an abbreviation candidate y in the sentence as a sequence of letters (y1 , ..., yM ). We define a letter mapping a = (i, j) to indicate that the abbreviation letter yj is produced by the letter in the full form xi . A null mapping a = (i, 0) indicates that the letter in the sentence xi is unused to form the abbreviation. Similarly, a null mapping a = (0, j) indicates that the abbreviation letter yj does not ori"
C08-1083,P06-1009,0,0.022573,"2 displays the complete list of generation rules for unigram and bigram features4 , unigram(t) and bigram(s, t). For each generation rule in unigram(t) and bigram(s, t), we define boolean functions that test the possible values yielded by the corresponding atomic function(s). 2.3 Alignment candidates Formula 1 requires a sum over the possible alignments, which amounts to 2LM for a sentence (L letters) with an abbreviation (M letters). It is unrealistic to compute the partition factor of the formula directly; therefore, the factor has been computed by dynamic programing (McCallum et al., 2005; Blunsom and Cohn, 2006; Shimbo and Hara, 2007) or approximated by the n-best list of highly probable alignments (Och and Ney, 2002; Liu et al., 2005). Fortunately, we can prune alignments that are unlikely to present full forms, by introducing the natural assumptions for abbreviation definitions: 4 In Table 2, a set of curly brackets {} denotes a list (array) rather than a mathematical set. Operators ⊕ and ⊗ present concatenation and Cartesian product of lists. For instance, when A = {a, b} and B = {c, d}, A ⊕ B = {a, b, c, d} and A ⊗ B = {ac, ad, bc, bd}. 660 min(|y|+5, 2|y|) = 8 words, (|y |= 4; y = &quot;TTF-1&quot;) ai="
C08-1083,P05-1057,0,0.0188745,"rule in unigram(t) and bigram(s, t), we define boolean functions that test the possible values yielded by the corresponding atomic function(s). 2.3 Alignment candidates Formula 1 requires a sum over the possible alignments, which amounts to 2LM for a sentence (L letters) with an abbreviation (M letters). It is unrealistic to compute the partition factor of the formula directly; therefore, the factor has been computed by dynamic programing (McCallum et al., 2005; Blunsom and Cohn, 2006; Shimbo and Hara, 2007) or approximated by the n-best list of highly probable alignments (Och and Ney, 2002; Liu et al., 2005). Fortunately, we can prune alignments that are unlikely to present full forms, by introducing the natural assumptions for abbreviation definitions: 4 In Table 2, a set of curly brackets {} denotes a list (array) rather than a mathematical set. Operators ⊕ and ⊗ present concatenation and Cartesian product of lists. For instance, when A = {a, b} and B = {c, d}, A ⊕ B = {a, b, c, d} and A ⊗ B = {ac, ad, bc, bd}. 660 min(|y|+5, 2|y|) = 8 words, (|y |= 4; y = &quot;TTF-1&quot;) ai= 4 9 13 x: investigate ~ ~ 0 0 #0 0 0 #1 0 0 #2 0 0 #3 0 #4 Shffle 0 0 #5 Shffle 0 0 #6 Shffle 0 0 #7 Shffle 0 0 #8 Shffle 0 . ."
C08-1083,P02-1038,0,0.013388,"For each generation rule in unigram(t) and bigram(s, t), we define boolean functions that test the possible values yielded by the corresponding atomic function(s). 2.3 Alignment candidates Formula 1 requires a sum over the possible alignments, which amounts to 2LM for a sentence (L letters) with an abbreviation (M letters). It is unrealistic to compute the partition factor of the formula directly; therefore, the factor has been computed by dynamic programing (McCallum et al., 2005; Blunsom and Cohn, 2006; Shimbo and Hara, 2007) or approximated by the n-best list of highly probable alignments (Och and Ney, 2002; Liu et al., 2005). Fortunately, we can prune alignments that are unlikely to present full forms, by introducing the natural assumptions for abbreviation definitions: 4 In Table 2, a set of curly brackets {} denotes a list (array) rather than a mathematical set. Operators ⊕ and ⊗ present concatenation and Cartesian product of lists. For instance, when A = {a, b} and B = {c, d}, A ⊕ B = {a, b, c, d} and A ⊗ B = {ac, ad, bc, bd}. 660 min(|y|+5, 2|y|) = 8 words, (|y |= 4; y = &quot;TTF-1&quot;) ai= 4 9 13 x: investigate ~ ~ 0 0 #0 0 0 #1 0 0 #2 0 0 #3 0 #4 Shffle 0 0 #5 Shffle 0 0 #6 Shffle 0 0 #7 Shffle"
C08-1083,P02-1021,0,0.515377,"tions including named entity recognition, information retrieval, and question answering. c 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. The task of abbreviation recognition, in which abbreviations and their expanded forms appearing in actual text are extracted, addresses the term variation problem caused by the increase in the number of abbreviations (Chang and Sch¨utze, 2006). Furthermore, abbreviation recognition is also crucial for disambiguating abbreviations (Pakhomov, 2002; Gaudan et al., 2005; Yu et al., 2006), providing sense inventories (lists of abbreviation definitions), training corpora (context information of full forms), and local definitions of abbreviations. Hence, abbreviation recognition plays a key role in abbreviation management. Numerous researchers have proposed a variety of heuristics for recognizing abbreviation definitions, e.g., the use of initials, capitalizations, syllable boundaries, stop words, lengths of abbreviations, and co-occurrence statistics (Park and Byrd, 2001; Wren and Garner, 2002; Liu and Friedman, 2003; Okazaki and Ananiadou"
C08-1083,W01-0516,0,0.670892,"abbreviation recognition is also crucial for disambiguating abbreviations (Pakhomov, 2002; Gaudan et al., 2005; Yu et al., 2006), providing sense inventories (lists of abbreviation definitions), training corpora (context information of full forms), and local definitions of abbreviations. Hence, abbreviation recognition plays a key role in abbreviation management. Numerous researchers have proposed a variety of heuristics for recognizing abbreviation definitions, e.g., the use of initials, capitalizations, syllable boundaries, stop words, lengths of abbreviations, and co-occurrence statistics (Park and Byrd, 2001; Wren and Garner, 2002; Liu and Friedman, 2003; Okazaki and Ananiadou, 2006; Zhou et al., 2006; Jain et al., 2007). Schwartz and Hearst (2003) implemented a simple algorithm that finds the shortest expression containing all alphanumerical letters of an abbreviation. Adar (2004) presented four scoring rules to choose the most likely expanded form in multiple candidates. Ao and Takagi (2005) designed more detailed conditions for accepting or discarding candidates of abbreviation definitions. However, these studies have limitations in discovering an optimal combination of heuristic rules from ma"
C08-1083,W02-0312,0,0.115089,"Missing"
C08-1083,D07-1064,0,0.0289905,"list of generation rules for unigram and bigram features4 , unigram(t) and bigram(s, t). For each generation rule in unigram(t) and bigram(s, t), we define boolean functions that test the possible values yielded by the corresponding atomic function(s). 2.3 Alignment candidates Formula 1 requires a sum over the possible alignments, which amounts to 2LM for a sentence (L letters) with an abbreviation (M letters). It is unrealistic to compute the partition factor of the formula directly; therefore, the factor has been computed by dynamic programing (McCallum et al., 2005; Blunsom and Cohn, 2006; Shimbo and Hara, 2007) or approximated by the n-best list of highly probable alignments (Och and Ney, 2002; Liu et al., 2005). Fortunately, we can prune alignments that are unlikely to present full forms, by introducing the natural assumptions for abbreviation definitions: 4 In Table 2, a set of curly brackets {} denotes a list (array) rather than a mathematical set. Operators ⊕ and ⊗ present concatenation and Cartesian product of lists. For instance, when A = {a, b} and B = {c, d}, A ⊕ B = {a, b, c, d} and A ⊗ B = {ac, ad, bc, bd}. 660 min(|y|+5, 2|y|) = 8 words, (|y |= 4; y = &quot;TTF-1&quot;) ai= 4 9 13 x: investigate ~"
C08-1095,P06-4020,0,0.0112176,"CCG predicate-argument dependency structures following the CCG derivation, not directly through DAG parsing. Similarly, the HPSG parser of Miyao and Tsujii (2005) builds the HPSG predicate-argument dependency structure following unification operations during HPSG parsing. Sagae et al. (2007) use a dependency parsing combined with an HPSG parser to produce predicate-argument dependencies. However, the dependency parser is used only to produce a dependency tree backbone, which the HPSG parser then uses to produce the more general dependency graph. A similar strategy is used in the RASP parser (Briscoe et al., 2006), which builds a dependency graph through unification operations performed during a phrase structure tree parsing process. Conclusion We have presented a framework for dependency DAG parsing, using a novel algorithm for projective DAGs that extends existing shift-reduce algorithms for parsing with dependency trees, and pseudo-projective transformations applied to DAG structures. We have demonstrated that the parsing approach is effective in analysis of predicateargument structure in English using data from the HPSG Treebank (Miyao et al., 2004), and in parsing of Danish using a rich dependency"
C08-1095,W06-2920,0,0.071525,"as simple as current data-driven transition-based dependency parsing frameworks, but outputs directed acyclic graphs (DAGs). We demonstrate the benefits of DAG parsing in two experiments where its advantages over dependency tree parsing can be clearly observed: predicate-argument analysis of English and syntactic analysis of Danish with a representation that includes long-distance dependencies and anaphoric reference links. 1 Introduction Natural language parsing with data-driven dependency-based frameworks has received an increasing amount of attention in recent years (McDonald et al., 2005; Buchholz and Marsi, 2006; Nivre et al., 2006). Dependency representations directly reflect word-to-word relation † This work was conducted while the author was at the Computer Science Department of the University of Tokyo. © 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-ncsa/3.0). Some rights reserved. ships in a dependency graph, where the words in a sentence are the nodes, and labeled edges correspond to head-dependent syntactic relations. In addition to being inherently lexicalized, dependency analyses can be generated"
C08-1095,P02-1042,0,0.0314778,"AG parsing (that could also easily be applied to cyclic structures) using approximate inference in an edge-factored dependency model starting from dependency trees. In their model, the addition of extra arcs to the tree was learned with the parameters to build the initial tree itself, which shows the power and flexibility of approximate inference in graph-based dependency models. Other parsing approaches that produce dependency graphs that are not limited to tree structures include those based on linguisticallymotivated lexicalized grammar formalisms, such as HPSG, CCG and LFG. In particular, Clark et al. (2002) use a probabilistic model of dependency DAGs extracted from the CCGBank (Hockenmeier and Steedman, 2007) in a CCG parser that builds the CCG predicate-argument dependency structures following the CCG derivation, not directly through DAG parsing. Similarly, the HPSG parser of Miyao and Tsujii (2005) builds the HPSG predicate-argument dependency structure following unification operations during HPSG parsing. Sagae et al. (2007) use a dependency parsing combined with an HPSG parser to produce predicate-argument dependencies. However, the dependency parser is used only to produce a dependency tre"
C08-1095,D07-1024,0,0.0123265,"ent of the University of Tokyo. © 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-ncsa/3.0). Some rights reserved. ships in a dependency graph, where the words in a sentence are the nodes, and labeled edges correspond to head-dependent syntactic relations. In addition to being inherently lexicalized, dependency analyses can be generated efficiently and have been show to be useful in a variety of practical tasks, such as question answering (Wang et al., 2007), information extraction in biomedical text (Erkan et al., 2007; Saetre et al, 2007) and machine translation (Quirk and Corston-Oliver, 2006). However, despite rapid progress in the development of parsers for several languages (Nivre et al., 2007) and algorithms for more linguistically adequate non-projective structures (McDonald et al., 2005; Nivre and Nilsson, 2006), most of the current data-driven dependency parsing approaches are limited to producing only dependency trees, where each word has exactly one head. Although trees have desirable properties from both computational and linguistic perspectives, the structure of linguistic phenomena that goes b"
C08-1095,J07-3004,0,0.0428449,"Missing"
C08-1095,P05-1011,1,0.602649,"Missing"
C08-1095,P05-1013,0,0.0135962,"o be a DAG where all arcs can be drawn above the sentence (written sequentially in its original order) in a way such that no arcs cross and there are no covered roots (although a root is not a concept associated with DAGs, we borrow the term from trees to denote words with no heads in the sentence). However, nonprojectivity is predictably more wide-spread in DAG representations, since there are at least as many arcs as in a tree representation, and often more, including arcs that represent non-local relationships. We then discuss the application of pseudo-projective transformations (Nivre and Nilsson, 2005) and an additional arc-reversing transform to dependency DAGs. Using a shiftreduce algorithm that allows multiple heads per word and pseudo-projective transformations to754 gether forms a complete dependency DAG parsing framework. 2.1 (a) Desired output: Basic shift-reduce parsing with multiple heads X Like Nivre (2004), we consider the direction of the dependency arc to be from the head to the dependent. X Z Initial state: Stack Input tokens The basic bottom-up left-to-right dependency parsing algorithm described by Nivre (2004) keeps a list of tokens (initialized to contain the input string)"
C08-1095,N07-1050,0,0.0138519,"output in the overall parsing framework. An alternative to using pseudo-projective transformations is to develop an algorithm for DAG parsing based on the family of algorithms described by Covington (2001), in the same way the algorithms in sections 2.1 and 2.2 were developed based on the algorithms described by Nivre (2004). Although this may be straightforward, a potential drawback of such an approach is that the number of parse actions taken in a Covington-style algorithm is always quadratic on the length of the input sentence, resulting in parsers that are more costly to train and to run (Nivre, 2007). The algorithms presented here, however, behave identically to their linear runtime tree counterparts when they are trained with graphs that are limited to tree structures. Additional actions are necessary only when words with more than one head are encountered. For data sets where most words have only one head, the performance the algorithms described in sections 2.1 and 2.2 should be close to that of shiftreduce projective parsing for dependency trees. In data sets where most words have multiple heads (resulting in higher arc density), the use of a Covington-style algorithm may be advantage"
C08-1095,D07-1111,1,0.916223,"a few structures in the data contain cycles, but most of the structures in the treebank are DAGs. In the experiments presented below, the algorithm described in section 2.1 was used. We believe the use of the arc-eager algorithm described in section 2.2 would produce similar results, but this is left as future work. 3.1 Learning component The DAG parsing framework, as described so far, must decide when to apply each appropriate parser action. As with other data-driven dependency parsing approaches with shift-reduce algorithms, we use a classifier to make these decisions. Following the work of Sagae and Tsujii (2007), we use maximum entropy models for classification. During training, the DAGs are first projectivized with pseudo-projective transformations. They are then processed by the parsing algorithm, which records each action necessary to build the correct structure in the training data, along with their corresponding parser configurations (stack and input list contents). From each of these parser configurations, a set of features is extracted and used with the correct parsing action as a training example for the maximum entropy classifier. The specific features we used in both experiments are the sam"
C08-1095,E06-1011,0,0.100265,"Missing"
C08-1095,H05-1059,1,0.658499,"Missing"
C08-1095,H05-1066,0,0.645671,"pproach that is nearly as simple as current data-driven transition-based dependency parsing frameworks, but outputs directed acyclic graphs (DAGs). We demonstrate the benefits of DAG parsing in two experiments where its advantages over dependency tree parsing can be clearly observed: predicate-argument analysis of English and syntactic analysis of Danish with a representation that includes long-distance dependencies and anaphoric reference links. 1 Introduction Natural language parsing with data-driven dependency-based frameworks has received an increasing amount of attention in recent years (McDonald et al., 2005; Buchholz and Marsi, 2006; Nivre et al., 2006). Dependency representations directly reflect word-to-word relation † This work was conducted while the author was at the Computer Science Department of the University of Tokyo. © 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-ncsa/3.0). Some rights reserved. ships in a dependency graph, where the words in a sentence are the nodes, and labeled edges correspond to head-dependent syntactic relations. In addition to being inherently lexicalized, dependency"
C08-1095,D07-1003,0,0.0117498,"conducted while the author was at the Computer Science Department of the University of Tokyo. © 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-ncsa/3.0). Some rights reserved. ships in a dependency graph, where the words in a sentence are the nodes, and labeled edges correspond to head-dependent syntactic relations. In addition to being inherently lexicalized, dependency analyses can be generated efficiently and have been show to be useful in a variety of practical tasks, such as question answering (Wang et al., 2007), information extraction in biomedical text (Erkan et al., 2007; Saetre et al, 2007) and machine translation (Quirk and Corston-Oliver, 2006). However, despite rapid progress in the development of parsers for several languages (Nivre et al., 2007) and algorithms for more linguistically adequate non-projective structures (McDonald et al., 2005; Nivre and Nilsson, 2006), most of the current data-driven dependency parsing approaches are limited to producing only dependency trees, where each word has exactly one head. Although trees have desirable properties from both computational and linguistic"
C08-1095,W04-0308,0,\N,Missing
C08-1095,P07-1079,1,\N,Missing
C08-1106,W99-0606,0,0.0124845,"der generative probabilistic models of paired input sequences and label sequences, such as HMMs (Freitag & McCallum 2000; Kupiec 1992) or multilevel Markov models (Bikel et al. 1999). The generative model provides well-understood training and inference but requires stringent conditional independence assumptions. To accommodate multiple overlapping features on observations, some other approaches view the sequence labeling problem as a sequence of classification problems, including support vector machines (SVMs) (Kudo & Matsumoto 2001) and a variety of other classifiers (Punyakanok & Roth 2001; Abney et al. 1999; Ratnaparkhi 1996). Since these classifiers cannot trade off decisions at different positions against each other (Lafferty et al. 2001), the best classifier based shallow parsers are forced to resort to heuristic combinations of multiple classifiers. A significant amount of recent work has shown the power of CRFs for sequence labeling tasks. CRFs use an exponential distribution to model the entire sequence, allowing for non-local dependencies between states and observations (Lafferty et al. 2001). Lafferty et al. (2001) showed that CRFs outperform classification models as well as HMMs on synt"
C08-1106,W02-1001,0,0.218668,"Missing"
C08-1106,N01-1025,0,0.218911,"cores of various phrase types in text. The paradigmatic shallow parsing problem is noun phrase chunking, in which the non-recursive cores of noun phrases, called base NPs, are identified. As the representative problem in shallow parsing, noun phrase chunking has received much attention, with the development of standard evaluation datasets and with c 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. extensive comparisons among methods (McDonald 2005; Sha & Pereira 2003; Kudo & Matsumoto 2001). Syntactic contexts often have a complex underlying structure. Chunk labels are usually far too general to fully encapsulate the syntactic behavior of word sequences. In practice, and given the limited data, the relationship between specific words and their syntactic contexts may be best modeled at a level finer than chunk tags but coarser than lexical identities. For example, in the noun phrase (NP) chunking task, suppose that there are two lexical sequences, “He is her –” and “He gave her – ”. The observed sequences, “He is her” and “He gave her”, would both be conventionally labeled by ‘BO"
C08-1106,W02-2018,0,0.0112059,"g previous studies on shallow parsing, our experiments are performed on the CoNLL 2000 LDCRF for Shallow Parsing We implemented LDCRFs in C++, and optimized the system to cope with large scale problems, in which the feature dimension is beyond millions. We employ similar predicate sets defined in Sha & Pereira (2003). We follow them in using predicates that depend on words as well as POS tags in the neighborhood of a given position, taking into account only those 417,835 features which occur at least once in the training data. The features are listed in Table 3. As for numerical optimization (Malouf 2002; Wallach 2002), we performed gradient decent with the Limited-Memory BFGS (L-BFGS) optimization technique (Nocedal & Wright 1999). L-BFGS is a second-order Quasi-Newton method that numerically estimates the curvature from previous gradients and updates. With no requirement on specialized Hessian approximation, L-BFGS can handle large-scale problems in an efficient manner. We implemented an L-BFGS optimizer in C++ by modifying the OWLQN package (Andrew & Gao 2007) developed by Galen Andrew. In our experiments, storing 10 pairs of previous gradients for the approximation of the function’s inver"
C08-1106,P05-1010,1,0.787072,"as shown the power of CRFs for sequence labeling tasks. CRFs use an exponential distribution to model the entire sequence, allowing for non-local dependencies between states and observations (Lafferty et al. 2001). Lafferty et al. (2001) showed that CRFs outperform classification models as well as HMMs on synthetic data and on POS tagging tasks. As for the task of shallow parsing, CRFs also outperform many other state-of-the-art models (Sha & Pereira 2003; McDonald et al. 2005). When the data has distinct sub-structures, models that exploit hidden state variables are advantageous in learning (Matsuzaki et al. 2005; Petrov et al. 2007). Sutton et al. (2004) presented an extension to CRF called dynamic conditional random field (DCRF) model. As stated by the authors, training a DCRF model with unobserved nodes (hidden variables) makes their approach difficult to optimize. In the vision community, the LDCRF model was recently proposed by Morency et al. (2007), and shown to outperform CRFs, SVMs, and HMMs for visual sequence labeling. In this paper, we introduce the concept of latentdynamics for shallow parsing, showing how hidden states automatically learned by the model present similar characteristics. We"
C08-1106,H05-1124,0,0.502342,"ssifier based shallow parsers are forced to resort to heuristic combinations of multiple classifiers. A significant amount of recent work has shown the power of CRFs for sequence labeling tasks. CRFs use an exponential distribution to model the entire sequence, allowing for non-local dependencies between states and observations (Lafferty et al. 2001). Lafferty et al. (2001) showed that CRFs outperform classification models as well as HMMs on synthetic data and on POS tagging tasks. As for the task of shallow parsing, CRFs also outperform many other state-of-the-art models (Sha & Pereira 2003; McDonald et al. 2005). When the data has distinct sub-structures, models that exploit hidden state variables are advantageous in learning (Matsuzaki et al. 2005; Petrov et al. 2007). Sutton et al. (2004) presented an extension to CRF called dynamic conditional random field (DCRF) model. As stated by the authors, training a DCRF model with unobserved nodes (hidden variables) makes their approach difficult to optimize. In the vision community, the LDCRF model was recently proposed by Morency et al. (2007), and shown to outperform CRFs, SVMs, and HMMs for visual sequence labeling. In this paper, we introduce the conc"
C08-1106,W95-0107,0,0.054769,"Missing"
C08-1106,W96-0213,0,0.150328,"abilistic models of paired input sequences and label sequences, such as HMMs (Freitag & McCallum 2000; Kupiec 1992) or multilevel Markov models (Bikel et al. 1999). The generative model provides well-understood training and inference but requires stringent conditional independence assumptions. To accommodate multiple overlapping features on observations, some other approaches view the sequence labeling problem as a sequence of classification problems, including support vector machines (SVMs) (Kudo & Matsumoto 2001) and a variety of other classifiers (Punyakanok & Roth 2001; Abney et al. 1999; Ratnaparkhi 1996). Since these classifiers cannot trade off decisions at different positions against each other (Lafferty et al. 2001), the best classifier based shallow parsers are forced to resort to heuristic combinations of multiple classifiers. A significant amount of recent work has shown the power of CRFs for sequence labeling tasks. CRFs use an exponential distribution to model the entire sequence, allowing for non-local dependencies between states and observations (Lafferty et al. 2001). Lafferty et al. (2001) showed that CRFs outperform classification models as well as HMMs on synthetic data and on P"
C08-1106,W00-0726,0,0.0690631,"33 1.00 1.00 0.98 1.00 1.00 1.00 1.00 0.99 0.99 0.88 0.73 0.67 1.00 1.00 0.62 0.94 0.93 0.92 0.97 0.94 0.92 Word Features: {wi−2 , wi−1 , wi , wi+1 , wi+2 , wi−1 wi , wi wi+1 } ×{hi , hi−1 hi , hi−2 hi−1 hi } POS Features: {ti−1 , ti , ti+1 , ti−2 ti−1 , ti−1 ti , ti ti+1 , ti+1 ti+2 , ti−2 ti−1 ti , ti−1 ti ti+1 , ti ti+1 ti+2 } ×{hi , hi−1 hi , hi−2 hi−1 hi } Table 3: Feature templates used in the experiments. wi is the current word; ti is current POS tag; and hi is the current hidden state (for the case of latent models) or the current label (for the case of conventional models). data set (Sang & Buchholz 2000; Ramshow & Marcus 1995). The training set consists of 8,936 sentences, and the test set consists of 2,012 sentences. The standard evaluation metrics for this task are precision p (the fraction of output chunks matching the reference chunks), recall r (the fraction of reference chunks returned), and the Fmeasure given by F = 2pr/(p + r). 6.1 Table 2: Latent-dynamics learned automatically by the LDCRF model. This table shows the top three words and their gold-standard POS tags for each hidden states. lar roles in modeling the dynamics in shallow parsing. Further, the singular proper nouns and t"
C08-1106,N03-1028,0,0.245876,"s the non-recursive cores of various phrase types in text. The paradigmatic shallow parsing problem is noun phrase chunking, in which the non-recursive cores of noun phrases, called base NPs, are identified. As the representative problem in shallow parsing, noun phrase chunking has received much attention, with the development of standard evaluation datasets and with c 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. extensive comparisons among methods (McDonald 2005; Sha & Pereira 2003; Kudo & Matsumoto 2001). Syntactic contexts often have a complex underlying structure. Chunk labels are usually far too general to fully encapsulate the syntactic behavior of word sequences. In practice, and given the limited data, the relationship between specific words and their syntactic contexts may be best modeled at a level finer than chunk tags but coarser than lexical identities. For example, in the noun phrase (NP) chunking task, suppose that there are two lexical sequences, “He is her –” and “He gave her – ”. The observed sequences, “He is her” and “He gave her”, would both be conve"
C08-1106,A00-2007,0,\N,Missing
C08-2011,W02-1006,0,0.0377747,"y for an observation sequence x is calculated by p(y|x) = destroy man Sense number A sense number is the number of a sense of a word in W ORD N ET. Since senses of a word are ordered according to frequency, the sense number can act as a powerful feature for WSD, which offers a preference for frequent senses, and especially as a back-off feature, which enables our model to output the first sense when no other feature is available for that word. 2.3 ROOT &lt;ROOT> hX 1 exp λj fj (e, x, y) Z(x) e∈E,j i X + µk gk (v, x, y) (1) 3.3 Vertex features Most of the vertex features we use are those used by Lee and Ng (2002). All these features are combined with each of the four sense labels sn (v), and incorporated as gk in Equation (1). v∈V,k where E and V are the sets of edges and vertices, fj and gk are the feature vectors for an edge and a vertex, λj and µk are the weight vectors for them, and Z(x) is the normalization function. For a detailed description of TCRFs, see Tang et al. (2006). • Word form, lemma, and part of speech. • Word forms, lemmas, and parts of speech of the head and dependents in a dependency tree. 44 Development Brown-1 Brown-2 S ENSEVAL-3 #sentences 470 10,712 8,956 300 #words 5,178 100,"
C08-2011,W04-0838,0,0.0302444,"Inter-word sense dependencies Since the all-words task requires us to disambiguate all content words, it seems reasonable to assume that we could perform better WSD by considering the sense dependencies among words, and optimizing word senses over the whole sentence. Specifically, we base our model on the assumption that there are strong sense dependencies between a head word and its dependents in a dependency tree; therefore, we employ the dependency tree structures for modeling the sense dependencies. There have been a few WSD systems that incorporate the inter-word sense dependencies (e.g. Mihalcea and Faruque (2004)). However, to the extent of our knowledge, their effectiveness has not explicitly examined thus far for supervised WSD. Introduction Word sense disambiguation (WSD) is one of the fundamental underlying problems in computational linguistics. The task of WSD is to determine the appropriate sense for each polysemous word within a given text. Traditionally, there are two task settings for WSD: the lexical sample task, in which only one targeted word is disambiguated given its context, and the all-words task, in which all content words within a text are disambiguated. Whilst most of the WSD resear"
C08-2011,S07-1090,0,0.0290068,"Missing"
C08-2011,D07-1111,1,0.851866,"Missing"
C08-2011,W06-1670,0,0.0130526,"ORD N ET, with which each noun or verb synset is associated. Since 43 Coling 2008: Companion volume – Posters and Demonstrations, pages 43–46 Manchester, August 2008 ROOT they are originally introduced for ease of lexicographers’ work, their classification is fairly general, but not too abstract, and is hence expected to act as good coarse-grained semantic categories. The numbers of the supersenses are 26 and 15 for nouns and verbs. The effectiveness of the use of supersenses and other coarse-grained tagsets for WSD has been recently shown by several researchers (e.g. Kohomban and Lee (2005), Ciaramita and Altun (2006), and Mihalcea et al. (2007)). &lt;SBJ> &lt;ROOT> the destroy &lt;OBJ> confidence &lt;NMOD> &lt;NMOD> in &lt;PMOD> &lt;SBJ> &lt;OBJ> man confidence &lt;NMOD> : in bank bank Figure 1: An example sentence described as a dependency tree structure. 3 WSD Model using Tree-structured CRFs 3.1 Overview Let us consider the following sentence. (i) The man destroys confidence in banks. In the beginning, we parse a given sentence by using a dependency parser. The left-hand side of Figure 1 shows the dependency tree for Sentence (i) in the CoNLL-X dependency format. Next, we convert the outputted tree into a tree of content words,"
C08-2011,W04-0811,0,0.0948917,"Missing"
C08-2011,W04-0827,0,0.0766079,"Missing"
C08-2011,S07-1057,0,0.0315553,"Missing"
C08-2011,P05-1005,0,0.0163155,"cographers’ file ID in W ORD N ET, with which each noun or verb synset is associated. Since 43 Coling 2008: Companion volume – Posters and Demonstrations, pages 43–46 Manchester, August 2008 ROOT they are originally introduced for ease of lexicographers’ work, their classification is fairly general, but not too abstract, and is hence expected to act as good coarse-grained semantic categories. The numbers of the supersenses are 26 and 15 for nouns and verbs. The effectiveness of the use of supersenses and other coarse-grained tagsets for WSD has been recently shown by several researchers (e.g. Kohomban and Lee (2005), Ciaramita and Altun (2006), and Mihalcea et al. (2007)). &lt;SBJ> &lt;ROOT> the destroy &lt;OBJ> confidence &lt;NMOD> &lt;NMOD> in &lt;PMOD> &lt;SBJ> &lt;OBJ> man confidence &lt;NMOD> : in bank bank Figure 1: An example sentence described as a dependency tree structure. 3 WSD Model using Tree-structured CRFs 3.1 Overview Let us consider the following sentence. (i) The man destroys confidence in banks. In the beginning, we parse a given sentence by using a dependency parser. The left-hand side of Figure 1 shows the dependency tree for Sentence (i) in the CoNLL-X dependency format. Next, we convert the outputted tree in"
C08-2016,J96-1002,0,0.0941699,"ell et al., 1999). The junction tree algorithm is a generic algorithm for exact inference on any graphical model, and it allows for efficient inference on sparse graphs. The method converts a graph into a junction tree, which is a tree of cliques in the original graph. When we have a junction tree for each document, we can efficiently perform belief propagation in order to compute argmax in Equation (1), or the marginal probabilities of cliques and labels, necessary for the parameter estimation of machine learning classifiers, including perceptrons (Collins, 2002), and maximum entropy models (Berger et al., 1996). The computational complexity of the inference on junction trees is proportional to the exponential of the tree width, which is the maximum number of labels in a clique, minus one. An essential idea of this method is that a graphical model is constructed for each document. Even when features are defined on all pairs of labels, active features for a specific document are limited. When combined with feature selection, this method greatly increases the sparsity of the resulting graphs, which is key to efficiency. A weakness of this method comes from the assumption of feature sparseness. We are f"
C08-2016,W02-1001,0,0.397861,"ce, such as the junction tree algorithm (Cowell et al., 1999). The junction tree algorithm is a generic algorithm for exact inference on any graphical model, and it allows for efficient inference on sparse graphs. The method converts a graph into a junction tree, which is a tree of cliques in the original graph. When we have a junction tree for each document, we can efficiently perform belief propagation in order to compute argmax in Equation (1), or the marginal probabilities of cliques and labels, necessary for the parameter estimation of machine learning classifiers, including perceptrons (Collins, 2002), and maximum entropy models (Berger et al., 1996). The computational complexity of the inference on junction trees is proportional to the exponential of the tree width, which is the maximum number of labels in a clique, minus one. An essential idea of this method is that a graphical model is constructed for each document. Even when features are defined on all pairs of labels, active features for a specific document are limited. When combined with feature selection, this method greatly increases the sparsity of the resulting graphs, which is key to efficiency. A weakness of this method comes f"
C08-2016,W07-1017,0,0.0261428,"Missing"
C08-2016,W07-1013,0,0.187409,"of the decision for other labels, this method cannot be sensitive to label correlations, or the tendency of label cooccurrences. A recent research effort has been devoted to the modeling of label correlations. While a number of approaches have been proposed for dealing with label correlations (see Tsoumakas and c 2008. ° Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. 63 Coling 2008: Companion volume – Posters and Demonstrations, pages 63–66 Manchester, August 2008 Katakis (2007) for the comprehensive survey), the intuitively-appealing method is to incorporate features on two labels into the model (Ghamrawi and McCallum, 2005). The following label correlation feature indicates a cooccurrence of two labels and a word: ( fl,l0 ,w (x, y) = 3 cmc2007 reuters10 reuters90 # train 978 6,490 7,770 # test 976 2,545 3,019 # labels 45 10 90 card. 1.23 1.10 1.24 Table 1: Statistics of evaluation data sets l, l0 cmc2007 reuters10 reuters90 cx (w) if ∈ y, 0 otherwise. κ 1,000 5,000 5,000 ν 10 20 80 c 0 5 5 Table 2: Parameters for evaluation data sets A Method for Exact Inference A"
C08-2032,J90-2002,0,0.22229,"Missing"
C08-2032,N03-1017,0,0.00647893,"f by a polysemous pivot word wp 1 . Previous work addressed the polysemy problem in pivot-based methods (Tanaka and Umemura, 1994; Schafer and Yarowsky, 2002). Pivot-based methods also suffer from a mismatch problem, in which a pivot word wp from a source word wf does not exist in the bilingual lexicon Lp – Le 2 . Moreover, a bilingual lexicon for technical terms is prone to include a number of pivot terms that are not included in another lexicon. This paper proposes a method for building a bilingual lexicon through a pivot language by using phrase-based statistical machine translation (SMT) (Koehn et al., 2003). We build a translation model between Lf and Le by assuming two lexicons Lf –Lp and Lp –Le as parallel corpora, in order to increase the obtained lexicon size by handling multi-word expressions appropriately. The main advantage of this method is its ability to incorporate various translation models that associate languages Lf –Le ; for example, we can further improve the translation model by integrating a small bilingual lexicon Lf –Le . 1 A Japanese term “ख”: dote, embankment, may be associated with a Chinese term “ၿߦ,” y´ıngh´ang: banking institution, using the pivot word bank in English."
C08-2032,P07-2045,0,0.00974321,", in order to improve both the merged lexicon size and its accuracy. Recently, several researchers proposed the use of the pivot language for phrase-based SMT (Utiyama and Isahara, 2007; Wu and Wang, 2007). We employ a similar approach for obtaining phrase translations with the translation probabilities by assuming the bilingual lexicons as parallel corpora. Figure 1 illustrates the framework of our approach. Let us suppose that we have two bilingual lexicons Lf –Lp and Lp –Le . We obtain word alignments of these lexicons by applying GIZA++ (Och and Ney, 2003), and grow-diag-final heuristics (Koehn et al., 2007). Let w ¯x be a phrase that represents a sequence of words in the language ¯p , w ¯f ) and (w ¯e , w ¯p ), the Lx . For phrase pairs (w ¯f ) and p(w ¯e |w ¯p ) translation probabilities p(w ¯p |w are computed using the maximum likelihood estimation from the co-occurrence frequencies, consistent with the word alignment in the bilingual lexicons. We calculate the direct translation probabilities between source and target phrases, ¯f ) =  p(w ¯e |w w ¯e w ¯p  p(w ¯e |w ¯p )p(w ¯p |w ¯f ) w ¯p p(w ¯e |w ¯p )p(w ¯p |w ¯f ) . (1) We employ the log-linear model of phrase-based SMT (Och and Ney, 2"
C08-2032,P02-1038,0,0.0126593,"et al., 2007). Let w ¯x be a phrase that represents a sequence of words in the language ¯p , w ¯f ) and (w ¯e , w ¯p ), the Lx . For phrase pairs (w ¯f ) and p(w ¯e |w ¯p ) translation probabilities p(w ¯p |w are computed using the maximum likelihood estimation from the co-occurrence frequencies, consistent with the word alignment in the bilingual lexicons. We calculate the direct translation probabilities between source and target phrases, ¯f ) =  p(w ¯e |w w ¯e w ¯p  p(w ¯e |w ¯p )p(w ¯p |w ¯f ) w ¯p p(w ¯e |w ¯p )p(w ¯p |w ¯f ) . (1) We employ the log-linear model of phrase-based SMT (Och and Ney, 2002) for translating the source term w ¯f in the lexicon Lf –Lp into the tarˆ¯ e that maximizes get language by finding a term w the translation probability, ˆ¯ e = argmax w¯e Pr(w w ¯e |w ¯f ) = argmax w¯e M  m=1  i Merging two bilingual lexicons  =1− ¯f ) ED(w ¯e , w , max(w ¯e , w ¯f ) (3) where ED(x, y) represents a Levenshtein distance of characters between the two terms x and y 3 . We also define an additional bilingual lexicon feature, Figure 1: Framework of our approach 2 ¯e , w ¯f ) sim (w λm hm (w ¯e , w ¯f ), (2) (i) (i) log p (w ¯e(i) |w ¯f ), (4) (i) ¯f represent an i-th translate"
C08-2032,J03-1002,0,0.00224552,"introduce phrase-based SMT for merging the lexicons, in order to improve both the merged lexicon size and its accuracy. Recently, several researchers proposed the use of the pivot language for phrase-based SMT (Utiyama and Isahara, 2007; Wu and Wang, 2007). We employ a similar approach for obtaining phrase translations with the translation probabilities by assuming the bilingual lexicons as parallel corpora. Figure 1 illustrates the framework of our approach. Let us suppose that we have two bilingual lexicons Lf –Lp and Lp –Le . We obtain word alignments of these lexicons by applying GIZA++ (Och and Ney, 2003), and grow-diag-final heuristics (Koehn et al., 2007). Let w ¯x be a phrase that represents a sequence of words in the language ¯p , w ¯f ) and (w ¯e , w ¯p ), the Lx . For phrase pairs (w ¯f ) and p(w ¯e |w ¯p ) translation probabilities p(w ¯p |w are computed using the maximum likelihood estimation from the co-occurrence frequencies, consistent with the word alignment in the bilingual lexicons. We calculate the direct translation probabilities between source and target phrases, ¯f ) =  p(w ¯e |w w ¯e w ¯p  p(w ¯e |w ¯p )p(w ¯p |w ¯f ) w ¯p p(w ¯e |w ¯p )p(w ¯p |w ¯f ) . (1) We employ the"
C08-2032,W02-2026,0,0.0955309,"ilingual lexicon for every language pair; thus, comprehensible bilingual lexicons are available only for a limited number of language pairs. One of the solutions is to build a bilingual lexicon of the source language Lf and the target Le through a pivot language Lp , when large bilingual c 2008.  Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. lexicons Lf –Lp and Lp –Le are available. Numerous researchers have explored the use of pivot languages (Tanaka and Umemura, 1994; Schafer and Yarowsky, 2002; Zhang et al., 2005). This approach is advantageous because we can obtain a bilingual lexicon between Le and Lf , even if no bilingual lexicon exists between these languages. Pivot-based methods for dictionary construction may produce incorrect translations when the word we is translated from a word wf by a polysemous pivot word wp 1 . Previous work addressed the polysemy problem in pivot-based methods (Tanaka and Umemura, 1994; Schafer and Yarowsky, 2002). Pivot-based methods also suffer from a mismatch problem, in which a pivot word wp from a source word wf does not exist in the bilingual l"
C08-2032,N07-1061,0,0.0383126,"d additional bilingual lexicon. We define a character-based similarity feature, Lf-Lp lexicon Le-Lp lexicon Word alignment & grow-diag-final method Lf-Lp translation Le-Lp translation phrase table phrase table Merging phrase tables Lf-Le translation phrase table Additional features hchar Phrase-based Le: translations of SMT system Lf-Lp lexicon INPUT OUTPUT hadd ¯e , w ¯f ) lex (w = We introduce phrase-based SMT for merging the lexicons, in order to improve both the merged lexicon size and its accuracy. Recently, several researchers proposed the use of the pivot language for phrase-based SMT (Utiyama and Isahara, 2007; Wu and Wang, 2007). We employ a similar approach for obtaining phrase translations with the translation probabilities by assuming the bilingual lexicons as parallel corpora. Figure 1 illustrates the framework of our approach. Let us suppose that we have two bilingual lexicons Lf –Lp and Lp –Le . We obtain word alignments of these lexicons by applying GIZA++ (Och and Ney, 2003), and grow-diag-final heuristics (Koehn et al., 2007). Let w ¯x be a phrase that represents a sequence of words in the language ¯p , w ¯f ) and (w ¯e , w ¯p ), the Lx . For phrase pairs (w ¯f ) and p(w ¯e |w ¯p ) transl"
C08-2032,P07-1108,0,0.118778,"con. We define a character-based similarity feature, Lf-Lp lexicon Le-Lp lexicon Word alignment & grow-diag-final method Lf-Lp translation Le-Lp translation phrase table phrase table Merging phrase tables Lf-Le translation phrase table Additional features hchar Phrase-based Le: translations of SMT system Lf-Lp lexicon INPUT OUTPUT hadd ¯e , w ¯f ) lex (w = We introduce phrase-based SMT for merging the lexicons, in order to improve both the merged lexicon size and its accuracy. Recently, several researchers proposed the use of the pivot language for phrase-based SMT (Utiyama and Isahara, 2007; Wu and Wang, 2007). We employ a similar approach for obtaining phrase translations with the translation probabilities by assuming the bilingual lexicons as parallel corpora. Figure 1 illustrates the framework of our approach. Let us suppose that we have two bilingual lexicons Lf –Lp and Lp –Le . We obtain word alignments of these lexicons by applying GIZA++ (Och and Ney, 2003), and grow-diag-final heuristics (Koehn et al., 2007). Let w ¯x be a phrase that represents a sequence of words in the language ¯p , w ¯f ) and (w ¯e , w ¯p ), the Lx . For phrase pairs (w ¯f ) and p(w ¯e |w ¯p ) translation probabilities"
C08-2032,C94-1048,0,\N,Missing
C08-2032,P07-2055,0,\N,Missing
C10-1003,C02-1166,0,0.387978,"Missing"
C10-1003,J93-1003,0,0.375128,"Missing"
C10-1003,W09-1117,0,0.357106,"mber of co-occurrences of two words in the corpus. All this work is closely related to our work because they solely consider context similarity, whereas context is deﬁned using a word window. The work in (Rapp, 1999; Fung, 1998; Chiao and Zweigenbaum, 2002) will form the baselines for our experiments in Section 4.2 This baseline is also similar to the baseline in (Gaussier et al., 2004), which showed that it can be difﬁcult to beat such a feature vector approach. In principle our method is not restricted to how context is deﬁned; we could also use, for example, modiﬁers and head words, as in (Garera et al., 2009). Although, we found in a preliminary experiment that using a dependency parser to differentiate between modiﬁers and head words like in (Garera et al., 2009), instead of a bag-of-words model, in our setting, actually decreased accuracy due to the narrow dependency window. However, our method could be combined with a backtranslation step, which is expected to improve translation quality as in (Haghighi et al., 2008), which performs indirectly a back-translation by matching all nouns mutually exclusive across corpora. Notably, there also exist promising approaches which use both types of inform"
C10-1003,D08-1047,1,0.801952,"Missing"
C10-1003,P04-1067,0,0.528273,"n the 19 Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 19–27, Beijing, August 2010 tion of (Fung, 1998), uses tf.idf, but suggests to normalize the term frequency by the maximum number of co-occurrences of two words in the corpus. All this work is closely related to our work because they solely consider context similarity, whereas context is deﬁned using a word window. The work in (Rapp, 1999; Fung, 1998; Chiao and Zweigenbaum, 2002) will form the baselines for our experiments in Section 4.2 This baseline is also similar to the baseline in (Gaussier et al., 2004), which showed that it can be difﬁcult to beat such a feature vector approach. In principle our method is not restricted to how context is deﬁned; we could also use, for example, modiﬁers and head words, as in (Garera et al., 2009). Although, we found in a preliminary experiment that using a dependency parser to differentiate between modiﬁers and head words like in (Garera et al., 2009), instead of a bag-of-words model, in our setting, actually decreased accuracy due to the narrow dependency window. However, our method could be combined with a backtranslation step, which is expected to improve"
C10-1003,P99-1067,0,0.213707,"alculates the chance of an accidental overlap of pivot words using the hypergeometric distribution. We implemented a wide variety of previously suggested methods. Testing in two conditions, a small comparable corpora pair and a large but unrelated corpora pair, both written in disparate languages, we show that our approach consistently outperforms the other systems. 1 • Spelling distance or transliterations, which are useful to identify loan words (Koehn and Knight, 2002). • Context similarity, helpful since two words with identical meaning are often used in similar contexts across languages (Rapp, 1999). The ﬁrst type of information is quite speciﬁc; it can only be helpful in a few cases, and can thereby engender high-precision systems with low recall, as described for example in (Koehn and Knight, 2002). The latter is more general. It holds for most words including loan words. Usually the context of a word is deﬁned by the words which occur around it (bag-of-words model). Let us brieﬂy recall the main idea for using context similarity to ﬁnd translation pairs. First, the degree of association between the query word and all content words is measured with respect to the corpus at hand. The sa"
C10-1003,P08-1088,0,0.0968515,"e difﬁcult to beat such a feature vector approach. In principle our method is not restricted to how context is deﬁned; we could also use, for example, modiﬁers and head words, as in (Garera et al., 2009). Although, we found in a preliminary experiment that using a dependency parser to differentiate between modiﬁers and head words like in (Garera et al., 2009), instead of a bag-of-words model, in our setting, actually decreased accuracy due to the narrow dependency window. However, our method could be combined with a backtranslation step, which is expected to improve translation quality as in (Haghighi et al., 2008), which performs indirectly a back-translation by matching all nouns mutually exclusive across corpora. Notably, there also exist promising approaches which use both types of information, spelling distance, and context similarity in a joint framework, see (Haghighi et al., 2008), or (D´ejean et al., 2002) which include knowledge of a thesaurus. In our work here, we concentrate on the use of degrees of association as an effective means to extract word translations. In this application, to measure association robustly, often the Log-Likelihood Ratio (LLR) measurement is suggested (Rapp, 1999; Mo"
C10-1003,W02-0902,0,0.136668,"Bayesian method for estimating Point-wise Mutual Information provides improved accuracy. In the second step, matching is done in a novel way that calculates the chance of an accidental overlap of pivot words using the hypergeometric distribution. We implemented a wide variety of previously suggested methods. Testing in two conditions, a small comparable corpora pair and a large but unrelated corpora pair, both written in disparate languages, we show that our approach consistently outperforms the other systems. 1 • Spelling distance or transliterations, which are useful to identify loan words (Koehn and Knight, 2002). • Context similarity, helpful since two words with identical meaning are often used in similar contexts across languages (Rapp, 1999). The ﬁrst type of information is quite speciﬁc; it can only be helpful in a few cases, and can thereby engender high-precision systems with low recall, as described for example in (Koehn and Knight, 2002). The latter is more general. It holds for most words including loan words. Usually the context of a word is deﬁned by the words which occur around it (bag-of-words model). Let us brieﬂy recall the main idea for using context similarity to ﬁnd translation pair"
C10-1003,W04-3230,0,0.0322109,"Missing"
C10-1003,W04-3243,0,0.0630923,"se only one translation in the dictionary, which we select by comparing the relative frequencies. We also made a second run of the experiments where we manually selected the correct translations for the ﬁrst half of the most frequent pivots – Results did not change signiﬁcantly. 1 For example ”car” and ”tire” are expected to have a high (positive) degree of association, and ”car” and ”apple” is expected to have a high (negative) degree of association. 20 two such random variables (Dunning, 1993), and is known to be equal to Mutual Information that is linearly scaled by the size of the corpus (Moore, 2004). This means it is a measure for how much the occurrence of word A makes the occurrence of word B more likely, which we term positive association, and how much the absence of word A makes the occurrence of word B more likely, which we term negative association. However, our experiments show that only positive association is beneﬁcial for aligning words cross-lingually. In fact, LLR can still be used for extracting positive associations by ﬁltering in a pre-processing step words with possibly negative associations (Moore, 2005). Nevertheless a problem which cannot be easily remedied is that con"
C10-1003,H05-1011,0,0.0717994,"tual Information that is linearly scaled by the size of the corpus (Moore, 2004). This means it is a measure for how much the occurrence of word A makes the occurrence of word B more likely, which we term positive association, and how much the absence of word A makes the occurrence of word B more likely, which we term negative association. However, our experiments show that only positive association is beneﬁcial for aligning words cross-lingually. In fact, LLR can still be used for extracting positive associations by ﬁltering in a pre-processing step words with possibly negative associations (Moore, 2005). Nevertheless a problem which cannot be easily remedied is that conﬁdence estimates using LLR are unreliable for small sample sizes (Moore, 2004). We suggest a more principled approach that measures from the start only how much the occurrence of word A makes the occurrence of word B more likely, which is designated as Robust PMI. Another point that is common to (Rapp, 1999; Morin et al., 2007; Chiao and Zweigenbaum, 2002; Garera et al., 2009; Gaussier et al., 2004) is that word association is compared in a ﬁnegrained way, i.e. they compare the degree of association3 with every pivot word, eve"
C10-1003,P07-1084,0,0.834104,"8), which performs indirectly a back-translation by matching all nouns mutually exclusive across corpora. Notably, there also exist promising approaches which use both types of information, spelling distance, and context similarity in a joint framework, see (Haghighi et al., 2008), or (D´ejean et al., 2002) which include knowledge of a thesaurus. In our work here, we concentrate on the use of degrees of association as an effective means to extract word translations. In this application, to measure association robustly, often the Log-Likelihood Ratio (LLR) measurement is suggested (Rapp, 1999; Morin et al., 2007; Chiao and Zweigenbaum, 2002). The occurrence of a word in a document is modeled as a binary random variable. The LLR measurement measures stochastic dependency between one pivot word. We deﬁne the degree of association, as a measurement for ﬁnding words that cooccur, or which do not co-occur, more often than we would expect by pure chance.1 We argue that common ways for comparing similarity vectors across different corpora perform worse because they assume that degree of associations are very similar across languages and can be compared without much preprocessing. We therefore suggest a new"
C10-1003,C02-2020,0,\N,Missing
C10-1088,W04-3224,0,0.0147143,"PAS Figure 5: Format conversion dependencies in six parsers. Formats adopted for the evaluation are shown in solid boxes. SD: Stanford Dependency format, CCG: Combinatory Categorial Grammar output format, PTB: Penn Treebank format, and PAS: Predicate Argument Structure in Enju format. CONJ Figure 3: CoNLL-X dependency tree noun_arg1 arg1 Conll-X CCG prep_arg12 prep_arg12 arg1 arg2 NFAT/AP-1 complex formed only with P and P2 verb_arg1 arg1 adj_arg1 coord_arg12 coord_arg12 arg1 arg1 arg2 Figure 4: Predicate Argument Structure parsers are GDep (Sagae and Tsujii, 2007), the Bikel parser (Bikel) (Bikel, 2004), the Stanford parser with two probabilistic context-free grammar (PCFG) models1 (Wall Street Journal (WSJ) model (Stanford WSJ) and “augmented English” model (Stanford eng)) (Klein and Manning, 2003), the Charniak-Johnson reranking parser, using David McClosky’s self-trained biomedical parsing model (MC) (McClosky, 2009), the C&C CCG parser, adapted to biomedical text (C&C) (Rimell and Clark, 2009), and the Enju parser with the GENIA model (Miyao et al., 2009). The formats are Stanford Dependencies (SD) (Figure 2), the CoNLL-X dependency format (CoNLL) (Figure 3) and the predicateargument str"
C10-1088,W09-1402,0,0.127233,"Missing"
C10-1088,de-marneffe-etal-2006-generating,0,0.160065,"Missing"
C10-1088,W07-2416,0,0.039232,"format containing predicate argument structures along with a phrase structure tree in Enju format, which can be converted into PTB format (Miyao et al., 2009). For direct comparison and for the study of contribution of the formats in which the six parsers output their analyses to task performance, we apply a number of conversions between the outputs, shown in Figure 5. The Enju PAS output is converted into PTB using the method introduced by (Miyao et al., 2009). SD is generated from PTB by the Stanford tools (de Marneffe et al., 2006), and CoNLL generated from PTB by using Treebank Converter (Johansson and Nugues, 2007). With the exception of GDep, all CoNLL outputs are generated by the conversion and thus share dependency types. We note that all of these conversions can introduce some errors in the conversion process. 781 4 Evaluation Setting 4.1 Event Extraction Evaluation Event extraction performance is evaluated using the evaluation script provided by the BioNLP’09 shared task organizers for the development data set, and the online evaluation system of the task for the test data set2 . Results are reported under the official evaluation criterion of the task, i.e. the “Approximate Span Matching/Approximat"
C10-1088,W09-1401,1,0.485832,"rresponding to locations and sites considered in Task 2. Theme phosphorylation Theme TRAF2 2.1 Binding binding Theme Theme TRAF2 CD40 Figure 1: Event Extraction. sults shows that performance against gold standard annotations is not always correlated with event extraction performance. We further find that the dependency types and overall structures employed by the different dependency representations have specific advantages and disadvantages for the event extraction task. 2 Bio-molecular Event Extraction In this study, we adopt the event extraction task defined in the BioNLP 2009 Shared Task (Kim et al., 2009) as a model information extraction task. Figure 1 shows an example illustrating the task of event extraction from a sentence. The shared task provided common and consistent task definitions, data sets for training and evaluation, and evaluation criteria. The shared task defined five simple events (Gene expression, Transcription, Protein catabolism, Phosphorylation, and Localization) that take one core argument, a multiparticipant binding event (Binding), and three regulation events (Regulation, Positive regulation, and Negative regulation) used to capture both biological regulation and general"
C10-1088,P03-1054,0,0.00137982,"ar output format, PTB: Penn Treebank format, and PAS: Predicate Argument Structure in Enju format. CONJ Figure 3: CoNLL-X dependency tree noun_arg1 arg1 Conll-X CCG prep_arg12 prep_arg12 arg1 arg2 NFAT/AP-1 complex formed only with P and P2 verb_arg1 arg1 adj_arg1 coord_arg12 coord_arg12 arg1 arg1 arg2 Figure 4: Predicate Argument Structure parsers are GDep (Sagae and Tsujii, 2007), the Bikel parser (Bikel) (Bikel, 2004), the Stanford parser with two probabilistic context-free grammar (PCFG) models1 (Wall Street Journal (WSJ) model (Stanford WSJ) and “augmented English” model (Stanford eng)) (Klein and Manning, 2003), the Charniak-Johnson reranking parser, using David McClosky’s self-trained biomedical parsing model (MC) (McClosky, 2009), the C&C CCG parser, adapted to biomedical text (C&C) (Rimell and Clark, 2009), and the Enju parser with the GENIA model (Miyao et al., 2009). The formats are Stanford Dependencies (SD) (Figure 2), the CoNLL-X dependency format (CoNLL) (Figure 3) and the predicateargument structure (PAS) format used by Enju (Figure 4). With the exception of Stanford and Enju, the analyses of these parsers were provided by the BioNLP 2009 Shared Task organizers. The six parsers operate in"
C10-1088,W10-1905,1,0.178147,"vents as arguments, creating complex event structures. We consider two subtasks, Task 1 and Task 2, out of the three defined in the shared task. Task 1 focuses on core event extraction, and Task 2 involves augmenting extracted events with secondary arguments (Kim et al., 2009). Events are represented with a textual trigger, type, and arguments, where the trigger is a span of text that states the event in text. In Task 1 the event arguments that need to be extracted are restricted to the core Theme and Cause roles, with secondary arEvent Extraction System For evaluation, we apply the system of Miwa et al. (2010b). The system was originally developed for finding core events (Task 1) using the native output of the Enju and GDep parsers. The system consists of three supervised classification-based modules: a trigger detector, an event edge detector, and a complex event detector. The trigger detector classifies each word into the appropriate event types, the event edge detector classifies each edge between an event and a candidate participant into an argument type, and the complex event detector classifies event candidates constructed by all edge combinations, deciding between event and non-event. The s"
C10-1088,W07-1004,1,0.839951,"e results on the test data set. Results on simple, binding, regulation, and all events are shown. GDep and Enju with PAS are used. Results by Miwa et al. (2010b), Bj¨orne et al. (2009), Riedel et al. (2009), and Baseline for Task 1 and Task 2 are shown for comparison. Baseline results are produced by removing dependency information from the parse results of GDep and Enju. The best score in each result is shown in bold. tem. 6 Related Work Many approaches for parser comparison have been proposed, and most comparisons have used gold treebanks with intermediate formats (Clegg and Shepherd, 2007; Pyysalo et al., 2007). Parser comparison has also been proposed on specific tasks such as unbounded dependencies (Rimell ¨ et al., 2009) and textual entailment (Onder Eker, 7 2009) . Among them, application-oriented parser comparison across several formats was first introduced by Miyao et al. (2009), who compared eight parsers and five formats for the protein-protein interaction (PPI) extraction task. PPI extraction, the 785 7 http://pete.yuret.com/ recognition of binary relations of between proteins, is one of the most basic information extraction tasks in the BioNLP field. Our findings do not conflict with those"
C10-1088,W09-1406,1,0.675638,"the event extraction (shown with the gold treebank data in Table 3), but the types and relations of CoNLL were well predicted, and MC and Enju performed better for CoNLL than for SD in total. 5.5 Performance of Event Extraction System Several systems are compared by the extraction performance on the shared task test data in Table 5. GDep and Enju with PAS are used for the evaluation, which is the same evaluation setting with the original system by Miwa et al. (2010b). The performance of the best systems in the original shared task is shown for reference ((Bj¨orne et al., 2009) in Task 1 and (Riedel et al., 2009) in Task 2). The event extraction system performs significantly better than the best systems in the shared task, further outperforming the original system. This shows that the comparison of the parsers is performed with a state-of-the-art sys784 Baseline Bikel Stanford WSJ Stanford eng GDep MC C&C Enju GENIA SD 51.05 53.29 53.51 55.02 55.60 56.09 55.48 56.34 Task 1 CoNLL 53.22 54.38 53.66 55.70 56.01 55.74 56.09 PAS 50.42 56.57 57.94 SD 49.17 51.40 52.02 53.41 53.94 54.27 54.06 55.04 Task 2 CoNLL 51.27 52.04 52.74 54.37 54.51 54.37 54.57 PAS 48.88 55.31 56.40 Table 3: Comparison of F-score res"
C10-1088,D09-1085,0,0.0439988,"Missing"
C10-1088,D07-1111,1,0.192022,"lex formed only with P and P2 NMOD VMOD PMOD SD PTB PAS Figure 5: Format conversion dependencies in six parsers. Formats adopted for the evaluation are shown in solid boxes. SD: Stanford Dependency format, CCG: Combinatory Categorial Grammar output format, PTB: Penn Treebank format, and PAS: Predicate Argument Structure in Enju format. CONJ Figure 3: CoNLL-X dependency tree noun_arg1 arg1 Conll-X CCG prep_arg12 prep_arg12 arg1 arg2 NFAT/AP-1 complex formed only with P and P2 verb_arg1 arg1 adj_arg1 coord_arg12 coord_arg12 arg1 arg1 arg2 Figure 4: Predicate Argument Structure parsers are GDep (Sagae and Tsujii, 2007), the Bikel parser (Bikel) (Bikel, 2004), the Stanford parser with two probabilistic context-free grammar (PCFG) models1 (Wall Street Journal (WSJ) model (Stanford WSJ) and “augmented English” model (Stanford eng)) (Klein and Manning, 2003), the Charniak-Johnson reranking parser, using David McClosky’s self-trained biomedical parsing model (MC) (McClosky, 2009), the C&C CCG parser, adapted to biomedical text (C&C) (Rimell and Clark, 2009), and the Enju parser with the GENIA model (Miyao et al., 2009). The formats are Stanford Dependencies (SD) (Figure 2), the CoNLL-X dependency format (CoNLL)"
C10-1088,I05-2038,1,0.573522,"ax have been successfully applied to a number of tasks in BioNLP. Several parsers and representations have been applied in high-performing methods both in domain studies in general and in the BioNLP’09 shared task in particular, but no direct comparison of parsers or representations has been performed. Likewise, a number of evaluation of parser outputs against gold standard corpora have been performed in the domain, but the broader implications of the results of such intrinsic evaluations are rarely considered. The BioNLP’09 shared task involved documents contained also in the GENIA treebank (Tateisi et al., 2005), creating an opportunity for direct study of intrinsic and task-oriented evaluation results. As the treebank can be converted into various dependency formats using existing format conversion methods, evaluation can further be extended to cover the effects of different representations. 1 Introduction Advanced syntactic parsing methods have been shown to effective for many information extraction tasks. The BioNLP 2009 Shared Task, a recent bio-molecular event extraction task, is one such task: analysis showed that the application of a parser correlated with high rank in the task (Kim In this th"
C10-1089,W08-0601,0,0.0955005,"heir method might remove important information for a given target relation. For example, they might accidentally simplify a noun phrase that is needed to extract the relation. Still, they improved overall PPI extraction recall using such simplifications. To remove unnecessary information from a sentence, some works have addressed sentence simplification by iteratively removing unnecessary phrases. Most of this work is not task-specific; it is intended to compress all information in a target sentence into a few words (Dorr et al., 2003; Vanderwende et al., 2007). Among them, Vickrey and Koller (2008) applied sentence simplification to semantic role labeling. With retaining all arguments of a verb, Vickrey simplified the sentence by removing some information outside of the verb and arguments. 3 Entity-Focused Sentence Simplification We simplify a target sentence using simple rules applicable to the output of a deep parser called Mogura (Matsuzaki et al., 2007), to remove noisy information for relation extraction. Our method relies on the deep parser; the rules depend on the Head-driven Phrase Structure Grammar (HPSG) used by Mogura, and all the rules are written for the parser Enju XML out"
C10-1089,H05-1091,0,0.0707068,"sed as an example relation extraction problem. A dozen simple rules are defined on output from a deep parser. Each rule specifically examines the entities in one target interaction pair. These simple rules were tested using several PPI corpora. The PPI extraction performance was improved on all the PPI corpora. Recently, machine-learning methods, boosted by NLP techniques, have proved to be effective for RE. These methods are usually intended to highlight or select the relation-related regions in parsed sentences using feature vectors or kernels. The shortest paths between a pair of entities (Bunescu and Mooney, 2005) or pair-enclosed trees (Zhang et al., 2006) are widely used as focus regions. These regions are useful, but they can include unnecessary sub-paths such as appositions, which cause noisy features. 1 Introduction Relation extraction (RE) is the task of finding a relevant semantic relation between two given target entities in a sentence (Sarawagi, 2008). Some example relation types are person–organization relations (Doddington et al., 2004), protein– protein interactions (PPI), and disease–gene associations (DGA) (Chun et al., 2006). Among the possible RE tasks, we chose the PPI extraction probl"
C10-1089,W09-1304,0,0.0196783,"es sufficient information to determine the value of the relation in these examples. Relation-related mentions remained for most of the simplification error cases. There were only five critical errors, which changed the truth-value of the relation, out of 46 errors in 241 pairs shown in Table 8. Please note that some rules can be dangerous for other relation extraction tasks. For example, the sentence clause rule could remove modality information (negation, speculation, etc.) modifying the clause, but there are few such cases in the PPI corpora (see Table 8). Also, the task of hedge detection (Morante and Daelemans, 2009) can be solved separately, in the original sentences, after the interacting pairs have been found. For example, in the BioNLP shared task challenge and the BioInfer corpus, interaction detection and modality are treated as two different tasks. Once other NLP tasks, like static relation (Pyysalo et 794 al., 2009) or coreference resolution, become good enough, they can supplement or even substitute some of the proposed rules. There are different difficulties in the BioInfer and AIMed corpora. BioInfer includes more complicated sentences and problems than the other corpora do, because 1) the appo"
C10-1089,W09-1301,1,0.461091,"Missing"
C10-1089,doddington-etal-2004-automatic,0,0.0213557,"ded to highlight or select the relation-related regions in parsed sentences using feature vectors or kernels. The shortest paths between a pair of entities (Bunescu and Mooney, 2005) or pair-enclosed trees (Zhang et al., 2006) are widely used as focus regions. These regions are useful, but they can include unnecessary sub-paths such as appositions, which cause noisy features. 1 Introduction Relation extraction (RE) is the task of finding a relevant semantic relation between two given target entities in a sentence (Sarawagi, 2008). Some example relation types are person–organization relations (Doddington et al., 2004), protein– protein interactions (PPI), and disease–gene associations (DGA) (Chun et al., 2006). Among the possible RE tasks, we chose the PPI extraction problem. PPI extraction is a major RE task; In this paper, we propose a method to remove information that is deemed unnecessary for RE. Instead of selecting the whole region between a target pair, the target sentence is simplified into simpler, pair-related, sentences using general, task-independent, rules. By addressing particularly the target entities, the rules do not affect important relation-related expressions between the target entities"
C10-1089,W03-0501,0,0.00953737,"k grammar parser by simplifying the target sentence in a general manner, so their method might remove important information for a given target relation. For example, they might accidentally simplify a noun phrase that is needed to extract the relation. Still, they improved overall PPI extraction recall using such simplifications. To remove unnecessary information from a sentence, some works have addressed sentence simplification by iteratively removing unnecessary phrases. Most of this work is not task-specific; it is intended to compress all information in a target sentence into a few words (Dorr et al., 2003; Vanderwende et al., 2007). Among them, Vickrey and Koller (2008) applied sentence simplification to semantic role labeling. With retaining all arguments of a verb, Vickrey simplified the sentence by removing some information outside of the verb and arguments. 3 Entity-Focused Sentence Simplification We simplify a target sentence using simple rules applicable to the output of a deep parser called Mogura (Matsuzaki et al., 2007), to remove noisy information for relation extraction. Our method relies on the deep parser; the rules depend on the Head-driven Phrase Structure Grammar (HPSG) used by"
C10-1089,P08-1040,0,0.189425,"eneral manner, so their method might remove important information for a given target relation. For example, they might accidentally simplify a noun phrase that is needed to extract the relation. Still, they improved overall PPI extraction recall using such simplifications. To remove unnecessary information from a sentence, some works have addressed sentence simplification by iteratively removing unnecessary phrases. Most of this work is not task-specific; it is intended to compress all information in a target sentence into a few words (Dorr et al., 2003; Vanderwende et al., 2007). Among them, Vickrey and Koller (2008) applied sentence simplification to semantic role labeling. With retaining all arguments of a verb, Vickrey simplified the sentence by removing some information outside of the verb and arguments. 3 Entity-Focused Sentence Simplification We simplify a target sentence using simple rules applicable to the output of a deep parser called Mogura (Matsuzaki et al., 2007), to remove noisy information for relation extraction. Our method relies on the deep parser; the rules depend on the Head-driven Phrase Structure Grammar (HPSG) used by Mogura, and all the rules are written for the parser Enju XML out"
C10-1089,P06-1104,0,0.166397,"ozen simple rules are defined on output from a deep parser. Each rule specifically examines the entities in one target interaction pair. These simple rules were tested using several PPI corpora. The PPI extraction performance was improved on all the PPI corpora. Recently, machine-learning methods, boosted by NLP techniques, have proved to be effective for RE. These methods are usually intended to highlight or select the relation-related regions in parsed sentences using feature vectors or kernels. The shortest paths between a pair of entities (Bunescu and Mooney, 2005) or pair-enclosed trees (Zhang et al., 2006) are widely used as focus regions. These regions are useful, but they can include unnecessary sub-paths such as appositions, which cause noisy features. 1 Introduction Relation extraction (RE) is the task of finding a relevant semantic relation between two given target entities in a sentence (Sarawagi, 2008). Some example relation types are person–organization relations (Doddington et al., 2004), protein– protein interactions (PPI), and disease–gene associations (DGA) (Chun et al., 2006). Among the possible RE tasks, we chose the PPI extraction problem. PPI extraction is a major RE task; In th"
C10-1096,P07-1083,0,0.0097517,"ity Sensitive Hash (LSH) function (Andoni and Indyk, 2008), which preserves the property of cosine similarity. The essence of this function is to map strings into N -bit hash values where the bitwise hamming distance between the hash values of two strings approximately corresponds to the angle of the two strings. Ravichandran et al. (2005) applied LSH to the task of noun clustering. Adapting this algorithm to approximate dictionary matching, we discussed its performance in Section 3. Several researchers have presented refined similarity measures for strings (Winkler, 1999; Cohen et al., 2003; Bergsma and Kondrak, 2007; Davis et al., 2007). Although these studies are sometimes regarded as a research topic of approximate dictionary matching, they assume that two strings for the target of similarity computation are given; in other words, it is out of their scope to find strings in a large collection that are similar to a given string. Thus, it is a reasonable approach for an approximate dictionary matching to quickly collect candidate strings with a loose similarity threshold, and for a refined similarity measure to scrutinize each candidate string for the target application. 5 Conclusions We present a simple"
C10-1096,P05-1077,0,0.464443,"exactly by a τ -overlap join (Sarawagi and Kirpal, 2004) of inverted lists. Then we present CPMerge, which is a simple and efficient algorithm for the τ -overlap join. In addition, the algorithm is easily implemented. 2. We demonstrate the efficiency of the algorithm on three large-scale datasets with person names, biomedical concept names, 851 Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 851–859, Beijing, August 2010 and general English words. We compare the algorithm with state-of-the-art algorithms, including Locality Sensitive Hashing (Ravichandran et al., 2005; Andoni and Indyk, 2008) and DivideSkip (Li et al., 2008). The proposed algorithm retrieves strings the most rapidly, e.g., in 1.1 ms from Google Web1T unigrams (with cosine similarity and threshold 0.7). 2 Proposed Method 2.1 In this paper, we assume that the features of a string are represented arbitrarily by a set. Although it is important to design a string representation for an accurate similarity measure, we do not address this problem: our emphasis is not on designing a better representation for string similarity but on establishing an efficient algorithm. The most popular representati"
C10-1144,W03-3006,0,0.0295547,"hm to train a CCG parser. Different from their work, we focused on improving the performance of the deep parser by refining the training method for supertagging. Ninomiya et al. (2007) used the supertagging probabilities as a reference distribution for the log-linear model for HPSG, which aimed to consistently integrate supertagging into probabilistic HPSG parsing. Prins et al. (2001) trained a POStagger on an automatic parser-generated lexical entry corpus as a filter for Dutch HPSG parsing to improve the parsing speed and accuracy. 1287 Related Work The existing work most similar to ours is Boullier (2003). He presented a non-statistical parsing-based supertagger for LTAG. Similar to his method, we used a CFG to approximate the original lexicalized grammar. The main difference between these two methods is that we consider the grammar constraints in the training phase of the supertagger, not only in the supertagging test phase and our main objective is to improve the performance of the final parser. 6 Conclusions and Future Work In this paper, based on the observation that supertaggers are commonly trained separately from lexicalized parsers without global grammar constraints, we proposed a fore"
C10-1144,W03-1006,0,0.0595728,"Missing"
C10-1144,C04-1041,0,0.0370791,"Missing"
C10-1144,W07-1202,0,0.0337935,"Missing"
C10-1144,W02-2203,0,0.0712976,"Missing"
C10-1144,W02-1001,0,0.0540738,"ansitive verbs in non-3rd person present form, which indicates that the head syntactic category of “like” is verb and it has an NP subject and an NP complement. With such fine-grained grammatical type distinctions, the number of supertags is very large. Compared to the 45 part-of-speech (POS) tags defined in the PennTreebank, the HPSG grammar we used contains 2,308 supertags. The large number and the complexity of the supertags makes supertagging harder than the POS tagging task. Supertagging can be formulated as a sequence labeling task. Here, we follow the definition of Collins’ perceptron (Collins, 2002). The training objective of supertagging is to learn the mapping from a POS-tagged word sentence w = (w1 /p1 , ..., wn /pn ) to a sequence of supertags s = (s1 , ..., sn ). We use function GEN (w) to indicate all candidates of supertag sequences given input w. Feature function Φ maps a sample (w, s) to a point in the feature space Rd . θ is the vector of feature weights. Given an input w, the most plausible supertag sequence is found by the prediction function defined as follows: F (w) = argmax θ · Φ(w, s) (1) s∈GEN(w) 2.3 CFG-filtering CFG-filtering (Kiefer and Krieger, 2000) is a technique t"
C10-1144,P07-1037,0,0.049781,"Missing"
C10-1144,2000.iwpt-1.15,0,0.0491666,"ion of Collins’ perceptron (Collins, 2002). The training objective of supertagging is to learn the mapping from a POS-tagged word sentence w = (w1 /p1 , ..., wn /pn ) to a sequence of supertags s = (s1 , ..., sn ). We use function GEN (w) to indicate all candidates of supertag sequences given input w. Feature function Φ maps a sample (w, s) to a point in the feature space Rd . θ is the vector of feature weights. Given an input w, the most plausible supertag sequence is found by the prediction function defined as follows: F (w) = argmax θ · Φ(w, s) (1) s∈GEN(w) 2.3 CFG-filtering CFG-filtering (Kiefer and Krieger, 2000) is a technique to find a superset of (packed) HPSG parse trees that satisfy the constraints in a grammar. A CFG that approximates the original HPSG grammar is used for efficiently finding such trees without doing full-fledged HPSG parsing that is computationally demanding because the schema application involves unification operations among large feature structures (signs). The number of possible signs is infinite in general and hence 1282 Figure 1: HPSG parsing for the sentence “They like coffee.” some features (e.g., the number agreement feature) are ignored in the approximating CFG so that"
C10-1144,J99-2004,0,0.161949,"Missing"
C10-1144,P05-1011,1,0.841523,"g methods was also investigated. 4.1 Corpus Description The HPSG grammar used in the experiments is Enju version 2.32 . It is semi-automatically converted from the WSJ portion of PennTreebank (Miyao, 2006). The grammar consists of 2,308 supertags in total. Sections 02-21 were used to train different supertagging models and the HPSG parser. Section 22 and section 23 were used as the development set and the test set respectively. We evaluated the HPSG parser performance by labeled precision (LP) and labeled recall (LR) of predicate-argument relations of the parser’s output as in previous works (Miyao, 2005). All experiments were conducted on an AMD Opteron 2.4GHz server. Template Type Word POS Word-POS Template wi ,wi−1 ,wi+1 , wi−1 &wi , wi &wi+1 pi , pi−1 , pi−2 , pi+1 , pi+2 , pi−1 &pi , pi−2 &pi−1 , pi−1 &pi+1 , pi &pi+1 , pi+1 &pi+2 pi−1 &wi , pi &wi , pi+1 &wi Table 1: Feature templates used for supertagging models. 1 “UNK” supertags are ignored in evaluation as in previous works. 2 http://www-tsujii.is.s.u-tokyo.ac.jp/enju/index.html 1284 4.2 Baseline Models and Settings We used a point-wise averaged perceptron (PW) to train a baseline supertagger. Point-wise classifiers have been reporte"
C10-1144,W07-0702,0,0.0405674,"Missing"
C10-1144,W06-1619,1,0.908214,"Missing"
C10-1144,W07-2208,1,0.900258,"Missing"
C10-1144,W01-1815,0,0.0835441,"Missing"
C10-1144,W09-3832,1,0.612735,"Missing"
C10-2098,W04-3204,0,0.0257792,"ide variety of semantic categories (e.g., gene, disease, etc.). For example, ER may denote protein estrogen receptor in one context, but cell subunit endoplasmic reticulum in another, One way to entity disambiguation is classifying an entity into pre-defined semantic categories, based on its context (e.g., (Bunescu and Pas¸ca, 2006)). Existing classifiers, such as maximum entropy model, achieved satisfactory results on the “majority” classes with abundant training instances, but failed on the “minority” ones with few or even zero training instances, i.e., the knowledge acquisition bottleneck (Agirre and Martinez, 2004). Furthermore, it is often infeasible to create enough training data for all existing semantic classes. In addition, too many training instances for certain majority classes lead to increased computational complexity for training, and a biased system ignoring the minority ones. These correspond to two previously addressed difficulties in imbalanced learning: “... either (i) you have far more data than your algorithms can deal with, 851 Coling 2010: Poster Volume, pages 851–859, Beijing, August 2010 and you have to select a sample, or (ii) you have no data at all and you have to go through an i"
C10-2098,E06-1002,0,0.0570834,"Missing"
C10-2162,P00-1058,0,0.0298208,"d particular attention to a different grammar framework, i.e. HPSG, with the analysis of more Chinese constructions, such as the serial verb construction. In addition, in our on-going deep parsing work, we use the developed Chinese HPSG grammar, i.e. the lexical entries, to train a full-fledged HPSG parser directly. Additionally, there are some works that induce lexicalized grammar from corpora for other languages. For example, by using the Penn Treebank, Miyao et al. (2005) automatically extracted a large HPSG lexicon, Xia (1999), Chen and Shanker (2000), Hockenmaier and Steedman (2002), and Chiang (2000) invented LTAG/CCG specific procedures for lexical entry extraction. From the German Tiger corpus, Cramer and Zhang (2009) constructed a German HPSG grammar; Hockenmaier (2006) created a German CCGbank; and Rehbei and Genabith (2009) acquired LFG resources. In addition, Schluter and Genabith (2009) automatically obtained widecoverage LFG resources from a French Treebank. Our work implements a similar idea to these works, but we apply different grammar design and annotation rules, which are specific to Chinese. Furthermore, we obtained a comparative result to state-of-the-art works for English."
C10-2162,P04-1014,0,0.103169,"Missing"
C10-2162,W09-2605,0,0.0464518,"ons, such as the serial verb construction. In addition, in our on-going deep parsing work, we use the developed Chinese HPSG grammar, i.e. the lexical entries, to train a full-fledged HPSG parser directly. Additionally, there are some works that induce lexicalized grammar from corpora for other languages. For example, by using the Penn Treebank, Miyao et al. (2005) automatically extracted a large HPSG lexicon, Xia (1999), Chen and Shanker (2000), Hockenmaier and Steedman (2002), and Chiang (2000) invented LTAG/CCG specific procedures for lexical entry extraction. From the German Tiger corpus, Cramer and Zhang (2009) constructed a German HPSG grammar; Hockenmaier (2006) created a German CCGbank; and Rehbei and Genabith (2009) acquired LFG resources. In addition, Schluter and Genabith (2009) automatically obtained widecoverage LFG resources from a French Treebank. Our work implements a similar idea to these works, but we apply different grammar design and annotation rules, which are specific to Chinese. Furthermore, we obtained a comparative result to state-of-the-art works for English. There are some researchers who worked on Chinese HPSG grammar development manually. Zhang (2004) implemented a Chinese HP"
C10-2162,hockenmaier-steedman-2002-acquiring,0,0.475048,"cted adjuncts. But in our grammar, we only deal with extracted arguments, and the gap in a relative clause (as indicated in the dash-boxed part in Figure 12). When the extracted phrase is an adjunct of the relative clause, we simply view the clause as a modifier of the extracted phrase. shown in Figure 13, we obtain a lexical entry for the word ‘写/write’ as shown in Figure 14. 3.1.2 Rules for Correcting Inconsistency There are some inconsistencies in the annotation of the CTB, which presents difficulties for performing the derivation tree annotation. Therefore, we define 49 rules, as done in (Hockenmaier and Steedman, 2002) for English, to mitigate inconsistencies before annotation (refer to Table 3). 3.1.3 Rules for Assisting Annotation We also define 48 rules (refer to Table 2), which are similar to the rules used in (Miyao, 2006) for English, to help the derivation tree annotation. For example, 12 pattern rules are defined to assign the schemas to corresponding constituents. Figure 13. HPSG derivation tree for Figure 8. Rule Type Rules for correcting inconsistent annotation Rule Description Rule # Fix tree annotation 37 Fix phrase tag annotation 5 Fix functional tag annotation 5 Fix POS tag annotation 2 Slash"
C10-2162,J08-1002,1,0.92028,"Missing"
C10-2162,schluter-van-genabith-2008-treebank,0,0.0354136,"Missing"
C10-2162,Y09-2048,1,0.594404,"Missing"
C10-2162,C02-1145,0,0.0248821,"work, we paid particular attention to a different grammar framework, i.e. HPSG, with the analysis of more Chinese constructions, such as the serial verb construction. In addition, in our on-going deep parsing work, we use the developed Chinese HPSG grammar, i.e. the lexical entries, to train a full-fledged HPSG parser directly. Additionally, there are some works that induce lexicalized grammar from corpora for other languages. For example, by using the Penn Treebank, Miyao et al. (2005) automatically extracted a large HPSG lexicon, Xia (1999), Chen and Shanker (2000), Hockenmaier and Steedman (2002), and Chiang (2000) invented LTAG/CCG specific procedures for lexical entry extraction. From the German Tiger corpus, Cramer and Zhang (2009) constructed a German HPSG grammar; Hockenmaier (2006) created a German CCGbank; and Rehbei and Genabith (2009) acquired LFG resources. In addition, Schluter and Genabith (2009) automatically obtained widecoverage LFG resources from a French Treebank. Our work implements a similar idea to these works, but we apply different grammar design and annotation rules, which are specific to Chinese. Furthermore, we obtained a comparative result to state-of-the-art"
C10-2162,N04-1013,0,\N,Missing
C10-2162,W02-1502,0,\N,Missing
C10-2162,P06-1064,0,\N,Missing
C10-2162,W08-1700,0,\N,Missing
C14-1150,J10-4006,0,0.284746,"birth place of A is B”), most of expressions are not (such as“A comes from B”). We call a set of pairs which define a relation as Extension set of a relation, while we call their observed expressions in text as Manifestation set. While these two sets are only partially given, they define relations which we are interested in. Such duality of a relation has been recognized by many previous work and has been exploited in relation mining and relation expression mining. (Bollegala et al., 2010), for example, used the duality in their work on co-clustering of entity-pairs and relation expressions. (Baroni and Lenci, 2010) presented a more general approach which defines a tensor associating a triplet < e1 , l, e2 > with a weight. e1 and e2 are entity pairs, while l is a linking expression in text. By projecting the tensor to matrices, they showed that diverse concepts used in distributional semantics could be captured in a unified manner. In particular, their tensors capture directly the duality of entity pairs and their linking expressions (i.e. relation expressions). These previous works implicitly assume that the semantic space of entity pairs and that of relation ex1580 pressions are tightly coupled. That i"
C14-1150,P06-1017,0,0.037852,"guities of each relation expression and entity-pair. Yao et al. (2011; 2012) proposed a new triplet clustering method through a generative probabilistic model. The model used surrounding contexts as features in both a sentence and document level to identify the meaning of each triplet. They demonstrated the effectiveness of their models compared with USP (Poon and Domingos, 2009) or DIRT (Lin and Pantel, 2001). Min et al. (2012) provided a simple and scalable triplet clustering algorithm in an unsupervised way and enables to incorporate various resources about entity and relation expressions. Chen et al. (2006) proposed a label propagation algorithm for relation extraction as a semi-supervised learning method by utilizing the information of parsing. 6 Conclusion We propose a common space embedding framework which constructs a semantic space in which both entity-pairs and relation expressions are represented. We showed that our framework is effective to construct the extension set and the manifestation set of a relation R in this space. The results of experiments showed that the common space is further refined for tasks such as relation and relation expression mining, compared with the original two s"
C14-1150,D11-1142,0,0.0234393,"is difficult to incorporate the similarity information between entity-pairs or similarities between relation expressions into the decomposition procedure in contract to our framework based on MVPLS. Lin and Pantel (2001) proposed a weakly supervised framework of mining paraphrases based on shortest paths as basic units to be mined. Our work can be viewed as an extension by mixing entity-pair characterizations with the extended distributional hypothesis by embedding. Many other previous work have been proposed to construct a knowledge base, including relation expressions (Carlson et al., 2010; Fader et al., 2011; Nakashole et al., 2012). However, they cannot interactively predict semantic meanings of objects through labeled objects of the other space. As for treatment of ambiguity, some previous work has focused on triplet clustering to disambiguate each triplet object known as relation extraction. Unlike other mining tasks, this task requires a system to disambiguate the meaning of a relation expression r in hr, e1 , e2 i which appears in a specific context. We did not treat this task in this paper, however, our framework would discharge the burden by showing the insight of ambiguities of each relat"
C14-1150,P05-1045,0,0.0100123,"semantic categories: ACQUISITION, HEADQUARTERS, FIELD, CEO, and BIRTHPLACE. We use the ENT dataset not only for evaluation of relation mining but also for examining the characteristics of the common space for relation expression mining. Note that, due to the nature of snippets, the dataset is very noisy. It contains many non-sentences and even non-English texts, which may adversely affect the performance of mining systems. 4.1.2 Entity and Entity-Pair Extraction We first extracted entities from the ENT dataset. After splitting snippets into sentences, we applied named entity recognizer (NER) (Finkel et al., 2005) to recognize entities in sentences. We used Stanford Core NLP tools 21 for sentence splitting and NER. As relevant semantic classes for the ENT dataset, entities which are recognized as ORGANIZATION, LOCATION, or PERSON are treated as entities in the further process. We only used sentences in which at least two entities of these three classes appear. 4.1.3 Extraction of Relation Expressions The definition of relation expressions which link two entities in text is not trivial. We adopt two methods of extracting candidates of relation expressions, and compare them in experiments. The first meth"
C14-1150,D12-1094,0,0.0207956,"ession r in hr, e1 , e2 i which appears in a specific context. We did not treat this task in this paper, however, our framework would discharge the burden by showing the insight of ambiguities of each relation expression and entity-pair. Yao et al. (2011; 2012) proposed a new triplet clustering method through a generative probabilistic model. The model used surrounding contexts as features in both a sentence and document level to identify the meaning of each triplet. They demonstrated the effectiveness of their models compared with USP (Poon and Domingos, 2009) or DIRT (Lin and Pantel, 2001). Min et al. (2012) provided a simple and scalable triplet clustering algorithm in an unsupervised way and enables to incorporate various resources about entity and relation expressions. Chen et al. (2006) proposed a label propagation algorithm for relation extraction as a semi-supervised learning method by utilizing the information of parsing. 6 Conclusion We propose a common space embedding framework which constructs a semantic space in which both entity-pairs and relation expressions are represented. We showed that our framework is effective to construct the extension set and the manifestation set of a relati"
C14-1150,P05-1011,1,0.597538,"constraints based on part-of-speech tags, lexical-syntactic information, etc. Our contention is that such ad-hoc constraints unnecessarily restrict a set of relation expressions. Our method treats ambiguous expressions (e.g. “of”, “in”, “with”, etc.) as relation expressions. Instead, the effectiveness or the degree of ambiguities of a relation expression is captured in the common space after embedding. 1 http://nlp.stanford.edu/software/corenlp.shtml 1584 The second method is based on dependency parsing. We obtain the dependency tree of a sentence by a publicly available deep parser, Enju32 (Miyao and Tsujii, 2005; Miyao and Tsujii, 2008), and then extract shortest paths between two entities. Unlike the first method, this method uses linguistic information to extract the skeleton of a relation expression. Each node in shortest paths consists of a base form (e.g., “like”, “player”), syntactic category (e.g., “verb”, “noun”), and predicate-argument links. The length of shortest paths was restricted to the range from 1 to 6. Compared with the first method, a set of shortest paths contains much less noises, so that we do not filter out those with low frequency. In the same way as the first method, a set of"
C14-1150,J08-1002,1,0.81757,"rt-of-speech tags, lexical-syntactic information, etc. Our contention is that such ad-hoc constraints unnecessarily restrict a set of relation expressions. Our method treats ambiguous expressions (e.g. “of”, “in”, “with”, etc.) as relation expressions. Instead, the effectiveness or the degree of ambiguities of a relation expression is captured in the common space after embedding. 1 http://nlp.stanford.edu/software/corenlp.shtml 1584 The second method is based on dependency parsing. We obtain the dependency tree of a sentence by a publicly available deep parser, Enju32 (Miyao and Tsujii, 2005; Miyao and Tsujii, 2008), and then extract shortest paths between two entities. Unlike the first method, this method uses linguistic information to extract the skeleton of a relation expression. Each node in shortest paths consists of a base form (e.g., “like”, “player”), syntactic category (e.g., “verb”, “noun”), and predicate-argument links. The length of shortest paths was restricted to the range from 1 to 6. Compared with the first method, a set of shortest paths contains much less noises, so that we do not filter out those with low frequency. In the same way as the first method, a set of shortest paths contains"
C14-1150,D12-1104,0,0.0208131,"rporate the similarity information between entity-pairs or similarities between relation expressions into the decomposition procedure in contract to our framework based on MVPLS. Lin and Pantel (2001) proposed a weakly supervised framework of mining paraphrases based on shortest paths as basic units to be mined. Our work can be viewed as an extension by mixing entity-pair characterizations with the extended distributional hypothesis by embedding. Many other previous work have been proposed to construct a knowledge base, including relation expressions (Carlson et al., 2010; Fader et al., 2011; Nakashole et al., 2012). However, they cannot interactively predict semantic meanings of objects through labeled objects of the other space. As for treatment of ambiguity, some previous work has focused on triplet clustering to disambiguate each triplet object known as relation extraction. Unlike other mining tasks, this task requires a system to disambiguate the meaning of a relation expression r in hr, e1 , e2 i which appears in a specific context. We did not treat this task in this paper, however, our framework would discharge the burden by showing the insight of ambiguities of each relation expression and entity"
C14-1150,D09-1001,0,0.0120598,"s a system to disambiguate the meaning of a relation expression r in hr, e1 , e2 i which appears in a specific context. We did not treat this task in this paper, however, our framework would discharge the burden by showing the insight of ambiguities of each relation expression and entity-pair. Yao et al. (2011; 2012) proposed a new triplet clustering method through a generative probabilistic model. The model used surrounding contexts as features in both a sentence and document level to identify the meaning of each triplet. They demonstrated the effectiveness of their models compared with USP (Poon and Domingos, 2009) or DIRT (Lin and Pantel, 2001). Min et al. (2012) provided a simple and scalable triplet clustering algorithm in an unsupervised way and enables to incorporate various resources about entity and relation expressions. Chen et al. (2006) proposed a label propagation algorithm for relation extraction as a semi-supervised learning method by utilizing the information of parsing. 6 Conclusion We propose a common space embedding framework which constructs a semantic space in which both entity-pairs and relation expressions are represented. We showed that our framework is effective to construct the e"
C14-1150,D12-1110,0,0.0185427,"ity can be explicitly checked by their proximities to known sets of entity-pairs. We also experimentally validate the effectiveness of the common space for relation mining and relation expression mining. 1 Introduction Learning continuous vector representation for expressions which consist of more than one word has gained attention in recent years. Various representations have been constructed and used to measure semantic similarities between expressions in various tasks, such as analogical reasoning (Turney et al., 2003; Mikolov et al., 2013) and sentiment analysis (Turney and Littman, 2003; Socher et al., 2012). Many algorithms have been proposed to construct such continuous representations, depending on specific tasks in mind. In this paper, we propose a method for constructing a vector representation for binary relations, i.e., relations with two arguments. We demonstrate the effectiveness of the representation for relation mining and relation expression mining. The method exploits the duality of a relation (Bollegala et al., 2010). While Bollegala et al. (2010) uses the duality in their co-clustering algorithm, we construct an explicit semantic space which reflects the two aspects of a given rela"
C14-1150,D11-1135,0,0.0227569,"er, they cannot interactively predict semantic meanings of objects through labeled objects of the other space. As for treatment of ambiguity, some previous work has focused on triplet clustering to disambiguate each triplet object known as relation extraction. Unlike other mining tasks, this task requires a system to disambiguate the meaning of a relation expression r in hr, e1 , e2 i which appears in a specific context. We did not treat this task in this paper, however, our framework would discharge the burden by showing the insight of ambiguities of each relation expression and entity-pair. Yao et al. (2011; 2012) proposed a new triplet clustering method through a generative probabilistic model. The model used surrounding contexts as features in both a sentence and document level to identify the meaning of each triplet. They demonstrated the effectiveness of their models compared with USP (Poon and Domingos, 2009) or DIRT (Lin and Pantel, 2001). Min et al. (2012) provided a simple and scalable triplet clustering algorithm in an unsupervised way and enables to incorporate various resources about entity and relation expressions. Chen et al. (2006) proposed a label propagation algorithm for relatio"
C14-1150,P12-1075,0,0.0421837,"Missing"
C82-1062,C80-1065,0,\N,Missing
C86-1021,P84-1086,1,\N,Missing
C86-1021,J85-2001,1,\N,Missing
C86-1029,C86-1021,1,\N,Missing
C86-1029,C82-1040,1,\N,Missing
C86-1029,P84-1057,1,\N,Missing
C86-1029,P84-1069,1,\N,Missing
C86-1029,P84-1086,1,\N,Missing
C86-1029,J85-2001,1,\N,Missing
C86-1155,J82-2005,0,0.0527636,"Missing"
C86-1155,J85-2002,0,\N,Missing
C86-1155,J85-1002,0,\N,Missing
C86-1155,J85-2004,0,\N,Missing
C86-1155,C86-1021,1,\N,Missing
C86-1155,C80-1068,0,\N,Missing
C86-1155,C82-1038,0,\N,Missing
C86-1155,C86-1147,0,\N,Missing
C86-1155,C80-1067,0,\N,Missing
C86-1155,C86-1029,1,\N,Missing
C86-1155,C86-1150,0,\N,Missing
C86-1155,C82-1004,0,\N,Missing
C86-1155,P84-1018,0,\N,Missing
C86-1155,P84-1100,0,\N,Missing
C86-1155,P84-1057,1,\N,Missing
C86-1155,P84-1069,1,\N,Missing
C86-1155,P84-1086,1,\N,Missing
C86-1155,P84-1011,0,\N,Missing
C86-1155,P84-1050,0,\N,Missing
C86-1155,J85-2001,1,\N,Missing
C86-1155,J85-2003,0,\N,Missing
C86-1155,J85-1001,0,\N,Missing
C86-1155,J85-1003,0,\N,Missing
C86-1155,J85-2005,0,\N,Missing
C88-2141,P84-1057,1,\N,Missing
C88-2142,1987.mtsummit-1.23,1,0.745733,"Missing"
C88-2142,C86-1021,1,\N,Missing
C88-2142,C86-1149,0,\N,Missing
C88-2142,J85-2001,1,\N,Missing
C90-3048,C86-1007,0,0.0237955,"Missing"
C90-3048,1993.tmi-1.14,0,0.0828039,"Missing"
C90-3048,C86-1077,0,0.248654,"Missing"
C90-3048,C82-1034,0,0.4329,"Missing"
C90-3048,C88-2155,0,0.605756,"Missing"
C90-3048,P88-1019,1,\N,Missing
C92-2085,J86-3002,0,\N,Missing
C92-2085,A92-1014,1,\N,Missing
C92-2085,C90-1005,0,\N,Missing
C92-2085,A88-1019,0,\N,Missing
C92-2085,P91-1030,0,\N,Missing
C92-2085,P90-1032,0,\N,Missing
C92-2102,E91-1048,0,0.202503,"Missing"
C92-2102,E89-1037,0,\N,Missing
C92-2102,C90-3044,0,\N,Missing
C92-2102,P91-1021,0,\N,Missing
C94-2122,J93-1007,0,\N,Missing
C94-2122,A88-1019,0,\N,Missing
C94-2122,P90-1034,0,\N,Missing
C94-2122,P91-1034,0,\N,Missing
C94-2134,P91-1027,0,0.133197,"Missing"
C94-2134,E93-1027,1,0.883725,"Missing"
C94-2134,H91-1067,0,\N,Missing
C94-2134,C92-1022,0,\N,Missing
C94-2134,P93-1032,0,\N,Missing
C94-2134,P89-1013,0,\N,Missing
C94-2134,C92-2072,0,\N,Missing
C94-2192,J92-4007,0,\N,Missing
C96-2160,P95-1013,0,\N,Missing
C96-2160,P85-1018,0,\N,Missing
C98-2128,1995.iwpt-1.9,0,0.0445456,"ms is able to achieve the efficiency we established as our goal. Moreover, these two systems have serious disadvantages as a framework for practical applications. The ProFIT approach, for example, tends to consume too much memory for execution. It is also difficult, if not impossible, to combine them with other techniques like parallel parsing, etc., because these two systems have been embedded in Prolog. 1.2 O u r Approach One of the promising directions of improving the efficiency of handling TFSs while retaining a necessary amount of flexibility is to take up the idea of AMAVL proposed in (Carpenter and Qu, 1995) to design a general programming system based on TFS. LiLFeS is a logic programming system thus designed and developed by our group, based on AMAVLimplementation. LiLFeS can be characterized as follows. • Architecture based on an AMAVL implementation, which compiles a TFS into a sequence of abstract machine instructions, and performs unification of the TFS by emulating the execution of those instructions. Although the proposal of such an A M A V L was already made in 1995, no serious implementation has been reported. We believe that LiLFeS is the first serious treatment of the proposal. • Rich"
C98-2128,E95-1025,0,0.0251135,"y the project of Japan Society for the Promotion of Science (JSPS-RFTF96P00502). I LIFE (AYt-kaci et al., 1994) is also famous, but we do not discuss it because it does not follow Carpenter's TFS definition. Moreover, our separate experiments show that LIFE is more than 10 times slower than emulator-based LiLFeS. As for AMALIA (Wintner, 1997), we cannot make experiments since it is not freely distributed. His experiments in his dissertation shows that AMALIA is 15 time faster than ALE at maximum; it is close to emulator-based LiLFeS, and is outperformed by native-code compiler of LiLFeS. 807 (Erbach, 1995), a TFS-to-Prolog-term compiler. However, as the comparison of these systems with our system (Section 3.2) shows, neither of these two systems is able to achieve the efficiency we established as our goal. Moreover, these two systems have serious disadvantages as a framework for practical applications. The ProFIT approach, for example, tends to consume too much memory for execution. It is also difficult, if not impossible, to combine them with other techniques like parallel parsing, etc., because these two systems have been embedded in Prolog. 1.2 O u r Approach One of the promising directions"
C98-2128,C96-2160,1,0.413816,"• A na'fve parser using a CYK-like algorithm. Although using a simple algorithm, the parser utilizes the full capabilities provided by LiLFeS, such as built-in predicates (TFS copy, array op4 The grammar does not contain semantic analysis such as coreference resolution. 809 Condition: 600 sentences from EDR Japanese corpus (average length 21 words), Average in the parsing of successflflly parsed 539 sentences Environment: DEC Alpha 500/400MHz with 256MB memory Table 2 Parsing Performance Evaluation with a Practical Grammar eration, etc.). • A parser based on the Torisawa's parsing algorithm (Torisawa and Tsujii, 1996). This algorithm compiles an HPSG grammar into 2 parts: its CFG skeletons and a remaining part, and parses a sentence in two phases. Although the parser is not a complete implementation of the algorithm, its efficiency benefits from its 2phase parsing, which reduces the amount of unification. These parsers and grammars are used for the performance evaluations in the next section. 4.2 P e r f o r m a n c e E v a l u a t i o n We evaluated the pertbrmance of the LiLFeS system over three aspects: Parsing performance of LiLFeS, comparison to other TFS systems, and comparison to different Prolog sy"
C98-2128,P98-2144,1,0.686497,"Missing"
C98-2128,J94-2001,0,\N,Missing
C98-2128,C94-1027,0,\N,Missing
C98-2128,C98-2139,1,\N,Missing
C98-2139,J94-4001,0,0.16372,"e. The way how we abandon the t r e a t m e n t of rare linguistic p h e n o m e n a is by introducing additional constraints in feature structures. Regarding (i) and (ii), we introduce &apos;pseudo-principles&apos;, which are unified with ID schemata in the same way principles are unified. Regarding (iii), we add some feature structures to LEs/LETs. 3.1 P o s t p o s i t i o n &apos;Wa&apos; The main usage of the postposition &apos;wa&apos; is divided into the following two patternsS: • If two PPs with the postposition &apos;wa&apos; appear consecutively, we treat the first PP as 5These patterns are almost similar to tile ones in (Kurohashi and Nagao, 1994). (a) (b)* ......... ~. . . . . . . . . . . . . . . . + ...... d>....... ....... lyOtl (c) WX d.d,. &apos; T..... i ÷--+--+ +--+--+ &apos; l .... &|~itX ! • ...... + I 4--4--4 +-+-+ I lq &apos; d.., IX Figure 2: Correct parse tree for Sentence (4) (d)* ......... l ........ ...... i +-+-+ +-+-+ &apos; [ ; ....... i........... *1 +_+_+ +-+-+ ,._ho@,N E} where Figure 1: (a) Correct / (b) incorrect parse tree for Sentence (2); (c) correct / (d) incorrect parse tree for Sentence (3) a complement of a predicate just before the second PP. • Otherwise, P P with the postposition &apos;wa&apos; is treated as the complement of the la"
C98-2139,P98-2132,1,0.726761,"such a nominal suffix and comma are often used adverbially (Sentence (6) & Figure 4(a) ), while general NPs with a comma are used in coordinate structures (Sentence (7) gz Figure 4(b) ). I went to Kyoto andNara. In order to restrict the behavior of NPs with nonfinal time suffixes and commas to adverbial usage only, we added the following constraint to the LE of a comma, constructing a coordinate structure: [ MARKISYNILOCALIN-SUFFIX - ] This prohibits an NP with a nominal suffix fl&apos;om being marked by a comma for coordination. 4 Experiments We implemented our parser and g r a m m a r in LiLFeS (Makino et al., 1998) s, a featurestructure descrit)tion language developed 1).~ our group. We tested randomly selected 10000 sentences from the Japanese E D R corpus (EDR, 1996). The EDR Corl)us is a Japanese version of treebank with morphological, structural, and semantic information. In our experiments, we used only the structural information, that is, parse trees. Both the parse trees in our parser and the parse trees in the E D R Corpus are first converted into bunsetsu dependencies, and they are compared when calculating accuracy. Note that the internal structures of bunsetsus, e.8. structm&apos;es of compound no"
C98-2139,C96-2160,1,0.795733,"s. And grammar (e) using the combination of the three constraints still works with no side effect. We also measured average parsing time per sentence for the original grammar (a) and the fully augmented grammar (e). The parser we adopted is a naive CKY-style parser. Table 3 gives the average parsing time per sentence for those 2 grammars. Pseudo-principles and further constraints on LEs/LETs also make parsing more time-efficient. Even though they are sometimes considered to be slow in practical application because of their heavy feature structures, actually we found them to improve speed. In (Torisawa and Tsujii, 1996), an efficient HPSG parser is proposed, and our preliminary experiments show that the parsing time of the efficient parser is about three times shorter than that of the naive one. Thus, the average parsing time per sentence will be about 300 msec., and we believe our grammar will achive a practical speed. Other techniques to speed-up the parser are proposed in (Makino et al., 1998). 5 Average parsing time per sentence 1277 (msec) 838 (msec) Discussion This section focuses on the behavior of commas. Out of randomly selected 119 errors in experiment (e), 34 errors are considered to have been cau"
C98-2139,C98-2128,1,\N,Missing
C98-2154,A88-1010,0,0.0160053,"Missing"
C98-2154,P98-2132,1,0.905963,"PS-RFTF96P00502). 968 J u n &apos; i c h i t* / ] Figure 1: Agent-based System with the PSTFS ing or semantic processing, are divided into several pieces which can be simultaneously computed by several agents. Several parallel NLP systems have been developed previously. But most of them have been neither efficient nor practical enough (Adriaens and Hahn, 1994). On the other hand, our PSTFS provides the following features. • An efficient communication scheme for messages including Typed Feature Structures ( T F S s ) ( C a r p e n t e r , 1992). • Efficient treatment of TFSs by an abstract machine (Makino et al., 1998). Another possible way to develop parallel NLP systems with TFSs is to use a full concurrent logic programming language (Clark and Gregory, 1986; Ueda, 1985). However, we have observed that it is necessary to control parallelism in a flexible way to achieve high-performance. (Fixed concurrency in a logic programming language does not provide sufficient flexibility.) Our agent-based architecture is suitable for accomplishing such flexibility in parallelism. The next section discusses PSTFS from a programmers&apos; point of view. Section 3 describes the PSTFS architecture in detail. Section 4 describ"
C98-2154,P98-2144,1,\N,Missing
C98-2154,C98-2139,1,\N,Missing
C98-2154,C98-2128,1,\N,Missing
D07-1111,J93-1002,0,0.0200513,"stack, and an LR table is consulted to determine the next parser action. In our case, the parser state is encoded as a set of features derived from the contents of the stack S and queue Q, and the next parser action is determined according to that set of features. In the deterministic case described above, the procedure used for determining parser actions (a classifier, in our case) returns a single action. If, instead, this procedure returns a list of several possible actions with corresponding probabilities, we can then parse with a model similar to the probabilistic LR models described by Briscoe and Carroll (1993), where the probability of a parse tree is the product of the probabilities of each of the actions taken in its derivation. To find the most probable parse tree according to the probabilistic LR model, we use a best-first strategy. This involves an extension of the deterministic shift-reduce into a best-first shift-reduce algorithm. To describe this extension, we first introduce a new data structure Ti that represents a parser state, which includes a stack Si, a queue Qi, and a probability Pi. The deterministic algorithm is a special case of the probabilistic algorithm where we have a single p"
D07-1111,W06-2920,0,0.0818631,"ng tree voting scheme. In the domain adaptation track, we use two models to parse unlabeled data in the target domain to supplement the labeled out-ofdomain training set, in a scheme similar to one iteration of co-training. 1 2. We generalize the standard deterministic stepwise framework to probabilistic parsing, with the use of a best-first search strategy similar to the one employed in constituent parsing by Ratnaparkhi (1997) and later by Sagae and Lavie (2006); Introduction There are now several approaches for multilingual dependency parsing, as demonstrated in the CoNLL 2006 shared task (Buchholz and Marsi, 2006). The dependency parsing approach presented here extends the existing body of work mainly in four ways: 1. Although stepwise 1 dependency parsing has commonly been performed using parsing algo1 Stepwise parsing considers each step in a parsing algorithm separately, while all-pairs parsing considers entire 3. We provide additional evidence that the parser ensemble approach proposed by Sagae and Lavie (2006a) can be used to improve parsing accuracy, even when only a single parsing algorithm is used, as long as variation can be obtained, for example, by using different learning techniques or chan"
D07-1111,W07-2416,0,0.0384768,"Missing"
D07-1111,W03-1018,1,0.677446,"w is inserted into the heap H. Once new states have been inserted onto H for each of the n parser actions, we move on to the next iteration of the algorithm. 3 Multilingual Parsing Experiments For each of the ten languages for which training data was provided in the multilingual track of the CoNLL 2007 shared task, we trained three LR models as follows. The first LR model for each language uses maximum entropy classification (Berger et al., 1996) to determine possible parser actions and their probabilities4. To control overfitting in the MaxEnt models, we used box-type inequality constraints (Kazama and Tsujii, 2003). The second LR model for each language also uses MaxEnt classification, but parsing is performed backwards, which is accomplished simply by reversing the input string before parsing starts. Sagae and Lavie (2006a) and Zeman and Žabokrtský (2005) have observed that reversing the direction of stepwise parsers can be beneficial in parser combinations. The third model uses support vector machines 5 (Vapnik, 1995) using the polynomial 4 Implementation by Yoshimasa Tsuruoka, available at http://www-tsujii.is.s.u-tokyo.ac.jp/~tsuruoka/maxent/ 5 Implementation by Taku Kudo, available at http://chasen"
D07-1111,W04-3111,0,0.0185485,"Missing"
D07-1111,P05-1012,0,0.187099,"Missing"
D07-1111,W03-3017,0,0.0428782,"Missing"
D07-1111,P05-1013,0,0.0215164,"use a classifier with features derived from much of the same information contained in an LR table: the top few items on the stack, and the next few items of lookahead in the remaining input string. Additionally, following Sagae and Lavie (2006), we extend the basic deterministic LR algorithm with a bestfirst search, which results in a parsing strategy similar to generalized LR parsing (Tomita, 1987; 1990), except that we do not perform Tomita’s stack-merging operations. The resulting algorithm is projective, and nonprojectivity is handled by pseudo-projective transformations as described in (Nivre and Nilsson, 2005). We use Nivre and Nilsson’s PATH scheme2. For clarity, we first describe the basic variant of the LR algorithm for dependency parsing, which is a deterministic stepwise algorithm. We then show how we extend the deterministic parser into a bestfirst probabilistic parser. 2.1 Dependency Parsing with a Data-Driven Variant of the LR Algorithm The two main data structures in the algorithm are a stack S and a queue Q. S holds subtrees of the final dependency tree for an input sentence, and Q holds the words in an input sentence. S is initialized to be empty, and Q is initialized to hold every word"
D07-1111,W97-0301,0,0.0476573,"different learners. In the multilingual track, we train three LR models for each of the ten languages, and combine the analyses obtained with each individual model with a maximum spanning tree voting scheme. In the domain adaptation track, we use two models to parse unlabeled data in the target domain to supplement the labeled out-ofdomain training set, in a scheme similar to one iteration of co-training. 1 2. We generalize the standard deterministic stepwise framework to probabilistic parsing, with the use of a best-first search strategy similar to the one employed in constituent parsing by Ratnaparkhi (1997) and later by Sagae and Lavie (2006); Introduction There are now several approaches for multilingual dependency parsing, as demonstrated in the CoNLL 2006 shared task (Buchholz and Marsi, 2006). The dependency parsing approach presented here extends the existing body of work mainly in four ways: 1. Although stepwise 1 dependency parsing has commonly been performed using parsing algo1 Stepwise parsing considers each step in a parsing algorithm separately, while all-pairs parsing considers entire 3. We provide additional evidence that the parser ensemble approach proposed by Sagae and Lavie (200"
D07-1111,N06-2033,1,0.74441,"ilingual track, we train three LR models for each of the ten languages, and combine the analyses obtained with each individual model with a maximum spanning tree voting scheme. In the domain adaptation track, we use two models to parse unlabeled data in the target domain to supplement the labeled out-ofdomain training set, in a scheme similar to one iteration of co-training. 1 2. We generalize the standard deterministic stepwise framework to probabilistic parsing, with the use of a best-first search strategy similar to the one employed in constituent parsing by Ratnaparkhi (1997) and later by Sagae and Lavie (2006); Introduction There are now several approaches for multilingual dependency parsing, as demonstrated in the CoNLL 2006 shared task (Buchholz and Marsi, 2006). The dependency parsing approach presented here extends the existing body of work mainly in four ways: 1. Although stepwise 1 dependency parsing has commonly been performed using parsing algo1 Stepwise parsing considers each step in a parsing algorithm separately, while all-pairs parsing considers entire 3. We provide additional evidence that the parser ensemble approach proposed by Sagae and Lavie (2006a) can be used to improve parsing a"
D07-1111,J87-1004,0,0.10801,"ous in parser 1045 ensembles. The main difference between our parser and a traditional LR parser is that we do not use an LR table derived from an explicit grammar to determine shift/reduce actions. Instead, we use a classifier with features derived from much of the same information contained in an LR table: the top few items on the stack, and the next few items of lookahead in the remaining input string. Additionally, following Sagae and Lavie (2006), we extend the basic deterministic LR algorithm with a bestfirst search, which results in a parsing strategy similar to generalized LR parsing (Tomita, 1987; 1990), except that we do not perform Tomita’s stack-merging operations. The resulting algorithm is projective, and nonprojectivity is handled by pseudo-projective transformations as described in (Nivre and Nilsson, 2005). We use Nivre and Nilsson’s PATH scheme2. For clarity, we first describe the basic variant of the LR algorithm for dependency parsing, which is a deterministic stepwise algorithm. We then show how we extend the deterministic parser into a bestfirst probabilistic parser. 2.1 Dependency Parsing with a Data-Driven Variant of the LR Algorithm The two main data structures in the"
D07-1111,W03-3023,0,0.0851389,"Missing"
D07-1111,W05-1518,0,0.066031,"ata was provided in the multilingual track of the CoNLL 2007 shared task, we trained three LR models as follows. The first LR model for each language uses maximum entropy classification (Berger et al., 1996) to determine possible parser actions and their probabilities4. To control overfitting in the MaxEnt models, we used box-type inequality constraints (Kazama and Tsujii, 2003). The second LR model for each language also uses MaxEnt classification, but parsing is performed backwards, which is accomplished simply by reversing the input string before parsing starts. Sagae and Lavie (2006a) and Zeman and Žabokrtský (2005) have observed that reversing the direction of stepwise parsers can be beneficial in parser combinations. The third model uses support vector machines 5 (Vapnik, 1995) using the polynomial 4 Implementation by Yoshimasa Tsuruoka, available at http://www-tsujii.is.s.u-tokyo.ac.jp/~tsuruoka/maxent/ 5 Implementation by Taku Kudo, available at http://chasen.org/~taku/software/TinySVM/ and all vs. all was used for multi-class classification. kernel with degree 2. Probabilities were estimated for SVM outputs using the method described in (Platt, 1999), but accuracy improvements were not observed duri"
D07-1111,J93-2004,0,\N,Missing
D07-1111,J96-1002,0,\N,Missing
D07-1111,P06-2089,1,\N,Missing
D07-1111,D07-1096,0,\N,Missing
D07-1111,C90-1012,0,\N,Missing
D08-1047,H05-1120,0,0.606315,"p yoshimasa.tsuruoka@manchester.ac.uk Sophia Ananiadou‡ Jun’ichi Tsujii†‡ sophia.ananiadou@manchester.ac.uk tsujii@is.s.u-tokyo.ac.jp † Graduate School of Information Science and Technology University of Tokyo 7-3-1 Hongo, Bunkyo-ku Tokyo 113-8656, Japan ‡ School of Computer Science, University of Manchester National Centre for Text Mining (NaCTeM) Manchester Interdisciplinary Biocentre 131 Princess Street, Manchester M1 7DN, UK Abstract matching (Navarro, 2001), and duplicate record detection (Bilenko and Mooney, 2003). Recent studies have formalized the task in the discriminative framework (Ahmad and Kondrak, 2005; Li et al., 2006; Chen et al., 2007), String transformation, which maps a source string s into its desirable form t∗ , is related to various applications including stemming, lemmatization, and spelling correction. The essential and important step for string transformation is to generate candidates to which the given string s is likely to be transformed. This paper presents a discriminative approach for generating candidate strings. We use substring substitution rules as features and score them using an L1 -regularized logistic regression model. We also propose a procedure to generate negative"
D08-1047,I08-1007,0,0.0973065,"native framework of string similarity. MaCallum et al. (2005) proposed a method to train the costs of edit operations using Conditional Random Fields (CRFs). Bergsma and Kondrak (2007) correct comparative and superlative adjectives, e.g., unpopular → unpopularer → unpopularest and refundable → refundabler → refundablest. Therefore, we removed inflection entries for comparative and superlative adjectives from the dataset. presented an alignment-based discriminative string similarity. They extracted features from substring pairs that are consistent to a character-based alignment of two strings. Aramaki et al. (2008) also used features that express the different segments of the two strings. However, these studies are not suited for a candidate generator because the processes of string transformations are intractable in their discriminative models. Dalianis and Jongejan (2006) presented a lemmatiser based on suffix rules. Although they proposed a method to obtain suffix rules from a training data, the method did not use counter-examples (negatives) for reducing incorrect string transformations. Tsuruoka et al. (2008) proposed a scoring method for discovering a list of normalization rules for dictionary loo"
D08-1047,J96-1002,0,0.0553682,"be enumerated by an efficient algorithm because the processes of string transformation are tractable in the model. We demonstrate the remarkable performance of the proposed method in normalizing inflected words and spelling variations. 1 t∗ = argmax P (t|s). (1) t∈gen(s) Here, the candidate generator gen(s) enumerates candidates of destination (correct) strings, and the scorer P (t|s) denotes the conditional probability of the string t for the given s. The scorer was modeled by a noisy-channel model (Shannon, 1948; Brill and Moore, 2000; Ahmad and Kondrak, 2005) and maximum entropy framework (Berger et al., 1996; Li et al., 2006; Chen et al., 2007). The candidate generator gen(s) also affects the accuracy of the string transformation. Previous studies of spelling correction mostly defined gen(s), gen(s) = {t |dist(s, t) &lt; δ}. Introduction String transformation maps a source string s into its destination string t∗ . In the broad sense, string transformation can include labeling tasks such as partof-speech tagging and shallow parsing (Brill, 1995). However, this study addresses string transformation in its narrow sense, in which a part of a source string is rewritten with a substring. Typical applicati"
D08-1047,P07-1083,0,0.0802925,"ormance of the L1 regularized logistic regression as a discriminative model, we also built two classifiers based on the Support Vector Machine (SVM). These SVM classifiers were implemented by the SVMperf 7 on a linear kernel8 . An SVM classifier employs the same feature set (substitution rules) as the proposed method so that we can directly compare the L1 regularized logistic regression and the linear-kernel SVM. Another SVM classifier incorporates the five string metrics; this system can be considered as our reproduction of the discriminative string similarity proposed by Bergsma and Kondrak (2007). Table 3 reports the precision (P), recall (R), and F1 score (F1) based on the number of correct decisions for positive instances. The proposed method outperformed the baseline systems, achieving 0.919, 0.888, and 0.984 of F1 scores, respectively. Porter’s stemmer worked on the Inflection set, but not on the Orthography set, which is beyond the scope of the stemming algorithms. CST’s lemmatizer suffered from low recall on the Inflection set because it removed suffixes of base forms, e.g., (cloning, clone) → (clone, clo). Morpha and CST’s lemma6 We used CST’s lemmatiser version 2.13: http://ww"
D08-1047,P00-1037,0,0.103699,"ary of the model. The advantage of this approach is that candidate strings can be enumerated by an efficient algorithm because the processes of string transformation are tractable in the model. We demonstrate the remarkable performance of the proposed method in normalizing inflected words and spelling variations. 1 t∗ = argmax P (t|s). (1) t∈gen(s) Here, the candidate generator gen(s) enumerates candidates of destination (correct) strings, and the scorer P (t|s) denotes the conditional probability of the string t for the given s. The scorer was modeled by a noisy-channel model (Shannon, 1948; Brill and Moore, 2000; Ahmad and Kondrak, 2005) and maximum entropy framework (Berger et al., 1996; Li et al., 2006; Chen et al., 2007). The candidate generator gen(s) also affects the accuracy of the string transformation. Previous studies of spelling correction mostly defined gen(s), gen(s) = {t |dist(s, t) &lt; δ}. Introduction String transformation maps a source string s into its destination string t∗ . In the broad sense, string transformation can include labeling tasks such as partof-speech tagging and shallow parsing (Brill, 1995). However, this study addresses string transformation in its narrow sense, in whi"
D08-1047,J95-4004,0,0.0169888,"iven s. The scorer was modeled by a noisy-channel model (Shannon, 1948; Brill and Moore, 2000; Ahmad and Kondrak, 2005) and maximum entropy framework (Berger et al., 1996; Li et al., 2006; Chen et al., 2007). The candidate generator gen(s) also affects the accuracy of the string transformation. Previous studies of spelling correction mostly defined gen(s), gen(s) = {t |dist(s, t) &lt; δ}. Introduction String transformation maps a source string s into its destination string t∗ . In the broad sense, string transformation can include labeling tasks such as partof-speech tagging and shallow parsing (Brill, 1995). However, this study addresses string transformation in its narrow sense, in which a part of a source string is rewritten with a substring. Typical applications of this task include stemming, lemmatization, spelling correction (Brill and Moore, 2000; Wilbur et al., 2006; Carlson and Fette, 2007), OCR error correction (Kolak and Resnik, 2002), approximate string (2) Here, the function dist(s, t) denotes the weighted Levenshtein distance (Levenshtein, 1966) between strings s and t. Furthermore, the threshold δ requires the distance between the source string s and a candidate string t to be less"
D08-1047,D07-1019,0,0.710798,"ia Ananiadou‡ Jun’ichi Tsujii†‡ sophia.ananiadou@manchester.ac.uk tsujii@is.s.u-tokyo.ac.jp † Graduate School of Information Science and Technology University of Tokyo 7-3-1 Hongo, Bunkyo-ku Tokyo 113-8656, Japan ‡ School of Computer Science, University of Manchester National Centre for Text Mining (NaCTeM) Manchester Interdisciplinary Biocentre 131 Princess Street, Manchester M1 7DN, UK Abstract matching (Navarro, 2001), and duplicate record detection (Bilenko and Mooney, 2003). Recent studies have formalized the task in the discriminative framework (Ahmad and Kondrak, 2005; Li et al., 2006; Chen et al., 2007), String transformation, which maps a source string s into its desirable form t∗ , is related to various applications including stemming, lemmatization, and spelling correction. The essential and important step for string transformation is to generate candidates to which the given string s is likely to be transformed. This paper presents a discriminative approach for generating candidate strings. We use substring substitution rules as features and score them using an L1 -regularized logistic regression model. We also propose a procedure to generate negative instances that affect the decision b"
D08-1047,dalianis-jongejan-2006-hand,0,0.022719,"larer → unpopularest and refundable → refundabler → refundablest. Therefore, we removed inflection entries for comparative and superlative adjectives from the dataset. presented an alignment-based discriminative string similarity. They extracted features from substring pairs that are consistent to a character-based alignment of two strings. Aramaki et al. (2008) also used features that express the different segments of the two strings. However, these studies are not suited for a candidate generator because the processes of string transformations are intractable in their discriminative models. Dalianis and Jongejan (2006) presented a lemmatiser based on suffix rules. Although they proposed a method to obtain suffix rules from a training data, the method did not use counter-examples (negatives) for reducing incorrect string transformations. Tsuruoka et al. (2008) proposed a scoring method for discovering a list of normalization rules for dictionary look-ups. However, their objective was to transform given strings, so that strings (e.g., studies and study) referring to the same concept in the dictionary are mapped into the same string (e.g., stud); in contrast, this study maps strings into their destination stri"
D08-1047,2005.mtsummit-papers.40,0,0.165856,"a.tsuruoka@manchester.ac.uk Sophia Ananiadou‡ Jun’ichi Tsujii†‡ sophia.ananiadou@manchester.ac.uk tsujii@is.s.u-tokyo.ac.jp † Graduate School of Information Science and Technology University of Tokyo 7-3-1 Hongo, Bunkyo-ku Tokyo 113-8656, Japan ‡ School of Computer Science, University of Manchester National Centre for Text Mining (NaCTeM) Manchester Interdisciplinary Biocentre 131 Princess Street, Manchester M1 7DN, UK Abstract matching (Navarro, 2001), and duplicate record detection (Bilenko and Mooney, 2003). Recent studies have formalized the task in the discriminative framework (Ahmad and Kondrak, 2005; Li et al., 2006; Chen et al., 2007), String transformation, which maps a source string s into its desirable form t∗ , is related to various applications including stemming, lemmatization, and spelling correction. The essential and important step for string transformation is to generate candidates to which the given string s is likely to be transformed. This paper presents a discriminative approach for generating candidate strings. We use substring substitution rules as features and score them using an L1 -regularized logistic regression model. We also propose a procedure to generate negative"
D08-1047,P06-1129,0,0.339768,"nd refundable → refundabler → refundablest. Therefore, we removed inflection entries for comparative and superlative adjectives from the dataset. presented an alignment-based discriminative string similarity. They extracted features from substring pairs that are consistent to a character-based alignment of two strings. Aramaki et al. (2008) also used features that express the different segments of the two strings. However, these studies are not suited for a candidate generator because the processes of string transformations are intractable in their discriminative models. Dalianis and Jongejan (2006) presented a lemmatiser based on suffix rules. Although they proposed a method to obtain suffix rules from a training data, the method did not use counter-examples (negatives) for reducing incorrect string transformations. Tsuruoka et al. (2008) proposed a scoring method for discovering a list of normalization rules for dictionary look-ups. However, their objective was to transform given strings, so that strings (e.g., studies and study) referring to the same concept in the dictionary are mapped into the same string (e.g., stud); in contrast, this study maps strings into their destination stri"
D08-1047,J99-1003,0,0.038867,"f the tasks: classification (Section 3.2) and normalization (Section 3.3). 3.2 Experiment 1: Candidate classification In this experiment, we measured the performance of the classification task in which pairs of strings were assigned with positive or negative labels. We trained and evaluated the proposed method by performing ten-fold cross validation on each dataset5 . Eight baseline systems were prepared for comparison: Levenshtein distance (LD), normalized Levenshtein distance (NLD), Dice coefficient on letter bigrams (DICE) (Adamson and Boreham, 1974), Longest Common Substring Ratio (LCSR) (Melamed, 1999), Longest Common Prefix Ratio (PREFIX) (Kondrak, 2005), Porter’s stemmer (Porter, 1980), Morpha (Minnen et al., 2001), and CST’s lemmatiser (Dalianis and Jonge3 LRSPL table includes trivial spelling variants that can be handled using simple character/string operations. For example, the table contains spelling variants related to case sensitivity (e.g., deg and Deg) and symbols (e.g., Feb and Feb.). 4 LRAGR table also provides agreement information even when word forms do not change. For example, the table contains an entry indicating that the first-singular present form of the verb study is st"
D08-1047,P99-1004,0,\N,Missing
D09-1013,D07-1024,0,0.440421,"ume that the feature space is same, and that the labels may be different in only some examples, while most of DA methods assume that the labels are the same, and that the feature space is different. Among the methods, we use adaptive SVM (aSVM) (Yang et al., 2007), singular value decomposition (SVD) based alternating structure optimization (SVDASO) (Ando et al., 2005), and transfer AdaBoost (TrAdaBoost) (Dai et al., 2007) to compare with SVM-CW. We do not use semi-supervised learning (SSL) methods, because it would be considerably costly to generate enough clean unlabeled data needed for SSL (Erkan et al., 2007). aSVM is seen as a promising DA method among several modifications of SVM including SVM-CW. aSVM tries to find a model that is close to the one made from other classification problems. SVDASO is one of the most successful SSL, DA, or multi-task learning methods in NLP. The method tries to find an additional useful feature space by solving auxiliary problems that are close to the target problem. With well-designed auxiliary problems, the method has been applied to text classification, text chunking, and word sense disambiguation (Ando, 2006). The method was reported to perform better than or c"
D09-1013,W06-2911,0,0.0115377,"enough clean unlabeled data needed for SSL (Erkan et al., 2007). aSVM is seen as a promising DA method among several modifications of SVM including SVM-CW. aSVM tries to find a model that is close to the one made from other classification problems. SVDASO is one of the most successful SSL, DA, or multi-task learning methods in NLP. The method tries to find an additional useful feature space by solving auxiliary problems that are close to the target problem. With well-designed auxiliary problems, the method has been applied to text classification, text chunking, and word sense disambiguation (Ando, 2006). The method was reported to perform better than or comparable to the best state-of-the-art systems in all of these tasks. TrAdaBoost was proposed as an ITL method. In training, the method reduces the effect of incompatible examples by decreasing their weights, and thereby tries to use useful examples from source corpora. The method has been applied to text classification, and the reported performance was better than SVM and transductive SVM (Dai et al., 2007). Related Works While sentence-based, pair-wise PPI extraction was initially tackled by using simple methods based on co-occurrences, la"
D09-1013,P08-1006,1,0.760868,"es (PAS) from Enju, and by using the dependency trees from KSDEP. 3.1.1 3.1 Feature Vector Bag-of-Words (BOW) Features The BOW feature includes the lemma form of a word, its relative position to the target pair of proteins (Before, Middle, After), and its frequency in the target sentence. BOW features form the BOW kernel in the original kernel method. BOW features for the pair in Figure 2 are shown in Figure 4. We propose a feature vector with three types of features, corresponding to the three different kernels, which were each combined with the two parsers: the Enju 2.3.0, and KSDEP beta 1 (Miyao et al., 2008); this feature vector is used because the kernels with these parsers were shown to be effective for PPI extraction by Miwa et al. (2008), and because it is important to start from a good performance single corpus system. Both parsers were retrained using the GENIA Treebank corpus provided by Kim et al. (2003). By using our linear feature vector, we can perform calculations faster by using fast linear classifiers like L2-SVM, and we also obtain a more accurate extraction, than by using the original kernel method. Figure 3 summarizes the way in which the feature vector is constructed. The system"
D09-1013,W02-1001,0,\N,Missing
D09-1121,D07-1112,0,0.246361,"Missing"
D09-1121,gimenez-marquez-2008-towards,0,0.113426,"Missing"
D09-1121,H94-1020,0,0.03626,": three wrong outputs for “ARG2” of “on” (Error 1) and “ARG1” of “our” (Error 2) and “work” (Error 3), excess relation “ARG1” of “force” (Error 4), and missing relation “ARG1” for “today” (Error 5). By correcting each of the errors 1, 2, 3 and 4, all of these errors are corrected together, and therefore classified into the same cooccurring error group. Although error 5 cannot participate in the group, correcting error 5 can correct all of the errors in the group, and therefore an We applied our approaches to parsing errors given by the HPSG parser Enju, which was trained on the Penn Treebank (Marcus et al., 1994) section 2-21. We first examined each approach, and then explored the combination of the approaches. 4.1 Evaluation of descriptive approach We examined our descriptive approach. We first parsed sentences in the Penn Treebank section 22 with Enju, and then observed the errors. Based on the observation, we next described the patterns as shown in Section 3. After that, we parsed section 0 and then applied the patterns to the errors. Table 3 summarizes the extracted errors. As the table shows, with the 14 error patterns, we successfully matched 1,671 locations in error outputs and covered 2,078 of"
D09-1121,D07-1013,0,0.105428,"ptured as parameters for model training, or policies for re-ranking the parse candidates. The combination of our approaches would give us interesting clues for planning effective strategies for improving the parser. Our challenges for combining the two approaches are now in the preliminary stage and there would be many possibilities for further detailed analysis. 5 Related work Although there have been many researches which analyzed errors on their own systems in the part of the experiments, there have been few researches which focused mainly on error analysis itself. In the field of parsing, McDonald and Nivre (2007) compared parsing errors between graphbased and transition-based parsers. They observed the accuracy transitions from various points of view, and the obtained statistical data suggested that error propagation seemed to occur in the graph structures of parsing outputs. Our research proceeded for one step in this point, and attempted to reveal the way of the propagations. In examining the combination of the two types of parsing, McDonald and Nivre (2007) utilized similar approaches to our empirical analysis. They allowed a parser to give only structures given by the parsers. They implemented the"
D09-1121,P05-1011,1,0.933823,"ous combinations of error patterns as organized error phenomena on the basis of linguistic knowledge, and then extract such combinations from given errors. In our empirical approach, on the other and, we re-parse a sentence under the condition where a target error is corrected, and errors which are additionally corrected are regarded as dependent errors. By capturing dependencies among parsing errors through systematic approaches, we can effectively collect errors which are related to the same linguistic properties. In the experiments, we applied both of our approaches to an HPSG parser Enju (Miyao and Tsujii, 2005; Ninomiya et al., 2006), and then evaluated the obtained error classes. After examining the individual approaches, we explored the combination of them. 2 Parser and its evaluation A parser is a system which interprets structures of given sentences from some grammatical or in some cases semantical viewpoints, and interpreted structures are utilized as essential information for various natural language tasks such as information extraction, machine translation, and so on. In most cases, an output structure of a parser is based on a certain grammatical framework such as CFG, CCG (Steedman, 2000),"
D09-1121,W06-1619,1,0.906675,"Missing"
D09-1138,C08-1002,0,0.0336512,"Missing"
D09-1138,P98-1013,0,0.0632726,"ailability of a lexicon, such as in word sense disambiguation (WSD) (McCarthy et al., 2004), and in token-level verb class disambiguation (Lapata and Brew, 2004; Girju et Jun’ichi Tsujii University of Tokyo University of Manchester National Center for Text Mining Hongo 7-3-1, Bunkyo-ku, Tokyo, Japan tsujii@is.s.u-tokyo.ac.jp al., 2005; Li and Brew, 2007; Abend et al., 2008). In other words, those methods are heavily dependent on the availability of a semantic lexicon. Therefore, recent research efforts have invested in developing semantic resources, such as WordNet (Fellbaum, 1998), FrameNet (Baker et al., 1998), and VerbNet (Kipper et al., 2000; Kipper-Schuler, 2005), which greatly advanced research in semantic processing. However, the construction of such resources is expensive, and it is unrealistic to presuppose the availability of full-coverage lexicons; this is the case because unknown words always appear in real texts, and word-semantics associations may vary (Abend et al., 2008). This paper explores a method for the supervised learning of a probabilistic model for the VerbNet lexicon. We target the automatic classification of arbitrary verbs, including polysemous verbs, into all VerbNet class"
D09-1138,C96-1055,0,0.105412,"Missing"
D09-1138,E03-1040,0,0.0250868,"resenting the statistics of syntactic frames, are extracted from the unannotated corpora. Additionally, as the classes represent semantic commonalities, semantically inspired features, like distributionally similar words, are used. These features can be considered as a generalized representation of verbs, and we expect that the obtained probabilistic model predicts VerbNet classes of the unknown words. Our model is evaluated in two tasks of typelevel verb classification: one is the classification of monosemous verbs into a small subset of the classes, which was studied in some previous works (Joanis and Stevenson, 2003; Joanis et al., 2008). The other task is the classification of all verbs into the full set of VerbNet classes, which has not yet 1328 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1328–1337, c Singapore, 6-7 August 2009. 2009 ACL and AFNLP been attempted. In the experiments, training instances are obtained from VerbNet and/or SemLink (Loper et al., 2007), while features are extracted from the British National Corpus or from Wall Street Journal. We empirically compare several settings for model learning by varying the set of features, the source"
D09-1138,kipper-etal-2006-extending,0,0.0299393,"Missing"
D09-1138,W04-2606,0,0.0592579,"Missing"
D09-1138,P03-1009,0,0.337213,"Missing"
D09-1138,W02-0907,0,0.0231614,"Missing"
D09-1138,J04-1003,0,0.214269,"Missing"
D09-1138,P98-2127,0,0.219608,"Missing"
D09-1138,P04-1036,0,0.052023,"Missing"
D09-1138,J01-3003,0,0.031791,"om the British National Corpus or from Wall Street Journal. We empirically compare several settings for model learning by varying the set of features, the source domain and the size of a corpus for feature extraction, and the use of the token-level statistics obtained from a manually disambiguated corpus. We also provide the analysis of the remaining errors, which will lead us to further improve the supervised learning of a probabilistic semantic lexicon. Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008). However, their focus has been limited to a small subset of verb classes, and a limited number of monosemous verbs. The main contributions of the present work are: i) to provide empirical results for the automatic classification of all verbs, including polysemous ones, into all VerbNet classes, and ii) to empirically explore the effective settings for the supervised learning of a probabilistic lexicon of verb semantic classes. 2 2.1 43 Emission 43.1 Light Emission beam, glow, sparkle, . . . 43.2 Sound Emission blare"
D09-1138,J08-1002,1,0.86468,"Missing"
D09-1138,J05-1004,0,0.167846,"Missing"
D09-1138,C00-2108,0,0.0699652,"Missing"
D09-1138,E03-1037,0,0.0471195,"Missing"
D09-1138,W03-0410,0,0.247258,"pus or from Wall Street Journal. We empirically compare several settings for model learning by varying the set of features, the source domain and the size of a corpus for feature extraction, and the use of the token-level statistics obtained from a manually disambiguated corpus. We also provide the analysis of the remaining errors, which will lead us to further improve the supervised learning of a probabilistic semantic lexicon. Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008). However, their focus has been limited to a small subset of verb classes, and a limited number of monosemous verbs. The main contributions of the present work are: i) to provide empirical results for the automatic classification of all verbs, including polysemous ones, into all VerbNet classes, and ii) to empirically explore the effective settings for the supervised learning of a probabilistic lexicon of verb semantic classes. 2 2.1 43 Emission 43.1 Light Emission beam, glow, sparkle, . . . 43.2 Sound Emission blare, chime, jangle, . . . ... 4"
D09-1138,E99-1007,0,0.0172543,"e features are extracted from the British National Corpus or from Wall Street Journal. We empirically compare several settings for model learning by varying the set of features, the source domain and the size of a corpus for feature extraction, and the use of the token-level statistics obtained from a manually disambiguated corpus. We also provide the analysis of the remaining errors, which will lead us to further improve the supervised learning of a probabilistic semantic lexicon. Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008). However, their focus has been limited to a small subset of verb classes, and a limited number of monosemous verbs. The main contributions of the present work are: i) to provide empirical results for the automatic classification of all verbs, including polysemous ones, into all VerbNet classes, and ii) to empirically explore the effective settings for the supervised learning of a probabilistic lexicon of verb semantic classes. 2 2.1 43 Emission 43.1 Light Emission beam, glow, sparkle, . ."
D09-1138,W99-0503,0,0.00941077,"oper et al., 2007), while features are extracted from the British National Corpus or from Wall Street Journal. We empirically compare several settings for model learning by varying the set of features, the source domain and the size of a corpus for feature extraction, and the use of the token-level statistics obtained from a manually disambiguated corpus. We also provide the analysis of the remaining errors, which will lead us to further improve the supervised learning of a probabilistic semantic lexicon. Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008). However, their focus has been limited to a small subset of verb classes, and a limited number of monosemous verbs. The main contributions of the present work are: i) to provide empirical results for the automatic classification of all verbs, including polysemous ones, into all VerbNet classes, and ii) to empirically explore the effective settings for the supervised learning of a probabilistic lexicon of verb semantic classes. 2 2.1 43 Emission 43.1 Light Emissio"
D09-1138,J93-2004,0,\N,Missing
D09-1138,C98-1013,0,\N,Missing
D09-1138,C98-2122,0,\N,Missing
D09-1157,W04-3204,0,0.0232618,"ython, users may like to see the results grouped into the following categories: a type of snake, a programming language, or a film (Bunescu and Pas¸ca, 2006). One approach to such lexical disambiguation tasks is supervised classification. However, such techniques suffer from the knowledge acquisition bottleneck, meaning that manually annotating training data is costly and can never satisfy the need by the machine learning algorithms. In addition, supervised techniques may not yield reliable results when the distributions of the semantic classes are different in the training and test datasets (Agirre and Martinez, 2004; Koeling et al., 2005). For example, on the task of word sense disambiguation, a model trained on a dataset where the predominant sense of the word star is “heavenly body”, may not work well on text mainly composed of entertainment news. Such problems are also major concerns when developing a system to disambiguate biomedical named entities (e.g., protein, 1513 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1513–1522, c Singapore, 6-7 August 2009. 2009 ACL and AFNLP gene, and disease), for which some researchers rely on hand-crafted rules in addi"
D09-1157,W08-0601,0,0.0270035,"Missing"
D09-1157,P06-4020,0,0.0248306,"OS tags and punctuation labels that were derived from the CLAWS-7 tagset,10 whereas our dataset uses POS labels from the Penn Treebank tagset (Marcus et al., 1994). As RASP does not recognise the Penn tagset, we used its build-in POS tagger. Minipar, on the other hand, does not support input of tokenised or POStagged text, and therefore took split sentences as input. Secondly, the output representations of the parsers are different and we preferred a format that depicts relations between words instead of syntactic constituents. In total, 4 representations were used: grammatical relation (GR) (Briscoe et al., 2006), Stanford typed dependency (SD) (de Marneffe et al., 2006), Minipar’s own representation (Lin, 1998), and ENJU’s predicate-argument structure (PAS). All the above representations define relations of words in triples, where a dependency triple (i.e., GR, SD and Minipar) consists of head, dependent and relation, and a PAS triple contains predicate, argument, and relation. Figure 1 shows a sentence parsed by ENJU in PAS representation. The right-most column in Table 1 lists the output representation of each parser. A syntactic path between an entity and a species word was represented by a sequen"
D09-1157,E06-1002,0,0.0994251,"Missing"
D09-1157,J07-4004,0,0.0353843,"Missing"
D09-1157,de-marneffe-etal-2006-generating,0,0.0109684,"Missing"
D09-1157,D07-1024,0,0.0604273,"Missing"
D09-1157,W07-2202,1,0.896692,"Missing"
D09-1157,E06-1015,0,0.0312211,"niversity of Manchester, UK ‡ National Centre for Text Mining, UK ∗ Department of Computer Science, University of Tokyo, Japan {xinglong.wang,j.tsujii,sophia.ananiadou}@manchester.ac.uk Abstract exploited by similarity measures or machine learning algorithms. For example, Erkan et al. (2007) used the shortest path between two genes according to edit distance in a dependency tree to define a kernel function for extracting gene interactions. Miwa et al. (2008) comparably evaluated a number of kernels for incorporating syntactic features, including the bag-of-word kernel, the subset tree kernel (Moschitti, 2006) and the graph kernel (Airola et al., 2008), and they concluded that combining all kernels achieved better results than using any individual one. Miyao et al. (2008) used syntactic paths as one of the features to train a support vector machines (SVM) model for PPIs and also discussed how different parsers and output representations affected the end results. Named entity disambiguation concerns linking a potentially ambiguous mention of named entity in text to an unambiguous identifier in a standard database. One approach to this task is supervised classification. However, the availability of t"
D09-1157,I05-2038,1,0.746845,"ed in Table 1): • Dependency parsers identify one word as the head of a sentence and all other words are either a dependent of that word, or else dependent on some other word that connects to the headword through a sequence of dependencies. We used Minipar (Lin, 1998) and RASP (Briscoe et al., 2006) for the experiments; • Constituent-structured parsers split a sentence into syntactic constituents such as noun phrases or verb phrases. We used the Stanford parser (Klein and Manning, 2003), and also a variant of the Stanford parser (i.e., Stanford-Genia), which was trained on the GENIA treebank (Tateisi et al., 2005) for biomedical text; • Deep parsers aim to compute in-depth syntactic and semantic structures based on syntactic theories such as HPSG (Pollard and Sag, 1994) and CCG (Steedman, 2000). We used the C&C parser (Clark and Curran, 2007), ENJU (Miyao and Tsujii, 2008), and a variant of ENJU (Hara et al., 2007) adapted for the biomedical domain (i.e., ENJU-Genia); Figure 2: A syntactic feature obtained from the ENJU parser. 4.3.2 Syntactic Features Given a sentence, a natural language parser automatically recognises its syntactic structure and outputs a parse tree, in which nodes represent words or"
D09-1157,wang-grover-2008-learning,1,0.829921,"ue words for species are words denoting names of model organisms (e.g., mouse as in 1 http://www.ncbi.nlm.nih.gov/RefSeq http://www.ncbi.nlm.nih.gov/sites/ entrez?db=taxonomy 2 phrase “mouse p53”). Another clue is the presence of the species-indicating prefixes in gene and protein names. For instance, prefix ‘h’ in entity “hSos-1” suggests that it is a human protein. Throughout this paper, we refer to such cue words (e.g., mouse, hSos-1) as “species words”. Note that a species “word” may contain multiple tokens (e.g., E. Coli). We encoded this knowledge in a rule-based species tagging system (Wang and Grover, 2008). The system takes a 2-step approach. First, it marks up species words in the document using a speciesword detection program,3 which searches every word in a dictionary of model organisms and assigns a species ID to the word if a match is found. The dictionary was built using the NCBI taxonomy4 and the UniProt controlled vocabulary of species,5 and in total it contains 420,224 species words for 324,157 species IDs. When species words are identified, we disambiguate an entity mention using one of the following rules: 1. previous species word: If the word preceding an entity is a species word, a"
D09-1157,C00-2137,0,0.0505769,"Missing"
D09-1157,P03-1054,0,0.00368557,"ul to infer biological relations (e.g., Airola et al., 2008; Miwa et al., 2008). We experimented with the following parsers (summarised in Table 1): • Dependency parsers identify one word as the head of a sentence and all other words are either a dependent of that word, or else dependent on some other word that connects to the headword through a sequence of dependencies. We used Minipar (Lin, 1998) and RASP (Briscoe et al., 2006) for the experiments; • Constituent-structured parsers split a sentence into syntactic constituents such as noun phrases or verb phrases. We used the Stanford parser (Klein and Manning, 2003), and also a variant of the Stanford parser (i.e., Stanford-Genia), which was trained on the GENIA treebank (Tateisi et al., 2005) for biomedical text; • Deep parsers aim to compute in-depth syntactic and semantic structures based on syntactic theories such as HPSG (Pollard and Sag, 1994) and CCG (Steedman, 2000). We used the C&C parser (Clark and Curran, 2007), ENJU (Miyao and Tsujii, 2008), and a variant of ENJU (Hara et al., 2007) adapted for the biomedical domain (i.e., ENJU-Genia); Figure 2: A syntactic feature obtained from the ENJU parser. 4.3.2 Syntactic Features Given a sentence, a na"
D09-1157,H05-1053,0,0.0316519,"e the results grouped into the following categories: a type of snake, a programming language, or a film (Bunescu and Pas¸ca, 2006). One approach to such lexical disambiguation tasks is supervised classification. However, such techniques suffer from the knowledge acquisition bottleneck, meaning that manually annotating training data is costly and can never satisfy the need by the machine learning algorithms. In addition, supervised techniques may not yield reliable results when the distributions of the semantic classes are different in the training and test datasets (Agirre and Martinez, 2004; Koeling et al., 2005). For example, on the task of word sense disambiguation, a model trained on a dataset where the predominant sense of the word star is “heavenly body”, may not work well on text mainly composed of entertainment news. Such problems are also major concerns when developing a system to disambiguate biomedical named entities (e.g., protein, 1513 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1513–1522, c Singapore, 6-7 August 2009. 2009 ACL and AFNLP gene, and disease), for which some researchers rely on hand-crafted rules in addition to a small amount"
D09-1157,J08-1002,1,0.82825,"Missing"
D09-1157,P08-1006,1,0.821202,"iadou}@manchester.ac.uk Abstract exploited by similarity measures or machine learning algorithms. For example, Erkan et al. (2007) used the shortest path between two genes according to edit distance in a dependency tree to define a kernel function for extracting gene interactions. Miwa et al. (2008) comparably evaluated a number of kernels for incorporating syntactic features, including the bag-of-word kernel, the subset tree kernel (Moschitti, 2006) and the graph kernel (Airola et al., 2008), and they concluded that combining all kernels achieved better results than using any individual one. Miyao et al. (2008) used syntactic paths as one of the features to train a support vector machines (SVM) model for PPIs and also discussed how different parsers and output representations affected the end results. Named entity disambiguation concerns linking a potentially ambiguous mention of named entity in text to an unambiguous identifier in a standard database. One approach to this task is supervised classification. However, the availability of training data is often limited, and the available data sets tend to be imbalanced and, in some cases, heterogeneous. We propose a new method that distinguishes a name"
D09-1157,J93-2004,0,\N,Missing
D09-1157,P08-1000,0,\N,Missing
D14-1177,C10-1003,1,0.905281,"Missing"
D14-1177,C12-1046,0,0.380621,"ance of context vectors drastically decreases for lower frequency terms (Kontonatsios et al., 2014; Morin and Daille, 2010). Our work is more closely related to a second class of term alignment methods that exploits the internal structure of terms between a source and a target language. Compositional translation algorithms are based on the principal of compositionality (Keenan and Faltz, 1985), which claims that the translation of the whole is a function of the translation of its parts. Lexical (Morin and Daille, 2010; Daille, 2012; Robitaille et al., 2006; 1702 Tanaka, 2002) and sub-lexical (Delpech et al., 2012) compositional algorithms are knowledgerich approaches that proceed in two steps, namely generation and selection. In the generation step, an input source term is segmented into basic translation units: words (lexical compositional methods) or morphemes (sub-lexical methods). Then a pre-compiled, seed dictionary of words or morphemes is used to translate the components of the source term. Finally, a permutation function generates candidate translations using the list of the translated segments. In the selection step, candidate translations are ranked according to their frequency (Morin and Dai"
D14-1177,J93-1003,0,0.221366,"lemmatisation and Part-of-Speech (PoS) tagging. For English, Spanish and French we used the TreeTagger (Schmid, 1994) while for Greek we used the ILSP toolkit (Papageorgiou et al., 2000). The Japanese corpus was segmented and PoS-tagged using Juman (Kurohashi and Kawahara, 2005). In succession, monolingual context vectors are compiled by considering all lexical units that occur within a window of 3 words before or after a term (a seven-word window). Only lexical units (seeds) that occur in a bilingual dictionary are retained The values in context vectors are LogLikelihood Ratio associations (Dunning, 1993) of the term and a seed lexical unit occurring in it. In a second step, we use the translations in the seed dictionary to map target context vectors into the source vector space. If there are several translations for a term, they are all considered with equal weights. Finally, candidate translations are ranked in descending order of the cosine of the angle between the mapped target vectors and the source 1704 seed term dictionary seed word dictionary Train Project character n-gram model context vectors Annotate the Wikipedia interlingual links to retrieve thematically related articles in each"
D14-1177,P98-1069,0,0.795113,"a linear model and we show substantial improvements over the two translation signals. 1 Introduction Bilingual dictionaries of technical terms are resources useful for various tasks, such as computeraided human translation (Dagan and Church, 1994; Fung and McKeown, 1997), Statistical Machine Translation (Och and Ney, 2003) and CrossLanguage Information Retrieval (Ballesteros and Croft, 1997). In the last two decades, researchers have focused on automatically compiling bilingual term dictionaries either from parallel (Smadja et al., 1996; Van der Eijk, 1993) or comparable corpora (Rapp, 1999; Fung and Yee, 1998). While parallel corpora contain the same sentences in two languages, comparable corpora consist of bilingual pieces of text that share some features, only, such as topic, domain, or time period. Comparable corpora can be constructed more easily than parallel corpora. Freely available, up-to-date, on-line resources (e.g., Wikipedia) can be employed. In this paper, we exploit two different sources of information to extract bilingual terminology from comparable corpora: the compositional and the contextual clue. The compositional clue is the hypothesis that the representations of a term in any p"
D14-1177,N13-1056,0,0.378841,"s to train the n-gram models and (b) a dictionary of word-to-word correspondences to translate target context vectors. The n-gram and context vector methods are used separately to score term pairs. The n-gram model computes the value of the compositional clue while the context vector estimates the score of the contextual clue. The hybrid model combines both methods by using the corresponding scores as features to train a linear classifier. For this, we used a linear-SVM of the LIBSVM package with default values for all parameters. 4 Data Following previous research (Prochasson and Fung, 2011; Irvine and Callison-Burch, 2013; Klementiev et al., 2012), we construct comparable biomedical corpora using Wikipedia as a freely available resource. Starting with a list of 4K biomedical English terms (query-terms), we collected 4K English Wikipedia articles, by matching query-terms to the topic signatures of articles. Then, we followed Seed dictionaries As shown in Figure 1, the term alignment methods require two seed bilingual dictionaries: a term and a word dictionary. The character n-gram models rely on a bilingual term dictionary to learn associations of n-grams that appear often in technical terms. The dictionary may"
D14-1177,C02-2020,0,0.257592,"s.kontonatsios/ Software/LogReg-TermAlign.tar.gz 2 Related Work Context-based methods (Fung and Yee, 1998; Rapp, 1999) adapt the Distributional Hypothesis (Harris, 1954), i.e., words that occur in similar lexical context tend to have the same meaning, in a multilingual environment. They represent the context of each term t as a context vector, usually following the bag-of-words model. Each dimension of the vector corresponds to a context word occurring within a predefined window, while the corresponding value is computed by a correlation metric, e.g., Log-Likelihood Ratio (Morin et al., 2007; Chiao and Zweigenbaum, 2002) or Point-wise Mutual Information (Andrade et al., 2010). A general bilingual dictionary is then used to translate/project the target context vectors into the source language. As a result, the source and target context vectors become directly comparable. In a final step, candidate translations are being ranked according to a distance metric, e.g., cosine similarity (Tamura et al., 2012) or Jaccard index (Zanzotto et al., 2010; Apidianaki et al., 2012). Whilst context-based methods have become a common practise for bilingual dictionary extraction from comparable corpora, nonetheless, their perf"
D14-1177,A94-1006,0,0.115245,"ervation, we use an existing context-based approach. For evaluation, we investigate the performance of compositional and context-based methods on: (a) similar and unrelated languages, (b) corpora of different degree of comparability and (c) the translation of frequent and rare terms. Finally, we combine the two translation clues, namely string and contextual similarity, in a linear model and we show substantial improvements over the two translation signals. 1 Introduction Bilingual dictionaries of technical terms are resources useful for various tasks, such as computeraided human translation (Dagan and Church, 1994; Fung and McKeown, 1997), Statistical Machine Translation (Och and Ney, 2003) and CrossLanguage Information Retrieval (Ballesteros and Croft, 1997). In the last two decades, researchers have focused on automatically compiling bilingual term dictionaries either from parallel (Smadja et al., 1996; Van der Eijk, 1993) or comparable corpora (Rapp, 1999; Fung and Yee, 1998). While parallel corpora contain the same sentences in two languages, comparable corpora consist of bilingual pieces of text that share some features, only, such as topic, domain, or time period. Comparable corpora can be constr"
D14-1177,C12-1110,0,0.0296836,"Missing"
D14-1177,E12-1014,0,0.0796051,"(b) a dictionary of word-to-word correspondences to translate target context vectors. The n-gram and context vector methods are used separately to score term pairs. The n-gram model computes the value of the compositional clue while the context vector estimates the score of the contextual clue. The hybrid model combines both methods by using the corresponding scores as features to train a linear classifier. For this, we used a linear-SVM of the LIBSVM package with default values for all parameters. 4 Data Following previous research (Prochasson and Fung, 2011; Irvine and Callison-Burch, 2013; Klementiev et al., 2012), we construct comparable biomedical corpora using Wikipedia as a freely available resource. Starting with a list of 4K biomedical English terms (query-terms), we collected 4K English Wikipedia articles, by matching query-terms to the topic signatures of articles. Then, we followed Seed dictionaries As shown in Figure 1, the term alignment methods require two seed bilingual dictionaries: a term and a word dictionary. The character n-gram models rely on a bilingual term dictionary to learn associations of n-grams that appear often in technical terms. The dictionary may contain both singleword a"
D14-1177,P07-2045,0,0.00455687,"dels rely on a bilingual term dictionary to learn associations of n-grams that appear often in technical terms. The dictionary may contain both singleword and multi-word terms. For English-Spanish and English-French we used UMLS (Bodenreider, 2004) while for English-Japanese we used an electronic dictionary of medical terms (Denshika and Kenkyukai, 1991). An English-Greek biomedical dictionary was not available at the time of conducting these experiments, thus we automatically compiled a dictionary from a parallel corpus. For this, we trained a standard Statistical Machine Translation system (Koehn et al., 2007) on EMEA (Tiedemann, 2009), a biomedical parallel corpus containing sentencealigned documents from the European Medicines Agency. Then, we extracted all English-Greek pairs for which: (a) the English sequence was listed in UMLS and (b) the translation probability was equal or higher to 0.7. The sizes of the seed term dictionaries vary significantly, e.g., 500K entries for English-French but only 20K entries for English-Greek. However, the character n-gram models require a relatively small portion of the corresponding dictionary to converge. In the reported experiments, we used 10K translation"
D14-1177,E14-4022,1,0.816528,"p-to-date, on-line resources (e.g., Wikipedia) can be employed. In this paper, we exploit two different sources of information to extract bilingual terminology from comparable corpora: the compositional and the contextual clue. The compositional clue is the hypothesis that the representations of a term in any pair of languages tend to consist of corresponding lexical or sub-lexical units, e.g., prefixes, suffices and morphemes. In order to capture associations of textual units across languages, we investigate three different character n-gram approaches, namely a Random Forest (RF) classifier (Kontonatsios et al., 2014), Support Vector Machines with an RBF kernel (SVM-RBF) and a Logistic Regression (LogReg) classifier. Whilst the previous approaches take as an input monolingual features and then try to find cross-lingual mappings, our proposed method (LogReg classifier) considers multilingual features, i.e., tuples of cooccurring n-grams. The contextual clue is the hypothesis that mutual translations of a term tend to occur in similar lexical context. Context-based approaches are unsupervised methods that compare the context distributions of a source and a target term. A bilingual seed dictionary is used to"
D14-1177,C10-1073,0,0.0655114,"M-RBF) and a Logistic Regression (LogReg) classifier. Whilst the previous approaches take as an input monolingual features and then try to find cross-lingual mappings, our proposed method (LogReg classifier) considers multilingual features, i.e., tuples of cooccurring n-grams. The contextual clue is the hypothesis that mutual translations of a term tend to occur in similar lexical context. Context-based approaches are unsupervised methods that compare the context distributions of a source and a target term. A bilingual seed dictionary is used to map context vector dimensions of two languages. Li and Gaussier (2010) suggested that the seed dictionary can be used to estimate the degree of comparability of a bilingual corpus. Given a seed dictionary, the corpus comparability is the expectation of finding for each word of the source corpus, its translation in the target part of the corpus. The performance of context-based methods has been shown to depend on the frequency of terms to be translated and the 1701 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1701–1712, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics corpu"
D14-1177,papageorgiou-etal-2000-unified,0,0.00884113,"idate translations by classification margin. 3.2 Context vectors We follow a standard approach to calculate context similarity of source and target terms (Rapp, 1999; Morin and Daille, 2010; Morin and Prochasson, 2011a; Delpech et al., 2012). Context vectors of candidate terms in the source and target language are populated after normalising each bilingual corpus, separately. Normalisation consists of stop-word filtering, tokenisation, lemmatisation and Part-of-Speech (PoS) tagging. For English, Spanish and French we used the TreeTagger (Schmid, 1994) while for Greek we used the ILSP toolkit (Papageorgiou et al., 2000). The Japanese corpus was segmented and PoS-tagged using Juman (Kurohashi and Kawahara, 2005). In succession, monolingual context vectors are compiled by considering all lexical units that occur within a window of 3 words before or after a term (a seven-word window). Only lexical units (seeds) that occur in a bilingual dictionary are retained The values in context vectors are LogLikelihood Ratio associations (Dunning, 1993) of the term and a seed lexical unit occurring in it. In a second step, we use the translations in the seed dictionary to map target context vectors into the source vector s"
D14-1177,P11-1133,0,0.0678086,"ry of term translation pairs to train the n-gram models and (b) a dictionary of word-to-word correspondences to translate target context vectors. The n-gram and context vector methods are used separately to score term pairs. The n-gram model computes the value of the compositional clue while the context vector estimates the score of the contextual clue. The hybrid model combines both methods by using the corresponding scores as features to train a linear classifier. For this, we used a linear-SVM of the LIBSVM package with default values for all parameters. 4 Data Following previous research (Prochasson and Fung, 2011; Irvine and Callison-Burch, 2013; Klementiev et al., 2012), we construct comparable biomedical corpora using Wikipedia as a freely available resource. Starting with a list of 4K biomedical English terms (query-terms), we collected 4K English Wikipedia articles, by matching query-terms to the topic signatures of articles. Then, we followed Seed dictionaries As shown in Figure 1, the term alignment methods require two seed bilingual dictionaries: a term and a word dictionary. The character n-gram models rely on a bilingual term dictionary to learn associations of n-grams that appear often in te"
D14-1177,P99-1067,0,0.682704,"milarity, in a linear model and we show substantial improvements over the two translation signals. 1 Introduction Bilingual dictionaries of technical terms are resources useful for various tasks, such as computeraided human translation (Dagan and Church, 1994; Fung and McKeown, 1997), Statistical Machine Translation (Och and Ney, 2003) and CrossLanguage Information Retrieval (Ballesteros and Croft, 1997). In the last two decades, researchers have focused on automatically compiling bilingual term dictionaries either from parallel (Smadja et al., 1996; Van der Eijk, 1993) or comparable corpora (Rapp, 1999; Fung and Yee, 1998). While parallel corpora contain the same sentences in two languages, comparable corpora consist of bilingual pieces of text that share some features, only, such as topic, domain, or time period. Comparable corpora can be constructed more easily than parallel corpora. Freely available, up-to-date, on-line resources (e.g., Wikipedia) can be employed. In this paper, we exploit two different sources of information to extract bilingual terminology from comparable corpora: the compositional and the contextual clue. The compositional clue is the hypothesis that the representatio"
D14-1177,W11-1205,0,0.0718035,"ale, linear classifications problems. LIBLINEAR implements two linear classification algorithms: LogReg and linear-SVM. Both models solve the same optimisation problem, i.e., determine the optimal separating plane, but they adopt different loss functions. Since LIBLINEAR does not support decision value estimations for the linear-SVM, we only experimented with LogReg. Similarly to SVM-RBF, LogReg ranks candidate translations by classification margin. 3.2 Context vectors We follow a standard approach to calculate context similarity of source and target terms (Rapp, 1999; Morin and Daille, 2010; Morin and Prochasson, 2011a; Delpech et al., 2012). Context vectors of candidate terms in the source and target language are populated after normalising each bilingual corpus, separately. Normalisation consists of stop-word filtering, tokenisation, lemmatisation and Part-of-Speech (PoS) tagging. For English, Spanish and French we used the TreeTagger (Schmid, 1994) while for Greek we used the ILSP toolkit (Papageorgiou et al., 2000). The Japanese corpus was segmented and PoS-tagged using Juman (Kurohashi and Kawahara, 2005). In succession, monolingual context vectors are compiled by considering all lexical units that oc"
D14-1177,W02-2026,0,0.0328349,"for rare terms. Finally, we hypothesised that the n-gram and context-based methods provide complimentary information. To test this hypothesis, we developed a hybrid method that combines compositional and contextual similarity scores as features in a linear classifier. The hybrid model achieved significantly better top-20 translation accuracy than the two methods separately but minor improvements were observed in terms of top-1 accuracy. As future work, we plan to improve the quality of the extracted dictionary further by exploiting additional translation signals. For example, previous works (Schafer and Yarowsky, 2002; Klementiev et al., 2012) have reported that the temporal and topic similarity are clues that indicate translation equivalence. It would be interesting to investigate the contribution of different clues for various 1709 experimental parameters, e.g., domain, distance of languages, types of comparable corpora. Acknowledgements The authors would like to thank Dr. Danushka Bollegala for providing feedback on this paper and the three anonymous reviewers for their useful comments and suggestions. This work was funded by the European Community’s Seventh Framework Program (FP7/2007-2013) [grant numb"
D14-1177,P07-1084,0,0.154948,".uk/postgrad/georgios.kontonatsios/ Software/LogReg-TermAlign.tar.gz 2 Related Work Context-based methods (Fung and Yee, 1998; Rapp, 1999) adapt the Distributional Hypothesis (Harris, 1954), i.e., words that occur in similar lexical context tend to have the same meaning, in a multilingual environment. They represent the context of each term t as a context vector, usually following the bag-of-words model. Each dimension of the vector corresponds to a context word occurring within a predefined window, while the corresponding value is computed by a correlation metric, e.g., Log-Likelihood Ratio (Morin et al., 2007; Chiao and Zweigenbaum, 2002) or Point-wise Mutual Information (Andrade et al., 2010). A general bilingual dictionary is then used to translate/project the target context vectors into the source language. As a result, the source and target context vectors become directly comparable. In a final step, candidate translations are being ranked according to a distance metric, e.g., cosine similarity (Tamura et al., 2012) or Jaccard index (Zanzotto et al., 2010; Apidianaki et al., 2012). Whilst context-based methods have become a common practise for bilingual dictionary extraction from comparable co"
D14-1177,J03-1002,0,0.0270204,"e the performance of compositional and context-based methods on: (a) similar and unrelated languages, (b) corpora of different degree of comparability and (c) the translation of frequent and rare terms. Finally, we combine the two translation clues, namely string and contextual similarity, in a linear model and we show substantial improvements over the two translation signals. 1 Introduction Bilingual dictionaries of technical terms are resources useful for various tasks, such as computeraided human translation (Dagan and Church, 1994; Fung and McKeown, 1997), Statistical Machine Translation (Och and Ney, 2003) and CrossLanguage Information Retrieval (Ballesteros and Croft, 1997). In the last two decades, researchers have focused on automatically compiling bilingual term dictionaries either from parallel (Smadja et al., 1996; Van der Eijk, 1993) or comparable corpora (Rapp, 1999; Fung and Yee, 1998). While parallel corpora contain the same sentences in two languages, comparable corpora consist of bilingual pieces of text that share some features, only, such as topic, domain, or time period. Comparable corpora can be constructed more easily than parallel corpora. Freely available, up-to-date, on-line"
D14-1177,J96-1001,0,0.129838,"mbine the two translation clues, namely string and contextual similarity, in a linear model and we show substantial improvements over the two translation signals. 1 Introduction Bilingual dictionaries of technical terms are resources useful for various tasks, such as computeraided human translation (Dagan and Church, 1994; Fung and McKeown, 1997), Statistical Machine Translation (Och and Ney, 2003) and CrossLanguage Information Retrieval (Ballesteros and Croft, 1997). In the last two decades, researchers have focused on automatically compiling bilingual term dictionaries either from parallel (Smadja et al., 1996; Van der Eijk, 1993) or comparable corpora (Rapp, 1999; Fung and Yee, 1998). While parallel corpora contain the same sentences in two languages, comparable corpora consist of bilingual pieces of text that share some features, only, such as topic, domain, or time period. Comparable corpora can be constructed more easily than parallel corpora. Freely available, up-to-date, on-line resources (e.g., Wikipedia) can be employed. In this paper, we exploit two different sources of information to extract bilingual terminology from comparable corpora: the compositional and the contextual clue. The comp"
D14-1177,D12-1003,0,0.341756,"nsion of the vector corresponds to a context word occurring within a predefined window, while the corresponding value is computed by a correlation metric, e.g., Log-Likelihood Ratio (Morin et al., 2007; Chiao and Zweigenbaum, 2002) or Point-wise Mutual Information (Andrade et al., 2010). A general bilingual dictionary is then used to translate/project the target context vectors into the source language. As a result, the source and target context vectors become directly comparable. In a final step, candidate translations are being ranked according to a distance metric, e.g., cosine similarity (Tamura et al., 2012) or Jaccard index (Zanzotto et al., 2010; Apidianaki et al., 2012). Whilst context-based methods have become a common practise for bilingual dictionary extraction from comparable corpora, nonetheless, their performance is subject to various factors, one of which is the quality of the comparable corpus. Li and Gaussier (2010) introduced the corpus comparability metric and showed that it is related to the performance of context vectors. The higher the corpus comparability is, the higher the performance of context vectors is. Furthermore, context vector approaches are sensitive to the frequency o"
D14-1177,C02-1065,0,0.109739,"Missing"
D14-1177,E93-1015,0,0.519059,"Missing"
D14-1177,C10-1142,1,0.883552,"Missing"
D14-1177,C98-1066,0,\N,Missing
D14-1177,E06-1029,0,\N,Missing
D17-1001,P09-1053,0,0.0349677,"i training (Brown et al., 1993) together with a beam search of size µb ∈ N on the featureenhanced EM. Also, mini-batch training (Liang 1(aslex = ·, ascat = ·, atlex = ·, atcat = ·) 1(SurfaceSim(aslex = ·, atlex = ·)) 1(WordnetSim(aslex = ·, atlex = ·)) 1(EmbeddingSim(aslex = ·, atlex = ·)) 1(IsPrepositionPair(aslex = ·, atlex = ·)) 1(ascat = ·, atcat = ·) 1(IsSameCategory(ascat = ·, atcat = ·)) The first feature is an indicator invoked only at specific values. On the other hand, the rest of the 3 We also tried features based on the configurations of the source and target sub-trees similar to (Das and Smith, 2009) as well as features based on the spans of null-alignments. However, none of them contributed to alignment quality. 6 6 features are invoked across multiple values, allowing general patterns to be learned. The second feature is invoked if two heads are identical or a head is a substring of another. The third feature is invoked if two heads are synonyms or derivations that are extracted from the WordNet4 . The fourth feature is invoked if the cosine similarity between word embeddings of two heads is larger than a threshold. The fifth feature is invoked when the heads are both prepositions to ca"
D17-1001,N10-1083,0,0.0337933,"Missing"
D17-1001,C04-1051,0,0.566767,"Missing"
D17-1001,P16-1180,0,0.0237666,"Missing"
D17-1001,W16-1718,0,0.0659557,"Missing"
D17-1001,J93-2003,0,0.0655065,"Missing"
D17-1001,N13-1092,0,0.210697,"Missing"
D17-1001,W06-2920,0,0.0236187,". The performance of the human annotators was assessed by considering one annotator as the test and the other two as the gold-standard, and then taking the averages, which is the same setting as our method. We regard this as the pseudo interwhere Ha is a set of alignments, while G and G0 are the ones that two of annotators produce, respectively. The function of |· |counts the elements in a set. There are three combinations for G and G0 because we had three annotators. The final precision and recall values are their averages. Parsing Quality The parsing quality was evaluated using the CONLL-X (Buchholz and Marsi, 2006) standard. Dependencies were extracted from the output HPSG trees, and evaluated using the official script6 . Due to this conversion, the accuracy on the relation labels is less important. Thus, we reported only the unlabeled attachment score (UAS)7 . The development and test sets provide 2, 371 and 6, 957 dependencies, respectively. 6 http://ilk.uvt.nl/conll/software.html Although omitted, the labeled attachment score showed the same tendency as UAS. 7 8 annotator agreement, since the conventional interannotator agreement is not directly applicable due to variations in aligned phrases. Our me"
D17-1001,N10-1015,0,0.144834,"arget i [τ ]i phrases Ψ[τm ] = {ψk m } and null-alignments function PACK(hτ s , τ t i, hα, hhm , hn ii, A) if hτ s , τ t i ∈ A[τ s ] then A[τ s ] ← A[τ s ] ∪ hα, hhm , hn ii . Merge supports and their inside probability else A[τ s ] ← (hτ s , τ t i, hα, hhm , hn ii) support relations are packed as a support list2 by the PACK function. 4.3 Non-Compositional Alignment t ∈ h A monotonic alignment requires τm m and t τn ∈ hn to have an LCA, which adheres to the compositionality in language. However, previous studies declared that the compositionality is violated in a monolingual phrase alignment (Burkett et al., 2010; Weese et al., 2014). Heilman and Smith (2010) discuss complex phrase reordering is prevalent in paraphrases and entailed text. A non-monotonic alignment occurs when corresponding phrases have largely different orders, t ) is an ancestor of another i.e., one of them (e.g., τm t (e.g., τn ) or the same phrase. Such a case could t has nullbe exceptionally compatible, when τm alignments and all the aligned phrases of τnt fit in these null-alignments. A new alignment hτis , τit (= t )i would be non-monotonically formed. Fig. 5 τm shows a real example of non-compositional alignment produced by our"
D17-1001,N10-1145,0,0.031299,"null-alignments function PACK(hτ s , τ t i, hα, hhm , hn ii, A) if hτ s , τ t i ∈ A[τ s ] then A[τ s ] ← A[τ s ] ∪ hα, hhm , hn ii . Merge supports and their inside probability else A[τ s ] ← (hτ s , τ t i, hα, hhm , hn ii) support relations are packed as a support list2 by the PACK function. 4.3 Non-Compositional Alignment t ∈ h A monotonic alignment requires τm m and t τn ∈ hn to have an LCA, which adheres to the compositionality in language. However, previous studies declared that the compositionality is violated in a monolingual phrase alignment (Burkett et al., 2010; Weese et al., 2014). Heilman and Smith (2010) discuss complex phrase reordering is prevalent in paraphrases and entailed text. A non-monotonic alignment occurs when corresponding phrases have largely different orders, t ) is an ancestor of another i.e., one of them (e.g., τm t (e.g., τn ) or the same phrase. Such a case could t has nullbe exceptionally compatible, when τm alignments and all the aligned phrases of τnt fit in these null-alignments. A new alignment hτis , τit (= t )i would be non-monotonically formed. Fig. 5 τm shows a real example of non-compositional alignment produced by our method. The target phrase τnt (“through the sp"
D17-1001,D08-1092,0,0.175738,"unpacked. 5 Source: Relying on team spirit, expedition members defeated difficulties. Target: Members of the scientific team overcame difficulties through the spirit of teamwork. ?????? S S S ?????? VP NP ?? ???? S VP NP ?? ???? VP ?? PP ???? VP Relying on ⋯ spirit , ⋯ members defeated difficulties Members ⋯ overcame difficulties through ⋯ teamwork Figure 5: Example of a non-compositional alignment gether with a new alignment pair hτis , τit i where s , τ s ) and τ t = τ t . τis = lca(τm n m i 4.4 and Klein, 2009) is applied. Such an approximation for efficiency is common in parallel parsing (Burkett and Klein, 2008; Burkett et al., 2010). In addition, an alignment supported by distant descendants tends to fail to reach a root-pair alignment. Thus, we restrict the generation gap between a support alignment and its LCA to be less than or equal to µg ∈ N. Forest Alignment Although we have discussed using trees for clarity, the alignment is conducted on forests. The alignment process is basically the same. The only difference is that the same pair has multiple LCAs. Hence, we need to verify if the sub-trees can be on the same tree when identifying their LCAs since multiple nodes may cover the same span with"
D17-1001,W13-0808,0,0.0553215,"Missing"
D17-1001,W13-0807,0,0.0612393,"Missing"
D17-1001,N09-1069,0,0.0248754,"in (Srikumar and Roth, 2013) as a preposition dictionary to compute the feature function. In addition, we extend W using word embeddings; we use the MVLSA word embeddings (Rastogi et al., 2015) given the superior performance in word similarity tasks. Specifically, we compute the cosine similarity of embeddings; words with a higher similarity value than a threshold are determined as similar words. The threshold is empirically set as the 100th highest similarity value between words in the training corpus. Since our method allows null-alignments, it has a degenerate maximum likelihood solution (Liang and Klein, 2009) that makes every phrase nullalignment. Similarly, a degenerate solution overly conducts non-compositional alignment. To avoid these issues, a penalty is incorporated:   exp{−(|τis |φ + |τit |φ + µc + 1)µn }     (non-compositional alignment) Pe (τis , τit ) =  exp{−(|τis |φ + |τit |φ + 1)µn }     (otherwise) 5.4 Evaluation 5 NIST OpenMT corpora: LDC2010T14, LDC2010T17, LDC2010T21, LDC2010T23, LDC2013T03 http://wordnet.princeton.edu 7 annotated gold-trees to paraphrasal sentence pairs sampled from the training corpus. To diversify the data, only one reference pair per sentence of a s"
D17-1001,P15-1118,0,0.16624,"Missing"
D17-1001,D08-1084,0,0.526892,"Missing"
D17-1001,J08-4005,0,0.0784849,"Missing"
D17-1001,J08-1002,1,0.703543,"ons, null (i.e., for τ∅ ), and others. 5.3 As discussed in Sec. 2, previous studies have not conducted syntactic phrase alignment on parse trees. A direct metric does not exist to compare paraphrases that cover different spans, i.e., our syntactic paraphrases and paraphrases of n-grams. Thus, we compared the alignment quality to that of humans as a realistic way to evaluate the performance of our method. We also evaluated the parsing quality. Similar to the alignment quality, differences in phrase structures disturb the comparisons (Sagae et al., 2008). Our method applies an HPSG parser Enju (Miyao and Tsujii, 2008) to derive parse forests due to its state-of-the-art performance and ability to provide rich properties of phrases. Hence, we compared our parsing quality to the 1-best parses of Enju. 6.1 Penalty Function where |· |φ computes the span of internal nullalignments, and µn ≥ 1.0 and µc ∈ R+ control the strength of the penalties of the nullalignment and the non-compositional alignment, respectively. The penalty function is multiplied by Eq. (1) as a soft-constraint for re-ranking alignment pairs in Algorithm 4.1. Combination with Parse Probability Following the spirit of parallel parsing that simu"
D17-1001,N15-1058,0,0.0410684,"Missing"
D17-1001,Q15-1025,0,0.0811191,"Missing"
D17-1001,W05-0908,0,0.0574695,"Missing"
D17-1001,J97-3002,0,0.310906,"Missing"
D17-1001,D13-1056,0,0.160392,"Missing"
D17-1001,W06-3104,0,0.109294,"Missing"
D17-1001,N15-1154,0,0.0513309,"Missing"
D17-1001,W04-3207,0,0.0463742,"Missing"
D17-1001,D13-1170,0,0.011653,"Missing"
D17-1001,Q13-1019,0,0.012782,"urces We used reference translations to evaluate machine translations5 as sentential paraphrases (Weese et al., 2014). The reference translations of 10 to 30 words were extracted and paired, giving 41K pairs as a training corpus. We use different kinds of dictionaries to obtain word alignments W as well as to compute feature functions. First, we extract synonyms and words with derivational relationship using WordNet. Then we handcraft derivation rules (e.g., create, creation, creator) and extract potentially derivational words from the training corpus. Finally, we use prepositions defined in (Srikumar and Roth, 2013) as a preposition dictionary to compute the feature function. In addition, we extend W using word embeddings; we use the MVLSA word embeddings (Rastogi et al., 2015) given the superior performance in word similarity tasks. Specifically, we compute the cosine similarity of embeddings; words with a higher similarity value than a threshold are determined as similar words. The threshold is empirically set as the 100th highest similarity value between words in the training corpus. Since our method allows null-alignments, it has a degenerate maximum likelihood solution (Liang and Klein, 2009) that m"
D17-1001,P15-1150,0,0.0609628,"Missing"
D17-1001,C12-2120,0,0.450083,"Missing"
D17-1001,E14-1021,0,0.226773,"Ψ[τm ] = {ψk m } and null-alignments function PACK(hτ s , τ t i, hα, hhm , hn ii, A) if hτ s , τ t i ∈ A[τ s ] then A[τ s ] ← A[τ s ] ∪ hα, hhm , hn ii . Merge supports and their inside probability else A[τ s ] ← (hτ s , τ t i, hα, hhm , hn ii) support relations are packed as a support list2 by the PACK function. 4.3 Non-Compositional Alignment t ∈ h A monotonic alignment requires τm m and t τn ∈ hn to have an LCA, which adheres to the compositionality in language. However, previous studies declared that the compositionality is violated in a monolingual phrase alignment (Burkett et al., 2010; Weese et al., 2014). Heilman and Smith (2010) discuss complex phrase reordering is prevalent in paraphrases and entailed text. A non-monotonic alignment occurs when corresponding phrases have largely different orders, t ) is an ancestor of another i.e., one of them (e.g., τm t (e.g., τn ) or the same phrase. Such a case could t has nullbe exceptionally compatible, when τm alignments and all the aligned phrases of τnt fit in these null-alignments. A new alignment hτis , τit (= t )i would be non-monotonically formed. Fig. 5 τm shows a real example of non-compositional alignment produced by our method. The target p"
D19-1542,S17-2001,0,0.0247172,"te reports its issues.10 GLUE tasks can be categorized according to their aims as follows. Semantic Equivalence Assessment Tasks (MRPC, STS-B, QQP) These are the primary targets of our method, which are used to verify hypothesis H1. Paraphrase identification assesses semantic equivalence in a sentence pair by binary judgments. Microsoft Paraphrase Corpus (MRPC) (Dolan et al., 2004) consists of sentence pairs drawn from news articles, while Quora Question Pairs (QQP)11 consists of question pairs from the community QA website. STS assesses semantic equivalence by grading. STS benchmark (STS-B) (Cer et al., 2017) provides sentence pairs drawn from heterogeneous sources, which are human-annotated with a level of equivalence from 1 to 5. NLI Tasks (MNLI-m/mm, RTE, QNLI) We use natural language inference (NLI) tasks to verify hypothesis H3 because they constitute a class of problems relevant to semantic equivalence assessment. NLI tasks are different from semantic equivalence assessment in that they often require logical inference and understanding of commonsense knowledge. The Multi-Genre Natural Language Inference Corpus (MNLI) (Williams et al., 2018) is a crowd-sourced corpus and covers heterogeneous"
D19-1542,P17-1152,0,0.0289595,", ht ), (b) taking the element-wise product hs ∗ ht , and (c) finding the absolute element-wise difference |hs − ht |. The final vector of R4λ is fed into a classifier.2 Because our method aims to generate representations for semantic equivalence assessment, the classifier should be simple (Logeswaran and Lee, 2018). Otherwise, a sophisticated classifier would fit itself with the task instead of the representations. We use a single fully-connected layer culminating in a softmax layer as our classifier. Previous studies have calculated interactions between words (He and Lin, 2016) and phrases (Chen et al., 2017) using the final hidden states of bidirectional RNN or recursive neural networks when composing a sentence representation. Our approach differs from these by giving explicit supervision of which phrase pairs have semantic interactions (i.e., paraphrases). 2 Our follow-up study confirms that a simpler feature generation improves the generality of our model to contribute not only to semantic equivalent assessment but also natural language inference. For details, please refer to the Appendix. 5396 Negative Example Selection In paraphrase identification, non-paraphrases with large lexical differen"
D19-1542,D17-1070,0,0.0485455,"enized sentence pair is encoded by the BERT model. For the input sequence of N tokens {wi }i=1,...,N , we obtain the final hidden states {hi }i=1,...,N (i.e., output of the bidirectional Transformer): hi = Transformer(w1 , . . . , wN ), where hi ∈ Rλ and λ is the hidden size. We then combine {hi }i for a phrase pair with an alignment h(j, k), (m, n)i where 2 ≤ j < k < m < n ≤ N − 1 represent indexes of the beginning and ending of phrases (recall that the first and last tokens are always special tokens in BERT). As a combination function, we apply max-pooling that showed strong performance in (Conneau et al., 2017) to generate a representation of source and target phrases: hs = max-pooling(hj , . . . , hk ), (1) ht = max-pooling(hm , . . . , hn ). (2) The max-pooling(·) function selects the maximum value over each dimension of the hidden units. Then hs and ht are converted to a single vector. To extract relations between hs and ht , three matching methods are used (Conneau et al., 2017): (a) concatenating the representations (hs , ht ), (b) taking the element-wise product hs ∗ ht , and (c) finding the absolute element-wise difference |hs − ht |. The final vector of R4λ is fed into a classifier.2 Because"
D19-1542,N19-1423,0,0.381041,"ujii?2 1 Osaka University, Japan ? Artificial Intelligence Research Center (AIRC), AIST, Japan 2 NaCTeM, School of Computer Science, University of Manchester, UK arase@ist.osaka-u.ac.jp, j-tsujii@aist.go.jp Abstract A semantic equivalence assessment is defined as a task that assesses semantic equivalence in a sentence pair by binary judgment (i.e., paraphrase identification) or grading (i.e., semantic textual similarity measurement). It constitutes a set of tasks crucial for research on natural language understanding. Recently, BERT realized a breakthrough in sentence representation learning (Devlin et al., 2019), which is broadly transferable to various NLP tasks. While BERT’s performance improves by increasing its model size, the required computational power is an obstacle preventing practical applications from adopting the technology. Herein, we propose to inject phrasal paraphrase relations into BERT in order to generate suitable representations for semantic equivalence assessment instead of increasing the model size. Experiments on standard natural language understanding tasks confirm that our method effectively improves a smaller BERT model while maintaining the model size. The generated model e"
D19-1542,C04-1051,0,0.0645745,"ies on MRPC and QQP and Spearman correlation on STS-B are omitted due to space limitations. Note that they showed the same trends as F1 and Pearson correlation, respectively, in our experiment. WNLI was excluded because the GLUE web site reports its issues.10 GLUE tasks can be categorized according to their aims as follows. Semantic Equivalence Assessment Tasks (MRPC, STS-B, QQP) These are the primary targets of our method, which are used to verify hypothesis H1. Paraphrase identification assesses semantic equivalence in a sentence pair by binary judgments. Microsoft Paraphrase Corpus (MRPC) (Dolan et al., 2004) consists of sentence pairs drawn from news articles, while Quora Question Pairs (QQP)11 consists of question pairs from the community QA website. STS assesses semantic equivalence by grading. STS benchmark (STS-B) (Cer et al., 2017) provides sentence pairs drawn from heterogeneous sources, which are human-annotated with a level of equivalence from 1 to 5. NLI Tasks (MNLI-m/mm, RTE, QNLI) We use natural language inference (NLI) tasks to verify hypothesis H3 because they constitute a class of problems relevant to semantic equivalence assessment. NLI tasks are different from semantic equivalence"
D19-1542,D18-1045,0,0.0259746,"Missing"
D19-1542,N16-1108,0,0.0249868,"nating the representations (hs , ht ), (b) taking the element-wise product hs ∗ ht , and (c) finding the absolute element-wise difference |hs − ht |. The final vector of R4λ is fed into a classifier.2 Because our method aims to generate representations for semantic equivalence assessment, the classifier should be simple (Logeswaran and Lee, 2018). Otherwise, a sophisticated classifier would fit itself with the task instead of the representations. We use a single fully-connected layer culminating in a softmax layer as our classifier. Previous studies have calculated interactions between words (He and Lin, 2016) and phrases (Chen et al., 2017) using the final hidden states of bidirectional RNN or recursive neural networks when composing a sentence representation. Our approach differs from these by giving explicit supervision of which phrase pairs have semantic interactions (i.e., paraphrases). 2 Our follow-up study confirms that a simpler feature generation improves the generality of our model to contribute not only to semantic equivalent assessment but also natural language inference. For details, please refer to the Appendix. 5396 Negative Example Selection In paraphrase identification, non-paraphr"
D19-1542,P13-1151,0,0.0330766,"enMT Simple Wikipedia Twitter URL corpus Para-NMT Total Sentence 47k 97k 50k 3.9M 4.1M Phrase 711k 1.4M 396k 26.7M 29.2M Table 1: Numbers of sentential and phrasal paraphrases after the phrase alignment process. (LDC) or authors’ websites. The following bullets describe the sources. • NIST OpenMT4 : We randomly paired reference translations of the same source sentence as was done in (Arase and Tsujii, 2017). • Twitter URL corpus (Lan et al., 2017): This corpus was collected from Twitter by linking tweets through shared URLs. We used a threemonth collection of paraphrases.5 • Simple Wikipedia (Kauchak, 2013): This corpus aligned English Wikipedia and Simple English Wikipedia for text simplification. We used “sentence-aligned, version 2.0.”6 • Para-NMT (Wieting and Gimpel, 2018): This corpus was created by translating the Czech side of a large Czech-English parallel corpus and pairing the translated English and originally target-side English as paraphrases. We used “Para-nmt-5m-processed.”7 Note that these sentential and phrasal paraphrases are obtained by automatic methods. On the contrary, dataset creation for downstream tasks generally requires expensive human annotation. We employed the pre-tr"
D19-1542,D17-1126,0,0.0476346,"Missing"
D19-1542,P19-1441,0,0.0228953,"adford et al., 2018) replaced ELMo’s bidirectional RNN for language modeling with the Transformer (Vaswani et al., 2017) decoder. More recently, BERT combined the approaches of Quick-Thoughts (i.e., a nextsentence prediction approach) and language modeling on top of the deep bidirectional Transformer. BERT broke the records of the previous stateof-the-art methods in eleven different NLP tasks. While BERT’s pre-training generates generic representations that are broadly transferable to various NLP tasks, we aim to fit them for semantic equivalence assessment by injecting paraphrasal relations. Liu et al. (2019) showed that BERT’s performance improves when fine-tuning with a multi-task learning setting, which is applicable to our trained model for further improvement. 3 Sentence representation learning is an active research area due to its importance in various downstream tasks. Early studies employed supervised learning where a sentence representation is learned in an end-to-end manner using an annotated corpus. Among these, the importance of phrase structures in representation learning has been discussed (Tai et al., 2015; Wu et al., 2018). In this paper, we use structural relations in sentence pai"
D19-1542,N18-1202,0,0.0304145,"ing attempt, learns to generate surrounding sentences given a sentence in a document (Kiros et al., 2015). This can be interpreted as an extension of the distributional hypothesis on sentences. Quick-Thoughts, a successor of Skip-Thought, conducts classification to discriminate surrounding sentences instead of generation (Logeswaran and Lee, 2018). GenSen combines these approaches in massive multi-task learning (Subramanian et al., 2018) based on the premise that learning dependent tasks enriches sentence representations. Embeddings from Language Models (ELMo) made a significant step forward (Peters et al., 2018). ELMo uses language modeling with bidirectional recurrent neural networks (RNN) to improve word embeddings. ELMo’s embedding contributes to the performance of various downstream tasks. OpenAI GPT (Radford et al., 2018) replaced ELMo’s bidirectional RNN for language modeling with the Transformer (Vaswani et al., 2017) decoder. More recently, BERT combined the approaches of Quick-Thoughts (i.e., a nextsentence prediction approach) and language modeling on top of the deep bidirectional Transformer. BERT broke the records of the previous stateof-the-art methods in eleven different NLP tasks. Whil"
D19-1542,P18-1080,0,0.0639663,"Missing"
D19-1542,D16-1264,0,0.0480506,"c Equivalence MRPC STS-B QQP 88.3 84.7 71.2 88.6 86.0 72.1 89.2 87.4 71.2 NLI MNLI (m/mm) RTE 84.3/83.0 59.8 86.2/85.5 65.5 83.9/83.1 64.8 QNLI 89.1 92.7 89.3 Single-Sent. SST CoLA 93.3 52.7 94.1 55.7 93.1 47.2 Table 3: GLUE test results scored by the GLUE evaluation server. The best scores are represented in bold and scores higher than those of BERT-base are underlined. NLI task while MNLI-mm is a cross-domain NLI task. The Recognizing Textual Entailment (RTE) corpus12 was created from news and Wikipedia. Question-answering NLI (QNLI) was created from The Stanford Question Answering Dataset (Rajpurkar et al., 2016) on which all the sentences were drawn from Wikipedia. evaluation, performances were stable when setting the same hyper-parameters, but further investigation is our future work. Single-Sentence Tasks (SST, CoLA) We use these tasks to verify hypothesis H4. They aim to estimate features in a single sentence, which has little interaction with semantic equivalence assessment in a sentence pair. The Stanford Sentiment Treebank (SST) (Socher et al., 2013) task is a binary sentiment classification, while The Corpus of Linguistic Acceptability (CoLA) (Warstadt et al., 2018) task is a binary classifica"
D19-1542,D13-1170,0,0.00698848,"ent (RTE) corpus12 was created from news and Wikipedia. Question-answering NLI (QNLI) was created from The Stanford Question Answering Dataset (Rajpurkar et al., 2016) on which all the sentences were drawn from Wikipedia. evaluation, performances were stable when setting the same hyper-parameters, but further investigation is our future work. Single-Sentence Tasks (SST, CoLA) We use these tasks to verify hypothesis H4. They aim to estimate features in a single sentence, which has little interaction with semantic equivalence assessment in a sentence pair. The Stanford Sentiment Treebank (SST) (Socher et al., 2013) task is a binary sentiment classification, while The Corpus of Linguistic Acceptability (CoLA) (Warstadt et al., 2018) task is a binary classification of grammatical acceptability. Table 3 shows fine-tuning results on GLUE; our model, denoted as Transfer Fine-Tuning, is compared against BERT-base and BERT-large. The first set of columns shows the results of semantic equivalence assessment tasks. Our model outperformed BERT-base on MRPC (+0.9 points) and STS-B (+2.7 points). Furthermore, it outperformed even BERT-large by 0.6 points on MRPC and by 1.4 points on STS-B, despite BERT-large having"
D19-1542,D18-1412,0,0.0259522,"hing true paraphrases and phrases in the paraphrasal sentence pair t. These may provide sub-phrases or ancestor phrases of true paraphrases as difficult negative examples, which tend to retain the same topic and similar wordings. To prepare such examples, for each phrase pair h(j, k), (m, n)i ∈ A, the target span (m, n) is replaced by a randomly chosen phrase span in t. Phrasal paraphrase classification aims to give explicit supervision of semantic relations among phrases in representation learning. It also introduces structures in sentences, which is completely missed in BERT’s pre-training. Swayamdipta et al. (2018) showed that supervision of phrasebased syntax improves the performance of a task relevant to semantics, e.g., semantic role labeling. Sentential Paraphrase Classification The left side of Fig. 2 illustrates the sentential paraphrase classification. The process is simple; the final hidden state of the [CLS] token, i.e., h1 , is fed into a classifier to discriminate whether a sentence pair is a paraphrase or a random sentence combination. Note that these random sentence pairs provide random phrases for the phrasal paraphrase classification described above. 4.2 Training Setting We collected para"
D19-1542,P15-1150,0,0.120018,"Missing"
D19-1542,P18-1042,0,0.180449,"nd the 9th International Joint Conference on Natural Language Processing, pages 5393–5404, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics tified by Arase and Tsujii (2017) to improve semantic equivalent assessment tasks. Specifically, our method learns to discriminate phrasal and sentential paraphrases on top of the representations generated by BERT. This approach explicitly introduces the concept of the phrase to BERT and supervises semantic relations between phrases. Due to studies on sentential paraphrase collection (Lan et al., 2017) and generation (Wieting and Gimpel, 2018), a million-scale paraphrase corpus is ready for use. We empirically show that further training of a pre-trained model on relevant tasks transfers well to downstream tasks of the same kind, which we name as transfer fine-tuning. The contributions of our paper are: • We empirically demonstrate that transfer finetuning using paraphrasal relations allows a smaller BERT to generate representations suitable for semantic equivalence assessment. The generated model exhibits superior performance to the larger BERT while maintaining the small model size. • Our experiments indicate that phrasal paraphra"
D19-1542,N18-1101,0,0.0298888,"semantic equivalence by grading. STS benchmark (STS-B) (Cer et al., 2017) provides sentence pairs drawn from heterogeneous sources, which are human-annotated with a level of equivalence from 1 to 5. NLI Tasks (MNLI-m/mm, RTE, QNLI) We use natural language inference (NLI) tasks to verify hypothesis H3 because they constitute a class of problems relevant to semantic equivalence assessment. NLI tasks are different from semantic equivalence assessment in that they often require logical inference and understanding of commonsense knowledge. The Multi-Genre Natural Language Inference Corpus (MNLI) (Williams et al., 2018) is a crowd-sourced corpus and covers heterogeneous domains. MNLI-m is an in-domain 10 https://gluebenchmark.com/faq https://data.quora.com/ First-Quora-Dataset-Release-Question-Pairs 11 9 Task paraphrase STS paraphrase in-domain NLI cross-domain NLI NLI QA/NLI sentiment acceptability 5398 Task Model BERT-base BERT-large Transfer Fine-Tuning Semantic Equivalence MRPC STS-B QQP 88.3 84.7 71.2 88.6 86.0 72.1 89.2 87.4 71.2 NLI MNLI (m/mm) RTE 84.3/83.0 59.8 86.2/85.5 65.5 83.9/83.1 64.8 QNLI 89.1 92.7 89.3 Single-Sent. SST CoLA 93.3 52.7 94.1 55.7 93.1 47.2 Table 3: GLUE test results scored by t"
D19-1542,D18-1408,0,0.0130384,"equivalence assessment by injecting paraphrasal relations. Liu et al. (2019) showed that BERT’s performance improves when fine-tuning with a multi-task learning setting, which is applicable to our trained model for further improvement. 3 Sentence representation learning is an active research area due to its importance in various downstream tasks. Early studies employed supervised learning where a sentence representation is learned in an end-to-end manner using an annotated corpus. Among these, the importance of phrase structures in representation learning has been discussed (Tai et al., 2015; Wu et al., 2018). In this paper, we use structural relations in sentence pairs for sentence representations. Specifically, we employ phrasal paraphrase relations that introduce the notion of a phrase to the model. 3.1 Background Phrase Alignment for Paraphrases In order to obtain phrasal paraphrases, we used the phrase alignment method proposed in (Arase and Tsujii, 2017) and apply it to our paraphrase corpora. The alignment method aligns phrasal paraphrases on the parse forests of a sentential paraphrase pair as illustrated in Fig. 1. According to the evaluation results reported in (Arase and Tsujii, 2017),"
D19-1542,D17-1001,1,\N,Missing
E03-1047,2000.iwpt-1.9,0,0.0223572,"us heuristic and statistical methods are applicable. Consistency between the grammar rules and the obtained lexical entries is assured independently of the methods of annotation. Lastly, the validity of the grammar theories is evaluated on real-world texts. A degree of low coverage by a linguistically motivated grammar does not necessarily reflect inadequacy of the grammar theories; a lack of appropriate lexical entries may also be responsible. The analysis of obtained grammars gives us grounds for discussing the pros and cons of the theories. The studies on the extraction of LTAG (Xia, 1999; Chen and Vijay-Shanker, 2000; Chiang, 2000) and CCG (Hockenmaier and Steedman, 2002) represent the first attempts at the acquisition of linguistically motivated grammars from annotated corpora. Those studies are limited to specific formalisms, and can be interpreted as instances of our approach as described in Section 3. This paper does not describe any concrete algorithms for grammar acquisition that depend on specific grammar formalisms. The contribution of our work is to formally state the conditions required for the acquisition of lexicalized grammars and to demon127 strate that it can be applied to lexicalized gramm"
E03-1047,P00-1058,0,0.0287483,"ethods are applicable. Consistency between the grammar rules and the obtained lexical entries is assured independently of the methods of annotation. Lastly, the validity of the grammar theories is evaluated on real-world texts. A degree of low coverage by a linguistically motivated grammar does not necessarily reflect inadequacy of the grammar theories; a lack of appropriate lexical entries may also be responsible. The analysis of obtained grammars gives us grounds for discussing the pros and cons of the theories. The studies on the extraction of LTAG (Xia, 1999; Chen and Vijay-Shanker, 2000; Chiang, 2000) and CCG (Hockenmaier and Steedman, 2002) represent the first attempts at the acquisition of linguistically motivated grammars from annotated corpora. Those studies are limited to specific formalisms, and can be interpreted as instances of our approach as described in Section 3. This paper does not describe any concrete algorithms for grammar acquisition that depend on specific grammar formalisms. The contribution of our work is to formally state the conditions required for the acquisition of lexicalized grammars and to demon127 strate that it can be applied to lexicalized grammars other than"
E03-1047,P02-1043,0,0.105742,"Consistency between the grammar rules and the obtained lexical entries is assured independently of the methods of annotation. Lastly, the validity of the grammar theories is evaluated on real-world texts. A degree of low coverage by a linguistically motivated grammar does not necessarily reflect inadequacy of the grammar theories; a lack of appropriate lexical entries may also be responsible. The analysis of obtained grammars gives us grounds for discussing the pros and cons of the theories. The studies on the extraction of LTAG (Xia, 1999; Chen and Vijay-Shanker, 2000; Chiang, 2000) and CCG (Hockenmaier and Steedman, 2002) represent the first attempts at the acquisition of linguistically motivated grammars from annotated corpora. Those studies are limited to specific formalisms, and can be interpreted as instances of our approach as described in Section 3. This paper does not describe any concrete algorithms for grammar acquisition that depend on specific grammar formalisms. The contribution of our work is to formally state the conditions required for the acquisition of lexicalized grammars and to demon127 strate that it can be applied to lexicalized grammars other than LTAG and CCG, such as HPSG. 2 Lexicalized"
E03-1047,H94-1020,0,0.177212,"Missing"
E03-1047,C88-2121,0,0.806242,"T (Japan Science and Technology Corporation) Honcho 4-1-8, Kawaguchi-shi, Saitama 332-0012 JAPAN {yusuke, ninomi , tsuj ii}@is s .u-tokyo ac jp Abstract This paper presents a formalization of automatic grammar acquisition that is based on lexicalized grammar formalisms (e.g. LTAG and HPSG). We state the conditions for the consistent acquisition of a unique lexicalized grammar from an annotated corpus. 1 Introduction Linguistically motivated and computationally oriented grammar theories take the form of lexicalized grammar formalisms; examples include Lexicalized Tree Adjoining Grammar (LTAG) (Schabes et al., 1988), Combinatory Categorial Grammar (Steedman, 2000), and Head-driven Phrase Structure Grammar (HPSG) (Sag and Wasow, 1999). They have been a great success in terms of linguistic analysis and efficiency in the parsing of real-world texts. However, such grammars have not generally been considered suitable for the syntactic analysis within practical NLP systems because considerable effort is required to develop and maintain lexicalized grammars that are both robust and provide broad coverage. One novel approach to grammar development is based on the automatic acquisition of lexicalized grammars fro"
E09-1088,P08-1024,0,0.0372396,"erence (LDI), by systematically combining an efficient search strategy with the dynamic programming. The LDI is an exact inference method producing the most probable label sequence. In addition, we also propose an approximated LDI algorithm for faster speed. We show that the approximated LDI performs as well as the exact one. We will also discuss a post-processing method for the LDI algorithm: the minimum bayesian risk reranking. Introduction When data have distinct sub-structures, models exploiting latent variables are advantageous in learning (Matsuzaki et al., 2005; Petrov and Klein, 2007; Blunsom et al., 2008). Actually, discriminative probabilistic latent variable models (DPLVMs) have recently become popular choices for performing a variety of tasks with sub-structures, e.g., vision recognition (Morency et al., 2007), syntactic parsing (Petrov and Klein, 2008), and syntactic chunking (Sun et al., 2008). Morency et al. (2007) demonstrated that DPLVM models could efficiently learn sub-structures of natural problems, and outperform several widelyused conventional models, e.g., support vector machines (SVMs), conditional random fields (CRFs) The subsequent section describes an overview of DPLVM models"
E09-1088,W04-1213,0,0.0129012,"fine tuning strategy were kept the same. Table 1: Feature templates used in the Bio-NER experiments. wi is the current word, ti is the current POS tag, oi is the orthography mode of the current word, and hi is the current latent variable (for the case of latent models) or the current label (for the case of conventional models). No globally dependent features were used; also, no external resources were used. 4.1 BioNLP/NLPBA-2004 Shared Task (Bio-NER) Our first experiment used the data from the BioNLP/NLPBA-2004 shared task. It is a biomedical named-entity recognition task on the GENIA corpus (Kim et al., 2004). Named entity recognition aims to identify and classify technical terms in a given domain (here, molecular biology) that refer to concepts of interest to domain experts. The training set consists of 2,000 abstracts from MEDLINE; and the evaluation set consists of 404 abstracts from MEDLINE. We divided the original training set into 1,800 abstracts for the training data and 200 abstracts for the development data. The task adopts the BIO encoding scheme, i.e., B-x for words beginning an entity x, I-x for words continuing an entity x, and O for words being outside of all entities. The Bio-NER ta"
E09-1088,P05-1010,1,0.872438,"se a new inference algorithm, latent dynamic inference (LDI), by systematically combining an efficient search strategy with the dynamic programming. The LDI is an exact inference method producing the most probable label sequence. In addition, we also propose an approximated LDI algorithm for faster speed. We show that the approximated LDI performs as well as the exact one. We will also discuss a post-processing method for the LDI algorithm: the minimum bayesian risk reranking. Introduction When data have distinct sub-structures, models exploiting latent variables are advantageous in learning (Matsuzaki et al., 2005; Petrov and Klein, 2007; Blunsom et al., 2008). Actually, discriminative probabilistic latent variable models (DPLVMs) have recently become popular choices for performing a variety of tasks with sub-structures, e.g., vision recognition (Morency et al., 2007), syntactic parsing (Petrov and Klein, 2008), and syntactic chunking (Sun et al., 2008). Morency et al. (2007) demonstrated that DPLVM models could efficiently learn sub-structures of natural problems, and outperform several widelyused conventional models, e.g., support vector machines (SVMs), conditional random fields (CRFs) The subsequen"
E09-1088,P06-1059,1,0.852877,"original training set into 1,800 abstracts for the training data and 200 abstracts for the development data. The task adopts the BIO encoding scheme, i.e., B-x for words beginning an entity x, I-x for words continuing an entity x, and O for words being outside of all entities. The Bio-NER task contains 5 different named entities with 11 BIO encoding labels. The standard evaluation metrics for this task are precision p (the fraction of output entities matching the reference entities), recall r (the fraction of reference entities returned), and the F-measure given by F = 2pr/(p + r). Following Okanohara et al. (2006), we used word features, POS features and orthography features (prefix, postfix, uppercase/lowercase, etc.), as listed in Table 1. However, their globally dependent features, like preceding-entity features, were not used in our system. Also, to speed up the training, features that appeared rarely in the training data were removed. For DPLVM models, we tuned the number of latent variables per label from 2 to 5 on preliminary experiments, and used the Word Features: {wi−2 , wi−1 , wi , wi+1 , wi+2 , wi−1 wi , wi wi+1 } ×{hi , hi−1 hi } Table 2: Feature templates used in the NPchunking experiment"
E09-1088,N07-1051,0,0.05382,"ithm, latent dynamic inference (LDI), by systematically combining an efficient search strategy with the dynamic programming. The LDI is an exact inference method producing the most probable label sequence. In addition, we also propose an approximated LDI algorithm for faster speed. We show that the approximated LDI performs as well as the exact one. We will also discuss a post-processing method for the LDI algorithm: the minimum bayesian risk reranking. Introduction When data have distinct sub-structures, models exploiting latent variables are advantageous in learning (Matsuzaki et al., 2005; Petrov and Klein, 2007; Blunsom et al., 2008). Actually, discriminative probabilistic latent variable models (DPLVMs) have recently become popular choices for performing a variety of tasks with sub-structures, e.g., vision recognition (Morency et al., 2007), syntactic parsing (Petrov and Klein, 2008), and syntactic chunking (Sun et al., 2008). Morency et al. (2007) demonstrated that DPLVM models could efficiently learn sub-structures of natural problems, and outperform several widelyused conventional models, e.g., support vector machines (SVMs), conditional random fields (CRFs) The subsequent section describes an o"
E09-1088,W00-0726,0,0.0189397,"ime(Ks) only poor features available. For example, in POStagging task and Chinese/Japanese word segmentation task, there are only word features available. For this reason, it is necessary to check the performance of the LDI on poor feature-set. We chose another popular task, the NP-chunking, for this study. Here, we used only poor feature-set, i.e., feature templates that depend only on words (see Table 2 for details), taking into account 200K features. No external resources were used. The NP-chunking data was extracted from the training/test data of the CoNLL-2000 shallowparsing shared task (Sang and Buchholz, 2000). In this task, the non-recursive cores of noun phrases called base NPs are identified. The training set consists of 8,936 sentences, and the test set consists of 2,012 sentences. Our preliminary experiments in this task suggested the use of 5 latent variables for each label on latent models. 70 70 69 69 68 #latent-path Table 3: On the test data of the Bio-NER task, experimental comparisons among various inference algorithms on DPLVMs, and the performance of CRFs. S.A. signifies sentence accuracy. As can be seen, at a much lower cost, the LDI-A (A signifies approximation) performed slightly be"
E09-1088,N03-1028,0,0.0338243,", ti−1 ti ti+1 , ti ti+1 ti+2 } ×{hi , hi−1 hi } Orth. Features: {oi−2 , oi−1 , oi , oi+1 , oi+2 , oi−2 oi−1 , oi−1 oi , oi oi+1 , oi+1 oi+2 } ×{hi , hi−1 hi } The training stage was kept the same as Morency et al. (2007). In other words, there is no need to change the conventional parameter estimation method on DPLVM models for adapting the various inference algorithms in this paper. For more information on training DPLVMs, refer to Morency et al. (2007) and Petrov and Klein (2008). Since the CRF model is one of the most successful models in sequential labeling tasks (Lafferty et al., 2001; Sha and Pereira, 2003), in this paper, we choosed CRFs as a baseline model for the comparison. Note that the feature sets were kept the same in DPLVMs and CRFs. Also, the optimizer and fine tuning strategy were kept the same. Table 1: Feature templates used in the Bio-NER experiments. wi is the current word, ti is the current POS tag, oi is the orthography mode of the current word, and hi is the current latent variable (for the case of latent models) or the current label (for the case of conventional models). No globally dependent features were used; also, no external resources were used. 4.1 BioNLP/NLPBA-2004 Shar"
E09-1088,C08-1106,1,0.923339,"ables: An Exact Inference Algorithm and Its Efficient Approximation Xu Sun† Jun’ichi Tsujii†‡§ † Department of Computer Science, University of Tokyo, Japan ‡ School of Computer Science, University of Manchester, UK § National Centre for Text Mining, Manchester, UK {sunxu, tsujii}@is.s.u-tokyo.ac.jp Abstract and hidden Markov models (HMMs). Petrov and Klein (2008) reported on a syntactic parsing task that DPLVM models can learn more compact and accurate grammars than the conventional techniques without latent variables. The effectiveness of DPLVMs was also shown on a syntactic chunking task by Sun et al. (2008). Latent conditional models have become popular recently in both natural language processing and vision processing communities. However, establishing an effective and efficient inference method on latent conditional models remains a question. In this paper, we describe the latent-dynamic inference (LDI), which is able to produce the optimal label sequence on latent conditional models by using efficient search strategy and dynamic programming. Furthermore, we describe a straightforward solution on approximating the LDI, and show that the approximated LDI performs as well as the exact LDI, while"
E09-1088,W02-1019,0,\N,Missing
E09-1090,W06-2920,0,0.0543335,"Missing"
E09-1090,P05-1022,0,0.749547,"obstacles that discourage the use of full parsing in large-scale natural language processing applications is its computational cost. For example, the MEDLINE corpus, a collection of abstracts of biomedical papers, consists of 70 million sentences and would require more than two years of processing time if the parser needs one second to process a sentence. Generative models based on lexicalized PCFGs enjoyed great success as the machine learning framework for full parsing (Collins, 1999; Charniak, 2000), but recently discriminative models attract more attention due to their superior accuracy (Charniak and Johnson, 2005; Huang, 2008) Proceedings of the 12th Conference of the European Chapter of the ACL, pages 790–798, c Athens, Greece, 30 March – 3 April 2009. 2009 Association for Computational Linguistics 790 NP VBN VP QP NN VBD DT JJ CD CD NNS . NP VBD NP . ounces . Estimated volume was a light 2.4 million ounces . volume was Figure 1: Chunking, the first (base) level. Figure 3: Chunking, the 3rd level. NP S NP VBD DT JJ volume was a light QP million NNS . ounces . Figure 2: Chunking, the 2nd level. NP VP . volume was . Figure 4: Chunking, the 4th level. chain CRF model to perform chunking. Although our pa"
E09-1090,A00-2018,0,0.0635557,"Missing"
E09-1090,C04-1041,0,0.0107461,"of history-based approaches, it is one step closer to the whole-sentence approaches because the parser uses a whole-sequence model (i.e. CRFs) for individual chunking tasks. In other words, our parser could be located somewhere between traditional history-based approaches and whole-sentence approaches. One of our motivations for this work was that our parsing model may achieve a better balance between accuracy and speed than existing parsers. It is also worth mentioning that our approach is similar in spirit to supertagging for parsing with lexicalized grammar formalisms such as CCG and HPSG (Clark and Curran, 2004; Ninomiya et al., 2006), in which significant speed-ups for parsing time are achieved. In this paper, we show that our approach is indeed appealing in that the parser runs very fast and gives competitive accuracy. We evaluate our parser on the standard data set for parsing experiments (i.e. the Penn Treebank) and compare it with existing approaches to full parsing. This paper is organized as follows. Section 2 presents the overall chunk parsing strategy. Section 3 describes the CRF model used to perform individual chunking steps. Section 4 describes the depth-first algorithm for finding the b"
E09-1090,P07-1104,0,0.0328599,"a sentence. We examined the distribution of the heights of the trees in sections 2-21 of the Wall Street Journal (WSJ) corpus. The result is shown in Figure 5. Most of the sentences have less than 20 levels. The average was 10.0, which means we need to perform, on average, 10 chunking tasks to obtain a full parse tree for a sentence if the parsing is performed in a deterministic manner. 3 T X K X where R(λ) is introduced for the purpose of regularization which prevents the model from overfitting the training data. The L1 or L2 norm is commonly used in statistical natural language processing (Gao et al., 2007). We used L1-regularization, which is defined as R(λ) = Chunking with CRFs K 1 X |λk |, C k=1 where C is the meta-parameter that controls the degree of regularization. We used the OWL-QN algorithm (Andrew and Gao, 2007) to obtain the parameters that maximize the L1-regularized conditional log-likelihood. The accuracy of chunk parsing is highly dependent on the accuracy of each level of chunking. This section describes our approach to the chunking task. A common approach to the chunking problem is to convert the problem into a sequence tagging task by using the “BIO” (B for beginning, I for ins"
E09-1090,P08-1067,0,0.269083,"ms. Ratnaparkhi (1997) performs full parsing in a bottom-up and left-toright manner and uses a maximum entropy classifier to make decisions to construct individual phrases. Sagae and Lavie (2006) use the shiftreduce parsing framework and a maximum entropy model for local classification to decide parsing actions. These approaches are often called history-based approaches. A more recent approach to discriminative full parsing is to treat the task as a single structured prediction problem. Finkel et al. (2008) incorporated rich local features into a tree CRF model and built a competitive parser. Huang (2008) proposed to use a parse forest to incorporate non-local features. They used a perceptron algorithm to optimize the weights of the features and achieved state-of-the-art accuracy. Petrov and Klein (2008) introduced latent variables in tree CRFs and proposed a caching mechanism to speed up the computation. In general, the latter whole-sentence approaches give better accuracy than history-based approaches because they can better trade off decisions made in different parts in a parse tree. However, the whole-sentence approaches tend to require a large computational cost both in training and parsi"
E09-1090,P95-1037,0,0.0328373,"recovered from the chunking history in a straightforward way. This idea of converting full parsing into a series of chunking tasks is not new by any means— the history of this kind of approach dates back to 1950s (Joshi and Hopely, 1996). More recently, Brants (1999) used a cascaded Markov model to parse German text. Tjong Kim Sang (2001) used the IOB tagging method to represent chunks and memory-based learning, and achieved an f-score of 80.49 on the WSJ corpus. Tsuruoka and Tsujii (2005) improved upon their approach by using 1 The head word is identified by using the headpercolation table (Magerman, 1995). 791 # sentences 5000 3.1 Linear Chain CRFs 4000 A linear chain CRF defines a single log-linear probabilistic distribution over all possible tag sequences y for the input sequence x: 3000 K T X X 1 λk fk (t, yt , yt−1 , x), exp p(y|x) = Z(x) t=1 k=1 2000 1000 where fk (t, yt , yt−1 , x) is typically a binary function indicating the presence of feature k, λk is the weight of the feature, and Z(X) is a normalization function: 0 0 5 10 15 Height 20 25 30 Figure 5: Distribution of tree height in WSJ sections 2-21. Z(x) = X exp y a maximum entropy classifier and achieved an fscore of 85.9. However"
E09-1090,N06-1020,0,0.0327579,"porate some useful restrictions in producing chunking hypotheses. For example, we could naturally incorporate the restriction that every chunk has to contain at least one symbol that has just been created in the previous level3 . It is hard for the normal CRF model to incorporate such restrictions. Introducing latent variables into the CRF model may be another promising approach. This is the main idea of Petrov and Klein (2008), which significantly improved parsing accuracy. A totally different approach to improving the accuracy of our parser is to use the idea of “selftraining” described in (McClosky et al., 2006). The basic idea is to create a larger set of training data by applying an accurate parser (e.g. reranking parser) to a large amount of raw text. We can then use the automatically created treebank as the additional training data for our parser. This approach suggests that accurate (but slow) parsers and fast (but not-so-accurate) parsers can actually help each other. Also, since it is not difficult to extend our parser to produce N-best parsing hypotheses, one could build a fast reranking parser by using the parser as the base (hypotheses generating) parser. from the history-based model’s inab"
E09-1090,P08-1006,1,0.693944,"computed as the product of the probabilities of individual chunking results. The parsing is performed in a bottom-up manner and the best derivation is efficiently obtained by using a depthfirst search algorithm. Experimental results demonstrate that this simple parsing framework produces a fast and reasonably accurate parser. 1 Introduction Full parsing analyzes the phrase structure of a sentence and provides useful input for many kinds of high-level natural language processing such as summarization (Knight and Marcu, 2000), pronoun resolution (Yang et al., 2006), and information extraction (Miyao et al., 2008). One of the major obstacles that discourage the use of full parsing in large-scale natural language processing applications is its computational cost. For example, the MEDLINE corpus, a collection of abstracts of biomedical papers, consists of 70 million sentences and would require more than two years of processing time if the parser needs one second to process a sentence. Generative models based on lexicalized PCFGs enjoyed great success as the machine learning framework for full parsing (Collins, 1999; Charniak, 2000), but recently discriminative models attract more attention due to their s"
E09-1090,W06-1619,1,0.941698,"University of Manchester, UK ‡ National Centre for Text Mining (NaCTeM), UK ∗ Department of Computer Science, University of Tokyo, Japan {yoshimasa.tsuruoka,j.tsujii,sophia.ananiadou}@manchester.ac.uk Abstract and adaptability to new grammars and languages (Buchholz and Marsi, 2006). A traditional approach to discriminative full parsing is to convert a full parsing task into a series of classification problems. Ratnaparkhi (1997) performs full parsing in a bottom-up and left-toright manner and uses a maximum entropy classifier to make decisions to construct individual phrases. Sagae and Lavie (2006) use the shiftreduce parsing framework and a maximum entropy model for local classification to decide parsing actions. These approaches are often called history-based approaches. A more recent approach to discriminative full parsing is to treat the task as a single structured prediction problem. Finkel et al. (2008) incorporated rich local features into a tree CRF model and built a competitive parser. Huang (2008) proposed to use a parse forest to incorporate non-local features. They used a perceptron algorithm to optimize the weights of the features and achieved state-of-the-art accuracy. Pet"
E09-1090,W97-0301,0,0.205987,"Comparison with Previous Work Table 6 shows the performance of our parser on the test data and summarizes the results of previous work. Our parser achieved an f-score of 88.4 on the test data, which is comparable to the accuracy achieved by recent discriminative approaches such as Finkel et al. (2008) and Petrov & Klein (2008), but is not as high as the state-of-the-art accuracy achieved by the parsers that can incorporate global features such as Huang (2008) and Charniak & Johnson (2005). Our parser was more accurate than traditional history-based approaches such as Sagae & Lavie (2006) and Ratnaparkhi (1997), and was significantly better than previous cascaded chunking approaches such as Tsuruoka & Tsujii (2005) and Tjong Kim Sang (2001). Although the comparison presented in the table is not perfectly fair because of the differences in hardware platforms, the results show that our parsing model is a promising addition to the parsing frameworks for building a fast and accurate parser. 8 Conclusion 7 Discussion Although the idea of treating full parsing as a series of chunking problems has a long history, there has not been a competitive parser based on this parsing framework. In this paper, we hav"
E09-1090,P06-2089,0,0.308636,"mputer Science, University of Manchester, UK ‡ National Centre for Text Mining (NaCTeM), UK ∗ Department of Computer Science, University of Tokyo, Japan {yoshimasa.tsuruoka,j.tsujii,sophia.ananiadou}@manchester.ac.uk Abstract and adaptability to new grammars and languages (Buchholz and Marsi, 2006). A traditional approach to discriminative full parsing is to convert a full parsing task into a series of classification problems. Ratnaparkhi (1997) performs full parsing in a bottom-up and left-toright manner and uses a maximum entropy classifier to make decisions to construct individual phrases. Sagae and Lavie (2006) use the shiftreduce parsing framework and a maximum entropy model for local classification to decide parsing actions. These approaches are often called history-based approaches. A more recent approach to discriminative full parsing is to treat the task as a single structured prediction problem. Finkel et al. (2008) incorporated rich local features into a tree CRF model and built a competitive parser. Huang (2008) proposed to use a parse forest to incorporate non-local features. They used a perceptron algorithm to optimize the weights of the features and achieved state-of-the-art accuracy. Pet"
E09-1090,N03-1028,0,0.504054,"is highly dependent on the accuracy of each level of chunking. This section describes our approach to the chunking task. A common approach to the chunking problem is to convert the problem into a sequence tagging task by using the “BIO” (B for beginning, I for inside, and O for outside) representation. For example, the chunking process given in Figure 1 is expressed as the following BIO sequences. 3.2 Features Table 1 shows the features used in chunking for the base level. Since the task is basically identical to shallow parsing by CRFs, we follow the feature sets used in the previous work by Sha and Pereira (2003). We use unigrams, bigrams, and trigrams of part-of-speech (POS) tags and words. The difference between our CRF chunker and that in (Sha and Pereira, 2003) is that we could not use second-order CRF models, hence we could not use trigram features on the BIO states. We B-NP I-NP O O O B-QP I-QP O O This representation enables us to use the linearchain CRF model to perform chunking, since the task is simply assigning appropriate labels to a sequence. 792 Symbol Unigrams Symbol Bigrams Symbol Trigrams Word Unigrams Word Bigrams Word Trigrams s−2 , s−1 , s0 , s+1 , s+2 s−2 s−1 , s−1 s0 , s0 s+1 , s"
E09-1090,W05-1514,1,0.908906,", and converts it into NP. This process is repeated until the whole sentence is chunked at the fourth level. The full parse tree is recovered from the chunking history in a straightforward way. This idea of converting full parsing into a series of chunking tasks is not new by any means— the history of this kind of approach dates back to 1950s (Joshi and Hopely, 1996). More recently, Brants (1999) used a cascaded Markov model to parse German text. Tjong Kim Sang (2001) used the IOB tagging method to represent chunks and memory-based learning, and achieved an f-score of 80.49 on the WSJ corpus. Tsuruoka and Tsujii (2005) improved upon their approach by using 1 The head word is identified by using the headpercolation table (Magerman, 1995). 791 # sentences 5000 3.1 Linear Chain CRFs 4000 A linear chain CRF defines a single log-linear probabilistic distribution over all possible tag sequences y for the input sequence x: 3000 K T X X 1 λk fk (t, yt , yt−1 , x), exp p(y|x) = Z(x) t=1 k=1 2000 1000 where fk (t, yt , yt−1 , x) is typically a binary function indicating the presence of feature k, λk is the weight of the feature, and Z(X) is a normalization function: 0 0 5 10 15 Height 20 25 30 Figure 5: Distribution"
E09-1090,P06-1006,0,0.0111322,"king. The probability of an entire parse tree is computed as the product of the probabilities of individual chunking results. The parsing is performed in a bottom-up manner and the best derivation is efficiently obtained by using a depthfirst search algorithm. Experimental results demonstrate that this simple parsing framework produces a fast and reasonably accurate parser. 1 Introduction Full parsing analyzes the phrase structure of a sentence and provides useful input for many kinds of high-level natural language processing such as summarization (Knight and Marcu, 2000), pronoun resolution (Yang et al., 2006), and information extraction (Miyao et al., 2008). One of the major obstacles that discourage the use of full parsing in large-scale natural language processing applications is its computational cost. For example, the MEDLINE corpus, a collection of abstracts of biomedical papers, consists of 70 million sentences and would require more than two years of processing time if the parser needs one second to process a sentence. Generative models based on lexicalized PCFGs enjoyed great success as the machine learning framework for full parsing (Collins, 1999; Charniak, 2000), but recently discrimina"
E09-1090,E99-1016,0,\N,Missing
E09-1090,J03-4003,0,\N,Missing
E09-1090,P08-1109,0,\N,Missing
E12-1044,P09-1109,0,0.377853,"ation disambiguation with an existing parsing model, our approach resembles the approach by Hogan (2007). She detected noun phrase coordinations by finding symmetry in conjunct structure and the dependency between the lexical heads of the conjuncts. They are used to rerank the n-best outputs of the Bikel parser (2004), whereas two models interact with each other in our method. Shimbo and Hara (2007) proposed an alignment-based method for detecting and disambiguating non-nested coordination structures. They disambiguated coordination structures based on the edit distance between two conjuncts. Hara et al. (2009) extended the method, dealing with nested coordinations as well. We used their method as one of the two sub-models. 3 Background 3.1 Coordination structure analysis with alignment-based local features Coordination structure analysis with alignmentbased local features (Hara et al., 2009) is a hybrid approach to coordination disambiguation that combines a simple grammar to ensure consistent global structure of coordinations in a sentence, and features based on sequence alignment to capture local symmetry of conjuncts. In this section, we describe the method briefly. A sentence is denoted by x ="
E12-1044,P07-1086,0,0.435112,"s paper is as follows. First, we describe three basic methods required in the technique we propose: 1) coordination structure analysis with alignment-based local features, 2) HPSG parsing, and 3) dual decomposition. Finally, we show experimental results that demonstrate the effectiveness of our approach. We compare three methods: coordination structure analysis with alignment-based local features, HPSG parsing, and the dual-decomposition-based approach that combines both. 2 Related Work Many previous studies for coordination disambiguation have focused on a particular type of NP coordination (Hogan, 2007). Resnik (1999) disambiguated coordination structures by using semantic similarity of the conjuncts in a taxonomy. He dealt with two kinds of patterns, [n0 n1 and n2 n3 ] and [n1 and n2 n3 ], where ni are all nouns. He detected coordination structures based on similarity of form, meaning and conceptual association between n1 and n2 and between n1 and n3 . Nakov and Hearst (2005) used the Web as a training set and applied it to a task that is similar to Resnik’s. In terms of integrating coordination disambiguation with an existing parsing model, our approach resembles the approach by Hogan (200"
E12-1044,J93-2004,0,0.0429474,"Missing"
E12-1044,C04-1204,1,0.84166,"disambiguation remains a difficult sub-problem in parsing, even for state-of-the-art parsers. One approach to solve this problem is a grammatical approach. This approach, however, often fails in noun and adjective coordinations because there are many possible structures in these coordinations that are grammatically correct. For example, a noun sequence of the form “n0 n1 and n2 n3 ” has as many as five possible structures (Resnik, 1999). Therefore, a grammatical approach is not sufficient to disambiguate coordination structures. In fact, the Stanford parser (Klein and Manning, 2003) and Enju (Miyao and Tsujii, 2004) fail to disambiguate a sentence I am a freshman advertising and marketing major. Table 1 shows the output from them and the correct coordination structure. The coordination structure above is obvious to humans because there is a symmetry of conjuncts (-ing) in the sentence. Coordination structures often have such structural and semantic symmetry of conjuncts. One approach is to capture local symmetry of conjuncts. However, this approach fails in VP and sentential coordinations, which can easily be detected by a grammatical approach. This is because conjuncts in these coordinations do not nece"
E12-1044,J08-1002,1,0.887845,"Missing"
E12-1044,H05-1105,0,0.0244554,"lignment-based local features, HPSG parsing, and the dual-decomposition-based approach that combines both. 2 Related Work Many previous studies for coordination disambiguation have focused on a particular type of NP coordination (Hogan, 2007). Resnik (1999) disambiguated coordination structures by using semantic similarity of the conjuncts in a taxonomy. He dealt with two kinds of patterns, [n0 n1 and n2 n3 ] and [n1 and n2 n3 ], where ni are all nouns. He detected coordination structures based on similarity of form, meaning and conceptual association between n1 and n2 and between n1 and n3 . Nakov and Hearst (2005) used the Web as a training set and applied it to a task that is similar to Resnik’s. In terms of integrating coordination disambiguation with an existing parsing model, our approach resembles the approach by Hogan (2007). She detected noun phrase coordinations by finding symmetry in conjunct structure and the dependency between the lexical heads of the conjuncts. They are used to rerank the n-best outputs of the Bikel parser (2004), whereas two models interact with each other in our method. Shimbo and Hara (2007) proposed an alignment-based method for detecting and disambiguating non-nested c"
E12-1044,D10-1001,0,0.14598,"pty SUBJ feature. When the corpus is annoThe sign of the tated with least these offeatures, the lexical Figure 3: at Construction coordination in Enjuenpeatedly applytries required to explain the sentence are uniquely ns. Finally, the determined. In this study, we define partiallyinto efficiently solvable sub-problems. is output on the composed specified derivation trees as tree structures annoIttated is becoming popular in the NLP community with schema names and HPSG signs includand been shown to effectively inghas the specifications of work the above features.on seve Penn eral We NLP tasks (Rush et al., 2010). describe the process of grammar developWe consider an optimization problem ment in terms of the four phases: specification, externalization, extraction, and verification. rammar develarg max(f (x) + g(x)) (2) o be annotated x 3.1 Specification ons, and ii) adwhich is difficult to solve (e.g. NP-hard), while General grammatical constraints are defined in grammar rules arg maxx f (x) and arg maxx g(x) are effectively this phase, and in HPSG, they are represented history of rule In dual decomposition, solve Figthrough the design of the sign andwe schemata. tree annotated solvable. uremin 1 show"
E12-1044,D07-1064,0,0.382384,"m, meaning and conceptual association between n1 and n2 and between n1 and n3 . Nakov and Hearst (2005) used the Web as a training set and applied it to a task that is similar to Resnik’s. In terms of integrating coordination disambiguation with an existing parsing model, our approach resembles the approach by Hogan (2007). She detected noun phrase coordinations by finding symmetry in conjunct structure and the dependency between the lexical heads of the conjuncts. They are used to rerank the n-best outputs of the Bikel parser (2004), whereas two models interact with each other in our method. Shimbo and Hara (2007) proposed an alignment-based method for detecting and disambiguating non-nested coordination structures. They disambiguated coordination structures based on the edit distance between two conjuncts. Hara et al. (2009) extended the method, dealing with nested coordinations as well. We used their method as one of the two sub-models. 3 Background 3.1 Coordination structure analysis with alignment-based local features Coordination structure analysis with alignmentbased local features (Hara et al., 2009) is a hybrid approach to coordination disambiguation that combines a simple grammar to ensure con"
E12-2021,W05-0620,0,0.0614673,"Missing"
E12-2021,doddington-etal-2004-automatic,0,0.0628567,"an be set up to support most text annotation tasks. The most basic annotation primitive identifies a text span and assigns it a type (or tag or label), marking for e.g. POS-tagged tokens, chunks or entity mentions (Figure 1 top). These base annotations can be connected by binary relations – either directed or undirected – which can be configured for e.g. simple relation extraction, or verb frame annotation BRAT (Figure 1 middle and bottom). n-ary associations of annotations are also supported, allowing the annotation of event structures such as those targeted in the MUC (Sundheim, 1996), ACE (Doddington et al., 2004), and BioNLP (Kim et al., 2011) Information Extraction (IE) tasks (Figure 2). Additional aspects of annotations can be marked using attributes, binary or multi-valued flags that can be added to other annotations. Finally, annotators can attach free-form text notes to any annotation. In addition to information extraction tasks, these annotation primitives allow BRAT to be configured for use in various other tasks, such as chunking (Abney, 1991), Semantic Role Labeling (Gildea and Jurafsky, 2002; Carreras and M`arquez, 2005), and dependency annotation (Nivre, 2003) (See Figure 1 for examples). F"
E12-2021,J02-3001,0,0.0249721,", allowing the annotation of event structures such as those targeted in the MUC (Sundheim, 1996), ACE (Doddington et al., 2004), and BioNLP (Kim et al., 2011) Information Extraction (IE) tasks (Figure 2). Additional aspects of annotations can be marked using attributes, binary or multi-valued flags that can be added to other annotations. Finally, annotators can attach free-form text notes to any annotation. In addition to information extraction tasks, these annotation primitives allow BRAT to be configured for use in various other tasks, such as chunking (Abney, 1991), Semantic Role Labeling (Gildea and Jurafsky, 2002; Carreras and M`arquez, 2005), and dependency annotation (Nivre, 2003) (See Figure 1 for examples). Further, both the BRAT client and server implement full support for the Unicode standard, which allow the tool to support the annotation of text using e.g. Chinese or Devan¯agar¯ı characters. BRAT is distributed with examples from over 20 corpora for a variety of tasks, involving texts in seven different languages and including examples from corpora such as those introduced for the CoNLL shared tasks on language-independent named entity recognition (Tjong Kim Sang and De Meulder, 2003) and mult"
E12-2021,W11-1801,1,0.354037,"Missing"
E12-2021,W03-3017,0,0.0113928,"dheim, 1996), ACE (Doddington et al., 2004), and BioNLP (Kim et al., 2011) Information Extraction (IE) tasks (Figure 2). Additional aspects of annotations can be marked using attributes, binary or multi-valued flags that can be added to other annotations. Finally, annotators can attach free-form text notes to any annotation. In addition to information extraction tasks, these annotation primitives allow BRAT to be configured for use in various other tasks, such as chunking (Abney, 1991), Semantic Role Labeling (Gildea and Jurafsky, 2002; Carreras and M`arquez, 2005), and dependency annotation (Nivre, 2003) (See Figure 1 for examples). Further, both the BRAT client and server implement full support for the Unicode standard, which allow the tool to support the annotation of text using e.g. Chinese or Devan¯agar¯ı characters. BRAT is distributed with examples from over 20 corpora for a variety of tasks, involving texts in seven different languages and including examples from corpora such as those introduced for the CoNLL shared tasks on language-independent named entity recognition (Tjong Kim Sang and De Meulder, 2003) and multilingual dependency parsing (Buchholz and Marsi, 2006). BRAT also imple"
E12-2021,N06-4006,0,0.00738235,"the semantic class disambiguation component (Stenetorp et al., 2011a). Although further research is needed to establish the benefits of this approach in various annotation tasks, we view the results of this initial experiment as promising regarding the potential of our approach to using machine learning to support annotation efforts. 5 informed by experience from several annotation tasks and research efforts spanning more than a decade. A variety of previously introduced annotation tools and approaches also served to guide our design decisions, including the fast annotation mode of Knowtator (Ogren, 2006), the search capabilities of the XConc tool (Kim et al., 2008), and the design of web-based systems such as MyMiner (Salgado et al., 2010), and GATE Teamware (Cunningham et al., 2011). Using machine learning to accelerate annotation by supporting human judgements is well documented in the literature for tasks such as entity annotation (Tsuruoka et al., 2008) and translation (Mart´ınezG´omez et al., 2011), efforts which served as inspiration for our own approach. BRAT , along with conversion tools and extensive documentation, is freely available under the open-source MIT license from its homepa"
E12-2021,W11-1816,1,0.207231,"Missing"
E12-2021,X96-1048,0,0.137156,"lly configurable and can be set up to support most text annotation tasks. The most basic annotation primitive identifies a text span and assigns it a type (or tag or label), marking for e.g. POS-tagged tokens, chunks or entity mentions (Figure 1 top). These base annotations can be connected by binary relations – either directed or undirected – which can be configured for e.g. simple relation extraction, or verb frame annotation BRAT (Figure 1 middle and bottom). n-ary associations of annotations are also supported, allowing the annotation of event structures such as those targeted in the MUC (Sundheim, 1996), ACE (Doddington et al., 2004), and BioNLP (Kim et al., 2011) Information Extraction (IE) tasks (Figure 2). Additional aspects of annotations can be marked using attributes, binary or multi-valued flags that can be added to other annotations. Finally, annotators can attach free-form text notes to any annotation. In addition to information extraction tasks, these annotation primitives allow BRAT to be configured for use in various other tasks, such as chunking (Abney, 1991), Semantic Role Labeling (Gildea and Jurafsky, 2002; Carreras and M`arquez, 2005), and dependency annotation (Nivre, 2003)"
E12-2021,W08-0605,1,0.44524,"experience from several annotation tasks and research efforts spanning more than a decade. A variety of previously introduced annotation tools and approaches also served to guide our design decisions, including the fast annotation mode of Knowtator (Ogren, 2006), the search capabilities of the XConc tool (Kim et al., 2008), and the design of web-based systems such as MyMiner (Salgado et al., 2010), and GATE Teamware (Cunningham et al., 2011). Using machine learning to accelerate annotation by supporting human judgements is well documented in the literature for tasks such as entity annotation (Tsuruoka et al., 2008) and translation (Mart´ınezG´omez et al., 2011), efforts which served as inspiration for our own approach. BRAT , along with conversion tools and extensive documentation, is freely available under the open-source MIT license from its homepage at http://brat.nlplab.org Acknowledgements The authors would like to thank early adopters of BRAT who have provided us with extensive feedback and feature suggestions. This work was supported by Grant-in-Aid for Specially Promoted Research (MEXT, Japan), the UK Biotechnology and Biological Sciences Research Council (BBSRC) under project Automated Biologic"
E12-2021,W06-2920,0,\N,Missing
E12-2021,W03-0419,0,\N,Missing
E12-2021,P05-1013,0,\N,Missing
E14-4022,C10-1003,1,0.850791,"extracted from comparable corpora can be used to dynamically augment an SMT system in order to better translate Out-of-Vocabulary (OOV) terms. are created by randomly matching non-translation pairs of terms. We used an equal number of positive and negative instances for training the model. Starting from 20, 000 translation pairs we generated a training dataset of 40, 000 positive and negative instances. 2 The context projection method was first proposed by (Fung and Yee, 1998; Rapp, 1999) and since then different variations have been suggested (Chiao and Zweigenbaum, 2002; Morin et al., 2007; Andrade et al., 2010; Morin and Prochasson, 2011). Our implementation more closely follows the context vector method introduced by (Morin and Prochasson, 2011). As a preprocessing step, stop words are removed using an online list 2 and lemmatisation is performed using TreeTagger (Schmid, 1994) on both the English and Spanish part of the comparable corpus. Afterwards, the method proceeds in three steps. Firstly, for each source and target term of the comparable corpus, i.e., i, we collect all lexical units that: (a) occur within a window of 3 words around i (a seven-word window) and (b) are listed in the seed bili"
E14-4022,P07-1084,0,0.555352,"w that dictionaries extracted from comparable corpora can be used to dynamically augment an SMT system in order to better translate Out-of-Vocabulary (OOV) terms. are created by randomly matching non-translation pairs of terms. We used an equal number of positive and negative instances for training the model. Starting from 20, 000 translation pairs we generated a training dataset of 40, 000 positive and negative instances. 2 The context projection method was first proposed by (Fung and Yee, 1998; Rapp, 1999) and since then different variations have been suggested (Chiao and Zweigenbaum, 2002; Morin et al., 2007; Andrade et al., 2010; Morin and Prochasson, 2011). Our implementation more closely follows the context vector method introduced by (Morin and Prochasson, 2011). As a preprocessing step, stop words are removed using an online list 2 and lemmatisation is performed using TreeTagger (Schmid, 1994) on both the English and Spanish part of the comparable corpus. Afterwards, the method proceeds in three steps. Firstly, for each source and target term of the comparable corpus, i.e., i, we collect all lexical units that: (a) occur within a window of 3 words around i (a seven-word window) and (b) are l"
E14-4022,J03-1002,0,0.00526689,"ora. We evaluate the RF classifier against a popular term alignment method, namely context vectors, and we report an improvement of the translation accuracy. As an application, we use the automatically extracted dictionary in combination with a trained Statistical Machine Translation (SMT) system to more accurately translate unknown terms. The dictionary extraction method described in this paper is freely available 1 . 1 Background Bilingual dictionaries of technical terms are important resources for many Natural Language Processing (NLP) tasks including Statistical Machine Translation (SMT) (Och and Ney, 2003) and Cross-Language Information Retrieval (Ballesteros and Croft, 1997). However, manually creating and updating such resources is an expensive process. In addition to this, new terms are constantly emerging. Especially in the biomedical domain, which is the focus of this work, there is a vast number of neologisms, i.e., newly coined terms, (Pustejovsky et al., 2001). Early work on bilingual lexicon extraction focused on clean, parallel corpora providing satisfactory results (Melamed, 1997; Kay and R¨oscheisen, 1993). However, parallel corpora are expensive to construct and for some domains an"
E14-4022,P02-1040,0,0.0886851,"serve in Table 3, a strong language model can more accurately select the correct translation among top-k candidates. The dictionary extracted by the RF improved the translation performance by 2.5 BLEU points for the top-10 candidates and context vectors by 0.45 for the top-20 candidates. Results We evaluated the translation performance of the SMT that uses the dictionary extracted by the RF against the following baselines: (i) Moses using only the training parallel data (Moses), (ii) Moses using the dictionary extracted by context vectors (Moses+context vector). The evaluation metric is BLEU (Papineni et al., 2002). Table 2 shows the BLEU score achieved by the SMT systems when we append the top-k translations to the phrase table. Moses Moses+ RF Moses+ Context Vectors BLEU on top-k translations 1 10 24.22 24.22 20 24.22 25.32 24.626 24.42 23.88 23.69 23.74 Moses Moses+ RF Moses+ Context Vectors BLEU on top-k translations 1 10 28.85 28.85 20 28.85 30.98 31.35 31.2 28.18 29.17 29.3 Table 3: Translation performance when adding top-k translations to the phrase table. SMT systems use a language model trained on training and test Spanish sentences of the parallel corpus. Table 2: Translation performance when"
E14-4022,C02-2020,0,0.65547,"cancer”. Furthermore, we show that dictionaries extracted from comparable corpora can be used to dynamically augment an SMT system in order to better translate Out-of-Vocabulary (OOV) terms. are created by randomly matching non-translation pairs of terms. We used an equal number of positive and negative instances for training the model. Starting from 20, 000 translation pairs we generated a training dataset of 40, 000 positive and negative instances. 2 The context projection method was first proposed by (Fung and Yee, 1998; Rapp, 1999) and since then different variations have been suggested (Chiao and Zweigenbaum, 2002; Morin et al., 2007; Andrade et al., 2010; Morin and Prochasson, 2011). Our implementation more closely follows the context vector method introduced by (Morin and Prochasson, 2011). As a preprocessing step, stop words are removed using an online list 2 and lemmatisation is performed using TreeTagger (Schmid, 1994) on both the English and Spanish part of the comparable corpus. Afterwards, the method proceeds in three steps. Firstly, for each source and target term of the comparable corpus, i.e., i, we collect all lexical units that: (a) occur within a window of 3 words around i (a seven-word w"
E14-4022,P98-1069,0,0.750459,"arable corpus of Wikipedia articles that are related to the medical sub-domain of “breast cancer”. Furthermore, we show that dictionaries extracted from comparable corpora can be used to dynamically augment an SMT system in order to better translate Out-of-Vocabulary (OOV) terms. are created by randomly matching non-translation pairs of terms. We used an equal number of positive and negative instances for training the model. Starting from 20, 000 translation pairs we generated a training dataset of 40, 000 positive and negative instances. 2 The context projection method was first proposed by (Fung and Yee, 1998; Rapp, 1999) and since then different variations have been suggested (Chiao and Zweigenbaum, 2002; Morin et al., 2007; Andrade et al., 2010; Morin and Prochasson, 2011). Our implementation more closely follows the context vector method introduced by (Morin and Prochasson, 2011). As a preprocessing step, stop words are removed using an online list 2 and lemmatisation is performed using TreeTagger (Schmid, 1994) on both the English and Spanish part of the comparable corpus. Afterwards, the method proceeds in three steps. Firstly, for each source and target term of the comparable corpus, i.e., i"
E14-4022,P99-1067,0,0.331174,"ipedia articles that are related to the medical sub-domain of “breast cancer”. Furthermore, we show that dictionaries extracted from comparable corpora can be used to dynamically augment an SMT system in order to better translate Out-of-Vocabulary (OOV) terms. are created by randomly matching non-translation pairs of terms. We used an equal number of positive and negative instances for training the model. Starting from 20, 000 translation pairs we generated a training dataset of 40, 000 positive and negative instances. 2 The context projection method was first proposed by (Fung and Yee, 1998; Rapp, 1999) and since then different variations have been suggested (Chiao and Zweigenbaum, 2002; Morin et al., 2007; Andrade et al., 2010; Morin and Prochasson, 2011). Our implementation more closely follows the context vector method introduced by (Morin and Prochasson, 2011). As a preprocessing step, stop words are removed using an online list 2 and lemmatisation is performed using TreeTagger (Schmid, 1994) on both the English and Spanish part of the comparable corpus. Afterwards, the method proceeds in three steps. Firstly, for each source and target term of the comparable corpus, i.e., i, we collect"
E14-4022,P07-2045,0,0.00791894,"nglish terms for which we are extracting translations and ranki is the position of the first correct translation from returned list of candidates 5 nlm.nih.gov/research/umls 6 http://en.wikipedia.org/wiki/Help:Searching 4 7 each frequency range contains 100 randomly sampled terms 113 ing the SMT and 1K sentences for evaluation. The test sentences contain 1, 200 terms that do not appear in the training parallel corpus. These terms occur in the Wikipedia comparable corpus. Hence, the previously extracted dictionaries list a possible translation. Using the PubMed parallel corpus, we train Moses (Koehn et al., 2007), a phrase-based SMT system. 4.2 To further investigate the effect of the language model on the translation performance of the augmented SMT systems, we conducted an oracle experiment. In this ideal setting, we assume a strong language model, that is trained on both training and test Spanish sentences of the parallel corpus, in order to assign a higher probability to a correct translation if it exists in the deployed dictionary. As we observe in Table 3, a strong language model can more accurately select the correct translation among top-k candidates. The dictionary extracted by the RF improve"
E14-4022,W13-2512,1,0.78176,"Missing"
E14-4022,D12-1003,0,0.125027,"omly select only one. Negative instances 3 Baseline method Experiments In this section, we evaluate the two dictionary extraction methods, namely context vectors and RF, on a comparable corpus of Wikipedia articles. For the evaluation metric, we use the top-k translation accuracy 3 and the mean reciprocal 2 http://members.unine.ch/jacques.savoy/clef/index.html the percentage of English terms whose top k candidates contain a correct translation 3 112 rank (MRR) 4 as in previous approaches (Chiao and Zweigenbaum, 2002; Chiao and Zweigenbaum, 2002; Morin and Prochasson, 2011; Morin et al., 2007; Tamura et al., 2012). As a reference list, we use the UMLS metathesaurus5 . In addition to this, considering that in several cases the dictionary extraction methods retrieved synonymous translations that do not appear in the reference list, we manually inspected the answers. Finally, unlike previous approaches (Chiao and Zweigenbaum, 2002), we do not restrict the test list only to those English terms whose Spanish translations are known to occur in the target corpus. In such cases, the performance of dictionary extraction methods have been shown to achieve a lower performance (Tamura et al., 2012). 3.1 curacy and"
E14-4022,P97-1039,0,0.0524334,"es for many Natural Language Processing (NLP) tasks including Statistical Machine Translation (SMT) (Och and Ney, 2003) and Cross-Language Information Retrieval (Ballesteros and Croft, 1997). However, manually creating and updating such resources is an expensive process. In addition to this, new terms are constantly emerging. Especially in the biomedical domain, which is the focus of this work, there is a vast number of neologisms, i.e., newly coined terms, (Pustejovsky et al., 2001). Early work on bilingual lexicon extraction focused on clean, parallel corpora providing satisfactory results (Melamed, 1997; Kay and R¨oscheisen, 1993). However, parallel corpora are expensive to construct and for some domains and language pairs are scarce resources. For these reasons, the focus has shifted to comparable corpora 1 http://personalpages.manchester. ac.uk/postgrad/georgios.kontonatsios/ Software/RF-TermAlign.tar.gz 111 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 111–116, c Gothenburg, Sweden, April 26-30 2014. 2014 Association for Computational Linguistics English-Spanish comparable corpus of Wikipedia articles that are related to"
E14-4022,C08-1125,0,0.0607731,"Missing"
E14-4022,W11-1205,0,\N,Missing
E14-4022,C98-1066,0,\N,Missing
E14-4022,J93-1006,0,\N,Missing
E17-1093,I08-2085,0,0.0249469,"er of the Association for Computational Linguistics: Volume 1, Long Papers, pages 991–1001, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Linguistics followed the description-comes-first (DCF) paradigm (Osi´nski et al., 2004; Weiss, 2006; Zhang, 2009). DCF-based methods work by firstly identifying a set of cluster labels, and subsequently forming document clusters by measuring the relevance of each document to a potential cluster label. DCF-based approaches have several shortcomings, which include poor clustering performance and low readability of cluster descriptors (Lee et al., 2008; Carpineto et al., 2009). More recent developments in descriptive clustering have proposed alternative techniques, which approach the problems of improving clustering performance and descriptive label quality from various different angles. For instance, Scaiella et al. (2012) identifies Wikipedia concepts in documents and then computes relatedness between documents according to the linked structure of Wikipedia. Navigli and Crisafulli (2010) propose a method that takes into account synonymy and polysemy. Their method utilises the Google Web1T corpus to identify word senses based on word co-oc"
E17-1093,D10-1012,0,0.0273314,"nt to a potential cluster label. DCF-based approaches have several shortcomings, which include poor clustering performance and low readability of cluster descriptors (Lee et al., 2008; Carpineto et al., 2009). More recent developments in descriptive clustering have proposed alternative techniques, which approach the problems of improving clustering performance and descriptive label quality from various different angles. For instance, Scaiella et al. (2012) identifies Wikipedia concepts in documents and then computes relatedness between documents according to the linked structure of Wikipedia. Navigli and Crisafulli (2010) propose a method that takes into account synonymy and polysemy. Their method utilises the Google Web1T corpus to identify word senses based on word co-occurrences and computes the similarity between documents using the extracted sense information. More recently, Mu et al. (2016) presented their co-embedding based descriptive clustering approach that learns a common co-embedding vector space of documents and candidate descriptive phrases. The co-embedded space simplifies the clustering and cluster labelling task into a more straightforward process of computing similarity between pairs of docum"
E17-1093,Y06-1066,0,0.0464941,"in order to predict word occurrences given a local context (Mnih and Hinton, 2009; Mikolov et al., 2013b; Mikolov et al., 2013a; Pennington et al., 2014). Subsequently, the PV model was proposed to learn representations of both words and documents (Le and Mikolov, 2014; Dai et al., 2015). The PV model has been Related Work Descriptive Clustering Descriptive clustering methods typically use an unsupervised approach to firstly group documents into flat or hierarchical clusters (Steinbach et al., 2000). Document clusters are then characterised using a set of informative and discriminative words (Zhu et al., 2006), phrases (Mu et al., 2016; Li et al., 2008) or sentences (Kim et al., 2015). Early approaches to descriptive clustering 992 shown to be capable of learning a semantically richer representation of documents compared to unstructured BoW models. To our knowledge, our work constitutes the first attempt to use distributed representation models to co-embed documents and phrases for unsupervised descriptive clustering. local context: X 1 X log p(pt |dt ) + log p(pt |c) (1) |Ct | t∈TP c∈Ct X 1 X log p(ws |c) + log p(ws |ds ) + |Cs | 3 where TP is the set of training phrase instances; pt ∈ P is the t-"
E17-1093,D14-1162,0,0.0791339,"ority of our methods in both clustering performance and labelling quality. 2 2.1 2.2 Distributed Representation Distributed representation techniques are becoming increasingly important in a number of supervised learning tasks, e.g., sentiment analysis (Dai et al., 2015), text classification (Dai et al., 2015; Ma et al., 2015) and named entity recognition (Turian et al., 2010). A number of models have been proposed to learn distributed word or phrase representations in order to predict word occurrences given a local context (Mnih and Hinton, 2009; Mikolov et al., 2013b; Mikolov et al., 2013a; Pennington et al., 2014). Subsequently, the PV model was proposed to learn representations of both words and documents (Le and Mikolov, 2014; Dai et al., 2015). The PV model has been Related Work Descriptive Clustering Descriptive clustering methods typically use an unsupervised approach to firstly group documents into flat or hierarchical clusters (Steinbach et al., 2000). Document clusters are then characterised using a set of informative and discriminative words (Zhu et al., 2006), phrases (Mu et al., 2016; Li et al., 2008) or sentences (Kim et al., 2015). Early approaches to descriptive clustering 992 shown to be"
E17-1093,P10-1040,0,0.0237372,"bels to them), and the previously introduced CEDL method (Mu et al., 2016), which carries out both clustering and labelling. Experimental results based on publicly available benchmark text collections demonstrate the effectiveness and superiority of our methods in both clustering performance and labelling quality. 2 2.1 2.2 Distributed Representation Distributed representation techniques are becoming increasingly important in a number of supervised learning tasks, e.g., sentiment analysis (Dai et al., 2015), text classification (Dai et al., 2015; Ma et al., 2015) and named entity recognition (Turian et al., 2010). A number of models have been proposed to learn distributed word or phrase representations in order to predict word occurrences given a local context (Mnih and Hinton, 2009; Mikolov et al., 2013b; Mikolov et al., 2013a; Pennington et al., 2014). Subsequently, the PV model was proposed to learn representations of both words and documents (Le and Mikolov, 2014; Dai et al., 2015). The PV model has been Related Work Descriptive Clustering Descriptive clustering methods typically use an unsupervised approach to firstly group documents into flat or hierarchical clusters (Steinbach et al., 2000). Do"
E17-1093,D15-1094,0,\N,Missing
E93-1027,C92-1022,0,0.241413,"Missing"
E93-1027,P89-1013,0,0.0620963,"Missing"
E93-1027,C92-2085,1,0.889133,"Missing"
E93-1027,C92-1033,0,0.0357938,"Missing"
E93-1027,C92-2072,0,\N,Missing
H05-1059,J96-1002,0,0.11937,"As an example, consider the situation where we are going to annotate a three-word sentence with part-of-speech tags. Figure 1 shows the four possible ways of decomposition. They correspond to the following equations: (a) P (t1 ...t3 |o) = P (t1 |o)P (t2 |t1 o)P (t3 |t2 o) (5) (b) P (t1 ...t3 |o) = P (t3 |o)P (t2 |t3 o)P (t1 |t2 o) (6) p(ti |ti−1 o). (3) i=1 Then we can employ a probabilistic classifier trained with the preceding tag and observations in order to obtain p(ti |ti−1 o) for local classification. A common choice for the local probabilistic classifier is maximum entropy classifiers (Berger et al., 1996). The best tag sequence can be efficiently computed by using a Viterbi decoding algorithm in polynomial time. 468 (c) P (t1 ...t3 |o) = P (t1 |o)P (t3 |o)P (t2 |t3 t1 o) (7) (d) P (t1 ...t3 |o) = P (t2 |o)P (t1 |t2 o)P (t3 |t2 o) (8) (a) and (b) are the standard left-to-right and rightto-left decompositions. Notice that in decomposition (c), the local classifier can use the information about the tags on both sides when deciding t2 . If, for example, the second word is difficult to tag (e.g. an unknown word), we might as well take the decomposition structure (c) because the local classifier can"
H05-1059,A00-1031,0,0.174993,"Missing"
H05-1059,W02-1001,0,0.126992,"S Previous POS POS two back Next POS POS two ahead Bigram POS features Trigram POS features Previous tag Tag two back Next tag Tag two ahead Bigram tag features wi wi−1 wi−2 wi+1 wi+2 wi−2 , wi−1 wi−1 , wi wi , wi+1 wi+1 , wi+2 pi pi−1 pi−2 pi+1 pi+2 pi−2 , pi−1 pi−1 , pi pi , pi+1 pi+1 , pi+2 pi−2 , pi−1 , pi pi−1 , pi , pi+1 pi , pi+1 , pi+2 ti−1 ti−2 ti+1 ti+2 ti−2 , ti−1 ti−1 , ti+1 ti+1 , ti+2 & ti & ti & ti & ti & ti & ti & ti & ti & ti & ti & ti & ti & ti & ti & ti & ti & ti & ti & ti & ti & ti & ti & ti & ti & ti & ti & ti & ti Table 4: Feature templates used in chunking experiments. (Collins, 2002) and used POS-trigrams as well. Table 4 lists the features used in chunking experiments. Table 5 shows the results on the development set. Again, bidirectional methods exhibit better performance than unidirectional methods. The difference is bigger with the Start/End representation. Dependency networks did not work well for this chunking task, especially with the Start/End representation. We applied the best model on the development set in each chunk representation type to the test data. Table 6 summarizes the performance on the test set. Our bidirectional methods achieved Fscores of 93.63 and"
H05-1059,W03-1018,1,0.624902,"Markov assumption obviously allows us to skip most of the probability updates, resulting in O(kn) invocations of local classifiers. This enables us to build a very efficient tagger. 3 Maximum Entropy Classifier For local classifiers, we used a maximum entropy model which is a common choice for incorporating various types of features for classification problems in natural language processing (Berger et al., 1996). Regularization is important in maximum entropy modeling to avoid overfitting to the training data. For this purpose, we use the maximum entropy modeling with inequality constraints (Kazama and Tsujii, 2003). The model gives equally good performance as the maximum entropy modeling with Gaussian priors (Chen and Rosenfeld, 1999), and the size of the resulting model is much smaller than that of Gaussian priors because most of the parameters become zero. This characteristic enables us to easily handle the model data and carry out quick decoding, which is convenient when we repetitively perform experiments. This modeling has one parameter to tune, which is called the width factor. We tuned this parameter using the development data in each type of experiments. Current word Previous word Next word Bigr"
H05-1059,N01-1025,0,0.134836,"methods for structured data share problems of computational cost (Altun et al., 2003). Another advantage is that one can employ a variety of machine learning algorithms as the local classifier. There is huge amount of work about developing classification algorithms that have high generalization performance in the machine learning community. Being able to incorporate such state-of-theart machine learning algorithms is important. Indeed, sequential classification approaches with kernel support vector machines offer competitive performance in POS tagging and chunking (Gimenez and Marquez, 2003; Kudo and Matsumoto, 2001). One obvious way to improve the performance of sequential classification approaches is to enrich the information that the local classifiers can use. In standard decomposition techniques, the local classifiers cannot use the information about future tags (e.g. the right-side tags in left-to-right decoding), which would be helpful in predicting the tag of the target word. To make use of the information about future tags, Toutanova et al. proposed a tagging algorithm based on bidirectional dependency networks 467 Proceedings of Human Language Technology Conference and Conference on Empirical Met"
H05-1059,W00-0730,0,0.0351022,"Missing"
H05-1059,W00-0726,0,0.148821,"Missing"
H05-1059,E99-1023,0,0.0050195,"py modeling can achieve comparable performance to other state-of-the-art POS tagging methods. 4.2 Chunking Experiments The task of chunking is to find non-recursive phrases in a sentence. For example, a chunker segments the sentence “He reckons the current account deficit will narrow to only 1.8 billion in September” into the following, [NP He] [VP reckons] [NP the current account deficit] [VP will narrow] [PP to] [NP only 1.8 billion] [PP in] [NP September] . We can regard chunking as a tagging task by converting chunks into tags on tokens. There are several ways of representing text chunks (Sang and Veenstra, 1999). We tested the Start/End representation in addition to the popular IOB2 representation since local classifiers can have fine-grained information on the neighboring tags in the Start/End representation. For training and testing, we used the data set provided for the CoNLL-2000 shared task. The training set consists of section 15-18 of the WSJ corpus, and the test set is section 20. In addition, we made the development set from section 21 3 . We basically adopted the feature set provided in 3 We used the Perl script http://ilk.kub.nl/˜ sabine/chunklink/ provided on 472 Current POS Previous POS"
H05-1059,P03-1064,0,0.0349904,"Missing"
H05-1059,N03-1033,0,0.399624,"s can use. In standard decomposition techniques, the local classifiers cannot use the information about future tags (e.g. the right-side tags in left-to-right decoding), which would be helpful in predicting the tag of the target word. To make use of the information about future tags, Toutanova et al. proposed a tagging algorithm based on bidirectional dependency networks 467 Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language c Processing (HLT/EMNLP), pages 467–474, Vancouver, October 2005. 2005 Association for Computational Linguistics (Toutanova et al., 2003) and achieved the best accuracy on POS tagging on the Wall Street Journal corpus. As they pointed out in their paper, however, their method potentially suffers from “collusion” effects which make the model lock onto conditionally consistent but jointly unlikely sequences. In their modeling, the local classifiers can always use the information about future tags, but that could cause a double-counting effect of tag information. In this paper we propose an alternative way of making use of future tags. Our inference method considers all possible ways of decomposition and chooses the “best” decompo"
H05-1059,J93-2004,0,\N,Missing
H92-1051,P91-1021,0,0.0431876,"ing units of bi-lingual knowledge. It is our contention that any MT system, whichever paradigm it belongs to, has to store a set of translationequivalent units for a pair of languages and combine these to produce larger units of translation. In EBMT, for example, a set of translation examples has to be stored and several of t h e m have to be combined properly to produce translation. Because of the declarative 255 nature of translation examples, E B M T inevitably encounters the same complexities of combining translation units in declarative representation as L B M T does. Research in L B M T [1] [2] [31 [10] [12] has revealed that difficulties in the declarative representation of bi-lingual knowledge stem mainly from the t r e a t m e n t of idiosyncratic structural changes caused by lexical items, and interactions of such idiosyncratic structural changes when they co-exist in single sentences. These types of structural changes also cause problems when they are combined with general or ordinary linguistic phenomena such as coordination. A formal framework to cope with these matters is essential in other approaches, such as EBMT, as in LBMT, if the translation is produced in a composi"
H92-1051,C90-3044,0,0.0643477,"ameworks for expressing Bi-lingual Knowledge • Integration of Knowledge-based Processing and Contextual Processing with the Translation Process • Effective Exploitation of D o m a i n / T e x t T y p e Specificity (or Sublanguageness) in MT and Discovery Processes for such Specificities Though new frameworks of MT such as Statistics-based MT (SBMT), Example-based M T (EBMT), Analogybased MT (ABMT), Knowledge-based MT (KBMT) etc. look radically different from conventional linguisticbased MT (LBMT) such as Transfer-based, they address one or two of the above focal issues and ignore the rest [7] [9]. In particular, the new paradigms of MT tend to ignore the first issue ie. declarative representation of bi-lingual knowledge and the complexities involved n the process of combining units of bi-lingual knowledge. It is our contention that any MT system, whichever paradigm it belongs to, has to store a set of translationequivalent units for a pair of languages and combine these to produce larger units of translation. In EBMT, for example, a set of translation examples has to be stored and several of t h e m have to be combined properly to produce translation. Because of the declarative 255 na"
H92-1051,E91-1048,0,0.0776511,"bi-lingual knowledge. It is our contention that any MT system, whichever paradigm it belongs to, has to store a set of translationequivalent units for a pair of languages and combine these to produce larger units of translation. In EBMT, for example, a set of translation examples has to be stored and several of t h e m have to be combined properly to produce translation. Because of the declarative 255 nature of translation examples, E B M T inevitably encounters the same complexities of combining translation units in declarative representation as L B M T does. Research in L B M T [1] [2] [31 [10] [12] has revealed that difficulties in the declarative representation of bi-lingual knowledge stem mainly from the t r e a t m e n t of idiosyncratic structural changes caused by lexical items, and interactions of such idiosyncratic structural changes when they co-exist in single sentences. These types of structural changes also cause problems when they are combined with general or ordinary linguistic phenomena such as coordination. A formal framework to cope with these matters is essential in other approaches, such as EBMT, as in LBMT, if the translation is produced in a compositional way. I"
H92-1051,E89-1037,0,\N,Missing
hanaoka-etal-2010-japanese,W07-1522,0,\N,Missing
I05-1018,P02-1036,0,0.0239988,"Missing"
I05-1018,P05-1011,1,0.642017,"Missing"
I05-1018,J96-1002,0,0.004459,"Missing"
I05-1018,A00-2021,0,0.447388,"Missing"
I05-1018,P04-1014,0,0.0421718,"Missing"
I05-1018,H94-1020,0,\N,Missing
I05-1018,P99-1069,0,\N,Missing
I05-1028,W04-3239,0,0.0120462,"mpts to analyze reviews deeply to improve accuracy. Mullen [10] used features from various information sources such as references to the “work” or “artist”, which were annotated by hand, and showed that these features have the potential to improve the accuracy. We use reference features, which are the words around the ﬁxed review target word (book), while Mullen annotated the references by hand. 316 D. Okanohara and J. Tsujii Turney [20] used semantic orientation, which measures the distance from phrases to “excellent” or “poor” by using search engine results and gives the word polarity. Kudo [8] developed decision stumps, which can capture substructures embedded in text (such as word-based dependency), and suggested that subtree features are important for opinion/modality classiﬁcation. Independently of and in parallel with our work, two other papers consider the degree of polarity for sentiment classiﬁcation. Koppel [6] exploited a neutral class and applied a regression method as ours. Pang [12] applied a metric labeling method for the task. Our work is diﬀerent from their works in several respects. We exploited square errors instead of precision for the evaluation and used ﬁve dist"
I05-1028,W04-3253,0,0.0163717,"ussion groups, online shops, and blog systems on the Internet have gained popularity and the number of documents, such as reviews, is growing dramatically. Sentiment classiﬁcation refers to classifying reviews not by their topics but by the polarity of their sentiment (e.g, positive or negative). It is useful for recommendation systems, ﬁne-grained information retrieval systems, and business applications that collect opinions about a commercial product. Recently, sentiment classiﬁcation has been actively studied and experimental results have shown that machine learning approaches perform well [13,11,10,20]. We argue, however, that we can estimate the polarity of a review more ﬁnely. For example, both reviews A and B in Table 1 would be classiﬁed simply as positive in binary classiﬁcation. Obviously, this classiﬁcation loses the information about the diﬀerence in the degree of polarity apparent in the review text. We propose a novel type of document classiﬁcation task where we evaluate reviews with scores like ﬁve stars. We call this score the sentiment polarity score (sp-score). If, for example, the range of the score is from one to ﬁve, we could give ﬁve to review A and four to review B. This"
I05-1028,P04-1035,0,0.0643226,"ussion groups, online shops, and blog systems on the Internet have gained popularity and the number of documents, such as reviews, is growing dramatically. Sentiment classiﬁcation refers to classifying reviews not by their topics but by the polarity of their sentiment (e.g, positive or negative). It is useful for recommendation systems, ﬁne-grained information retrieval systems, and business applications that collect opinions about a commercial product. Recently, sentiment classiﬁcation has been actively studied and experimental results have shown that machine learning approaches perform well [13,11,10,20]. We argue, however, that we can estimate the polarity of a review more ﬁnely. For example, both reviews A and B in Table 1 would be classiﬁed simply as positive in binary classiﬁcation. Obviously, this classiﬁcation loses the information about the diﬀerence in the degree of polarity apparent in the review text. We propose a novel type of document classiﬁcation task where we evaluate reviews with scores like ﬁve stars. We call this score the sentiment polarity score (sp-score). If, for example, the range of the score is from one to ﬁve, we could give ﬁve to review A and four to review B. This"
I05-1028,P05-1015,0,0.132477,"anohara and J. Tsujii Turney [20] used semantic orientation, which measures the distance from phrases to “excellent” or “poor” by using search engine results and gives the word polarity. Kudo [8] developed decision stumps, which can capture substructures embedded in text (such as word-based dependency), and suggested that subtree features are important for opinion/modality classiﬁcation. Independently of and in parallel with our work, two other papers consider the degree of polarity for sentiment classiﬁcation. Koppel [6] exploited a neutral class and applied a regression method as ours. Pang [12] applied a metric labeling method for the task. Our work is diﬀerent from their works in several respects. We exploited square errors instead of precision for the evaluation and used ﬁve distinct scores in our experiments while Koppel used three and Pang used three/four distinct scores in their experiments. 3 Analyzing Reviews with Polarity Scores In this section we present a novel task setting where we predict the degree of sentiment polarity of a review. We ﬁrst present the deﬁnition of sp-scores and the task of assigning them to review documents. We then explain an evaluation data set. Usin"
I05-1028,W02-1011,0,0.0265097,"ussion groups, online shops, and blog systems on the Internet have gained popularity and the number of documents, such as reviews, is growing dramatically. Sentiment classiﬁcation refers to classifying reviews not by their topics but by the polarity of their sentiment (e.g, positive or negative). It is useful for recommendation systems, ﬁne-grained information retrieval systems, and business applications that collect opinions about a commercial product. Recently, sentiment classiﬁcation has been actively studied and experimental results have shown that machine learning approaches perform well [13,11,10,20]. We argue, however, that we can estimate the polarity of a review more ﬁnely. For example, both reviews A and B in Table 1 would be classiﬁed simply as positive in binary classiﬁcation. Obviously, this classiﬁcation loses the information about the diﬀerence in the degree of polarity apparent in the review text. We propose a novel type of document classiﬁcation task where we evaluate reviews with scores like ﬁve stars. We call this score the sentiment polarity score (sp-score). If, for example, the range of the score is from one to ﬁve, we could give ﬁve to review A and four to review B. This"
I05-1028,P02-1053,0,0.0524031,"ussion groups, online shops, and blog systems on the Internet have gained popularity and the number of documents, such as reviews, is growing dramatically. Sentiment classiﬁcation refers to classifying reviews not by their topics but by the polarity of their sentiment (e.g, positive or negative). It is useful for recommendation systems, ﬁne-grained information retrieval systems, and business applications that collect opinions about a commercial product. Recently, sentiment classiﬁcation has been actively studied and experimental results have shown that machine learning approaches perform well [13,11,10,20]. We argue, however, that we can estimate the polarity of a review more ﬁnely. For example, both reviews A and B in Table 1 would be classiﬁed simply as positive in binary classiﬁcation. Obviously, this classiﬁcation loses the information about the diﬀerence in the degree of polarity apparent in the review text. We propose a novel type of document classiﬁcation task where we evaluate reviews with scores like ﬁve stars. We call this score the sentiment polarity score (sp-score). If, for example, the range of the score is from one to ﬁve, we could give ﬁve to review A and four to review B. This"
I05-2038,W04-3111,0,0.0170745,"e phrase illustrated in Figure 2a and Figure 2b shows another problem of the annotation scheme. Both annotators fail to indicate that it is ‘mediated’ that was to be after ‘IL-1’ because there is no mechanism of coindexing a null element with a part of a token. This problem of ellipsis can frequently occur in research abstracts, and it can be argued that the tokenization criteria must be changed for texts in biomedical domain (Yamamoto and Sa224 tou, 2004) so that such fragment as ‘IL-18’ and ‘mediated’ in ‘IL-18-ediated’ should be regarede as separate tokens. The Pennsylvania biology corpus (Kulick et al., 2004) partially solves this problem by separating a token where two or more subtokens are connected with hyphens, but in the cases where a shared part of the word is not separated by a hyphen (e.g. ‘metric’ of ‘stereo- and isometric alleles’) the word including the part is left uncut. The current GTB follows the GENIA corpus that it retains the tokenization criteria of the original Penn Treebank, but this must be reconsidered in future. For analysis of coordination with ellipsis, if the information on full forms is available, one strategy would be to leave the inside structure of coordination unann"
I05-2038,C04-1204,1,0.801337,"Missing"
I05-2038,tateisi-tsujii-2004-part,1,0.929229,"text of GTB is that of the GENIA corpus constructed at University of Tokyo (Kim et al., 2003), which is a collection of research abstracts selected from the search results of MEDLINE database with keywords (MeSH terms) human, blood cells and transcription factors. In the GENIA corpus, the abstracts are encoded in an XML scheme where each abstract is numbered with MEDLINE UID and contains title and abstract. The text of title and abstract is segmented into sentences in which biological terms are annotated with their semantic classes. The GENIA corpus is also annotated for part-ofspeech (POS) (Tateisi and Tsujii, 2004), and coreference is also annotated in a part of the GENIA corpus by MedCo project at Institute for Infocomm Research, Singapore (Yang et al, 2004). GTB is the addition of syntactic information to the GENIA corpus. By annotating various linguistic information on a same set of text, the GENIA corpus will be a resource not only for individual purpose such as named entity extraction or training parsers but also for integrated systems such as information extraction using deep linguistic analysis. Similar attempt of constructing integrated corpora is being done in University of Pennsylvania, where"
I05-2038,wermter-hahn-2004-annotated,0,0.0405149,"Missing"
I05-2038,A00-1031,0,0.0065685,"Missing"
I08-1060,P98-2127,0,0.20388,"Missing"
I08-1060,P01-1008,0,0.0390569,"ctionaries. Many studies extract synonyms from large monolingual corpora by using context information around target terms (Croach and Yang, 1992; Park and Choi, 1996; Waterman, 1996; Curran, 2004). Some researchers (Hindle, 1990; Grefenstette, 1994; Lin, 458 1998) classify terms by similarities based on their distributional syntactic patterns. These methods often extract not only synonyms, but also semantically related terms, such as antonyms, hyponyms and coordinate terms such as ‘cat’ and ‘dog.’ Some studies make use of bilingual corpora or dictionaries to ﬁnd synonyms in a target language (Barzilay and McKeown, 2001; Shimohata and Sumita, 2002; Wu and Zhou, 2003; Lin et al., 2003). Lin et al. (2003) chose a set of synonym candidates for a term by using a bilingual dictionary and computing distributional similarities in the candidate set to extract synonyms. They adopt the bilingual information to exclude non-synonyms (e.g., antonyms and hyponyms) that may be used in the similar contexts. Although they make use of bilingual dictionaries, this study aims at ﬁnding bilingual synonyms directly. In the approaches based on monolingual dictionaries, the similarities of deﬁnitions of lexical items are important"
I08-1060,J96-1002,0,0.0196307,"aluating the performance of the identiﬁcation. We employ a supervised machine learning technique with features related to spelling variations and so on. Figure 3 shows the framework for this method. At ﬁrst we prepare a bilingual lexicon with synonymous information as training data, and generate a list consisting of all bilingual lexical item 460 Figure 3: Overview of our framework pairs in the bilingual lexicon. The presence or absence of bilingual synonymous relations is attached to each element of the list. Then, we build a classiﬁer learned by training data, using a maximum entropy model (Berger et al., 1996) and the features related to spelling variations in Table 3. We apply some preprocessings for extracting some features. For English, we transform all terms into lower-case, and do not apply any other transformations such as tokenization by symbols. For Japanese, we apply a morphological analyzer JUMAN (Kurohashi et al., 1994) and obtain hiragana representations5 as much as possible6 . We may require other language-speciﬁc preprocessings for applying this method to other languages. We employed binary or real-valued features described in Table 3. Moreover, we introduce the following combinatoria"
I08-1060,P90-1034,0,0.212858,"nonym extraction and spelling variations. Section 3 describes the overview and deﬁnition of bilingual synonyms, the proposed method and employed features. In Section 4 we evaluate our method and conclude this paper. 2 Related work There have been many approaches for detecting synonyms and constructing thesauri. Two main resources for synonym extraction are large text corpora and dictionaries. Many studies extract synonyms from large monolingual corpora by using context information around target terms (Croach and Yang, 1992; Park and Choi, 1996; Waterman, 1996; Curran, 2004). Some researchers (Hindle, 1990; Grefenstette, 1994; Lin, 458 1998) classify terms by similarities based on their distributional syntactic patterns. These methods often extract not only synonyms, but also semantically related terms, such as antonyms, hyponyms and coordinate terms such as ‘cat’ and ‘dog.’ Some studies make use of bilingual corpora or dictionaries to ﬁnd synonyms in a target language (Barzilay and McKeown, 2001; Shimohata and Sumita, 2002; Wu and Zhou, 2003; Lin et al., 2003). Lin et al. (2003) chose a set of synonym candidates for a term by using a bilingual dictionary and computing distributional similariti"
I08-1060,C04-1176,0,0.0692008,"Missing"
I08-1060,W06-3811,0,0.0124467,"t al., 2003). Lin et al. (2003) chose a set of synonym candidates for a term by using a bilingual dictionary and computing distributional similarities in the candidate set to extract synonyms. They adopt the bilingual information to exclude non-synonyms (e.g., antonyms and hyponyms) that may be used in the similar contexts. Although they make use of bilingual dictionaries, this study aims at ﬁnding bilingual synonyms directly. In the approaches based on monolingual dictionaries, the similarities of deﬁnitions of lexical items are important clues for identifying synonyms (Blondel et al., 2004; Muller et al., 2006). For instance, Blondel et al. (2004) constructed an associated dictionary graph whose vertices are the terms, and whose edges from v1 to v2 represent occurrence of v2 in the deﬁnition for v1 . They choose synonyms from the graph by collecting terms pointed to and from the same terms. Another strategy for ﬁnding synonyms is to consider the terms themselves. We divide it into two approaches: rule-based and distance-based. Rule-based approaches implement rules with language-speciﬁc patterns and detect variations by applying rules to terms. Stemming (Lovins, 1968; Porter, 1980) is one of the rule"
I08-1060,shimohata-sumita-2002-automatic,0,0.0181973,"ract synonyms from large monolingual corpora by using context information around target terms (Croach and Yang, 1992; Park and Choi, 1996; Waterman, 1996; Curran, 2004). Some researchers (Hindle, 1990; Grefenstette, 1994; Lin, 458 1998) classify terms by similarities based on their distributional syntactic patterns. These methods often extract not only synonyms, but also semantically related terms, such as antonyms, hyponyms and coordinate terms such as ‘cat’ and ‘dog.’ Some studies make use of bilingual corpora or dictionaries to ﬁnd synonyms in a target language (Barzilay and McKeown, 2001; Shimohata and Sumita, 2002; Wu and Zhou, 2003; Lin et al., 2003). Lin et al. (2003) chose a set of synonym candidates for a term by using a bilingual dictionary and computing distributional similarities in the candidate set to extract synonyms. They adopt the bilingual information to exclude non-synonyms (e.g., antonyms and hyponyms) that may be used in the similar contexts. Although they make use of bilingual dictionaries, this study aims at ﬁnding bilingual synonyms directly. In the approaches based on monolingual dictionaries, the similarities of deﬁnitions of lexical items are important clues for identifying synony"
I08-1060,W03-1610,0,0.0268203,"olingual corpora by using context information around target terms (Croach and Yang, 1992; Park and Choi, 1996; Waterman, 1996; Curran, 2004). Some researchers (Hindle, 1990; Grefenstette, 1994; Lin, 458 1998) classify terms by similarities based on their distributional syntactic patterns. These methods often extract not only synonyms, but also semantically related terms, such as antonyms, hyponyms and coordinate terms such as ‘cat’ and ‘dog.’ Some studies make use of bilingual corpora or dictionaries to ﬁnd synonyms in a target language (Barzilay and McKeown, 2001; Shimohata and Sumita, 2002; Wu and Zhou, 2003; Lin et al., 2003). Lin et al. (2003) chose a set of synonym candidates for a term by using a bilingual dictionary and computing distributional similarities in the candidate set to extract synonyms. They adopt the bilingual information to exclude non-synonyms (e.g., antonyms and hyponyms) that may be used in the similar contexts. Although they make use of bilingual dictionaries, this study aims at ﬁnding bilingual synonyms directly. In the approaches based on monolingual dictionaries, the similarities of deﬁnitions of lexical items are important clues for identifying synonyms (Blondel et al.,"
I08-1060,C98-2122,0,\N,Missing
I08-2122,I05-1018,1,0.788734,"se. In order to observe such differences, we need to integrate available combinations of tools into a workflow and to compare the combinatorial results. Although generic frameworks like UIMA (Unstructured Information Management Architecture) provide interoperability to solve this problem, the solution they provide is only partial. In order for truly interoperable toolkits to become a reality, we also need 1 Introduction Recently, an increasing number of TM/NLP tools such as part-of-speech (POS) taggers (Tsuruoka et al., 2005), named entity recognizers (NERs) (Settles, 2005) syntactic parsers (Hara et al., 2005) and relation or event extractors (ERs) have been developed. Nevertheless, it is still very difficult to integrate independently developed tools into an aggregated application that achieves a specific task. The difficulties are caused not only by differences in programming platforms and different input/output data formats, but also by the lack of higher level interoperability among modules developed by different groups. 859 uima.jcas.cas.TOP tcas.uima.Annotation -begin: int -end: int SyntacticAnnotation POS SemanticAnnotation UnknownPOS PennPOS -posType: String Token Sentence Phrase NamedEntit"
I08-2122,W04-1213,0,0.0372378,"Missing"
I08-2122,J93-2004,0,0.0293416,"type systems have to be related through a sharable type system, which our platform defines. Such a shared type system can bridge modules with different type systems, though the bridging module may lose some information during the translation process. Whether such a sharable type system can be defined or not is dependent on the nature of each problem. For example, a sharable type system for POS tags in English can be defined rather easily, since most of POS-related modules (such as POS taggers, shallow parsers, etc.) more or less follow the well established types defined by the Penn Treebank (Marcus et al., 1993) tag set. Figure 1 shows a part of our sharable type system. We deliberately define a highly organized type hierarchy as described above. Secondly we should consider that the type system may be used to compare a similar sort of tools. Types should be defined in a distinct and 861 hierarchical manner. For example, both tokenizers and POS taggers output an object of type Token, but their roles are different when we assume a cascaded pipeline. We defined Token as a supertvpe, POSToken as subtypes of Token. Each tool should have an individual type to make clear which tool generated which instance,"
I08-2122,E06-1015,0,0.0352696,"Missing"
I08-2122,J96-1002,0,0.0129233,"Missing"
I08-2127,W06-0103,0,0.0295316,"his paper. 2 Japanese Abbreviation Survey Researchers have proposed several approaches to abbreviation recognition for non-alphabetical languages. Hisamitsu and Niwa (2001) compared different statistical measures (e.g., χ2 test, log likeTable 1: Parenthetical expressions used in Japanese newspaper articles lihood ratio) to assess the co-occurrence strength between the inner and outer phrases of parenthetical expressions X (Y). Yamamoto (2002) utilized the similarity of local contexts to measure the paraphrase likelihood of two expressions based on the distributional hypothesis (Harris, 1954). Chang and Teng (2006) formalized the generative processes of Chinese abbreviations with a noisy channel model. Sasano et al. (2007) designed rules about letter types and occurrence frequency to collect lexical paraphrases used for coreference resolution. How are these approaches effective in recognizing Japanese abbreviation definitions? As a preliminary study, we examined abbreviations described in parenthetical expressions in Japanese newspaper articles. We used the 7,887 parenthetical expressions that occurred more than eight times in Japanese articles published by the Mainichi Newspapers and Yomiuri Shimbun in"
I08-2127,W02-2016,0,0.0287337,"es the paraphrase ratio of the expressions X and Y, PR(X, Y ) = dpara (X, Y ) . d(X, Y ) (1) In this formula, dpara (X, Y ) denotes the number of documents satisfying the above conditions, and d(X, Y ) presents the number of documents having the parenthetical expression X(Y ). The function PR(X, Y) ranges from 0 (no abbreviation instance) to 1 (all parenthetical expressions introduce the abbreviation). Similarity of local contexts We regard words that have dependency relations from/to the target expression as the local contexts of the expression, applying all sentences to a dependency parser (Kudo and Matsumoto, 2002). Collecting the local context of the target expressions, we compute the skew divergence (Lee, 2001), which is a weighted version of 892 SKEWα (P ||Q) = KL(P ||αQ + (1 − α)P ), (2) KL(P ||Q) = X i P (i) log P (i) . Q(i) (3) Other features In addition, we designed twelve features for abbreviation recognition: five features, freq(X), freq(Y ), freq(X, Y ), χ2 (X, Y ), and LLR(X, Y ) to measure the co-occurrence strength of the expressions X and Y (Hisamitsu and Niwa, 2001), match(X, Y ) feature to test whether or not all letters in an abbreviation appear in its full form, three features letter t"
I08-2127,P06-2083,1,0.780857,"lications by recognizing a set of expressions referring to the same entity/concept. For example, text retrieval systems can associate a query with alternative words to find documents where the query is not obviously stated. 889 Abbreviations are among a highly productive type of term variants, which substitutes fully expanded terms with shortened term-forms. Most previous studies aimed at establishing associations between abbreviations and their full forms in English (Park and Byrd, 2001; Pakhomov, 2002; Schwartz and Hearst, 2003; Adar, 2004; Nadeau and Turney, 2005; Chang and Sch¨utze, 2006; Okazaki and Ananiadou, 2006). Although researchers have proposed various approaches to solving abbreviation recognition through methods such as deterministic algorithm, scoring function, and machine learning, these studies rely on the phenomenon specific to English abbreviations: all letters in an abbreviation appear in its full form. However, abbreviation phenomena are heavily dependent on languages. For example, the term onesegment broadcasting is usually abbreviated as oneseg in Japanese; English speakers may find this peculiar as the term is likely to be abbreviated as 1SB or OSB in English. We show that letters do n"
I08-2127,P02-1021,0,0.0165456,"s or syntactic structures. Lexical resources such as WordNet (Miller et al., 1990) enhance various NLP applications by recognizing a set of expressions referring to the same entity/concept. For example, text retrieval systems can associate a query with alternative words to find documents where the query is not obviously stated. 889 Abbreviations are among a highly productive type of term variants, which substitutes fully expanded terms with shortened term-forms. Most previous studies aimed at establishing associations between abbreviations and their full forms in English (Park and Byrd, 2001; Pakhomov, 2002; Schwartz and Hearst, 2003; Adar, 2004; Nadeau and Turney, 2005; Chang and Sch¨utze, 2006; Okazaki and Ananiadou, 2006). Although researchers have proposed various approaches to solving abbreviation recognition through methods such as deterministic algorithm, scoring function, and machine learning, these studies rely on the phenomenon specific to English abbreviations: all letters in an abbreviation appear in its full form. However, abbreviation phenomena are heavily dependent on languages. For example, the term onesegment broadcasting is usually abbreviated as oneseg in Japanese; English spe"
I08-2127,W01-0516,0,0.112671,"sing alternative words or syntactic structures. Lexical resources such as WordNet (Miller et al., 1990) enhance various NLP applications by recognizing a set of expressions referring to the same entity/concept. For example, text retrieval systems can associate a query with alternative words to find documents where the query is not obviously stated. 889 Abbreviations are among a highly productive type of term variants, which substitutes fully expanded terms with shortened term-forms. Most previous studies aimed at establishing associations between abbreviations and their full forms in English (Park and Byrd, 2001; Pakhomov, 2002; Schwartz and Hearst, 2003; Adar, 2004; Nadeau and Turney, 2005; Chang and Sch¨utze, 2006; Okazaki and Ananiadou, 2006). Although researchers have proposed various approaches to solving abbreviation recognition through methods such as deterministic algorithm, scoring function, and machine learning, these studies rely on the phenomenon specific to English abbreviations: all letters in an abbreviation appear in its full form. However, abbreviation phenomena are heavily dependent on languages. For example, the term onesegment broadcasting is usually abbreviated as oneseg in Japan"
I08-2127,W02-1411,0,0.0178085,"sents a supervised learning approach to Japanese abbreviations. We then evaluate the proposed method on a test corpus from newspaper articles in Section 4 and conclude this paper. 2 Japanese Abbreviation Survey Researchers have proposed several approaches to abbreviation recognition for non-alphabetical languages. Hisamitsu and Niwa (2001) compared different statistical measures (e.g., χ2 test, log likeTable 1: Parenthetical expressions used in Japanese newspaper articles lihood ratio) to assess the co-occurrence strength between the inner and outer phrases of parenthetical expressions X (Y). Yamamoto (2002) utilized the similarity of local contexts to measure the paraphrase likelihood of two expressions based on the distributional hypothesis (Harris, 1954). Chang and Teng (2006) formalized the generative processes of Chinese abbreviations with a noisy channel model. Sasano et al. (2007) designed rules about letter types and occurrence frequency to collect lexical paraphrases used for coreference resolution. How are these approaches effective in recognizing Japanese abbreviation definitions? As a preliminary study, we examined abbreviations described in parenthetical expressions in Japanese newsp"
I11-1084,W06-1615,0,0.0623828,"me parsing system can be applied to a novel domain. However, there are some cases where we cannot achieve such high parsing accuracy as parsing 2 Related work Since domain adaptation is an extensive research area in parsing research (Nivre et al., 2007), many ideas have been proposed, including un- or 749 Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 749–757, c Chiang Mai, Thailand, November 8 – 13, 2011. 2011 AFNLP and imperatives. In the experiments, we also used QuestionBank for comparison. semi-supervised approaches (Roark and Bacchiani, 2003; Blitzer et al., 2006; Steedman et al., 2003; McClosky et al., 2006; Clegg and Shepherd, 2005; McClosky et al., 2010) and supervised approaches (Titov and Henderson, 2006; Hara et al., 2007). The main focus of these works is on adapting parsing models trained with a specific genre of text (in most cases the Penn Treebank WSJ) to other genres of text, such as biomedical research papers and broadcast news. The major problem tackled in such tasks is the handling of unknown words and domain-specific manners of expression. However, parsing imperatives and questions involves a significantly different problem; even when"
I11-1084,W07-2208,1,0.7578,"et al., 2006)3 , respectively. Although the publicly available implementation of each parser also has the option to restrict the output to a projective dependency tree, we used the non-projective versions because the dependency structures converted from the question sentences in the Brown Corpus included many non-projective dependencies. We used the pennconverter (Johansson and Nugues, 2007) 4 to convert a PTB-style treebank into dependency trees 5 . To evaluate the output from each of the parsers, we used the labeled attachment accuracy excluding punctuation. 3.2 HPSG parser The Enju parser (Ninomiya et al., 2007)6 is a deep parser based on the HPSG (Head Driven Phrase Structure Grammar) formalism. It produces an analysis of a sentence including the syntactic structure (i.e., parse tree) and the semantic structure represented as a set of predicateargument dependencies. We used the toolkit distributed with the Enju parser to train the parser with a PTB-style treebank. The toolkit initially converts a PTB-style treebank into an HPSG treebank and then trains the parser on this. The HPSG treebank converted from the test section was used as the gold-standard in the evaluation. As evaluation metrics for the"
I11-1084,J07-4004,0,0.025712,"accuracy of state-of-the-art parsers on questions, and proposed a supervised parser adaptation by manually creating a treebank of questions.1 The question sentences are annotated with phrase structure trees in the Penn Treebank scheme, although function tags and empty categories are omitted. QuestionBank was used for the supervised training of an LFG parser, resulting in a significant improvement in parsing accuracy. Rimell and Clark (2008) also worked on the problem of question parsing in the context of domain adaptation, and proposed a supervised method for the adaptation of the C&C parser (Clark and Curran, 2007). In this work, question sentences were collected from TREC 9-12 competitions and annotated with POS and CCG lexical categories. The authors reported a significant improvement in CCG parsing without phrase structure annotations. Our work further extends Judge et al. (2006) and Rimell and Clark (2008), while covering a wider range of sentence constructions. Although QuestionBank and the resource of Rimell and Clark (2008) claim to be corpora of questions, they are biased because the sentences come from QA queries. For example, such queries rarely include yes/no questions or tag questions. For o"
I11-1084,P81-1022,0,0.773475,"Missing"
I11-1084,W05-1102,0,0.0323997,"re some cases where we cannot achieve such high parsing accuracy as parsing 2 Related work Since domain adaptation is an extensive research area in parsing research (Nivre et al., 2007), many ideas have been proposed, including un- or 749 Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 749–757, c Chiang Mai, Thailand, November 8 – 13, 2011. 2011 AFNLP and imperatives. In the experiments, we also used QuestionBank for comparison. semi-supervised approaches (Roark and Bacchiani, 2003; Blitzer et al., 2006; Steedman et al., 2003; McClosky et al., 2006; Clegg and Shepherd, 2005; McClosky et al., 2010) and supervised approaches (Titov and Henderson, 2006; Hara et al., 2007). The main focus of these works is on adapting parsing models trained with a specific genre of text (in most cases the Penn Treebank WSJ) to other genres of text, such as biomedical research papers and broadcast news. The major problem tackled in such tasks is the handling of unknown words and domain-specific manners of expression. However, parsing imperatives and questions involves a significantly different problem; even when all words in a sentence are known, the sentence has a very different str"
I11-1084,W07-2202,1,0.899758,"adaptation is an extensive research area in parsing research (Nivre et al., 2007), many ideas have been proposed, including un- or 749 Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 749–757, c Chiang Mai, Thailand, November 8 – 13, 2011. 2011 AFNLP and imperatives. In the experiments, we also used QuestionBank for comparison. semi-supervised approaches (Roark and Bacchiani, 2003; Blitzer et al., 2006; Steedman et al., 2003; McClosky et al., 2006; Clegg and Shepherd, 2005; McClosky et al., 2010) and supervised approaches (Titov and Henderson, 2006; Hara et al., 2007). The main focus of these works is on adapting parsing models trained with a specific genre of text (in most cases the Penn Treebank WSJ) to other genres of text, such as biomedical research papers and broadcast news. The major problem tackled in such tasks is the handling of unknown words and domain-specific manners of expression. However, parsing imperatives and questions involves a significantly different problem; even when all words in a sentence are known, the sentence has a very different structure from declarative sentences. Compared to domain adaptation, structural types of sentences h"
I11-1084,D08-1050,0,0.0186762,"ptation, structural types of sentences have received little attention to date. A notable exception is the work on QuestionBank (Judge et al., 2006). This work highlighted the low accuracy of state-of-the-art parsers on questions, and proposed a supervised parser adaptation by manually creating a treebank of questions.1 The question sentences are annotated with phrase structure trees in the Penn Treebank scheme, although function tags and empty categories are omitted. QuestionBank was used for the supervised training of an LFG parser, resulting in a significant improvement in parsing accuracy. Rimell and Clark (2008) also worked on the problem of question parsing in the context of domain adaptation, and proposed a supervised method for the adaptation of the C&C parser (Clark and Curran, 2007). In this work, question sentences were collected from TREC 9-12 competitions and annotated with POS and CCG lexical categories. The authors reported a significant improvement in CCG parsing without phrase structure annotations. Our work further extends Judge et al. (2006) and Rimell and Clark (2008), while covering a wider range of sentence constructions. Although QuestionBank and the resource of Rimell and Clark (20"
I11-1084,N03-1027,0,0.0233071,"stems, and therefore the same parsing system can be applied to a novel domain. However, there are some cases where we cannot achieve such high parsing accuracy as parsing 2 Related work Since domain adaptation is an extensive research area in parsing research (Nivre et al., 2007), many ideas have been proposed, including un- or 749 Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 749–757, c Chiang Mai, Thailand, November 8 – 13, 2011. 2011 AFNLP and imperatives. In the experiments, we also used QuestionBank for comparison. semi-supervised approaches (Roark and Bacchiani, 2003; Blitzer et al., 2006; Steedman et al., 2003; McClosky et al., 2006; Clegg and Shepherd, 2005; McClosky et al., 2010) and supervised approaches (Titov and Henderson, 2006; Hara et al., 2007). The main focus of these works is on adapting parsing models trained with a specific genre of text (in most cases the Penn Treebank WSJ) to other genres of text, such as biomedical research papers and broadcast news. The major problem tackled in such tasks is the handling of unknown words and domain-specific manners of expression. However, parsing imperatives and questions involves a significantly differe"
I11-1084,W07-2416,0,0.0948282,"and Malt parsers The MST and Malt parsers are dependency parsers that produce non-projective dependency trees, using the spanning tree algorithm (McDonald et al., 2005a; McDonald et al., 2005b)2 and transitionbased algorithm (Nivre et al., 2006)3 , respectively. Although the publicly available implementation of each parser also has the option to restrict the output to a projective dependency tree, we used the non-projective versions because the dependency structures converted from the question sentences in the Brown Corpus included many non-projective dependencies. We used the pennconverter (Johansson and Nugues, 2007) 4 to convert a PTB-style treebank into dependency trees 5 . To evaluate the output from each of the parsers, we used the labeled attachment accuracy excluding punctuation. 3.2 HPSG parser The Enju parser (Ninomiya et al., 2007)6 is a deep parser based on the HPSG (Head Driven Phrase Structure Grammar) formalism. It produces an analysis of a sentence including the syntactic structure (i.e., parse tree) and the semantic structure represented as a set of predicateargument dependencies. We used the toolkit distributed with the Enju parser to train the parser with a PTB-style treebank. The toolkit"
I11-1084,P06-1063,0,0.244348,"Missing"
I11-1084,E03-1008,0,0.085386,"be applied to a novel domain. However, there are some cases where we cannot achieve such high parsing accuracy as parsing 2 Related work Since domain adaptation is an extensive research area in parsing research (Nivre et al., 2007), many ideas have been proposed, including un- or 749 Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 749–757, c Chiang Mai, Thailand, November 8 – 13, 2011. 2011 AFNLP and imperatives. In the experiments, we also used QuestionBank for comparison. semi-supervised approaches (Roark and Bacchiani, 2003; Blitzer et al., 2006; Steedman et al., 2003; McClosky et al., 2006; Clegg and Shepherd, 2005; McClosky et al., 2010) and supervised approaches (Titov and Henderson, 2006; Hara et al., 2007). The main focus of these works is on adapting parsing models trained with a specific genre of text (in most cases the Penn Treebank WSJ) to other genres of text, such as biomedical research papers and broadcast news. The major problem tackled in such tasks is the handling of unknown words and domain-specific manners of expression. However, parsing imperatives and questions involves a significantly different problem; even when all words in a sentence"
I11-1084,P06-1043,0,0.0275101,"omain. However, there are some cases where we cannot achieve such high parsing accuracy as parsing 2 Related work Since domain adaptation is an extensive research area in parsing research (Nivre et al., 2007), many ideas have been proposed, including un- or 749 Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 749–757, c Chiang Mai, Thailand, November 8 – 13, 2011. 2011 AFNLP and imperatives. In the experiments, we also used QuestionBank for comparison. semi-supervised approaches (Roark and Bacchiani, 2003; Blitzer et al., 2006; Steedman et al., 2003; McClosky et al., 2006; Clegg and Shepherd, 2005; McClosky et al., 2010) and supervised approaches (Titov and Henderson, 2006; Hara et al., 2007). The main focus of these works is on adapting parsing models trained with a specific genre of text (in most cases the Penn Treebank WSJ) to other genres of text, such as biomedical research papers and broadcast news. The major problem tackled in such tasks is the handling of unknown words and domain-specific manners of expression. However, parsing imperatives and questions involves a significantly different problem; even when all words in a sentence are known, the sentenc"
I11-1084,W06-2902,0,0.0245006,"2 Related work Since domain adaptation is an extensive research area in parsing research (Nivre et al., 2007), many ideas have been proposed, including un- or 749 Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 749–757, c Chiang Mai, Thailand, November 8 – 13, 2011. 2011 AFNLP and imperatives. In the experiments, we also used QuestionBank for comparison. semi-supervised approaches (Roark and Bacchiani, 2003; Blitzer et al., 2006; Steedman et al., 2003; McClosky et al., 2006; Clegg and Shepherd, 2005; McClosky et al., 2010) and supervised approaches (Titov and Henderson, 2006; Hara et al., 2007). The main focus of these works is on adapting parsing models trained with a specific genre of text (in most cases the Penn Treebank WSJ) to other genres of text, such as biomedical research papers and broadcast news. The major problem tackled in such tasks is the handling of unknown words and domain-specific manners of expression. However, parsing imperatives and questions involves a significantly different problem; even when all words in a sentence are known, the sentence has a very different structure from declarative sentences. Compared to domain adaptation, structural"
I11-1084,N10-1004,0,0.0265445,"not achieve such high parsing accuracy as parsing 2 Related work Since domain adaptation is an extensive research area in parsing research (Nivre et al., 2007), many ideas have been proposed, including un- or 749 Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 749–757, c Chiang Mai, Thailand, November 8 – 13, 2011. 2011 AFNLP and imperatives. In the experiments, we also used QuestionBank for comparison. semi-supervised approaches (Roark and Bacchiani, 2003; Blitzer et al., 2006; Steedman et al., 2003; McClosky et al., 2006; Clegg and Shepherd, 2005; McClosky et al., 2010) and supervised approaches (Titov and Henderson, 2006; Hara et al., 2007). The main focus of these works is on adapting parsing models trained with a specific genre of text (in most cases the Penn Treebank WSJ) to other genres of text, such as biomedical research papers and broadcast news. The major problem tackled in such tasks is the handling of unknown words and domain-specific manners of expression. However, parsing imperatives and questions involves a significantly different problem; even when all words in a sentence are known, the sentence has a very different structure from declarative"
I11-1084,P05-1012,0,0.0476,"come from QA queries. For example, such queries rarely include yes/no questions or tag questions. For our study, sentences were collected from the Brown Corpus, which includes a wider range of types of questions 3 Target Parsers and POS tagger We examined the performance of two dependency parsers and a deep parser on the target text sets. All parsers assumed that the input was already POS-tagged. We used the tagger in Tsuruoka et al. (2005). 3.1 MST and Malt parsers The MST and Malt parsers are dependency parsers that produce non-projective dependency trees, using the spanning tree algorithm (McDonald et al., 2005a; McDonald et al., 2005b)2 and transitionbased algorithm (Nivre et al., 2006)3 , respectively. Although the publicly available implementation of each parser also has the option to restrict the output to a projective dependency tree, we used the non-projective versions because the dependency structures converted from the question sentences in the Brown Corpus included many non-projective dependencies. We used the pennconverter (Johansson and Nugues, 2007) 4 to convert a PTB-style treebank into dependency trees 5 . To evaluate the output from each of the parsers, we used the labeled attachment"
I11-1084,H05-1066,0,0.0651665,"Missing"
I11-1084,P05-1011,1,0.816729,"iate. As a result, we extracted 750 imperative sentences and 1,241 question sentences from 24,243 sentences. Examples of extracted sentences are shown in Figure 1. Table 1 gives the statistics of the extracted sentences, which show that each genre contains top-level / embedded imperative and question sentences to some extent.7 As described below, we also used QuestionBank in the experiments. The advantage, however, of using the Brown treebank is that it includes annotations of function tags and empty categories, and therefore, we can apply the Penn Treebank-to-HPSG conversion program of Enju (Miyao and Tsujii, 2005), which relies on function tags and empty categories. Hence, we show experimental results for Enju only with the Brown data. It should also be noted that, a constituencyto-dependency converter, pennconverter (Johansson and Nugues, 2007), provides a more accurate conversion when function tags and empty categories are available (see footnote 4). Imperatives - Let &apos;s face it ! ! - Let this generation have theirs . - Believe me . - Make up your mind to pool your resources and get the most out of your remaining years of life . - Believe me ! ! - Find out what you like to do most and really give it"
I11-1084,nivre-etal-2006-maltparser,0,\N,Missing
I11-1084,D07-1096,0,\N,Missing
I11-1136,P04-1015,0,0.226468,".t ◦ s0 .t (b) s0 .w ◦ d s0 .t ◦ d s0 .w ◦ s0 .vl s1 .w ◦ s1 .vr s1 .w ◦ s1 .vl s0 .lc.w s0 .lc.t s1 .lc.w s1 .lc.t s1 .rc2 .w s1 .rc2 .t s0 .t ◦ s0 .lc.t ◦ s0 .lc2 .t s1 .t ◦ s1 .lc.t ◦ s1 .lc2 .t (c) j s2 .t s1 .w s1 .t s0 .w s0 .t (d) d s0 .vl s0 .lc.w s1 .rc.w s0 .lc2 .w s1 .rc2 .w s0 .lc2 .t s1 .rc2 .t To deal with conflicts between more than one of these actions, each action is associated with a score, and the score of a parser state is the total score of the actions that have been applied. To train the model, we adopt the averaged perceptron algorithm (Collins, 2002) with early update (Collins and Roark, 2004), following Huang and Sagae (2010). With the early update, whenever the gold action sequence falls off from the beam, the parameters are immediately updated with the rest of the sentence neglected. 2.2.2 Merging equivalent states Dynamic programming is enabled by merging equivalent states: if two states produce the same feature vector, they are merged into one state. Formally, a parser state (or configuration) ψ is described by h`, i, j, Si, where ` is the current step, [i . . . j] is the span of the top tree s0 in the stack S = (sd−1 , . . . , s0 ), where d is the depth of the stack. The equi"
I11-1136,W02-1001,0,0.181731,"plicable to any languages for which a projective shift-reduce parser works well. 2 Baseline Models First of all, we describe our baseline POS tagger and dependency parsers. These models will later be combined into pipelined models, which are then used as the baseline models in Section 4. 2.1 Baseline POS Tagger We build a baseline POS tagger, which uses the same POS-tagging features as those used in the state-of-the-art joint word segmentation and POS tagging model for Chinese (Zhang and Clark, 2008a). The list of features are shown in Table 1. We train the model with the averaged perceptron (Collins, 2002), and the decoding is performed using the Viterbi algorithm with beam search. Following Zhang and Clark (2008a), we use a tag dictionary and closed-set tags, which lead to improvement in both speed and accuracy. During training, the model stores all word–tag pairs into a tag dictionary, and for each word occurring more 2.2 Baseline Parsers For the baseline parsers for experiments, we build two dependency parsers: a reimplementation of the parser by Huang and Sagae (2010) (hereinafter Parser-HS), which is a shift-reduce dependency parser enhanced with dynamic programming (DP) using graph-struct"
I11-1136,D07-1098,0,0.0991258,"first-/secondwhere Φ order delayed features generated by action a be4 Experiment ing applied to ψ. When a S HIFT (t) action is performed, the model fills in the argument in the de4.1 Experimental Settings layed features with the newly-assigned tag t, as well as adding new delayed features it generates: We evaluate the performance of our joint parsers ~ 1 (ψ, SH (t)) + T (t, d~2 ), Φ ~ 2 (ψ, SH (t))i, and baseline models on the Chinese Penn Treehd~1 , d~2 i ← hΦ bank (CTB) 5 dataset. We use the standard split where T (t, d~2 ) is the resulting feature vector afof CTB-5 described in Duan et al. (2007) and the ter tag t is filled in to the first argument of the head-finding rules in Zhang and Clark (2008b). features in d~2 . Note that action SH (t) also adds We iteratively train each of the models and d~0 = T (t, d~1 ) to its (non-delayed) feature vector. choose the best model, in terms of the tagging acNote that the above formulation with the decuracy (for tagger) or word-level dependency aclayed features is equivalent to the model with full curacy (for parsers and joint parsers) on the devellook-ahead features if the exact decoding is peropment set, to use in the final evaluation. When fo"
I11-1136,P10-1110,0,0.220606,"ith underspecified POS tags of look-ahead words, we overcome this issue by introducing so-called delayed features. Our joint approach achieved substantial improvements over the pipeline and baseline systems in both POS tagging and dependency parsing task, achieving the new state-of-the-art performance on this joint task. 1    Introduction The tasks of part-of-speech (POS) tagging and dependency parsing have been widely investigated since the early stages of NLP research. Among mainstream approaches to dependency parsing, an incremental parsing framework is commonly used (e.g. Nivre (2008); Huang and Sagae (2010)), mainly because it achieves state-of-the-art accuracy while retaining linear-time computational complexity, and is also considered to reflect how humans process natural language sentences (Frazier and Rayner, 1982). However, although some of the Chinese POS tags require long-range syntactic information in order to be disambiguated, to the extent of our knowledge, none of the previous approaches have addressed the joint modeling of these two tasks in an incremental framework. Also, since POS tagging is a preliminary step for dependency parsing, the traditional pipeline approach may suffer fro"
I11-1136,P08-1102,0,0.0699744,"Missing"
I11-1136,C08-1049,0,0.0608786,"Missing"
I11-1136,P03-1054,0,0.00400021,"years, joint segmentation and tagging have been widely investigated (e.g. Zhang and Clark (2010); Kruengkrai et al. (2009); Zhang and Clark (2008a); Jiang et al. (2008a); Jiang et al. (2008b)). Particularly, our framework of using a single perceptron to solve the joint problem is motivated by Zhang and Clark (2008a). Also, our joint parsing framework is an extension of Huang and Sagae (2010)’s framework, which is described in detail in Section 2.2. In constituency parsing, the parsing naturally involves the POS tagging since the non-terminal symbols are commonly associated with POS tags (e.g. Klein and Manning (2003)). Rush et al. (2010) proposed to use dual composition to combine a constituency parser and a trigram POS tagger, showing the effectiveness of taking advantage of these two systems. In dependency parsing, Lee et al. (2011) recently proposed a discriminative graphical model that solves morphological disambiguation and dependency parsing jointly. However, their main focus was to capture interaction between morphology and syntax in morphologically-rich, highlyinflected languages (such as Latin and Ancient Greek), which are unlike Chinese. More recently, Li et al. (2011) proposed the first joint m"
I11-1136,P09-1058,0,0.14012,"Missing"
I11-1136,P11-1089,0,0.0529511,"g a single perceptron to solve the joint problem is motivated by Zhang and Clark (2008a). Also, our joint parsing framework is an extension of Huang and Sagae (2010)’s framework, which is described in detail in Section 2.2. In constituency parsing, the parsing naturally involves the POS tagging since the non-terminal symbols are commonly associated with POS tags (e.g. Klein and Manning (2003)). Rush et al. (2010) proposed to use dual composition to combine a constituency parser and a trigram POS tagger, showing the effectiveness of taking advantage of these two systems. In dependency parsing, Lee et al. (2011) recently proposed a discriminative graphical model that solves morphological disambiguation and dependency parsing jointly. However, their main focus was to capture interaction between morphology and syntax in morphologically-rich, highlyinflected languages (such as Latin and Ancient Greek), which are unlike Chinese. More recently, Li et al. (2011) proposed the first joint model for Chinese POS tagging and dependency parsing in a graph-based parsing framework, which is one of our baseline systems. On the other hand, our work is the first incremental approach to this joint task. 6 Conclusion I"
I11-1136,D11-1109,0,0.41298,"ith POS tags (e.g. Klein and Manning (2003)). Rush et al. (2010) proposed to use dual composition to combine a constituency parser and a trigram POS tagger, showing the effectiveness of taking advantage of these two systems. In dependency parsing, Lee et al. (2011) recently proposed a discriminative graphical model that solves morphological disambiguation and dependency parsing jointly. However, their main focus was to capture interaction between morphology and syntax in morphologically-rich, highlyinflected languages (such as Latin and Ancient Greek), which are unlike Chinese. More recently, Li et al. (2011) proposed the first joint model for Chinese POS tagging and dependency parsing in a graph-based parsing framework, which is one of our baseline systems. On the other hand, our work is the first incremental approach to this joint task. 6 Conclusion In this paper, we have presented the first joint approach that successfully solves POS tagging and dependency parsing on an incremental framework. The proposed joint models outperform the pipeline models in terms of both tagging and dependency parsing accuracies, and our best model achieved the new state-of-the-art performance on this joint task, whi"
I11-1136,J08-4003,0,0.136623,"difficulties with underspecified POS tags of look-ahead words, we overcome this issue by introducing so-called delayed features. Our joint approach achieved substantial improvements over the pipeline and baseline systems in both POS tagging and dependency parsing task, achieving the new state-of-the-art performance on this joint task. 1    Introduction The tasks of part-of-speech (POS) tagging and dependency parsing have been widely investigated since the early stages of NLP research. Among mainstream approaches to dependency parsing, an incremental parsing framework is commonly used (e.g. Nivre (2008); Huang and Sagae (2010)), mainly because it achieves state-of-the-art accuracy while retaining linear-time computational complexity, and is also considered to reflect how humans process natural language sentences (Frazier and Rayner, 1982). However, although some of the Chinese POS tags require long-range syntactic information in order to be disambiguated, to the extent of our knowledge, none of the previous approaches have addressed the joint modeling of these two tasks in an incremental framework. Also, since POS tagging is a preliminary step for dependency parsing, the traditional pipeline"
I11-1136,D10-1001,0,0.0463127,"nd tagging have been widely investigated (e.g. Zhang and Clark (2010); Kruengkrai et al. (2009); Zhang and Clark (2008a); Jiang et al. (2008a); Jiang et al. (2008b)). Particularly, our framework of using a single perceptron to solve the joint problem is motivated by Zhang and Clark (2008a). Also, our joint parsing framework is an extension of Huang and Sagae (2010)’s framework, which is described in detail in Section 2.2. In constituency parsing, the parsing naturally involves the POS tagging since the non-terminal symbols are commonly associated with POS tags (e.g. Klein and Manning (2003)). Rush et al. (2010) proposed to use dual composition to combine a constituency parser and a trigram POS tagger, showing the effectiveness of taking advantage of these two systems. In dependency parsing, Lee et al. (2011) recently proposed a discriminative graphical model that solves morphological disambiguation and dependency parsing jointly. However, their main focus was to capture interaction between morphology and syntax in morphologically-rich, highlyinflected languages (such as Latin and Ancient Greek), which are unlike Chinese. More recently, Li et al. (2011) proposed the first joint model for Chinese POS"
I11-1136,J95-2002,0,0.0248697,"ociated with dependency labels and head information of stack elements, are not included since our framework is based on unlabeled dependencies and the arc-standard strategy. The additional features for Parser-ZN− require the features in Table 2 (d) to be added into the set of kernel features. 2.2.4 Beam search with DP In the shift-reduce parsing with dynamic programming, we cannot simply apply beam search as in a non-DP shift-reduce parsing, because each state does not have a unique score any more. To decide the ordering of states within the beam, the concept of prefix score and inside score (Stolcke, 1995) is adopted. The prefix score ξ is the total score of the best action sequence from the initial state to the current state, while the inside score η 1218 (a) q0 .t q0 .w ◦ q0 .t s0 .t ◦ q0 .t ◦ q1 .t s0 .w ◦ q0 .t ◦ q1 .t (b) t ◦ s0 .w t ◦ s0 .w ◦ q0 .w t ◦ B(s0 .w) ◦ q0 .w t ◦ s0 .t ◦ s0 .rc.t t ◦ s0 .w ◦ s0 .t ◦ s0 .rc.t (c) j s2 .t q0 .w s1 .w s1 .t s0 .w s0 .t is the score of the tree on the top of the stack. With these scores and a set of predictor states Π(ψ) of state ψ, the full description of state ψ takes the form ψ : h`, i, j, S; ξ, η, Πi. The calculation of the prefix and inside sco"
I11-1136,P11-1139,0,0.048531,"Missing"
I11-1136,P08-1101,0,0.443622,"sion on the results and error analysis. Although we specifically focus on Chinese in this work, our joint model is applicable to any languages for which a projective shift-reduce parser works well. 2 Baseline Models First of all, we describe our baseline POS tagger and dependency parsers. These models will later be combined into pipelined models, which are then used as the baseline models in Section 4. 2.1 Baseline POS Tagger We build a baseline POS tagger, which uses the same POS-tagging features as those used in the state-of-the-art joint word segmentation and POS tagging model for Chinese (Zhang and Clark, 2008a). The list of features are shown in Table 1. We train the model with the averaged perceptron (Collins, 2002), and the decoding is performed using the Viterbi algorithm with beam search. Following Zhang and Clark (2008a), we use a tag dictionary and closed-set tags, which lead to improvement in both speed and accuracy. During training, the model stores all word–tag pairs into a tag dictionary, and for each word occurring more 2.2 Baseline Parsers For the baseline parsers for experiments, we build two dependency parsers: a reimplementation of the parser by Huang and Sagae (2010) (hereinafter P"
I11-1136,D08-1059,0,0.701122,"sion on the results and error analysis. Although we specifically focus on Chinese in this work, our joint model is applicable to any languages for which a projective shift-reduce parser works well. 2 Baseline Models First of all, we describe our baseline POS tagger and dependency parsers. These models will later be combined into pipelined models, which are then used as the baseline models in Section 4. 2.1 Baseline POS Tagger We build a baseline POS tagger, which uses the same POS-tagging features as those used in the state-of-the-art joint word segmentation and POS tagging model for Chinese (Zhang and Clark, 2008a). The list of features are shown in Table 1. We train the model with the averaged perceptron (Collins, 2002), and the decoding is performed using the Viterbi algorithm with beam search. Following Zhang and Clark (2008a), we use a tag dictionary and closed-set tags, which lead to improvement in both speed and accuracy. During training, the model stores all word–tag pairs into a tag dictionary, and for each word occurring more 2.2 Baseline Parsers For the baseline parsers for experiments, we build two dependency parsers: a reimplementation of the parser by Huang and Sagae (2010) (hereinafter P"
I11-1136,D10-1082,0,0.130141,"Missing"
I11-1136,P11-2033,0,0.280377,"se a tag dictionary and closed-set tags, which lead to improvement in both speed and accuracy. During training, the model stores all word–tag pairs into a tag dictionary, and for each word occurring more 2.2 Baseline Parsers For the baseline parsers for experiments, we build two dependency parsers: a reimplementation of the parser by Huang and Sagae (2010) (hereinafter Parser-HS), which is a shift-reduce dependency parser enhanced with dynamic programming (DP) using graph-structured stack (GSS; Tomita (1991)), and our extension of Parser-HS by incorporating a richer set of features taken from Zhang and Nivre (2011) (hereinafter Parser-ZN), which is originally a non-DP arc-eager dependency parser and achieves the current state-of-theart performance for Chinese dependency parsing. In this section, we briefly describe these models since the features and DP formalism serve as a basis for the joint models described in Section 3. 2.2.1 Shift-reduce parsing Shift-reduce dependency parsing algorithms incrementally process an input sentence from left to right. In the framework known as “arc-standard” (Nivre, 2008), the parser performs one of the following three actions at each step: • S HIFT (SH): move the first"
I11-1136,P08-1000,0,\N,Missing
J08-1002,J97-4005,0,0.00941329,"ammars (Johnson et al. 1999; Riezler et al. 2000, 2002; Geman and Johnson 2002; Clark and Curran 2003, 2004b; Kaplan et al. 2004; Carroll and Oepen 2005). Previous studies on probabilistic models for HPSG (Oepen, Toutanova et al. 2002; Toutanova and Manning 2002; Baldridge and Osborne 2003; Malouf and van Noord 2004) have also adopted log-linear models. This is because these grammar formalisms exploit feature structures to represent linguistic constraints. Such constraints are known to introduce inconsistencies in probabilistic models estimated using simple relative frequency, as discussed in Abney (1997). The maximum entropy model is a reasonable choice for credible probabilistic models. It also allows various overlapping features to be incorporated, and we can expect higher accuracy in disambiguation. A maximum entropy model gives a probabilistic distribution that maximizes the likelihood of training data under given feature functions. Given training data E = {x, y}, a maximum entropy model gives conditional probability p(y|x) as follows. Deﬁnition 1 (Maximum entropy model) A maximum entropy model is deﬁned as the solution of the following optimization problem. pM (y|x) = argmax p    −"
J08-1002,W03-0403,0,0.217533,", statistical modeling of these grammars is attracting considerable attention. This is because natural language processing applications usually require disambiguated or ranked parse results, and statistical modeling of syntactic/semantic preference is one of the most promising methods for disambiguation. The focus of this article is the problem of probabilistic modeling of wide-coverage HPSG parsing. Although previous studies have proposed maximum entropy models (Berger, Della Pietra, and Della Pietra 1996) of HPSG-style parse trees (Oepen, Toutanova, et al. 2002b; Toutanova and Manning 2002; Baldridge and Osborne 2003; Malouf and van Noord 2004), the straightforward application of maximum entropy models to wide-coverage HPSG parsing is infeasible because estimation of maximum entropy models is computationally expensive, especially when targeting wide-coverage parsing. In general, complete structures, such as transition sequences in Markov models and parse trees, have an exponential number of ambiguities. This causes an exponential explosion when estimating the parameters of maximum entropy models. We therefore require solutions to make model estimation tractable. This article ﬁrst proposes feature forest m"
J08-1002,J96-1002,0,0.0442627,"Missing"
J08-1002,I05-1015,0,0.0106983,"ure forest models to probabilistic HPSG parsing. Section 5 presents an empirical evaluation of probabilistic HPSG parsing, and Section 6 introduces research related to our proposals. Section 7 concludes. 2. Problem Maximum entropy models (Berger, Della Pietra, and Della Pietra 1996) are now becoming the de facto standard approach for disambiguation models for lexicalized or 36 Miyao and Tsujii Feature Forest Models for Probabilistic HPSG Parsing feature structure grammars (Johnson et al. 1999; Riezler et al. 2000, 2002; Geman and Johnson 2002; Clark and Curran 2003, 2004b; Kaplan et al. 2004; Carroll and Oepen 2005). Previous studies on probabilistic models for HPSG (Oepen, Toutanova et al. 2002; Toutanova and Manning 2002; Baldridge and Osborne 2003; Malouf and van Noord 2004) have also adopted log-linear models. This is because these grammar formalisms exploit feature structures to represent linguistic constraints. Such constraints are known to introduce inconsistencies in probabilistic models estimated using simple relative frequency, as discussed in Abney (1997). The maximum entropy model is a reasonable choice for credible probabilistic models. It also allows various overlapping features to be incor"
J08-1002,A00-2018,0,0.0198204,"reebank derived from Sections 02–21 of the Wall Street Journal portion of the Penn Treebank, that is, the same set used for lexicon extraction. For training of the disambiguation models, we eliminated sentences of 40 words or more and sentences for which the parser could not produce the correct parses. The resulting training set consists of 33,604 sentences (when n = 10 and = 0.95; see Section 5.4 for details). The treebanks derived from Sections 22 and 23 were used as the development and ﬁnal test sets, respectively. Following previous studies on parsing with PCFG-based models (Collins 1997; Charniak 2000), accuracy is measured for sentences of less than 40 words and for those with less than 100 words. Table 5 shows the speciﬁcations of the test data. The measure for evaluating parsing accuracy is precision/recall of predicate– argument dependencies output by the parser. A predicate–argument dependency is deﬁned as a tuple wh , wn , π, ρ, where wh is the head word of the predicate, wn is the head word of the argument, π is the type of the predicate (e.g., adjective, intransitive verb), and ρ is an argument label (MODARG, ARG1, . . ., ARG4). For example, He tried running has three dependencies"
J08-1002,P05-1022,0,0.0543457,"hether dynamic programming or sampling can deliver a better balance of estimation efﬁciency and accuracy. The answer will differ in different problems. When most effective features can be represented locally in tractablesize feature forests, dynamic programming methods including ours are suitable. However, when global context features are indispensable for high accuracy, sampling methods might be better. We should also investigate compromise solutions such as dynamic CRFs (McCallum, Rohanimanesh, and Sutton 2003; Sutton, Rohanimanesh, and McCallum 2004) and reranking techniques (Collins 2000; Charniak and Johnson 2005). There is no analytical way of predicting the best solution, and it must be investigated experimentally for each target task. 7. Conclusion A dynamic programming algorithm was presented for maximum entropy modeling and shown to provide a solution to the parameter estimation of probabilistic models of complete structures without the independence assumption. We ﬁrst deﬁned the notion of a feature forest, which is a packed representation of an exponential number of trees of features. When training data is represented with feature forests, model parameters are estimated at a tractable cost withou"
J08-1002,W03-1013,0,0.191187,"o, Ninomiya, and Tsujii 2003; Miyao and Tsujii 2003, 2005). Together with the parameter estimation algorithm for feature forest models, these methods constitute a complete procedure for the probabilistic modeling of wide-coverage HPSG parsing. The methods we propose here were applied to an English HPSG parser, Enju (Tsujii Laboratory 2004). We report on an extensive evaluation of the parser through parsing experiments on the Wall Street Journal portion of the Penn Treebank (Marcus et al. 1994). The content of this article is an extended version of our earlier work reported in Miyao and Tsujii (2002, 2003, 2005) and Miyao, Ninomiya, and Tsujii (2003). The major contribution of this article is a strict mathematical deﬁnition of the feature forest model and the parameter estimation algorithm, which are substantially reﬁned and extended from Miyao and Tsujii (2002). Another contribution is that this article thoroughly discusses the relationships between the feature forest model and its application to HPSG parsing. We also provide an extensive empirical evaluation of the resulting HPSG parsing approach using real-world text. Section 2 discusses a problem of conventional probabilistic models for lexicaliz"
J08-1002,C04-1041,0,0.034581,"ning, transitive verb, ARG2 running, he, intransitive verb, ARG1 Labeled precision/recall (LP/LR) is the ratio of tuples correctly identiﬁed by the parser, and unlabeled precision/recall (UP/UR) is the ratio of wh and wn correctly identiﬁed regardless of π and ρ. F-score is the harmonic mean of LP and LR. Sentence accuracy is the exact match accuracy of complete predicate–argument relations in a sentence. These measures correspond to those used in other studies measuring the accuracy of predicate–argument dependencies in CCG parsing (Clark, Hockenmaier, and Steedman 2002; Hockenmaier 2003; Clark and Curran 2004b) and LFG parsing (Burke et al. 2004), although exact ﬁgures cannot be compared directly because the deﬁnitions of dependencies are different. All predicate–argument dependencies in a sentence are the target of evaluation except quotation marks and periods. The accuracy is measured by parsing test sentences with gold-standard part-of-speech tags from the Penn Treebank unless otherwise noted. The Gaussian prior was used for smoothing (Chen and Rosenfeld 1999a), and its hyper-parameter was tuned for each model to maximize F-score for the development set. The algorithm for parameter estimation w"
J08-1002,P04-1014,0,0.029243,"Missing"
J08-1002,P02-1042,0,0.0160518,"Missing"
J08-1002,P97-1003,0,0.0325988,"was the HPSG treebank derived from Sections 02–21 of the Wall Street Journal portion of the Penn Treebank, that is, the same set used for lexicon extraction. For training of the disambiguation models, we eliminated sentences of 40 words or more and sentences for which the parser could not produce the correct parses. The resulting training set consists of 33,604 sentences (when n = 10 and = 0.95; see Section 5.4 for details). The treebanks derived from Sections 22 and 23 were used as the development and ﬁnal test sets, respectively. Following previous studies on parsing with PCFG-based models (Collins 1997; Charniak 2000), accuracy is measured for sentences of less than 40 words and for those with less than 100 words. Table 5 shows the speciﬁcations of the test data. The measure for evaluating parsing accuracy is precision/recall of predicate– argument dependencies output by the parser. A predicate–argument dependency is deﬁned as a tuple wh , wn , π, ρ, where wh is the head word of the predicate, wn is the head word of the argument, π is the type of the predicate (e.g., adjective, intransitive verb), and ρ is an argument label (MODARG, ARG1, . . ., ARG4). For example, He tried running has th"
J08-1002,J03-4003,0,0.0849956,"Missing"
J08-1002,1995.tmi-1.2,0,0.0621836,"or the wh-extraction of the object of love (left) and for the control construction of try (right). The ﬁrst condition is satisﬁed because both lexical entries refer to CONT|HOOK of argument signs in SUBJ, COMPS, and SLASH. None of the lexical entries directly access ARGX of the arguments. The second condition is also satisﬁed because the values of CONT|HOOK of all of the argument signs are percolated to ARGX of the mother. In addition, the elements in CONT|RELS are percolated to the mother by the Semantic Principle. Compositional semantics usually satisﬁes the above conditions, including MRS (Copestake et al. 1995, 2006). The composition of MRS refers to HOOK, and no internal structures of daughters. The Semantic Principle of MRS also assures that all semantic relations in RELS are percolated to the mother. When these conditions are satisﬁed, semantics may include any constraints, such as selectional restrictions, although the grammar we used in the experiments does not include semantic restrictions to constrain parse forests. Under these conditions, local structures of predicate–argument structures are encoded into a conjunctive node when the values of all of its arguments have been instantiated. We i"
J08-1002,P02-1036,0,0.40534,"o, Ninomiya, and Tsujii 2003; Miyao and Tsujii 2003, 2005). Together with the parameter estimation algorithm for feature forest models, these methods constitute a complete procedure for the probabilistic modeling of wide-coverage HPSG parsing. The methods we propose here were applied to an English HPSG parser, Enju (Tsujii Laboratory 2004). We report on an extensive evaluation of the parser through parsing experiments on the Wall Street Journal portion of the Penn Treebank (Marcus et al. 1994). The content of this article is an extended version of our earlier work reported in Miyao and Tsujii (2002, 2003, 2005) and Miyao, Ninomiya, and Tsujii (2003). The major contribution of this article is a strict mathematical deﬁnition of the feature forest model and the parameter estimation algorithm, which are substantially reﬁned and extended from Miyao and Tsujii (2002). Another contribution is that this article thoroughly discusses the relationships between the feature forest model and its application to HPSG parsing. We also provide an extensive empirical evaluation of the resulting HPSG parsing approach using real-world text. Section 2 discusses a problem of conventional probabilistic models for lexicaliz"
J08-1002,W01-0521,0,0.0472805,"Missing"
J08-1002,P03-1046,0,0.0161053,"arsing tried, running, transitive verb, ARG2 running, he, intransitive verb, ARG1 Labeled precision/recall (LP/LR) is the ratio of tuples correctly identiﬁed by the parser, and unlabeled precision/recall (UP/UR) is the ratio of wh and wn correctly identiﬁed regardless of π and ρ. F-score is the harmonic mean of LP and LR. Sentence accuracy is the exact match accuracy of complete predicate–argument relations in a sentence. These measures correspond to those used in other studies measuring the accuracy of predicate–argument dependencies in CCG parsing (Clark, Hockenmaier, and Steedman 2002; Hockenmaier 2003; Clark and Curran 2004b) and LFG parsing (Burke et al. 2004), although exact ﬁgures cannot be compared directly because the deﬁnitions of dependencies are different. All predicate–argument dependencies in a sentence are the target of evaluation except quotation marks and periods. The accuracy is measured by parsing test sentences with gold-standard part-of-speech tags from the Penn Treebank unless otherwise noted. The Gaussian prior was used for smoothing (Chen and Rosenfeld 1999a), and its hyper-parameter was tuned for each model to maximize F-score for the development set. The algorithm for"
J08-1002,hockenmaier-steedman-2002-acquiring,0,0.00870125,"s is possible when they are represented by feature forests. This article also describes methods for representing HPSG syntactic structures and predicate–argument structures with feature forests. Hence, we describe a complete strategy for developing probabilistic models for HPSG parsing. The effectiveness of the proposed methods is empirically evaluated through parsing experiments on the Penn Treebank, and the promise of applicability to parsing of real-world sentences is discussed. 1. Introduction Following the successful development of wide-coverage lexicalized grammars (Riezler et al. 2000; Hockenmaier and Steedman 2002; Burke et al. 2004; Miyao, Ninomiya, and ∗ Department of Computer Science, University of Tokyo, Hongo 7-3-1, Bunkyo-ku, Tokyo 113-0033 Japan. E-mail: yusuke@is.s.u-tokyo.ac.jp. ∗∗ Department of Computer Science, University of Tokyo, Hongo 7-3-1, Bunkyo-ku, Tokyo 113-0033 Japan. E-mail: tsujii@is.s.u-tokyo.ac.jp. Submission received: 11 June 2006; revised submission received: 2 March 2007; accepted for publication: 5 May 2007. © 2008 Association for Computational Linguistics Computational Linguistics Volume 34, Number 1 Tsujii 2005), statistical modeling of these grammars is attracting conside"
J08-1002,P99-1069,0,0.0207855,"d grammars. Section 3 proposes feature forest models for solving this problem. Section 4 describes the application of feature forest models to probabilistic HPSG parsing. Section 5 presents an empirical evaluation of probabilistic HPSG parsing, and Section 6 introduces research related to our proposals. Section 7 concludes. 2. Problem Maximum entropy models (Berger, Della Pietra, and Della Pietra 1996) are now becoming the de facto standard approach for disambiguation models for lexicalized or 36 Miyao and Tsujii Feature Forest Models for Probabilistic HPSG Parsing feature structure grammars (Johnson et al. 1999; Riezler et al. 2000, 2002; Geman and Johnson 2002; Clark and Curran 2003, 2004b; Kaplan et al. 2004; Carroll and Oepen 2005). Previous studies on probabilistic models for HPSG (Oepen, Toutanova et al. 2002; Toutanova and Manning 2002; Baldridge and Osborne 2003; Malouf and van Noord 2004) have also adopted log-linear models. This is because these grammar formalisms exploit feature structures to represent linguistic constraints. Such constraints are known to introduce inconsistencies in probabilistic models estimated using simple relative frequency, as discussed in Abney (1997). The maximum e"
J08-1002,A00-2021,0,0.0119105,"experimentally in Section 5.4. We have several ways to integrate p¯ with the estimated model p(t|T(w)). In the experiments, we will empirically compare the following methods in terms of accuracy and estimation time. Filtering only: The unigram probability p¯ is used only for ﬁltering in training. Product: The probability is deﬁned as the product of p¯ and the estimated model p. Reference distribution: p¯ is used as a reference distribution of p. Feature function: log p¯ is used as a feature function of p. This method has been shown to be a generalization of the reference distribution method (Johnson and Riezler 2000). 4.5 Features Feature functions in maximum entropy models are designed to capture the characteristics of em , el , er . In this article, we investigate combinations of the atomic features listed Figure 19 Filtering of lexical entries for saw. 57 Computational Linguistics Volume 34, Number 1 Table 1 Templates for atomic features. RULE DIST COMMA SPAN SYM WORD POS LE ARG name of the applied schema distance between the head words of the daughters whether a comma exists between daughters and/or inside of daughter phrases number of words dominated by the phrase symbol of the phrasal category (e."
J08-1002,N04-1013,0,0.0192178,"Missing"
J08-1002,W02-2018,0,0.0626834,"local ambiguities in parse trees potentially cause exponential growth in the number of structures assigned to sub-sequences of words, resulting in billions of structures for whole sentences. For example, when we apply rewriting rule S → NP VP, and the left NP and the right VP, respectively, have n and m ambiguous subtrees, the result of the rule application generates n × m trees. This is problematic because the complexity of parameter estimation is proportional to the size of Y(x). The cost of the parameter estimation algorithms is bound by the computation of model expectation, µi , given as (Malouf 2002):   ˜ p(x) µi = x∈X y∈Y(x)   ˜ p(x) = x∈X y∈Y(x) fi (x, y)p(y|x)    fi (x, y) 1 exp  λj fj (x, y) Z(x) (1) j As shown in this deﬁnition, the computation of model expectation requires the summation over Y(x) for every x in the training data. The complexity of the overall estimation algorithm is O(|Y˜ ||F˜ ||E |), where |Y˜ |and |F˜ |are the average numbers of y and activated features for an event, respectively, and |E |is the number of events. When Y(x) grows exponentially, the parameter estimation becomes intractable. In PCFGs, the problem of computing probabilities of parse trees is"
J08-1002,H94-1020,0,0.0157438,"sing. We describe methods for representing HPSG parse trees and predicate–argument structures using feature forests (Miyao, Ninomiya, and Tsujii 2003; Miyao and Tsujii 2003, 2005). Together with the parameter estimation algorithm for feature forest models, these methods constitute a complete procedure for the probabilistic modeling of wide-coverage HPSG parsing. The methods we propose here were applied to an English HPSG parser, Enju (Tsujii Laboratory 2004). We report on an extensive evaluation of the parser through parsing experiments on the Wall Street Journal portion of the Penn Treebank (Marcus et al. 1994). The content of this article is an extended version of our earlier work reported in Miyao and Tsujii (2002, 2003, 2005) and Miyao, Ninomiya, and Tsujii (2003). The major contribution of this article is a strict mathematical deﬁnition of the feature forest model and the parameter estimation algorithm, which are substantially reﬁned and extended from Miyao and Tsujii (2002). Another contribution is that this article thoroughly discusses the relationships between the feature forest model and its application to HPSG parsing. We also provide an extensive empirical evaluation of the resulting HPSG"
J08-1002,W03-0430,0,0.0168907,"ecause feature forests can represent Markov chains. In an analogy, CRFs correspond to HMMs, whereas feature forest models correspond to PCFGs. 71 Computational Linguistics Volume 34, Number 1 Extensions of CRFs, such as semi-Markov CRFs (Sarawagi and Cohen 2004), are also regarded as instances of feature forest models. This fact implies that our algorithm is applicable to not only parsing but also to other tasks. CRFs are now widely used for sequence-based tasks, such as parts-of-speech tagging and named entity recognition, and have been shown to achieve the best performance in various tasks (McCallum and Li 2003; McCallum, Rohanimanesh, and Sutton 2003; Pinto et al. 2003; Sha and Pereira 2003; Peng and McCallum 2004; Roark et al. 2004; Settles 2004; Sutton, Rohanimanesh, and McCallum 2004). These results suggest that the method proposed in the present article will achieve high accuracy when applied to various statistical models with tree structures. Dynamic CRFs (McCallum, Rohanimanesh, and Sutton 2003; Sutton, Rohanimanesh, and McCallum 2004) provide us with an interesting inspiration for extending feature forest models. The purpose of dynamic CRFs is to incorporate feature functions that are not re"
J08-1002,P06-1128,1,0.50409,"Missing"
J08-1002,W03-0401,1,0.836753,"Missing"
J08-1002,P05-1011,1,0.550494,"o, Ninomiya, and Tsujii 2003; Miyao and Tsujii 2003, 2005). Together with the parameter estimation algorithm for feature forest models, these methods constitute a complete procedure for the probabilistic modeling of wide-coverage HPSG parsing. The methods we propose here were applied to an English HPSG parser, Enju (Tsujii Laboratory 2004). We report on an extensive evaluation of the parser through parsing experiments on the Wall Street Journal portion of the Penn Treebank (Marcus et al. 1994). The content of this article is an extended version of our earlier work reported in Miyao and Tsujii (2002, 2003, 2005) and Miyao, Ninomiya, and Tsujii (2003). The major contribution of this article is a strict mathematical deﬁnition of the feature forest model and the parameter estimation algorithm, which are substantially reﬁned and extended from Miyao and Tsujii (2002). Another contribution is that this article thoroughly discusses the relationships between the feature forest model and its application to HPSG parsing. We also provide an extensive empirical evaluation of the resulting HPSG parsing approach using real-world text. Section 2 discusses a problem of conventional probabilistic models for lexicaliz"
J08-1002,W05-1510,1,0.463395,"Missing"
J08-1002,W05-1511,1,0.72648,"test sentences with gold-standard part-of-speech tags from the Penn Treebank unless otherwise noted. The Gaussian prior was used for smoothing (Chen and Rosenfeld 1999a), and its hyper-parameter was tuned for each model to maximize F-score for the development set. The algorithm for parameter estimation was the limited-memory BFGS method (Nocedal 1980; Nocedal and Wright 1999). The parser was implemented in C++ with the LiLFeS library (Makino et al. 2002), and various speed-up techniques for HPSG parsing were used such as quick check and iterative beam search (Tsuruoka, Miyao, and Tsujii 2004; Ninomiya et al. 2005). Other efﬁcient parsing techniques, including global thresholding, hybrid parsing with a chunk parser, and large constituent inhibition, were not used. The results obtained using these techniques are given in Ninomiya et al. A limit on the number of constituents was set for time-out; the parser stopped parsing when the number of constituents created during parsing exceeded 50,000. In such a case, the parser output nothing, and the recall was computed as zero. Features occurring more than twice were included in the probabilistic models. A method of ﬁltering lexical entries was applied to the p"
J08-1002,A00-2022,0,0.0617176,"Missing"
J08-1002,C02-2025,0,0.0131052,"Missing"
J08-1002,C00-1085,0,0.16404,"le approach to avoid this problem is to develop a fully restrictive grammar that never causes an exponential explosion, although the development of such a grammar requires considerable effort and it cannot be acquired from treebanks using existing approaches. We think that exponential explosion is inevitable, particularly with the large-scale wide-coverage grammars required to analyze real-world texts. In such cases, these methods of model estimation are intractable. Another approach to estimating log-linear models for HPSG was to extract a small informative sample from the original set T(w) (Osborne 2000). The method was successfully applied to Dutch HPSG parsing (Malouf and van Noord 2004). A possible problem with this method is in the approximation of exponentially many parse trees by a polynomial-size sample. However, their method has an advantage in that any features on parse results can be incorporated into a model, whereas our method forces feature functions to be deﬁned locally on conjunctive nodes. We will discuss the trade-off between the approximation solution and the locality of feature functions in Section 6.3. Non-probabilistic statistical classiﬁers have also been applied to disa"
J08-1002,N04-1042,0,0.00894058,"ure forest models correspond to PCFGs. 71 Computational Linguistics Volume 34, Number 1 Extensions of CRFs, such as semi-Markov CRFs (Sarawagi and Cohen 2004), are also regarded as instances of feature forest models. This fact implies that our algorithm is applicable to not only parsing but also to other tasks. CRFs are now widely used for sequence-based tasks, such as parts-of-speech tagging and named entity recognition, and have been shown to achieve the best performance in various tasks (McCallum and Li 2003; McCallum, Rohanimanesh, and Sutton 2003; Pinto et al. 2003; Sha and Pereira 2003; Peng and McCallum 2004; Roark et al. 2004; Settles 2004; Sutton, Rohanimanesh, and McCallum 2004). These results suggest that the method proposed in the present article will achieve high accuracy when applied to various statistical models with tree structures. Dynamic CRFs (McCallum, Rohanimanesh, and Sutton 2003; Sutton, Rohanimanesh, and McCallum 2004) provide us with an interesting inspiration for extending feature forest models. The purpose of dynamic CRFs is to incorporate feature functions that are not represented locally, and the solution is to apply a variational method, which is an algorithm of numerical c"
J08-1002,P02-1035,0,0.0100621,"Missing"
J08-1002,P00-1061,0,0.176824,"of any data structures is possible when they are represented by feature forests. This article also describes methods for representing HPSG syntactic structures and predicate–argument structures with feature forests. Hence, we describe a complete strategy for developing probabilistic models for HPSG parsing. The effectiveness of the proposed methods is empirically evaluated through parsing experiments on the Penn Treebank, and the promise of applicability to parsing of real-world sentences is discussed. 1. Introduction Following the successful development of wide-coverage lexicalized grammars (Riezler et al. 2000; Hockenmaier and Steedman 2002; Burke et al. 2004; Miyao, Ninomiya, and ∗ Department of Computer Science, University of Tokyo, Hongo 7-3-1, Bunkyo-ku, Tokyo 113-0033 Japan. E-mail: yusuke@is.s.u-tokyo.ac.jp. ∗∗ Department of Computer Science, University of Tokyo, Hongo 7-3-1, Bunkyo-ku, Tokyo 113-0033 Japan. E-mail: tsujii@is.s.u-tokyo.ac.jp. Submission received: 11 June 2006; revised submission received: 2 March 2007; accepted for publication: 5 May 2007. © 2008 Association for Computational Linguistics Computational Linguistics Volume 34, Number 1 Tsujii 2005), statistical modeling of these"
J08-1002,W04-3223,0,0.0738302,"aused by errors of argument/modiﬁer distinction in to-inﬁnitive clauses. A signiﬁcant portion of the errors discussed above cannot be resolved by the features we investigated in this study, and the design of other features will be necessary for improving parsing accuracy. 6. Discussion 6.1 Probabilistic Modeling of Complete Structures The model described in this article was ﬁrst published in Miyao and Tsujii (2002), and has been applied to probabilistic models for parsing with lexicalized grammars. Applications to CCG parsing (Clark and Curran 2003, 2004b) and LFG parsing (Kaplan et al. 2004; Riezler and Vasserman 2004) demonstrated that feature forest models attained higher accuracy than other models. These researchers applied feature forests to representations of the packed parse results of LFG and the dependency/derivation structures of CCG. Their work demonstrated the applicability and effectiveness of feature forest models in parsing with wide-coverage lexicalized grammars. Feature forest models were also shown to be effective for wide-coverage sentence realization (Nakanishi, Miyao, and Tsujii 2005). This work demonstrated that feature forest models are generic enough to be applied to natural language"
J08-1002,P04-1007,0,0.0238318,"pond to PCFGs. 71 Computational Linguistics Volume 34, Number 1 Extensions of CRFs, such as semi-Markov CRFs (Sarawagi and Cohen 2004), are also regarded as instances of feature forest models. This fact implies that our algorithm is applicable to not only parsing but also to other tasks. CRFs are now widely used for sequence-based tasks, such as parts-of-speech tagging and named entity recognition, and have been shown to achieve the best performance in various tasks (McCallum and Li 2003; McCallum, Rohanimanesh, and Sutton 2003; Pinto et al. 2003; Sha and Pereira 2003; Peng and McCallum 2004; Roark et al. 2004; Settles 2004; Sutton, Rohanimanesh, and McCallum 2004). These results suggest that the method proposed in the present article will achieve high accuracy when applied to various statistical models with tree structures. Dynamic CRFs (McCallum, Rohanimanesh, and Sutton 2003; Sutton, Rohanimanesh, and McCallum 2004) provide us with an interesting inspiration for extending feature forest models. The purpose of dynamic CRFs is to incorporate feature functions that are not represented locally, and the solution is to apply a variational method, which is an algorithm of numerical computation, to obta"
J08-1002,H92-1019,0,0.506127,"Missing"
J08-1002,W04-1221,0,0.00766165,"omputational Linguistics Volume 34, Number 1 Extensions of CRFs, such as semi-Markov CRFs (Sarawagi and Cohen 2004), are also regarded as instances of feature forest models. This fact implies that our algorithm is applicable to not only parsing but also to other tasks. CRFs are now widely used for sequence-based tasks, such as parts-of-speech tagging and named entity recognition, and have been shown to achieve the best performance in various tasks (McCallum and Li 2003; McCallum, Rohanimanesh, and Sutton 2003; Pinto et al. 2003; Sha and Pereira 2003; Peng and McCallum 2004; Roark et al. 2004; Settles 2004; Sutton, Rohanimanesh, and McCallum 2004). These results suggest that the method proposed in the present article will achieve high accuracy when applied to various statistical models with tree structures. Dynamic CRFs (McCallum, Rohanimanesh, and Sutton 2003; Sutton, Rohanimanesh, and McCallum 2004) provide us with an interesting inspiration for extending feature forest models. The purpose of dynamic CRFs is to incorporate feature functions that are not represented locally, and the solution is to apply a variational method, which is an algorithm of numerical computation, to obtain approximate"
J08-1002,N03-1028,0,0.592524,"o, Ninomiya, and Tsujii 2003; Miyao and Tsujii 2003, 2005). Together with the parameter estimation algorithm for feature forest models, these methods constitute a complete procedure for the probabilistic modeling of wide-coverage HPSG parsing. The methods we propose here were applied to an English HPSG parser, Enju (Tsujii Laboratory 2004). We report on an extensive evaluation of the parser through parsing experiments on the Wall Street Journal portion of the Penn Treebank (Marcus et al. 1994). The content of this article is an extended version of our earlier work reported in Miyao and Tsujii (2002, 2003, 2005) and Miyao, Ninomiya, and Tsujii (2003). The major contribution of this article is a strict mathematical deﬁnition of the feature forest model and the parameter estimation algorithm, which are substantially reﬁned and extended from Miyao and Tsujii (2002). Another contribution is that this article thoroughly discusses the relationships between the feature forest model and its application to HPSG parsing. We also provide an extensive empirical evaluation of the resulting HPSG parsing approach using real-world text. Section 2 discusses a problem of conventional probabilistic models for lexicaliz"
J08-1002,P85-1018,0,0.56965,"uare boxes (ci ) are conjunctive nodes, and di disjunctive nodes. A solid arrow represents a disjunctive daughter function, and a dotted line expresses a conjunctive daughter function. Formally, a chart E, Er , α is mapped into a feature forest C, D, R, γ, δ as follows.6 r r r C = {em , el , er |em ∈ E ∧ (el , er ) ∈ α(em )} ∪ {w|w ∈ w} D=E R = {em , el , er |em ∈ Er ∧ em , el , er  ∈ C} 4 For simplicity, only binary trees are considered. Extension to unary and n-ary (n > 2) trees is trivial. 5 We assume that CONT and DTRS (a feature used to represent daughter signs) are restricted (Shieber 1985), and we will discuss a method for encoding CONT in a feature forest in Section 4.3. We also assume that parse trees are packed according to equivalence relations rather than subsumption relations (Oepen and Carroll 2000). We cannot simply map parse forests packed under subsumption into feature forests, because they over-generate possible unpacked trees. 6 For ease of explanation, the deﬁnition of the root node is different from the original deﬁnition given in Section 3. In this section, we deﬁne R as a set of conjunctive nodes rather than a single node r. The deﬁnition here is translated into"
J08-1002,W04-3201,0,0.0180033,"slight, trivial extension of PCFG. As described herein, however, feature forests can represent structures beyond CFG parse trees. Furthermore, because feature forests are a generalized representation of ambiguous structures, each node in a feature forest need not correspond to a node in a PCFG parse forest. That is, a node in a feature forest may represent any linguistic entity, including a fragment of a syntactic structure, a semantic relation, or other sentence-level information. The idea of feature forest models could be applied to non-probabilistic machine learning methods. Taskar et al. (2004) proposed a dynamic programming algorithm for the learning of large-margin classiﬁers including support vector machines (Vapnik 1995), and presented its application to disambiguation in CFG parsing. Their algorithm resembles feature forest models; an optimization function is computed by a dynamic programing algorithm without unpacking packed forest structures. From the discussion in this article, it is evident that if the main part of an update formula is represented 72 Miyao and Tsujii Feature Forest Models for Probabilistic HPSG Parsing with (the exponential of) linear combinations, a method"
J08-1002,W02-2030,0,0.363425,"me 34, Number 1 Tsujii 2005), statistical modeling of these grammars is attracting considerable attention. This is because natural language processing applications usually require disambiguated or ranked parse results, and statistical modeling of syntactic/semantic preference is one of the most promising methods for disambiguation. The focus of this article is the problem of probabilistic modeling of wide-coverage HPSG parsing. Although previous studies have proposed maximum entropy models (Berger, Della Pietra, and Della Pietra 1996) of HPSG-style parse trees (Oepen, Toutanova, et al. 2002b; Toutanova and Manning 2002; Baldridge and Osborne 2003; Malouf and van Noord 2004), the straightforward application of maximum entropy models to wide-coverage HPSG parsing is infeasible because estimation of maximum entropy models is computationally expensive, especially when targeting wide-coverage parsing. In general, complete structures, such as transition sequences in Markov models and parse trees, have an exponential number of ambiguities. This causes an exponential explosion when estimating the parameters of maximum entropy models. We therefore require solutions to make model estimation tractable. This article ﬁr"
J08-1002,W04-3222,0,0.0246333,"Missing"
J08-1002,H05-1059,1,0.722295,"Missing"
J08-1002,P87-1015,0,0.142407,"Missing"
J08-1002,W06-1634,1,\N,Missing
J08-1002,W07-1204,0,\N,Missing
J09-4003,C02-1140,1,0.851582,"Missing"
J09-4003,C02-1059,1,0.871134,"Missing"
J09-4003,1997.iwpt-1.16,1,0.629816,"y of Manchester, UK Hozumi Tanaka—or Tanaka-sensei as he was fondly known to his colleagues and students in Japanese—passed away at the age of 67 in the early morning of 27 July 2009. He is survived by his wife Reiko and two sons. Tanaka-sensei’s primary contributions to natural language processing (NLP) are in parsing and semantic analysis. In parsing, he extended the GLR parsing algorithm to incorporate probabilities, multiple connection tables, and simultaneously carry out morphological and syntactic analysis for non-segmenting languages such as Japanese (Tanaka, Tokunaga, and Aizawa 1993; Inui et al. 1997; Shirai et al. 2000). His research on semantic analysis covered a broad spectrum, encompassing word sense disambiguation (Fujii et al. 1998), spoken language understanding for virtual agent systems (Shinyama, Tokunaga, and Tanaka 2000), lexical semantic approaches to query expansion in information retrieval (Mandala, Tokunaga, and Tanaka 2000), and metaphor processing (Iwayama, Tokunaga, and Tanaka 1990). He also carried out research on machine translation (Tanaka, Isahara, and Yasuhara 1983; Tanaka 1999b; Baldwin and Tanaka 2000), computer-assisted language learning (Bilac, Baldwin, and Tana"
J09-4003,1999.mtsummit-1.1,0,0.0383844,"is for non-segmenting languages such as Japanese (Tanaka, Tokunaga, and Aizawa 1993; Inui et al. 1997; Shirai et al. 2000). His research on semantic analysis covered a broad spectrum, encompassing word sense disambiguation (Fujii et al. 1998), spoken language understanding for virtual agent systems (Shinyama, Tokunaga, and Tanaka 2000), lexical semantic approaches to query expansion in information retrieval (Mandala, Tokunaga, and Tanaka 2000), and metaphor processing (Iwayama, Tokunaga, and Tanaka 1990). He also carried out research on machine translation (Tanaka, Isahara, and Yasuhara 1983; Tanaka 1999b; Baldwin and Tanaka 2000), computer-assisted language learning (Bilac, Baldwin, and Tanaka 2002), speech recognition (Itou, Hayamizu, and Tanaka 1992; Li, Tanaka, and Tokunaga 1995), dialogue systems (Akiba and Tanaka 1994; Funakoshi, Tokunaga, and Tanaka 2002), and automatic music generation (Suzuki, Tokunaga, and Tanaka 1999). He was the author or editor of a number of popular introductory texts on NLP in Japanese (Tanaka 1989, 1999a). Tanaka-sensei was the technical lead on the Japanese government-funded CICC Machine Translation Project (1987–1995) between East and South-East Asian langua"
J09-4003,1993.iwpt-1.10,0,0.382482,"Missing"
J85-2001,P84-1069,1,0.586282,"Missing"
J85-2001,P84-1011,0,0.447574,"Missing"
J85-2001,P84-1057,1,0.817703,"Missing"
J85-2001,P84-1086,1,\N,Missing
kano-etal-2010-u,W09-1401,1,\N,Missing
kano-etal-2010-u,W04-1213,0,\N,Missing
kano-etal-2010-u,W09-1504,1,\N,Missing
L18-1220,D17-1001,1,0.883729,"paraphrase identification on sentential paraphrases (Yao et al., 2013); however, the units of correspondence in previous studies are defined as sequences of words and not syntactic phrases due to difficulties caused by the non-homographic nature of phrase correspondences. To overcome these challenges, one promising approach is phrase alignment on paraphrasal sentence pairs based on their syntactic structures derived by linguistically motived grammar. A flexible mechanism to allow noncompositional phrase correspondences is also required. We have published our initial attempt on this direction (Arase and Tsujii, 2017). For systematic research on syntactic phrase alignment in paraphrases, an evaluation dataset as well as evaluation measures are essential. Hence, we constructed the SPADE (Syntactic Phrase Alignment Dataset for Evaluation) and released it through Linguistic Data Consortium1 (catalog ID: LDC2018T092 ). In the SPADE, 201 sentential paraphrases are annotated gold parse trees, on which 20, 276 phrases exist. Three annotators annotated align1 https://www.ldc.upenn.edu/ https://catalog.ldc.upenn.edu/LDC2018T09 (will be effective since March 2018) ments among these phrases as shown in Figure 1, resu"
L18-1220,P14-1133,0,0.0281862,"metrics are proposed to evaluate to what extent the automatic phrase alignment results agree with the ones identified by humans. These metrics allow objective comparison of performances of different methods evaluated on the SPADE. Benchmarks to show performances of humans and the state-of-the-art method are presented as a reference for future SPADE users. Keywords: phrase alignment, paraphrase detection 1. Introduction Paraphrases have been applied to various NLP applications, and recently, they are recognized as a useful resource for natural language understanding, such as semantic parsing (Berant and Liang, 2014) and automatic question answering (Dong et al., 2017). While most previous studies focused on sentential paraphrase detection, e.g., (Dolan et al., 2004), finer grained paraphrases, i.e., phrasal paraphrases, are desired by the applications. In addition, syntactic structures are important in modeling sentences, e.g., their sentiments and semantic similarities (Socher et al., 2013; Tai et al., 2015). A few studies worked on phrasal paraphrase identification on sentential paraphrases (Yao et al., 2013); however, the units of correspondence in previous studies are defined as sequences of words an"
L18-1220,P15-1118,0,0.0242379,"Figure 1: SPADE data example: part of gold trees and phrase alignments on a sentence pair of “Hence, I also have a reason to believe that her life is excellent and wonderful.” and “So I have reason to believe that she also has a very splendid life.” tional scope of paraphrasing, such as entailment, inference, and drastic summarization. Although costly, manually generating paraphrases is the way to produce a high-quality dataset. SICK (Marelli et al., 2014) was constructed from image and video captions through sentence alignment and careful edits to exclude undesired linguistic phenomena. In (Choe and McClosky, 2015), a linguist manually generated paraphrases to given sentences. To scale up the process trading off the quality, crowd-sourcing has been explored (Jiang et al., 2017). As for phrasal paraphrase datasets, there are only a few; PPDB (Ganitkevitch et al., 2013) and its extension annotated levels of paraphrasability (Wieting et al., 2015). PPDB uses bilingual pivoting on parallel corpora; multiple translations of the same source phrase are regarded as paraphrases. While researchers proposed methods to identify phrasal correspondences for natural language inferences (MacCartney et al., 2008; Thadan"
L18-1220,C04-1051,0,0.564807,"ve comparison of performances of different methods evaluated on the SPADE. Benchmarks to show performances of humans and the state-of-the-art method are presented as a reference for future SPADE users. Keywords: phrase alignment, paraphrase detection 1. Introduction Paraphrases have been applied to various NLP applications, and recently, they are recognized as a useful resource for natural language understanding, such as semantic parsing (Berant and Liang, 2014) and automatic question answering (Dong et al., 2017). While most previous studies focused on sentential paraphrase detection, e.g., (Dolan et al., 2004), finer grained paraphrases, i.e., phrasal paraphrases, are desired by the applications. In addition, syntactic structures are important in modeling sentences, e.g., their sentiments and semantic similarities (Socher et al., 2013; Tai et al., 2015). A few studies worked on phrasal paraphrase identification on sentential paraphrases (Yao et al., 2013); however, the units of correspondence in previous studies are defined as sequences of words and not syntactic phrases due to difficulties caused by the non-homographic nature of phrase correspondences. To overcome these challenges, one promising a"
L18-1220,D17-1091,0,0.0199551,"atic phrase alignment results agree with the ones identified by humans. These metrics allow objective comparison of performances of different methods evaluated on the SPADE. Benchmarks to show performances of humans and the state-of-the-art method are presented as a reference for future SPADE users. Keywords: phrase alignment, paraphrase detection 1. Introduction Paraphrases have been applied to various NLP applications, and recently, they are recognized as a useful resource for natural language understanding, such as semantic parsing (Berant and Liang, 2014) and automatic question answering (Dong et al., 2017). While most previous studies focused on sentential paraphrase detection, e.g., (Dolan et al., 2004), finer grained paraphrases, i.e., phrasal paraphrases, are desired by the applications. In addition, syntactic structures are important in modeling sentences, e.g., their sentiments and semantic similarities (Socher et al., 2013; Tai et al., 2015). A few studies worked on phrasal paraphrase identification on sentential paraphrases (Yao et al., 2013); however, the units of correspondence in previous studies are defined as sequences of words and not syntactic phrases due to difficulties caused by"
L18-1220,N13-1092,0,0.0600842,"Missing"
L18-1220,P17-2017,0,0.0254722,"ful.” and “So I have reason to believe that she also has a very splendid life.” tional scope of paraphrasing, such as entailment, inference, and drastic summarization. Although costly, manually generating paraphrases is the way to produce a high-quality dataset. SICK (Marelli et al., 2014) was constructed from image and video captions through sentence alignment and careful edits to exclude undesired linguistic phenomena. In (Choe and McClosky, 2015), a linguist manually generated paraphrases to given sentences. To scale up the process trading off the quality, crowd-sourcing has been explored (Jiang et al., 2017). As for phrasal paraphrase datasets, there are only a few; PPDB (Ganitkevitch et al., 2013) and its extension annotated levels of paraphrasability (Wieting et al., 2015). PPDB uses bilingual pivoting on parallel corpora; multiple translations of the same source phrase are regarded as paraphrases. While researchers proposed methods to identify phrasal correspondences for natural language inferences (MacCartney et al., 2008; Thadani et al., 2012; Yao et al., 2013), the unit of phrase was simply n-gram and syntax in paraphrases was out of their scope. Part of PPDB provides syntactic paraphrases"
L18-1220,D17-1126,0,0.0207502,"uthentic paraphrases containing purely a paraphrasal phenomenon. Although the amount of reference translations are relatively large thanks to efforts by the research community, they require severe human workloads for creation and expansion. To explore more abundant resources to extract paraphrases, Microsoft Research Paraphrase Corpus (Dolan et al., 2004) aligns news texts published at the same timing as paraphrases. Twitter URL Corpus takes a similar approach on news headlines and comments to them published at Twitter3 : it uses attached URLs as a primary clew to find paraphrasal candidates (Lan et al., 2017). Since paraphrases in these datasets are not strictly constrained like in the ones extracted from machine translation evaluation corpora, they involve variety of linguistic phenomena beyond the conven2 3 1379 https://twitter.com/ … S VP … S Annotator #1 VP VP Annotator #2 NP ADJP NP NP COOD … her life is excellent and wonderful Annotator #3 ADJP … she also has a very splendid life Figure 1: SPADE data example: part of gold trees and phrase alignments on a sentence pair of “Hence, I also have a reason to believe that her life is excellent and wonderful.” and “So I have reason to believe that s"
L18-1220,D08-1084,0,0.0351868,"a. In (Choe and McClosky, 2015), a linguist manually generated paraphrases to given sentences. To scale up the process trading off the quality, crowd-sourcing has been explored (Jiang et al., 2017). As for phrasal paraphrase datasets, there are only a few; PPDB (Ganitkevitch et al., 2013) and its extension annotated levels of paraphrasability (Wieting et al., 2015). PPDB uses bilingual pivoting on parallel corpora; multiple translations of the same source phrase are regarded as paraphrases. While researchers proposed methods to identify phrasal correspondences for natural language inferences (MacCartney et al., 2008; Thadani et al., 2012; Yao et al., 2013), the unit of phrase was simply n-gram and syntax in paraphrases was out of their scope. Part of PPDB provides syntactic paraphrases under the synchronous context free grammar (SCFG); however, SCFG captures only a fraction of paraphrasing phenomenon (Weese et al., 2014). Hence, the SPADE is unique for providing fully syntactic and phrasal paraphrases. 3. Construction of SPADE We create the SPADE for evaluation on syntactic phrase alignment in paraphrases. Two rounds of annotations were carefully conducted to annotate gold parse trees and phrase alignmen"
L18-1220,marelli-etal-2014-sick,0,0.0374356,"r.com/ … S VP … S Annotator #1 VP VP Annotator #2 NP ADJP NP NP COOD … her life is excellent and wonderful Annotator #3 ADJP … she also has a very splendid life Figure 1: SPADE data example: part of gold trees and phrase alignments on a sentence pair of “Hence, I also have a reason to believe that her life is excellent and wonderful.” and “So I have reason to believe that she also has a very splendid life.” tional scope of paraphrasing, such as entailment, inference, and drastic summarization. Although costly, manually generating paraphrases is the way to produce a high-quality dataset. SICK (Marelli et al., 2014) was constructed from image and video captions through sentence alignment and careful edits to exclude undesired linguistic phenomena. In (Choe and McClosky, 2015), a linguist manually generated paraphrases to given sentences. To scale up the process trading off the quality, crowd-sourcing has been explored (Jiang et al., 2017). As for phrasal paraphrase datasets, there are only a few; PPDB (Ganitkevitch et al., 2013) and its extension annotated levels of paraphrasability (Wieting et al., 2015). PPDB uses bilingual pivoting on parallel corpora; multiple translations of the same source phrase a"
L18-1220,D13-1170,0,0.00372328,"paraphrase detection 1. Introduction Paraphrases have been applied to various NLP applications, and recently, they are recognized as a useful resource for natural language understanding, such as semantic parsing (Berant and Liang, 2014) and automatic question answering (Dong et al., 2017). While most previous studies focused on sentential paraphrase detection, e.g., (Dolan et al., 2004), finer grained paraphrases, i.e., phrasal paraphrases, are desired by the applications. In addition, syntactic structures are important in modeling sentences, e.g., their sentiments and semantic similarities (Socher et al., 2013; Tai et al., 2015). A few studies worked on phrasal paraphrase identification on sentential paraphrases (Yao et al., 2013); however, the units of correspondence in previous studies are defined as sequences of words and not syntactic phrases due to difficulties caused by the non-homographic nature of phrase correspondences. To overcome these challenges, one promising approach is phrase alignment on paraphrasal sentence pairs based on their syntactic structures derived by linguistically motived grammar. A flexible mechanism to allow noncompositional phrase correspondences is also required. We h"
L18-1220,P15-1150,0,0.0650562,"Missing"
L18-1220,C12-2120,0,0.0280188,"2015), a linguist manually generated paraphrases to given sentences. To scale up the process trading off the quality, crowd-sourcing has been explored (Jiang et al., 2017). As for phrasal paraphrase datasets, there are only a few; PPDB (Ganitkevitch et al., 2013) and its extension annotated levels of paraphrasability (Wieting et al., 2015). PPDB uses bilingual pivoting on parallel corpora; multiple translations of the same source phrase are regarded as paraphrases. While researchers proposed methods to identify phrasal correspondences for natural language inferences (MacCartney et al., 2008; Thadani et al., 2012; Yao et al., 2013), the unit of phrase was simply n-gram and syntax in paraphrases was out of their scope. Part of PPDB provides syntactic paraphrases under the synchronous context free grammar (SCFG); however, SCFG captures only a fraction of paraphrasing phenomenon (Weese et al., 2014). Hence, the SPADE is unique for providing fully syntactic and phrasal paraphrases. 3. Construction of SPADE We create the SPADE for evaluation on syntactic phrase alignment in paraphrases. Two rounds of annotations were carefully conducted to annotate gold parse trees and phrase alignments. 3.1. Corpus Paraph"
L18-1220,E14-1021,0,0.132031,"alignment on the SPADE, which have been used as official evaluation metrics in (Arase and Tsujii, 2017). These measures allow objective comparison of performances of different methods evaluated on the SPADE. 2. Related Work Extensive research efforts have been made for sentential paraphrase detection. One of promising resources that provide paraphrases is machine translation evaluation corpora. In such a corpus, a source sentence is translated into multiple translations in a target language. These translations are called reference translations, which can be regarded as sentential paraphrases (Weese et al., 2014). Since the reference translations are constrained to convey the same information in similar structures with the source sentences, they can be regarded as authentic paraphrases containing purely a paraphrasal phenomenon. Although the amount of reference translations are relatively large thanks to efforts by the research community, they require severe human workloads for creation and expansion. To explore more abundant resources to extract paraphrases, Microsoft Research Paraphrase Corpus (Dolan et al., 2004) aligns news texts published at the same timing as paraphrases. Twitter URL Corpus take"
L18-1220,Q15-1025,0,0.0165662,"Although costly, manually generating paraphrases is the way to produce a high-quality dataset. SICK (Marelli et al., 2014) was constructed from image and video captions through sentence alignment and careful edits to exclude undesired linguistic phenomena. In (Choe and McClosky, 2015), a linguist manually generated paraphrases to given sentences. To scale up the process trading off the quality, crowd-sourcing has been explored (Jiang et al., 2017). As for phrasal paraphrase datasets, there are only a few; PPDB (Ganitkevitch et al., 2013) and its extension annotated levels of paraphrasability (Wieting et al., 2015). PPDB uses bilingual pivoting on parallel corpora; multiple translations of the same source phrase are regarded as paraphrases. While researchers proposed methods to identify phrasal correspondences for natural language inferences (MacCartney et al., 2008; Thadani et al., 2012; Yao et al., 2013), the unit of phrase was simply n-gram and syntax in paraphrases was out of their scope. Part of PPDB provides syntactic paraphrases under the synchronous context free grammar (SCFG); however, SCFG captures only a fraction of paraphrasing phenomenon (Weese et al., 2014). Hence, the SPADE is unique for"
L18-1220,D13-1056,0,0.0366168,"Missing"
N09-1007,W06-1655,0,0.354127,"entation Bakeoff (the second SIGHAN CWS bakeoff) (Emerson, 2005), two of the highest scoring systems in the closed track competition were based on a CRF model (Tseng et al., 2005; Asahara et al., 2005). While the CRF model is quite effective compared with other models designed for CWS, it may be limited by its restrictive independence assumptions on non-adjacent labels. Although the window can in principle be widened by increasing the Markov order, this may not be a practical solution, because the complexity of training and decoding a linearchain CRF grows exponentially with the Markov order (Andrew, 2006). To address this difﬁculty, a choice is to relax the Markov assumption by using the semi-Markov conditional random ﬁeld model (semi-CRF) (Sarawagi and Cohen, 2004). Despite the theoretical advantage of semi-CRFs over CRFs, however, some previous studies (Andrew, 2006; Liang, 2005) exploring the use of a semi-CRF for Chinese word segmentation did not ﬁnd signiﬁcant gains over the CRF ones. As discussed in Andrew (2006), the reason may be that despite the greater representational power of the semi-CRF, there are some valuable features that could be more naturally expressed in a character-based"
N09-1007,I05-3018,0,0.0345224,"s beling task (Xue, 2003). Labels are assigned to each character in the sentence, indicating whether the character xi is the start (Labeli = B), middle or end of a multi-character word (Labeli = C). A popular discriminative model that have been used for this task is the conditional random ﬁelds (CRFs) (Lafferty et al., 2001), starting with the model of Peng et al. (2004). In the Second International Chinese Word Segmentation Bakeoff (the second SIGHAN CWS bakeoff) (Emerson, 2005), two of the highest scoring systems in the closed track competition were based on a CRF model (Tseng et al., 2005; Asahara et al., 2005). While the CRF model is quite effective compared with other models designed for CWS, it may be limited by its restrictive independence assumptions on non-adjacent labels. Although the window can in principle be widened by increasing the Markov order, this may not be a practical solution, because the complexity of training and decoding a linearchain CRF grows exponentially with the Markov order (Andrew, 2006). To address this difﬁculty, a choice is to relax the Markov assumption by using the semi-Markov conditional random ﬁeld model (semi-CRF) (Sarawagi and Cohen, 2004). Despite the theoretica"
N09-1007,I05-3019,0,0.342649,"Missing"
N09-1007,I05-3017,0,0.737646,"the North American Chapter of the ACL, pages 56–64, c Boulder, Colorado, June 2009. 2009 Association for Computational Linguistics beling task (Xue, 2003). Labels are assigned to each character in the sentence, indicating whether the character xi is the start (Labeli = B), middle or end of a multi-character word (Labeli = C). A popular discriminative model that have been used for this task is the conditional random ﬁelds (CRFs) (Lafferty et al., 2001), starting with the model of Peng et al. (2004). In the Second International Chinese Word Segmentation Bakeoff (the second SIGHAN CWS bakeoff) (Emerson, 2005), two of the highest scoring systems in the closed track competition were based on a CRF model (Tseng et al., 2005; Asahara et al., 2005). While the CRF model is quite effective compared with other models designed for CWS, it may be limited by its restrictive independence assumptions on non-adjacent labels. Although the window can in principle be widened by increasing the Markov order, this may not be a practical solution, because the complexity of training and decoding a linearchain CRF grows exponentially with the Markov order (Andrew, 2006). To address this difﬁculty, a choice is to relax t"
N09-1007,P07-1104,0,0.0508784,"nt Variable Segmenter 2.1 Discriminative Probabilistic Latent Variable Model Given data with latent structures, the task is to learn a mapping between a sequence of observations x = x1 , x2 , . . . , xm and a sequence of labels y = y1 , y2 , . . . , ym . Each yj is a class label for the j’th character of an input sequence, and is a member of a set Y of possible class labels. For each sequence, the model also assumes a sequence of latent variables h = h1 , h2 , . . . , hm , which is unobservable in training examples. The DPLVM is deﬁned as follows (Morency et al., 2 The system was also used in Gao et al. (2007), with an improved performance in CWS. 3 In practice, one may add a few extra labels based on linguistic intuitions (Xue, 2003). 2007): P (y|x, Θ) =  P (y|h, x, Θ)P (h|x, Θ), (1) h where Θ are the parameters of the model. DPLVMs can be seen as a natural extension of CRF models, and CRF models can be seen as a special case of DPLVMs that have only one latent variable for each label. To make the training and inference efﬁcient, the model is restricted to have disjoint sets of latent variables associated with each class label. Each hj is a member in a set Hyj of possible latent variables for the"
N09-1007,C04-1081,0,0.541571,"Missing"
N09-1007,E09-1088,1,0.796261,"onal loglikelihood of the training data. The second term is a regularizer that is used for reducing overﬁtting in parameter estimation. For decoding in the test stage, given a test sequence x, we want to ﬁnd the most probable label sequence, y∗ : y∗ = argmaxy P (y|x, Θ∗ ). (5) For latent conditional models like DPLVMs, the best label path y∗ cannot directly be produced by the 4 It means that Eq. 2 is from Eq. 1 with additional deﬁnition. 58 Viterbi algorithm because of the incorporation of hidden states. In this paper, we use a technique based on A∗ search and dynamic programming described in Sun and Tsujii (2009), for producing the most probable label sequence y∗ on DPLVM. In detail, an A∗ search algorithm5 (Hart et al., 1968) with a Viterbi heuristic function is adopted to produce top-n latent paths, h1 , h2 , . . . hn . In addition, a forward-backward-style algorithm is used to compute the exact probabilities of their corresponding label paths, y1 , y2 , . . . yn . The model then tries to determine the optimal label path based on the top-n statistics, without enumerating the remaining low-probability paths, which could be exponentially enormous. The optimal label path y∗ is ready when the following"
N09-1007,I05-3027,0,0.634414,"utational Linguistics beling task (Xue, 2003). Labels are assigned to each character in the sentence, indicating whether the character xi is the start (Labeli = B), middle or end of a multi-character word (Labeli = C). A popular discriminative model that have been used for this task is the conditional random ﬁelds (CRFs) (Lafferty et al., 2001), starting with the model of Peng et al. (2004). In the Second International Chinese Word Segmentation Bakeoff (the second SIGHAN CWS bakeoff) (Emerson, 2005), two of the highest scoring systems in the closed track competition were based on a CRF model (Tseng et al., 2005; Asahara et al., 2005). While the CRF model is quite effective compared with other models designed for CWS, it may be limited by its restrictive independence assumptions on non-adjacent labels. Although the window can in principle be widened by increasing the Markov order, this may not be a practical solution, because the complexity of training and decoding a linearchain CRF grows exponentially with the Markov order (Andrew, 2006). To address this difﬁculty, a choice is to relax the Markov assumption by using the semi-Markov conditional random ﬁeld model (semi-CRF) (Sarawagi and Cohen, 2004)."
N09-1007,W06-0121,0,0.114226,"Missing"
N09-1007,O03-4002,0,0.816245,"purposes, e.g., full-text indexing. However, as is illustrated, recognizing long words (without sacriﬁcing the performance on short words) is challenging. Conventional approaches to Chinese word segmentation treat the problem as a character-based la1 Following previous work, in this paper, words can also refer to multi-word expressions, including proper names, long named entities, idioms, etc. Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 56–64, c Boulder, Colorado, June 2009. 2009 Association for Computational Linguistics beling task (Xue, 2003). Labels are assigned to each character in the sentence, indicating whether the character xi is the start (Labeli = B), middle or end of a multi-character word (Labeli = C). A popular discriminative model that have been used for this task is the conditional random ﬁelds (CRFs) (Lafferty et al., 2001), starting with the model of Peng et al. (2004). In the Second International Chinese Word Segmentation Bakeoff (the second SIGHAN CWS bakeoff) (Emerson, 2005), two of the highest scoring systems in the closed track competition were based on a CRF model (Tseng et al., 2005; Asahara et al., 2005). Wh"
N09-1007,P07-1106,0,0.496064,"h row represents a CWS model. For each group, the rows marked by ∗ represent our models with hybrid word/character information. Best05 represents the best system of the Second International Chinese Word Segmentation Bakeoff on the corresponding data; A06 represents the semi-CRF model in Andrew (2006)10 , which was also used in Gao et al. (2007) (denoted as G07) with an improved performance; Z06-a and Z06-b represents the pure subword CRF model and the conﬁdence-based combination of CRF and rule-based models, respectively (Zhang et al., 2006); ZC07 represents the word-based perceptron model in Zhang and Clark (2007); T05 represents the CRF model in Tseng et al. (2005); C05 represents the system in Chen et al. 10 It is a hybrid Markov/semi-Markov CRF model which outperforms conventional semi-CRF models (Andrew, 2006). However, in general, as discussed in Andrew (2006), it is essentially still a semi-CRF model. 61 (2005). The best F-score and recall of OOV words of each group is shown in bold. As is shown in the table, we achieved the best F-score in two out of the three corpora. We also achieved the best recall rate of OOV words on those two corpora. Both of the MSR and PKU Corpus use simpliﬁed Chinese, w"
N09-1007,N06-2049,0,0.152838,"ts are grouped into three sub-tables according to different corpora. Each row represents a CWS model. For each group, the rows marked by ∗ represent our models with hybrid word/character information. Best05 represents the best system of the Second International Chinese Word Segmentation Bakeoff on the corresponding data; A06 represents the semi-CRF model in Andrew (2006)10 , which was also used in Gao et al. (2007) (denoted as G07) with an improved performance; Z06-a and Z06-b represents the pure subword CRF model and the conﬁdence-based combination of CRF and rule-based models, respectively (Zhang et al., 2006); ZC07 represents the word-based perceptron model in Zhang and Clark (2007); T05 represents the CRF model in Tseng et al. (2005); C05 represents the system in Chen et al. 10 It is a hybrid Markov/semi-Markov CRF model which outperforms conventional semi-CRF models (Andrew, 2006). However, in general, as discussed in Andrew (2006), it is essentially still a semi-CRF model. 61 (2005). The best F-score and recall of OOV words of each group is shown in bold. As is shown in the table, we achieved the best F-score in two out of the three corpora. We also achieved the best recall rate of OOV words on"
N09-1048,2007.mtsummit-papers.9,0,0.832225,"e purely monolingual corpora, wherein frequency-based expectation-maximization (EM, refer to (Dempster et al., 1977)) algorithms and cognate clues play a central role (Koehn and Knight, 2002). Haghighi et al. (2008) presented a generative model based on canonical correlation analysis, in which monolingual features such as the context and orthographic substrings of words were taken into account. The other is multilingual parallel and comparable corpora (e.g., Wikipedia1 ), wherein features such as cooccurrence frequency and context are popularly employed (Cheng et al., 2004; Shao and Ng, 2004; Cao et al., 2007; Lin et al., 2008). In this paper, we focus on a special type of comparable corpus, parenthetical translations. The issue is motivated by the observation that Web pages and technical papers written in Asian languages (e.g., Chinese, Japanese) sometimes annotate named entities or technical terms with their translations in English inside a pair of parentheses. This is considered to be a traditional way to annotate new terms, personal names or other named entities with their English translations expressed in brackets. Formally, a parenthetical translation can be expressed by the following patter"
N09-1048,P04-1068,0,0.0151842,"for automatic lexicon mining. One is the purely monolingual corpora, wherein frequency-based expectation-maximization (EM, refer to (Dempster et al., 1977)) algorithms and cognate clues play a central role (Koehn and Knight, 2002). Haghighi et al. (2008) presented a generative model based on canonical correlation analysis, in which monolingual features such as the context and orthographic substrings of words were taken into account. The other is multilingual parallel and comparable corpora (e.g., Wikipedia1 ), wherein features such as cooccurrence frequency and context are popularly employed (Cheng et al., 2004; Shao and Ng, 2004; Cao et al., 2007; Lin et al., 2008). In this paper, we focus on a special type of comparable corpus, parenthetical translations. The issue is motivated by the observation that Web pages and technical papers written in Asian languages (e.g., Chinese, Japanese) sometimes annotate named entities or technical terms with their translations in English inside a pair of parentheses. This is considered to be a traditional way to annotate new terms, personal names or other named entities with their English translations expressed in brackets. Formally, a parenthetical translation can"
N09-1048,P08-1088,0,0.0250461,"neologisms (e.g., new technical terms, personal names, abbreviations, etc.), the difficulty of keeping up with the neologisms for lexicographers, etc. In order to turn the facts to a better way, one of the simplest strategies is to automatically mine large-scale lexicons from corpora such as the daily updated Web. 424 Generally, there are two kinds of corpora used for automatic lexicon mining. One is the purely monolingual corpora, wherein frequency-based expectation-maximization (EM, refer to (Dempster et al., 1977)) algorithms and cognate clues play a central role (Koehn and Knight, 2002). Haghighi et al. (2008) presented a generative model based on canonical correlation analysis, in which monolingual features such as the context and orthographic substrings of words were taken into account. The other is multilingual parallel and comparable corpora (e.g., Wikipedia1 ), wherein features such as cooccurrence frequency and context are popularly employed (Cheng et al., 2004; Shao and Ng, 2004; Cao et al., 2007; Lin et al., 2008). In this paper, we focus on a special type of comparable corpus, parenthetical translations. The issue is motivated by the observation that Web pages and technical papers written"
N09-1048,P07-2045,0,0.00890689,"nyins). Similar models have been compared in (Oh et al., 2006) for English-to-Korean and Englishto-Japanese transliteration. All the three models are phrase-based, i.e., adjacent phonemes or graphemes are allowable to form phrase-level transliteration units. Building the correspondences on phrase level can effectively tackle the missing or redundant 428 phoneme/grapheme problem during transliteration. For example, when Aamodt is transliterated into a m¯o t`e5 , a and d are missing. The problem can be easily solved when taking Aa and dt as single units for transliterating. Making use of Moses (Koehn et al., 2007), a phrase-based SMT system, Matthews (2007) has shown that the performance was comparable to recent state-of-the-art work (Jiang et al., 2007) in English-to-Chinese personal name transliteration. Matthews (2007) took transliteration as translation at the surface level. Inspired by his idea, we also implemented our transliteration models employing Moses. The main difference is that, while Matthews (2007) tokenized the English names into individual letters before training in Moses, we split them into syllables using the heuristic rules described in (Jiang et al., 2007), such that one syllable o"
N09-1048,W02-0902,0,0.0308471,"e continuous emergence of neologisms (e.g., new technical terms, personal names, abbreviations, etc.), the difficulty of keeping up with the neologisms for lexicographers, etc. In order to turn the facts to a better way, one of the simplest strategies is to automatically mine large-scale lexicons from corpora such as the daily updated Web. 424 Generally, there are two kinds of corpora used for automatic lexicon mining. One is the purely monolingual corpora, wherein frequency-based expectation-maximization (EM, refer to (Dempster et al., 1977)) algorithms and cognate clues play a central role (Koehn and Knight, 2002). Haghighi et al. (2008) presented a generative model based on canonical correlation analysis, in which monolingual features such as the context and orthographic substrings of words were taken into account. The other is multilingual parallel and comparable corpora (e.g., Wikipedia1 ), wherein features such as cooccurrence frequency and context are popularly employed (Cheng et al., 2004; Shao and Ng, 2004; Cao et al., 2007; Lin et al., 2008). In this paper, we focus on a special type of comparable corpus, parenthetical translations. The issue is motivated by the observation that Web pages and t"
N09-1048,P06-1142,0,0.0310302,"ation extraction, self-trained transliteration models and cascaded translation models are described in Section 4, 5, and 6, respectively. In Section 7, we evaluate our mined lexicons by Wikipedia. We conclude in Section 8 finally. 2 Related Work Numerous researchers have proposed a variety of automatic approaches to mine lexicons from the Web pages or other large-scale corpora. Shao and Ng (2004) presented a method to mine new translations from Chinese and English news documents of the same period from different news agencies, combining both transliteration and context information. Kuo et al. (2006) used active learning and unsupervised learning for mining transliteration lexicon from the Web pages, in which an EM process was used for estimating the phonetic similarities between English syllables and Chinese characters. Cao et al. (2007) split parenthetical translation mining task into two parts, transliteration detection and translation detection. They employed a transliteration lexicon for constructing a grapheme-based transliteration model and annotated boundaries manually to train a classifier. Lin et al. (2008) applied a frequency-based word alignment approach, Competitive Link (Mel"
N09-1048,P08-1113,0,0.035358,"Missing"
N09-1048,C08-1071,0,0.0137806,"algorithm Chinese Web pages Require: L, U = {f1J (eI1 )}, T , M ¤L, (labeled) training set; U , (unlabeled) candidate set; T , test set; M, the transliteration or translation model. Parenthetical expression extraction{C(E)} S-MSRSeg Chinese word segmentation{c…(e…)} (Lin et al., 2008) Heuristic filtering{c…(e…)} Section 4 Bilingual abbreviation mining Section 5 Transliteration lexicon mining Section 6 Translation lexicon mining Figure 1: The system framework of mining lexicons from Chinese Web pages. (Zhu, 2007), such as self-training in word sense disambiguation (Yarowsky, 2005) and parsing (McClosky et al., 2008). In this paper, we apply selftraining to a new topic, lexicon mining. 3 System Framework and Self-Training Algorithm Figure 1 illustrates our system framework for mining lexicons from Chinese Web pages. First, parenthetical expressions matching Pattern 1 are extracted. Then, pre-parenthetical Chinese sequences are segmented into word sequences by S-MSRSeg2 (Gao et al., 2006). The initial parenthetical translation corpus is constructed by applying the heuristic rules defined in (Lin et al., 2008)3 . Based on this corpus, we mine three lexicons step by step, a bilingual abbreviation lexicon, a"
N09-1048,J00-2004,0,0.0753482,"Missing"
N09-1048,J03-1002,0,0.00467483,"Lin et al., 2008), wherein the lengthes of prefixes and suffixes of English words were assumed to be three bytes, we segment words into morphemes (sequences of prefixes, stems, and suffixes) by Morfessor 0.9.211 , an unsupervised language-independent morphological analyzer (Creutz and Lagus, 2007). We use the morpheme-level translation similarity explicitly in our cascaded translation model (Wu et al., 2008), which makes use of morpheme, word, and phrase level translation units. We train Moses to gain a phrase-level translation table. To gain a morpheme-level translation table, we run GIZA++ (Och and Ney, 2003) on both directions between English morphemes and Chinese characters, and take the intersection of Viterbi alignments. The Englishto-Chinese translation probabilities computed by GIZA++ are attached to each morpheme-character element in the intersection set. 6.1 Experiment The Wanfang Chinese-English technical term dictionary12 , which contains 525,259 entries in total, was used for training and testing. 10,000 entries were randomly selected as the test set and the remaining as the training set. Again, we investigated the scalability of the self-trained cascaded translation model by respective"
N09-1048,P02-1040,0,0.075371,"Missing"
N09-1048,C04-1089,0,0.175275,"n mining. One is the purely monolingual corpora, wherein frequency-based expectation-maximization (EM, refer to (Dempster et al., 1977)) algorithms and cognate clues play a central role (Koehn and Knight, 2002). Haghighi et al. (2008) presented a generative model based on canonical correlation analysis, in which monolingual features such as the context and orthographic substrings of words were taken into account. The other is multilingual parallel and comparable corpora (e.g., Wikipedia1 ), wherein features such as cooccurrence frequency and context are popularly employed (Cheng et al., 2004; Shao and Ng, 2004; Cao et al., 2007; Lin et al., 2008). In this paper, we focus on a special type of comparable corpus, parenthetical translations. The issue is motivated by the observation that Web pages and technical papers written in Asian languages (e.g., Chinese, Japanese) sometimes annotate named entities or technical terms with their translations in English inside a pair of parentheses. This is considered to be a traditional way to annotate new terms, personal names or other named entities with their English translations expressed in brackets. Formally, a parenthetical translation can be expressed by th"
N09-1048,2008.amta-papers.19,1,0.894739,"ods have been proposed. However, supervised 425 approaches are restricted by the quality and quantity of manually constructed training data, and unsupervised approaches are totally frequency-based without using any semantic clues. In contrast, we propose a semi-supervised framework for mining parenthetical translations. We apply a monolingual abbreviation extraction approach to bilingual abbreviation extraction. We construct an English-syllable to Chinese-pinyin transliteration model which is selftrained using phonemic similarity measurements. We further employ our cascaded translation model (Wu et al., 2008) which is self-trained based on morpheme-level translation similarity. This paper is organized as follows. We briefly review the related work in the next section. Our system framework and self-training algorithm is described in Section 3. Bilingual abbreviation extraction, self-trained transliteration models and cascaded translation models are described in Section 4, 5, and 6, respectively. In Section 7, we evaluate our mined lexicons by Wikipedia. We conclude in Section 8 finally. 2 Related Work Numerous researchers have proposed a variety of automatic approaches to mine lexicons from the Web"
N09-1048,P95-1026,0,0.403079,"Missing"
N09-1048,J05-4005,0,\N,Missing
N09-1048,H91-1026,0,\N,Missing
N09-2025,P06-1029,0,0.0671582,"Missing"
N09-2025,P07-1104,0,0.0592463,"and Theeiler, 2003), which incrementally adds features like boosting, but it can converge to the global optimum. We use L1 regularization because we can obtain a sparse parameter vector, for which many of the parameter values are exactly zero. In other words, learning with L1 regularization naturally has an intrinsic effect of feature selection, which results in an Proceedings of NAACL HLT 2009: Short Papers, pages 97–100, c Boulder, Colorado, June 2009. 2009 Association for Computational Linguistics efficient and interpretable inference with almost the same performance as L2 regularization (Gao et al., 2007). The heart of our algorithm is a way to find a feature that has the largest gradient value of likelihood from among the huge set of candidates. To solve this problem, we propose an example-wise algorithm with filtering. This algorithm is very simple and easy to implement, but effective in practice. We applied the proposed methods to NLP tasks, and found that our methods can achieve the same high performance as kernel methods, whereas the number of active combination features is relatively small, such as several thousands. 2 Preliminaries 2.1 i,y Logistic Regression Model In this paper, we con"
N09-2025,W04-3239,0,0.0720182,"Missing"
N09-2025,C04-1002,0,0.296263,"hat many candidate features will be removed just before adding. 4 Experiments To measure the effectiveness of the proposed method (called L1 -Comb), we conducted experiments on the dependency analysis task, and the document classification task. In all experiments, the parameter C was tuned using the development data set. In the first experiment, we performed Japanese dependency analysis. We used the Kyoto Text Corpus (Version 3.0), Jan. 1, 3-8 as the training data, Jan. 10 as the development data, and Jan. 9 as the test data so that the result could be compared to those from previous studies (Sassano, 2004)2 . We used the shift-reduce dependency algorithm (Sassano, 2004). The number of training events was 11, 3332, each of which consisted of two word positions as inputs, and y = {0, 1} as an output indicating the dependency relation. For the training data, the number of original features was 78570, and the number of combination features of degrees 2 and 3 was 5787361, and 169430335, respectively. Note that we need not see all of them using our algorithm. 2 The data set is different from that in the CoNLL shared task. This data set is more difficult. Table 1: The performance of the Japanese depen"
N09-2031,C02-2020,0,0.560072,"ionary plays an important role in many natural language processing tasks. For example, machine translation uses bilingual dictionary to reinforce word and phrase alignment (Och and Ney, 2003), crosslanguage information retrieval uses bilingual dictionary for query translation (Grefenstette, 1998). The direct way of bilingual dictionary acquisition is aligning translation candidates using parallel corpora (Wu, 1994). But for some languages, collecting parallel corpora is not easy. Therefore, many researchers paid attention to bilingual dictionary extraction from comparable corpora (Fung, 2000; Chiao and Zweigenbaum, 2002; Daille and Morin, 2008; Robitaille et al., 2006; Morin et al., 2007; Otero, 2008), in which texts are not exact translation of each other but share common features. Context-based approach, which is based on the observation that a term and its translation appear in similar lexical contexts (Daille and Morin, 2008), is the most popular approach for extracting bilingual dictionary from comparable corpora and has shown its effectiveness in terminology extraction (Fung, 2000; Chiao and Zweigenbaum, 2002; Robitaille et al., 2006; Morin et al., 2007). But it only concerns about the lexical context"
N09-2031,W95-0114,0,0.804315,"ical contexts (Daille and Morin, 2008), is the most popular approach for extracting bilingual dictionary from comparable corpora and has shown its effectiveness in terminology extraction (Fung, 2000; Chiao and Zweigenbaum, 2002; Robitaille et al., 2006; Morin et al., 2007). But it only concerns about the lexical context around translation candidates in a restricted window. Besides, in comparable corpora, some words may appear in similar context even if they are not translation of each other. For example, using a Chinese-English comparable corpus from Wikipedia and following the definition in (Fung, 1995), we get context heterogeneity vector of three words (see Table 1). The Euclidean distance between the vector of ‘经济学(economics)’ and ‘economContext Heterogeneity Vector (0.185, 0.006) (0.101, 0.013) (0.113,0.028) To solve this problem, we investigate a comparable corpora from Wikipedia and find the following phenomenon: if we preprocessed the corpora with a dependency syntactic analyzer, a word in source language shares similar head and modifiers with its translation in target language, no matter whether they occur in similar context or not. We call this phenomenon as dependency heterogeneity"
N09-2031,1998.amta-tutorials.5,0,0.141815,"results using 250 randomly selected translation pairs prove that the proposed approach significantly outperforms the traditional contextbased approach that uses bag-of-words around translation candidates. 1 Table 1. Context heterogeneity vector of words. Word 经济学(economics) economics medicine Introduction Bilingual dictionary plays an important role in many natural language processing tasks. For example, machine translation uses bilingual dictionary to reinforce word and phrase alignment (Och and Ney, 2003), crosslanguage information retrieval uses bilingual dictionary for query translation (Grefenstette, 1998). The direct way of bilingual dictionary acquisition is aligning translation candidates using parallel corpora (Wu, 1994). But for some languages, collecting parallel corpora is not easy. Therefore, many researchers paid attention to bilingual dictionary extraction from comparable corpora (Fung, 2000; Chiao and Zweigenbaum, 2002; Daille and Morin, 2008; Robitaille et al., 2006; Morin et al., 2007; Otero, 2008), in which texts are not exact translation of each other but share common features. Context-based approach, which is based on the observation that a term and its translation appear in sim"
N09-2031,P07-1084,0,0.333389,"Missing"
N09-2031,P07-2055,0,0.0154395,"ize 4 medicine is tends include moved means requires includes were has may Bilingual Dictionary Extraction with Dependency Heterogeneity Based on the observation of dependency heterogeneity in comparable corpora, we propose an approach to extract bilingual dictionary using dependency heterogeneity similarity. 4.1 Comparable Corpora Preprocessing Before calculating dependency heterogeneity similarity, we need to preprocess the comparable corpora. In this work, we focus on Chinese-English bilingual dictionary extraction for single-nouns. Therefore, we first use a Chinese morphological analyzer (Nakagawa and Uchimoto, 2007) and an English pos-tagger (Tsuruoka et al., 2005) to analyze the raw corpora. Then we use MaltParser (Nivre et al., 2007) to get syntactic dependency of both the Chinese corpus and the English corpus. The dependency labels produced by MaltParser (e.g. SUB) are used to decide the type of heads and modifiers. After that, the analyzed corpora are refined through following steps: (1) we use a stemmer1 to do stemming for the English corpus. Considering that only nouns are treated as translation candidates, we use stems for translation candidate but keep the original form of their heads and modifie"
N09-2031,J03-1002,0,0.00224588,"based on the observation that a word and its translation share similar dependency relations. Experimental results using 250 randomly selected translation pairs prove that the proposed approach significantly outperforms the traditional contextbased approach that uses bag-of-words around translation candidates. 1 Table 1. Context heterogeneity vector of words. Word 经济学(economics) economics medicine Introduction Bilingual dictionary plays an important role in many natural language processing tasks. For example, machine translation uses bilingual dictionary to reinforce word and phrase alignment (Och and Ney, 2003), crosslanguage information retrieval uses bilingual dictionary for query translation (Grefenstette, 1998). The direct way of bilingual dictionary acquisition is aligning translation candidates using parallel corpora (Wu, 1994). But for some languages, collecting parallel corpora is not easy. Therefore, many researchers paid attention to bilingual dictionary extraction from comparable corpora (Fung, 2000; Chiao and Zweigenbaum, 2002; Daille and Morin, 2008; Robitaille et al., 2006; Morin et al., 2007; Otero, 2008), in which texts are not exact translation of each other but share common feature"
N09-2031,2007.mtsummit-papers.26,0,0.575516,"The main difference between these works and our approach is still our usage of syntactic dependency other than bag-of-words. In addition, except for a morphological analyzer and a dependency parser, our approach does not need other external resources, such as the external dictionary. Because of the well-developed morphological and syntactic analysis research in recent years, the requirement of analyzers will not bring too much burden to the proposed approach. Besides of using window-based contexts, there were also some works utilizing syntactic information for bilingual dictionary extraction. Otero (2007) extracted lexico-syntactic templates from parallel corpora first, and then used them as seeds to calculate similarity between translation candidates. Otero (2008) defined syntactic rules to get lexico-syntactic contexts of words, and then used an external bilingual dictionary to fulfill similarity calculation between the lexico-syntactic context vectors of translation candidates. Our approach differs from these works in two ways: (1) both the above works defined syntactic rules or templates by hand to get syntactic information. Our approach uses data-driven syntactic analyzers for acquiring d"
N09-2031,E06-1029,0,0.205134,"Missing"
N09-2031,1994.amta-1.26,0,0.108141,"ontextbased approach that uses bag-of-words around translation candidates. 1 Table 1. Context heterogeneity vector of words. Word 经济学(economics) economics medicine Introduction Bilingual dictionary plays an important role in many natural language processing tasks. For example, machine translation uses bilingual dictionary to reinforce word and phrase alignment (Och and Ney, 2003), crosslanguage information retrieval uses bilingual dictionary for query translation (Grefenstette, 1998). The direct way of bilingual dictionary acquisition is aligning translation candidates using parallel corpora (Wu, 1994). But for some languages, collecting parallel corpora is not easy. Therefore, many researchers paid attention to bilingual dictionary extraction from comparable corpora (Fung, 2000; Chiao and Zweigenbaum, 2002; Daille and Morin, 2008; Robitaille et al., 2006; Morin et al., 2007; Otero, 2008), in which texts are not exact translation of each other but share common features. Context-based approach, which is based on the observation that a term and its translation appear in similar lexical contexts (Daille and Morin, 2008), is the most popular approach for extracting bilingual dictionary from com"
N09-2031,I08-1013,0,\N,Missing
N10-1090,J99-2004,0,0.799406,"Missing"
N10-1090,W03-1006,0,0.0618424,"Missing"
N10-1090,W02-2203,0,0.237479,"Missing"
N10-1090,P07-1037,0,0.0586619,"Missing"
N10-1090,P05-1012,0,0.0520652,"do. However, current widely used sequence labeling models have the limited ability to catch these longdistance syntactic relations. In supertagging stage, tree structures are still not constructed. Dependency formalism is an alternative way to describe these two syntactic properties. Based on this observation, we think dependency information could assist supertag prediction. To model the dependency, we follow mainstream dependency parsing formalism. Two representative methods for dependency parsing are transitionbased model like MaltParser (Nivre, 2003) and graph-based model like MSTParser1 (McDonald et al., 2005). Previous research (Nivre and McDonald, 2008) showed that MSTParser is more accurate than MaltParser for long dependencies. Since our motivation is to capture long-distance dependency as a complement for local supertagging models, we use the projective MSTParser formalism to model dependencies. MOD-IN MOD-OUT Table 1: Non-local feature templates used for supertagging. Here, p, w and s represent POS, word and schema respectively. Direction (Left/Right) from MODIN/MODOUT word to the current word is also considered in the feature templates. 3.2 Figure 1: Model structure of incorporating dependen"
N10-1090,W06-1619,1,0.927482,"Missing"
N10-1090,P08-1108,0,0.0151596,"labeling models have the limited ability to catch these longdistance syntactic relations. In supertagging stage, tree structures are still not constructed. Dependency formalism is an alternative way to describe these two syntactic properties. Based on this observation, we think dependency information could assist supertag prediction. To model the dependency, we follow mainstream dependency parsing formalism. Two representative methods for dependency parsing are transitionbased model like MaltParser (Nivre, 2003) and graph-based model like MSTParser1 (McDonald et al., 2005). Previous research (Nivre and McDonald, 2008) showed that MSTParser is more accurate than MaltParser for long dependencies. Since our motivation is to capture long-distance dependency as a complement for local supertagging models, we use the projective MSTParser formalism to model dependencies. MOD-IN MOD-OUT Table 1: Non-local feature templates used for supertagging. Here, p, w and s represent POS, word and schema respectively. Direction (Left/Right) from MODIN/MODOUT word to the current word is also considered in the feature templates. 3.2 Figure 1: Model structure of incorporating dependency information into the supertagging stage. Do"
N10-1090,W03-3017,0,0.0457851,"he word well, supertagging would be an easier job to do. However, current widely used sequence labeling models have the limited ability to catch these longdistance syntactic relations. In supertagging stage, tree structures are still not constructed. Dependency formalism is an alternative way to describe these two syntactic properties. Based on this observation, we think dependency information could assist supertag prediction. To model the dependency, we follow mainstream dependency parsing formalism. Two representative methods for dependency parsing are transitionbased model like MaltParser (Nivre, 2003) and graph-based model like MSTParser1 (McDonald et al., 2005). Previous research (Nivre and McDonald, 2008) showed that MSTParser is more accurate than MaltParser for long dependencies. Since our motivation is to capture long-distance dependency as a complement for local supertagging models, we use the projective MSTParser formalism to model dependencies. MOD-IN MOD-OUT Table 1: Non-local feature templates used for supertagging. Here, p, w and s represent POS, word and schema respectively. Direction (Left/Right) from MODIN/MODOUT word to the current word is also considered in the feature temp"
N10-1090,P07-1079,1,0.916812,"Missing"
N10-1090,P03-1064,0,0.0637293,"Missing"
N10-1090,W09-3832,1,0.802062,"es encode possible syntactic behavior of a word. Although the number of supertags is far larger than the 45 POS tags defined in Penn Treebank, sequence labeling techniques are still effective for supertagging. Previous research (Clark, 2002) showed that a POS sequence is very informative for supertagging, and some extent of local syntactic information can be captured by the context of surrounding words and POS tags. However, since the context window length is limited for the computational cost reasons, there are still long-range dependencies which are not easily captured in sequential models (Zhang et al., 2009). In practice, the multi-tagging technique proposed by Clark (2002) assigned more than one supertag to each word and let the ambiguous supertags be selected by the parser. As for other NLP applications which use supertags, resolving more supertag ambiguities in supertagging stage is preferred. With this consideration, we focus on supertagging and aim to make it as accurate as possible. In this paper, we incorporated long-distance information into supertagging. First, we used dependency parser formalism to model long-distance relationships between the input words, which is hard to model in sequ"
N10-1090,W07-0702,0,\N,Missing
N10-1090,P08-1000,0,\N,Missing
N15-1143,S07-1103,0,0.0166829,"5, 2015. 2015 Association for Computational Linguistics clues and determines the sizes of many physical objects simultaneously. The approach consists of two steps: (i) many different types of clues to the numerical attribute are collected from various linguistics resources, and (ii) those collected clues are brought together by a combined regression and ranking. 2 Related Work Hovy et al. (2002) pointed out the importance of the knowledge on the numerical attributes in question answering. They hand-coded the possible range of a numerical attribute. Akiba et al. (2004), Fujihata et al. (2001), Aramaki et al. (2007), and Bakalov et al. (2011) made similar attempts. Their target, however, is the fixed numerical attributes of the named entities, while our target is the numerical attributes of general physical objects, not restricted to the named entities. Davidov and Rappoport (2010) collected various types of text fragments indicating values of numerical attributes of physical objects. Our work differs from theirs in that we explore more subtle linguistic clues in addition to those used in the previous work, by using a global mathematical model that brings together all the clues. Narisawa et al. (2013) tr"
N15-1143,P10-1133,0,0.459083,"resources, and (ii) those collected clues are brought together by a combined regression and ranking. 2 Related Work Hovy et al. (2002) pointed out the importance of the knowledge on the numerical attributes in question answering. They hand-coded the possible range of a numerical attribute. Akiba et al. (2004), Fujihata et al. (2001), Aramaki et al. (2007), and Bakalov et al. (2011) made similar attempts. Their target, however, is the fixed numerical attributes of the named entities, while our target is the numerical attributes of general physical objects, not restricted to the named entities. Davidov and Rappoport (2010) collected various types of text fragments indicating values of numerical attributes of physical objects. Our work differs from theirs in that we explore more subtle linguistic clues in addition to those used in the previous work, by using a global mathematical model that brings together all the clues. Narisawa et al. (2013) tried to determine whether a given amount is large, small, or normal as a size of an object, making good use of clue words such as only; The sentence “This laptop weighs only 0.7kg” means that laptops are usually heavier than 0.7kg. 3 Fragmentary clues to sizes 3.1 Physica"
N15-1143,C02-1042,0,0.0462621,"We have therefore developed a mathematical model that uses these 1305 Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 1305–1310, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics clues and determines the sizes of many physical objects simultaneously. The approach consists of two steps: (i) many different types of clues to the numerical attribute are collected from various linguistics resources, and (ii) those collected clues are brought together by a combined regression and ranking. 2 Related Work Hovy et al. (2002) pointed out the importance of the knowledge on the numerical attributes in question answering. They hand-coded the possible range of a numerical attribute. Akiba et al. (2004), Fujihata et al. (2001), Aramaki et al. (2007), and Bakalov et al. (2011) made similar attempts. Their target, however, is the fixed numerical attributes of the named entities, while our target is the numerical attributes of general physical objects, not restricted to the named entities. Davidov and Rappoport (2010) collected various types of text fragments indicating values of numerical attributes of physical objects."
N15-1143,P13-1038,0,0.475201,"1), Aramaki et al. (2007), and Bakalov et al. (2011) made similar attempts. Their target, however, is the fixed numerical attributes of the named entities, while our target is the numerical attributes of general physical objects, not restricted to the named entities. Davidov and Rappoport (2010) collected various types of text fragments indicating values of numerical attributes of physical objects. Our work differs from theirs in that we explore more subtle linguistic clues in addition to those used in the previous work, by using a global mathematical model that brings together all the clues. Narisawa et al. (2013) tried to determine whether a given amount is large, small, or normal as a size of an object, making good use of clue words such as only; The sentence “This laptop weighs only 0.7kg” means that laptops are usually heavier than 0.7kg. 3 Fragmentary clues to sizes 3.1 Physical objects We first collect physical objects, i.e., objects for which the size can be defined. However, the numerical attribute of a word depends on the sense in which the word is being used. We will therefore determine the size of each sense instead of each word. Specifically, we determine the size of each noun synset in the"
N15-1143,W09-3401,0,\N,Missing
nguyen-etal-2008-challenges,H05-1059,1,\N,Missing
nguyen-etal-2008-challenges,P06-1005,0,\N,Missing
nguyen-etal-2008-challenges,P06-1006,0,\N,Missing
nguyen-etal-2008-challenges,P07-1107,0,\N,Missing
nguyen-etal-2008-challenges,J01-4004,0,\N,Missing
nguyen-etal-2008-challenges,P98-2143,0,\N,Missing
nguyen-etal-2008-challenges,C98-2138,0,\N,Missing
ohta-etal-2006-linguistic,I05-2038,1,\N,Missing
ohta-etal-2006-linguistic,W05-1308,0,\N,Missing
ohta-etal-2006-linguistic,W04-1207,0,\N,Missing
ohta-etal-2006-linguistic,W05-0304,0,\N,Missing
ohta-etal-2006-linguistic,W04-3111,0,\N,Missing
ohta-etal-2006-linguistic,tateisi-tsujii-2004-part,1,\N,Missing
ohta-etal-2006-linguistic,W04-1201,0,\N,Missing
P00-1002,C00-1030,1,0.802491,"Missing"
P00-1034,O92-1001,0,\N,Missing
P00-1034,W96-0213,0,\N,Missing
P00-1034,C94-1027,0,\N,Missing
P00-1044,A97-1028,0,\N,Missing
P00-1044,O97-1012,0,\N,Missing
P03-1038,W99-0615,1,0.899042,"Missing"
P03-1038,P94-1025,0,0.146263,"Missing"
P03-1038,W96-0213,0,\N,Missing
P03-1038,A00-1031,0,\N,Missing
P03-2033,W97-1506,0,0.0736449,"Missing"
P03-2033,W02-1508,0,0.0189577,"l-purpose grammar through using language intuition encoded in syntactically tagged corpora in XML format. Second, it records data of grammar defects to allow developers to have a whole picture of parsing errors found in the target corpora to save debugging time and effort by prioritizing them. 2 What Is the Ideal Grammar Debugging? There are already other grammar developing tools, such as a grammar writer of XTAG (Paroubek et al., 1992), ALEP (Schmidt et al., 1996), ConTroll (G¨otz and Meurers, 1997), a tool by Nara Institute of Science and Technology (Miyata et al., 1999), and [incr tsdb()] (Oepen et al., 2002). But these tools have following problems; they largely depend on human debuggers’ language intuition, they do not help users to handle large amount of parsing results effectively, and they let human debuggers correct the bugs one after another manually and locally. To cope with these shortcomings, willex proposes an alternative method for more efficient debugging process. The workflow of the conventional grammar developing tools and willex are different in the following ways. With the conventional tools, human debuggers must check each sentence to find out grammar defects and modify them one"
P03-2033,A92-1030,0,0.0317506,"n effort. Hence, we have developed willex that helps to improve the general-purpose grammars. Willex has two major functions. First, it reduces a human workload to improve the general-purpose grammar through using language intuition encoded in syntactically tagged corpora in XML format. Second, it records data of grammar defects to allow developers to have a whole picture of parsing errors found in the target corpora to save debugging time and effort by prioritizing them. 2 What Is the Ideal Grammar Debugging? There are already other grammar developing tools, such as a grammar writer of XTAG (Paroubek et al., 1992), ALEP (Schmidt et al., 1996), ConTroll (G¨otz and Meurers, 1997), a tool by Nara Institute of Science and Technology (Miyata et al., 1999), and [incr tsdb()] (Oepen et al., 2002). But these tools have following problems; they largely depend on human debuggers’ language intuition, they do not help users to handle large amount of parsing results effectively, and they let human debuggers correct the bugs one after another manually and locally. To cope with these shortcomings, willex proposes an alternative method for more efficient debugging process. The workflow of the conventional grammar deve"
P03-2033,C96-1049,0,0.0133803,"oped willex that helps to improve the general-purpose grammars. Willex has two major functions. First, it reduces a human workload to improve the general-purpose grammar through using language intuition encoded in syntactically tagged corpora in XML format. Second, it records data of grammar defects to allow developers to have a whole picture of parsing errors found in the target corpora to save debugging time and effort by prioritizing them. 2 What Is the Ideal Grammar Debugging? There are already other grammar developing tools, such as a grammar writer of XTAG (Paroubek et al., 1992), ALEP (Schmidt et al., 1996), ConTroll (G¨otz and Meurers, 1997), a tool by Nara Institute of Science and Technology (Miyata et al., 1999), and [incr tsdb()] (Oepen et al., 2002). But these tools have following problems; they largely depend on human debuggers’ language intuition, they do not help users to handle large amount of parsing results effectively, and they let human debuggers correct the bugs one after another manually and locally. To cope with these shortcomings, willex proposes an alternative method for more efficient debugging process. The workflow of the conventional grammar developing tools and willex are d"
P03-2033,W98-0141,1,0.819893,"ons. First, it decreases ambiguity of the parsing results by comparing them to an annotated corpus and removing wrong partial results both automatically and manually. Second, willex accumulates parsing errors as data for the developers to clarify the defects of the grammar statistically. We applied willex to a large-scale HPSG-style grammar as an example. 1 Introduction There is an increasing need for syntactical parsers for practical usages, such as information extraction. For example, Yakushiji et al. (2001) extracted argument structures from biomedical papers using a parser based on XHPSG (Tateisi et al., 1998), which is a large-scale HPSG. Although large-scale and general-purpose grammars have been developed, they have a problem of limited coverage. The limits are derived from deficiencies of grammars themselves. For example, XHPSG cannot treat coordinations of verbs (ex. “Molybdate slowed but did not prevent the conversion.”) nor reduced relatives (ex. “Rb mutants derived from patients with retinoblastoma.”). Finding these grammar defects and modifying them require tremendous human effort. Hence, we have developed willex that helps to improve the general-purpose grammars. Willex has two major func"
P03-2033,W98-1118,0,\N,Missing
P03-2033,C00-2102,0,\N,Missing
P03-2033,P02-1060,0,\N,Missing
P03-2036,P81-1022,0,0.0851703,"Missing"
P03-2036,2000.iwpt-1.15,0,0.76089,"ations sometimes exhibit quite different performance in each grammar formalism (Yoshida et al., 1999; Yoshinaga et al., 2001). If we could identify an algorithmic difference that causes performance difference, it would reveal advantages and disadvantages of the different realizations. This should also allow us to integrate the advantages of the realizations into one generic parsing technique, which yields the further advancement of the whole parsing community. In this paper, we compare CFG filtering techniques for LTAG (Harbusch, 1990; Poller and Becker, 1998) and HPSG (Torisawa et al., 2000; Kiefer and Krieger, 2000), following an approach to parsing comparison among different grammar formalisms (Yoshinaga et al., 2001). The key idea of the approach is to use strongly equivalent grammars, which generate equivalent parse results for the same input, obtained by a grammar conversion as demonstrated by Yoshinaga and Miyao (2001). The parsers with CFG filtering predict possible parse trees by a CFG approximated from a given grammar. Comparison of those parsers are interesting because effective CFG filters allow us to bring the empirical time complexity of the parsers close to that of CFG parsing. Investigating"
P03-2036,J93-2004,0,0.0307471,"from a given grammar. Comparison of those parsers are interesting because effective CFG filters allow us to bring the empirical time complexity of the parsers close to that of CFG parsing. Investigating the difference between the ways of context-free (CF) approximation of LTAG and HPSG will thereby enlighten a way of further optimization for both techniques. We performed a comparison between the existing CFG filtering techniques for LTAG (Poller and Becker, 1998) and HPSG (Torisawa et al., 2000), using strongly equivalent grammars obtained by converting LTAGs extracted from the Penn Treebank (Marcus et al., 1993) into HPSG-style. We compared the parsers with respect to the size of the approximated CFG and its effectiveness as a filter. 2 Background In this section, we introduce a grammar conversion (Yoshinaga and Miyao, 2001) and CFG filtering (Harbusch, 1990; Poller and Becker, 1998; Torisawa et al., 2000; Kiefer and Krieger, 2000). 2.1 Grammar conversion The grammar conversion consists of a conversion of LTAG elementary trees to HPSG lexical entries and an emulation of substitution and adjunction by Tree 5: Tree 9: S 5.ε NP5.1 VP5.2 V 5.2.1 NP5.2.2 S 9.ε NP9.1 VP 9.2 V 9.2.1 S 9.2.2 CFG rules NP VP"
P03-2036,J93-4001,0,0.584153,"Missing"
P03-2036,E03-1047,1,0.493609,",115 58,356 68,239 118,464 Table 2: Parsing performance (sec.) with the strongly equivalent grammars for Section 2 of WSJ Parser PB TNT 3 G2 1.4 0.044 G2-4 9.1 0.097 G2-6 17.4 0.144 G2-8 24.0 0.182 G2-10 34.2 0.224 G2-21 124.3 0.542 Comparison with CFG filtering In this section, we compare a pair of CFG filtering techniques for LTAG (Poller and Becker, 1998) and HPSG (Torisawa et al., 2000) described in Section 2.2.1 and 2.2.2. We hereafter refer to PB and TNT for the C++ implementations of the former and a valiant1 of the latter, respectively.2 We first acquired LTAGs by a method proposed in Miyao et al. (2003) from Sections 2-21 of the Wall Street Journal (WSJ) in the Penn Treebank (Marcus et al., 1993) and its subsets.3 We then converted them into strongly equivalent HPSG-style grammars using the grammar conversion described in Section 2.1. Table 1 shows the size of CFG approximated from the strongly equivalent grammars. Gx , CFGPB , and CFGTNT henceforth refer to the LTAG extracted from Section x of WSJ and CFGs approximated from Gx by PB and TNT, respectively. The size of CFGTNT is much larger than that of CFGPB . By investigating parsing performance using these CFGs, we show that the larger siz"
P03-2036,W98-0134,0,0.552982,"t al., 1999; Torisawa et al., 2000). However, these realizations sometimes exhibit quite different performance in each grammar formalism (Yoshida et al., 1999; Yoshinaga et al., 2001). If we could identify an algorithmic difference that causes performance difference, it would reveal advantages and disadvantages of the different realizations. This should also allow us to integrate the advantages of the realizations into one generic parsing technique, which yields the further advancement of the whole parsing community. In this paper, we compare CFG filtering techniques for LTAG (Harbusch, 1990; Poller and Becker, 1998) and HPSG (Torisawa et al., 2000; Kiefer and Krieger, 2000), following an approach to parsing comparison among different grammar formalisms (Yoshinaga et al., 2001). The key idea of the approach is to use strongly equivalent grammars, which generate equivalent parse results for the same input, obtained by a grammar conversion as demonstrated by Yoshinaga and Miyao (2001). The parsers with CFG filtering predict possible parse trees by a CFG approximated from a given grammar. Comparison of those parsers are interesting because effective CFG filters allow us to bring the empirical time complexity"
P03-2036,C88-2121,0,0.322459,"Missing"
P03-2036,P90-1036,0,\N,Missing
P04-3017,P98-2132,1,0.723841,"y of Medicine’s MEDLINE database. We deﬁned topical nouns as the names tagged as protein, peptide, amino acid, DNA, RNA, or nucleic acid. We chose PASs which take one or more topical nouns as an argument or arguments, and substrings matched by POS patterns which include topical nouns. All names tagged in the corpus were replaced by their head nouns in order to reduce complexity of sentences and thus reduce the task of the parser and the POS pattern matcher. 4.1 Implementation of PAS method We implemented PAS method on LiLFeS, a uniﬁcation-based programming system for typed feature structures (Makino et al., 1998; Miyao et al., 2000). The selection in Step 2 described in Section 3 is realized by matching PASs with nine PAS templates. Four of the templates are illustrated in Figure 3. 4.2 POS Pattern Method We constructed a POS pattern matcher with a partial verb chunking function according to (Hatzivassiloglou and Weng, 2002). Because the original matcher has problems in recall (its verb group detector has low coverage) and precision (it does not consider other words to detect relations between verb groups and topical nouns), we implemented 2 (a) may be selected if the anaphora (“it”) is resolved. But"
P04-3017,P03-1029,0,0.0362329,"Missing"
P04-3017,C98-2128,1,\N,Missing
P05-1010,A00-2018,0,\N,Missing
P05-1010,J98-4004,0,\N,Missing
P05-1010,W96-0214,0,\N,Missing
P05-1010,C02-1126,0,\N,Missing
P05-1010,J03-4003,0,\N,Missing
P05-1010,P03-1054,0,\N,Missing
P05-1010,N03-1014,0,\N,Missing
P05-1010,P04-1014,0,\N,Missing
P05-1010,W03-3021,0,\N,Missing
P05-1010,W04-3327,0,\N,Missing
P05-1010,P96-1024,0,\N,Missing
P05-1011,J97-4005,0,0.235291,"02; Geman and Johnson, 2002; Miyao and Tsujii, 2002; Clark and Curran, 2004b; Kaplan et al., 2004). Previous studies on probabilistic models for HPSG (Toutanova and Manning, 2002; Baldridge and Osborne, 2003; Malouf and van Noord, 2004) also adopted log-linear models. HPSG exploits feature structures to represent linguistic constraints. Such constraints are known 83 Proceedings of the 43rd Annual Meeting of the ACL, pages 83–90, c Ann Arbor, June 2005. 2005 Association for Computational Linguistics to introduce inconsistencies in probabilistic models estimated using simple relative frequency (Abney, 1997). Log-linear models are required for credible probabilistic models and are also beneficial for incorporating various overlapping features. This study follows previous studies on the probabilistic models for HPSG. The probability,  , of producing the parse result  from a given sentence  is defined as           ´ µ                      where    is a reference distribution (usually assumed to be a uniform distribution), and   is a set of parse candidates assigned to . The feature function    represents the characteristic"
P05-1011,W02-2018,0,0.0802697,"ls for HPSG. The probability,  , of producing the parse result  from a given sentence  is defined as           ´ µ                      where    is a reference distribution (usually assumed to be a uniform distribution), and   is a set of parse candidates assigned to . The feature function    represents the characteristics of  and , while the corresponding model parameter    is its weight. Model parameters that maximize the loglikelihood of the training data are computed using a numerical optimization method (Malouf, 2002). Estimation of the above model requires a set of pairs    , where  is the correct parse for sentence . While  is provided by a treebank,   is computed by parsing each  in the treebank. Previous studies assumed   could be enumerated; however, the assumption is impractical because the size of   is exponentially related to the length of . The problem of exponential explosion is inevitable in the wide-coverage parsing of real-world texts because many parse candidates are produced to support various constructions in long sentences. Figure 1: Chart for parsing “he saw a girl wi"
P05-1011,H94-1020,0,0.0768769,"et al., 2004). A large treebank can be used as training and test data for statistical models. Therefore, we now have the basis for the development and the evaluation of statistical disambiguation models for wide-coverage HPSG parsing. Jun’ichi Tsujii Department of Computer Science University of Tokyo Hongo 7-3-1, Bunkyo-ku, Tokyo, Japan CREST, JST tsujii@is.s.u-tokyo.ac.jp The aim of this paper is to report the development of log-linear models for the disambiguation in widecoverage HPSG parsing, and their empirical evaluation through the parsing of the Wall Street Journal of Penn Treebank II (Marcus et al., 1994). This is challenging because the estimation of log-linear models is computationally expensive, and we require solutions to make the model estimation tractable. We apply two techniques for reducing the training cost. One is the estimation on a packed representation of HPSG parse trees (Section 3). The other is the filtering of parse candidates according to a preliminary probability distribution (Section 4). To our knowledge, this work provides the first results of extensive experiments of parsing Penn Treebank with a probabilistic HPSG. The results from the Wall Street Journal are significant"
P05-1011,W03-0403,0,0.0962518,"t because the complexity of the sentences is different from that of short sentences. Experiments of the parsing of realworld sentences can properly evaluate the effectiveness and possibility of parsing models for HPSG. 2 Disambiguation models for HPSG Discriminative log-linear models are now becoming a de facto standard for probabilistic disambiguation models for deep parsing (Johnson et al., 1999; Riezler et al., 2002; Geman and Johnson, 2002; Miyao and Tsujii, 2002; Clark and Curran, 2004b; Kaplan et al., 2004). Previous studies on probabilistic models for HPSG (Toutanova and Manning, 2002; Baldridge and Osborne, 2003; Malouf and van Noord, 2004) also adopted log-linear models. HPSG exploits feature structures to represent linguistic constraints. Such constraints are known 83 Proceedings of the 43rd Annual Meeting of the ACL, pages 83–90, c Ann Arbor, June 2005. 2005 Association for Computational Linguistics to introduce inconsistencies in probabilistic models estimated using simple relative frequency (Abney, 1997). Log-linear models are required for credible probabilistic models and are also beneficial for incorporating various overlapping features. This study follows previous studies on the probabilistic"
P05-1011,C04-1041,0,0.577901,"tensive experiments of parsing Penn Treebank with a probabilistic HPSG. The results from the Wall Street Journal are significant because the complexity of the sentences is different from that of short sentences. Experiments of the parsing of realworld sentences can properly evaluate the effectiveness and possibility of parsing models for HPSG. 2 Disambiguation models for HPSG Discriminative log-linear models are now becoming a de facto standard for probabilistic disambiguation models for deep parsing (Johnson et al., 1999; Riezler et al., 2002; Geman and Johnson, 2002; Miyao and Tsujii, 2002; Clark and Curran, 2004b; Kaplan et al., 2004). Previous studies on probabilistic models for HPSG (Toutanova and Manning, 2002; Baldridge and Osborne, 2003; Malouf and van Noord, 2004) also adopted log-linear models. HPSG exploits feature structures to represent linguistic constraints. Such constraints are known 83 Proceedings of the 43rd Annual Meeting of the ACL, pages 83–90, c Ann Arbor, June 2005. 2005 Association for Computational Linguistics to introduce inconsistencies in probabilistic models estimated using simple relative frequency (Abney, 1997). Log-linear models are required for credible probabilistic mod"
P05-1011,P04-1014,0,0.728437,"tensive experiments of parsing Penn Treebank with a probabilistic HPSG. The results from the Wall Street Journal are significant because the complexity of the sentences is different from that of short sentences. Experiments of the parsing of realworld sentences can properly evaluate the effectiveness and possibility of parsing models for HPSG. 2 Disambiguation models for HPSG Discriminative log-linear models are now becoming a de facto standard for probabilistic disambiguation models for deep parsing (Johnson et al., 1999; Riezler et al., 2002; Geman and Johnson, 2002; Miyao and Tsujii, 2002; Clark and Curran, 2004b; Kaplan et al., 2004). Previous studies on probabilistic models for HPSG (Toutanova and Manning, 2002; Baldridge and Osborne, 2003; Malouf and van Noord, 2004) also adopted log-linear models. HPSG exploits feature structures to represent linguistic constraints. Such constraints are known 83 Proceedings of the 43rd Annual Meeting of the ACL, pages 83–90, c Ann Arbor, June 2005. 2005 Association for Computational Linguistics to introduce inconsistencies in probabilistic models estimated using simple relative frequency (Abney, 1997). Log-linear models are required for credible probabilistic mod"
P05-1011,C02-2025,0,0.045094,"of log-linear models requires high computational cost, especially with widecoverage grammars. Using techniques to reduce the estimation cost, we trained the models using 20 sections of Penn Treebank. A series of experiments empirically evaluated the estimation techniques, and also examined the performance of the disambiguation models on the parsing of real-world sentences. 1 Introduction Head-Driven Phrase Structure Grammar (HPSG) (Pollard and Sag, 1994) has been studied extensively from both linguistic and computational points of view. However, despite research on HPSG processing efficiency (Oepen et al., 2002a), the application of HPSG parsing is still limited to specific domains and short sentences (Oepen et al., 2002b; Toutanova and Manning, 2002). Scaling up HPSG parsing to assess real-world texts is an emerging research field with both theoretical and practical applications. Recently, a wide-coverage grammar and a large treebank have become available for English HPSG (Miyao et al., 2004). A large treebank can be used as training and test data for statistical models. Therefore, we now have the basis for the development and the evaluation of statistical disambiguation models for wide-coverage HP"
P05-1011,C00-1085,0,0.0745644,"rst assigned to each word a small number of supertags, which correspond to lexical entries in our case, and parsed supertagged sentences. Since they did not mention the probabilities of supertags, their method corresponds to our “filtering only” method. However, they also applied the same supertagger in a parsing stage, and this seemed to be crucial for high accuracy. This means that they estimated the probability of producing a parse tree from a supertagged sentence. Another approach to estimating log-linear models for HPSG is to extract a small informative sample from the original set   (Osborne, 2000). Malouf and van Noord (2004) successfully applied this method to German HPSG. The problem with this method was in the approximation of exponentially many parse trees by a polynomial-size sample. However, their method has the advantage that any features on a parse tree can be incorporated into the model. The trade-off between approximation and locality of features is an outstanding problem. Other discriminative classifiers were applied to the disambiguation in HPSG parsing (Baldridge and Osborne, 2003; Toutanova et al., 2004). The problem of exponential explosion is also inevitable for 7 Discu"
P05-1011,P02-1036,0,0.530025,"ledge, this work provides the first results of extensive experiments of parsing Penn Treebank with a probabilistic HPSG. The results from the Wall Street Journal are significant because the complexity of the sentences is different from that of short sentences. Experiments of the parsing of realworld sentences can properly evaluate the effectiveness and possibility of parsing models for HPSG. 2 Disambiguation models for HPSG Discriminative log-linear models are now becoming a de facto standard for probabilistic disambiguation models for deep parsing (Johnson et al., 1999; Riezler et al., 2002; Geman and Johnson, 2002; Miyao and Tsujii, 2002; Clark and Curran, 2004b; Kaplan et al., 2004). Previous studies on probabilistic models for HPSG (Toutanova and Manning, 2002; Baldridge and Osborne, 2003; Malouf and van Noord, 2004) also adopted log-linear models. HPSG exploits feature structures to represent linguistic constraints. Such constraints are known 83 Proceedings of the 43rd Annual Meeting of the ACL, pages 83–90, c Ann Arbor, June 2005. 2005 Association for Computational Linguistics to introduce inconsistencies in probabilistic models estimated using simple relative frequency (Abney, 1997). Log-linear mo"
P05-1011,P02-1035,0,0.153978,"ection 4). To our knowledge, this work provides the first results of extensive experiments of parsing Penn Treebank with a probabilistic HPSG. The results from the Wall Street Journal are significant because the complexity of the sentences is different from that of short sentences. Experiments of the parsing of realworld sentences can properly evaluate the effectiveness and possibility of parsing models for HPSG. 2 Disambiguation models for HPSG Discriminative log-linear models are now becoming a de facto standard for probabilistic disambiguation models for deep parsing (Johnson et al., 1999; Riezler et al., 2002; Geman and Johnson, 2002; Miyao and Tsujii, 2002; Clark and Curran, 2004b; Kaplan et al., 2004). Previous studies on probabilistic models for HPSG (Toutanova and Manning, 2002; Baldridge and Osborne, 2003; Malouf and van Noord, 2004) also adopted log-linear models. HPSG exploits feature structures to represent linguistic constraints. Such constraints are known 83 Proceedings of the 43rd Annual Meeting of the ACL, pages 83–90, c Ann Arbor, June 2005. 2005 Association for Computational Linguistics to introduce inconsistencies in probabilistic models estimated using simple relative frequency (Ab"
P05-1011,A00-2021,0,0.233003,"ween the parsing cost and the accuracy will be examined experimentally. We have several ways to integrate with the estimated model   . In the experiments, we will empirically compare the following methods in terms of accuracy and estimation time. Filtering only The unigram probability only for filtering. is used Product The probability is defined as the product of and the estimated model . Reference distribution tribution of . is used as a reference disFeature function  is used as a feature function of . This method was shown to be a generalization of the reference distribution method (Johnson and Riezler, 2000). 5 Features Feature functions in the log-linear models are designed to capture the characteristics of      . In this paper, we investigate combinations of the atomic features listed in Table 1. The following combinations are used for representing the characteristics of the binary/unary schema applications. binary   RULE,DIST,COMMA  SPAN  SYM  WORD  POS  LE   SPAN   SYM   WORD   POS   LE  unary  RULE,SYM,WORD,POS,LE  In addition, the following is for expressing the condition of the root node of the parse tree. root  SYM,WORD,POS,LE  86 Figure 4: Example feat"
P05-1011,W04-3201,0,0.037174,"Missing"
P05-1011,P99-1069,0,0.344409,"bility distribution (Section 4). To our knowledge, this work provides the first results of extensive experiments of parsing Penn Treebank with a probabilistic HPSG. The results from the Wall Street Journal are significant because the complexity of the sentences is different from that of short sentences. Experiments of the parsing of realworld sentences can properly evaluate the effectiveness and possibility of parsing models for HPSG. 2 Disambiguation models for HPSG Discriminative log-linear models are now becoming a de facto standard for probabilistic disambiguation models for deep parsing (Johnson et al., 1999; Riezler et al., 2002; Geman and Johnson, 2002; Miyao and Tsujii, 2002; Clark and Curran, 2004b; Kaplan et al., 2004). Previous studies on probabilistic models for HPSG (Toutanova and Manning, 2002; Baldridge and Osborne, 2003; Malouf and van Noord, 2004) also adopted log-linear models. HPSG exploits feature structures to represent linguistic constraints. Such constraints are known 83 Proceedings of the 43rd Annual Meeting of the ACL, pages 83–90, c Ann Arbor, June 2005. 2005 Association for Computational Linguistics to introduce inconsistencies in probabilistic models estimated using simple"
P05-1011,W02-2030,0,0.211656,"on cost, we trained the models using 20 sections of Penn Treebank. A series of experiments empirically evaluated the estimation techniques, and also examined the performance of the disambiguation models on the parsing of real-world sentences. 1 Introduction Head-Driven Phrase Structure Grammar (HPSG) (Pollard and Sag, 1994) has been studied extensively from both linguistic and computational points of view. However, despite research on HPSG processing efficiency (Oepen et al., 2002a), the application of HPSG parsing is still limited to specific domains and short sentences (Oepen et al., 2002b; Toutanova and Manning, 2002). Scaling up HPSG parsing to assess real-world texts is an emerging research field with both theoretical and practical applications. Recently, a wide-coverage grammar and a large treebank have become available for English HPSG (Miyao et al., 2004). A large treebank can be used as training and test data for statistical models. Therefore, we now have the basis for the development and the evaluation of statistical disambiguation models for wide-coverage HPSG parsing. Jun’ichi Tsujii Department of Computer Science University of Tokyo Hongo 7-3-1, Bunkyo-ku, Tokyo, Japan CREST, JST tsujii@is.s.u-to"
P05-1011,N04-1013,0,0.367443,"rsing Penn Treebank with a probabilistic HPSG. The results from the Wall Street Journal are significant because the complexity of the sentences is different from that of short sentences. Experiments of the parsing of realworld sentences can properly evaluate the effectiveness and possibility of parsing models for HPSG. 2 Disambiguation models for HPSG Discriminative log-linear models are now becoming a de facto standard for probabilistic disambiguation models for deep parsing (Johnson et al., 1999; Riezler et al., 2002; Geman and Johnson, 2002; Miyao and Tsujii, 2002; Clark and Curran, 2004b; Kaplan et al., 2004). Previous studies on probabilistic models for HPSG (Toutanova and Manning, 2002; Baldridge and Osborne, 2003; Malouf and van Noord, 2004) also adopted log-linear models. HPSG exploits feature structures to represent linguistic constraints. Such constraints are known 83 Proceedings of the 43rd Annual Meeting of the ACL, pages 83–90, c Ann Arbor, June 2005. 2005 Association for Computational Linguistics to introduce inconsistencies in probabilistic models estimated using simple relative frequency (Abney, 1997). Log-linear models are required for credible probabilistic models and are also benefi"
P05-1011,A00-2018,0,\N,Missing
P05-1011,J03-4003,0,\N,Missing
P06-1059,W04-1221,0,0.154531,"Missing"
P06-1059,M95-1002,0,0.046288,"Missing"
P06-1059,W05-1514,1,0.831367,"Missing"
P06-1059,A97-1029,0,0.0419464,"Missing"
P06-1059,W04-1219,0,0.184969,"Missing"
P06-1059,W04-1217,0,0.0945932,"Missing"
P06-1059,P05-1045,0,0.0374246,"Missing"
P06-1059,W04-1213,0,0.505862,"Missing"
P06-1059,I05-1057,0,0.131398,"Missing"
P06-1128,P05-1022,0,0.00458241,"atabases. For biomedical terms other than genes/gene products, the Unified Medical Language System (UMLS) meta-thesaurus (Lindberg et al., 1993) is a large database that contains various names of biomedical and health-related concepts. Ontology databases provide mappings between textual expressions and entities in the real world. For example, Table 1 indicates that CRP, MGC88244, and PTX1 denote the same gene conceptually. Hence, these resources enable us to canonicalize variations of textual expressions of ontological entities. 2.2 Parsing technologies Recently, state-of-the-art CFG parsers (Charniak and Johnson, 2005) can compute phrase structures of natural sentences at fairly high accuracy. These parsers have been used in various NLP tasks including IE and text mining. In addition, parsers that compute deeper analyses, such as predicate argument structures, have become available for 1018 the processing of real-world sentences (Miyao and Tsujii, 2005). Predicate argument structures are canonicalized representations of sentence meanings, and express the semantic relations of words explicitly. Figure 1 shows an output of an HPSG parser (Miyao and Tsujii, 2005) for the sentence “A normal serum CRP measuremen"
P06-1128,I05-1018,1,0.746453,"aphies of articles, about half of which have abstracts. Research on IE and text mining in biomedical science has focused mainly on MEDLINE. In the present paper, we target all articles indexed in MEDLINE at the end of 2004 (14,785,094 articles). The following sections explain in detail off-/on-line processing for the text retrieval system for MEDLINE. 3.1 Off-line processing: HPSG parsing and term recognition We first parsed all sentences using an HPSG parser (Miyao and Tsujii, 2005) to obtain their predicate argument structures. Because our target is biomedical texts, we re-trained a parser (Hara et al., 2005) with the GENIA treebank (Tateisi et al., 2005), and also applied a bidirectional part-ofspeech tagger (Tsuruoka and Tsujii, 2005) trained with the GENIA treebank as a preprocessor. Because parsing speed is still unrealistic for parsing the entire MEDLINE on a single machine, we used two geographically separated computer clusters having 170 nodes (340 Xeon CPUs). These clusters are separately administered and not dedicated for use in the present study. In order to effectively use such an environment, GXP (Taura, 2004) was used to connect these clusters and distribute the load among them. Our p"
P06-1128,W04-3102,0,0.0566302,"nd: Resources and Tools for Semantic Annotations The proposed system for the retrieval of relational concepts is a product of recent developments in NLP resources and tools. In this section, ontology databases, deep parsers, and search algorithms for structured data are introduced. 2.1 Ontology databases Ontology databases are collections of words and phrases in specific domains. Such databases have been constructed extensively for the systematic management of domain knowledge by organizing textual expressions of ontological entities that are detached from actual sentences. For example, GENA (Koike and Takagi, 2004) is a database of genes and gene products that is semi-automatically collected from well-known databases, including HUGO, OMIM, Genatlas, Locuslink, GDB, MGI, FlyBase, WormBase, Figure 1: An output of HPSG parsing Figure 2: A predicate argument structure CYGD, and SGD. Table 1 shows an example of a GENA entry. “Symbol” and “Name” denote short forms and nomenclatures of genes, respectively. “Species” represents the organism species in which this gene is observed. “Synonym” is a list of synonyms and name variations. “Product” gives a list of products of this gene, such as proteins coded by this"
P06-1128,P05-1011,1,0.829945,"advance with semantic structures and are stored in a structured database. User requests are converted on the fly into patterns of these semantic annotations, and texts are retrieved by matching these patterns with the pre-computed semantic annotations. The accurate retrieval of relational concepts is attained because we can precisely describe relational concepts using semantic annotations. In addition, real-time retrieval is possible because semantic annotations are computed in advance. This framework has been implemented for a text retrieval system for MEDLINE. We first apply a deep parser (Miyao and Tsujii, 2005) and a dictionary-based term recognizer (Tsuruoka and Tsujii, 2004) to MEDLINE and obtain annotations of predicate argument structures and ontological identifiers of genes, gene products, diseases, and events. We then provide a search engine for these annotated sentences. User requests are converted into queries of region algebra (Clarke et al., 1995) extended with variables (Masuda et al., 2006) on these annotations. A search engine for the extended region algebra efficiently finds sentences having semantic annotations that match the input queries. In this paper, we evaluate this system with"
P06-1128,I05-2038,1,0.581796,"abstracts. Research on IE and text mining in biomedical science has focused mainly on MEDLINE. In the present paper, we target all articles indexed in MEDLINE at the end of 2004 (14,785,094 articles). The following sections explain in detail off-/on-line processing for the text retrieval system for MEDLINE. 3.1 Off-line processing: HPSG parsing and term recognition We first parsed all sentences using an HPSG parser (Miyao and Tsujii, 2005) to obtain their predicate argument structures. Because our target is biomedical texts, we re-trained a parser (Hara et al., 2005) with the GENIA treebank (Tateisi et al., 2005), and also applied a bidirectional part-ofspeech tagger (Tsuruoka and Tsujii, 2005) trained with the GENIA treebank as a preprocessor. Because parsing speed is still unrealistic for parsing the entire MEDLINE on a single machine, we used two geographically separated computer clusters having 170 nodes (340 Xeon CPUs). These clusters are separately administered and not dedicated for use in the present study. In order to effectively use such an environment, GXP (Taura, 2004) was used to connect these clusters and distribute the load among them. Our processes were given the lowest priority so that"
P06-1128,H05-1059,1,0.143897,"ainly on MEDLINE. In the present paper, we target all articles indexed in MEDLINE at the end of 2004 (14,785,094 articles). The following sections explain in detail off-/on-line processing for the text retrieval system for MEDLINE. 3.1 Off-line processing: HPSG parsing and term recognition We first parsed all sentences using an HPSG parser (Miyao and Tsujii, 2005) to obtain their predicate argument structures. Because our target is biomedical texts, we re-trained a parser (Hara et al., 2005) with the GENIA treebank (Tateisi et al., 2005), and also applied a bidirectional part-ofspeech tagger (Tsuruoka and Tsujii, 2005) trained with the GENIA treebank as a preprocessor. Because parsing speed is still unrealistic for parsing the entire MEDLINE on a single machine, we used two geographically separated computer clusters having 170 nodes (340 Xeon CPUs). These clusters are separately administered and not dedicated for use in the present study. In order to effectively use such an environment, GXP (Taura, 2004) was used to connect these clusters and distribute the load among them. Our processes were given the lowest priority so that our task would not disturb other users. We finished parsing the entire MEDLINE in"
P06-2091,copestake-flickinger-2000-open,0,0.0261048,"Missing"
P06-2091,hockenmaier-steedman-2002-acquiring,0,0.0768649,"Missing"
P06-2091,C04-1180,0,\N,Missing
P06-2091,H94-1020,0,\N,Missing
P06-2109,J96-1002,0,0.00824673,"th of the compressed sentence; that is, they divide the log probability by the length of the compressed sentence. Turner and Charniak (Turner and Charniak, 2005) added some special rules and applied this method to unsupervised learning to overcome the lack of training data. However their model also has the same problem. McDonald (McDonald, 2006) independently proposed a new machine learning approach. He does not trim input parse trees but uses rich features about syntactic trees and improved performance. 2.2 count (joint(rl , rs )) . count(rs ) Maximum Entropy Model The maximum entropy model (Berger et al., 1996) estimates a probability distribution from training data. The model creates the most “uniform” distribution within the constraints given by users. The distribution with the maximum entropy is considered the most uniform. Given two finite sets of event variables, X and Y, we estimate their joint probability distribution, P (x, y). An output, y (∈ Y), is produced, and Finally, new subtrees grow from new daughter nodes in each expanded node. In Figure 1, (E (G g) (H h)) grows from E. The PCFG scores, Pcfg , of these subtrees are calculated. Then, each probability is assumed to be independent of t"
P06-2109,P05-1022,0,0.0297862,"Missing"
P06-2109,W03-0501,0,0.0742534,"hat our method produced more grammatical and informative compressed sentences than other methods. 1 Introduction In most automatic summarization approaches, text is summarized by extracting sentences from a given document without modifying the sentences themselves. Although these methods have been significantly improved to extract good sentences as summaries, they are not intended to shorten sentences; i.e., the output often has redundant words or phrases. These methods cannot be used to make a shorter sentence from an input sentence or for other applications such as generating headline news (Dorr et al., 2003) or messages for the small screens of mobile devices. We need to compress sentences to obtain short and useful summaries. 850 Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 850–857, c Sydney, July 2006. 2006 Association for Computational Linguistics 2 Background 2.1 A The Noisy-Channel Model for Sentence Compression B D Knight and Marcu proposed a sentence compression method using a noisy-channel model (Knight and Marcu, 2000). This model assumes that a long sentence was originally a short one and that the longer sentence was generated because some unnecessary words"
P06-2109,A00-2023,0,0.0139546,"tree contains rs = (B → D F ). They assume that rs was expanded into rl , and count the node pairs as joint events. The expansion probability of two rules is given by: Pexpand (rl |rs ) = d A C E F G H g h c f B C D F d f c Figure 1: Examples of original and compressed parse trees. subtrees: P (l|s) = Y Pexpand (rl |rs ) · (rl ,rs )∈R Y Pcfg (r), r∈R0 where R is the set of rule pairs, and R 0 is the set of generation rules in new subtrees. To compress an input sentence, they create a tree with the highest score of all possible trees. They pack all possible trees in a shared-forest structure (Langkilde, 2000). The forest structure is represented by an AND-OR tree, and it contains many tree structures. The forest representation saves memory and makes calculation faster because the trees share sub structures, and this can reduce the total number of calculations. They normalize each log probability using the length of the compressed sentence; that is, they divide the log probability by the length of the compressed sentence. Turner and Charniak (Turner and Charniak, 2005) added some special rules and applied this method to unsupervised learning to overcome the lack of training data. However their mode"
P06-2109,W04-1013,0,0.0103954,"om the tree of Figure 2, (S (S · · · ) (, , ) (NP I) (VP said) (. .)), a new tree, (S (S · · · ) (. .)), is extracted. However, the rule (S → S .) is ungrammatical. 4 Experiment 4.1 Evaluation Method We evaluated each sentence compression method using word F -measures, bigram F -measures, and B LEU scores (Papineni et al., 2002). B LEU scores are usually used for evaluating machine translation quality. A B LEU score is defined as the weighted geometric average of n-gram precisions with length penalties. We used from unigram to 4-gram precisions and uniform weights for the B LEU scores. ROUGE (Lin, 2004) is a set of recall-based criteria that is mainly used for evaluating summarization tasks. ROUGE -N uses average N-gram recall, and ROUGE -1 is word recall. ROUGE -L uses the length of the longest common subsequence (LCS) of the original and summarized sentences. In our model, the length of the LCS is equal to the number of common words, and ROUGE -L is equal to the unigram F -measure because words are not rearranged. ROUGE -L and ROUGE -1 are supposed to be appropriate for the headline gener853 ation task (Lin, 2004). This is not our task, but it is the most similar task in his paper. We also"
P06-2109,E06-1038,0,0.118005,"y an AND-OR tree, and it contains many tree structures. The forest representation saves memory and makes calculation faster because the trees share sub structures, and this can reduce the total number of calculations. They normalize each log probability using the length of the compressed sentence; that is, they divide the log probability by the length of the compressed sentence. Turner and Charniak (Turner and Charniak, 2005) added some special rules and applied this method to unsupervised learning to overcome the lack of training data. However their model also has the same problem. McDonald (McDonald, 2006) independently proposed a new machine learning approach. He does not trim input parse trees but uses rich features about syntactic trees and improved performance. 2.2 count (joint(rl , rs )) . count(rs ) Maximum Entropy Model The maximum entropy model (Berger et al., 1996) estimates a probability distribution from training data. The model creates the most “uniform” distribution within the constraints given by users. The distribution with the maximum entropy is considered the most uniform. Given two finite sets of event variables, X and Y, we estimate their joint probability distribution, P (x,"
P06-2109,P02-1040,0,0.0729168,"VP VP . never think so . never think so Figure 2: Example of parse tree pair that cannot be matched. A A B D d C E F G H g h c f B C D E d G c g Figure 3: Example of bottom-up method. Note that this “tree” is not guaranteed to be a grammatical “parse tree” by the CFG grammar. For example, from the tree of Figure 2, (S (S · · · ) (, , ) (NP I) (VP said) (. .)), a new tree, (S (S · · · ) (. .)), is extracted. However, the rule (S → S .) is ungrammatical. 4 Experiment 4.1 Evaluation Method We evaluated each sentence compression method using word F -measures, bigram F -measures, and B LEU scores (Papineni et al., 2002). B LEU scores are usually used for evaluating machine translation quality. A B LEU score is defined as the weighted geometric average of n-gram precisions with length penalties. We used from unigram to 4-gram precisions and uniform weights for the B LEU scores. ROUGE (Lin, 2004) is a set of recall-based criteria that is mainly used for evaluating summarization tasks. ROUGE -N uses average N-gram recall, and ROUGE -1 is word recall. ROUGE -L uses the length of the longest common subsequence (LCS) of the original and summarized sentences. In our model, the length of the LCS is equal to the numb"
P06-2109,P05-1036,0,0.550148,"n input sentence, they create a tree with the highest score of all possible trees. They pack all possible trees in a shared-forest structure (Langkilde, 2000). The forest structure is represented by an AND-OR tree, and it contains many tree structures. The forest representation saves memory and makes calculation faster because the trees share sub structures, and this can reduce the total number of calculations. They normalize each log probability using the length of the compressed sentence; that is, they divide the log probability by the length of the compressed sentence. Turner and Charniak (Turner and Charniak, 2005) added some special rules and applied this method to unsupervised learning to overcome the lack of training data. However their model also has the same problem. McDonald (McDonald, 2006) independently proposed a new machine learning approach. He does not trim input parse trees but uses rich features about syntactic trees and improved performance. 2.2 count (joint(rl , rs )) . count(rs ) Maximum Entropy Model The maximum entropy model (Berger et al., 1996) estimates a probability distribution from training data. The model creates the most “uniform” distribution within the constraints given by u"
P06-2109,W04-1015,0,0.0632249,"Missing"
P06-4005,P05-1011,1,0.777319,"Missing"
P06-4005,I05-1018,1,0.813084,"eather conditions forced them to scrub Monday’s scheduled return.” 3 MEDIE: a search engine for MEDLINE Figure 2 shows the top page of the MEDIE. MEDIE is an intelligent search engine for the accurate retrieval of relational concepts from MEDLINE 2 (Miyao et al., 2006). Prior to retrieval, all sentences are annotated with predicate argument structures and ontological identifiers by applying Enju and a term recognizer. 3.1 Automatically Annotated Corpus First, we applied a POS analyzer and then Enju. The POS analyzer and HPSG parser are trained by using the GENIA corpus (Tsuruoka et al., 2005; Hara et al., 2005), which comprises around 2,000 MEDLINE abstracts annotated with POS and Penn Treebank style syntactic parse trees (Tateisi et al., 2005). The HPSG parser generates parse trees in a stand-off format that can be converted to XML by combining it with the original text. We also annotated technical terms of genes and diseases in our developed corpus. Technical terms are annotated simply by exact matching of dictio2 Functions of MEDIE 4 Info-PubMed: a GUI-based MEDLINE search tool Info-PubMed is a MEDLINE search tool with GUI, helping users to find information about biomedical entities such as genes"
P06-4005,W05-1511,1,0.867094,"Missing"
P06-4005,I05-2038,1,0.900999,"e predicate type (e.g., adjective, intransitive verb), wh is the head word of the predicate, a is the argument label (MOD, ARG1, ..., ARG4), and wa is the head word of the argument. Precision/recall is the ratio of tuples correctly identified by the parser. The lexicon of the grammar was extracted from Sections 02-21 of Penn Treebank (39,832 sentences). In the table, ‘HPSG-PTB’ means that the statistical model was trained on Penn Treebank. ‘HPSG-GENIA’ means that the statistical model was trained on both Penn Treebank and GENIA treebank as described in (Hara et al., 2005). The GENIA treebank (Tateisi et al., 2005) consists of 500 abstracts (4,446 sentences) extracted from MEDLINE. Figure 1 shows a part of the parse tree and feaRecently, biomedical researchers have been facing the vast repository of research papers, e.g. MEDLINE. These researchers are eager to search biomedical correlations such as protein-protein or gene-disease associations. The use of natural language processing technology is expected to reduce their burden, and various attempts of information extraction using NLP has been being made (Blaschke and Valencia, 2002; Hao et al., 2005; Chun et al., 2006). However, the framework of traditi"
P06-4005,W04-3102,0,0.0153543,"of Informatics, Kogakuin University ¶ Information Technology Center, University of Tokyo † 1 http://www-tsujii.is.s.u-tokyo.ac.jp/enju/ 17 Proceedings of the COLING/ACL 2006 Interactive Presentation Sessions, pages 17–20, c Sydney, July 2006. 2006 Association for Computational Linguistics nary entries and the terms separated by space, tab, period, comma, hat, colon, semi-colon, brackets, square brackets and slash in MEDLINE. The entire dictionary was generated by applying the automatic generation method of name variations (Tsuruoka and Tsujii, 2004) to the GENA dictionary for the gene names (Koike and Takagi, 2004) and the UMLS (Unified Medical Language System) meta-thesaurus for the disease names (Lindberg et al., 1993). It was generated by applying the name-variation generation method, and we obtained 4,467,855 entries of a gene and disease dictionary. 3.2 MEDIE provides three types of search, semantic search, keyword search, GCL search. GCL search provides us the most fundamental and powerful functions in which users can specify the boolean relations, linear order relation and structural relations with variables. Trained users can enjoy all functions in MEDIE by the GCL search, but it is not easy for"
P06-4005,P06-1128,1,\N,Missing
P07-1010,H05-1027,0,\N,Missing
P07-1010,P05-1011,1,\N,Missing
P07-1010,P05-1044,0,\N,Missing
P07-1010,P03-1004,0,\N,Missing
P07-1079,J99-2004,0,0.09211,"minutes, since two dependency parsers are used sequentially. 5 Related work There are other approaches that combine shallow processing with deep parsing (Crysmann et al., 2002; Frank et al., 2003; Daum et al., 2003) to improve parsing efficiency. Typically, shallow parsing is used to create robust minimal recursion semantics, which are used as constraints to limit ambiguity during parsing. Our approach, in contrast, uses syntactic dependencies to achieve a significant improvement in the accuracy of wide-coverage HPSG parsing. Additionally, our approach is in many ways similar to supertagging (Bangalore and Joshi, 1999), which uses sequence labeling techniques as an efficient way to pre-compute parsing constraints (specifically, the assignment of lexical entries to input words). 6 Conclusion We have presented a novel framework for taking advantage of the strengths of a shallow parsing approach and a deep parsing approach. We have shown that by constraining the application of rules in HPSG parsing according to results from a dependency parser, we can significantly improve the accuracy of deep parsing by using shallow syntactic analyses. To illustrate how this framework allows for improvements in the accuracy"
P07-1079,W06-2920,0,0.0263757,"uistically sophisticated formalism. In addition, improvements in dependency parsing accuracy are converted directly into improvements in HPSG parsing accuracy. From the point of view of dependency parsing, the application of HPSG rules to structures generated by a surface dependency model provides a principled and linguistically motivated way to identify deep syntactic phenomena, such as long-distance dependencies, raising and control. Several efficient, accurate and robust approaches to data-driven dependency parsing have been proposed recently (Nivre and Scholz, 2004; McDonald et al., 2005; Buchholz and Marsi, 2006) for syntactic analysis of natural language using bilexical dependency relations (Eisner, 1996). Much of the appeal of these approaches is tied to the use of a simple formalism, which allows for the use of efficient parsing algorithms, as well as straightforward ways to train discriminative models to perform disambiguation. At the same time, there is growing interest in parsing with more sophisticated lexicalized grammar formalisms, such as Lexical Functional Grammar We begin by describing our dependency and (LFG) (Bresnan, 1982), Lexicalized Tree Adjoin- HPSG parsing approaches in section 2."
P07-1079,E03-1052,0,0.0224732,"Missing"
P07-1079,C96-1058,0,0.019801,"directly into improvements in HPSG parsing accuracy. From the point of view of dependency parsing, the application of HPSG rules to structures generated by a surface dependency model provides a principled and linguistically motivated way to identify deep syntactic phenomena, such as long-distance dependencies, raising and control. Several efficient, accurate and robust approaches to data-driven dependency parsing have been proposed recently (Nivre and Scholz, 2004; McDonald et al., 2005; Buchholz and Marsi, 2006) for syntactic analysis of natural language using bilexical dependency relations (Eisner, 1996). Much of the appeal of these approaches is tied to the use of a simple formalism, which allows for the use of efficient parsing algorithms, as well as straightforward ways to train discriminative models to perform disambiguation. At the same time, there is growing interest in parsing with more sophisticated lexicalized grammar formalisms, such as Lexical Functional Grammar We begin by describing our dependency and (LFG) (Bresnan, 1982), Lexicalized Tree Adjoin- HPSG parsing approaches in section 2. In section ing Grammar (LTAG) (Schabes et al., 1988), Head- 3, we present our framework for HPS"
P07-1079,P03-1014,0,0.0366287,"Missing"
P07-1079,P06-1088,0,0.0377423,"Missing"
P07-1079,W02-2018,0,0.00702209,"s to lexical/phrasal structures, where L = hl1 , . . . , ln i are lexical entries and 625 p(li |W ) is the supertagging probability, i.e., the probability of assignining the lexical entry li to wi (Ninomiya et al., 2006). The probability p(T |L, W ) is a maximum entropy model on HPSG parse trees, where Z is a normalization factor, and feature functions fi (T ) represent syntactic characteristics, such as head words, lengths of phrases, and applied schemas. Given the HPSG treebank as training data, the model parameters λi are estimated so as to maximize the log-likelihood of the training data (Malouf, 2002). 3 HPSG parsing with dependency constraints While a number of fairly straightforward models can be applied successfully to dependency parsing, designing and training HPSG parsing models has been regarded as a significantly more complex task. Although it seems intuitive that a more sophisticated linguistic formalism should be more difficult to parameterize properly, we argue that the difference in complexity between HPSG and dependency structures can be seen as incremental, and that the use of accurate and efficient techniques to determine the surface dependency structure of a sentence provide"
P07-1079,J93-2004,0,0.0326234,"encies that roughly correspond to deep syntax. The second step is to perform HPSG parsing, as described in section 2.2, but using the shallow dependency tree to constrain the application of HPSG rules. We now discuss these two steps in more detail. 3.1 Determining shallow dependencies in HPSG structures using dependency parsing In order to apply a data-driven dependency approach to the task of identifying the shallow dependency tree in HPSG structures, we first need a corpus of such dependency trees to serve as training data. We created a dependency training corpus based on the Penn Treebank (Marcus et al., 1993), or more specifically on the HPSG Treebank generated from the Penn Treebank (see section 2.2). For each HPSG structure in the HPSG Treebank, a dependency tree is extracted in two steps. First, the HPSG tree is converted into a CFG-style tree, simply by removing long-distance dependency links between nodes. A dependency tree is then extracted from the resulting lexicalized CFG-style tree, as is commonly done for converting constituent trees into dependency trees after the application of a headpercolation table (Collins, 1999). Once a dependency training corpus is available, it is used to train"
P07-1079,H05-1066,0,0.208417,"a more complex and linguistically sophisticated formalism. In addition, improvements in dependency parsing accuracy are converted directly into improvements in HPSG parsing accuracy. From the point of view of dependency parsing, the application of HPSG rules to structures generated by a surface dependency model provides a principled and linguistically motivated way to identify deep syntactic phenomena, such as long-distance dependencies, raising and control. Several efficient, accurate and robust approaches to data-driven dependency parsing have been proposed recently (Nivre and Scholz, 2004; McDonald et al., 2005; Buchholz and Marsi, 2006) for syntactic analysis of natural language using bilexical dependency relations (Eisner, 1996). Much of the appeal of these approaches is tied to the use of a simple formalism, which allows for the use of efficient parsing algorithms, as well as straightforward ways to train discriminative models to perform disambiguation. At the same time, there is growing interest in parsing with more sophisticated lexicalized grammar formalisms, such as Lexical Functional Grammar We begin by describing our dependency and (LFG) (Bresnan, 1982), Lexicalized Tree Adjoin- HPSG parsin"
P07-1079,P05-1011,1,0.918834,"accurate, but also efficient. The deterministic shift/reduce classifier-based dependency parsing approach (Nivre and Scholz, 2004) has been shown to offer state-of-the-art accuracy (Nivre et al., 2006) with high efficiency due to a greedy search strategy. Our approach is based on Nivre and Scholz’s approach, using support vector machines for classification of shift/reduce actions. 2.2 Wide-coverage HPSG parsing Figure 2: Extracting HPSG lexical entries from the Penn Treebank we finally obtain an HPSG parse tree that covers the entire sentence. In this paper, we use an HPSG parser developed by Miyao and Tsujii (2005). This parser has a widecoverage HPSG lexicon which is extracted from the Penn Treebank. Figure 2 illustrates their method for extraction of HPSG lexical entries. First, given a parse tree from the Penn Treebank (top), HPSGstyle constraints are added and an HPSG-style parse tree is obtained (middle). Lexical entries are then extracted from the terminal nodes of the HPSG parse tree (bottom). This way, in addition to a widecoverage lexicon, we also obtain an HPSG treebank, which can be used as training data for disambiguation models. The disambiguation model of this parser is based on a maximum"
P07-1079,W06-1619,1,0.853739,"xical entries express subcategorization frames and predicate argument structures. Parsing proceeds by app(T |W ) = p(T |L, W )p(L|W ) plying schemas to lexical entries. In this example, ! X Y the Head-Complement Schema is applied to the lex1 = exp λ f (T ) p(lj |W ), i i ical entries of “tried” and “running”. We then obtain Z i j a phrasal structure for “tried running”. By repeatedly applying schemas to lexical/phrasal structures, where L = hl1 , . . . , ln i are lexical entries and 625 p(li |W ) is the supertagging probability, i.e., the probability of assignining the lexical entry li to wi (Ninomiya et al., 2006). The probability p(T |L, W ) is a maximum entropy model on HPSG parse trees, where Z is a normalization factor, and feature functions fi (T ) represent syntactic characteristics, such as head words, lengths of phrases, and applied schemas. Given the HPSG treebank as training data, the model parameters λi are estimated so as to maximize the log-likelihood of the training data (Malouf, 2002). 3 HPSG parsing with dependency constraints While a number of fairly straightforward models can be applied successfully to dependency parsing, designing and training HPSG parsing models has been regarded as"
P07-1079,C04-1010,0,0.410319,"dapting these models to a more complex and linguistically sophisticated formalism. In addition, improvements in dependency parsing accuracy are converted directly into improvements in HPSG parsing accuracy. From the point of view of dependency parsing, the application of HPSG rules to structures generated by a surface dependency model provides a principled and linguistically motivated way to identify deep syntactic phenomena, such as long-distance dependencies, raising and control. Several efficient, accurate and robust approaches to data-driven dependency parsing have been proposed recently (Nivre and Scholz, 2004; McDonald et al., 2005; Buchholz and Marsi, 2006) for syntactic analysis of natural language using bilexical dependency relations (Eisner, 1996). Much of the appeal of these approaches is tied to the use of a simple formalism, which allows for the use of efficient parsing algorithms, as well as straightforward ways to train discriminative models to perform disambiguation. At the same time, there is growing interest in parsing with more sophisticated lexicalized grammar formalisms, such as Lexical Functional Grammar We begin by describing our dependency and (LFG) (Bresnan, 1982), Lexicalized T"
P07-1079,W06-2933,0,0.00677943,"Prague, Czech Republic, June 2007. 2007 Association for Computational Linguistics Figure 1: HPSG parsing evaluate this framework empirically. Sections 5 and 6 discuss related work and conclusions. 2 2.1 Fast dependency parsing and wide-coverage HPSG parsing Data-driven dependency parsing Because we use dependency parsing as a step in deep parsing, it is important that we choose a parsing approach that is not only accurate, but also efficient. The deterministic shift/reduce classifier-based dependency parsing approach (Nivre and Scholz, 2004) has been shown to offer state-of-the-art accuracy (Nivre et al., 2006) with high efficiency due to a greedy search strategy. Our approach is based on Nivre and Scholz’s approach, using support vector machines for classification of shift/reduce actions. 2.2 Wide-coverage HPSG parsing Figure 2: Extracting HPSG lexical entries from the Penn Treebank we finally obtain an HPSG parse tree that covers the entire sentence. In this paper, we use an HPSG parser developed by Miyao and Tsujii (2005). This parser has a widecoverage HPSG lexicon which is extracted from the Penn Treebank. Figure 2 illustrates their method for extraction of HPSG lexical entries. First, given a"
P07-1079,W05-1513,1,0.770179,"onverted into a CFG-style tree, simply by removing long-distance dependency links between nodes. A dependency tree is then extracted from the resulting lexicalized CFG-style tree, as is commonly done for converting constituent trees into dependency trees after the application of a headpercolation table (Collins, 1999). Once a dependency training corpus is available, it is used to train a dependency parser as described in section 2.1. This is done by training a classifier to determine parser actions based on local features that represent the current state of the parser (Nivre and Scholz, 2004; Sagae and Lavie, 2005). Training data for the classifier is obtained by applying the parsing algorithm over the training sentences (for which the correct dependency structures are known) and recording the appropriate parser actions that result in the formation of the correct dependency trees, coupled with the features that represent the state of the parser mentioned in section 2.1. An evaluation of the resulting dependency parser and its efficacy in aiding HPSG parsing is presented in section 4. 3.2 Parsing with dependency constraints Given a set of dependencies, the bottom-up process of HPSG parsing can be constra"
P07-1079,N06-2033,1,0.641306,"ser combination has been shown to be a powerful way to obtain very high accuracy in dependency parsing (Sagae and Lavie, 2006). Using dependency constraints allows us to improve HPSG parsing accuracy simply by using an existing parser combination approach. As a first step, we train two additional parsers with the dependencies extracted from the HPSG Treebank. The first uses the same shiftreduce framework described in section 2.1, but it process the input from right to left (RL). This has been found to work well in previous work on depenˇ dency parser combination (Zeman and Zabokrtsk´ y, 2005; Sagae and Lavie, 2006). The second parser is MSTParser, the large-margin maximum spanning tree parser described in (McDonald et al., 2005)3 . We examine the use of two combination schemes: one using two parsers, and one using three parsers. The first combination approach is to keep only dependencies for which there is agreement between the two parsers. In other words, dependencies that are proposed by one parser but not the other are simply discarded. Using the left-to-right shift-reduce parser and MSTParser, we find that this results in very high precision of surface dependencies on the development data. In the se"
P07-1079,C88-2121,0,0.104986,"ral language using bilexical dependency relations (Eisner, 1996). Much of the appeal of these approaches is tied to the use of a simple formalism, which allows for the use of efficient parsing algorithms, as well as straightforward ways to train discriminative models to perform disambiguation. At the same time, there is growing interest in parsing with more sophisticated lexicalized grammar formalisms, such as Lexical Functional Grammar We begin by describing our dependency and (LFG) (Bresnan, 1982), Lexicalized Tree Adjoin- HPSG parsing approaches in section 2. In section ing Grammar (LTAG) (Schabes et al., 1988), Head- 3, we present our framework for HPSG parsing with driven Phrase Structure Grammar (HPSG) (Pollard shallow dependency constraints, and in section 4 we 624 Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 624–631, c Prague, Czech Republic, June 2007. 2007 Association for Computational Linguistics Figure 1: HPSG parsing evaluate this framework empirically. Sections 5 and 6 discuss related work and conclusions. 2 2.1 Fast dependency parsing and wide-coverage HPSG parsing Data-driven dependency parsing Because we use dependency parsing as a step"
P07-1079,W05-1518,0,0.0398449,"Missing"
P07-1079,J96-1002,0,\N,Missing
P07-1079,P02-1056,0,\N,Missing
P08-1006,P06-2006,0,0.0284334,"on, our system cannot be compared directly to the results reported by Erkan et al. (2007) and Katrenko and Adriaans (2006), while Sætre et al. (2007) presented better results than theirs in the same evaluation criterion. 5 Related Work Though the evaluation of syntactic parsers has been a major concern in the parsing community, and a couple of works have recently presented the comparison of parsers based on different frameworks, their methods were based on the comparison of the parsing accuracy in terms of a certain intermediate parse representation (Ringger et al., 2004; Kaplan et al., 2004; Briscoe and Carroll, 2006; Clark and Curran, 2007; Miyao et al., 2007; Clegg and Shepherd, 2007; Pyysalo et al., 2007b; Pyysalo et al., 2007a; Sagae et al., 2008). Such evaluation requires gold standard data in an intermediate representation. However, it has been argued that the conversion of parsing results into an intermediate representation is difficult and far from perfect. The relationship between parsing accuracy and task accuracy has been obscure for many years. Quirk and Corston-Oliver (2006) investigated the impact of parsing accuracy on statistical MT. However, this work was only concerned with a single depe"
P08-1006,P04-1056,0,0.02889,"gher dependency accuracy when trained only with GENIA. We therefore only input GENIA as the training data for the retraining of dependency parsers. For the other parsers, we input the concatenation of WSJ and GENIA for the retraining, while the reranker of RERANK was not retrained due to its cost. Since the parsers other than NO-RERANK and RERANK require an external POS tagger, a WSJ-trained POS tagger is used with WSJtrained parsers, and geniatagger (Tsuruoka et al., 2005) is used with GENIA-retrained parsers. 4 Experiments 4.1 Experiment settings In the following experiments, we used AImed (Bunescu and Mooney, 2004), which is a popular corpus for the evaluation of PPI extraction systems. The corpus consists of 225 biomedical paper abstracts (1970 sentences), which are sentence-split, tokenized, and annotated with proteins and PPIs. We use gold protein annotations given in the corpus. Multi-word protein names are concatenated and treated as single words. The accuracy is measured by abstract-wise 10-fold cross validation and the one-answer-per-occurrence criterion (Giuliano et al., 2006). A threshold for SVMs is moved to adjust the balance of precision and recall, and the maximum f-scores are reported for"
P08-1006,P05-1022,0,0.0663263,"parsing) using five different parse representations. We run a PPI system with several combinations of parser and parse representation, and examine their impact on PPI identification accuracy. Our experiments show that the levels of accuracy obtained with these different parsers are similar, but that accuracy improvements vary when the parsers are retrained with domain-specific data. 1 Introduction Parsing technologies have improved considerably in the past few years, and high-performance syntactic parsers are no longer limited to PCFG-based frameworks (Charniak, 2000; Klein and Manning, 2003; Charniak and Johnson, 2005; Petrov and Klein, 2007), but also include dependency parsers (McDonald and Pereira, 2006; Nivre and Nilsson, 2005; Sagae and Tsujii, 2007) and deep parsers (Kaplan et al., 2004; Clark and Curran, 2004; Miyao and Tsujii, 2008). However, efforts to perform extensive comparisons of syntactic parsers based on different frameworks have been limited. The most popular method for parser comparison involves the direct measurement of the parser output accuracy in terms of metrics such as bracketing precision and recall, or dependency accuracy. This assumes the existence of a gold-standard test corpus,"
P08-1006,P04-1014,0,0.498927,"s show that the levels of accuracy obtained with these different parsers are similar, but that accuracy improvements vary when the parsers are retrained with domain-specific data. 1 Introduction Parsing technologies have improved considerably in the past few years, and high-performance syntactic parsers are no longer limited to PCFG-based frameworks (Charniak, 2000; Klein and Manning, 2003; Charniak and Johnson, 2005; Petrov and Klein, 2007), but also include dependency parsers (McDonald and Pereira, 2006; Nivre and Nilsson, 2005; Sagae and Tsujii, 2007) and deep parsers (Kaplan et al., 2004; Clark and Curran, 2004; Miyao and Tsujii, 2008). However, efforts to perform extensive comparisons of syntactic parsers based on different frameworks have been limited. The most popular method for parser comparison involves the direct measurement of the parser output accuracy in terms of metrics such as bracketing precision and recall, or dependency accuracy. This assumes the existence of a gold-standard test corpus, such as the Penn Treebank (Marcus et al., 1994). It is difficult to apply this method to compare parsers based on different frameworks, because parse representations are often framework-specific and di"
P08-1006,P07-1032,0,0.0427625,"mpared directly to the results reported by Erkan et al. (2007) and Katrenko and Adriaans (2006), while Sætre et al. (2007) presented better results than theirs in the same evaluation criterion. 5 Related Work Though the evaluation of syntactic parsers has been a major concern in the parsing community, and a couple of works have recently presented the comparison of parsers based on different frameworks, their methods were based on the comparison of the parsing accuracy in terms of a certain intermediate parse representation (Ringger et al., 2004; Kaplan et al., 2004; Briscoe and Carroll, 2006; Clark and Curran, 2007; Miyao et al., 2007; Clegg and Shepherd, 2007; Pyysalo et al., 2007b; Pyysalo et al., 2007a; Sagae et al., 2008). Such evaluation requires gold standard data in an intermediate representation. However, it has been argued that the conversion of parsing results into an intermediate representation is difficult and far from perfect. The relationship between parsing accuracy and task accuracy has been obscure for many years. Quirk and Corston-Oliver (2006) investigated the impact of parsing accuracy on statistical MT. However, this work was only concerned with a single dependency parser, and did n"
P08-1006,P02-1034,0,0.0398489,"ng classifiers (Katrenko and Adriaans, 2006; Erkan et al., 2007; Sætre et al., 2007). For the protein pair IL-8 and CXCR1 in Figure 4, a dependency parser outputs a dependency tree shown in Figure 1. From this dependency tree, we can extract a dependency path shown in Figure 5, which appears to be a strong clue in knowing that these proteins are mentioned as interacting. (dep_path (SBJ (ENTITY1 recognizes)) (rOBJ (recognizes ENTITY2))) Figure 6: Tree representation of a dependency path We follow the PPI extraction method of Sætre et al. (2007), which is based on SVMs with SubSet Tree Kernels (Collins and Duffy, 2002; Moschitti, 2006), while using different parsers and parse representations. Two types of features are incorporated in the classifier. The first is bag-of-words features, which are regarded as a strong baseline for IE systems. Lemmas of words before, between and after the pair of target proteins are included, and the linear kernel is used for these features. These features are commonly included in all of the models. Filtering by a stop-word list is not applied because this setting made the scores higher than Sætre et al. (2007)’s setting. The other type of feature is syntactic features. For de"
P08-1006,P97-1003,0,0.0850946,"ank. Penn Treebank-style phrase structure trees without function tags and empty nodes. This is the default output format for phrase structure parsers. We also create this representation by converting ENJU’s output by tree structure matching, although this conversion is not perfect because forms of PTB and ENJU’s output are not necessarily compatible. PTB Dependency trees of syntactic heads (Figure 8). This representation is obtained by converting PTB trees. We first determine lexical heads of nonterminal nodes by using Bikel’s implementation of Collins’ head detection algorithm9 (Bikel, 2004; Collins, 1997). We then convert lexicalized trees into dependencies between lexical heads. HD The Stanford dependency format (Figure 9). This format was originally proposed for extracting dependency relations useful for practical applications (de Marneffe et al., 2006). A program to convert PTB is attached to the Stanford parser. Although the concept looks similar to CoNLL, this representaSD 8 http://nlp.cs.lth.se/pennconverter/ http://www.cis.upenn.edu/˜dbikel/software. html 9 Figure 9: Stanford dependencies tion does not necessarily form a tree structure, and is designed to express more fine-grained relat"
P08-1006,de-marneffe-etal-2006-generating,0,0.148922,"Missing"
P08-1006,C96-1058,0,0.0205152,"for the sentence “IL-8 recognizes and activates CXCR1.” An advantage of dependency parsing is that dependency trees are a reasonable approximation of the semantics of sentences, and are readily usable in NLP applications. Furthermore, the efficiency of popular approaches to dependency parsing compare favorable with those of phrase structure parsing or deep parsing. While a number of approaches have been proposed for dependency parsing, this paper focuses on two typical methods. McDonald and Pereira (2006)’s dependency 1 parser, based on the Eisner algorithm for projective dependency parsing (Eisner, 1996) with the secondorder factorization. MST 1 http://sourceforge.net/projects/mstparser 47 Figure 2: Penn Treebank-style phrase structure tree Sagae and Tsujii (2007)’s dependency parser,2 based on a probabilistic shift-reduce algorithm extended by the pseudo-projective parsing technique (Nivre and Nilsson, 2005). KSDEP 2.2 Phrase structure parsing Owing largely to the Penn Treebank, the mainstream of data-driven parsing research has been dedicated to the phrase structure parsing. These parsers output Penn Treebank-style phrase structure trees, although function tags and empty categories are stri"
P08-1006,D07-1024,0,0.532677,"e for NLP researchers in choosing an appropriate parser for their purposes. In this paper, we present a comparative evaluation of syntactic parsers and their output representations based on different frameworks: dependency parsing, phrase structure parsing, and deep parsing. Our approach to parser evaluation is to measure accuracy improvement in the task of identifying protein-protein interaction (PPI) information in biomedical papers, by incorporating the output of different parsers as statistical features in a machine learning classifier (Yakushiji et al., 2005; Katrenko and Adriaans, 2006; Erkan et al., 2007; Sætre et al., 2007). PPI identification is a reasonable task for parser evaluation, because it is a typical information extraction (IE) application, and because recent studies have shown the effectiveness of syntactic parsing in this task. Since our evaluation method is applicable to any parser output, and is grounded in a real application, it allows for a fair comparison of syntactic parsers based on different frameworks. Parser evaluation in PPI extraction also illuminates domain portability. Most state-of-the-art parsers for English were trained with the Wall Street Journal (WSJ) portion"
P08-1006,W01-0521,0,0.0556271,"parsing in this task. Since our evaluation method is applicable to any parser output, and is grounded in a real application, it allows for a fair comparison of syntactic parsers based on different frameworks. Parser evaluation in PPI extraction also illuminates domain portability. Most state-of-the-art parsers for English were trained with the Wall Street Journal (WSJ) portion of the Penn Treebank, and high accuracy has been reported for WSJ text; however, these parsers rely on lexical information to attain high accuracy, and it has been criticized that these parsers may overfit to WSJ text (Gildea, 2001; 46 Proceedings of ACL-08: HLT, pages 46–54, c Columbus, Ohio, USA, June 2008. 2008 Association for Computational Linguistics Klein and Manning, 2003). Another issue for discussion is the portability of training methods. When training data in the target domain is available, as is the case with the GENIA Treebank (Kim et al., 2003) for biomedical papers, a parser can be retrained to adapt to the target domain, and larger accuracy improvements are expected, if the training method is sufficiently general. We will examine these two aspects of domain portability by comparing the original parsers w"
P08-1006,E06-1051,0,0.19788,"used with GENIA-retrained parsers. 4 Experiments 4.1 Experiment settings In the following experiments, we used AImed (Bunescu and Mooney, 2004), which is a popular corpus for the evaluation of PPI extraction systems. The corpus consists of 225 biomedical paper abstracts (1970 sentences), which are sentence-split, tokenized, and annotated with proteins and PPIs. We use gold protein annotations given in the corpus. Multi-word protein names are concatenated and treated as single words. The accuracy is measured by abstract-wise 10-fold cross validation and the one-answer-per-occurrence criterion (Giuliano et al., 2006). A threshold for SVMs is moved to adjust the balance of precision and recall, and the maximum f-scores are reported for each setting. 4.2 Comparison of accuracy improvements Tables 1 and 2 show the accuracy obtained by using the output of each parser in each parse representation. The row “baseline” indicates the accuracy obtained with bag-of-words features. Table 3 shows the time for parsing the entire AImed corpus, and Table 4 shows the time required for 10-fold cross validation with GENIA-retrained parsers. When using the original WSJ-trained parsers (Table 1), all parsers achieved almost t"
P08-1006,W07-2202,1,0.375843,"en used in parser evaluation and applications. PAS is a graph structure that represents syntactic/semantic relations among words (Figure 3). The concept is therefore similar to CoNLL dependencies, though PAS expresses deeper relations, and may include reentrant structures. In this work, we chose the two versions of the Enju parser (Miyao and Tsujii, 2008). The HPSG parser that consists of an HPSG grammar extracted from the Penn Treebank, and a maximum entropy model trained with an HPSG treebank derived from the Penn Treebank.7 ENJU The HPSG parser adapted to biomedical texts, by the method of Hara et al. (2007). Because this parser is trained with both WSJ and GENIA, we compare it parsers that are retrained with GENIA (see section 3.3). ENJU-GENIA 3 Evaluation Methodology In our approach to parser evaluation, we measure the accuracy of a PPI extraction system, in which 6 http://nlp.stanford.edu/software/lex-parser. shtml 7 http://www-tsujii.is.s.u-tokyo.ac.jp/enju/ 48 the parser output is embedded as statistical features of a machine learning classifier. We run a classifier with features of every possible combination of a parser and a parse representation, by applying conversions between representat"
P08-1006,W07-2416,0,0.333541,"ntences, and are readily usable in NLP applications. Furthermore, the efficiency of popular approaches to dependency parsing compare favorable with those of phrase structure parsing or deep parsing. While a number of approaches have been proposed for dependency parsing, this paper focuses on two typical methods. McDonald and Pereira (2006)’s dependency 1 parser, based on the Eisner algorithm for projective dependency parsing (Eisner, 1996) with the secondorder factorization. MST 1 http://sourceforge.net/projects/mstparser 47 Figure 2: Penn Treebank-style phrase structure tree Sagae and Tsujii (2007)’s dependency parser,2 based on a probabilistic shift-reduce algorithm extended by the pseudo-projective parsing technique (Nivre and Nilsson, 2005). KSDEP 2.2 Phrase structure parsing Owing largely to the Penn Treebank, the mainstream of data-driven parsing research has been dedicated to the phrase structure parsing. These parsers output Penn Treebank-style phrase structure trees, although function tags and empty categories are stripped off (Figure 2). While most of the state-of-the-art parsers are based on probabilistic CFGs, the parameterization of the probabilistic model of each parser var"
P08-1006,N04-1013,0,0.601287,"uracy. Our experiments show that the levels of accuracy obtained with these different parsers are similar, but that accuracy improvements vary when the parsers are retrained with domain-specific data. 1 Introduction Parsing technologies have improved considerably in the past few years, and high-performance syntactic parsers are no longer limited to PCFG-based frameworks (Charniak, 2000; Klein and Manning, 2003; Charniak and Johnson, 2005; Petrov and Klein, 2007), but also include dependency parsers (McDonald and Pereira, 2006; Nivre and Nilsson, 2005; Sagae and Tsujii, 2007) and deep parsers (Kaplan et al., 2004; Clark and Curran, 2004; Miyao and Tsujii, 2008). However, efforts to perform extensive comparisons of syntactic parsers based on different frameworks have been limited. The most popular method for parser comparison involves the direct measurement of the parser output accuracy in terms of metrics such as bracketing precision and recall, or dependency accuracy. This assumes the existence of a gold-standard test corpus, such as the Penn Treebank (Marcus et al., 1994). It is difficult to apply this method to compare parsers based on different frameworks, because parse representations are often f"
P08-1006,P03-1054,0,0.166071,"ructure parsing, or deep parsing) using five different parse representations. We run a PPI system with several combinations of parser and parse representation, and examine their impact on PPI identification accuracy. Our experiments show that the levels of accuracy obtained with these different parsers are similar, but that accuracy improvements vary when the parsers are retrained with domain-specific data. 1 Introduction Parsing technologies have improved considerably in the past few years, and high-performance syntactic parsers are no longer limited to PCFG-based frameworks (Charniak, 2000; Klein and Manning, 2003; Charniak and Johnson, 2005; Petrov and Klein, 2007), but also include dependency parsers (McDonald and Pereira, 2006; Nivre and Nilsson, 2005; Sagae and Tsujii, 2007) and deep parsers (Kaplan et al., 2004; Clark and Curran, 2004; Miyao and Tsujii, 2008). However, efforts to perform extensive comparisons of syntactic parsers based on different frameworks have been limited. The most popular method for parser comparison involves the direct measurement of the parser output accuracy in terms of metrics such as bracketing precision and recall, or dependency accuracy. This assumes the existence of"
P08-1006,P05-1010,1,0.242557,"parser/ http://bllip.cs.brown.edu/resources.shtml 4 We set n = 50 in this paper. 5 http://nlp.cs.berkeley.edu/Main.html#Parsing This study demonstrates that IL-8 recognizes and activates CXCR1, CXCR2, and the Duffy antigen by distinct mechanisms. The molar ratio of serum retinol-binding protein (RBP) to transthyretin (TTR) is not useful to assess vitamin A status during infection in hospitalised children. Figure 3: Predicate argument structure timized automatically by assigning latent variables to each nonterminal node and estimating the parameters of the latent variables by the EM algorithm (Matsuzaki et al., 2005). Stanford’s unlexicalized parser (Klein and Manning, 2003).6 Unlike NO-RERANK, probabilities are not parameterized on lexical heads. Figure 4: Sentences including protein names SBJ OBJ ENTITY1(IL-8) −→ recognizes ←− ENTITY2(CXCR1) Figure 5: Dependency path STANFORD 2.3 Deep parsing Recent research developments have allowed for efficient and robust deep parsing of real-world texts (Kaplan et al., 2004; Clark and Curran, 2004; Miyao and Tsujii, 2008). While deep parsers compute theory-specific syntactic/semantic structures, predicate argument structures (PAS) are often used in parser evaluation"
P08-1006,E06-1011,0,0.0491874,"inations of parser and parse representation, and examine their impact on PPI identification accuracy. Our experiments show that the levels of accuracy obtained with these different parsers are similar, but that accuracy improvements vary when the parsers are retrained with domain-specific data. 1 Introduction Parsing technologies have improved considerably in the past few years, and high-performance syntactic parsers are no longer limited to PCFG-based frameworks (Charniak, 2000; Klein and Manning, 2003; Charniak and Johnson, 2005; Petrov and Klein, 2007), but also include dependency parsers (McDonald and Pereira, 2006; Nivre and Nilsson, 2005; Sagae and Tsujii, 2007) and deep parsers (Kaplan et al., 2004; Clark and Curran, 2004; Miyao and Tsujii, 2008). However, efforts to perform extensive comparisons of syntactic parsers based on different frameworks have been limited. The most popular method for parser comparison involves the direct measurement of the parser output accuracy in terms of metrics such as bracketing precision and recall, or dependency accuracy. This assumes the existence of a gold-standard test corpus, such as the Penn Treebank (Marcus et al., 1994). It is difficult to apply this method to"
P08-1006,J08-1002,1,0.858485,"f accuracy obtained with these different parsers are similar, but that accuracy improvements vary when the parsers are retrained with domain-specific data. 1 Introduction Parsing technologies have improved considerably in the past few years, and high-performance syntactic parsers are no longer limited to PCFG-based frameworks (Charniak, 2000; Klein and Manning, 2003; Charniak and Johnson, 2005; Petrov and Klein, 2007), but also include dependency parsers (McDonald and Pereira, 2006; Nivre and Nilsson, 2005; Sagae and Tsujii, 2007) and deep parsers (Kaplan et al., 2004; Clark and Curran, 2004; Miyao and Tsujii, 2008). However, efforts to perform extensive comparisons of syntactic parsers based on different frameworks have been limited. The most popular method for parser comparison involves the direct measurement of the parser output accuracy in terms of metrics such as bracketing precision and recall, or dependency accuracy. This assumes the existence of a gold-standard test corpus, such as the Penn Treebank (Marcus et al., 1994). It is difficult to apply this method to compare parsers based on different frameworks, because parse representations are often framework-specific and differ from parser to parse"
P08-1006,E06-1015,0,0.0153065,"and Adriaans, 2006; Erkan et al., 2007; Sætre et al., 2007). For the protein pair IL-8 and CXCR1 in Figure 4, a dependency parser outputs a dependency tree shown in Figure 1. From this dependency tree, we can extract a dependency path shown in Figure 5, which appears to be a strong clue in knowing that these proteins are mentioned as interacting. (dep_path (SBJ (ENTITY1 recognizes)) (rOBJ (recognizes ENTITY2))) Figure 6: Tree representation of a dependency path We follow the PPI extraction method of Sætre et al. (2007), which is based on SVMs with SubSet Tree Kernels (Collins and Duffy, 2002; Moschitti, 2006), while using different parsers and parse representations. Two types of features are incorporated in the classifier. The first is bag-of-words features, which are regarded as a strong baseline for IE systems. Lemmas of words before, between and after the pair of target proteins are included, and the linear kernel is used for these features. These features are commonly included in all of the models. Filtering by a stop-word list is not applied because this setting made the scores higher than Sætre et al. (2007)’s setting. The other type of feature is syntactic features. For dependency-based par"
P08-1006,P05-1013,0,0.273658,"representation, and examine their impact on PPI identification accuracy. Our experiments show that the levels of accuracy obtained with these different parsers are similar, but that accuracy improvements vary when the parsers are retrained with domain-specific data. 1 Introduction Parsing technologies have improved considerably in the past few years, and high-performance syntactic parsers are no longer limited to PCFG-based frameworks (Charniak, 2000; Klein and Manning, 2003; Charniak and Johnson, 2005; Petrov and Klein, 2007), but also include dependency parsers (McDonald and Pereira, 2006; Nivre and Nilsson, 2005; Sagae and Tsujii, 2007) and deep parsers (Kaplan et al., 2004; Clark and Curran, 2004; Miyao and Tsujii, 2008). However, efforts to perform extensive comparisons of syntactic parsers based on different frameworks have been limited. The most popular method for parser comparison involves the direct measurement of the parser output accuracy in terms of metrics such as bracketing precision and recall, or dependency accuracy. This assumes the existence of a gold-standard test corpus, such as the Penn Treebank (Marcus et al., 1994). It is difficult to apply this method to compare parsers based on"
P08-1006,N07-1051,0,0.0565978,"t parse representations. We run a PPI system with several combinations of parser and parse representation, and examine their impact on PPI identification accuracy. Our experiments show that the levels of accuracy obtained with these different parsers are similar, but that accuracy improvements vary when the parsers are retrained with domain-specific data. 1 Introduction Parsing technologies have improved considerably in the past few years, and high-performance syntactic parsers are no longer limited to PCFG-based frameworks (Charniak, 2000; Klein and Manning, 2003; Charniak and Johnson, 2005; Petrov and Klein, 2007), but also include dependency parsers (McDonald and Pereira, 2006; Nivre and Nilsson, 2005; Sagae and Tsujii, 2007) and deep parsers (Kaplan et al., 2004; Clark and Curran, 2004; Miyao and Tsujii, 2008). However, efforts to perform extensive comparisons of syntactic parsers based on different frameworks have been limited. The most popular method for parser comparison involves the direct measurement of the parser output accuracy in terms of metrics such as bracketing precision and recall, or dependency accuracy. This assumes the existence of a gold-standard test corpus, such as the Penn Treeban"
P08-1006,W07-1004,0,0.29972,"). This format was originally proposed for extracting dependency relations useful for practical applications (de Marneffe et al., 2006). A program to convert PTB is attached to the Stanford parser. Although the concept looks similar to CoNLL, this representaSD 8 http://nlp.cs.lth.se/pennconverter/ http://www.cis.upenn.edu/˜dbikel/software. html 9 Figure 9: Stanford dependencies tion does not necessarily form a tree structure, and is designed to express more fine-grained relations such as apposition. Research groups for biomedical NLP recently adopted this representation for corpus annotation (Pyysalo et al., 2007a) and parser evaluation (Clegg and Shepherd, 2007; Pyysalo et al., 2007b). Predicate-argument structures. This is the default output format for ENJU and ENJU-GENIA. PAS Although only CoNLL is available for dependency parsers, we can create four representations for the phrase structure parsers, and five for the deep parsers. Dotted arrows in Figure 7 indicate imperfect conversion, in which the conversion inherently introduces errors, and may decrease the accuracy. We should therefore take caution when comparing the results obtained by imperfect conversion. We also measure the accuracy obtained"
P08-1006,W06-1608,0,0.0654162,"n of the parsing accuracy in terms of a certain intermediate parse representation (Ringger et al., 2004; Kaplan et al., 2004; Briscoe and Carroll, 2006; Clark and Curran, 2007; Miyao et al., 2007; Clegg and Shepherd, 2007; Pyysalo et al., 2007b; Pyysalo et al., 2007a; Sagae et al., 2008). Such evaluation requires gold standard data in an intermediate representation. However, it has been argued that the conversion of parsing results into an intermediate representation is difficult and far from perfect. The relationship between parsing accuracy and task accuracy has been obscure for many years. Quirk and Corston-Oliver (2006) investigated the impact of parsing accuracy on statistical MT. However, this work was only concerned with a single dependency parser, and did not focus on parsers based on different frameworks. 6 Conclusion and Future Work We have presented our attempts to evaluate syntactic parsers and their representations that are based on different frameworks; dependency parsing, phrase structure parsing, or deep parsing. The basic idea is to measure the accuracy improvements of the PPI extraction task by incorporating the parser output as statistical features of a machine learning classifier. Experiments"
P08-1006,ringger-etal-2004-using,0,0.102376,"owever, efforts to perform extensive comparisons of syntactic parsers based on different frameworks have been limited. The most popular method for parser comparison involves the direct measurement of the parser output accuracy in terms of metrics such as bracketing precision and recall, or dependency accuracy. This assumes the existence of a gold-standard test corpus, such as the Penn Treebank (Marcus et al., 1994). It is difficult to apply this method to compare parsers based on different frameworks, because parse representations are often framework-specific and differ from parser to parser (Ringger et al., 2004). The lack of such comparisons is a serious obstacle for NLP researchers in choosing an appropriate parser for their purposes. In this paper, we present a comparative evaluation of syntactic parsers and their output representations based on different frameworks: dependency parsing, phrase structure parsing, and deep parsing. Our approach to parser evaluation is to measure accuracy improvement in the task of identifying protein-protein interaction (PPI) information in biomedical papers, by incorporating the output of different parsers as statistical features in a machine learning classifier (Ya"
P08-1006,D07-1111,1,0.727784,"ine their impact on PPI identification accuracy. Our experiments show that the levels of accuracy obtained with these different parsers are similar, but that accuracy improvements vary when the parsers are retrained with domain-specific data. 1 Introduction Parsing technologies have improved considerably in the past few years, and high-performance syntactic parsers are no longer limited to PCFG-based frameworks (Charniak, 2000; Klein and Manning, 2003; Charniak and Johnson, 2005; Petrov and Klein, 2007), but also include dependency parsers (McDonald and Pereira, 2006; Nivre and Nilsson, 2005; Sagae and Tsujii, 2007) and deep parsers (Kaplan et al., 2004; Clark and Curran, 2004; Miyao and Tsujii, 2008). However, efforts to perform extensive comparisons of syntactic parsers based on different frameworks have been limited. The most popular method for parser comparison involves the direct measurement of the parser output accuracy in terms of metrics such as bracketing precision and recall, or dependency accuracy. This assumes the existence of a gold-standard test corpus, such as the Penn Treebank (Marcus et al., 1994). It is difficult to apply this method to compare parsers based on different frameworks, bec"
P08-1006,1993.iwpt-1.22,0,0.0459583,"e on par with each other, while parsing speed differs significantly. We also found that accuracy improvements vary when parsers are retrained with domainspecific data, indicating the importance of domain adaptation and the differences in the portability of parser training methods. Although we restricted ourselves to parsers trainable with Penn Treebank-style treebanks, our methodology can be applied to any English parsers. Candidates include RASP (Briscoe and Carroll, 53 2006), the C&C parser (Clark and Curran, 2004), the XLE parser (Kaplan et al., 2004), MINIPAR (Lin, 1998), and Link Parser (Sleator and Temperley, 1993; Pyysalo et al., 2006), but the domain adaptation of these parsers is not straightforward. It is also possible to evaluate unsupervised parsers, which is attractive since evaluation of such parsers with goldstandard data is extremely problematic. A major drawback of our methodology is that the evaluation is indirect and the results depend on a selected task and its settings. This indicates that different results might be obtained with other tasks. Hence, we cannot conclude the superiority of parsers/representations only with our results. In order to obtain general ideas on parser performance,"
P08-1006,A00-2018,0,\N,Missing
P08-1006,J93-2004,0,\N,Missing
P08-1006,J04-4004,0,\N,Missing
P09-1003,P98-1013,0,0.424254,"Missing"
P09-1003,S07-1018,0,0.157223,"Missing"
P09-1003,C04-1100,0,0.3293,"training instances. Some recent studies have addressed alternative approaches to generalizing semantic roles across different frames (Gordon and Swanson, 2007; Zapi1 Introduction Semantic Role Labeling (SRL) is a task of analyzing predicate-argument structures in texts. More specifically, SRL identifies predicates and their arguments with appropriate semantic roles. Resolving surface divergence of texts (e.g., voice of verbs and nominalizations) into unified semantic representations, SRL has attracted much attention from researchers into various NLP applications including question answering (Narayanan and Harabagiu, 2004; Shen and Lapata, 2007; 19 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 19–27, c Suntec, Singapore, 2-7 August 2009. 2009 ACL and AFNLP Recipient Transfer::Recipient Giving::Recipient Agent Buyer Commerce_sell::Buyer Commerce_buy::Buyer Transfer::Donor Giving::Donor Commerce_sell::Seller Donor Seller Commerce_buy::Seller role-to-role relation hierarchical class thematic role role descriptor Figure 2: An example of role groupings using different criteria. tic role, Baldewein et al. (2004) re-used the training instances of other roles that were simila"
P09-1003,W04-0817,0,0.429327,"Missing"
P09-1003,J05-1004,0,0.394865,"Missing"
P09-1003,S07-1102,0,0.026985,"Missing"
P09-1003,D07-1002,0,0.0954278,"t studies have addressed alternative approaches to generalizing semantic roles across different frames (Gordon and Swanson, 2007; Zapi1 Introduction Semantic Role Labeling (SRL) is a task of analyzing predicate-argument structures in texts. More specifically, SRL identifies predicates and their arguments with appropriate semantic roles. Resolving surface divergence of texts (e.g., voice of verbs and nominalizations) into unified semantic representations, SRL has attracted much attention from researchers into various NLP applications including question answering (Narayanan and Harabagiu, 2004; Shen and Lapata, 2007; 19 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 19–27, c Suntec, Singapore, 2-7 August 2009. 2009 ACL and AFNLP Recipient Transfer::Recipient Giving::Recipient Agent Buyer Commerce_sell::Buyer Commerce_buy::Buyer Transfer::Donor Giving::Donor Commerce_sell::Seller Donor Seller Commerce_buy::Seller role-to-role relation hierarchical class thematic role role descriptor Figure 2: An example of role groupings using different criteria. tic role, Baldewein et al. (2004) re-used the training instances of other roles that were similar to the target role. A"
P09-1003,P05-1022,0,0.0285632,"Missing"
P09-1003,P03-1002,0,0.363972,"e roles having a small number of instances affect the average more than the micro average. Micro 89.00 90.78 90.23 90.25 90.36 89.50 91.10 Macro 68.50 76.58 76.19 72.41 74.51 69.21 75.92 −Err. 0.00 16.17 11.23 11.40 12.38 4.52 19.16 Table 1: The accuracy and error reduction rate of role classification for each type of role group. 6.1 Experimental settings We constructed a baseline classifier that uses only the x-role features. The feature design is similar to that of the previous studies (M`arquez et al., 2008). The characteristics of x are: frame, frame evoking word, head word, content word (Surdeanu et al., 2003), first/last word, head word of left/right sister, phrase type, position, voice, syntactic path (directed/undirected/partial), governing category (Gildea and Jurafsky, 2002), WordNet supersense in the phrase, combination features of frame evoking word & headword, combination features of frame evoking word & phrase type, and combination features of voice & phrase type. We also used PoS tags and stem forms as extra features of any word-features. We employed Charniak and Johnson’s reranking parser (Charniak and Johnson, 2005) to analyze syntactic trees. As an alternative for the traditional named"
P09-1003,W06-1670,0,0.0267192,"Missing"
P09-1003,N07-1069,0,0.220313,"ormance of the machinelearning approach, because these definitions produce many roles that have few training instances. PropBank defines a frame for each sense of predicates (e.g., buy.01), and semantic roles are defined in a frame-specific manner (e.g., buyer and seller for buy.01). In addition, these roles are associated with tags such as ARG0-5 and AM-*, which are commonly used in different frames. Most SRL studies on PropBank have used these tags in order to gather a sufficient amount of training data, and to generalize semantic-role classifiers across different frames. However, Yi et al. (2007) reported that tags ARG2–ARG5 were inconsistent and not that suitable as training instances. Some recent studies have addressed alternative approaches to generalizing semantic roles across different frames (Gordon and Swanson, 2007; Zapi1 Introduction Semantic Role Labeling (SRL) is a task of analyzing predicate-argument structures in texts. More specifically, SRL identifies predicates and their arguments with appropriate semantic roles. Resolving surface divergence of texts (e.g., voice of verbs and nominalizations) into unified semantic representations, SRL has attracted much attention from"
P09-1003,J02-3001,0,0.965792,". For instance, FrameNet specifies the constraint that Self motion::Area should be filled by phrases whose semantic type is Location. Since these types suggest a coarse-grained categorization of semantic roles, we construct role groups that contain roles whose semantic types are identical. c˜ = argmax c∈{m(y)|y∈Yf } Pm (c|f, x). (2) Here, Pm (c|f, x) presents the probability of the role group c for f and x. The role y˜ is determined uniquely iff a single role y ∈ Yf is associated with c˜. Some previous studies have employed this idea to remedy the data sparseness problem in the training data (Gildea and Jurafsky, 2002). However, we cannot apply this approach when multiple roles in Yf are contained in the same class. For example, we can construct a semantic-type group St::State of affairs in which Giving::Reason and Giving::Means are included, as illustrated in Figure 4. If c˜ = St::State of affairs, we cannot disambiguate which original role is correct. In addition, it may be more effective to use various 4.4 Thematic roles of VerbNet VerbNet thematic roles are 23 frame-independent semantic categories for arguments of verbs, such as Agent, Patient, Theme and Source. These categories have been used as consis"
P09-1003,P08-1063,0,0.0768702,"Missing"
P09-1003,P06-1117,0,0.0551762,"Missing"
P09-1003,P07-1025,0,0.0341839,"defined in a frame-specific manner (e.g., buyer and seller for buy.01). In addition, these roles are associated with tags such as ARG0-5 and AM-*, which are commonly used in different frames. Most SRL studies on PropBank have used these tags in order to gather a sufficient amount of training data, and to generalize semantic-role classifiers across different frames. However, Yi et al. (2007) reported that tags ARG2–ARG5 were inconsistent and not that suitable as training instances. Some recent studies have addressed alternative approaches to generalizing semantic roles across different frames (Gordon and Swanson, 2007; Zapi1 Introduction Semantic Role Labeling (SRL) is a task of analyzing predicate-argument structures in texts. More specifically, SRL identifies predicates and their arguments with appropriate semantic roles. Resolving surface divergence of texts (e.g., voice of verbs and nominalizations) into unified semantic representations, SRL has attracted much attention from researchers into various NLP applications including question answering (Narayanan and Harabagiu, 2004; Shen and Lapata, 2007; 19 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 19–27, c Sunt"
P09-1003,J08-2001,0,0.228549,"Missing"
P09-1003,W05-0630,0,0.0142305,"ns. 2 3 Role Classification SRL is a complex task wherein several problems are intertwined: frame-evoking word identification, frame disambiguation (selecting a correct frame from candidates for the evoking word), rolephrase identification (identifying phrases that fill semantic roles), and role classification (assigning correct roles to the phrases). In this paper, we focus on role classification, in which the role generalization is particularly critical to the machine learning approach. In the role classification task, we are given a sentence, a frame evoking word, a frame, and Related Work Moschitti et al. (2005) first classified roles by using four coarse-grained classes (Core Roles, Adjuncts, Continuation Arguments and Co-referring Arguments), and built a classifier for each coarsegrained class to tag PropBank ARG tags. Even though the initial classifiers could perform rough estimations of semantic roles, this step was not able to solve the ambiguity problem in PropBank ARG2-5. When training a classifier for a seman20 Commerce_pay::Buyer C_pay::Buyer Evading::Evader Evading::Evader Giving::Donor Avoiding::Agent GIVING::Donor Avoiding::Agent Intentionall_act::Agent Intentionally_ACT::Agent Theme::Age"
P09-1003,P07-1098,0,0.0419443,"Missing"
P09-1003,C98-1013,0,\N,Missing
P09-1054,P04-1014,0,0.00603949,"tal results demonstrate that our method can produce compact and accurate models much more quickly than a state-of-the-art quasiNewton method for L1-regularized loglinear models. 1 Introduction Log-linear models (a.k.a maximum entropy models) are one of the most widely-used probabilistic models in the field of natural language processing (NLP). The applications range from simple classification tasks such as text classification and history-based tagging (Ratnaparkhi, 1996) to more complex structured prediction tasks such as partof-speech (POS) tagging (Lafferty et al., 2001), syntactic parsing (Clark and Curran, 2004) and semantic role labeling (Toutanova et al., 2005). Loglinear models have a major advantage over other 477 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 477–485, c Suntec, Singapore, 2-7 August 2009. 2009 ACL and AFNLP An alternative approach to training a log-linear model is to use stochastic gradient descent (SGD) methods. SGD uses approximate gradients estimated from subsets of the training data and updates the weights of the features in an online fashion—the weights are updated much more frequently than batch training algorithms. This learning f"
P09-1054,W05-0622,0,0.00845476,"i If the structure is a sequence, the model is called a linear-chain CRF model, and the marginal probabilities of the features and the partition function can be efficiently computed by using the forwardbackward algorithm. The model is used for a variety of sequence labeling tasks such as POS tagging, chunking, and named entity recognition. If the structure is a tree, the model is called a tree CRF model, and the marginal probabilities can be computed by using the inside-outside algorithm. The model can be used for tasks like syntactic parsing (Finkel et al., 2008) and semantic role labeling (Cohn and Blunsom, 2005). In this paper, we present a simple method for solving these two problems in SGD learning. The main idea is to keep track of the total penalty and the penalty that has been applied to each weight, so that the L1 penalty is applied based on the difference between those cumulative values. That way, the application of L1 penalty is needed only for the features that are used in the current sample, and also the effect of noisy gradient is smoothed away. 2.1 Training The weights of the features in a log-linear model are optimized in such a way that they maximize the regularized conditional log-like"
P09-1054,W02-1001,0,0.468961,"Missing"
P09-1054,P06-1059,1,0.746285,",10 which provided POS tags and chunk tags. We did not use any information on the named entity tags output by the GENIA tagger. For the features, we used unigrams of neighboring chunk tags, substrings (shorter than 10 characters) of the current word, and the shape of the word (e.g. “IL-2” is converted into “AA-#”), on top of the features used in the text chunking experiments. The results are shown in Figure 5 and Table 2. The trend in the results is the same as that of the text chunking task: our SGD algorithms show much faster convergence than the OWL-QN algorithm and produce compact models. Okanohara et al. (2006) report an f-score of 71.48 on the same data, using semi-Markov CRFs. -2.6 -2.8 -3 -3.2 -3.4 OWL-QN SGD-L1 (Clipping) SGD-L1 (Cumulative) SGD-L1 (Cumulative + ED) -3.6 -3.8 0 10 20 30 40 50 Passes Figure 5: NLPBA 2004 named entity recognition task: Objective. -1.8 -1.9 Objective function -2 -2.1 -2.2 -2.3 -2.4 -2.5 OWL-QN SGD-L1 (Clipping) SGD-L1 (Cumulative) SGD-L1 (Cumulative + ED) -2.6 -2.7 -2.8 0 10 20 30 40 50 Passes Figure 6: POS tagging task: Objective. 4.3 Part-Of-Speech Tagging ing because it was “prohibitive” (7-8 days for sections 0-18 of the WSJ corpus). For the features, we used u"
P09-1054,W96-0213,0,0.530357,"evaluate the effectiveness of our method in three applications: text chunking, named entity recognition, and part-of-speech tagging. Experimental results demonstrate that our method can produce compact and accurate models much more quickly than a state-of-the-art quasiNewton method for L1-regularized loglinear models. 1 Introduction Log-linear models (a.k.a maximum entropy models) are one of the most widely-used probabilistic models in the field of natural language processing (NLP). The applications range from simple classification tasks such as text classification and history-based tagging (Ratnaparkhi, 1996) to more complex structured prediction tasks such as partof-speech (POS) tagging (Lafferty et al., 2001), syntactic parsing (Clark and Curran, 2004) and semantic role labeling (Toutanova et al., 2005). Loglinear models have a major advantage over other 477 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 477–485, c Suntec, Singapore, 2-7 August 2009. 2009 ACL and AFNLP An alternative approach to training a log-linear model is to use stochastic gradient descent (SGD) methods. SGD uses approximate gradients estimated from subsets of the training data and u"
P09-1054,P07-1096,0,0.0378243,"Missing"
P09-1054,D08-1016,0,0.0112981,"log-linear model is to use stochastic gradient descent (SGD) methods. SGD uses approximate gradients estimated from subsets of the training data and updates the weights of the features in an online fashion—the weights are updated much more frequently than batch training algorithms. This learning framework is attracting attention because it often requires much less training time in practice than batch training algorithms, especially when the training data is large and redundant. SGD was recently used for NLP tasks including machine translation (Tillmann and Zhang, 2006) and syntactic parsing (Smith and Eisner, 2008; Finkel et al., 2008). Also, SGD is very easy to implement because it does not need to use the Hessian information on the objective function. The implementation could be as simple as the perceptron algorithm. pact and accurate models much more quickly than the OWL-QN algorithm. This paper is organized as follows. Section 2 provides a general description of log-linear models used in NLP. Section 3 describes our stochastic gradient descent method for L1-regularized loglinear models. Experimental results are presented in Section 4. Some related work is discussed in Section 5. Section 6 gives som"
P09-1054,P08-1109,0,0.0549519,"Missing"
P09-1054,P07-1104,0,0.0274516,"ion, which aims to obtain the weights of the features that maximize the conditional likelihood of the training data. In maximum likelihood training, regularization is normally needed to prevent the model from overfitting the training data, The two most common regularization methods are called L1 and L2 regularization. L1 regularization penalizes the weight vector for its L1-norm (i.e. the sum of the absolute values of the weights), whereas L2 regularization uses its L2-norm. There is usually not a considerable difference between the two methods in terms of the accuracy of the resulting model (Gao et al., 2007), but L1 regularization has a significant advantage in practice. Because many of the weights of the features become zero as a result of L1-regularized training, the size of the model can be much smaller than that produced by L2-regularization. Compact models require less space on memory and storage, and enable the application to start up quickly. These merits can be of vital importance when the application is deployed in resource-tight environments such as cell-phones. A common way to train a large-scale L1regularized model is to use a quasi-Newton method. Kazama and Tsujii (2003) describe a m"
P09-1054,P06-1091,0,0.00589602,"L and AFNLP An alternative approach to training a log-linear model is to use stochastic gradient descent (SGD) methods. SGD uses approximate gradients estimated from subsets of the training data and updates the weights of the features in an online fashion—the weights are updated much more frequently than batch training algorithms. This learning framework is attracting attention because it often requires much less training time in practice than batch training algorithms, especially when the training data is large and redundant. SGD was recently used for NLP tasks including machine translation (Tillmann and Zhang, 2006) and syntactic parsing (Smith and Eisner, 2008; Finkel et al., 2008). Also, SGD is very easy to implement because it does not need to use the Hessian information on the objective function. The implementation could be as simple as the perceptron algorithm. pact and accurate models much more quickly than the OWL-QN algorithm. This paper is organized as follows. Section 2 provides a general description of log-linear models used in NLP. Section 3 describes our stochastic gradient descent method for L1-regularized loglinear models. Experimental results are presented in Section 4. Some related work"
P09-1054,P05-1073,0,0.00918396,"compact and accurate models much more quickly than a state-of-the-art quasiNewton method for L1-regularized loglinear models. 1 Introduction Log-linear models (a.k.a maximum entropy models) are one of the most widely-used probabilistic models in the field of natural language processing (NLP). The applications range from simple classification tasks such as text classification and history-based tagging (Ratnaparkhi, 1996) to more complex structured prediction tasks such as partof-speech (POS) tagging (Lafferty et al., 2001), syntactic parsing (Clark and Curran, 2004) and semantic role labeling (Toutanova et al., 2005). Loglinear models have a major advantage over other 477 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 477–485, c Suntec, Singapore, 2-7 August 2009. 2009 ACL and AFNLP An alternative approach to training a log-linear model is to use stochastic gradient descent (SGD) methods. SGD uses approximate gradients estimated from subsets of the training data and updates the weights of the features in an online fashion—the weights are updated much more frequently than batch training algorithms. This learning framework is attracting attention because it often re"
P09-1054,W03-1018,1,0.434131,"he resulting model (Gao et al., 2007), but L1 regularization has a significant advantage in practice. Because many of the weights of the features become zero as a result of L1-regularized training, the size of the model can be much smaller than that produced by L2-regularization. Compact models require less space on memory and storage, and enable the application to start up quickly. These merits can be of vital importance when the application is deployed in resource-tight environments such as cell-phones. A common way to train a large-scale L1regularized model is to use a quasi-Newton method. Kazama and Tsujii (2003) describe a method for training a L1-regularized log-linear model with a bound constrained version of the BFGS algorithm (Nocedal, 1980). Andrew and Gao (2007) present an algorithm called OrthantWise Limited-memory Quasi-Newton (OWLQN), which can work on the BFGS algorithm without bound constraints and achieve faster convergence. Stochastic gradient descent (SGD) uses approximate gradients estimated from subsets of the training data and updates the parameters in an online fashion. This learning framework is attractive because it often requires much less training time in practice than batch tra"
P09-1054,wellner-vilain-2006-leveraging,0,0.0282722,"Missing"
P09-1054,W04-1213,0,0.0388038,"eature set as ours.8 Their library uses the OWL-QN algorithm for optimization. Although direct comparison of training times is not impor6 http://crfpp.sourceforge.net/ http://www.chokkan.org/software/crfsuite/benchmark.html 8 ditto 7 482 tant due to the differences in implementation and hardware platforms, these results demonstrate that our algorithm can actually result in a very fast implementation of a CRF trainer. -2.2 Objective function -2.4 4.2 Named Entity Recognition The second set of experiments used the named entity recognition data set provided for the BioNLP/NLPBA 2004 shared task (Kim et al., 2004).9 The training data consist of 18,546 sentences in which each token is annotated with the “IOB” tags representing biomedical named entities such as the names of proteins and RNAs. The training and test data were preprocessed by the GENIA tagger,10 which provided POS tags and chunk tags. We did not use any information on the named entity tags output by the GENIA tagger. For the features, we used unigrams of neighboring chunk tags, substrings (shorter than 10 characters) of the current word, and the shape of the word (e.g. “IL-2” is converted into “AA-#”), on top of the features used in the tex"
P09-1054,J93-2004,0,\N,Missing
P09-1102,P08-2016,0,0.319572,"tperformed five out of six state-of-the-art abbreviation recognizers. 1 Introduction Abbreviations represent fully expanded forms (e.g., hidden markov model) through the use of shortened forms (e.g., HMM). At the same time, abbreviations increase the ambiguity in a text. For example, in computational linguistics, the acronym HMM stands for hidden markov model, whereas, in the field of biochemistry, HMM is generally an abbreviation for heavy meromyosin. Associating abbreviations with their fully expanded forms is of great importance in various NLP applications (Pakhomov, 2002; Yu et al., 2006; HaCohen-Kerner et al., 2008). The core technology for abbreviation disambiguation is to recognize the abbreviation defini905 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 905–913, c Suntec, Singapore, 2-7 August 2009. 2009 ACL and AFNLP pol y g l y c ol i c ac i d PS S S P S S S S S S S S PS S S (a): English Abbreviation Generation [PGA] Institute of History and Philology at Academia Sinica 历史语 言 研究所 S P P S S S P [史语所] y2 ym y1 y2 ym h1 h2 hm x1 x2 xm x1 x2 xm CRF (b): Chinese Abbreviation Generation y1 DPLVM Figure 2: CRF vs. DPLVM. Variables x, y, and h represent observation,"
P09-1102,C08-1083,1,0.911993,"1.9 92.0 90.0 87.1 96.9 94.8 97.8 97.7 98.1 F 95.9 90.5 94.0 92.1 91.0 97.1 92.1 95.9 95.1 96.1 Table 6: Results of English abbreviation recognition. belings. Other labelings are impossible, because they will generate an abbreviation that is not AP. If the first or second labeling is generated, AP is selected as an abbreviation of arterial pressure. If the third or fourth labeling is generated, then AP is selected as an abbreviation of cannulate for arterial pressure. Finally, the fifth labeling (NULL) indicates that AP is not an abbreviation. To evaluate the recognizer, we use the corpus6 of Okazaki et al. (2008), which contains 864 abbreviation definitions collected from 1,000 MEDLINE scientific abstracts. In implementing the recognizer, we simply use the model from the abbreviation generator, with the same feature templates (31,868 features) and training method; the major difference is in the restriction (according to the PE) of the decoding stage and penalizing the probability values of the NULL labelings7 . For the evaluation metrics, following Okazaki et al. (2008), we use precision (P = k/m), recall (R = k/n), and the F-score defined by Recognition as a Generation Task We directly migrate this m"
P09-1102,P02-1021,0,0.485619,"sed model worked robustly, and outperformed five out of six state-of-the-art abbreviation recognizers. 1 Introduction Abbreviations represent fully expanded forms (e.g., hidden markov model) through the use of shortened forms (e.g., HMM). At the same time, abbreviations increase the ambiguity in a text. For example, in computational linguistics, the acronym HMM stands for hidden markov model, whereas, in the field of biochemistry, HMM is generally an abbreviation for heavy meromyosin. Associating abbreviations with their fully expanded forms is of great importance in various NLP applications (Pakhomov, 2002; Yu et al., 2006; HaCohen-Kerner et al., 2008). The core technology for abbreviation disambiguation is to recognize the abbreviation defini905 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 905–913, c Suntec, Singapore, 2-7 August 2009. 2009 ACL and AFNLP pol y g l y c ol i c ac i d PS S S P S S S S S S S S PS S S (a): English Abbreviation Generation [PGA] Institute of History and Philology at Academia Sinica 历史语 言 研究所 S P P S S S P [史语所] y2 ym y1 y2 ym h1 h2 hm x1 x2 xm x1 x2 xm CRF (b): Chinese Abbreviation Generation y1 DPLVM Figure 2: CRF vs. DPLV"
P09-1102,W01-0516,0,0.56495,"ocally defined in a document. Therefore, a number of studies have attempted to model the generation processes of abbreviations: e.g., inferring the abbreviating mechanism of the hidden markov model into HMM. An obvious approach is to manually design rules for abbreviations. Early studies attempted to determine the generic rules that humans use to intuitively abbreviate given words (Barrett and Grems, 1960; Bourne and Ford, 1961). Since the late 1990s, researchers have presented various methods by which to extract abbreviation definitions that appear in actual texts (Taghva and Gilbreth, 1999; Park and Byrd, 2001; Wren and Garner, 2002; Schwartz and Hearst, 2003; Adar, 2004; Ao and Takagi, 2005). For example, Schwartz and Hearst (2003) implemented a simple algorithm that mapped all alpha-numerical letters in an abbreviation to its expanded form, starting from the end of both the abbreviation and its expanded forms, and moving from right to left. These studies performed highly, especially for English abbreviations. However, a more extensive investigation of abbreviations is needed in order to further improve definition extraction. In addition, we cannot simply transfer the knowledge of the hand-crafted"
P09-1102,N03-1028,0,0.0125058,"uring training and validation, we set σ = 1 for the DPLVM generators. We also set four latent variables for each label, in order to make a compromise between accuracy and efficiency. Note that, for the label encoding with global information, many label transitions (e.g., P2 S3 ) are actually impossible: the label transitions are strictly constrained, i.e., yi yi+1 ∈ {Pj Sj , Pj Pj+1 , Sj Pj+1 , Sj Sj }. These constraints on the model topology (forward-backward lattice) are enforced by giving appropriate features a weight of −∞, thereby forcing all forbidden labelings to have zero probability. Sha and Pereira (2003) originally proposed this concept of implementing transition restrictions. Table 1: Language-independent features (#1 to #3), Chinese-specific features (#4 through #7), and English-specific features (#8 through #11). the other hand, such duplication detection features are not so useful for English abbreviations. Feature templates #8–#11 are designed for English abbreviations. Features #8 and #9 encode the orthographic information of expanded forms. Features #10 and #11 represent a contextual n-gram with a large window size. Since the number of letters in Chinese (more than 10K characters) is m"
P09-1102,W05-1304,1,0.408495,"y outperform previous abbreviation generation studies. In addition, we apply the proposed models to the task of abbreviation recognition, in which a model extracts the abbreviation definitions in a given text. To the extent of our knowledge, this is the first model that can perform both abbreviation generation and recognition at the state-of-the-art level, across different languages and with a simple feature set. other languages, including Chinese and Japanese, do not have word boundaries or case sensitivity. A number of recent studies have investigated the use of machine learning techniques. Tsuruoka et al. (2005) formalized the processes of abbreviation generation as a sequence labeling problem. In the present study, each character in the expanded form is tagged with a label, y ∈ {P, S}1 , where the label P produces the current character and the label S skips the current character. In Figure 1 (a), the abbreviation PGA is generated from the full form polyglycolic acid because the underlined characters are tagged with P labels. In Figure 1 (b), the abbreviation is generated using the 2nd and 3rd characters, skipping the subsequent three characters, and then using the 7th character. In order to formaliz"
P09-2008,J94-2001,0,0.0106901,"l, confidence and threshold estimation, and output optimization. The following sections will explain the steps in detail. Confidence and Threshold Estimation T = f (C) 2.1 Baseline Word Segmentation Model We use the tri-gram Hidden Markov Model (HMM) of (Lee et al., 2007) as the baseline WS model; however, we adopt the Maximum Likelihood (ML) decoding strategy to independently find the best word spacing states. ML-decoding allows us to directly compare each output to the threshold. There is little discrepancy in accuracy when using ML-decoding, as compared to Viterbidecoding, as mentioned in (Merialdo, 1994).1 Let o1,n be a sequence of n-character user input without WBMs, xt be the best word spacing state for ot where 1 ≤ t ≤ n. Assume that xt is either 1 (space after ot ) or 0 (no space after ot ). Then each best word spacing state x ˆt for all t can be found by using Equation 1. xˆt = argmax P (xt = i|o1,n ) i∈(0,1) = argmax P (o1,n , xt = i) = argmax i∈(0,1) i∈(0,1) × X X Then, we define the confidence as is done in Equation 5. Because calculating such a variable is impossible, we estimate the value by substituting the word spacing states produced by the S baseline WS model, xW 1,n , with the"
P09-2008,C04-1067,0,0.0753508,"Missing"
P10-1034,W07-0702,0,0.101471,"orner of Figure 2. In order to include richer context information and account for multiple interpretations of unaligned words of foreign language, minimal rules which share adjacent tree fragments are connected together to form composed rules (Galley et al., Considering the parse error problem in the 1-best or k-best parse trees, Mi and Huang (2008) extracted tree-to-string translation rules from aligned packed forest-string pairs. A forest compactly encodes exponentially many trees 327 fine-grained tree-to-string rule extraction, rather than string-to-string translation (Hassan et al., 2007; Birch et al., 2007). The Logon project2 (Oepen et al., 2007) for Norwegian-English translation integrates in-depth grammatical analysis of Norwegian (using lexical functional grammar, similar to (Riezler and Maxwell, 2006)) with semantic representations in the minimal recursion semantics framework, and fully grammar-based generation for English using HPSG. A hybrid (of rule-based and data-driven) architecture with a semantic transfer backbone is taken as the vantage point of this project. In contrast, the fine-grained tree-to-string translation rule extraction approaches in this paper are totally data-driven, an"
P10-1034,N09-1025,0,0.0888322,"e set that is more appropriate to reflect the real translation situations. This motivates our proposal of using deep syntactic information to obtain a fine-grained translation rule set. We name the information such as the voice of a verb in a tree fragment as deep syntactic information. We use a head-driven phrase structure grammar (HPSG) parser to obtain the Introduction Tree-to-string translation rules are generic and applicable to numerous linguistically syntax-based Statistical Machine Translation (SMT) systems, such as string-to-tree translation (Galley et al., 2004; Galley et al., 2006; Chiang et al., 2009), tree-to-string translation (Liu et al., 2006; Huang et al., 2006), and forest-to-string translation (Mi et al., 2008; Mi and Huang, 2008). The algorithms proposed by Galley et al. (2004; 2006) are frequently used for extracting minimal and composed rules from aligned 1-best tree-string pairs. Dealing with the parse error problem and rule sparseness problem, Mi and Huang (2008) replaced the 1-best parse tree with a packed forest which compactly encodes exponentially many parses for treeto-string rule extraction. However, current tree-to-string rules only make use of Probabilistic Context-Free"
P10-1034,P05-1033,0,0.680418,"a binarized rule set. These glue rules can be seen as an extension from X to {Xm }of the two glue rules described in (Chiang, 2007). 331 # of sentences # of Jp words # of En words Train 994K 28.2M 24.7M Dev. 2K 57.4K 50.3K Test 2K 57.1K 49.9K tree nodes # rules # tree types extract time Table 3: Statistics of the JST corpus. PRS TFS 0.9 0.4 3.5 C3S POS 62.1 23.5 - C3 TFS 83.9 34.7 98.6 FS POS 92.5 40.6 - F TFS 103.7 45.2 121.2 Table 4: Statistics of several kinds of tree-to-string rules. Here, the number is in million level and the time is in hour. use the Algorithm 3 described in (Huang and Chiang, 2005) to extract its k-best (k = 500 in our experiments) derivations. Since different derivations may lead to the same target language string, we further adopt Algorithm 3’s modification, i.e., keep a hash-table to maintain the unique target sentences (Huang et al., 2006), to efficiently generate the unique k-best translations. 200 for English-to-Japanese translation and 500 for Japanese-to-English translation. We used four dual core Xeon machines (4×3.0GHz×2CPU, 4×64GB memory) to run all the experiments. 4.4 Results 4.3 Setups Table 4 illustrates the statistics of several translation rule sets, wh"
P10-1034,J07-2003,0,0.549376,"rsal of the terminal nodes in Et . At each terminal node, we seek its minimum covering tree, retrieve PRS, and update the hash-table. For example, suppose we are decoding an HPSG tree (with gray nodes) shown in Figure 2. At t1 , we can extract its minimum covering tree with the root node to be c0 , then take this tree fragment as the key to retrieve PRS, and consequently put c0 and the available rules in the hash-table. When decoding at c0 , we can directly access the hash-table looking for available PASbased rules. In contrast, we use a CKY-style algorithm with beam-pruning and cube-pruning (Chiang, 2007) to decode Japanese sentences. For each Japanese sentence F , the output of the chart-parsing algorithm is expressed as a hypergraph representing a set of derivations. Given such a hypergraph, we r∈d This equation reflects that the translation rules in one d come from three sets. Inspired by (Liu et al., 2009b), it is appealing to combine these rule sets together in one decoder because PTT provides excellent rule coverages while TRS and PRS offer linguistically motivated phrase selections and nonlocal reorderings. Each f (r) is in turn a product of five features: f (r) = p(s|t)λ3 · p(t|s)λ4 ·"
P10-1034,N04-1035,0,0.791221,"ormation to “killed”, we are gaining a rule set that is more appropriate to reflect the real translation situations. This motivates our proposal of using deep syntactic information to obtain a fine-grained translation rule set. We name the information such as the voice of a verb in a tree fragment as deep syntactic information. We use a head-driven phrase structure grammar (HPSG) parser to obtain the Introduction Tree-to-string translation rules are generic and applicable to numerous linguistically syntax-based Statistical Machine Translation (SMT) systems, such as string-to-tree translation (Galley et al., 2004; Galley et al., 2006; Chiang et al., 2009), tree-to-string translation (Liu et al., 2006; Huang et al., 2006), and forest-to-string translation (Mi et al., 2008; Mi and Huang, 2008). The algorithms proposed by Galley et al. (2004; 2006) are frequently used for extracting minimal and composed rules from aligned 1-best tree-string pairs. Dealing with the parse error problem and rule sparseness problem, Mi and Huang (2008) replaced the 1-best parse tree with a packed forest which compactly encodes exponentially many parses for treeto-string rule extraction. However, current tree-to-string rules"
P10-1034,P06-1121,0,0.0797987,"we are gaining a rule set that is more appropriate to reflect the real translation situations. This motivates our proposal of using deep syntactic information to obtain a fine-grained translation rule set. We name the information such as the voice of a verb in a tree fragment as deep syntactic information. We use a head-driven phrase structure grammar (HPSG) parser to obtain the Introduction Tree-to-string translation rules are generic and applicable to numerous linguistically syntax-based Statistical Machine Translation (SMT) systems, such as string-to-tree translation (Galley et al., 2004; Galley et al., 2006; Chiang et al., 2009), tree-to-string translation (Liu et al., 2006; Huang et al., 2006), and forest-to-string translation (Mi et al., 2008; Mi and Huang, 2008). The algorithms proposed by Galley et al. (2004; 2006) are frequently used for extracting minimal and composed rules from aligned 1-best tree-string pairs. Dealing with the parse error problem and rule sparseness problem, Mi and Huang (2008) replaced the 1-best parse tree with a packed forest which compactly encodes exponentially many parses for treeto-string rule extraction. However, current tree-to-string rules only make use of Prob"
P10-1034,J03-1002,0,0.00648777,"Missing"
P10-1034,P03-1021,0,0.0143227,"shown in the bottom-right corner of Figure 2. The definition supplies us a linear-time algorithm to directly find the tree fragment that covers a PAS during both rule extracting and rule matching when decoding an HPSG tree. 4 Experiments 4.1 Translation models We use a tree-to-string model and a string-to-tree model for bidirectional Japanese-English translations. Both models use a phrase translation table (PTT), an HPSG tree-based rule set (TRS), and a PAS-based rule set (PRS). Since the three rule sets are independently extracted and estimated, we 330 use Minimum Error Rate Training (MERT) (Och, 2003) to tune the weights of the features from the three rule sets on the development set. Given a 1-best (localized) HPSG tree Et , the tree-to-string decoder searches for the optimal derivation d∗ that transforms Et into a Japanese string among the set of all possible derivations D: The string-to-tree decoder searches for the optimal derivation d∗ that parses a Japanese string F into a packed forest of the set of all possible derivations D: d∗ = arg max{λ1 log pLM (τ (d)) + λ2 |τ (d)| d∈D + λ3 g(d) + log s(d|F )}. d∗ = arg max{λ1 log pLM (τ (d)) + λ2 |τ (d)| This formula differs from Equation 2 b"
P10-1034,P07-1037,0,0.0377843,"Missing"
P10-1034,2007.tmi-papers.18,0,0.0670195,"Missing"
P10-1034,W05-1506,0,0.0605444,"pearing in a binarized rule set. These glue rules can be seen as an extension from X to {Xm }of the two glue rules described in (Chiang, 2007). 331 # of sentences # of Jp words # of En words Train 994K 28.2M 24.7M Dev. 2K 57.4K 50.3K Test 2K 57.1K 49.9K tree nodes # rules # tree types extract time Table 3: Statistics of the JST corpus. PRS TFS 0.9 0.4 3.5 C3S POS 62.1 23.5 - C3 TFS 83.9 34.7 98.6 FS POS 92.5 40.6 - F TFS 103.7 45.2 121.2 Table 4: Statistics of several kinds of tree-to-string rules. Here, the number is in million level and the time is in hour. use the Algorithm 3 described in (Huang and Chiang, 2005) to extract its k-best (k = 500 in our experiments) derivations. Since different derivations may lead to the same target language string, we further adopt Algorithm 3’s modification, i.e., keep a hash-table to maintain the unique target sentences (Huang et al., 2006), to efficiently generate the unique k-best translations. 200 for English-to-Japanese translation and 500 for Japanese-to-English translation. We used four dual core Xeon machines (4×3.0GHz×2CPU, 4×64GB memory) to run all the experiments. 4.4 Results 4.3 Setups Table 4 illustrates the statistics of several translation rule sets, wh"
P10-1034,2006.amta-papers.8,0,0.715916,"tions. This motivates our proposal of using deep syntactic information to obtain a fine-grained translation rule set. We name the information such as the voice of a verb in a tree fragment as deep syntactic information. We use a head-driven phrase structure grammar (HPSG) parser to obtain the Introduction Tree-to-string translation rules are generic and applicable to numerous linguistically syntax-based Statistical Machine Translation (SMT) systems, such as string-to-tree translation (Galley et al., 2004; Galley et al., 2006; Chiang et al., 2009), tree-to-string translation (Liu et al., 2006; Huang et al., 2006), and forest-to-string translation (Mi et al., 2008; Mi and Huang, 2008). The algorithms proposed by Galley et al. (2004; 2006) are frequently used for extracting minimal and composed rules from aligned 1-best tree-string pairs. Dealing with the parse error problem and rule sparseness problem, Mi and Huang (2008) replaced the 1-best parse tree with a packed forest which compactly encodes exponentially many parses for treeto-string rule extraction. However, current tree-to-string rules only make use of Probabilistic Context-Free Grammar tree fragments, in which part-of-speech (POS) or 1 For exa"
P10-1034,P07-2045,0,0.00800156,"t can be segmented properly into a set of tree fragments, each of which can be used to generate a tree-to-string translation rule. 2.3 Rich syntactic information for SMT Before describing our approaches of applying deep syntactic information yielded by an HPSG parser for fine-grained rule extraction, we would like to briefly review what kinds of deep syntactic information have been employed for SMT. Two kinds of supertags, from Lexicalized TreeAdjoining Grammar and Combinatory Categorial Grammar (CCG), have been used as lexical syntactic descriptions (Hassan et al., 2007) for phrasebased SMT (Koehn et al., 2007). By introducing supertags into the target language side, i.e., the target language model and the target side of the phrase table, significant improvement was achieved for Arabic-to-English translation. Birch et al. (2007) also reported a significant improvement for Dutch-English translation by applying CCG supertags at a word level to a factorized SMT system (Koehn et al., 2007). In this paper, we also make use of supertags on the English language side. In an HPSG parse tree, these lexical syntactic descriptions are included in the LEXENTRY feature (refer to Table 2) of a lexical node (Matsuz"
P10-1034,P02-1040,0,0.0839341,"Missing"
P10-1034,P09-4007,0,0.278936,"Missing"
P10-1034,N06-1032,0,0.0194013,"are connected together to form composed rules (Galley et al., Considering the parse error problem in the 1-best or k-best parse trees, Mi and Huang (2008) extracted tree-to-string translation rules from aligned packed forest-string pairs. A forest compactly encodes exponentially many trees 327 fine-grained tree-to-string rule extraction, rather than string-to-string translation (Hassan et al., 2007; Birch et al., 2007). The Logon project2 (Oepen et al., 2007) for Norwegian-English translation integrates in-depth grammatical analysis of Norwegian (using lexical functional grammar, similar to (Riezler and Maxwell, 2006)) with semantic representations in the minimal recursion semantics framework, and fully grammar-based generation for English using HPSG. A hybrid (of rule-based and data-driven) architecture with a semantic transfer backbone is taken as the vantage point of this project. In contrast, the fine-grained tree-to-string translation rule extraction approaches in this paper are totally data-driven, and easily applicable to numerous language pairs by taking English as the source or target language. rather than the 1-best tree used by Galley et al. (2004; 2006). Two problems were managed to be tackled"
P10-1034,2007.mtsummit-papers.63,0,0.168113,"Missing"
P10-1034,P09-1063,0,0.106393,"Missing"
P10-1034,P09-1065,0,0.154289,") and f (r) are identical with those used in Equation 2. d∈D + log s(d|Et )}. (3) (2) Here, the first item is the language model (LM) probability where τ (d) is the target string of derivation d; the second item is the translation length penalty; and the third item is the translation score, which is decomposed into a product of feature values of rules: ∏ s(d|Et ) = f (r∈P T T )f (r∈T RS )f (r∈P RS ). 4.2 Decoding algorithms In our translation models, we have made use of three kinds of translation rule sets which are trained separately. We perform derivation-level combination as described in (Liu et al., 2009b) for mixing different types of translation rules within one derivation. For tree-to-string translation, we use a bottomup beam search algorithm (Liu et al., 2006) for decoding an HPSG tree Et . We keep at most 10 best derivations with distinct τ (d)s at each node. Recall the definition of minimum covering tree, which supports a faster way to retrieve available rules from PRS without generating all the subtrees. That is, when node n fortunately to be the root of some minimum covering tree(s), we use the tree(s) to seek available PAS-based rules in PRS. We keep a hash-table with the key to be"
P10-1034,N06-1033,0,0.0309826,"e rule sets together in one decoder because PTT provides excellent rule coverages while TRS and PRS offer linguistically motivated phrase selections and nonlocal reorderings. Each f (r) is in turn a product of five features: f (r) = p(s|t)λ3 · p(t|s)λ4 · l(s|t)λ5 · l(t|s)λ6 · eλ7 . Here, s/t represent the source/target part of a rule in PTT, TRS, or PRS; p(·|·) and l(·|·) are translation probabilities and lexical weights of rules from PTT, TRS, and PRS. The derivation length penalty is controlled by λ7 . In our string-to-tree model, for efficient decoding with integrated n-gram LM, we follow (Zhang et al., 2006) and inversely binarize all translation rules into Chomsky Normal Forms that contain at most two variables and can be incrementally scored by LM. In order to make use of the binarized rules in the CKY decoding, we add two kinds of glues rules: S → Xm (1) , Xm (1) ; S → S (1) Xm (2) , S (1) Xm (2) . Here Xm ranges over the nonterminals appearing in a binarized rule set. These glue rules can be seen as an extension from X to {Xm }of the two glue rules described in (Chiang, 2007). 331 # of sentences # of Jp words # of En words Train 994K 28.2M 24.7M Dev. 2K 57.4K 50.3K Test 2K 57.1K 49.9K tree no"
P10-1034,D08-1022,0,0.449701,"o obtain a fine-grained translation rule set. We name the information such as the voice of a verb in a tree fragment as deep syntactic information. We use a head-driven phrase structure grammar (HPSG) parser to obtain the Introduction Tree-to-string translation rules are generic and applicable to numerous linguistically syntax-based Statistical Machine Translation (SMT) systems, such as string-to-tree translation (Galley et al., 2004; Galley et al., 2006; Chiang et al., 2009), tree-to-string translation (Liu et al., 2006; Huang et al., 2006), and forest-to-string translation (Mi et al., 2008; Mi and Huang, 2008). The algorithms proposed by Galley et al. (2004; 2006) are frequently used for extracting minimal and composed rules from aligned 1-best tree-string pairs. Dealing with the parse error problem and rule sparseness problem, Mi and Huang (2008) replaced the 1-best parse tree with a packed forest which compactly encodes exponentially many parses for treeto-string rule extraction. However, current tree-to-string rules only make use of Probabilistic Context-Free Grammar tree fragments, in which part-of-speech (POS) or 1 For example, “John has killed Mary.” versus “John was killed by Mary.” 325 Proc"
P10-1034,P08-1023,0,0.115331,"tic information to obtain a fine-grained translation rule set. We name the information such as the voice of a verb in a tree fragment as deep syntactic information. We use a head-driven phrase structure grammar (HPSG) parser to obtain the Introduction Tree-to-string translation rules are generic and applicable to numerous linguistically syntax-based Statistical Machine Translation (SMT) systems, such as string-to-tree translation (Galley et al., 2004; Galley et al., 2006; Chiang et al., 2009), tree-to-string translation (Liu et al., 2006; Huang et al., 2006), and forest-to-string translation (Mi et al., 2008; Mi and Huang, 2008). The algorithms proposed by Galley et al. (2004; 2006) are frequently used for extracting minimal and composed rules from aligned 1-best tree-string pairs. Dealing with the parse error problem and rule sparseness problem, Mi and Huang (2008) replaced the 1-best parse tree with a packed forest which compactly encodes exponentially many parses for treeto-string rule extraction. However, current tree-to-string rules only make use of Probabilistic Context-Free Grammar tree fragments, in which part-of-speech (POS) or 1 For example, “John has killed Mary.” versus “John was kill"
P10-1034,P06-1077,0,\N,Missing
P11-1003,P05-1033,0,0.60377,"y generate target function words during decoding. Extensive experiments involving large-scale English-toJapanese translation revealed a significant improvement of 1.8 points in BLEU score, as compared with a strong forest-to-string baseline system. 1 Introduction Rule generalization remains a key challenge for current syntax-based statistical machine translation (SMT) systems. On the one hand, there is a tendency to integrate richer syntactic information into a translation rule in order to better express the translation phenomena. Thus, flat phrases (Koehn et al., 2003), hierarchical phrases (Chiang, 2005), and syntactic tree fragments (Galley et al., 2006; Mi and Huang, 2008; Wu et al., 2010) are gradually used in SMT. On the other hand, the use of syntactic phrases continues due to the requirement for phrase coverage in most syntax-based systems. For example, 22 Mi et al. (2008) achieved a 3.1-point improvement in BLEU score (Papineni et al., 2002) by including bilingual syntactic phrases in their forest-based system. Compared with flat phrases, syntactic rules are good at capturing global reordering, which has been reported to be essential for translating between languages with substantial s"
P11-1003,P10-1146,0,0.0878591,"sh and Japanese, which is a subject-objectverb language (Xu et al., 2009). Forest-based translation frameworks, which make use of packed parse forests on the source and/or target language side(s), are an increasingly promising approach to syntax-based SMT, being both algorithmically appealing (Mi et al., 2008) and empirically successful (Mi and Huang, 2008; Liu et al., 2009). However, forest-based translation systems, and, in general, most linguistically syntax-based SMT systems (Galley et al., 2004; Galley et al., 2006; Liu et al., 2006; Zhang et al., 2007; Mi et al., 2008; Liu et al., 2009; Chiang, 2010), are built upon word aligned parallel sentences and thus share a critical dependence on word alignments. For example, even a single spurious word alignment can invalidate a large number of otherwise extractable rules, and unaligned words can result in an exponentially large set of extractable rules for the interpretation of these unaligned words (Galley et al., 2006). What makes word alignment so fragile? In order to investigate this problem, we manually analyzed the alignments of the first 100 parallel sentences in our English-Japanese training data (to be shown in Table 2). The alignments w"
P11-1003,N04-1035,0,0.767859,"n reported to be essential for translating between languages with substantial structural differences, such as English and Japanese, which is a subject-objectverb language (Xu et al., 2009). Forest-based translation frameworks, which make use of packed parse forests on the source and/or target language side(s), are an increasingly promising approach to syntax-based SMT, being both algorithmically appealing (Mi et al., 2008) and empirically successful (Mi and Huang, 2008; Liu et al., 2009). However, forest-based translation systems, and, in general, most linguistically syntax-based SMT systems (Galley et al., 2004; Galley et al., 2006; Liu et al., 2006; Zhang et al., 2007; Mi et al., 2008; Liu et al., 2009; Chiang, 2010), are built upon word aligned parallel sentences and thus share a critical dependence on word alignments. For example, even a single spurious word alignment can invalidate a large number of otherwise extractable rules, and unaligned words can result in an exponentially large set of extractable rules for the interpretation of these unaligned words (Galley et al., 2006). What makes word alignment so fragile? In order to investigate this problem, we manually analyzed the alignments of the"
P11-1003,P06-1121,0,0.104629,"ing. Extensive experiments involving large-scale English-toJapanese translation revealed a significant improvement of 1.8 points in BLEU score, as compared with a strong forest-to-string baseline system. 1 Introduction Rule generalization remains a key challenge for current syntax-based statistical machine translation (SMT) systems. On the one hand, there is a tendency to integrate richer syntactic information into a translation rule in order to better express the translation phenomena. Thus, flat phrases (Koehn et al., 2003), hierarchical phrases (Chiang, 2005), and syntactic tree fragments (Galley et al., 2006; Mi and Huang, 2008; Wu et al., 2010) are gradually used in SMT. On the other hand, the use of syntactic phrases continues due to the requirement for phrase coverage in most syntax-based systems. For example, 22 Mi et al. (2008) achieved a 3.1-point improvement in BLEU score (Papineni et al., 2002) by including bilingual syntactic phrases in their forest-based system. Compared with flat phrases, syntactic rules are good at capturing global reordering, which has been reported to be essential for translating between languages with substantial structural differences, such as English and Japanese"
P11-1003,W05-1506,0,0.0758848,"Missing"
P11-1003,N03-1017,0,0.02613,"xponential number of parse trees to properly generate target function words during decoding. Extensive experiments involving large-scale English-toJapanese translation revealed a significant improvement of 1.8 points in BLEU score, as compared with a strong forest-to-string baseline system. 1 Introduction Rule generalization remains a key challenge for current syntax-based statistical machine translation (SMT) systems. On the one hand, there is a tendency to integrate richer syntactic information into a translation rule in order to better express the translation phenomena. Thus, flat phrases (Koehn et al., 2003), hierarchical phrases (Chiang, 2005), and syntactic tree fragments (Galley et al., 2006; Mi and Huang, 2008; Wu et al., 2010) are gradually used in SMT. On the other hand, the use of syntactic phrases continues due to the requirement for phrase coverage in most syntax-based systems. For example, 22 Mi et al. (2008) achieved a 3.1-point improvement in BLEU score (Papineni et al., 2002) by including bilingual syntactic phrases in their forest-based system. Compared with flat phrases, syntactic rules are good at capturing global reordering, which has been reported to be essential for translating"
P11-1003,P07-2045,0,0.00850188,"orest 96.52 91.36 93.55 42.1 52.2 1.22 1.23 15.7 27.07 Min-F Y A′ forest 144.91 92.98 72.98 26.3 58.6 11.2 1.37 1.37 22.4 27.93 C3-F Y A′ forest 228.59 162.71 120.08 18.6 130.7 29.0 2.18 2.15 35.4 28.89 Table 3: Statistics and translation results for four types of tree-to-string rules. With the exception of ‘# nodes/tree’, the numbers in the table are in millions and the time is in hours. Here, fw denotes function word, and DT denotes the decoding time, and the BLEU scores were computed on the test set. We performed GIZA++ (Och and Ney, 2003) and the grow-diag-final-and symmetrizing strategy (Koehn et al., 2007) on the training set to obtain alignments. The SRI Language Modeling Toolkit (Stolcke, 2002) was employed to train a five-gram Japanese LM on the training set. We evaluated the translation quality using the BLEU-4 metric (Papineni et al., 2002). Joshua v1.3 (Li et al., 2009), which is a freely available decoder for hierarchical phrasebased SMT (Chiang, 2005), is used as an external baseline system for comparison. We extracted 4.5M translation rules from the training set for the 4K English sentences in the development and test sets. We used the default configuration of Joshua, with the exceptio"
P11-1003,W02-2016,0,0.0394124,"nstead of a 1best tree, as in the case of (Galley et al., 2006). Multiple interpretations of unaligned function words for an aligned tree-string pair result in a derivation forest. Now, we have a packed parse forest in which each tree corresponds to a derivation forest. Thus, pruning free attachments of function words is practically important in order to extract composed rules from this “(derivation) forest of (parse) forest”. In the English-to-Japanese translation test case of the present study, the target chunk set is yielded by a state-of-the-art Japanese dependency parser, Cabocha v0.535 (Kudo and Matsumoto, 2002). The output of Cabocha is a list of chunks. A chunk contains roughly one content word (usually the head) and affixed function words, such as case markers (e.g., ga) and verbal morphemes (e.g., sa re ta, which indicate past tense and passive voice). For example, the Japanese sentence in Figure 1 is separated into four chunks, and the dependencies among these chunks are identified by arrows. These arrows point out the head chunk that the current chunk modifies. Moreover, we also hope to gain a fine-grained alignment among these syntactic chunks and source tree fragments. Thereby, during decodin"
P11-1003,P09-4007,0,0.0147167,"g rules. With the exception of ‘# nodes/tree’, the numbers in the table are in millions and the time is in hours. Here, fw denotes function word, and DT denotes the decoding time, and the BLEU scores were computed on the test set. We performed GIZA++ (Och and Ney, 2003) and the grow-diag-final-and symmetrizing strategy (Koehn et al., 2007) on the training set to obtain alignments. The SRI Language Modeling Toolkit (Stolcke, 2002) was employed to train a five-gram Japanese LM on the training set. We evaluated the translation quality using the BLEU-4 metric (Papineni et al., 2002). Joshua v1.3 (Li et al., 2009), which is a freely available decoder for hierarchical phrasebased SMT (Chiang, 2005), is used as an external baseline system for comparison. We extracted 4.5M translation rules from the training set for the 4K English sentences in the development and test sets. We used the default configuration of Joshua, with the exception of the maximum number of items/rules, and the value of k (of the k-best outputs) is set to be 200. 4.2 Results Table 3 lists the statistics of the following translation rule sets: • C3-T: a composed rule set extracted from the derivation forests of 1-best HPSG trees that w"
P11-1003,P09-1063,0,0.0938612,"Missing"
P11-1003,D07-1038,0,0.123961,"nd ‘the’, which may appear anywhere in an English sentence. Following these problematic alignments, we are forced to make use of relatively large English tree fragments to construct translation rules that tend to be ill-formed and less generalized. This is the motivation of the present approach of re-aligning the target function words to source tree fragments, so that the influence of incorrect alignments is reduced and the function words can be generated by tree fragments on the fly. However, the current dominant research only uses 1-best trees for syntactic realignment (Galley et al., 2006; May and Knight, 2007; Wang et al., 2010), which adversely affects the rule set quality due to parsing errors. Therefore, we realign target function words to a packed forest that compactly encodes exponentially many parses. Given aligned forest-string pairs, we extract composed tree-to-string translation rules that account for multiple interpretations of both aligned and unaligned target function words. In order to constrain the exhaustive attachments of function words, we further limit the function words to bind to their surrounding chunks yielded by a dependency parser. Using the composed rules of the present st"
P11-1003,D08-1022,0,0.21945,"ments involving large-scale English-toJapanese translation revealed a significant improvement of 1.8 points in BLEU score, as compared with a strong forest-to-string baseline system. 1 Introduction Rule generalization remains a key challenge for current syntax-based statistical machine translation (SMT) systems. On the one hand, there is a tendency to integrate richer syntactic information into a translation rule in order to better express the translation phenomena. Thus, flat phrases (Koehn et al., 2003), hierarchical phrases (Chiang, 2005), and syntactic tree fragments (Galley et al., 2006; Mi and Huang, 2008; Wu et al., 2010) are gradually used in SMT. On the other hand, the use of syntactic phrases continues due to the requirement for phrase coverage in most syntax-based systems. For example, 22 Mi et al. (2008) achieved a 3.1-point improvement in BLEU score (Papineni et al., 2002) by including bilingual syntactic phrases in their forest-based system. Compared with flat phrases, syntactic rules are good at capturing global reordering, which has been reported to be essential for translating between languages with substantial structural differences, such as English and Japanese, which is a subject"
P11-1003,P10-1145,0,0.259825,"Missing"
P11-1003,P08-1023,0,0.418907,"zation remains a key challenge for current syntax-based statistical machine translation (SMT) systems. On the one hand, there is a tendency to integrate richer syntactic information into a translation rule in order to better express the translation phenomena. Thus, flat phrases (Koehn et al., 2003), hierarchical phrases (Chiang, 2005), and syntactic tree fragments (Galley et al., 2006; Mi and Huang, 2008; Wu et al., 2010) are gradually used in SMT. On the other hand, the use of syntactic phrases continues due to the requirement for phrase coverage in most syntax-based systems. For example, 22 Mi et al. (2008) achieved a 3.1-point improvement in BLEU score (Papineni et al., 2002) by including bilingual syntactic phrases in their forest-based system. Compared with flat phrases, syntactic rules are good at capturing global reordering, which has been reported to be essential for translating between languages with substantial structural differences, such as English and Japanese, which is a subject-objectverb language (Xu et al., 2009). Forest-based translation frameworks, which make use of packed parse forests on the source and/or target language side(s), are an increasingly promising approach to synta"
P11-1003,J08-1002,1,0.822257,"binding particles, conjunctive particles, and phrasal particles. Japanese grammar also uses auxiliary verbs to give further semantic or syntactic information about the preceding main or full verb. Alike English, the extra meaning provided by a Japanese auxiliary verb alters the basic meaning of the main verb so that the main verb has one or more of the following functions: passive voice, progressive aspect, perfect aspect, modality, dummy, or emphasis. 2.2 HPSG forests Following our precious work (Wu et al., 2010), we use head-drive phrase structure grammar (HPSG) forests generated by Enju2 (Miyao and Tsujii, 2008), which is a state-of-the-art HPSG parser for English. HPSG (Pollard and Sag, 1994; Sag et al., 2003) is a lexicalist grammar framework. In HPSG, linguistic entities such as words and phrases are represented by a data structure called a sign. A sign gives a factored representation of the syntactic features of a word/phrase, as well as a representation of their semantic content. Phrases and words represented by signs are collected into larger phrases by the applications of schemata. The semantic representation of the new phrase is calculated at the same time. As such, an HPSG parse forest can b"
P11-1003,J03-1002,0,0.00515132,"ge of 82.3 trees in a parse forest. 6 http://www.jst.go.jp M&H-F N A forest 96.52 91.36 93.55 42.1 52.2 1.22 1.23 15.7 27.07 Min-F Y A′ forest 144.91 92.98 72.98 26.3 58.6 11.2 1.37 1.37 22.4 27.93 C3-F Y A′ forest 228.59 162.71 120.08 18.6 130.7 29.0 2.18 2.15 35.4 28.89 Table 3: Statistics and translation results for four types of tree-to-string rules. With the exception of ‘# nodes/tree’, the numbers in the table are in millions and the time is in hours. Here, fw denotes function word, and DT denotes the decoding time, and the BLEU scores were computed on the test set. We performed GIZA++ (Och and Ney, 2003) and the grow-diag-final-and symmetrizing strategy (Koehn et al., 2007) on the training set to obtain alignments. The SRI Language Modeling Toolkit (Stolcke, 2002) was employed to train a five-gram Japanese LM on the training set. We evaluated the translation quality using the BLEU-4 metric (Papineni et al., 2002). Joshua v1.3 (Li et al., 2009), which is a freely available decoder for hierarchical phrasebased SMT (Chiang, 2005), is used as an external baseline system for comparison. We extracted 4.5M translation rules from the training set for the 4K English sentences in the development and te"
P11-1003,P02-1040,0,0.0817636,"al machine translation (SMT) systems. On the one hand, there is a tendency to integrate richer syntactic information into a translation rule in order to better express the translation phenomena. Thus, flat phrases (Koehn et al., 2003), hierarchical phrases (Chiang, 2005), and syntactic tree fragments (Galley et al., 2006; Mi and Huang, 2008; Wu et al., 2010) are gradually used in SMT. On the other hand, the use of syntactic phrases continues due to the requirement for phrase coverage in most syntax-based systems. For example, 22 Mi et al. (2008) achieved a 3.1-point improvement in BLEU score (Papineni et al., 2002) by including bilingual syntactic phrases in their forest-based system. Compared with flat phrases, syntactic rules are good at capturing global reordering, which has been reported to be essential for translating between languages with substantial structural differences, such as English and Japanese, which is a subject-objectverb language (Xu et al., 2009). Forest-based translation frameworks, which make use of packed parse forests on the source and/or target language side(s), are an increasingly promising approach to syntax-based SMT, being both algorithmically appealing (Mi et al., 2008) and"
P11-1003,2007.mtsummit-papers.63,0,0.299106,"Missing"
P11-1003,J10-2004,0,0.249488,"pear anywhere in an English sentence. Following these problematic alignments, we are forced to make use of relatively large English tree fragments to construct translation rules that tend to be ill-formed and less generalized. This is the motivation of the present approach of re-aligning the target function words to source tree fragments, so that the influence of incorrect alignments is reduced and the function words can be generated by tree fragments on the fly. However, the current dominant research only uses 1-best trees for syntactic realignment (Galley et al., 2006; May and Knight, 2007; Wang et al., 2010), which adversely affects the rule set quality due to parsing errors. Therefore, we realign target function words to a packed forest that compactly encodes exponentially many parses. Given aligned forest-string pairs, we extract composed tree-to-string translation rules that account for multiple interpretations of both aligned and unaligned target function words. In order to constrain the exhaustive attachments of function words, we further limit the function words to bind to their surrounding chunks yielded by a dependency parser. Using the composed rules of the present study in a baseline fo"
P11-1003,P10-1034,1,0.916471,"e-scale English-toJapanese translation revealed a significant improvement of 1.8 points in BLEU score, as compared with a strong forest-to-string baseline system. 1 Introduction Rule generalization remains a key challenge for current syntax-based statistical machine translation (SMT) systems. On the one hand, there is a tendency to integrate richer syntactic information into a translation rule in order to better express the translation phenomena. Thus, flat phrases (Koehn et al., 2003), hierarchical phrases (Chiang, 2005), and syntactic tree fragments (Galley et al., 2006; Mi and Huang, 2008; Wu et al., 2010) are gradually used in SMT. On the other hand, the use of syntactic phrases continues due to the requirement for phrase coverage in most syntax-based systems. For example, 22 Mi et al. (2008) achieved a 3.1-point improvement in BLEU score (Papineni et al., 2002) by including bilingual syntactic phrases in their forest-based system. Compared with flat phrases, syntactic rules are good at capturing global reordering, which has been reported to be essential for translating between languages with substantial structural differences, such as English and Japanese, which is a subject-objectverb langua"
P11-1003,N09-1028,0,0.0420268,"gradually used in SMT. On the other hand, the use of syntactic phrases continues due to the requirement for phrase coverage in most syntax-based systems. For example, 22 Mi et al. (2008) achieved a 3.1-point improvement in BLEU score (Papineni et al., 2002) by including bilingual syntactic phrases in their forest-based system. Compared with flat phrases, syntactic rules are good at capturing global reordering, which has been reported to be essential for translating between languages with substantial structural differences, such as English and Japanese, which is a subject-objectverb language (Xu et al., 2009). Forest-based translation frameworks, which make use of packed parse forests on the source and/or target language side(s), are an increasingly promising approach to syntax-based SMT, being both algorithmically appealing (Mi et al., 2008) and empirically successful (Mi and Huang, 2008; Liu et al., 2009). However, forest-based translation systems, and, in general, most linguistically syntax-based SMT systems (Galley et al., 2004; Galley et al., 2006; Liu et al., 2006; Zhang et al., 2007; Mi et al., 2008; Liu et al., 2009; Chiang, 2010), are built upon word aligned parallel sentences and thus sh"
P11-1003,2007.mtsummit-papers.71,0,0.0466022,"with substantial structural differences, such as English and Japanese, which is a subject-objectverb language (Xu et al., 2009). Forest-based translation frameworks, which make use of packed parse forests on the source and/or target language side(s), are an increasingly promising approach to syntax-based SMT, being both algorithmically appealing (Mi et al., 2008) and empirically successful (Mi and Huang, 2008; Liu et al., 2009). However, forest-based translation systems, and, in general, most linguistically syntax-based SMT systems (Galley et al., 2004; Galley et al., 2006; Liu et al., 2006; Zhang et al., 2007; Mi et al., 2008; Liu et al., 2009; Chiang, 2010), are built upon word aligned parallel sentences and thus share a critical dependence on word alignments. For example, even a single spurious word alignment can invalidate a large number of otherwise extractable rules, and unaligned words can result in an exponentially large set of extractable rules for the interpretation of these unaligned words (Galley et al., 2006). What makes word alignment so fragile? In order to investigate this problem, we manually analyzed the alignments of the first 100 parallel sentences in our English-Japanese traini"
P11-1003,P06-1077,0,\N,Missing
P12-1110,D07-1022,0,0.0208608,"Missing"
P12-1110,P04-1015,0,0.256701,"ark (2010), the POS tag is assigned to the word when its first character is shifted, and the word–tag pairs observed in the training data and the closed-set tags (Xia, 2000) are used to prune (1) All subtrees spanning M consecutive characters unlikely derivations. Because 33 tags are defined in have the same index 2M − 1. the CTB tag set (Xia, 2000), our model exploits a (2) All terminal states have the same step index 2N total of 36 actions. (including the root arc), where N is the number To train the model, we use the averaged percepof characters in the sentence. tron with the early update (Collins and Roark, 2004). (3) Every action increases the index. In our joint model, the early update is invoked by Note that the number of shifted characters is also mistakes in any of word segmentation, POS tagging, necessary to meet condition (3). Otherwise, it allows or dependency parsing. an unlimited number of SH(t) actions without incrementing the step index. Figure 1 portrays how the 3.2 Alignment of States states are aligned using the proposed scheme, where When dependency parsing is integrated into the task a subtree is denoted as a rectangle with its partial of joint word segmentation and POS tagging, it is"
P12-1110,D07-1098,0,0.0272609,"st setting we found is σp = 0.5: this result suggests that we probably should resolve remaining errors by preferentially using the local n-gram based features at the early stage of training. Otherwise, the premature incorporation of the non-local syntactic dependencies might engender overfitting to the training data. 4 4.1 Experiment Experimental Settings We use the Chinese Penn Treebank ver. 5.1, 6.0, and 7.0 (hereinafter CTB-5, CTB-6, and CTB-7) for evaluation. These corpora are split into training, development, and test sets, according to previous works. For CTB-5, we refer to the split by Duan et al. (2007) as CTB-5d, and to the split by Jiang et al. (2008) as CTB-5j. We also prepare a dataset for cross validation: the dataset CTB-5c consists of sentences from CTB-5 excluding the development and test sets of CTB-5d and CTB-5j. We split CTB5c into five sets (CTB-5c-n), and alternatively use four of these as the training set and the rest as the test set. CTB-6 is split according to the official split described in the documentation, and CTB-7 is split according to Wang et al. (2011). The statistics of these splits are shown in Table 2. As external dictionaries, we use the HowNet Word List3 , consis"
P12-1110,P08-1043,0,0.0320765,"Missing"
P12-1110,I11-1136,1,0.801089,"their model is practically ten times as fast as their original model. To incorporate the word-level features into the character-based decoder, the features are decomposed into substring-level features, which are effective for incomplete words to have comparable scores to complete words in the beam. Because we found that even an incremental approach with beam search is intractable if we perform the wordbased decoding, we take a character-based approach to produce our joint model. The incremental framework of our model is based on the joint POS tagging and dependency parsing model for Chinese (Hatori et al., 2011), which is an extension of the shift-reduce dependency parser with dynamic programming (Huang and Sagae, 2010). They specifically modified the shift action so that it assigns the POS tag when a word is shifted onto the stack. However, because they regarded word segmentation as given, their model did not consider the interaction between segmentation and POS tagging. 3 Model 3.1 Incremental Joint Segmentation, POS Tagging, and Dependency Parsing Based on the joint POS tagging and dependency parsing model by Hatori et al. (2011), we build our joint model to solve word segmentation, POS tagging, a"
P12-1110,P10-1110,0,0.192797,"ese characters causes numerous oversegmentation errors for OOV words. Based on these observations, we aim at building a joint model that simultaneously processes word segmentation, POS tagging, and dependency parsing, trying to capture global interaction among 1045 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 1045–1053, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics these three tasks. To handle the increased computational complexity, we adopt the incremental parsing framework with dynamic programming (Huang and Sagae, 2010), and propose an efficient method of character-based decoding over candidate structures. Two major challenges exist in formalizing the joint segmentation and dependency parsing task in the character-based incremental framework. First, we must address the problem of how to align comparable states effectively in the beam. Because the number of dependency arcs varies depending on how words are segmented, we devise a step alignment scheme using the number of character-based arcs, which enables effective joint decoding for the three tasks. Second, although the feature set is fundamentally a combina"
P12-1110,P08-1102,0,0.0101226,"sts that we probably should resolve remaining errors by preferentially using the local n-gram based features at the early stage of training. Otherwise, the premature incorporation of the non-local syntactic dependencies might engender overfitting to the training data. 4 4.1 Experiment Experimental Settings We use the Chinese Penn Treebank ver. 5.1, 6.0, and 7.0 (hereinafter CTB-5, CTB-6, and CTB-7) for evaluation. These corpora are split into training, development, and test sets, according to previous works. For CTB-5, we refer to the split by Duan et al. (2007) as CTB-5d, and to the split by Jiang et al. (2008) as CTB-5j. We also prepare a dataset for cross validation: the dataset CTB-5c consists of sentences from CTB-5 excluding the development and test sets of CTB-5d and CTB-5j. We split CTB5c into five sets (CTB-5c-n), and alternatively use four of these as the training set and the rest as the test set. CTB-6 is split according to the official split described in the documentation, and CTB-7 is split according to Wang et al. (2011). The statistics of these splits are shown in Table 2. As external dictionaries, we use the HowNet Word List3 , consisting of 91,015 words, and page names from the Chine"
P12-1110,P09-1058,0,0.0291533,"to peace-operation-related groups. 1 Introduction In processing natural languages that do not include delimiters (e.g. spaces) between words, word segmentation is the crucial first step that is necessary to perform virtually all NLP tasks. Furthermore, the word-level information is often augmented with the POS tags, which, along with segmentation, form the basic foundation of statistical NLP. Because the tasks of word segmentation and POS tagging have strong interactions, many studies have been devoted to the task of joint word segmentation and POS tagging for languages such as Chinese (e.g. Kruengkrai et al. (2009)). This is because some of the segmentation ambiguities cannot be resolved without considering the surrounding grammatical constructions encoded in a sequence of POS tags. The joint approach to word segmentation and POS tagging has been reported to improve word segmentation and POS tagging accuracies by more than âS the only difference is the existence of the last word ; however, whether or not this word exists changes the whole syntactic structure and segmentation of the sentence. This is an example in which word segmentation cannot be handled properly without considering long-range syntactic"
P12-1110,D11-1109,0,0.170218,"Missing"
P12-1110,W03-1025,0,0.220404,"Missing"
P12-1110,P94-1010,0,0.471525,"Missing"
P12-1110,P11-1139,0,0.0620577,"Missing"
P12-1110,I11-1035,0,0.116556,"are split into training, development, and test sets, according to previous works. For CTB-5, we refer to the split by Duan et al. (2007) as CTB-5d, and to the split by Jiang et al. (2008) as CTB-5j. We also prepare a dataset for cross validation: the dataset CTB-5c consists of sentences from CTB-5 excluding the development and test sets of CTB-5d and CTB-5j. We split CTB5c into five sets (CTB-5c-n), and alternatively use four of these as the training set and the rest as the test set. CTB-6 is split according to the official split described in the documentation, and CTB-7 is split according to Wang et al. (2011). The statistics of these splits are shown in Table 2. As external dictionaries, we use the HowNet Word List3 , consisting of 91,015 words, and page names from the Chinese Wikipedia4 as of Oct 26, 2011, consisting of 709,352 words. These dictionaries only consist of word forms with no frequency or POS information. We use standard measures of word-level precision, recall, and F1 score, for evaluating each task. The output of dependencies cannot be correct unless the syntactic head and dependent of the dependency relation are both segmented correctly. Following the standard setting in dependency"
P12-1110,P08-1101,0,0.0702245,"an existing morphological analyzer. In addition, the lattice does not include word segmentation ambiguities crossing boundaries of space-delimited tokens. In contrast, because the Chinese language does not have spaces between words, we fundamentally need to consider the lattice structure of the whole sentence. Therefore, we place no restriction on the segmentation possibilities to consider, and we assess the full potential of the joint segmentation and dependency parsing model. Among the many recent works on joint segmentation and POS tagging for Chinese, the linear-time incremental models by Zhang and Clark (2008) and Zhang and Clark (2010) largely inspired our model. Zhang and Clark (2008) proposed an incremental joint segmentation and POS tagging model, with an effective feature set for Chinese. However, it requires to computationally expensive multiple beams to compare words of different lengths using beam search. More recently, Zhang and Clark (2010) proposed an efficient character-based decoder for their word-based model. In their new model, a single beam suffices for decoding; hence, they reported that their model is practically ten times as fast as their original model. To incorporate the word-l"
P12-1110,D10-1082,0,0.120486,"haracter-based decoding over candidate structures. Two major challenges exist in formalizing the joint segmentation and dependency parsing task in the character-based incremental framework. First, we must address the problem of how to align comparable states effectively in the beam. Because the number of dependency arcs varies depending on how words are segmented, we devise a step alignment scheme using the number of character-based arcs, which enables effective joint decoding for the three tasks. Second, although the feature set is fundamentally a combination of those used in previous works (Zhang and Clark, 2010; Huang and Sagae, 2010), to integrate them in a single incremental framework is not straightforward. Because we must perform decisions of three kinds (segmentation, tagging, and parsing) in an incremental framework, we must adjust which features are to be activated when, and how they are combined with which action labels. We have also found that we must balance the learning rate between features for segmentation and tagging decisions, and those for dependency parsing. We perform experiments using the Chinese Treebank (CTB) corpora, demonstrating that the accuracies of the three tasks can be i"
P12-1110,P11-2033,0,0.0886955,"Missing"
P12-1110,J96-3004,0,\N,Missing
P12-3022,J07-2003,0,0.0770155,"uracies of Akamon and the baseline systems on the NTCIR-9 English-to-Japanese translation task (Wu et al., 2011b). * stands for only using 2 million parallel sentences of the total 3 million data. Here, HPSG forests were used in Akamon. SRILM • language models: Akamon can make use of one or many n-gram language models trained by using SRILM5 (Stolcke, 2002) or the Berkeley language model toolkit, berkeleylm-1.0b36 (Pauls and Klein, 2011). The weights of multiple language models are tuned under minimum error rate training (MERT) (Och, 2003). • pruning: traditional beam-pruning and cubepruning (Chiang, 2007) techniques are incorporated in Akamon to make decoding feasible for large-scale rule sets. Before decoding, we also perform the marginal probability-based inside-outside algorithm based pruning (Mi et al., 2008) on the original parsing forest to control the decoding time. • MERT: Akamon has its own MERT module which optimizes weights of the features so as to maximize some automatic evaluation metric, such as BLEU (Papineni et al., 2002), on a development set. 5 6 http://www.speech.sri.com/projects/srilm/ http://code.google.com/p/berkeleylm/ 129 dev f.seg.lw clean dev.e tokenize e.clean f.clea"
P12-3022,P05-1067,0,0.201691,"g PCFG trees (Galley et al., 2004) and HPSG trees/forests (Wu et al., 2010). 1 Introduction Syntax-based statistical machine translation (SMT) systems have achieved promising improvements in recent years. Depending on the type of input, the systems are divided into two categories: stringbased systems whose input is a string to be simultaneously parsed and translated by a synchronous grammar (Wu, 1997; Chiang, 2005; Galley et al., 2006; Shen et al., 2008), and tree/forest-based systems whose input is already a parse tree or a packed forest to be directly converted into a target tree or string (Ding and Palmer, 2005; Quirk et al., 2005; Liu et al., 2006; Huang et al., 2006; Mi et al., 2008; Mi and Huang, 2008; Zhang et al., 2009; Wu et al., 2010; Wu et al., 2011a). ∗ Work done when all the authors were in The University of Tokyo. Depending on whether or not parsers are explicitly used for obtaining linguistically annotated data during training, the systems are also divided into two categories: formally syntax-based systems that do not use additional parsers (Wu, 1997; Chiang, 2005; Xiong et al., 2006), and linguistically syntax-based systems that use PCFG parsers (Liu et al., 2006; Huang et al., 2006; Ga"
P12-3022,I11-1153,1,0.853242,"penal2 3 izes word order mistakes. In this table, Akamon-Forest differs from Akamon-Comb by using different configurations: Akamon-Forest used only 2/3 of the total training data (limited by the experiment environments and time). Akamon-Comb represents the system combination result by combining Akamon-Forest and other phrase-based SMT systems, which made use of pre-ordering methods of head finalization as described in (Isozaki et al., 2010b) and used the total 3 million training data. The detail of the pre-ordering approach and the combination method can be found in (Sudoh et al., 2011) and (Duh et al., 2011). Also, Moses (hierarchical) stands for the hierarchical phrase-based SMT system and Moses (phrase) stands for the flat phrase-based SMT system. For intuitive comparison (note that the result achieved by Google is only for reference and not a comparison, since it uses a different and unknown training data) and following (Goto et al., 2011), the scores achieved by using the Google online translation system4 are also listed in this table. Here is a brief description of Akamon’s main features: Code available at https://sites.google.com/site/xianchaowu2012 Code available at http://www.kecl.ntt.co."
P12-3022,N04-1035,0,0.544876,"ft.com Abstract We describe Akamon, an open source toolkit for tree and forest-based statistical machine translation (Liu et al., 2006; Mi et al., 2008; Mi and Huang, 2008). Akamon implements all of the algorithms required for tree/forestto-string decoding using tree-to-string translation rules: multiple-thread forest-based decoding, n-gram language model integration, beam- and cube-pruning, k-best hypotheses extraction, and minimum error rate training. In terms of tree-to-string translation rule extraction, the toolkit implements the traditional maximum likelihood algorithm using PCFG trees (Galley et al., 2004) and HPSG trees/forests (Wu et al., 2010). 1 Introduction Syntax-based statistical machine translation (SMT) systems have achieved promising improvements in recent years. Depending on the type of input, the systems are divided into two categories: stringbased systems whose input is a string to be simultaneously parsed and translated by a synchronous grammar (Wu, 1997; Chiang, 2005; Galley et al., 2006; Shen et al., 2008), and tree/forest-based systems whose input is already a parse tree or a packed forest to be directly converted into a target tree or string (Ding and Palmer, 2005; Quirk et al"
P12-3022,P06-1121,0,0.42834,"otheses extraction, and minimum error rate training. In terms of tree-to-string translation rule extraction, the toolkit implements the traditional maximum likelihood algorithm using PCFG trees (Galley et al., 2004) and HPSG trees/forests (Wu et al., 2010). 1 Introduction Syntax-based statistical machine translation (SMT) systems have achieved promising improvements in recent years. Depending on the type of input, the systems are divided into two categories: stringbased systems whose input is a string to be simultaneously parsed and translated by a synchronous grammar (Wu, 1997; Chiang, 2005; Galley et al., 2006; Shen et al., 2008), and tree/forest-based systems whose input is already a parse tree or a packed forest to be directly converted into a target tree or string (Ding and Palmer, 2005; Quirk et al., 2005; Liu et al., 2006; Huang et al., 2006; Mi et al., 2008; Mi and Huang, 2008; Zhang et al., 2009; Wu et al., 2010; Wu et al., 2011a). ∗ Work done when all the authors were in The University of Tokyo. Depending on whether or not parsers are explicitly used for obtaining linguistically annotated data during training, the systems are also divided into two categories: formally syntax-based systems t"
P12-3022,W05-1506,0,0.0386043,"ch optimizes weights of the features so as to maximize some automatic evaluation metric, such as BLEU (Papineni et al., 2002), on a development set. 5 6 http://www.speech.sri.com/projects/srilm/ http://code.google.com/p/berkeleylm/ 129 dev f.seg.lw clean dev.e tokenize e.clean f.clean dev.f word segmentation e.tok Enju i.e., first construct a translation forest by applying the tree-to-string translation rules to the original parsing forest of the source sentence, and then collect k-best hypotheses for the root node(s) of the translation forest using Algorithm 2 or Algorithm 3 as described in (Huang and Chiang, 2005). Later, the k-best hypotheses are used both for parameter tuning on additional development set(s) and for final optimal translation result extracting. pre-processing word segment tokenize GIZA++ e.forests alignment f.seg lowercase e.tok.lw lowercase f.seg.lw Enju rule extraction e.forests N-gram LM Rule set Akamon Decoder (MERT) Figure 1: Training and tuning process of the Akamon system. Here, e = source English language, f = target foreign language. • translation rule extraction: as former mentioned, we extract tree-to-string translation rules for Akamon. In particular, we implemented the GH"
P12-3022,2006.amta-papers.8,0,0.0880683,"et al., 2010). 1 Introduction Syntax-based statistical machine translation (SMT) systems have achieved promising improvements in recent years. Depending on the type of input, the systems are divided into two categories: stringbased systems whose input is a string to be simultaneously parsed and translated by a synchronous grammar (Wu, 1997; Chiang, 2005; Galley et al., 2006; Shen et al., 2008), and tree/forest-based systems whose input is already a parse tree or a packed forest to be directly converted into a target tree or string (Ding and Palmer, 2005; Quirk et al., 2005; Liu et al., 2006; Huang et al., 2006; Mi et al., 2008; Mi and Huang, 2008; Zhang et al., 2009; Wu et al., 2010; Wu et al., 2011a). ∗ Work done when all the authors were in The University of Tokyo. Depending on whether or not parsers are explicitly used for obtaining linguistically annotated data during training, the systems are also divided into two categories: formally syntax-based systems that do not use additional parsers (Wu, 1997; Chiang, 2005; Xiong et al., 2006), and linguistically syntax-based systems that use PCFG parsers (Liu et al., 2006; Huang et al., 2006; Galley et al., 2006; Mi et al., 2008; Mi and Huang, 2008; Zh"
P12-3022,D10-1092,0,0.106682,"Missing"
P12-3022,W10-1736,0,0.0606869,"forest-based translations in the past several years without re-implementing the systems or general algorithms from scratch. 2 Akamon Toolkit Features Limited by the successful parsing rate and coverage of linguistic phrases, Akamon currently achieves comparable translation accuracies compared with the most frequently used SMT baseline system, Moses (Koehn et al., 2007). Table 2 shows the automatic translation accuracies (case-sensitive) of Akamon and Moses. Besides BLEU and NIST score, we further list RIBES score3 , , i.e., the software implementation of Normalized Kendall’s τ as proposed by (Isozaki et al., 2010a) to automatically evaluate the translation between distant language pairs based on rank correlation coefficients and significantly penal2 3 izes word order mistakes. In this table, Akamon-Forest differs from Akamon-Comb by using different configurations: Akamon-Forest used only 2/3 of the total training data (limited by the experiment environments and time). Akamon-Comb represents the system combination result by combining Akamon-Forest and other phrase-based SMT systems, which made use of pre-ordering methods of head finalization as described in (Isozaki et al., 2010b) and used the total 3"
P12-3022,P07-2045,0,0.0219959,"extraction (Galley et al., 2004; Mi and Huang, 2008; Wu et al., 2010; Wu et al., 2011a) and tree/forest-based decoding (Liu et al., 2006; Mi et al., 2008). We hope this system will help related researchers to catch up with the achievements of tree/forest-based translations in the past several years without re-implementing the systems or general algorithms from scratch. 2 Akamon Toolkit Features Limited by the successful parsing rate and coverage of linguistic phrases, Akamon currently achieves comparable translation accuracies compared with the most frequently used SMT baseline system, Moses (Koehn et al., 2007). Table 2 shows the automatic translation accuracies (case-sensitive) of Akamon and Moses. Besides BLEU and NIST score, we further list RIBES score3 , , i.e., the software implementation of Normalized Kendall’s τ as proposed by (Isozaki et al., 2010a) to automatically evaluate the translation between distant language pairs based on rank correlation coefficients and significantly penal2 3 izes word order mistakes. In this table, Akamon-Forest differs from Akamon-Comb by using different configurations: Akamon-Forest used only 2/3 of the total training data (limited by the experiment environments"
P12-3022,P09-1063,0,0.0771927,"Missing"
P12-3022,P09-1065,0,0.0305507,"Missing"
P12-3022,D08-1022,0,0.676027,"-based statistical machine translation (SMT) systems have achieved promising improvements in recent years. Depending on the type of input, the systems are divided into two categories: stringbased systems whose input is a string to be simultaneously parsed and translated by a synchronous grammar (Wu, 1997; Chiang, 2005; Galley et al., 2006; Shen et al., 2008), and tree/forest-based systems whose input is already a parse tree or a packed forest to be directly converted into a target tree or string (Ding and Palmer, 2005; Quirk et al., 2005; Liu et al., 2006; Huang et al., 2006; Mi et al., 2008; Mi and Huang, 2008; Zhang et al., 2009; Wu et al., 2010; Wu et al., 2011a). ∗ Work done when all the authors were in The University of Tokyo. Depending on whether or not parsers are explicitly used for obtaining linguistically annotated data during training, the systems are also divided into two categories: formally syntax-based systems that do not use additional parsers (Wu, 1997; Chiang, 2005; Xiong et al., 2006), and linguistically syntax-based systems that use PCFG parsers (Liu et al., 2006; Huang et al., 2006; Galley et al., 2006; Mi et al., 2008; Mi and Huang, 2008; Zhang et al., 2009), HPSG parsers (Wu e"
P12-3022,P10-1145,0,0.0900326,"Missing"
P12-3022,P08-1023,0,0.455039,"troduction Syntax-based statistical machine translation (SMT) systems have achieved promising improvements in recent years. Depending on the type of input, the systems are divided into two categories: stringbased systems whose input is a string to be simultaneously parsed and translated by a synchronous grammar (Wu, 1997; Chiang, 2005; Galley et al., 2006; Shen et al., 2008), and tree/forest-based systems whose input is already a parse tree or a packed forest to be directly converted into a target tree or string (Ding and Palmer, 2005; Quirk et al., 2005; Liu et al., 2006; Huang et al., 2006; Mi et al., 2008; Mi and Huang, 2008; Zhang et al., 2009; Wu et al., 2010; Wu et al., 2011a). ∗ Work done when all the authors were in The University of Tokyo. Depending on whether or not parsers are explicitly used for obtaining linguistically annotated data during training, the systems are also divided into two categories: formally syntax-based systems that do not use additional parsers (Wu, 1997; Chiang, 2005; Xiong et al., 2006), and linguistically syntax-based systems that use PCFG parsers (Liu et al., 2006; Huang et al., 2006; Galley et al., 2006; Mi et al., 2008; Mi and Huang, 2008; Zhang et al., 2009)"
P12-3022,J03-1002,0,0.00652201,"g and Decoding Frameworks Figure 1 shows the training and tuning progress of the Akamon system. Given original bilingual parallel corpora, we first tokenize and lowercase the source and target sentences (e.g., word segmentation of Chinese and Japanese, punctuation segmentation of English). The pre-processed monolingual sentences will be used by SRILM (Stolcke, 2002) or BerkeleyLM (Pauls and Klein, 2011) to train a n-gram language model. In addition, we filter out too long sentences here, i.e., only relatively short sentence pairs will be used to train word alignments. Then, we can use GIZA++ (Och and Ney, 2003) and symmetric strategies, such as grow-diag-final (Koehn et al., 2007), on the tokenized parallel corpus to obtain a wordaligned parallel corpus. The source sentence and its packed forest, the target sentence, and the word alignment are used for tree-to-string translation rule extraction. Since a 1best tree is a special case of a packed forest, we will focus on using the term ‘forest’ in the continuing discussion. Then, taking the target language model, the rule set, and the preprocessed development set as inputs, we perform MERT on the decoder to tune the weights of the features. The Akamon"
P12-3022,P03-1021,0,0.036344,"e.tok f.seg lowercase lowercase e.tok.lw Table 2: Translation accuracies of Akamon and the baseline systems on the NTCIR-9 English-to-Japanese translation task (Wu et al., 2011b). * stands for only using 2 million parallel sentences of the total 3 million data. Here, HPSG forests were used in Akamon. SRILM • language models: Akamon can make use of one or many n-gram language models trained by using SRILM5 (Stolcke, 2002) or the Berkeley language model toolkit, berkeleylm-1.0b36 (Pauls and Klein, 2011). The weights of multiple language models are tuned under minimum error rate training (MERT) (Och, 2003). • pruning: traditional beam-pruning and cubepruning (Chiang, 2007) techniques are incorporated in Akamon to make decoding feasible for large-scale rule sets. Before decoding, we also perform the marginal probability-based inside-outside algorithm based pruning (Mi et al., 2008) on the original parsing forest to control the decoding time. • MERT: Akamon has its own MERT module which optimizes weights of the features so as to maximize some automatic evaluation metric, such as BLEU (Papineni et al., 2002), on a development set. 5 6 http://www.speech.sri.com/projects/srilm/ http://code.google.co"
P12-3022,P02-1040,0,0.0860203,"in, 2011). The weights of multiple language models are tuned under minimum error rate training (MERT) (Och, 2003). • pruning: traditional beam-pruning and cubepruning (Chiang, 2007) techniques are incorporated in Akamon to make decoding feasible for large-scale rule sets. Before decoding, we also perform the marginal probability-based inside-outside algorithm based pruning (Mi et al., 2008) on the original parsing forest to control the decoding time. • MERT: Akamon has its own MERT module which optimizes weights of the features so as to maximize some automatic evaluation metric, such as BLEU (Papineni et al., 2002), on a development set. 5 6 http://www.speech.sri.com/projects/srilm/ http://code.google.com/p/berkeleylm/ 129 dev f.seg.lw clean dev.e tokenize e.clean f.clean dev.f word segmentation e.tok Enju i.e., first construct a translation forest by applying the tree-to-string translation rules to the original parsing forest of the source sentence, and then collect k-best hypotheses for the root node(s) of the translation forest using Algorithm 2 or Algorithm 3 as described in (Huang and Chiang, 2005). Later, the k-best hypotheses are used both for parameter tuning on additional development set(s) and"
P12-3022,P11-1027,0,0.0837387,".2773 0.2799 0.3948 NIST 6.830 7.795 7.881 6.905 7.258 8.713 RIBES 0.6991 0.7200 0.7068 0.6619 0.6861 0.7813 corpus e.tok f.seg lowercase lowercase e.tok.lw Table 2: Translation accuracies of Akamon and the baseline systems on the NTCIR-9 English-to-Japanese translation task (Wu et al., 2011b). * stands for only using 2 million parallel sentences of the total 3 million data. Here, HPSG forests were used in Akamon. SRILM • language models: Akamon can make use of one or many n-gram language models trained by using SRILM5 (Stolcke, 2002) or the Berkeley language model toolkit, berkeleylm-1.0b36 (Pauls and Klein, 2011). The weights of multiple language models are tuned under minimum error rate training (MERT) (Och, 2003). • pruning: traditional beam-pruning and cubepruning (Chiang, 2007) techniques are incorporated in Akamon to make decoding feasible for large-scale rule sets. Before decoding, we also perform the marginal probability-based inside-outside algorithm based pruning (Mi et al., 2008) on the original parsing forest to control the decoding time. • MERT: Akamon has its own MERT module which optimizes weights of the features so as to maximize some automatic evaluation metric, such as BLEU (Papineni"
P12-3022,P05-1034,0,0.271418,"al., 2004) and HPSG trees/forests (Wu et al., 2010). 1 Introduction Syntax-based statistical machine translation (SMT) systems have achieved promising improvements in recent years. Depending on the type of input, the systems are divided into two categories: stringbased systems whose input is a string to be simultaneously parsed and translated by a synchronous grammar (Wu, 1997; Chiang, 2005; Galley et al., 2006; Shen et al., 2008), and tree/forest-based systems whose input is already a parse tree or a packed forest to be directly converted into a target tree or string (Ding and Palmer, 2005; Quirk et al., 2005; Liu et al., 2006; Huang et al., 2006; Mi et al., 2008; Mi and Huang, 2008; Zhang et al., 2009; Wu et al., 2010; Wu et al., 2011a). ∗ Work done when all the authors were in The University of Tokyo. Depending on whether or not parsers are explicitly used for obtaining linguistically annotated data during training, the systems are also divided into two categories: formally syntax-based systems that do not use additional parsers (Wu, 1997; Chiang, 2005; Xiong et al., 2006), and linguistically syntax-based systems that use PCFG parsers (Liu et al., 2006; Huang et al., 2006; Galley et al., 2006; M"
P12-3022,P08-1066,0,0.103141,"nd minimum error rate training. In terms of tree-to-string translation rule extraction, the toolkit implements the traditional maximum likelihood algorithm using PCFG trees (Galley et al., 2004) and HPSG trees/forests (Wu et al., 2010). 1 Introduction Syntax-based statistical machine translation (SMT) systems have achieved promising improvements in recent years. Depending on the type of input, the systems are divided into two categories: stringbased systems whose input is a string to be simultaneously parsed and translated by a synchronous grammar (Wu, 1997; Chiang, 2005; Galley et al., 2006; Shen et al., 2008), and tree/forest-based systems whose input is already a parse tree or a packed forest to be directly converted into a target tree or string (Ding and Palmer, 2005; Quirk et al., 2005; Liu et al., 2006; Huang et al., 2006; Mi et al., 2008; Mi and Huang, 2008; Zhang et al., 2009; Wu et al., 2010; Wu et al., 2011a). ∗ Work done when all the authors were in The University of Tokyo. Depending on whether or not parsers are explicitly used for obtaining linguistically annotated data during training, the systems are also divided into two categories: formally syntax-based systems that do not use addit"
P12-3022,2011.eamt-1.3,0,0.0197478,"icients and significantly penal2 3 izes word order mistakes. In this table, Akamon-Forest differs from Akamon-Comb by using different configurations: Akamon-Forest used only 2/3 of the total training data (limited by the experiment environments and time). Akamon-Comb represents the system combination result by combining Akamon-Forest and other phrase-based SMT systems, which made use of pre-ordering methods of head finalization as described in (Isozaki et al., 2010b) and used the total 3 million training data. The detail of the pre-ordering approach and the combination method can be found in (Sudoh et al., 2011) and (Duh et al., 2011). Also, Moses (hierarchical) stands for the hierarchical phrase-based SMT system and Moses (phrase) stands for the flat phrase-based SMT system. For intuitive comparison (note that the result achieved by Google is only for reference and not a comparison, since it uses a different and unknown training data) and following (Goto et al., 2011), the scores achieved by using the Google online translation system4 are also listed in this table. Here is a brief description of Akamon’s main features: Code available at https://sites.google.com/site/xianchaowu2012 Code available at"
P12-3022,P10-1034,1,0.357096,"ource toolkit for tree and forest-based statistical machine translation (Liu et al., 2006; Mi et al., 2008; Mi and Huang, 2008). Akamon implements all of the algorithms required for tree/forestto-string decoding using tree-to-string translation rules: multiple-thread forest-based decoding, n-gram language model integration, beam- and cube-pruning, k-best hypotheses extraction, and minimum error rate training. In terms of tree-to-string translation rule extraction, the toolkit implements the traditional maximum likelihood algorithm using PCFG trees (Galley et al., 2004) and HPSG trees/forests (Wu et al., 2010). 1 Introduction Syntax-based statistical machine translation (SMT) systems have achieved promising improvements in recent years. Depending on the type of input, the systems are divided into two categories: stringbased systems whose input is a string to be simultaneously parsed and translated by a synchronous grammar (Wu, 1997; Chiang, 2005; Galley et al., 2006; Shen et al., 2008), and tree/forest-based systems whose input is already a parse tree or a packed forest to be directly converted into a target tree or string (Ding and Palmer, 2005; Quirk et al., 2005; Liu et al., 2006; Huang et al.,"
P12-3022,P11-1003,1,0.433477,"achieved promising improvements in recent years. Depending on the type of input, the systems are divided into two categories: stringbased systems whose input is a string to be simultaneously parsed and translated by a synchronous grammar (Wu, 1997; Chiang, 2005; Galley et al., 2006; Shen et al., 2008), and tree/forest-based systems whose input is already a parse tree or a packed forest to be directly converted into a target tree or string (Ding and Palmer, 2005; Quirk et al., 2005; Liu et al., 2006; Huang et al., 2006; Mi et al., 2008; Mi and Huang, 2008; Zhang et al., 2009; Wu et al., 2010; Wu et al., 2011a). ∗ Work done when all the authors were in The University of Tokyo. Depending on whether or not parsers are explicitly used for obtaining linguistically annotated data during training, the systems are also divided into two categories: formally syntax-based systems that do not use additional parsers (Wu, 1997; Chiang, 2005; Xiong et al., 2006), and linguistically syntax-based systems that use PCFG parsers (Liu et al., 2006; Huang et al., 2006; Galley et al., 2006; Mi et al., 2008; Mi and Huang, 2008; Zhang et al., 2009), HPSG parsers (Wu et al., 2010; Wu et al., 2011a), or dependency parsers"
P12-3022,J97-3002,0,0.0325341,"cube-pruning, k-best hypotheses extraction, and minimum error rate training. In terms of tree-to-string translation rule extraction, the toolkit implements the traditional maximum likelihood algorithm using PCFG trees (Galley et al., 2004) and HPSG trees/forests (Wu et al., 2010). 1 Introduction Syntax-based statistical machine translation (SMT) systems have achieved promising improvements in recent years. Depending on the type of input, the systems are divided into two categories: stringbased systems whose input is a string to be simultaneously parsed and translated by a synchronous grammar (Wu, 1997; Chiang, 2005; Galley et al., 2006; Shen et al., 2008), and tree/forest-based systems whose input is already a parse tree or a packed forest to be directly converted into a target tree or string (Ding and Palmer, 2005; Quirk et al., 2005; Liu et al., 2006; Huang et al., 2006; Mi et al., 2008; Mi and Huang, 2008; Zhang et al., 2009; Wu et al., 2010; Wu et al., 2011a). ∗ Work done when all the authors were in The University of Tokyo. Depending on whether or not parsers are explicitly used for obtaining linguistically annotated data during training, the systems are also divided into two categori"
P12-3022,P06-1066,0,0.128388,"nput is already a parse tree or a packed forest to be directly converted into a target tree or string (Ding and Palmer, 2005; Quirk et al., 2005; Liu et al., 2006; Huang et al., 2006; Mi et al., 2008; Mi and Huang, 2008; Zhang et al., 2009; Wu et al., 2010; Wu et al., 2011a). ∗ Work done when all the authors were in The University of Tokyo. Depending on whether or not parsers are explicitly used for obtaining linguistically annotated data during training, the systems are also divided into two categories: formally syntax-based systems that do not use additional parsers (Wu, 1997; Chiang, 2005; Xiong et al., 2006), and linguistically syntax-based systems that use PCFG parsers (Liu et al., 2006; Huang et al., 2006; Galley et al., 2006; Mi et al., 2008; Mi and Huang, 2008; Zhang et al., 2009), HPSG parsers (Wu et al., 2010; Wu et al., 2011a), or dependency parsers (Ding and Palmer, 2005; Quirk et al., 2005; Shen et al., 2008). A classification1 of syntax-based SMT systems is shown in Table 1. Translation rules can be extracted from aligned string-string (Chiang, 2005), tree-tree (Ding and Palmer, 2005) and tree/forest-string (Galley et al., 2004; Mi and Huang, 2008; Wu et al., 2011a) data structures. Lev"
P12-3022,P09-1020,0,0.0219495,"achine translation (SMT) systems have achieved promising improvements in recent years. Depending on the type of input, the systems are divided into two categories: stringbased systems whose input is a string to be simultaneously parsed and translated by a synchronous grammar (Wu, 1997; Chiang, 2005; Galley et al., 2006; Shen et al., 2008), and tree/forest-based systems whose input is already a parse tree or a packed forest to be directly converted into a target tree or string (Ding and Palmer, 2005; Quirk et al., 2005; Liu et al., 2006; Huang et al., 2006; Mi et al., 2008; Mi and Huang, 2008; Zhang et al., 2009; Wu et al., 2010; Wu et al., 2011a). ∗ Work done when all the authors were in The University of Tokyo. Depending on whether or not parsers are explicitly used for obtaining linguistically annotated data during training, the systems are also divided into two categories: formally syntax-based systems that do not use additional parsers (Wu, 1997; Chiang, 2005; Xiong et al., 2006), and linguistically syntax-based systems that use PCFG parsers (Liu et al., 2006; Huang et al., 2006; Galley et al., 2006; Mi et al., 2008; Mi and Huang, 2008; Zhang et al., 2009), HPSG parsers (Wu et al., 2010; Wu et a"
P12-3022,P06-1077,0,\N,Missing
P12-3022,P05-1033,0,\N,Missing
P16-2062,P15-2080,0,0.0140714,"by humans for each topic. Each constraint for each topic is then modeled as a multinomial distribution over the constrained set of words that were identified as mutually related by humans. In Section 4, we consider a variant of ITM, whose constraints are instead inferred using external word embeddings. As regards short texts, a well-known topic model is Biterm Topic Model (BTM) (Yan et al., 2013). BTM directly models the generation of biterms (pairs of words) in the whole corpus. However, the assumption that pairs of cooccurring words should be assigned to the same topic might be too strong (Chen et al., 2015). Figure 1: Projected latent concepts on the word embedding space. Concept vectors are annotated with their representative concepts in parentheses. words, we expect topically-related latent concepts to co-occur many times, even in short texts with diverse usage of words. This in turn promotes topic inference in LCTM. LCTM further has the advantage of using continuous word embedding. Traditional LDA assumes a fixed vocabulary of word types. This modeling assumption prevents LDA from handling out of vocabulary (OOV) words in held-out documents. On the other hands, since our topic model operates"
P16-2062,P15-1077,0,0.193735,", 2003), are widely used to uncover hidden topics within a text corpus. LDA models each document as a mixture of topics where each topic is a distribution over words. In essence, LDA reveals latent topics in a corpus by implicitly capturing document-level word cooccurrence patterns (Wang and McCallum, 2006). In recent years, Social Networking Services and blogs have become increasingly prevalent due to 380 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 380–386, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics (Das et al., 2015), which models each topic as a Gaussian distribution over the word embedding space. However, the assumption that topics are unimodal in the embedding space is not appropriate, since topically related words such as ‘neural’ and ‘networks’ can occur distantly from each other in the embedding space. Nguyen et al. (2015) proposed topic models that incorporate information of word vectors in modeling topic-word distributions. Similarly, Petterson et al. (Petterson et al., 2010) exploits external word features to improve the Dirichlet prior of the topic-word distributions. However, both of the models"
P16-2062,Q15-1022,0,0.284651,"cent years, Social Networking Services and blogs have become increasingly prevalent due to 380 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 380–386, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics (Das et al., 2015), which models each topic as a Gaussian distribution over the word embedding space. However, the assumption that topics are unimodal in the embedding space is not appropriate, since topically related words such as ‘neural’ and ‘networks’ can occur distantly from each other in the embedding space. Nguyen et al. (2015) proposed topic models that incorporate information of word vectors in modeling topic-word distributions. Similarly, Petterson et al. (Petterson et al., 2010) exploits external word features to improve the Dirichlet prior of the topic-word distributions. However, both of the models cannot handle OOV words, because they assume fixed word types. Latent concepts in LCTM are closely related to ‘constraints’ in interactive topic models (ITM) (Hu et al., 2014). Both latent concepts and constraints are designed to group conceptually similar words using external knowledge in an attempt to aid topic in"
P16-2062,D14-1162,0,0.0958745,"fically, LCTM models each topic as a distribution over the latent concepts, where each latent concept is a localized Gaussian distribution over the word embedding space. Since the number of unique concepts in a corpus is often much smaller than the number of unique words, LCTM is less susceptible to the data sparsity. Experiments on the 20Newsgroups show the effectiveness of LCTM in dealing with short texts as well as the capability of the model in handling held-out documents with a high degree of OOV words. 1 Recently, word embedding models, such as word2vec (Mikolov et al., 2013) and GloVe (Pennington et al., 2014) have gained much attention with their ability to form clusters of conceptually similar words in the embedding space. Inspired by this, we propose a latent concept topic model (LCTM) that infers topics based on documentlevel co-occurrence of references to the same concept. More specifically, we introduce a new latent variable, termed a latent concept to capture conceptual similarity of words, and redefine each topic as a distribution over the latent concepts. Each latent concept is then modeled as a localized Gaussian distribution over the embedding space. This is illustrated in Figure 1, wher"
P84-1057,C82-1062,1,\N,Missing
P84-1057,P84-1069,1,\N,Missing
P84-1057,P84-1086,1,\N,Missing
P84-1057,P84-1011,0,\N,Missing
P84-1069,C82-1004,0,\N,Missing
P84-1069,P84-1057,1,\N,Missing
P84-1069,P84-1086,1,\N,Missing
P84-1069,P84-1011,0,\N,Missing
P84-1086,P84-1069,1,0.782474,"Missing"
P84-1086,P84-1057,1,0.553084,"Missing"
P84-1086,P84-1011,0,0.513078,"Missing"
P84-1086,C82-1062,1,0.78735,"Missing"
P94-1042,C88-2156,0,0.0671832,"Missing"
P98-2132,H90-1055,0,0.0393049,"Missing"
P98-2132,W96-0102,0,0.0235503,"Missing"
P98-2132,J94-2001,0,0.0240852,"Missing"
P98-2132,C94-1027,0,0.0136296,"in the inputs for tagging and that there are 50 POSs. The n-gram models must estin]ate 50 T = 7.8e + 11 n-grams, while the single-neuro tagger with the longest input uses 805 only 70,000 weights, which can be calculated by nipt • n h i d q- n h i d • nopt w h e r e n i p t , n h i d , and nopt are, respectively, the number of units in the input, the hidden, and the output layers, and nhid is set to be nipt/2. T h a t neuro models require few parameters m a y offer another advantage: their performance is less affected by a small amount of training d a t a than that of the statistical methods (Schmid, 1994). Neuro taggers also offer fast tagging compared to other models, although its training stage is longer. 5 Experimental Results The Thai corpus used in the computer experiments contains 10,452 sentences that are randomly divided into two sets: one with 8,322 sentences for training and another with 2,130 sentences for testing. The training and testing sets contain, respectively, 22,311 and 6,717 ambiguous words that serve as more than one POS and were used for training and testing. Because there are 47 types of POSs in Thai (Charoenporn et al., 1997), n in (6), (10), and (14) was set at 47. The"
P98-2132,C96-2160,1,\N,Missing
P98-2132,E95-1025,0,\N,Missing
P98-2132,P98-2144,1,\N,Missing
P98-2132,C98-2139,1,\N,Missing
P98-2144,J94-4001,0,0.213984,"abandon the t r e a t m e n t of rare linguistic p h e n o m e n a is by i n t r o d u c i n g a d d i t i o n a l constraints in feature structures. Regarding (i) and (ii), we introduce 'pseudo-principles', which are unified with ID schemata in the same way principles are unified. Regarding (iii), we add some feature structures to LEs/LETs. 3.1 P o s t p o s i t i o n 'Wa' The main usage of the postposition 'wa' is divided into the following two patternsS: • If two PPs with the postposition 'wa' appear consecutively, we treat the first PP as 5These patterns are almost similar to the ones in (Kurohashi and Nagao, 1994). (a) / (b)* ......... 1. . . . . . . . . . . . . . . . .... ........ .... (c) (~)....... l' I Chil~ . . . . ' .... l_-. I ............i ....... '-"" I tt&x"" g~., ko u. Figure 2: Correct parse tree for Sentence (4) (d)* ......... l ........ T 1 ........ (i) ........ *........... ; *------'t----4 ....... '-'1 ! .._ho(N El D Figure 1: (a) Correct / (b) incorrect parse tree for Sentence (2); (c) correct / (d) incorrect parse tree for Sentence (3) where .a_hc(-, --, --). .a_hc(+, --, 4-). .a_hc(-, +, +). (B) W h e n applying head-modifier schema, also apply: a complement of a predicate just before"
P98-2144,P98-2132,1,0.744407,"epresenting Time and Commas Noun phrases (NPs) with nominal suffixes such as nen (year), gatsu (month), and ji (hour) represent information about time. Such NPs are sometimes used adverbially, rather t h a n nominally. Especially NPs with such a nominal suffix and c o m m a are often used adverbially (Sentence (6) & Figure 4(a) ), while general S P s with a c o m m a are used in coordinate structures (Sentence (7) & Figure 4(b) ). (6) 1995 nen, jishin ga okita. year earthquake -SUBJ Occur-PAST An earthquake occurred in 1995. 4 Experiments We implemented our parser and g r a m m a r in LiLFeS (Makino et al., 1998) s, a featurestructure description language developed by our group. We tested randomly selected 10000 sentences fi'om the Japanese E D R corpus (EDR, 1996). Tile EDR Corpus is a Japanese version of treebank with morphological, structural, and semantic information. In our experiments, we used only the structural information, that is, parse trees. Both the parse trees in our parser and the parse trees in the E D R Corpus are first converted into bunsetsu dependencies, and they are compared when calculating accuracy. Note that the internal structures of bunsetsus, e.~. structures of c o m p o u n"
P98-2144,C96-2160,1,0.798548,"s. And grammar (e) using the combination of the three constraints still works with no side effect. We also measured average parsing time per sentence for the original grammar (a) and the fully augmented grammar (e). The parser we adopted is a naive CKY-style parser. Table 3 gives the average parsing time per sentence for those 2 grammars. Pseudo-principles and further constraints on LEs/LETs also make parsing more time-efficient. Even though they are sometimes considered to be slow in practical application because of their heavy feature structures, actually we found them to improve speed. In (Torisawa and Tsujii, 1996), an efficient HPSG parser is proposed, and our preliminary experiments show that the parsing time of the effident parser is about three times shorter than that of the naive one. Thus, the average parsing time per sentence will be about 300 msec., and we believe our g r a m m a r will achive a practical speed. Other techniques to speed-up the parser are proposed in (Makino et al., 1998). 5 Average parsing time per sentence 1277 (msec) 838 (msec) Discussion This section focuses on the behavior of commas. Out of randomly selected 119 errors in experiment (e), 34 errors are considered to have bee"
P98-2144,C98-2128,1,\N,Missing
P98-2159,P98-2132,1,0.896804,"SPS(JSPS-RFTF96P00502). 968 J u n ' i c h i t$ Rep~ Order~! Figure 1: Agent-based System with the PSTFS ing or semantic processing, are divided into several pieces which can be simultaneously computed by several agents. Several parallel NLP systems have been developed previously. But most of them have been neither efficient nor practical enough (Adriaens and Hahn, 1994). On the other hand, our PSTFS provides the following features. • An efficient communication scheme for messages including Typed Feature Structures (TFSs) (Carpenter, 1992). • Efficient treatment of TFSs by an abstract machine (Makino et al., 1998). Another possible way to develop parallel NLP systems with TFSs is to use a full concurrent logic programming language (Clark and Gregory, 1986; Ueda, 1985). However, we have observed that it is necessary to control parallelism in a flexible way to achieve high-performance. (Fixed concurrency in a logic programming language does not provide sufficient flexibility.) Our agent-based architecture is suitable for accomplishing such flexibility in parallelism. The next section discusses PSTFS from a programmers' point of view. Section 3 describes the PSTFS architecture in detail. Section 4 describ"
P98-2159,P98-2144,1,0.781453,"nally. when computation of Fi (using Fi k and Fk j for all k(i &lt; k &lt; j ) ) is completed, Ci,J d]strlbutes • . . Fi,3 to other agents waiting for Fij. Parsing ]s completed when the computation of F0 n is completed. We have done a series of experiments on a shared-memory parallel machine, SUN Ultra Enterprise 10000 consisting of 64 nodes (each node is a 250 MHz UltraSparc) and 6 GByte shared memory. The corpus consists of 879 random sentences from the EDR Japanese corpus written in Japanese (average length of sentences is 20.8) 4 . The grammar we used is an underspecified Japanese HPSG grammar (Mitsuishi et al., 1998) consisting of 6 ID-schemata and 39 lexical entries (assigned to functional words) and 41 lexical-entry-templates (assigned to parts of speech)• This grammar has wide coverage and high accuracy for real-world texts s. Table 1 shows the result and comparison with a parser written in LiLFeS. Figure 6 shows its speed-up. From the Figure 6, we observe that the maximum speedup reaches up to 12.4 times. The average parsing time is 85 msec per 3CSAs cannot be added dynamically in our implementation. So, to gain the maximum parallelism, we assigned a CSA to each processor. Each Cij asks the CSA on the"
P98-2159,A88-1010,0,\N,Missing
P98-2159,C98-2139,1,\N,Missing
P98-2159,C98-2128,1,\N,Missing
saetre-etal-2008-connecting,N03-2020,1,\N,Missing
saetre-etal-2008-connecting,P06-1128,1,\N,Missing
saetre-etal-2008-connecting,P05-1011,1,\N,Missing
saetre-etal-2008-connecting,nenadic-etal-2006-towards,1,\N,Missing
tateisi-etal-2008-genia,W03-2401,0,\N,Missing
tateisi-etal-2008-genia,de-marneffe-etal-2006-generating,0,\N,Missing
tateisi-etal-2008-genia,N04-1013,0,\N,Missing
tateisi-etal-2008-genia,J93-2004,0,\N,Missing
tateisi-etal-2008-genia,E03-1025,0,\N,Missing
tateisi-etal-2008-genia,W07-1004,0,\N,Missing
tateisi-etal-2008-genia,W07-2202,1,\N,Missing
tateisi-etal-2008-genia,W04-3111,0,\N,Missing
tateisi-etal-2008-genia,P06-4020,0,\N,Missing
tateisi-etal-2008-genia,P07-1032,0,\N,Missing
tateisi-etal-2008-genia,P06-2006,0,\N,Missing
tateisi-etal-2008-genia,ohta-etal-2006-linguistic,1,\N,Missing
tateisi-tsujii-2004-part,C02-1110,0,\N,Missing
tateisi-tsujii-2004-part,W04-3111,0,\N,Missing
tateisi-tsujii-2004-part,J96-2004,0,\N,Missing
tsunakawa-etal-2008-building-bilingual,H05-1059,1,\N,Missing
tsunakawa-etal-2008-building-bilingual,W02-2026,0,\N,Missing
tsunakawa-etal-2008-building-bilingual,C94-1048,0,\N,Missing
tsunakawa-etal-2008-building-bilingual,J90-2002,0,\N,Missing
tsunakawa-etal-2008-building-bilingual,2001.mtsummit-papers.10,0,\N,Missing
tsunakawa-etal-2008-building-bilingual,P07-2055,0,\N,Missing
tsunakawa-etal-2008-building-bilingual,P07-1108,0,\N,Missing
tsunakawa-etal-2008-building-bilingual,I05-1059,0,\N,Missing
tsunakawa-etal-2008-building-bilingual,N07-1061,0,\N,Missing
tsunakawa-etal-2008-building-bilingual,J03-1002,0,\N,Missing
W00-0904,A97-1029,0,0.081688,"Missing"
W00-0904,W98-1118,0,0.0668506,"Missing"
W00-0904,P96-1041,0,0.0313857,"J A P A N B u n k y o - k u , T o k y o , 113-0033 J A P A N nova@crl, go. j p {nigel, tsuj ii}@is, s. u-tokyo, ac. jp Abstract made by these measures against actual system performance. Recently IE systems based on supervised learning paradigms such as hidden Markov models (Bikel et al., 1997), maximum entropy (Borthwick et al., 1998) and decision trees (Sekine et al., 1998) have emerged that should be easier to adapt to new domains than the dictionary-based systems of the past. Much of this work has taken advantage of smoothing techniques to overcome problems associated with data sparseness (Chen and Goodman, 1996). The two corpora we use in our NE experiments represent the following domains: We present two measures for comparing corpora based on infbrmation theory statistics such as gain ratio as well as simple term-class ~equency counts. We tested the predictions made by these measures about corpus difficulty in two domains - - news and molecular biology - - using the result of two well-used paradigms for NE, decision trees and HMMs and found that gain ratio was the more reliable predictor. 1 Introduction With the advent of the information society and increasing availability of large m o u n t s of in"
W00-0904,C00-1030,1,0.897199,"annot hope to achieve performance limits. What we aim to do is to compare model performance against the predictions of corpus difficulty made by two different methods. In the rest of this paper we firstly introduce the NE models used for evaluation, the two corpora we examined and then the difficulty comparison metrics. Predictive scores from the metrics are examined against the actual performance of the NE models. 2 Models Recent studies into the use of supervised learningbased modeels for the NE task in the molecularbiology domain have shown that models based on hidden Markov models (HMMs) (Collier et al., 2000) and decision trees (Nobata et al., 1999) are not only adaptable to this highly technical domain, but are also much more generalizable to new classes of words than systems based on traditional hand-built heuristic rules such as (Fukuda et al., 1998). W e now describe two models used in our experiments based on the decision trees package C4.5 (Quiuian, 1993) and H M M s (Rabiner and Juang, 1986). 2.1 Decision tree n a m e d recogniser:NE-DT entity A decision tree is a type of classifier which has ""leaf nodes"" indicating classes and ""decision nodes"" that specify some test to be carried out, with"
W00-0904,A97-1028,0,0.0545242,"Missing"
W00-0904,W96-0213,0,0.0734193,"ts ( S e i n e et al., 1998). It has two phases, one for creating the decision tree from training d a t a and the other for generating the class-tagged text based on the decision tree. When generating decision trees, trigrams of words were used. For this system, words are considered to be quadruple features. The following features are used to generate conditions in the decision tree: P a r t - o f - s p e e c h i n f o r m a t i o n : There are 45 part-of-speech categories, whose definitions are based on Pennsylvania Treebank&apos;s categories. We use a tagger based on Adwait Ratnaparkhi&apos;s method (Ratnaparkhi, 1996). Character type i n f o r m a t i o n : Orthographic information is considered such as upper case, lower case, capitalization, numerical expressions, symbols. These character features are the same as those used by N E H M M described in the next section and shown in T a b l e 1. W o r d lists s p e c i f i c t o t h e d o m a i n : Word lists are made from the training corpus. Only the 200 highest fxequency words are used. 2.2 Hidden Markov model named entity reco~.iser: NEHMM HMMs are a widely u ~ d class of learning algorithms and can be considered to be stochastic finite state machines. In"
W00-0904,W98-1120,0,0.0369914,"I n f o r m a t i o n Science Communications Research Laboratory G r a d u a t e School o f Science 588-2 I w a o k a , I w a o k a - c h o , N i s h i - k u U n i v e r s i t y o f T o k y o , H o n g o 7-3-1 K o b e , H y o g o , 65].-2492 J A P A N B u n k y o - k u , T o k y o , 113-0033 J A P A N nova@crl, go. j p {nigel, tsuj ii}@is, s. u-tokyo, ac. jp Abstract made by these measures against actual system performance. Recently IE systems based on supervised learning paradigms such as hidden Markov models (Bikel et al., 1997), maximum entropy (Borthwick et al., 1998) and decision trees (Sekine et al., 1998) have emerged that should be easier to adapt to new domains than the dictionary-based systems of the past. Much of this work has taken advantage of smoothing techniques to overcome problems associated with data sparseness (Chen and Goodman, 1996). The two corpora we use in our NE experiments represent the following domains: We present two measures for comparing corpora based on infbrmation theory statistics such as gain ratio as well as simple term-class ~equency counts. We tested the predictions made by these measures about corpus difficulty in two domains - - news and molecular biology - - u"
W00-0904,M93-1007,0,\N,Missing
W00-1704,W98-1507,0,0.0605934,"ties” and the tasks we attempt involve recognition and classification of the names of substances and their locations, just as named entity recognition task in MUC conferences. We therefore try to model our annotation task after the definition of “EnameX” (Chincor, 1998a) of MUC conferences. Unlike in MUC conferences, we don’t make a precise definition of how the recognized names are used in further information extraction task such as event identification, because we want the recognition technology to be independent of the further task. Our work is also compared to word-sense annotation (e.g.,(Bruce and Wiebe, 1998)) where instances of words that have multiple senses are labelled for the sense it denotes according to a certain dictionary or thesaurus. We first built a conceptual model (ontology) of substances and sources (substance location), and designed a tag set based on the ontology which conforms to SGML/XML format. Using the tag set, we annotated the entities such names that appears in the abstracts of research papers taken from the MEDLINE database. In this paper we report on this new corpus, its ontological basis, and our experience in designing the annotation scheme. Experimental results are sho"
W00-1704,M98-1001,0,0.0143654,"sentations than the methods based on dictionaries and hand-constructed heuristic rules. We think that a corpus-based, machine-learning approach is quite promising, and to support this we are building a corpus of annotated abstracts taken from National Library of Medicine (NLM)’s MEDLINE database. Corpus annotation is now a key topic for all areas of natural language processing and linguistically annotated corpus such as treebanks are now established. In information extraction task, annotated corpora have been made mainly for the judgment set of information extraction competitions such as MUC (Chinchor, 1998). We think that technical terms of a scientific domain share common characteristics with the “Named Entities” and the tasks we attempt involve recognition and classification of the names of substances and their locations, just as named entity recognition task in MUC conferences. We therefore try to model our annotation task after the definition of “EnameX” (Chincor, 1998a) of MUC conferences. Unlike in MUC conferences, we don’t make a precise definition of how the recognized names are used in further information extraction task such as event identification, because we want the recognition tech"
W00-1704,M98-1028,0,0.067007,"sing and linguistically annotated corpus such as treebanks are now established. In information extraction task, annotated corpora have been made mainly for the judgment set of information extraction competitions such as MUC (Chinchor, 1998). We think that technical terms of a scientific domain share common characteristics with the “Named Entities” and the tasks we attempt involve recognition and classification of the names of substances and their locations, just as named entity recognition task in MUC conferences. We therefore try to model our annotation task after the definition of “EnameX” (Chincor, 1998a) of MUC conferences. Unlike in MUC conferences, we don’t make a precise definition of how the recognized names are used in further information extraction task such as event identification, because we want the recognition technology to be independent of the further task. Our work is also compared to word-sense annotation (e.g.,(Bruce and Wiebe, 1998)) where instances of words that have multiple senses are labelled for the sense it denotes according to a certain dictionary or thesaurus. We first built a conceptual model (ontology) of substances and sources (substance location), and designed a"
W00-1704,M98-1024,0,0.127633,"sing and linguistically annotated corpus such as treebanks are now established. In information extraction task, annotated corpora have been made mainly for the judgment set of information extraction competitions such as MUC (Chinchor, 1998). We think that technical terms of a scientific domain share common characteristics with the “Named Entities” and the tasks we attempt involve recognition and classification of the names of substances and their locations, just as named entity recognition task in MUC conferences. We therefore try to model our annotation task after the definition of “EnameX” (Chincor, 1998a) of MUC conferences. Unlike in MUC conferences, we don’t make a precise definition of how the recognized names are used in further information extraction task such as event identification, because we want the recognition technology to be independent of the further task. Our work is also compared to word-sense annotation (e.g.,(Bruce and Wiebe, 1998)) where instances of words that have multiple senses are labelled for the sense it denotes according to a certain dictionary or thesaurus. We first built a conceptual model (ontology) of substances and sources (substance location), and designed a"
W00-1704,P99-1032,0,0.0500168,"Missing"
W01-1510,W00-2006,0,0.022452,"to LTAG derivation trees. All modules other than the last one are related to the conversion process from LTAG into HPSG, and the last one enables to obtain LTAG analysis from the obtained HPSG analysis. Tateisi et al. also translated LTAG into HPSG (Tateisi et al., 1998). However, their method depended on translator’s intuitive analysis of the original grammar. Thus the translation was manual and grammar dependent. The manual translation demanded considerable efforts from the translator, and obscures the equivalence between the original and obtained grammars. Other works (Kasper et al., 1995; Becker and Lopez, 2000) convert HPSG grammars into LTAG grammars. However, given the greater expressive power of HPSG, it is impossible to convert an arbitrary HPSG grammar into an LTAG grammar. Therefore, a conversion from HPSG into LTAG often requires some restrictions on the HPSG grammar to suppress its generative capacity. Thus, the conversion loses the equivalence of the grammars, and we cannot gain the above advantages. Section 2 reviews the source and the target grammar formalisms of the conversion algorithm. Section 3 describes the conversion algorithm which the core module in the RenTAL system uses. Section"
W01-1510,2000.iwpt-1.9,0,0.035923,"Missing"
W01-1510,P00-1058,0,0.0138769,"r (Yoshinaga and Miyao, 2001). Strong equivalence means that both grammars generate exactly equivalent parse results, and that we can share the LTAG grammars and lexicons in HPSG applications. Our system can reduce considerable workload to develop a huge resource (grammars and lexicons) from scratch. Our concern is, however, not limited to the sharing of grammars and lexicons. Strongly equivalent grammars enable the sharing of ideas developed in each formalism. There have been many studies on parsing techniques (Poller and Becker, 1998; Flickinger et al., 2000), ones on disambiguation models (Chiang, 2000; Kanayama et al., 2000), and ones on programming/grammar-development environ1 In this paper, we use the term LTAG to refer to FBLTAG, if not confusing. LTAG-based application RenTAL System HPSG-based application LTAG Resources Tree converter HPSG Resources Grammar: Elementary tree templates Lexicon Type hierarchy extractor Lexicon converter LTAG parsers anchor * Grammar: Lexical entry templates Initial tree foot node Auxiliary tree α2 S substitution node α1 NP Lexicon VP VP V N V can We run NP β1 VP * HPSG parsers Figure 2: Elementary trees Derivation trees Derivation translator Parse trees F"
W01-1510,C00-1060,1,0.921494,"nd Miyao, 2001). Strong equivalence means that both grammars generate exactly equivalent parse results, and that we can share the LTAG grammars and lexicons in HPSG applications. Our system can reduce considerable workload to develop a huge resource (grammars and lexicons) from scratch. Our concern is, however, not limited to the sharing of grammars and lexicons. Strongly equivalent grammars enable the sharing of ideas developed in each formalism. There have been many studies on parsing techniques (Poller and Becker, 1998; Flickinger et al., 2000), ones on disambiguation models (Chiang, 2000; Kanayama et al., 2000), and ones on programming/grammar-development environ1 In this paper, we use the term LTAG to refer to FBLTAG, if not confusing. LTAG-based application RenTAL System HPSG-based application LTAG Resources Tree converter HPSG Resources Grammar: Elementary tree templates Lexicon Type hierarchy extractor Lexicon converter LTAG parsers anchor * Grammar: Lexical entry templates Initial tree foot node Auxiliary tree α2 S substitution node α1 NP Lexicon VP VP V N V can We run NP β1 VP * HPSG parsers Figure 2: Elementary trees Derivation trees Derivation translator Parse trees Figure 1: The RenTAL Syst"
W01-1510,W98-0134,0,0.0195913,"matically converts an FB-LTAG grammar into a strongly equivalent HPSG-style grammar (Yoshinaga and Miyao, 2001). Strong equivalence means that both grammars generate exactly equivalent parse results, and that we can share the LTAG grammars and lexicons in HPSG applications. Our system can reduce considerable workload to develop a huge resource (grammars and lexicons) from scratch. Our concern is, however, not limited to the sharing of grammars and lexicons. Strongly equivalent grammars enable the sharing of ideas developed in each formalism. There have been many studies on parsing techniques (Poller and Becker, 1998; Flickinger et al., 2000), ones on disambiguation models (Chiang, 2000; Kanayama et al., 2000), and ones on programming/grammar-development environ1 In this paper, we use the term LTAG to refer to FBLTAG, if not confusing. LTAG-based application RenTAL System HPSG-based application LTAG Resources Tree converter HPSG Resources Grammar: Elementary tree templates Lexicon Type hierarchy extractor Lexicon converter LTAG parsers anchor * Grammar: Lexical entry templates Initial tree foot node Auxiliary tree α2 S substitution node α1 NP Lexicon VP VP V N V can We run NP β1 VP * HPSG parsers Figure 2"
W01-1510,W00-1605,0,0.0466259,"Missing"
W01-1510,C88-2121,0,0.469209,"Missing"
W01-1510,W98-0141,1,0.83789,"rarchy extractor module extracts the symbols of the node, features, and feature values from the LTAG elementary tree templates and lexicon, and construct the type hierarchy from them. The lexicon converter module converts LTAG elementary tree templates into HPSG lexical entries. The derivation translator module takes HPSG parse trees, and map them to LTAG derivation trees. All modules other than the last one are related to the conversion process from LTAG into HPSG, and the last one enables to obtain LTAG analysis from the obtained HPSG analysis. Tateisi et al. also translated LTAG into HPSG (Tateisi et al., 1998). However, their method depended on translator’s intuitive analysis of the original grammar. Thus the translation was manual and grammar dependent. The manual translation demanded considerable efforts from the translator, and obscures the equivalence between the original and obtained grammars. Other works (Kasper et al., 1995; Becker and Lopez, 2000) convert HPSG grammars into LTAG grammars. However, given the greater expressive power of HPSG, it is impossible to convert an arbitrary HPSG grammar into an LTAG grammar. Therefore, a conversion from HPSG into LTAG often requires some restrictions"
W01-1510,P95-1013,0,0.83404,"trees, and map them to LTAG derivation trees. All modules other than the last one are related to the conversion process from LTAG into HPSG, and the last one enables to obtain LTAG analysis from the obtained HPSG analysis. Tateisi et al. also translated LTAG into HPSG (Tateisi et al., 1998). However, their method depended on translator’s intuitive analysis of the original grammar. Thus the translation was manual and grammar dependent. The manual translation demanded considerable efforts from the translator, and obscures the equivalence between the original and obtained grammars. Other works (Kasper et al., 1995; Becker and Lopez, 2000) convert HPSG grammars into LTAG grammars. However, given the greater expressive power of HPSG, it is impossible to convert an arbitrary HPSG grammar into an LTAG grammar. Therefore, a conversion from HPSG into LTAG often requires some restrictions on the HPSG grammar to suppress its generative capacity. Thus, the conversion loses the equivalence of the grammars, and we cannot gain the above advantages. Section 2 reviews the source and the target grammar formalisms of the conversion algorithm. Section 3 describes the conversion algorithm which the core module in the Re"
W01-1510,P98-2144,1,0.85278,"ure 6 shows a rule application to “can run” and “we”. There are a variety of works on efficient parsing with HPSG, which allow the use of HPSGbased processing in practical application contexts (Flickinger et al., 2000). Stanford University is developing the English Resource Grammar, an HPSG grammar for English, as a part of the Linguistic Grammars Online (LinGO) project (Flickinger, 2000). In practical context, German, English, and Japanese HPSG-based grammars are developed and used in the Verbmobil project (Kay et al., 1994). Our group has developed a wide-coverage HPSG grammar for Japanese (Mitsuishi et al., 1998), which is used in a high-accuracy Japanese dependency analyzer (Kanayama et al., 2000). S anchor * foot node substitution node trunk NP VP V S* think think: Sym: V Sym : VP Arg: Leaf : S Dir : right Foot?: + , Sym : S Leaf : NP Dir : left Foot?: _ Figure 8: A conversion from a canonical elementary tree into an HPSG lexical entry  h mother Sym : Arg :  1 i 2 X*X2XXX 3 4 Arg : 4 5j Sym : 3 Arg : h i substitution node 2 Sym : 1 Leaf : 3 Dir : lef t Foot? : 2 + 3 5 trunk node Figure 9: Left substitution rule 3 Grammar conversion The grammar conversion from LTAG to HPSG (Yoshinaga and Miyao"
W01-1510,W98-0131,0,0.0541757,"Missing"
W01-1510,C88-2147,0,0.0507767,"rammar conversion from an FB-LTAG grammar to a strongly equivalent HPSG-style grammar. The system is applied to the latest version of the XTAG English grammar. Experimental results show that the obtained HPSG-style grammar successfully worked with an HPSG parser, and achieved a drastic speed-up against an LTAG parser. This system enables to share not only grammars and lexicons but also parsing techniques. 1 Introduction This paper describes an approach for sharing resources in various grammar formalisms such as Feature-Based Lexicalized Tree Adjoining Grammar (FB-LTAG1 ) (Vijay-Shanker, 1987; Vijay-Shanker and Joshi, 1988) and Head-Driven Phrase Structure Grammar (HPSG) (Pollard and Sag, 1994) by a method of grammar conversion. The RenTAL system automatically converts an FB-LTAG grammar into a strongly equivalent HPSG-style grammar (Yoshinaga and Miyao, 2001). Strong equivalence means that both grammars generate exactly equivalent parse results, and that we can share the LTAG grammars and lexicons in HPSG applications. Our system can reduce considerable workload to develop a huge resource (grammars and lexicons) from scratch. Our concern is, however, not limited to the sharing of grammars and lexicons. Strongly"
W01-1510,J93-2004,0,\N,Missing
W01-1510,C98-2139,1,\N,Missing
W01-1510,C98-2128,1,\N,Missing
W01-1510,P98-2132,1,\N,Missing
W02-0301,J96-1002,0,0.00235563,"ta et al., 2002) has been developed, and at this time it is the largest biomedical annotated corpus available to public, containing 670 annotated abstracts of the MEDLINE database. Another reason for low accuracies is that biomedical named entities are essentially hard to recognize using standard feature sets compared with the named entities in newswire articles (Nobata et al., 2000). Thus, we need to employ powerful machine learning techniques which can incorporate various and complex features in a consistent way. Support Vector Machines (SVMs) (Vapnik, 1995) and Maximum Entropy (ME) method (Berger et al., 1996) are powerful learning methods that satisfy such requirements, and are applied successfully to other NLP tasks (Kudo and Matsumoto, 2000; Nakagawa et al., 2001; Ratnaparkhi, 1996). In this paper, we apply Support Vector Machines to biomedical named entity recognition and train them with the GENIA corpus. We formulate the named entity recognition as the classification of each word with context to one of the classes that represent region and named entity’s semantic class. Although there is a previous work that applied SVMs to biomedical named entity task in this formulation (Yamada et al., 2000)"
W02-0301,C00-1030,1,0.436132,"al IE systems. Conceptually, named entity recognition consists of two tasks: identification, which finds the region of a named entity in a text, and classification, which determines the semantic class of that named entity. The following illustrates biomedical named entity recognition. “Thus, CIITAPROTEIN not only activates the expression of class II genes DNA but recruits another B cell-specific coactivator to increase transcriptional activity of class II promoters in DNA B cellsCELLTYPE .” Machine learning approach has been applied to biomedical named entity recognition (Nobata et al., 1999; Collier et al., 2000; Yamada et al., 2000; Shimpuku, 2002). However, no work has achieved sufficient recognition accuracy. One reason is the lack of annotated corpora for training as is often the case of a new domain. Nobata et al. (1999) and Collier et al. (2000) trained their model with only 100 annotated paper abstracts from the MEDLINE database (National Library of Medicine, 1999), and Yamada et al. (2000) used only 77 annotated paper abstracts. In addition, it is difficult to compare the techniques used in each study because they used a closed and different corpus. To overcome such a situation, the GENIA cor"
W02-0301,W00-0730,0,0.559664,"670 annotated abstracts of the MEDLINE database. Another reason for low accuracies is that biomedical named entities are essentially hard to recognize using standard feature sets compared with the named entities in newswire articles (Nobata et al., 2000). Thus, we need to employ powerful machine learning techniques which can incorporate various and complex features in a consistent way. Support Vector Machines (SVMs) (Vapnik, 1995) and Maximum Entropy (ME) method (Berger et al., 1996) are powerful learning methods that satisfy such requirements, and are applied successfully to other NLP tasks (Kudo and Matsumoto, 2000; Nakagawa et al., 2001; Ratnaparkhi, 1996). In this paper, we apply Support Vector Machines to biomedical named entity recognition and train them with the GENIA corpus. We formulate the named entity recognition as the classification of each word with context to one of the classes that represent region and named entity’s semantic class. Although there is a previous work that applied SVMs to biomedical named entity task in this formulation (Yamada et al., 2000), their method to construct a classifier using SVMs, one-vs-rest, fails to train a classifier with entire GENIA corpus, since the cost o"
W02-0301,N01-1025,0,0.28155,"“temp”. 2 Table 1: Basic statistics of the GENIA corpus # of sentences # of words # of named entities # of words in NEs # of words not in NEs Av. length of NEs (σ) 5,109 152,216 23,793 50,229 101,987 2.11 (1.40) 3 Named Entity Recognition Using SVMs 3.1 Named Entity Recognition as Classification We formulate the named entity task as the classification of each word with context to one of the classes that represent region information and named entity’s semantic class. Several representations to encode region information are proposed and examined (Ramshaw and Marcus, 1995; Uchimoto et al., 2000; Kudo and Matsumoto, 2001). In this paper, we employ the simplest BIO representation, which is also used in (Yamada et al., 2000). We modify this representation in Section 5.1 in order to accelerate the SVM training. In the BIO representation, the region information is represented as the class prefixes “B-” and “I-”, and a class “O”. B- means that the current word is at the beginning of a named entity, I- means that the current word is in a named entity (but not at the beginning), and O means the word is not in a named entity. For each named entity class C, class B-C and I-C are produced. Therefore, if we have N named"
W02-0301,W00-0904,1,0.737839,"Yamada et al. (2000) used only 77 annotated paper abstracts. In addition, it is difficult to compare the techniques used in each study because they used a closed and different corpus. To overcome such a situation, the GENIA corpus (Ohta et al., 2002) has been developed, and at this time it is the largest biomedical annotated corpus available to public, containing 670 annotated abstracts of the MEDLINE database. Another reason for low accuracies is that biomedical named entities are essentially hard to recognize using standard feature sets compared with the named entities in newswire articles (Nobata et al., 2000). Thus, we need to employ powerful machine learning techniques which can incorporate various and complex features in a consistent way. Support Vector Machines (SVMs) (Vapnik, 1995) and Maximum Entropy (ME) method (Berger et al., 1996) are powerful learning methods that satisfy such requirements, and are applied successfully to other NLP tasks (Kudo and Matsumoto, 2000; Nakagawa et al., 2001; Ratnaparkhi, 1996). In this paper, we apply Support Vector Machines to biomedical named entity recognition and train them with the GENIA corpus. We formulate the named entity recognition as the classificat"
W02-0301,W95-0107,0,0.0356952,"such expressions are annotated as a dummy class “temp”. 2 Table 1: Basic statistics of the GENIA corpus # of sentences # of words # of named entities # of words in NEs # of words not in NEs Av. length of NEs (σ) 5,109 152,216 23,793 50,229 101,987 2.11 (1.40) 3 Named Entity Recognition Using SVMs 3.1 Named Entity Recognition as Classification We formulate the named entity task as the classification of each word with context to one of the classes that represent region information and named entity’s semantic class. Several representations to encode region information are proposed and examined (Ramshaw and Marcus, 1995; Uchimoto et al., 2000; Kudo and Matsumoto, 2001). In this paper, we employ the simplest BIO representation, which is also used in (Yamada et al., 2000). We modify this representation in Section 5.1 in order to accelerate the SVM training. In the BIO representation, the region information is represented as the class prefixes “B-” and “I-”, and a class “O”. B- means that the current word is at the beginning of a named entity, I- means that the current word is in a named entity (but not at the beginning), and O means the word is not in a named entity. For each named entity class C, class B-C an"
W02-0301,W96-0213,0,0.284109,"Another reason for low accuracies is that biomedical named entities are essentially hard to recognize using standard feature sets compared with the named entities in newswire articles (Nobata et al., 2000). Thus, we need to employ powerful machine learning techniques which can incorporate various and complex features in a consistent way. Support Vector Machines (SVMs) (Vapnik, 1995) and Maximum Entropy (ME) method (Berger et al., 1996) are powerful learning methods that satisfy such requirements, and are applied successfully to other NLP tasks (Kudo and Matsumoto, 2000; Nakagawa et al., 2001; Ratnaparkhi, 1996). In this paper, we apply Support Vector Machines to biomedical named entity recognition and train them with the GENIA corpus. We formulate the named entity recognition as the classification of each word with context to one of the classes that represent region and named entity’s semantic class. Although there is a previous work that applied SVMs to biomedical named entity task in this formulation (Yamada et al., 2000), their method to construct a classifier using SVMs, one-vs-rest, fails to train a classifier with entire GENIA corpus, since the cost of SVM training is super-linear to the size"
W02-0301,P00-1042,0,0.00524534,"tated as a dummy class “temp”. 2 Table 1: Basic statistics of the GENIA corpus # of sentences # of words # of named entities # of words in NEs # of words not in NEs Av. length of NEs (σ) 5,109 152,216 23,793 50,229 101,987 2.11 (1.40) 3 Named Entity Recognition Using SVMs 3.1 Named Entity Recognition as Classification We formulate the named entity task as the classification of each word with context to one of the classes that represent region information and named entity’s semantic class. Several representations to encode region information are proposed and examined (Ramshaw and Marcus, 1995; Uchimoto et al., 2000; Kudo and Matsumoto, 2001). In this paper, we employ the simplest BIO representation, which is also used in (Yamada et al., 2000). We modify this representation in Section 5.1 in order to accelerate the SVM training. In the BIO representation, the region information is represented as the class prefixes “B-” and “I-”, and a class “O”. B- means that the current word is at the beginning of a named entity, I- means that the current word is in a named entity (but not at the beginning), and O means the word is not in a named entity. For each named entity class C, class B-C and I-C are produced. The"
W02-2227,P95-1013,0,0.0217347,"nature of strong equivalence (Yoshinaga et al., 2001b; Yoshinaga et al., 2001a), applications which contribute much to the developments of the two formalisms. In the past decades, LTAG and HPSG have received considerable attention as approaches to the formalization of natural languages in the field of computational linguistics. Discussion of the correspondences between the two formalisms has accompanied their development; that is, their linguistic relationships and differences have been investigated (Abeill´e, 1993; Kasper, 1998), as has conversion between two grammars in the two formalisms (Kasper et al., 1995; Tateisi et al., 1998; Becker and Lopez, 2000). These ongoing efforts have contributed greatly to the development of the two formalisms. Following this direction, in our earlier work (Yoshinaga and Miyao, 2001), we provided a method for converting grammars from LTAG to HPSG-style, which is the notion that we defined according to the computational device that underlies HPSG. We used the grammar conversion to obtain an HPSG-style grammar from LTAG (The XTAG Research Group, 2001), and then empirically showed strong equivalence between the LTAG and the obtained HPSG-style grammar for the sentence"
W02-2227,W00-2027,0,0.0223548,"rcinkiewicz, 1994). We exploited the nature of strong equivalence between the LTAG and the HPSG-style grammars to provide some applications such as sharing of existing resources between the two grammar formalisms (Yoshinaga et al., 2001b), a comparison of performance between parsers based on the two different formalisms (Yoshinaga et al., 2001a), and linguistic correspondence between the HPSG-style grammar and HPSG. As the most important result for the LTAG community, through the experiments of parsing within the above sentences, we showed that the empirical time complexity of an LTAG parser (Sarkar, 2000) is higher than that of an HPSG parser (Torisawa et al., 2000). This result is contrary to the general expectations from the viewpoint of the theoretical bound of worst time complexity, which is worth exploring further. However, the lack of the formal proof of strong equivalence restricts scope of the applications of our grammar conversion to grammars which are empirically attested the strong equivalence, and this prevents the applications from maximizing their true potential. In this paper we give a formal proof of strong equivalence between any LTAG and an HPSG-style grammar converted from"
W02-2227,C88-2121,0,0.21412,"Missing"
W02-2227,J95-4002,0,0.0747168,"Missing"
W02-2227,W01-1510,1,0.881133,"Missing"
W02-2227,J93-2004,0,\N,Missing
W02-2227,W98-0141,1,\N,Missing
W02-2232,2000.iwpt-1.9,0,0.265486,"n are in real-world texts, extracted grammars are practical for natural language processing. However, automatically extracted grammars are not systematically arranged according to syntactic classes their anchors belong to, like the XTAG grammar. Because of this, automatically extracted grammars tend to be strongly dependent on the corpus. This limitation can be a critical disadvantage of such extracted grammars when the grammars are used for various applications. Then, we want to arrange an extracted grammar according to the syntactic classes ofwords, without loosing the benefit for the cost. Chen and Vijay-Shanker (2000) proposed the solution to the issue. To improve the coverage of an extracted LTAG grammar, they classified the extracted elementary trees according to the tree families in the XTAG English grammar. First, the method searches for a tree farnily that contains an elementary tree template of extracted elementary tree et. Next, the method collects other possible tree templates in the tree family and makes elementary trees with the anchor of et and the tree templates. By using tree families, the method can add only proper elementary trees that correspond to the syntactic class of anchors. Chen and V"
W02-2232,C88-2121,0,0.20761,"Missing"
W02-2232,J93-2004,0,\N,Missing
W02-2232,P00-1058,0,\N,Missing
W03-0401,J97-4005,0,0.833904,"guation models of lexicalized grammars should be totally different from that of LPCFG, because the grammars define the relation of syntax and semantics, and can restrict the possible structure of parsing results. Parsing results cannot simply be decomposed into primitive dependencies, because the complete structure is determined by solving the syntactic constraints of a complete sentence. For example, when we apply a unification-based grammar, LPCFG-like modeling results in an inconsistent probability model because the model assigns probabilities to parsing results not allowed by the grammar (Abney, 1997). We have only two ways of adhering to LPCFG models: preserve the consistency of probability models by abandoning improvements to the lexicalized grammars using complex constraints (Chiang, 2000), or ignore the inconsistency in probability models (Clark et al., 2002). This paper provides a new model of syntactic disambiguation in which lexicalized grammars can restrict the possible structures of parsing results. Our modeling aims at providing grounds for i) producing a consistent probabilistic model of lexicalized grammars, as well as ii) evaluating the contributions of syntactic and semantic"
W03-0401,J96-1002,0,0.00877629,"Missing"
W03-0401,2000.iwpt-1.9,0,0.665026,"e dependencies of two words, our model selects the most probable parsing result from a set of candidates allowed by a lexicalized grammar. Since parsing results given by the lexicalized grammar cannot be decomposed into independent sub-events, we apply a maximum entropy model for feature forests, which allows probabilistic modeling without the independence assumption. Our approach provides a general method of producing a consistent probabilistic model of parsing results given by lexicalized grammars. 1 Introduction Recent studies on the automatic extraction of lexicalized grammars (Xia, 1999; Chen and Vijay-Shanker, 2000; Hockenmaier and Steedman, 2002a) allow the modeling of syntactic disambiguation based on linguistically motivated grammar theories including LTAG (Chiang, 2000) and CCG (Clark et al., 2002; Hockenmaier and Steedman, 2002b). However, existing models of disambiguation with lexicalized grammars are a mere extension of lexicalized probabilistic context-free grammars (LPCFG) (Collins, 1996; Collins, 1997; Charniak, 1997), which are based on the decomposition of parsing results into the syntactic/semantic dependencies of two words in a sentence under the assumption of independence of the dependenc"
W03-0401,hockenmaier-steedman-2002-acquiring,0,0.100344,"ur model selects the most probable parsing result from a set of candidates allowed by a lexicalized grammar. Since parsing results given by the lexicalized grammar cannot be decomposed into independent sub-events, we apply a maximum entropy model for feature forests, which allows probabilistic modeling without the independence assumption. Our approach provides a general method of producing a consistent probabilistic model of parsing results given by lexicalized grammars. 1 Introduction Recent studies on the automatic extraction of lexicalized grammars (Xia, 1999; Chen and Vijay-Shanker, 2000; Hockenmaier and Steedman, 2002a) allow the modeling of syntactic disambiguation based on linguistically motivated grammar theories including LTAG (Chiang, 2000) and CCG (Clark et al., 2002; Hockenmaier and Steedman, 2002b). However, existing models of disambiguation with lexicalized grammars are a mere extension of lexicalized probabilistic context-free grammars (LPCFG) (Collins, 1996; Collins, 1997; Charniak, 1997), which are based on the decomposition of parsing results into the syntactic/semantic dependencies of two words in a sentence under the assumption of independence of the dependencies. While LPCFG models have pro"
W03-0401,P02-1043,0,0.0485315,"ur model selects the most probable parsing result from a set of candidates allowed by a lexicalized grammar. Since parsing results given by the lexicalized grammar cannot be decomposed into independent sub-events, we apply a maximum entropy model for feature forests, which allows probabilistic modeling without the independence assumption. Our approach provides a general method of producing a consistent probabilistic model of parsing results given by lexicalized grammars. 1 Introduction Recent studies on the automatic extraction of lexicalized grammars (Xia, 1999; Chen and Vijay-Shanker, 2000; Hockenmaier and Steedman, 2002a) allow the modeling of syntactic disambiguation based on linguistically motivated grammar theories including LTAG (Chiang, 2000) and CCG (Clark et al., 2002; Hockenmaier and Steedman, 2002b). However, existing models of disambiguation with lexicalized grammars are a mere extension of lexicalized probabilistic context-free grammars (LPCFG) (Collins, 1996; Collins, 1997; Charniak, 1997), which are based on the decomposition of parsing results into the syntactic/semantic dependencies of two words in a sentence under the assumption of independence of the dependencies. While LPCFG models have pro"
W03-0401,P99-1069,0,0.644014,"002; Geman and Johnson, 2002), which allows parameters to be efficiently estimated. The algorithm enables probabilistic modeling of complete structures, such as transition sequences in Markov models and parse trees, without dividing them into independent sub-events. The algorithm avoids exponential explosion by representing a probabilistic event by a packed representation of a feature space. If a complete structure is represented with a feature forest of a tractable size, the parameters can be efficiently estimated by dynamic programming. A series of studies on parsing with wide-coverage LFG (Johnson et al., 1999; Riezler et al., 2000; Riezler et al., 2002) have had a similar motivation to ours. Their models have also been based on a discriminative model to select a parsing result from all candidates given by the grammar. A significant difference is that we apply maximum entropy estimation for feature forests to avoid the inherent problem with estimation: the exponential explosion of parsing results given by the grammar. They assumed that parsing results would be suppressed to a reasonable number through using heuristic rules, or by carefully implementing a fully restrictive and wide-coverage grammar,"
W03-0401,C94-1024,0,0.229074,"tactic categories. Formally, p(A|w) = p(c|w)p(A|c). The first probability in the above formula is the probability of syntactic categories, i.e., the probability of selecting a sequence of syntactic categories in a sentence. Since syntactic categories in lexicalized grammars determine the syntactic constraints of words, this expresses the syntactic preference of each word in a sentence. Note that our objective is not only to improve parsing accuracy but also to investigate the relation between syntax and semantics. We have not adopted the local contexts of words as in the supertaggers in LTAG (Joshi and Srinivas, 1994) because they partially include the semantic preferences of a sentence. The probability is purely unigram to select the probable syntactic category for each word. The probability is then given by the product of probabilities to select a syntactic category for each word from a set of candidate categories allowed by the lexicon.  p(ci |wi ) p(c|w) = i The second describes the probability of semantics, which expresses the semantic preferences of relating the words in a sentence. Note that the semantics probability is dependent on the syntactic categories determined by the syntax probability, bec"
W03-0401,H94-1020,0,0.0568261,"probabilities to parsing results not allowed by the grammar. • Since the syntax and semantics probabilities are separate, we can improve them individually. For example, the syntax model can be improved by smoothing using the syntactic classes of words, while the semantics model should be able to be improved by using semantic classes. In addition, the model can be a starting point that allows the theory of syntax and semantics to be evaluated through consulting an extensive corpus. We evaluated the validity of our model through experiments on a disambiguation task of parsing the Penn Treebank (Marcus et al., 1994) with an automatically acquired LTAG grammar. To assess the contribution of the syntax and semantics probabilities to the accuracy of parsing and to evaluate the validity of applying maximum entropy estimation for feature forests, we compared three models trained with the same training set and the same set of features. Following the experimental results, we concluded that i) a parser with the syntax probability only achieved high accuracy with the lexicalized grammar, ii) the incorporation of preferences for lexical association through the semantics probability resulted in significant improvem"
W03-0401,P00-1061,0,0.109313,", 2002), which allows parameters to be efficiently estimated. The algorithm enables probabilistic modeling of complete structures, such as transition sequences in Markov models and parse trees, without dividing them into independent sub-events. The algorithm avoids exponential explosion by representing a probabilistic event by a packed representation of a feature space. If a complete structure is represented with a feature forest of a tractable size, the parameters can be efficiently estimated by dynamic programming. A series of studies on parsing with wide-coverage LFG (Johnson et al., 1999; Riezler et al., 2000; Riezler et al., 2002) have had a similar motivation to ours. Their models have also been based on a discriminative model to select a parsing result from all candidates given by the grammar. A significant difference is that we apply maximum entropy estimation for feature forests to avoid the inherent problem with estimation: the exponential explosion of parsing results given by the grammar. They assumed that parsing results would be suppressed to a reasonable number through using heuristic rules, or by carefully implementing a fully restrictive and wide-coverage grammar, which requires a cons"
W03-0401,P00-1058,0,0.490694,"icalized grammar cannot be decomposed into independent sub-events, we apply a maximum entropy model for feature forests, which allows probabilistic modeling without the independence assumption. Our approach provides a general method of producing a consistent probabilistic model of parsing results given by lexicalized grammars. 1 Introduction Recent studies on the automatic extraction of lexicalized grammars (Xia, 1999; Chen and Vijay-Shanker, 2000; Hockenmaier and Steedman, 2002a) allow the modeling of syntactic disambiguation based on linguistically motivated grammar theories including LTAG (Chiang, 2000) and CCG (Clark et al., 2002; Hockenmaier and Steedman, 2002b). However, existing models of disambiguation with lexicalized grammars are a mere extension of lexicalized probabilistic context-free grammars (LPCFG) (Collins, 1996; Collins, 1997; Charniak, 1997), which are based on the decomposition of parsing results into the syntactic/semantic dependencies of two words in a sentence under the assumption of independence of the dependencies. While LPCFG models have proved that the incorporation of lexical associations (i.e., dependencies of words) significantly improves the accuracy of parsing, t"
W03-0401,P02-1035,0,0.100317,"parameters to be efficiently estimated. The algorithm enables probabilistic modeling of complete structures, such as transition sequences in Markov models and parse trees, without dividing them into independent sub-events. The algorithm avoids exponential explosion by representing a probabilistic event by a packed representation of a feature space. If a complete structure is represented with a feature forest of a tractable size, the parameters can be efficiently estimated by dynamic programming. A series of studies on parsing with wide-coverage LFG (Johnson et al., 1999; Riezler et al., 2000; Riezler et al., 2002) have had a similar motivation to ours. Their models have also been based on a discriminative model to select a parsing result from all candidates given by the grammar. A significant difference is that we apply maximum entropy estimation for feature forests to avoid the inherent problem with estimation: the exponential explosion of parsing results given by the grammar. They assumed that parsing results would be suppressed to a reasonable number through using heuristic rules, or by carefully implementing a fully restrictive and wide-coverage grammar, which requires a considerable amount of effo"
W03-0401,P02-1042,0,0.638011,"be decomposed into independent sub-events, we apply a maximum entropy model for feature forests, which allows probabilistic modeling without the independence assumption. Our approach provides a general method of producing a consistent probabilistic model of parsing results given by lexicalized grammars. 1 Introduction Recent studies on the automatic extraction of lexicalized grammars (Xia, 1999; Chen and Vijay-Shanker, 2000; Hockenmaier and Steedman, 2002a) allow the modeling of syntactic disambiguation based on linguistically motivated grammar theories including LTAG (Chiang, 2000) and CCG (Clark et al., 2002; Hockenmaier and Steedman, 2002b). However, existing models of disambiguation with lexicalized grammars are a mere extension of lexicalized probabilistic context-free grammars (LPCFG) (Collins, 1996; Collins, 1997; Charniak, 1997), which are based on the decomposition of parsing results into the syntactic/semantic dependencies of two words in a sentence under the assumption of independence of the dependencies. While LPCFG models have proved that the incorporation of lexical associations (i.e., dependencies of words) significantly improves the accuracy of parsing, this idea has been naively in"
W03-0401,P96-1025,0,0.448362,"ethod of producing a consistent probabilistic model of parsing results given by lexicalized grammars. 1 Introduction Recent studies on the automatic extraction of lexicalized grammars (Xia, 1999; Chen and Vijay-Shanker, 2000; Hockenmaier and Steedman, 2002a) allow the modeling of syntactic disambiguation based on linguistically motivated grammar theories including LTAG (Chiang, 2000) and CCG (Clark et al., 2002; Hockenmaier and Steedman, 2002b). However, existing models of disambiguation with lexicalized grammars are a mere extension of lexicalized probabilistic context-free grammars (LPCFG) (Collins, 1996; Collins, 1997; Charniak, 1997), which are based on the decomposition of parsing results into the syntactic/semantic dependencies of two words in a sentence under the assumption of independence of the dependencies. While LPCFG models have proved that the incorporation of lexical associations (i.e., dependencies of words) significantly improves the accuracy of parsing, this idea has been naively inherited in the recent studies on disambiguation models of lexicalized grammars. Jun’ichi Tsujii Department of Computer Science, University of Tokyo CREST, JST (Japan Science and Technology Corporatio"
W03-0401,C88-2121,0,0.233619,"Missing"
W03-0401,P97-1003,0,0.781556,"ing a consistent probabilistic model of parsing results given by lexicalized grammars. 1 Introduction Recent studies on the automatic extraction of lexicalized grammars (Xia, 1999; Chen and Vijay-Shanker, 2000; Hockenmaier and Steedman, 2002a) allow the modeling of syntactic disambiguation based on linguistically motivated grammar theories including LTAG (Chiang, 2000) and CCG (Clark et al., 2002; Hockenmaier and Steedman, 2002b). However, existing models of disambiguation with lexicalized grammars are a mere extension of lexicalized probabilistic context-free grammars (LPCFG) (Collins, 1996; Collins, 1997; Charniak, 1997), which are based on the decomposition of parsing results into the syntactic/semantic dependencies of two words in a sentence under the assumption of independence of the dependencies. While LPCFG models have proved that the incorporation of lexical associations (i.e., dependencies of words) significantly improves the accuracy of parsing, this idea has been naively inherited in the recent studies on disambiguation models of lexicalized grammars. Jun’ichi Tsujii Department of Computer Science, University of Tokyo CREST, JST (Japan Science and Technology Corporation) tsujii@is.s."
W03-0401,P02-1036,0,0.565105,"imum entropy models (Berger et al., 1996) and support vector machines (Vapnik, 1995) provide grounds for this type of modeling, because it allows various dependent features to be incorporated into the model without the independence assumption. The above approach, however, has a serious deficiency: a lexicalized grammar assigns exponentially many parsing results because of local ambiguities in a sentence, which is problematic in estimating the parameters of a probability model. To cope with this, we adopted an algorithm of maximum entropy estimation for feature forests (Miyao and Tsujii, 2002; Geman and Johnson, 2002), which allows parameters to be efficiently estimated. The algorithm enables probabilistic modeling of complete structures, such as transition sequences in Markov models and parse trees, without dividing them into independent sub-events. The algorithm avoids exponential explosion by representing a probabilistic event by a packed representation of a feature space. If a complete structure is represented with a feature forest of a tractable size, the parameters can be efficiently estimated by dynamic programming. A series of studies on parsing with wide-coverage LFG (Johnson et al., 1999; Riezler"
W03-0416,C02-1114,0,\N,Missing
W03-0416,J92-4003,0,\N,Missing
W03-0416,J98-1004,0,\N,Missing
W03-0416,P98-2124,0,\N,Missing
W03-0416,C98-2119,0,\N,Missing
W03-0417,P01-1005,0,0.0560337,"Missing"
W03-0417,J96-1002,0,0.0230026,"Missing"
W03-0417,W99-0613,0,0.0464709,"ne of the major obstacles to applying machine learning techniques to real-world NLP applications. Recently, learning algorithms called minimally supervised learning or unsupervised learning that can make use of unlabeled data have received much attention. Since collecting unlabeled data is generally much easier than annotating data, such techniques have potential for solving the problem of annotation cost. Those approaches include a naive Bayes classifier combined with the EM algorithm (Dempster et al., 1977; Nigam et al., 2000; Pedersen and Bruce, 1998), Co-training (Blum and Mitchell, 1998; Collins and Singer, 1999; Nigam and Ghani, 2000), and Transductive Support Vector Machines (Joachims, 1999). These algorithms have been applied to some tasks including text classification and word sense disambiguation and their effectiveness has been demonstrated to some extent. Combining a naive Bayes classifier with the EM algorithm is one of the promising minimally supervised approaches because its computational cost is low (linear to the size of unlabeled data), and it does not require the features to be split into two independent sets unlike cotraining. However, the use of unlabeled data via the basic EM algorit"
W03-0417,A00-2009,0,0.0173806,"and how to impose this constraint on the learning process. Section 4 describes the problem of confusion set disambiguation and the features used in the experiments. Experimental results are presented in Section 5. Related work is discussed in Section 6. Section 7 offers some concluding remarks. 2 Naive Bayes Classifier The naive Bayes classifier is a simple but effective classifier which has been used in numerous applications of information processing such as image recognition, natural language processing, information retrieval, etc. (Escudero et al., 2000; Lewis, 1998; Nigam and Ghani, 2000; Pedersen, 2000). In this section, we briefly review the naive Bayes classifier and the EM algorithm that is used for making use of unlabeled data. 2.1 Naive Bayes Model Let x be a vector we want to classify, and c k be a possible class. What we want to know is the probability that the vector x belongs to the class c k . We first transform the probability P (c k |x) using Bayes’ rule, P (x|ck ) . P (x) word occurrences. Despite this apparent violation of the assumption, the naive Bayes classifier exhibits good performance for various natural language processing tasks. There are some implementation varian"
W03-0417,P94-1013,0,0.0404462,"n is defined as the problem of choosing the correct word from a set of words that are commonly confused. For example, quite may easily be mistyped as quiet. An automatic proofreading system would need to judge which is the correct use given the context surrounding the target. Example confusion sets include: {principle, principal}, {then, than}, and {weather, whether}. Until now, many methods have been proposed for this problem including winnow-based algorithms (Golding and Roth, 1999), differential grammars (Powers, 1998), transformation based learning (Mangu and Brill, 1997), decision lists (Yarowsky, 1994). Confusion set disambiguation has very similar characteristics to a word sense disambiguation problem in which the system has to identify the meaning of a polysemous word given the surrounding context. The merit of using confusion set disambiguation as a test-bed for a learning algorithm is that since one does not need to annotate the examples to make labeled data, one can conduct experiments using an arbitrary amount of labeled data. 4.1 Features As the input of the classifier, the context of the target must be represented in the form of a vector. We use a binary feature vector which contain"
W03-0417,P95-1026,0,0.172867,"Missing"
W03-0417,W97-1011,0,\N,Missing
W03-1018,J96-1002,0,0.0110736,"n feature expectations. However, the equality constraint is inappropriate for sparse and therefore unreliable features. This study explores an ME model with box-type inequality constraints, where the equality can be violated to reflect this unreliability. We evaluate the inequality ME model using text categorization datasets. We also propose an extension of the inequality ME model, which results in a natural integration with the Gaussian MAP estimation. Experimental results demonstrate the advantage of the inequality models and the proposed extension. 1 Introduction The maximum entropy model (Berger et al., 1996; Pietra et al., 1997) has attained great popularity in the NLP field due to its power, robustness, and successful performance in various NLP tasks (Ratnaparkhi, 1996; Nigam et al., 1999; Borthwick, 1999). In the ME estimation, an event is decomposed into features, which indicate the strength of certain aspects in the event, and the most uniform model among the models that satisfy: to the model being estimated. A powerful and robust estimation is possible since the features can be as specific or general as required and does not need to be independent of each other, and since the most uniform m"
W03-1018,W02-2018,0,0.0935761,"(x, y)), (7) pλ (y|x) = Z(x) i  where Z(x) = y exp( i λi fi (x, y)). The dual objective function becomes:    L(λ) = x p˜(x) y p˜(y|x) i λi fi (x, y) (8)    − x p˜(x) log y exp( i λi fi (x, y)).  The ME estimation becomes the maximization of L(λ). And it is equivalent to the  maximization of the log-likelihood: LL(λ) = log x,y pλ (y|x)p˜(x,y) . This optimization can be solved using algorithms such as the GIS algorithm (Darroch and Ratcliff, 1972) and the IIS algorithm (Pietra et al., 1997). In addition, gradient-based algorithms can be applied since the objective function is concave. Malouf (2002) compares several algorithms for the ME estimation including GIS, IIS, and the limitedmemory variable metric (LMVM) method, which is a gradient-based method, and shows that the LMVM method requires much less time to converge for real NLP datasets. We also observed that the LMVM method converges very quickly for the text categorization datasets with an improvement in accuracy. Therefore, we use the LMVM method (and its variant for the inequality models) throughout the experiments. Thus, we only show the gradient when mentioning the training. The gradient of the objective function (8) is compute"
W03-1018,W96-0213,0,0.04206,"equality constraints, where the equality can be violated to reflect this unreliability. We evaluate the inequality ME model using text categorization datasets. We also propose an extension of the inequality ME model, which results in a natural integration with the Gaussian MAP estimation. Experimental results demonstrate the advantage of the inequality models and the proposed extension. 1 Introduction The maximum entropy model (Berger et al., 1996; Pietra et al., 1997) has attained great popularity in the NLP field due to its power, robustness, and successful performance in various NLP tasks (Ratnaparkhi, 1996; Nigam et al., 1999; Borthwick, 1999). In the ME estimation, an event is decomposed into features, which indicate the strength of certain aspects in the event, and the most uniform model among the models that satisfy: to the model being estimated. A powerful and robust estimation is possible since the features can be as specific or general as required and does not need to be independent of each other, and since the most uniform model avoids overfitting the training data. In spite of these advantages, the ME model still suffers from a lack of data as long as it imposes the equality constraint"
W03-1306,W02-0301,1,0.356425,"2001; Thomas et al., 2000; Ono et al., 2001). To extract information of proteins, one has to first recognize protein names in a text. This kind of problem has been studied in the field of natural language processing as named entity recognition tasks. Ohta et al. (2002) provided the GENIA corpus, an annotated corpus of MEDLINE abstracts, which can be used as a gold-standard for evaluating and training named entity recognition algorithms. There are some research efforts using machine learning techniques to recognize biological entities in texts (Takeuchi and Collier, 2002; Kim and Tsujii, 2002; Kazama et al., 2002). One drawback of these machine learning based approaches is that they do not provide identification information of recognized terms. For the purpose of information extraction of protein-protein interaction, the ID information of recognized proteins, such as GenBank 1 ID or SwissProt 2 ID, is indispensable to integrate the extracted information with the data in other information sources. Dictionary-based approaches, on the other hand, intrinsically provide ID information because they recognize a term by searching the most similar (or identical) one in the dictionary to the target term. This ad"
W03-1306,A00-2009,0,0.0124049,"duct binary classification (“accept” or “reject”) on each candidate. The candidates that are classified into “rejected” are filtered out. In other words, only the candidates that are classified into “accepted” are recognized as protein names. In this paper, we use a naive Bayes classifier for this classification task. 4.1 Naive Bayes classifier The naive Bayes classifier is a simple but effective classifier which has been used in numerous applications of information processing such as image recognition, natural language processing and information retrieval (Lewis, 1998; Escudero et al., 2000; Pedersen, 2000; Nigam and Ghani, 2000). Here we briefly review the naive Bayes model. Let ~x be a vector we want to classify, and c k be a possible class. What we want to know is the probability that the vector ~x belongs to the class c k . We first transform the probability P (c k |~x) using Bayes’ rule, P (~x|ck ) (5) P (ck |~x) = P (ck ) × P (~x) Class probability P (ck ) can be estimated from training data. However, direct estimation of P (c k |~x) is impossible in most cases because of the sparseness of training data. By assuming the conditional independence among the elements of a vector, P (~x|c k )"
W03-1306,W02-2029,0,0.0332044,"the most important tasks today (Marcotte et al., 2001; Thomas et al., 2000; Ono et al., 2001). To extract information of proteins, one has to first recognize protein names in a text. This kind of problem has been studied in the field of natural language processing as named entity recognition tasks. Ohta et al. (2002) provided the GENIA corpus, an annotated corpus of MEDLINE abstracts, which can be used as a gold-standard for evaluating and training named entity recognition algorithms. There are some research efforts using machine learning techniques to recognize biological entities in texts (Takeuchi and Collier, 2002; Kim and Tsujii, 2002; Kazama et al., 2002). One drawback of these machine learning based approaches is that they do not provide identification information of recognized terms. For the purpose of information extraction of protein-protein interaction, the ID information of recognized proteins, such as GenBank 1 ID or SwissProt 2 ID, is indispensable to integrate the extracted information with the data in other information sources. Dictionary-based approaches, on the other hand, intrinsically provide ID information because they recognize a term by searching the most similar (or identical) one i"
W03-1313,W03-2416,1,0.916478,"emas, XPointer, SAX, etc. The higher level standards, of meta-data (RDF) and ontologies (OWL) have been especially influential in encoding biomedical resources. However, there remains the question how to best encode the structure of the text themselves, how to mark-up added linguistic analyses, and how to implement linkages between the text and and further resources, such as lexica, thesauri and ontologies. As discussed in (Ide and Brew, 2000), in order to qualify as a “good” annotated corpus, its encoding should provide for reusabilty and extensibily. In this paper we build on previous work (Erjavec et al., 2003) and show how to develop a standardised encoding for biomedical corpora. We base our discussion on the case of the GENIA corpus (Ohta et al., 2002), which is originaly encoded in GPML, the GENIA Project Markup Language, an XML DTD. We re-encode the corpus into a standardised annotation scheme, based on the Text Encoding Initiative Guidelines P4 (Sperberg-McQueen and Burnard, 2002), and specify a constructive mapping from the original DTD to the developed encoding via a XSLT transformation. One of the motivations for such an re-encoding is that TEI is well-designed and widely accepted architect"
W03-1313,W02-1706,0,0.0309681,"r direct reference to the token stream of the text, so if this is incorrect, errors will propagate to all other annotations. It is also interesting to note that current annotation practice is more and more leaning toward standoff markup, i.e., annotations that are separated from the primary data (text) and make reference to it only via pointers. However, it is beneficial to have some markup in the primary data to which it is possible to refer, and this markup is, almost exclusivelly, that of tokens; see e.g., (Freese et al., 2003). Version V1.1 of GENIA has been also annotated with LTG tools (Grover et al., 2002). In short, the corpus is tokenised, and then part-of-speech tagged with two taggers, each one using a different tagset, and the nouns and verbs lemmatised. Additionally, the deverbal nominalisations are assigned their verbal stems. The conversion to TEI is also able to handle this additional markup, by using the TEI.analysis module. The word and punctuation tokens are encoded as hwi and hci elements respectively, which are further marked with type and lemma and the locally defined c1, c2 and vstem. An example of such markup is given in Figure 3. Given the high density of technical terms, biom"
W04-3314,J87-3002,0,0.256158,"Missing"
W04-3314,P02-1029,0,0.0307799,"e attempt to compensate for the lack of recall by being less strict in filtering out less likely SCFs, then we will end up with a larger number of lexical entries. This is fatal for parsing Jun’ichi Tsujii†‡ ‡ CREST, JST 4-1-8, Honcho, Kawaguchi-shi, Saitama, 332-0012 Japan tsujii@is.s.u-tokyo.ac.jp with lexicalized grammars, because empirical parsing efficiency and syntactic ambiguity of lexicalized grammars are known to be proportional to the number of lexical entries used in parsing (Sarkar et al., 2000). We therefore need some method to improve the quality of the acquired SCFs. Schulte im Walde and Brew (2002) and Korhonen (2003) employed clustering of verb SCF (probability) distributions to induce verb semantic classes. Their studies are based on the assumption that verb SCF distributions are closely related to verb semantic classes. Conversely, if we could induce word classes whose element words have the same set of SCFs, we can eliminate SCFs acquired in error from the corpora and predict plausible SCFs unseen in the corpora. This kind of generalization would be useful to improve the quality of the acquired SCFs. In this paper, we present a method of generalizing SCFs acquired from corpora in or"
W04-3314,A97-1052,0,0.163114,"f the acquired SCFs by clustering vectors obtained from the acquired SCF lexicon and the lexicon of the target grammar. We apply our method to SCFs acquired from corpora by using a subset of the SCF lexicon of the XTAG English grammar. A comparison between the resulting SCF lexicon and the rest of the lexicon of the XTAG English grammar reveals that we can achieve higher precision and recall compared to naive frequency cut-off. 1 Introduction Recently, a variety of methods have been proposed for automatic acquisition of subcategorization frames (SCFs) from corpora (Brent, 1993; Manning, 1993; Briscoe and Carroll, 1997; Sarkar and Zeman, 2000; Korhonen, 2002). Although these research efforts aimed at enhancing lexicon resources, there has been little work on evaluating the impact of acquired SCFs on grammar coverage using large-scale lexicalized grammars with the exception of (Carroll and Fang, 2004). The problem when we combine acquired SCFs with existing lexicalized grammars is lower quality of the acquired SCFs, since they are acquired in an unsupervised manner, rather than being manually coded. If we attempt to compensate for the lack of recall by being less strict in filtering out less likely SCFs, the"
W04-3314,C94-1042,0,0.101034,"Missing"
W04-3314,P03-1009,0,0.0307254,"Missing"
W04-3314,C00-2100,0,0.0194804,"tering vectors obtained from the acquired SCF lexicon and the lexicon of the target grammar. We apply our method to SCFs acquired from corpora by using a subset of the SCF lexicon of the XTAG English grammar. A comparison between the resulting SCF lexicon and the rest of the lexicon of the XTAG English grammar reveals that we can achieve higher precision and recall compared to naive frequency cut-off. 1 Introduction Recently, a variety of methods have been proposed for automatic acquisition of subcategorization frames (SCFs) from corpora (Brent, 1993; Manning, 1993; Briscoe and Carroll, 1997; Sarkar and Zeman, 2000; Korhonen, 2002). Although these research efforts aimed at enhancing lexicon resources, there has been little work on evaluating the impact of acquired SCFs on grammar coverage using large-scale lexicalized grammars with the exception of (Carroll and Fang, 2004). The problem when we combine acquired SCFs with existing lexicalized grammars is lower quality of the acquired SCFs, since they are acquired in an unsupervised manner, rather than being manually coded. If we attempt to compensate for the lack of recall by being less strict in filtering out less likely SCFs, then we will end up with a"
W04-3314,W00-1605,0,0.0176536,"the acquired SCFs, since they are acquired in an unsupervised manner, rather than being manually coded. If we attempt to compensate for the lack of recall by being less strict in filtering out less likely SCFs, then we will end up with a larger number of lexical entries. This is fatal for parsing Jun’ichi Tsujii†‡ ‡ CREST, JST 4-1-8, Honcho, Kawaguchi-shi, Saitama, 332-0012 Japan tsujii@is.s.u-tokyo.ac.jp with lexicalized grammars, because empirical parsing efficiency and syntactic ambiguity of lexicalized grammars are known to be proportional to the number of lexical entries used in parsing (Sarkar et al., 2000). We therefore need some method to improve the quality of the acquired SCFs. Schulte im Walde and Brew (2002) and Korhonen (2003) employed clustering of verb SCF (probability) distributions to induce verb semantic classes. Their studies are based on the assumption that verb SCF distributions are closely related to verb semantic classes. Conversely, if we could induce word classes whose element words have the same set of SCFs, we can eliminate SCFs acquired in error from the corpora and predict plausible SCFs unseen in the corpora. This kind of generalization would be useful to improve the qual"
W04-3314,C88-2121,0,0.0788923,"Missing"
W04-3314,P93-1032,0,\N,Missing
W04-3323,W98-0108,0,0.0297342,"mmar and LTAG grammars which are extracted from the Penn Treebank, and investigated characteristics of the obtained CFGs. We perform CFG filtering for LTAG by the obtained CFG. In the experiments, we describe that the obtained CFG is useful for CFG filtering for LTAG parser. 1 Introduction Recently, lexicalized grammars such as Lexicalized Tree Adjoining Grammar (LTAG) (Schabes et al., 1988) and Head-Driven Phrase Structure Grammar (HPSG) (Pollard and Sag, 1994) have attracted much attention in practical application context (Deep Thought Project, 2003; Kototoi Project, 2001; Kay et al., 1994; Carroll et al., 1998). However, inefficiency of parsing with those grammars have prevented us from adopting them for practical usage. Especially in the LTAG framework, although many studies proposed parsers that are theoretically efficient (Vijay-Shanker and Joshi, 1985; Schabes and Joshi, 1988; van Noord, 1994; Nederhof, 1998), we do not attain any practical LTAG parser that runs efficiently with large-scale hand-crafted grammars such as the XTAG English grammar (XTAG Research Group, 2001). Yoshinaga et al. (Yoshinaga et al., 2003) demonstrated that a drastic speed-up of LTAG parsing can be achieved when a LTAG g"
W04-3323,P90-1036,0,0.209058,"s foot node Input “I can run” run correct parse trees parsing by the lexicalized grammar parsing by CFG N I Figure 2: CFG filtering Figure 1: LTAG: elementary trees, substitution and adjunction S “NP” of α1 is replaced by α2 , which has a root node labeled “NP.” Adjunction replaces an internal node of an elementary tree by another elementary tree whose root node and one leaf node called a foot node have the same label as the internal node. In Figure 1, the internal node labeled “VP” of α1 is replaced by β 2 , which has a root node and a foot node labeled “VP.” 2.2 CFG filtering CFG filtering (Harbusch, 1990; Maxwell III and Kaplan, 1993; Torisawa and Tsujii, 1996) is a parsing scheme that filters out impossible parse trees using a CFG extracted from a given grammar prior to parsing. In CFG filtering, we first perform an off-line extraction of a CFG from a given grammar, (Context-free (CF) approximation). By using the obtained CFG we can compute efficiently the necessary condition for parse trees the original grammar could generate. Parsing using the obtained CFG as a filter comprises two phases (Figure 2). In the first phase, we parse a sentence by the obtained CFG. In this phase, the necessary"
W04-3323,2000.iwpt-1.15,0,0.281558,"Especially in the LTAG framework, although many studies proposed parsers that are theoretically efficient (Vijay-Shanker and Joshi, 1985; Schabes and Joshi, 1988; van Noord, 1994; Nederhof, 1998), we do not attain any practical LTAG parser that runs efficiently with large-scale hand-crafted grammars such as the XTAG English grammar (XTAG Research Group, 2001). Yoshinaga et al. (Yoshinaga et al., 2003) demonstrated that a drastic speed-up of LTAG parsing can be achieved when a LTAG grammar is compiled into a HPSG (Yoshinaga and Miyao, 2002) and a CFG filtering technique for HPSG-Style grammar (Kiefer and Krieger, 2000; Torisawa et al., 2000) is applied to the obtained HPSG. In experiments with the XTAG English grammar, they found that an HPSG parser with CFG filtering (Torisawa et al., 2000) outperformed a theoretically efficient LTAG parser (Sarkar, 2000) in terms of empirical time complexity. Although their approach does not guarantee the theoretical bound of parsing complexity, O(n 6 ) for a sentence of length n, the empirical results of their CFG filtering are still satisfactory. In this paper, we propose a novel context-free approximation method for LTAG by reinterpreting the method by Yoshinaga et al"
W04-3323,J93-4001,0,0.0275722,"Missing"
W04-3323,E03-1047,1,0.804804,"by Yoshinaga et al. in the context of LTAG parsing. A fundamental idea is to enumerate partial parse results that can be generated during parsing. We assign CFG nonterminal labels to the partial parse results, and then regard their possible combinations as CFG rules. In order to investigate the characteristics of CFGs produced by our method, we applied our method to two kinds of LTAG grammars. One is the XTAG English grammar, which is a large-scale hand-crafted LTAG, and the other is LTAG grammars extracted from Penn Treebank Wall Street Journal by the grammar extraction method described in (Miyao et al., 2003). Then, we compare parsing speed of a CKY parser using the obtained CFG with parsing speed of an existing LTAG parser. The remainder of the paper is organized as follows. Section 2 introduces background of our research. Section 3 describes our approximation method. Section 4 reports experimental results with the two kinds of LTAG grammars. 2 Background 2.1 Lexicalized Tree-Adjoining Grammar (LTAG) An LTAG consists of a set of tree structures, which are assigned to words, called elementary trees. A parse tree is derived by combining elementary trees using two grammar rules called substitution a"
W04-3323,P98-2156,0,0.0242972,"rammars such as Lexicalized Tree Adjoining Grammar (LTAG) (Schabes et al., 1988) and Head-Driven Phrase Structure Grammar (HPSG) (Pollard and Sag, 1994) have attracted much attention in practical application context (Deep Thought Project, 2003; Kototoi Project, 2001; Kay et al., 1994; Carroll et al., 1998). However, inefficiency of parsing with those grammars have prevented us from adopting them for practical usage. Especially in the LTAG framework, although many studies proposed parsers that are theoretically efficient (Vijay-Shanker and Joshi, 1985; Schabes and Joshi, 1988; van Noord, 1994; Nederhof, 1998), we do not attain any practical LTAG parser that runs efficiently with large-scale hand-crafted grammars such as the XTAG English grammar (XTAG Research Group, 2001). Yoshinaga et al. (Yoshinaga et al., 2003) demonstrated that a drastic speed-up of LTAG parsing can be achieved when a LTAG grammar is compiled into a HPSG (Yoshinaga and Miyao, 2002) and a CFG filtering technique for HPSG-Style grammar (Kiefer and Krieger, 2000; Torisawa et al., 2000) is applied to the obtained HPSG. In experiments with the XTAG English grammar, they found that an HPSG parser with CFG filtering (Torisawa et al.,"
W04-3323,W98-0134,0,0.0229444,"enerated parse trees, and eliminate overgenerated parse trees. The performance of parsers with CFG filtering depends on the degree of the CF approximation (Yoshinaga et al., 2003). If CF approximation is good, the number of overgenerated parse trees is small. Thus, the key to achieve efficiency in LTAG parsing is to maintain grammatical restrictions in CFG as efficiently as possible. The more of the grammatical constraints in the given grammar the obtained CFG captures, the more effectively we can restrict the search space. There are existing CFG filtering techniques for LTAG (Harbusch, 1990; Poller and Becker, 1998). These techniques extract CFG rules by simply dividing elementary trees into branching structures as shown in Figure 3. Since the obtained CFG can capture only local constraints CFG rules VP NP V S NP VP NP VP V NP Figure 3: The existing CF approximation for LTAG given in the elementary trees, we must examine many global constraints in the second phase. CFG filtering techniques have also been developed for HPSG (Torisawa and Tsujii, 1996; Torisawa et al., 2000; Kiefer and Krieger, 2000). CFG rules are extracted by applying grammar rules to lexical entries and by enumerating partial parse resu"
W04-3323,W00-2027,0,0.0176303,"ciently with large-scale hand-crafted grammars such as the XTAG English grammar (XTAG Research Group, 2001). Yoshinaga et al. (Yoshinaga et al., 2003) demonstrated that a drastic speed-up of LTAG parsing can be achieved when a LTAG grammar is compiled into a HPSG (Yoshinaga and Miyao, 2002) and a CFG filtering technique for HPSG-Style grammar (Kiefer and Krieger, 2000; Torisawa et al., 2000) is applied to the obtained HPSG. In experiments with the XTAG English grammar, they found that an HPSG parser with CFG filtering (Torisawa et al., 2000) outperformed a theoretically efficient LTAG parser (Sarkar, 2000) in terms of empirical time complexity. Although their approach does not guarantee the theoretical bound of parsing complexity, O(n 6 ) for a sentence of length n, the empirical results of their CFG filtering are still satisfactory. In this paper, we propose a novel context-free approximation method for LTAG by reinterpreting the method by Yoshinaga et al. in the context of LTAG parsing. A fundamental idea is to enumerate partial parse results that can be generated during parsing. We assign CFG nonterminal labels to the partial parse results, and then regard their possible combinations as CFG"
W04-3323,P88-1032,0,0.0836776,"er. 1 Introduction Recently, lexicalized grammars such as Lexicalized Tree Adjoining Grammar (LTAG) (Schabes et al., 1988) and Head-Driven Phrase Structure Grammar (HPSG) (Pollard and Sag, 1994) have attracted much attention in practical application context (Deep Thought Project, 2003; Kototoi Project, 2001; Kay et al., 1994; Carroll et al., 1998). However, inefficiency of parsing with those grammars have prevented us from adopting them for practical usage. Especially in the LTAG framework, although many studies proposed parsers that are theoretically efficient (Vijay-Shanker and Joshi, 1985; Schabes and Joshi, 1988; van Noord, 1994; Nederhof, 1998), we do not attain any practical LTAG parser that runs efficiently with large-scale hand-crafted grammars such as the XTAG English grammar (XTAG Research Group, 2001). Yoshinaga et al. (Yoshinaga et al., 2003) demonstrated that a drastic speed-up of LTAG parsing can be achieved when a LTAG grammar is compiled into a HPSG (Yoshinaga and Miyao, 2002) and a CFG filtering technique for HPSG-Style grammar (Kiefer and Krieger, 2000; Torisawa et al., 2000) is applied to the obtained HPSG. In experiments with the XTAG English grammar, they found that an HPSG parser wi"
W04-3323,C88-2121,0,0.079462,"Missing"
W04-3323,P03-2036,1,0.829607,"cation context (Deep Thought Project, 2003; Kototoi Project, 2001; Kay et al., 1994; Carroll et al., 1998). However, inefficiency of parsing with those grammars have prevented us from adopting them for practical usage. Especially in the LTAG framework, although many studies proposed parsers that are theoretically efficient (Vijay-Shanker and Joshi, 1985; Schabes and Joshi, 1988; van Noord, 1994; Nederhof, 1998), we do not attain any practical LTAG parser that runs efficiently with large-scale hand-crafted grammars such as the XTAG English grammar (XTAG Research Group, 2001). Yoshinaga et al. (Yoshinaga et al., 2003) demonstrated that a drastic speed-up of LTAG parsing can be achieved when a LTAG grammar is compiled into a HPSG (Yoshinaga and Miyao, 2002) and a CFG filtering technique for HPSG-Style grammar (Kiefer and Krieger, 2000; Torisawa et al., 2000) is applied to the obtained HPSG. In experiments with the XTAG English grammar, they found that an HPSG parser with CFG filtering (Torisawa et al., 2000) outperformed a theoretically efficient LTAG parser (Sarkar, 2000) in terms of empirical time complexity. Although their approach does not guarantee the theoretical bound of parsing complexity, O(n 6 ) f"
W04-3323,C96-2160,1,0.905031,"e trees parsing by the lexicalized grammar parsing by CFG N I Figure 2: CFG filtering Figure 1: LTAG: elementary trees, substitution and adjunction S “NP” of α1 is replaced by α2 , which has a root node labeled “NP.” Adjunction replaces an internal node of an elementary tree by another elementary tree whose root node and one leaf node called a foot node have the same label as the internal node. In Figure 1, the internal node labeled “VP” of α1 is replaced by β 2 , which has a root node and a foot node labeled “VP.” 2.2 CFG filtering CFG filtering (Harbusch, 1990; Maxwell III and Kaplan, 1993; Torisawa and Tsujii, 1996) is a parsing scheme that filters out impossible parse trees using a CFG extracted from a given grammar prior to parsing. In CFG filtering, we first perform an off-line extraction of a CFG from a given grammar, (Context-free (CF) approximation). By using the obtained CFG we can compute efficiently the necessary condition for parse trees the original grammar could generate. Parsing using the obtained CFG as a filter comprises two phases (Figure 2). In the first phase, we parse a sentence by the obtained CFG. In this phase, the necessary condition represented by the CFG acts as a filter of parse"
W04-3323,P85-1011,0,0.130692,"for CFG filtering for LTAG parser. 1 Introduction Recently, lexicalized grammars such as Lexicalized Tree Adjoining Grammar (LTAG) (Schabes et al., 1988) and Head-Driven Phrase Structure Grammar (HPSG) (Pollard and Sag, 1994) have attracted much attention in practical application context (Deep Thought Project, 2003; Kototoi Project, 2001; Kay et al., 1994; Carroll et al., 1998). However, inefficiency of parsing with those grammars have prevented us from adopting them for practical usage. Especially in the LTAG framework, although many studies proposed parsers that are theoretically efficient (Vijay-Shanker and Joshi, 1985; Schabes and Joshi, 1988; van Noord, 1994; Nederhof, 1998), we do not attain any practical LTAG parser that runs efficiently with large-scale hand-crafted grammars such as the XTAG English grammar (XTAG Research Group, 2001). Yoshinaga et al. (Yoshinaga et al., 2003) demonstrated that a drastic speed-up of LTAG parsing can be achieved when a LTAG grammar is compiled into a HPSG (Yoshinaga and Miyao, 2002) and a CFG filtering technique for HPSG-Style grammar (Kiefer and Krieger, 2000; Torisawa et al., 2000) is applied to the obtained HPSG. In experiments with the XTAG English grammar, they fou"
W04-3323,H86-1020,0,\N,Missing
W04-3323,C98-2151,0,\N,Missing
W05-1304,W03-1018,1,\N,Missing
W05-1304,J96-1002,0,\N,Missing
W05-1304,nenadic-etal-2002-automatic,1,\N,Missing
W05-1510,C00-1007,0,0.486249,"niques exported from parsing to generation worked well while the effects were slightly different in detail. The Nitrogen system (Langkilde and Knight, 1998; Langkilde, 2000) maps semantic relations to a packed forest containing all realizations and selects the best one with a bigram model. Our method extends their approach in that we can utilize syntactic features in the disambiguation model in addition to the bigram. From the perspective of using a lexicalized grammar developed for parsing and importing parsing techniques, our method is similar to the following approaches. The Fergus system (Bangalore and Rambow, 2000) uses LTAG (Lexicalized Tree Adjoining Grammar (Schabes et al., 1988)) for generating a word lattice containing realizations and selects the best one using a trigram model. White and Baldridge (2003) developed a chart generator for CCG (Combinatory Categorial Grammar (Steedman, 2000)) and proposed several techniques for efficient generation such as best-first search, beam thresholding and chunking the input logical forms (White, 2004). Although some of the techniques look effective, the models to rank candidates are still limited to simple language models. Carroll et al. (1999) developed a cha"
W05-1510,2001.mtsummit-papers.68,0,0.021304,"Missing"
W05-1510,C88-2121,0,0.0611593,"re slightly different in detail. The Nitrogen system (Langkilde and Knight, 1998; Langkilde, 2000) maps semantic relations to a packed forest containing all realizations and selects the best one with a bigram model. Our method extends their approach in that we can utilize syntactic features in the disambiguation model in addition to the bigram. From the perspective of using a lexicalized grammar developed for parsing and importing parsing techniques, our method is similar to the following approaches. The Fergus system (Bangalore and Rambow, 2000) uses LTAG (Lexicalized Tree Adjoining Grammar (Schabes et al., 1988)) for generating a word lattice containing realizations and selects the best one using a trigram model. White and Baldridge (2003) developed a chart generator for CCG (Combinatory Categorial Grammar (Steedman, 2000)) and proposed several techniques for efficient generation such as best-first search, beam thresholding and chunking the input logical forms (White, 2004). Although some of the techniques look effective, the models to rank candidates are still limited to simple language models. Carroll et al. (1999) developed a chart generator using HPSG. After the generator outputs all the sentence"
W05-1510,J96-1002,0,0.0114489,"onstruct the final output, but slow down the generation because they can be combined with the rest of the input to construct grammatically correct phrases or sentences. Carroll et al. (1999) and White (2004) proposed different algorithms to address the same problem. We adopted Kay’s simple solution in the current ongoing work, but logical form chunking proposed by White is also applicable to our system. 2.3 Probabilistic models for generation with HPSG Some existing studies on probabilistic models for HPSG parsing (Malouf and van Noord, 2004; Miyao and Tsujii, 2005) adopted log-linear models (Berger et al., 1996). Since log-linear models allow us to 1 To introduce an edge with no semantic relations as mentioned in the previous section, we need to combine the edges with edges having no relations. e v er b : e  H E AD  SUBCAT   he bought the book e n o u n : y  H E AD  SUBCAT   bought the book v er b : e  N P : y  H E AD  N O N L O C |SL ASH  E q u iv alen t class y n o u n : y  H E AD SUBCAT    he bought e v er b : e   H E AD SUBCAT N P : x N P : y     v er b : e  H E AD   N P :x  SUBCAT  N O N L O C |SL ASH N P : y  the book z b u y e  x   y  past   n o"
W05-1510,W02-2030,0,0.153481,"Missing"
W05-1510,P96-1027,0,0.530597,"er of solutions including the methods of estimating loglinear models using packed forests of parse trees and pruning improbable candidates during parsing. The aim of this paper is to apply these techniques to generation. Since parsing and generation both output the best probable tree under some constraints, we expect that techniques that work effectively in parsing are also beneficial for generation. First, we enabled estimation of log-linear models with less cost by representing a set of generation trees in a packed forest. The forest representation was obtained by adopting chart generation (Kay, 1996; Car93 Proceedings of the Ninth International Workshop on Parsing Technologies (IWPT), pages 93–102, c Vancouver, October 2005. 2005 Association for Computational Linguistics roll et al., 1999) where ambiguous candidates are packed into an equivalence class and mapping a chart into a forest in the same way as parsing. Second, we reduced the search space in runtime by adopting iterative beam search (Tsuruoka and Tsujii, 2004) that efficiently pruned improbable candidates. We evaluated the generator on the Penn Treebank (Marcus et al., 1993), which is highly reliable corpus consisting of real-w"
W05-1510,P98-1116,0,0.1183,"e compared the performance of several disambiguation models following an existing study (Velldal and Oepen, 2005) and examined how the performance changed according to the size of training data, the feature set, and the beam width. Comparing the latter half of the experimental results with those on parsing (Miyao and Tsujii, 2005), we investigated similarities and differences between probabilistic models for parsing and generation. The results indicated that the techniques exported from parsing to generation worked well while the effects were slightly different in detail. The Nitrogen system (Langkilde and Knight, 1998; Langkilde, 2000) maps semantic relations to a packed forest containing all realizations and selects the best one with a bigram model. Our method extends their approach in that we can utilize syntactic features in the disambiguation model in addition to the bigram. From the perspective of using a lexicalized grammar developed for parsing and importing parsing techniques, our method is similar to the following approaches. The Fergus system (Bangalore and Rambow, 2000) uses LTAG (Lexicalized Tree Adjoining Grammar (Schabes et al., 1988)) for generating a word lattice containing realizations and"
W05-1510,W02-2103,0,0.842009,"Missing"
W05-1510,A00-2023,0,0.881498,"f several disambiguation models following an existing study (Velldal and Oepen, 2005) and examined how the performance changed according to the size of training data, the feature set, and the beam width. Comparing the latter half of the experimental results with those on parsing (Miyao and Tsujii, 2005), we investigated similarities and differences between probabilistic models for parsing and generation. The results indicated that the techniques exported from parsing to generation worked well while the effects were slightly different in detail. The Nitrogen system (Langkilde and Knight, 1998; Langkilde, 2000) maps semantic relations to a packed forest containing all realizations and selects the best one with a bigram model. Our method extends their approach in that we can utilize syntactic features in the disambiguation model in addition to the bigram. From the perspective of using a lexicalized grammar developed for parsing and importing parsing techniques, our method is similar to the following approaches. The Fergus system (Bangalore and Rambow, 2000) uses LTAG (Lexicalized Tree Adjoining Grammar (Schabes et al., 1988)) for generating a word lattice containing realizations and selects the best"
W05-1510,J93-2004,0,0.035825,"forest representation was obtained by adopting chart generation (Kay, 1996; Car93 Proceedings of the Ninth International Workshop on Parsing Technologies (IWPT), pages 93–102, c Vancouver, October 2005. 2005 Association for Computational Linguistics roll et al., 1999) where ambiguous candidates are packed into an equivalence class and mapping a chart into a forest in the same way as parsing. Second, we reduced the search space in runtime by adopting iterative beam search (Tsuruoka and Tsujii, 2004) that efficiently pruned improbable candidates. We evaluated the generator on the Penn Treebank (Marcus et al., 1993), which is highly reliable corpus consisting of real-world texts. Through a series of experiments, we compared the performance of several disambiguation models following an existing study (Velldal and Oepen, 2005) and examined how the performance changed according to the size of training data, the feature set, and the beam width. Comparing the latter half of the experimental results with those on parsing (Miyao and Tsujii, 2005), we investigated similarities and differences between probabilistic models for parsing and generation. The results indicated that the techniques exported from parsing"
W05-1510,P05-1011,1,0.548976,"h is more fluent and easier to understand than others. In principle, we need to enumerate all alternative realizations in order to estimate a log-linear model for generation. It therefore requires high computational cost to estimate a probabilistic model for a wide-coverage grammar because there are considerable ambiguities and the alternative realizations are hard to enumerate explicitly. Moreover, even after the model has been estimated, to explore all possible candidates in runtime is also expensive. The same problems also arise with HPSG parsing, and recent studies (Tsuruoka et al., 2004; Miyao and Tsujii, 2005; Ninomiya et al., 2005) proposed a number of solutions including the methods of estimating loglinear models using packed forests of parse trees and pruning improbable candidates during parsing. The aim of this paper is to apply these techniques to generation. Since parsing and generation both output the best probable tree under some constraints, we expect that techniques that work effectively in parsing are also beneficial for generation. First, we enabled estimation of log-linear models with less cost by representing a set of generation trees in a packed forest. The forest representation was"
W05-1510,2005.mtsummit-papers.15,0,0.240419,"ssociation for Computational Linguistics roll et al., 1999) where ambiguous candidates are packed into an equivalence class and mapping a chart into a forest in the same way as parsing. Second, we reduced the search space in runtime by adopting iterative beam search (Tsuruoka and Tsujii, 2004) that efficiently pruned improbable candidates. We evaluated the generator on the Penn Treebank (Marcus et al., 1993), which is highly reliable corpus consisting of real-world texts. Through a series of experiments, we compared the performance of several disambiguation models following an existing study (Velldal and Oepen, 2005) and examined how the performance changed according to the size of training data, the feature set, and the beam width. Comparing the latter half of the experimental results with those on parsing (Miyao and Tsujii, 2005), we investigated similarities and differences between probabilistic models for parsing and generation. The results indicated that the techniques exported from parsing to generation worked well while the effects were slightly different in detail. The Nitrogen system (Langkilde and Knight, 1998; Langkilde, 2000) maps semantic relations to a packed forest containing all realizatio"
W05-1510,W03-2316,0,0.124088,"a packed forest containing all realizations and selects the best one with a bigram model. Our method extends their approach in that we can utilize syntactic features in the disambiguation model in addition to the bigram. From the perspective of using a lexicalized grammar developed for parsing and importing parsing techniques, our method is similar to the following approaches. The Fergus system (Bangalore and Rambow, 2000) uses LTAG (Lexicalized Tree Adjoining Grammar (Schabes et al., 1988)) for generating a word lattice containing realizations and selects the best one using a trigram model. White and Baldridge (2003) developed a chart generator for CCG (Combinatory Categorial Grammar (Steedman, 2000)) and proposed several techniques for efficient generation such as best-first search, beam thresholding and chunking the input logical forms (White, 2004). Although some of the techniques look effective, the models to rank candidates are still limited to simple language models. Carroll et al. (1999) developed a chart generator using HPSG. After the generator outputs all the sentences the grammar allows, the ranking mod94  R EL  INDEX   A R G 1  A R G 2 T ENS E bu y  e  x   y  p a s t  t h e"
W05-1510,P02-1040,0,\N,Missing
W05-1510,C98-1112,0,\N,Missing
W05-1510,W05-1511,1,\N,Missing
W05-1511,E03-1052,0,0.140946,"-tokyo.ac.jp Yoshimasa Tsuruoka CREST, JST and Department of Computer Science The University of Tokyo tsuruoka@is.s.u-tokyo.ac.jp Yusuke Miyao Department of Computer Science The University of Tokyo yusuke@is.s.u-tokyo.ac.jp Jun’ichi Tsujii Department of Computer Science The University of Tokyo and School of Informatics University of Manchester and CREST, JST tsujii@is.s.u-tokyo.ac.jp Abstract Next, we applied parsing techniques developed for deep parsing, including quick check (Malouf et al., 2000), large constituent inhibition (Kaplan et al., 2004) and hybrid parsing with a CFG chunk parser (Daum et al., 2003; Frank et al., 2003; Frank, 2004). The experiments showed how each technique contributes to the final output of parsing in terms of precision, recall, and speed for the Penn treebank. We investigated the performance efficacy of beam search parsing and deep parsing techniques in probabilistic HPSG parsing using the Penn treebank. We first tested the beam thresholding and iterative parsing developed for PCFG parsing with an HPSG. Next, we tested three techniques originally developed for deep parsing: quick check, large constituent inhibition, and hybrid parsing with a CFG chunk parser. The cont"
W05-1511,P03-1014,0,0.137203,"asa Tsuruoka CREST, JST and Department of Computer Science The University of Tokyo tsuruoka@is.s.u-tokyo.ac.jp Yusuke Miyao Department of Computer Science The University of Tokyo yusuke@is.s.u-tokyo.ac.jp Jun’ichi Tsujii Department of Computer Science The University of Tokyo and School of Informatics University of Manchester and CREST, JST tsujii@is.s.u-tokyo.ac.jp Abstract Next, we applied parsing techniques developed for deep parsing, including quick check (Malouf et al., 2000), large constituent inhibition (Kaplan et al., 2004) and hybrid parsing with a CFG chunk parser (Daum et al., 2003; Frank et al., 2003; Frank, 2004). The experiments showed how each technique contributes to the final output of parsing in terms of precision, recall, and speed for the Penn treebank. We investigated the performance efficacy of beam search parsing and deep parsing techniques in probabilistic HPSG parsing using the Penn treebank. We first tested the beam thresholding and iterative parsing developed for PCFG parsing with an HPSG. Next, we tested three techniques originally developed for deep parsing: quick check, large constituent inhibition, and hybrid parsing with a CFG chunk parser. The contributions of the lar"
W05-1511,C04-1185,0,0.103326,"JST and Department of Computer Science The University of Tokyo tsuruoka@is.s.u-tokyo.ac.jp Yusuke Miyao Department of Computer Science The University of Tokyo yusuke@is.s.u-tokyo.ac.jp Jun’ichi Tsujii Department of Computer Science The University of Tokyo and School of Informatics University of Manchester and CREST, JST tsujii@is.s.u-tokyo.ac.jp Abstract Next, we applied parsing techniques developed for deep parsing, including quick check (Malouf et al., 2000), large constituent inhibition (Kaplan et al., 2004) and hybrid parsing with a CFG chunk parser (Daum et al., 2003; Frank et al., 2003; Frank, 2004). The experiments showed how each technique contributes to the final output of parsing in terms of precision, recall, and speed for the Penn treebank. We investigated the performance efficacy of beam search parsing and deep parsing techniques in probabilistic HPSG parsing using the Penn treebank. We first tested the beam thresholding and iterative parsing developed for PCFG parsing with an HPSG. Next, we tested three techniques originally developed for deep parsing: quick check, large constituent inhibition, and hybrid parsing with a CFG chunk parser. The contributions of the large constituent"
W05-1511,P02-1036,0,0.582034,"ity is guaranteed. The interesting point of this approach is that, once the exhaustive parsing is completed, the probabilities of non-local dependencies, which cannot be computed during parsing, are computed after making a packed parse forest. Probabilistic models where probabilities are assigned to the CFG backbone of the unification-based grammar have been developed (Kasper et al., 1996; Briscoe and Carroll, 1993; Kiefer et al., 2002), and the most probable parse is found by PCFG parsing. This model is based on PCFG and not probabilistic unification-based grammar parsing. Geman and Johnson (Geman and Johnson, 2002) proposed a dynamic programming algorithm for finding the most probable parse in a packed parse forest generated by unification-based grammars without expanding the forest. However, the efficiency of this algorithm is inherently limited by the inefficiency of exhaustive parsing. In this paper we describe the performance of beam thresholding, including iterative parsing, in probabilistic HPSG parsing for a large-scale corpora, the Penn treebank. We show how techniques developed for efficient deep parsing can improve the efficiency of probabilistic parsing. These techniques were evaluated in exp"
W05-1511,W97-0302,0,0.13169,"plications, a number of studies have focused on improving the parsing efficiency of unificationbased grammars (Oepen et al., 2002). Although significant improvements in efficiency have been made, parsing speed is still not high enough for practical applications. Introduction We investigated the performance efficacy of beam search parsing and deep parsing techniques in probabilistic head-driven phrase structure grammar (HPSG) parsing for the Penn treebank. We first applied beam thresholding techniques developed for CFG parsing to HPSG parsing, including local thresholding, global thresholding (Goodman, 1997), and iterative parsing (Tsuruoka and Tsujii, 2005b). The recent introduction of probabilistic models of wide-coverage unification-based grammars (Malouf and van Noord, 2004; Kaplan et al., 2004; Miyao and Tsujii, 2005) has opened up the novel possibility of increasing parsing speed by guiding the search path using probabilities. That is, since we often require only the most probable parse result, we can compute partial parse results that are likely to contribute to the final parse result. This approach has been extensively studied in the field of probabilistic 103 Proceedings of the Ninth Int"
W05-1511,J97-4005,0,0.0385754,"ion provides the phrasal sign of the mother. The sign of the larger constituent is obtained by repeatedly applying schemata to lexical/phrasal signs. Finally, the parse result is output as a phrasal sign that dominates the sentence. Given set W of words and set F of feature structures, an HPSG is formulated as a tuple, G = hL, Ri, where L = {l = hw, F i|w ∈ W, F ∈ F} is a set of lexical entries, and R is a set of schemata, i.e., r ∈ R is a partial function: F × F → F. Given a sentence, an HPSG computes a set of phrasal signs, i.e., feature structures, as a result of parsing. Previous studies (Abney, 1997; Johnson et al., 1999; Riezler et al., 2000; Miyao et al., 2003; Malouf and van Noord, 2004; Kaplan et al., 2004; Miyao and Tsujii, 2005) defined a probabilistic model of unification-based grammars as a log-linear model or maximum entropy model (Berger et al., 1996). The probability of parse result T assigned to given sentence w = hw1 , . . . , wn i is ! X 1 exp λi fi (T ) p(T |w) = Zw i Zw = X T0 exp X i ! λi fi (T 0 ) , where λi is a model parameter, and fi is a feature function that represents a characteristic of parse tree T . Intuitively, the probability is defined as the normalized prod"
W05-1511,P03-1046,0,0.0915903,"Tsujii (2005a). Table 1 shows the abbreviations used in presenting the results. We measured the accuracy of the predicateargument relations output by the parser. A predicate-argument relation is defined as a tuple hσ, wh , a, wa i, where σ is the predicate type (e.g., adjective, intransitive verb), wh is the head word of the predicate, a is the argument label (MODARG, ARG1, ..., ARG4), and wa is the head word of the argument. Precision/recall is the ratio of tuples correctly identified by the parser. This evaluation scheme was the same as used in previous evaluations of lexicalized grammars (Hockenmaier, 2003; Clark and Curran, 2004; Miyao and Tsujii, 2005). The experiments were conducted on an AMD Opteron server with a 2.4-GHz CPU. Section 22 of the Treebank was used as the development set, and performance was evaluated using sentences of less than 40 words in Section 23 (2,164 sentences, 20.3 words/sentence). The performance of each parsing technique was analyzed using the sentences in Section 24 of less than 15 words (305 sentences) and less than 40 words (1145 sentences). Table 2 shows the parsing performance using all"
W05-1511,J96-1002,0,0.0108747,"set F of feature structures, an HPSG is formulated as a tuple, G = hL, Ri, where L = {l = hw, F i|w ∈ W, F ∈ F} is a set of lexical entries, and R is a set of schemata, i.e., r ∈ R is a partial function: F × F → F. Given a sentence, an HPSG computes a set of phrasal signs, i.e., feature structures, as a result of parsing. Previous studies (Abney, 1997; Johnson et al., 1999; Riezler et al., 2000; Miyao et al., 2003; Malouf and van Noord, 2004; Kaplan et al., 2004; Miyao and Tsujii, 2005) defined a probabilistic model of unification-based grammars as a log-linear model or maximum entropy model (Berger et al., 1996). The probability of parse result T assigned to given sentence w = hw1 , . . . , wn i is ! X 1 exp λi fi (T ) p(T |w) = Zw i Zw = X T0 exp X i ! λi fi (T 0 ) , where λi is a model parameter, and fi is a feature function that represents a characteristic of parse tree T . Intuitively, the probability is defined as the normalized product of the weights exp(λi ) when a characteristic corresponding to fi appears in parse result T . Model parameters λi are estimated using numerical optimization methods (Malouf, 2002) so as to maximize the log-likelihood of the training data. However, the above model"
W05-1511,P99-1069,0,0.0875204,"the phrasal sign of the mother. The sign of the larger constituent is obtained by repeatedly applying schemata to lexical/phrasal signs. Finally, the parse result is output as a phrasal sign that dominates the sentence. Given set W of words and set F of feature structures, an HPSG is formulated as a tuple, G = hL, Ri, where L = {l = hw, F i|w ∈ W, F ∈ F} is a set of lexical entries, and R is a set of schemata, i.e., r ∈ R is a partial function: F × F → F. Given a sentence, an HPSG computes a set of phrasal signs, i.e., feature structures, as a result of parsing. Previous studies (Abney, 1997; Johnson et al., 1999; Riezler et al., 2000; Miyao et al., 2003; Malouf and van Noord, 2004; Kaplan et al., 2004; Miyao and Tsujii, 2005) defined a probabilistic model of unification-based grammars as a log-linear model or maximum entropy model (Berger et al., 1996). The probability of parse result T assigned to given sentence w = hw1 , . . . , wn i is ! X 1 exp λi fi (T ) p(T |w) = Zw i Zw = X T0 exp X i ! λi fi (T 0 ) , where λi is a model parameter, and fi is a feature function that represents a characteristic of parse tree T . Intuitively, the probability is defined as the normalized product of the weights exp"
W05-1511,J93-1002,0,0.0695129,"hout using probabilities and then select the highest probability parse. The behavior of their algorithms is like that of the Viterbi algorithm for PCFG parsing, so the correct parse with the highest probability is guaranteed. The interesting point of this approach is that, once the exhaustive parsing is completed, the probabilities of non-local dependencies, which cannot be computed during parsing, are computed after making a packed parse forest. Probabilistic models where probabilities are assigned to the CFG backbone of the unification-based grammar have been developed (Kasper et al., 1996; Briscoe and Carroll, 1993; Kiefer et al., 2002), and the most probable parse is found by PCFG parsing. This model is based on PCFG and not probabilistic unification-based grammar parsing. Geman and Johnson (Geman and Johnson, 2002) proposed a dynamic programming algorithm for finding the most probable parse in a packed parse forest generated by unification-based grammars without expanding the forest. However, the efficiency of this algorithm is inherently limited by the inefficiency of exhaustive parsing. In this paper we describe the performance of beam thresholding, including iterative parsing, in probabilistic HPSG"
W05-1511,J98-2004,0,0.0287342,"on-terminal symbols, and prevent the HPSG parser from generating edges that cross brackets. 4 4.1 Beam thresholding for HPSG parsing Simple beam thresholding Many algorithms for improving the efficiency of PCFG parsing have been extensively investigated. They include grammar compilation (Tomita, 1986; Nederhof, 2000), the Viterbi algorithm, controlling search strategies without FOM such as left-corner parsing (Rosenkrantz and Lewis II, 1970) or headcorner parsing (Kay, 1989; van Noord, 1997), and with FOM such as the beam search, the best-first search or A* search (Chitrao and Grishman, 1990; Caraballo and Charniak, 1998; Collins, 1999; Ratnaparkhi, 1999; Charniak, 2000; Roark, 2001; Klein 106 and Manning, 2003). The beam search and bestfirst search algorithms significantly reduce the time required for finding the best parse at the cost of losing the guarantee of finding the correct parse. The CYK algorithm, which is essentially a bottomup parser, is a natural choice for non-probabilistic HPSG parsers. Many of the constraints are expressed as lexical entries in HPSG, and bottom-up parsers can use those constraints to reduce the search space in the early stages of parsing. For PCFG, extending the CYK algorithm"
W05-1511,A00-2018,0,0.0825924,"edges that cross brackets. 4 4.1 Beam thresholding for HPSG parsing Simple beam thresholding Many algorithms for improving the efficiency of PCFG parsing have been extensively investigated. They include grammar compilation (Tomita, 1986; Nederhof, 2000), the Viterbi algorithm, controlling search strategies without FOM such as left-corner parsing (Rosenkrantz and Lewis II, 1970) or headcorner parsing (Kay, 1989; van Noord, 1997), and with FOM such as the beam search, the best-first search or A* search (Chitrao and Grishman, 1990; Caraballo and Charniak, 1998; Collins, 1999; Ratnaparkhi, 1999; Charniak, 2000; Roark, 2001; Klein 106 and Manning, 2003). The beam search and bestfirst search algorithms significantly reduce the time required for finding the best parse at the cost of losing the guarantee of finding the correct parse. The CYK algorithm, which is essentially a bottomup parser, is a natural choice for non-probabilistic HPSG parsers. Many of the constraints are expressed as lexical entries in HPSG, and bottom-up parsers can use those constraints to reduce the search space in the early stages of parsing. For PCFG, extending the CYK algorithm to output the Viterbi parse is straightforward (N"
W05-1511,N04-1013,0,0.39086,"artment of Computer Science The University of Tokyo ninomi@is.s.u-tokyo.ac.jp Yoshimasa Tsuruoka CREST, JST and Department of Computer Science The University of Tokyo tsuruoka@is.s.u-tokyo.ac.jp Yusuke Miyao Department of Computer Science The University of Tokyo yusuke@is.s.u-tokyo.ac.jp Jun’ichi Tsujii Department of Computer Science The University of Tokyo and School of Informatics University of Manchester and CREST, JST tsujii@is.s.u-tokyo.ac.jp Abstract Next, we applied parsing techniques developed for deep parsing, including quick check (Malouf et al., 2000), large constituent inhibition (Kaplan et al., 2004) and hybrid parsing with a CFG chunk parser (Daum et al., 2003; Frank et al., 2003; Frank, 2004). The experiments showed how each technique contributes to the final output of parsing in terms of precision, recall, and speed for the Penn treebank. We investigated the performance efficacy of beam search parsing and deep parsing techniques in probabilistic HPSG parsing using the Penn treebank. We first tested the beam thresholding and iterative parsing developed for PCFG parsing with an HPSG. Next, we tested three techniques originally developed for deep parsing: quick check, large constituent in"
W05-1511,W89-0206,0,0.237937,"from the Penn treebank. The principal idea of using the chunk parser is to use the bracket information, i.e., parse trees without non-terminal symbols, and prevent the HPSG parser from generating edges that cross brackets. 4 4.1 Beam thresholding for HPSG parsing Simple beam thresholding Many algorithms for improving the efficiency of PCFG parsing have been extensively investigated. They include grammar compilation (Tomita, 1986; Nederhof, 2000), the Viterbi algorithm, controlling search strategies without FOM such as left-corner parsing (Rosenkrantz and Lewis II, 1970) or headcorner parsing (Kay, 1989; van Noord, 1997), and with FOM such as the beam search, the best-first search or A* search (Chitrao and Grishman, 1990; Caraballo and Charniak, 1998; Collins, 1999; Ratnaparkhi, 1999; Charniak, 2000; Roark, 2001; Klein 106 and Manning, 2003). The beam search and bestfirst search algorithms significantly reduce the time required for finding the best parse at the cost of losing the guarantee of finding the correct parse. The CYK algorithm, which is essentially a bottomup parser, is a natural choice for non-probabilistic HPSG parsers. Many of the constraints are expressed as lexical entries in"
W05-1511,P99-1061,0,0.0644723,"rse results. 3 Techniques for efficient deep parsing Many of the techniques for improving the parsing efficiency of deep linguistic analysis have been developed in the framework of lexicalized grammars such as lexical functional grammar (LFG) (Bresnan, 105 1982), lexicalized tree adjoining grammar (LTAG) (Shabes et al., 1988), HPSG (Pollard and Sag, 1994) or combinatory categorial grammar (CCG) (Steedman, 2000). Most of them were developed for exhaustive parsing, i.e., producing all parse results that are given by the grammar (Matsumoto et al., 1983; Maxwell and Kaplan, 1993; van Noord, 1997; Kiefer et al., 1999; Malouf et al., 2000; Torisawa et al., 2000; Oepen et al., 2002; Penn and Munteanu, 2003). The strategy of exhaustive parsing has been widely used in grammar development and in parameter training for probabilistic models. We tested three of these techniques. Quick check Quick check filters out non-unifiable feature structures (Malouf et al., 2000). Suppose we have two non-unifiable feature structures. They are destructively unified by traversing and modifying them, and then finally they are found to be not unifiable in the middle of the unification process. Quick check quickly judges their un"
W05-1511,P04-1014,0,0.182952,"hat is, only a nonterminal symbol of a mother is considered in further processing by ignoring the structure of its daughters. With this assumption, we can compute the figures of merit (FOMs) of partial parse results. This assumption restricts the possibility of feature functions that represent non-local dependencies expressed in a parse result. Since unification-based grammars can express semantic relations, such as predicate-argument relations, in their structure, the assumption unjustifiably restricts the flexibility of probabilistic modeling. However, previous research (Miyao et al., 2003; Clark and Curran, 2004; Kaplan et al., 2004) showed that predicate-argument relations can be represented under the assumption of feature locality. We thus assumed the locality of feature functions and exploited it for the efficient search of probable parse results. 3 Techniques for efficient deep parsing Many of the techniques for improving the parsing efficiency of deep linguistic analysis have been developed in the framework of lexicalized grammars such as lexical functional grammar (LFG) (Bresnan, 105 1982), lexicalized tree adjoining grammar (LTAG) (Shabes et al., 1988), HPSG (Pollard and Sag, 1994) or combinat"
W05-1511,C02-1075,0,0.0132895,"d then select the highest probability parse. The behavior of their algorithms is like that of the Viterbi algorithm for PCFG parsing, so the correct parse with the highest probability is guaranteed. The interesting point of this approach is that, once the exhaustive parsing is completed, the probabilities of non-local dependencies, which cannot be computed during parsing, are computed after making a packed parse forest. Probabilistic models where probabilities are assigned to the CFG backbone of the unification-based grammar have been developed (Kasper et al., 1996; Briscoe and Carroll, 1993; Kiefer et al., 2002), and the most probable parse is found by PCFG parsing. This model is based on PCFG and not probabilistic unification-based grammar parsing. Geman and Johnson (Geman and Johnson, 2002) proposed a dynamic programming algorithm for finding the most probable parse in a packed parse forest generated by unification-based grammars without expanding the forest. However, the efficiency of this algorithm is inherently limited by the inefficiency of exhaustive parsing. In this paper we describe the performance of beam thresholding, including iterative parsing, in probabilistic HPSG parsing for a large-s"
W05-1511,N03-1016,0,0.0639364,"Missing"
W05-1511,W02-2018,0,0.0412473,"f unification-based grammars as a log-linear model or maximum entropy model (Berger et al., 1996). The probability of parse result T assigned to given sentence w = hw1 , . . . , wn i is ! X 1 exp λi fi (T ) p(T |w) = Zw i Zw = X T0 exp X i ! λi fi (T 0 ) , where λi is a model parameter, and fi is a feature function that represents a characteristic of parse tree T . Intuitively, the probability is defined as the normalized product of the weights exp(λi ) when a characteristic corresponding to fi appears in parse result T . Model parameters λi are estimated using numerical optimization methods (Malouf, 2002) so as to maximize the log-likelihood of the training data. However, the above model cannot be easily estimated because the estimation requires the computation of p(T |w) for all parse candidates assigned to sentence w. Because the number of parse candidates is exponentially related to the length of the sentence, the estimation is intractable for long sentences. To make the model estimation tractable, Geman and Johnson (Geman and Johnson, 2002) and Miyao and Tsujii (Miyao and Tsujii, 2002) proposed a dynamic programming algorithm for estimating p(T |w). They assumed that features are functions"
W05-1511,J93-4001,0,0.0395362,"it for the efficient search of probable parse results. 3 Techniques for efficient deep parsing Many of the techniques for improving the parsing efficiency of deep linguistic analysis have been developed in the framework of lexicalized grammars such as lexical functional grammar (LFG) (Bresnan, 105 1982), lexicalized tree adjoining grammar (LTAG) (Shabes et al., 1988), HPSG (Pollard and Sag, 1994) or combinatory categorial grammar (CCG) (Steedman, 2000). Most of them were developed for exhaustive parsing, i.e., producing all parse results that are given by the grammar (Matsumoto et al., 1983; Maxwell and Kaplan, 1993; van Noord, 1997; Kiefer et al., 1999; Malouf et al., 2000; Torisawa et al., 2000; Oepen et al., 2002; Penn and Munteanu, 2003). The strategy of exhaustive parsing has been widely used in grammar development and in parameter training for probabilistic models. We tested three of these techniques. Quick check Quick check filters out non-unifiable feature structures (Malouf et al., 2000). Suppose we have two non-unifiable feature structures. They are destructively unified by traversing and modifying them, and then finally they are found to be not unifiable in the middle of the unification proces"
W05-1511,P03-1026,0,0.0211687,"ving the parsing efficiency of deep linguistic analysis have been developed in the framework of lexicalized grammars such as lexical functional grammar (LFG) (Bresnan, 105 1982), lexicalized tree adjoining grammar (LTAG) (Shabes et al., 1988), HPSG (Pollard and Sag, 1994) or combinatory categorial grammar (CCG) (Steedman, 2000). Most of them were developed for exhaustive parsing, i.e., producing all parse results that are given by the grammar (Matsumoto et al., 1983; Maxwell and Kaplan, 1993; van Noord, 1997; Kiefer et al., 1999; Malouf et al., 2000; Torisawa et al., 2000; Oepen et al., 2002; Penn and Munteanu, 2003). The strategy of exhaustive parsing has been widely used in grammar development and in parameter training for probabilistic models. We tested three of these techniques. Quick check Quick check filters out non-unifiable feature structures (Malouf et al., 2000). Suppose we have two non-unifiable feature structures. They are destructively unified by traversing and modifying them, and then finally they are found to be not unifiable in the middle of the unification process. Quick check quickly judges their unifiability by peeping the values of the given paths. If one of the path values is not unif"
W05-1511,P00-1061,0,0.132155,"e mother. The sign of the larger constituent is obtained by repeatedly applying schemata to lexical/phrasal signs. Finally, the parse result is output as a phrasal sign that dominates the sentence. Given set W of words and set F of feature structures, an HPSG is formulated as a tuple, G = hL, Ri, where L = {l = hw, F i|w ∈ W, F ∈ F} is a set of lexical entries, and R is a set of schemata, i.e., r ∈ R is a partial function: F × F → F. Given a sentence, an HPSG computes a set of phrasal signs, i.e., feature structures, as a result of parsing. Previous studies (Abney, 1997; Johnson et al., 1999; Riezler et al., 2000; Miyao et al., 2003; Malouf and van Noord, 2004; Kaplan et al., 2004; Miyao and Tsujii, 2005) defined a probabilistic model of unification-based grammars as a log-linear model or maximum entropy model (Berger et al., 1996). The probability of parse result T assigned to given sentence w = hw1 , . . . , wn i is ! X 1 exp λi fi (T ) p(T |w) = Zw i Zw = X T0 exp X i ! λi fi (T 0 ) , where λi is a model parameter, and fi is a feature function that represents a characteristic of parse tree T . Intuitively, the probability is defined as the normalized product of the weights exp(λi ) when a character"
W05-1511,J01-2004,0,0.0203056,"s brackets. 4 4.1 Beam thresholding for HPSG parsing Simple beam thresholding Many algorithms for improving the efficiency of PCFG parsing have been extensively investigated. They include grammar compilation (Tomita, 1986; Nederhof, 2000), the Viterbi algorithm, controlling search strategies without FOM such as left-corner parsing (Rosenkrantz and Lewis II, 1970) or headcorner parsing (Kay, 1989; van Noord, 1997), and with FOM such as the beam search, the best-first search or A* search (Chitrao and Grishman, 1990; Caraballo and Charniak, 1998; Collins, 1999; Ratnaparkhi, 1999; Charniak, 2000; Roark, 2001; Klein 106 and Manning, 2003). The beam search and bestfirst search algorithms significantly reduce the time required for finding the best parse at the cost of losing the guarantee of finding the correct parse. The CYK algorithm, which is essentially a bottomup parser, is a natural choice for non-probabilistic HPSG parsers. Many of the constraints are expressed as lexical entries in HPSG, and bottom-up parsers can use those constraints to reduce the search space in the early stages of parsing. For PCFG, extending the CYK algorithm to output the Viterbi parse is straightforward (Ney, 1991; Jur"
W05-1511,P80-1024,0,0.762838,"Missing"
W05-1511,C88-2121,0,0.072195,"Missing"
W05-1511,P05-1011,1,0.829882,"is still not high enough for practical applications. Introduction We investigated the performance efficacy of beam search parsing and deep parsing techniques in probabilistic head-driven phrase structure grammar (HPSG) parsing for the Penn treebank. We first applied beam thresholding techniques developed for CFG parsing to HPSG parsing, including local thresholding, global thresholding (Goodman, 1997), and iterative parsing (Tsuruoka and Tsujii, 2005b). The recent introduction of probabilistic models of wide-coverage unification-based grammars (Malouf and van Noord, 2004; Kaplan et al., 2004; Miyao and Tsujii, 2005) has opened up the novel possibility of increasing parsing speed by guiding the search path using probabilities. That is, since we often require only the most probable parse result, we can compute partial parse results that are likely to contribute to the final parse result. This approach has been extensively studied in the field of probabilistic 103 Proceedings of the Ninth International Workshop on Parsing Technologies (IWPT), pages 103–114, c Vancouver, October 2005. 2005 Association for Computational Linguistics CFG (PCFG) parsing, such as Viterbi parsing and beam thresholding. While many"
W05-1511,J00-1003,0,0.0194491,"on. The grammar for the chunk parser is automatically extracted from the CFG treebank translated from the HPSG treebank, which is generated during grammar extraction from the Penn treebank. The principal idea of using the chunk parser is to use the bracket information, i.e., parse trees without non-terminal symbols, and prevent the HPSG parser from generating edges that cross brackets. 4 4.1 Beam thresholding for HPSG parsing Simple beam thresholding Many algorithms for improving the efficiency of PCFG parsing have been extensively investigated. They include grammar compilation (Tomita, 1986; Nederhof, 2000), the Viterbi algorithm, controlling search strategies without FOM such as left-corner parsing (Rosenkrantz and Lewis II, 1970) or headcorner parsing (Kay, 1989; van Noord, 1997), and with FOM such as the beam search, the best-first search or A* search (Chitrao and Grishman, 1990; Caraballo and Charniak, 1998; Collins, 1999; Ratnaparkhi, 1999; Charniak, 2000; Roark, 2001; Klein 106 and Manning, 2003). The beam search and bestfirst search algorithms significantly reduce the time required for finding the best parse at the cost of losing the guarantee of finding the correct parse. The CYK algorit"
W05-1511,W05-1514,1,0.885649,"Missing"
W05-1511,J97-3004,0,0.449191,"Missing"
W05-1511,J93-2004,0,\N,Missing
W05-1511,J03-4003,0,\N,Missing
W05-1514,N01-1025,0,0.0416873,"overed from the chunking history. This parsing strategy converts the problem of full parsing into smaller and simpler problems, namely, chunking, where we only need to recognize flat structures (base phrases). Sang used the IOB tagging method proposed by Ramshow(Ramshaw and Marcus, 1995) and memory-based learning for each level of chunking and achieved an f-score of 80.49 on the Penn Treebank corpus. 3 Chunking with a sliding-window approach of machine learning techniques that have been developed for sequence labeling problems such as Hidden Markov Models, sequential classification with SVMs (Kudo and Matsumoto, 2001), and Conditional Random Fields (Sha and Pereira, 2003). One of our claims in this paper is that we should not convert the chunking problem into a tagging task. Instead, we use a classical sliding-window method for chunking, where we consider all subsequences as phrase candidates and classify them with a machine learning algorithm. Suppose, for example, we are about to perform chunking on the sequence in Figure 4. NP-volume VBD-was .-. We consider the following sub sequences as the phrase candidates in this level of chunking. 1. (NP-volume) VBD-was .-. 2. NP-volume (VBD-was) .-. 3. NP-volume V"
W05-1514,P05-1024,0,0.0119296,"rules even when we have constructed the rule dictionary using the whole training data (note that the dotted line does not saturate). Additional feature sets for the maximum entropy classifiers could improve the performance. The bottom-up parsing strategy allows us to use information about sub-trees that have already been constructed. We thus do not need to restrict ourselves to use only head-information of the partial parses. Since many researchers have reported that information on partial parse trees plays an important role for achieving high performance (Bod, 1992; Collins and Duffy, 2002; Kudo et al., 2005), we expect that additional features will improve the performance of chunk parsing. Also, the methods for searching the best parse presented in sections 7.2 and 7.3 have much room for improvement. the search method does not have the device to avoid repetitive computations on the same nonterminal sequence in parsing. A chart-like structure which effectively stores the partial parse results could enable the parser to explore a broader search space and produce better parses. Our chunk parser exhibited a considerable improvement in parsing accuracy over the previous study on chunk parsing. However"
W05-1514,P95-1037,0,0.0610444,", the 3rd iteration. NP S NP VBD DT JJ volume was a light QP million NNS . ounces . Figure 2: Chunk parsing, the 2nd iteration. NP VP . volume was . Figure 4: Chunk parsing, the 4th iteration. 2 Chunk Parsing For the overall strategy of chunk parsing, we follow the method proposed by Sang (Tjong Kim Sang, 2001). Figures 1 to 4 show an example of chunk parsing. In the first iteration, the chunker identifies two base phrases, (NP Estimated volume) and (QP 2.4 million), and replaces each phrase with its nonterminal symbol and head. The head word is identified by using the head-percolation table (Magerman, 1995). In the second iteration, the chunker identifies (NP a light million ounces) and converts this phrase into NP. This chunking procedure is repeated until the whole sentence is chunked at the fourth iteration, and the full parse tree is easily recovered from the chunking history. This parsing strategy converts the problem of full parsing into smaller and simpler problems, namely, chunking, where we only need to recognize flat structures (base phrases). Sang used the IOB tagging method proposed by Ramshow(Ramshaw and Marcus, 1995) and memory-based learning for each level of chunking and achieved"
W05-1514,W05-1511,1,0.828986,"ider only the chunks whose probabilities are within the predefined margin from , .. In other words, the chunks whose probabilities 435 , .0/21 are larger than are considered as assured chunks, and thus are fixed when we generate alternative hypotheses of chunking..-7The chunks 435 , 6 whose probabilities are smaller than /21 are simply ignored.      We generate alternative hypotheses in each level of chunking, and search the best parse in a depthfirst manner. 7.3 Iterative parsing We also tried an iterative parsing strategy, which was successfully used in probabilistic HPSG parsing (Ninomiya et al., 2005). The parsing strategy is simple. The parser starts with a very low margin and tries to find a successful parse. If the parser cannot find a successful parse, then it increases the margin by a certain step and tries to parse with the wider margin. Threshold 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 8 Experiments We ran parsing experiments using the Penn Treebank corpus, which is widely used for evaluating parsing algorithms. The training set consists of sections 02-21. We used section 22 as the development data, with which we tuned the feature set and parameters for parsing. The test set consists of"
W05-1514,W95-0107,0,0.0275409,"and head. The head word is identified by using the head-percolation table (Magerman, 1995). In the second iteration, the chunker identifies (NP a light million ounces) and converts this phrase into NP. This chunking procedure is repeated until the whole sentence is chunked at the fourth iteration, and the full parse tree is easily recovered from the chunking history. This parsing strategy converts the problem of full parsing into smaller and simpler problems, namely, chunking, where we only need to recognize flat structures (base phrases). Sang used the IOB tagging method proposed by Ramshow(Ramshaw and Marcus, 1995) and memory-based learning for each level of chunking and achieved an f-score of 80.49 on the Penn Treebank corpus. 3 Chunking with a sliding-window approach of machine learning techniques that have been developed for sequence labeling problems such as Hidden Markov Models, sequential classification with SVMs (Kudo and Matsumoto, 2001), and Conditional Random Fields (Sha and Pereira, 2003). One of our claims in this paper is that we should not convert the chunking problem into a tagging task. Instead, we use a classical sliding-window method for chunking, where we consider all subsequences as"
W05-1514,W96-0213,0,0.107079,") .-. 3. NP-volume VBD-was (.-.) 4. (NP-volume VBD-was) .-. 5. NP-volume (VBD-was .-.) The performance of chunk parsing heavily depends on the performance of each level of chunking. The popular approach to this shallow parsing is to convert the problem into a tagging task and use a variety 134 6. (NP-volume VBD-was .-.) The merit of taking the sliding window approach is that we can make use of a richer set of features on recognizing a phrase than in the sequential labeling    4 Filtering with the CFG Rule Dictionary We use an idea that is similar to the method proposed by Ratnaparkhi (Ratnaparkhi, 1996) for partof-speech tagging. They used a Tag Dictionary, with which the tagger considers only the tag-word pairs that appear in the training sentences as the candidate tags. A similar method can be used for reducing the number of phrase candidates. We first construct a rule dictionary consisting of all the CFG rules used in the training data. In both training and parsing, we filter out all the sub-sequences that do not match any of the entry in the dictionary. 4.1 Normalization The rules used in the training data do not cover all the rules in unseen sentences. Therefore, if we take a naive filt"
W05-1514,W97-0301,0,0.512586,"Missing"
W05-1514,J96-1002,0,0.0103728,"owing information as the features. The Right-Hand-Side (RHS) of the CFG rule The left-adjacent nonterminal symbol. The right-adjacent nonterminal symbol. By assuming the conditional independence among the features, we can compute the probability for filtering as follows:                                                        6 Phrase Recognition with a Maximum Entropy Classifier For the candidates which are not filtered out in the above two phases, we perform classification with maximum entropy classifiers (Berger et al., 1996). We construct a binary classifier for each type of phrases using the entire training set. The training samples for maximum entropy consist of the phrase candidates that have not been filtered out by the CFG rule dictionary and the naive Bayes classifier. One of the merits of using a maximum entropy classifier is that we can obtain a probability from the classifier in each decision. The probability of each decision represents how likely the candidate is a correct chunk. We accept a chunk only when the probability is larger than the predefined threshold. With this thresholding scheme, we can co"
W05-1514,N03-1028,0,0.0464251,"nverts the problem of full parsing into smaller and simpler problems, namely, chunking, where we only need to recognize flat structures (base phrases). Sang used the IOB tagging method proposed by Ramshow(Ramshaw and Marcus, 1995) and memory-based learning for each level of chunking and achieved an f-score of 80.49 on the Penn Treebank corpus. 3 Chunking with a sliding-window approach of machine learning techniques that have been developed for sequence labeling problems such as Hidden Markov Models, sequential classification with SVMs (Kudo and Matsumoto, 2001), and Conditional Random Fields (Sha and Pereira, 2003). One of our claims in this paper is that we should not convert the chunking problem into a tagging task. Instead, we use a classical sliding-window method for chunking, where we consider all subsequences as phrase candidates and classify them with a machine learning algorithm. Suppose, for example, we are about to perform chunking on the sequence in Figure 4. NP-volume VBD-was .-. We consider the following sub sequences as the phrase candidates in this level of chunking. 1. (NP-volume) VBD-was .-. 2. NP-volume (VBD-was) .-. 3. NP-volume VBD-was (.-.) 4. (NP-volume VBD-was) .-. 5. NP-volume (V"
W05-1514,E99-1016,0,0.0609981,"the probabilities output by the maximum entropy classifiers, and show that the search method can further improve the parsing accuracy. The performance of chunk parsing is heavily dependent on the performance of phrase recognition in each level of chunking. We show in this paper that the chunk parsing strategy is indeed appealing in that it can give considerably better performance than previously reported by using a different approach for phrase recognition and that it enables us to build a very fast parser that gives high-precision outputs. 1 Introduction Chunk parsing (Tjong Kim Sang, 2001; Brants, 1999) is a simple parsing strategy both in implementation and concept. The parser first performs chunking by identifying base phrases, and convert the identified phrases to non-terminal symbols. The parser again performs chunking on the updated sequence and convert the newly recognized phrases into non-terminal symbols. The parser repeats this procedure until there are no phrases to be chunked. After finishing these chunking processes, we can reconstruct the complete parse tree of the sentence from the chunking results. This advantage could open up the possibility of using full parsers for large-sc"
W05-1514,A00-2018,0,0.0519962,"Japan Science and Technology Corporation)  Department of Computer Science, University of Tokyo  School of Informatics, University of Manchester  tsuruoka,tsujii  @is.s.u-tokyo.ac.jp Abstract Although the conceptual simplicity of chunk parsing is appealing, satisfactory performance for practical use has not yet been achieved with this parsing strategy. Sang achieved an f-score of 80.49 on the Penn Treebank by using the IOB tagging method for each level of chunking (Tjong Kim Sang, 2001). However, there is a very large gap between their performance and that of widely-used practical parsers (Charniak, 2000; Collins, 1999). Chunk parsing is conceptually appealing but its performance has not been satisfactory for practical use. In this paper we show that chunk parsing can perform significantly better than previously reported by using a simple slidingwindow method and maximum entropy classifiers for phrase recognition in each level of chunking. Experimental results with the Penn Treebank corpus show that our chunk parser can give high-precision parsing outputs with very high speed (14 msec/sentence). We also present a parsing method for searching the best parse by considering the probabilities out"
W05-1514,P02-1034,0,0.0208674,"till we will face unknown rules even when we have constructed the rule dictionary using the whole training data (note that the dotted line does not saturate). Additional feature sets for the maximum entropy classifiers could improve the performance. The bottom-up parsing strategy allows us to use information about sub-trees that have already been constructed. We thus do not need to restrict ourselves to use only head-information of the partial parses. Since many researchers have reported that information on partial parse trees plays an important role for achieving high performance (Bod, 1992; Collins and Duffy, 2002; Kudo et al., 2005), we expect that additional features will improve the performance of chunk parsing. Also, the methods for searching the best parse presented in sections 7.2 and 7.3 have much room for improvement. the search method does not have the device to avoid repetitive computations on the same nonterminal sequence in parsing. A chart-like structure which effectively stores the partial parse results could enable the parser to explore a broader search space and produce better parses. Our chunk parser exhibited a considerable improvement in parsing accuracy over the previous study on ch"
W05-1514,W03-1018,1,0.829715,"he merits of using a maximum entropy classifier is that we can obtain a probability from the classifier in each decision. The probability of each decision represents how likely the candidate is a correct chunk. We accept a chunk only when the probability is larger than the predefined threshold. With this thresholding scheme, we can control the trade-off between precision and recall by changing the threshold value. Regularization is important in maximum entropy modeling to avoid overfitting to the training data. For this purpose, we use the maximum entropy modeling with inequality constraints (Kazama and Tsujii, 2003). This modeling has one parameter to tune as in Gaussian prior modeling. The parameter is called the width factor. We set this parameter to be 1.0 throughout the experiments. For numerical optimization, we used the Limited-Memory Variable-Metric (LMVM) algorithm (Benson and Mor´e, 2001).  where is a binary output indicating whether  the candidate is a phrase of the  target type or not, is the RHS of the CFG rule, is the symbol on the left, and is the symbol on the right. We used the Laplace smoothing method for computing each probability. Note that the information about the result of the ru"
W05-1514,H05-1059,1,0.812454,"changing the maximum number of nodes in the search. The uncertainty margin for chunk recognition was 0.3. Figure 6 shows that Collins parser clearly outperforms our chunk parser when the beam size is large. However, the performance significantly drops with a smaller beam size. The break-even point is at around 200 sec (83 msec/sentence). 8.2 Comparison with previous work Table 6 summarizes our parsing performance on section 23 together with the results of previous studies. In order to make the results directly comparable, we produced POS tags as the input of our parsers by using a POS tagger (Tsuruoka and Tsujii, 2005) which was trained on sections 0-18 in the WSJ corpus. The table also shows the performance achieved 9 Discussion 90 F-Score 85 80 75 70 Chunk parser Collins parser 65 0 50 100 150 200 250 300 350 400 450 500 Time (sec) Figure 6: Time vs F-score on section 23. The xaxis represents the time required to parse the entire section. The time required for making a hash table in Collins parser is excluded. Ratnaparkhi (1997) Collins (1999) Charniak (2000) Kudo (2005) Sang (2001) Deterministic (tagger-POSs) Deterministic (gold-POSs) Search (tagger-POSs) Search (gold-POSs) Iterative Search (tagger-POSs)"
W05-1514,J03-4003,0,\N,Missing
W06-1619,J97-4005,0,0.142664,"verb SUBJ &lt; 1 &gt; COMPS &lt; &gt; come Spring/NN HEAD verb 2 SUBJ &lt; 1 &gt; COMPS &lt;&gt; has/VBZ come/VBN flex= &lt;spring, NN, Figure 1: HPSG parsing. Ã X T0 exp &gt; (2005) also introduced a preliminary probabilistic model p0 (T |w) whose estimation does not require the parsing of a treebank. This model is introduced as a reference distribution of the probabilistic HPSG model; i.e., the computation of parse trees given low probabilities by the model is omitted in the estimation stage. We have ! X 1 exp λu fu (T ) phpsg (T |w) = Zw u Ã X HEAD noun SUBJ &lt;&gt; COMPS &lt;&gt; Figure 2: Example of features. Previous studies (Abney, 1997; Johnson et al., 1999; Riezler et al., 2000; Malouf and van Noord, 2004; Kaplan et al., 2004; Miyao and Tsujii, 2005) defined a probabilistic model of unification-based grammars including HPSG as a log-linear model or maximum entropy model (Berger et al., 1996). The probability that a parse result T is assigned to a given sentence w = hw1 , . . . , wn i is Zw = HEAD verb SUBJ &lt;NP&gt; COMPS &lt;&gt; (Previous probabilistic HPSG) Ã ! X 1 exp λu fu (T ) phpsg0 (T |w) = p0 (T |w) Zw u ! λu fu (T 0 ) , u where λu is a model parameter, fu is a feature function that represents a characteristic of parse tree"
W06-1619,J99-2004,0,0.501786,"studies vary in the design of the probabilistic models, the fundamental conception of probabilistic modeling is intended to capture characteristics of phrase structures or grammar rules. Although lexical information, such as head words, is known to significantly improve the parsing accuracy, it was also used to augment information on phrase structures. Another interesting approach to this problem was using supertagging (Clark and Curran, 2004b; Clark and Curran, 2004a; Wang and Harper, 2004; Nasr and Rambow, 2004), which was originally developed for lexicalized tree adjoining grammars (LTAG) (Bangalore and Joshi, 1999). Supertagging is a process where words in an input sentence are tagged with ‘supertags,’ which are lexical entries in lexicalized grammars, e.g., elementary trees in LTAG, lexical categories in CCG, and lexical entries in HPSG. Supertagging was, in the first place, a technique to reduce the cost of parsing with lexicalized grammars; ambiguity in assigning lexical entries to words is reduced by the light-weight process of supertagging before the heavy process of parsing. Bangalore and Joshi (1999) claimed that if words can be assigned correct supertags, syntactic parsing is almost trivial. Wha"
W06-1619,J96-1002,0,0.016666,"arsing of a treebank. This model is introduced as a reference distribution of the probabilistic HPSG model; i.e., the computation of parse trees given low probabilities by the model is omitted in the estimation stage. We have ! X 1 exp λu fu (T ) phpsg (T |w) = Zw u Ã X HEAD noun SUBJ &lt;&gt; COMPS &lt;&gt; Figure 2: Example of features. Previous studies (Abney, 1997; Johnson et al., 1999; Riezler et al., 2000; Malouf and van Noord, 2004; Kaplan et al., 2004; Miyao and Tsujii, 2005) defined a probabilistic model of unification-based grammars including HPSG as a log-linear model or maximum entropy model (Berger et al., 1996). The probability that a parse result T is assigned to a given sentence w = hw1 , . . . , wn i is Zw = HEAD verb SUBJ &lt;NP&gt; COMPS &lt;&gt; (Previous probabilistic HPSG) Ã ! X 1 exp λu fu (T ) phpsg0 (T |w) = p0 (T |w) Zw u ! λu fu (T 0 ) , u where λu is a model parameter, fu is a feature function that represents a characteristic of parse tree T , and Zw is the sum over the set of all possible parse trees for the sentence. Intuitively, the probability is defined as the normalized product of the weights exp(λu ) when a characteristic corresponding to fu appears in parse result T . The model parameters,"
W06-1619,P05-1022,0,0.134381,"Missing"
W06-1619,C04-1041,0,0.527929,"ment of Computer Science University of Tokyo Yoshimasa Tsuruoka School of Informatics University of Manchester Yusuke Miyao Department of Computer Science University of Tokyo Jun’ichi Tsujii Department of Computer Science, University of Tokyo School of Informatics, University of Manchester SORST, Japan Science and Technology Agency Hongo 7-3-1, Bunkyo-ku, Tokyo, 113-0033, Japan {ninomi, matuzaki, tsuruoka, yusuke, tsujii}@is.s.u-tokyo.ac.jp Abstract niak and Johnson, 2005) or over complex phrase structures of head-driven phrase structure grammar (HPSG) or combinatory categorial grammar (CCG) (Clark and Curran, 2004b; Malouf and van Noord, 2004; Miyao and Tsujii, 2005). Although these studies vary in the design of the probabilistic models, the fundamental conception of probabilistic modeling is intended to capture characteristics of phrase structures or grammar rules. Although lexical information, such as head words, is known to significantly improve the parsing accuracy, it was also used to augment information on phrase structures. Another interesting approach to this problem was using supertagging (Clark and Curran, 2004b; Clark and Curran, 2004a; Wang and Harper, 2004; Nasr and Rambow, 2004), which wa"
W06-1619,P05-1011,1,0.474032,"a Tsuruoka School of Informatics University of Manchester Yusuke Miyao Department of Computer Science University of Tokyo Jun’ichi Tsujii Department of Computer Science, University of Tokyo School of Informatics, University of Manchester SORST, Japan Science and Technology Agency Hongo 7-3-1, Bunkyo-ku, Tokyo, 113-0033, Japan {ninomi, matuzaki, tsuruoka, yusuke, tsujii}@is.s.u-tokyo.ac.jp Abstract niak and Johnson, 2005) or over complex phrase structures of head-driven phrase structure grammar (HPSG) or combinatory categorial grammar (CCG) (Clark and Curran, 2004b; Malouf and van Noord, 2004; Miyao and Tsujii, 2005). Although these studies vary in the design of the probabilistic models, the fundamental conception of probabilistic modeling is intended to capture characteristics of phrase structures or grammar rules. Although lexical information, such as head words, is known to significantly improve the parsing accuracy, it was also used to augment information on phrase structures. Another interesting approach to this problem was using supertagging (Clark and Curran, 2004b; Clark and Curran, 2004a; Wang and Harper, 2004; Nasr and Rambow, 2004), which was originally developed for lexicalized tree adjoining"
W06-1619,P04-1014,0,0.410288,"ment of Computer Science University of Tokyo Yoshimasa Tsuruoka School of Informatics University of Manchester Yusuke Miyao Department of Computer Science University of Tokyo Jun’ichi Tsujii Department of Computer Science, University of Tokyo School of Informatics, University of Manchester SORST, Japan Science and Technology Agency Hongo 7-3-1, Bunkyo-ku, Tokyo, 113-0033, Japan {ninomi, matuzaki, tsuruoka, yusuke, tsujii}@is.s.u-tokyo.ac.jp Abstract niak and Johnson, 2005) or over complex phrase structures of head-driven phrase structure grammar (HPSG) or combinatory categorial grammar (CCG) (Clark and Curran, 2004b; Malouf and van Noord, 2004; Miyao and Tsujii, 2005). Although these studies vary in the design of the probabilistic models, the fundamental conception of probabilistic modeling is intended to capture characteristics of phrase structures or grammar rules. Although lexical information, such as head words, is known to significantly improve the parsing accuracy, it was also used to augment information on phrase structures. Another interesting approach to this problem was using supertagging (Clark and Curran, 2004b; Clark and Curran, 2004a; Wang and Harper, 2004; Nasr and Rambow, 2004), which wa"
W06-1619,E03-1071,0,0.0114331,"k and Curran, 2004b). The CCG supertagger uses a maximum entropy classifier and is similar to our model. We evaluated the performance of our probabilistic model as a supertagger. The accuracy of the resulting supertagger on our development set (Section 22) is given in Table 5 and Table 6. The test sentences were automatically POS-tagged. Results of other supertaggers for automatically exWhen compared with other supertag sets of automatically extracted lexicalized grammars, the (effective) size of our supertag set, 1,361 lexical entries, is between the CCG supertag set (398 categories) used by Curran and Clark (2003) and the LTAG supertag set (2920 elementary trees) used by Shen and Joshi (2003). The relative order based on the sizes of the tag sets exactly matches the order based on the accuracies of corresponding supertaggers. 161 ambiguation of phrase structures. We have not yet investigated whether our results can be reproduced with other lexicalized grammars. Our results might hold only for HPSG because HPSG has strict feature constraints and has lexical entries with rich syntactic information such as wh-movement. 5.2 Efficacy of extremely lexicalized models The implemented parsers of models 1 and 2"
W06-1619,W04-3308,0,0.07953,"ar (CCG) (Clark and Curran, 2004b; Malouf and van Noord, 2004; Miyao and Tsujii, 2005). Although these studies vary in the design of the probabilistic models, the fundamental conception of probabilistic modeling is intended to capture characteristics of phrase structures or grammar rules. Although lexical information, such as head words, is known to significantly improve the parsing accuracy, it was also used to augment information on phrase structures. Another interesting approach to this problem was using supertagging (Clark and Curran, 2004b; Clark and Curran, 2004a; Wang and Harper, 2004; Nasr and Rambow, 2004), which was originally developed for lexicalized tree adjoining grammars (LTAG) (Bangalore and Joshi, 1999). Supertagging is a process where words in an input sentence are tagged with ‘supertags,’ which are lexical entries in lexicalized grammars, e.g., elementary trees in LTAG, lexical categories in CCG, and lexical entries in HPSG. Supertagging was, in the first place, a technique to reduce the cost of parsing with lexicalized grammars; ambiguity in assigning lexical entries to words is reduced by the light-weight process of supertagging before the heavy process of parsing. Bangalore and Jos"
W06-1619,P02-1036,0,0.0599348,"he weights exp(λu ) when a characteristic corresponding to fu appears in parse result T . The model parameters, λu , are estimated using numerical optimization methods (Malouf, 2002) to maximize the log-likelihood of the training data. However, the above model cannot be easily estimated because the estimation requires the computation of p(T |w) for all parse candidates assigned to sentence w. Because the number of parse candidates is exponentially related to the length of the sentence, the estimation is intractable for long sentences. To make the model estimation tractable, Geman and Johnson (Geman and Johnson, 2002) and Miyao and Tsujii (Miyao and Tsujii, 2002) proposed a dynamic programming algorithm for estimating p(T |w). Miyao and Tsujii Zw = X Ã 0 p0 (T |w) exp ! 0 λu fu (T ) u T0 p0 (T |w) = X n Y p(li |wi ), i=1 where li is a lexical entry assigned to word wi in T and p(li |wi ) is the probability of selecting lexical entry li for wi . In the experiments, we compared our model with the probabilistic HPSG model of Miyao and Tsujii (2005). The features used in their model are combinations of the feature templates listed in Table 1. The feature templates fbinary and funary are defined for constituent"
W06-1619,W05-1511,1,0.805598,"← α + ∆α; β ← β + ∆β; κ ← κ + ∆κ; δ ← δ + ∆δ; θ ← θ + ∆θ; 4.2 We evaluated the speed and accuracy of parsing with extremely lexicalized models by using Enju 2.1, the HPSG grammar for English (Miyao et al., 2005; Miyao and Tsujii, 2005). The lexicon of the grammar was extracted from Sections 02-21 of the Penn Treebank (Marcus et al., 1994) (39,832 sentences). The grammar consisted of 3,797 lexical entries for 10,536 words1 . The probabilistic models were trained using the same portion of the treebank. We used beam thresholding, global thresholding (Goodman, 1997), preserved iterative parsing (Ninomiya et al., 2005) and other techFigure 3: Pseudo-code of iterative parsing for HPSG. Zw = X exp Ã X l0 ! 0 λu fu (l , w, i) , u where Zw is the sum over all possible lexical entries for the word wi . The feature templates used in our model are listed in Table 2 and are word trigrams and POS 5-grams. 4 Evaluation Experiments 1 An HPSG treebank is automatically generated from the Penn Treebank. Those lexical entries were generated by applying lexical rules to observed lexical entries in the HPSG treebank (Nakanishi et al., 2004). The lexicon, however, included many lexical entries that do not appear in the HPSG"
W06-1619,W97-0302,0,0.1716,", i). The FOM of a newly created partial parse, F , is computed by summing the values of ρ of the daughters and an additional FOM of F if the model is the previous model or model 3. The FOM for models 1 and 2 is computed by only summing the values of ρ of the daughters; i.e., weights exp(λu ) in the figure are assigned zero. The terms κ and δ are the thresholds of the number of phrasal signs in the chart cell and the beam width for signs in the chart cell. The terms α and β are the thresholds of the number and the beam width of lexical entries, and θ is the beam width for global thresholding (Goodman, 1997). Table 2: Features for the probabilities of lexical entry selection. procedure Parsing(hw1 , . . . , wn i, hL, Ri, α, β, κ, δ, θ) for i = 1 to n foreachP F 0 ∈ {F |hwi , F i ∈ L} p= λu fu (F 0 ) u π[i − 1, i] ← π[i − 1, i] ∪ {F 0 } if (p &gt; ρ[i − 1, i, F 0 ]) then ρ[i − 1, i, F 0 ] ← p LocalThresholding(i − 1, i,α, β) for d = 1 to n for i = 0 to n − d j =i+d for k = i + 1 to j − 1 foreach Fs ∈ φ[i, k], Ft ∈ φ[k, j], r ∈ R if F = r(Fs , Ft ) has succeeded P λu fu (F ) p = ρ[i, k, Fs ] + ρ[k, j, Ft ] + u π[i, j] ← π[i, j] ∪ {F } if (p &gt; ρ[i, j, F ]) then ρ[i, j, F ] ← p LocalThresholding(i, j,κ,"
W06-1619,P03-1046,0,0.045596,"f the parser. A predicate-argument relation is defined as a tuple hσ, wh , a, wa i, where σ is the predicate type (e.g., adjective, intransitive verb), wh is the head word of the predicate, a is the argument label (MODARG, ARG1, ..., ARG4), and wa is the head word of the argument. Labeled precision (LP)/labeled recall (LR) is the ratio of tuples correctly identified by the parser3 . Unlabeled precision (UP)/unlabeled recall (UR) is the ratio of tuples without the predicate type and the argument label. This evaluation scheme was the same as used in previous evaluations of lexicalized grammars (Hockenmaier, 2003; Clark and Curran, 2004b; Miyao and Tsujii, 2005). The experiments were conducted on an AMD Opteron server with a 2.4-GHz CPU. Section 22 of the Treebank was used as the development set, and the performance was evaluated using sentences of ≤ 40 and 100 words in Section 23. The performance of each parsing technique was analyzed using the sentences in Section 24 of ≤ 100 words. Table 3 details the numbers and average lengths of the tested sentences of ≤ 40 and 100 words in Sections 23 and 24, and the total numbers of sentences in Sections 23 and 24. The parsing performance for Section 23 is sho"
W06-1619,P99-1069,0,0.0141744,"1 &gt; COMPS &lt; &gt; come Spring/NN HEAD verb 2 SUBJ &lt; 1 &gt; COMPS &lt;&gt; has/VBZ come/VBN flex= &lt;spring, NN, Figure 1: HPSG parsing. Ã X T0 exp &gt; (2005) also introduced a preliminary probabilistic model p0 (T |w) whose estimation does not require the parsing of a treebank. This model is introduced as a reference distribution of the probabilistic HPSG model; i.e., the computation of parse trees given low probabilities by the model is omitted in the estimation stage. We have ! X 1 exp λu fu (T ) phpsg (T |w) = Zw u Ã X HEAD noun SUBJ &lt;&gt; COMPS &lt;&gt; Figure 2: Example of features. Previous studies (Abney, 1997; Johnson et al., 1999; Riezler et al., 2000; Malouf and van Noord, 2004; Kaplan et al., 2004; Miyao and Tsujii, 2005) defined a probabilistic model of unification-based grammars including HPSG as a log-linear model or maximum entropy model (Berger et al., 1996). The probability that a parse result T is assigned to a given sentence w = hw1 , . . . , wn i is Zw = HEAD verb SUBJ &lt;NP&gt; COMPS &lt;&gt; (Previous probabilistic HPSG) Ã ! X 1 exp λu fu (T ) phpsg0 (T |w) = p0 (T |w) Zw u ! λu fu (T 0 ) , u where λu is a model parameter, fu is a feature function that represents a characteristic of parse tree T , and Zw is the sum"
W06-1619,P00-1061,0,0.0728146,"ing/NN HEAD verb 2 SUBJ &lt; 1 &gt; COMPS &lt;&gt; has/VBZ come/VBN flex= &lt;spring, NN, Figure 1: HPSG parsing. Ã X T0 exp &gt; (2005) also introduced a preliminary probabilistic model p0 (T |w) whose estimation does not require the parsing of a treebank. This model is introduced as a reference distribution of the probabilistic HPSG model; i.e., the computation of parse trees given low probabilities by the model is omitted in the estimation stage. We have ! X 1 exp λu fu (T ) phpsg (T |w) = Zw u Ã X HEAD noun SUBJ &lt;&gt; COMPS &lt;&gt; Figure 2: Example of features. Previous studies (Abney, 1997; Johnson et al., 1999; Riezler et al., 2000; Malouf and van Noord, 2004; Kaplan et al., 2004; Miyao and Tsujii, 2005) defined a probabilistic model of unification-based grammars including HPSG as a log-linear model or maximum entropy model (Berger et al., 1996). The probability that a parse result T is assigned to a given sentence w = hw1 , . . . , wn i is Zw = HEAD verb SUBJ &lt;NP&gt; COMPS &lt;&gt; (Previous probabilistic HPSG) Ã ! X 1 exp λu fu (T ) phpsg0 (T |w) = p0 (T |w) Zw u ! λu fu (T 0 ) , u where λu is a model parameter, fu is a feature function that represents a characteristic of parse tree T , and Zw is the sum over the set of all po"
W06-1619,N04-1013,0,0.0264089,"e/VBN flex= &lt;spring, NN, Figure 1: HPSG parsing. Ã X T0 exp &gt; (2005) also introduced a preliminary probabilistic model p0 (T |w) whose estimation does not require the parsing of a treebank. This model is introduced as a reference distribution of the probabilistic HPSG model; i.e., the computation of parse trees given low probabilities by the model is omitted in the estimation stage. We have ! X 1 exp λu fu (T ) phpsg (T |w) = Zw u Ã X HEAD noun SUBJ &lt;&gt; COMPS &lt;&gt; Figure 2: Example of features. Previous studies (Abney, 1997; Johnson et al., 1999; Riezler et al., 2000; Malouf and van Noord, 2004; Kaplan et al., 2004; Miyao and Tsujii, 2005) defined a probabilistic model of unification-based grammars including HPSG as a log-linear model or maximum entropy model (Berger et al., 1996). The probability that a parse result T is assigned to a given sentence w = hw1 , . . . , wn i is Zw = HEAD verb SUBJ &lt;NP&gt; COMPS &lt;&gt; (Previous probabilistic HPSG) Ã ! X 1 exp λu fu (T ) phpsg0 (T |w) = p0 (T |w) Zw u ! λu fu (T 0 ) , u where λu is a model parameter, fu is a feature function that represents a characteristic of parse tree T , and Zw is the sum over the set of all possible parse trees for the sentence. Intuitively,"
W06-1619,P03-1064,0,0.184888,"similar to our model. We evaluated the performance of our probabilistic model as a supertagger. The accuracy of the resulting supertagger on our development set (Section 22) is given in Table 5 and Table 6. The test sentences were automatically POS-tagged. Results of other supertaggers for automatically exWhen compared with other supertag sets of automatically extracted lexicalized grammars, the (effective) size of our supertag set, 1,361 lexical entries, is between the CCG supertag set (398 categories) used by Curran and Clark (2003) and the LTAG supertag set (2920 elementary trees) used by Shen and Joshi (2003). The relative order based on the sizes of the tag sets exactly matches the order based on the accuracies of corresponding supertaggers. 161 ambiguation of phrase structures. We have not yet investigated whether our results can be reproduced with other lexicalized grammars. Our results might hold only for HPSG because HPSG has strict feature constraints and has lexical entries with rich syntactic information such as wh-movement. 5.2 Efficacy of extremely lexicalized models The implemented parsers of models 1 and 2 were around four times faster than the previous model without a loss of accuracy"
W06-1619,P03-1054,0,0.0219404,"hrasestructure-based model. The hybrid model is not only significantly faster but also significantly more accurate by two points of precision and recall compared to the previous model. 1 Introduction For the last decade, accurate and wide-coverage parsing for real-world text has been intensively and extensively pursued. In most of state-of-theart parsers, probabilistic events are defined over phrase structures because phrase structures are supposed to dominate syntactic configurations of sentences. For example, probabilities were defined over grammar rules in probabilistic CFG (Collins, 1999; Klein and Manning, 2003; Char155 Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 155–163, c Sydney, July 2006. 2006 Association for Computational Linguistics probabilistic model is defined as the probability of unigram supertagging. So, the hybrid model can be regarded as an extension of supertagging from unigram to n-gram. The hybrid model can also be regarded as a variant of the statistical CDG parser (Wang, 2003; Wang and Harper, 2004), in which the parse tree probabilities are defined as the product of the supertagging probabilities and the dependency pr"
W06-1619,H05-1059,1,0.811846,"nd the performance was evaluated using sentences of ≤ 40 and 100 words in Section 23. The performance of each parsing technique was analyzed using the sentences in Section 24 of ≤ 100 words. Table 3 details the numbers and average lengths of the tested sentences of ≤ 40 and 100 words in Sections 23 and 24, and the total numbers of sentences in Sections 23 and 24. The parsing performance for Section 23 is shown in Table 4. The upper half of the table shows the performance using the correct POSs in the Penn Treebank, and the lower half shows the performance using the POSs given by a POS tagger (Tsuruoka and Tsujii, 2005). The left and right sides of the table show the performances for the sentences of ≤ 40 and ≤ 100 words. Our models significantly increased not only the parsing speed but also the parsing accuracy. Model 3 was around three to four times faster and had around two points higher precision and recall than the previous model. Surprisingly, model 1, which used only lexical information, was very fast and as accurate as the previous model. Model 2 also improved the accuracy slightly without information of phrase structures. When the automatic POS tagger was introduced, both precision and recall droppe"
W06-1619,W04-0307,0,0.294366,"natory categorial grammar (CCG) (Clark and Curran, 2004b; Malouf and van Noord, 2004; Miyao and Tsujii, 2005). Although these studies vary in the design of the probabilistic models, the fundamental conception of probabilistic modeling is intended to capture characteristics of phrase structures or grammar rules. Although lexical information, such as head words, is known to significantly improve the parsing accuracy, it was also used to augment information on phrase structures. Another interesting approach to this problem was using supertagging (Clark and Curran, 2004b; Clark and Curran, 2004a; Wang and Harper, 2004; Nasr and Rambow, 2004), which was originally developed for lexicalized tree adjoining grammars (LTAG) (Bangalore and Joshi, 1999). Supertagging is a process where words in an input sentence are tagged with ‘supertags,’ which are lexical entries in lexicalized grammars, e.g., elementary trees in LTAG, lexical categories in CCG, and lexical entries in HPSG. Supertagging was, in the first place, a technique to reduce the cost of parsing with lexicalized grammars; ambiguity in assigning lexical entries to words is reduced by the light-weight process of supertagging before the heavy process of pa"
W06-1619,W02-2018,0,0.00906575,"iven sentence w = hw1 , . . . , wn i is Zw = HEAD verb SUBJ &lt;NP&gt; COMPS &lt;&gt; (Previous probabilistic HPSG) Ã ! X 1 exp λu fu (T ) phpsg0 (T |w) = p0 (T |w) Zw u ! λu fu (T 0 ) , u where λu is a model parameter, fu is a feature function that represents a characteristic of parse tree T , and Zw is the sum over the set of all possible parse trees for the sentence. Intuitively, the probability is defined as the normalized product of the weights exp(λu ) when a characteristic corresponding to fu appears in parse result T . The model parameters, λu , are estimated using numerical optimization methods (Malouf, 2002) to maximize the log-likelihood of the training data. However, the above model cannot be easily estimated because the estimation requires the computation of p(T |w) for all parse candidates assigned to sentence w. Because the number of parse candidates is exponentially related to the length of the sentence, the estimation is intractable for long sentences. To make the model estimation tractable, Geman and Johnson (Geman and Johnson, 2002) and Miyao and Tsujii (Miyao and Tsujii, 2002) proposed a dynamic programming algorithm for estimating p(T |w). Miyao and Tsujii Zw = X Ã 0 p0 (T |w) exp ! 0"
W06-1619,J93-2004,0,\N,Missing
W06-1619,J03-4003,0,\N,Missing
W06-1634,H94-1020,0,0.10923,"Missing"
W06-1634,P03-1029,0,0.0540281,"ons for information like protein-protein interactions. Full parsing is more effective for acquiring generalized data from long-length words than shallow parsing. The sentences at left in Figure 1 exemplify the advantages of full parsing. The gerund “activating” in the last sentence takes a non-local semantic subject “ENTITY1”, and shallow parsing cannot recognize this relation because “ENTITY1” and “activating” are in different phrases. Full parsing, on the other hand, can identify both the subject of the whole sentence and the semantic subject of “activating” have been shared. 3 Related Work Sudo et al. (2003), Culotta and Sorensen (2004) and Bunescu and Mooney (2005) acquired substructures derived from dependency trees as extraction patterns for IE in general domains. Their approaches were similar to our approach using PASs derived from full parsing. However, one problem with their systems is that they could not treat non-local dependencies such as semantic subjects of gerund constructions (discussed in Section 2), and thus rules acquired from the constructions were partial. Bunescu and Mooney (2006) also learned extraction patterns for protein-protein interactions by SVM with a generalized subseq"
W06-1634,P04-1056,0,0.125732,"Missing"
W06-1634,H05-1091,0,0.0138968,"s. Full parsing is more effective for acquiring generalized data from long-length words than shallow parsing. The sentences at left in Figure 1 exemplify the advantages of full parsing. The gerund “activating” in the last sentence takes a non-local semantic subject “ENTITY1”, and shallow parsing cannot recognize this relation because “ENTITY1” and “activating” are in different phrases. Full parsing, on the other hand, can identify both the subject of the whole sentence and the semantic subject of “activating” have been shared. 3 Related Work Sudo et al. (2003), Culotta and Sorensen (2004) and Bunescu and Mooney (2005) acquired substructures derived from dependency trees as extraction patterns for IE in general domains. Their approaches were similar to our approach using PASs derived from full parsing. However, one problem with their systems is that they could not treat non-local dependencies such as semantic subjects of gerund constructions (discussed in Section 2), and thus rules acquired from the constructions were partial. Bunescu and Mooney (2006) also learned extraction patterns for protein-protein interactions by SVM with a generalized subsequence kernel. Their patterns are sequences of words, POSs,"
W06-1634,P05-1052,0,0.0403215,"within the high-recall range. Compared to theirs, one of our problems is that our method could not handle attributives. One example is “binding property of ENTITY1 to ENTITY2”. We could not obtain “binding” because the smallest set of PASs connecting “ENTITY1” and “ENTITY2” includes only the PASs of “property”, “of” and “to”. To handle these attributives, we need distinguish necessary attributives from those that are general4 by semantic analysis or bootstrapping. Another approach to improve our method is to include local information in sentences, such as surface words between protein names. Zhao and Grishman (2005) reported that adding local information to deep syntactic information improved IE results. This approach is also applicable to IE in other domains, where related entities are in a short 5.3.2 Lack of Necessary Patterns and Learning of Inappropriate Patterns There are two different reasons causing the problems with the lack of necessary patterns and the learning of inappropriate patterns: (1) the training corpus was not sufﬁciently large to saturate IE accuracy and (2) our method of pattern construction was too limited. Effect of Training Corpus Size To investigate whether the training corpus w"
W06-1634,P04-1054,0,0.0565752,"like protein-protein interactions. Full parsing is more effective for acquiring generalized data from long-length words than shallow parsing. The sentences at left in Figure 1 exemplify the advantages of full parsing. The gerund “activating” in the last sentence takes a non-local semantic subject “ENTITY1”, and shallow parsing cannot recognize this relation because “ENTITY1” and “activating” are in different phrases. Full parsing, on the other hand, can identify both the subject of the whole sentence and the semantic subject of “activating” have been shared. 3 Related Work Sudo et al. (2003), Culotta and Sorensen (2004) and Bunescu and Mooney (2005) acquired substructures derived from dependency trees as extraction patterns for IE in general domains. Their approaches were similar to our approach using PASs derived from full parsing. However, one problem with their systems is that they could not treat non-local dependencies such as semantic subjects of gerund constructions (discussed in Section 2), and thus rules acquired from the constructions were partial. Bunescu and Mooney (2006) also learned extraction patterns for protein-protein interactions by SVM with a generalized subsequence kernel. Their patterns"
W06-1634,P05-1053,0,0.0780532,"Missing"
W06-3327,I05-1018,1,0.898746,"Missing"
W06-3327,A00-2021,0,0.0413661,"Missing"
W06-3327,W03-1018,1,0.866569,"Missing"
W06-3327,W04-3111,0,0.0269627,"Missing"
W06-3327,I05-1006,0,0.0437125,"Missing"
W06-3327,J93-2004,0,0.0311788,"Missing"
W06-3327,W96-0213,0,0.284311,"Missing"
W06-3327,tateisi-tsujii-2004-part,1,0.904918,"Missing"
W06-3327,N03-1033,0,0.019641,"Missing"
W07-1033,J96-1002,0,0.0149599,"contribute to the performances of named entity recognizers (Peshkin and Pfefer, 2003), our system uses the original tagset of the training data, except that the ‘BOS’ label is added to represent the state before the beginning of sentences. Probability of state transition to the i-th label of a sentence is calculated by the following formula: P exp( j λj fj (li , li−1 , S)) P . (1) P (li |li−1 , S) = P l exp( j λj fj (l, li−1 , S)) 211 where li is the next BIO tag, li−1 is the previous BIO tag, S is the target sentence, and fj and lj are feature functions and parameters of a log-linear model (Berger et al., 1996). As a first order MEMM, the probability of a label li is dependent on the previous label li−1 , and when we calculate the normalization constant in the right hand side (i.e. the denominator of the fraction), we limit the range of l to the possible successors of the previous label. This probability is multiplied to obtain the probability of a label sequence for a sentence: P (l1...n |S) = Y P (li |li−1 ). (2) i The probability in Eq. 1. is estimated as a single log-linear model, regardless to the types of the target labels. N-best tag sequences of input sentences are obtained by well-known com"
W07-1033,P04-1056,0,0.0758601,"Missing"
W07-1033,P05-1022,0,0.147312,"Missing"
W07-1033,W04-1213,0,0.247403,"variety of named entity expressions used in the domain. It is common for practical protein or gene databases to contain hundreds of thousands of items. Such a large variety of vocabulary naturally leads to long names with productive use of general words, making the task difficult to be solved by systems with naive Markov assumption of label sequences, because such systems must perform their prediction without seeing the entire string of the entities. Importance of the treatment of long names might be implicitly indicated in the performance comparison of the participants of JNLPBA shared task (Kim et al., 2004), where the best performing system (Zhou and Su, 2004) attains their scores by extensive post-processing, which enabled the system to make use of global information of the entity labels. After the shared task, many researchers tackled the task by using conditional random fields (CRFs) (Lafferty et al., 2001), which seemed to promise improvement over locally optimized models like maximum entropy Markov models (MEMMs) (McCallum et al., 2000). However, many of the CRF systems developed after the shared task failed to reach the best performance achieved by Zhou et al. One of the reasons may be the"
W07-1033,P06-1141,0,0.100307,"Missing"
W07-1033,N06-1020,0,0.0493792,"Missing"
W07-1033,P06-1059,1,0.956,"ipated in the shared task. The systems use various classification models including CRFs, hidden Markov models (HMMs), support vector machines (SVMs), and MEMMs, with various features and external resources. Though it is impossible to observe clear correlation between the performance and classification models or resources used, an important characteristic of the best system by Zhou et al. (2004) seems to be extensive use of rule-based post processing they apply to the output of their classifier. After the shared task, several researchers tackled the problem using the CRFs and their extensions. Okanohara et al. (2006) applied semiCRFs (Sarawagi and Cohen, 2004), which can treat multiple words as corresponding to a single state. Friedrich et al. (2006) used CRFs with features from the external gazetteer. Current state-of-the-art for the shared-task is achieved by Tsai et al. (2006), whose improvement depends on careful design of features including the normalization of numeric expressions, and use of post-processing by automatically extracted patterns. IL-2 B-DNA gene I-DNA expression O requires O reactive O oxygen O production O by O 5-lipoxygenase B-protein . O Figure 1: Example sentence from the training"
W07-1033,W04-1219,0,0.0340595,"n. It is common for practical protein or gene databases to contain hundreds of thousands of items. Such a large variety of vocabulary naturally leads to long names with productive use of general words, making the task difficult to be solved by systems with naive Markov assumption of label sequences, because such systems must perform their prediction without seeing the entire string of the entities. Importance of the treatment of long names might be implicitly indicated in the performance comparison of the participants of JNLPBA shared task (Kim et al., 2004), where the best performing system (Zhou and Su, 2004) attains their scores by extensive post-processing, which enabled the system to make use of global information of the entity labels. After the shared task, many researchers tackled the task by using conditional random fields (CRFs) (Lafferty et al., 2001), which seemed to promise improvement over locally optimized models like maximum entropy Markov models (MEMMs) (McCallum et al., 2000). However, many of the CRF systems developed after the shared task failed to reach the best performance achieved by Zhou et al. One of the reasons may be the deficiency of the dynamic programming-based systems,"
W07-2202,A00-2021,0,0.0726437,"Missing"
W07-2202,I05-1006,0,0.0756674,"Missing"
W07-2202,H94-1020,0,0.181523,"Missing"
W07-2202,J99-2004,0,0.068113,"Missing"
W07-2202,J96-1002,0,0.00405831,"Missing"
W07-2202,W06-1615,0,0.0552113,"Missing"
W07-2202,P06-1012,0,0.0317632,"Missing"
W07-2202,C04-1041,0,0.0205609,"Missing"
W07-2202,P04-1014,0,0.0114937,"Missing"
W07-2202,N06-1019,0,0.0615363,"Missing"
W07-2202,W05-1102,0,0.112719,"Missing"
W07-2202,I05-1018,1,0.493404,"Missing"
W07-2202,P06-1043,0,0.0634495,"Missing"
W07-2202,P05-1011,1,0.486667,"Missing"
W07-2202,W06-1619,1,0.793255,"Missing"
W07-2202,W04-1203,0,0.0172303,"Missing"
W07-2202,N03-1027,0,0.104549,"Missing"
W07-2202,E03-1008,0,0.0935117,"Missing"
W07-2202,W06-2902,0,0.126602,"Missing"
W07-2208,P06-1041,0,0.0124517,"ciency were investigated for estimation (Geman and Johnson, 2002; Miyao and Tsujii, 2002; Malouf and van Noord, 2004) and parsing (Clark and Curran, 2004b; Clark and Curran, 2004a; Ninomiya et al., 2005). An interesting approach to the problem of parsing efficiency was using supertagging (Clark and Cur60 Proceedings of the 10th Conference on Parsing Technologies, pages 60–68, c Prague, Czech Republic, June 2007. 2007 Association for Computational Linguistics ran, 2004b; Clark and Curran, 2004a; Wang, 2003; Wang and Harper, 2004; Nasr and Rambow, 2004; Ninomiya et al., 2006; Foth et al., 2006; Foth and Menzel, 2006), which was originally developed for lexicalized tree adjoining grammars (LTAG) (Bangalore and Joshi, 1999). Supertagging is a process where words in an input sentence are tagged with ‘supertags,’ which are lexical entries in lexicalized grammars, e.g., elementary trees in LTAG, lexical categories in CCG, and lexical entries in HPSG. The concept of supertagging is simple and interesting, and the effects of this were recently demonstrated in the case of a CCG parser (Clark and Curran, 2004a) with the result of a drastic improvement in the parsing speed. Wang and Harper (2004) also demonstrated"
W07-2208,J97-4005,0,0.510494,"ar (LFG) (Bresnan, 1982). They are preferred because they give precise and in-depth analyses for explaining linguistic phenomena, such as passivization, control verbs and relative clauses. The main difficulty of developing parsers in these formalisms was how to model a well-defined probabilistic model for graph structures such as feature structures. This was overcome by a probabilistic model which provides probabilities of discriminating a correct parse tree among candidates of parse trees in a log-linear model or maximum entropy model (Berger et al., 1996) with many features for parse trees (Abney, 1997; Johnson et al., 1999; Riezler et al., 2000; Malouf and van Noord, 2004; Kaplan et al., 2004; Miyao and Tsujii, 2005). Following this discriminative approach, techniques for efficiency were investigated for estimation (Geman and Johnson, 2002; Miyao and Tsujii, 2002; Malouf and van Noord, 2004) and parsing (Clark and Curran, 2004b; Clark and Curran, 2004a; Ninomiya et al., 2005). An interesting approach to the problem of parsing efficiency was using supertagging (Clark and Cur60 Proceedings of the 10th Conference on Parsing Technologies, pages 60–68, c Prague, Czech Republic, June 2007. 2007"
W07-2208,J99-2004,0,0.0748299,"n Noord, 2004) and parsing (Clark and Curran, 2004b; Clark and Curran, 2004a; Ninomiya et al., 2005). An interesting approach to the problem of parsing efficiency was using supertagging (Clark and Cur60 Proceedings of the 10th Conference on Parsing Technologies, pages 60–68, c Prague, Czech Republic, June 2007. 2007 Association for Computational Linguistics ran, 2004b; Clark and Curran, 2004a; Wang, 2003; Wang and Harper, 2004; Nasr and Rambow, 2004; Ninomiya et al., 2006; Foth et al., 2006; Foth and Menzel, 2006), which was originally developed for lexicalized tree adjoining grammars (LTAG) (Bangalore and Joshi, 1999). Supertagging is a process where words in an input sentence are tagged with ‘supertags,’ which are lexical entries in lexicalized grammars, e.g., elementary trees in LTAG, lexical categories in CCG, and lexical entries in HPSG. The concept of supertagging is simple and interesting, and the effects of this were recently demonstrated in the case of a CCG parser (Clark and Curran, 2004a) with the result of a drastic improvement in the parsing speed. Wang and Harper (2004) also demonstrated the effects of supertagging with a statistical constraint dependency grammar (CDG) parser by showing accura"
W07-2208,J96-1002,0,0.0413835,"grammar (CCG) (Steedman, 2000) and lexical function grammar (LFG) (Bresnan, 1982). They are preferred because they give precise and in-depth analyses for explaining linguistic phenomena, such as passivization, control verbs and relative clauses. The main difficulty of developing parsers in these formalisms was how to model a well-defined probabilistic model for graph structures such as feature structures. This was overcome by a probabilistic model which provides probabilities of discriminating a correct parse tree among candidates of parse trees in a log-linear model or maximum entropy model (Berger et al., 1996) with many features for parse trees (Abney, 1997; Johnson et al., 1999; Riezler et al., 2000; Malouf and van Noord, 2004; Kaplan et al., 2004; Miyao and Tsujii, 2005). Following this discriminative approach, techniques for efficiency were investigated for estimation (Geman and Johnson, 2002; Miyao and Tsujii, 2002; Malouf and van Noord, 2004) and parsing (Clark and Curran, 2004b; Clark and Curran, 2004a; Ninomiya et al., 2005). An interesting approach to the problem of parsing efficiency was using supertagging (Clark and Cur60 Proceedings of the 10th Conference on Parsing Technologies, pages 6"
W07-2208,C04-1041,0,0.456892,"es such as feature structures. This was overcome by a probabilistic model which provides probabilities of discriminating a correct parse tree among candidates of parse trees in a log-linear model or maximum entropy model (Berger et al., 1996) with many features for parse trees (Abney, 1997; Johnson et al., 1999; Riezler et al., 2000; Malouf and van Noord, 2004; Kaplan et al., 2004; Miyao and Tsujii, 2005). Following this discriminative approach, techniques for efficiency were investigated for estimation (Geman and Johnson, 2002; Miyao and Tsujii, 2002; Malouf and van Noord, 2004) and parsing (Clark and Curran, 2004b; Clark and Curran, 2004a; Ninomiya et al., 2005). An interesting approach to the problem of parsing efficiency was using supertagging (Clark and Cur60 Proceedings of the 10th Conference on Parsing Technologies, pages 60–68, c Prague, Czech Republic, June 2007. 2007 Association for Computational Linguistics ran, 2004b; Clark and Curran, 2004a; Wang, 2003; Wang and Harper, 2004; Nasr and Rambow, 2004; Ninomiya et al., 2006; Foth et al., 2006; Foth and Menzel, 2006), which was originally developed for lexicalized tree adjoining grammars (LTAG) (Bangalore and Joshi, 1999). Supertagging is a proc"
W07-2208,P04-1014,0,0.211016,"es such as feature structures. This was overcome by a probabilistic model which provides probabilities of discriminating a correct parse tree among candidates of parse trees in a log-linear model or maximum entropy model (Berger et al., 1996) with many features for parse trees (Abney, 1997; Johnson et al., 1999; Riezler et al., 2000; Malouf and van Noord, 2004; Kaplan et al., 2004; Miyao and Tsujii, 2005). Following this discriminative approach, techniques for efficiency were investigated for estimation (Geman and Johnson, 2002; Miyao and Tsujii, 2002; Malouf and van Noord, 2004) and parsing (Clark and Curran, 2004b; Clark and Curran, 2004a; Ninomiya et al., 2005). An interesting approach to the problem of parsing efficiency was using supertagging (Clark and Cur60 Proceedings of the 10th Conference on Parsing Technologies, pages 60–68, c Prague, Czech Republic, June 2007. 2007 Association for Computational Linguistics ran, 2004b; Clark and Curran, 2004a; Wang, 2003; Wang and Harper, 2004; Nasr and Rambow, 2004; Ninomiya et al., 2006; Foth et al., 2006; Foth and Menzel, 2006), which was originally developed for lexicalized tree adjoining grammars (LTAG) (Bangalore and Joshi, 1999). Supertagging is a proc"
W07-2208,P06-1037,0,0.0803821,"are tagged with ‘supertags,’ which are lexical entries in lexicalized grammars, e.g., elementary trees in LTAG, lexical categories in CCG, and lexical entries in HPSG. The concept of supertagging is simple and interesting, and the effects of this were recently demonstrated in the case of a CCG parser (Clark and Curran, 2004a) with the result of a drastic improvement in the parsing speed. Wang and Harper (2004) also demonstrated the effects of supertagging with a statistical constraint dependency grammar (CDG) parser by showing accuracy as high as the state-of-the-art parsers, and Foth et al. (2006) and Foth and Menzel (2006) reported that accuracy was significantly improved by incorporating the supertagging probabilities into manually tuned Weighted CDG. Ninomiya et al. (2006) showed the parsing model using only supertagging probabilities could achieve accuracy as high as the probabilistic model for phrase structures. This means that syntactic structures are almost determined by supertags as is claimed by Bangalore and Joshi (1999). However, supertaggers themselves were heuristically used as an external tagger. They filter out unlikely lexical entries just to help parsing (Clark and Cur"
W07-2208,P02-1036,0,0.316566,"in these formalisms was how to model a well-defined probabilistic model for graph structures such as feature structures. This was overcome by a probabilistic model which provides probabilities of discriminating a correct parse tree among candidates of parse trees in a log-linear model or maximum entropy model (Berger et al., 1996) with many features for parse trees (Abney, 1997; Johnson et al., 1999; Riezler et al., 2000; Malouf and van Noord, 2004; Kaplan et al., 2004; Miyao and Tsujii, 2005). Following this discriminative approach, techniques for efficiency were investigated for estimation (Geman and Johnson, 2002; Miyao and Tsujii, 2002; Malouf and van Noord, 2004) and parsing (Clark and Curran, 2004b; Clark and Curran, 2004a; Ninomiya et al., 2005). An interesting approach to the problem of parsing efficiency was using supertagging (Clark and Cur60 Proceedings of the 10th Conference on Parsing Technologies, pages 60–68, c Prague, Czech Republic, June 2007. 2007 Association for Computational Linguistics ran, 2004b; Clark and Curran, 2004a; Wang, 2003; Wang and Harper, 2004; Nasr and Rambow, 2004; Ninomiya et al., 2006; Foth et al., 2006; Foth and Menzel, 2006), which was originally developed for lexic"
W07-2208,W97-0302,0,0.0999642,"and Tsujii (2005)’s model, but ‘our model 1’ achieved 0.56 points higher F-score, and ‘our model 2’ achieved 0.8 points higher F-score. When the automatic POS tagger was introduced, Fscore dropped by around 2.4 points for all models. We also compared our model with Matsuzaki et al. (2007)’s model. Matsuzaki et al. (2007) proThe terms κ and δ are the thresholds of the number of phrasal signs in the chart cell and the beam width for signs in the chart cell. The terms α and β are the thresholds of the number and the beam width of lexical entries, and θ is the beam width for global thresholding (Goodman, 1997). The terms with suffixes 0 are the initial values. The parser iterates parsing until it succeeds to generate a parse tree. The parameters increase for each iteration by the terms prefixed by ∆, and parsing finishes when the parameters reach the terms with suffixes last. Details of the parameters are written in (Ninomiya et al., 2005). The beam thresholding parameters for ‘our model 2’ are α0 = 18, ∆α = 6, αlast = 42, β0 = 9.0, ∆β = 3.0, βlast = 21.0, δ0 = 18, ∆δ = 6, δlast = 42, κ0 = 9.0, ∆κ = 3.0, κlast = 21.0. In ‘our model 2’, the global thresholding was not used. 66 posed a technique for"
W07-2208,P03-1046,0,0.0277048,"f the parser. A predicate-argument relation is defined as a tuple hσ, wh , a, wa i, where σ is the predicate type (e.g., adjective, intransitive verb), wh is the head word of the predicate, a is the argument label (MODARG, ARG1, ..., ARG4), and wa is the head word of the argument. Labeled precision (LP)/labeled recall (LR) is the ratio of tuples correctly identified by the parser2 . Unlabeled precision (UP)/unlabeled recall (UR) is the ratio of tuples without the predicate type and the argument label. This evaluation scheme was the same as used in previous evaluations of lexicalized grammars (Hockenmaier, 2003; Clark The HPSG treebank is used for training the probabilistic model for lexical entry selection, and hence, those lexical entries that do not appear in the treebank are rarely selected by the probabilistic model. The ‘effective’ tag set size, therefore, is around 1,361, the number of lexical entries without those never-seen lexical entries. 2 When parsing fails, precision and recall are evaluated, although nothing is output by the parser; i.e., recall decreases greatly. 65 and Curran, 2004b; Miyao and Tsujii, 2005). The experiments were conducted on an AMD Opteron server with a 2.4-GHz CPU."
W07-2208,A00-2021,0,0.0236621,"ence w = hw1 , . . . , wn i is to sentence w. Because the number of parse candidates is exponentially related to the length of the sentence, the estimation is intractable for long sentences. To make the model estimation tractable, Geman and Johnson (Geman and Johnson, 2002) and Miyao and Tsujii (Miyao and Tsujii, 2002) proposed a dynamic programming algorithm for estimating p(T |w). Miyao and Tsujii (2005) also introduced a preliminary probabilistic model p0 (T |w) whose estimation does not require the parsing of a treebank. This model is introduced as a reference distribution (Jelinek, 1998; Johnson and Riezler, 2000) of the probabilistic HPSG model; i.e., the computation of parse trees given low probabilities by the model is omitted in the estimation stage (Miyao and Tsujii, 2005), or a probabilistic model can be augmented by several distributions estimated from the larger and simpler corpus (Johnson and Riezler, 2000). In (Miyao and Tsujii, 2005), p0 (T |w) is defined as the product of probabilities of selecting lexical entries with word and POS unigram features: (Miyao and Tsujii (2005)’s model) 1 puniref (T |w) = p0 (T |w) exp Zw Zw = (Probabilistic HPSG) Zw = T0 Ã 0 p0 (T |w) exp T0 phpsg (T |w) = X X"
W07-2208,P99-1069,0,0.311858,"snan, 1982). They are preferred because they give precise and in-depth analyses for explaining linguistic phenomena, such as passivization, control verbs and relative clauses. The main difficulty of developing parsers in these formalisms was how to model a well-defined probabilistic model for graph structures such as feature structures. This was overcome by a probabilistic model which provides probabilities of discriminating a correct parse tree among candidates of parse trees in a log-linear model or maximum entropy model (Berger et al., 1996) with many features for parse trees (Abney, 1997; Johnson et al., 1999; Riezler et al., 2000; Malouf and van Noord, 2004; Kaplan et al., 2004; Miyao and Tsujii, 2005). Following this discriminative approach, techniques for efficiency were investigated for estimation (Geman and Johnson, 2002; Miyao and Tsujii, 2002; Malouf and van Noord, 2004) and parsing (Clark and Curran, 2004b; Clark and Curran, 2004a; Ninomiya et al., 2005). An interesting approach to the problem of parsing efficiency was using supertagging (Clark and Cur60 Proceedings of the 10th Conference on Parsing Technologies, pages 60–68, c Prague, Czech Republic, June 2007. 2007 Association for Comput"
W07-2208,N04-1013,0,0.253832,"nalyses for explaining linguistic phenomena, such as passivization, control verbs and relative clauses. The main difficulty of developing parsers in these formalisms was how to model a well-defined probabilistic model for graph structures such as feature structures. This was overcome by a probabilistic model which provides probabilities of discriminating a correct parse tree among candidates of parse trees in a log-linear model or maximum entropy model (Berger et al., 1996) with many features for parse trees (Abney, 1997; Johnson et al., 1999; Riezler et al., 2000; Malouf and van Noord, 2004; Kaplan et al., 2004; Miyao and Tsujii, 2005). Following this discriminative approach, techniques for efficiency were investigated for estimation (Geman and Johnson, 2002; Miyao and Tsujii, 2002; Malouf and van Noord, 2004) and parsing (Clark and Curran, 2004b; Clark and Curran, 2004a; Ninomiya et al., 2005). An interesting approach to the problem of parsing efficiency was using supertagging (Clark and Cur60 Proceedings of the 10th Conference on Parsing Technologies, pages 60–68, c Prague, Czech Republic, June 2007. 2007 Association for Computational Linguistics ran, 2004b; Clark and Curran, 2004a; Wang, 2003; Wa"
W07-2208,W05-1511,1,0.920749,"y a probabilistic model which provides probabilities of discriminating a correct parse tree among candidates of parse trees in a log-linear model or maximum entropy model (Berger et al., 1996) with many features for parse trees (Abney, 1997; Johnson et al., 1999; Riezler et al., 2000; Malouf and van Noord, 2004; Kaplan et al., 2004; Miyao and Tsujii, 2005). Following this discriminative approach, techniques for efficiency were investigated for estimation (Geman and Johnson, 2002; Miyao and Tsujii, 2002; Malouf and van Noord, 2004) and parsing (Clark and Curran, 2004b; Clark and Curran, 2004a; Ninomiya et al., 2005). An interesting approach to the problem of parsing efficiency was using supertagging (Clark and Cur60 Proceedings of the 10th Conference on Parsing Technologies, pages 60–68, c Prague, Czech Republic, June 2007. 2007 Association for Computational Linguistics ran, 2004b; Clark and Curran, 2004a; Wang, 2003; Wang and Harper, 2004; Nasr and Rambow, 2004; Ninomiya et al., 2006; Foth et al., 2006; Foth and Menzel, 2006), which was originally developed for lexicalized tree adjoining grammars (LTAG) (Bangalore and Joshi, 1999). Supertagging is a process where words in an input sentence are tagged wi"
W07-2208,W06-1619,1,0.158093,"scriminative approach, techniques for efficiency were investigated for estimation (Geman and Johnson, 2002; Miyao and Tsujii, 2002; Malouf and van Noord, 2004) and parsing (Clark and Curran, 2004b; Clark and Curran, 2004a; Ninomiya et al., 2005). An interesting approach to the problem of parsing efficiency was using supertagging (Clark and Cur60 Proceedings of the 10th Conference on Parsing Technologies, pages 60–68, c Prague, Czech Republic, June 2007. 2007 Association for Computational Linguistics ran, 2004b; Clark and Curran, 2004a; Wang, 2003; Wang and Harper, 2004; Nasr and Rambow, 2004; Ninomiya et al., 2006; Foth et al., 2006; Foth and Menzel, 2006), which was originally developed for lexicalized tree adjoining grammars (LTAG) (Bangalore and Joshi, 1999). Supertagging is a process where words in an input sentence are tagged with ‘supertags,’ which are lexical entries in lexicalized grammars, e.g., elementary trees in LTAG, lexical categories in CCG, and lexical entries in HPSG. The concept of supertagging is simple and interesting, and the effects of this were recently demonstrated in the case of a CCG parser (Clark and Curran, 2004a) with the result of a drastic improvement in the parsing speed"
W07-2208,P00-1061,0,0.388867,"preferred because they give precise and in-depth analyses for explaining linguistic phenomena, such as passivization, control verbs and relative clauses. The main difficulty of developing parsers in these formalisms was how to model a well-defined probabilistic model for graph structures such as feature structures. This was overcome by a probabilistic model which provides probabilities of discriminating a correct parse tree among candidates of parse trees in a log-linear model or maximum entropy model (Berger et al., 1996) with many features for parse trees (Abney, 1997; Johnson et al., 1999; Riezler et al., 2000; Malouf and van Noord, 2004; Kaplan et al., 2004; Miyao and Tsujii, 2005). Following this discriminative approach, techniques for efficiency were investigated for estimation (Geman and Johnson, 2002; Miyao and Tsujii, 2002; Malouf and van Noord, 2004) and parsing (Clark and Curran, 2004b; Clark and Curran, 2004a; Ninomiya et al., 2005). An interesting approach to the problem of parsing efficiency was using supertagging (Clark and Cur60 Proceedings of the 10th Conference on Parsing Technologies, pages 60–68, c Prague, Czech Republic, June 2007. 2007 Association for Computational Linguistics ra"
W07-2208,W02-2018,0,0.014737,"SG) Zw = T0 Ã 0 p0 (T |w) exp T0 phpsg (T |w) = X X 1 exp Zw Ã exp Ã X X ! p0 (T |w) = λu fu (T ) X Ã X u ! λu fu (T ) ! 0 λu fu (T ) u n Y p(li |wi ), i=1 u ! 0 λu fu (T ) , u where λu is a model parameter, fu is a feature function that represents a characteristic of parse tree T , and Zw is the sum over the set of all possible parse trees for the sentence. Intuitively, the probability is defined as the normalized product of the weights exp(λu ) when a characteristic corresponding to fu appears in parse result T . The model parameters, λu , are estimated using numerical optimization methods (Malouf, 2002) to maximize the log-likelihood of the training data. However, the above model cannot be easily estimated because the estimation requires the computation of p(T |w) for all parse candidates assigned 62 where li is a lexical entry assigned to word wi in T and p(li |wi ) is the probability of selecting lexical entry li for wi . In the experiments, we compared our model with other two types of probabilistic models using a supertagger (Ninomiya et al., 2006). The first one is the simplest probabilistic model, which is defined with only the probabilities of lexical entry selection. It is defined si"
W07-2208,H05-1059,1,0.820103,"Missing"
W07-2208,W04-0307,0,0.13942,"04; Miyao and Tsujii, 2005). Following this discriminative approach, techniques for efficiency were investigated for estimation (Geman and Johnson, 2002; Miyao and Tsujii, 2002; Malouf and van Noord, 2004) and parsing (Clark and Curran, 2004b; Clark and Curran, 2004a; Ninomiya et al., 2005). An interesting approach to the problem of parsing efficiency was using supertagging (Clark and Cur60 Proceedings of the 10th Conference on Parsing Technologies, pages 60–68, c Prague, Czech Republic, June 2007. 2007 Association for Computational Linguistics ran, 2004b; Clark and Curran, 2004a; Wang, 2003; Wang and Harper, 2004; Nasr and Rambow, 2004; Ninomiya et al., 2006; Foth et al., 2006; Foth and Menzel, 2006), which was originally developed for lexicalized tree adjoining grammars (LTAG) (Bangalore and Joshi, 1999). Supertagging is a process where words in an input sentence are tagged with ‘supertags,’ which are lexical entries in lexicalized grammars, e.g., elementary trees in LTAG, lexical categories in CCG, and lexical entries in HPSG. The concept of supertagging is simple and interesting, and the effects of this were recently demonstrated in the case of a CCG parser (Clark and Curran, 2004a) with the result"
W07-2208,P05-1011,1,0.268471,"g linguistic phenomena, such as passivization, control verbs and relative clauses. The main difficulty of developing parsers in these formalisms was how to model a well-defined probabilistic model for graph structures such as feature structures. This was overcome by a probabilistic model which provides probabilities of discriminating a correct parse tree among candidates of parse trees in a log-linear model or maximum entropy model (Berger et al., 1996) with many features for parse trees (Abney, 1997; Johnson et al., 1999; Riezler et al., 2000; Malouf and van Noord, 2004; Kaplan et al., 2004; Miyao and Tsujii, 2005). Following this discriminative approach, techniques for efficiency were investigated for estimation (Geman and Johnson, 2002; Miyao and Tsujii, 2002; Malouf and van Noord, 2004) and parsing (Clark and Curran, 2004b; Clark and Curran, 2004a; Ninomiya et al., 2005). An interesting approach to the problem of parsing efficiency was using supertagging (Clark and Cur60 Proceedings of the 10th Conference on Parsing Technologies, pages 60–68, c Prague, Czech Republic, June 2007. 2007 Association for Computational Linguistics ran, 2004b; Clark and Curran, 2004a; Wang, 2003; Wang and Harper, 2004; Nasr"
W07-2208,W04-3308,0,0.0211161,"005). Following this discriminative approach, techniques for efficiency were investigated for estimation (Geman and Johnson, 2002; Miyao and Tsujii, 2002; Malouf and van Noord, 2004) and parsing (Clark and Curran, 2004b; Clark and Curran, 2004a; Ninomiya et al., 2005). An interesting approach to the problem of parsing efficiency was using supertagging (Clark and Cur60 Proceedings of the 10th Conference on Parsing Technologies, pages 60–68, c Prague, Czech Republic, June 2007. 2007 Association for Computational Linguistics ran, 2004b; Clark and Curran, 2004a; Wang, 2003; Wang and Harper, 2004; Nasr and Rambow, 2004; Ninomiya et al., 2006; Foth et al., 2006; Foth and Menzel, 2006), which was originally developed for lexicalized tree adjoining grammars (LTAG) (Bangalore and Joshi, 1999). Supertagging is a process where words in an input sentence are tagged with ‘supertags,’ which are lexical entries in lexicalized grammars, e.g., elementary trees in LTAG, lexical categories in CCG, and lexical entries in HPSG. The concept of supertagging is simple and interesting, and the effects of this were recently demonstrated in the case of a CCG parser (Clark and Curran, 2004a) with the result of a drastic improveme"
W07-2208,J93-2004,0,\N,Missing
W08-0504,J96-1002,0,0.0067758,"Missing"
W08-0504,D07-1024,0,0.016794,"OBJ This study demonstrates that IL-8 recognizes and activates CXCR1, CXCR2, and the Duffy antigen by distinct mechanisms. ROOT SBJ COORD CC ROOT IL-8 recognizes and activates CXCR1 The molar ratio of serum retinol-binding protein (RBP) to transthyretin (TTR) is not useful to assess vitamin A status during infection in hospitalized children. Figure 1: A dependency tree Figure 2: Example sentences with protein names SBJ is a positive example, and &lt;RBP, TTR&gt; is a negative example. Following recent work on using dependency parsing in systems that identify protein interactions in biomedical text (Erkan et al., 2007; Sætre et al., 2007; Katrenko and Adriaans, 2006), we have built a system for PPI extraction that uses dependency relations as features. As exemplified, for the protein pair IL-8 and CXCR1 in the first sentence of Figure 2, a dependency parser outputs a dependency tree shown in Figure 1. From this dependency tree, we can extract a dependency path between IL-8 and CXCR1 (Figure 3), which appears to be a strong clue in knowing that these proteins are mentioned as interacting. The system we use in this paper is similar to the one described in Sætre et al. (2007), except that it uses syntactic de"
W08-0504,W07-2202,1,0.851396,"of the Wall Street Journal portion of the Penn Treebank (Marcus et al., 1994) during development and training. While this claim is supported by convincing evaluations that show that parsers trained on the WSJ Penn Treebank alone perform poorly on biomedical text in terms of accuracy of dependencies or bracketing of phrase structure, the benefits of using domainspecific data in terms of practical system performance have not been quantified. These expected benefits drive the development of domain-specific resources, such as the GENIA treebank (Tateisi et al., 2005), and parser domain adaption (Hara et al., 2007), which are of clear importance in parsing research, but of largely unconfirmed impact on practical systems. Quirk and Corston-Oliver (2006) examine a similar issue, the relationship between parser accuracy and overall system accuracy in syntaxinformed machine translation. Their research is similar to the work presented here, but they focused on the use of varying amounts of out-ofdomain training data for the parser, measuring how a translation system for technical text performed when its syntactic parser was trained with varying amounts of Wall Street Journal text. Our work, in contrast, inve"
W08-0504,W04-3111,0,0.0291568,"Missing"
W08-0504,I05-1006,0,0.120707,"Missing"
W08-0504,P08-1006,1,0.923319,"n section 2 we discuss our motivation and related efforts. Section 3 describes the system for identification of protein-protein interactions used in our experiments, and in section 4 describes the syntactic parser that provides the analyses for the PPI system, and the data used to train the parser. We describe our experiments, results and analysis in section 5, and conclude in section 6. 2 Motivation and related work While recent work has addressed questions relating to the use of different parsers or different types of syntactic representations in the PPI extraction task (Sætre et al., 2007, Miyao et al., 2008), little concrete evidence has been provided for potential benefits of improved parsers or additional resources for training syntactic parsers. In fact, although there is increasing interest in parser evaluation in the biomedical domain in terms of precision/recall of brackets and dependency accuracy (Clegg and Shepherd, 2007; Pyysalo et al., 2007; Sagae et al., 2008), the relationship between these evaluation metrics and the performance of practical information extraction systems remains unclear. In the parsing community, relatively small accuracy gains are often reported as success stories,"
W08-0504,W07-1004,0,0.0158531,"s in section 5, and conclude in section 6. 2 Motivation and related work While recent work has addressed questions relating to the use of different parsers or different types of syntactic representations in the PPI extraction task (Sætre et al., 2007, Miyao et al., 2008), little concrete evidence has been provided for potential benefits of improved parsers or additional resources for training syntactic parsers. In fact, although there is increasing interest in parser evaluation in the biomedical domain in terms of precision/recall of brackets and dependency accuracy (Clegg and Shepherd, 2007; Pyysalo et al., 2007; Sagae et al., 2008), the relationship between these evaluation metrics and the performance of practical information extraction systems remains unclear. In the parsing community, relatively small accuracy gains are often reported as success stories, but again, the 15 precise impact of such improvements on practical tasks in bioinformatics has not been established. One aspect of this issue is the question of domain portability and domain adaptation for parsers and other NLP modules. Clegg and Shepherd (2007) mention that available statistical parsers appear to overfit to the newswire domain, b"
W08-0504,W06-1608,0,0.0307019,"m is supported by convincing evaluations that show that parsers trained on the WSJ Penn Treebank alone perform poorly on biomedical text in terms of accuracy of dependencies or bracketing of phrase structure, the benefits of using domainspecific data in terms of practical system performance have not been quantified. These expected benefits drive the development of domain-specific resources, such as the GENIA treebank (Tateisi et al., 2005), and parser domain adaption (Hara et al., 2007), which are of clear importance in parsing research, but of largely unconfirmed impact on practical systems. Quirk and Corston-Oliver (2006) examine a similar issue, the relationship between parser accuracy and overall system accuracy in syntaxinformed machine translation. Their research is similar to the work presented here, but they focused on the use of varying amounts of out-ofdomain training data for the parser, measuring how a translation system for technical text performed when its syntactic parser was trained with varying amounts of Wall Street Journal text. Our work, in contrast, investigates the use of domain-specific training material in parsers for biomedical text, a domain where significant amounts of effort are alloc"
W08-0504,P06-2089,1,0.841027,"protein names split, tokenized, and annotated with proteins and PPIs. 4 A data-driven dependency parser for biomedical text The parser we used as component of our PPI extraction system was a shift-reduce dependency parser that uses maximum entropy models to determine the parser’s actions. Our overall parsing approach uses a best-first probabilistic shift-reduce algorithm, working left-to right to find labeled dependencies one at a time. The algorithm is essentially a dependency version of the constituent parsing algorithm for probabilistic parsing with LR-like data-driven models described by Sagae and Lavie (2006). This dependency parser has been shown to have state-of-the-art accuracy in the CoNLL shared tasks on dependency parsing (Buchholz and Marsi, 2006; Nivre, 2007). Sagae and Tsujii (2007) present a detailed description of the parsing approach used in our work, including the parsing algorithm and the features used to classify parser actions. In summary, the parser uses an algorithm similar to the LR parsing algorithm (Knuth, 1965), keeping a stack of partially built syntactic structures, and a queue of remaining input tokens. At each step in the parsing process, the parser can apply a shift acti"
W08-0504,I05-2038,1,0.931944,"the newswire domain, because of their extensive use of the Wall Street Journal portion of the Penn Treebank (Marcus et al., 1994) during development and training. While this claim is supported by convincing evaluations that show that parsers trained on the WSJ Penn Treebank alone perform poorly on biomedical text in terms of accuracy of dependencies or bracketing of phrase structure, the benefits of using domainspecific data in terms of practical system performance have not been quantified. These expected benefits drive the development of domain-specific resources, such as the GENIA treebank (Tateisi et al., 2005), and parser domain adaption (Hara et al., 2007), which are of clear importance in parsing research, but of largely unconfirmed impact on practical systems. Quirk and Corston-Oliver (2006) examine a similar issue, the relationship between parser accuracy and overall system accuracy in syntaxinformed machine translation. Their research is similar to the work presented here, but they focused on the use of varying amounts of out-ofdomain training data for the parser, measuring how a translation system for technical text performed when its syntactic parser was trained with varying amounts of Wall"
W08-0504,D07-1096,0,\N,Missing
W08-0605,P96-1042,0,0.0596406,"he scope of text mining applications. In the biomedical domain, for example, several annotated corpora such as GENIA (Kim et al., 2003), PennBioIE (Kulick et al., 2004), and GENETAG (Tanabe et al., 2005) have been created and made publicly available, but the named entity categories annotated in these corpora are tailored to their specific needs and not always sufficient or suitable for text mining tasks that other researchers need to address. Active learning is a framework which can be used for reducing the amount of human effort required to create a training corpus (Dagan and Engelson, 1995; Engelson and Dagan, 1996; Thompson et al., 1999; Shen et al., 2004). In active learning, samples that need to be annotated by the human annotator are picked up by a machine learning model in an iterative and interactive manner, considering the informativeness of the samples. Active learning has been shown to be effective in several natural language processing tasks including named entity recognition. Named entities play a central role in conveying important domain specific information in text, and good named entity recognizers are often required in building practical information extraction systems. Previous studies h"
W08-0605,W04-1213,0,0.0305898,"Missing"
W08-0605,W04-3111,0,0.0196017,"naniadou1,3 1 School of Computer Science, The University of Manchester, UK 2 Department of Computer Science, The University of Tokyo, Japan 3 National Centre for Text Mining (NaCTeM), Manchester, UK yoshimasa.tsuruoka@manchester.ac.uk tsujii@is.s.u-tokyo.ac.jp sophia.ananiadou@manchester.ac.uk Abstract 1 Introduction However, the lack of annotated corpora, which are indispensable for training machine learning models, makes it difficult to broaden the scope of text mining applications. In the biomedical domain, for example, several annotated corpora such as GENIA (Kim et al., 2003), PennBioIE (Kulick et al., 2004), and GENETAG (Tanabe et al., 2005) have been created and made publicly available, but the named entity categories annotated in these corpora are tailored to their specific needs and not always sufficient or suitable for text mining tasks that other researchers need to address. Active learning is a framework which can be used for reducing the amount of human effort required to create a training corpus (Dagan and Engelson, 1995; Engelson and Dagan, 1996; Thompson et al., 1999; Shen et al., 2004). In active learning, samples that need to be annotated by the human annotator are picked up by a mac"
W08-0605,P06-1059,1,0.831588,"been shown to be effective in several natural language processing tasks including named entity recognition. Named entities play a central role in conveying important domain specific information in text, and good named entity recognizers are often required in building practical information extraction systems. Previous studies have shown that automatic named entity recognition can be performed with a reasonable level of accuracy by using various machine learning models such as support vector machines (SVMs) or conditional random fields (CRFs) (Tjong Kim Sang and De Meulder, 2003; Settles, 2004; Okanohara et al., 2006). The problem with active learning is, however, that the resulting annotated data is highly dependent on the machine learning algorithm and the sampling strategy employed, because active learning annotates only a subset of the given corpus. This sampling bias is not a serious problem if one is to use the annotated corpus only for their own machine learning purpose and with the same machine learning algorithm. However, the existence of bias is not desirable if one also wants the corpus to be used by other applications or researchers. For the same reason, acThis paper presents an active learning"
W08-0605,W04-1221,0,0.0116076,"e learning has been shown to be effective in several natural language processing tasks including named entity recognition. Named entities play a central role in conveying important domain specific information in text, and good named entity recognizers are often required in building practical information extraction systems. Previous studies have shown that automatic named entity recognition can be performed with a reasonable level of accuracy by using various machine learning models such as support vector machines (SVMs) or conditional random fields (CRFs) (Tjong Kim Sang and De Meulder, 2003; Settles, 2004; Okanohara et al., 2006). The problem with active learning is, however, that the resulting annotated data is highly dependent on the machine learning algorithm and the sampling strategy employed, because active learning annotates only a subset of the given corpus. This sampling bias is not a serious problem if one is to use the annotated corpus only for their own machine learning purpose and with the same machine learning algorithm. However, the existence of bias is not desirable if one also wants the corpus to be used by other applications or researchers. For the same reason, acThis paper pr"
W08-0605,P04-1075,0,0.0346889,"edical domain, for example, several annotated corpora such as GENIA (Kim et al., 2003), PennBioIE (Kulick et al., 2004), and GENETAG (Tanabe et al., 2005) have been created and made publicly available, but the named entity categories annotated in these corpora are tailored to their specific needs and not always sufficient or suitable for text mining tasks that other researchers need to address. Active learning is a framework which can be used for reducing the amount of human effort required to create a training corpus (Dagan and Engelson, 1995; Engelson and Dagan, 1996; Thompson et al., 1999; Shen et al., 2004). In active learning, samples that need to be annotated by the human annotator are picked up by a machine learning model in an iterative and interactive manner, considering the informativeness of the samples. Active learning has been shown to be effective in several natural language processing tasks including named entity recognition. Named entities play a central role in conveying important domain specific information in text, and good named entity recognizers are often required in building practical information extraction systems. Previous studies have shown that automatic named entity recog"
W08-0605,W03-0419,0,0.0384343,"Missing"
W08-0605,D07-1051,0,0.0159407,"l dictionaries or using more sophisticated probabilistic models such as semi-Markov CRFs (Sarawagi and Cohen, 2004). These enhancements should further improve the coverage, keeping 36 the same degree of cost reduction. The idea of improving the efficiency of annotation work by using automatic taggers is certainly not new. Tanabe et al. (2005) applied a gene/protein name tagger to the target sentences and modified the results manually. Culotta and McCallum (2005) proposed to have the human annotator select the correct annotation from multiple choices produced by a CRF tagger for each sentence. Tomanek et al. (2007) discuss the reusability of named entityannotated corpora created by an active learning approach and show that it is possible to build a corpus that is useful to different machine learning algorithms to a certain degree. The limitation of our framework is that it is useful only when the target named entities are sparse because the upper bound of cost saving is limited by the proportion of the relevant sentences in the corpus. Our framework may therefore not be suitable for a situation where one wants to make annotations for named entities of many categories simultaneously (e.g. creating a corp"
W08-0605,W06-3328,0,0.0242091,"Missing"
W08-1305,P06-4020,0,0.125901,"n tree to an HPSG (Pollard and Sag, 1994) phrase structure tree with AVM) 2. Converting analyses given in different framework-specific formats to some simpler format proposed as a framework-independent evaluation schema (e.g. converting from c 2008. ° Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. 29 Coling 2008: Proceedings of the workshop on Cross-Framework and Cross-Domain Parser Evaluation, pages 29–35 Manchester, August 2008 HPSG phrase structure tree with AVM to GR (Briscoe et al., 2006)) of a binary relation between a token assigned as the head of the relation and other tokens assigned as its dependents. Notice however that grammar frameworks considerably disagree in the way they assign heads and non-heads. This would raise the doubt that, no matter how much information is removed, there could still remain disagreements between grammar formalisms in what is left. The simplicity of GR, or other dependencybased metrics, may give the impression that conversion from a more complex representation into it is easier than conversion between two complex representations. In other word"
W08-1305,de-marneffe-etal-2006-generating,0,0.0635719,"Missing"
W08-1305,C96-2120,0,0.01087,"and complex sentence, is a probable reason for the annotation errors in the gold standard described in (Briscoe et al., 2006). To avoid the same problem in our creation of a gold standard, we propose to allow non-exhaustive annotation. In fact, our proposal is to limit the number of phenomena assigned to a sentence to one. This decision on which phenomenon to be assigned is made, when the test suite is constructed, for each of the sentences contained in it. Following the traditional approach, we include every sentence in the test suite, along with the core phenomenon we intend to test it on (Lehmann and Oepen, 1996). Thus, Sentence 1 would be assigned the phenomenon of unshifted ditransitive. Sentence 2 would be assigned the phenomenon of 1. assigns a monotransitive verb analysis to ‘give’ and an adjunct analysis to ‘to Mary’ in 1 2. assigns a ditransitive verb analysis to ‘give’ in 2 The list of pairs we obtain from running the recogniser on the results produced by batch parsing the test suite with the parser to be evaluated is the following: h1,hproper noun,monotransitive,preposition,adjunctii, h2, hproper noun,dative-shifted ditransitivei i 3.3 Performance Measure Calculation Comparing the two list of"
W09-1301,doddington-etal-2004-automatic,0,0.0136738,"e.g. database curation efforts, most domain RE efforts target relations involving biologically relevant changes in the involved entities, commonly to the complete exclusion of static relations. However, static relations such as entity membership in a family and one entity being a part of another are not only 1 relevant IE targets in themselves but can also play an important supporting role in IE systems not primarily targeting them. In this paper, we investigate the role of static relations in causal RE and event extraction. Here, we use relation extraction in the MUC and ACE (Sundheim, 1995; Doddington et al., 2004) sense to refer to the task of extracting binary relations, ordered pairs of entities, where both participating entities must be specified and their roles (agent, patient, etc.) are fixed by the relation. By contrast, event extraction is understood to involve events (things that happen) and representations where the number and roles of participants may vary more freely. We refer to relations where one one entity causes another to change as causal relations; typical domain examples are phosphorylation and activation. Static relations, by contrast, hold between two entities without implication o"
W09-1301,S07-1003,0,0.00870172,"Missing"
W09-1301,W09-1401,1,0.666271,"well studied and several biomedProceedings of the Workshop on BioNLP, pages 1–9, c Boulder, Colorado, June 2009. 2009 Association for Computational Linguistics ical NER systems are available (see e.g. (Wilbur et al., 2007; Leaman and Gonzalez, 2008)), and most domain IE approaches are NE-driven: a typical way to cast the RE task is as deciding for each pair of co-occurring NEs whether a relevant relation is stated for them in context. Like the previous LLL and BioCreative2-PPI relation extraction tasks (N´edellec, 2005; Krallinger et al., 2007), the BioNLP’09 shared task on event extraction (Kim et al., 2009) similarly proceeds from NEs, requiring participants to detect events and determine the roles given NEs play in them. Any domain IE approach targeting nontrivial causal NE relations or events necessarily involves decisions relating to static relations. Consider, for example, the decision whether to extract a relation between NE1 and NE2 in the following cases (affects should here be understood as a placeholder for any relevant statement of causal relation): 1) NE1 affects NE2 gene 2) NE1 affects NE2 promoter 3) NE1 affects NE2 mutant 4) NE1 affects NE2 antibody 5) NE1 affects NE2 activator The"
W09-1301,de-marneffe-etal-2006-generating,0,0.054107,"Missing"
W09-1301,W01-0511,0,0.0903008,"nt relations To avoid unnecessary division of relations that imply in our context similar interpretation and processing, we define a task-specific Variant relation that encompasses a set of possible relation types holding between an NE and its variants along multiple different axes. One significant class of cases annotated as Variant includes expressions such as NE gene and NE protein, under the interpretation that NE refers to the abstract information that is “realized” as either DNA, RNA or protein form, and the entity to one of these realizations (for alternative interpretations, see e.g. (Rosario and Hearst, 2001; Heimonen et al., 2008)). The Variant relation is also used to annotate NEentity relations where the entity expresses a different state of the NE, such as a phosphorylated or mutated state. While each possible post-translational modification, for example, could alternatively be assigned a specific relation type, in the present IE context these would only increase the difficulty of the task without increasing the applicability of the resulting annotation. the corpus contained frequent cases where the stated relationship of the NE to the entity involved different types of relevant relations (e."
W09-1301,M95-1002,0,0.226052,"biologists and e.g. database curation efforts, most domain RE efforts target relations involving biologically relevant changes in the involved entities, commonly to the complete exclusion of static relations. However, static relations such as entity membership in a family and one entity being a part of another are not only 1 relevant IE targets in themselves but can also play an important supporting role in IE systems not primarily targeting them. In this paper, we investigate the role of static relations in causal RE and event extraction. Here, we use relation extraction in the MUC and ACE (Sundheim, 1995; Doddington et al., 2004) sense to refer to the task of extracting binary relations, ordered pairs of entities, where both participating entities must be specified and their roles (agent, patient, etc.) are fixed by the relation. By contrast, event extraction is understood to involve events (things that happen) and representations where the number and roles of participants may vary more freely. We refer to relations where one one entity causes another to change as causal relations; typical domain examples are phosphorylation and activation. Static relations, by contrast, hold between two enti"
W09-1301,I05-2038,1,0.170621,"ation criteria, NE and entity types, granularity of relations, etc.), we find the outcome — which was neither planned for nor forced on the data — a very encouraging sign of the sufficiency of the task setting for this and related domain IE tasks. 3.4 We created the data set by building on the annotation of the GENIA Event corpus (Kim et al., 2008), making use of the rich set of annotations already contained in the corpus: term annotation for NEs and other entities (Ohta et al., 2002), annotation of events between these terms, and treebank structure closely following the Penn Treebank scheme (Tateisi et al., 2005). Other/Out annotation We apply a catch-all category, Other/Out, for annotating candidate (NE, entity) pairs between which there is no relevant static relation. This label is thus applied to a number of quite different cases: causal relations, both implied (e.g. NE receptors, NE response element) and explicitly stated (NE binds the [site]), relations where the entity is considered too far removed from the NE to support reliable inference of a role for the NE in causal relations/events involving the entity (e.g. [antibodies] for NE), and cases where no relation is stated (e.g. NE and other [pro"
W09-1321,P98-1013,0,0.0124696,"on (Linguis4cally‐oriented seman4cs) Class: Mo)on Theme: NF‐kappa B Source: from cytosol Goal: to nucleus Class: Releasing Theme: IL‐6 Agent: PBMC GENIA expression (Biologically‐oriented seman4cs) Class: Localiza)on Theme: NF‐kappa B FromLoc: cytosol ToLoc: nucleus Theme: IL‐6 FromLoc: (inside of) PMBC ToLoc: (outside of) PMBC Figure 1: A comparison of the linguistically-oriented and biologicallyoriented structure of semantics event corpus. Expressions mentioning the four classes were examined and manually classiﬁed into linguistically-oriented frames, represented by those deﬁned in FrameNet (Baker et al., 1998). FN frames associated to a bio-molecular event class constitute a list of possible perspectives in mentioning phenomena of the class. The rest of this paper is structured in the following way: Section 2 reviews the existing work on semantic structures and expression varieties in the bio-medical domain, and provides a comparison to our work. In section 3, we describe the GENIA event corpus, and the FrameNet frames used as linguistically-oriented classes in our investigation. Sections 4 and 5 explain the methods and results of the corpus investigation; in particular the sections investigate how"
W09-1321,S07-1018,0,0.0588238,"Missing"
W09-1321,J02-3001,0,0.00863432,"ioInfer (Pyysalo et al., 2007) and the GENIA event corpus (Kim et al., 2008) provide annotations of such semantic structures on col162 lections of bio-medical articles. Domain-oriented semantic structures are valuable assets because their representation suits information needs in the domain; however, the extraction of such structures is difﬁcult due to the large gap between the text and these structures. On the other hand, the extraction of linguisticallyoriented semantics from text has long been studied in computational linguistics, and has recently been formalized as Semantic Role Labeling (Gildea and Jurafsky, 2002), and semantic structure extraction (Baker et al., 2007)(Surdeanu et al., 2008). Semantic structures in such tasks are exempliﬁed in the middle of ﬁgure 1. The linguistically-oriented semantic structures are easier to extract, although the information is not practical to the domain. We aim at relating linguistically-oriented frames of semantics with domain-oriented classes, thus making a step forward in utilizing the computational linguistic resources for the bio-medical TM. Of all the differences in the two type of semantics, we focused on the fact that the former frames are more sensitive to"
W09-1321,W04-3111,0,0.0815074,"Missing"
W09-1321,W04-2705,0,0.0871914,"and conclusion in section 6 and 7. 2 Related Work Existing work on semantics approached domainoriented semantic structures from linguisticallyoriented semantics. In contrast, our approach uses domain-oriented semantics to ﬁnd the linguistic semantics that represent them. We believe that the two different approaches could complement each other. The PASbio(Wattarujeekrit et al., 2004) proposes Predicate Argument Structures (PASs), a type of linguistically-oriented semantic structures, for domain-speciﬁc lexical items, based on PASs de163 ﬁned in PropBank(Wattarujeekrit et al., 2004) and NomBank(Meyers et al., 2004). The PASs are deﬁned per lexical item, and is therefore distinct from a biologically-oriented representation of events. (Cohen et al., 2008) investigated syntactic alternations of verbs and their nominalized forms which occurred in the PennBioIE corpus(Kulick et al., 2004), whilst keeping PASs of the PASBio in their minds. The BioFrameNet(Dolbey et al., 2006) is an attempt to extend the FrameNet with speciﬁc frames to the bio-medical domain, and to apply the frames to corpus annotation. Our attempts were similar, in that both were: 1) utilizing the FN frames or their extensions to classify me"
W09-1321,W08-2121,0,0.0424515,"Missing"
W09-1321,C98-1013,0,\N,Missing
W09-1401,P05-1022,0,0.128444,"of event extraction, we prepared publicly available BioNLP resources readily available for the shared task. Several fundamental BioNLP tools were provided through U-Compare (Kano et al., 2009)2 , which included tools for tokenization, sentence segmentation, part-of-speech tagging, chunking and syntactic parsing. Participants were also provided with the syntactic analyses created by a selection of parsers. We applied two mainstream Penn Treebank (PTB) phrase structure parsers: the Bikel parser3 , implementing Collins’ parsing model (Bikel, 2004) and trained on PTB, and the reranking parser of (Charniak and Johnson, 2005) with the self-trained biomedical parsing model of (McClosky and Charniak, 2008)4 . We also applied the GDep5 , native dependency parser trained on the GENIA Treebank 2 http://u-compare.org/ http://www.cis.upenn.edu/∼dbikel/software.html 4 http://www.cs.brown.edu/∼dmcc/biomedical.html 5 http://www.cs.cmu.edu/∼sagae/parser/gdep/ 3 Team UTurku JULIELab Task 1-1-- Org 3C+2BI 1C+2L+2B ConcordU 1-3 3C Word Porter OpenNLP Porter Stanford UT+DBCLS 12- 2C Porter VIBGhent UTokyo 1-3 1-- 2C+1B 3C Porter, GTag UNSW UZurich 1-1-- 1C+1B 3C ASU+HU+BU 123 6C+2BI LingPipe, Morpha Porter Cam UAntwerp 1-12- 3C"
W09-1401,M98-1001,0,0.713763,"sub-tasks, each of which addresses bio-molecular event extraction at a different level of specificity. The data was developed based on the GENIA event corpus. The shared task was run over 12 weeks, drawing initial interest from 42 teams. Of these teams, 24 submitted final results. The evaluation results are encouraging, indicating that state-of-the-art performance is approaching a practically applicable level and revealing some remaining challenges. 1 Introduction The history of text mining (TM) shows that shared tasks based on carefully curated resources, such as those organized in the MUC (Chinchor, 1998), TREC (Voorhees, 2007) and ACE (Strassel et al., 2008) events, have significantly contributed to the progress of their respective fields. This has also been the case in bio-TM. Examples include the TREC Genomics track (Hersh et al., 2007), JNLPBA (Kim et al., 2004), LLL (N´edellec, 2005), and BioCreative (Hirschman et al., 2007). While the first two addressed bio-IR (information retrieval) and bio-NER (named entity recognition), respectively, the last two focused on bio-IE (information extraction), seeking relations between bio-molecules. With the emergence of NER systems with performance cap"
W09-1401,de-marneffe-etal-2006-generating,0,0.604282,"Missing"
W09-1401,W04-1213,1,0.414702,"ted final results. The evaluation results are encouraging, indicating that state-of-the-art performance is approaching a practically applicable level and revealing some remaining challenges. 1 Introduction The history of text mining (TM) shows that shared tasks based on carefully curated resources, such as those organized in the MUC (Chinchor, 1998), TREC (Voorhees, 2007) and ACE (Strassel et al., 2008) events, have significantly contributed to the progress of their respective fields. This has also been the case in bio-TM. Examples include the TREC Genomics track (Hersh et al., 2007), JNLPBA (Kim et al., 2004), LLL (N´edellec, 2005), and BioCreative (Hirschman et al., 2007). While the first two addressed bio-IR (information retrieval) and bio-NER (named entity recognition), respectively, the last two focused on bio-IE (information extraction), seeking relations between bio-molecules. With the emergence of NER systems with performance capable of supporting practical applications, the recent interest of the bio-TM community is shifting toward IE. Similarly to LLL and BioCreative, the BioNLP’09 Shared Task (the BioNLP task, hereafter) also addresses bio-IE, but takes a definitive step further toward f"
W09-1401,P08-2026,0,0.440867,"ailable for the shared task. Several fundamental BioNLP tools were provided through U-Compare (Kano et al., 2009)2 , which included tools for tokenization, sentence segmentation, part-of-speech tagging, chunking and syntactic parsing. Participants were also provided with the syntactic analyses created by a selection of parsers. We applied two mainstream Penn Treebank (PTB) phrase structure parsers: the Bikel parser3 , implementing Collins’ parsing model (Bikel, 2004) and trained on PTB, and the reranking parser of (Charniak and Johnson, 2005) with the self-trained biomedical parsing model of (McClosky and Charniak, 2008)4 . We also applied the GDep5 , native dependency parser trained on the GENIA Treebank 2 http://u-compare.org/ http://www.cis.upenn.edu/∼dbikel/software.html 4 http://www.cs.brown.edu/∼dmcc/biomedical.html 5 http://www.cs.cmu.edu/∼sagae/parser/gdep/ 3 Team UTurku JULIELab Task 1-1-- Org 3C+2BI 1C+2L+2B ConcordU 1-3 3C Word Porter OpenNLP Porter Stanford UT+DBCLS 12- 2C Porter VIBGhent UTokyo 1-3 1-- 2C+1B 3C Porter, GTag UNSW UZurich 1-1-- 1C+1B 3C ASU+HU+BU 123 6C+2BI LingPipe, Morpha Porter Cam UAntwerp 1-12- 3C 3C Porter GTag UNIMAN 1-- 4C+2BI Porter GTag SCAI UAveiro USzeged 1-1-1-3 1C 1C+"
W09-1401,W09-1313,1,0.461582,"differences in annotation principles compared to other biomedical NE corpora. For instance, the NE annotation in the widely applied GENETAG corpus (Tanabe et al., 2005) does not differentiate proteins from genes, while GENIA annotation does. Such differences have caused significant inconsistency in methods and resources following different annotation schemes. To remove or reduce the inconsistency, GENETAG-style NE annotation, which we term gene-or-gene-product (GGP) annotation, has been added to the GENIA corpus, with appropriate revision of the original annotation. For details, we refer to (Ohta et al., 2009). The NE annotation used in the BioNLP task data is based on this annotation. 3.2 Argument revision The GENIA event annotation was made based on the GENIA event ontology, which uses a loose typing system for the arguments of each event class. For example, in Figure 2(a), it is expressed that the binding event involves two proteins, TRAF2 and CD40, and that, in the case of CD40, its cytoplasmic domain takes part in the binding. Without constraints on the type of theme arguments, the following two annotations are both legitimate: (Type:Binding, Theme:TRAF2, Theme:CD40) (Type:Binding, Theme:TRAF2"
W09-1401,W09-1301,1,0.380624,"om (a) PMID7541987 (simplified), (b) PMID10224278, (c) PMID10090931, (d) PMID9243743, (e) PMID7635985. (Type:Binding, Theme1:TRAF2, Theme2:CD40, Site2:cytoplasmic domain) Note that the protein, CD40, and its domain, cytoplasmic domain, are associated by argument numbering. To resolve issues related to the mapping between proteins and related entities systematically, we introduced partial static relation annotation for relations such as Part-Whole, drawing in part on similar annotation of the BioInfer corpus (Pyysalo et al., 2007). For details of this part of the revision process, we refer to (Pyysalo et al., 2009). Figure 2 shows some challenging cases. In (b), the site GATA motifs is not identified as an argument of the binding event, because the protein containing it is not stated. In (c), among the two sites (PEBP2 site and promoter) of the gene GM-CSF, only the more specific one, PEBP2, is annotated. The equivalent entity annotation in the revised GENIA corpus covers also cases other than simple apposition, illustrated in Figure 3. A frequent case in biomedical literature involves use of the slash symbol (“/”) to state synonyms. The slash symbol is ambiguous as it is used also to indicate dimerized"
W09-1401,strassel-etal-2008-linguistic,0,0.0454823,"r event extraction at a different level of specificity. The data was developed based on the GENIA event corpus. The shared task was run over 12 weeks, drawing initial interest from 42 teams. Of these teams, 24 submitted final results. The evaluation results are encouraging, indicating that state-of-the-art performance is approaching a practically applicable level and revealing some remaining challenges. 1 Introduction The history of text mining (TM) shows that shared tasks based on carefully curated resources, such as those organized in the MUC (Chinchor, 1998), TREC (Voorhees, 2007) and ACE (Strassel et al., 2008) events, have significantly contributed to the progress of their respective fields. This has also been the case in bio-TM. Examples include the TREC Genomics track (Hersh et al., 2007), JNLPBA (Kim et al., 2004), LLL (N´edellec, 2005), and BioCreative (Hirschman et al., 2007). While the first two addressed bio-IR (information retrieval) and bio-NER (named entity recognition), respectively, the last two focused on bio-IE (information extraction), seeking relations between bio-molecules. With the emergence of NER systems with performance capable of supporting practical applications, the recent i"
W09-1401,I05-2038,1,0.385704,"bioinformaticians (BI), biologists (B) and liguists (L). This may be attributed in part to the fact that the event extraction task required complex computational modeling. The role of computer scientists may be emphasized in part due to the fact that the task was novel to most participants, requiring particular efforts in framework design and implementation and computational resources. This also suggests there is room for improvement from more input from biologists. In total, 42 teams showed interest in the shared task and registered for participation, and 24 teams sub7.2 Evaluation results (Tateisi et al., 2005), and a version of the C&C CCG deep parser6 adapted to biomedical text (Rimell and Clark, 2008). The text of all documents was segmented and tokenized using the GENIA Sentence Splitter and the GENIA Tagger, provided by U-Compare. The same segmentation was enforced for all parsers, which were run using default settings. Both the native output of each parser and a representation in the popular Stanford Dependency (SD) format (de Marneffe et al., 2006) were provided. The SD representation was created using the Stanford tools7 to convert from the PTB scheme, the custom conversion introduced by (Ri"
W09-1401,J04-4004,0,\N,Missing
W09-1406,W08-2121,0,0.0653558,"Missing"
W09-1406,W09-1401,1,0.118204,"Missing"
W09-1406,W08-2125,1,0.57079,"formulations of Select all roles for mantic Role Labelling (Surdeanu et al., 2008). assignment of argument tokens to these roles. Hence it may be possible to re-use or adapt the For a non-binding event clue c, c c in we rst coland then create one event per If we would re-convert C and L from equation successful approaches in SRL in order to improve 2 and 3, respectively, we could return to our origbio-molecular event extraction. inal event structure in gure 1. Since our apHowever, conproach is inspired by the Markov Logic role laverting back and forth is not loss-free in general. beller in (Riedel and Meza-Ruiz, 2008), this work For example, if we have a non-binding event in can be seen as an attempt in this direction. the original For a sentence with given P, L rithm 1 presents our mapping from E , algoE to (L, C). and For brevity we omit a more detailed description E set with two arguments A and B with the same role Theme, the round-trip conversion would generate two events: one with A as Theme and one with B as Theme. of the algorithm. Note that for our running example eventsToLinks would return 4 C = {(1, neg_reg) , (2, pos_reg) , (5, gene_expr)} Markov Logic (Richardson and Domingos, 2006) (2) 43 Mark"
W09-1406,J07-4004,0,\N,Missing
W09-1406,N09-1018,1,\N,Missing
W09-1406,P05-1022,0,\N,Missing
W09-1406,P08-2026,0,\N,Missing
W09-1414,W07-1033,1,\N,Missing
W09-1504,W05-0304,0,0.0208375,"ork, a component which generates CASes is called a Collection Reader. We have developed several collection readers which read annotated corpora and generates annotations using the U-Compare type system. Because our primary target domain was biomedical field, there are corpus readers for the biomedical corpora; Aimed corpus (Bunescu et al., 2006) reader and BioNLP ’09 shared task format reader generate event annotations like protein-protein interaction annotations; Readers for BIO/IOB format, Bio1 corpus (Tateisi et al., 2000), BioCreative (Hirschman et al., 2004) task 1a format, BioIE corpus (Bies et al., 2005), NLPBA shared task dataset (Kim et al., 2004), Texas Corpus (Bunescu et al., 2005), Yapex Corpus (Kristofer Franzen et al., 2002), generate biomedical named entities, and Genia Treebank corpus (Tateisi et al., 2005) reader generates Penn Treebank (Marcus et al., 1993) style bracketing and part-of-speech annotations. Format readers require users to prepare annotated data, while others include corpora themselves, automatically downloaded as an archive on users’ demand. In addition, there is File System Collection Reader from Apache UIMA which reads files as plain text. We have developed an onli"
W09-1504,de-marneffe-etal-2006-generating,0,0.00947855,"Missing"
W09-1504,W04-1213,0,0.0318457,"d a Collection Reader. We have developed several collection readers which read annotated corpora and generates annotations using the U-Compare type system. Because our primary target domain was biomedical field, there are corpus readers for the biomedical corpora; Aimed corpus (Bunescu et al., 2006) reader and BioNLP ’09 shared task format reader generate event annotations like protein-protein interaction annotations; Readers for BIO/IOB format, Bio1 corpus (Tateisi et al., 2000), BioCreative (Hirschman et al., 2004) task 1a format, BioIE corpus (Bies et al., 2005), NLPBA shared task dataset (Kim et al., 2004), Texas Corpus (Bunescu et al., 2005), Yapex Corpus (Kristofer Franzen et al., 2002), generate biomedical named entities, and Genia Treebank corpus (Tateisi et al., 2005) reader generates Penn Treebank (Marcus et al., 1993) style bracketing and part-of-speech annotations. Format readers require users to prepare annotated data, while others include corpora themselves, automatically downloaded as an archive on users’ demand. In addition, there is File System Collection Reader from Apache UIMA which reads files as plain text. We have developed an online interactive text reader, named Input Text R"
W09-1504,J93-2004,0,0.0317693,"e corpus readers for the biomedical corpora; Aimed corpus (Bunescu et al., 2006) reader and BioNLP ’09 shared task format reader generate event annotations like protein-protein interaction annotations; Readers for BIO/IOB format, Bio1 corpus (Tateisi et al., 2000), BioCreative (Hirschman et al., 2004) task 1a format, BioIE corpus (Bies et al., 2005), NLPBA shared task dataset (Kim et al., 2004), Texas Corpus (Bunescu et al., 2005), Yapex Corpus (Kristofer Franzen et al., 2002), generate biomedical named entities, and Genia Treebank corpus (Tateisi et al., 2005) reader generates Penn Treebank (Marcus et al., 1993) style bracketing and part-of-speech annotations. Format readers require users to prepare annotated data, while others include corpora themselves, automatically downloaded as an archive on users’ demand. In addition, there is File System Collection Reader from Apache UIMA which reads files as plain text. We have developed an online interactive text reader, named Input Text Reader. 27 Analysis Engine Components There are many tools covering from basic syntactic annotations to the biomedical annotations. Some of the tools are running as web services, but users can freely mix local services and w"
W09-1504,I08-2122,1,0.908976,"ry of type system compatible components. These all implement the UCompare type system described in section 3. 3 2 Component and Capability In the UIMA framework, Annotation is a base type which has begin and end offset values. In this paper we call any objects (any subtype of TOP) as annotations. http://www.oasis-open.org/committees/uima/ 23 2.2.1 Related Works There also exist several other public UIMA component repositories: CMU UIMA component repository, BioNLP UIMA repository (Baumgartner et al., 2008), JCoRe (Hahn et al., 2008), Tsujii Lab Component Repository at the University of Tokyo (Kano et al., 2008a), etc. Each group uses their own type system, and so components provided by each group are incompatible. Unlike U-Compare these repositories are basically only collections of UIMA components, UCompare goes further by providing a fully integrated set of UIMA tools and utilities. 2.2.2 Integrated Platform U-Compare provides a variety of features as part of an integrated platform. The system can be launched with a single click in a web browser; all required libraries are downloaded and updated automatically in background. The Workflow Manager GUI helps users to create workflows in an easy drag-"
W09-1504,I05-2038,1,0.765119,"r primary target domain was biomedical field, there are corpus readers for the biomedical corpora; Aimed corpus (Bunescu et al., 2006) reader and BioNLP ’09 shared task format reader generate event annotations like protein-protein interaction annotations; Readers for BIO/IOB format, Bio1 corpus (Tateisi et al., 2000), BioCreative (Hirschman et al., 2004) task 1a format, BioIE corpus (Bies et al., 2005), NLPBA shared task dataset (Kim et al., 2004), Texas Corpus (Bunescu et al., 2005), Yapex Corpus (Kristofer Franzen et al., 2002), generate biomedical named entities, and Genia Treebank corpus (Tateisi et al., 2005) reader generates Penn Treebank (Marcus et al., 1993) style bracketing and part-of-speech annotations. Format readers require users to prepare annotated data, while others include corpora themselves, automatically downloaded as an archive on users’ demand. In addition, there is File System Collection Reader from Apache UIMA which reads files as plain text. We have developed an online interactive text reader, named Input Text Reader. 27 Analysis Engine Components There are many tools covering from basic syntactic annotations to the biomedical annotations. Some of the tools are running as web se"
W09-1504,J08-1002,1,\N,Missing
W09-3814,W04-1906,0,0.0468702,"Missing"
W09-3814,W03-1008,0,0.0183122,"ntactic information to semantic analysis. 1 2 Related Work Besides (Frank and Semeck´y, 2004)’s work, as mentioned above, there have been several studies on the relationship between deep syntax and semantic parsing. Although the studies did not focus on direct mappings between deep syntax and shallow semantics, they suggested a strong relationship between the two. (Miyao and Tsujii, 2004) evaluated the accuracy of an HPSG parser against PropBank semantic annotations, and showed that the HPSG dependants correlated with semantic arguments of the PropBank, particularly with “core” arguments. In (Gildea and Hockenmaier, 2003) and (Zhang et al., 2008), features from deep parses were used for semantic parsing, together with features from CFG or dependency parses. The deep features were reported to contribute to a performance gain. Introduction This paper presents semantic parsing based only on HPSG parses, and examines the contribution of the syntactic information to semantic analysis. In computational linguistics, many researchers have studied the relationship between syntax and semantics. Its quantitative analysis was formalized as semantic parsing, or semantic role labeling, and has attracted the attention of res"
W09-3814,W04-0803,0,0.0314738,"ot trivial even when using deep analysis: Some semantic arguments are not direct syntactic dependants of their predicates - especially of noun predicates. In sentence e) in ﬁgure 2, the Evaluee phrase depends on the predicate praise, through the support verb receive. The deep analysis would be advantageous in capturing such dependencies, because it provides receive with direct links to the phrases of the role and the predicate. 5 Semantic Parsing Based on FrameNet We employed FrameNet (FN) as a semantic corpus. Furthermore, we evaluated our semantic parsing on the SRL task data of Senseval-3 (Litkowski, 2004), which consists of FN annotations. In FN, semantic frames are deﬁned, and each frame is associated with predicates that evoke the frame. For instance, the verb and noun praise are predicates of the Judgment communication frame, and they share the same set of semantic roles. The Senseval-3 data is a standard for evaluation of semantic parsing. The task is deﬁned as identifying phrases and their semantic roles for a given sentence, predicate, and frame. The data includes null instantiations of roles1 , which are “conceptually salient”, but do not appear in the text. Problematic when using only"
W09-3814,C04-1204,1,0.785886,"ntically annotated corpus, and semantic parsing was performed by mapping HPSG dependencies to FrameNet relations. The semantic parsing was evaluated in a Senseval-3 task; the results suggested that there is a high contribution of syntactic information to semantic analysis. 1 2 Related Work Besides (Frank and Semeck´y, 2004)’s work, as mentioned above, there have been several studies on the relationship between deep syntax and semantic parsing. Although the studies did not focus on direct mappings between deep syntax and shallow semantics, they suggested a strong relationship between the two. (Miyao and Tsujii, 2004) evaluated the accuracy of an HPSG parser against PropBank semantic annotations, and showed that the HPSG dependants correlated with semantic arguments of the PropBank, particularly with “core” arguments. In (Gildea and Hockenmaier, 2003) and (Zhang et al., 2008), features from deep parses were used for semantic parsing, together with features from CFG or dependency parses. The deep features were reported to contribute to a performance gain. Introduction This paper presents semantic parsing based only on HPSG parses, and examines the contribution of the syntactic information to semantic analys"
W09-3814,W08-2126,0,0.0303851,"lysis. 1 2 Related Work Besides (Frank and Semeck´y, 2004)’s work, as mentioned above, there have been several studies on the relationship between deep syntax and semantic parsing. Although the studies did not focus on direct mappings between deep syntax and shallow semantics, they suggested a strong relationship between the two. (Miyao and Tsujii, 2004) evaluated the accuracy of an HPSG parser against PropBank semantic annotations, and showed that the HPSG dependants correlated with semantic arguments of the PropBank, particularly with “core” arguments. In (Gildea and Hockenmaier, 2003) and (Zhang et al., 2008), features from deep parses were used for semantic parsing, together with features from CFG or dependency parses. The deep features were reported to contribute to a performance gain. Introduction This paper presents semantic parsing based only on HPSG parses, and examines the contribution of the syntactic information to semantic analysis. In computational linguistics, many researchers have studied the relationship between syntax and semantics. Its quantitative analysis was formalized as semantic parsing, or semantic role labeling, and has attracted the attention of researchers. Recently, an im"
W09-3828,D07-1112,0,0.0673841,"Missing"
W09-3828,gimenez-marquez-2008-towards,0,0.0279212,"Missing"
W09-3828,W07-2202,1,0.842975,"dination” and one error of “this” with the category “Head selection for noun phrase.” When we correct an error in the interdependent error group (a), the correction leads to not only correction of the other errors in (a) but also correction of the error in (b) via correction propagation from (a) to (b). Therefore, a correction effect of an error in group (a) results in 6.0. 6 Further applications of our methods In this section, as an example of the further application of our methods, we attempt to analyze parsing behaviors in domain adaptation from the viewpoints of error cause categories. In Hara et al. (2007), we proposed a method for adapting Enju to a target domain, and then succeeded in improving the parser performance for the GENIA corpus (Kim et al., 2003), a biomedical domain. Table 6 summarizes the parsing results for three types of settings respectively: parsing PTB with Enju (“Enju for PTB”), parsing GENIA with Enju (“Enju for GENIA”), and parsing GENIA with the adapted model (“Adapted for GENIA”). We then analyzed the performance transition among these settings from the viewpoint of the cause categories given in Section 4.1 (Table 7). In order to compare the error frequencies among diffe"
W09-3828,H94-1020,0,0.175357,"ect evaluation methods and the methods of analyzing errors. Table 3: Errors classified into cause categories (c) shows what the resultant network looks like. An inter-dependent error group of 1, 2, 3 and 4 is recognized by [Step 3] and represented as a single node. Error 5 is propagated to this node in the final network. 5 1,811 (1,009) 4,709 (3,085) 1,978 501 90.69 (90.78/90.59) Table 4: Summary of inter-dependencies 㪝㫉㪼㫈㫌㪼㫅㪺㫐 Cause categories of errors Experiments We applied our methods to the analyses of actual errors produced by Enju. This version of Enju was trained on the Penn Treebank (Marcus et al., 1994) Section 2-21. 5.1 Observation of identified cause categories We first parsed sentences in PTB Section 22, and based on the observation of errors, we defined the patterns in Section 4. We then parsed sentences in Section 0. The errors in Section 0 were mapped to error cause categories by the pattern rules created for Section 22. Table 3 summarizes the distribution across the causes of errors. The left and right numbers in the table show the number of erroneous triplets classified into the categories and the frequency of the patterns matched, respectively. The table shows that, with the 14 patt"
W09-3828,D07-1013,0,0.124779,"Missing"
W09-3828,P05-1011,1,0.81661,"hallow parsing is followed by another component for semantic label assignment. In order to address these two issues, we propose two methods in this paper. One is to recognize cause categories of errors and the other is to capture inter-dependencies among errors. The former method defines various patterns of errors to identify categories of error causes. The latter method re-parses a sentence with a single target error corrected, and regards the errors which are corrected in re-parse as errors dependent on the target. Although these two methods are implemented for a specific parser using HPSG (Miyao and Tsujii, 2005; Ninomiya et al., 2006), the same ideas can be applied to any type of parsing models. In this paper, we propose two methods for analyzing errors in parsing. One is to classify errors into categories which grammar developers can easily associate with defects in grammar or a parsing model and thus its improvement. The other is to discover inter-dependencies among errors, and thus grammar developers can focus on errors which are crucial for improving the performance of a parsing model. The first method uses patterns of errors to associate them with categories of causes for those errors, such as"
W09-3828,W06-1619,1,0.846152,"Missing"
W09-3832,W02-2203,0,0.628695,"Missing"
W09-3832,C04-1041,0,0.207871,"Missing"
W09-3832,W02-1001,0,0.099489,",308 supertags. Because of this, it is often very hard or even impossible to apply computationary demanding methods to HPSG supertagging. 3 POS Word-POS Supertag† Substructure Table 1: Feature templates for point-wise model and sequential model. Templates with † are only used by sequential model. ssi,j represents j-th substructure of supertag at i. For briefness, si is omitted for each template. “×” means set-product. e.g., {a,b}×{A,B}={a&A,a&B,b&A,b&B} Perceptron and Bayes Point Machine Perceptron is an efficient online discriminative training method. We used perceptron with weightaveraging (Collins, 2002) as the basis of our supertagging model. We also use perceptron-based Bayes point machine (BPM) (Herbrich et al., 2001) in some of the experiments. In short, a BPM is an average of a number of averaged perceptrons’ weights. We use average of 10 averaged perceptrons, each of which is trained on a different random permutation of the training data. 3.1 3.2 Here we follow the definition of Collins’ perceptron to learn a mapping from the input space (w, p) ∈ W × P to the supertag space s ∈ S. We use function GEN(w,p) to indicate all candidates given input (w, p). Feature function f maps a training"
W09-3832,P07-1037,0,0.405152,"Missing"
W09-3832,2000.iwpt-1.15,0,0.471397,"Missing"
W09-3832,W06-1619,1,0.909984,"Missing"
W09-3832,J99-2004,0,\N,Missing
W09-3832,W03-1006,0,\N,Missing
W09-3832,P03-1064,0,\N,Missing
W10-1816,A00-2031,0,0.0367762,"sentences. The annotation of more sentences in the science domain is ongoing. The current annotation of the NICT Chinese Treebank is informative for some language analysis tasks, such as syntactic parsing and word segmentation. However, the deep information, which includes both the grammatical functional tags and the traces, are omitted in the annotation. Without grammatical functions, the simple bracketing structure is not informative enough to represent the semantics for Chinese. Furthermore, the traces are critical elements in detecting long-distance dependencies. Gabbard et al. (2006) and Blaheta and Charniak (2000) applied machine learning models to automatically assign the empty categories and functional tags to an English treebank. However, considering about the different domains that the Penn Chinese Treebank and the NICT Chinese Treebank belong to, the machine learning model trained on the Penn Chinese Treebank may not work successfully on the NICT Chinese Treebank. In order to guarantee the high annotation quality, in our work, we manually re-annotate both the grammatical functional tags and the traces to the NICT Chinese Treebank. With the deep re-annotation, the NICT Chinese Treebank could be use"
W10-1816,N06-1024,0,0.017187,"00 Chinese sentences. The annotation of more sentences in the science domain is ongoing. The current annotation of the NICT Chinese Treebank is informative for some language analysis tasks, such as syntactic parsing and word segmentation. However, the deep information, which includes both the grammatical functional tags and the traces, are omitted in the annotation. Without grammatical functions, the simple bracketing structure is not informative enough to represent the semantics for Chinese. Furthermore, the traces are critical elements in detecting long-distance dependencies. Gabbard et al. (2006) and Blaheta and Charniak (2000) applied machine learning models to automatically assign the empty categories and functional tags to an English treebank. However, considering about the different domains that the Penn Chinese Treebank and the NICT Chinese Treebank belong to, the machine learning model trained on the Penn Chinese Treebank may not work successfully on the NICT Chinese Treebank. In order to guarantee the high annotation quality, in our work, we manually re-annotate both the grammatical functional tags and the traces to the NICT Chinese Treebank. With the deep re-annotation, the NI"
W10-1816,hockenmaier-steedman-2002-acquiring,0,0.0151834,"k and the NICT Chinese Treebank belong to, the machine learning model trained on the Penn Chinese Treebank may not work successfully on the NICT Chinese Treebank. In order to guarantee the high annotation quality, in our work, we manually re-annotate both the grammatical functional tags and the traces to the NICT Chinese Treebank. With the deep re-annotation, the NICT Chinese Treebank could be used not only for the shallow natural language processing tasks, but also as a resource for deep applications, such as the lexicalized grammar development from treebanks (Miyao 2006; Guo 2009; Xia 1999; Hockenmaier and Steedman 2002). Considering that the translation quality of the sentences in the NICT Chinese Treebank may affect the quality of re-annotation, in the current phase, we only selected 2,363 sentences that are of good translation quality, for re-annotation. In the future, with the expansion of the NICT Chinese Treebank, we will continue this reannotation work on large-scale sentences. 2 Content of Re-annotation Because the NICT Chinese Treebank follows the annotation guideline of the Penn Chinese Treebank, our re-annotation uses similar annotation criteria in the Penn Chinese Treebank. Figure 1 exemplifies ou"
W10-1816,C02-1145,0,0.0993283,"Missing"
W10-1903,W09-1403,0,0.198686,"Missing"
W10-1903,W09-1313,1,0.711856,"applied to statements that involve the occurrence of a change in the state of an entity – even if stated as having occurred in the past, or only hypothetically – but not in cases merely discussing the state or properties of entities, even if these can serve as the basis for inference that a specific change has occurred. We found that many of the spans an2.5 Annotation results The new PTM annotation covers 157 PubMed abstracts. Following the model of the BioNLP shared task, all mentions of specific gene or gene product names in the abstracts were annotated, applying the annotation criteria of (Ohta et al., 2009). This new named entity annotation covers 1031 gene/gene product mentions, thus averaging more than six mentions per annotated abstract. In total, 422 events of which 405 are of the novel PTM 23 Event type Glycosylation Hydroxylation Methylation Acetylation Positive reg. Phosphorylation Protein modification TOTAL Count 122 103 90 90 12 3 2 422 applies a pipeline architecture consisting of three supervised classification-based modules: a trigger detector, an event edge detector, and an event detector. In evaluation on the BioNLP shared task test data, the system extracted phosphorylation events"
W10-1903,W09-1401,1,0.848379,"and the specific modified site are expected to be of more practical interest. However, we note that the greater number of multi-argument events is expected to make the dataset more challenging as an extraction target. 3 Evaluation 3.2 To estimate the capacity of the newly annotated resource to support the extraction of the targeted PTM events and the performance of current event extraction methods at open-domain PTM extraction, we performed a set of experiments using an event extraction method competitive with the state of the art, as established in the BioNLP shared task on event extraction (Kim et al., 2009a; Bj¨orne et al., 2009). 3.1 Data Preparation The corpus data was split into training and test sets on the document level with a sampling strategy that aimed to preserve a roughly 3:1 ratio of occurrences of each event type between training and test data. The test data was held out during system development and parameter selection and only applied in a single final experiment. The event extraction system was trained using the 112 abstracts of the training set, further using 24 of the abstracts Methods 6 We note that in the BioNLP shared task data, all arguments were contained within single se"
W10-1903,P06-4005,1,\N,Missing
W10-1903,P06-1128,1,\N,Missing
W10-1904,W09-1402,1,0.418613,"Missing"
W10-1904,de-marneffe-etal-2006-generating,0,0.0630604,"Missing"
W10-1904,W09-1313,1,0.821387,"terms to ones involving the associated genes/proteins. The first challenge, gene/protein name normalization, is a wellstudied task in biomedical NLP for which a number of systems with promising performance have been proposed (Morgan and Hirschman, 2007). The second we believe to be novel. In the following, we propose a method for resolving this task. We base the decision on how to map events referencing broadly defined terms to ones referencing associated gene/protein names in part on a recently introduced dataset of “static relations” (Pyysalo et al., 2009) between named entities and terms (Ohta et al., 2009b). This dataset was created based on approximately 10,000 cases where GGP NEs, as annotated in the GENIA GGP corpus (Ohta et al., 2009a), were embedded in terms, as annotated in the GENIA term corpus (Ohta et al., 2002). For each such case, the relation between the NE and the term was annotated using a set of introduced relation types whose granularity was defined with reference to MeSH terms (see Table 2, Ohta et al., 2009b). From this data, we extracted prefix and suffix strings that, when affixed to a GGP name, produced a term with a predictable relation (within the dataset) to the GGP. Th"
W10-1904,W03-1018,1,0.827599,"achieves results close to the best published on the standard GENETAG dataset and was reported to have the best performance in a recent study comparing publicly available taggers (Kabiljo et al., 2009). Titles and abstracts of all 17.8M citations in the 2009 distribution of PubMed are processed through the BANNER system. Titles and abstracts of PubMed citations in which at least one named entity was identified, and 29 which therefore contain a possible target for event extraction, are subsequently split into sentences using a maximum-entropy based sentence splitter trained on the GENIA corpus (Kazama and Tsujii, 2003) with limited rule-based post-processing for some common errors. All sentences containing at least one named entity are then parsed with the domain-adapted McClosky-Charniak parser (McClosky and Charniak, 2008; McClosky, 2009), which has achieved the currently best published performance on the GENIA Treebank (Tateisi et al., 2005). The constituency parse trees are then transformed to the collapsed-ccprocessed variant of the Stanford Dependency scheme using the conversion tool2 introduced by de Marneffe et al. (2006). Finally, events are extracted using the Turku Event Extraction System of Bj¨o"
W10-1904,W09-1301,1,0.800599,"apping from the events involving automatically extracted terms to ones involving the associated genes/proteins. The first challenge, gene/protein name normalization, is a wellstudied task in biomedical NLP for which a number of systems with promising performance have been proposed (Morgan and Hirschman, 2007). The second we believe to be novel. In the following, we propose a method for resolving this task. We base the decision on how to map events referencing broadly defined terms to ones referencing associated gene/protein names in part on a recently introduced dataset of “static relations” (Pyysalo et al., 2009) between named entities and terms (Ohta et al., 2009b). This dataset was created based on approximately 10,000 cases where GGP NEs, as annotated in the GENIA GGP corpus (Ohta et al., 2009a), were embedded in terms, as annotated in the GENIA term corpus (Ohta et al., 2002). For each such case, the relation between the NE and the term was annotated using a set of introduced relation types whose granularity was defined with reference to MeSH terms (see Table 2, Ohta et al., 2009b). From this data, we extracted prefix and suffix strings that, when affixed to a GGP name, produced a term with a pred"
W10-1904,W09-1401,1,0.830255,"Missing"
W10-1904,I05-2038,1,0.208821,"stracts of PubMed citations in which at least one named entity was identified, and 29 which therefore contain a possible target for event extraction, are subsequently split into sentences using a maximum-entropy based sentence splitter trained on the GENIA corpus (Kazama and Tsujii, 2003) with limited rule-based post-processing for some common errors. All sentences containing at least one named entity are then parsed with the domain-adapted McClosky-Charniak parser (McClosky and Charniak, 2008; McClosky, 2009), which has achieved the currently best published performance on the GENIA Treebank (Tateisi et al., 2005). The constituency parse trees are then transformed to the collapsed-ccprocessed variant of the Stanford Dependency scheme using the conversion tool2 introduced by de Marneffe et al. (2006). Finally, events are extracted using the Turku Event Extraction System of Bj¨orne et al. which achieved the best performance in the BioNLP’09 Shared Task and remains fully competitive with even the most recent advances (Miwa et al., 2010). We use a recent publicly available revision of the event extraction system that performs also extraction of Shared Task subtask 2 and 3 information, providing additional"
W10-1904,P08-2026,0,0.00479175,"es and abstracts of all 17.8M citations in the 2009 distribution of PubMed are processed through the BANNER system. Titles and abstracts of PubMed citations in which at least one named entity was identified, and 29 which therefore contain a possible target for event extraction, are subsequently split into sentences using a maximum-entropy based sentence splitter trained on the GENIA corpus (Kazama and Tsujii, 2003) with limited rule-based post-processing for some common errors. All sentences containing at least one named entity are then parsed with the domain-adapted McClosky-Charniak parser (McClosky and Charniak, 2008; McClosky, 2009), which has achieved the currently best published performance on the GENIA Treebank (Tateisi et al., 2005). The constituency parse trees are then transformed to the collapsed-ccprocessed variant of the Stanford Dependency scheme using the conversion tool2 introduced by de Marneffe et al. (2006). Finally, events are extracted using the Turku Event Extraction System of Bj¨orne et al. which achieved the best performance in the BioNLP’09 Shared Task and remains fully competitive with even the most recent advances (Miwa et al., 2010). We use a recent publicly available revision of"
W10-1905,W07-1004,1,0.870303,"task, further outperforming the original system by Miwa et al. (2010). This result shows that the system applied for the comparison of syntactic parsers achieves state-of-the-art performance at event extraction. This result also shows that the system originally developed only for core events extraction can be easily extended for other arguments simply by treating the other arguments as additional arguments. 5 Related Work Many approaches for parser comparison have been proposed in the BioNLP field. Most comparisons have used gold treebanks with intermediate formats (Clegg and Shepherd, 2007; Pyysalo et al., 2007). Application-oriented parser comparison across several formats was first introduced by Miyao et al. (2009), who compared eight parsers and five formats for the protein-protein interaction (PPI) extraction task. PPI extraction, the recognition of binary relations of between proteins, is one of the most basic information extraction tasks in the BioNLP field. Our findings do not conflict with those of Miyao et al. Event extraction can be viewed as an additional extrinsic evaluation task for syntactic parsers, providing more reliable and evaluation and a broader perspective into parser performanc"
W10-1905,W04-3224,0,0.0429439,"ll and Clark, 2009)5 , and the Enju parser with the GENIA model (Miyao et al., 2009)6 . The formats are Stanford Dependencies (SD) (Figure 1), the CoNLL-X dependency format (Figure 2) and the predicate-argument structure (PAS) format used by Enju (Figure 3). With the exception of Enju, the analyses of these parsers were provided by the BioNLP 2009 Shared Task organizers. Analysis of system features in the task found that the use of parser output with one of Five parsers and three formats are adopted for the evaluation. The parsers are GDep (Sagae and Tsujii, 2007)2 , the Bikel parser (Bikel) (Bikel, 2004)3 , the Charniak-Johnson reranking parser, using David McClosky’s self-trained biomedical parsing model (MC) (McClosky, 2009)4 , the C&C CCG parser, adapted to biomedical text 1 http://www-tsujii.is.s.u-tokyo.ac.jp/ GENIA/SharedTask/ 2 http://www.cs.cmu.edu/∼sagae/parser/ gdep/ 3 http://www.cis.upenn.edu/∼dbikel/ software.html 4 http://www.cs.brown.edu/∼dmcc/ biomedical.html 5 http://svn.ask.it.usyd.edu.au/trac/ candc/ 6 http://www-tsujii.is.s.u-tokyo.ac.jp/ enju/ 38 formance on this data. However, it does not invalidate comparison within the dataset. We further note that the models do not inc"
W10-1905,W09-1402,0,0.315036,"Missing"
W10-1905,W09-1406,1,0.494963,"8.48 / 51.95 N/A 23.05 / 48.19 / 31.19 26.32 / 41.81 / 32.30 36.90 / 55.59 / 44.35 Riedel Task 2 Ours 65.77 / 75.29 / 70.21 47.56 / 49.55 / 48.54 38.24 / 53.57 / 44.62 49.48 / 61.87 / 54.99 Riedel N/A 22.35 / 46.99 / 30.29 25.75 / 40.75 / 31.56 35.86 / 54.08 / 43.12 Table 6: Comparison of Recall / Precision / F-score results on the test data set. MC with CoNLL-X format and Enju with Predicate Argument Structure in Enju format are used for the evaluation. Results on simple, binding, regulation, and all events are shown. Results by Miwa et al. (2010) (Miwa), Bj¨orne et al. (2009) (Bj¨orne), and Riedel et al. (2009) (Riedel) for Task 1 and Task 2 are shown for comparison. The best score in each result is shown in bold. semble of the three parser outputs are +0.01 for Task 1, and -0.26 for Task 2. This result suggests that adding more different parsers does not always improve the performance. The ensemble of three parser outputs, however, shows stable performance across categories, scoring in the top two for binding, regulation, and all events, in the top four for simple events. ple, is good for extracting regulation events, but produced weaker results for simple events. The ensembles of two parser output"
W10-1905,D07-1111,1,0.529816,"e in Enju format. 2.2 Parsers and Formats (C&C) (Rimell and Clark, 2009)5 , and the Enju parser with the GENIA model (Miyao et al., 2009)6 . The formats are Stanford Dependencies (SD) (Figure 1), the CoNLL-X dependency format (Figure 2) and the predicate-argument structure (PAS) format used by Enju (Figure 3). With the exception of Enju, the analyses of these parsers were provided by the BioNLP 2009 Shared Task organizers. Analysis of system features in the task found that the use of parser output with one of Five parsers and three formats are adopted for the evaluation. The parsers are GDep (Sagae and Tsujii, 2007)2 , the Bikel parser (Bikel) (Bikel, 2004)3 , the Charniak-Johnson reranking parser, using David McClosky’s self-trained biomedical parsing model (MC) (McClosky, 2009)4 , the C&C CCG parser, adapted to biomedical text 1 http://www-tsujii.is.s.u-tokyo.ac.jp/ GENIA/SharedTask/ 2 http://www.cs.cmu.edu/∼sagae/parser/ gdep/ 3 http://www.cis.upenn.edu/∼dbikel/ software.html 4 http://www.cs.brown.edu/∼dmcc/ biomedical.html 5 http://svn.ask.it.usyd.edu.au/trac/ candc/ 6 http://www-tsujii.is.s.u-tokyo.ac.jp/ enju/ 38 formance on this data. However, it does not invalidate comparison within the dataset."
W10-1905,de-marneffe-etal-2006-generating,0,0.191901,"Missing"
W10-1905,I05-2038,1,0.0972984,"g the method introduced by (Miyao et al., 2009). SD is generated from PTB by the Stanford tools (de Marneffe et al., 2006)7 , and CoNLLX dependencies are generated from PTB by using Treebank Converter (Johansson and Nugues, 2007)8 . We note that all of these conversions can introduce some errors in the conversion process. With the exception of Bikel, all the applied parsers have models specifically adapted for biomedical text. Further, all of the biomedical domain models have been created with reference and for many parsers with direct training on the data of (a subset of) the GENIA treebank (Tateisi et al., 2005). The results of parsing with these models as provided for the BioNLP Shared Task are used in this comparison. However, we note that the shared task data, drawn from the GENIA event corpus (Kim et al., 2008), contains abstracts that are also in the GENIA treebank. This implies that the parsers are likely to perform better on the texts used in the shared task than on other biomedical domain text, and similarly that systems building on their output are expected to achieve best per2.3 Event Extraction System The system by Miwa et al. (2010) is adopted for the evaluation. The system was originally"
W10-1905,W07-2416,0,0.0103989,"Phrase Structure Grammar (HPSG) and produces a format containing predicate argument structures (PAS) along with a phrase structure tree in Enju format. To study the contribution of the formats in which the five parsers output their analyses to task performance, we apply a number of conversions between the outputs, shown in Figure 4. The Enju PAS output is converted into Penn Treebank format using the method introduced by (Miyao et al., 2009). SD is generated from PTB by the Stanford tools (de Marneffe et al., 2006)7 , and CoNLLX dependencies are generated from PTB by using Treebank Converter (Johansson and Nugues, 2007)8 . We note that all of these conversions can introduce some errors in the conversion process. With the exception of Bikel, all the applied parsers have models specifically adapted for biomedical text. Further, all of the biomedical domain models have been created with reference and for many parsers with direct training on the data of (a subset of) the GENIA treebank (Tateisi et al., 2005). The results of parsing with these models as provided for the BioNLP Shared Task are used in this comparison. However, we note that the shared task data, drawn from the GENIA event corpus (Kim et al., 2008),"
W10-1905,W09-1418,0,0.0634223,"viously introduced base system is here improved with two modifications. One modification is removing two classes of features from the original features (for details of the original feature representation, we refer to (Miwa et al., 2010)); specifically the features representing governordependent relationships from the target word, and the features representing each event edges in the complex event detector are removed. The other modification is to use head words in a trigger expression as a gold trigger word. This modification is inspired by the part-of-speech (POS) based selection proposed by Kilicoglu and Bergler (2009). 7 http://www-nlp.stanford.edu/software/ lex-parser.shtml 8 http://nlp.cs.lth.se/software/ treebank converter/ 39 The system uses a head word “in” as a trigger word in a trigger expression “in the presence of” instead of using all the words of the expression. In cases where there is no head word information in a parser output, head words are selected heuristically: if a word does not modify another word in the trigger expression, the word is selected as a head word. The system is also modified to find secondary arguments (Task 2 in the BioNLP 2009 Shared Task). The second arguments are treate"
W10-1905,W09-1401,1,0.697119,"t of the BioNLP 2009 Shared Task. Section 2.2 then summarizes the five syntactic parsers and three formats adopted for the comparison. Section 2.3 described how the state-of-the-art event extraction system of Miwa et al. (2010) is modified and used for the comparison. prep pobj cc NFAT/AP-1 complex formed only with P and P2 nsubj dep conj Figure 1: Stanford basic dependency tree VMOD PMOD COORD ROOT root NFAT/AP-1 complex formed only with P and P2 2.1 Bio-molecular Event Extraction The bio-molecular event extraction task considered in this study is that defined in the BioNLP 2009 Shared Task (Kim et al., 2009)1 . The shared task provided common and consistent task definitions, data sets for training and evaluation, and evaluation criteria. The shared task consists of three subtasks: core event extraction (Task 1), augmenting events with secondary arguments (Task 2), and the recognition of speculation and negation of the events (Task 3) (Kim et al., 2009). In this paper we consider Task 1 and Task 2. The shared task defined nine event types, which can be divided into five simple events (Gene expression, Transcription, Protein catabolism, Phosphorylation, and Localization) that take one core argument"
W10-1919,P08-2026,0,0.0668102,"Missing"
W10-1919,W09-1402,0,0.0253425,"Missing"
W10-1919,W09-1313,1,0.872817,"Missing"
W10-1919,de-marneffe-etal-2006-generating,0,0.0410254,"Missing"
W10-1919,W09-1401,1,\N,Missing
W11-0208,H92-1045,0,0.0635838,"Missing"
W11-0208,P10-1029,0,0.0154374,"values are larger than a threshold (θ). In this way, a self-training algorithm obtains a huge amount of training data. 6. Add the merged data (Unew ) to the training data (Ti ). 2. Annotate recognized expressions as NEs. 69 In this study, we prepared seed data of 683,000 tokens (T0 in Figure 6). In each step, 227,000 tokens were sampled from the remaining set (U ). Because the remaining set U has high precision and low recall, we need not revise NEs that were annotated in Section 2.3. It might lower the quality of the training data to merge annotated entities, thus we used confidence values (Huang and Riloff, 2010) to revise annotations. Therefore, we retain the NE annotations of the remaining set U and overwrite a span of a non-NE annotation only if the current model predicts the span as an NE with high confidence. We compute the confidence of the prediction (f (x)) which a token x is predicted as label y as, f (x) = s(x, y) − max(∀z6=y s(x, z)). Here, s(x, y) denotes the score (the sum of feature weights) computed using the SVM model described in the beginning of Section 2. A confidence score presents the difference of scores between the predicted (the best) label and the second-best label. The confid"
W11-0208,C02-1054,0,0.0477503,"e biomedical field. A na´ıve approach to NER handles the task as a dictionary-matching problem: Prepare a dictionary (gazetteer) containing textual expressions of named entities of specific semantic types. Scan an input text, and recognize a text span as a named entity if the dictionary includes the expression of the span. Although this approach seemingly works well, it presents some critical issues. First, the dictionary Nadeau and Sekine (2007) reported that a strong trend exists recently in applying machine learning (ML) techniques such as Support Vector Machine (SVM) (Kazama et al., 2002; Isozaki and Kazawa, 2002) and Conditional Random Field (CRF) (Settles, 2004) to NER, which can address these issues. In this approach, NER is formalized as a classification problem in which a given expression is classified into a semantic class or other (non-NE) expressions. Because the classification problem is usually modeled using supervised learning methods, we need a manually annotated corpus for training NER classifier. However, preparing manually annotated corpus for a target domain of text and semantic types is cost-intensive and time-consuming because human experts are needed to reliably annotate NEs in text."
W11-0208,W02-0301,1,0.731641,"), and diseases in the biomedical field. A na´ıve approach to NER handles the task as a dictionary-matching problem: Prepare a dictionary (gazetteer) containing textual expressions of named entities of specific semantic types. Scan an input text, and recognize a text span as a named entity if the dictionary includes the expression of the span. Although this approach seemingly works well, it presents some critical issues. First, the dictionary Nadeau and Sekine (2007) reported that a strong trend exists recently in applying machine learning (ML) techniques such as Support Vector Machine (SVM) (Kazama et al., 2002; Isozaki and Kazawa, 2002) and Conditional Random Field (CRF) (Settles, 2004) to NER, which can address these issues. In this approach, NER is formalized as a classification problem in which a given expression is classified into a semantic class or other (non-NE) expressions. Because the classification problem is usually modeled using supervised learning methods, we need a manually annotated corpus for training NER classifier. However, preparing manually annotated corpus for a target domain of text and semantic types is cost-intensive and time-consuming because human experts are needed to rel"
W11-0208,W09-1313,1,0.82982,"onary in Entrez Gene. The training data consisted of nine hundred million tokens. We constructed a NER classifier using only four million tokens of the training data because of memory limitations. For evaluation, we used the Epigenetics and Post-translational Modification (EPI) corpus BioNLP 2011 Shared Task (SIGBioMed, 2011). Only development data and training data are released as the EPI corpus at present, we used both of the data sets for evaluation in this experiment. Named entities in the corpus are annotated exhaustively and belong to a single semantic class, Gene or Gene Product (GGP) (Ohta et al., 2009). We evaluated the performance of the 67 Method dictionary matching trained on acquired data A 92.09 85.76 P 39.03 10.18 R 42.69 23.83 F1 40.78 14.27 Table 2: Results of the preliminary experiment. (a) It is clear that in culture media of AM, cystatin C and cathepsin B are present as proteinase–antiproteinase complexes. (b) Temperature in the puerperium is higher in AM, and lower in PM. Figure 2: Dictionary-based gene name annotating example (annotated words are shown in italic typeface). NER on four measures: Accuracy (a), Precision (P), Recall (R), and F1-measure (F1). We used the strict mat"
W11-0208,W09-1119,0,0.0473033,"Our process to construct a NER classifier is as follows: We apply the GENIA tagger (Tsuruoka et al., 2005) to split the training data into tokens and to attach part of speech (POS) tags and chunk tags. In this work, tokenization is performed by an external program that separates tokens by a space, hyphen, comma, period, semicolon, or colon character. Part of speech tags present grammatical roles of tokens, e.g. verbs, nouns, and prepositions. Chunk tags compose tokens into syntactically correlated segments, e.g. verb phrases, noun phrases, and prepositional phrases. We use the IOBES notation (Ratinov and Roth, 2009) to represent NE mentions with label sequences, thereby NER is formalized as a multiclass classification problem in which a given token is classified into IOBES labels. To classify labels of tokens, we use a linear kernel SVM which applies the one-vs.-the-rest method (Weston and Watkins, 1999) to extend binary classification to multi-class classification. Given the t-th token xt in a sentence, we predict the label yt , yt = argmax s(y|xt , yt−1 ). y In this equation, s(y|xt , yt−1 ) presents the score (sum of feature weights) when the token xt is labeled y. We use yt−1 (the label of the previo"
W11-0208,W04-1221,0,0.0481332,"as a dictionary-matching problem: Prepare a dictionary (gazetteer) containing textual expressions of named entities of specific semantic types. Scan an input text, and recognize a text span as a named entity if the dictionary includes the expression of the span. Although this approach seemingly works well, it presents some critical issues. First, the dictionary Nadeau and Sekine (2007) reported that a strong trend exists recently in applying machine learning (ML) techniques such as Support Vector Machine (SVM) (Kazama et al., 2002; Isozaki and Kazawa, 2002) and Conditional Random Field (CRF) (Settles, 2004) to NER, which can address these issues. In this approach, NER is formalized as a classification problem in which a given expression is classified into a semantic class or other (non-NE) expressions. Because the classification problem is usually modeled using supervised learning methods, we need a manually annotated corpus for training NER classifier. However, preparing manually annotated corpus for a target domain of text and semantic types is cost-intensive and time-consuming because human experts are needed to reliably annotate NEs in text. For this reason, manually annotated corpora for NE"
W11-0208,W06-3328,0,0.0306408,"9 shows the added distribution of entity length (Added) differs from the original one (Original). Results of this analysis show that selftraining mainly annotates entities of the length one and barely recognizes entities of the length two or more. It might be necessary to devise a means to follow the corpus statistics of the ratio among the number of entities of different length as the self-training iteration proceeds. 4 Related Work Our study focuses mainly on achieving high performance NER without manual annotation. Several previous studies aimed at reducing the cost of manual annotations. Vlachos and Gasperin (2006) obtained noisy training data from FlyBase1 with few manually annotated abstracts from FlyBase. This study suggested the possibility of acquiring high-quality training data from noisy training data. It used a bootstrapping method and a highly context-based classifiers to increase the number of NE mentions in the training data. Even though the method achieved a high-performance NER in the biomedical domain, it requires curated seed data. Whitelaw et al. (2008) attempted to create extremely huge training data from the Web using a seed set of entities and relations. In generating training data au"
W11-0208,I08-5007,0,0.017219,"chieving high performance NER without manual annotation. Several previous studies aimed at reducing the cost of manual annotations. Vlachos and Gasperin (2006) obtained noisy training data from FlyBase1 with few manually annotated abstracts from FlyBase. This study suggested the possibility of acquiring high-quality training data from noisy training data. It used a bootstrapping method and a highly context-based classifiers to increase the number of NE mentions in the training data. Even though the method achieved a high-performance NER in the biomedical domain, it requires curated seed data. Whitelaw et al. (2008) attempted to create extremely huge training data from the Web using a seed set of entities and relations. In generating training data automatically, this study used context-based tagging. They reported that quite a few good resources (e.g., Wikipedia2 ) listed entities for obtaining training data automatically. 1 2 http://flybase.org/ http://www.wikipedia.org/ 72 This paper described an approach to the acquisition of huge amounts of training data for highperformance Bio NER automatically from a lexical database and unlabeled text. The results demonstrated that the proposed method outperformed"
W11-0208,P10-3016,0,0.0227351,"e larger than θ Ti+1 ← Ti ∪ Unew D ← DU i←i+1 end while Output Tn end Figure 5: Coordination analysis algorithm. 2.4 Self-training The method described in Section 2.3 reduces false negatives based on coordination structures. However, the training data contain numerous false negatives that cannot be solved through coordination analysis. Therefore, we used a self-training algorithm to automatically correct the training data. In general, a self-training algorithm obtains training data with a small amount of annotated data (seed) and a vast amount of unlabeled text, iterating this process (Zadeh Kaljahi, 2010): 1. Construct a classification model from a seed, then apply the model on the unlabeled text. Figure 6: Self-training algorithm. In contrast, our case is that we have a large amount of training data with numerous false negatives. Therefore, we adapt a self-training algorithm to revise the training data obtained using the method described in Section 2.3. Figure 6 shows the algorithm. We split the data set (D) obtained in Section 2.3 into a seed set (T0 ) and remaining set (DT0 ). Then, we iterate the cycle (0 ≤ i ≤ n): 1. Construct a classification model (Mi ) trained on the training data (Ti"
W11-0214,W09-1401,1,0.957728,"relations representing associations such as protein-protein interactions (Pyysalo et al., 2008; Tikk et al., 2010). In recent years, an increasing number of resources and methods pursuing more detailed representations of extracted information are becoming available (Pyysalo et al., 2007; Kim et al., 2008; Thompson et al., 2009; Bj¨orne et al., 2010). The main thrust of this move toward structured, finegrained information extraction falls under the heading of event extraction (Ananiadou et al., 2010), an approach popularized and represented in particular by the BioNLP Shared Task (BioNLP ST) (Kim et al., 2009a; Kim et al., 2011). While a detailed representation of extracted information on biomolecular events has several potential applications ranging from semantic search to database curation support (Ananiadou et al., 2010), the number of practical applications making use of this technology has arguably so far been rather limited. In this study, we pursue in particular the opportunities that event extraction holds for pathway annotation support,1 arguing that the match between The construction of pathways is a major focus of present-day biology. Typical pathways involve large numbers of entities o"
W11-0214,W11-1801,1,0.896597,"Missing"
W11-0214,P09-1113,0,0.0159016,"bustness. Conversion from pathways into the event representation opens up a number of opportunities, such as the ability to directly query large-scale event repositories (e.g. (Bj¨orne et al., 2010)) for specific pathway reactions. For pathways where reactions are marked with literature references, conversion further allows event annotations relevant to specific documents to be created automatically, sparing manual annotation costs. While such event annotations will not be bound to specific text expressions, they could be used through the application of techniques such as distant supervision (Mintz et al., 2009). As a first attempt, the conversion introduced in this work is limited in a number of ways, but we hope it can serve as a starting point for both wider adoption of pathway resources for event extraction and further research into accurate conversions between the two. The conversion software, SBML-to-event, is freely available for research purposes. 6 Discussion and Conclusions Over the last decade, the bio-community has invested enormous efforts in the construction of detailed models of the function of a large variety of biological systems in the form of pathways. These efforts toward building"
W11-0214,W09-1313,1,0.927341,"pe Drug. The same holds (with somewhat less specificity) for GENIA I NOR GANIC COMPOUND . Finally, although annotated in GENIA, the category of protein complexes has no correspondence among the entities considered in the BioNLP ST representation. Thus, information extraction systems applying the core BioNLP ST entity types will entirely lack coverage for protein complexes and will not be able 6 While the term P ROTEIN appears to suggest that the class consists only of protein forms, these entities are in fact annotated in the BioNLP ST data according to the GENIA gene/gene product guidelines (Ohta et al., 2009) and thus include also DNA and RNA forms. The type could arguably more accurately be named G ENE OR GENE PRODUCT. Pathway CellDesigner BioPAX State transition BiochemicalReaction Truncation BiochemicalReaction Transcription BiochemicalReaction Translation BiochemicalReaction Association ComplexAssembly Dissociation ComplexAssembly Transport Transport w/reaction Degradation Degradation Catalysis Catalysis Physical stimulation Control Modulation Control Trigger Control Inhibition Control Event ST’11 (see Table 3) Catabolism Catabolism Transcription Transcription Binding Binding Localization Loca"
W11-0214,W11-1803,1,0.87717,"Missing"
W11-0214,W11-1804,1,0.847779,"Missing"
W11-0215,W11-1828,0,0.226162,"Missing"
W11-0215,W09-1402,0,0.0757843,"Missing"
W11-0215,W10-1904,1,0.895723,"Missing"
W11-0215,W09-1403,0,0.369926,"Missing"
W11-0215,W11-1801,1,0.888281,"Missing"
W11-0215,C10-1088,1,0.850253,"criteria of the BioNLP Shared Task. The evaluation is event instance-based and uses the standard precision/recall/F1 -score metrics. We modified the shared task evaluation software to support the newly defined event types and ran experiments with the standard approximate span matching and partial recursive matching criteria (see (Kim et al., 2009)). We further follow the EPI task evaluation in reporting results separately for the extraction of only Theme and Cause arguments (core task) and for the full argument set. 5.2 Event extraction method We applied the EventMine event extraction system (Miwa et al., 2010a; Miwa et al., 2010b), an SVMbased pipeline system using an architecture similar to that of the best-performing system in the BioNLP ST’09 (Bj¨orne et al., 2009); we refer to the studies of Miwa et al. for detailed description of the base system. For analysing sentence structure, we applied the mogura 2.4.1 (Matsuzaki and Miyao, 2007) and GDep beta2 (Sagae and Tsujii, 2007) parsers. For the present study, we modified the base EventMine system as follows. First, to improve efficiency and generalizability, instead of using all words as trigger candidates as in the base system, we filtered candi"
W11-0215,nawaz-etal-2010-meta,0,0.0203,"egulation of modifi1 GO structure and statistics from data retrieved Dec. 2010. Figure 2: Comparison of hypothetical text-bound GO annotation with specific terms (top) and event annotation with general GO terms (bottom). cation, as these are captured using separate events in the applied representation, as illustrated in Figure 1. For an analogous reason, we do not separately include type-level distinctions for “magnitude” variants of terms (e.g. monoubiquitination, polyubiquitination) as these can be systematically modeled as aspects that can mark any event (cf. the low/neutral/high Manner of Nawaz et al. (2010)). Second, a number of the GO terms identify reactions that are in scope of previously defined (nonmodification) event types in existing resources. To avoid introducing redundant or conflicting annotation with e.g. the GENIA Event corpus (Kim et al., 2008) or BioNLP ST resources, we excluded terms that involve predominantly (or exclusively) noncovalent binding (included in the scope of the event type B INDING) and terms involving the removal of or binding between the amino acids of a protein, including protein maturation by peptide bond cleavage (annotated – arguably somewhat inaccurately – as"
W11-0215,W09-1313,1,0.83441,"performance. 4 Annotation This section presents the entity and event annotation approach, document selection, and the statistics of the created annotation. 8 The remarkably high coverage for a single type reflects the Zipfian distribution of the modification types; see e.g. Ohta et al. (2010). 4.1 To maximize compatibility with existing eventannotated resources, we chose to follow the general representation and annotation guidelines applied in the annotation of GENIA/BioNLP ST resources, specifically the BioNLP ST 2011 EPI task corpus. Correspondingly, we followed the GENIA gene/gene product (Ohta et al., 2009) annotation guidelines for marking protein mentions, extended the GENIA event corpus guidelines (Kim et al., 2008) for the annotation of protein modification events, and marked C ATALYSIS events following the EPI task representation. For compatibility, we also marked event negation and speculation as in these resources. We followed the GO definitions for individual modification types, and in the rare cases where a modification discussed in text had no existing GO definition, we extrapolated from the way in which protein modifications are generally defined in GO, consulting other domain ontolog"
W11-0215,W10-1903,1,0.913681,"ced substantially in recent years, shifting from the detection of simple binary associations such as protein-protein interactions toward resources and methods for the extraction of multiple types of structured associations of varying numbers participants in specific roles. These IE approaches are frequently termed event extraction (Ananiadou et al., 2010). While protein modifications have been considered in numerous IE studies in the domain (e.g. (Friedman et al., 2001; Rzhetsky et al., 2004; Hu et al., 2005; Narayanaswamy et al., 2005; Saric et al., 2006; Yuan et al., 2006; Lee et al., 2008; Ohta et al., 2010), event extraction efforts have brought increased focus also on the extraction of protein modifications: in the BioNLP Shared Task series that has popularized event extraction, the 2009 shared task (Kim et al., 2009) involved the extraction of nine event types including one PTM, and in the 2011 follow-up event (Kim et al., 2011) the Epigenetics and Post-translational modifications (EPI) task (Ohta et al., 2011) targeted six PTM types, their re114 Proceedings of the 2011 Workshop on Biomedical Natural Language Processing, ACL-HLT 2011, pages 114–123, c Portland, Oregon, USA, June 23-24, 2011. 2"
W11-0215,W11-1803,1,0.879677,"Missing"
W11-0215,D07-1111,1,0.793565,"he EPI task evaluation in reporting results separately for the extraction of only Theme and Cause arguments (core task) and for the full argument set. 5.2 Event extraction method We applied the EventMine event extraction system (Miwa et al., 2010a; Miwa et al., 2010b), an SVMbased pipeline system using an architecture similar to that of the best-performing system in the BioNLP ST’09 (Bj¨orne et al., 2009); we refer to the studies of Miwa et al. for detailed description of the base system. For analysing sentence structure, we applied the mogura 2.4.1 (Matsuzaki and Miyao, 2007) and GDep beta2 (Sagae and Tsujii, 2007) parsers. For the present study, we modified the base EventMine system as follows. First, to improve efficiency and generalizability, instead of using all words as trigger candidates as in the base system, we filtered candidates using a dictionary extracted from training data and expanded by using the UMLS specialist lexicon (Bodenreider, 2004) and the “hypernyms” and “similar to” relations in WordNet (Fellbaum, 120 1998). Second, to allow generalization across argument types, we added support for solving a single classification problem for event argument detection instead of solving multiple"
W11-0218,W10-1904,1,0.871169,"Missing"
W11-0218,W10-1911,0,0.033661,"Missing"
W11-0218,W04-1213,0,0.0888074,"Missing"
W11-0218,W11-1802,0,0.0476946,"Missing"
W11-0218,W11-1803,1,0.872374,"Missing"
W11-0218,C10-1096,1,0.81728,"elated lexical resource. For example, if we observe the text “Carbonic anhydrase IV” marked as P ROTEIN and have an entry for “Carbonic anhydrase 4” in a lexical resource, a machine learning method can learn to associate the resource with the P ROTEIN category (at specific similarity thresholds) despite syntactic differences. In this study, we aim to construct such a system and to demonstrate that it outperforms strict string matching approaches. We refer to our system as SimSem, as in “Similarity” and “Semantic”. 2.2 SimString SimString1 is a software library utilising the CPMerge algorithm (Okazaki and Tsujii, 2010) to enable fast approximate string matching. The software makes it possible to find matches in a collection with over ten million entries using cosine similarity and a similarity threshold of 0.7 in approximately 1 millisecond with modest modern hardware. This makes it useful for querying a large collection of strings to 1 http://www.chokkan.org/software/simstring/ 137 find entries which may differ from the query string only superficially and may still be members of the same semantic category. As an example, if we construct a SimString database using an American English wordlist2 and query it"
W11-0218,W11-1804,1,0.849299,"Missing"
W11-0218,W09-1119,0,0.022841,"nce. We evaluate our results on six corpora representing a variety of disambiguation tasks. While the integration of approximate string matching features is shown to substantially improve performance on one corpus, results are modest or negative for others. We suggest possible explanations and future research directions. Our lexical resources and implementation are made freely available for research purposes at: http://github.com/ninjin/ simsem 1 Introduction The use of dictionaries for boosting performance has become commonplace for Named Entity Recognition (NER) systems (Torii et al., 2009; Ratinov and Roth, 2009). In particular, dictionaries can give an Most previous work studying the use of dictionary resources in entity mention-related tasks has focused on single-class NER, in particular this is true for BioNLP where it has mainly concerned the detection of proteins. These efforts include Tsuruoka and Tsujii (2003), utilising dictionaries for protein detection by considering each dictionary entry using a novel distance measure, and Sasaki et al. (2008), applying dictionaries to restrain the contexts in which proteins appear in text. In this work, we do not consider entity mention detection, but inst"
W11-0218,W08-0609,0,0.020694,"troduction The use of dictionaries for boosting performance has become commonplace for Named Entity Recognition (NER) systems (Torii et al., 2009; Ratinov and Roth, 2009). In particular, dictionaries can give an Most previous work studying the use of dictionary resources in entity mention-related tasks has focused on single-class NER, in particular this is true for BioNLP where it has mainly concerned the detection of proteins. These efforts include Tsuruoka and Tsujii (2003), utilising dictionaries for protein detection by considering each dictionary entry using a novel distance measure, and Sasaki et al. (2008), applying dictionaries to restrain the contexts in which proteins appear in text. In this work, we do not consider entity mention detection, but instead focus solely on the related task of disambiguating the semantic category for a given continuous sequence of characters (a textual span), doing so we side-step the issue of boundary detection in favour of focusing on novel aspects of semantic category disambiguation. Also, we are yet to see a high-performing multi-class biomedical NER system, this motivates our desire to include multiple semantic categories. 136 Proceedings of the 2011 Worksho"
W11-0218,W03-1306,1,0.875157,"rch directions. Our lexical resources and implementation are made freely available for research purposes at: http://github.com/ninjin/ simsem 1 Introduction The use of dictionaries for boosting performance has become commonplace for Named Entity Recognition (NER) systems (Torii et al., 2009; Ratinov and Roth, 2009). In particular, dictionaries can give an Most previous work studying the use of dictionary resources in entity mention-related tasks has focused on single-class NER, in particular this is true for BioNLP where it has mainly concerned the detection of proteins. These efforts include Tsuruoka and Tsujii (2003), utilising dictionaries for protein detection by considering each dictionary entry using a novel distance measure, and Sasaki et al. (2008), applying dictionaries to restrain the contexts in which proteins appear in text. In this work, we do not consider entity mention detection, but instead focus solely on the related task of disambiguating the semantic category for a given continuous sequence of characters (a textual span), doing so we side-step the issue of boundary detection in favour of focusing on novel aspects of semantic category disambiguation. Also, we are yet to see a high-performi"
W11-0218,W09-1401,1,\N,Missing
W11-0407,W97-1502,0,0.0760306,"parser that converts the human annotations automatically into a richly annotated HPSG treebank. In order to check the proposed scheme’s effectiveness, we performed automatic pseudo-annotations that emulate the system’s idealized behavior and measured the performance of the parser trained on those annotations. In addition, we implemented a prototype system and conducted manual annotation experiments on a small test set. There has been a number of research projects to efficiently develop richly annotated corpora with the help of parsers, one of which is called a discriminant-based treebanking (Carter, 1997). In discriminant-based treebanking, the annotation process consists of two steps: a parser first generates the parse trees, which are annotation candidates, and then a human annotator selects the most plausible one. One of the most important characteristics of this methodology is to use easily-understandable questions called discriminants for picking up the final annotation results. Human annotators can perform annotations simply by answering those questions without closely examining the whole tree. Although this approach has been successful in breaking down the difficult annotations into a s"
W11-0407,W07-2202,1,0.843251,"tic dependency. In both of S-full and SL-full, the improvement from the baseline is significant. Especially, SL-full for “in-chart” data has almost complete agreement with the gold-standard HPSG annotations. The detailed figures are shown in Table 4. Therefore, we can therefore conclude that high quality CFG annotations lead to high quality HPSG annotations when the are combined with a good statistical HPSG parser. 4.2 Domain Adaptation We evaluated the parser accuracy adapted with the automatically created treebank on the Brown Corpus. In this experiment, we used the adaptation algorithm by (Hara et al., 2007), with the same hyperparameters used there. Table 5 shows the result of the adapted parser. Each line of this table stands for the parser adapted with different data. “Gold” is the result adapted on the gold-standard annotations, and “Gold (only covered)” is that adapted on the gold data which is covered by the original Enju HPSG grammar that was extracted from the WSJ portion of the Penn Treebank. “SL-full” is the result adapted on our automatically created data. “Baseline” is the result by the original Enju parser, which is trained only on the WSJ-PTB and whose grammar was extracted from the"
W11-0407,P10-2013,0,0.0218851,"or an unavailable link due to the death of the source edge. tion is that the stochastic model of the HPSG parser properly resolves the remaining ambiguities in the HPSG annotation within the constraints given by a part of the CFG trees. In order to check the validity of this expectation and to measure to what extent the CFG-based annotations can achieve correct HPSG annotations, we performed a pseudo-annotation experiment. In this experiment, we used bracketed sentences in the Brown Corpus (Kuˇcera and Francis, 1967), and a court transcript portion of the Manually Annotated Sub-Corpus (MASC) (Ide et al., 2010). We automatically created HPSG annotations that mimic the annotation results by an ideal annotator in the following four steps. First, HPSG treebanks for these sentences are created by the treebank conversion program distributed with the Enju parser. This program converts a syntactic tree annotated by Penn Treebank style into an HPSG tree. Since this program cannot convert the sentences that are not covered by the basic design of the grammar, we used only those that are successfully converted by the program throughout the experiments and considered this converted treebank as the gold-standard"
W11-0407,H94-1020,0,0.066519,"ottleneck to reduce the cost of annotator training and can restrict the size of annotations. Introduction On the basis of the success of the research on the corpus-based development in NLP, the demand for a variety of corpora has increased, for use as both a training resource and an evaluation data-set. However, the development of a richly annotated corpus such as an HPSG treebank is not an easy task, since the traditional two-step annotation, in which a parser first generates the candidates and then an annotator checks each candidate, needs intensive efforts even for well-trained annotators (Marcus et al., 1994; Kurohashi and Nagao, 1998). Among many NLP problems, adapting a parser for out-domain texts, which is usually referred to as domain adaptation problem, is one of the most remarkable problems. The main cause of this problem is the lack of corpora in that domain. Because it is difficult to prepare a sufficient corpus for each domain without Interactive predictive parsing (S´anchez-S´aez et al., 2009; S´anchez-S´aez et al., 2010) is another approach of annotations, which focuses on CFG trees. In this system, an annotator revises the currently proposed CFG tree until he or she gets the correct t"
W11-0407,W07-2208,1,0.841369,"to each word, and then, the lexical signs for “Dogs” and “run” are combined by SubjectHead schema. In this way, lexical signs and phrasal signs are combined until the whole sentence becomes one sign. Compared to Context Free Grammar (CFG), since each sign of HPSG has rich information about the phrase, such as subcategorization frame or predicate-argument structure, a corpus annotated in an HPSG manner is more difficult to build than CFG corpus. In our system, we aim at building HPSG treebanks with low-cost in which even nonexperts can perform annotations. 2.2 HPSG Deep Parser The Enju parser (Ninomiya et al., 2007) is a statistical deep parser based on the HPSG formalism. It produces an analysis of a sentence that includes the 57 2 3 HEAD noun 6 7 &lt;> 5 4SUBJ COMPS &lt;> Dogs 2 3 HEAD verb 6 7 &lt; noun >5 4SUBJ COMPS &lt;> Drung ⇓ 2 3 HEAD verb 6 7 &lt;> 5 4SUBJ COMPS &lt;> 2 Subject 3 HEAD noun 6 7 &lt;> 5 1 4SUBJ COMPS &lt;> Headj 3 HEAD verb 6 7 6SUBJ &lt; 1 >7 4 5 COMPS &lt;> 2 Figure 1: Example of HPSG parsing for “Dogs run.” syntactic structure (i.e., parse tree) and the semantic structure represented as a set of predicate-argument dependencies. The grammar design is based on the standard HPSG analysis of English (Pollard a"
W11-0407,W09-3835,0,0.0520196,"Missing"
W11-0407,N10-2010,0,0.046945,"Missing"
W11-0407,C10-2166,0,0.0201854,"3, and annotated 200 sentences in total on the system. Half of the sentences were taken from the Brown corpus and the other half were taken from a court-debate section of the MASC corpus. All of the sentences were annotated twice by two annotators. Both of the annotators has background in computer science and linguistics. Table 6 shows the statistics of the annotation procedures. This table indicates that human annotators strongly prefer “S” operation to others, and that the manual annotation on the prototype system is at least comparable to the recent discriminant-based annotation system by (Zhang and Kordoni, 2010), although the comparison is not strict because of the difference of the text. Table 7 shows the automatic evaluation results. We can see that the interactive annotation gave slight improvements in all accuracy metrics. The improvements were however not as much as we desired. By classifying the remaining errors in the annotation results, we identified several classes of major errors: 1. Truly ambiguous structures, which require the context or world-knowledge to correctly resolve them. Brown (train.) MASC in 10,576 / 10,394 864 / 857 out 7,190 / 6,464 489 / 449 in+out 17,766 / 16,858 1,353 / 1,"
W11-1203,C10-1003,1,0.841841,"Missing"
W11-1203,W09-1117,0,0.502059,"sujii‡ † Department of Computer Science, University of Tokyo {daniel.andrade, matuzaki}@is.s.u-tokyo.ac.jp ‡ Microsoft Research Asia, Beijing jtsujii@microsoft.com Abstract ular (Laroche and Langlais, 2010; Andrade et al., 2010; Ismail and Manandhar, 2010; Laws et al., 2010; Garera et al., 2009). The general idea is based on the assumption that similar words have similar contexts across languages. The context of a word can be described by the sentence in which it occurs (Laroche and Langlais, 2010) or a surrounding word-window (Rapp, 1999; Haghighi et al., 2008). A few previous studies, like (Garera et al., 2009), suggested to use the predecessor and successors from the dependency-parse tree, instead of a word window. In (Andrade et al., 2011), we showed that including dependency-parse tree context positions together with a sentence bag-of-words context can improve word translation accuracy. However previous works do not make an attempt to find an optimal combination of these different context positions. Using comparable corpora to find new word translations is a promising approach for extending bilingual dictionaries (semi-) automatically. The basic idea is based on the assumption that similar words"
W11-1203,P08-1088,0,0.0416332,"rpora Daniel Andrade† , Takuya Matsuzaki† , Jun’ichi Tsujii‡ † Department of Computer Science, University of Tokyo {daniel.andrade, matuzaki}@is.s.u-tokyo.ac.jp ‡ Microsoft Research Asia, Beijing jtsujii@microsoft.com Abstract ular (Laroche and Langlais, 2010; Andrade et al., 2010; Ismail and Manandhar, 2010; Laws et al., 2010; Garera et al., 2009). The general idea is based on the assumption that similar words have similar contexts across languages. The context of a word can be described by the sentence in which it occurs (Laroche and Langlais, 2010) or a surrounding word-window (Rapp, 1999; Haghighi et al., 2008). A few previous studies, like (Garera et al., 2009), suggested to use the predecessor and successors from the dependency-parse tree, instead of a word window. In (Andrade et al., 2011), we showed that including dependency-parse tree context positions together with a sentence bag-of-words context can improve word translation accuracy. However previous works do not make an attempt to find an optimal combination of these different context positions. Using comparable corpora to find new word translations is a promising approach for extending bilingual dictionaries (semi-) automatically. The basic"
W11-1203,C10-2055,0,0.373772,"Missing"
W11-1203,C10-1070,0,0.412999,"ndency-parsing Information for Finding Translations with Comparable Corpora Daniel Andrade† , Takuya Matsuzaki† , Jun’ichi Tsujii‡ † Department of Computer Science, University of Tokyo {daniel.andrade, matuzaki}@is.s.u-tokyo.ac.jp ‡ Microsoft Research Asia, Beijing jtsujii@microsoft.com Abstract ular (Laroche and Langlais, 2010; Andrade et al., 2010; Ismail and Manandhar, 2010; Laws et al., 2010; Garera et al., 2009). The general idea is based on the assumption that similar words have similar contexts across languages. The context of a word can be described by the sentence in which it occurs (Laroche and Langlais, 2010) or a surrounding word-window (Rapp, 1999; Haghighi et al., 2008). A few previous studies, like (Garera et al., 2009), suggested to use the predecessor and successors from the dependency-parse tree, instead of a word window. In (Andrade et al., 2011), we showed that including dependency-parse tree context positions together with a sentence bag-of-words context can improve word translation accuracy. However previous works do not make an attempt to find an optimal combination of these different context positions. Using comparable corpora to find new word translations is a promising approach for"
W11-1203,C10-2070,0,0.296323,"Missing"
W11-1203,P05-1012,0,0.038244,"ection of complaints concerning automobiles compiled by the USA National Highway Traffic Safety Administration (NHTSA)3 . Both corpora are publicly available. The corpora are non-parallel, but are comparable in terms of content. The part of MLIT and NHTSA which we used for our experiments, contains 24090 and 47613 sentences, respectively. The Japanese MLIT corpus was morphologically analyzed and dependency parsed using Juman and KNP4 . The English corpus NHTSA was POS-tagged and stemmed with Stepp Tagger (Tsuruoka et al., 2005; Okazaki et al., 2008) and dependency parsed using the MST parser (McDonald et al., 2005). Using the Japanese-English dictionary JMDic5 , we found 1796 content words in Japanese which have a translation which is in the English corpus. These content words and their translations cor. respond to our pivot words in Japanese and English, respectively.6 2 http://www.mlit.go.jp/jidosha/carinf/rcl/defects.html http://www-odi.nhtsa.dot.gov/downloads/index.cfm 4 http://www-lab25.kuee.kyoto-u.ac.jp/nlresource/juman.html and http://www-lab25.kuee.kyotou.ac.jp/nl-resource/knp.html 5 http://www.csse.monash.edu.au/ jwb/edict doc.html 6 Recall that we assume a one-to-one correspondence between a"
W11-1203,D08-1047,1,0.799856,"nd, Infrastructure, Transport and Tourism (MLIT)2 and another collection of complaints concerning automobiles compiled by the USA National Highway Traffic Safety Administration (NHTSA)3 . Both corpora are publicly available. The corpora are non-parallel, but are comparable in terms of content. The part of MLIT and NHTSA which we used for our experiments, contains 24090 and 47613 sentences, respectively. The Japanese MLIT corpus was morphologically analyzed and dependency parsed using Juman and KNP4 . The English corpus NHTSA was POS-tagged and stemmed with Stepp Tagger (Tsuruoka et al., 2005; Okazaki et al., 2008) and dependency parsed using the MST parser (McDonald et al., 2005). Using the Japanese-English dictionary JMDic5 , we found 1796 content words in Japanese which have a translation which is in the English corpus. These content words and their translations cor. respond to our pivot words in Japanese and English, respectively.6 2 http://www.mlit.go.jp/jidosha/carinf/rcl/defects.html http://www-odi.nhtsa.dot.gov/downloads/index.cfm 4 http://www-lab25.kuee.kyoto-u.ac.jp/nlresource/juman.html and http://www-lab25.kuee.kyotou.ac.jp/nl-resource/knp.html 5 http://www.csse.monash.edu.au/ jwb/edict doc."
W11-1203,P99-1067,0,0.757235,"omparable Corpora Daniel Andrade† , Takuya Matsuzaki† , Jun’ichi Tsujii‡ † Department of Computer Science, University of Tokyo {daniel.andrade, matuzaki}@is.s.u-tokyo.ac.jp ‡ Microsoft Research Asia, Beijing jtsujii@microsoft.com Abstract ular (Laroche and Langlais, 2010; Andrade et al., 2010; Ismail and Manandhar, 2010; Laws et al., 2010; Garera et al., 2009). The general idea is based on the assumption that similar words have similar contexts across languages. The context of a word can be described by the sentence in which it occurs (Laroche and Langlais, 2010) or a surrounding word-window (Rapp, 1999; Haghighi et al., 2008). A few previous studies, like (Garera et al., 2009), suggested to use the predecessor and successors from the dependency-parse tree, instead of a word window. In (Andrade et al., 2011), we showed that including dependency-parse tree context positions together with a sentence bag-of-words context can improve word translation accuracy. However previous works do not make an attempt to find an optimal combination of these different context positions. Using comparable corpora to find new word translations is a promising approach for extending bilingual dictionaries (semi-)"
W11-1203,E03-1023,0,0.0132709,"asic idea for finding a translation for a word q (query), is to measure the context of q and then to compare the context with each possible translation candidate, using an existing dictionary. We will call words for which we have a translation in the given dictionary, pivot words. First, using the source corpus, they calculate the degree of association of a query word q with all pivot words. The degree of association is a measure which is based on the cooccurrence frequency of q and the pivot word in a certain context position. A context (position) can be a word-window (Rapp, 1999), sentence (Utsuro et al., 2003), or a certain position in the dependencyparse tree (Garera et al., 2009; Andrade et al., 2011). In this way, they get a context vector for q, which contains the degree of association to the pivot words in different context positions. Using the target corpus, they then calculate a context vector for each 11 possible translation candidate x, in the same way. Finally, they compare the context vector of q with the context vector of each candidate x, and retrieve a ranked list of possible translation candidates. In the next section, we explain the baseline which is based on that previous research."
W11-1801,S10-1006,0,0.034309,"Missing"
W11-1801,W11-1802,1,0.673169,"ined bio-IE in these directions is emphasized as the main theme of the second event. This paper summarizes the entire BioNLP-ST 2011, covering the relationships between tasks and similar broad issues. Each task is presented in detail in separate overview papers and extraction systems in papers by participants. 1 Proceedings of BioNLP Shared Task 2011 Workshop, pages 1–6, c Portland, Oregon, USA, 24 June, 2011. 2011 Association for Computational Linguistics 2 Main tasks BioNLP-ST 2011 includes four main tracks (with five tasks) representing fine-grained bio-IE. 2.1 Genia task (GE) The GE task (Kim et al., 2011) preserves the task definition of BioNLP-ST 2009, arranged based on the Genia corpus (Kim et al., 2008). The data represents a focused domain of molecular biology: transcription factors in human blood cells. The purpose of the GE task is two-fold: to measure the progress of the community since the last event, and to evaluate generalization of the technology to full papers. For the second purpose, the provided data is composed of two collections: the abstract collection, identical to the BioNLP-ST 2009 data, and the new full paper collection. Progress on the task is measured through the unchang"
W11-1801,W10-1905,1,0.365086,"t.Bossy@jouy.inra.fr Ngan Nguyen Jun’ichi Tsujii University of Tokyo Microsoft Research Asia 7-3-1 Hongo, Bunkyo-ku, Tokyo 5 Dan Ling Street, Haiian District, Beijing nltngan@is.s.u-tokyo.ac.jp jtsujii@microsoft.com Abstract mance. Also, as the complexity of the task was high and system development time limited, we encouraged focus on fine-grained IE by providing gold annotation for named entities as well as various supporting resources. BioNLP-ST 2009 attracted wide attention, with 24 teams submitting final results. The task setup and data since have served as the basis for numerous studies (Miwa et al., 2010b; Poon and Vanderwende, 2010; Vlachos, 2010; Miwa et al., 2010a; Bj¨orne et al., 2010). The BioNLP Shared Task 2011, an information extraction task held over 6 months up to March 2011, met with community-wide participation, receiving 46 final submissions from 24 teams. Five main tasks and three supporting tasks were arranged, and their results show advances in the state of the art in fine-grained biomedical domain information extraction and demonstrate that extraction methods successfully generalize in various aspects. 1 Introduction The BioNLP Shared Task (BioNLP-ST, hereafter) series repres"
W11-1801,W11-1811,1,0.812584,"Missing"
W11-1801,W10-1903,1,0.814538,"Missing"
W11-1801,W11-1803,1,0.724562,"Missing"
W11-1801,N10-1123,0,0.139216,"Ngan Nguyen Jun’ichi Tsujii University of Tokyo Microsoft Research Asia 7-3-1 Hongo, Bunkyo-ku, Tokyo 5 Dan Ling Street, Haiian District, Beijing nltngan@is.s.u-tokyo.ac.jp jtsujii@microsoft.com Abstract mance. Also, as the complexity of the task was high and system development time limited, we encouraged focus on fine-grained IE by providing gold annotation for named entities as well as various supporting resources. BioNLP-ST 2009 attracted wide attention, with 24 teams submitting final results. The task setup and data since have served as the basis for numerous studies (Miwa et al., 2010b; Poon and Vanderwende, 2010; Vlachos, 2010; Miwa et al., 2010a; Bj¨orne et al., 2010). The BioNLP Shared Task 2011, an information extraction task held over 6 months up to March 2011, met with community-wide participation, receiving 46 final submissions from 24 teams. Five main tasks and three supporting tasks were arranged, and their results show advances in the state of the art in fine-grained biomedical domain information extraction and demonstrate that extraction methods successfully generalize in various aspects. 1 Introduction The BioNLP Shared Task (BioNLP-ST, hereafter) series represents a community-wide move to"
W11-1801,W09-1301,1,0.390547,"sk (Pyysalo et al., 2011b) involves the recognition of two binary part-of relations between entities: P ROTEIN -C OMPONENT and S UBUNITC OMPLEX. The task is motivated by specific challenges: the identification of the components of proteins in text is relevant e.g. to the recognition of Site arguments (cf. GE, EPI and ID tasks), and relations between proteins and their complexes relevant to any task involving them. REL setup is informed by recent semantic relation tasks (Hendrickx et al., 2010). The task data, consisting of new annotations for GE data, extends a previously introduced resource (Pyysalo et al., 2009; Ohta et al., 2010a). Supporting tasks 3.3 Gene renaming task (REN) BioNLP-ST 2011 includes three supporting tasks designed to assist in primary the extraction tasks. Other supporting resources made available to participants are presented in (Stenetorp et al., 2011). 3.1 Protein coreference task (CO) The CO task (Nguyen et al., 2011) concerns the recognition of coreferences to protein references. It is motivated from a finding from BioNLP-ST 2009 result analysis: coreference structures in biomedical text hinder the extraction results of fine-grained IE systems. While finding connections betwe"
W11-1801,W10-1919,1,0.755144,"diseases from full-text publica2 tions. The task follows the basic design of BioNLPST 2009, and the ID entities and extraction targets are a superset of the GE ones. The task extends considerably on core entities, adding to P ROTEIN four new entity types, including C HEMICAL and O RGANISM. The events extend on the GE definitions in allowing arguments of the new entity types as well as in introducing a new event category for high-level biological processes. The task was implemented in collaboration with domain experts and informed by prior studies on domain information extraction requirements (Pyysalo et al., 2010; Ananiadou et al., 2011), including the support of systems such as PATRIC (http://patricbrc.org). 2.4 Bacteria track The bacteria track consists of two tasks, BB and BI. 2.4.1 Bacteria biotope task (BB) The aim of the BB task (Bossy et al., 2011) is to extract the habitats of bacteria mentioned in textbooklevel texts written for non-experts. The texts are Web pages about the state of the art knowledge about bacterial species. BB targets general relations, Localization and PartOf , and is challenging in that texts contain more coreferences than usual, habitat references are not necessarily nam"
W11-1801,W11-1804,1,0.768915,"Missing"
W11-1801,W11-1812,1,0.906991,"elevant to task topics, including major protein modification types and their reverse reactions. For capturing the ways in which different entities participate in these events, the task extends the GE argument roles with two new roles specific to the domain, Sidechain and Contextgene. The task design and setup are oriented toward the needs of pathway extraction and curation for domain databases (Wu et al., 2003; Ongenaert et al., 2008) and are informed by previous studies on extraction of the target events (Ohta et al., 2010b; Ohta et al., 2010c). 2.3 Infectious diseases task (ID) The ID task (Pyysalo et al., 2011a) concerns the extraction of events relevant to biomolecular mechanisms of infectious diseases from full-text publica2 tions. The task follows the basic design of BioNLPST 2009, and the ID entities and extraction targets are a superset of the GE ones. The task extends considerably on core entities, adding to P ROTEIN four new entity types, including C HEMICAL and O RGANISM. The events extend on the GE definitions in allowing arguments of the new entity types as well as in introducing a new event category for high-level biological processes. The task was implemented in collaboration with domai"
W11-1801,W11-1816,1,0.850837,"Missing"
W11-1801,I05-2038,1,0.684261,"O task (Nguyen et al., 2011) concerns the recognition of coreferences to protein references. It is motivated from a finding from BioNLP-ST 2009 result analysis: coreference structures in biomedical text hinder the extraction results of fine-grained IE systems. While finding connections between event triggers and protein references is a major part of event extraction, it becomes much harder if one is replaced with a coreferencing expression. The CO task seeks to address this problem. The data sets for the task were produced based on MedCO annotation (Su et al., 2008) and other Genia resources (Tateisi et al., 2005; Kim et al., 2008). 3 The REN task (Jourde et al., 2011) objective is to extract renaming pairs of Bacillus subtilis gene/protein names from PubMed abstracts, motivated by discrepancies between nomenclature databases that interfere with search and complicate normalization. REN relations partially overlap several concepts: explicit renaming mentions, synonymy, and renaming deduced from biological proof. While the task is related to synonymy relation extraction (Yu and Agichtein, 2003), it has a novel definition of renaming, one name permanently replacing the other. 4 Schedule Table 2 shows the"
W11-1801,W10-1901,0,0.042682,"University of Tokyo Microsoft Research Asia 7-3-1 Hongo, Bunkyo-ku, Tokyo 5 Dan Ling Street, Haiian District, Beijing nltngan@is.s.u-tokyo.ac.jp jtsujii@microsoft.com Abstract mance. Also, as the complexity of the task was high and system development time limited, we encouraged focus on fine-grained IE by providing gold annotation for named entities as well as various supporting resources. BioNLP-ST 2009 attracted wide attention, with 24 teams submitting final results. The task setup and data since have served as the basis for numerous studies (Miwa et al., 2010b; Poon and Vanderwende, 2010; Vlachos, 2010; Miwa et al., 2010a; Bj¨orne et al., 2010). The BioNLP Shared Task 2011, an information extraction task held over 6 months up to March 2011, met with community-wide participation, receiving 46 final submissions from 24 teams. Five main tasks and three supporting tasks were arranged, and their results show advances in the state of the art in fine-grained biomedical domain information extraction and demonstrate that extraction methods successfully generalize in various aspects. 1 Introduction The BioNLP Shared Task (BioNLP-ST, hereafter) series represents a community-wide move toward fine-grain"
W11-1801,W09-1401,1,\N,Missing
W11-1801,W11-1809,1,\N,Missing
W11-1801,W11-1810,1,\N,Missing
W11-1803,W11-1828,0,0.113486,"Missing"
W11-1803,W09-1403,0,0.0278857,"Missing"
W11-1803,P05-1022,0,0.0198621,"sk, machine learning-based systems remain dominant overall, although there is considerable divergence in the specific methods applied. In addition to domain mainstays such as support vector machines and maximum entropy models, we find increased application of joint models (Riedel et al., 2011; McClosky et al., 2011; Riedel and McCallum, 2011) as opposed to pure pipeline systems (Bj¨orne and Salakoski, 2011; Quirk et al., 2011) . Remarkably, the application of full pars21 ing together with dependency-based representations of syntactic analyses is adopted by all participants, with the parser of Charniak and Johnson (2005) with the biomedical domain model of McClosky (2009) is applied in all but one system (Liu et al., 2011) and the Stanford Dependency representation (de Marneffe et al., 2006) in all. These choices may be motivated in part by the success of systems using the tools in the previous shared task and the availability of the analyses as supporting resources (Stenetorp et al., 2011). Despite the availability of PTM and DNA methylation resources other than those specifically introduced for the task and the P HOSPHORYLATION annotations in the GE task (Kim et al., 2011b), no participant chose to apply ot"
W11-1803,de-marneffe-etal-2006-generating,0,0.0986335,"Missing"
W11-1803,W11-1827,0,0.255622,"her than those specifically introduced for the task and the P HOSPHORYLATION annotations in the GE task (Kim et al., 2011b), no participant chose to apply other corpora for training. With the exception of externally acquired unlabeled data such as PubMed-derived word clusters applied by three groups, the task results thus reflect a closed task setting in which only the given data is used for training. 5.2 Evaluation results Table 4 presents a the primary results by event type, and Table 5 summarizes these results. We note that only two teams, UTurku (Bj¨orne and Salakoski, 2011) and ConcordU (Kilicoglu and Bergler, 2011), predicted event modifications, and only UTurku predicted additional (non-core) event arguments (data not shown). The other five systems thus addressed MSRCCP- ConUTurku FAUST NLP UMass Stanford BTMG cordU Size H YDROXYLATION 42.25 10.26 10.20 12.80 9.45 12.84 6.32 139 D EHYDROXYLATION - 1 67.12 51.61 50.00 49.18 40.98 47.06 44.44 130 P HOSPHORYLATION D EPHOSPHORYLATION 0.00 0.00 0.00 0.00 0.00 50.00 0.00 3 75.34 72.95 67.88 72.94 67.44 70.87 69.97 340 U BIQUITINATION D EUBIQUITINATION 54.55 40.00 0.00 31.58 0.00 42.11 14.29 17 60.21 31.21 34.54 23.82 31.02 15.65 8.22 416 DNA METHYLATION DNA"
W11-1803,W11-1801,1,0.73904,"Missing"
W11-1803,W11-1802,0,0.660347,"ation, events are typed n-ary associations of participants (entities or other events) in specific roles. Events are bound to specific expressions in text (the event trigger or text binding) and are primary objects of annotation, allowing them to be marked in turn e.g. as negated or as participants in other events. Figure 2 illustrates these concepts. In its specific formulation, EPI broadly follows the definition of the BioNLP’09 shared task on event extraction. Basic modification events are defined similarly to the P HOSPHORYLATION event type targeted in the ’09 and the 2011 GE and ID tasks (Kim et al., 2011b; Pyysalo et al., 2011b), with the full task extending previously defined arguments with two additional ones, Sidechain and Contextgene. 2.1 Entities The EPI task follows the general policy of the BioNLP Shared Task in isolating the basic task of named entity recognition from the event extraction task by providing task participants with manually annotated gene and gene product entities as a starting point for extraction. The entity types follow the BioNLP’09 Shared Task scheme, where genes and their products are simply marked as P ROTEIN.1 In addition to the given P ROTEIN entities, some even"
W11-1803,W11-1826,0,0.0276367,"fic methods applied. In addition to domain mainstays such as support vector machines and maximum entropy models, we find increased application of joint models (Riedel et al., 2011; McClosky et al., 2011; Riedel and McCallum, 2011) as opposed to pure pipeline systems (Bj¨orne and Salakoski, 2011; Quirk et al., 2011) . Remarkably, the application of full pars21 ing together with dependency-based representations of syntactic analyses is adopted by all participants, with the parser of Charniak and Johnson (2005) with the biomedical domain model of McClosky (2009) is applied in all but one system (Liu et al., 2011) and the Stanford Dependency representation (de Marneffe et al., 2006) in all. These choices may be motivated in part by the success of systems using the tools in the previous shared task and the availability of the analyses as supporting resources (Stenetorp et al., 2011). Despite the availability of PTM and DNA methylation resources other than those specifically introduced for the task and the P HOSPHORYLATION annotations in the GE task (Kim et al., 2011b), no participant chose to apply other corpora for training. With the exception of externally acquired unlabeled data such as PubMed-derive"
W11-1803,W11-1806,0,0.132325,". The full task results are considered the primary evaluation for the task e.g. for the purposes of determining the ranking of participating systems. 5 5.1 Results Participation Table 3 summarizes the participating groups and the features of their extraction systems. We note that, similarly to the ’09 task, machine learning-based systems remain dominant overall, although there is considerable divergence in the specific methods applied. In addition to domain mainstays such as support vector machines and maximum entropy models, we find increased application of joint models (Riedel et al., 2011; McClosky et al., 2011; Riedel and McCallum, 2011) as opposed to pure pipeline systems (Bj¨orne and Salakoski, 2011; Quirk et al., 2011) . Remarkably, the application of full pars21 ing together with dependency-based representations of syntactic analyses is adopted by all participants, with the parser of Charniak and Johnson (2005) with the biomedical domain model of McClosky (2009) is applied in all but one system (Liu et al., 2011) and the Stanford Dependency representation (de Marneffe et al., 2006) in all. These choices may be motivated in part by the success of systems using the tools in the previous shared ta"
W11-1803,W09-1313,1,0.127393,"Missing"
W11-1803,W10-1903,1,0.788921,"Missing"
W11-1803,W11-0215,1,0.844108,"yped n-ary associations of participants (entities or other events) in specific roles. Events are bound to specific expressions in text (the event trigger or text binding) and are primary objects of annotation, allowing them to be marked in turn e.g. as negated or as participants in other events. Figure 2 illustrates these concepts. In its specific formulation, EPI broadly follows the definition of the BioNLP’09 shared task on event extraction. Basic modification events are defined similarly to the P HOSPHORYLATION event type targeted in the ’09 and the 2011 GE and ID tasks (Kim et al., 2011b; Pyysalo et al., 2011b), with the full task extending previously defined arguments with two additional ones, Sidechain and Contextgene. 2.1 Entities The EPI task follows the general policy of the BioNLP Shared Task in isolating the basic task of named entity recognition from the event extraction task by providing task participants with manually annotated gene and gene product entities as a starting point for extraction. The entity types follow the BioNLP’09 Shared Task scheme, where genes and their products are simply marked as P ROTEIN.1 In addition to the given P ROTEIN entities, some events involve other entiti"
W11-1803,W11-1804,1,0.370325,"Missing"
W11-1803,W11-1825,0,0.0576857,"ranking of participating systems. 5 5.1 Results Participation Table 3 summarizes the participating groups and the features of their extraction systems. We note that, similarly to the ’09 task, machine learning-based systems remain dominant overall, although there is considerable divergence in the specific methods applied. In addition to domain mainstays such as support vector machines and maximum entropy models, we find increased application of joint models (Riedel et al., 2011; McClosky et al., 2011; Riedel and McCallum, 2011) as opposed to pure pipeline systems (Bj¨orne and Salakoski, 2011; Quirk et al., 2011) . Remarkably, the application of full pars21 ing together with dependency-based representations of syntactic analyses is adopted by all participants, with the parser of Charniak and Johnson (2005) with the biomedical domain model of McClosky (2009) is applied in all but one system (Liu et al., 2011) and the Stanford Dependency representation (de Marneffe et al., 2006) in all. These choices may be motivated in part by the success of systems using the tools in the previous shared task and the availability of the analyses as supporting resources (Stenetorp et al., 2011). Despite the availability"
W11-1803,W11-1807,0,0.057453,"are considered the primary evaluation for the task e.g. for the purposes of determining the ranking of participating systems. 5 5.1 Results Participation Table 3 summarizes the participating groups and the features of their extraction systems. We note that, similarly to the ’09 task, machine learning-based systems remain dominant overall, although there is considerable divergence in the specific methods applied. In addition to domain mainstays such as support vector machines and maximum entropy models, we find increased application of joint models (Riedel et al., 2011; McClosky et al., 2011; Riedel and McCallum, 2011) as opposed to pure pipeline systems (Bj¨orne and Salakoski, 2011; Quirk et al., 2011) . Remarkably, the application of full pars21 ing together with dependency-based representations of syntactic analyses is adopted by all participants, with the parser of Charniak and Johnson (2005) with the biomedical domain model of McClosky (2009) is applied in all but one system (Liu et al., 2011) and the Stanford Dependency representation (de Marneffe et al., 2006) in all. These choices may be motivated in part by the success of systems using the tools in the previous shared task and the availability of t"
W11-1803,W11-1808,0,0.285928,"extract core targets. The full task results are considered the primary evaluation for the task e.g. for the purposes of determining the ranking of participating systems. 5 5.1 Results Participation Table 3 summarizes the participating groups and the features of their extraction systems. We note that, similarly to the ’09 task, machine learning-based systems remain dominant overall, although there is considerable divergence in the specific methods applied. In addition to domain mainstays such as support vector machines and maximum entropy models, we find increased application of joint models (Riedel et al., 2011; McClosky et al., 2011; Riedel and McCallum, 2011) as opposed to pure pipeline systems (Bj¨orne and Salakoski, 2011; Quirk et al., 2011) . Remarkably, the application of full pars21 ing together with dependency-based representations of syntactic analyses is adopted by all participants, with the parser of Charniak and Johnson (2005) with the biomedical domain model of McClosky (2009) is applied in all but one system (Liu et al., 2011) and the Stanford Dependency representation (de Marneffe et al., 2006) in all. These choices may be motivated in part by the success of systems using the tools in"
W11-1803,W11-1816,1,0.608476,"Missing"
W11-1803,W09-1401,1,\N,Missing
W11-1804,W11-1828,0,0.0564394,"Missing"
W11-1804,P05-1022,0,0.0320164,"ced to only core arguments, event modifications are removed, and resulting duplicate events removed. We term this the core task. In terms of the subtask division applied in the BioNLP’09 Shared Task and the GE task of 2011, the core task is analogous to subtask 1 and the full task analogous to the combination of subtasks 1–3. 5 5.1 Results Participation Final results to the task were successfully submitted by seven participants. Table 5 summarizes the information provided by the participating teams. We note that full parsing is applied in all systems, with the specific choice of the parser of Charniak and Johnson (2005) with the biomedical domain model of McClosky (2009) and conversion into the Stanford Dependency representation (de Marneffe et al., 2006) being adopted by five participants. Further, five of the seven systems are predominantly machine learning-based. These can be seen as extensions of trends that were noted in analysis of the BioNLP Rank Team Org 1 FAUST 3NLP 2 UMass 1NLP 3 Stanford 3NLP Word CoreNLP, SnowBall CoreNLP, SnowBall CoreNLP 4 ConcordU 2NLP - 5 UTurku 6 PNNL 7 PredX 1BI Porter 1CS, 1NLP, Porter 2BI 1CS, 1NLP LGP NLP Parse Trig. McCCJ + SD Arg. Events Group. Other resources Modif. C"
W11-1804,de-marneffe-etal-2006-generating,0,0.13301,"Missing"
W11-1804,W11-1827,0,0.406561,"ons are largely compatible with ID ones (see detailed results below). This is encouraging for future applications of the event extraction approach: as manual annotation requires considerable effort and time, the ability to use existing annotations is important for the feasibility of adaptation of the approach to new domains. While several participants made use of supporting syntactic analyses provided by the organizers (Stenetorp et al., 2011), none applied the analyses for supporting tasks, such as coreference or entity relation extraction results – at least in cases due to time constraints (Kilicoglu and Bergler, 2011). 5.2 Evaluation results Table 6 presents the primary results by event type, and Table 7 summarizes these results. The full task requires the extraction of additional arguments and event modifications and involves multiple novel challenges from previously addressed domain tasks including a new subdomain, full-text documents, several new entity types and a new event category. 31 Team FAUST UMass Stanford ConcordU UTurku PNNL PredX recall 48.03 46.92 46.30 49.00 37.85 27.75 22.56 prec. 65.97 62.02 55.86 40.27 48.62 52.36 35.18 F-score 55.59 53.42 50.63 44.21 42.57 36.27 27.49 Table 7: Primary ev"
W11-1804,W11-1801,1,0.755037,"Missing"
W11-1804,W11-1802,0,0.446286,"ected by participants addressing the full task. 2.2 Relations The ID task involves one relation, E QUIV, defining entities (of any of the core types) to be equivalent. This relation is used to annotate abbreviations and local aliases and it is not a target of extraction, but provided for reference and applied in evaluation, where references to any of a set of equivalent entities are treated identically. 2.3 Events The primary extraction targets of the ID task are the event types summarized in Table 1. These are a superset of those targeted in the BioNLP ST’09 and its repeat, the 2011 GE task (Kim et al., 2011b). This design makes it possible to study aspects of domain adaptation by having the same extraction targets in two subdomains of biomedicine, that of transcription factors in human blood cells (GE) and infectious diseases. The events in the ID task extend on those of GE in the inclusion of additional entity types as participants in previously considered event types and the introduction of a new type, P ROCESS. We next briefly discuss the semantics of these events, defined (as in GE) with reference to the communitystandard Gene Ontology (Ashburner et al., 2000). We refer to (Kim et al., 2008;"
W11-1804,W11-1806,0,0.20456,"Missing"
W11-1804,W11-1818,0,0.0187268,"Missing"
W11-1804,W09-1313,1,0.0825789,"es is not part of the ID task. As named entity recognition (NER) is considered in other prominent domain evaluations (Krallinger et al., 2008), we have chosen to isolate aspects of extraction performance relating to NER from the main task of interest, event extraction, by providing participants with human-created gold annotations for core entities. These annotations are briefly presented in the following. Mentions of names of genes and their products (RNA and proteins) are annotated with a single type, without differentiating between subtypes, following the guidelines of the GENIA GGP corpus (Ohta et al., 2009). This type is named P RO TEIN to maintain consistency with related tasks (e.g. BioNLP ST’09), despite slight inaccuracy for cases specifically referencing RNA or DNA forms. Two-component systems, consisting of two proteins, frequently have names derived from the names of the proteins involved (e.g. PhoP-PhoR or SsrA/SsrB). Mentions of TCSs are annotated as T WO - COMPONENT- SYSTEM, nesting P ROTEIN annotations if present. Regulons and operons are collections of genes whose expression is jointly regulated. Like the names of TCSs, their names may derive from the names of the involved genes and"
W11-1804,W11-1803,1,0.370525,"Missing"
W11-1804,W10-1919,1,0.809456,"2010 2008–2010 2008–2010 2007–2010 2008–2010 2008–2009 2007–2008 Entity type P ROTEIN C HEMICAL O RGANISM T WO - COMPONENT- SYSTEM prec. 54.64 32.24 90.38 87.69 rec. 39.64 19.05 47.70 47.24 F 45.95 23.95 62.44 61.40 Table 3: Automatic core entity tagging performance. Table 2: Corpus composition. Journals in which selected articles were published with number of articles (#) and publication years. following tools and settings were adopted, with parameters tuned on initial annotation for two documents: design was guided by previous studies on NER and event extraction in a closely related domain (Pyysalo et al., 2010; Ananiadou et al., 2011). P ROTEIN: NeMine (Sasaki et al., 2008) trained on the JNLPBA data (Kim et al., 2004) with threshold 0.05, filtered to only GENE and PROTEIN types. 3.1 O RGANISM: Linnaeus (Gerner et al., 2010) with “variant matching” for species names variants. Document selection The training and test data were drawn from the primary text content of recent full-text PMC open access documents selected by infectious diseases domain experts (Virginia Tech team) as representative publications on two-component regulatory systems. Table 2 presents some characteristics of the corpus composi"
W11-1804,W11-1807,0,0.0968254,"Missing"
W11-1804,W11-1808,0,0.49423,"Missing"
W11-1804,W08-0609,1,0.0820893,"Entity type P ROTEIN C HEMICAL O RGANISM T WO - COMPONENT- SYSTEM prec. 54.64 32.24 90.38 87.69 rec. 39.64 19.05 47.70 47.24 F 45.95 23.95 62.44 61.40 Table 3: Automatic core entity tagging performance. Table 2: Corpus composition. Journals in which selected articles were published with number of articles (#) and publication years. following tools and settings were adopted, with parameters tuned on initial annotation for two documents: design was guided by previous studies on NER and event extraction in a closely related domain (Pyysalo et al., 2010; Ananiadou et al., 2011). P ROTEIN: NeMine (Sasaki et al., 2008) trained on the JNLPBA data (Kim et al., 2004) with threshold 0.05, filtered to only GENE and PROTEIN types. 3.1 O RGANISM: Linnaeus (Gerner et al., 2010) with “variant matching” for species names variants. Document selection The training and test data were drawn from the primary text content of recent full-text PMC open access documents selected by infectious diseases domain experts (Virginia Tech team) as representative publications on two-component regulatory systems. Table 2 presents some characteristics of the corpus composition. To focus efforts on natural language text likely to express"
W11-1804,W11-1816,1,0.465395,"Missing"
W11-1804,W09-1401,1,\N,Missing
W11-1804,W04-1213,0,\N,Missing
W11-1804,C10-1088,1,\N,Missing
W11-1811,W97-1306,0,0.0342516,"cted to an anaphora through more than one surface link, we call it an indirect protein antecedent, and the antecedents in the middle of the chain intermediate antecedents. The performance evaluated in this mode may be directly connected to the potential performance in main IE tasks: the more the (anaphoric) protein references are found, the more the protein-related events may be found. For this reason, the protein coreference mode is chosen as the primary evaluation mode. Evaluation results for both evaluation modes are given in traditional precision, recall and f-score, which are similar to (Baldwin, 1997). Team UU Member 1 NLP 5.1 UZ CU UT US UC 5 NLP 2 NLP 1 biochemist 2 AI 3 NLP, 1 BioNLP Surface coreference A response expression is matched with a gold expression following partial match criterion. In particular, a response expression is considered correct when it covers the minimal boundary, and is included in the maximal boundary of expression. Maximal boundary is the span of expression annotation, and minimal boundary is the head of expression, as defined in MUC annotation schemes (Chinchor, 1998). A response link is correct when its two argument expressions are correctly matched with thos"
W11-1811,P10-1143,0,0.0685994,"Missing"
W11-1811,M98-1001,0,0.172105,"as standard tasks of information extraction (IE), coreference resolution (Ng, 2010; Bejan and Harabagiu, 2010) is more and more recognized as an important component of IE for a higher performance. Without coreference resolution, the performance of IE is often substantially limited due to an abundance of coreference structures in natural language text, i.e. information pieces written in text with involvement of a coreference structure are hard to be captured (Miwa et al., 2010). There have been several attempts for coreference resolution, particularly for newswire texts (Strassel et al., 2008; Chinchor, 1998). It is also one of the lessons from BioNLP Shared Task (BioNLPST, hereafter) 2009 that coreference structures in biomedical text substantially hinder the progress of fine-grained IE (Kim et al., 2009). To address the problem of coreference resolution in molecular biology literature, the Protein Coreference (COREF) task is arranged in BioNLP-ST 2011 as a supporting task. While the task itself is not an IE task, it is expected to be a useful component in performing the main IE tasks more effectively. To establish a stable evaluation and to observe the effect of the results of the task to the ma"
W11-1811,O04-1011,0,0.276438,"ch more non-anaphoric definite noun phrases than anaphoric ones, which making it difficult to train an effective classier for anaphoricity determination. We have to seek for a better method for solving the DNP links, in order to significantly improve protein coreference resolution system. Concerning the PRON type, Table 8 shows that except for that-1, no other figures are higher than 50 percent f-score. This is an interesting observation because pronominal anaphora problem has been reported with much higher results on other domains(Raghunathan et al., 2010), and also on other bio data (hsiang Lin and Liang, 2004). One of the reasons for the low recall is because target anaphoric pronouns in the bio domain are neutralgender and third-person pronouns(Nguyen and Kim, 2008), which are difficult to resolve than other types of pronouns(Stoyanov et al., 2010a). 8.3 Protein coreference analysis - Intermediate antecedent As mentioned in the task setting, anaphors can directly link to their antecedent, or indirectly link via one or more intermediate antecedents. We counted the numbers of correct direct and indirect protein coreference links in each submission (Table 12). Sub type both 1 it 1 such 2 their 1 thes"
W11-1811,P10-1142,0,0.0656707,"Missing"
W11-1811,C08-1079,1,0.852972,"ve to seek for a better method for solving the DNP links, in order to significantly improve protein coreference resolution system. Concerning the PRON type, Table 8 shows that except for that-1, no other figures are higher than 50 percent f-score. This is an interesting observation because pronominal anaphora problem has been reported with much higher results on other domains(Raghunathan et al., 2010), and also on other bio data (hsiang Lin and Liang, 2004). One of the reasons for the low recall is because target anaphoric pronouns in the bio domain are neutralgender and third-person pronouns(Nguyen and Kim, 2008), which are difficult to resolve than other types of pronouns(Stoyanov et al., 2010a). 8.3 Protein coreference analysis - Intermediate antecedent As mentioned in the task setting, anaphors can directly link to their antecedent, or indirectly link via one or more intermediate antecedents. We counted the numbers of correct direct and indirect protein coreference links in each submission (Table 12). Sub type both 1 it 1 such 2 their 1 these 2 this 2 whose 1 Type PRON PRON DNP PRON DNP DNP RELAT Count 2 17 2 27 26 20 1 Sub type both 2 its 1 that 1 them 1 they 1 those 1 whose 2 Type PRON PRON RELAT"
W11-1811,D10-1048,0,0.0577386,"his type. A possi80 ble reason of this is because there are much more non-anaphoric definite noun phrases than anaphoric ones, which making it difficult to train an effective classier for anaphoricity determination. We have to seek for a better method for solving the DNP links, in order to significantly improve protein coreference resolution system. Concerning the PRON type, Table 8 shows that except for that-1, no other figures are higher than 50 percent f-score. This is an interesting observation because pronominal anaphora problem has been reported with much higher results on other domains(Raghunathan et al., 2010), and also on other bio data (hsiang Lin and Liang, 2004). One of the reasons for the low recall is because target anaphoric pronouns in the bio domain are neutralgender and third-person pronouns(Nguyen and Kim, 2008), which are difficult to resolve than other types of pronouns(Stoyanov et al., 2010a). 8.3 Protein coreference analysis - Intermediate antecedent As mentioned in the task setting, anaphors can directly link to their antecedent, or indirectly link via one or more intermediate antecedents. We counted the numbers of correct direct and indirect protein coreference links in each submis"
W11-1811,P10-2029,0,0.0631713,"improve protein coreference resolution system. Concerning the PRON type, Table 8 shows that except for that-1, no other figures are higher than 50 percent f-score. This is an interesting observation because pronominal anaphora problem has been reported with much higher results on other domains(Raghunathan et al., 2010), and also on other bio data (hsiang Lin and Liang, 2004). One of the reasons for the low recall is because target anaphoric pronouns in the bio domain are neutralgender and third-person pronouns(Nguyen and Kim, 2008), which are difficult to resolve than other types of pronouns(Stoyanov et al., 2010a). 8.3 Protein coreference analysis - Intermediate antecedent As mentioned in the task setting, anaphors can directly link to their antecedent, or indirectly link via one or more intermediate antecedents. We counted the numbers of correct direct and indirect protein coreference links in each submission (Table 12). Sub type both 1 it 1 such 2 their 1 these 2 this 2 whose 1 Type PRON PRON DNP PRON DNP DNP RELAT Count 2 17 2 27 26 20 1 Sub type both 2 its 1 that 1 them 1 they 1 those 1 whose 2 Type PRON PRON RELAT PRON PRON PRON RELAT Count 4 61 37 1 5 9 0 Sub type either 1 one 2 the 2 these 1 t"
W11-1811,strassel-etal-2008-linguistic,0,0.0555665,"Missing"
W11-1811,I05-2038,1,0.7862,"considered in the secondary evaluation mode. See section 5 for more detail. 4 Type RELAT PRON Anaphora DNP APPOS N/C Antecedent TOTAL Train 1193 738 296 9 11 2116 4363 Dev 254 149 58 1 1 451 914 Test 349 269 91 3 2 674 1388 Table 1: Statistics of coreference entities in COREF data sets: N/C = not-classified. DNP indicates definite NPs or demonstrative NPs, e.g. NPs that begin with the, this, etc. Data Preparation The data sets for the COREF task are produced based on three resources: MedCO coreference annotation (Su et al., 2008), Genia event annotation (Kim et al., 2008), and Genia Treebank (Tateisi et al., 2005). Although the three have been developed independently from each other, they are annotations made to the same corpus, the Genia corpus (Kim et al., 2008). Since COREF was focused on finding anaphoric references to proteins (or genes), only relevant annotations were extracted from the MedCO corpus though the following process: 1. From MedCo annotation, coreference entities that were pronouns and definite base NPs were extracted, which became candidate anaphoric expressions. The base NPs were determined by consulting Genia Tree Bank. 2. Among the candidate anaphoric expressions, those that could"
W11-1811,W09-1401,1,\N,Missing
W11-1812,W11-1828,0,0.0979135,"Missing"
W11-1812,W09-1402,0,0.0278135,"Missing"
W11-1812,P05-1022,0,0.0187501,"Missing"
W11-1812,de-marneffe-etal-2006-generating,0,0.214276,"Missing"
W11-1812,W11-1824,0,0.318099,"he available training data. 6 Discussion The REL task was explicitly cast in a support role for the main event extraction tasks, and REL participants were encouraged to make their predictions of the task extraction targets for the various main task datasets available to main task participants. The UTurku team responded to this call for supporting analyses, running their top-ranking REL task system on all main task datasets and making its output available as a supporting resource (Stenetorp et al., 2011). In the main tasks, we are so far aware of one application of this data: the BMI@ASU team (Emadzadeh et al., 2011) applied the UTurku REL predictions as part of their GE task system for resolving the Site arguments in events such as B IND ING and P HOSPHORYLATION (see Figure 1). While more extensive use of the data would have been desirable, we find this application of the REL analyses very appropriate to our general design for the role of the supporting and main tasks and hope to see other groups pursue similar possibilities in future work. 86 7 Conclusions We have presented the preparation, resources, results and analysis of the Entity Relations (REL) task, a supporting task of the BioNLP Shared Task 20"
W11-1812,W11-1827,0,0.0177329,"Relation matching is exact: for a submitted relation to match a gold one, both its type and the related entities must match. 5 5.1 Closky (2009), converted into Stanford Dependency form using the Stanford tools (de Marneffe et al., 2006). These specific choices may perhaps be influenced by the success of systems building on them in the 2009 shared task (e.g. Bj¨orne et al. (2009)). While UTurku (Bj¨orne and Salakoski, 2011) and VIBGhent (Van Landeghem et al., 2011) further agree in the choice of Support Vector Machines for the recognition of entities and the extraction of relations, ConcordU (Kilicoglu and Bergler, 2011) and HCMUS (Le Minh et al., 2011) pursue approaches building on dictionary- and rule-based extraction. Only the VIBGhent system makes use of resources external to those provided for the task, extracting specific semantic entity types from the GENIA corpus as well as inducing word similarities from a large unannotated corpus of PubMed abstracts. 5.2 Results Participation Table 2 summarizes the participating groups and approaches. We find a remarkable number of similarities between the approaches of the systems, with all four utilizing full parsing and a dependency representation of the syntacti"
W11-1812,W11-1801,1,0.80836,"Missing"
W11-1812,W11-1802,0,0.0934158,"256 Protein 9,297 2,080 3,589 Relation 1,857 480 497 P ROTEIN -C OMPONENT 1,302 314 334 S UBUNIT-C OMPLEX 555 166 163 Table 1: REL dataset statistics. and a complex that it is a subunit of. Following the biological motivation and the general practice in the shared task to term genes and gene products P RO TEIN for simplicity, we named these two relations P ROTEIN -C OMPONENT and S UBUNIT-C OMPLEX. Figure 1 shows an illustration of a simple relation with an associated event (not part of REL). Events with Site arguments such as that shown in the figure are targeted in the GE, EPI, and ID tasks (Kim et al., 2011b; Ohta et al., 2011; Pyysalo et al., 2011) that REL is intended to support. 3 Data The task dataset consists of new annotations for the GENIA corpus (Kim et al., 2008), building on the existing biomedical term annotation (Ohta et al., 2002), the gene and gene product name annotation (Ohta et al., 2009) and the syntactic annotation (Tateisi et al., 2005) of the corpus. The general features of the annotation are presented by Pyysalo et al. (2009), describing a previous release of a subset of the data. The REL task annotation effort extended the coverage of the previously released annotation to"
W11-1812,W11-1822,0,0.122014,"Missing"
W11-1812,W11-1811,1,0.729969,"Missing"
W11-1812,W09-1313,1,0.356791,"P RO TEIN for simplicity, we named these two relations P ROTEIN -C OMPONENT and S UBUNIT-C OMPLEX. Figure 1 shows an illustration of a simple relation with an associated event (not part of REL). Events with Site arguments such as that shown in the figure are targeted in the GE, EPI, and ID tasks (Kim et al., 2011b; Ohta et al., 2011; Pyysalo et al., 2011) that REL is intended to support. 3 Data The task dataset consists of new annotations for the GENIA corpus (Kim et al., 2008), building on the existing biomedical term annotation (Ohta et al., 2002), the gene and gene product name annotation (Ohta et al., 2009) and the syntactic annotation (Tateisi et al., 2005) of the corpus. The general features of the annotation are presented by Pyysalo et al. (2009), describing a previous release of a subset of the data. The REL task annotation effort extended the coverage of the previously released annotation to all relations of the targeted types stated within sentence scope in the GENIA corpus. For compatibility with the BioNLP ST’09 and its repeat as the GE task in 2011 (Kim et al., 2011b), the REL task training/development/test set division of the GENIA corpus abstracts matches that of the BioNLP ST’09 data"
W11-1812,W11-1803,1,0.742344,"Missing"
W11-1812,W09-1301,1,0.95227,"ity Relations in BioNLP ST’11. This paper presents the Entity Relations (REL) supporting task. Task Setting In the design of the REL task, we followed the general policy of the shared task in assuming named entity recognition (NER) as a given starting point: participants were provided with manually annotated gold standard annotations identifying gene/protein names in all of the training, development, and final test data. By limiting effects due to NER performance, the task remains more specifically focused on the key challenge studied. Following the results and analysis from previous studies (Pyysalo et al., 2009; Ohta et al., 2010), we chose to limit the task specifically to relations involving a gene/protein named entity (NE) and one other entity. Fixing one entity involved in each relation to an NE helps assure that the relations are “anchored” to real-world entities, and the specific choice of the gene/protein NE class further provides a category with several existing systems and substantial ongoing efforts addressing the identification of those referents through named entity recognition and normalization (Leaman and Gonzalez, 2008; Hakenberg et al., 2008; Krallinger et al., 2008; Morgan et al., 2"
W11-1812,W11-1804,1,0.69244,"Missing"
W11-1812,W11-1816,1,0.600281,"Missing"
W11-1812,I05-2038,1,0.155548,"tions P ROTEIN -C OMPONENT and S UBUNIT-C OMPLEX. Figure 1 shows an illustration of a simple relation with an associated event (not part of REL). Events with Site arguments such as that shown in the figure are targeted in the GE, EPI, and ID tasks (Kim et al., 2011b; Ohta et al., 2011; Pyysalo et al., 2011) that REL is intended to support. 3 Data The task dataset consists of new annotations for the GENIA corpus (Kim et al., 2008), building on the existing biomedical term annotation (Ohta et al., 2002), the gene and gene product name annotation (Ohta et al., 2009) and the syntactic annotation (Tateisi et al., 2005) of the corpus. The general features of the annotation are presented by Pyysalo et al. (2009), describing a previous release of a subset of the data. The REL task annotation effort extended the coverage of the previously released annotation to all relations of the targeted types stated within sentence scope in the GENIA corpus. For compatibility with the BioNLP ST’09 and its repeat as the GE task in 2011 (Kim et al., 2011b), the REL task training/development/test set division of the GENIA corpus abstracts matches that of the BioNLP ST’09 data. The statistics of the corpus are presented in Tabl"
W11-1812,W10-1921,1,0.819205,"Missing"
W11-1812,W11-1821,0,0.0685336,"Missing"
W11-1812,W09-1401,1,\N,Missing
W11-1816,I05-2038,1,\N,Missing
W11-1816,W11-1825,0,\N,Missing
W11-1816,de-marneffe-etal-2006-generating,0,\N,Missing
W11-1816,W11-1802,1,\N,Missing
W11-1816,W11-1803,1,\N,Missing
W11-1816,J93-2004,0,\N,Missing
W11-1816,W11-1812,1,\N,Missing
W11-1816,D10-1096,0,\N,Missing
W11-1816,N10-1123,0,\N,Missing
W11-1816,J08-1002,1,\N,Missing
W11-1816,J04-4004,0,\N,Missing
W11-1816,W06-2920,0,\N,Missing
W11-1816,W09-1401,1,\N,Missing
W11-1816,P96-1025,0,\N,Missing
W11-1816,D07-1111,1,\N,Missing
W11-1816,P05-1022,0,\N,Missing
W11-1816,P06-1055,0,\N,Missing
W11-1816,P08-1006,1,\N,Missing
W11-1816,W10-3006,0,\N,Missing
W11-1816,P04-1014,0,\N,Missing
W11-1816,W07-2416,0,\N,Missing
W11-1816,C10-1088,1,\N,Missing
W11-1816,W11-1809,0,\N,Missing
W11-1816,W11-1824,0,\N,Missing
W11-1816,W11-1810,0,\N,Missing
W11-1816,W11-1801,1,\N,Missing
W11-1816,W11-1804,1,\N,Missing
W11-2907,W09-1201,0,0.0278561,"Missing"
W11-2907,N04-1013,0,0.190045,"Missing"
W11-2907,W09-1206,0,0.0202207,"nglish. Instead of training a parser based on the obtained LFG resources, Guo used an external PCFG parser to create cstructure trees, and then mapped the c-structure trees into f-structures using their annotation rules (Guo, 2009). Besides of Guo’s work, some researchers worked on joint dependency parsing and semantic role labeling to fulfill Chinese deep parsing (Li et al., 2010; Morante et al., 2009; Gesmundo et al., 2009; Dai et al., 2009; Lluis et al., 2009); other researchers focused on performing semantic role labeling after syntactic parsing (Fung et al., 2007; Sun and Jurafsky, 2004; Bjorkelund et al., 2009; Meza-Ruiz and Riedel, 2009; Zhao et al., 2009). There were also some previous works that focused on building the language resources with lexicalized grammars, but not parsing with these resources. With the hand-crafted conversion rules, Yu et al. (2010) built a Chinese HPSG Treebank semi-automatically from the Penn Chinese Treebank. Guo (2009) also used rules to convert the Penn Chinese Treebank into LFG resources. Moreover, Tse and Curran (2010) built a Chinese CCGbank, which was also automatically induced from the Penn Chinese Treebank. (the person who wrote the book) Figure 12: The syntac"
W11-2907,J94-4001,0,0.317999,"Missing"
W11-2907,P04-1014,0,0.027432,"Missing"
W11-2907,P03-1056,0,0.547035,"arser can be explained in the following way: Given a segmented and pos-tagged input sentence, (1) the supertagger offers all the maybeparsable supertag (i.e. lexical template) sequences with scores to the parser; (2) the feature forest model applies beam threshold on the scored supertag sequences, and then obtains a well-formed HPSG parse tree. 吃/eat 过 了 。 ((Somebody) has eaten (something).) (b) A Chinese sentence with both subject and object pro-drop Figure 1: Examples of pro-drop in Chinese The other significant linguistic property in Chinese is the frequent pro-drop phenomena. For example, Levy and Manning (2003) showed that unlike English, the subject pro-drop (the null realization of uncontrolled pronominal subjects) is widespread in Chinese; this is exemplified in Figure 1 (a). Huang (1989) further provided a detailed analysis to show that subjects as well as objects may drop from finite Chinese sentences (as shown in Figure 1 (b)). 3 3.1 Figure 2 shows a supertag sequence provided by the supertagger for a Chinese sentence, in which the supertag of the word ‘写(wrote)’ indicates a lexical template for the transitive verb with an extracted object. Figure 3 illustrates the HPSG parse tree output from"
W11-2907,W09-1202,0,0.0187412,"ese Treebank. This is the only previous work that had been conducted on Chinese deep parsing based on lexicalized grammars, although many related works had been done on English. Instead of training a parser based on the obtained LFG resources, Guo used an external PCFG parser to create cstructure trees, and then mapped the c-structure trees into f-structures using their annotation rules (Guo, 2009). Besides of Guo’s work, some researchers worked on joint dependency parsing and semantic role labeling to fulfill Chinese deep parsing (Li et al., 2010; Morante et al., 2009; Gesmundo et al., 2009; Dai et al., 2009; Lluis et al., 2009); other researchers focused on performing semantic role labeling after syntactic parsing (Fung et al., 2007; Sun and Jurafsky, 2004; Bjorkelund et al., 2009; Meza-Ruiz and Riedel, 2009; Zhao et al., 2009). There were also some previous works that focused on building the language resources with lexicalized grammars, but not parsing with these resources. With the hand-crafted conversion rules, Yu et al. (2010) built a Chinese HPSG Treebank semi-automatically from the Penn Chinese Treebank. Guo (2009) also used rules to convert the Penn Chinese Treebank into LFG resources. Mo"
W11-2907,W09-1212,0,0.0170433,"is the only previous work that had been conducted on Chinese deep parsing based on lexicalized grammars, although many related works had been done on English. Instead of training a parser based on the obtained LFG resources, Guo used an external PCFG parser to create cstructure trees, and then mapped the c-structure trees into f-structures using their annotation rules (Guo, 2009). Besides of Guo’s work, some researchers worked on joint dependency parsing and semantic role labeling to fulfill Chinese deep parsing (Li et al., 2010; Morante et al., 2009; Gesmundo et al., 2009; Dai et al., 2009; Lluis et al., 2009); other researchers focused on performing semantic role labeling after syntactic parsing (Fung et al., 2007; Sun and Jurafsky, 2004; Bjorkelund et al., 2009; Meza-Ruiz and Riedel, 2009; Zhao et al., 2009). There were also some previous works that focused on building the language resources with lexicalized grammars, but not parsing with these resources. With the hand-crafted conversion rules, Yu et al. (2010) built a Chinese HPSG Treebank semi-automatically from the Penn Chinese Treebank. Guo (2009) also used rules to convert the Penn Chinese Treebank into LFG resources. Moreover, Tse and Curra"
W11-2907,2007.tmi-papers.10,0,0.0165638,"hough many related works had been done on English. Instead of training a parser based on the obtained LFG resources, Guo used an external PCFG parser to create cstructure trees, and then mapped the c-structure trees into f-structures using their annotation rules (Guo, 2009). Besides of Guo’s work, some researchers worked on joint dependency parsing and semantic role labeling to fulfill Chinese deep parsing (Li et al., 2010; Morante et al., 2009; Gesmundo et al., 2009; Dai et al., 2009; Lluis et al., 2009); other researchers focused on performing semantic role labeling after syntactic parsing (Fung et al., 2007; Sun and Jurafsky, 2004; Bjorkelund et al., 2009; Meza-Ruiz and Riedel, 2009; Zhao et al., 2009). There were also some previous works that focused on building the language resources with lexicalized grammars, but not parsing with these resources. With the hand-crafted conversion rules, Yu et al. (2010) built a Chinese HPSG Treebank semi-automatically from the Penn Chinese Treebank. Guo (2009) also used rules to convert the Penn Chinese Treebank into LFG resources. Moreover, Tse and Curran (2010) built a Chinese CCGbank, which was also automatically induced from the Penn Chinese Treebank. (the"
W11-2907,W06-2932,0,0.08923,"Missing"
W11-2907,W09-1213,0,0.0213047,"ng a parser based on the obtained LFG resources, Guo used an external PCFG parser to create cstructure trees, and then mapped the c-structure trees into f-structures using their annotation rules (Guo, 2009). Besides of Guo’s work, some researchers worked on joint dependency parsing and semantic role labeling to fulfill Chinese deep parsing (Li et al., 2010; Morante et al., 2009; Gesmundo et al., 2009; Dai et al., 2009; Lluis et al., 2009); other researchers focused on performing semantic role labeling after syntactic parsing (Fung et al., 2007; Sun and Jurafsky, 2004; Bjorkelund et al., 2009; Meza-Ruiz and Riedel, 2009; Zhao et al., 2009). There were also some previous works that focused on building the language resources with lexicalized grammars, but not parsing with these resources. With the hand-crafted conversion rules, Yu et al. (2010) built a Chinese HPSG Treebank semi-automatically from the Penn Chinese Treebank. Guo (2009) also used rules to convert the Penn Chinese Treebank into LFG resources. Moreover, Tse and Curran (2010) built a Chinese CCGbank, which was also automatically induced from the Penn Chinese Treebank. (the person who wrote the book) Figure 12: The syntactic dependency tree correspo"
W11-2907,N04-1032,0,0.0174061,"works had been done on English. Instead of training a parser based on the obtained LFG resources, Guo used an external PCFG parser to create cstructure trees, and then mapped the c-structure trees into f-structures using their annotation rules (Guo, 2009). Besides of Guo’s work, some researchers worked on joint dependency parsing and semantic role labeling to fulfill Chinese deep parsing (Li et al., 2010; Morante et al., 2009; Gesmundo et al., 2009; Dai et al., 2009; Lluis et al., 2009); other researchers focused on performing semantic role labeling after syntactic parsing (Fung et al., 2007; Sun and Jurafsky, 2004; Bjorkelund et al., 2009; Meza-Ruiz and Riedel, 2009; Zhao et al., 2009). There were also some previous works that focused on building the language resources with lexicalized grammars, but not parsing with these resources. With the hand-crafted conversion rules, Yu et al. (2010) built a Chinese HPSG Treebank semi-automatically from the Penn Chinese Treebank. Guo (2009) also used rules to convert the Penn Chinese Treebank into LFG resources. Moreover, Tse and Curran (2010) built a Chinese CCGbank, which was also automatically induced from the Penn Chinese Treebank. (the person who wrote the bo"
W11-2907,C10-1122,0,0.16879,"et al., 2009); other researchers focused on performing semantic role labeling after syntactic parsing (Fung et al., 2007; Sun and Jurafsky, 2004; Bjorkelund et al., 2009; Meza-Ruiz and Riedel, 2009; Zhao et al., 2009). There were also some previous works that focused on building the language resources with lexicalized grammars, but not parsing with these resources. With the hand-crafted conversion rules, Yu et al. (2010) built a Chinese HPSG Treebank semi-automatically from the Penn Chinese Treebank. Guo (2009) also used rules to convert the Penn Chinese Treebank into LFG resources. Moreover, Tse and Curran (2010) built a Chinese CCGbank, which was also automatically induced from the Penn Chinese Treebank. (the person who wrote the book) Figure 12: The syntactic dependency tree corresponding to Figure 10 (the reason that someone wrote the book) Figure 13: The syntactic dependency tree corresponding to Figure 11 However, since the syntactic analysis does not consider predicate-argument dependencies, such an ambiguity in semantic parsing does not exist in syntactic parsing. For instance, for both the two semantic analyses listed in Figure 10 and Figure 11, the syntactic dependencies are similar, as shown"
W11-2907,J08-1002,1,0.878911,"sentences (as shown in Figure 1 (b)). 3 3.1 Figure 2 shows a supertag sequence provided by the supertagger for a Chinese sentence, in which the supertag of the word ‘写(wrote)’ indicates a lexical template for the transitive verb with an extracted object. Figure 3 illustrates the HPSG parse tree output from the parser with this supertag sequence. Chinese Deep Parsing based on HPSG Parsing Model In this paper, we used an HPSG parser - Enju1, which was successfully applied in English deep parsing, to obtain the deep analysis of Chinese. This HPSG parser uses the feature forest model proposed by Miyao and Tsujii (2008), which is a maximum entropy model that is defined over feature forests, as a parsing disambiguation model. The feature forest model provides a solution to the problem of probabilistic modeling of complex data structures. Moreover, in order to reduce the search space and further increase the parsing efficiency, in this parser, a supertagger (Matsuzaki et al. 2007) is applied before parsing. This supertager provides the maybe-parsable supertag (i.e. lexical template) sequences to the parser. In short, in the HPSG parser, the probability, p(t|w), of producing a parse tree t for a given sentence"
W11-2907,W09-1219,0,0.0532946,"Missing"
W11-2907,W09-1205,0,0.0193571,"uced from the Penn Chinese Treebank. This is the only previous work that had been conducted on Chinese deep parsing based on lexicalized grammars, although many related works had been done on English. Instead of training a parser based on the obtained LFG resources, Guo used an external PCFG parser to create cstructure trees, and then mapped the c-structure trees into f-structures using their annotation rules (Guo, 2009). Besides of Guo’s work, some researchers worked on joint dependency parsing and semantic role labeling to fulfill Chinese deep parsing (Li et al., 2010; Morante et al., 2009; Gesmundo et al., 2009; Dai et al., 2009; Lluis et al., 2009); other researchers focused on performing semantic role labeling after syntactic parsing (Fung et al., 2007; Sun and Jurafsky, 2004; Bjorkelund et al., 2009; Meza-Ruiz and Riedel, 2009; Zhao et al., 2009). There were also some previous works that focused on building the language resources with lexicalized grammars, but not parsing with these resources. With the hand-crafted conversion rules, Yu et al. (2010) built a Chinese HPSG Treebank semi-automatically from the Penn Chinese Treebank. Guo (2009) also used rules to convert the Penn Chinese Treebank into"
W11-2907,C10-2162,1,0.789144,"esearchers worked on joint dependency parsing and semantic role labeling to fulfill Chinese deep parsing (Li et al., 2010; Morante et al., 2009; Gesmundo et al., 2009; Dai et al., 2009; Lluis et al., 2009); other researchers focused on performing semantic role labeling after syntactic parsing (Fung et al., 2007; Sun and Jurafsky, 2004; Bjorkelund et al., 2009; Meza-Ruiz and Riedel, 2009; Zhao et al., 2009). There were also some previous works that focused on building the language resources with lexicalized grammars, but not parsing with these resources. With the hand-crafted conversion rules, Yu et al. (2010) built a Chinese HPSG Treebank semi-automatically from the Penn Chinese Treebank. Guo (2009) also used rules to convert the Penn Chinese Treebank into LFG resources. Moreover, Tse and Curran (2010) built a Chinese CCGbank, which was also automatically induced from the Penn Chinese Treebank. (the person who wrote the book) Figure 12: The syntactic dependency tree corresponding to Figure 10 (the reason that someone wrote the book) Figure 13: The syntactic dependency tree corresponding to Figure 11 However, since the syntactic analysis does not consider predicate-argument dependencies, such an am"
W11-2907,W09-1208,0,0.0161437,"ained LFG resources, Guo used an external PCFG parser to create cstructure trees, and then mapped the c-structure trees into f-structures using their annotation rules (Guo, 2009). Besides of Guo’s work, some researchers worked on joint dependency parsing and semantic role labeling to fulfill Chinese deep parsing (Li et al., 2010; Morante et al., 2009; Gesmundo et al., 2009; Dai et al., 2009; Lluis et al., 2009); other researchers focused on performing semantic role labeling after syntactic parsing (Fung et al., 2007; Sun and Jurafsky, 2004; Bjorkelund et al., 2009; Meza-Ruiz and Riedel, 2009; Zhao et al., 2009). There were also some previous works that focused on building the language resources with lexicalized grammars, but not parsing with these resources. With the hand-crafted conversion rules, Yu et al. (2010) built a Chinese HPSG Treebank semi-automatically from the Penn Chinese Treebank. Guo (2009) also used rules to convert the Penn Chinese Treebank into LFG resources. Moreover, Tse and Curran (2010) built a Chinese CCGbank, which was also automatically induced from the Penn Chinese Treebank. (the person who wrote the book) Figure 12: The syntactic dependency tree corresponding to Figure 10 ("
W11-2907,J93-2004,0,\N,Missing
W11-2907,J08-2001,0,\N,Missing
W11-2907,J05-3003,0,\N,Missing
W11-2907,P10-1113,0,\N,Missing
W11-2907,D07-1096,0,\N,Missing
W12-3806,W11-1828,0,0.0482957,"Missing"
W12-3806,W10-3001,0,0.0645895,"Missing"
W12-3806,W10-3010,0,0.161775,"sk, and although negation and speculation were also considered in three main tasks for the 2011 follow-up event (Kim et al., 2011a), the trend continued, with only two participants addressing the negation/speculation aspects of the task. We are aware of only two studies exploring the relationship between the cue-and-scope and event-based representations: in a manual analysis of scope overlap with tagged events, Vincze et al. (2011) identified a number of issues and mismatches in annotation scope and criteria, which may explain in part the lack of methods combining these two lines of research. Kilicoglu and Bergler (2010) approached the problem from the opposite direction and used an existing EE system to extract cue-and-scope annotations in the CoNLL-2010 Shared Task. In this work, we take a high-level perspective, 48 seeking to bridge the linguistically oriented framework and the more application-oriented event framework to overcome the mismatches demonstrated by Vincze et al. (2011). Specifically, we aim to determine how cue-and-scope recognition systems can be used to produce a state-of-the-art negation/speculation detection system for the EE task. 2 Resources Several existing resources can support the inv"
W12-3806,W11-1827,0,0.0114523,"us used to train the CLiPSNESP system, the GE test set does not, and thus test set results are not expected to be overfit. We noted when performing development set experiments that training machine learning-based methods on the negation/speculation annotations of the event-annotated corpora was problematic due to the sparseness of these flags in the annotation. To address this issue, we merge the training data of the three corpora in all experiments with machine learning methods. 5.2 Baseline methods We use the event analyses created by the UTurku (Bj¨orne and Salakoski, 2011) and UConcordia (Kilicoglu and Bergler, 2011) systems for the BioNLP 2011, the only systems that included negation and speculation analyses. To investigate the impact on a system that did not include a negation/speculation component, we further consider analyses created Negation (R/P/F) EPI GE ID H HR 29.23/31.67/30.40 27.69/32.73/30.00 53.92/52.84/53.38 53.24/71.89/61.18 44.00/31.88/36.97 44.00/37.93/40.74 M ME MC MCE 47.69/20.00/28.18 60.00/66.10/62.90 40.00/74.29/52.00 58.46/73.08/64.96 43.00/25.25/31.82 58.36/70.08/63.69 58.36/76.34/66.15 61.77/83.03/70.84 46.00/26.74/33.82 54.00/69.23/60.67 52.00/61.90/56.52 58.00/70.73/63.74 Table"
W12-3806,W09-1401,1,0.94131,"ion/Speculation Annotations: A Bridge Not Too Far Pontus Stenetorp1 Sampo Pyysalo2,3 Tomoko Ohta2,3 Sophia Ananiadou2,3 and Jun’ichi Tsujii2,3,4 1 Department of Computer Science, University of Tokyo, Tokyo, Japan 2 School of Computer Science, University of Manchester, Manchester, United Kingdom 3 National Centre for Text Mining, University of Manchester, Manchester, United Kingdom 4 Microsoft Research Asia, Beijing, People’s Republic of China {pontus,smp,okap}@is.s.u-tokyo.ac.jp sophia.ananiadou@manchester.ac.uk jtsujii@microsoft.com Abstract some marking of certainty and polarity (LDC, 2005; Kim et al., 2009; Saur and Pustejovsky, 2009; Kim et al., 2011a; Thompson et al., 2011). We study two approaches to the marking of extra-propositional aspects of statements in text: the task-independent cue-and-scope representation considered in the CoNLL-2010 Shared Task, and the tagged-event representation applied in several recent event extraction tasks. Building on shared task resources and the analyses from state-of-the-art systems representing the two broad lines of research, we identify specific points of mismatch between the two perspectives and propose ways of addressing them. We demonstrate the feas"
W12-3806,W11-1801,1,0.855017,"addressing the latter task in detail. Only three out of the 24 participants in the BioNLP Shared Task 2009 submitted results for the non-mandatory negation/speculation task, and although negation and speculation were also considered in three main tasks for the 2011 follow-up event (Kim et al., 2011a), the trend continued, with only two participants addressing the negation/speculation aspects of the task. We are aware of only two studies exploring the relationship between the cue-and-scope and event-based representations: in a manual analysis of scope overlap with tagged events, Vincze et al. (2011) identified a number of issues and mismatches in annotation scope and criteria, which may explain in part the lack of methods combining these two lines of research. Kilicoglu and Bergler (2010) approached the problem from the opposite direction and used an existing EE system to extract cue-and-scope annotations in the CoNLL-2010 Shared Task. In this work, we take a high-level perspective, 48 seeking to bridge the linguistically oriented framework and the more application-oriented event framework to overcome the mismatches demonstrated by Vincze et al. (2011). Specifically, we aim to determine"
W12-3806,W11-1802,0,0.0691394,"Missing"
W12-3806,W11-1806,0,0.142886,"2.43 48.08/51.02/49.50 25.65/10.84/15.24 22.08/42.24/29.00 27.27/50.30/35.37 31.82/53.85/40.00 45.83/10.58/17.19 29.17/28.00/28.57 37.50/31.03/33.96 33.33/42.11/37.21 Table 5: Results for Speculation for our two heuristics and the four combinations of ML features. by the FAUST system, which achieved the highest performance at two of the three tasks considered (Riedel et al., 2011). The UTurku system is a pipeline ML-based EE system, while the UConcordia system is strictly rule-based. FAUST is an ML-based model combination system incorporating information from the parser-based Stanford system (McClosky et al., 2011) and the jointly-modelled UMass system (Riedel and McCallum, 2011). We also performed preliminary experiments for the other released submissions to the BioNLP 2011 Shared Task, but due to space limitations focus only on the three above-mentioned systems. results tables we abbreviate the feature set names as done in Table 3 and use H for the heuristic method and R for its root extension. As our machine learning component we use LIBLINEAR (Fan et al., 2008) with a L2-regularised L2-loss SVM model. We optimise the SVM regularisation parameter C using 10-fold cross-validation on the training data."
W12-3806,W09-1304,0,0.0270192,"icallyoriented and task-oriented perspectives on negation/speculation detection. In this study, we make use of the following resources. First, we study the three BioNLP 2011 Shared Task corpora that include annotation for negation and speculation: the GE, EPI and ID main task corpora (Table 1). Second, we make use of supporting analyses provided for these corpora in response to a call sent by the BioNLP Shared Task organisers to the developers of third-party systems (Stenetorp et al., 2011). Specifically, we use the output of the BiographTA NeSp Scope Labeler (here referred to as CLiPS-NESP) (Morante and Daelemans, 2009; Morante et al., 2010) provided by the University of Antwerp CLiPS center. This system provides cue-and-scope analyses for negation and speculation and was demonstrated to have state-of-the-art performance at the relevant CoNLL-2010 Shared Task. Finally, we make use of the event analyses created by systems that participated in the BioNLP Shared Task, made available to the research community for the majority of the shared task submissions (Pyysalo et al., 2012). These analyses represent the stateof-the-art in event extraction and their capability to detect event structures as well as marking t"
W12-3806,W10-3006,0,0.158592,"Missing"
W12-3806,W11-1803,1,0.674875,"addressing the latter task in detail. Only three out of the 24 participants in the BioNLP Shared Task 2009 submitted results for the non-mandatory negation/speculation task, and although negation and speculation were also considered in three main tasks for the 2011 follow-up event (Kim et al., 2011a), the trend continued, with only two participants addressing the negation/speculation aspects of the task. We are aware of only two studies exploring the relationship between the cue-and-scope and event-based representations: in a manual analysis of scope overlap with tagged events, Vincze et al. (2011) identified a number of issues and mismatches in annotation scope and criteria, which may explain in part the lack of methods combining these two lines of research. Kilicoglu and Bergler (2010) approached the problem from the opposite direction and used an existing EE system to extract cue-and-scope annotations in the CoNLL-2010 Shared Task. In this work, we take a high-level perspective, 48 seeking to bridge the linguistically oriented framework and the more application-oriented event framework to overcome the mismatches demonstrated by Vincze et al. (2011). Specifically, we aim to determine"
W12-3806,W11-1804,1,0.674411,"addressing the latter task in detail. Only three out of the 24 participants in the BioNLP Shared Task 2009 submitted results for the non-mandatory negation/speculation task, and although negation and speculation were also considered in three main tasks for the 2011 follow-up event (Kim et al., 2011a), the trend continued, with only two participants addressing the negation/speculation aspects of the task. We are aware of only two studies exploring the relationship between the cue-and-scope and event-based representations: in a manual analysis of scope overlap with tagged events, Vincze et al. (2011) identified a number of issues and mismatches in annotation scope and criteria, which may explain in part the lack of methods combining these two lines of research. Kilicoglu and Bergler (2010) approached the problem from the opposite direction and used an existing EE system to extract cue-and-scope annotations in the CoNLL-2010 Shared Task. In this work, we take a high-level perspective, 48 seeking to bridge the linguistically oriented framework and the more application-oriented event framework to overcome the mismatches demonstrated by Vincze et al. (2011). Specifically, we aim to determine"
W12-3806,W12-2412,1,0.850965,"(Stenetorp et al., 2011). Specifically, we use the output of the BiographTA NeSp Scope Labeler (here referred to as CLiPS-NESP) (Morante and Daelemans, 2009; Morante et al., 2010) provided by the University of Antwerp CLiPS center. This system provides cue-and-scope analyses for negation and speculation and was demonstrated to have state-of-the-art performance at the relevant CoNLL-2010 Shared Task. Finally, we make use of the event analyses created by systems that participated in the BioNLP Shared Task, made available to the research community for the majority of the shared task submissions (Pyysalo et al., 2012). These analyses represent the stateof-the-art in event extraction and their capability to detect event structures as well as marking them for negation and speculation. The above three resources present us with many opportunities to relate scope-based annotations to three highly relevant event-based corpora containing negation/speculation annotations. 3 Manual Analysis To gain deeper insight into the data and the challenges in combining the cue-and-scope and eventoriented perspectives, we performed a manual analysis of the corpus annotations using the manually Name Negated Events Speculated Ev"
W12-3806,W11-1807,0,0.0123498,"27/50.30/35.37 31.82/53.85/40.00 45.83/10.58/17.19 29.17/28.00/28.57 37.50/31.03/33.96 33.33/42.11/37.21 Table 5: Results for Speculation for our two heuristics and the four combinations of ML features. by the FAUST system, which achieved the highest performance at two of the three tasks considered (Riedel et al., 2011). The UTurku system is a pipeline ML-based EE system, while the UConcordia system is strictly rule-based. FAUST is an ML-based model combination system incorporating information from the parser-based Stanford system (McClosky et al., 2011) and the jointly-modelled UMass system (Riedel and McCallum, 2011). We also performed preliminary experiments for the other released submissions to the BioNLP 2011 Shared Task, but due to space limitations focus only on the three above-mentioned systems. results tables we abbreviate the feature set names as done in Table 3 and use H for the heuristic method and R for its root extension. As our machine learning component we use LIBLINEAR (Fan et al., 2008) with a L2-regularised L2-loss SVM model. We optimise the SVM regularisation parameter C using 10-fold cross-validation on the training data. We use the training, development and test set partition provided"
W12-3806,W11-1808,0,0.183789,"addressing the latter task in detail. Only three out of the 24 participants in the BioNLP Shared Task 2009 submitted results for the non-mandatory negation/speculation task, and although negation and speculation were also considered in three main tasks for the 2011 follow-up event (Kim et al., 2011a), the trend continued, with only two participants addressing the negation/speculation aspects of the task. We are aware of only two studies exploring the relationship between the cue-and-scope and event-based representations: in a manual analysis of scope overlap with tagged events, Vincze et al. (2011) identified a number of issues and mismatches in annotation scope and criteria, which may explain in part the lack of methods combining these two lines of research. Kilicoglu and Bergler (2010) approached the problem from the opposite direction and used an existing EE system to extract cue-and-scope annotations in the CoNLL-2010 Shared Task. In this work, we take a high-level perspective, 48 seeking to bridge the linguistically oriented framework and the more application-oriented event framework to overcome the mismatches demonstrated by Vincze et al. (2011). Specifically, we aim to determine"
W12-3806,W11-1816,1,0.908546,"Missing"
W12-3806,W08-0606,0,0.206797,"ons of text statements have explicitly included Although extra-propositional aspects are recognised as important, there is no clear consensus on how to address their annotation and extraction from text. Some comparatively early efforts focused on the detection of negation cue phrases associated with specific (previously detected) terms through regular expression-based rules (Chapman et al., 2001). A number of later efforts identified the scope of negation cues with phrases in constituency analyses in sentence structure (Huang and Lowe, 2007). Drawing in part on this work, the BioScope corpus (Vincze et al., 2008) applied a representation where both cues and their associated scopes are marked as contiguous spans of text (Figure 1 bottom). This approach was also applied in the CoNLL-2010 Shared Task (Farkas et al., 2010), in which 13 participating groups proposed approaches for Task 2, which required the identification of uncertainty cues and their associated scopes in text. In the following, we will term this task-independent, linguisticallymotivated approach as the cue-and-scope representation (please see Vincze et al. (2008) for details regarding the representation). For IE efforts, more task-oriente"
W12-4304,H92-1045,0,0.108814,"g. For PubMed, we simply selected a random set of citations and extracted their abstract and title texts. For PMC, we initially extracted all non-overlapping section texts (PMC XML <sec> elements) as well as caption texts (<caption> elements), and then selected a random set of extracts. This selection seeks to maximize the diversity of the texts in the full-text section of the corpus, and the selection of extracts larger than isolated sentences aims to allow the corpus to be used to study methods making use of broader context, e.g. by incorporating constraints such as one sense per discourse (Gale et al., 1992). 2 3 We selected a total of 500 documents using this protocol, half from PubMed and half from PMC document extracts. (Descriptive statistics of the abstracts and full-text extracts subcorpora are given later in Table 3.) 2.6 Annotation Process Primary annotation was created by a PhD biologist with extensive experience in domain information extraction and text annotation (TO). The use of any relevant resources, such as the full article being annotated or species-specific anatomy ontologies in the OBO foundry, was encouraged for resolving unclear or ambiguous cases during annotation. Initial an"
W12-4304,W10-1909,0,0.127256,"rehensive analysis must include entities at multiple levels of biological organization, from the molecular to the organism level. The detection of references to anatomical entities such as “kidney” and “blood” is thus required for the automatic structured analysis of biomedical scientific text. Although a wealth of lexical and ontological resources covering anatomical entities are available (Rosse and Mejino, 2003; Smith et al., 2007; Bodenreider, 2004; Haendel et al., 2009), such resources do not alone confer the ability to reliably detect mentions of anatomical entities in natural language (Gerner et al., 2010a; Travillian et al., 2011; Pyysalo et al., 2012b). To support the development and evaluation of reliable anatomical entity mention detection methods, corpus resources annotated specifically for the task are necessary. In this study, we aim to create a reference standard for evaluating methods for anatomical entity mention detection and for training machine learningbased methods for the task. We seek to select a set of texts that are representative of the relevant scientific literature, i.e. open-domain in the sense of avoiding bias toward, for example, specific species, levels of biological o"
W12-4304,W04-1213,0,0.115942,"ue to occupying different levels at different stages of development, we adopt a separate D EVELOPING ANATOMICAL STRUCTURE category, as done also in e.g. Uberon (Haendel et al., 2009). 1 http://obofoundry.org/ 28 Annotation Scope We diverge from the scope of anatomy ontologies in two important aspects in our annotation. First, ontologies of anatomy commonly incorporate everything from molecules to whole organisms within their scope. However, in entity mention detection, many molecular level anatomical entities fall within the scope of the established gene/protein mention detection tasks (e.g. (Kim et al., 2004; Tanabe et al., 2005)), and whole organism mentions similarly largely within what is covered by existing methods and resources for organism mention detection (Gerner et al., 2010b; Naderi et al., 2011). To avoid overlap with established tasks and to focus on the novel aspects of anatomical entity mention detection, we exclude biological macromolecules and mentions of organism names from the scope of our annotation, as argued in (Pyysalo et al., 2012b). Second, these ontologies typically represent canonical anatomy, an idealized state that is rarely (if ever) encountered in reality (Bada and H"
W12-4304,W11-1901,0,0.0257235,"mical entities (e.g. “muscle tissue”). Both names and nominal mentions are annotated similarly, without distinction. We exclude pronouns (it, that) from annotation even when they un29 cytoplasm Tissue Part-of Cell of phagocytic microglia Frag Tissue thyroid and eye muscle membranes Figure 2: Part-of relation marking entity mention spanning a prepositional phrase (above) and Frag relation marking coordination with ellipsis (below). ambiguously refer to an anatomical entity; we consider the identification and resolution of such mentions part of the distinct coreference resolution task (see e.g. Pradhan et al. (2011)). In addition to names and nominal mentions, we mark adjectives that have an unambiguous sense of relating to a specific anatomical entity. Thus, for example, both “kidney” and “renal” (relating to the kidneys) are annotated as O RGAN in expressions such as “kidney failure” and “renal failure”. The choice to annotate adjectival references is motivated by the expected needs of applications making use of automatically detected anatomical entity mentions. For example, for semantic search targeting documents relating to organ failure, a document discussing “renal failure” is obviously relevant an"
W12-4304,E12-2021,1,0.874312,"Missing"
W12-4304,W03-0419,0,0.0245807,"Missing"
W13-2009,P05-1022,0,0.013255,"could provide further insight into the relative strengths and weaknesses of these two systems. Table 6: Primary evaluation results Event Extraction System8 (Bj¨orne et al., 2011) (TEES). The two systems share the same overall architecture, a one-best pipeline with SVMbased stages for event trigger detection, triggerargument relation detection, argument grouping into event structures, and modification prediction. The feature representations of both systems draw on substructures of dependency-like representations of sentence syntax, derived from full parses of input sentences. TEES applies the Charniak and Johnson (2005) parser with the McClosky (2009) biomedical model, converting the phrasestructure parses into dependencies using the Stanford tools (de Marneffe et al., 2006). By contrast, EventMine uses a combination of the predicateargument structure analyses created by the deep parser Enju (Miyao and Tsujii, 2008) and the output of the the GDep best-first shift-reduce dependency parser (Sagae and Tsujii, 2007). All three parsers have models trained in part on the biomedical domain GENIA treebank (Tateisi et al., 2005). Interestingly, both systems make use of the GE task data, but the application of EventMi"
W13-2009,W08-0608,0,0.0128679,"Missing"
W13-2009,W11-1828,0,0.0171244,"Missing"
W13-2009,W11-0214,1,0.596369,"in interactions (Krallinger et al., 2007; Pyysalo et al., 2008; Tikk et al., 2010). However, most such efforts have employed simple representations, such as entity pairs, that are not sufficient for capturing molecular reactions to the level of detail required to support the curation of pathway models. Additionally, previous efforts have not directly involved the semantics (e.g. reaction type definitions) of such models. Perhaps in part due to these reasons, natural language processing and information extraction methods have not been widely embraced by biomedical pathway curation communities (Ohta et al., 2011c; Ohta et al., 2011a). We believe that the extraction of structured event representations (Figure 1) pursued in the BioNLP Shared Tasks offers many opportunities to make significant contributions to support the development, evaluation and maintenance of biomolecular pathways. The Pathway Curation (PC) task, a main task of the BioNLP Shared Task 2013, is proposed as a step toward realizing these opportunities. The PC task aims to evaluate the applicability of event extraction systems to pathway curation and to encourage the further development of methods for related tasks. The design of the ta"
W13-2009,W12-4304,1,0.900591,"Missing"
W13-2009,W11-0215,1,0.854069,"iyao and Tsujii, 2008) and the output of the the GDep best-first shift-reduce dependency parser (Sagae and Tsujii, 2007). All three parsers have models trained in part on the biomedical domain GENIA treebank (Tateisi et al., 2005). Interestingly, both systems make use of the GE task data, but the application of EventMine extends on this considerably by applying a stacked model (Miwa et al., 2013b) with predictions also from models trained on the BioNLP ST 2011 EPI and ID tasks (Pyysalo et al., 2012) as well as from four corpora introduced outside of the shared tasks by Thompson et al. (2011), Pyysalo et al. (2011), Ohta et al. (2011b) and Ohta et al. (2011c). 4.2 5 Although participation in this initial run of the PC task was somewhat limited, the two participating systems have been applied to a large variety of event extraction tasks over the last years and have shown consistently competitive performance with the state of the art (Bj¨orne and Salakoski, 2011; Miwa et al., 2012). It is thus reasonable to assume that the higher performance achieved by the Evaluation results Table 6 summarizes the primary evaluation results. The two systems demonstrate broadly similar performance in terms of F-scores, wi"
W13-2009,D07-1111,1,0.639577,"prediction. The feature representations of both systems draw on substructures of dependency-like representations of sentence syntax, derived from full parses of input sentences. TEES applies the Charniak and Johnson (2005) parser with the McClosky (2009) biomedical model, converting the phrasestructure parses into dependencies using the Stanford tools (de Marneffe et al., 2006). By contrast, EventMine uses a combination of the predicateargument structure analyses created by the deep parser Enju (Miyao and Tsujii, 2008) and the output of the the GDep best-first shift-reduce dependency parser (Sagae and Tsujii, 2007). All three parsers have models trained in part on the biomedical domain GENIA treebank (Tateisi et al., 2005). Interestingly, both systems make use of the GE task data, but the application of EventMine extends on this considerably by applying a stacked model (Miwa et al., 2013b) with predictions also from models trained on the BioNLP ST 2011 EPI and ID tasks (Pyysalo et al., 2012) as well as from four corpora introduced outside of the shared tasks by Thompson et al. (2011), Pyysalo et al. (2011), Ohta et al. (2011b) and Ohta et al. (2011c). 4.2 5 Although participation in this initial run of"
W13-2009,I05-2038,1,0.757182,"s of sentence syntax, derived from full parses of input sentences. TEES applies the Charniak and Johnson (2005) parser with the McClosky (2009) biomedical model, converting the phrasestructure parses into dependencies using the Stanford tools (de Marneffe et al., 2006). By contrast, EventMine uses a combination of the predicateargument structure analyses created by the deep parser Enju (Miyao and Tsujii, 2008) and the output of the the GDep best-first shift-reduce dependency parser (Sagae and Tsujii, 2007). All three parsers have models trained in part on the biomedical domain GENIA treebank (Tateisi et al., 2005). Interestingly, both systems make use of the GE task data, but the application of EventMine extends on this considerably by applying a stacked model (Miwa et al., 2013b) with predictions also from models trained on the BioNLP ST 2011 EPI and ID tasks (Pyysalo et al., 2012) as well as from four corpora introduced outside of the shared tasks by Thompson et al. (2011), Pyysalo et al. (2011), Ohta et al. (2011b) and Ohta et al. (2011c). 4.2 5 Although participation in this initial run of the PC task was somewhat limited, the two participating systems have been applied to a large variety of event"
W13-2009,J08-1002,1,0.790519,"ion, triggerargument relation detection, argument grouping into event structures, and modification prediction. The feature representations of both systems draw on substructures of dependency-like representations of sentence syntax, derived from full parses of input sentences. TEES applies the Charniak and Johnson (2005) parser with the McClosky (2009) biomedical model, converting the phrasestructure parses into dependencies using the Stanford tools (de Marneffe et al., 2006). By contrast, EventMine uses a combination of the predicateargument structure analyses created by the deep parser Enju (Miyao and Tsujii, 2008) and the output of the the GDep best-first shift-reduce dependency parser (Sagae and Tsujii, 2007). All three parsers have models trained in part on the biomedical domain GENIA treebank (Tateisi et al., 2005). Interestingly, both systems make use of the GE task data, but the application of EventMine extends on this considerably by applying a stacked model (Miwa et al., 2013b) with predictions also from models trained on the BioNLP ST 2011 EPI and ID tasks (Pyysalo et al., 2012) as well as from four corpora introduced outside of the shared tasks by Thompson et al. (2011), Pyysalo et al. (2011),"
W13-2009,de-marneffe-etal-2006-generating,0,\N,Missing
W13-2009,W09-1401,1,\N,Missing
W13-2009,E12-2021,1,\N,Missing
W13-2512,P02-1051,0,0.118069,"Missing"
W13-2512,W03-0428,0,0.0102285,"capture. Secondly, RF is considered one of the most accurate classifier available (D´ıaz-Uriarte and De Andres, 2006; Jiang et al., 2007). Finally, RF is reported to cope well with datasets where the number of features is larger than the number of observations (D´ıaz-Uriarte and De Andres, 2006). In our dataset, the number of features is almost four times more than that of the observations. We represent pairs of terms using character gram features (i.e., first order features). Such shallow features have been proven effective in a number of NLP applications including: Named Entity Recognition (Klein et al., 2003), Multilingual Named Entity Transliteration (Klementiev and Roth, 2006; Freitag and Khadivi, 2007) and 2 Related Work In this section, we review previous approaches that exploit the internal structure of sequences to align terms or named entities across languages. (Klementiev and Roth, 2006; Freitag and Khadivi, 2007) use character gram features, similar to the feature space that we propose in this paper, to train discriminative, supervised models. Klementiev and Roth (2006) introduce a supervised Perceptron model for English and Russian named entities. They construct a character gram feature"
W13-2512,J93-2003,0,0.0365567,"nnot directly associate the source with the target character grams. 2 3 98 www2.chkd.cnki.net/kns50/ nlm.nih.gov/research/umls Figure 1: Example of a term construction rule as a branch in a decision tree. Input pair of English-French terms : (e1 , e2 , e3 , f1 , f2 , f3 ) English first order French first order Second order φ1 (e1 , e2 ) φ1 (f1 , f2 ) φ1 (e1 e2 , f1 f2 ), φ1 (e1 e2 , f2 f3 ) φ1 (e2 , e3 ) φ1 (f2 , f3 ) φ1 (e2 e3 , f1 f2 ), φ1 (e2 e3 , f2 f3 ) Table 2: Example of first and second order features using a predefined n-gram size of 2. open source implementation of the 5 IBM-models (Brown et al., 1993). GIZA++ is traditionally trained on a bilingual, parallel corpus of aligned sentences and estimates the probability P (s|t) of a source translation unit (typically a word), s, given a target unit t. To apply GIZA++ on our dataset, we consider the list of terms as parallel sentences. GIZA++, trained on a list of terms, estimates the alignment probability of English-Chinese and English-French textual units, i.e. character ngrams. Each entry i, j in the translation table is the probability P (si |tj ), where si and tj are the source and target character n-grams in row i and column j, respectivel"
W13-2512,P06-1103,0,0.0299983,"ssifier available (D´ıaz-Uriarte and De Andres, 2006; Jiang et al., 2007). Finally, RF is reported to cope well with datasets where the number of features is larger than the number of observations (D´ıaz-Uriarte and De Andres, 2006). In our dataset, the number of features is almost four times more than that of the observations. We represent pairs of terms using character gram features (i.e., first order features). Such shallow features have been proven effective in a number of NLP applications including: Named Entity Recognition (Klein et al., 2003), Multilingual Named Entity Transliteration (Klementiev and Roth, 2006; Freitag and Khadivi, 2007) and 2 Related Work In this section, we review previous approaches that exploit the internal structure of sequences to align terms or named entities across languages. (Klementiev and Roth, 2006; Freitag and Khadivi, 2007) use character gram features, similar to the feature space that we propose in this paper, to train discriminative, supervised models. Klementiev and Roth (2006) introduce a supervised Perceptron model for English and Russian named entities. They construct a character gram feature space as follows: firstly, they extract all distinct character grams f"
W13-2512,W02-0902,0,0.0326977,"erms from comparable corpora would be to directly classify all possible pairs of terms into translations or non-translations. However, in comparable corpora, the size of the search space is quadratic to the input data. Therefore, the classification task is much more challenging since the distribution of positive and negative instances is highly skewed. To cope with the vast search space of comparable corpora, we plan to incorporate context-based approaches with the RF classification method. Context-based approaches, such as distributional vector similarity (Fung and McKeown, 1997; Rapp, 1995; Koehn and Knight, 2002; Haghighi et al., 2008), can be used to limit the number of candidate translations by filtering out pairs of terms with low contextual similarity. Finally, the proposed method can be also used to online augment the phrase table of Statistical Machine Translation (SMT) in order to better handle the Out-of-Vocabulary problem i.e. inability to translate textual units that consist of one or more words and do not occur in the training data (Habash, 2008). creased. Table 3 summarises the highest performance achieved by the RF, SVMs, GIZA++ and Levenshtein distance all trained and tested on the same"
W13-2512,2005.iwslt-1.1,0,0.0371976,"ure space that we propose in this paper, to train discriminative, supervised models. Klementiev and Roth (2006) introduce a supervised Perceptron model for English and Russian named entities. They construct a character gram feature space as follows: firstly, they extract all distinct character grams from both source and target named entity. Then, they pair character grams of the source named entity with character grams of the corresponding target named entity into features. In or96 Lepage and Denoual (2005) presented an analogical learning machine translation system as part of the IWSLT task (Eck and Hori, 2005) that requires no training process and it is able to achieve state-of-the art performance. The core method of their system models relationships between sequences of characters, e.g., sentences, phrases or words, across languages using proportional analogies, i.e., [a : b = c : d], “a is to b as c is to d”, and is able to solve unknown analogical equations, i.e., [x : y = z :?] (Lepage, 1998). Analogical learning has been proven effective in translating unseen words (Langlais and Patry, 2007). Furthermore, analogical learning is reported to achieve a better precision but a lower recall than a p"
W13-2512,P07-2045,0,0.00387293,"hat are transliterated, i.e. present phonetic similarities, across languages. SMT-based approaches built on top of existing SMT frameworks to identify translation pairs of terms (Tsunakawa et al., 2008; Wu et al., 2008). Tsunakawa et al. (2008), align terms between a source language Ls and a target language Lt using a pivot language Lp . They assume that two bilingual dictionaries exist: from Ls to Lp and from Lp to Lt . Then, they train GIZA++ (Och and Ney, 2003) on both directions and they merge the resulting phrase tables into one table between Ls and Lt , using grow-diag-final heuristics (Koehn et al., 2007). Wu et al. (2008), use morphemes instead of words as translation units to train a phrase based SMT system for technical terms in English and Chinese. The use of shorter lexical fragments, e.g. lemmas, stems and suffixes, as translation units has reportedly reduced the Out-Of-Vocabulary problem (Virpioja et al., 2007; Popovic and Ney, 2004; Oflazer and ElKahlout, 2007). Hybrid methods exploit that a term or a named entity can be translated in various ways across languages (Shao and Ng, 2004; Feng et al., 2004; Lu and Zhao, 2006). For instance, person names are usually translated by translitera"
W13-2512,W04-3248,0,0.0319651,"phrase tables into one table between Ls and Lt , using grow-diag-final heuristics (Koehn et al., 2007). Wu et al. (2008), use morphemes instead of words as translation units to train a phrase based SMT system for technical terms in English and Chinese. The use of shorter lexical fragments, e.g. lemmas, stems and suffixes, as translation units has reportedly reduced the Out-Of-Vocabulary problem (Virpioja et al., 2007; Popovic and Ney, 2004; Oflazer and ElKahlout, 2007). Hybrid methods exploit that a term or a named entity can be translated in various ways across languages (Shao and Ng, 2004; Feng et al., 2004; Lu and Zhao, 2006). For instance, person names are usually translated by transliteration (i.e., words exhibiting pronunciation similarities across languages, are likely to be mutual translations) while technical terms are likely to be translated by meaning (i.e., the same semantic units are used to generate the translation of the term in the target language). The resulting hybrid systems were reported to perform at least as well as existing SMT systems (Feng et al., 2004). 3 Methodology Let em = (e1 , · · · , em ) be an English term consisting of m translation units and f n = (f1 , · · · , f"
W13-2512,D07-1025,0,0.381372,"iarte and De Andres, 2006; Jiang et al., 2007). Finally, RF is reported to cope well with datasets where the number of features is larger than the number of observations (D´ıaz-Uriarte and De Andres, 2006). In our dataset, the number of features is almost four times more than that of the observations. We represent pairs of terms using character gram features (i.e., first order features). Such shallow features have been proven effective in a number of NLP applications including: Named Entity Recognition (Klein et al., 2003), Multilingual Named Entity Transliteration (Klementiev and Roth, 2006; Freitag and Khadivi, 2007) and 2 Related Work In this section, we review previous approaches that exploit the internal structure of sequences to align terms or named entities across languages. (Klementiev and Roth, 2006; Freitag and Khadivi, 2007) use character gram features, similar to the feature space that we propose in this paper, to train discriminative, supervised models. Klementiev and Roth (2006) introduce a supervised Perceptron model for English and Russian named entities. They construct a character gram feature space as follows: firstly, they extract all distinct character grams from both source and target n"
W13-2512,D07-1092,0,0.0172441,"age and Denoual (2005) presented an analogical learning machine translation system as part of the IWSLT task (Eck and Hori, 2005) that requires no training process and it is able to achieve state-of-the art performance. The core method of their system models relationships between sequences of characters, e.g., sentences, phrases or words, across languages using proportional analogies, i.e., [a : b = c : d], “a is to b as c is to d”, and is able to solve unknown analogical equations, i.e., [x : y = z :?] (Lepage, 1998). Analogical learning has been proven effective in translating unseen words (Langlais and Patry, 2007). Furthermore, analogical learning is reported to achieve a better precision but a lower recall than a phrasebased machine translation system when translating medical terms (Langlais et al., 2009). der to reduce the number of features, they link only those character grams whose position offsets in the source and target sequence differs by -1, 0 or 1. Freitag and Khadivi (2007) employ the same character gram feature space but they do not constraint the included character-grams to their relative position offsets in the source and target sequence. The boolean features are defined for every distin"
W13-2512,E09-1056,0,0.0190269,"the art performance. The core method of their system models relationships between sequences of characters, e.g., sentences, phrases or words, across languages using proportional analogies, i.e., [a : b = c : d], “a is to b as c is to d”, and is able to solve unknown analogical equations, i.e., [x : y = z :?] (Lepage, 1998). Analogical learning has been proven effective in translating unseen words (Langlais and Patry, 2007). Furthermore, analogical learning is reported to achieve a better precision but a lower recall than a phrasebased machine translation system when translating medical terms (Langlais et al., 2009). der to reduce the number of features, they link only those character grams whose position offsets in the source and target sequence differs by -1, 0 or 1. Freitag and Khadivi (2007) employ the same character gram feature space but they do not constraint the included character-grams to their relative position offsets in the source and target sequence. The boolean features are defined for every distinct character-grams observed in the data of length k or shorter. Using this feature space they train an Averaged Perceptron model, able to incorporate an arbitrary number of features in the input v"
W13-2512,P08-2015,0,0.0219904,"th the RF classification method. Context-based approaches, such as distributional vector similarity (Fung and McKeown, 1997; Rapp, 1995; Koehn and Knight, 2002; Haghighi et al., 2008), can be used to limit the number of candidate translations by filtering out pairs of terms with low contextual similarity. Finally, the proposed method can be also used to online augment the phrase table of Statistical Machine Translation (SMT) in order to better handle the Out-of-Vocabulary problem i.e. inability to translate textual units that consist of one or more words and do not occur in the training data (Habash, 2008). creased. Table 3 summarises the highest performance achieved by the RF, SVMs, GIZA++ and Levenshtein distance all trained and tested on the same dataset. The resulting performance of the RF compared with GIZA++ is statistically significant (p &lt; 0.0001) in all experiments. Comparing the RF with the SVMs, we note that in the EnglishFrench dataset, the performance of the SVM-RBF is approximately the same with the performance of our proposed method. However, this comes with a cost. Firstly, SVMs can possibly achieve a comparable performance to the RF when using multilingual, second order feature"
W13-2512,P98-1120,0,0.0165174,"y with character grams of the corresponding target named entity into features. In or96 Lepage and Denoual (2005) presented an analogical learning machine translation system as part of the IWSLT task (Eck and Hori, 2005) that requires no training process and it is able to achieve state-of-the art performance. The core method of their system models relationships between sequences of characters, e.g., sentences, phrases or words, across languages using proportional analogies, i.e., [a : b = c : d], “a is to b as c is to d”, and is able to solve unknown analogical equations, i.e., [x : y = z :?] (Lepage, 1998). Analogical learning has been proven effective in translating unseen words (Langlais and Patry, 2007). Furthermore, analogical learning is reported to achieve a better precision but a lower recall than a phrasebased machine translation system when translating medical terms (Langlais et al., 2009). der to reduce the number of features, they link only those character grams whose position offsets in the source and target sequence differs by -1, 0 or 1. Freitag and Khadivi (2007) employ the same character gram feature space but they do not constraint the included character-grams to their relative"
W13-2512,P08-1088,0,0.0277,"pora would be to directly classify all possible pairs of terms into translations or non-translations. However, in comparable corpora, the size of the search space is quadratic to the input data. Therefore, the classification task is much more challenging since the distribution of positive and negative instances is highly skewed. To cope with the vast search space of comparable corpora, we plan to incorporate context-based approaches with the RF classification method. Context-based approaches, such as distributional vector similarity (Fung and McKeown, 1997; Rapp, 1995; Koehn and Knight, 2002; Haghighi et al., 2008), can be used to limit the number of candidate translations by filtering out pairs of terms with low contextual similarity. Finally, the proposed method can be also used to online augment the phrase table of Statistical Machine Translation (SMT) in order to better handle the Out-of-Vocabulary problem i.e. inability to translate textual units that consist of one or more words and do not occur in the training data (Habash, 2008). creased. Table 3 summarises the highest performance achieved by the RF, SVMs, GIZA++ and Levenshtein distance all trained and tested on the same dataset. The resulting"
W13-2512,Y06-1018,0,0.0190735,"one table between Ls and Lt , using grow-diag-final heuristics (Koehn et al., 2007). Wu et al. (2008), use morphemes instead of words as translation units to train a phrase based SMT system for technical terms in English and Chinese. The use of shorter lexical fragments, e.g. lemmas, stems and suffixes, as translation units has reportedly reduced the Out-Of-Vocabulary problem (Virpioja et al., 2007; Popovic and Ney, 2004; Oflazer and ElKahlout, 2007). Hybrid methods exploit that a term or a named entity can be translated in various ways across languages (Shao and Ng, 2004; Feng et al., 2004; Lu and Zhao, 2006). For instance, person names are usually translated by transliteration (i.e., words exhibiting pronunciation similarities across languages, are likely to be mutual translations) while technical terms are likely to be translated by meaning (i.e., the same semantic units are used to generate the translation of the term in the target language). The resulting hybrid systems were reported to perform at least as well as existing SMT systems (Feng et al., 2004). 3 Methodology Let em = (e1 , · · · , em ) be an English term consisting of m translation units and f n = (f1 , · · · , fn ) a French or Chin"
W13-2512,2007.mtsummit-papers.65,0,0.0368839,"Missing"
W13-2512,J03-1002,0,0.00851808,"ove character gram based methods mainly focused on aligning named entities of the general domain, i.e. person names, locations, organizations, etc., that are transliterated, i.e. present phonetic similarities, across languages. SMT-based approaches built on top of existing SMT frameworks to identify translation pairs of terms (Tsunakawa et al., 2008; Wu et al., 2008). Tsunakawa et al. (2008), align terms between a source language Ls and a target language Lt using a pivot language Lp . They assume that two bilingual dictionaries exist: from Ls to Lp and from Lp to Lt . Then, they train GIZA++ (Och and Ney, 2003) on both directions and they merge the resulting phrase tables into one table between Ls and Lt , using grow-diag-final heuristics (Koehn et al., 2007). Wu et al. (2008), use morphemes instead of words as translation units to train a phrase based SMT system for technical terms in English and Chinese. The use of shorter lexical fragments, e.g. lemmas, stems and suffixes, as translation units has reportedly reduced the Out-Of-Vocabulary problem (Virpioja et al., 2007; Popovic and Ney, 2004; Oflazer and ElKahlout, 2007). Hybrid methods exploit that a term or a named entity can be translated in va"
W13-2512,2008.amta-papers.19,1,0.858003,"Missing"
W13-2512,W07-0704,0,0.0362204,"Missing"
W13-2512,popovic-ney-2004-towards,0,0.0255953,"They assume that two bilingual dictionaries exist: from Ls to Lp and from Lp to Lt . Then, they train GIZA++ (Och and Ney, 2003) on both directions and they merge the resulting phrase tables into one table between Ls and Lt , using grow-diag-final heuristics (Koehn et al., 2007). Wu et al. (2008), use morphemes instead of words as translation units to train a phrase based SMT system for technical terms in English and Chinese. The use of shorter lexical fragments, e.g. lemmas, stems and suffixes, as translation units has reportedly reduced the Out-Of-Vocabulary problem (Virpioja et al., 2007; Popovic and Ney, 2004; Oflazer and ElKahlout, 2007). Hybrid methods exploit that a term or a named entity can be translated in various ways across languages (Shao and Ng, 2004; Feng et al., 2004; Lu and Zhao, 2006). For instance, person names are usually translated by transliteration (i.e., words exhibiting pronunciation similarities across languages, are likely to be mutual translations) while technical terms are likely to be translated by meaning (i.e., the same semantic units are used to generate the translation of the term in the target language). The resulting hybrid systems were reported to perform at least"
W13-2512,P95-1050,0,0.180709,"onaries of terms from comparable corpora would be to directly classify all possible pairs of terms into translations or non-translations. However, in comparable corpora, the size of the search space is quadratic to the input data. Therefore, the classification task is much more challenging since the distribution of positive and negative instances is highly skewed. To cope with the vast search space of comparable corpora, we plan to incorporate context-based approaches with the RF classification method. Context-based approaches, such as distributional vector similarity (Fung and McKeown, 1997; Rapp, 1995; Koehn and Knight, 2002; Haghighi et al., 2008), can be used to limit the number of candidate translations by filtering out pairs of terms with low contextual similarity. Finally, the proposed method can be also used to online augment the phrase table of Statistical Machine Translation (SMT) in order to better handle the Out-of-Vocabulary problem i.e. inability to translate textual units that consist of one or more words and do not occur in the training data (Habash, 2008). creased. Table 3 summarises the highest performance achieved by the RF, SVMs, GIZA++ and Levenshtein distance all traine"
W13-2512,C04-1089,0,0.0282872,"merge the resulting phrase tables into one table between Ls and Lt , using grow-diag-final heuristics (Koehn et al., 2007). Wu et al. (2008), use morphemes instead of words as translation units to train a phrase based SMT system for technical terms in English and Chinese. The use of shorter lexical fragments, e.g. lemmas, stems and suffixes, as translation units has reportedly reduced the Out-Of-Vocabulary problem (Virpioja et al., 2007; Popovic and Ney, 2004; Oflazer and ElKahlout, 2007). Hybrid methods exploit that a term or a named entity can be translated in various ways across languages (Shao and Ng, 2004; Feng et al., 2004; Lu and Zhao, 2006). For instance, person names are usually translated by transliteration (i.e., words exhibiting pronunciation similarities across languages, are likely to be mutual translations) while technical terms are likely to be translated by meaning (i.e., the same semantic units are used to generate the translation of the term in the target language). The resulting hybrid systems were reported to perform at least as well as existing SMT systems (Feng et al., 2004). 3 Methodology Let em = (e1 , · · · , em ) be an English term consisting of m translation units and f"
W13-2512,W03-1719,0,0.0259876,"of the 6th Workshop on Building and Using Comparable Corpora, pages 95–104, c Sofia, Bulgaria, August 8, 2013. 2013 Association for Computational Linguistics English Morpheme: -ache head-ache back-ache ear-ache Chinese Morpheme: 痛 头-痛 痛 腰-痛 痛 耳朵-痛 痛 French Morpheme: -mal mal de tˆete mal au dos mal d’oreille Table 1: An example of English, Chinese and French terms consisting of the same morphemes predicting authorship (Stamatatos, 2006). In addition, by selecting character n-grams instead of word n-grams, one avoids to segment words in Chinese which has been proven to be a challenging topic (Sproat and Emerson, 2003). We evaluate our proposed method on two datasets of biomedical terms (English-French and English-Chinese) that contain equal numbers of positive and negative instances. RF achieves higher classification performance than baseline methods. To boost SVM’s performance further, we used a second order feature space to represent the data. It consists of pairs of character grams that co-occur in translation pairs. In the second order feature space, the performance of SVMs improved significantly. The rest of the paper is structured as follows. In Section 2, we present previous approaches in identifyin"
W13-2512,tsunakawa-etal-2008-building-bilingual,1,0.923421,"fined for every distinct character-grams observed in the data of length k or shorter. Using this feature space they train an Averaged Perceptron model, able to incorporate an arbitrary number of features in the input vectors, for English and Arabic named entities. The above character gram based methods mainly focused on aligning named entities of the general domain, i.e. person names, locations, organizations, etc., that are transliterated, i.e. present phonetic similarities, across languages. SMT-based approaches built on top of existing SMT frameworks to identify translation pairs of terms (Tsunakawa et al., 2008; Wu et al., 2008). Tsunakawa et al. (2008), align terms between a source language Ls and a target language Lt using a pivot language Lp . They assume that two bilingual dictionaries exist: from Ls to Lp and from Lp to Lt . Then, they train GIZA++ (Och and Ney, 2003) on both directions and they merge the resulting phrase tables into one table between Ls and Lt , using grow-diag-final heuristics (Koehn et al., 2007). Wu et al. (2008), use morphemes instead of words as translation units to train a phrase based SMT system for technical terms in English and Chinese. The use of shorter lexical frag"
W13-2512,W04-3211,0,\N,Missing
W13-2512,W04-3242,0,\N,Missing
W13-2512,C98-1116,0,\N,Missing
W13-2512,W03-1726,0,\N,Missing
W13-4403,C02-1126,0,0.0866221,"Missing"
W13-4403,A00-2018,0,0.0149568,"Missing"
W13-4403,D09-1127,0,0.077969,"Missing"
W13-4403,J93-2004,0,0.0424905,"Missing"
W13-4403,P03-1056,0,0.085125,"Missing"
W13-4403,W12-6332,1,0.808052,"Missing"
W13-4403,W09-3825,0,0.0151052,"tion Penn Treebank (PTB) was built based on the idea of context-free PSG (Marcus et al., 1993). It is now a common practice to develop data-driven English parsers using PTB annotation and encouraging performances have been reported (Collins, 2000; Charniak, 2000). Following the success of PTB, Xue et al. 2000 built Penn Chinese Treebank (CTB). CTB is also based on context-free PSG. Since CTB provides training data for Chinese parsing, researchers attempted to train Chinese parsing with CTB (Bikel and Chiang, 2000; Chiang and Bikel, 2002; Levy and Manning, 2003; Bikel, 2004; Wang et al., 2006; Zhang and Clark, 2009; 1a. Students process data 1b. Data processing system 1c. Data was processed 2a. 学生 处理 数据 Student process data Students process data 2b. 数据 处理 系统 Data process system Data processing system 11 Proceedings of the Seventh SIGHAN Workshop on Chinese Language Processing (SIGHAN-7), pages 11–19, Nagoya, Japan, 14 October 2013. 2c. 数据 处理 了 Data process le Data was processed 4a. 鸟儿 飞 向 南方 Bird fly towards south Birds fly towards the south 4b. *鸟儿 吃 向 南方 Bird eat towards south Birds eat towards the south 4c. *鸟儿 喜欢 向 南方 Bird like towards south Birds like towards the south 5a. Agent Direction V 5b. Age"
W13-4403,W00-1201,0,\N,Missing
W13-4403,P06-1054,0,\N,Missing
W98-0127,P87-1033,0,0.0223494,"Missing"
W98-0127,P98-2132,1,0.604697,"ssigned to a word, and many constituents are produced during parsing. The experimental results show that our method leads to a significant speed-up. The results also suggest the possihility of optimizing the XTAG system by introducing packing offeature structures and packing of tree structures, although these operations are not currently so apparent. 2 The XHPSG System This section describes the current status of the XHPSG system and the efficiency problem in the system. Both of the grammar and the parser in the XHPSG system are implemented with feature structure description language, LiLFeS (Makino et al., 1998). The grammar consists of lexical entries for about 317 ,000 words, and 10 schemata, which follows schemata of the &apos;This work is partially founded by Japan Society for the Promotion of Science (JSPS-RFTF96P00502). 104 twfica5on ol Pad<ed FllSllK9 Sltvetvn&quot; CKYTable Orir;lnLl XllPSG SJ&gt;lcm SJ&gt;ltm w!lh lhe Pacldo& Moowo Figure 1: Data flow in the parsers for the X11PSG system. HPSG framework in (Pollard and Sag, 199~) with slight modifications. The parser is a simpltCKY-based parser. Currently, the parsing speed of this system is not satisfactory, and we need further impro&apos;e· ment of the parsin"
W98-0127,W98-0141,1,0.86765,"Missing"
W98-0127,C96-2160,1,0.340345,"Missing"
W98-0141,P98-2132,1,0.831682,"Missing"
W98-0141,W98-0127,1,0.868547,"Missing"
W98-0141,C96-2160,1,0.844226,"Missing"
W98-0141,C98-2128,1,\N,Missing
wang-etal-2012-biomedical,W06-0127,0,\N,Missing
wang-etal-2012-biomedical,W00-1313,0,\N,Missing
Y09-2040,W04-1217,0,0.0124046,"Include the example in the section of related guidelines. Create a link to the annotation in the annotation layer from the guideline. Figure 5 shows an example of feature vector generation. When the cursor stops at the word “Noreen” in the text “to comment on Mr. Noreen’s claims,”Pemberton said. ,” the system produce the feature vector from the target word, “Noreen” and its surrounding words. We considered the preceding and following three words. Three features are extracted from each word: word form, word shape, and part-of-speech (POS). These features work well for named entity recognition (Finkel et al., 2004). For word shape, we considered 6 types: aa (all lower-case letters), AA (all capital letters), aA (mixed, begins with a lower-case letter), Aa (mixed, begins with a capical letter), Num (all numerical letters) and Num a (mixed with numerical and alphabet letters). 6 Evaluation The purpose of this work is to provide an integrated framework for corpus annotation and guideline production, and to improve the accessibility between an annotated corpus and guidelines. In this section, we present two evaluations. The first evaluation is for the advantage of the proposed framework in the actual annota"
Y09-2040,W04-3111,0,0.0245017,"ented annotation guidelines are indispensable tools for the proper use of Penn Treebank. The annotaiton policy of the SUSANNE corpus (Sampson, 2002) is published as a part of a book. Despite the importance of annotation guidelines, the process of guideline production has not been studied extensively. Even some of the latest annotation projects relied on traditional ways of communication and documentation for guideline production. For example the Caderige project (Alphonse et al., 2004) used e-mails for communication between annotators, and the archive became database of guidelines. PennBioIE (Kulick et al., 2004) repeatdly updated a web page dedicated to documentation of guidelines. GENIA made use of a Wiki system.2 Although adopting web-based documentation enhanced the guideline production and utilization in sharing and searching, it is difficult to conclude that the guideline production process is well integrated with the annotation process. On the other hand, we produce the guideline using the examples from annotated corpus, and we annotate the corpus using the annotation guideline. The annotator must very often switch between the annotation system and guideline management system during annotation"
Y09-2040,J93-2004,0,0.0315193,"ple, MMAX is designed for multi-level annotation. Knowtator puts its focus on ontology-based annotation. GATE is a language engineering infrastructure. Both WordFreak and XConc Suite focus on flexibility of the format of corpus and annotation. As far as the authors know, however, there is no tool supporting guideline production in an integrated way. 2.2 Annotation guidelines and their production Although only few studies on guideline production exist, researchers have long recognized the importance of documenting the annotation policy. One of the most popular annotated corpora, Penn Treebank (Marcus et al., 1993), is also well known for the comprehensive documentation of its annotation policy. Its well documented annotation guidelines are indispensable tools for the proper use of Penn Treebank. The annotaiton policy of the SUSANNE corpus (Sampson, 2002) is published as a part of a book. Despite the importance of annotation guidelines, the process of guideline production has not been studied extensively. Even some of the latest annotation projects relied on traditional ways of communication and documentation for guideline production. For example the Caderige project (Alphonse et al., 2004) used e-mails"
Y09-2040,N03-4009,0,0.0339052,"ropose a framework in which the association between the guidelines and the corpus is important, and support the accessibility between the guideline and the corpus. In addition, we can systemically integrate the management of the annotation guideline into the annotation process in the proposed framework. We present GuideLink, an implementation of the annotation framework, which is integrated with the existing annotation tool, XConc Suite.1 2 Related works 2.1 Tools for corpus annotation Many software tools have been developed for supporting corpus annotation. Well-known ones include WordFreak (Morton and LaCivita, 2003), MMAX (Mueller and Strube, 2001), Knowtator (Ogren, 2006), GATE (Cunningham et al., 2002) and XConc Suite. While they are all widely used, each has its own strength. For example, MMAX is designed for multi-level annotation. Knowtator puts its focus on ontology-based annotation. GATE is a language engineering infrastructure. Both WordFreak and XConc Suite focus on flexibility of the format of corpus and annotation. As far as the authors know, however, there is no tool supporting guideline production in an integrated way. 2.2 Annotation guidelines and their production Although only few studies"
Y09-2040,W04-1207,0,\N,Missing
Y09-2048,W02-1502,0,0.136567,"; these two structures involve dislocation of phrases from their basic positions which these six schemas require. These structures are covered by our Chinese HPSG framework, and we will introduce the details in the next section. 3 The Design of Chinese HPSG Framework The formalized framework HPSG uses a small number of rule schemas and a large number of lexical entries to describe language. Our basic policy of Chinese HPSG is to exploit rule schemas defined for English with minimum changes. Although a possible solution would be to create an initial grammar with the help of the Grammar Matrix (Bender et al., 2002), we refer to the rule schemas used in an existing HPSG parser (Miyao, 2006), because we intend to apply the technology of this parser to our Chinese parser. This does not only reduce the cost of development of Chinese grammar but also confirm the assumption that, despite surface diversity, human languages share the same organization principles. For example, we do not introduce new rule schemas specific to Chinese unless they are absolutely necessary. We generalize Chinese syntactic structures into five structures based on the Chinese syntactic structure system that we proposed in the previous"
Y09-2048,P05-1022,0,0.0135203,"PSG framework. As the first step of our work, we design a Chinese HPSG framework, which can be used as the basis for a practical parser. In this paper, 1) we present a Chinese syntactic structure system and 2) we design a primary Chinese HPSG framework. Keywords: HPSG, data-driven parsing, Chinese HPSG framework, coverage, consistency. 1 Introduction Data-driven parsing has been proven to be the most effective approach to development of a practical parser. It can deliver a parser with broad-coverage and high-accuracy. Some English data-driven syntactic parsers have been developed in the past (Charniak and Johnson, 2005; McDonald and Pereira, 2006; Miyao and Tsujii, 2005). Following the success of the research on English data-driven parsing, the same methodology has been applied to Chinese parsing (Levy and Manning, 2003; Wang et al., 2005; Guo et al., 2007). The goal of our research is to develop a data-driven Chinese parser that is based on Head-driven Phrase Structure Grammar (HPSG) (Sag et al., 2003). Since an English data-driven parser based on the HPSG framework has been developed by our group (Miyao and Tsujii, 2005), we follow the same methodology for developing a Chinese parser. We first convert an"
Y09-2048,P03-1056,0,0.0293544,"we design a primary Chinese HPSG framework. Keywords: HPSG, data-driven parsing, Chinese HPSG framework, coverage, consistency. 1 Introduction Data-driven parsing has been proven to be the most effective approach to development of a practical parser. It can deliver a parser with broad-coverage and high-accuracy. Some English data-driven syntactic parsers have been developed in the past (Charniak and Johnson, 2005; McDonald and Pereira, 2006; Miyao and Tsujii, 2005). Following the success of the research on English data-driven parsing, the same methodology has been applied to Chinese parsing (Levy and Manning, 2003; Wang et al., 2005; Guo et al., 2007). The goal of our research is to develop a data-driven Chinese parser that is based on Head-driven Phrase Structure Grammar (HPSG) (Sag et al., 2003). Since an English data-driven parser based on the HPSG framework has been developed by our group (Miyao and Tsujii, 2005), we follow the same methodology for developing a Chinese parser. We first convert an existing Chinese treebank into an HPSG treebank, based on which we can obtain a large lexicon and a statistical model for choosing the most plausible interpretation. Since the HPSG framework for English ha"
Y09-2048,E06-1011,0,0.0317725,"step of our work, we design a Chinese HPSG framework, which can be used as the basis for a practical parser. In this paper, 1) we present a Chinese syntactic structure system and 2) we design a primary Chinese HPSG framework. Keywords: HPSG, data-driven parsing, Chinese HPSG framework, coverage, consistency. 1 Introduction Data-driven parsing has been proven to be the most effective approach to development of a practical parser. It can deliver a parser with broad-coverage and high-accuracy. Some English data-driven syntactic parsers have been developed in the past (Charniak and Johnson, 2005; McDonald and Pereira, 2006; Miyao and Tsujii, 2005). Following the success of the research on English data-driven parsing, the same methodology has been applied to Chinese parsing (Levy and Manning, 2003; Wang et al., 2005; Guo et al., 2007). The goal of our research is to develop a data-driven Chinese parser that is based on Head-driven Phrase Structure Grammar (HPSG) (Sag et al., 2003). Since an English data-driven parser based on the HPSG framework has been developed by our group (Miyao and Tsujii, 2005), we follow the same methodology for developing a Chinese parser. We first convert an existing Chinese treebank in"
Y09-2048,P05-1011,1,0.743126,"a Chinese HPSG framework, which can be used as the basis for a practical parser. In this paper, 1) we present a Chinese syntactic structure system and 2) we design a primary Chinese HPSG framework. Keywords: HPSG, data-driven parsing, Chinese HPSG framework, coverage, consistency. 1 Introduction Data-driven parsing has been proven to be the most effective approach to development of a practical parser. It can deliver a parser with broad-coverage and high-accuracy. Some English data-driven syntactic parsers have been developed in the past (Charniak and Johnson, 2005; McDonald and Pereira, 2006; Miyao and Tsujii, 2005). Following the success of the research on English data-driven parsing, the same methodology has been applied to Chinese parsing (Levy and Manning, 2003; Wang et al., 2005; Guo et al., 2007). The goal of our research is to develop a data-driven Chinese parser that is based on Head-driven Phrase Structure Grammar (HPSG) (Sag et al., 2003). Since an English data-driven parser based on the HPSG framework has been developed by our group (Miyao and Tsujii, 2005), we follow the same methodology for developing a Chinese parser. We first convert an existing Chinese treebank into an HPSG treebank, base"
Y09-2048,W05-1516,0,0.0188136,"nese HPSG framework. Keywords: HPSG, data-driven parsing, Chinese HPSG framework, coverage, consistency. 1 Introduction Data-driven parsing has been proven to be the most effective approach to development of a practical parser. It can deliver a parser with broad-coverage and high-accuracy. Some English data-driven syntactic parsers have been developed in the past (Charniak and Johnson, 2005; McDonald and Pereira, 2006; Miyao and Tsujii, 2005). Following the success of the research on English data-driven parsing, the same methodology has been applied to Chinese parsing (Levy and Manning, 2003; Wang et al., 2005; Guo et al., 2007). The goal of our research is to develop a data-driven Chinese parser that is based on Head-driven Phrase Structure Grammar (HPSG) (Sag et al., 2003). Since an English data-driven parser based on the HPSG framework has been developed by our group (Miyao and Tsujii, 2005), we follow the same methodology for developing a Chinese parser. We first convert an existing Chinese treebank into an HPSG treebank, based on which we can obtain a large lexicon and a statistical model for choosing the most plausible interpretation. Since the HPSG framework for English has been studied comp"
Y10-1055,C04-1180,0,0.198739,"conversion program can fill this gap, and consequently we achieve a wide-coverage system for translating unrestricted natural language texts into predicate logic formulas. 481 482 Poster Papers A notable advantage of the modular architecture is that system development is comparatively easy. As we prove in the experiments, a compact grammar of SCT and a small program for parsed syntactic structure conversion can achieve high coverage on real-world texts. This attests the portability of our method to other languages, as well as to other forms of semantic representation. To our knowledge, Boxer (Bos et al., 2004; Curran et al., 2007) is the only alternative to our system that translates unrestricted texts into logical formulas (Discourse Representation Structures) with resolved intra and inter argument dependencies and anaphoric dependencies. We discuss the relationship with Boxer in Section 6 and note some added benefits of our approach. 2 Scope Control Theory Scope Control Theory or SCT (Butler, 2010) is a system of semantic evaluation that attempts to approximate the dependency restrictions of natural language by a fine-grained and restricted management of binding dependencies, following insights"
Y10-1055,P05-1022,0,0.0465809,"Missing"
Y10-1055,P04-1014,0,0.0364764,"Missing"
Y10-1055,P07-2009,0,0.143156,"can fill this gap, and consequently we achieve a wide-coverage system for translating unrestricted natural language texts into predicate logic formulas. 481 482 Poster Papers A notable advantage of the modular architecture is that system development is comparatively easy. As we prove in the experiments, a compact grammar of SCT and a small program for parsed syntactic structure conversion can achieve high coverage on real-world texts. This attests the portability of our method to other languages, as well as to other forms of semantic representation. To our knowledge, Boxer (Bos et al., 2004; Curran et al., 2007) is the only alternative to our system that translates unrestricted texts into logical formulas (Discourse Representation Structures) with resolved intra and inter argument dependencies and anaphoric dependencies. We discuss the relationship with Boxer in Section 6 and note some added benefits of our approach. 2 Scope Control Theory Scope Control Theory or SCT (Butler, 2010) is a system of semantic evaluation that attempts to approximate the dependency restrictions of natural language by a fine-grained and restricted management of binding dependencies, following insights from static reformulat"
Y10-1055,E06-1011,0,0.0490435,"Missing"
Y10-1055,J08-1002,1,0.920856,"at is extra to a conventional parsed form. Notably the co-indexing of syntactic constituents is rendered unnecessary. Moreover sentence information and discourse information are dealt with equally at the time of evaluation as providing binding information, which allows for their seamless integration. The language formalism of SCT is supplemented by a compact grammar to determine the contribution of morphosyntactic information. This offers flexibility in what is acceptable as input, so there is no need to develop a SCT-specific parser from scratch. Here we adopt an HPSG-based syntactic parser (Miyao and Tsujii, 2008), because it offers wide coverage and high accuracy, and provides detailed syntactic information that is sufficient for SCT. Although the output of the parser is not directly compatible with the input assumed by SCT, a small conversion program can fill this gap, and consequently we achieve a wide-coverage system for translating unrestricted natural language texts into predicate logic formulas. 481 482 Poster Papers A notable advantage of the modular architecture is that system development is comparatively easy. As we prove in the experiments, a compact grammar of SCT and a small program for pa"
Y10-1055,P05-1013,0,0.0922836,"Missing"
Y10-1055,J93-2004,0,\N,Missing
