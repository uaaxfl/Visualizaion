2011.mtsummit-papers.28,P05-1033,0,0.676099,"atures many of which have led to the improved performance for Hiero. However, it seems that all the syntactic constituent features cannot efficiently work together in the Hiero optimized by MERT. In this paper, we propose a more general soft syntactic constraint model based on discriminative classifiers for each constituent type and integrate all of them into the translation model with a unified form. The experimental results show that our method significantly improves the performance on the NIST05 Chinese-toEnglish translation task. 1 Introduction Hierarchical phrase-based translation model (Chiang, 2005) is a compromise of two popular translation models: syntax based model and phrase based model. It is a formal syntax grammar(Chiang, 2005; Wu, 1997), which does not take linguistic analysis into account when compared with other pure syntax systems (Liu et al., 2006;Yamada and Knight, 2001;Galley et al., 2006). It promises to improve the performance by adding syntax information to phrase based as well as formal syntax based translation models. Chiang (2005) first introduced syntax knowledge into the hierarchical phrase based model. To make the model sensitive to the syntax structure, a constitu"
2011.mtsummit-papers.28,N09-1025,0,0.0381264,"ffort to improve performance for hierarchical phrase-based machine translation by employing linguistic knowledge. Some of the work which is closely related with ours is reviewed in this section. As presented previously, our work generalizes heavily from (Marton and Resnik, 2008). Besides exploring the soft syntactic constraints on hierarchical phrase model, ours investigates a way to make all the SSC models work together efficiently. (Stein et al., 2010) focuses on the syntactic constraint not only via the constituent parse but also via the dependency parse tree of source or target sentence. (Chiang et al., 2009; Chiang, 2010) similarly define many syntactic features including both source and target sides but integrated them into translation model by MIRA algorithm to optimize their weights. Their work proposes the heuristic syntactic features, while ours employ the discriminative syntactic models. Zollmann and Venugopal (2006) use a constituent parse tree of target to provide constraints on the synchronous rules. They refine 258 the translation grammar with the syntactic constituent types, while ours integrates syntactic knowledge as a sub-model. The idea to design the labels of our SSC models is bo"
2011.mtsummit-papers.28,P10-1146,0,0.084852,"ormance for hierarchical phrase-based machine translation by employing linguistic knowledge. Some of the work which is closely related with ours is reviewed in this section. As presented previously, our work generalizes heavily from (Marton and Resnik, 2008). Besides exploring the soft syntactic constraints on hierarchical phrase model, ours investigates a way to make all the SSC models work together efficiently. (Stein et al., 2010) focuses on the syntactic constraint not only via the constituent parse but also via the dependency parse tree of source or target sentence. (Chiang et al., 2009; Chiang, 2010) similarly define many syntactic features including both source and target sides but integrated them into translation model by MIRA algorithm to optimize their weights. Their work proposes the heuristic syntactic features, while ours employ the discriminative syntactic models. Zollmann and Venugopal (2006) use a constituent parse tree of target to provide constraints on the synchronous rules. They refine 258 the translation grammar with the syntactic constituent types, while ours integrates syntactic knowledge as a sub-model. The idea to design the labels of our SSC models is bought from their"
2011.mtsummit-papers.28,P10-2002,1,0.708011,"der a prior distribution of all GCLs. We define the following feature: Ԅୗୗେ ሺሻ ൌ ൫൫ ሺሻ൯  כୋେሺ୰ሻ ሺ ൌ ͳȁሺ ሺሻ൯ሺͳͲሻǡ where ൫ ሺሻ൯ is a prior probability for GCLs and it isestimated by M.L.E. in the training examples. The ୗୗେ ሺሻ can be described as follows: ୗୗେ ሺሻ ൌ  ൫ ሺሻ൯  כୋେሺ୰ሻ ሺ ൌ ͳȁ൫ ሺሻ൯ሺͳͳሻǤ ୰אୢ It is a Bayes-style combination. 5 5.1 Training SSCModels Training instances Unlike training models for ordinary classification tasks, our training instances are not available obviously because they are latent and by-product for machine translation. Cui et al. (2010) presented an efficient method to acquire training instances for rule selection model. Different from their method, ours is based on the derivations of the source sentence. Bilingual parsing (Wu, 1997) is a very efficient way to get the latent derivation for source language. In order to speed up the bilingual parsing, we limit the phrase table for each source sentence in the training data. When running bilingual parser for each source sentence, we just use the rules extracted from it and its reference during rule extraction step. Although our method will prune some derivations which can derive"
2011.mtsummit-papers.28,P06-1121,0,0.0605476,"s for each constituent type and integrate all of them into the translation model with a unified form. The experimental results show that our method significantly improves the performance on the NIST05 Chinese-toEnglish translation task. 1 Introduction Hierarchical phrase-based translation model (Chiang, 2005) is a compromise of two popular translation models: syntax based model and phrase based model. It is a formal syntax grammar(Chiang, 2005; Wu, 1997), which does not take linguistic analysis into account when compared with other pure syntax systems (Liu et al., 2006;Yamada and Knight, 2001;Galley et al., 2006). It promises to improve the performance by adding syntax information to phrase based as well as formal syntax based translation models. Chiang (2005) first introduced syntax knowledge into the hierarchical phrase based model. To make the model sensitive to the syntax structure, a constituent feature was integrated into the translation model with the soft constraint method. It was defined as follows: it gains 1 for rules whose source side respect syntactic phrase boundary in the parse tree, and 0 otherwise. However, it did not achieve statistically significant improvement in the experiment. Ma"
2011.mtsummit-papers.28,C08-1041,0,0.470529,"Missing"
2011.mtsummit-papers.28,D10-1014,0,0.0218127,"rly define many syntactic features including both source and target sides but integrated them into translation model by MIRA algorithm to optimize their weights. Their work proposes the heuristic syntactic features, while ours employ the discriminative syntactic models. Zollmann and Venugopal (2006) use a constituent parse tree of target to provide constraints on the synchronous rules. They refine 258 the translation grammar with the syntactic constituent types, while ours integrates syntactic knowledge as a sub-model. The idea to design the labels of our SSC models is bought from their work. Huang et al. (2010) decorate the syntax structure into the non-terminal in hierarchical rules as a feature vector. During decoding time, they calculate the similarity between the syntax of the source side and the rules used to derive translations, and then they add the similarity measure to translation model as an additional feature. Their work differs from ours in that they don’t directly use the syntax knowledge to calculate the additional feature score, but use it to derive a latent syntactic distribution. He et al.(2008) and Cui et al.(2010) employ the syntax knowledge as some of features to construct rule s"
2011.mtsummit-papers.28,N03-1017,0,0.0579851,"Missing"
2011.mtsummit-papers.28,W04-3250,0,0.525542,"Missing"
2011.mtsummit-papers.28,P06-1077,0,0.0457671,"model based on discriminative classifiers for each constituent type and integrate all of them into the translation model with a unified form. The experimental results show that our method significantly improves the performance on the NIST05 Chinese-toEnglish translation task. 1 Introduction Hierarchical phrase-based translation model (Chiang, 2005) is a compromise of two popular translation models: syntax based model and phrase based model. It is a formal syntax grammar(Chiang, 2005; Wu, 1997), which does not take linguistic analysis into account when compared with other pure syntax systems (Liu et al., 2006;Yamada and Knight, 2001;Galley et al., 2006). It promises to improve the performance by adding syntax information to phrase based as well as formal syntax based translation models. Chiang (2005) first introduced syntax knowledge into the hierarchical phrase based model. To make the model sensitive to the syntax structure, a constituent feature was integrated into the translation model with the soft constraint method. It was defined as follows: it gains 1 for rules whose source side respect syntactic phrase boundary in the parse tree, and 0 otherwise. However, it did not achieve statistically"
2011.mtsummit-papers.28,P00-1056,0,0.0727833,"L Size GCL Size to train our SSC models; IP 0.09M LIP/R 2.98M x Syntactic features, which are the general VP 0.59M LVP 0.36M constituent labels defined in section 2.1 for NP 0.95M LVP/R 2.31M the spans of r and the nonterminal symbols LIP 1.15M NP/R 0.62M in the source side. IP/R 0.94M LNP/R 1.06M x Parts-of-speech (POS) features, which are the POS of the words immediately to Table 1ˊThe distribution of the training examples for the left and right of ߙ and those of the partial general constituent labels. boundary words covered by the nonterminal symbols in the source side. We run GIZA++ (Och and Ney, 2000) on the training corpus in both directions (Koehn et al., x Length features, which are the length of sub-phrases covered by the nonterminal 2003) to obtain the word alignment for each sentence pair. Then, we employ Stanford parser symbols in the source side. (Klein and Manning, 2003) to generate the parse In fact, our models can be extended to include tree for the source side of the data. We acquire other features, especially those in the target side. In about 15.85M training examples among which are order to compare our models with the work of 6.81M positive and 9.04M negative examples Marton"
2011.mtsummit-papers.28,P02-1038,0,0.159763,"ǡୣሻ ୰אୢ ሺǡ ሻ denotes the set of synchronous derivation trees with (f,e) as their leaves, and d is a derivation member in ሺǡ ሻ . In decoding step, it is intractable to find the extract solution as (1), since the number of elements in ሺǡ ሻ is exponential. In fact, one can approximate the optimal solution via MAP: ො ൌ  ቀୢאୈሺሻ ሺሻቁሺ͵ሻ  ሺሻ denotes the derivation set which can induce f in the source side and ሺሻ denotes the target translation corresponding to derivation d. In hierarchical phrase translation, the rule probability can be represented as the log-linear (Och and Ney, 2002) combination of some feature functions: ሺሻ ൌ ς୧ Ԅ୧  ሺሻ ሺͶሻ, ǡ Ԅ୧ is a feature function, and ɉ୧ is its feature weight and it can be optimized via MERT (Och, 2003) on the development set. The features (Koehn et al.,2003; Chiang, 2005) can be taken as following: x the phrase translation probability ሺȽȁɀሻǡ ሺɀȁȽሻ; x the lexical weights  ሺȽȁɀሻǡ  ሺɀȁȽሻ; x a penalty for hierarchical rules; x a penalty for glue rules; x a word penalty; x language model. The decoding process is similar as the monolingual CKY parse and it can be considered as the transduction of source language into t"
2011.mtsummit-papers.28,P03-1021,0,0.0301372,"extract solution as (1), since the number of elements in ሺǡ ሻ is exponential. In fact, one can approximate the optimal solution via MAP: ො ൌ  ቀୢאୈሺሻ ሺሻቁሺ͵ሻ  ሺሻ denotes the derivation set which can induce f in the source side and ሺሻ denotes the target translation corresponding to derivation d. In hierarchical phrase translation, the rule probability can be represented as the log-linear (Och and Ney, 2002) combination of some feature functions: ሺሻ ൌ ς୧ Ԅ୧  ሺሻ ሺͶሻ, ǡ Ԅ୧ is a feature function, and ɉ୧ is its feature weight and it can be optimized via MERT (Och, 2003) on the development set. The features (Koehn et al.,2003; Chiang, 2005) can be taken as following: x the phrase translation probability ሺȽȁɀሻǡ ሺɀȁȽሻ; x the lexical weights  ሺȽȁɀሻǡ  ሺɀȁȽሻ; x a penalty for hierarchical rules; x a penalty for glue rules; x a word penalty; x language model. The decoding process is similar as the monolingual CKY parse and it can be considered as the transduction of source language into target language. 3 3.1 Discriminative Soft Syntactic Constraint Models Soft Syntactic Constraints For different syntactic categories (e.g. NP), Marton and Resnik (2008) defined"
2011.mtsummit-papers.28,2010.amta-papers.8,0,0.0320368,"ty BLEU-4 27.70 28.35* Table 4ˊThe translation effect of unifying methods for MaxEnt based SSC models on MT NIST05 test set. 7 Related Work There has been much effort to improve performance for hierarchical phrase-based machine translation by employing linguistic knowledge. Some of the work which is closely related with ours is reviewed in this section. As presented previously, our work generalizes heavily from (Marton and Resnik, 2008). Besides exploring the soft syntactic constraints on hierarchical phrase model, ours investigates a way to make all the SSC models work together efficiently. (Stein et al., 2010) focuses on the syntactic constraint not only via the constituent parse but also via the dependency parse tree of source or target sentence. (Chiang et al., 2009; Chiang, 2010) similarly define many syntactic features including both source and target sides but integrated them into translation model by MIRA algorithm to optimize their weights. Their work proposes the heuristic syntactic features, while ours employ the discriminative syntactic models. Zollmann and Venugopal (2006) use a constituent parse tree of target to provide constraints on the synchronous rules. They refine 258 the translat"
2011.mtsummit-papers.28,J97-3002,0,0.0733194,"rk together in the Hiero optimized by MERT. In this paper, we propose a more general soft syntactic constraint model based on discriminative classifiers for each constituent type and integrate all of them into the translation model with a unified form. The experimental results show that our method significantly improves the performance on the NIST05 Chinese-toEnglish translation task. 1 Introduction Hierarchical phrase-based translation model (Chiang, 2005) is a compromise of two popular translation models: syntax based model and phrase based model. It is a formal syntax grammar(Chiang, 2005; Wu, 1997), which does not take linguistic analysis into account when compared with other pure syntax systems (Liu et al., 2006;Yamada and Knight, 2001;Galley et al., 2006). It promises to improve the performance by adding syntax information to phrase based as well as formal syntax based translation models. Chiang (2005) first introduced syntax knowledge into the hierarchical phrase based model. To make the model sensitive to the syntax structure, a constituent feature was integrated into the translation model with the soft constraint method. It was defined as follows: it gains 1 for rules whose source"
2011.mtsummit-papers.28,P06-1066,0,0.125386,"se constituent type models can work together efficiently. Although (Marton and Resnik 2008) did not give the experiments to support the positive answer, as presented previously, Chiang (2005) provided the evidence that their constituent models could not work together. (Chiang et al.,2008) thought one of its reasons is the limitation of MERT(Och,2003) with many features. We think there are two other reasons in addition to their suggestion. On the one hand, their models are heuristic, and they are not sensitive for other features such as boundary word information. However, in the previous work (Xiong et al., 2006), these features were shown to be helpful for the translation model. On the other hand, uniform combination of all the constituent models may cause a model bias, since some constituent types happen more often than others. In this paper, we will address the question above. First, a discriminative soft constraint model is proposed for each syntactic constituent type. The model can be integrated with much context information. We consider several classifiers with different accuracy to construct soft constraint models, and our aim is to study the effect of the 253 accuracy of the classifiers on the"
2011.mtsummit-papers.28,P09-1036,0,0.0295341,"Missing"
2011.mtsummit-papers.28,P01-1067,0,0.075868,"iscriminative classifiers for each constituent type and integrate all of them into the translation model with a unified form. The experimental results show that our method significantly improves the performance on the NIST05 Chinese-toEnglish translation task. 1 Introduction Hierarchical phrase-based translation model (Chiang, 2005) is a compromise of two popular translation models: syntax based model and phrase based model. It is a formal syntax grammar(Chiang, 2005; Wu, 1997), which does not take linguistic analysis into account when compared with other pure syntax systems (Liu et al., 2006;Yamada and Knight, 2001;Galley et al., 2006). It promises to improve the performance by adding syntax information to phrase based as well as formal syntax based translation models. Chiang (2005) first introduced syntax knowledge into the hierarchical phrase based model. To make the model sensitive to the syntax structure, a constituent feature was integrated into the translation model with the soft constraint method. It was defined as follows: it gains 1 for rules whose source side respect syntactic phrase boundary in the parse tree, and 0 otherwise. However, it did not achieve statistically significant improvement"
2011.mtsummit-papers.28,W06-3119,0,0.0594281,"Missing"
2020.aacl-main.1,W16-3415,0,0.0144565,"ed approach, Gonz´alez-Rubio et al. (2016); Cheng et al. (2016) introduce interaction methods that allow users to correct errors at arbitrary position in a machine hypothesis, while Weng et al. (2019) also preventing repeat mistakes by memorizing revision actions. Hokamp and Liu (2017) propose grid beam search to incorporate lexical constraints like words and phrases provided by human translators and force the constraints to appear in hypothesis. Recently, some researchers resort to more flexible interactions, which only require mouse click or touch actions. For example, Marie and Max (2015); Domingo et al. (2016) propose interactive translation methods which ask user to select correct or incorrect segments of a translation with mouse only. Similar to our work, Grangier and Auli (2018) propose a mouse based interactive method which allows users to simply mark the incorrect words in draft machine hypotheses and expect the system to generate refined translations. Herbig et al. (2019, 2020) propose a multi-modal interface for post-editors which takes pen, touch, and speech modalities into consideration. The protocol that given an initial translation to generate a refined translation, is also used in polis"
2020.aacl-main.1,J09-1002,0,0.748515,"Missing"
2020.aacl-main.1,2012.amta-papers.22,0,0.0154104,"tic post-editing (APE) task (Lagarda et al., 2009; Pal et al., 2016). The idea of multi-source encoder is also widely used in the field of APE research (Chatterjee et al., 2018, 2019). In human-machine interaction scenarios, the human feedback is used as extra information in polishing process. Related Work Post-editing is a pragmatic method that allows human translators to directly correct errors in draft machine translations (Simard et al., 2007). Comparing to purely manual translation, it achieves higher productivity while maintaining the human translation quality (Plitt and Masselot, 2010; Federico et al., 2012). Many notable works introduce different levels of human-machine interactions in post-editing. Barrachina et al. (2009) propose a prefix-based interactive method which enable users to correct the first translation error from left to right in each iteration. 6 Conclusion In this paper, we propose Touch Editing, a flexible and effective interaction approach which allows human translators to revise machine translation results via touch actions. The actions we introduce can be provided with gestures like tapping, panning, swiping or long pressing on touch screens to represent human editing intenti"
2020.aacl-main.1,D18-1048,0,0.0181626,"o select correct or incorrect segments of a translation with mouse only. Similar to our work, Grangier and Auli (2018) propose a mouse based interactive method which allows users to simply mark the incorrect words in draft machine hypotheses and expect the system to generate refined translations. Herbig et al. (2019, 2020) propose a multi-modal interface for post-editors which takes pen, touch, and speech modalities into consideration. The protocol that given an initial translation to generate a refined translation, is also used in polishing mechanism in machine translation (Xia et al., 2017; Geng et al., 2018) and automatic post-editing (APE) task (Lagarda et al., 2009; Pal et al., 2016). The idea of multi-source encoder is also widely used in the field of APE research (Chatterjee et al., 2018, 2019). In human-machine interaction scenarios, the human feedback is used as extra information in polishing process. Related Work Post-editing is a pragmatic method that allows human translators to directly correct errors in draft machine translations (Simard et al., 2007). Comparing to purely manual translation, it achieves higher productivity while maintaining the human translation quality (Plitt and Masse"
2020.aacl-main.1,K16-1020,0,0.0429466,"Missing"
2020.aacl-main.1,N18-1025,0,0.191393,"ssing at a particular position, and our method is expected to insert the correct word. To this end, we present a neural network model by augmenting Transformer (Vaswani et al., 2017) with an extra encoder for a hypothesis and its actions. Since it is impractical to manually annotate large-scale action dataset to train the model, we thereby adopt the algorithm of TER (Snover et al., 2006) to automatically extract actions from a draft hypothesis and its reference. To evaluate our method, we conduct simulated experiments on translation datasets the same as in other works (Denkowski et al., 2014; Grangier and Auli, 2018), The results demonstrate that our method can address the well-known challenging issues in machine translation including overWe propose a touch-based editing method for translation, which is more flexible than traditional keyboard-mouse-based translation postediting. This approach relies on touch actions that users perform to indicate translation errors. We present a dual-encoder model to handle the actions and generate refined translations. To mimic the user feedback, we adopt the TER algorithm comparing between draft translations and references to automatically extract the simulated actions"
2020.aacl-main.1,2014.iwslt-evaluation.1,0,0.0223119,"Missing"
2020.aacl-main.1,D14-1130,0,0.186457,"We observe that the users with Touch Editing tends to correct an error for multiple times when the system cannot predict a word they want, while the users with keyboard input tends to modify more content of initial translation and spend more time on choosing words. We then conduct an unstructured interview on the usability of our method. The result of the interview shows that Touch Editing is convenient and intuitive but lack of ability of generating final accurate translation. It can be treated as a light-weight proofreading method, and suitable for Pre-Post-Editing (Marie and Max, 2015). 5 Green et al. (2014) implement a prefix-based interactive translation system and Huang et al. (2015) adopt the prefix constrained translation candidates into a novel input method for translators. Peris et al. (2017) further extend this idea to neural machine translation. The prefix-based protocol is inflexible since users have to follow the left-to-right order. To overcome the weakness of prefix-based approach, Gonz´alez-Rubio et al. (2016); Cheng et al. (2016) introduce interaction methods that allow users to correct errors at arbitrary position in a machine hypothesis, while Weng et al. (2019) also preventing r"
2020.aacl-main.1,W19-5402,0,0.0371464,"Missing"
2020.aacl-main.1,2020.acl-main.155,0,0.324106,"Missing"
2020.aacl-main.1,P16-2046,0,0.0266837,"Missing"
2020.aacl-main.1,P17-1141,0,0.285801,"to further prove the robustness and effectiveness of our method. 1 Introduction Neural machine translation (NMT) has made great success during the past few years (Sutskever et al., 2014; Bahdanau et al., 2014; Wu et al., 2016; Vaswani et al., 2017), but automatic machine translation is still far from perfect and cannot meet the strict requirements of users in real applications (Petrushkov et al., 2018). Many notable humanmachine interaction approaches have been proposed for allowing professional translators to improve machine translation results (Wuebker et al., 2016; Knowles and Koehn, 2016; Hokamp and Liu, 2017). As an instance of such approaches, post-editing directly requires translators to modify outputs from machine translation (Simard et al., 2007). However, traditional post-editing requires intensive keyboard interaction, which is inconvenient on mobile devices. Grangier and Auli (2018) suggest a one-time interaction approach with lightweight editing ef1 Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing, pages 1–11 c December 4 - 7, 2020. 2020 Association for C"
2020.aacl-main.1,P18-2052,0,0.0167696,"our method significantly improves original translation of Transformer (up to 25.31 BLEU) and outperforms existing interactive translation methods (up to 16.64 BLEU). We also conduct experiments on post-editing dataset to further prove the robustness and effectiveness of our method. 1 Introduction Neural machine translation (NMT) has made great success during the past few years (Sutskever et al., 2014; Bahdanau et al., 2014; Wu et al., 2016; Vaswani et al., 2017), but automatic machine translation is still far from perfect and cannot meet the strict requirements of users in real applications (Petrushkov et al., 2018). Many notable humanmachine interaction approaches have been proposed for allowing professional translators to improve machine translation results (Wuebker et al., 2016; Knowles and Koehn, 2016; Hokamp and Liu, 2017). As an instance of such approaches, post-editing directly requires translators to modify outputs from machine translation (Simard et al., 2007). However, traditional post-editing requires intensive keyboard interaction, which is inconvenient on mobile devices. Grangier and Auli (2018) suggest a one-time interaction approach with lightweight editing ef1 Proceedings of the 1st Confe"
2020.aacl-main.1,D10-1092,0,0.0679881,"Missing"
2020.aacl-main.1,2016.amta-researchers.9,0,0.152262,"on post-editing dataset to further prove the robustness and effectiveness of our method. 1 Introduction Neural machine translation (NMT) has made great success during the past few years (Sutskever et al., 2014; Bahdanau et al., 2014; Wu et al., 2016; Vaswani et al., 2017), but automatic machine translation is still far from perfect and cannot meet the strict requirements of users in real applications (Petrushkov et al., 2018). Many notable humanmachine interaction approaches have been proposed for allowing professional translators to improve machine translation results (Wuebker et al., 2016; Knowles and Koehn, 2016; Hokamp and Liu, 2017). As an instance of such approaches, post-editing directly requires translators to modify outputs from machine translation (Simard et al., 2007). However, traditional post-editing requires intensive keyboard interaction, which is inconvenient on mobile devices. Grangier and Auli (2018) suggest a one-time interaction approach with lightweight editing ef1 Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing, pages 1–11 c December 4 - 7, 2020."
2020.aacl-main.1,N09-2055,0,0.0869955,"Missing"
2020.aacl-main.1,2006.amta-papers.25,0,0.553195,"d by users through various of gestures on touch screen devices. By using these actions, our method is able to capture the editing intention from users to generate better translations: for instance, INSERTION indicates a word is missing at a particular position, and our method is expected to insert the correct word. To this end, we present a neural network model by augmenting Transformer (Vaswani et al., 2017) with an extra encoder for a hypothesis and its actions. Since it is impractical to manually annotate large-scale action dataset to train the model, we thereby adopt the algorithm of TER (Snover et al., 2006) to automatically extract actions from a draft hypothesis and its reference. To evaluate our method, we conduct simulated experiments on translation datasets the same as in other works (Denkowski et al., 2014; Grangier and Auli, 2018), The results demonstrate that our method can address the well-known challenging issues in machine translation including overWe propose a touch-based editing method for translation, which is more flexible than traditional keyboard-mouse-based translation postediting. This approach relies on touch actions that users perform to indicate translation errors. We presen"
2020.aacl-main.1,D15-1166,0,0.0602627,"9.47 56.47 0.48 0.39 0.37 0.24 32.53 41.20 38.96 57.84 0.55 0.43 0.42 0.28 21.89 29.78 29.17 45.67 0.61 0.51 0.51 0.33 Model WMT’17 EN-ZH ZH-EN BLEU TER BLEU TER Table 1: Results of different systems measured in BLEU and TER. † denotes the results from Quick Edit. QuickEdit‡ is our reimplementation based on Transformer. Touch baseline is the result modified from initial hypothesis by deleting and reordering words. Touch Editing is our model trained with all actions described in Section 2.1. tains 4698 sentence pairs. For WMT’14 EnglishGerman dataset, we use the same data and preprocessing as (Luong et al., 2015). The dataset consists of 4.5M sentence pairs for training1 . We take newstest2013 for validation and newstest2014 for testing. For Chinese to English dataset, we use CWMT portion which is a subset of WMT’17 training data containing 9M sentence pairs. We validate on newsdev2017 and test on newstest2017. As for vocabulary, the English and German datasets are encoded using byte-pair encoding (Sennrich et al., 2015) with a shared vocabulary of 8k tokens for IWSLT’14 and 32k tokens for WMT’14. For Chinese to English dataset, the English vocabulary is set to 30k subwords, while the Chinese data is"
2020.aacl-main.1,D18-1458,0,0.0281639,"Missing"
2020.aacl-main.1,D15-1120,0,0.124853,"or TED talks (IWSLT). However in real world, data may be from any other domains. For model inconsistency, we use Transformer to build our training data while the translation model used 4.5 Discussion on Real Scenarios So far, the experiments we conducted are based on simulated human feedbacks, in which the actions are extracted from initial machine translation results and their corresponding references to simulate human editing actions. Thus in our simulated setting, the references are used in inference phase to simulate human behavior, as in other interaction methods (Denkowski et al., 2014; Marie and Max, 2015; Grangier and Auli, 2018). These experiments show that our method can significantly 7 improve the initial translation with similated actions. However, whether the actions are convenient to perform is a key point in real applications. To investigate the usability and applicable scenarios of our method, we implement a real mobile application on iPhone, in which the actions can be performed on multi-touch screens. For a given source sentence, the application provides an initial machine translation. The text area of translation can response to several gestures 4 : Tap indicated a missing word sho"
2020.aacl-main.1,W18-6304,0,0.0371598,"Missing"
2020.aacl-main.1,W18-6471,0,0.0192956,"- (3) Where P E∗ denote the action positional embedding matrixes in Figure 3. The learned action positional embedding is used in hypothesis encoder to replace the fixed sinusoids positional encoding in Transformer encoder. Next, the encoder adds the word embedding w and the action positional embedding p to obtain input embedding e = {w1 + p1 , · · · , wl + pl }. The following part of hypothesis encoder lies the same as Transformer encoder. P (yn |y<n , x, m(y0 ), a; θ). (2) n=1 As shown in Figure 2, the neural network model we developed is a dual encoder model based on Transformer similar to Tebbifakhr et al. (2018). Specifically, besides encoding the source sentence x with source encoder (the left part of Figure 2), our model additionally encodes A(y0 ) with an extra hypothesis encoder (the right part of Figure 2) and integrates the encoded representations into decoding network using dual multi-head attention. Decoding The output of hypothesis encoder, together with the output of source encoder, are fed into the decoder. To combine both of the encoders’ outputs, we apply dual multi-head attention in each layer of decoder: the attention sub-layer attends to both encoders’ outputs by performing multi-head"
2020.aacl-main.1,P16-1008,0,0.0205888,"ng, jjzhang, cqzong}@nlpr.ia.ac.cn, {redmondliu, donkeyhuang}@tencent.com Abstract forts, QuickEdit, in which users are asked to simply mark incorrect words in a translation hypothesis for one time in the hope that the system will change them. QuickEdit delivers appealing improvements on draft hypotheses while maintaining the flexibility of human-machine interaction. Unfortunately, only marking incorrect words is far from adequate: for example, it does not indicate the missing information beyond the original hypothesis, which is a typical issue called under-translation in machine translation (Tu et al., 2016). In this paper, we propose a novel one-time interaction method called Touch Editing, which is flexible for users and more adequate for a system to generate better translations. Inspired by human editing process, the proposed method relies on a series of touch-based actions including SUBSTITU TION , DELETION , INSERTION and REORDERING . These actions do not include lexical information and thus can be flexibly provided by users through various of gestures on touch screen devices. By using these actions, our method is able to capture the editing intention from users to generate better translatio"
2020.aacl-main.1,P16-1007,0,0.142257,"so conduct experiments on post-editing dataset to further prove the robustness and effectiveness of our method. 1 Introduction Neural machine translation (NMT) has made great success during the past few years (Sutskever et al., 2014; Bahdanau et al., 2014; Wu et al., 2016; Vaswani et al., 2017), but automatic machine translation is still far from perfect and cannot meet the strict requirements of users in real applications (Petrushkov et al., 2018). Many notable humanmachine interaction approaches have been proposed for allowing professional translators to improve machine translation results (Wuebker et al., 2016; Knowles and Koehn, 2016; Hokamp and Liu, 2017). As an instance of such approaches, post-editing directly requires translators to modify outputs from machine translation (Simard et al., 2007). However, traditional post-editing requires intensive keyboard interaction, which is inconvenient on mobile devices. Grangier and Auli (2018) suggest a one-time interaction approach with lightweight editing ef1 Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing, pages 1–1"
2020.acl-main.35,D16-1011,0,\N,Missing
2020.acl-main.35,D16-1249,0,\N,Missing
2020.acl-main.35,P17-1106,0,\N,Missing
2020.acl-main.35,N19-4009,0,\N,Missing
2020.acl-main.35,P19-1124,1,\N,Missing
2020.acl-main.35,W19-5201,0,\N,Missing
2020.acl-main.35,W19-4805,0,\N,Missing
2020.acl-main.35,N16-1082,0,\N,Missing
2020.acl-main.757,P18-1163,0,0.0362862,"Missing"
2020.acl-main.757,P05-1033,0,0.158897,"the learning of the gates with supervision automatically generated using pointwise mutual information. Extensive experiments on 4 translation datasets demonstrate that the proposed model obtains an averaged gain of 1.0 BLEU score over a strong Transformer baseline. 1 Attention h si ti Introduction An essence to modeling translation is how to learn an effective context from a sentence pair. Statistical machine translation (SMT) models the source context from the source-side of a translation model and models the target context from a target-side language model (Koehn et al., 2003; Koehn, 2009; Chiang, 2005). These two models are trained independently. On the contrary, neural machine translation (NMT) advocates a unified manner to jointly learn source and target context using an encoderdecoder framework with an attention mechanism, leading to substantial gains over SMT in translation quality (Sutskever et al., 2014; Bahdanau et al., 2014; Gehring et al., 2017; Vaswani et al., 2017). Prior work on attention mechanism (Luong et al., 2015; Liu et al., 2016; Mi et al., 2016; Chen et al., 2018; Li et al., 2018; Elbayad et al., 2018; Yang et al., 2020) have shown a better context representation is help"
2020.acl-main.757,K18-1010,0,0.0154869,"t context from a target-side language model (Koehn et al., 2003; Koehn, 2009; Chiang, 2005). These two models are trained independently. On the contrary, neural machine translation (NMT) advocates a unified manner to jointly learn source and target context using an encoderdecoder framework with an attention mechanism, leading to substantial gains over SMT in translation quality (Sutskever et al., 2014; Bahdanau et al., 2014; Gehring et al., 2017; Vaswani et al., 2017). Prior work on attention mechanism (Luong et al., 2015; Liu et al., 2016; Mi et al., 2016; Chen et al., 2018; Li et al., 2018; Elbayad et al., 2018; Yang et al., 2020) have shown a better context representation is helpful to translation performance. zi 1 − zi + Transformer: I often play golf with my colleagues . Context Gates: I often play golf with my colleagues . Regularized : Context Gates I often play soccer with my colleagues . Figure 1: A running example to raise the context control problem. Both original and context gated Transformer obtain an unfaithful translation by wrongly translate “ti¯ q´ıu” into “play golf” because referring too much target context. By regularizing the context gates, the purposed method corrects the transla"
2020.acl-main.757,P09-5002,0,0.0325421,"thod to guide the learning of the gates with supervision automatically generated using pointwise mutual information. Extensive experiments on 4 translation datasets demonstrate that the proposed model obtains an averaged gain of 1.0 BLEU score over a strong Transformer baseline. 1 Attention h si ti Introduction An essence to modeling translation is how to learn an effective context from a sentence pair. Statistical machine translation (SMT) models the source context from the source-side of a translation model and models the target context from a target-side language model (Koehn et al., 2003; Koehn, 2009; Chiang, 2005). These two models are trained independently. On the contrary, neural machine translation (NMT) advocates a unified manner to jointly learn source and target context using an encoderdecoder framework with an attention mechanism, leading to substantial gains over SMT in translation quality (Sutskever et al., 2014; Bahdanau et al., 2014; Gehring et al., 2017; Vaswani et al., 2017). Prior work on attention mechanism (Luong et al., 2015; Liu et al., 2016; Mi et al., 2016; Chen et al., 2018; Li et al., 2018; Elbayad et al., 2018; Yang et al., 2020) have shown a better context represe"
2020.acl-main.757,N03-1017,0,0.0987422,"a regularization method to guide the learning of the gates with supervision automatically generated using pointwise mutual information. Extensive experiments on 4 translation datasets demonstrate that the proposed model obtains an averaged gain of 1.0 BLEU score over a strong Transformer baseline. 1 Attention h si ti Introduction An essence to modeling translation is how to learn an effective context from a sentence pair. Statistical machine translation (SMT) models the source context from the source-side of a translation model and models the target context from a target-side language model (Koehn et al., 2003; Koehn, 2009; Chiang, 2005). These two models are trained independently. On the contrary, neural machine translation (NMT) advocates a unified manner to jointly learn source and target context using an encoderdecoder framework with an attention mechanism, leading to substantial gains over SMT in translation quality (Sutskever et al., 2014; Bahdanau et al., 2014; Gehring et al., 2017; Vaswani et al., 2017). Prior work on attention mechanism (Luong et al., 2015; Liu et al., 2016; Mi et al., 2016; Chen et al., 2018; Li et al., 2018; Elbayad et al., 2018; Yang et al., 2020) have shown a better co"
2020.acl-main.757,D15-1166,0,0.0472561,"models the source context from the source-side of a translation model and models the target context from a target-side language model (Koehn et al., 2003; Koehn, 2009; Chiang, 2005). These two models are trained independently. On the contrary, neural machine translation (NMT) advocates a unified manner to jointly learn source and target context using an encoderdecoder framework with an attention mechanism, leading to substantial gains over SMT in translation quality (Sutskever et al., 2014; Bahdanau et al., 2014; Gehring et al., 2017; Vaswani et al., 2017). Prior work on attention mechanism (Luong et al., 2015; Liu et al., 2016; Mi et al., 2016; Chen et al., 2018; Li et al., 2018; Elbayad et al., 2018; Yang et al., 2020) have shown a better context representation is helpful to translation performance. zi 1 − zi + Transformer: I often play golf with my colleagues . Context Gates: I often play golf with my colleagues . Regularized : Context Gates I often play soccer with my colleagues . Figure 1: A running example to raise the context control problem. Both original and context gated Transformer obtain an unfaithful translation by wrongly translate “ti¯ q´ıu” into “play golf” because referring too muc"
2020.acl-main.757,P18-2053,0,0.0354561,"Missing"
2020.acl-main.757,D16-1249,0,0.0143158,"urce-side of a translation model and models the target context from a target-side language model (Koehn et al., 2003; Koehn, 2009; Chiang, 2005). These two models are trained independently. On the contrary, neural machine translation (NMT) advocates a unified manner to jointly learn source and target context using an encoderdecoder framework with an attention mechanism, leading to substantial gains over SMT in translation quality (Sutskever et al., 2014; Bahdanau et al., 2014; Gehring et al., 2017; Vaswani et al., 2017). Prior work on attention mechanism (Luong et al., 2015; Liu et al., 2016; Mi et al., 2016; Chen et al., 2018; Li et al., 2018; Elbayad et al., 2018; Yang et al., 2020) have shown a better context representation is helpful to translation performance. zi 1 − zi + Transformer: I often play golf with my colleagues . Context Gates: I often play golf with my colleagues . Regularized : Context Gates I often play soccer with my colleagues . Figure 1: A running example to raise the context control problem. Both original and context gated Transformer obtain an unfaithful translation by wrongly translate “ti¯ q´ıu” into “play golf” because referring too much target context. By regularizing t"
2020.acl-main.757,P02-1040,0,0.107896,"gy to measure the correlation between yi and a sentence (x or y<i ). Indeed, it is similar to use the average strategy, but we did not find its gains over max in our experiments. 3 Experiments The proposed methods are evaluated on NIST ZH⇒EN 3 , WMT14 EN⇒DE 4 , IWSLT14 DE⇒EN 5 and IWSLT17 FR⇒EN 6 tasks. To make our NMT models capable of open-vocabulary translation, all datasets are preprocessed with Byte Pair Encoding (Sennrich et al., 2015). All proposed methods are implemented on top of Transformer (Vaswani et al., 2017) which is the state-of-the-art NMT system. Case-insensitive BLEU score (Papineni et al., 2002) is used to evaluate translation quality of ZH⇒EN, DE⇒EN and FR⇒EN. For the fair comparison with the related work, EN⇒DE is evaluated with case-sensitive BLEU score. Setup details are described in Appendix A. 3.1 Tuning Regularization Coefficient In the beginning of our experiments, we tune the regularization coefficient λ on the DE⇒EN task. Table 2 shows the robustness of λ, because the translation performance only fluctuates slightly over various λ. In particular, the best performance 3 where C (µ) and C (ν) are word counts, C (µ, ν) is the co-occurrence count of words µ and ν, and Z is the"
2020.acl-main.757,Q17-1007,0,0.46272,"slation by wrongly translate “ti¯ q´ıu” into “play golf” because referring too much target context. By regularizing the context gates, the purposed method corrects the translation of “ti¯ q´ıu” into “play soccer”. The light font denotes the target words to be translated in the future. For original Transformer, the source and target context are added directly without any rebalancing. However, a standard NMT system is incapable of effectively controlling the contributions from source and target contexts (He et al., 2018) to deliver highly adequate translations as shown in Figure 1. As a result, Tu et al. (2017) carefully designed context gates to dynamically control the influence from source and target contexts and observed significant improvements in the recurrent neural network (RNN) based NMT. Although Transformer (Vaswani et al., 2017) delivers significant gains over RNN for translation, there are still one third translation errors related to context control problem as described in Section 3.3. Obviously, it is feasible to extend the context gates in RNN based NMT into Transformer, but an obstacle to accomplishing this goal is the complicated archi8555 Proceedings of the 58th Annual Meeting of t"
2020.acl-main.757,P19-1124,1,0.917702,"l Linguistics, pages 8555–8562 c July 5 - 10, 2020. 2020 Association for Computational Linguistics tecture in Transformer, where the source and target words are tightly coupled. Thus, it is challenging to put context gates into practice in Transformer. In this paper, under the Transformer architecture, we firstly provide a way to define the source and target contexts and then obtain our model by combining both source and target contexts with context gates, which actually induces a probabilistic model indicating whether the next generated word is contributed from the source or target sentence (Li et al., 2019). In our preliminary experiments, this model only achieves modest gains over Transformer because the context selection error reduction is very limited as described in Section 3.3. To further address this issue, we propose a probabilistic model whose loss function is derived from external supervision as regularization for the context gates. This probabilistic model is jointly trained with the context gates in NMT. As it is too costly to annotate this supervision for a large-scale training corpus manually, we instead propose a simple yet effective method to automatically generate supervision usi"
2020.acl-main.757,N18-1125,1,0.840967,"models the target context from a target-side language model (Koehn et al., 2003; Koehn, 2009; Chiang, 2005). These two models are trained independently. On the contrary, neural machine translation (NMT) advocates a unified manner to jointly learn source and target context using an encoderdecoder framework with an attention mechanism, leading to substantial gains over SMT in translation quality (Sutskever et al., 2014; Bahdanau et al., 2014; Gehring et al., 2017; Vaswani et al., 2017). Prior work on attention mechanism (Luong et al., 2015; Liu et al., 2016; Mi et al., 2016; Chen et al., 2018; Li et al., 2018; Elbayad et al., 2018; Yang et al., 2020) have shown a better context representation is helpful to translation performance. zi 1 − zi + Transformer: I often play golf with my colleagues . Context Gates: I often play golf with my colleagues . Regularized : Context Gates I often play soccer with my colleagues . Figure 1: A running example to raise the context control problem. Both original and context gated Transformer obtain an unfaithful translation by wrongly translate “ti¯ q´ıu” into “play golf” because referring too much target context. By regularizing the context gates, the purposed metho"
2020.acl-main.757,C16-1291,1,0.731547,"ontext from the source-side of a translation model and models the target context from a target-side language model (Koehn et al., 2003; Koehn, 2009; Chiang, 2005). These two models are trained independently. On the contrary, neural machine translation (NMT) advocates a unified manner to jointly learn source and target context using an encoderdecoder framework with an attention mechanism, leading to substantial gains over SMT in translation quality (Sutskever et al., 2014; Bahdanau et al., 2014; Gehring et al., 2017; Vaswani et al., 2017). Prior work on attention mechanism (Luong et al., 2015; Liu et al., 2016; Mi et al., 2016; Chen et al., 2018; Li et al., 2018; Elbayad et al., 2018; Yang et al., 2020) have shown a better context representation is helpful to translation performance. zi 1 − zi + Transformer: I often play golf with my colleagues . Context Gates: I often play golf with my colleagues . Regularized : Context Gates I often play soccer with my colleagues . Figure 1: A running example to raise the context control problem. Both original and context gated Transformer obtain an unfaithful translation by wrongly translate “ti¯ q´ıu” into “play golf” because referring too much target context."
2020.acl-main.757,D09-1051,0,0.022876,"golden zi∗ are inaccessible for each word yi in the training corpus, we ideally have to annotate it manually. However, it is costly for human to label such a large scale dataset. Instead, we propose an automatic method to generate its value in practice in the next subsection. 2.3 Generating Supervision zi∗ To decide whether yi is contributed from the source (x) or target sentence (y<i ) (Li et al., 2019), a metric to measure the correlation between a pair of words (hyi , xj i or hyi , yk i for k < i) is first required. This is closely related to a well-studied problem, i.e., word collocation (Liu et al., 2009), and we simply employ the pointwise mutual information (PMI) to measure the correlation between a word pair hµ, νi following Bouma (2009): (µ,ν) pmi (µ, ν) = log PP(µ)P (ν) C(µ,ν) = log Z + log C(µ)C(ν) , (6) possible (µ, ν) pairs. To obtain the context gates, we define two types of PMI according to different C (µ, ν) including two scenarios as follows. PMI in the Bilingual Scenario For each parallel sentence pair hx, yi in training set, C (yi , xj ) is added by one if both yi ∈ y and xj ∈ x. PMI in the Monolingual Scenario In the translation scenario, only the words in the preceding context"
2020.acl-main.757,D18-1049,0,0.0253974,"Missing"
2020.acl-main.757,D18-1036,0,0.0295875,"Missing"
2020.findings-emnlp.401,W19-4828,0,0.011255,"ome may may come test today The Figure 1: Constituency trees of a right-branching language and its reversed (left-branching) language. The tree at the bottom is obtained by reversing the tree at the top. Introduction Neural language models such as LSTM (Merity et al., 2018; Peters et al., 2018), GPT2 (Radford et al., 2019), and BERT (Devlin et al., 2019; Liu et al., 2019) have achieved state-of-the-art performance in various downstream NLP tasks. Many recent works try to interpret their success by revealing the linguistic properties captured by these language models (Hewitt and Manning, 2019; Clark et al., 2019; Jawahar et al., 2019; Tenney et al., 2019). One interesting line of these works tries to extract discrete constituency trees from pre-trained language models (Mareˇcek and Rosa, 2018, 2019; Kim et al., 2020; Wu et al., 2020). The core of these works is to extract syntax in two stages. Firstly, it defines the feature scores based on a language model, namely, the feature definition stage. Secondly, it leverages the feature scores to build a constituency tree, namely, the parsing stage. However, the degree to which the extracted constituency trees match gold constituency annotations may impreci"
2020.findings-emnlp.401,N19-1423,0,0.0371162,"on the branching bias, namely parsing algorithms, feature definitions, and language models. Experiments show that several existing works exhibit branching biases, and some implementations of these three factors can introduce the branching bias. 1 The today test come may may come test today The Figure 1: Constituency trees of a right-branching language and its reversed (left-branching) language. The tree at the bottom is obtained by reversing the tree at the top. Introduction Neural language models such as LSTM (Merity et al., 2018; Peters et al., 2018), GPT2 (Radford et al., 2019), and BERT (Devlin et al., 2019; Liu et al., 2019) have achieved state-of-the-art performance in various downstream NLP tasks. Many recent works try to interpret their success by revealing the linguistic properties captured by these language models (Hewitt and Manning, 2019; Clark et al., 2019; Jawahar et al., 2019; Tenney et al., 2019). One interesting line of these works tries to extract discrete constituency trees from pre-trained language models (Mareˇcek and Rosa, 2018, 2019; Kim et al., 2020; Wu et al., 2020). The core of these works is to extract syntax in two stages. Firstly, it defines the feature scores based on a"
2020.findings-emnlp.401,N19-1419,0,0.0306393,"g bias. 1 The today test come may may come test today The Figure 1: Constituency trees of a right-branching language and its reversed (left-branching) language. The tree at the bottom is obtained by reversing the tree at the top. Introduction Neural language models such as LSTM (Merity et al., 2018; Peters et al., 2018), GPT2 (Radford et al., 2019), and BERT (Devlin et al., 2019; Liu et al., 2019) have achieved state-of-the-art performance in various downstream NLP tasks. Many recent works try to interpret their success by revealing the linguistic properties captured by these language models (Hewitt and Manning, 2019; Clark et al., 2019; Jawahar et al., 2019; Tenney et al., 2019). One interesting line of these works tries to extract discrete constituency trees from pre-trained language models (Mareˇcek and Rosa, 2018, 2019; Kim et al., 2020; Wu et al., 2020). The core of these works is to extract syntax in two stages. Firstly, it defines the feature scores based on a language model, namely, the feature definition stage. Secondly, it leverages the feature scores to build a constituency tree, namely, the parsing stage. However, the degree to which the extracted constituency trees match gold constituency ann"
2020.findings-emnlp.401,P19-1356,0,0.0262215,"Missing"
2020.findings-emnlp.401,2021.ccl-1.108,0,0.0240199,"Missing"
2020.findings-emnlp.401,J93-2004,0,0.0697929,"syntax extracting pipeline (i.e., both the feature definition and parsing algorithm are fair) and then calculate the branching gap using the well-trained language models on languages L and L0 . Since there is no branching bias within our selected extracting method, the branching bias can be attributed to the input itself, if a branching gap is observed. 3 3.1 Experiments Settings Data We choose English as the main language in our experiments. The English data used for training language models is the concatenation of 1M lines of Wikipedia data (Devlin et al., 2019) and the Penn TreeBank (PTB) (Marcus et al., 1993) training data. We use PTB-22 and PTB-23 for validation and test, respectively. Besides, to rule out the impact of other linguistic properties, we also conduct part of our experiments on German and Chinese. We use the German Treebank from the SPMRL (Seddah et al., 2014) and Penn Chinese TreeBank (CTB) (Xue et al., 2005) with their provided test sets to evaluate previous methods on those two languages, respectively. Language Models In our experiments, we train three different language models (i.e., BERT, GPT2, LSTM) for English and its reversed language 4 . The BERT and GPT2 models are trained"
2020.findings-emnlp.401,W18-5444,0,0.0443675,"Missing"
2020.findings-emnlp.401,W19-4827,0,0.0208798,"Missing"
2020.findings-emnlp.401,2020.acl-main.383,0,0.0887257,"uage models such as LSTM (Merity et al., 2018; Peters et al., 2018), GPT2 (Radford et al., 2019), and BERT (Devlin et al., 2019; Liu et al., 2019) have achieved state-of-the-art performance in various downstream NLP tasks. Many recent works try to interpret their success by revealing the linguistic properties captured by these language models (Hewitt and Manning, 2019; Clark et al., 2019; Jawahar et al., 2019; Tenney et al., 2019). One interesting line of these works tries to extract discrete constituency trees from pre-trained language models (Mareˇcek and Rosa, 2018, 2019; Kim et al., 2020; Wu et al., 2020). The core of these works is to extract syntax in two stages. Firstly, it defines the feature scores based on a language model, namely, the feature definition stage. Secondly, it leverages the feature scores to build a constituency tree, namely, the parsing stage. However, the degree to which the extracted constituency trees match gold constituency annotations may imprecisely reflect the model’s competence of capturing syntax, since their final performance may benefit from the branching bias. For example, as pointed out by Dyer et al. (2019), the syntax extracted from the ordered neuron based"
2020.findings-emnlp.401,N18-1202,0,0.0191411,"methods. Furthermore, we analyze the impacts of three factors on the branching bias, namely parsing algorithms, feature definitions, and language models. Experiments show that several existing works exhibit branching biases, and some implementations of these three factors can introduce the branching bias. 1 The today test come may may come test today The Figure 1: Constituency trees of a right-branching language and its reversed (left-branching) language. The tree at the bottom is obtained by reversing the tree at the top. Introduction Neural language models such as LSTM (Merity et al., 2018; Peters et al., 2018), GPT2 (Radford et al., 2019), and BERT (Devlin et al., 2019; Liu et al., 2019) have achieved state-of-the-art performance in various downstream NLP tasks. Many recent works try to interpret their success by revealing the linguistic properties captured by these language models (Hewitt and Manning, 2019; Clark et al., 2019; Jawahar et al., 2019; Tenney et al., 2019). One interesting line of these works tries to extract discrete constituency trees from pre-trained language models (Mareˇcek and Rosa, 2018, 2019; Kim et al., 2020; Wu et al., 2020). The core of these works is to extract syntax in t"
2020.findings-emnlp.401,W14-6111,0,0.0353882,"Missing"
2021.acl-demo.1,C18-1139,0,0.0140491,"d the URL is available on the web page.4 The text matching API is used to calculate the similarity between a pair of sentences. Similar to the text understanding API, the text matching API also supports access via HTTP-POST and the URL is available on the web page.5 Coarse-grained NER The difference between fine-grained and coarse-grained NERs is that the former involves more entity types with a finer granularity. We implement coarse-grained NER using supervised learning methods, including conditional random field (CRF) (Lafferty et al., 2001) based and deep neural network (DNN) based models (Akbik et al., 2018; Liu et al., 2019; Li et al., 2020). Constituency Parsing We implement the constituency parsing model based on the work (Kitaev and Klein, 2018). Kitaev and Klein (2018) build the parser by combining a sentence encoder with a chart decoder based on the self-attention mechanism. Different from work (Kitaev and Klein, 2018) , we use pre-trained BERT model as the text encoder to extract features to support the subsequent decoder-based parsing. Our model achieves excellent performance and has low search complexity. Semantic Role Labeling Semantic role labeling (also called shallow semantic parsin"
2021.acl-demo.1,P81-1022,0,0.278718,"lar expressions or supervised sequence tagging methods to recognize time and quantity entities. However, it is difficult for those methods to derive structured or deep semantic information of entities. To overcome this problem, time and quantity entities are parsed in TexSmart by Context Free Grammar (CFG), which is more expressive than regular expressions. Its key idea is similar to that in Shi et al. (2015) and can be described as follows: First, CFG grammar rules are manually written according to possible natural language expressions of a specific entity type. Second, the Earley algorithm (Earley, 1970) is employed to parse a piece of text to obtain semantic trees of entities. Finally, deep semantic representations of entities are derived from the semantic trees. 2.2 Other Modules Deep Semantic Representation Word Segmentation In order to support different application scenarios, TexSmart provides word segmentation results of two granularity levels: word level (or basic level), and phrase level. For phraselevel segmentation, some phrases (especially noun phrases) may contained as a unit. An unsupervised algorithm is implemented in TexSmart for both English and Chinese word segmentation. We ch"
2021.acl-demo.1,U15-1010,0,0.0276516,"TTP API employs a larger knowledge base and supports more 4 3 https://ai.tencent.com/ailab/nlp/texsmart/ table_html/tc_label_set.html. 5 5 https://texsmart.qq.com/api https://texsmart.qq.com/api/match_text. Quality SE ZH EN 79.5 80.5 PTB for English and CTB 9.0 for Chinese. We use their corresponding test sets to evaluate all the models. Coarse-grained NER To ensure better generalization to industrial applications, we combine several public training sets together for English NER. They are CoNLL2003 (Sang and De Meulder, 2003), BTC (Derczynski et al., 2016), GMB (Bos et al., 2017), SEC_FILING (Alvarado et al., 2015), WikiGold (Balasuriya et al., 2009; Nothman et al., 2013), and WNUT17 (Derczynski et al., 2017). Since the label set for all these datasets are slightly different, we only maintain three common labels (Person, Location and Organization) for training and testing. For Chinese, we create a NER dataset including about 80 thousand sentences labeled with 12 entity types, by following a similar guideline to that of the Ontonotes dataset. We randomly split it into a training set and a test set with ratio of 3:1. We evaluate two algorithms for coarse-grained NER: CRF and DNN. For DNN, we implement the"
2021.acl-demo.1,W18-2501,0,0.0662142,"Missing"
2021.acl-demo.1,W09-3302,0,0.0687657,"Missing"
2021.acl-demo.1,C92-2082,0,0.277702,"de web search (e.g., for query suggestion) and recommendation systems. Semantic expansion task was firstly introduced in Han et al. (2020), and it was addressed by a neural method. However, this method is not as efficient as one expected for some industrial applications. Therefore, we propose a light-weight alternative approach in TexSmart for this task. This approach includes two offline steps and two online ones, as illustrated in Figure 2. During the offline procedure, Hearst patterns are first applied to a large-scale text corpus to obtain a is-a map (or called a hyponym-to-hypernym map) (Hearst, 1992; Zhang et al., 2011). Then a clustering algorithm is employed to build a collection of term clusters from all the hyponyms, allowing a hyponym to belong to multiple clusters. Each term cluster is labeled by one or more hypernyms (or called type names). Term similarity scores used in the clustering algorithm are calculated by a combination of word embedding, distributional similarity, and patternbased methods (Mikolov et al., 2013; Song et al., 2018; Shi et al., 2010). During the online testing time, clusters containing the target entity mention are first retrieved by referring to the cluster"
2021.acl-demo.1,C10-3004,0,0.154401,"Missing"
2021.acl-demo.1,P08-1067,0,0.029343,"ven by TexSmart for “24 months ago” is a structured string with a precise date in JSON format: {&quot;value&quot;: [2019, 3]} if the screenshot time was Mar. 2021. Deep semantic representation is important for applications like task-oriented chatbots, where the precise meanings of some entities are required. So far, most public text understanding tools do not provide such a feaPart-of-Speech Tagging Part-of-Speech (POS) denotes the syntactic role of each word in a sentence, also known as word classes or syntactic categories and it is helpful for many downstream text understanding tasks such as parsing (Huang, 2008; Chen and Manning, 2014; Liu et al., 2018a). We implement three models among many popular ones for part-of-speech tagging (Ratnaparkhi, 1996; Huang et al., 2015; Li et al., 2021b): Log-linear based model (Ratnaparkhi, 1996), conditional random field (CRF) based model (Lafferty et al., 2001) and deep neural network (DNN) based model (Akbik 2.1.3 4 et al., 2018; Liu et al., 2019). We denote them as: log_linear, crf and dnn, respectively. text understanding tasks and algorithms. The detailed comparison between the SDK and the HTTP API is available in https://ai.tencent.com/ ailab/nlp/texsmart/en"
2021.acl-demo.1,D14-1082,0,0.0308477,"rt for “24 months ago” is a structured string with a precise date in JSON format: {&quot;value&quot;: [2019, 3]} if the screenshot time was Mar. 2021. Deep semantic representation is important for applications like task-oriented chatbots, where the precise meanings of some entities are required. So far, most public text understanding tools do not provide such a feaPart-of-Speech Tagging Part-of-Speech (POS) denotes the syntactic role of each word in a sentence, also known as word classes or syntactic categories and it is helpful for many downstream text understanding tasks such as parsing (Huang, 2008; Chen and Manning, 2014; Liu et al., 2018a). We implement three models among many popular ones for part-of-speech tagging (Ratnaparkhi, 1996; Huang et al., 2015; Li et al., 2021b): Log-linear based model (Ratnaparkhi, 1996), conditional random field (CRF) based model (Lafferty et al., 2001) and deep neural network (DNN) based model (Akbik 2.1.3 4 et al., 2018; Liu et al., 2019). We denote them as: log_linear, crf and dnn, respectively. text understanding tasks and algorithms. The detailed comparison between the SDK and the HTTP API is available in https://ai.tencent.com/ ailab/nlp/texsmart/en/instructions.html. Offl"
2021.acl-demo.1,D18-1536,0,0.0331353,"Missing"
2021.acl-demo.1,P18-1249,0,0.0167912,"he text understanding API, the text matching API also supports access via HTTP-POST and the URL is available on the web page.5 Coarse-grained NER The difference between fine-grained and coarse-grained NERs is that the former involves more entity types with a finer granularity. We implement coarse-grained NER using supervised learning methods, including conditional random field (CRF) (Lafferty et al., 2001) based and deep neural network (DNN) based models (Akbik et al., 2018; Liu et al., 2019; Li et al., 2020). Constituency Parsing We implement the constituency parsing model based on the work (Kitaev and Klein, 2018). Kitaev and Klein (2018) build the parser by combining a sentence encoder with a chart decoder based on the self-attention mechanism. Different from work (Kitaev and Klein, 2018) , we use pre-trained BERT model as the text encoder to extract features to support the subsequent decoder-based parsing. Our model achieves excellent performance and has low search complexity. Semantic Role Labeling Semantic role labeling (also called shallow semantic parsing) tries to assign role labels to words or phrases in a sentence. TexSmart takes a sequence labeling model with BERT as the text encoder for sema"
2021.acl-demo.1,2021.findings-emnlp.18,1,0.731087,"Missing"
2021.acl-demo.1,P17-1152,0,0.0965523,"Missing"
2021.acl-demo.1,C16-1111,0,0.0255628,"HTTP API and the SDK may be slightly different, because the HTTP API employs a larger knowledge base and supports more 4 3 https://ai.tencent.com/ailab/nlp/texsmart/ table_html/tc_label_set.html. 5 5 https://texsmart.qq.com/api https://texsmart.qq.com/api/match_text. Quality SE ZH EN 79.5 80.5 PTB for English and CTB 9.0 for Chinese. We use their corresponding test sets to evaluate all the models. Coarse-grained NER To ensure better generalization to industrial applications, we combine several public training sets together for English NER. They are CoNLL2003 (Sang and De Meulder, 2003), BTC (Derczynski et al., 2016), GMB (Bos et al., 2017), SEC_FILING (Alvarado et al., 2015), WikiGold (Balasuriya et al., 2009; Nothman et al., 2013), and WNUT17 (Derczynski et al., 2017). Since the label set for all these datasets are slightly different, we only maintain three common labels (Person, Location and Organization) for training and testing. For Chinese, we create a NER dataset including about 80 thousand sentences labeled with 12 entity types, by following a similar guideline to that of the Ontonotes dataset. We randomly split it into a training set and a test set with ratio of 3:1. We evaluate two algorithms fo"
2021.acl-demo.1,2021.naacl-main.116,1,0.743751,"entation is important for applications like task-oriented chatbots, where the precise meanings of some entities are required. So far, most public text understanding tools do not provide such a feaPart-of-Speech Tagging Part-of-Speech (POS) denotes the syntactic role of each word in a sentence, also known as word classes or syntactic categories and it is helpful for many downstream text understanding tasks such as parsing (Huang, 2008; Chen and Manning, 2014; Liu et al., 2018a). We implement three models among many popular ones for part-of-speech tagging (Ratnaparkhi, 1996; Huang et al., 2015; Li et al., 2021b): Log-linear based model (Ratnaparkhi, 1996), conditional random field (CRF) based model (Lafferty et al., 2001) and deep neural network (DNN) based model (Akbik 2.1.3 4 et al., 2018; Liu et al., 2019). We denote them as: log_linear, crf and dnn, respectively. text understanding tasks and algorithms. The detailed comparison between the SDK and the HTTP API is available in https://ai.tencent.com/ ailab/nlp/texsmart/en/instructions.html. Offline Toolkit (SDK) So far the SDK supports Linux, Windows, and Windows Subsystem for Linux (WSL). Mac OS support will be added in v0.3.0. Programming langu"
2021.acl-demo.1,W96-0213,0,0.664796,"ime was Mar. 2021. Deep semantic representation is important for applications like task-oriented chatbots, where the precise meanings of some entities are required. So far, most public text understanding tools do not provide such a feaPart-of-Speech Tagging Part-of-Speech (POS) denotes the syntactic role of each word in a sentence, also known as word classes or syntactic categories and it is helpful for many downstream text understanding tasks such as parsing (Huang, 2008; Chen and Manning, 2014; Liu et al., 2018a). We implement three models among many popular ones for part-of-speech tagging (Ratnaparkhi, 1996; Huang et al., 2015; Li et al., 2021b): Log-linear based model (Ratnaparkhi, 1996), conditional random field (CRF) based model (Lafferty et al., 2001) and deep neural network (DNN) based model (Akbik 2.1.3 4 et al., 2018; Liu et al., 2019). We denote them as: log_linear, crf and dnn, respectively. text understanding tasks and algorithms. The detailed comparison between the SDK and the HTTP API is available in https://ai.tencent.com/ ailab/nlp/texsmart/en/instructions.html. Offline Toolkit (SDK) So far the SDK supports Linux, Windows, and Windows Subsystem for Linux (WSL). Mac OS support will"
2021.acl-demo.1,C18-1166,0,0.0468114,"s a structured string with a precise date in JSON format: {&quot;value&quot;: [2019, 3]} if the screenshot time was Mar. 2021. Deep semantic representation is important for applications like task-oriented chatbots, where the precise meanings of some entities are required. So far, most public text understanding tools do not provide such a feaPart-of-Speech Tagging Part-of-Speech (POS) denotes the syntactic role of each word in a sentence, also known as word classes or syntactic categories and it is helpful for many downstream text understanding tasks such as parsing (Huang, 2008; Chen and Manning, 2014; Liu et al., 2018a). We implement three models among many popular ones for part-of-speech tagging (Ratnaparkhi, 1996; Huang et al., 2015; Li et al., 2021b): Log-linear based model (Ratnaparkhi, 1996), conditional random field (CRF) based model (Lafferty et al., 2001) and deep neural network (DNN) based model (Akbik 2.1.3 4 et al., 2018; Liu et al., 2019). We denote them as: log_linear, crf and dnn, respectively. text understanding tasks and algorithms. The detailed comparison between the SDK and the HTTP API is available in https://ai.tencent.com/ ailab/nlp/texsmart/en/instructions.html. Offline Toolkit (SDK)"
2021.acl-demo.1,W03-0419,0,0.678216,"Missing"
2021.acl-demo.1,D15-1135,1,0.740722,"ient. In this sense, both methods are general in practice. ture. As a result, applications using these tools have to implement deep semantic representation by themselves. Some NLP toolkits make use of regular expressions or supervised sequence tagging methods to recognize time and quantity entities. However, it is difficult for those methods to derive structured or deep semantic information of entities. To overcome this problem, time and quantity entities are parsed in TexSmart by Context Free Grammar (CFG), which is more expressive than regular expressions. Its key idea is similar to that in Shi et al. (2015) and can be described as follows: First, CFG grammar rules are manually written according to possible natural language expressions of a specific entity type. Second, the Earley algorithm (Earley, 1970) is employed to parse a piece of text to obtain semantic trees of entities. Finally, deep semantic representations of entities are derived from the semantic trees. 2.2 Other Modules Deep Semantic Representation Word Segmentation In order to support different application scenarios, TexSmart provides word segmentation results of two granularity levels: word level (or basic level), and phrase level."
2021.acl-demo.1,W02-0109,0,0.557609,"Missing"
2021.acl-demo.1,C10-1112,1,0.519815,"rocedure, Hearst patterns are first applied to a large-scale text corpus to obtain a is-a map (or called a hyponym-to-hypernym map) (Hearst, 1992; Zhang et al., 2011). Then a clustering algorithm is employed to build a collection of term clusters from all the hyponyms, allowing a hyponym to belong to multiple clusters. Each term cluster is labeled by one or more hypernyms (or called type names). Term similarity scores used in the clustering algorithm are calculated by a combination of word embedding, distributional similarity, and patternbased methods (Mikolov et al., 2013; Song et al., 2018; Shi et al., 2010). During the online testing time, clusters containing the target entity mention are first retrieved by referring to the cluster collection. Generally, there may be multiple (ambiguous) clusters containing the target entity mention and thus it is necessary to pick the best cluster through disambiguation. Once the best cluster is chosen, its members (or instances) can be returned as the expansion results. Now the core challenge is how to calculate the System Modules Compared to most other public text understanding systems, TexSmart supports three unique modules, i.e., fine-grained NER, semantic"
2021.acl-demo.1,P14-5010,0,0.00644413,"Missing"
2021.acl-demo.1,N18-2028,1,0.809382,"uring the offline procedure, Hearst patterns are first applied to a large-scale text corpus to obtain a is-a map (or called a hyponym-to-hypernym map) (Hearst, 1992; Zhang et al., 2011). Then a clustering algorithm is employed to build a collection of term clusters from all the hyponyms, allowing a hyponym to belong to multiple clusters. Each term cluster is labeled by one or more hypernyms (or called type names). Term similarity scores used in the clustering algorithm are calculated by a combination of word embedding, distributional similarity, and patternbased methods (Mikolov et al., 2013; Song et al., 2018; Shi et al., 2010). During the online testing time, clusters containing the target entity mention are first retrieved by referring to the cluster collection. Generally, there may be multiple (ambiguous) clusters containing the target entity mention and thus it is necessary to pick the best cluster through disambiguation. Once the best cluster is chosen, its members (or instances) can be returned as the expansion results. Now the core challenge is how to calculate the System Modules Compared to most other public text understanding systems, TexSmart supports three unique modules, i.e., fine-gra"
2021.acl-demo.1,J93-2004,0,0.0740651,"ng a similar guideline to that of the Ontonotes dataset. We randomly split it into a training set and a test set with ratio of 3:1. We evaluate two algorithms for coarse-grained NER: CRF and DNN. For DNN, we implement the RoBERTa-CRF and Flair models. As we found RoBERTa-CRF performs better on the Chinese dataset while Flair is better on the English dataset, we report results of RoBERTa-CRF for Chinese and Flair for English in our experiments. Constituency Parsing We conduct parsing experiments on both English and Chinese datasets. For English task, we use WSJ sections in Penn Treebank (PTB) (Marcus et al., 1993), and we follow the standard splits: the training data ranges from section 2 to section 21; the development data is section 24; and the test data is section 23. For Chinese task, we use the Penn Chinese Treebank (CTB) of the version 5.1 (Xue et al., 2005). The training data includes the articles 001-270 and articles 440-1151; the development data is the articles 301- 325; and the test data is the articles 271-300. SRL Semantic role labeling experiments are conducted on both English and Chinese datasets. We use the CoNLL 2012 datasets (Pradhan et al., 2013) and follow the standard splits for th"
2021.acl-demo.1,W13-3516,0,0.0334885,"e WSJ sections in Penn Treebank (PTB) (Marcus et al., 1993), and we follow the standard splits: the training data ranges from section 2 to section 21; the development data is section 24; and the test data is section 23. For Chinese task, we use the Penn Chinese Treebank (CTB) of the version 5.1 (Xue et al., 2005). The training data includes the articles 001-270 and articles 440-1151; the development data is the articles 301- 325; and the test data is the articles 271-300. SRL Semantic role labeling experiments are conducted on both English and Chinese datasets. We use the CoNLL 2012 datasets (Pradhan et al., 2013) and follow the standard splits for the training, development and test sets. The network parameters of our model are initialized using RoBERTa. The batch size is set to 32 and the learning rate is 5×10−5 . Text Matching Two text matching algorithms are evaluated: ESIM and Linkage. The datasets used in evaluating English text matching are MRPC6 and QUORA7 . For Chinese text matching, four datasets are involved: LCQMC (Liu et al., 2018b), AFQMC (Xu et al., 2020), BQ_CORPUS (Chen et al., 2018), and PAWSzh (Zhang et al., 2019). We evaluate the quality FGNER Base Hybrid 45.9 53.8 Table 1: Semantic"
2021.acl-demo.1,P11-1116,1,0.692266,"(e.g., for query suggestion) and recommendation systems. Semantic expansion task was firstly introduced in Han et al. (2020), and it was addressed by a neural method. However, this method is not as efficient as one expected for some industrial applications. Therefore, we propose a light-weight alternative approach in TexSmart for this task. This approach includes two offline steps and two online ones, as illustrated in Figure 2. During the offline procedure, Hearst patterns are first applied to a large-scale text corpus to obtain a is-a map (or called a hyponym-to-hypernym map) (Hearst, 1992; Zhang et al., 2011). Then a clustering algorithm is employed to build a collection of term clusters from all the hyponyms, allowing a hyponym to belong to multiple clusters. Each term cluster is labeled by one or more hypernyms (or called type names). Term similarity scores used in the clustering algorithm are calculated by a combination of word embedding, distributional similarity, and patternbased methods (Mikolov et al., 2013; Song et al., 2018; Shi et al., 2010). During the online testing time, clusters containing the target entity mention are first retrieved by referring to the cluster collection. Generally"
2021.acl-demo.1,P13-4009,0,0.0710732,"Missing"
2021.acl-demo.1,N19-1131,0,0.045871,"Missing"
2021.acl-long.246,P19-1175,0,0.448052,"a et al., 2011; Robinson, 2012; Huang et al., 2021). A TM generally provides valuable translation information particularly for those input sentences preferably matching the source sentences in the TM, and many efforts have been devoted to integrating a TM into statistical machine translation (Simard and Isabelle, 2009; Koehn and Senellart, 2010a; Ma et al., 2011; Wang et al., 2013; Liu et al., 2019). Recently there are increasing interests in improving neural machine translation (NMT) with a ∗ Corresponding author. TM (Li et al., 2016; Farajian et al., 2017; Gu et al., 2018; Xia et al., 2019; Bulte and Tezcan, 2019; Xu et al., 2020). Many notable approaches have been proposed to augment an NMT model by using a TM. For example, Zhang et al. (2018) and He et al. (2019) extract scored n-grams from a TM and then reward each partial translation once it matches an extracted n-gram during beam search. Gu et al. (2018) and Xia et al. (2019) use an auxiliary network to encode a TM and then integrate it into the NMT architecture. Bulte and Tezcan (2019) and Xu et al. (2020) employ data augmentation to train an NMT model whose training instances are bilingual sentences augmented by their TMs. Despite their improve"
2021.acl-long.246,2021.acl-long.567,1,0.808584,"018) and Xia et al. (2019) employ an auxiliary network to encode TMs and integrate it into the NMT architecture. Our model architecture is simpler than Gu et al. (2018) and Xia et al. (2019) and we encode a single TM target sentence and utilize simple attention mechanisms on the TM. And the architecture is more efficient and leads to a faster translation speed compared with Gu et al. (2018) and Xia et al. (2019). In particular, we propose a novel training criterion to make the TM-based NMT model more robust in different translation situations (with or without a TM). In parallel with our work, Cai et al. (2021) extend the translation memory from the bilingual setting to the monolingual setting through a cross-lingual retrieval technique, and Khandelwal et al. (2021) report significant improvements in quality on general translation tasks as ours, but their inference speed is two orders of magnitude slower than Transformer because they perform contextual word retrieval whose search space is much larger than that of sentence retrieval. 7 Conclusion This paper presents a simple TM-based NMT model that employs a single bilingual sentence as its TM and thus is fast in training and inference. Although the"
2021.acl-long.246,2021.acl-long.369,0,0.0364788,"y&lt;i while key and value are from the representation of TM m. After the two parallel attention operators, two resulting sequences are passed to Add & Norm operator and a new sequence is obtained as the query for the next multi-head attention (i.e. MH-Att (y&lt;i , x)). The following sub-layer is the same as Transformer and P (yi |x, y&lt;i , m) can be obtained similar to the definition of standard NMT P (yi |x, y&lt;i ) as presented in Section 2. We skip those formal equations to rewrite P (yi |x, y&lt;i , m) due to space limitation. 2 Although some advanced word alignment toolkits (Dou and Neubig, 2021; Chen et al., 2021; Jalili Sabet et al., 2020) may lead to better performance, we still employ fast-align to be in line with previous work for fair comparison (Zhang et al., 2018; Xia et al., 2019). 3 In the rest of this paper, we may drop θ in the model for easier notations. Training Suppose the training corpus is D = {hxi , yi , xitm , yitm i |i ∈ [1, N ]}, where hxi , yi i is a bilingual sentence, and hxitm , yitm i is the related TM which consists of a single bilingual sentence. Our goal is to learn the parameter θ of the TM-based NMT model P (y |x, xtm , ytm ; θ) defined in Eq.(9) using D. The common wisdo"
2021.acl-long.246,P15-1166,0,0.0323308,"it will generate accurate translations for those input source sentences whose TM is with high similarity. On the other hand, the second term induced by D0 teaches the model to output good translations without information from a TM. Additionally, this makes it possible that a single unified model can handle both translation scenarios (with or without a TM), which is practical for online services. Note that the proposed method is slightly different from standard data augmentation (Sennrich et al., 2016a; Fadaee et al., 2017; Fadaee and Monz, 2018; Wang et al., 2018) and multiple-task learning (Dong et al., 2015; Kiperwasser and Ballesteros, 2018; Wang et al., 2020) in NMT research. These data augmentation techniques automatically generate pseudo data based on the original training data and then train a model using both original and generated data. However, the dataset D0 is 4 In the experiments, we implement null as the sentence including a single word, i.e. “heosi”. Algorithm 1: Joint Training Algorithm Input: Mini-batch size b, maximal iteration M , a learning rate schema η and two corpus: D = {hxi , yi , xitm , yitm i | i ∈ [1, N ]} and D0 = {hxi , yi , null, nulli |i ∈ [1, N ]} Output: The param"
2021.acl-long.246,2021.eacl-main.181,0,0.0354972,"and its query is from y&lt;i while key and value are from the representation of TM m. After the two parallel attention operators, two resulting sequences are passed to Add & Norm operator and a new sequence is obtained as the query for the next multi-head attention (i.e. MH-Att (y&lt;i , x)). The following sub-layer is the same as Transformer and P (yi |x, y&lt;i , m) can be obtained similar to the definition of standard NMT P (yi |x, y&lt;i ) as presented in Section 2. We skip those formal equations to rewrite P (yi |x, y&lt;i , m) due to space limitation. 2 Although some advanced word alignment toolkits (Dou and Neubig, 2021; Chen et al., 2021; Jalili Sabet et al., 2020) may lead to better performance, we still employ fast-align to be in line with previous work for fair comparison (Zhang et al., 2018; Xia et al., 2019). 3 In the rest of this paper, we may drop θ in the model for easier notations. Training Suppose the training corpus is D = {hxi , yi , xitm , yitm i |i ∈ [1, N ]}, where hxi , yi i is a bilingual sentence, and hxitm , yitm i is the related TM which consists of a single bilingual sentence. Our goal is to learn the parameter θ of the TM-based NMT model P (y |x, xtm , ytm ; θ) defined in Eq.(9) using"
2021.acl-long.246,N13-1073,0,0.0257533,"takes the similarity score into account and it defines m as follows: m = stm × Etm (6) where stm = sim(x, xtm ) is the similarity score and the symbol × denotes the scalar-multiplication. Method 3: sentence with alignment (TF-SA) As shown in Figure 1, xtm consists of the matched parts (in orange color) and the unmatched parts (in dark color) to x. Since each word in the TM is not of the same importance to the source sentence x, we should pay more attention to the words that 3172 are in the matched parts. So, we further obtain the word alignment between xtm and ytm through fast-align toolkit (Dyer et al., 2013).2 Suppose Atm is the word alignment between xtm and ytm : Ajtm = 1 denotes yj is aligned to some xi otherwise Ajtm = 0, where xi is also in x . Therefore, the third method defines m as follows:  m = Atm ◦ stm × Etm (7) where the symbol ◦ denotes an operator between a vector and a matrix such that ( j stm × Etm if Ajtm = 0 mj = (8) j Etm if Ajtm = 1 In summary The entire model architecture is illustrated in Figure 1: the dashed box in the right part shows the memory encoder, and the left part shows how the memory representation is used in the NMT model similar to the Transformer. In our model"
2021.acl-long.246,P17-2090,0,0.081147,"induced by D guides the model to use the information from a TM for prediction, and thereby it will generate accurate translations for those input source sentences whose TM is with high similarity. On the other hand, the second term induced by D0 teaches the model to output good translations without information from a TM. Additionally, this makes it possible that a single unified model can handle both translation scenarios (with or without a TM), which is practical for online services. Note that the proposed method is slightly different from standard data augmentation (Sennrich et al., 2016a; Fadaee et al., 2017; Fadaee and Monz, 2018; Wang et al., 2018) and multiple-task learning (Dong et al., 2015; Kiperwasser and Ballesteros, 2018; Wang et al., 2020) in NMT research. These data augmentation techniques automatically generate pseudo data based on the original training data and then train a model using both original and generated data. However, the dataset D0 is 4 In the experiments, we implement null as the sentence including a single word, i.e. “heosi”. Algorithm 1: Joint Training Algorithm Input: Mini-batch size b, maximal iteration M , a learning rate schema η and two corpus: D = {hxi , yi , xitm"
2021.acl-long.246,D18-1040,0,0.0168471,"the model to use the information from a TM for prediction, and thereby it will generate accurate translations for those input source sentences whose TM is with high similarity. On the other hand, the second term induced by D0 teaches the model to output good translations without information from a TM. Additionally, this makes it possible that a single unified model can handle both translation scenarios (with or without a TM), which is practical for online services. Note that the proposed method is slightly different from standard data augmentation (Sennrich et al., 2016a; Fadaee et al., 2017; Fadaee and Monz, 2018; Wang et al., 2018) and multiple-task learning (Dong et al., 2015; Kiperwasser and Ballesteros, 2018; Wang et al., 2020) in NMT research. These data augmentation techniques automatically generate pseudo data based on the original training data and then train a model using both original and generated data. However, the dataset D0 is 4 In the experiments, we implement null as the sentence including a single word, i.e. “heosi”. Algorithm 1: Joint Training Algorithm Input: Mini-batch size b, maximal iteration M , a learning rate schema η and two corpus: D = {hxi , yi , xitm , yitm i | i ∈ [1, N ]"
2021.acl-long.246,W17-4713,0,0.0945536,"anslated (Garcia, 2009; Koehn and Senellart, 2010b; Utiyama et al., 2011; Robinson, 2012; Huang et al., 2021). A TM generally provides valuable translation information particularly for those input sentences preferably matching the source sentences in the TM, and many efforts have been devoted to integrating a TM into statistical machine translation (Simard and Isabelle, 2009; Koehn and Senellart, 2010a; Ma et al., 2011; Wang et al., 2013; Liu et al., 2019). Recently there are increasing interests in improving neural machine translation (NMT) with a ∗ Corresponding author. TM (Li et al., 2016; Farajian et al., 2017; Gu et al., 2018; Xia et al., 2019; Bulte and Tezcan, 2019; Xu et al., 2020). Many notable approaches have been proposed to augment an NMT model by using a TM. For example, Zhang et al. (2018) and He et al. (2019) extract scored n-grams from a TM and then reward each partial translation once it matches an extracted n-gram during beam search. Gu et al. (2018) and Xia et al. (2019) use an auxiliary network to encode a TM and then integrate it into the NMT architecture. Bulte and Tezcan (2019) and Xu et al. (2020) employ data augmentation to train an NMT model whose training instances are biling"
2021.acl-long.246,2020.findings-emnlp.147,0,0.047852,"Missing"
2021.acl-long.246,2021.acl-short.47,0,0.0174332,"Gu et al. (2018) and Xia et al. (2019) and we encode a single TM target sentence and utilize simple attention mechanisms on the TM. And the architecture is more efficient and leads to a faster translation speed compared with Gu et al. (2018) and Xia et al. (2019). In particular, we propose a novel training criterion to make the TM-based NMT model more robust in different translation situations (with or without a TM). In parallel with our work, Cai et al. (2021) extend the translation memory from the bilingual setting to the monolingual setting through a cross-lingual retrieval technique, and Khandelwal et al. (2021) report significant improvements in quality on general translation tasks as ours, but their inference speed is two orders of magnitude slower than Transformer because they perform contextual word retrieval whose search space is much larger than that of sentence retrieval. 7 Conclusion This paper presents a simple TM-based NMT model that employs a single bilingual sentence as its TM and thus is fast in training and inference. Although the presented model with the standard training outperforms strong TM-based baselines, it suffers from a robustness issue: its performance highly depends on the si"
2021.acl-long.246,Q18-1017,0,0.0231217,"ccurate translations for those input source sentences whose TM is with high similarity. On the other hand, the second term induced by D0 teaches the model to output good translations without information from a TM. Additionally, this makes it possible that a single unified model can handle both translation scenarios (with or without a TM), which is practical for online services. Note that the proposed method is slightly different from standard data augmentation (Sennrich et al., 2016a; Fadaee et al., 2017; Fadaee and Monz, 2018; Wang et al., 2018) and multiple-task learning (Dong et al., 2015; Kiperwasser and Ballesteros, 2018; Wang et al., 2020) in NMT research. These data augmentation techniques automatically generate pseudo data based on the original training data and then train a model using both original and generated data. However, the dataset D0 is 4 In the experiments, we implement null as the sentence including a single word, i.e. “heosi”. Algorithm 1: Joint Training Algorithm Input: Mini-batch size b, maximal iteration M , a learning rate schema η and two corpus: D = {hxi , yi , xitm , yitm i | i ∈ [1, N ]} and D0 = {hxi , yi , null, nulli |i ∈ [1, N ]} Output: The parameter θ. 1 for 1 ≤ t ≤ M do 2 Sample"
2021.acl-long.246,2010.jec-1.4,0,0.53151,"ively optimized through a novel training criterion. Extensive experiments on six TM-specialized tasks show that the proposed approach substantially surpasses several strong baselines that use multiple TMs, in terms of BLEU and running time. In particular, the proposed approach also advances the strong baselines on two general tasks (WMT news Zh→En and En→De). 1 Introduction A translation memory (TM) is originally collected from the translation history of professional translators, and provides the most similar source-target sentence pairs for the source sentence to be translated (Garcia, 2009; Koehn and Senellart, 2010b; Utiyama et al., 2011; Robinson, 2012; Huang et al., 2021). A TM generally provides valuable translation information particularly for those input sentences preferably matching the source sentences in the TM, and many efforts have been devoted to integrating a TM into statistical machine translation (Simard and Isabelle, 2009; Koehn and Senellart, 2010a; Ma et al., 2011; Wang et al., 2013; Liu et al., 2019). Recently there are increasing interests in improving neural machine translation (NMT) with a ∗ Corresponding author. TM (Li et al., 2016; Farajian et al., 2017; Gu et al., 2018; Xia et al"
2021.acl-long.246,D19-1570,1,0.844954,"ided to heavily depend on the given TM target ytm during training. In this way, if an input source sentence x has a high similarity with its given TM, the model will output 3173 high-quality results, as we also observed in Table 5. On the contrary, once an input sentence is provided with a low similar TM hxtm , ytm i (for instance, the similarity between 0 and 0.3, as shown in Table 4), the translation quality of its output rapidly decreases. Training criterion In order to avoid the TM over-fitting, we propose a simple yet elegant method, inspired by data augmentation (van Dyk and Meng, 2001; Li et al., 2019; Zhong et al., 2020) and multiple-task learning (Ben-David and Borbely, 2008; Qiu et al., 2013; Liu et al., 2016). Specifically, we first construct another corpus D0 = {hxi , yi , null, nulli |i ∈ [1, N ]} from D = {hxi , yi , xitm , yitm i |i ∈ [1, N ]}. In the constructed corpus, hnull, nulli plays a role of a TM, but both source and target sides of the TM are empty sentences.4 Then we train the model P (y |x, xtm , ytm ; θ) using both D and D0 , i.e. joint training, which is similar to multiple-task learning. Formally, we minimize the following joint loss function: N  X `(D, D0 ; θ) = − l"
2021.acl-long.246,L18-1146,0,0.0698251,"Missing"
2021.acl-long.246,D12-1037,1,0.764146,"uristic score to decide whether the extracted segments should be used as decoding constraints or not, then hardly constrain SMT to decode for those unmatched parts of the source sentence. Ma et al. (2011) design a fine-grained classifier, rather than the heuristic score, to predict the score for making more reliable decisions. Simard and Isabelle (2009), Wang et al. (2013) and Wang et al. (2014) add the extracted bilingual segments to the translation table of SMT, and then bias the decoder in a 3177 soft constraint manner when decoding the source sentence with the augmented translation table. Liu et al. (2012) use the retrieved bilingual sentences to update the parameters for the log-linear model based SMT. In recent years, many efforts are made on neural machine translation (NMT) associated with a TM. Li et al. (2016) and Farajian et al. (2017) make full use of the retrieved TM sentence pairs to fine-tune the pre-trained NMT model on-the-fly. The most obvious drawback of fine-tuning is that the delay is too long for testing sentences. To avoid the online tuning process, Zhang et al. (2018) and He et al. (2019) dynamically integrate translation pieces, based on n-grams extracted from the matched se"
2021.acl-long.246,C16-1291,1,0.78144,"s a high similarity with its given TM, the model will output 3173 high-quality results, as we also observed in Table 5. On the contrary, once an input sentence is provided with a low similar TM hxtm , ytm i (for instance, the similarity between 0 and 0.3, as shown in Table 4), the translation quality of its output rapidly decreases. Training criterion In order to avoid the TM over-fitting, we propose a simple yet elegant method, inspired by data augmentation (van Dyk and Meng, 2001; Li et al., 2019; Zhong et al., 2020) and multiple-task learning (Ben-David and Borbely, 2008; Qiu et al., 2013; Liu et al., 2016). Specifically, we first construct another corpus D0 = {hxi , yi , null, nulli |i ∈ [1, N ]} from D = {hxi , yi , xitm , yitm i |i ∈ [1, N ]}. In the constructed corpus, hnull, nulli plays a role of a TM, but both source and target sides of the TM are empty sentences.4 Then we train the model P (y |x, xtm , ytm ; θ) using both D and D0 , i.e. joint training, which is similar to multiple-task learning. Formally, we minimize the following joint loss function: N  X `(D, D0 ; θ) = − log P (yi |xi , xitm , yitm ; θ) i  + λ × log P (yi |xi , null, null; θ) (10) where 0 &lt; λ is a coefficient to trad"
2021.acl-long.246,P11-1124,0,0.0415096,"Missing"
2021.acl-long.246,D18-1100,0,0.0199325,"formation from a TM for prediction, and thereby it will generate accurate translations for those input source sentences whose TM is with high similarity. On the other hand, the second term induced by D0 teaches the model to output good translations without information from a TM. Additionally, this makes it possible that a single unified model can handle both translation scenarios (with or without a TM), which is practical for online services. Note that the proposed method is slightly different from standard data augmentation (Sennrich et al., 2016a; Fadaee et al., 2017; Fadaee and Monz, 2018; Wang et al., 2018) and multiple-task learning (Dong et al., 2015; Kiperwasser and Ballesteros, 2018; Wang et al., 2020) in NMT research. These data augmentation techniques automatically generate pseudo data based on the original training data and then train a model using both original and generated data. However, the dataset D0 is 4 In the experiments, we implement null as the sentence including a single word, i.e. “heosi”. Algorithm 1: Joint Training Algorithm Input: Mini-batch size b, maximal iteration M , a learning rate schema η and two corpus: D = {hxi , yi , xitm , yitm i | i ∈ [1, N ]} and D0 = {hxi , yi"
2021.acl-long.246,P02-1040,0,0.114175,"es which do not share the same bilingual sentences to promote diversity, i.e., D and D0 are independently and randomly sampled. In our experiments, we employ Adam (Kingma and Ba, 2014) with default settings as the learning rate schema. 5 Experiments In this section, we validate the effectiveness of the proposed approach: robustness for handling both translation situations (with or without a TM), running efficiency compared with the previous TMbased NMT models, translation quality on both TM-specialized tasks and general MT tasks. We use the case-insensitive BLEU score as the automatic metric (Papineni et al., 2002) for the translation quality evaluation. 5.1 Setup TM-specialized tasks We evaluate our proposed models with the JRC-Acquis corpora, which include three language pairs and lead to six translation tasks in total: English↔German (En↔De), 3174 TM-specialized Tasks General WMT Tasks Fr↔EnEs↔EnDe↔En En→De Zh→En Train/Sent(#) 740467 673856 693011 4558262 20605452 Dev/Sent(#) 2649 2511 2440 Test/Sent(#) 2650 2585 2461 3003/3004 2001 34.00 34.22 25.46 23.03 En/Word(#) 29.44 32.68 Other/Word(#) 33.35 35.58 3000 28.94 29.90 2002 Table 1: Statistics of the datasets. The last two lines are average sentenc"
2021.acl-long.246,2020.emnlp-main.75,0,0.0308312,"t source sentences whose TM is with high similarity. On the other hand, the second term induced by D0 teaches the model to output good translations without information from a TM. Additionally, this makes it possible that a single unified model can handle both translation scenarios (with or without a TM), which is practical for online services. Note that the proposed method is slightly different from standard data augmentation (Sennrich et al., 2016a; Fadaee et al., 2017; Fadaee and Monz, 2018; Wang et al., 2018) and multiple-task learning (Dong et al., 2015; Kiperwasser and Ballesteros, 2018; Wang et al., 2020) in NMT research. These data augmentation techniques automatically generate pseudo data based on the original training data and then train a model using both original and generated data. However, the dataset D0 is 4 In the experiments, we implement null as the sentence including a single word, i.e. “heosi”. Algorithm 1: Joint Training Algorithm Input: Mini-batch size b, maximal iteration M , a learning rate schema η and two corpus: D = {hxi , yi , xitm , yitm i | i ∈ [1, N ]} and D0 = {hxi , yi , null, nulli |i ∈ [1, N ]} Output: The parameter θ. 1 for 1 ≤ t ≤ M do 2 Sample a mini-batch B with"
2021.acl-long.246,D13-1062,0,0.0338448,"rms of BLEU and running time. 6 Related Work In the statistical machine translation (SMT) diagram, Koehn and Senellart (2010a) extract bilingual segments from a TM which matches the source sentence to be translated, and employ a heuristic score to decide whether the extracted segments should be used as decoding constraints or not, then hardly constrain SMT to decode for those unmatched parts of the source sentence. Ma et al. (2011) design a fine-grained classifier, rather than the heuristic score, to predict the score for making more reliable decisions. Simard and Isabelle (2009), Wang et al. (2013) and Wang et al. (2014) add the extracted bilingual segments to the translation table of SMT, and then bias the decoder in a 3177 soft constraint manner when decoding the source sentence with the augmented translation table. Liu et al. (2012) use the retrieved bilingual sentences to update the parameters for the log-linear model based SMT. In recent years, many efforts are made on neural machine translation (NMT) associated with a TM. Li et al. (2016) and Farajian et al. (2017) make full use of the retrieved TM sentence pairs to fine-tune the pre-trained NMT model on-the-fly. The most obvious"
2021.acl-long.246,2020.acl-main.144,0,0.820482,", 2012; Huang et al., 2021). A TM generally provides valuable translation information particularly for those input sentences preferably matching the source sentences in the TM, and many efforts have been devoted to integrating a TM into statistical machine translation (Simard and Isabelle, 2009; Koehn and Senellart, 2010a; Ma et al., 2011; Wang et al., 2013; Liu et al., 2019). Recently there are increasing interests in improving neural machine translation (NMT) with a ∗ Corresponding author. TM (Li et al., 2016; Farajian et al., 2017; Gu et al., 2018; Xia et al., 2019; Bulte and Tezcan, 2019; Xu et al., 2020). Many notable approaches have been proposed to augment an NMT model by using a TM. For example, Zhang et al. (2018) and He et al. (2019) extract scored n-grams from a TM and then reward each partial translation once it matches an extracted n-gram during beam search. Gu et al. (2018) and Xia et al. (2019) use an auxiliary network to encode a TM and then integrate it into the NMT architecture. Bulte and Tezcan (2019) and Xu et al. (2020) employ data augmentation to train an NMT model whose training instances are bilingual sentences augmented by their TMs. Despite their improvements on the TM-sp"
2021.acl-long.246,P16-1009,0,0.0642162,"uitively, the first term induced by D guides the model to use the information from a TM for prediction, and thereby it will generate accurate translations for those input source sentences whose TM is with high similarity. On the other hand, the second term induced by D0 teaches the model to output good translations without information from a TM. Additionally, this makes it possible that a single unified model can handle both translation scenarios (with or without a TM), which is practical for online services. Note that the proposed method is slightly different from standard data augmentation (Sennrich et al., 2016a; Fadaee et al., 2017; Fadaee and Monz, 2018; Wang et al., 2018) and multiple-task learning (Dong et al., 2015; Kiperwasser and Ballesteros, 2018; Wang et al., 2020) in NMT research. These data augmentation techniques automatically generate pseudo data based on the original training data and then train a model using both original and generated data. However, the dataset D0 is 4 In the experiments, we implement null as the sentence including a single word, i.e. “heosi”. Algorithm 1: Joint Training Algorithm Input: Mini-batch size b, maximal iteration M , a learning rate schema η and two corpus"
2021.acl-long.246,P16-1162,0,0.0578994,"uitively, the first term induced by D guides the model to use the information from a TM for prediction, and thereby it will generate accurate translations for those input source sentences whose TM is with high similarity. On the other hand, the second term induced by D0 teaches the model to output good translations without information from a TM. Additionally, this makes it possible that a single unified model can handle both translation scenarios (with or without a TM), which is practical for online services. Note that the proposed method is slightly different from standard data augmentation (Sennrich et al., 2016a; Fadaee et al., 2017; Fadaee and Monz, 2018; Wang et al., 2018) and multiple-task learning (Dong et al., 2015; Kiperwasser and Ballesteros, 2018; Wang et al., 2020) in NMT research. These data augmentation techniques automatically generate pseudo data based on the original training data and then train a model using both original and generated data. However, the dataset D0 is 4 In the experiments, we implement null as the sentence including a single word, i.e. “heosi”. Algorithm 1: Joint Training Algorithm Input: Mini-batch size b, maximal iteration M , a learning rate schema η and two corpus"
2021.acl-long.246,2009.mtsummit-papers.14,0,0.441993,"tasks (WMT news Zh→En and En→De). 1 Introduction A translation memory (TM) is originally collected from the translation history of professional translators, and provides the most similar source-target sentence pairs for the source sentence to be translated (Garcia, 2009; Koehn and Senellart, 2010b; Utiyama et al., 2011; Robinson, 2012; Huang et al., 2021). A TM generally provides valuable translation information particularly for those input sentences preferably matching the source sentences in the TM, and many efforts have been devoted to integrating a TM into statistical machine translation (Simard and Isabelle, 2009; Koehn and Senellart, 2010a; Ma et al., 2011; Wang et al., 2013; Liu et al., 2019). Recently there are increasing interests in improving neural machine translation (NMT) with a ∗ Corresponding author. TM (Li et al., 2016; Farajian et al., 2017; Gu et al., 2018; Xia et al., 2019; Bulte and Tezcan, 2019; Xu et al., 2020). Many notable approaches have been proposed to augment an NMT model by using a TM. For example, Zhang et al. (2018) and He et al. (2019) extract scored n-grams from a TM and then reward each partial translation once it matches an extracted n-gram during beam search. Gu et al. ("
2021.acl-long.246,N18-1120,0,0.0758209,"t sentences preferably matching the source sentences in the TM, and many efforts have been devoted to integrating a TM into statistical machine translation (Simard and Isabelle, 2009; Koehn and Senellart, 2010a; Ma et al., 2011; Wang et al., 2013; Liu et al., 2019). Recently there are increasing interests in improving neural machine translation (NMT) with a ∗ Corresponding author. TM (Li et al., 2016; Farajian et al., 2017; Gu et al., 2018; Xia et al., 2019; Bulte and Tezcan, 2019; Xu et al., 2020). Many notable approaches have been proposed to augment an NMT model by using a TM. For example, Zhang et al. (2018) and He et al. (2019) extract scored n-grams from a TM and then reward each partial translation once it matches an extracted n-gram during beam search. Gu et al. (2018) and Xia et al. (2019) use an auxiliary network to encode a TM and then integrate it into the NMT architecture. Bulte and Tezcan (2019) and Xu et al. (2020) employ data augmentation to train an NMT model whose training instances are bilingual sentences augmented by their TMs. Despite their improvements on the TM-specialized translation tasks (aka JRC-Acquis corpora) where a TM is very similar to test sentences, they consume cons"
2021.acl-long.246,2011.mtsummit-papers.37,0,0.903269,"vel training criterion. Extensive experiments on six TM-specialized tasks show that the proposed approach substantially surpasses several strong baselines that use multiple TMs, in terms of BLEU and running time. In particular, the proposed approach also advances the strong baselines on two general tasks (WMT news Zh→En and En→De). 1 Introduction A translation memory (TM) is originally collected from the translation history of professional translators, and provides the most similar source-target sentence pairs for the source sentence to be translated (Garcia, 2009; Koehn and Senellart, 2010b; Utiyama et al., 2011; Robinson, 2012; Huang et al., 2021). A TM generally provides valuable translation information particularly for those input sentences preferably matching the source sentences in the TM, and many efforts have been devoted to integrating a TM into statistical machine translation (Simard and Isabelle, 2009; Koehn and Senellart, 2010a; Ma et al., 2011; Wang et al., 2013; Liu et al., 2019). Recently there are increasing interests in improving neural machine translation (NMT) with a ∗ Corresponding author. TM (Li et al., 2016; Farajian et al., 2017; Gu et al., 2018; Xia et al., 2019; Bulte and Tezc"
2021.acl-long.246,P13-1002,0,0.803214,"M) is originally collected from the translation history of professional translators, and provides the most similar source-target sentence pairs for the source sentence to be translated (Garcia, 2009; Koehn and Senellart, 2010b; Utiyama et al., 2011; Robinson, 2012; Huang et al., 2021). A TM generally provides valuable translation information particularly for those input sentences preferably matching the source sentences in the TM, and many efforts have been devoted to integrating a TM into statistical machine translation (Simard and Isabelle, 2009; Koehn and Senellart, 2010a; Ma et al., 2011; Wang et al., 2013; Liu et al., 2019). Recently there are increasing interests in improving neural machine translation (NMT) with a ∗ Corresponding author. TM (Li et al., 2016; Farajian et al., 2017; Gu et al., 2018; Xia et al., 2019; Bulte and Tezcan, 2019; Xu et al., 2020). Many notable approaches have been proposed to augment an NMT model by using a TM. For example, Zhang et al. (2018) and He et al. (2019) extract scored n-grams from a TM and then reward each partial translation once it matches an extracted n-gram during beam search. Gu et al. (2018) and Xia et al. (2019) use an auxiliary network to encode a"
2021.acl-long.246,C14-1039,0,0.0364095,"Missing"
2021.acl-long.3,W19-3619,0,0.0833955,"Missing"
2021.acl-long.370,E14-2007,0,0.0614733,"Missing"
2021.acl-long.370,J09-1002,0,0.119679,"Missing"
2021.acl-long.370,N16-1148,0,0.0142305,"ient interaction ways of CAT have emerged (Vasconcellos and Le´on, 1985; Green et al., 2014; Hokamp and Liu, 2017; Weng et al., 2019; Wang et al., 2020). Among those different methods, the autocompletion is the most related to our work. Therefore, we will first describe previous works in both sentence-level and word-level autocompletion, then show the relation to other tasks and scenarios. Sentence-level Autocompletion Most of previous work in autocompletion for CAT focus on sentence-level completion. A common use case in this line is interactive machine translation (IMT) (Green et al., 2014; Cheng et al., 2016; Peris et al., 2017; Knowles and Koehn, 2016; Santy et al., 2019). IMT systems utilize MT systems to complete the rest of a translation after human translators editing a prefix translation (Alabau et al., 2014; Zhao et al., 2020). For most IMT systems, the core to achieve this completion is prefix-constrained decoding (Wuebker et al., 2016). Another sentence-level autocompletion method, lexically constrained decoding (LCD) (Hokamp and Liu, 2017; Post and Vilar, 2018), recently attracts lots of attention (Hasler et al., 2018; Susanto et al., 2020; Kajiwara, 2019). Compared with IMT, LCD relaxe"
2021.acl-long.370,P05-1033,0,0.0842133,"part is the completion generated by MT system. Both (b) and (c) are word-level autocompletion, underlined text “sp” is the human typed characters and the words in the rounded rectangles are word-level autocompletion candidates. Introduction Machine translation (MT) has witnessed great advancements with the emergence of neural machine translation (NMT) (Sutskever et al., 2014; Bahdanau et al., 2015; Wu et al., 2016; Gehring et al., 2017; Vaswani et al., 2017), which is able to produce much higher quality translation results than statistical machine translation (SMT) models (Koehn et al., 2003; Chiang, 2005; Koehn, 1 The information of benchmark datasets is in https: //github.com/ghrua/gwlan 2009). In spite of this, MT systems cannot replace human translators, especially in the scenarios with rigorous translation quality requirements (e.g., translating product manuals, patent documents, government policies, and other official documents). Therefore, how to leverage the pros of MT systems to help human translators, namely, Computer-aided translation (CAT), attracts the attention of researchers (Barrachina et al., 2009; Green et al., 2014; Knowles and Koehn, 2016; Santy et al., 2019). Among all CAT"
2021.acl-long.370,N19-1423,0,0.0497258,"h decomposes the whole word autocompletion process into two parts: model the distribution of the target word w based on the source sequence x and the translation context c, and find the most possible word w based on the distribution and human typed sequence s. (2) where h is the representation of [MASK], φ is a linear network that projects the hidden representation h to a vector with dimension of target vocabulary size V , and softmax(d)[w] takes the component regarding to w after the softmax operation over a vector d ∈ RV . Inspired by the attention-based architectures (Vaswani et al., 2017; Devlin et al., 2019)3 , we 3 Because the use of attention-based models has become ubiquitous recently, we omit an exhaustive background de4795 Bidirectional Masked Attention Key / Value [SOS] the aircraft [MASK] ... rapidly [EOS] ... &lt;latexit sha1_base64=&quot;cXNtnBINXcmwhkwKN8rhFMu8CHE=&quot;&gt;AAAB7XicbVDLSsNAFL3xWeur6tLNYBFclUQUXRbduKxgH9CGMplO2rGTSZi5EUroP7hxoYhb/8edf+OkzUJbDwwczrmHufcEiRQGXffbWVldW9/YLG2Vt3d29/YrB4ctE6ea8SaLZaw7ATVcCsWbKFDyTqI5jQLJ28H4NvfbT1wbEasHnCTcj+hQiVAwilZq9QYxmnK/UnVr7gxkmXgFqUKBRr/yZYMsjbhCJqkxXc9N0M+oRsEkn5Z7qeEJZWM65F1LFY248bPZtlNyapUBCWNtn0IyU38nMhoZM4kCOxlRHJlFLxf/87ophtd+JlSSIlds/lGYSoIxyU"
2021.acl-long.370,P19-1294,0,0.0125111,"tems, the core to achieve this completion is prefix-constrained decoding (Wuebker et al., 2016). Another sentence-level autocompletion method, lexically constrained decoding (LCD) (Hokamp and Liu, 2017; Post and Vilar, 2018), recently attracts lots of attention (Hasler et al., 2018; Susanto et al., 2020; Kajiwara, 2019). Compared with IMT, LCD relaxes the constraints provided by human translators from prefixes to general forms: LCD completes a translation based on some unordered words (i.e., lexical constraints), which are not necessary to be continuous (Hokamp and Liu, 2017; Hu et al., 2019; Dinu et al., 2019; Song et al., 2019). Although it does not need additional training, its inference is typically less efficient compared with the standard NMT. Therefore, other works propose efficient methods (Li et al., 2020; Song et al., 2019) by using lexical constraints in a soft manner rather than a hard manner as in LCD. Word-level Autocompletion Word-level autocompletion for CAT is less studied than sentencelevel autocompletion. Langlais et al. (2000); Santy et al. (2019) consider to complete a target word based on human typed characters and a translation prefix. But they require the target word to be t"
2021.acl-long.370,D14-1130,0,0.260663,"statistical machine translation (SMT) models (Koehn et al., 2003; Chiang, 2005; Koehn, 1 The information of benchmark datasets is in https: //github.com/ghrua/gwlan 2009). In spite of this, MT systems cannot replace human translators, especially in the scenarios with rigorous translation quality requirements (e.g., translating product manuals, patent documents, government policies, and other official documents). Therefore, how to leverage the pros of MT systems to help human translators, namely, Computer-aided translation (CAT), attracts the attention of researchers (Barrachina et al., 2009; Green et al., 2014; Knowles and Koehn, 2016; Santy et al., 2019). Among all CAT technologies (such as translation memory, terminology management, sample sentence search, etc.), autocompletion plays an important role in a CAT system in enhancing translation efficiency. Autocompletion suggests translation results according to the text pieces provided by human translators. We note two limitations in previous research on the topic of autocompletion for CAT. First, most of previous studies aim to save human efforts by sentence-level autocompletion (Figure 1 a). Nevertheless, word-level autocompletion (Figure 1 b 479"
2021.acl-long.370,N18-2081,0,0.0245608,"Missing"
2021.acl-long.370,P17-1141,0,0.0583182,"onstruct the first public benchmark to facilitate research in this topic. • We propose a joint training strategy to optimize the model parameters on different types of contexts together. 2 2 Related Work Computer-aided translation (CAT) is a widely used practice when using MT technology in the industry. 2 This approach has been implemented into a humanmachine interactive translation system TranSmart (Huang et al., 2021) at www.transmart.qq.com. As the the MT systems advanced and improved, various efficient interaction ways of CAT have emerged (Vasconcellos and Le´on, 1985; Green et al., 2014; Hokamp and Liu, 2017; Weng et al., 2019; Wang et al., 2020). Among those different methods, the autocompletion is the most related to our work. Therefore, we will first describe previous works in both sentence-level and word-level autocompletion, then show the relation to other tasks and scenarios. Sentence-level Autocompletion Most of previous work in autocompletion for CAT focus on sentence-level completion. A common use case in this line is interactive machine translation (IMT) (Green et al., 2014; Cheng et al., 2016; Peris et al., 2017; Knowles and Koehn, 2016; Santy et al., 2019). IMT systems utilize MT syst"
2021.acl-long.370,N19-1090,0,0.0330547,"Missing"
2021.acl-long.370,P18-4024,0,0.0267046,"the proposed word-level autocompletion is more general and can be applied to real-world scenarios such as post-editing (Vasconcellos and Le´on, 1985; 4793 Green et al., 2013) and LCD, where human translators need to input some words (corrections or constraints). Huang et al. (2015) propose a method to predict a target word based on human typed characters, however, this method only uses the source side information and does not consider translation context, leading to limited performance compared with our work. Others Our work may also be related to previous works in input method editors (IME) (Huang et al., 2018; Lee et al., 2007). However, they are in the monolingual setting and not capable of using the useful multilingual information. 3 Task and Benchmark In this section, we first describe why we need wordlevel autocompletion in real-world CAT scenarios. We then present the details of the GWLAN task and the construction of benchmark. Why GWLAN? Word level autocompletion is beneficial for improving input efficiency (Langlais et al., 2000). Previous works assume that the translation context should be a prefix and the desired word is next to the prefix as shown in Figure 1 (b), where the context is “W"
2021.acl-long.370,P19-1607,0,0.0132864,"(IMT) (Green et al., 2014; Cheng et al., 2016; Peris et al., 2017; Knowles and Koehn, 2016; Santy et al., 2019). IMT systems utilize MT systems to complete the rest of a translation after human translators editing a prefix translation (Alabau et al., 2014; Zhao et al., 2020). For most IMT systems, the core to achieve this completion is prefix-constrained decoding (Wuebker et al., 2016). Another sentence-level autocompletion method, lexically constrained decoding (LCD) (Hokamp and Liu, 2017; Post and Vilar, 2018), recently attracts lots of attention (Hasler et al., 2018; Susanto et al., 2020; Kajiwara, 2019). Compared with IMT, LCD relaxes the constraints provided by human translators from prefixes to general forms: LCD completes a translation based on some unordered words (i.e., lexical constraints), which are not necessary to be continuous (Hokamp and Liu, 2017; Hu et al., 2019; Dinu et al., 2019; Song et al., 2019). Although it does not need additional training, its inference is typically less efficient compared with the standard NMT. Therefore, other works propose efficient methods (Li et al., 2020; Song et al., 2019) by using lexical constraints in a soft manner rather than a hard manner as"
2021.acl-long.370,2016.amta-researchers.9,0,0.144968,"translation (SMT) models (Koehn et al., 2003; Chiang, 2005; Koehn, 1 The information of benchmark datasets is in https: //github.com/ghrua/gwlan 2009). In spite of this, MT systems cannot replace human translators, especially in the scenarios with rigorous translation quality requirements (e.g., translating product manuals, patent documents, government policies, and other official documents). Therefore, how to leverage the pros of MT systems to help human translators, namely, Computer-aided translation (CAT), attracts the attention of researchers (Barrachina et al., 2009; Green et al., 2014; Knowles and Koehn, 2016; Santy et al., 2019). Among all CAT technologies (such as translation memory, terminology management, sample sentence search, etc.), autocompletion plays an important role in a CAT system in enhancing translation efficiency. Autocompletion suggests translation results according to the text pieces provided by human translators. We note two limitations in previous research on the topic of autocompletion for CAT. First, most of previous studies aim to save human efforts by sentence-level autocompletion (Figure 1 a). Nevertheless, word-level autocompletion (Figure 1 b 4792 Proceedings of the 59th"
2021.acl-long.370,P09-5002,0,0.109029,"Missing"
2021.acl-long.370,N03-1017,0,0.0432236,"ion, where the gray part is the completion generated by MT system. Both (b) and (c) are word-level autocompletion, underlined text “sp” is the human typed characters and the words in the rounded rectangles are word-level autocompletion candidates. Introduction Machine translation (MT) has witnessed great advancements with the emergence of neural machine translation (NMT) (Sutskever et al., 2014; Bahdanau et al., 2015; Wu et al., 2016; Gehring et al., 2017; Vaswani et al., 2017), which is able to produce much higher quality translation results than statistical machine translation (SMT) models (Koehn et al., 2003; Chiang, 2005; Koehn, 1 The information of benchmark datasets is in https: //github.com/ghrua/gwlan 2009). In spite of this, MT systems cannot replace human translators, especially in the scenarios with rigorous translation quality requirements (e.g., translating product manuals, patent documents, government policies, and other official documents). Therefore, how to leverage the pros of MT systems to help human translators, namely, Computer-aided translation (CAT), attracts the attention of researchers (Barrachina et al., 2009; Green et al., 2014; Knowles and Koehn, 2016; Santy et al., 2019)."
2021.acl-long.370,W00-0507,0,0.365678,"letes a translation based on some unordered words (i.e., lexical constraints), which are not necessary to be continuous (Hokamp and Liu, 2017; Hu et al., 2019; Dinu et al., 2019; Song et al., 2019). Although it does not need additional training, its inference is typically less efficient compared with the standard NMT. Therefore, other works propose efficient methods (Li et al., 2020; Song et al., 2019) by using lexical constraints in a soft manner rather than a hard manner as in LCD. Word-level Autocompletion Word-level autocompletion for CAT is less studied than sentencelevel autocompletion. Langlais et al. (2000); Santy et al. (2019) consider to complete a target word based on human typed characters and a translation prefix. But they require the target word to be the next word of the translation prefix, which limits its application. In contrast, in our work the proposed word-level autocompletion is more general and can be applied to real-world scenarios such as post-editing (Vasconcellos and Le´on, 1985; 4793 Green et al., 2013) and LCD, where human translators need to input some words (corrections or constraints). Huang et al. (2015) propose a method to predict a target word based on human typed char"
2021.acl-long.370,N18-1119,0,0.0219315,"T focus on sentence-level completion. A common use case in this line is interactive machine translation (IMT) (Green et al., 2014; Cheng et al., 2016; Peris et al., 2017; Knowles and Koehn, 2016; Santy et al., 2019). IMT systems utilize MT systems to complete the rest of a translation after human translators editing a prefix translation (Alabau et al., 2014; Zhao et al., 2020). For most IMT systems, the core to achieve this completion is prefix-constrained decoding (Wuebker et al., 2016). Another sentence-level autocompletion method, lexically constrained decoding (LCD) (Hokamp and Liu, 2017; Post and Vilar, 2018), recently attracts lots of attention (Hasler et al., 2018; Susanto et al., 2020; Kajiwara, 2019). Compared with IMT, LCD relaxes the constraints provided by human translators from prefixes to general forms: LCD completes a translation based on some unordered words (i.e., lexical constraints), which are not necessary to be continuous (Hokamp and Liu, 2017; Hu et al., 2019; Dinu et al., 2019; Song et al., 2019). Although it does not need additional training, its inference is typically less efficient compared with the standard NMT. Therefore, other works propose efficient methods (Li et al., 202"
2021.acl-long.370,D19-3018,0,0.0897867,"(Koehn et al., 2003; Chiang, 2005; Koehn, 1 The information of benchmark datasets is in https: //github.com/ghrua/gwlan 2009). In spite of this, MT systems cannot replace human translators, especially in the scenarios with rigorous translation quality requirements (e.g., translating product manuals, patent documents, government policies, and other official documents). Therefore, how to leverage the pros of MT systems to help human translators, namely, Computer-aided translation (CAT), attracts the attention of researchers (Barrachina et al., 2009; Green et al., 2014; Knowles and Koehn, 2016; Santy et al., 2019). Among all CAT technologies (such as translation memory, terminology management, sample sentence search, etc.), autocompletion plays an important role in a CAT system in enhancing translation efficiency. Autocompletion suggests translation results according to the text pieces provided by human translators. We note two limitations in previous research on the topic of autocompletion for CAT. First, most of previous studies aim to save human efforts by sentence-level autocompletion (Figure 1 a). Nevertheless, word-level autocompletion (Figure 1 b 4792 Proceedings of the 59th Annual Meeting of th"
2021.acl-long.370,N19-1044,0,0.0154186,"chieve this completion is prefix-constrained decoding (Wuebker et al., 2016). Another sentence-level autocompletion method, lexically constrained decoding (LCD) (Hokamp and Liu, 2017; Post and Vilar, 2018), recently attracts lots of attention (Hasler et al., 2018; Susanto et al., 2020; Kajiwara, 2019). Compared with IMT, LCD relaxes the constraints provided by human translators from prefixes to general forms: LCD completes a translation based on some unordered words (i.e., lexical constraints), which are not necessary to be continuous (Hokamp and Liu, 2017; Hu et al., 2019; Dinu et al., 2019; Song et al., 2019). Although it does not need additional training, its inference is typically less efficient compared with the standard NMT. Therefore, other works propose efficient methods (Li et al., 2020; Song et al., 2019) by using lexical constraints in a soft manner rather than a hard manner as in LCD. Word-level Autocompletion Word-level autocompletion for CAT is less studied than sentencelevel autocompletion. Langlais et al. (2000); Santy et al. (2019) consider to complete a target word based on human typed characters and a translation prefix. But they require the target word to be the next word of the"
2021.acl-long.370,2020.acl-main.325,0,0.0111079,"ve machine translation (IMT) (Green et al., 2014; Cheng et al., 2016; Peris et al., 2017; Knowles and Koehn, 2016; Santy et al., 2019). IMT systems utilize MT systems to complete the rest of a translation after human translators editing a prefix translation (Alabau et al., 2014; Zhao et al., 2020). For most IMT systems, the core to achieve this completion is prefix-constrained decoding (Wuebker et al., 2016). Another sentence-level autocompletion method, lexically constrained decoding (LCD) (Hokamp and Liu, 2017; Post and Vilar, 2018), recently attracts lots of attention (Hasler et al., 2018; Susanto et al., 2020; Kajiwara, 2019). Compared with IMT, LCD relaxes the constraints provided by human translators from prefixes to general forms: LCD completes a translation based on some unordered words (i.e., lexical constraints), which are not necessary to be continuous (Hokamp and Liu, 2017; Hu et al., 2019; Dinu et al., 2019; Song et al., 2019). Although it does not need additional training, its inference is typically less efficient compared with the standard NMT. Therefore, other works propose efficient methods (Li et al., 2020; Song et al., 2019) by using lexical constraints in a soft manner rather than"
2021.acl-long.370,J85-2003,0,0.645723,"Missing"
2021.acl-long.370,2020.aacl-main.1,1,0.736288,"cilitate research in this topic. • We propose a joint training strategy to optimize the model parameters on different types of contexts together. 2 2 Related Work Computer-aided translation (CAT) is a widely used practice when using MT technology in the industry. 2 This approach has been implemented into a humanmachine interactive translation system TranSmart (Huang et al., 2021) at www.transmart.qq.com. As the the MT systems advanced and improved, various efficient interaction ways of CAT have emerged (Vasconcellos and Le´on, 1985; Green et al., 2014; Hokamp and Liu, 2017; Weng et al., 2019; Wang et al., 2020). Among those different methods, the autocompletion is the most related to our work. Therefore, we will first describe previous works in both sentence-level and word-level autocompletion, then show the relation to other tasks and scenarios. Sentence-level Autocompletion Most of previous work in autocompletion for CAT focus on sentence-level completion. A common use case in this line is interactive machine translation (IMT) (Green et al., 2014; Cheng et al., 2016; Peris et al., 2017; Knowles and Koehn, 2016; Santy et al., 2019). IMT systems utilize MT systems to complete the rest of a translati"
2021.acl-long.370,P16-1007,0,0.0160925,"then show the relation to other tasks and scenarios. Sentence-level Autocompletion Most of previous work in autocompletion for CAT focus on sentence-level completion. A common use case in this line is interactive machine translation (IMT) (Green et al., 2014; Cheng et al., 2016; Peris et al., 2017; Knowles and Koehn, 2016; Santy et al., 2019). IMT systems utilize MT systems to complete the rest of a translation after human translators editing a prefix translation (Alabau et al., 2014; Zhao et al., 2020). For most IMT systems, the core to achieve this completion is prefix-constrained decoding (Wuebker et al., 2016). Another sentence-level autocompletion method, lexically constrained decoding (LCD) (Hokamp and Liu, 2017; Post and Vilar, 2018), recently attracts lots of attention (Hasler et al., 2018; Susanto et al., 2020; Kajiwara, 2019). Compared with IMT, LCD relaxes the constraints provided by human translators from prefixes to general forms: LCD completes a translation based on some unordered words (i.e., lexical constraints), which are not necessary to be continuous (Hokamp and Liu, 2017; Hu et al., 2019; Dinu et al., 2019; Song et al., 2019). Although it does not need additional training, its infer"
2021.acl-long.567,2020.acl-main.692,0,0.0209311,"s shown in Table 3, our method performs better than BT with 2/4 bilingual pairs but performs worse with 1/4 bilingual pairs. Interestingly, the combination of BT and our method yields significant further gains, which demonstrates that our method is not only orthogonal but also complementary to BT. Non-parametric Domain Adaptation Lastly, the “plug and play” property of TM further motivates us to domain adaptation, where we adapt a single general-domain model to a specific domain by using domain-specific monolingual TM. Data To simulate a diverse multi-domain setting, we use the data splits in Aharoni and Goldberg (2020) originally collected by Koehn and Knowles (2017). It includes German-English parallel data for train/dev/test sets in five domains: Medical, Law, IT, Koran and Subtitles. Similar to the experiments in §4.3, we only use one fourth of bilingual pairs for training. The target side of the remaining data is treated as additional monolingual data for building domain-specific TM, and the source side is discarded. The data statistics can be found in the upper block of Table 4. The dev and test sets for each domain contains 2K instances. Models We first train a Transformer Base baseline (model #1) on"
2021.acl-long.567,P19-1175,0,0.185069,"dditional cues. Recent work has confirmed that TM can help Neural Machine Translation (NMT) models as well. In a similar spirit to prior work, TM-augmented NMT models do not discard the training corpus after training but keep exploiting it in the test time. These models perform translation in two stages: In the retrieval stage, a retriever searches for nearest neighbors (i.e., source-target pairs) from the training corpus based on source-side similarity such as lexical overlaps (Gu et al., 2018; Zhang et al., 2018; Xia et al., 2019), embedding-based matches (Cao and Xiong, 2018), or a hybrid (Bulte and Tezcan, 2019; Xu et al., 2020); In the generation stage, the retrieved translations are injected into a standard NMT model by attending over them with sophisticated memory networks (Gu et al., 2018; Cao and Xiong, 2018; Xia et al., 2019; He et al., 2021) or directly concatenating them to the source input (Bulte and Tezcan, 2019; Xu et al., 2020), or biasing the word distribution during decoding (Zhang et al., 2018). Most recently, Khandelwal et al. (2020) propose a token-level nearest neighbor search using complete translation context, i.e., both the source-side input and target-side prefix. Despite their"
2021.acl-long.567,N19-1124,1,0.849786,"Missing"
2021.acl-long.567,D19-1195,1,0.896459,"Missing"
2021.acl-long.567,1981.tc-1.7,0,0.702124,"Missing"
2021.acl-long.567,D18-1340,0,0.0657347,"statistical translation models as additional cues. Recent work has confirmed that TM can help Neural Machine Translation (NMT) models as well. In a similar spirit to prior work, TM-augmented NMT models do not discard the training corpus after training but keep exploiting it in the test time. These models perform translation in two stages: In the retrieval stage, a retriever searches for nearest neighbors (i.e., source-target pairs) from the training corpus based on source-side similarity such as lexical overlaps (Gu et al., 2018; Zhang et al., 2018; Xia et al., 2019), embedding-based matches (Cao and Xiong, 2018), or a hybrid (Bulte and Tezcan, 2019; Xu et al., 2020); In the generation stage, the retrieved translations are injected into a standard NMT model by attending over them with sophisticated memory networks (Gu et al., 2018; Cao and Xiong, 2018; Xia et al., 2019; He et al., 2021) or directly concatenating them to the source input (Bulte and Tezcan, 2019; Xu et al., 2020), or biasing the word distribution during decoding (Zhang et al., 2018). Most recently, Khandelwal et al. (2020) propose a token-level nearest neighbor search using complete translation context, i.e., both the source-side input"
2021.acl-long.567,P17-1171,0,0.024959,"den states for later nearest neighbor search at each decoding step, which is very compute-intensive. The distinctions between our work and prior work are obvious: (1) The TM in our framework is a collection of monolingual sentences rather than bilingual sentence pairs; (2) We use learnable task-specific retrieval rather than generic retrieval mechanisms. Retrieval for Text Generation Discrete retrieval as an intermediate step has been shown beneficial to a variety of natural language processing tasks. One typical use is to retrieve supporting evidence for open-domain question answering (e.g., Chen et al., 2017; Lee et al., 2019; Karpukhin et al., 2020). Recently, retrieval-guided generation has gained increasing interest in a wide range of text generation tasks such as language modeling (Guu et al., 2018; Khandelwal et al., 2019; Guu et al., 2020), dialogue response generation (Weston et al., 2018; Wu et al., 2019; Cai et al., 7308 Retrieval Model Input x source sentence encoder ?""#$ (?) target sentence encoder ?()( (?) Translation Memory ? Translation Model ?1 ?5 … Source Encoder ?(?, ?1 ) ?(?, ?5 ) … Memory Encoder relevant TM MIPS … Decoder relevance scores ?-,/ Output y 23 /01 bias attention de"
2021.acl-long.567,C18-1111,0,0.0275205,"Missing"
2021.acl-long.567,N19-1423,0,0.0187413,"optimized for the ultimate translation goal. Experiments show that the proposed method obtains substantial improvements. Remarkably, it even outperforms strong TM-augmented NMT baselines using bilingual TM. Owning to the ability to leverage monolingual data, our model also demonstrates effectiveness in low-resource and domain adaptation scenarios. 1 Introduction Augmenting parametric neural network models with non-parametric memory (Khandelwal et al., 2019; Guu et al., 2020; Lewis et al., 2020a,b) has recently emerged as a promising direction to relieve the demand for ever-larger model size (Devlin et al., 2019; Radford et al., 2019; Brown et al., 2020). For the task of Machine Translation (MT), inspired by the Computer-Aided Translation (CAT) tools by professional human translators for increasing productivity for decades (Yamada, 2011), the usefulness of Translation Memory (TM) has long been recognized (Huang et al., 2021). In general, TM is a database that stores pairs of source text and its corresponding translations. Like for human ∗ The work described in this paper is partially supported by a grant from the Research Grant Council of the Hong Kong Special Administrative Region, China (Project Co"
2021.acl-long.567,D18-1045,0,0.0241265,"s-lingual setting. NMT using Monolingual Data To our knowledge, the integration of monolingual data for NMT was first investigated by Gulcehre et al. (2015), who separately trained target-side language models using monolingual data, and then integrated them during decoding either through re-scoring the beam, or by feeding the hidden state of the language model to the NMT model. Jean et al. (2015) also explored re-ranking the NMT output with a n-gram language model. Another successful method for leveraging monolingual data in NMT is back-translation (Sennrich et al., 2016; Fadaee et al., 2017; Edunov et al., 2018; He et al., 2016), where a reverse translation model is used to translate monolingual sentences from the target language to the source language to generate synthetic parallel sentences. Recent studies (Jiao et al., 2021; He et al., 2019) showed that self-training, where the synthetic parallel sentences are created by translating monolingual sentences in the source language, is also helpful. Our method is orthogonal to previous work and bears a unique feature: it can use more monolingual data without re-training (see §4.3). 3 Proposed Approach We start by formalizing the translation task as a"
2021.acl-long.567,P17-2090,0,0.0274589,"nging due to the cross-lingual setting. NMT using Monolingual Data To our knowledge, the integration of monolingual data for NMT was first investigated by Gulcehre et al. (2015), who separately trained target-side language models using monolingual data, and then integrated them during decoding either through re-scoring the beam, or by feeding the hidden state of the language model to the NMT model. Jean et al. (2015) also explored re-ranking the NMT output with a n-gram language model. Another successful method for leveraging monolingual data in NMT is back-translation (Sennrich et al., 2016; Fadaee et al., 2017; Edunov et al., 2018; He et al., 2016), where a reverse translation model is used to translate monolingual sentences from the target language to the source language to generate synthetic parallel sentences. Recent studies (Jiao et al., 2021; He et al., 2019) showed that self-training, where the synthetic parallel sentences are created by translating monolingual sentences in the source language, is also helpful. Our method is orthogonal to previous work and bears a unique feature: it can use more monolingual data without re-training (see §4.3). 3 Proposed Approach We start by formalizing the t"
2021.acl-long.567,D17-1146,0,0.0127575,"even outperforming strong TMaugmented baselines. This is remarkable given that previous TM-augmented models rely on bilingual TM while our model only exploits the target side. (2) Our model can substantially boost translation quality in low-resource scenarios by utilizing extra monolingual TM that is not present in training pairs. (3) Our model gains a strong cross-domain transferability by hot-swapping domain-specific monolingual memory. 2 Related Work TM-augmented NMT This work contributes primarily to the research line of Translation Memory (TM) augmented Neural Machine Translation (NMT). Feng et al. (2017) augmented NMT with a bilingual dictionary to tackle infrequent word translation. Gu et al. (2018) proposed a model that retrieves examples similar to the test source sentence and encodes retrieved source-target pairs with keyvalue memory networks. Cao and Xiong (2018); Cao et al. (2019) used a gating mechanism to balance the impact of the translation memory. Zhang et al. (2018) proposed guiding models by retrieving n-grams and up-weighting the probabilities of retrieved n-grams. Bulte and Tezcan (2019) and Xu et al. (2020) used fuzzy-matching with translation memories and augment source seque"
2021.acl-long.567,P16-1154,0,0.0122898,"re Li is the length of the token sequence zi . We compute a cross attention over all TM sentences: exp(ht T Wm zi,j )) αij = PM PL T i i=1 k=1 exp(ht Wm zi,k ) ct = Wc Li M X X (1) αij zi,j i=1 j=1 where αij is the attention score of the j-th token in zi , ct is a weighted combination of memory embeddings, and Wm and Wc are trainable matrices. The cross attention is used twice during decoding. First, the decoder’s hidden state ht is updated by a weighted sum of memory embeddings, i.e., ht = ht + ct . Second, we consider each attention score as a probability of copying the corresponding token (Gu et al., 2016; See et al., 2017). Formally, the next-token probabilities are computed as: p(yt |·) = (1 − λt )Pv (yt ) + λt Li M X X αij 1zij =yt i=1 j=1 where 1 is the indicator function and λt is a gating variable computed by another feed-forward network λt = g(ht , ct ). Inspired by Lewis et al. (2020a), to enable the gradient flow from the translation output to the retrieval model, we bias the attention scores with the relevance scores, rewriting Eq. (1) as: exp(ht T Wm zi,j + βf (x, zi )) αij = PM PL T i i=1 k=1 exp(ht Wm zi,k + βf (x, zi )) (2) where β is a trainable scalar that controls the weight o"
2021.acl-long.567,Q18-1031,0,0.0281054,"lection of monolingual sentences rather than bilingual sentence pairs; (2) We use learnable task-specific retrieval rather than generic retrieval mechanisms. Retrieval for Text Generation Discrete retrieval as an intermediate step has been shown beneficial to a variety of natural language processing tasks. One typical use is to retrieve supporting evidence for open-domain question answering (e.g., Chen et al., 2017; Lee et al., 2019; Karpukhin et al., 2020). Recently, retrieval-guided generation has gained increasing interest in a wide range of text generation tasks such as language modeling (Guu et al., 2018; Khandelwal et al., 2019; Guu et al., 2020), dialogue response generation (Weston et al., 2018; Wu et al., 2019; Cai et al., 7308 Retrieval Model Input x source sentence encoder ?""#$ (?) target sentence encoder ?()( (?) Translation Memory ? Translation Model ?1 ?5 … Source Encoder ?(?, ?1 ) ?(?, ?5 ) … Memory Encoder relevant TM MIPS … Decoder relevance scores ?-,/ Output y 23 /01 bias attention dense index Figure 1: Overall framework. For an input sentence x in the source language, the retrieval model uses Maximum Inner Product Search (MIPS) to find the top-M TM sentences {zi }M i=1 in the t"
2021.acl-long.567,2021.acl-long.246,1,0.808584,"est time. These models perform translation in two stages: In the retrieval stage, a retriever searches for nearest neighbors (i.e., source-target pairs) from the training corpus based on source-side similarity such as lexical overlaps (Gu et al., 2018; Zhang et al., 2018; Xia et al., 2019), embedding-based matches (Cao and Xiong, 2018), or a hybrid (Bulte and Tezcan, 2019; Xu et al., 2020); In the generation stage, the retrieved translations are injected into a standard NMT model by attending over them with sophisticated memory networks (Gu et al., 2018; Cao and Xiong, 2018; Xia et al., 2019; He et al., 2021) or directly concatenating them to the source input (Bulte and Tezcan, 2019; Xu et al., 2020), or biasing the word distribution during decoding (Zhang et al., 2018). Most recently, Khandelwal et al. (2020) propose a token-level nearest neighbor search using complete translation context, i.e., both the source-side input and target-side prefix. Despite their differences, we identify two major limitations in previous research. First, the translation memory has to be a bilingual corpus consisting of aligned source-target pairs. This requirement limits the memory bank to bilingual pairs and preclud"
2021.acl-long.567,P10-1064,0,0.0531179,"Missing"
2021.acl-long.567,W15-3014,0,0.025228,"on from this line of research. However, retrieval-guided generation has so far been mainly investigated for knowledge retrieval in the same language. The memory retrieval in this work is more challenging due to the cross-lingual setting. NMT using Monolingual Data To our knowledge, the integration of monolingual data for NMT was first investigated by Gulcehre et al. (2015), who separately trained target-side language models using monolingual data, and then integrated them during decoding either through re-scoring the beam, or by feeding the hidden state of the language model to the NMT model. Jean et al. (2015) also explored re-ranking the NMT output with a n-gram language model. Another successful method for leveraging monolingual data in NMT is back-translation (Sennrich et al., 2016; Fadaee et al., 2017; Edunov et al., 2018; He et al., 2016), where a reverse translation model is used to translate monolingual sentences from the target language to the source language to generate synthetic parallel sentences. Recent studies (Jiao et al., 2021; He et al., 2019) showed that self-training, where the synthetic parallel sentences are created by translating monolingual sentences in the source language, is"
2021.acl-long.567,2021.acl-long.221,0,0.0323651,"gual data, and then integrated them during decoding either through re-scoring the beam, or by feeding the hidden state of the language model to the NMT model. Jean et al. (2015) also explored re-ranking the NMT output with a n-gram language model. Another successful method for leveraging monolingual data in NMT is back-translation (Sennrich et al., 2016; Fadaee et al., 2017; Edunov et al., 2018; He et al., 2016), where a reverse translation model is used to translate monolingual sentences from the target language to the source language to generate synthetic parallel sentences. Recent studies (Jiao et al., 2021; He et al., 2019) showed that self-training, where the synthetic parallel sentences are created by translating monolingual sentences in the source language, is also helpful. Our method is orthogonal to previous work and bears a unique feature: it can use more monolingual data without re-training (see §4.3). 3 Proposed Approach We start by formalizing the translation task as a retrieve-then-generate process in §3.1. Then in §3.2, we describe the model design for the crosslingual memory retrieval model. In §3.3, we describe the model design for the memory-augmented translation model. Lastly, we"
2021.acl-long.567,P10-2041,0,0.268089,"Missing"
2021.acl-long.567,2020.emnlp-main.550,0,0.0301666,"search at each decoding step, which is very compute-intensive. The distinctions between our work and prior work are obvious: (1) The TM in our framework is a collection of monolingual sentences rather than bilingual sentence pairs; (2) We use learnable task-specific retrieval rather than generic retrieval mechanisms. Retrieval for Text Generation Discrete retrieval as an intermediate step has been shown beneficial to a variety of natural language processing tasks. One typical use is to retrieve supporting evidence for open-domain question answering (e.g., Chen et al., 2017; Lee et al., 2019; Karpukhin et al., 2020). Recently, retrieval-guided generation has gained increasing interest in a wide range of text generation tasks such as language modeling (Guu et al., 2018; Khandelwal et al., 2019; Guu et al., 2020), dialogue response generation (Weston et al., 2018; Wu et al., 2019; Cai et al., 7308 Retrieval Model Input x source sentence encoder ?""#$ (?) target sentence encoder ?()( (?) Translation Memory ? Translation Model ?1 ?5 … Source Encoder ?(?, ?1 ) ?(?, ?5 ) … Memory Encoder relevant TM MIPS … Decoder relevance scores ?-,/ Output y 23 /01 bias attention dense index Figure 1: Overall framework. For"
2021.acl-long.567,W04-3250,0,0.189957,"al (fixed) 66.68 66.24 63.06 62.73 cross-lingual (fixed Etgt )† 67.66 67.16 63.73 63.22 cross-lingual† 67.73 67.42 64.18 63.86 Retriever De⇒En Dev Test En⇒De Dev Test 60.10 61.85 60.26 61.72 55.54 57.43 55.14 56.88 59.82 63.62 63.25 64.39 64.48 60.76 63.85 63.06 64.01 64.62 55.01 57.88 57.61 58.12 58.77 54.90 57.53 56.97 57.92 58.42 Table 2: Experimental results (BLEU scores) on four translation tasks. ∗ Results are from Xia et al. (2019). †The two variants of our method (model #4 and model #5) are significantly better than other baselines with p-value < 0.01, tested by bootstrap re-sampling (Koehn, 2004). 2. TM-augmented NMT using source similarity search. To isolate the effect of architectural changes in NMT models, we replace our cross-lingual memory retriever with traditional source-side similarity search. Specifically, we use the fuzzy match system used in Xia et al. (2019) and many others, which is based on BM25 and edit distance. steps throughout all experiments. When trained with asynchronous index refresh, the re-indexing interval is 3K training steps.1 4.2 Conventional Experiments Following prior work in TM-augmented NMT, we first conduct experiments in a setting where the bilingual"
2021.acl-long.567,W17-3204,0,0.0269431,"BT with 2/4 bilingual pairs but performs worse with 1/4 bilingual pairs. Interestingly, the combination of BT and our method yields significant further gains, which demonstrates that our method is not only orthogonal but also complementary to BT. Non-parametric Domain Adaptation Lastly, the “plug and play” property of TM further motivates us to domain adaptation, where we adapt a single general-domain model to a specific domain by using domain-specific monolingual TM. Data To simulate a diverse multi-domain setting, we use the data splits in Aharoni and Goldberg (2020) originally collected by Koehn and Knowles (2017). It includes German-English parallel data for train/dev/test sets in five domains: Medical, Law, IT, Koran and Subtitles. Similar to the experiments in §4.3, we only use one fourth of bilingual pairs for training. The target side of the remaining data is treated as additional monolingual data for building domain-specific TM, and the source side is discarded. The data statistics can be found in the upper block of Table 4. The dev and test sets for each domain contains 2K instances. Models We first train a Transformer Base baseline (model #1) on the concatenation of bilingual pairs in all domai"
2021.acl-long.567,2010.jec-1.4,0,0.0246038,"Translation (MT), inspired by the Computer-Aided Translation (CAT) tools by professional human translators for increasing productivity for decades (Yamada, 2011), the usefulness of Translation Memory (TM) has long been recognized (Huang et al., 2021). In general, TM is a database that stores pairs of source text and its corresponding translations. Like for human ∗ The work described in this paper is partially supported by a grant from the Research Grant Council of the Hong Kong Special Administrative Region, China (Project Code: 14200719). Correspondence to Yan Wang. translation, early work (Koehn and Senellart, 2010; He et al., 2010; Utiyama et al., 2011; Wang et al., 2013, inter alia) presents translations for similar source input to statistical translation models as additional cues. Recent work has confirmed that TM can help Neural Machine Translation (NMT) models as well. In a similar spirit to prior work, TM-augmented NMT models do not discard the training corpus after training but keep exploiting it in the test time. These models perform translation in two stages: In the retrieval stage, a retriever searches for nearest neighbors (i.e., source-target pairs) from the training corpus based on source-s"
2021.acl-long.567,P19-1612,0,0.0249513,"r nearest neighbor search at each decoding step, which is very compute-intensive. The distinctions between our work and prior work are obvious: (1) The TM in our framework is a collection of monolingual sentences rather than bilingual sentence pairs; (2) We use learnable task-specific retrieval rather than generic retrieval mechanisms. Retrieval for Text Generation Discrete retrieval as an intermediate step has been shown beneficial to a variety of natural language processing tasks. One typical use is to retrieve supporting evidence for open-domain question answering (e.g., Chen et al., 2017; Lee et al., 2019; Karpukhin et al., 2020). Recently, retrieval-guided generation has gained increasing interest in a wide range of text generation tasks such as language modeling (Guu et al., 2018; Khandelwal et al., 2019; Guu et al., 2020), dialogue response generation (Weston et al., 2018; Wu et al., 2019; Cai et al., 7308 Retrieval Model Input x source sentence encoder ?""#$ (?) target sentence encoder ?()( (?) Translation Memory ? Translation Model ?1 ?5 … Source Encoder ?(?, ?1 ) ?(?, ?5 ) … Memory Encoder relevant TM MIPS … Decoder relevance scores ?-,/ Output y 23 /01 bias attention dense index Figure 1"
2021.acl-long.567,P02-1040,0,0.111161,"in gradient estimate. We explore both options in our experiments. 4 Experiments We experiment with the proposed approach in three settings: (1) the conventional setting where the available TM is limited to the bilingual training corpus, (2) the low-resource setting where bilingual training pairs are scarce but extra monolingual data is exploited as additional TM, and (3) nonparametric domain adaptation using monolingual TM. Note that existing TM-augmented NMT models are only applicable to the first setting, the last two settings only become possible with our proposed model. We use BLEU score (Papineni et al., 2002) as the evaluation metric. 4.1 The second task is token-level cross-alignment, which aims to predict the tokens in the target language given the source sentence representation and vice versa. Formally, we use bag-of-words losses: X X (i) Ltok = − log p(wy |Xi ) + log p(wx |Yi ) wy ∈Yi Dataset En⇔Es En⇔De Implementation Details We build our model using Transformer blocks with the same configuration as Transformer Base (Vaswani et al., 2017) (8 attention heads, 512 dimensional hidden state, and 2048 dimensional feed-forward state). The number of Transformer blocks is 3 for the retrieval model, 4"
2021.acl-long.567,P17-1099,0,0.0281594,"th of the token sequence zi . We compute a cross attention over all TM sentences: exp(ht T Wm zi,j )) αij = PM PL T i i=1 k=1 exp(ht Wm zi,k ) ct = Wc Li M X X (1) αij zi,j i=1 j=1 where αij is the attention score of the j-th token in zi , ct is a weighted combination of memory embeddings, and Wm and Wc are trainable matrices. The cross attention is used twice during decoding. First, the decoder’s hidden state ht is updated by a weighted sum of memory embeddings, i.e., ht = ht + ct . Second, we consider each attention score as a probability of copying the corresponding token (Gu et al., 2016; See et al., 2017). Formally, the next-token probabilities are computed as: p(yt |·) = (1 − λt )Pv (yt ) + λt Li M X X αij 1zij =yt i=1 j=1 where 1 is the indicator function and λt is a gating variable computed by another feed-forward network λt = g(ht , ct ). Inspired by Lewis et al. (2020a), to enable the gradient flow from the translation output to the retrieval model, we bias the attention scores with the relevance scores, rewriting Eq. (1) as: exp(ht T Wm zi,j + βf (x, zi )) αij = PM PL T i i=1 k=1 exp(ht Wm zi,k + βf (x, zi )) (2) where β is a trainable scalar that controls the weight of the relevance sco"
2021.acl-long.567,P16-1009,0,0.219679,"his work is more challenging due to the cross-lingual setting. NMT using Monolingual Data To our knowledge, the integration of monolingual data for NMT was first investigated by Gulcehre et al. (2015), who separately trained target-side language models using monolingual data, and then integrated them during decoding either through re-scoring the beam, or by feeding the hidden state of the language model to the NMT model. Jean et al. (2015) also explored re-ranking the NMT output with a n-gram language model. Another successful method for leveraging monolingual data in NMT is back-translation (Sennrich et al., 2016; Fadaee et al., 2017; Edunov et al., 2018; He et al., 2016), where a reverse translation model is used to translate monolingual sentences from the target language to the source language to generate synthetic parallel sentences. Recent studies (Jiao et al., 2021; He et al., 2019) showed that self-training, where the synthetic parallel sentences are created by translating monolingual sentences in the source language, is also helpful. Our method is orthogonal to previous work and bears a unique feature: it can use more monolingual data without re-training (see §4.3). 3 Proposed Approach We start"
2021.acl-long.567,2011.mtsummit-papers.37,0,0.623043,"Aided Translation (CAT) tools by professional human translators for increasing productivity for decades (Yamada, 2011), the usefulness of Translation Memory (TM) has long been recognized (Huang et al., 2021). In general, TM is a database that stores pairs of source text and its corresponding translations. Like for human ∗ The work described in this paper is partially supported by a grant from the Research Grant Council of the Hong Kong Special Administrative Region, China (Project Code: 14200719). Correspondence to Yan Wang. translation, early work (Koehn and Senellart, 2010; He et al., 2010; Utiyama et al., 2011; Wang et al., 2013, inter alia) presents translations for similar source input to statistical translation models as additional cues. Recent work has confirmed that TM can help Neural Machine Translation (NMT) models as well. In a similar spirit to prior work, TM-augmented NMT models do not discard the training corpus after training but keep exploiting it in the test time. These models perform translation in two stages: In the retrieval stage, a retriever searches for nearest neighbors (i.e., source-target pairs) from the training corpus based on source-side similarity such as lexical overlaps"
2021.acl-long.567,P13-1002,0,0.17433,") tools by professional human translators for increasing productivity for decades (Yamada, 2011), the usefulness of Translation Memory (TM) has long been recognized (Huang et al., 2021). In general, TM is a database that stores pairs of source text and its corresponding translations. Like for human ∗ The work described in this paper is partially supported by a grant from the Research Grant Council of the Hong Kong Special Administrative Region, China (Project Code: 14200719). Correspondence to Yan Wang. translation, early work (Koehn and Senellart, 2010; He et al., 2010; Utiyama et al., 2011; Wang et al., 2013, inter alia) presents translations for similar source input to statistical translation models as additional cues. Recent work has confirmed that TM can help Neural Machine Translation (NMT) models as well. In a similar spirit to prior work, TM-augmented NMT models do not discard the training corpus after training but keep exploiting it in the test time. These models perform translation in two stages: In the retrieval stage, a retriever searches for nearest neighbors (i.e., source-target pairs) from the training corpus based on source-side similarity such as lexical overlaps (Gu et al., 2018;"
2021.acl-long.567,W18-5713,0,0.0240343,"task-specific retrieval rather than generic retrieval mechanisms. Retrieval for Text Generation Discrete retrieval as an intermediate step has been shown beneficial to a variety of natural language processing tasks. One typical use is to retrieve supporting evidence for open-domain question answering (e.g., Chen et al., 2017; Lee et al., 2019; Karpukhin et al., 2020). Recently, retrieval-guided generation has gained increasing interest in a wide range of text generation tasks such as language modeling (Guu et al., 2018; Khandelwal et al., 2019; Guu et al., 2020), dialogue response generation (Weston et al., 2018; Wu et al., 2019; Cai et al., 7308 Retrieval Model Input x source sentence encoder ?""#$ (?) target sentence encoder ?()( (?) Translation Memory ? Translation Model ?1 ?5 … Source Encoder ?(?, ?1 ) ?(?, ?5 ) … Memory Encoder relevant TM MIPS … Decoder relevance scores ?-,/ Output y 23 /01 bias attention dense index Figure 1: Overall framework. For an input sentence x in the source language, the retrieval model uses Maximum Inner Product Search (MIPS) to find the top-M TM sentences {zi }M i=1 in the target language. The translation M model takes {zi }M and corresponding relevance scores {f (x,"
2021.acl-long.567,2020.acl-main.144,0,0.573379,"ork has confirmed that TM can help Neural Machine Translation (NMT) models as well. In a similar spirit to prior work, TM-augmented NMT models do not discard the training corpus after training but keep exploiting it in the test time. These models perform translation in two stages: In the retrieval stage, a retriever searches for nearest neighbors (i.e., source-target pairs) from the training corpus based on source-side similarity such as lexical overlaps (Gu et al., 2018; Zhang et al., 2018; Xia et al., 2019), embedding-based matches (Cao and Xiong, 2018), or a hybrid (Bulte and Tezcan, 2019; Xu et al., 2020); In the generation stage, the retrieved translations are injected into a standard NMT model by attending over them with sophisticated memory networks (Gu et al., 2018; Cao and Xiong, 2018; Xia et al., 2019; He et al., 2021) or directly concatenating them to the source input (Bulte and Tezcan, 2019; Xu et al., 2020), or biasing the word distribution during decoding (Zhang et al., 2018). Most recently, Khandelwal et al. (2020) propose a token-level nearest neighbor search using complete translation context, i.e., both the source-side input and target-side prefix. Despite their differences, we i"
2021.acl-long.567,N18-1120,0,0.470857,", inter alia) presents translations for similar source input to statistical translation models as additional cues. Recent work has confirmed that TM can help Neural Machine Translation (NMT) models as well. In a similar spirit to prior work, TM-augmented NMT models do not discard the training corpus after training but keep exploiting it in the test time. These models perform translation in two stages: In the retrieval stage, a retriever searches for nearest neighbors (i.e., source-target pairs) from the training corpus based on source-side similarity such as lexical overlaps (Gu et al., 2018; Zhang et al., 2018; Xia et al., 2019), embedding-based matches (Cao and Xiong, 2018), or a hybrid (Bulte and Tezcan, 2019; Xu et al., 2020); In the generation stage, the retrieved translations are injected into a standard NMT model by attending over them with sophisticated memory networks (Gu et al., 2018; Cao and Xiong, 2018; Xia et al., 2019; He et al., 2021) or directly concatenating them to the source input (Bulte and Tezcan, 2019; Xu et al., 2020), or biasing the word distribution during decoding (Zhang et al., 2018). Most recently, Khandelwal et al. (2020) propose a token-level nearest neighbor search usi"
2021.emnlp-main.210,D19-1641,0,0.243407,"representation r mc and type representation r t into a shared space by φ (r mc , A) : r mc → Ar mc θ (r t , B) : r t → Br t , (3) where A ∈ Rds ×2dm and B ∈ Rds ×dt are learnable matrices. The matching score is defined as yt = φ (r mc , A)·θ (r t , B) = (Ar mc )&gt; Br t (4) During training, we match mention m with all the types in Ttrain , so the loss function for m is: Type Hierarchy-Aware (HA) Module In HA module, we use Transformer encoder (Vaswani et al., 2017) with mask-self-attention to capture the hierarchical information for better type representations. Besides, we take the encoder from Lin and Ji (2019) to learn the features of mentions and contexts. Then a similarity function is defined to compute the matching score between a mention and a candidate type based on the context. 2.3.1 Mention-Context Encoder In the mention-context encoder, an entity mention and its context are represented as the weighted sum of their ELMo word representations. Then the mention representation r m and context representation r c are concatenated as the final representation: r mc = r m ⊕ r c , where r m , r c ∈ Rdm , r mc ∈ R2dm , ⊕ denotes concatenation. 2.3.2 Hierarchy-Aware Type Encoder Given a type set T = Ttr"
2021.emnlp-main.210,E17-1075,0,0.0550379,"Missing"
2021.emnlp-main.210,N19-1423,0,0.208547,"e is the surface form of a type, which is a word or a phase, e.g., type name of /organization/corporation is corporation. (ii) Type hierarchy is the ontology structure connecting seen and unseen types. (iii) Background knowledge provides the external prior information that depicts types in detail, e.g., prototypes (Ma et al., 2016) and descriptions (Obeidat et al., 2019). MSF is composed of three modules, with each targeting a specific information source. (i) In the CA (Context-Consistency Aware) module, we measure the context consistency by large-scale pretrained language models, e.g., BERT (Devlin et al., 2019). By masking mentions and predicting the names of ground truth types through finetuning on the data of seen types, CA is expected to measure the context consistency of unseen types more precisely. (ii) In the HA (Type-Hierarchy Aware) module, we use Transformer encoder (Vaswani et al., 2017) to model the hierarchical dependency among types. There have been substantial works exploring type hierarchy in the supervised typing task (Shimaoka et al., 2017; Xu and Barbosa, 2018; Xiong et al., 2019), but only some preliminary research in ZFET (Ma et al., 2016; Zhang et al., 2020b). (iii) In the KA (B"
2021.emnlp-main.210,D19-1488,0,0.0282879,"with regard to macro F1 scores. More importantly, we further discuss the characteristics, merits and demerits of each information source and provide an intuitive understanding of the complementarity among them. Source2: Type Hierarchy Government … Overall Loss Fusion ?????? ?????? ?????? CA HA KA <Northwest and Midway are two of the …, /organization/corporation&gt; Fine-grained entity typing (FET) aims to detect the types of an entity mention given its context (Abhishek et al., 2017; Xu and Barbosa, 2018; Jin et al., 2019). The results of FET benefit lots of downstream tasks (Chen et al., 2020; Hu et al., 2019; Zhang et al., 2020a; Liu et al., 2021; Chu et al., 2020). In many scenarios, the type hierarchy is continuously evolving, which requires newly emerged types to be accounted into FET systems. As a result, zero-shot FET (ZFET) is welcomed to handle the new types which are unseen during training stage (Ma et al., 2016; Ren et al., 2020; Zhang et al., 2020b). The major challenge of ZFET is to build the semantic connections between the seen types (during training) and the unseen ones (during inference). Corresponding Author … Prototypes: western_union, quebecor, merrill, rtc, … Description: a bus"
2021.emnlp-main.210,C16-1017,0,0.308687,"ay are two of the …, /organization/corporation&gt; Fine-grained entity typing (FET) aims to detect the types of an entity mention given its context (Abhishek et al., 2017; Xu and Barbosa, 2018; Jin et al., 2019). The results of FET benefit lots of downstream tasks (Chen et al., 2020; Hu et al., 2019; Zhang et al., 2020a; Liu et al., 2021; Chu et al., 2020). In many scenarios, the type hierarchy is continuously evolving, which requires newly emerged types to be accounted into FET systems. As a result, zero-shot FET (ZFET) is welcomed to handle the new types which are unseen during training stage (Ma et al., 2016; Ren et al., 2020; Zhang et al., 2020b). The major challenge of ZFET is to build the semantic connections between the seen types (during training) and the unseen ones (during inference). Corresponding Author … Prototypes: western_union, quebecor, merrill, rtc, … Description: a business firm whose articles of incorporation have been approved in some state. Introduction ∗ Corporation Source3: Background Knowledge [Mention] 1 Organization [Context] [Type] Figure 1: Illustration of the proposed multi-source fusion model (MSF). Auxiliary information has been proved to be essential in this regard ("
2021.emnlp-main.210,N19-1087,0,0.0138639,"seen types (during training) and the unseen ones (during inference). Corresponding Author … Prototypes: western_union, quebecor, merrill, rtc, … Description: a business firm whose articles of incorporation have been approved in some state. Introduction ∗ Corporation Source3: Background Knowledge [Mention] 1 Organization [Context] [Type] Figure 1: Illustration of the proposed multi-source fusion model (MSF). Auxiliary information has been proved to be essential in this regard (Xian et al., 2019), with a variety of approaches focused on scattered information (Ma et al., 2016; Zhou et al., 2018; Obeidat et al., 2019; Ren et al., 2020; Zhang et al., 2020b). However, the power of auxiliary information has not been sufficiently exploited in existing solutions. Besides, the effects of each information source also remain to be clearly understood. In this paper, we propose a Multi-Source Fusion model (MSF) integrating three kinds of popular auxiliary information for ZFET, i.e., context consistency, type hierarchy, and background knowledge, as illustrated in Figure 1. (i) Context consistency means a correct type should be se2668 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Process"
2021.emnlp-main.210,D16-1144,0,0.176978,"score vector from a module for mention m towards all types t ∈ Ttest with x as component. µx and σx denote the mean and standard deviation of the vector x. The final decision score by our fusion model for type t is: scoret = λ1 s0t + λ2 yt0 + λ3 p0t , (11) where λ1 , λ2 , λ3 ≥ 0 are hyper-parameters and λ1 + λ2 + λ3 = 1. 3 3.1 Experimental Setup Datasets and Evaluation Metrics We evaluate our model on two widely-used datasets: {˜ r mc , r˜ tp , r˜ td , r˜ h } = W {r mc , r tp , r td , r h } (6) BBN (Weischedel and Brunstein, 2005) and Wiki 2671 (Ling and Weld, 2012). The version processed by Ren et al. (2016) is adopted for our experiments. Detailed statistics on two datasets are listed in Table 1. We do not use OntoNotes (Gillick et al., 2014) since it is hard to define the name, description and hierarchy for its special type /other. Types of both BBN and Wiki are organized into a 2-level hierarchy. There are 47 types in BBN and 113 types in Wiki. Following Ma et al. (2016); Zhang et al. (2020b), we use the coarse-grained (Level-1) types such as /organization for training (denoted as seen types), while the fine-grained (Level-2) types such as /organization/corporation are reserved for testing (de"
2021.emnlp-main.210,E17-1119,0,0.0290272,"ource. (i) In the CA (Context-Consistency Aware) module, we measure the context consistency by large-scale pretrained language models, e.g., BERT (Devlin et al., 2019). By masking mentions and predicting the names of ground truth types through finetuning on the data of seen types, CA is expected to measure the context consistency of unseen types more precisely. (ii) In the HA (Type-Hierarchy Aware) module, we use Transformer encoder (Vaswani et al., 2017) to model the hierarchical dependency among types. There have been substantial works exploring type hierarchy in the supervised typing task (Shimaoka et al., 2017; Xu and Barbosa, 2018; Xiong et al., 2019), but only some preliminary research in ZFET (Ma et al., 2016; Zhang et al., 2020b). (iii) In the KA (Background-Knowledge Aware) module, we introduce prototypes (Ma et al., 2016) and WordNet descriptions (Miller, 1995) as background knowledge of types. KA is embodied as natural language inference with a translation-based solution to better incorporate knowledge. Extensive experiments are carried out to verify the effectiveness of the proposed fusion model. We also conduct a deep analysis on the characteristics, merits and demerits of each information"
2021.emnlp-main.210,D19-1502,0,0.0474529,"Missing"
2021.emnlp-main.210,I17-1011,0,0.0226177,"in the KA module. Prototypes refer to the carefully selected mentions for a type based on Normalized Point-wise Mutual Information (NPMI), which provide a mention-level summary for types (Ma et al., 2016). Descriptions are queried from WordNet glosses (Miller, 1995) by type names, which provide a brief high-level summary for each type. 2.4.1 Inference from Background Knowledge We hope to infer whether a mention m matches a candidate type t, given the prototypes, type description and the context. In this work, we embody the KA module as natural language inference (NLI) from multiple premises (Lai et al., 2017). An ex1 The initialization details are presented in Appendix A ample is presented in Figure 2, with input the same 2670 Multiple Premises • • • Context-based premise: Northwest and Midway are two of the five airlines with which Budget has agreements. Prototypes-based premise: /organization/corporation has the following prototypes: western_union, … Description-based premise: /organization/corporation denotes a collection of business firms whose articles of incorporation have been approved in some state. where W ∈ Rdw ×2dm . We hope that r˜ mc + r˜ tp + r˜ td ≈ r˜ h when the hypothesis can be i"
2021.emnlp-main.210,D18-1121,0,0.0337611,"Missing"
2021.emnlp-main.210,N19-1084,0,0.0349593,"Missing"
2021.emnlp-main.210,N18-1002,0,0.0124422,"ontext-Consistency Aware) module, we measure the context consistency by large-scale pretrained language models, e.g., BERT (Devlin et al., 2019). By masking mentions and predicting the names of ground truth types through finetuning on the data of seen types, CA is expected to measure the context consistency of unseen types more precisely. (ii) In the HA (Type-Hierarchy Aware) module, we use Transformer encoder (Vaswani et al., 2017) to model the hierarchical dependency among types. There have been substantial works exploring type hierarchy in the supervised typing task (Shimaoka et al., 2017; Xu and Barbosa, 2018; Xiong et al., 2019), but only some preliminary research in ZFET (Ma et al., 2016; Zhang et al., 2020b). (iii) In the KA (Background-Knowledge Aware) module, we introduce prototypes (Ma et al., 2016) and WordNet descriptions (Miller, 1995) as background knowledge of types. KA is embodied as natural language inference with a translation-based solution to better incorporate knowledge. Extensive experiments are carried out to verify the effectiveness of the proposed fusion model. We also conduct a deep analysis on the characteristics, merits and demerits of each information source. We find that,"
2021.emnlp-main.210,2020.coling-main.7,0,0.102233,"acro F1 scores. More importantly, we further discuss the characteristics, merits and demerits of each information source and provide an intuitive understanding of the complementarity among them. Source2: Type Hierarchy Government … Overall Loss Fusion ?????? ?????? ?????? CA HA KA <Northwest and Midway are two of the …, /organization/corporation&gt; Fine-grained entity typing (FET) aims to detect the types of an entity mention given its context (Abhishek et al., 2017; Xu and Barbosa, 2018; Jin et al., 2019). The results of FET benefit lots of downstream tasks (Chen et al., 2020; Hu et al., 2019; Zhang et al., 2020a; Liu et al., 2021; Chu et al., 2020). In many scenarios, the type hierarchy is continuously evolving, which requires newly emerged types to be accounted into FET systems. As a result, zero-shot FET (ZFET) is welcomed to handle the new types which are unseen during training stage (Ma et al., 2016; Ren et al., 2020; Zhang et al., 2020b). The major challenge of ZFET is to build the semantic connections between the seen types (during training) and the unseen ones (during inference). Corresponding Author … Prototypes: western_union, quebecor, merrill, rtc, … Description: a business firm whose art"
2021.emnlp-main.210,D18-1231,0,0.0133239,"ctions between the seen types (during training) and the unseen ones (during inference). Corresponding Author … Prototypes: western_union, quebecor, merrill, rtc, … Description: a business firm whose articles of incorporation have been approved in some state. Introduction ∗ Corporation Source3: Background Knowledge [Mention] 1 Organization [Context] [Type] Figure 1: Illustration of the proposed multi-source fusion model (MSF). Auxiliary information has been proved to be essential in this regard (Xian et al., 2019), with a variety of approaches focused on scattered information (Ma et al., 2016; Zhou et al., 2018; Obeidat et al., 2019; Ren et al., 2020; Zhang et al., 2020b). However, the power of auxiliary information has not been sufficiently exploited in existing solutions. Besides, the effects of each information source also remain to be clearly understood. In this paper, we propose a Multi-Source Fusion model (MSF) integrating three kinds of popular auxiliary information for ZFET, i.e., context consistency, type hierarchy, and background knowledge, as illustrated in Figure 1. (i) Context consistency means a correct type should be se2668 Proceedings of the 2021 Conference on Empirical Methods in Na"
2021.emnlp-main.431,J92-4003,0,0.338507,"names, “company” and “fruit”, can be mapped to it. To resolve this problem, we cluster all mentions in M by an overlapping clustering algorithm and then assign a type name to each mention cluster, whose meaning may be less ambiguous. Standard overlapping clustering algorithms either need to pre-define the number of clusters or are inefficient (Jain et al., 1999). Instead, we design a new algorithm whose basic idea includes three steps: for each mention, it computes its neighbor set consisting of its top k mentions according to a term similarity score; then a hierarchical clustering algorithm (Brown et al., 1992) is applied to each neighbor set, leading to many small subsets in total; then it repeatedly merges a pair of subsets to a larger subset according to a pre-defined threshold and finally take all subsets as clusters. Since one neighbor set may overlap with another one, the resulted clusters can be overlapping. According to the clusters, we create a map from mention clusters to type names. For example, the cluster “[Apple, Microsoft, Google, Facebook, ...]” is mapped to “company”. There are unambiguous mentions such as “Google” and “Microsoft” in this cluster, so the ambiguity of “Apple” can be"
2021.emnlp-main.431,2020.acl-main.749,0,0.028741,"Missing"
2021.emnlp-main.431,2021.emnlp-main.210,1,0.771934,"re practical setting, i.e. Finegrained Entity Typing without Knowledge Base (FETw/oKB). That is, given an ontology L, we aim to infer the type set T for the mention x within a sentence c when there are no KBs available. Relation to Zero-shot FET Suppose the type ontology L is divided into two disjoint subontology L1 and L2 , and there is a labeled training dataset where the labels are all from L1 . Zero-shot FET aims to make a prediction on testing mentions whose labels are within L2 . A surge of efforts have been devoted to zero-shot FET (Ma et al., 2016; Xian et al., 2019; Ren et al., 2020; Chen et al., 2021), but all of them assume that training data is obtained from knowledge bases by distant supervision, similar to the conventional FET. Therefore, the proposed FETw/oKB is clearly different from zero-shot FET. Road Map Generally, it is challenging to train Suppose L is a fine-grained entity type ontology for a real-world application as illustrated in Figure 2, θ under the FETw/oKB setting. In the next sections, we propose a new framework to address which consists of a set of formal types organized as FETw/oKB. Its key idea is a two-step approach: hierarchical trees, and a dictionary used to expl"
2021.emnlp-main.431,P18-1009,0,0.0911923,"lts show that our method achieves competitive performance with respect to the models trained on the original KB-supervised datasets. Figure 1: A comparison between the previous FET framework (left) and our proposed FET framework (right). In the previous one, the type ontology is coupled with KB whereas it is not in ours. Despite its success, this framework has one limitation: The type ontology for training and testing is strongly restricted by the underlying KB. As a result, the application of the trained models under this framework can be hampered by the lack or the incompleteness of the KB (Choi et al., 2018). In real-world applications of FET, the ontology of the fine-grained types depends on the applications instead of the KB. Typically, there may not exist 1 Introduction a one-to-one mapping from the ontology of appliEntity Typing is a fundamental task in Natural cations to that of the existing KB. For example, Language Processing. Traditional entity typing re- a game-related application may need to identify search focuses on a limited number of entity types the named entities of some specific video games, while recent studies strive for finer granularity. A such as the heroes and items in DOTA"
2021.emnlp-main.431,2021.acl-long.141,0,0.0276714,"realworld application. Instead, the desired types and ontology depend on the specific application and usually there does not exist a one-to-one mapping from the desired types to KB types. 2.3 Free-form Entity Typing Choi et al. (2018) propose an alternative to the conventional FET framework. Their proposed ultrafine entity typing task aims to predict free-form types such as noun phrases that describe appropriate types for the role the target entity plays in the sentence. To address this free-form typing task, they automatically create a large dataset using some heuristic methods. In addition, Dai et al. (2021) extend the dataset in Choi et al. (2018) with pretrained langauge models and then train the typing task on the extended dataset. Since the entity types are replaced by free-form noun phrases, there is no a unified type ontology in their task. However, a formal, explicit specification of the target types plays an important role in the downstream tasks of fine-grained entity typing. Different from their work, our proposed task retains the type ontology as an input for the task, but disentangles it from the knowledge base so as to provide larger flexibility. 3 Problem Statement for FETw/oKB comm"
2021.emnlp-main.431,D15-1103,0,0.0614652,"Missing"
2021.emnlp-main.431,N19-1423,0,0.0332094,"to distributed representations (Yogatama et al., 2015; Ren et al., 2016a,b; Zhang et al., 2018) and more advanced neural network models (Dong et al., 2015; Shimaoka et al., 2016a,b; Murty et al., 2018). Shimaoka et al. (2016a) propose the first attentive neural model that outperforms feature-based methods with a simple cross-entropy loss. They use LSTM (Hochreiter and Schmidhuber, 1997) for context encoding. Murty et al. (2018) employ Convolutional Neural Networks to encode the entity mention and context. More recently, pretrained language models, such as ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019), have been employed to achieve better performance (Lin and Ji, 2019; Chen et al., 2020; Onoe et al., 2021). This research line relies on a knowledge base to automatically create training data, since it is challenging to build the humanannotated data (Ling and Weld, 2012; Gillick et al., 2014; Li et al., 2021a) Our contributions in this work are orthogonal to the above research line. As stated in Section 1, the focus of this work is not to develop more advanced models under the conventional FET framework. Actually, these models can be freely migrated to our proposed framework. It is worth noti"
2021.emnlp-main.431,C92-2082,0,0.554423,"ramework disentangles the type ontology from the KB and thus the given ontology can be an arbitrary one, defined by a practical application that does not correspond to any KB or by a specific KB as the conventional FET setting. Therefore, our proposed framework is more flexible than the previous one. Our framework consists of two steps: creating pseudo data from an unlabeled dataset and (iteratively) training the FET model based on the pseudo dataset. In the first step, we aim to create a large pseudo dataset with fine-grained type labels. We propose an automatic method using Hearst patterns (Hearst, 1992) to achieve this goal. There exists a difference between the pseudo data and the testing data in that each sentence in the pseudo data satisfies a (relaxed) Hearst pattern while few testing data do. Consequently, directly training a FET model on the pseudo data may limit its generalization ability. We thereby propose a self-training method with weak supervision from a coarse-grained NER model in the second step to alleviate this issue. We do not focus on designing novel models under the conventional FET setting in this work. Actually, our proposed two-step framework does not set a limit on the"
2021.emnlp-main.431,2021.acl-long.160,0,0.0658572,"anced neural network models (Dong et al., 2015; Shimaoka et al., 2016a,b; Murty et al., 2018). Shimaoka et al. (2016a) propose the first attentive neural model that outperforms feature-based methods with a simple cross-entropy loss. They use LSTM (Hochreiter and Schmidhuber, 1997) for context encoding. Murty et al. (2018) employ Convolutional Neural Networks to encode the entity mention and context. More recently, pretrained language models, such as ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019), have been employed to achieve better performance (Lin and Ji, 2019; Chen et al., 2020; Onoe et al., 2021). This research line relies on a knowledge base to automatically create training data, since it is challenging to build the humanannotated data (Ling and Weld, 2012; Gillick et al., 2014; Li et al., 2021a) Our contributions in this work are orthogonal to the above research line. As stated in Section 1, the focus of this work is not to develop more advanced models under the conventional FET framework. Actually, these models can be freely migrated to our proposed framework. It is worth noting that using Hearst patterns for FET was pioneered by Del Corro et al. (2015). However, they employed Hear"
2021.emnlp-main.431,2021.findings-emnlp.18,1,0.917958,"pplication. Instead, the desired types and ontology depend on the specific application and usually there does not exist a one-to-one mapping from the desired types to KB types. 2.3 Free-form Entity Typing Choi et al. (2018) propose an alternative to the conventional FET framework. Their proposed ultrafine entity typing task aims to predict free-form types such as noun phrases that describe appropriate types for the role the target entity plays in the sentence. To address this free-form typing task, they automatically create a large dataset using some heuristic methods. In addition, Dai et al. (2021) extend the dataset in Choi et al. (2018) with pretrained langauge models and then train the typing task on the extended dataset. Since the entity types are replaced by free-form noun phrases, there is no a unified type ontology in their task. However, a formal, explicit specification of the target types plays an important role in the downstream tasks of fine-grained entity typing. Different from their work, our proposed task retains the type ontology as an input for the task, but disentangles it from the knowledge base so as to provide larger flexibility. 3 Problem Statement for FETw/oKB comm"
2021.emnlp-main.431,D19-1641,0,0.150896,"a,b; Zhang et al., 2018) and more advanced neural network models (Dong et al., 2015; Shimaoka et al., 2016a,b; Murty et al., 2018). Shimaoka et al. (2016a) propose the first attentive neural model that outperforms feature-based methods with a simple cross-entropy loss. They use LSTM (Hochreiter and Schmidhuber, 1997) for context encoding. Murty et al. (2018) employ Convolutional Neural Networks to encode the entity mention and context. More recently, pretrained language models, such as ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019), have been employed to achieve better performance (Lin and Ji, 2019; Chen et al., 2020; Onoe et al., 2021). This research line relies on a knowledge base to automatically create training data, since it is challenging to build the humanannotated data (Ling and Weld, 2012; Gillick et al., 2014; Li et al., 2021a) Our contributions in this work are orthogonal to the above research line. As stated in Section 1, the focus of this work is not to develop more advanced models under the conventional FET framework. Actually, these models can be freely migrated to our proposed framework. It is worth noting that using Hearst patterns for FET was pioneered by Del Corro et"
2021.emnlp-main.431,W02-0109,0,0.461743,"oft, Google, Facebook, ...]” is mapped to “company”. There are unambiguous mentions such as “Google” and “Microsoft” in this cluster, so the ambiguity of “Apple” can be reduced. It is worth mentioning that Zhang et al. (2020); Liu et al. (2021) adopt a similar method to obtain a mention cluster map for fine-grained entity classification where entity mentions are unknown. In their work, the map is employed to infer the labels by exact matching during inference. Consequently, 2 their work can not address those mentions which We implement an in-house extraction algorithm similar to that in NLTK (Loper and Bird, 2002). are not covered in the map. Instead of directly us5312 ing the map for inference, we employ the map to generate pseudo data for training a typing model, as will be described later. In this way, the model is able to generalize on unseen mentions. nization/company” even if this sentence does not satisfy any Hearst patterns at all. In this sense, our generation method can be considered as a relaxed version of the Hearst pattern method. 4.2 5 Pseudo Data Generation by Matching & Mapping Self-Training via Weak Guidance Note that each sentence in the above pseudo data satisfies a relaxed Hearst pa"
2021.emnlp-main.431,C16-1017,0,0.0236827,"em Statement In this paper, we focus on FET under a more practical setting, i.e. Finegrained Entity Typing without Knowledge Base (FETw/oKB). That is, given an ontology L, we aim to infer the type set T for the mention x within a sentence c when there are no KBs available. Relation to Zero-shot FET Suppose the type ontology L is divided into two disjoint subontology L1 and L2 , and there is a labeled training dataset where the labels are all from L1 . Zero-shot FET aims to make a prediction on testing mentions whose labels are within L2 . A surge of efforts have been devoted to zero-shot FET (Ma et al., 2016; Xian et al., 2019; Ren et al., 2020; Chen et al., 2021), but all of them assume that training data is obtained from knowledge bases by distant supervision, similar to the conventional FET. Therefore, the proposed FETw/oKB is clearly different from zero-shot FET. Road Map Generally, it is challenging to train Suppose L is a fine-grained entity type ontology for a real-world application as illustrated in Figure 2, θ under the FETw/oKB setting. In the next sections, we propose a new framework to address which consists of a set of formal types organized as FETw/oKB. Its key idea is a two-step ap"
2021.emnlp-main.431,P18-1010,0,0.0152394,"ur proposed approach achieves competitive performance on two popular FET testing datasets. • To facilitate future research under this new setting, we make the created data publicly available. 1 1 The dataset will be available at https://github. 2 2.1 Related Work Fine-grained Entity Typing Starting from hand-crafted features (Ling and Weld, 2012; Yosef et al., 2013; Gillick et al., 2014), research on FET has moved to distributed representations (Yogatama et al., 2015; Ren et al., 2016a,b; Zhang et al., 2018) and more advanced neural network models (Dong et al., 2015; Shimaoka et al., 2016a,b; Murty et al., 2018). Shimaoka et al. (2016a) propose the first attentive neural model that outperforms feature-based methods with a simple cross-entropy loss. They use LSTM (Hochreiter and Schmidhuber, 1997) for context encoding. Murty et al. (2018) employ Convolutional Neural Networks to encode the entity mention and context. More recently, pretrained language models, such as ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019), have been employed to achieve better performance (Lin and Ji, 2019; Chen et al., 2020; Onoe et al., 2021). This research line relies on a knowledge base to automatically create tra"
2021.emnlp-main.431,D14-1162,0,0.0865649,"Missing"
2021.emnlp-main.431,N18-1202,0,0.0186901,"014), research on FET has moved to distributed representations (Yogatama et al., 2015; Ren et al., 2016a,b; Zhang et al., 2018) and more advanced neural network models (Dong et al., 2015; Shimaoka et al., 2016a,b; Murty et al., 2018). Shimaoka et al. (2016a) propose the first attentive neural model that outperforms feature-based methods with a simple cross-entropy loss. They use LSTM (Hochreiter and Schmidhuber, 1997) for context encoding. Murty et al. (2018) employ Convolutional Neural Networks to encode the entity mention and context. More recently, pretrained language models, such as ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019), have been employed to achieve better performance (Lin and Ji, 2019; Chen et al., 2020; Onoe et al., 2021). This research line relies on a knowledge base to automatically create training data, since it is challenging to build the humanannotated data (Ling and Weld, 2012; Gillick et al., 2014; Li et al., 2021a) Our contributions in this work are orthogonal to the above research line. As stated in Section 1, the focus of this work is not to develop more advanced models under the conventional FET framework. Actually, these models can be freely migrated to our propo"
2021.emnlp-main.431,P17-2052,0,0.0146013,"o et al. (2015). However, they employed Hearst patterns as well as a knowledge base to build an FET system. Hence they did not prove that it is feasible to address FET without any knowledge bases as we do. 2.2 Fine-grained Type Ontology Recent work introduced fine-grained type ontologies based on knowledge bases. Ling and Weld (2012) derive the type ontology consisting of 122 types from Freebase types. Later work (Murty et al., 2017) introduces a even more fine-grained ontology, which consists of over 1,941 types, obtained by manually annotating a mapping from 1,081 Freebase types to WordNet. Rabinovich and Klein (2017) derive the ontology from the Wikipedia categories and WordNet graphs. Del Corro et al. (2015) com/lemaoliu/fet-data. Part of this work is implemented in the public system TexSmart at https:// texsmart.qq.com. 5310 use the ontology derived from the entire WordNet hierarchy. Although driving an ontology from the type hierarchy of a knowledge base is an efficient way to build type ontology, it is not practical in realworld application. Instead, the desired types and ontology depend on the specific application and usually there does not exist a one-to-one mapping from the desired types to KB type"
2021.emnlp-main.431,D16-1144,0,0.10483,"roach (pseudo data generation and model training) to tackle the FET task. • Compared with training on KB-supervised datasets, our proposed approach achieves competitive performance on two popular FET testing datasets. • To facilitate future research under this new setting, we make the created data publicly available. 1 1 The dataset will be available at https://github. 2 2.1 Related Work Fine-grained Entity Typing Starting from hand-crafted features (Ling and Weld, 2012; Yosef et al., 2013; Gillick et al., 2014), research on FET has moved to distributed representations (Yogatama et al., 2015; Ren et al., 2016a,b; Zhang et al., 2018) and more advanced neural network models (Dong et al., 2015; Shimaoka et al., 2016a,b; Murty et al., 2018). Shimaoka et al. (2016a) propose the first attentive neural model that outperforms feature-based methods with a simple cross-entropy loss. They use LSTM (Hochreiter and Schmidhuber, 1997) for context encoding. Murty et al. (2018) employ Convolutional Neural Networks to encode the entity mention and context. More recently, pretrained language models, such as ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019), have been employed to achieve better performance ("
2021.emnlp-main.431,W16-1313,0,0.323817,"KB-supervised datasets, our proposed approach achieves competitive performance on two popular FET testing datasets. • To facilitate future research under this new setting, we make the created data publicly available. 1 1 The dataset will be available at https://github. 2 2.1 Related Work Fine-grained Entity Typing Starting from hand-crafted features (Ling and Weld, 2012; Yosef et al., 2013; Gillick et al., 2014), research on FET has moved to distributed representations (Yogatama et al., 2015; Ren et al., 2016a,b; Zhang et al., 2018) and more advanced neural network models (Dong et al., 2015; Shimaoka et al., 2016a,b; Murty et al., 2018). Shimaoka et al. (2016a) propose the first attentive neural model that outperforms feature-based methods with a simple cross-entropy loss. They use LSTM (Hochreiter and Schmidhuber, 1997) for context encoding. Murty et al. (2018) employ Convolutional Neural Networks to encode the entity mention and context. More recently, pretrained language models, such as ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019), have been employed to achieve better performance (Lin and Ji, 2019; Chen et al., 2020; Onoe et al., 2021). This research line relies on a knowledge base to"
2021.emnlp-main.431,P15-2048,0,0.0308514,"propose a two-step approach (pseudo data generation and model training) to tackle the FET task. • Compared with training on KB-supervised datasets, our proposed approach achieves competitive performance on two popular FET testing datasets. • To facilitate future research under this new setting, we make the created data publicly available. 1 1 The dataset will be available at https://github. 2 2.1 Related Work Fine-grained Entity Typing Starting from hand-crafted features (Ling and Weld, 2012; Yosef et al., 2013; Gillick et al., 2014), research on FET has moved to distributed representations (Yogatama et al., 2015; Ren et al., 2016a,b; Zhang et al., 2018) and more advanced neural network models (Dong et al., 2015; Shimaoka et al., 2016a,b; Murty et al., 2018). Shimaoka et al. (2016a) propose the first attentive neural model that outperforms feature-based methods with a simple cross-entropy loss. They use LSTM (Hochreiter and Schmidhuber, 1997) for context encoding. Murty et al. (2018) employ Convolutional Neural Networks to encode the entity mention and context. More recently, pretrained language models, such as ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019), have been employed to achieve be"
2021.emnlp-main.431,P13-4023,0,0.0336833,"ained entity typing based on a new setting: FET without a knowledge base. • Under the new setting, we propose a two-step approach (pseudo data generation and model training) to tackle the FET task. • Compared with training on KB-supervised datasets, our proposed approach achieves competitive performance on two popular FET testing datasets. • To facilitate future research under this new setting, we make the created data publicly available. 1 1 The dataset will be available at https://github. 2 2.1 Related Work Fine-grained Entity Typing Starting from hand-crafted features (Ling and Weld, 2012; Yosef et al., 2013; Gillick et al., 2014), research on FET has moved to distributed representations (Yogatama et al., 2015; Ren et al., 2016a,b; Zhang et al., 2018) and more advanced neural network models (Dong et al., 2015; Shimaoka et al., 2016a,b; Murty et al., 2018). Shimaoka et al. (2016a) propose the first attentive neural model that outperforms feature-based methods with a simple cross-entropy loss. They use LSTM (Hochreiter and Schmidhuber, 1997) for context encoding. Murty et al. (2018) employ Convolutional Neural Networks to encode the entity mention and context. More recently, pretrained language mod"
2021.emnlp-main.431,S18-2022,0,0.0248991,"Missing"
2021.findings-acl.193,W05-0909,0,0.0357683,"this community 1 . 2 Related Work In this section, we focus on unsupervised automatic evaluation metrics for dialogue system evaluation. In general, existing unsupervised metrics mainly measure turn-level qualities, which can be categorized into two main classes: word overlapping metrics and embedding-based metrics: Word-overlapping Metrics Such metrics quantify the amount of word-overlap between generated response and reference responses. For example, BLEU (Papineni et al., 2002) calculates the geometric mean of the precision for n-gram. ROUGE (Lin, 2004) is a recall-oriented metric. METEOR (Banerjee and Lavie, 2005) computes the harmonic mean of precision and recall with stemming and syn1 The source codes and data are available at https:// github.com/yhlleo/frechet-bert-distance. 2192 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 2192–2198 August 1–6, 2021. ©2021 Association for Computational Linguistics onyms. Embedding-based Metrics Embedding-based metrics align the generated response and the reference in latent semantic space. Some adopt the vector similarity of sentence embeddings as a quality measure. For example, Embedding Average (Foltz et al., 1998; Mitchell an"
2021.findings-acl.193,2021.findings-acl.432,1,0.84362,"RD, are developed and evaluated. Experiments on several dialogue corpora show that our proposed metrics correlate better with human judgments than existing metrics. 1 Introduction Dialogue generation is a special text generation task, which has drawn booming attention in the natural language processing community. It is widely agreed that one single input query is often associated with multiple valid responses in this task, which is termed as a 1-to-n relationship between a query and its responses (Vinyals and Le, 2015; Zhou et al., 2017; Zhao et al., 2017; Liu et al., 2018; Chen et al., 2021; Chan et al., 2021; Gao et al., 2021). It increases the challenges of automatically evaluating the performance of dialogue systems. In general, the previous evaluation metrics mainly focus on turn-level quality. For example, unsupervised word-overlapping or embedding-based metrics (Papineni et al., 2002; Lin, 2004; Mitchell and Lapata, 2008; Zhang et al., 2020) calculate the similarity or alignment between generated responses and reference responses, which is not wellsuited for open-end dialogue tasks. Learned classification or regression systems (Lowe et al., 2017; ∗ Equal contribution. Work was done during in"
2021.findings-acl.193,2021.acl-long.34,0,0.630021,"metrics, FBD and PRD, are developed and evaluated. Experiments on several dialogue corpora show that our proposed metrics correlate better with human judgments than existing metrics. 1 Introduction Dialogue generation is a special text generation task, which has drawn booming attention in the natural language processing community. It is widely agreed that one single input query is often associated with multiple valid responses in this task, which is termed as a 1-to-n relationship between a query and its responses (Vinyals and Le, 2015; Zhou et al., 2017; Zhao et al., 2017; Liu et al., 2018; Chen et al., 2021; Chan et al., 2021; Gao et al., 2021). It increases the challenges of automatically evaluating the performance of dialogue systems. In general, the previous evaluation metrics mainly focus on turn-level quality. For example, unsupervised word-overlapping or embedding-based metrics (Papineni et al., 2002; Lin, 2004; Mitchell and Lapata, 2008; Zhang et al., 2020) calculate the similarity or alignment between generated responses and reference responses, which is not wellsuited for open-end dialogue tasks. Learned classification or regression systems (Lowe et al., 2017; ∗ Equal contribution. Work"
2021.findings-acl.193,W14-3348,0,0.0123448,"//github.com/ huggingface/transformers USR https://github.com/Shikib/usr GRADE https://github.com/li3cmz/ GRADE/tree/main/evaluation Daily(Z) https://github.com/ZHAOTING/ dialog-processing/tree/master/ Persona(Z) src/tasks/response_eval ParlAI BLEU METEOR ROUGE-L Greedy Average Extrema Experiments 4.1 Datasets & Systems To verify the two proposed metrics, we conduct experiments on six public dialogue corpora. Baseline Metrics. We mainly compare with several widely-used metrics in text generation field: a) three word-overlapping metrics: BLEU (Papineni et al., 2002), ROUGE (Lin, 2004), METEOR (Denkowski and Lavie, 2014); b) four embedding-based metrics: Greedy Matching (Rus and Lintean, 2012), Embedding Average (Wieting et al., 2015), Vector Extrema (Forgues et al., 2014) and BERTScore (Zhang et al., 2020). All these metrics do not require task-specific training. Datasets. We collect three recently released evaluation corpora which consist of dialogue query and response samples of different systems, and the corresponding human annotations: • Persona(M): USR (Mehri and Esk´enazi, 2020) built an evaluation corpus based on PersonaChat (Zhang et al., 2018b), in which both four system outputs and the correspondin"
2021.findings-acl.193,N19-1423,0,0.0813087,"et al. (2020) introduced a better embedding-based metric BERTScore that computes word similarity using contextual embeddings from pre-trained language models. Our proposed methods are best placed in the literature of embedding-based metrics. However, there are two main differences from previous metrics in this field: (1) We compute the distribution distance between embedding sets as the system-level performance of a dialogue system, which does not require task-specific training/tuning; (2) We propose to extract sentence-level semantic representations directly from pre-trained language models (Devlin et al., 2019; Liu et al., 2019), where there are no operations of converting the wold-level embeddings to sentence-level embeddings. 3 3.1 Fr´echet Bert Distance Semantic representations {vi }N are extracted by a pre-trained language model, which encodes the contextual information of the sentences. The main intuition is that the distribution of semantic representations of generated sentences should be as close as possible to the distribution of semantic representations of real sentences in a successful system. To measure this, we assume that such semantic representations follow a multi-dimensional Gaussia"
2021.findings-acl.193,2021.findings-acl.220,0,0.481841,"nd evaluated. Experiments on several dialogue corpora show that our proposed metrics correlate better with human judgments than existing metrics. 1 Introduction Dialogue generation is a special text generation task, which has drawn booming attention in the natural language processing community. It is widely agreed that one single input query is often associated with multiple valid responses in this task, which is termed as a 1-to-n relationship between a query and its responses (Vinyals and Le, 2015; Zhou et al., 2017; Zhao et al., 2017; Liu et al., 2018; Chen et al., 2021; Chan et al., 2021; Gao et al., 2021). It increases the challenges of automatically evaluating the performance of dialogue systems. In general, the previous evaluation metrics mainly focus on turn-level quality. For example, unsupervised word-overlapping or embedding-based metrics (Papineni et al., 2002; Lin, 2004; Mitchell and Lapata, 2008; Zhang et al., 2020) calculate the similarity or alignment between generated responses and reference responses, which is not wellsuited for open-end dialogue tasks. Learned classification or regression systems (Lowe et al., 2017; ∗ Equal contribution. Work was done during internship at Tencent"
2021.findings-acl.193,W19-2310,0,0.115101,"valuating the performance of dialogue systems. In general, the previous evaluation metrics mainly focus on turn-level quality. For example, unsupervised word-overlapping or embedding-based metrics (Papineni et al., 2002; Lin, 2004; Mitchell and Lapata, 2008; Zhang et al., 2020) calculate the similarity or alignment between generated responses and reference responses, which is not wellsuited for open-end dialogue tasks. Learned classification or regression systems (Lowe et al., 2017; ∗ Equal contribution. Work was done during internship at Tencent AI Lab. Tao et al., 2018; Sellam et al., 2020; Ghazarian et al., 2019) are corpus-dependent because of requiring additional task-specific training or tuning, which run the risk of assigning lower quality to a better model in the overfitting or underfitting cases. In this paper, we provide a new perspective that distribution distance between generated conversations and real conversations can be applied to measure the performances of dialogue systems. There are three main contributions: (1) We firstly propose two unsupervised distribution-wise metrics (i.e., FBD and PRD) to solve the evaluation issue in this field. (2) The experimental results show that the propos"
2021.findings-acl.193,2020.emnlp-main.742,0,0.0282486,"Average (Wieting et al., 2015), Vector Extrema (Forgues et al., 2014) and BERTScore (Zhang et al., 2020). All these metrics do not require task-specific training. Datasets. We collect three recently released evaluation corpora which consist of dialogue query and response samples of different systems, and the corresponding human annotations: • Persona(M): USR (Mehri and Esk´enazi, 2020) built an evaluation corpus based on PersonaChat (Zhang et al., 2018b), in which both four system outputs and the corresponding human evaluation scores were collected. • Daily(H), Convai2, and Empathetic: GRADE (Huang et al., 2020) used three dialogue corpora, including DailyDialog (Lowe et al., 2017), Convai2 (Dinan et al., 2019) and EmpatheticDialogues (Rashkin et al., 2018), to do the evaluations and compared two dialogue models: Transformer-Ranker and Transformer-Generator collected from the ParlAI platform (Miller et al., 2017). • Daily(Z) and Persona(Z): Dialogue Evaluation (Zhao et al., 2020) used Dai2 For a distribution P with a finite state space V, we have v ∈ V and P (v) > 0. BERTScore https://github.com/ facebookresearch/ParlAI https://github.com/nltk/nltk https://github.com/Maluuba/ nlg-eval https://github."
2021.findings-acl.193,W04-1013,0,0.506964,"ommunity. It is widely agreed that one single input query is often associated with multiple valid responses in this task, which is termed as a 1-to-n relationship between a query and its responses (Vinyals and Le, 2015; Zhou et al., 2017; Zhao et al., 2017; Liu et al., 2018; Chen et al., 2021; Chan et al., 2021; Gao et al., 2021). It increases the challenges of automatically evaluating the performance of dialogue systems. In general, the previous evaluation metrics mainly focus on turn-level quality. For example, unsupervised word-overlapping or embedding-based metrics (Papineni et al., 2002; Lin, 2004; Mitchell and Lapata, 2008; Zhang et al., 2020) calculate the similarity or alignment between generated responses and reference responses, which is not wellsuited for open-end dialogue tasks. Learned classification or regression systems (Lowe et al., 2017; ∗ Equal contribution. Work was done during internship at Tencent AI Lab. Tao et al., 2018; Sellam et al., 2020; Ghazarian et al., 2019) are corpus-dependent because of requiring additional task-specific training or tuning, which run the risk of assigning lower quality to a better model in the overfitting or underfitting cases. In this paper"
2021.findings-acl.193,D18-1297,1,0.840076,"distribution-wise metrics, FBD and PRD, are developed and evaluated. Experiments on several dialogue corpora show that our proposed metrics correlate better with human judgments than existing metrics. 1 Introduction Dialogue generation is a special text generation task, which has drawn booming attention in the natural language processing community. It is widely agreed that one single input query is often associated with multiple valid responses in this task, which is termed as a 1-to-n relationship between a query and its responses (Vinyals and Le, 2015; Zhou et al., 2017; Zhao et al., 2017; Liu et al., 2018; Chen et al., 2021; Chan et al., 2021; Gao et al., 2021). It increases the challenges of automatically evaluating the performance of dialogue systems. In general, the previous evaluation metrics mainly focus on turn-level quality. For example, unsupervised word-overlapping or embedding-based metrics (Papineni et al., 2002; Lin, 2004; Mitchell and Lapata, 2008; Zhang et al., 2020) calculate the similarity or alignment between generated responses and reference responses, which is not wellsuited for open-end dialogue tasks. Learned classification or regression systems (Lowe et al., 2017; ∗ Equal"
2021.findings-acl.193,2021.ccl-1.108,0,0.0254893,"Missing"
2021.findings-acl.193,P17-1103,0,0.401324,"l., 2017; Liu et al., 2018; Chen et al., 2021; Chan et al., 2021; Gao et al., 2021). It increases the challenges of automatically evaluating the performance of dialogue systems. In general, the previous evaluation metrics mainly focus on turn-level quality. For example, unsupervised word-overlapping or embedding-based metrics (Papineni et al., 2002; Lin, 2004; Mitchell and Lapata, 2008; Zhang et al., 2020) calculate the similarity or alignment between generated responses and reference responses, which is not wellsuited for open-end dialogue tasks. Learned classification or regression systems (Lowe et al., 2017; ∗ Equal contribution. Work was done during internship at Tencent AI Lab. Tao et al., 2018; Sellam et al., 2020; Ghazarian et al., 2019) are corpus-dependent because of requiring additional task-specific training or tuning, which run the risk of assigning lower quality to a better model in the overfitting or underfitting cases. In this paper, we provide a new perspective that distribution distance between generated conversations and real conversations can be applied to measure the performances of dialogue systems. There are three main contributions: (1) We firstly propose two unsupervised dis"
2021.findings-acl.193,2020.acl-main.64,0,0.02133,"Missing"
2021.findings-acl.193,D17-2014,0,0.0211441,"the corresponding human annotations: • Persona(M): USR (Mehri and Esk´enazi, 2020) built an evaluation corpus based on PersonaChat (Zhang et al., 2018b), in which both four system outputs and the corresponding human evaluation scores were collected. • Daily(H), Convai2, and Empathetic: GRADE (Huang et al., 2020) used three dialogue corpora, including DailyDialog (Lowe et al., 2017), Convai2 (Dinan et al., 2019) and EmpatheticDialogues (Rashkin et al., 2018), to do the evaluations and compared two dialogue models: Transformer-Ranker and Transformer-Generator collected from the ParlAI platform (Miller et al., 2017). • Daily(Z) and Persona(Z): Dialogue Evaluation (Zhao et al., 2020) used Dai2 For a distribution P with a finite state space V, we have v ∈ V and P (v) > 0. BERTScore https://github.com/ facebookresearch/ParlAI https://github.com/nltk/nltk https://github.com/Maluuba/ nlg-eval https://github.com/Tiiiger/ bert_score Table 1: All the public resources in our experiments. Num. of Systems Samples Seq2Seq LSTM language model Persona(M) 60 Key-Value Profile Memory Network Generated Human-written Daily(H) 150 Transformer-Ranker Convai2 150 Transformer-Generator Empathetic 150 Seq2Seq Attentional Seq2S"
2021.findings-acl.193,P08-1028,0,0.78558,"t is widely agreed that one single input query is often associated with multiple valid responses in this task, which is termed as a 1-to-n relationship between a query and its responses (Vinyals and Le, 2015; Zhou et al., 2017; Zhao et al., 2017; Liu et al., 2018; Chen et al., 2021; Chan et al., 2021; Gao et al., 2021). It increases the challenges of automatically evaluating the performance of dialogue systems. In general, the previous evaluation metrics mainly focus on turn-level quality. For example, unsupervised word-overlapping or embedding-based metrics (Papineni et al., 2002; Lin, 2004; Mitchell and Lapata, 2008; Zhang et al., 2020) calculate the similarity or alignment between generated responses and reference responses, which is not wellsuited for open-end dialogue tasks. Learned classification or regression systems (Lowe et al., 2017; ∗ Equal contribution. Work was done during internship at Tencent AI Lab. Tao et al., 2018; Sellam et al., 2020; Ghazarian et al., 2019) are corpus-dependent because of requiring additional task-specific training or tuning, which run the risk of assigning lower quality to a better model in the overfitting or underfitting cases. In this paper, we provide a new perspect"
2021.findings-acl.193,P02-1040,0,0.11645,"l language processing community. It is widely agreed that one single input query is often associated with multiple valid responses in this task, which is termed as a 1-to-n relationship between a query and its responses (Vinyals and Le, 2015; Zhou et al., 2017; Zhao et al., 2017; Liu et al., 2018; Chen et al., 2021; Chan et al., 2021; Gao et al., 2021). It increases the challenges of automatically evaluating the performance of dialogue systems. In general, the previous evaluation metrics mainly focus on turn-level quality. For example, unsupervised word-overlapping or embedding-based metrics (Papineni et al., 2002; Lin, 2004; Mitchell and Lapata, 2008; Zhang et al., 2020) calculate the similarity or alignment between generated responses and reference responses, which is not wellsuited for open-end dialogue tasks. Learned classification or regression systems (Lowe et al., 2017; ∗ Equal contribution. Work was done during internship at Tencent AI Lab. Tao et al., 2018; Sellam et al., 2020; Ghazarian et al., 2019) are corpus-dependent because of requiring additional task-specific training or tuning, which run the risk of assigning lower quality to a better model in the overfitting or underfitting cases. In"
2021.findings-acl.193,W12-2018,0,0.226852,"onal Linguistics onyms. Embedding-based Metrics Embedding-based metrics align the generated response and the reference in latent semantic space. Some adopt the vector similarity of sentence embeddings as a quality measure. For example, Embedding Average (Foltz et al., 1998; Mitchell and Lapata, 2008) calculates sentence-level embeddings by averaging word representations. Vector Extrema (Forgues et al., 2014) computes sentence-level embeddings by taking the most extreme value for each dimension in all word vectors. Others adopt more fine-grained semantic matching. For example, Greedy Matching (Rus and Lintean, 2012) greedily matches each word in a generated response to a word in the reference response, and the final score is defined as the average of word-level similarity scores. Zhang et al. (2020) introduced a better embedding-based metric BERTScore that computes word similarity using contextual embeddings from pre-trained language models. Our proposed methods are best placed in the literature of embedding-based metrics. However, there are two main differences from previous metrics in this field: (1) We compute the distribution distance between embedding sets as the system-level performance of a dialog"
2021.findings-acl.193,2020.acl-main.704,0,0.0215767,"es of automatically evaluating the performance of dialogue systems. In general, the previous evaluation metrics mainly focus on turn-level quality. For example, unsupervised word-overlapping or embedding-based metrics (Papineni et al., 2002; Lin, 2004; Mitchell and Lapata, 2008; Zhang et al., 2020) calculate the similarity or alignment between generated responses and reference responses, which is not wellsuited for open-end dialogue tasks. Learned classification or regression systems (Lowe et al., 2017; ∗ Equal contribution. Work was done during internship at Tencent AI Lab. Tao et al., 2018; Sellam et al., 2020; Ghazarian et al., 2019) are corpus-dependent because of requiring additional task-specific training or tuning, which run the risk of assigning lower quality to a better model in the overfitting or underfitting cases. In this paper, we provide a new perspective that distribution distance between generated conversations and real conversations can be applied to measure the performances of dialogue systems. There are three main contributions: (1) We firstly propose two unsupervised distribution-wise metrics (i.e., FBD and PRD) to solve the evaluation issue in this field. (2) The experimental res"
2021.findings-acl.193,P18-1205,0,0.417248,"all the Fr´echet distance between the distribution R with mean (µr , Σr ) obtained from real sentence pairs and the distribution G with mean (µg , Σg ) obtained from generated sentence pairs as “Fr´echet Bert Distance” (FBD), which is formulated as: dFBD (R, G) = kµr − µg k+ Tr(Σr + Σg − 2(Σr Σg )1/2 ) (2) Once the distribution of generated data closes to the distribution of real data, the model indeed achieves low FBD scores. Similarly, such distance (Heusel et al., 2017) has been widely verified in various Generative Adversarial Networks (GANs) in computer vision tasks (Karras et al., 2017; Zhang et al., 2018a; Park et al., 2019), which is consistent with increasing disturbances and human judgment. Surprisingly, we observed that FBD works well in evaluating open-end dialogue systems. Proposed Methods Given a collect of sentence pairs {(xi , yi )}N , we assume that the corresponding semantic representations {vi }N can be extracted in this manner: vi = LM ([xi , yi ]) (1) where LM (·) refers to pre-trained language models(i.e., (Devlin et al., 2019; Liu et al., 2019; Yang et al., 2019; Clark et al., 2020)), [·, ·] refers to the concatenation operation. Intuitively, the differences between the distri"
2021.findings-acl.193,P17-1061,0,0.0301681,". Specifically, two distribution-wise metrics, FBD and PRD, are developed and evaluated. Experiments on several dialogue corpora show that our proposed metrics correlate better with human judgments than existing metrics. 1 Introduction Dialogue generation is a special text generation task, which has drawn booming attention in the natural language processing community. It is widely agreed that one single input query is often associated with multiple valid responses in this task, which is termed as a 1-to-n relationship between a query and its responses (Vinyals and Le, 2015; Zhou et al., 2017; Zhao et al., 2017; Liu et al., 2018; Chen et al., 2021; Chan et al., 2021; Gao et al., 2021). It increases the challenges of automatically evaluating the performance of dialogue systems. In general, the previous evaluation metrics mainly focus on turn-level quality. For example, unsupervised word-overlapping or embedding-based metrics (Papineni et al., 2002; Lin, 2004; Mitchell and Lapata, 2008; Zhang et al., 2020) calculate the similarity or alignment between generated responses and reference responses, which is not wellsuited for open-end dialogue tasks. Learned classification or regression systems (Lowe et"
2021.findings-acl.193,2020.acl-main.4,0,0.126149,"enazi, 2020) built an evaluation corpus based on PersonaChat (Zhang et al., 2018b), in which both four system outputs and the corresponding human evaluation scores were collected. • Daily(H), Convai2, and Empathetic: GRADE (Huang et al., 2020) used three dialogue corpora, including DailyDialog (Lowe et al., 2017), Convai2 (Dinan et al., 2019) and EmpatheticDialogues (Rashkin et al., 2018), to do the evaluations and compared two dialogue models: Transformer-Ranker and Transformer-Generator collected from the ParlAI platform (Miller et al., 2017). • Daily(Z) and Persona(Z): Dialogue Evaluation (Zhao et al., 2020) used Dai2 For a distribution P with a finite state space V, we have v ∈ V and P (v) > 0. BERTScore https://github.com/ facebookresearch/ParlAI https://github.com/nltk/nltk https://github.com/Maluuba/ nlg-eval https://github.com/Tiiiger/ bert_score Table 1: All the public resources in our experiments. Num. of Systems Samples Seq2Seq LSTM language model Persona(M) 60 Key-Value Profile Memory Network Generated Human-written Daily(H) 150 Transformer-Ranker Convai2 150 Transformer-Generator Empathetic 150 Seq2Seq Attentional Seq2Seq Daily(Z) 100 HRED Persona(Z) 150 VHRED GPT2-sm GPT2-md Corpus Tab"
2021.findings-acl.432,D19-1501,1,0.854308,"ogue evaluation Metric in latent Space. Experimental results on two real-world dialogue datasets confirm the superiority of our method for open-domain dialogue evaluation, where both Pearson and Spearman correlations with human judgments outperform all baselines. 1 Introduction With the surge of deep learning techniques, generation-based open-domain dialogue systems have witnessed significant improvement in recent years. Plenty of novel and effective models (Sutskever et al., 2014; Serban et al., 2016; Li et al., 2015; Serban et al., 2016; Zhao et al., 2017; Gu et al., 2018; Qiu et al., 2019; Chan et al., 2019b; Serban et al., 2017; Wolf et al., 2019; Hu et al., 2019; Chen et al., 2020) are proposed and have greatly promoted the development of the opendomain dialogue generation. Unlike the endless emergence of novel methods, however, there is still no meaningful and widely accepted automatic evaluation metric for dialogue generation yet. As we ∗ This work was done while Z. Chan was an intern at Tencent AI Lab. Corresponding Author: Rui Yan. know, automatic evaluation allows quick and effective comparison between different systems and is crucial for the development of natural language generation (NL"
2021.findings-acl.432,D19-1201,1,0.869864,"ogue evaluation Metric in latent Space. Experimental results on two real-world dialogue datasets confirm the superiority of our method for open-domain dialogue evaluation, where both Pearson and Spearman correlations with human judgments outperform all baselines. 1 Introduction With the surge of deep learning techniques, generation-based open-domain dialogue systems have witnessed significant improvement in recent years. Plenty of novel and effective models (Sutskever et al., 2014; Serban et al., 2016; Li et al., 2015; Serban et al., 2016; Zhao et al., 2017; Gu et al., 2018; Qiu et al., 2019; Chan et al., 2019b; Serban et al., 2017; Wolf et al., 2019; Hu et al., 2019; Chen et al., 2020) are proposed and have greatly promoted the development of the opendomain dialogue generation. Unlike the endless emergence of novel methods, however, there is still no meaningful and widely accepted automatic evaluation metric for dialogue generation yet. As we ∗ This work was done while Z. Chan was an intern at Tencent AI Lab. Corresponding Author: Rui Yan. know, automatic evaluation allows quick and effective comparison between different systems and is crucial for the development of natural language generation (NL"
2021.findings-acl.432,2020.emnlp-main.313,1,0.729602,"roved BLEU, which compares the generated response with multiply diverse references. Embedding-based Metrics. Unlike word overlap-based metrics comparing two raw sentences, Embedding Metrics (Mitchell and Lapata, 2008; Forgues et al., 2014; Rus and Lintean, 2012) map sentences to a high dimensional space, and calculate similarity based on the high-dimensional representations. Embedding Metrics are recently popular for evaluating the generation tasks, such as text summarization (Gao et al., 2020; Chen et al., 2021), question answer (Gao et al., 2019) and text generation (Hashimoto et al., 2019; Chan et al., 2020). Meanwhile, several works (Qiu et al., 2019; Chen et al., 2020; Gao et al., 2021) have shown their effectiveness in the open-domain dialogue systems. With the development of the large4890 Learning-based Metrics. Recent studies (Tao et al., 2018; Sinha et al., 2020) attempt to mitigate the one-to-many issue by considering the similarity of the generated response with the conversational contexts. The similarity is calculated by a designed discriminative model which learns to evaluate whether a response matches the conversational context well. The discriminative model is learned from tuples of d"
2021.findings-acl.432,2021.acl-long.34,0,0.469022,"ious one-to-many nature of the open-domain dialogues. Therefore, Yuma et al. (2020) proposed the improved BLEU, which compares the generated response with multiply diverse references. Embedding-based Metrics. Unlike word overlap-based metrics comparing two raw sentences, Embedding Metrics (Mitchell and Lapata, 2008; Forgues et al., 2014; Rus and Lintean, 2012) map sentences to a high dimensional space, and calculate similarity based on the high-dimensional representations. Embedding Metrics are recently popular for evaluating the generation tasks, such as text summarization (Gao et al., 2020; Chen et al., 2021), question answer (Gao et al., 2019) and text generation (Hashimoto et al., 2019; Chan et al., 2020). Meanwhile, several works (Qiu et al., 2019; Chen et al., 2020; Gao et al., 2021) have shown their effectiveness in the open-domain dialogue systems. With the development of the large4890 Learning-based Metrics. Recent studies (Tao et al., 2018; Sinha et al., 2020) attempt to mitigate the one-to-many issue by considering the similarity of the generated response with the conversational contexts. The similarity is calculated by a designed discriminative model which learns to evaluate whether a re"
2021.findings-acl.432,N19-1021,0,0.0144273,"dding. posterior qθ (z|hq ), i.e. a conditional Gaussian distribution. Training procedures. Previous works (Bowman et al., 2015; Zhao et al., 2017) mentioned that VAE and CVAE training is challenging due to the KL vanishing issue, where the decoder ignores the conditional information and all the resulting posteriors almost collapse to a same Gaussian prior. To mitigate this issue, first, we initialize our model with Optimus (Li et al., 2020), a large-scale VAE-based PLM model, while optimizing Eq. 2. To mitigate the same issue while optimizing Eq. 3, we use the cyclical KL annealing schedule (Fu et al., 2019). Specifically, we add a hyperparameter α to control the weight of the KLdivergence in Eq. 3. We set α close to zero in the first half of cyclic schedule, linearly anneal α to 1 in the next one-fourth of cyclic schedule and kept α = 1 in the remaining cyclic schedule. Moreover, the Free Bits (Bowman et al., 2015) is also crucial for the training. It replaces the KLdivergence in Eq. 3 by a hinge loss max(γ, KL(qφ (z|hq )||pθ (z|hp ))) (4) where γ is a hyperparameter which controls the information space for the each dimension of the latent variable. Finally, an extra bag-of-word loss (Zhao et al"
2021.findings-acl.432,2021.findings-acl.220,1,0.534829,"Embedding-based Metrics. Unlike word overlap-based metrics comparing two raw sentences, Embedding Metrics (Mitchell and Lapata, 2008; Forgues et al., 2014; Rus and Lintean, 2012) map sentences to a high dimensional space, and calculate similarity based on the high-dimensional representations. Embedding Metrics are recently popular for evaluating the generation tasks, such as text summarization (Gao et al., 2020; Chen et al., 2021), question answer (Gao et al., 2019) and text generation (Hashimoto et al., 2019; Chan et al., 2020). Meanwhile, several works (Qiu et al., 2019; Chen et al., 2020; Gao et al., 2021) have shown their effectiveness in the open-domain dialogue systems. With the development of the large4890 Learning-based Metrics. Recent studies (Tao et al., 2018; Sinha et al., 2020) attempt to mitigate the one-to-many issue by considering the similarity of the generated response with the conversational contexts. The similarity is calculated by a designed discriminative model which learns to evaluate whether a response matches the conversational context well. The discriminative model is learned from tuples of data, {conversational context, response reference, negative sample}, in an unsuperv"
2021.findings-acl.432,2020.acl-main.124,0,0.101111,"ignoring the notorious one-to-many nature of the open-domain dialogues. Therefore, Yuma et al. (2020) proposed the improved BLEU, which compares the generated response with multiply diverse references. Embedding-based Metrics. Unlike word overlap-based metrics comparing two raw sentences, Embedding Metrics (Mitchell and Lapata, 2008; Forgues et al., 2014; Rus and Lintean, 2012) map sentences to a high dimensional space, and calculate similarity based on the high-dimensional representations. Embedding Metrics are recently popular for evaluating the generation tasks, such as text summarization (Gao et al., 2020; Chen et al., 2021), question answer (Gao et al., 2019) and text generation (Hashimoto et al., 2019; Chan et al., 2020). Meanwhile, several works (Qiu et al., 2019; Chen et al., 2020; Gao et al., 2021) have shown their effectiveness in the open-domain dialogue systems. With the development of the large4890 Learning-based Metrics. Recent studies (Tao et al., 2018; Sinha et al., 2020) attempt to mitigate the one-to-many issue by considering the similarity of the generated response with the conversational contexts. The similarity is calculated by a designed discriminative model which learns to e"
2021.findings-acl.432,W19-2310,0,0.198948,"Missing"
2021.findings-acl.432,P19-1590,0,0.0127027,"rior distribution P (z|c) to capture the feasible latent reference information in the latent space. Specifically, when training CVAEs, P (z|c) is forced to be close to the posterior distribution Q(z|c, ri ) for any reference response ri as illustrated in Fig. 1. In this sense, if z is sampled from P (z|c), z may contain some information of any ri in some extent, and z can be used as a surrogate of {rk }K k=1 . Therefore, we can expect 4891 1 There is a brief proof in Apendix A. I(l; c, {rk }N k=1 ) ≥ I(l; c, ri , z) ≥ I(l; c, ri ). 3.2 3.3 Overall Architecture Previous works (Li et al., 2019; Gururangan et al., 2019; Li et al., 2020) concluded that VAEs can learn a smooth latent space through the regularization from the gaussian prior. Inspired by Li et al. (2020), we propose a novel architecture which can be regarded as a large-scale pretrained language model (PLM) based on VAEs/CVAEs. Encoder. Li et al. (2019) argue that the VAEs might benefit from initialization with a noncollapsed encoder, because the encoder provides useful information from the beginning of training. We use the Masked PLMs (Devlin et al., 2018; Liu et al., 2019) as the text encoder because of their impressive effectiveness in natura"
2021.findings-acl.432,N19-1169,0,0.0184016,"(2020) proposed the improved BLEU, which compares the generated response with multiply diverse references. Embedding-based Metrics. Unlike word overlap-based metrics comparing two raw sentences, Embedding Metrics (Mitchell and Lapata, 2008; Forgues et al., 2014; Rus and Lintean, 2012) map sentences to a high dimensional space, and calculate similarity based on the high-dimensional representations. Embedding Metrics are recently popular for evaluating the generation tasks, such as text summarization (Gao et al., 2020; Chen et al., 2021), question answer (Gao et al., 2019) and text generation (Hashimoto et al., 2019; Chan et al., 2020). Meanwhile, several works (Qiu et al., 2019; Chen et al., 2020; Gao et al., 2021) have shown their effectiveness in the open-domain dialogue systems. With the development of the large4890 Learning-based Metrics. Recent studies (Tao et al., 2018; Sinha et al., 2020) attempt to mitigate the one-to-many issue by considering the similarity of the generated response with the conversational contexts. The similarity is calculated by a designed discriminative model which learns to evaluate whether a response matches the conversational context well. The discriminative model is lear"
2021.findings-acl.432,D19-1370,0,0.23084,"ntation learning and dialogue modeling, we propose to learn the dialogue representations via VAEs/CVAEs for better evaluation. Equip with such dialogue representations, we obtain an Enhanced dialogue evaluation Metric in latent Space (EMS). EMS is a self-supervised evaluation metric with a two-stage training procedure. It represents dialogue sentences in a smooth latent space to both capture discourse-level context information and model more feasible latent references. Specifically, in the first stage, we build a VAE based model to map the dialogue sentences into a latent (or semantic) space. Li et al. (2019) showed that VAEs can be viewed as a regularized version of the auto-encoder and learn a smooth latent space through the regularization from the Gaussian prior. Then, we train our model by optimizing CVAEs’ objective which forces the prior distribution to capture the feasible latent references information (details in Section 3.3). In the second stage, we combine the dialogue representations and the captured feasible latent reference information to train a discriminative model. Meanwhile, we give a potential explanation of our motivation about why using feasible latent reference information can"
2021.findings-acl.432,2020.emnlp-main.378,0,0.250324,"to capture the feasible latent reference information in the latent space. Specifically, when training CVAEs, P (z|c) is forced to be close to the posterior distribution Q(z|c, ri ) for any reference response ri as illustrated in Fig. 1. In this sense, if z is sampled from P (z|c), z may contain some information of any ri in some extent, and z can be used as a surrogate of {rk }K k=1 . Therefore, we can expect 4891 1 There is a brief proof in Apendix A. I(l; c, {rk }N k=1 ) ≥ I(l; c, ri , z) ≥ I(l; c, ri ). 3.2 3.3 Overall Architecture Previous works (Li et al., 2019; Gururangan et al., 2019; Li et al., 2020) concluded that VAEs can learn a smooth latent space through the regularization from the gaussian prior. Inspired by Li et al. (2020), we propose a novel architecture which can be regarded as a large-scale pretrained language model (PLM) based on VAEs/CVAEs. Encoder. Li et al. (2019) argue that the VAEs might benefit from initialization with a noncollapsed encoder, because the encoder provides useful information from the beginning of training. We use the Masked PLMs (Devlin et al., 2018; Liu et al., 2019) as the text encoder because of their impressive effectiveness in natural language underst"
2021.findings-acl.432,I17-1099,0,0.0443873,"Missing"
2021.findings-acl.432,D16-1230,0,0.030509,"ning variational model to capture the feasible latent references; • Experiments performed on two large datasets demonstrate the effectiveness of our proposed model and outperform all baseline methods. 2 Related Work Word overlap-based Metrics. Several word overlap-based automatic evaluation metrics, such as BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005), and ROUGE (Lin, 2004), have been widely used to evaluate the quality of generated responses. These word overlap-based metrics measure how many words overlap in a given generated response when compared to a reference response. Liu et al. (2016); Lowe et al. (2017); Tao et al. (2018) argued that these word overlap-based metric scores are weakly correlated to human judgment due to ignoring the notorious one-to-many nature of the open-domain dialogues. Therefore, Yuma et al. (2020) proposed the improved BLEU, which compares the generated response with multiply diverse references. Embedding-based Metrics. Unlike word overlap-based metrics comparing two raw sentences, Embedding Metrics (Mitchell and Lapata, 2008; Forgues et al., 2014; Rus and Lintean, 2012) map sentences to a high dimensional space, and calculate similarity based on the"
2021.findings-acl.432,2021.ccl-1.108,0,0.0249961,"Missing"
2021.findings-acl.432,P17-1103,0,0.0589553,"del to capture the feasible latent references; • Experiments performed on two large datasets demonstrate the effectiveness of our proposed model and outperform all baseline methods. 2 Related Work Word overlap-based Metrics. Several word overlap-based automatic evaluation metrics, such as BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005), and ROUGE (Lin, 2004), have been widely used to evaluate the quality of generated responses. These word overlap-based metrics measure how many words overlap in a given generated response when compared to a reference response. Liu et al. (2016); Lowe et al. (2017); Tao et al. (2018) argued that these word overlap-based metric scores are weakly correlated to human judgment due to ignoring the notorious one-to-many nature of the open-domain dialogues. Therefore, Yuma et al. (2020) proposed the improved BLEU, which compares the generated response with multiply diverse references. Embedding-based Metrics. Unlike word overlap-based metrics comparing two raw sentences, Embedding Metrics (Mitchell and Lapata, 2008; Forgues et al., 2014; Rus and Lintean, 2012) map sentences to a high dimensional space, and calculate similarity based on the high-dimensional rep"
2021.findings-acl.432,P08-1028,0,0.819519,"ural language generation (NLG) tasks (Dathathri et al., 2019; Gu et al., 2019; Gao et al., 2019; Chan et al., 2019a, 2020). The lack of meaningful automatic evaluation metrics has become a significant impediment for open-domain dialog generation research. Over the past decade, many automatic evaluation metrics are proposed to evaluate the opendomain dialogue systems. Among them, the word overlap-based automatic evaluation metrics from NLG tasks, such as BLEU (Papineni et al., 2002) in machine translation and ROUGE (Lin, 2004) in text summarization, are popular. In addition, Embedding Metrics (Mitchell and Lapata, 2008; Forgues et al., 2014; Rus and Lintean, 2012) have been utilized to evaluate the open-domain dialogue systems (Gu et al., 2018; Chan et al., 2019b; Shen et al., 2018). Recently, with the fantastic development of the large-scale pre-training model (Devlin et al., 2018; Liu et al., 2019; Radford et al., 2019), researchers proposed to enhance the embedding metrics by converting the dialogue sentences to hidden space via pre-training model (Zhang et al., 2019; Sellam et al., 2020; Zhao et al., 2019; Xiang et al., 2021). The common idea behind these metrics is that they measure the semantic simila"
2021.findings-acl.432,P02-1040,0,0.116934,"an. know, automatic evaluation allows quick and effective comparison between different systems and is crucial for the development of natural language generation (NLG) tasks (Dathathri et al., 2019; Gu et al., 2019; Gao et al., 2019; Chan et al., 2019a, 2020). The lack of meaningful automatic evaluation metrics has become a significant impediment for open-domain dialog generation research. Over the past decade, many automatic evaluation metrics are proposed to evaluate the opendomain dialogue systems. Among them, the word overlap-based automatic evaluation metrics from NLG tasks, such as BLEU (Papineni et al., 2002) in machine translation and ROUGE (Lin, 2004) in text summarization, are popular. In addition, Embedding Metrics (Mitchell and Lapata, 2008; Forgues et al., 2014; Rus and Lintean, 2012) have been utilized to evaluate the open-domain dialogue systems (Gu et al., 2018; Chan et al., 2019b; Shen et al., 2018). Recently, with the fantastic development of the large-scale pre-training model (Devlin et al., 2018; Liu et al., 2019; Radford et al., 2019), researchers proposed to enhance the embedding metrics by converting the dialogue sentences to hidden space via pre-training model (Zhang et al., 2019;"
2021.findings-acl.432,P19-1372,1,0.904042,", an Enhanced dialogue evaluation Metric in latent Space. Experimental results on two real-world dialogue datasets confirm the superiority of our method for open-domain dialogue evaluation, where both Pearson and Spearman correlations with human judgments outperform all baselines. 1 Introduction With the surge of deep learning techniques, generation-based open-domain dialogue systems have witnessed significant improvement in recent years. Plenty of novel and effective models (Sutskever et al., 2014; Serban et al., 2016; Li et al., 2015; Serban et al., 2016; Zhao et al., 2017; Gu et al., 2018; Qiu et al., 2019; Chan et al., 2019b; Serban et al., 2017; Wolf et al., 2019; Hu et al., 2019; Chen et al., 2020) are proposed and have greatly promoted the development of the opendomain dialogue generation. Unlike the endless emergence of novel methods, however, there is still no meaningful and widely accepted automatic evaluation metric for dialogue generation yet. As we ∗ This work was done while Z. Chan was an intern at Tencent AI Lab. Corresponding Author: Rui Yan. know, automatic evaluation allows quick and effective comparison between different systems and is crucial for the development of natural lang"
2021.findings-acl.432,W12-2018,0,0.667018,"t al., 2019; Gu et al., 2019; Gao et al., 2019; Chan et al., 2019a, 2020). The lack of meaningful automatic evaluation metrics has become a significant impediment for open-domain dialog generation research. Over the past decade, many automatic evaluation metrics are proposed to evaluate the opendomain dialogue systems. Among them, the word overlap-based automatic evaluation metrics from NLG tasks, such as BLEU (Papineni et al., 2002) in machine translation and ROUGE (Lin, 2004) in text summarization, are popular. In addition, Embedding Metrics (Mitchell and Lapata, 2008; Forgues et al., 2014; Rus and Lintean, 2012) have been utilized to evaluate the open-domain dialogue systems (Gu et al., 2018; Chan et al., 2019b; Shen et al., 2018). Recently, with the fantastic development of the large-scale pre-training model (Devlin et al., 2018; Liu et al., 2019; Radford et al., 2019), researchers proposed to enhance the embedding metrics by converting the dialogue sentences to hidden space via pre-training model (Zhang et al., 2019; Sellam et al., 2020; Zhao et al., 2019; Xiang et al., 2021). The common idea behind these metrics is that they measure the semantic similarity between a reference response and a genera"
2021.findings-acl.432,2020.tacl-1.52,0,0.402268,"for Computational Linguistics responses with the conversational context. Specifically, these works design discriminative models which can judge whether the generated responses match the conversational context well, which learn from {conversational context, response reference, negative sample} pairs in unsupervised learning manner. Zhao et al. (2020) further proposed to enhance such discriminative evaluation metrics by finetuning on a few human-annotated data to improve the robustness. These discriminative metrics trained using a single relevant response and multiple negative samples. However, Sai et al. (2020) argued that such discriminative metrics should be trained on multiple relevant responses (i.e., positive samples) and multiple negative samples, to favor the one-to-many nature in open-domain dialogues. Therefore, they collected a new dataset which contains multiple relevant and irrelevant responses for any given conversational context to train their discriminative evaluation model and the model trained by multiple relevant responses shows impressive performance. However, there are no organized relevant multiple responses in most existing datasets. Collecting a new dataset is expensive and ti"
2021.findings-acl.432,2020.acl-main.704,0,0.0671024,"in machine translation and ROUGE (Lin, 2004) in text summarization, are popular. In addition, Embedding Metrics (Mitchell and Lapata, 2008; Forgues et al., 2014; Rus and Lintean, 2012) have been utilized to evaluate the open-domain dialogue systems (Gu et al., 2018; Chan et al., 2019b; Shen et al., 2018). Recently, with the fantastic development of the large-scale pre-training model (Devlin et al., 2018; Liu et al., 2019; Radford et al., 2019), researchers proposed to enhance the embedding metrics by converting the dialogue sentences to hidden space via pre-training model (Zhang et al., 2019; Sellam et al., 2020; Zhao et al., 2019; Xiang et al., 2021). The common idea behind these metrics is that they measure the semantic similarity between a reference response and a generated response, independent on the conversational context. However, due to the notorious one-to-many nature (Li et al., 2015; Zhao et al., 2017; Qiu et al., 2019; Gu et al., 2018) of open-domain dialogue, a good response should be related well to its context yet may be largely different from a reference response in semantics. Some other works (Tao et al., 2018; Ghazarian et al., 2019; Sinha et al., 2020) thereby proposed to build aut"
2021.findings-acl.432,P18-1205,0,0.100736,"Missing"
2021.findings-acl.432,P17-1061,0,0.20045,"space. Specifically, we present EMS, an Enhanced dialogue evaluation Metric in latent Space. Experimental results on two real-world dialogue datasets confirm the superiority of our method for open-domain dialogue evaluation, where both Pearson and Spearman correlations with human judgments outperform all baselines. 1 Introduction With the surge of deep learning techniques, generation-based open-domain dialogue systems have witnessed significant improvement in recent years. Plenty of novel and effective models (Sutskever et al., 2014; Serban et al., 2016; Li et al., 2015; Serban et al., 2016; Zhao et al., 2017; Gu et al., 2018; Qiu et al., 2019; Chan et al., 2019b; Serban et al., 2017; Wolf et al., 2019; Hu et al., 2019; Chen et al., 2020) are proposed and have greatly promoted the development of the opendomain dialogue generation. Unlike the endless emergence of novel methods, however, there is still no meaningful and widely accepted automatic evaluation metric for dialogue generation yet. As we ∗ This work was done while Z. Chan was an intern at Tencent AI Lab. Corresponding Author: Rui Yan. know, automatic evaluation allows quick and effective comparison between different systems and is crucial"
2021.findings-acl.432,2020.acl-main.4,0,0.417553,"9; Sinha et al., 2020) thereby proposed to build automatic dialogue evaluation metrics by considering the similarity of the generated 4889 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 4889–4900 August 1–6, 2021. ©2021 Association for Computational Linguistics responses with the conversational context. Specifically, these works design discriminative models which can judge whether the generated responses match the conversational context well, which learn from {conversational context, response reference, negative sample} pairs in unsupervised learning manner. Zhao et al. (2020) further proposed to enhance such discriminative evaluation metrics by finetuning on a few human-annotated data to improve the robustness. These discriminative metrics trained using a single relevant response and multiple negative samples. However, Sai et al. (2020) argued that such discriminative metrics should be trained on multiple relevant responses (i.e., positive samples) and multiple negative samples, to favor the one-to-many nature in open-domain dialogues. Therefore, they collected a new dataset which contains multiple relevant and irrelevant responses for any given conversational con"
2021.findings-acl.432,D19-1053,0,0.201466,"n and ROUGE (Lin, 2004) in text summarization, are popular. In addition, Embedding Metrics (Mitchell and Lapata, 2008; Forgues et al., 2014; Rus and Lintean, 2012) have been utilized to evaluate the open-domain dialogue systems (Gu et al., 2018; Chan et al., 2019b; Shen et al., 2018). Recently, with the fantastic development of the large-scale pre-training model (Devlin et al., 2018; Liu et al., 2019; Radford et al., 2019), researchers proposed to enhance the embedding metrics by converting the dialogue sentences to hidden space via pre-training model (Zhang et al., 2019; Sellam et al., 2020; Zhao et al., 2019; Xiang et al., 2021). The common idea behind these metrics is that they measure the semantic similarity between a reference response and a generated response, independent on the conversational context. However, due to the notorious one-to-many nature (Li et al., 2015; Zhao et al., 2017; Qiu et al., 2019; Gu et al., 2018) of open-domain dialogue, a good response should be related well to its context yet may be largely different from a reference response in semantics. Some other works (Tao et al., 2018; Ghazarian et al., 2019; Sinha et al., 2020) thereby proposed to build automatic dialogue eva"
2021.findings-acl.432,D18-1463,0,0.0149516,"s has become a significant impediment for open-domain dialog generation research. Over the past decade, many automatic evaluation metrics are proposed to evaluate the opendomain dialogue systems. Among them, the word overlap-based automatic evaluation metrics from NLG tasks, such as BLEU (Papineni et al., 2002) in machine translation and ROUGE (Lin, 2004) in text summarization, are popular. In addition, Embedding Metrics (Mitchell and Lapata, 2008; Forgues et al., 2014; Rus and Lintean, 2012) have been utilized to evaluate the open-domain dialogue systems (Gu et al., 2018; Chan et al., 2019b; Shen et al., 2018). Recently, with the fantastic development of the large-scale pre-training model (Devlin et al., 2018; Liu et al., 2019; Radford et al., 2019), researchers proposed to enhance the embedding metrics by converting the dialogue sentences to hidden space via pre-training model (Zhang et al., 2019; Sellam et al., 2020; Zhao et al., 2019; Xiang et al., 2021). The common idea behind these metrics is that they measure the semantic similarity between a reference response and a generated response, independent on the conversational context. However, due to the notorious one-to-many nature (Li et al., 201"
2021.findings-acl.432,2020.acl-main.220,0,0.122106,"ng model (Zhang et al., 2019; Sellam et al., 2020; Zhao et al., 2019; Xiang et al., 2021). The common idea behind these metrics is that they measure the semantic similarity between a reference response and a generated response, independent on the conversational context. However, due to the notorious one-to-many nature (Li et al., 2015; Zhao et al., 2017; Qiu et al., 2019; Gu et al., 2018) of open-domain dialogue, a good response should be related well to its context yet may be largely different from a reference response in semantics. Some other works (Tao et al., 2018; Ghazarian et al., 2019; Sinha et al., 2020) thereby proposed to build automatic dialogue evaluation metrics by considering the similarity of the generated 4889 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 4889–4900 August 1–6, 2021. ©2021 Association for Computational Linguistics responses with the conversational context. Specifically, these works design discriminative models which can judge whether the generated responses match the conversational context well, which learn from {conversational context, response reference, negative sample} pairs in unsupervised learning manner. Zhao et al. (2020) fur"
2021.findings-acl.432,2021.findings-acl.193,1,0.84362,"004) in text summarization, are popular. In addition, Embedding Metrics (Mitchell and Lapata, 2008; Forgues et al., 2014; Rus and Lintean, 2012) have been utilized to evaluate the open-domain dialogue systems (Gu et al., 2018; Chan et al., 2019b; Shen et al., 2018). Recently, with the fantastic development of the large-scale pre-training model (Devlin et al., 2018; Liu et al., 2019; Radford et al., 2019), researchers proposed to enhance the embedding metrics by converting the dialogue sentences to hidden space via pre-training model (Zhang et al., 2019; Sellam et al., 2020; Zhao et al., 2019; Xiang et al., 2021). The common idea behind these metrics is that they measure the semantic similarity between a reference response and a generated response, independent on the conversational context. However, due to the notorious one-to-many nature (Li et al., 2015; Zhao et al., 2017; Qiu et al., 2019; Gu et al., 2018) of open-domain dialogue, a good response should be related well to its context yet may be largely different from a reference response in semantics. Some other works (Tao et al., 2018; Ghazarian et al., 2019; Sinha et al., 2020) thereby proposed to build automatic dialogue evaluation metrics by co"
2021.findings-acl.432,2020.acl-srw.27,0,0.0191815,"rics. Several word overlap-based automatic evaluation metrics, such as BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005), and ROUGE (Lin, 2004), have been widely used to evaluate the quality of generated responses. These word overlap-based metrics measure how many words overlap in a given generated response when compared to a reference response. Liu et al. (2016); Lowe et al. (2017); Tao et al. (2018) argued that these word overlap-based metric scores are weakly correlated to human judgment due to ignoring the notorious one-to-many nature of the open-domain dialogues. Therefore, Yuma et al. (2020) proposed the improved BLEU, which compares the generated response with multiply diverse references. Embedding-based Metrics. Unlike word overlap-based metrics comparing two raw sentences, Embedding Metrics (Mitchell and Lapata, 2008; Forgues et al., 2014; Rus and Lintean, 2012) map sentences to a high dimensional space, and calculate similarity based on the high-dimensional representations. Embedding Metrics are recently popular for evaluating the generation tasks, such as text summarization (Gao et al., 2020; Chen et al., 2021), question answer (Gao et al., 2019) and text generation (Hashimo"
2021.findings-emnlp.159,C16-1236,0,0.32399,"able online1 . 1 Introduction Knowledge Base Question Answering (KBQA) aims at finding answers from the existing knowledge bases (KBs) such as freebase (Bollacker et al., 2008) and DBPedia (Lehmann et al., 2015) for the given questions expressed in natural language. KBQA has emerged as an important research topic in the last few years (Sun et al., 2018, 2019; Lan and Jiang, 2020; He et al., 2021), as the logically organized entities and relations in KBs can explicitly facilitate the QA process. Two mainstream methods including the semantic parsing based (SP-based) models (Berant et al., 2013; Bao et al., 2016; Liang et al., 2017; Lan and Jiang, 2020) and the information retrieval based (IR-based) models (Sun et al., 2018, 2019; Saxena et al., 2020; He et al., 2021) are commonly studied ∗ 1 Corresponding author. https://github.com/RUCKBReasoning/NumKBQA to solve KBQA task. The SP-based models heavily rely on the intermediate logic query parsed from the natural language question, which turns out to be the bottleneck of performance improvement (Lan et al., 2021). On the contrary, the IR-based models directly represent and rank the entities in a questionaware subgraph based on their relevance to the q"
2021.findings-emnlp.159,D13-1160,0,0.0649809,"nd datasets are available online1 . 1 Introduction Knowledge Base Question Answering (KBQA) aims at finding answers from the existing knowledge bases (KBs) such as freebase (Bollacker et al., 2008) and DBPedia (Lehmann et al., 2015) for the given questions expressed in natural language. KBQA has emerged as an important research topic in the last few years (Sun et al., 2018, 2019; Lan and Jiang, 2020; He et al., 2021), as the logically organized entities and relations in KBs can explicitly facilitate the QA process. Two mainstream methods including the semantic parsing based (SP-based) models (Berant et al., 2013; Bao et al., 2016; Liang et al., 2017; Lan and Jiang, 2020) and the information retrieval based (IR-based) models (Sun et al., 2018, 2019; Saxena et al., 2020; He et al., 2021) are commonly studied ∗ 1 Corresponding author. https://github.com/RUCKBReasoning/NumKBQA to solve KBQA task. The SP-based models heavily rely on the intermediate logic query parsed from the natural language question, which turns out to be the bottleneck of performance improvement (Lan et al., 2021). On the contrary, the IR-based models directly represent and rank the entities in a questionaware subgraph based on their"
2021.findings-emnlp.159,P14-1133,0,0.037051,"Missing"
2021.findings-emnlp.159,2020.emnlp-main.549,0,0.0843768,"Missing"
2021.findings-emnlp.159,2020.acl-main.91,0,0.107487,"BQA model to enhance its numerical reasoning ability. Extensive experiments on two KBQA benchmarks verify the effectiveness of our method to enhance the numerical reasoning ability for IR-based KBQA models. Our code and datasets are available online1 . 1 Introduction Knowledge Base Question Answering (KBQA) aims at finding answers from the existing knowledge bases (KBs) such as freebase (Bollacker et al., 2008) and DBPedia (Lehmann et al., 2015) for the given questions expressed in natural language. KBQA has emerged as an important research topic in the last few years (Sun et al., 2018, 2019; Lan and Jiang, 2020; He et al., 2021), as the logically organized entities and relations in KBs can explicitly facilitate the QA process. Two mainstream methods including the semantic parsing based (SP-based) models (Berant et al., 2013; Bao et al., 2016; Liang et al., 2017; Lan and Jiang, 2020) and the information retrieval based (IR-based) models (Sun et al., 2018, 2019; Saxena et al., 2020; He et al., 2021) are commonly studied ∗ 1 Corresponding author. https://github.com/RUCKBReasoning/NumKBQA to solve KBQA task. The SP-based models heavily rely on the intermediate logic query parsed from the natural languag"
2021.findings-emnlp.159,P17-1003,0,0.0181724,"ntroduction Knowledge Base Question Answering (KBQA) aims at finding answers from the existing knowledge bases (KBs) such as freebase (Bollacker et al., 2008) and DBPedia (Lehmann et al., 2015) for the given questions expressed in natural language. KBQA has emerged as an important research topic in the last few years (Sun et al., 2018, 2019; Lan and Jiang, 2020; He et al., 2021), as the logically organized entities and relations in KBs can explicitly facilitate the QA process. Two mainstream methods including the semantic parsing based (SP-based) models (Berant et al., 2013; Bao et al., 2016; Liang et al., 2017; Lan and Jiang, 2020) and the information retrieval based (IR-based) models (Sun et al., 2018, 2019; Saxena et al., 2020; He et al., 2021) are commonly studied ∗ 1 Corresponding author. https://github.com/RUCKBReasoning/NumKBQA to solve KBQA task. The SP-based models heavily rely on the intermediate logic query parsed from the natural language question, which turns out to be the bottleneck of performance improvement (Lan et al., 2021). On the contrary, the IR-based models directly represent and rank the entities in a questionaware subgraph based on their relevance to the question. Such an end"
2021.findings-emnlp.159,D16-1147,0,0.0237214,"into two groups: SP-based methods and IR-based methods. A detailed survey of the task can be referred to (Lan et al., 2021; Zhang et al., 2021). SP-based methods (Berant et al., 2013; Berant and Liang, 2014; Yih et al., 2015; Bao et al., 2016; Liang et al., 2017; Lan and Jiang, 2020) learn a semantic parser to convert natural language questions into logic queries, which are able to deal with ordinal constrained questions. However, they heavily rely on intermediate logic queries, which becomes the bottleneck of performance improvement. IR-based methods (Bordes et al., 2015; Dong et al., 2015; Miller et al., 2016; Sun et al., 2018, 2019; Saxena et al., 2020; He et al., 2021) directly retrieve answer candidates from the KBs and represent them to encode the semantic relationships with the questions. These methods are more faulttolerant, but are unable to deal with ordinal constrained questions. This paper aims to enhance the IR-based models for numerical reasoning. Numerical Reasoning. Numerical Reasoning has been studied for various tasks such as word embedding (Naik et al., 2019; Wallace et al., 2019), arithmetic word problems (AWP) (Wang et al., 2018; Zhang et al., 2020), and MRC (Yu et al., 2018; Ra"
2021.findings-emnlp.159,P19-1329,0,0.0246454,"ueries, which becomes the bottleneck of performance improvement. IR-based methods (Bordes et al., 2015; Dong et al., 2015; Miller et al., 2016; Sun et al., 2018, 2019; Saxena et al., 2020; He et al., 2021) directly retrieve answer candidates from the KBs and represent them to encode the semantic relationships with the questions. These methods are more faulttolerant, but are unable to deal with ordinal constrained questions. This paper aims to enhance the IR-based models for numerical reasoning. Numerical Reasoning. Numerical Reasoning has been studied for various tasks such as word embedding (Naik et al., 2019; Wallace et al., 2019), arithmetic word problems (AWP) (Wang et al., 2018; Zhang et al., 2020), and MRC (Yu et al., 2018; Ran et al., 2019; Chen et al., 2020). Word embedding and AWP are a little far from our task. Similar to KBQA, MRC also aims to answer questions, but infers the answers from passages instead of KBs. To enable numerical reasoning, NumNet (Ran et al., 2019) adopts a numerically-aware GNN to encode numbers and QDGAT (Chen et al., 2020) further extends the number graph with additional words. 3 We use “1 ≺ 2 ≺ 3” to denote that the magnitude beHowever, they are all end-to-end mo"
2021.findings-emnlp.159,D19-1251,0,0.227688,"s, making the entity representations fall short in the ability to support such numerical reasoning. In view of the issue, this paper targets at empowering the IR-based KBQA models with the ability of numerical reasoning to address the ordinal constrained questions. Ordinal constraint is summarized as one of the most important constraints via web query analysis (Bao et al., 2016) and ordinal is also defined as the second fundamental measurement to capture data in the forms of surveys2 . Some efforts have been made on numerical reasoning for machine reading comprehension (MRC) (Yu et al., 2018; Ran et al., 2019; Chen et al., 2020). For example, given a question and a passage from which the answer can be inferred, NumNet (Ran et al., 2019) is an end-to-end model to learn the number embeddings and the nonnumerical word embeddings together, which are encoded by graph neural network (GNN) (Kipf and Welling, 2017) and BERT (Devlin et al., 2019) respectively. QDGAT (Chen et al., 2020) further 2 https://www.questionpro.com/blog/nominal-ordinalinterval-ratio/ 1852 Findings of the Association for Computational Linguistics: EMNLP 2021, pages 1852–1861 November 7–11, 2021. ©2021 Association for Computational L"
2021.findings-emnlp.159,2020.acl-main.412,0,0.264145,"h as freebase (Bollacker et al., 2008) and DBPedia (Lehmann et al., 2015) for the given questions expressed in natural language. KBQA has emerged as an important research topic in the last few years (Sun et al., 2018, 2019; Lan and Jiang, 2020; He et al., 2021), as the logically organized entities and relations in KBs can explicitly facilitate the QA process. Two mainstream methods including the semantic parsing based (SP-based) models (Berant et al., 2013; Bao et al., 2016; Liang et al., 2017; Lan and Jiang, 2020) and the information retrieval based (IR-based) models (Sun et al., 2018, 2019; Saxena et al., 2020; He et al., 2021) are commonly studied ∗ 1 Corresponding author. https://github.com/RUCKBReasoning/NumKBQA to solve KBQA task. The SP-based models heavily rely on the intermediate logic query parsed from the natural language question, which turns out to be the bottleneck of performance improvement (Lan et al., 2021). On the contrary, the IR-based models directly represent and rank the entities in a questionaware subgraph based on their relevance to the question. Such an end-to-end paradigm is easier to train and more fault-tolerant. However, most of the IR-based models focus on the single- or"
2021.findings-emnlp.159,D18-1455,0,0.271784,"ugins for any IR-based KBQA model to enhance its numerical reasoning ability. Extensive experiments on two KBQA benchmarks verify the effectiveness of our method to enhance the numerical reasoning ability for IR-based KBQA models. Our code and datasets are available online1 . 1 Introduction Knowledge Base Question Answering (KBQA) aims at finding answers from the existing knowledge bases (KBs) such as freebase (Bollacker et al., 2008) and DBPedia (Lehmann et al., 2015) for the given questions expressed in natural language. KBQA has emerged as an important research topic in the last few years (Sun et al., 2018, 2019; Lan and Jiang, 2020; He et al., 2021), as the logically organized entities and relations in KBs can explicitly facilitate the QA process. Two mainstream methods including the semantic parsing based (SP-based) models (Berant et al., 2013; Bao et al., 2016; Liang et al., 2017; Lan and Jiang, 2020) and the information retrieval based (IR-based) models (Sun et al., 2018, 2019; Saxena et al., 2020; He et al., 2021) are commonly studied ∗ 1 Corresponding author. https://github.com/RUCKBReasoning/NumKBQA to solve KBQA task. The SP-based models heavily rely on the intermediate logic query pars"
2021.findings-emnlp.159,D19-1242,0,0.0126376,"original datasets and the retrieved subgraphs. Evaluation Metrics. We follow GRAFT-Net to rank candidate entities4 for each question by their predictive probabilities and then evaluate Hits@1 to reflect the accuracy of the top-1 prediction. Baselines. We compare with three IR-based KBQA models: GRAFT-Net (Sun et al., 2018), EmbedKGQA (Saxena et al., 2020) and NSM (He et al., 2021). Compared with the Vanilla GNN, GRAFT-Net and NSM incorporate questions into graph convolution. EmbedKGQA directly optimizes the triplet of (topic entity, question, answer) based on their direct embeddings. PullNet (Sun et al., 2019)—the advanced GRAFT-Net—is not evaluated due to the unreleased code. Implementation Details. We construct a train/valid/test set of 10000/3000/4000 number graphs for NumGNN pretraining and a train/valid/test set of 500/60/80 question-aware number graphs for NumTransformer pretraining. Dataset of this scale is capable of capturing the ordinal relationships since the initial question word embeddings and the number embeddings have already been pretrained. The scale of a number graph in both NumGNN and NumTransformer is controlled within 2 to 150 nodes to balance the efficiency and the effectivene"
2021.findings-emnlp.159,P15-1128,0,0.047072,"Missing"
2021.findings-emnlp.159,P16-2033,0,0.0548231,"021. ©2021 Association for Computational Linguistics wires the numbers and the words in a same graph and encode them together by GNN. However, most of them implicitly infer number embeddings based on the QA pairs without the explicit annotation of the magnitude and ordinal relationships of numbers. Such weak supervision signals bring difficulties to infer accurate number embeddings, which becomes more prominent when the ordinal supervision signals are rarely available in existing KBQA datasets. In fact, the three well-known KBQA benchmarks, MetaQA (Zhang et al., 2018), WebQuestionSP (WebQSP) (Yih et al., 2016) and ComplexWebQeustions (CWQ) (Talmor and Berant, 2018) only contain 0, 101 and 1821 ordinal constrained questions respectively. To tackle the above challenge, we propose a pretraining method with additional self-supervision signals to capture two critical ingredients for ordinal constrained KBQA: • Relative Magnitude: The relative magnitude between numbers, such as “1 ≺ 2 ≺ 3”, is to be preserved by number embeddings3 . • Ordinal Relationship: Based on the above relative magnitude, the ordinal relationship between each number and the ordinal determiner ( such as “largest” in the question ) i"
2021.findings-emnlp.159,2020.acl-main.362,0,0.0183885,"al., 2015; Dong et al., 2015; Miller et al., 2016; Sun et al., 2018, 2019; Saxena et al., 2020; He et al., 2021) directly retrieve answer candidates from the KBs and represent them to encode the semantic relationships with the questions. These methods are more faulttolerant, but are unable to deal with ordinal constrained questions. This paper aims to enhance the IR-based models for numerical reasoning. Numerical Reasoning. Numerical Reasoning has been studied for various tasks such as word embedding (Naik et al., 2019; Wallace et al., 2019), arithmetic word problems (AWP) (Wang et al., 2018; Zhang et al., 2020), and MRC (Yu et al., 2018; Ran et al., 2019; Chen et al., 2020). Word embedding and AWP are a little far from our task. Similar to KBQA, MRC also aims to answer questions, but infers the answers from passages instead of KBs. To enable numerical reasoning, NumNet (Ran et al., 2019) adopts a numerically-aware GNN to encode numbers and QDGAT (Chen et al., 2020) further extends the number graph with additional words. 3 We use “1 ≺ 2 ≺ 3” to denote that the magnitude beHowever, they are all end-to-end models weakly tween 1 and 2 is closer than that between 1 and 3 instead of supervised by the fina"
2021.findings-emnlp.159,N18-1059,0,0.0914828,"ics wires the numbers and the words in a same graph and encode them together by GNN. However, most of them implicitly infer number embeddings based on the QA pairs without the explicit annotation of the magnitude and ordinal relationships of numbers. Such weak supervision signals bring difficulties to infer accurate number embeddings, which becomes more prominent when the ordinal supervision signals are rarely available in existing KBQA datasets. In fact, the three well-known KBQA benchmarks, MetaQA (Zhang et al., 2018), WebQuestionSP (WebQSP) (Yih et al., 2016) and ComplexWebQeustions (CWQ) (Talmor and Berant, 2018) only contain 0, 101 and 1821 ordinal constrained questions respectively. To tackle the above challenge, we propose a pretraining method with additional self-supervision signals to capture two critical ingredients for ordinal constrained KBQA: • Relative Magnitude: The relative magnitude between numbers, such as “1 ≺ 2 ≺ 3”, is to be preserved by number embeddings3 . • Ordinal Relationship: Based on the above relative magnitude, the ordinal relationship between each number and the ordinal determiner ( such as “largest” in the question ) is to be captured, e.g., 3 in “1 ≺ 2 ≺ 3” is identified a"
2021.findings-emnlp.159,D19-1534,0,0.0216445,"es the bottleneck of performance improvement. IR-based methods (Bordes et al., 2015; Dong et al., 2015; Miller et al., 2016; Sun et al., 2018, 2019; Saxena et al., 2020; He et al., 2021) directly retrieve answer candidates from the KBs and represent them to encode the semantic relationships with the questions. These methods are more faulttolerant, but are unable to deal with ordinal constrained questions. This paper aims to enhance the IR-based models for numerical reasoning. Numerical Reasoning. Numerical Reasoning has been studied for various tasks such as word embedding (Naik et al., 2019; Wallace et al., 2019), arithmetic word problems (AWP) (Wang et al., 2018; Zhang et al., 2020), and MRC (Yu et al., 2018; Ran et al., 2019; Chen et al., 2020). Word embedding and AWP are a little far from our task. Similar to KBQA, MRC also aims to answer questions, but infers the answers from passages instead of KBs. To enable numerical reasoning, NumNet (Ran et al., 2019) adopts a numerically-aware GNN to encode numbers and QDGAT (Chen et al., 2020) further extends the number graph with additional words. 3 We use “1 ≺ 2 ≺ 3” to denote that the magnitude beHowever, they are all end-to-end models weakly tween 1 and"
2021.findings-emnlp.159,D18-1132,0,0.0249309,"methods (Bordes et al., 2015; Dong et al., 2015; Miller et al., 2016; Sun et al., 2018, 2019; Saxena et al., 2020; He et al., 2021) directly retrieve answer candidates from the KBs and represent them to encode the semantic relationships with the questions. These methods are more faulttolerant, but are unable to deal with ordinal constrained questions. This paper aims to enhance the IR-based models for numerical reasoning. Numerical Reasoning. Numerical Reasoning has been studied for various tasks such as word embedding (Naik et al., 2019; Wallace et al., 2019), arithmetic word problems (AWP) (Wang et al., 2018; Zhang et al., 2020), and MRC (Yu et al., 2018; Ran et al., 2019; Chen et al., 2020). Word embedding and AWP are a little far from our task. Similar to KBQA, MRC also aims to answer questions, but infers the answers from passages instead of KBs. To enable numerical reasoning, NumNet (Ran et al., 2019) adopts a numerically-aware GNN to encode numbers and QDGAT (Chen et al., 2020) further extends the number graph with additional words. 3 We use “1 ≺ 2 ≺ 3” to denote that the magnitude beHowever, they are all end-to-end models weakly tween 1 and 2 is closer than that between 1 and 3 instead of s"
2021.findings-emnlp.18,D18-1217,0,0.0580215,"Missing"
2021.findings-emnlp.18,N19-1423,0,0.216612,"rew, 2006; Kong from a pretraining language model. Secondly, et al., 2016; Ye and Ling, 2018; Liu et al., 2019a) we define a score for every segmentation candibased on Semi-Markov CRF (Sarawagi and Cohen, date and apply dynamic programming (DP) to 2005) adopts dynamic programming (DP) (Bellextract the candidate with the maximum score. man, 1966) to search for the optimal segmentation We have conducted extensive experiments on of a sentence. Unlike their counterparts (Clark 3 tasks, (e.g., syntactic chunking), across 7 datasets. LUA has established new state-ofet al., 2018; Akbik et al., 2018; Devlin et al., 2019; the-art performances on 6 of them. We have Li et al., 2021), these methods all train the senachieved even better results through incorpotence encoders from scratch, without exploiting the rating label correlations.1 knowledge from unlabeled corpora. Hence, none of them is even competitive with current best se1 Introduction quence labeling model. Plenty of tasks in natural language understanding In this paper, we propose lexical unit analysis (NLU), such as syntactic chunking, are essentially (LUA), a unified and effective span-based model a sequence segmentation problem, which partitions tha"
2021.findings-emnlp.18,C18-1139,0,0.153813,"an-based models (Andrew, 2006; Kong from a pretraining language model. Secondly, et al., 2016; Ye and Ling, 2018; Liu et al., 2019a) we define a score for every segmentation candibased on Semi-Markov CRF (Sarawagi and Cohen, date and apply dynamic programming (DP) to 2005) adopts dynamic programming (DP) (Bellextract the candidate with the maximum score. man, 1966) to search for the optimal segmentation We have conducted extensive experiments on of a sentence. Unlike their counterparts (Clark 3 tasks, (e.g., syntactic chunking), across 7 datasets. LUA has established new state-ofet al., 2018; Akbik et al., 2018; Devlin et al., 2019; the-art performances on 6 of them. We have Li et al., 2021), these methods all train the senachieved even better results through incorpotence encoders from scratch, without exploiting the rating label correlations.1 knowledge from unlabeled corpora. Hence, none of them is even competitive with current best se1 Introduction quence labeling model. Plenty of tasks in natural language understanding In this paper, we propose lexical unit analysis (NLU), such as syntactic chunking, are essentially (LUA), a unified and effective span-based model a sequence segmentation problem,"
2021.findings-emnlp.18,W06-1655,0,0.108807,"r, each of these methods suffers from normalized at span level, and therefore suffered its own defects, such as invalid predictions. In from the label bias problem (Lafferty et al., 2001). this work, we introduce a unified span-based Moreover, some of them (Yu et al., 2020; Li et al., model, lexical unit analysis (LUA), that ad2021) rely on heuristic rules to correct invalid predresses all these matters. Segmenting a lexical unit sequence involves two steps. Firstly, we dictions (e.g., span conflicts between two entities). embed every span by using the representations Early span-based models (Andrew, 2006; Kong from a pretraining language model. Secondly, et al., 2016; Ye and Ling, 2018; Liu et al., 2019a) we define a score for every segmentation candibased on Semi-Markov CRF (Sarawagi and Cohen, date and apply dynamic programming (DP) to 2005) adopts dynamic programming (DP) (Bellextract the candidate with the maximum score. man, 1966) to search for the optimal segmentation We have conducted extensive experiments on of a sentence. Unlike their counterparts (Clark 3 tasks, (e.g., syntactic chunking), across 7 datasets. LUA has established new state-ofet al., 2018; Akbik et al., 2018; Devlin et"
2021.findings-emnlp.18,P16-1039,0,0.0283561,"Liu et al., 2019c; Luo et al., 2020). Every token we embed every span of the sentence, inspired by in a sentence, according to its position in the cor- the finding that pretraining language models are responding segment, is labeled with a tag (e.g., very robust to rare tokens and the low-resource setB-PER). A representative work is Bidirectional ting (Liu et al., 2019b). Then, we assign a score to LSTM-CRF (Huang et al., 2015). every segmentation candidate and use DP to globRecently, there is a surge of interest in develop- ally search for the candidate with the maximum ing span-based models (Cai and Zhao, 2016; Zhai score. The score of a segmentation is computed et al., 2017; Li et al., 2020a; Yu et al., 2020; Li from the segment scores predicted by LUA. We et al., 2021). They regard spans rather than tokens minimize the hinge loss, instead of cross-entropy, as the basic units for labeling. For example, Li et al. to train our models. (2020a) model named entity recognition (NER) We have performed extensive experiments on 1 syntactic chunking, Chinese part-of-speech (POS) The source code for our work is publicly available at https://github.com/LeePleased/LUA. tagging, and NER across 7 datasets. Our m"
2021.findings-emnlp.18,N16-1030,0,0.0408992,"Missing"
2021.findings-emnlp.18,2020.acl-main.519,0,0.142602,"inspired by in a sentence, according to its position in the cor- the finding that pretraining language models are responding segment, is labeled with a tag (e.g., very robust to rare tokens and the low-resource setB-PER). A representative work is Bidirectional ting (Liu et al., 2019b). Then, we assign a score to LSTM-CRF (Huang et al., 2015). every segmentation candidate and use DP to globRecently, there is a surge of interest in develop- ally search for the candidate with the maximum ing span-based models (Cai and Zhao, 2016; Zhai score. The score of a segmentation is computed et al., 2017; Li et al., 2020a; Yu et al., 2020; Li from the segment scores predicted by LUA. We et al., 2021). They regard spans rather than tokens minimize the hinge loss, instead of cross-entropy, as the basic units for labeling. For example, Li et al. to train our models. (2020a) model named entity recognition (NER) We have performed extensive experiments on 1 syntactic chunking, Chinese part-of-speech (POS) The source code for our work is publicly available at https://github.com/LeePleased/LUA. tagging, and NER across 7 datasets. Our model 181 Findings of the Association for Computational Linguistics: EMNLP 2021, pag"
2021.findings-emnlp.18,2020.acl-main.574,1,0.894314,"inspired by in a sentence, according to its position in the cor- the finding that pretraining language models are responding segment, is labeled with a tag (e.g., very robust to rare tokens and the low-resource setB-PER). A representative work is Bidirectional ting (Liu et al., 2019b). Then, we assign a score to LSTM-CRF (Huang et al., 2015). every segmentation candidate and use DP to globRecently, there is a surge of interest in develop- ally search for the candidate with the maximum ing span-based models (Cai and Zhao, 2016; Zhai score. The score of a segmentation is computed et al., 2017; Li et al., 2020a; Yu et al., 2020; Li from the segment scores predicted by LUA. We et al., 2021). They regard spans rather than tokens minimize the hinge loss, instead of cross-entropy, as the basic units for labeling. For example, Li et al. to train our models. (2020a) model named entity recognition (NER) We have performed extensive experiments on 1 syntactic chunking, Chinese part-of-speech (POS) The source code for our work is publicly available at https://github.com/LeePleased/LUA. tagging, and NER across 7 datasets. Our model 181 Findings of the Association for Computational Linguistics: EMNLP 2021, pag"
2021.findings-emnlp.18,P19-1524,0,0.114558,"fects, such as invalid predictions. In from the label bias problem (Lafferty et al., 2001). this work, we introduce a unified span-based Moreover, some of them (Yu et al., 2020; Li et al., model, lexical unit analysis (LUA), that ad2021) rely on heuristic rules to correct invalid predresses all these matters. Segmenting a lexical unit sequence involves two steps. Firstly, we dictions (e.g., span conflicts between two entities). embed every span by using the representations Early span-based models (Andrew, 2006; Kong from a pretraining language model. Secondly, et al., 2016; Ye and Ling, 2018; Liu et al., 2019a) we define a score for every segmentation candibased on Semi-Markov CRF (Sarawagi and Cohen, date and apply dynamic programming (DP) to 2005) adopts dynamic programming (DP) (Bellextract the candidate with the maximum score. man, 1966) to search for the optimal segmentation We have conducted extensive experiments on of a sentence. Unlike their counterparts (Clark 3 tasks, (e.g., syntactic chunking), across 7 datasets. LUA has established new state-ofet al., 2018; Akbik et al., 2018; Devlin et al., 2019; the-art performances on 6 of them. We have Li et al., 2021), these methods all train the"
2021.findings-emnlp.18,P19-1233,0,0.519662,"fects, such as invalid predictions. In from the label bias problem (Lafferty et al., 2001). this work, we introduce a unified span-based Moreover, some of them (Yu et al., 2020; Li et al., model, lexical unit analysis (LUA), that ad2021) rely on heuristic rules to correct invalid predresses all these matters. Segmenting a lexical unit sequence involves two steps. Firstly, we dictions (e.g., span conflicts between two entities). embed every span by using the representations Early span-based models (Andrew, 2006; Kong from a pretraining language model. Secondly, et al., 2016; Ye and Ling, 2018; Liu et al., 2019a) we define a score for every segmentation candibased on Semi-Markov CRF (Sarawagi and Cohen, date and apply dynamic programming (DP) to 2005) adopts dynamic programming (DP) (Bellextract the candidate with the maximum score. man, 1966) to search for the optimal segmentation We have conducted extensive experiments on of a sentence. Unlike their counterparts (Clark 3 tasks, (e.g., syntactic chunking), across 7 datasets. LUA has established new state-ofet al., 2018; Akbik et al., 2018; Devlin et al., 2019; the-art performances on 6 of them. We have Li et al., 2021), these methods all train the"
2021.findings-emnlp.18,W13-3516,0,0.24678,"Missing"
2021.findings-emnlp.18,W00-0726,0,0.720062,"Missing"
2021.findings-emnlp.18,W03-0419,0,0.513531,"Missing"
2021.findings-emnlp.18,I17-1018,0,0.193475,"Missing"
2021.findings-emnlp.18,P17-1076,0,0.13538,"Missing"
2021.findings-emnlp.18,P16-1101,0,0.0486329,"language understanding In this paper, we propose lexical unit analysis (NLU), such as syntactic chunking, are essentially (LUA), a unified and effective span-based model a sequence segmentation problem, which partitions that circumvents all above problems. Our segmena sequence of lexical units into multiple labeled tation of a natural language sentence contains two segments. A classical approach to sequence seg- steps. Firstly, we utilize BERT (Devlin et al., 2019), mentation is to cast it into a sequence labeling task a powerful pretraining language model, to get conusing IOB tagging scheme (Ma and Hovy, 2016; textualized token representations, and with them Liu et al., 2019c; Luo et al., 2020). Every token we embed every span of the sentence, inspired by in a sentence, according to its position in the cor- the finding that pretraining language models are responding segment, is labeled with a tag (e.g., very robust to rare tokens and the low-resource setB-PER). A representative work is Bidirectional ting (Liu et al., 2019b). Then, we assign a score to LSTM-CRF (Huang et al., 2015). every segmentation candidate and use DP to globRecently, there is a surge of interest in develop- ally search for the"
2021.findings-emnlp.18,2020.coling-main.187,0,0.0704473,"Missing"
2021.findings-emnlp.18,P18-2038,0,0.0153554,"suffered its own defects, such as invalid predictions. In from the label bias problem (Lafferty et al., 2001). this work, we introduce a unified span-based Moreover, some of them (Yu et al., 2020; Li et al., model, lexical unit analysis (LUA), that ad2021) rely on heuristic rules to correct invalid predresses all these matters. Segmenting a lexical unit sequence involves two steps. Firstly, we dictions (e.g., span conflicts between two entities). embed every span by using the representations Early span-based models (Andrew, 2006; Kong from a pretraining language model. Secondly, et al., 2016; Ye and Ling, 2018; Liu et al., 2019a) we define a score for every segmentation candibased on Semi-Markov CRF (Sarawagi and Cohen, date and apply dynamic programming (DP) to 2005) adopts dynamic programming (DP) (Bellextract the candidate with the maximum score. man, 1966) to search for the optimal segmentation We have conducted extensive experiments on of a sentence. Unlike their counterparts (Clark 3 tasks, (e.g., syntactic chunking), across 7 datasets. LUA has established new state-ofet al., 2018; Akbik et al., 2018; Devlin et al., 2019; the-art performances on 6 of them. We have Li et al., 2021), these meth"
2021.findings-emnlp.18,2020.acl-main.577,0,0.348222,"{newmanli,redmondliu,shumingshi}@tencent.com Abstract as machine reading comprehension (MRC) (Seo et al., 2017), where entities are extracted as retrievThe span-based model enjoys great popularing answer spans. While span-based models have ity in recent works of sequence segmentation. achieved promising performances, they are locally However, each of these methods suffers from normalized at span level, and therefore suffered its own defects, such as invalid predictions. In from the label bias problem (Lafferty et al., 2001). this work, we introduce a unified span-based Moreover, some of them (Yu et al., 2020; Li et al., model, lexical unit analysis (LUA), that ad2021) rely on heuristic rules to correct invalid predresses all these matters. Segmenting a lexical unit sequence involves two steps. Firstly, we dictions (e.g., span conflicts between two entities). embed every span by using the representations Early span-based models (Andrew, 2006; Kong from a pretraining language model. Secondly, et al., 2016; Ye and Ling, 2018; Liu et al., 2019a) we define a score for every segmentation candibased on Semi-Markov CRF (Sarawagi and Cohen, date and apply dynamic programming (DP) to 2005) adopts dynamic p"
2021.findings-emnlp.18,2020.acl-main.302,0,0.0318771,"Missing"
2021.findings-emnlp.18,P18-1144,0,0.118475,"Missing"
2021.naacl-main.116,N19-1423,0,0.0492102,"Missing"
2021.naacl-main.116,N19-1116,0,0.0170054,"valuation time, we set ik = jk−1 + 1. Then, we separately predict the span and the label of a segment. We define a set Sk containing all valid span candidates for prediction: Sk = {(ik , ik ), (ik , ik + 1), · · · , (ik , n)}. (6) Inside algorithm (Lari and Young, 1990) is an- The probability of a span (i, j) ∈ Sk is other method to extracting phrase representation. p  Its advantage is to incorporate the potential hierQsk,i,j ∝ exp (hdk )&gt; Ws hi,j . (7) archical structure of natural language without usThe probability of a label l ∈ L for a span (i, j) is ing treebank annotation. For example, Drozdov et al. (2019) utilize inside algorithm to recursively  p Qlk,i,j,l ∝ exp El (l)&gt; Wl (hi,j ⊕ hdk ) . (8) compute the content representations. Despite the 3 attractive property, its time complexity O(n ) is too inefficient to practice use. The matrices Ws and Wl are learnable. 1478 Algorithm 1: Inference Procedure p Input: The representations for all the phrases, hi,j , 1 ≤ i ≤ j ≤ n. ˆ = [ˆ Output: The sequence of predicted segments, y y1 , yˆ2 , · · · , yˆm ˆ ]. 1 Set a list y as [] and set a counter as k = 1. 2 Denote the remaining input token list as x. 3 Initialize the representation for previous segme"
2021.naacl-main.116,N18-1091,0,0.0492487,"Missing"
2021.naacl-main.116,2020.acl-main.574,1,0.899017,"egment determination. Extensive experiments have been conducted on syntactic chunking and Chinese POS tagging across 3 datasets. We have achieved new state-of-the-art performances on all of them. Case study and the results on segmenting long-length sentences both verify the effectiveness of our framework in modeling long-term dependencies. There are two mainstream approaches to sequence segmentation. One of them treat sequence segmentation as a sequence labeling problem by using IOB tagging scheme (Huang et al., 2015; Xin et al., 2018; Clark et al., 2018; Akbik et al., 2018; Liu et al., 2019; Li et al., 2020a). Each token in a sentence is labeled as B-tag if it’s the beginning of a segment, I-tag if it is inside but not the first one within the segment, or O otherwise. This method is extensively studied by prior works and provides tons of state-of-the-art results. Xin et al. (2018) introduce a funnel-shaped convolutional architecture that learns a better internal structure for the tokens. Acknowledgments Akbik et al. (2018) propose an efficient characterlevel framework that uses pretrained character em- This work was done when the first author did internship at Ant Group. We thank anonymous revie"
2021.naacl-main.116,2021.findings-emnlp.18,1,0.719391,"Missing"
2021.naacl-main.116,2020.acl-main.10,1,0.765607,"egment determination. Extensive experiments have been conducted on syntactic chunking and Chinese POS tagging across 3 datasets. We have achieved new state-of-the-art performances on all of them. Case study and the results on segmenting long-length sentences both verify the effectiveness of our framework in modeling long-term dependencies. There are two mainstream approaches to sequence segmentation. One of them treat sequence segmentation as a sequence labeling problem by using IOB tagging scheme (Huang et al., 2015; Xin et al., 2018; Clark et al., 2018; Akbik et al., 2018; Liu et al., 2019; Li et al., 2020a). Each token in a sentence is labeled as B-tag if it’s the beginning of a segment, I-tag if it is inside but not the first one within the segment, or O otherwise. This method is extensively studied by prior works and provides tons of state-of-the-art results. Xin et al. (2018) introduce a funnel-shaped convolutional architecture that learns a better internal structure for the tokens. Acknowledgments Akbik et al. (2018) propose an efficient characterlevel framework that uses pretrained character em- This work was done when the first author did internship at Ant Group. We thank anonymous revie"
2021.naacl-main.116,P19-1233,0,0.160089,"el, where a single segment is represented by phrases. These segments are nonoverlapping and multiple token-level labels (e.g., transition actions). fully cover the input sentence. In previous works, there are two dominant ap- In spite of the adequacy, the labels used to model proaches to sequence segmentation. The most com- the relation among output segments are far more mon is to regard it as a sequence labeling prob- than the segments themselves. As demonstrated in Figure 1, modeling the transition between the lem with resorting to IOB tagging scheme (Huang et al., 2015; Akbik et al., 2018; Liu et al., 2019). two segments, “will be&quot; and “about $ 115 million&quot;, This method is simple yet very effective, provid- consumes 6 IOB tags or 8 transition actions. This ing tons of state-of-the-art performances. For ex- ill-posed design certainly limits the full potential of segmentation models to capture the long-term ample, Huang et al. (2015) present Bidirectional LSTM-CRF for named entity recognition (NER) dependencies among segments. and POS tagging, which adopts BiLSTM (HochrePreviously, Kong et al. (2015) attempted to de1476 Proceedings of the 2021 Conference of the North American Chapter of the Associ"
2021.naacl-main.116,P16-1101,0,0.0877272,"Missing"
2021.naacl-main.116,D14-1162,0,0.0897975,"Glyce + Lattice LSTM (Meng et al., 2019) BERT (Devlin et al., 2019) Glyce + BERT (Meng et al., 2019) Our Model This Work Our Model w/ BERT PTB9 92.16 91.89 92.34 92.13 92.38 92.29 93.15 92.56 93.38 UD1 90.01 89.41 89.75 90.09 90.87 94.79 96.14 91.65 96.43 Table 3: The results on the two datasets of Chinese POS tagging. tion is set as 1 × 10−6 and dropout ratio is set as 0.4 for reducing overfit. The above setting is obtained by grid search. We adopt Adam (Kingma and Ba, 2014) as the optimization algorithm and adopt the suggested hyper-parameters. For CoNLL2000 dataset, the cased, 300d Glove (Pennington et al., 2014) is used to initialize token embedding. CharCNN is not used in Chinese tasks. The batch size is set as 16. All our models in experiments are running on NVIDIA Tesla P100. At test time, following previous literature, we convert the prediction of our model into IOB format and use the standard conlleval script1 to get the F1 score. We select the model that works the best on development set, and then evaluate it on test set. In all the experiments, the improvements of our models over the baselines are statistically significant with p &lt; 0.05 under t-test. Segmental RNN It’s a segment-level model th"
2021.naacl-main.116,W13-3516,0,0.0316831,"conclusions. Firstly, Segmental RNN, a segment-level model, is very slow for both training and inference due to the high time complexity. For instance, its training and testing are respectively 6.26 and 9.05 times slower than ours. Secondly, our framework is very efficient. For example, training our model for one epoch is 1.33 times faster than training Bi-LSTM + CRF, a token-level model. 3.8 Results on NER While our model is tailored for sequence segmentation tasks, we have also tested its performances on two widely used NER datasets, CoNLL-2003 (Sang and De Meulder, 2003) and OntoNotes 5.0 (Pradhan et al., 2013). Note that, in NER, the label correlations among adjacent segments are very weak. This seems bad for our model. Table 8 diagrams the comparison of our model and strong NER baselines. The results of GRN and HCR are copied from Chen et al. (2019); Luo et al. (2020). For BiLSTM-CNN-CRF, its scores on • The adjacent segments labeled with (NP, VP) CoNLL-2003 and OntoNotes 5.0 are respectively from Chen et al. (2019) and our re-implementation. frequently appear in training data; From the table, we can see that our model is competitive with prior methods. For example, our • the token “present&quot; actin"
2021.naacl-main.116,D15-1211,0,0.0246003,"LSTM encoder using both labeled and unlabeled corpora. Despite the effectiveness, these models are at token level, relying on multiple token-level labels to represent a single segment. References This limits their full potential to capture the long- Alan Akbik, Duncan Blythe, and Roland Vollgraf. 2018. Contextual string embeddings for sequence term dependencies among segments. labeling. In Proceedings of the 27th International The other uses transition-based systems as the Conference on Computational Linguistics, pages backbone to incrementally segment and label an in1638–1649. put sequence (Qian et al., 2015; Zhang et al., 2016, 2018). For example, Qian et al. (2015) design spe- Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2014. Neural machine translation by jointly cial transition actions to jointly segment, tag, and learning to align and translate. arXiv preprint normalize a sentence. These models have many arXiv:1409.0473. advantages, such as theoretically lower time complexity and capturing non-local features. However, Hui Chen, Zijia Lin, Guiguang Ding, Jianguang Lou, Yusen Zhang, and Borje Karlsson. 2019. Grn: Gated they are still token-level models, which predict tranrelation networ"
2021.naacl-main.116,W00-0726,0,0.768482,"nificantly outperformed previous all baselines and achieved new stateof-the-art results. Moreover, qualitative analysis and the study on segmenting long-length sentences verify its effectiveness in modeling long-term dependencies. Tangible capital will be about $ 115 million . (Tangible capital, NP), (will be, VP), Segments (about $ 115 million, NP), (., O) B-NP I-NP B-VP I-VP IOB Tags B-NP I-NP I-NP I-NP O SHIFT SHIFT REDUCE-NP Transition SHIFT SHIFT REDUCE-VP Actions SHIFT SHIFT SHIFT SHIFT REDUCE-NP OUT Sentence Table 1: The first two rows show an example extracted from CoNLL-2000 dataset (Sang and Buchholz, 2000). The last two rows are two types of token-level labels commonly used to represent the segments. iter and Schmidhuber, 1997) to read the input sentence and CRF (Lafferty et al., 2001) to decode the label sequence. An alternative method employs a transition-based system to incrementally segment and label an input token sequence (Zhang et al., 2016, 2018). For instance, Zhang et al. (2016) 1 Introduction present a transition-based model for Chinese word segmentation that exploits not only character emSequence segmentation, as an important task in bedding but also token embedding. This type of na"
2021.naacl-main.116,W03-0419,0,0.376676,"Missing"
2021.naacl-main.116,I17-1018,0,0.0170239,"erred nese Treebank 9.0 (CTB9) (Xue et al., 2005) and in terms of the previous predicted segments Universal Dependencies 1.4 (UD1) (Nivre et al., [ˆ y1 , yˆ2 , · · · , yˆk−1 ]. Algorithm 1 demonstrates how 2016). CTB9 contains the source text in various our proposed framework makes inference. Note genres, covering its previous versions (e.g., CTB6). that Algorithm 1 uses greedy search to get yˆ be- We use the Chinese section of UD1. We follow cause it is both fast in speed and effective in ac- the same format and partition of the two datasets curacy in our experiments, although beam search as Shao et al. (2017). may be better in accuracy. We use the same neural network configuration for all 3 datasets. The dimensions of token embed3 Experiments ding and label embedding are respectively set as Extensive experiments have been conducted on syn- 300 and 50. The hidden unit sizes for the encoder tactic chunking and Chinese POS tagging across and decoder are 256 and 512, respectively. The 3 datasets. Firstly, our models have obtained new layers of two LSTMs are both 2. L2 regulariza1479 2.3 Training and Inference Approach Segmental RNN (Kong et al., 2015) Bi-LSTM + CRF (Huang et al., 2015) Char-IntNet-5 ("
2021.naacl-main.116,D18-1279,0,0.262428,"Lastly, case study and the results on segmenting longlength sentences confirm the effectiveness of the proposed framework in capturing the long-term dependencies among segments. 3.1 Settings Syntactic chunking segments a word sequence into multiple labeled groups of words. We use CoNLL2000 dataset (Sang and Buchholz, 2000), which During training, we use teacher forcing where defines 11 syntactic chunk types (NP, VP, PP, etc.). every segment yk is predicted using its previous ground-truth segments [y1 , y2 , · · · , yk−1 ]. A hy- Standard data includes a training set and a test set. Following Xin et al. (2018), we randomly sample brid loss is induced as 1000 sentences from the training set as the developX s l ment set. Chinese POS tagging converts a Chinese J =− (log Qk,ik ,jk + log Qk,ik ,jk ,lk ). (10) character sequence into a token sequence and assoyk ∈y ciates every word with a POS tag. We use Penn ChiAt test time, every segment yˆk is inferred nese Treebank 9.0 (CTB9) (Xue et al., 2005) and in terms of the previous predicted segments Universal Dependencies 1.4 (UD1) (Nivre et al., [ˆ y1 , yˆ2 , · · · , yˆk−1 ]. Algorithm 1 demonstrates how 2016). CTB9 contains the source text in various our p"
2021.naacl-main.116,2020.acl-main.577,0,0.0254394,"Missing"
2021.naacl-main.116,P16-1040,0,0.0887203,"., O) B-NP I-NP B-VP I-VP IOB Tags B-NP I-NP I-NP I-NP O SHIFT SHIFT REDUCE-NP Transition SHIFT SHIFT REDUCE-VP Actions SHIFT SHIFT SHIFT SHIFT REDUCE-NP OUT Sentence Table 1: The first two rows show an example extracted from CoNLL-2000 dataset (Sang and Buchholz, 2000). The last two rows are two types of token-level labels commonly used to represent the segments. iter and Schmidhuber, 1997) to read the input sentence and CRF (Lafferty et al., 2001) to decode the label sequence. An alternative method employs a transition-based system to incrementally segment and label an input token sequence (Zhang et al., 2016, 2018). For instance, Zhang et al. (2016) 1 Introduction present a transition-based model for Chinese word segmentation that exploits not only character emSequence segmentation, as an important task in bedding but also token embedding. This type of natural language understanding (NLU), partitions a method enjoys a number of attractive properties, sentence into multiple segments. The first two rows including theoretically lower time complexity and of Table 1 show a case from a syntactic chunking dataset. The input sentence is a sequence of to- capturing non-local features. The above two approa"
2021.naacl-main.116,P18-1144,0,0.0607731,"Missing"
2021.naacl-main.116,2020.acl-main.641,0,0.0383355,"Missing"
2021.naacl-main.116,P17-1076,0,0.0614068,"Missing"
2021.naacl-main.116,P16-1218,0,0.0309496,"d the unprocessed sequence xik ,n . Firstly, each token xk is represented as ek = Et (xk ) ⊕ CharCNN(xk ), (1) where Et is a token embedding matrix and ⊕ is the column-wise vector concatenation. Following previous works (Ma and Hovy, 2016; Liu et al., 2019), we use CharCNN to extract the characterlevel representations. → − Secondly, we utilize bidirectional LSTMs f e ← −e and f to compute the context-sensitive representation for each token xk : → −c → −e → −c   h k = f ( h k−1 , ek )  ← − ← − ← − (2) h ck = f e ( h ck+1 , ek ) .   −c ← −c  c → hk = h k ⊕ h k Inspired by previous works (Wang and Chang, 2016; Gaddy et al., 2018) in syntactic analysis, we integrate LSTM-minus features into phrase reprep sentations. Specifically, the representation hi,j for a phrase xi,j is computed as the concatenation of the difference of LSTM hidden states: p hi,j = hcj ⊕ (hcj − hci ) ⊕ hci . (3) Leftmost Segment Determination Figure 2 demonstrates how our method iteratively and incrementally segments the sentence in Table 1. LSTM is used as the backbone to model the iterative process. At every step, the input consists of the prior segment and the unprocessed sequence, and the output is the predicted segment. Fi"
C12-2071,N12-1047,0,0.0686776,"U optimizes the expected BLEU, a loss more approximate towards Corpus-BLEU compared with the generalized hinge loss, and it utilizes the projection distance metric instead of L2 as with MIRA. Further, ELBUU is a MERT-like batch mode which ultraconservatively updates the weight with all training examples, but MIRA is an online one which updates with each example (Watanabe et al., 2007) or parts of examples (Chiang et al., 2008). The batch mode has some advantages over online mode: more accurate sentence-wise BLEU towards Corpus-BLEU (Watanabe, 2012) and more promising experimental performance (Cherry and Foster, 2012). Additionally, our method is similar to (Liu et al., 2012). However, the main difference is that ours is a global training method instead of a local training method. 4 Experiments and Results 4.1 Experimental Setting We conduct our translation experiments on two language pairs: Chinese-to-English and Spanishto-English. For the Chinese-to-English task, the training data is FBIS corpus consisting of about 240k sentence pairs; the development set is NIST02 evaluation data; the test set NIST05 is used as the development test set for tuning hyperparameter λ in Eq. 6; and the test datasets are NIST"
C12-2071,D08-1024,0,0.588794,"mize Eq. 3. Motivated by (Och, 2003; Smith and Eisner, 2006; Zens et al., 2007), we use the expected loss to substitute the direct loss in Eq. 3 and we obtain the objective function as follows: d(W, Wk ) + n X λX n i=1 e∈ci Losser r or (ri ; e)Pα (e |f i ; W ), with Pα (e |f i ; W ) = P exp[αW · h( f i , e)] e′ ∈c i exp[αW · h( f i , e′ )] (4) , where α &gt; 0 is a real number, each h( f i , e) is a feature vector, and d is a distance metric defined on a pair of weights. Losser r or (ri ; e) in Eq. 4 is a sentence-wise direct loss, and in this paper we used a variant of sentence BLEU proposed by Chiang et al. (2008) which smoothes BLEU statistics with pseudo-document. 3.2 Distance Metric Based on Projection Euclidean distance ( L2 norm) is usually employed as in MIRA (Watanabe et al., 2007; Chiang et al., 2008). In this section we will specifically investigate another metric for ultraconservative update in SMT. In log-linear based translation models, since the decoding strategy is the maximal posterior probability, the translation results are the same for the weight W and its positive multiplication (see Eq. 2). Therefore, for a translation decoder, we wish that the distance of two weights satisfies the"
C12-2071,P11-2031,0,0.031614,"EU. Proceedings of COLING 2012: Posters, pages 723–732, COLING 2012, Mumbai, December 2012. 723 1 Introduction Minimum error rate training (Och, 2003), MERT, is an important component of statistical machine translation (SMT), and it has been the most popular method for tuning parameters for SMT systems. One of its major contributions is the use of an evaluation metric, such as BLEU (Papineni et al., 2002), as a direct loss function during its optimization procedure by interchanging decoding and optimization steps in each round. While MERT is successful in practice, it is known to be unstable (Clark et al., 2011). At the optimization step in each round, MERT tries to repeatedly optimize a loss function defined by the k-best candidate lists. Since new k-best lists are generated and merged with the previously generated lists at each round, the optimization objective function may change drastically between two adjacent rounds (Pauls et al., 2009), and the optimized weights of these two rounds may also be far from each other. Motivated by the above observation, this paper investigates a new tuning approach under the kbest lists framework, instead of the lattices or hypergraphs framework as Macherey et al."
C12-2071,W02-1001,0,0.0637373,"ng corpus in both directions (Koehn et al., 2003) to obtain the word alignment for each sentence pair. We train a 4-gram language model on the Xinhua portion of the English Gigaword corpus using the SRILM Toolkits (Stolcke, 2002) with modified Kneser-Ney smoothing (Chen and Goodman, 1998). In our experiments, the translation performances are measured by the case-insensitive BLEU4 metric (Papineni et al., 2002) and we use mteval-v13a.pl as the evaluation tool. The significance testing is performed by paired bootstrap re-sampling (Koehn, 2004). 3 At the end of tuning, we average the weights as (Collins, 2002). The norm of the averaged weight may nolonger be equal to 1, but it is irrelevant for testing, see discussion in Section 3.2. 728 Methods MERT ELBUU dev06(Dev) 28.85 28.67 test08 19.68 20.23 test09 21.36 21.72 test10 23.35 23.90+ test11 23.65 24.18+ Table 2: Comparison of two tuning methods, MERT and ELBUU, on Spanish-to-English translation tasks. + means the ELBUU method is significantly better than MERT with confidence p &lt; 0.05. Distance metrics L2 Projection NIST02(Dev) 29.95 30.06 NIST03 27.09 27.36 NIST04 29.65 29.89 NIST05 26.79 27.03 NIST06 25.98 26.30 NIST08 19.54 19.79 Table 3: Compa"
C12-2071,W04-3250,0,0.0518064,", test10, test11. We run GIZA++ (Och and Ney, 2000) on the training corpus in both directions (Koehn et al., 2003) to obtain the word alignment for each sentence pair. We train a 4-gram language model on the Xinhua portion of the English Gigaword corpus using the SRILM Toolkits (Stolcke, 2002) with modified Kneser-Ney smoothing (Chen and Goodman, 1998). In our experiments, the translation performances are measured by the case-insensitive BLEU4 metric (Papineni et al., 2002) and we use mteval-v13a.pl as the evaluation tool. The significance testing is performed by paired bootstrap re-sampling (Koehn, 2004). 3 At the end of tuning, we average the weights as (Collins, 2002). The norm of the averaged weight may nolonger be equal to 1, but it is irrelevant for testing, see discussion in Section 3.2. 728 Methods MERT ELBUU dev06(Dev) 28.85 28.67 test08 19.68 20.23 test09 21.36 21.72 test10 23.35 23.90+ test11 23.65 24.18+ Table 2: Comparison of two tuning methods, MERT and ELBUU, on Spanish-to-English translation tasks. + means the ELBUU method is significantly better than MERT with confidence p &lt; 0.05. Distance metrics L2 Projection NIST02(Dev) 29.95 30.06 NIST03 27.09 27.36 NIST04 29.65 29.89 NIST"
C12-2071,P07-2045,0,0.0187117,"test10 23.35 23.90+ test11 23.65 24.18+ Table 2: Comparison of two tuning methods, MERT and ELBUU, on Spanish-to-English translation tasks. + means the ELBUU method is significantly better than MERT with confidence p &lt; 0.05. Distance metrics L2 Projection NIST02(Dev) 29.95 30.06 NIST03 27.09 27.36 NIST04 29.65 29.89 NIST05 26.79 27.03 NIST06 25.98 26.30 NIST08 19.54 19.79 Table 3: Comparison of two distance metrics L2 and projection on Chinese-to-English translation tasks. The translation system is a phrase-based translation model (Koehn et al., 2003) and we use the open source toolkit MOSES (Koehn et al., 2007) as its implementation. In the experiments, the default setting is used for MOSES. The baseline tuning method is the standard algorithm MERT and the k-best-list size is set as 100 for tuning. For ELBUU, we empirically set α = 3.0 as (Och, 2003), η = 1, ε = 10−5 , K = 20, and we do not tune them further. We tune λ on NIST05 with λ = 1.0 for the Chinese-to-English translation tasks and we do not tune it again for the Spanish-to-English translation tasks. 4.2 Results Table 1 and Table 2 give the main results of ELBUU compared with the baseline MERT on Chinese-to-English and Spanish-to-English tra"
C12-2071,N03-1017,0,0.175345,"sk, the training data is FBIS corpus consisting of about 240k sentence pairs; the development set is NIST02 evaluation data; the test set NIST05 is used as the development test set for tuning hyperparameter λ in Eq. 6; and the test datasets are NIST03, NIST04, NIST05, NIST06, and NIST08. For the Spanish-to-English task, all the datasets are from WMT2011: the training data is the first 200k sentence pairs of Europarl corpus; the development set is dev06; and the test datasets are test07, test08,test09, test10, test11. We run GIZA++ (Och and Ney, 2000) on the training corpus in both directions (Koehn et al., 2003) to obtain the word alignment for each sentence pair. We train a 4-gram language model on the Xinhua portion of the English Gigaword corpus using the SRILM Toolkits (Stolcke, 2002) with modified Kneser-Ney smoothing (Chen and Goodman, 1998). In our experiments, the translation performances are measured by the case-insensitive BLEU4 metric (Papineni et al., 2002) and we use mteval-v13a.pl as the evaluation tool. The significance testing is performed by paired bootstrap re-sampling (Koehn, 2004). 3 At the end of tuning, we average the weights as (Collins, 2002). The norm of the averaged weight m"
C12-2071,P09-1019,0,0.0699435,"imization step in each round, MERT tries to repeatedly optimize a loss function defined by the k-best candidate lists. Since new k-best lists are generated and merged with the previously generated lists at each round, the optimization objective function may change drastically between two adjacent rounds (Pauls et al., 2009), and the optimized weights of these two rounds may also be far from each other. Motivated by the above observation, this paper investigates a new tuning approach under the kbest lists framework, instead of the lattices or hypergraphs framework as Macherey et al. (2008) and Kumar et al. (2009), to achieve a more stable loss function between optimization steps. We propose an expected loss-based ultraconservative update method, in which an expected loss is minimized using an ultraconservative update strategy (Crammer and Singer, 2003; Crammer et al., 2006). In the optimization step, we iteratively learn the weight which should not only minimize the error rates as in MERT but also not be far from the weight learned at the previous optimization step. Instead of using the L2 in Euclidean space to describe the distances between the two weights as in the Margin Infused Relaxed Algorithm ("
C12-2071,D12-1037,1,0.833347,"pus-BLEU compared with the generalized hinge loss, and it utilizes the projection distance metric instead of L2 as with MIRA. Further, ELBUU is a MERT-like batch mode which ultraconservatively updates the weight with all training examples, but MIRA is an online one which updates with each example (Watanabe et al., 2007) or parts of examples (Chiang et al., 2008). The batch mode has some advantages over online mode: more accurate sentence-wise BLEU towards Corpus-BLEU (Watanabe, 2012) and more promising experimental performance (Cherry and Foster, 2012). Additionally, our method is similar to (Liu et al., 2012). However, the main difference is that ours is a global training method instead of a local training method. 4 Experiments and Results 4.1 Experimental Setting We conduct our translation experiments on two language pairs: Chinese-to-English and Spanishto-English. For the Chinese-to-English task, the training data is FBIS corpus consisting of about 240k sentence pairs; the development set is NIST02 evaluation data; the test set NIST05 is used as the development test set for tuning hyperparameter λ in Eq. 6; and the test datasets are NIST03, NIST04, NIST05, NIST06, and NIST08. For the Spanish-to-"
C12-2071,D08-1076,0,0.0578676,"k et al., 2011). At the optimization step in each round, MERT tries to repeatedly optimize a loss function defined by the k-best candidate lists. Since new k-best lists are generated and merged with the previously generated lists at each round, the optimization objective function may change drastically between two adjacent rounds (Pauls et al., 2009), and the optimized weights of these two rounds may also be far from each other. Motivated by the above observation, this paper investigates a new tuning approach under the kbest lists framework, instead of the lattices or hypergraphs framework as Macherey et al. (2008) and Kumar et al. (2009), to achieve a more stable loss function between optimization steps. We propose an expected loss-based ultraconservative update method, in which an expected loss is minimized using an ultraconservative update strategy (Crammer and Singer, 2003; Crammer et al., 2006). In the optimization step, we iteratively learn the weight which should not only minimize the error rates as in MERT but also not be far from the weight learned at the previous optimization step. Instead of using the L2 in Euclidean space to describe the distances between the two weights as in the Margin Inf"
C12-2071,P03-1021,0,0.678266,"ative update, in which the combination of an expected task loss and the distance from the parameters in the previous round are minimized with a variant of gradient descent. Experiments on test datasets of both Chinese-to-English and Spanish-toEnglish translation show that our method can achieve improvements over MERT under the Moses system. KEYWORDS: statistical machine translation; tuning; minimum error rate training; ultraconservative update; expected BLEU. Proceedings of COLING 2012: Posters, pages 723–732, COLING 2012, Mumbai, December 2012. 723 1 Introduction Minimum error rate training (Och, 2003), MERT, is an important component of statistical machine translation (SMT), and it has been the most popular method for tuning parameters for SMT systems. One of its major contributions is the use of an evaluation metric, such as BLEU (Papineni et al., 2002), as a direct loss function during its optimization procedure by interchanging decoding and optimization steps in each round. While MERT is successful in practice, it is known to be unstable (Clark et al., 2011). At the optimization step in each round, MERT tries to repeatedly optimize a loss function defined by the k-best candidate lists."
C12-2071,P00-1056,0,0.0906742,"o-English and Spanishto-English. For the Chinese-to-English task, the training data is FBIS corpus consisting of about 240k sentence pairs; the development set is NIST02 evaluation data; the test set NIST05 is used as the development test set for tuning hyperparameter λ in Eq. 6; and the test datasets are NIST03, NIST04, NIST05, NIST06, and NIST08. For the Spanish-to-English task, all the datasets are from WMT2011: the training data is the first 200k sentence pairs of Europarl corpus; the development set is dev06; and the test datasets are test07, test08,test09, test10, test11. We run GIZA++ (Och and Ney, 2000) on the training corpus in both directions (Koehn et al., 2003) to obtain the word alignment for each sentence pair. We train a 4-gram language model on the Xinhua portion of the English Gigaword corpus using the SRILM Toolkits (Stolcke, 2002) with modified Kneser-Ney smoothing (Chen and Goodman, 1998). In our experiments, the translation performances are measured by the case-insensitive BLEU4 metric (Papineni et al., 2002) and we use mteval-v13a.pl as the evaluation tool. The significance testing is performed by paired bootstrap re-sampling (Koehn, 2004). 3 At the end of tuning, we average th"
C12-2071,P02-1040,0,0.0924311,"lish translation show that our method can achieve improvements over MERT under the Moses system. KEYWORDS: statistical machine translation; tuning; minimum error rate training; ultraconservative update; expected BLEU. Proceedings of COLING 2012: Posters, pages 723–732, COLING 2012, Mumbai, December 2012. 723 1 Introduction Minimum error rate training (Och, 2003), MERT, is an important component of statistical machine translation (SMT), and it has been the most popular method for tuning parameters for SMT systems. One of its major contributions is the use of an evaluation metric, such as BLEU (Papineni et al., 2002), as a direct loss function during its optimization procedure by interchanging decoding and optimization steps in each round. While MERT is successful in practice, it is known to be unstable (Clark et al., 2011). At the optimization step in each round, MERT tries to repeatedly optimize a loss function defined by the k-best candidate lists. Since new k-best lists are generated and merged with the previously generated lists at each round, the optimization objective function may change drastically between two adjacent rounds (Pauls et al., 2009), and the optimized weights of these two rounds may"
C12-2071,D09-1147,0,0.193527,"s the use of an evaluation metric, such as BLEU (Papineni et al., 2002), as a direct loss function during its optimization procedure by interchanging decoding and optimization steps in each round. While MERT is successful in practice, it is known to be unstable (Clark et al., 2011). At the optimization step in each round, MERT tries to repeatedly optimize a loss function defined by the k-best candidate lists. Since new k-best lists are generated and merged with the previously generated lists at each round, the optimization objective function may change drastically between two adjacent rounds (Pauls et al., 2009), and the optimized weights of these two rounds may also be far from each other. Motivated by the above observation, this paper investigates a new tuning approach under the kbest lists framework, instead of the lattices or hypergraphs framework as Macherey et al. (2008) and Kumar et al. (2009), to achieve a more stable loss function between optimization steps. We propose an expected loss-based ultraconservative update method, in which an expected loss is minimized using an ultraconservative update strategy (Crammer and Singer, 2003; Crammer et al., 2006). In the optimization step, we iterative"
C12-2071,P06-2101,0,0.28711,"ain the following objective function:  n  d(W, Wk ) + λLosser r or ri ; ˆe( f i ; W ) i=1 , (3) where d(W, Wk ) is a distance function of a pair of weights and it is used to penalize a weight far away from Wk . Losser r or is the objective function of MERT as defined in Eq. 1. λ ≥ 0 is the regularization penalty. When λ → ∞ Eq. 3 goes back to the objective function of MERT. 725 Because the first term d in Eq. 3 is not piecewise linear in respect to W , the exact line search routine in MERT does not hold anymore. Generally, it is not easy to directly minimize Eq. 3. Motivated by (Och, 2003; Smith and Eisner, 2006; Zens et al., 2007), we use the expected loss to substitute the direct loss in Eq. 3 and we obtain the objective function as follows: d(W, Wk ) + n X λX n i=1 e∈ci Losser r or (ri ; e)Pα (e |f i ; W ), with Pα (e |f i ; W ) = P exp[αW · h( f i , e)] e′ ∈c i exp[αW · h( f i , e′ )] (4) , where α &gt; 0 is a real number, each h( f i , e) is a feature vector, and d is a distance metric defined on a pair of weights. Losser r or (ri ; e) in Eq. 4 is a sentence-wise direct loss, and in this paper we used a variant of sentence BLEU proposed by Chiang et al. (2008) which smoothes BLEU statistics with ps"
C12-2071,N12-1026,1,0.836237,". However, there are also some differences between them. ELBUU optimizes the expected BLEU, a loss more approximate towards Corpus-BLEU compared with the generalized hinge loss, and it utilizes the projection distance metric instead of L2 as with MIRA. Further, ELBUU is a MERT-like batch mode which ultraconservatively updates the weight with all training examples, but MIRA is an online one which updates with each example (Watanabe et al., 2007) or parts of examples (Chiang et al., 2008). The batch mode has some advantages over online mode: more accurate sentence-wise BLEU towards Corpus-BLEU (Watanabe, 2012) and more promising experimental performance (Cherry and Foster, 2012). Additionally, our method is similar to (Liu et al., 2012). However, the main difference is that ours is a global training method instead of a local training method. 4 Experiments and Results 4.1 Experimental Setting We conduct our translation experiments on two language pairs: Chinese-to-English and Spanishto-English. For the Chinese-to-English task, the training data is FBIS corpus consisting of about 240k sentence pairs; the development set is NIST02 evaluation data; the test set NIST05 is used as the development test se"
C12-2071,D07-1080,1,0.957646,"ction as follows: d(W, Wk ) + n X λX n i=1 e∈ci Losser r or (ri ; e)Pα (e |f i ; W ), with Pα (e |f i ; W ) = P exp[αW · h( f i , e)] e′ ∈c i exp[αW · h( f i , e′ )] (4) , where α &gt; 0 is a real number, each h( f i , e) is a feature vector, and d is a distance metric defined on a pair of weights. Losser r or (ri ; e) in Eq. 4 is a sentence-wise direct loss, and in this paper we used a variant of sentence BLEU proposed by Chiang et al. (2008) which smoothes BLEU statistics with pseudo-document. 3.2 Distance Metric Based on Projection Euclidean distance ( L2 norm) is usually employed as in MIRA (Watanabe et al., 2007; Chiang et al., 2008). In this section we will specifically investigate another metric for ultraconservative update in SMT. In log-linear based translation models, since the decoding strategy is the maximal posterior probability, the translation results are the same for the weight W and its positive multiplication (see Eq. 2). Therefore, for a translation decoder, we wish that the distance of two weights satisfies the following property: the smaller the distance between them is, the more similar the translation results decoded with them are. However, L2 norm does not satisfy this property. In"
C12-2071,D07-1055,0,0.083524,"ive function:  n  d(W, Wk ) + λLosser r or ri ; ˆe( f i ; W ) i=1 , (3) where d(W, Wk ) is a distance function of a pair of weights and it is used to penalize a weight far away from Wk . Losser r or is the objective function of MERT as defined in Eq. 1. λ ≥ 0 is the regularization penalty. When λ → ∞ Eq. 3 goes back to the objective function of MERT. 725 Because the first term d in Eq. 3 is not piecewise linear in respect to W , the exact line search routine in MERT does not hold anymore. Generally, it is not easy to directly minimize Eq. 3. Motivated by (Och, 2003; Smith and Eisner, 2006; Zens et al., 2007), we use the expected loss to substitute the direct loss in Eq. 3 and we obtain the objective function as follows: d(W, Wk ) + n X λX n i=1 e∈ci Losser r or (ri ; e)Pα (e |f i ; W ), with Pα (e |f i ; W ) = P exp[αW · h( f i , e)] e′ ∈c i exp[αW · h( f i , e′ )] (4) , where α &gt; 0 is a real number, each h( f i , e) is a feature vector, and d is a distance metric defined on a pair of weights. Losser r or (ri ; e) in Eq. 4 is a sentence-wise direct loss, and in this paper we used a variant of sentence BLEU proposed by Chiang et al. (2008) which smoothes BLEU statistics with pseudo-document. 3.2 D"
C16-1291,D16-1162,0,0.0517274,"NMT1 and NMT2 are much worse than Moses with a substantial gap. This result is not difficult to understand: neural network systems typically require sufficient data to boost their performance, and thus low resource translation tasks are very challenging for them. Secondly, the proposed SA-NMT gains much over NMT2 similar to the case in the large scale task, and the gap towards Moses is narrowed substantially. While our SA-NMT does not advance the state-of-the-art Moses as in large scale translation, this is a strong result if we consider that previous works on low resource translation tasks: Arthur et al. (2016) gained over Moses on the Japanese-to-English BTEC corpus, but they resorted to a corpus consisting of 464k sentence pairs; Luong and Manning (2015) revealed the comparable performance to Moses on English-to-Vietnamese with 133k sentences pairs, which is more than 4 times of our corprus size. Our method is possible to advance Moses by using reranking as in (Neubig et al., 2015; Cohn et al., 2016), but it is beyond the scope of this paper and instead we remain it as future work. 5 Related Work Many recent works have led to notable improvements in the attention mechanism for neural machine trans"
C16-1291,J16-2001,0,0.0929956,"ved by this attention based NMT and conventional alignment models (54 VS 30 in terms of AER for Chinese-to-English as reported in (Cheng et al., 2016)). This discrepancy might be an indication that the potential of attentionbased NMT is limited. In addition, the attention in NMT is learned in an unsupervised manner without explicit prior knowledge about alignment.2 However, in conventional statistical machine translation (SMT), it is standard practice to learn reordering models in a supervised manner with the guidance from conventional alignment models (Xiong et al., 2006; Koehn et al., 2007; Bisazza and Federico, 2016). Inspired by the supervised reordering in conventional SMT, in this paper, we propose a Supervised Attention based NMT (SA-NMT) model. Specifically, similar to conventional SMT, we first run offthe-shelf aligners (GIZA++ (Och and Ney, 2000) or fast align (Dyer et al., 2013) etc.) to obtain the alignment of the bilingual training corpus in advance. Then, treating this alignment result as the supervision of attention, we jointly learn attention and translation, both in supervised manners. Since the This work is licensed under a Creative Commons Attribution 4.0 International Licence. http://crea"
C16-1291,J93-2003,0,0.0944848,"igures out those source words will be translated next, even though the next target word yt is unavailable. From this point of view, the attention mechanism plays a role in reordering and thus can be considered as a reordering model. Unlike this attention model, conventional alignment models define the alignment α directly over x and y as follows: exp(F (x, y, α)) 0 α0 exp(F (x, y, α )) p(α |x, y) = P where F denotes a feature function over a pair of sentences x and y together with their word alignment α, and it is either a log-probability log p(y, α |x) for a generative model like IBM models (Brown et al., 1993) or a well-designed feature function for discriminative models (Liu and Sun, 2015). In order to infer αt , alignment models can readily use the entire y, of course including yt as well, thereby they can model the alignment between x and y more sufficiently. As a result, the attention based NMT might not deliver satisfying alignments, as reported in (Cheng et al., 2016), compared to conventional alignment models. This may be a sign that the potential of attention-based NMT is limited in end-to-end translation. 3 Supervised Attention In this section, we introduce supervised attention to improve"
C16-1291,2016.amta-researchers.10,0,0.330705,"vel reordering model. The main difference is that in our approach the reordering model and translation model are trained jointly rather than separately as theirs. Supervising the attention variables for attention-based neural networks is pioneered by Liu et al. 3100 (2016). On image caption task, Liu et al. (2016) supervise the attention with external guidances in either a strong or a weak supervision manner. Their method requires the training data to be associated with direct annotation or indirect annotation. In parallel to our work, particularly on machine translation, Mi et al. (2016) and Chen et al. (2016) guide the attention for NMT from conventional word alignment models as teachers without any annotation on machine translation task. The differences of our work lie in that: we consider the attention as a form of a reordering model, which is thereby straightforward to be learned from conventional word alignment models; and we also provide a theoretical explanation why the attention leads to the worse alignment accuracy than the conventional word alignment models, standing upon the point view of reordering. 6 Conclusion It has been shown that attention mechanism in NMT is worse than conventiona"
C16-1291,P14-1129,0,0.0162995,"e original paper, αt is not explicitly dependent on the yt−1 in Eq.(2), but this dependency was explicitly retained in our direct baseline NMT2. 5 Although the alignment is loosely related to the downstream translation (Liu and Sun, 2015), substantial improvements in alignment usually leads to the improvements in translation as observed in our experiments. 3095 Therefore, we apply the following heuristics to preprocess the hard alignment: if a target word does not align to any source words, we inherit its affiliation from the closest aligned word with preference given to the right, following (Devlin et al., 2014); if a target word is aligned to multiple source words, we assume it aligns to each one evenly. In addition, in the implementation of NMT, there are two special tokens ‘eol’ added to both source and target sentences. We assume they are aligned to each other. In this way, we can obtain the final supervision of attention, denoted as α ˆ. 3.2 Jointly Supervising Translation and Attention We propose a soft constraint method to jointly supervise the translation and attention as follows: − X log p(yi |xi ; θ) + λ × ∆(αi , α ˆ i ; θ) (4) i where αi is as defined in Eq. (1), ∆ is a loss function that"
C16-1291,N13-1073,0,0.108639,"allowing the use of fewer hidden layers while still maintaining high levels of translation performance. An attention mechanism is designed to predict the alignment of a target word with respect to source words (Bahdanau et al., 2015). In order to facilitate incremental decoding, it tries to make this alignment prediction without the information about the target word itself, and thus this attention can be considered to be a form of a reordering model (see §2 for more details). In contrast, conventional alignment models are able to use the target word to infer its alignments (Och and Ney, 2000; Dyer et al., 2013; Liu and Sun, 2015), and as a result there is a substantial gap in quality between the alignments derived by this attention based NMT and conventional alignment models (54 VS 30 in terms of AER for Chinese-to-English as reported in (Cheng et al., 2016)). This discrepancy might be an indication that the potential of attentionbased NMT is limited. In addition, the attention in NMT is learned in an unsupervised manner without explicit prior knowledge about alignment.2 However, in conventional statistical machine translation (SMT), it is standard practice to learn reordering models in a supervise"
C16-1291,P07-2045,0,0.0608892,"the alignments derived by this attention based NMT and conventional alignment models (54 VS 30 in terms of AER for Chinese-to-English as reported in (Cheng et al., 2016)). This discrepancy might be an indication that the potential of attentionbased NMT is limited. In addition, the attention in NMT is learned in an unsupervised manner without explicit prior knowledge about alignment.2 However, in conventional statistical machine translation (SMT), it is standard practice to learn reordering models in a supervised manner with the guidance from conventional alignment models (Xiong et al., 2006; Koehn et al., 2007; Bisazza and Federico, 2016). Inspired by the supervised reordering in conventional SMT, in this paper, we propose a Supervised Attention based NMT (SA-NMT) model. Specifically, similar to conventional SMT, we first run offthe-shelf aligners (GIZA++ (Och and Ney, 2000) or fast align (Dyer et al., 2013) etc.) to obtain the alignment of the bilingual training corpus in advance. Then, treating this alignment result as the supervision of attention, we jointly learn attention and translation, both in supervised manners. Since the This work is licensed under a Creative Commons Attribution 4.0 Inter"
C16-1291,N06-1014,0,0.0172418,"because the supervision of α is more close to Ex than y as in Figure 1(b). In order to quantify the disagreement between αi and α ˆ i , three different methods are investigated in our experiments: • Mean Squared Error (MSE) ∆(αi , α ˆ i ; θ) = XX 1 m 2 n i α(θ)im,n − α ˆ m,n 2 MSE is widely used as a loss for regression tasks (Lehmann and Casella, 1998), and it directly i encourages α(θ)im,n to be equal to α ˆ m,n . • Multiplication (MUL) ∆(αi , α ˆ i ; θ) = − log XX m n i α(θ)im,n × α ˆ m,n  MUL is particularly designed for agreement in word alignment and it has been shown to be effective (Liang et al., 2006; Cheng et al., 2016). Note that different from those in (Cheng et al., 2016), α ˆ is not a parametrized variable but a constant in this paper. • Cross Entropy (CE) ∆(αi , α ˆ i ; θ) = − XX m n i α ˆ m,n × log α(θ)im,n Since for each t, α(θ)t is a distribution, it is natural to use CE as the metric to evaluate the disagreement (Rubinstein and Kroese, 2004). 4 Experiments We conducted experiments on two Chinese-to-English translation tasks: one is the NIST task oriented to NEWS domain, which is a large scale task and suitable to NMT; and the other is the speech translation oriented to travel do"
C16-1291,2015.iwslt-evaluation.11,0,0.28045,"1. The first row shows the alignments of the sentence pair from the training set while the second row shows the alignments from test sets. Methods GIZA++ NMT2 SA-NMT AER 30.6∗ 50.6 43.3∗ Table 4: Results on word alignment task for the large scale data. The evaluation metric is Alignment Error Rate (AER). ‘*’ denotes that the corresponding result is significanly better than NMT2 with p &lt; 0.01. Table 4 shows the overall alignment results on word alignment task in terms of the metric, alignment error rate. We used the manually-aligned dataset as in (Liu and Sun, 2015) as the test set. Following (Luong and Manning, 2015), we force-decode both the bilingual sentences including source and reference sentences to obtain the alignment matrices, and then for each target word we extract one-to-one alignments by picking up the source word with the highest alignment confidence as the hard alignment. From Table 4, we can see clearly that standard NMT (NMT2) is far behind GIZA++ in alignment quality. This shows that it is possible and promising to supervise the attention with GIZA++. With the help from GIZA++, our supervised attention based NMT (SA-NMT) significantly reduces the AER, compared with the unsupervised count"
C16-1291,D15-1166,0,0.100348,"ver the standard attention based NMT. 1 Introduction Neural Machine Translation (NMT) has achieved great successes on machine translation tasks recently (Bahdanau et al., 2015; Sutskever et al., 2015). Generally, it relies on a recurrent neural network under the Encode-Decode framework: it firstly encodes a source sentence into context vectors and then generates its translation token-by-token, selecting from the target vocabulary. Among different variants of NMT, attention based NMT, which is the focus of this paper,1 is attracting increasing interests in the community (Bahdanau et al., 2015; Luong et al., 2015). One of its advantages is that it is able to dynamically make use of the encoded context through an attention mechanism thereby allowing the use of fewer hidden layers while still maintaining high levels of translation performance. An attention mechanism is designed to predict the alignment of a target word with respect to source words (Bahdanau et al., 2015). In order to facilitate incremental decoding, it tries to make this alignment prediction without the information about the target word itself, and thus this attention can be considered to be a form of a reordering model (see §2 for more"
C16-1291,D16-1249,0,0.218565,"network based word-level reordering model. The main difference is that in our approach the reordering model and translation model are trained jointly rather than separately as theirs. Supervising the attention variables for attention-based neural networks is pioneered by Liu et al. 3100 (2016). On image caption task, Liu et al. (2016) supervise the attention with external guidances in either a strong or a weak supervision manner. Their method requires the training data to be associated with direct annotation or indirect annotation. In parallel to our work, particularly on machine translation, Mi et al. (2016) and Chen et al. (2016) guide the attention for NMT from conventional word alignment models as teachers without any annotation on machine translation task. The differences of our work lie in that: we consider the attention as a form of a reordering model, which is thereby straightforward to be learned from conventional word alignment models; and we also provide a theoretical explanation why the attention leads to the worse alignment accuracy than the conventional word alignment models, standing upon the point view of reordering. 6 Conclusion It has been shown that attention mechanism in NMT is"
C16-1291,W15-5003,0,0.0260557,"Moses is narrowed substantially. While our SA-NMT does not advance the state-of-the-art Moses as in large scale translation, this is a strong result if we consider that previous works on low resource translation tasks: Arthur et al. (2016) gained over Moses on the Japanese-to-English BTEC corpus, but they resorted to a corpus consisting of 464k sentence pairs; Luong and Manning (2015) revealed the comparable performance to Moses on English-to-Vietnamese with 133k sentences pairs, which is more than 4 times of our corprus size. Our method is possible to advance Moses by using reranking as in (Neubig et al., 2015; Cohn et al., 2016), but it is beyond the scope of this paper and instead we remain it as future work. 5 Related Work Many recent works have led to notable improvements in the attention mechanism for neural machine translation. Tu et al. (2016) introduced an explicit coverage vector into the attention mechanism to address the over-translation and under-translation inherent in NMT. Feng et al. (2016) proposed an additional recurrent structure for attention to capture long-term dependencies. Cheng et al. (2016) proposed an agreement-based bidirectional NMT model for symmetrizing alignment. Cohn"
C16-1291,P00-1056,0,0.102849,"mechanism thereby allowing the use of fewer hidden layers while still maintaining high levels of translation performance. An attention mechanism is designed to predict the alignment of a target word with respect to source words (Bahdanau et al., 2015). In order to facilitate incremental decoding, it tries to make this alignment prediction without the information about the target word itself, and thus this attention can be considered to be a form of a reordering model (see §2 for more details). In contrast, conventional alignment models are able to use the target word to infer its alignments (Och and Ney, 2000; Dyer et al., 2013; Liu and Sun, 2015), and as a result there is a substantial gap in quality between the alignments derived by this attention based NMT and conventional alignment models (54 VS 30 in terms of AER for Chinese-to-English as reported in (Cheng et al., 2016)). This discrepancy might be an indication that the potential of attentionbased NMT is limited. In addition, the attention in NMT is learned in an unsupervised manner without explicit prior knowledge about alignment.2 However, in conventional statistical machine translation (SMT), it is standard practice to learn reordering mo"
C16-1291,P14-1138,1,0.532331,"). This shows that the proposed approach is able to realize our intuition: the alignment is improved, leading to better translation performance. Note that there is still a gap between SA-NMT and GIZA++ as indicated in Table 4. Since SA-NMT was trained for machine translation instead of word alignment, it is possible to reduce its AER if we aim to the word alignment task only. For example, we can enlarge λ in Eq.(4) to bias the training objective towards word alignment task, or we can change the architecture slightly to add the target information crucial for alignment as in (Yang et al., 2013; Tamura et al., 2014). 3099 Systems Moses NMT1 NMT2 SA-NMT CSTAR03 44.1 33.4 36.5 39.8∗ IWSLT04 45.1 33.0 35.9 40.7∗ Table 5: BLEU comparison for low-resource translation task. CSTAR03 is the development set while IWSLT04 is the test set. ‘*’ denotes that SA-NMT is significantly better than both NMT1 and NMT2 with p &lt; 0.01. 4.2 Results on the Low Resource Translation Task For the low resource translation task, we used the BTEC corpus as the training data, which consists of 30k sentence pairs with 0.27M Chinese words and 0.33M English words. As development and test sets, we used the CSTAR03 and IWSLT04 held out set"
C16-1291,P16-1008,0,0.0286294,"over Moses on the Japanese-to-English BTEC corpus, but they resorted to a corpus consisting of 464k sentence pairs; Luong and Manning (2015) revealed the comparable performance to Moses on English-to-Vietnamese with 133k sentences pairs, which is more than 4 times of our corprus size. Our method is possible to advance Moses by using reranking as in (Neubig et al., 2015; Cohn et al., 2016), but it is beyond the scope of this paper and instead we remain it as future work. 5 Related Work Many recent works have led to notable improvements in the attention mechanism for neural machine translation. Tu et al. (2016) introduced an explicit coverage vector into the attention mechanism to address the over-translation and under-translation inherent in NMT. Feng et al. (2016) proposed an additional recurrent structure for attention to capture long-term dependencies. Cheng et al. (2016) proposed an agreement-based bidirectional NMT model for symmetrizing alignment. Cohn et al. (2016) incorporated multiple structural alignment biases into attention learning for better alignment. All of them improved the attention models that were learned in an unsupervised manner. While we do not modify the attention model itse"
C16-1291,P06-1066,0,0.0297014,"p in quality between the alignments derived by this attention based NMT and conventional alignment models (54 VS 30 in terms of AER for Chinese-to-English as reported in (Cheng et al., 2016)). This discrepancy might be an indication that the potential of attentionbased NMT is limited. In addition, the attention in NMT is learned in an unsupervised manner without explicit prior knowledge about alignment.2 However, in conventional statistical machine translation (SMT), it is standard practice to learn reordering models in a supervised manner with the guidance from conventional alignment models (Xiong et al., 2006; Koehn et al., 2007; Bisazza and Federico, 2016). Inspired by the supervised reordering in conventional SMT, in this paper, we propose a Supervised Attention based NMT (SA-NMT) model. Specifically, similar to conventional SMT, we first run offthe-shelf aligners (GIZA++ (Och and Ney, 2000) or fast align (Dyer et al., 2013) etc.) to obtain the alignment of the bilingual training corpus in advance. Then, treating this alignment result as the supervision of attention, we jointly learn attention and translation, both in supervised manners. Since the This work is licensed under a Creative Commons A"
C16-1291,P13-1017,0,0.0104401,"d counterpart (NMT2). This shows that the proposed approach is able to realize our intuition: the alignment is improved, leading to better translation performance. Note that there is still a gap between SA-NMT and GIZA++ as indicated in Table 4. Since SA-NMT was trained for machine translation instead of word alignment, it is possible to reduce its AER if we aim to the word alignment task only. For example, we can enlarge λ in Eq.(4) to bias the training objective towards word alignment task, or we can change the architecture slightly to add the target information crucial for alignment as in (Yang et al., 2013; Tamura et al., 2014). 3099 Systems Moses NMT1 NMT2 SA-NMT CSTAR03 44.1 33.4 36.5 39.8∗ IWSLT04 45.1 33.0 35.9 40.7∗ Table 5: BLEU comparison for low-resource translation task. CSTAR03 is the development set while IWSLT04 is the test set. ‘*’ denotes that SA-NMT is significantly better than both NMT1 and NMT2 with p &lt; 0.01. 4.2 Results on the Low Resource Translation Task For the low resource translation task, we used the BTEC corpus as the training data, which consists of 30k sentence pairs with 0.27M Chinese words and 0.33M English words. As development and test sets, we used the CSTAR03 an"
C16-1291,C14-1179,0,\N,Missing
D12-1037,P08-1024,0,0.14643,"mprovements up to 2.0 BLEU points, meanwhile its efficiency is comparable to that of the global method. 1 (1) e Introduction Och and Ney (2002) introduced the log-linear model for statistical machine translation (SMT), in which translation is considered as the following optimization problem: where f and e (e0 ) are source and target sentences, respectively. h is a feature vector which is scaled by a weight W . Parameter estimation is one of the most important components in SMT, and various training methods have been proposed to tune W . Some methods are based on likelihood (Och and Ney, 2002; Blunsom et al., 2008), error rate (Och, 2003; Zhao and Chen, 2009; Pauls et al., 2009; Galley and Quirk, 2011), margin (Watanabe et al., 2007; Chiang et al., 2008) and ranking (Hopkins and May, 2011), and among which minimum error rate training (MERT) (Och, 2003) is the most popular one. All these training methods follow the same pipeline: they train only a single weight on a given development set, and then use it to translate all the sentences in a test set. We call them a global training method. One of its advantages is that it allows us to train a single weight offline and thereby it is efficient. However, due"
D12-1037,D08-1024,0,0.623546,"introduced the log-linear model for statistical machine translation (SMT), in which translation is considered as the following optimization problem: where f and e (e0 ) are source and target sentences, respectively. h is a feature vector which is scaled by a weight W . Parameter estimation is one of the most important components in SMT, and various training methods have been proposed to tune W . Some methods are based on likelihood (Och and Ney, 2002; Blunsom et al., 2008), error rate (Och, 2003; Zhao and Chen, 2009; Pauls et al., 2009; Galley and Quirk, 2011), margin (Watanabe et al., 2007; Chiang et al., 2008) and ranking (Hopkins and May, 2011), and among which minimum error rate training (MERT) (Och, 2003) is the most popular one. All these training methods follow the same pipeline: they train only a single weight on a given development set, and then use it to translate all the sentences in a test set. We call them a global training method. One of its advantages is that it allows us to train a single weight offline and thereby it is efficient. However, due to the diversity and uneven distribution of source sentences(Li et al., 2010), there are some shortcomings in this pipeline. Firstly, on the d"
D12-1037,P05-1033,0,0.0879558,"r experiments the translation perfunction (5), the optimization algorithm MERT can formances are measured by case-insensitive BLEU4 not be applied for this question since the exact line metric (Papineni et al., 2002) and we use mtevalsearch routine does not hold here. Motivated by v13a.pl as the evaluation tool. The significance test(Och, 2003; Smith and Eisner, 2006), we approxi- ing is performed by paired bootstrap re-sampling mate the Error in (5) by the expected loss, and then (Koehn, 2004). We use an in-house developed hierarchical derive the following function: phrase-based translation (Chiang, 2005) as our baseK λ XX 1 2 kW −Wb k + Error(rj ; e)Pα (e|fj ; W ), line system, and we denote it as In-Hiero. To ob2 K tain satisfactory baseline performance, we tune Inj=1 e (6) Hiero system for 5 times using MERT, and then se406 Methods Global method Local method Steps Decoding Retrieval Local training Seconds 2.0 +0.6 +0.3 NIST02 NIST05 27.07 27.75+ 27.85+ NIST06 26.32 27.88+ 27.99+ NIST08 19.03 20.84+ 21.08+ Table 3: The performance comparison of local training methods (MBUU and EBUU) and a global method (MERT). NIST05 is the set used to tune λ for MBUU and EBUU, and NIST06 and NIST08 are test"
D12-1037,D11-1004,0,0.149234,"e global method. 1 (1) e Introduction Och and Ney (2002) introduced the log-linear model for statistical machine translation (SMT), in which translation is considered as the following optimization problem: where f and e (e0 ) are source and target sentences, respectively. h is a feature vector which is scaled by a weight W . Parameter estimation is one of the most important components in SMT, and various training methods have been proposed to tune W . Some methods are based on likelihood (Och and Ney, 2002; Blunsom et al., 2008), error rate (Och, 2003; Zhao and Chen, 2009; Pauls et al., 2009; Galley and Quirk, 2011), margin (Watanabe et al., 2007; Chiang et al., 2008) and ranking (Hopkins and May, 2011), and among which minimum error rate training (MERT) (Och, 2003) is the most popular one. All these training methods follow the same pipeline: they train only a single weight on a given development set, and then use it to translate all the sentences in a test set. We call them a global training method. One of its advantages is that it allows us to train a single weight offline and thereby it is efficient. However, due to the diversity and uneven distribution of source sentences(Li et al., 2010), there are"
D12-1037,P10-1064,0,0.0295431,"Missing"
D12-1037,2005.eamt-1.19,0,0.0784023,"arns a weight for each sentence. Further, our translation framework integrates the training and testing into one unit, instead of treating them separately. One of the advantages is that it can adapt the weights for each of the test sentences. Our method resorts to some translation examples, which is similar as example-based translation or translation memory (Watanabe and Sumita, 2003; He et al., 2010; Ma et al., 2011). Instead of using translation examples to construct translation rules for enlarging the decoding space, we employed them 409 to discriminatively learn local weights. Similar to (Hildebrand et al., 2005; L¨u et al., 2007), our method also employes IR methods to retrieve examples for a given test set. Their methods utilize the retrieved examples to acquire translation model and can be seen as the adaptation of translation model. However, ours uses the retrieved examples to tune the weights and thus can be considered as the adaptation of tuning. Furthermore, since ours does not change the translation model which needs to run GIZA++ and it incrementally trains local weights, our method can be applied for online translation service. 7 Conclusion and Future Work This paper proposes a novel local"
D12-1037,D11-1125,0,0.403742,"or statistical machine translation (SMT), in which translation is considered as the following optimization problem: where f and e (e0 ) are source and target sentences, respectively. h is a feature vector which is scaled by a weight W . Parameter estimation is one of the most important components in SMT, and various training methods have been proposed to tune W . Some methods are based on likelihood (Och and Ney, 2002; Blunsom et al., 2008), error rate (Och, 2003; Zhao and Chen, 2009; Pauls et al., 2009; Galley and Quirk, 2011), margin (Watanabe et al., 2007; Chiang et al., 2008) and ranking (Hopkins and May, 2011), and among which minimum error rate training (MERT) (Och, 2003) is the most popular one. All these training methods follow the same pipeline: they train only a single weight on a given development set, and then use it to translate all the sentences in a test set. We call them a global training method. One of its advantages is that it allows us to train a single weight offline and thereby it is efficient. However, due to the diversity and uneven distribution of source sentences(Li et al., 2010), there are some shortcomings in this pipeline. Firstly, on the document level, the performance of th"
D12-1037,N03-1017,0,0.114511,"Missing"
D12-1037,P07-2045,0,0.014159,".75+ 27.85+ NIST06 26.32 27.88+ 27.99+ NIST08 19.03 20.84+ 21.08+ Table 3: The performance comparison of local training methods (MBUU and EBUU) and a global method (MERT). NIST05 is the set used to tune λ for MBUU and EBUU, and NIST06 and NIST08 are test sets. + means the local method is significantly better than MERT with p &lt; 0.05. lect the best-performing one as our baseline for the following experiments. As Table 1 indicates, our baseline In-Hiero is comparable to the phrase-based MT (Moses) and the hierarchical phrase-based MT (Moses hier) implemented in Moses, an open source MT toolkit2 (Koehn et al., 2007). Both of these systems are with default setting. All three systems are trained by MERT with 100 best candidates. To compare the local training method in Algorithm 2, we use a standard global training method, MERT, as the baseline training method. We do not compare with Algorithm 1, in which retraining is performed for each input sentence, since retraining for the whole test set is impractical given that each sentence-wise retraining may take some hours or even days. Therefore, we just compare Algorithm 2 with MERT. 5.2 Runtime Results To run the Algorithm 2, we tune the baseline weight Wb on"
D12-1037,W04-3250,0,0.0660269,"h modified Kneser-Ney smoothing (Chen and Goodrj . Due to the existence of L2 norm in objective man, 1998). In our experiments the translation perfunction (5), the optimization algorithm MERT can formances are measured by case-insensitive BLEU4 not be applied for this question since the exact line metric (Papineni et al., 2002) and we use mtevalsearch routine does not hold here. Motivated by v13a.pl as the evaluation tool. The significance test(Och, 2003; Smith and Eisner, 2006), we approxi- ing is performed by paired bootstrap re-sampling mate the Error in (5) by the expected loss, and then (Koehn, 2004). We use an in-house developed hierarchical derive the following function: phrase-based translation (Chiang, 2005) as our baseK λ XX 1 2 kW −Wb k + Error(rj ; e)Pα (e|fj ; W ), line system, and we denote it as In-Hiero. To ob2 K tain satisfactory baseline performance, we tune Inj=1 e (6) Hiero system for 5 times using MERT, and then se406 Methods Global method Local method Steps Decoding Retrieval Local training Seconds 2.0 +0.6 +0.3 NIST02 NIST05 27.07 27.75+ 27.85+ NIST06 26.32 27.88+ 27.99+ NIST08 19.03 20.84+ 21.08+ Table 3: The performance comparison of local training methods (MBUU and EB"
D12-1037,C10-1075,0,0.091706,"009; Galley and Quirk, 2011), margin (Watanabe et al., 2007; Chiang et al., 2008) and ranking (Hopkins and May, 2011), and among which minimum error rate training (MERT) (Och, 2003) is the most popular one. All these training methods follow the same pipeline: they train only a single weight on a given development set, and then use it to translate all the sentences in a test set. We call them a global training method. One of its advantages is that it allows us to train a single weight offline and thereby it is efficient. However, due to the diversity and uneven distribution of source sentences(Li et al., 2010), there are some shortcomings in this pipeline. Firstly, on the document level, the performance of these methods is dependent on the choice of a development set, which may potentially lead to an unstable translation performance for testing. As referred in our experiment, the BLEU points on NIST08 are 402 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural c Language Learning, pages 402–411, Jeju Island, Korea, 12–14 July 2012. 2012 Association for Computational Linguistics  1, 0  h ( f 1 , e11 )  h ( f 1 , e12 )  2,0  h("
D12-1037,D07-1036,0,0.0396223,"Missing"
D12-1037,P11-1124,0,0.197989,"Missing"
D12-1037,P00-1056,0,0.0922694,"Missing"
D12-1037,P02-1038,0,0.282749,"method to address these two problems. Unlike a global training method, such as MERT, in which a single weight is learned and used for all the input sentences, we perform training and testing in one step by learning a sentencewise weight for each input sentence. We propose efficient incremental training methods to put the local training into practice. In NIST Chinese-to-English translation tasks, our local training method significantly outperforms MERT with the maximal improvements up to 2.0 BLEU points, meanwhile its efficiency is comparable to that of the global method. 1 (1) e Introduction Och and Ney (2002) introduced the log-linear model for statistical machine translation (SMT), in which translation is considered as the following optimization problem: where f and e (e0 ) are source and target sentences, respectively. h is a feature vector which is scaled by a weight W . Parameter estimation is one of the most important components in SMT, and various training methods have been proposed to tune W . Some methods are based on likelihood (Och and Ney, 2002; Blunsom et al., 2008), error rate (Och, 2003; Zhao and Chen, 2009; Pauls et al., 2009; Galley and Quirk, 2011), margin (Watanabe et al., 2007;"
D12-1037,P03-1021,0,0.66567,"meanwhile its efficiency is comparable to that of the global method. 1 (1) e Introduction Och and Ney (2002) introduced the log-linear model for statistical machine translation (SMT), in which translation is considered as the following optimization problem: where f and e (e0 ) are source and target sentences, respectively. h is a feature vector which is scaled by a weight W . Parameter estimation is one of the most important components in SMT, and various training methods have been proposed to tune W . Some methods are based on likelihood (Och and Ney, 2002; Blunsom et al., 2008), error rate (Och, 2003; Zhao and Chen, 2009; Pauls et al., 2009; Galley and Quirk, 2011), margin (Watanabe et al., 2007; Chiang et al., 2008) and ranking (Hopkins and May, 2011), and among which minimum error rate training (MERT) (Och, 2003) is the most popular one. All these training methods follow the same pipeline: they train only a single weight on a given development set, and then use it to translate all the sentences in a test set. We call them a global training method. One of its advantages is that it allows us to train a single weight offline and thereby it is efficient. However, due to the diversity and un"
D12-1037,P02-1040,0,0.0913028,"e eˆ(fj ; W ) is defined in Equation (1), and tence pair. We train a 4-gram language model on Error(rj , e) is the sentence-wise minus BLEU (Pa- the Xinhua portion of the English Gigaword corpineni et al., 2002) of a candidate e with respect to pus using the SRILM Toolkits (Stolcke, 2002) with modified Kneser-Ney smoothing (Chen and Goodrj . Due to the existence of L2 norm in objective man, 1998). In our experiments the translation perfunction (5), the optimization algorithm MERT can formances are measured by case-insensitive BLEU4 not be applied for this question since the exact line metric (Papineni et al., 2002) and we use mtevalsearch routine does not hold here. Motivated by v13a.pl as the evaluation tool. The significance test(Och, 2003; Smith and Eisner, 2006), we approxi- ing is performed by paired bootstrap re-sampling mate the Error in (5) by the expected loss, and then (Koehn, 2004). We use an in-house developed hierarchical derive the following function: phrase-based translation (Chiang, 2005) as our baseK λ XX 1 2 kW −Wb k + Error(rj ; e)Pα (e|fj ; W ), line system, and we denote it as In-Hiero. To ob2 K tain satisfactory baseline performance, we tune Inj=1 e (6) Hiero system for 5 times usi"
D12-1037,D09-1147,0,0.080957,"arable to that of the global method. 1 (1) e Introduction Och and Ney (2002) introduced the log-linear model for statistical machine translation (SMT), in which translation is considered as the following optimization problem: where f and e (e0 ) are source and target sentences, respectively. h is a feature vector which is scaled by a weight W . Parameter estimation is one of the most important components in SMT, and various training methods have been proposed to tune W . Some methods are based on likelihood (Och and Ney, 2002; Blunsom et al., 2008), error rate (Och, 2003; Zhao and Chen, 2009; Pauls et al., 2009; Galley and Quirk, 2011), margin (Watanabe et al., 2007; Chiang et al., 2008) and ranking (Hopkins and May, 2011), and among which minimum error rate training (MERT) (Och, 2003) is the most popular one. All these training methods follow the same pipeline: they train only a single weight on a given development set, and then use it to translate all the sentences in a test set. We call them a global training method. One of its advantages is that it allows us to train a single weight offline and thereby it is efficient. However, due to the diversity and uneven distribution of source sentences(Li"
D12-1037,2003.mtsummit-papers.54,1,0.859025,"nd the incremental training in line 5 of Algorithm 2. 3 Acquiring Training Examples In line 4 of Algorithm 2, to retrieve training examples for the sentence ti , we first need a metric to retrieve similar translation examples. We assume that the metric satisfy the property: more similar the test sentence and translation examples are, the better translation result one obtains when decoding the test sentence with the weight trained on the translation examples. The metric we consider here is derived from an example-based machine translation. To retrieve translation examples for a test sentence, (Watanabe and Sumita, 2003) defined a metric based on the combination of edit distance and TF-IDF (Manning and Sch¨utze, 1999) as follows: dist(f1 , f2 ) = θ × edit-dist(f1 , f2 )+ (1 − θ) × tf-idf(f1 , f2 ), (2) where θ(0 ≤ θ ≤ 1) is an interpolation weight, fi (i = 1, 2) is a word sequence and can be also considered as a document. In this paper, we extract similar examples from training data. Like examplebased translation in which similar source sentences have similar translations, we assume that the optimal translation weights of the similar source sentences are closer. 4 Incremental Training Based on Ultraconservati"
D12-1037,D07-1080,1,0.964911,"tion Och and Ney (2002) introduced the log-linear model for statistical machine translation (SMT), in which translation is considered as the following optimization problem: where f and e (e0 ) are source and target sentences, respectively. h is a feature vector which is scaled by a weight W . Parameter estimation is one of the most important components in SMT, and various training methods have been proposed to tune W . Some methods are based on likelihood (Och and Ney, 2002; Blunsom et al., 2008), error rate (Och, 2003; Zhao and Chen, 2009; Pauls et al., 2009; Galley and Quirk, 2011), margin (Watanabe et al., 2007; Chiang et al., 2008) and ranking (Hopkins and May, 2011), and among which minimum error rate training (MERT) (Och, 2003) is the most popular one. All these training methods follow the same pipeline: they train only a single weight on a given development set, and then use it to translate all the sentences in a test set. We call them a global training method. One of its advantages is that it allows us to train a single weight offline and thereby it is efficient. However, due to the diversity and uneven distribution of source sentences(Li et al., 2010), there are some shortcomings in this pipel"
D12-1037,N09-2006,0,0.20149,"ts efficiency is comparable to that of the global method. 1 (1) e Introduction Och and Ney (2002) introduced the log-linear model for statistical machine translation (SMT), in which translation is considered as the following optimization problem: where f and e (e0 ) are source and target sentences, respectively. h is a feature vector which is scaled by a weight W . Parameter estimation is one of the most important components in SMT, and various training methods have been proposed to tune W . Some methods are based on likelihood (Och and Ney, 2002; Blunsom et al., 2008), error rate (Och, 2003; Zhao and Chen, 2009; Pauls et al., 2009; Galley and Quirk, 2011), margin (Watanabe et al., 2007; Chiang et al., 2008) and ranking (Hopkins and May, 2011), and among which minimum error rate training (MERT) (Och, 2003) is the most popular one. All these training methods follow the same pipeline: they train only a single weight on a given development set, and then use it to translate all the sentences in a test set. We call them a global training method. One of its advantages is that it allows us to train a single weight offline and thereby it is efficient. However, due to the diversity and uneven distribution of"
D12-1037,C08-1074,0,\N,Missing
D14-1209,N12-1047,0,0.593468,"=1..|x| where top1w (·) is defined in Eq. (4), and δyx (d), defined in Table 1, is the generic metric for evaluating a partial derivation d which has two implementations (partial bleu or potential bleu). In order words we can obtain two implementations of search-aware M ERT methods, SA-M ERT par and SA-M ERTpot . Notice that the traditional M ERT is a special case of SA-M ERT where i is fixed to |x|. 4.2 From M IRA to Search-Aware M IRA M IRA is another popular tuning method for SMT. It firstly introduced in (Watanabe et al., 2007), and then was improved in (Chiang et al., 2008; Chiang, 2012; Cherry and Foster, 2012). Its main idea is to optimize a weight such that the model score difference of a pair of derivations is greater than their loss difference. In this paper, we follow the objective function in (Chiang, 2012; Cherry and Foster, 2012), where only the violation between hope and fear derivations is concerned. Formally, we define d+ (x, y) and d− (x, y) as the hope and fear derivations in the final bin (i.e., complete derivations): d+ (x, y) = argmax w0 · Φ(d) − δy (d) (10) d∈B|x |(x) − d (x, y) = argmax w0 · Φ(d) + δy (d) (11) d∈B|x |(x) where w0 is the current model. The loss function   of M IRA"
D14-1209,D08-1024,0,0.0466324,"fter tuning. 4.1 From M ERT to Search-Aware M ERT Suppose D is a tuning set of hx, yi pairs. Traditional M ERT learns the weight by iteratively reranking the complete translations towards those with higher B LEU in the final bin B|x |(x) for each x in D. Formally, it tries to minimize the document-level error of 1-best translations:   M `M ERT (D, w) = δy top1w (B|x |(x)) , hx,yi∈D (4) where top1w (S) is the best derivation in S under model w, and δ· (·) is the full derivation metric as defined in Table 1; in this paper we use δy (y 0 ) = −B LEU(y, y 0 ). Here we follow Och (2003) and Lopez (2008) to simplify P the notations, where the ⊕ operator (similar to ) is an over-simplification for B LEU which, as a document-level metric, is actually not factorizable across sentences. Besides reranking the complete translations as traditional M ERT, our search-aware M ERT (SAM ERT) also reranks the partial translations such that potential translations may survive in the middle bins during search. Formally, its objective function is defined as follows: `SA-M ERT(D, w) = M L hx,yi∈D i=1..|x| where top1w (·) is defined in Eq. (4), and δyx (d), defined in Table 1, is the generic metric for evaluati"
D14-1209,P11-2031,0,0.0972672,"for us, and thus we use ssmt07 as the tuning set, which is provided at the Third Symposium on Statistical Machine Translation (http://mitlab.hit.edu.cn/ssmt2007.html). option. The LM for E N -C H is trained on its target side; and that for C H -E N is trained on the Xinhua portion of Gigaword. We use B LEU-4 (Papineni et al., 2002) with “average ref-len” to evaluate the translation performance for all experiments. In particular, the character-based B LEU-4 is employed for E N -C H task. Since all tuning methods involve randomness, all scores reported are average of three runs, as suggested by Clark et al. (2011) for fairer comparisons. 5.2 Main Results on C H -E N Task Table 2 depicts the main results of our methods on C H -E N translation task. On all five test sets, our methods consistently achieve substantial improvements with two pruning options: SA-M ERT pot gains +1.2 B LEU points over M ERT on average; and SA-M IRApot gains +1.8 B LEU points over M IRA on average as well. SA-P RO pot , however, does not work out of the box when we use the entire nist02 as the tuning set, which might be attributed to the “Monster” behavior (Nakov et al., 2013). To alleviate this problem, we only use the 109 sho"
D14-1209,P04-1015,0,0.107435,"acteristic: they learn a weight by iteratively reranking a set of complete translations represented by k-best (Och, 2003; Watanabe et al., 2007; Chiang et al., 2008) or lattice (hypergraph) (Tromble et al., 2008; Kumar et al., 2009), and they do not care about search errors that potential partial translations may be pruned during decoding, even if they agree with 1950 that their decoders are built on the beam pruning based search. On the other hand, it is well-known that search errors can undermine the standard training for many beam search based NLP systems (Huang et al., 2012). As a result, Collins and Roark (2004) and Huang et al. (2012) propose the early-update and max-violation update to deal with the search errors. Their idea is to update on prefix or partial hypotheses when the correct solution falls out of the beam. This idea has been successfully used in many NLP tasks and improves the performance over the state-of-art NLP systems (Huang and Sagae, 2010; Huang et al., 2012; Zhang et al., 2013). Goldberg and Nivre (2012) propose the concept of “dynamic oracle” which is the absolute best potential of a partial derivation, and is more akin to a strictly admissible heuristic. This idea inspired and i"
D14-1209,P13-1110,0,0.510575,"envelope calculation for exact line search (see (Och, 2003)) in M ERT is less efficient than the update based on (sub-)gradient with inexact line search in M IRA and P RO. One possible solution to speed up SA-M ERT is the parallelization but we leave it for future work. 6 Related Work Many tuning methods have been proposed for SMT so far. These methods differ by the objective function or training mode: their objective functions are based on either evaluation-directed loss (Och, 2003; Galley and Quirk, 2011; Galley et al., 2013) or surrogate loss (Hopkins and May, 2011; Gimpel and Smith, 2012; Eidelman et al., 2013); they are either batch (Och, 2003; Hopkins and May, 2011; Cherry and Foster, 2012) or online mode (Watanabe, 2012; Simianer et al., 2012; Flanigan et al., 2013; Green et al., 2013). These methods share a common characteristic: they learn a weight by iteratively reranking a set of complete translations represented by k-best (Och, 2003; Watanabe et al., 2007; Chiang et al., 2008) or lattice (hypergraph) (Tromble et al., 2008; Kumar et al., 2009), and they do not care about search errors that potential partial translations may be pruned during decoding, even if they agree with 1950 that their de"
D14-1209,N13-1025,0,0.133581,"and P RO. One possible solution to speed up SA-M ERT is the parallelization but we leave it for future work. 6 Related Work Many tuning methods have been proposed for SMT so far. These methods differ by the objective function or training mode: their objective functions are based on either evaluation-directed loss (Och, 2003; Galley and Quirk, 2011; Galley et al., 2013) or surrogate loss (Hopkins and May, 2011; Gimpel and Smith, 2012; Eidelman et al., 2013); they are either batch (Och, 2003; Hopkins and May, 2011; Cherry and Foster, 2012) or online mode (Watanabe, 2012; Simianer et al., 2012; Flanigan et al., 2013; Green et al., 2013). These methods share a common characteristic: they learn a weight by iteratively reranking a set of complete translations represented by k-best (Och, 2003; Watanabe et al., 2007; Chiang et al., 2008) or lattice (hypergraph) (Tromble et al., 2008; Kumar et al., 2009), and they do not care about search errors that potential partial translations may be pruned during decoding, even if they agree with 1950 that their decoders are built on the beam pruning based search. On the other hand, it is well-known that search errors can undermine the standard training for many beam sear"
D14-1209,D11-1004,0,0.117729,"-M ERT is much slower than M ERT. The main reason is that, as the training examples increase dramatically, the envelope calculation for exact line search (see (Och, 2003)) in M ERT is less efficient than the update based on (sub-)gradient with inexact line search in M IRA and P RO. One possible solution to speed up SA-M ERT is the parallelization but we leave it for future work. 6 Related Work Many tuning methods have been proposed for SMT so far. These methods differ by the objective function or training mode: their objective functions are based on either evaluation-directed loss (Och, 2003; Galley and Quirk, 2011; Galley et al., 2013) or surrogate loss (Hopkins and May, 2011; Gimpel and Smith, 2012; Eidelman et al., 2013); they are either batch (Och, 2003; Hopkins and May, 2011; Cherry and Foster, 2012) or online mode (Watanabe, 2012; Simianer et al., 2012; Flanigan et al., 2013; Green et al., 2013). These methods share a common characteristic: they learn a weight by iteratively reranking a set of complete translations represented by k-best (Och, 2003; Watanabe et al., 2007; Chiang et al., 2008) or lattice (hypergraph) (Tromble et al., 2008; Kumar et al., 2009), and they do not care about search error"
D14-1209,D13-1201,0,0.449519,"an M ERT. The main reason is that, as the training examples increase dramatically, the envelope calculation for exact line search (see (Och, 2003)) in M ERT is less efficient than the update based on (sub-)gradient with inexact line search in M IRA and P RO. One possible solution to speed up SA-M ERT is the parallelization but we leave it for future work. 6 Related Work Many tuning methods have been proposed for SMT so far. These methods differ by the objective function or training mode: their objective functions are based on either evaluation-directed loss (Och, 2003; Galley and Quirk, 2011; Galley et al., 2013) or surrogate loss (Hopkins and May, 2011; Gimpel and Smith, 2012; Eidelman et al., 2013); they are either batch (Och, 2003; Hopkins and May, 2011; Cherry and Foster, 2012) or online mode (Watanabe, 2012; Simianer et al., 2012; Flanigan et al., 2013; Green et al., 2013). These methods share a common characteristic: they learn a weight by iteratively reranking a set of complete translations represented by k-best (Och, 2003; Watanabe et al., 2007; Chiang et al., 2008) or lattice (hypergraph) (Tromble et al., 2008; Kumar et al., 2009), and they do not care about search errors that potential parti"
D14-1209,E14-1002,0,0.019705,"rrect solution falls out of the beam. This idea has been successfully used in many NLP tasks and improves the performance over the state-of-art NLP systems (Huang and Sagae, 2010; Huang et al., 2012; Zhang et al., 2013). Goldberg and Nivre (2012) propose the concept of “dynamic oracle” which is the absolute best potential of a partial derivation, and is more akin to a strictly admissible heuristic. This idea inspired and is closely related to our potential B LEU, except that in our case, computing an admissible heuristic is too costly, so our potential B LEU is more like an average potential. Gesmundo and Henderson (2014) also consider the rankings between partial translation pairs as well. However, they evaluate a partial translation through extending it to a complete translation by re-decoding, and thus they need many passes of decoding for many partial translations, while ours only need one pass of decoding for all partial translations and thus is much more efficient. In summary, our tuning framework is more general and has potential to be employed over all the state-ofart tuning methods mentioned above, even though ours is only tested on three popular methods. 7 Conclusions and Future Work We have presente"
D14-1209,N12-1023,0,0.0419101,"rease dramatically, the envelope calculation for exact line search (see (Och, 2003)) in M ERT is less efficient than the update based on (sub-)gradient with inexact line search in M IRA and P RO. One possible solution to speed up SA-M ERT is the parallelization but we leave it for future work. 6 Related Work Many tuning methods have been proposed for SMT so far. These methods differ by the objective function or training mode: their objective functions are based on either evaluation-directed loss (Och, 2003; Galley and Quirk, 2011; Galley et al., 2013) or surrogate loss (Hopkins and May, 2011; Gimpel and Smith, 2012; Eidelman et al., 2013); they are either batch (Och, 2003; Hopkins and May, 2011; Cherry and Foster, 2012) or online mode (Watanabe, 2012; Simianer et al., 2012; Flanigan et al., 2013; Green et al., 2013). These methods share a common characteristic: they learn a weight by iteratively reranking a set of complete translations represented by k-best (Och, 2003; Watanabe et al., 2007; Chiang et al., 2008) or lattice (hypergraph) (Tromble et al., 2008; Kumar et al., 2009), and they do not care about search errors that potential partial translations may be pruned during decoding, even if they agree"
D14-1209,N09-2003,0,0.0203737,"ims to promote not only the accurate translations in the final bin, but more importantly those potentially promising partial derivations in non-final bins. The key challenge, however, is how to evaluate the “promise” or “potential” of a partial derivation. In this Section, we develop two such measures, a simple “partial B LEU” (Section 3.1) and a more principled “potential B LEU” (Section 3.2). In Section 4, we will then adapt traditional tuning methods to their search-aware versions using these partial evaluation metrics. 3.1 Solution 1: Simple and Naive Partial B LEU Inspired by a trick in (Li and Khudanpur, 2009) {d ◦ r |d ∈ Bi−j (x), |c(r) |= j})and (Chiang, 2012) for oracle or hope extraction, j=1..i we use a very simple metric to evaluate partial translations for tuning. For a given derivation d, where r is a rule covering j Chinese words, and the basic idea is to evaluate the (short) partial transtopkw0 (·) returns the top k derivations according lation e(d) against the (full) reference y, but using to the current model w0 . As a special case, note a “prorated” reference length proportional to c(d) that top1w0 (S) = argmaxd∈S w0 · Φ(d), so which is the number of Chinese words covered so top1w0 (B|"
D14-1209,D13-1111,0,0.123484,"-best decoding on nist05 test set, and calculate the oracle over these two k-best lists. The oracle B LEU comparison is shown in Table 4. On nist05 test set, for M ERT the oracle B LEU is 41.1; while for SA-M ERT its oracle B LEU is 42.7, i.e. with 1.6 B LEU improvements. Although search-aware tuning employs the objective different from the objective of evaluation on nist02 tuning set, it still gains 0.5 B LEU improvements. Diversity. A k-best list with higher diversity can better represent the entire decoding space, and thus tuning on such a k-best list may lead to better tesing performance (Gimpel et al., 2013). Intuitively, tuning with all bins will encourage the diversity in prefix, infix and suffix of complete translations in the final bin. To testify this, we need a diversity metric. Indeed, Gimpel et al. (2013) define a diversity metric based on the n-gram matches between two sentences y and y 0 as follows: 1948 0 d(y, y ) = − |y|−q |y 0 |−q X X i=1 j=1 0 [[yi:i+q = yj:j+q ]] Methods M ERT SA-M ERTpot M AX F ORCE M AX F ORCE M ERT SA-M ERTpot set nist02 nist02 nist02-px train-r-part nist02-r nist02-r tuning set # refs # sents 4 878 4 878 1 434 1 1225 1 92 1 92 # words 23181 23181 6227 22684 117"
D14-1209,P13-1031,0,0.180276,"r derivations are defined in Equations 10–13, and we define ∆δy (d1 , d2 ) = δy (d1 ) − δy (d2 ), and ∆δyx (d1 , d2 ) = δyx (d1 ) − δyx (d2 ). In addition, [θ]+ = max{θ, 0}. and fear derivations from the final bin to all bins: d+ i (x, y) = argmax w0 · Φ(d) − δy (d) (12) d∈ Bi (x) d− i (x, y) = argmax w0 · Φ(d) + δy (d) (13) d∈ Bi (x) The new loss function for SA-M IRA is Eq. 7 in Figure 4. Now instead of one update per sentence, we will perform |x |updates, each based on a pair − d+ i (x, y) and di (x, y). 4.3 From P RO to Search-Aware P RO Finally, the P RO algorithm (Hopkins and May, 2011; Green et al., 2013) aims to correlate the ranking under model score and the ranking under B LEU score, among all complete derivations in the final bin. For each preference-pair d1 , d2 ∈ B|x |(x) such that d1 has a higher B LEU score than d2 (i.e., δy (d1 ) &lt; δy (d2 )), we add one positive example Φ(d1 ) − Φ(d2 ) and one negative example Φ(d2 ) − Φ(d1 ). Now to adapt it to search-aware P RO (SAP RO), we will have many more examples to consider: besides the final bin, we will include all preference-pairs in the non-final bins as well. For each bin Bi (x), for each preference-pairs d1 , d2 ∈ Bi (x) such that d1 ha"
D14-1209,D11-1125,0,0.597913,"g methods, and extensive experiments on Chinese-to-English and English-to-Chinese translation show up to +2.6 B LEU gains over search-agnostic baselines. 1 2 3 4 (a) (b) Figure 1: (a) Some potentially promising partial translations (in red) fall out of the beam (bin 2); (b) We identify such partial translations and assign them higher model scores so that they are more likely to survive the search. 1 Introduction Parameter tuning has been a key problem for machine translation since the statistical revolution. However, most existing tuning algorithms treat the decoder as a black box (Och, 2003; Hopkins and May, 2011; Chiang, 2012), ignoring the fact that many potentially promising partial translations are pruned by the decoder due to the prohibitively large search space. For example, the popular beam-search decoding algorithm for phrase-based MT (Koehn, 2004) only explores O(nb) items for a sentence of n words (with a beam width of b), while the full search space is O(2n n2 ) or worse (Knight, 1999). As one of the very few exceptions to the “search-agnostic” majority, Yu et al. (2013) and Zhao et al. (2014) propose a variant of the perceptron algorithm that learns to keep the reference translations in th"
D14-1209,W05-1506,1,0.697366,"w0 . As a special case, note a “prorated” reference length proportional to c(d) that top1w0 (S) = argmaxd∈S w0 · Φ(d), so which is the number of Chinese words covered so top1w0 (B|x |(x)) is the final 1-best result.1 See Fig- far in d: ure 2 for an illustration. |y |· |c(d)|/|x| Bi (x) = topkw0 ( [ 3 Challenge: Evaluating Partial Derivations As mentioned in Section 1, the current mainstream tuning methods such as M ERT, M IRA, and P RO are 1 Actually B|x |(x) is an approximation to the k-best list since some derivations are merged by dynamic programming; to recover those we can use Alg. 3 of Huang and Chiang (2005). For example, if d has covered 2 words on a 8word Chinese sentence with a 12-word English reference, then the “effective reference length” is 12 × 2/8 = 3. We call this method “partial B LEU” since it does not complete the translation, and denote it by  δ¯y|x |(d) = −δ y, e(d); reflen = |y |· |c(d)|/|x |. (1) 1943 δ(y, y 0 ) = −Bleu+1 (y, y 0 ) string distance metric δy (d) = δ(y, e(d)) ( |x| δ¯y (d) x δy (d) = δ(y, e¯x (d)) x= full derivations eval reordering e¯x (d) = partial bleu (Sec. 3.1) potential bleu (Sec. 3.2) Table 1: Notations for evaluating full and partial deriva|x| tions. Funct"
D14-1209,P07-1019,1,0.85133,"ch preference-pairs d1 , d2 ∈ Bi (x) such that d1 has a higher partial or potential B LEU score than d2 (i.e., δyx (d1 ) &lt; δyx (d2 )), we add one positive example Φ(d1 ) − Φ(d2 ) and one negative example Φ(d2 ) − Φ(d1 ). In sum, searchaware P RO has |x |times more examples than traditional P RO. The loss functions of P RO and searchaware P RO are defined in Figure 4. 5 Experiments We evaluate our new tuning methods on two large scale NIST translation tasks: Chinese-to-English (C H -E N) and English-to-Chinese (E N -C H) tasks. 5.1 System Preparation and Data We base our experiments on Cubit2 (Huang and Chiang, 2007), a state-of-art phrase-based system in Python. We set phrase-limit to 7, beam size to 30 and distortion limit 6. We use the 11 dense features from Moses (Koehn et al., 2007), which can lead to good performance and are widely used in almost all SMT systems. The baseline tuning methods M ERT (Och, 2003), M IRA (Cherry and Foster, 2012), and P RO (Hopkins and May, 2011) are from the Moses toolkit, which are batch tuning methods based on k-best translations. The searchaware tuning methods are called SA-M ERT, SAM IRA, and SA-P RO, respectively. Their partial B LEU versions are marked with supersc"
D14-1209,N12-1015,1,0.872703,"These methods share a common characteristic: they learn a weight by iteratively reranking a set of complete translations represented by k-best (Och, 2003; Watanabe et al., 2007; Chiang et al., 2008) or lattice (hypergraph) (Tromble et al., 2008; Kumar et al., 2009), and they do not care about search errors that potential partial translations may be pruned during decoding, even if they agree with 1950 that their decoders are built on the beam pruning based search. On the other hand, it is well-known that search errors can undermine the standard training for many beam search based NLP systems (Huang et al., 2012). As a result, Collins and Roark (2004) and Huang et al. (2012) propose the early-update and max-violation update to deal with the search errors. Their idea is to update on prefix or partial hypotheses when the correct solution falls out of the beam. This idea has been successfully used in many NLP tasks and improves the performance over the state-of-art NLP systems (Huang and Sagae, 2010; Huang et al., 2012; Zhang et al., 2013). Goldberg and Nivre (2012) propose the concept of “dynamic oracle” which is the absolute best potential of a partial derivation, and is more akin to a strictly admissi"
D14-1209,J99-4005,0,0.0205655,". 1 Introduction Parameter tuning has been a key problem for machine translation since the statistical revolution. However, most existing tuning algorithms treat the decoder as a black box (Och, 2003; Hopkins and May, 2011; Chiang, 2012), ignoring the fact that many potentially promising partial translations are pruned by the decoder due to the prohibitively large search space. For example, the popular beam-search decoding algorithm for phrase-based MT (Koehn, 2004) only explores O(nb) items for a sentence of n words (with a beam width of b), while the full search space is O(2n n2 ) or worse (Knight, 1999). As one of the very few exceptions to the “search-agnostic” majority, Yu et al. (2013) and Zhao et al. (2014) propose a variant of the perceptron algorithm that learns to keep the reference translations in the beam or chart. However, there are several obstacles that prevent their method from becoming popular: First of all, they rely on “forced decoding” to track gold derivations that lead to the reference translation, but in practice only a small portion of (mostly very short) sentence pairs have at least one such derivation. Secondly, they learn the model on the training set, and while this"
D14-1209,P07-2045,0,0.0151024,"and one negative example Φ(d2 ) − Φ(d1 ). In sum, searchaware P RO has |x |times more examples than traditional P RO. The loss functions of P RO and searchaware P RO are defined in Figure 4. 5 Experiments We evaluate our new tuning methods on two large scale NIST translation tasks: Chinese-to-English (C H -E N) and English-to-Chinese (E N -C H) tasks. 5.1 System Preparation and Data We base our experiments on Cubit2 (Huang and Chiang, 2007), a state-of-art phrase-based system in Python. We set phrase-limit to 7, beam size to 30 and distortion limit 6. We use the 11 dense features from Moses (Koehn et al., 2007), which can lead to good performance and are widely used in almost all SMT systems. The baseline tuning methods M ERT (Och, 2003), M IRA (Cherry and Foster, 2012), and P RO (Hopkins and May, 2011) are from the Moses toolkit, which are batch tuning methods based on k-best translations. The searchaware tuning methods are called SA-M ERT, SAM IRA, and SA-P RO, respectively. Their partial B LEU versions are marked with superscript 1 and their potential B LEU versions are marked with superscript 2 , as explained in Section 3. All these search-aware tuning methods are implemented on the basis of Mos"
D14-1209,P13-2003,0,0.194064,"reported are average of three runs, as suggested by Clark et al. (2011) for fairer comparisons. 5.2 Main Results on C H -E N Task Table 2 depicts the main results of our methods on C H -E N translation task. On all five test sets, our methods consistently achieve substantial improvements with two pruning options: SA-M ERT pot gains +1.2 B LEU points over M ERT on average; and SA-M IRApot gains +1.8 B LEU points over M IRA on average as well. SA-P RO pot , however, does not work out of the box when we use the entire nist02 as the tuning set, which might be attributed to the “Monster” behavior (Nakov et al., 2013). To alleviate this problem, we only use the 109 short sentences with less than 10 words from nist02 as our new tuning data. To our supurise, this trick works really well (despite using much less data), and also made SA-P ROpot an order of magnitude faster. This further confirms that our search-aware tuning is consistent across all tuning methods and datasets. As discussed in Section 3, evaluation metrics of partial derivations are crucial for search-aware tuning. Besides the principled “potential B LEU” version of search-aware tuning (i.e. SA-M ERTpot , SA-M IRApot , and SA-P RO pot ), we als"
D14-1209,J03-1002,0,0.0118156,"-E N and E N -C H tasks is the same, and it is collected from the NIST2008 Open Machine Translation Campaign. It consists of about 1.8M sentence pairs, including about 40M/48M words in Chinese/English sides. For C H -E N task, the tuning set is nist02 (878 sents), and test sets are nist03 (919 sents), nist04 (1788 sents), nist05 (1082 sents), nist06 (616 sents from news portion) and nist08 (691 from news portion). For E N -C H task, the tuning set is ssmt07 (995 sents)3 , and the test set is nist08 (1859 sents). For both tasks, all the tuning and test sets contain 4 references. We use GIZA++ (Och and Ney, 2003) for word alignment, and SRILM (Stolcke, 2002) for 4-gram language models with the Kneser-Ney smoothing 3 On E N -C H task, there is only one test set available for us, and thus we use ssmt07 as the tuning set, which is provided at the Third Symposium on Statistical Machine Translation (http://mitlab.hit.edu.cn/ssmt2007.html). option. The LM for E N -C H is trained on its target side; and that for C H -E N is trained on the Xinhua portion of Gigaword. We use B LEU-4 (Papineni et al., 2002) with “average ref-len” to evaluate the translation performance for all experiments. In particular, the ch"
D14-1209,P03-1021,0,0.737286,"ioned tuning methods, and extensive experiments on Chinese-to-English and English-to-Chinese translation show up to +2.6 B LEU gains over search-agnostic baselines. 1 2 3 4 (a) (b) Figure 1: (a) Some potentially promising partial translations (in red) fall out of the beam (bin 2); (b) We identify such partial translations and assign them higher model scores so that they are more likely to survive the search. 1 Introduction Parameter tuning has been a key problem for machine translation since the statistical revolution. However, most existing tuning algorithms treat the decoder as a black box (Och, 2003; Hopkins and May, 2011; Chiang, 2012), ignoring the fact that many potentially promising partial translations are pruned by the decoder due to the prohibitively large search space. For example, the popular beam-search decoding algorithm for phrase-based MT (Koehn, 2004) only explores O(nb) items for a sentence of n words (with a beam width of b), while the full search space is O(2n n2 ) or worse (Knight, 1999). As one of the very few exceptions to the “search-agnostic” majority, Yu et al. (2013) and Zhao et al. (2014) propose a variant of the perceptron algorithm that learns to keep the refer"
D14-1209,P02-1040,0,0.10148,"12 × 2/8 = 3. We call this method “partial B LEU” since it does not complete the translation, and denote it by  δ¯y|x |(d) = −δ y, e(d); reflen = |y |· |c(d)|/|x |. (1) 1943 δ(y, y 0 ) = −Bleu+1 (y, y 0 ) string distance metric δy (d) = δ(y, e(d)) ( |x| δ¯y (d) x δy (d) = δ(y, e¯x (d)) x= full derivations eval reordering e¯x (d) = partial bleu (Sec. 3.1) potential bleu (Sec. 3.2) Table 1: Notations for evaluating full and partial deriva|x| tions. Functions δ¯y (·) and e¯x (·) are defined by Equations 1 and 3, respectively. where reflen is the effective length of reference translations, see (Papineni et al., 2002) for details. 3.1.1 Problem with Partial BLEU Simple as it is, this method does not work well in practice because comparison of partial derivations might be unfair for different derivations covering different set of Chinese words, as it will naturally favor those covering “easier” portions of the input sentence (which we do observe empirically). For instance, consider the following Chinese-toEnglish example which involves a reordering of the Chinese PP: (2) wˇo c´ong Sh`anghˇai f¯ei d`ao Bˇeij¯ıng I from Shanghai fly to Beijing “I flew from Shanghai to Beijing” Partial B LEU will prefer subtra"
D14-1209,P12-1002,0,0.0396503,"ct line search in M IRA and P RO. One possible solution to speed up SA-M ERT is the parallelization but we leave it for future work. 6 Related Work Many tuning methods have been proposed for SMT so far. These methods differ by the objective function or training mode: their objective functions are based on either evaluation-directed loss (Och, 2003; Galley and Quirk, 2011; Galley et al., 2013) or surrogate loss (Hopkins and May, 2011; Gimpel and Smith, 2012; Eidelman et al., 2013); they are either batch (Och, 2003; Hopkins and May, 2011; Cherry and Foster, 2012) or online mode (Watanabe, 2012; Simianer et al., 2012; Flanigan et al., 2013; Green et al., 2013). These methods share a common characteristic: they learn a weight by iteratively reranking a set of complete translations represented by k-best (Och, 2003; Watanabe et al., 2007; Chiang et al., 2008) or lattice (hypergraph) (Tromble et al., 2008; Kumar et al., 2009), and they do not care about search errors that potential partial translations may be pruned during decoding, even if they agree with 1950 that their decoders are built on the beam pruning based search. On the other hand, it is well-known that search errors can undermine the standard trai"
D14-1209,D08-1065,0,0.0705458,"ns are based on either evaluation-directed loss (Och, 2003; Galley and Quirk, 2011; Galley et al., 2013) or surrogate loss (Hopkins and May, 2011; Gimpel and Smith, 2012; Eidelman et al., 2013); they are either batch (Och, 2003; Hopkins and May, 2011; Cherry and Foster, 2012) or online mode (Watanabe, 2012; Simianer et al., 2012; Flanigan et al., 2013; Green et al., 2013). These methods share a common characteristic: they learn a weight by iteratively reranking a set of complete translations represented by k-best (Och, 2003; Watanabe et al., 2007; Chiang et al., 2008) or lattice (hypergraph) (Tromble et al., 2008; Kumar et al., 2009), and they do not care about search errors that potential partial translations may be pruned during decoding, even if they agree with 1950 that their decoders are built on the beam pruning based search. On the other hand, it is well-known that search errors can undermine the standard training for many beam search based NLP systems (Huang et al., 2012). As a result, Collins and Roark (2004) and Huang et al. (2012) propose the early-update and max-violation update to deal with the search errors. Their idea is to update on prefix or partial hypotheses when the correct solutio"
D14-1209,D07-1080,0,0.552656,"rmally, its objective function is defined as follows: `SA-M ERT(D, w) = M L hx,yi∈D i=1..|x| where top1w (·) is defined in Eq. (4), and δyx (d), defined in Table 1, is the generic metric for evaluating a partial derivation d which has two implementations (partial bleu or potential bleu). In order words we can obtain two implementations of search-aware M ERT methods, SA-M ERT par and SA-M ERTpot . Notice that the traditional M ERT is a special case of SA-M ERT where i is fixed to |x|. 4.2 From M IRA to Search-Aware M IRA M IRA is another popular tuning method for SMT. It firstly introduced in (Watanabe et al., 2007), and then was improved in (Chiang et al., 2008; Chiang, 2012; Cherry and Foster, 2012). Its main idea is to optimize a weight such that the model score difference of a pair of derivations is greater than their loss difference. In this paper, we follow the objective function in (Chiang, 2012; Cherry and Foster, 2012), where only the violation between hope and fear derivations is concerned. Formally, we define d+ (x, y) and d− (x, y) as the hope and fear derivations in the final bin (i.e., complete derivations): d+ (x, y) = argmax w0 · Φ(d) − δy (d) (10) d∈B|x |(x) − d (x, y) = argmax w0 · Φ(d)"
D14-1209,N12-1026,0,0.108006,"dient with inexact line search in M IRA and P RO. One possible solution to speed up SA-M ERT is the parallelization but we leave it for future work. 6 Related Work Many tuning methods have been proposed for SMT so far. These methods differ by the objective function or training mode: their objective functions are based on either evaluation-directed loss (Och, 2003; Galley and Quirk, 2011; Galley et al., 2013) or surrogate loss (Hopkins and May, 2011; Gimpel and Smith, 2012; Eidelman et al., 2013); they are either batch (Och, 2003; Hopkins and May, 2011; Cherry and Foster, 2012) or online mode (Watanabe, 2012; Simianer et al., 2012; Flanigan et al., 2013; Green et al., 2013). These methods share a common characteristic: they learn a weight by iteratively reranking a set of complete translations represented by k-best (Och, 2003; Watanabe et al., 2007; Chiang et al., 2008) or lattice (hypergraph) (Tromble et al., 2008; Kumar et al., 2009), and they do not care about search errors that potential partial translations may be pruned during decoding, even if they agree with 1950 that their decoders are built on the beam pruning based search. On the other hand, it is well-known that search errors can unde"
D14-1209,D13-1112,1,0.94509,"ce the statistical revolution. However, most existing tuning algorithms treat the decoder as a black box (Och, 2003; Hopkins and May, 2011; Chiang, 2012), ignoring the fact that many potentially promising partial translations are pruned by the decoder due to the prohibitively large search space. For example, the popular beam-search decoding algorithm for phrase-based MT (Koehn, 2004) only explores O(nb) items for a sentence of n words (with a beam width of b), while the full search space is O(2n n2 ) or worse (Knight, 1999). As one of the very few exceptions to the “search-agnostic” majority, Yu et al. (2013) and Zhao et al. (2014) propose a variant of the perceptron algorithm that learns to keep the reference translations in the beam or chart. However, there are several obstacles that prevent their method from becoming popular: First of all, they rely on “forced decoding” to track gold derivations that lead to the reference translation, but in practice only a small portion of (mostly very short) sentence pairs have at least one such derivation. Secondly, they learn the model on the training set, and while this does enable a sparse feature set, it is orders of magnitude slower compared to M ERT an"
D14-1209,koen-2004-pharaoh,0,0.704851,"beam (bin 2); (b) We identify such partial translations and assign them higher model scores so that they are more likely to survive the search. 1 Introduction Parameter tuning has been a key problem for machine translation since the statistical revolution. However, most existing tuning algorithms treat the decoder as a black box (Och, 2003; Hopkins and May, 2011; Chiang, 2012), ignoring the fact that many potentially promising partial translations are pruned by the decoder due to the prohibitively large search space. For example, the popular beam-search decoding algorithm for phrase-based MT (Koehn, 2004) only explores O(nb) items for a sentence of n words (with a beam width of b), while the full search space is O(2n n2 ) or worse (Knight, 1999). As one of the very few exceptions to the “search-agnostic” majority, Yu et al. (2013) and Zhao et al. (2014) propose a variant of the perceptron algorithm that learns to keep the reference translations in the beam or chart. However, there are several obstacles that prevent their method from becoming popular: First of all, they rely on “forced decoding” to track gold derivations that lead to the reference translation, but in practice only a small porti"
D14-1209,D13-1093,1,0.846073,"lt on the beam pruning based search. On the other hand, it is well-known that search errors can undermine the standard training for many beam search based NLP systems (Huang et al., 2012). As a result, Collins and Roark (2004) and Huang et al. (2012) propose the early-update and max-violation update to deal with the search errors. Their idea is to update on prefix or partial hypotheses when the correct solution falls out of the beam. This idea has been successfully used in many NLP tasks and improves the performance over the state-of-art NLP systems (Huang and Sagae, 2010; Huang et al., 2012; Zhang et al., 2013). Goldberg and Nivre (2012) propose the concept of “dynamic oracle” which is the absolute best potential of a partial derivation, and is more akin to a strictly admissible heuristic. This idea inspired and is closely related to our potential B LEU, except that in our case, computing an admissible heuristic is too costly, so our potential B LEU is more like an average potential. Gesmundo and Henderson (2014) also consider the rankings between partial translation pairs as well. However, they evaluate a partial translation through extending it to a complete translation by re-decoding, and thus th"
D14-1209,P09-1019,0,0.17176,"evaluation-directed loss (Och, 2003; Galley and Quirk, 2011; Galley et al., 2013) or surrogate loss (Hopkins and May, 2011; Gimpel and Smith, 2012; Eidelman et al., 2013); they are either batch (Och, 2003; Hopkins and May, 2011; Cherry and Foster, 2012) or online mode (Watanabe, 2012; Simianer et al., 2012; Flanigan et al., 2013; Green et al., 2013). These methods share a common characteristic: they learn a weight by iteratively reranking a set of complete translations represented by k-best (Och, 2003; Watanabe et al., 2007; Chiang et al., 2008) or lattice (hypergraph) (Tromble et al., 2008; Kumar et al., 2009), and they do not care about search errors that potential partial translations may be pruned during decoding, even if they agree with 1950 that their decoders are built on the beam pruning based search. On the other hand, it is well-known that search errors can undermine the standard training for many beam search based NLP systems (Huang et al., 2012). As a result, Collins and Roark (2004) and Huang et al. (2012) propose the early-update and max-violation update to deal with the search errors. Their idea is to update on prefix or partial hypotheses when the correct solution falls out of the be"
D14-1209,P14-2127,1,0.832957,"volution. However, most existing tuning algorithms treat the decoder as a black box (Och, 2003; Hopkins and May, 2011; Chiang, 2012), ignoring the fact that many potentially promising partial translations are pruned by the decoder due to the prohibitively large search space. For example, the popular beam-search decoding algorithm for phrase-based MT (Koehn, 2004) only explores O(nb) items for a sentence of n words (with a beam width of b), while the full search space is O(2n n2 ) or worse (Knight, 1999). As one of the very few exceptions to the “search-agnostic” majority, Yu et al. (2013) and Zhao et al. (2014) propose a variant of the perceptron algorithm that learns to keep the reference translations in the beam or chart. However, there are several obstacles that prevent their method from becoming popular: First of all, they rely on “forced decoding” to track gold derivations that lead to the reference translation, but in practice only a small portion of (mostly very short) sentence pairs have at least one such derivation. Secondly, they learn the model on the training set, and while this does enable a sparse feature set, it is orders of magnitude slower compared to M ERT and P RO. We instead prop"
D14-1209,Q13-1033,0,\N,Missing
D14-1209,P10-1110,1,\N,Missing
D17-1155,D11-1033,0,0.732683,"two instance weighting technologies, i.e., sentence weighting and domain weighting with a dynamic weight learning strategy, are proposed for NMT domain adaptation. Empirical results on the IWSLT EnglishGerman/French tasks show that the proposed methods can substantially improve NMT performance by up to 2.7-6.7 BLEU points, outperforming the existing baselines by up to 1.6-3.6 BLEU points. 1 Introduction In Statistical Machine Translation (SMT), unrelated additional corpora, known as out-ofdomain corpora, have been shown not to benefit some domains and tasks, such as TED-talks and IWSLT tasks (Axelrod et al., 2011; Luong and Manning, 2015). Several Phrase-based SMT (PBSMT) domain adaptation methods have been proposed to overcome this problem of the lack of substantial data in some specific domains and languages: i) Data selection. The main idea is to score the out-of-domain data using models trained from the in-domain and out-of-domain data, respectively. Then select training data by using these ranked scores (Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013; Hoang and Sima’an, 2014a,b; Durrani et al., 2015; Chen et al., 2016). ii) Model Linear Interpolation. Several PBSMT models, such as"
D17-1155,2015.iwslt-evaluation.1,0,0.0573695,"Missing"
D17-1155,2016.amta-researchers.8,0,0.201897,"it some domains and tasks, such as TED-talks and IWSLT tasks (Axelrod et al., 2011; Luong and Manning, 2015). Several Phrase-based SMT (PBSMT) domain adaptation methods have been proposed to overcome this problem of the lack of substantial data in some specific domains and languages: i) Data selection. The main idea is to score the out-of-domain data using models trained from the in-domain and out-of-domain data, respectively. Then select training data by using these ranked scores (Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013; Hoang and Sima’an, 2014a,b; Durrani et al., 2015; Chen et al., 2016). ii) Model Linear Interpolation. Several PBSMT models, such as language models, translation models, and reordering models, individually corresponding to each corpus, are trained. These models are then combined to achieve the best performance (Sennrich, 2012; Sennrich et al., 2013; Durrani et al., 2015, 2016; Imamura and Sumita, 2016). iii) Instance Weighting. Instance Weighting has been applied to several NLP domain adaptation tasks (Jiang and Zhai, 2007), such as POS tagging, entity type classification and especially PBSMT (Matsoukas et al., 2009; Shah et al., 2010; Foster et al., 2010; Rous"
D17-1155,P17-2061,0,0.26518,"anslation (NMT) domain adaptation, the sentence selection can also be used (Chen et al., 2016; Wang et al., 2017). Meanwhile, the model linear interpolation is not easily applied to NMT directly, because NMT is not a linear model. There are two methods for model combination of NMT: i) the in-domain model and out-of-domain model can be ensembled (Jean et al., 2015). ii) an NMT further training (fine-tuning) method (Luong and Manning, 2015). The training is performed in two steps: first, the NMT system is trained using out-of-domain data, and then further trained using in-domain data. Recently, Chu et al. (2017) make an empirical comparison of NMT further training (Luong and Manning, 2015) and domain control (Kobus et al., 2016), which applied word-level domain features to word embedding layer. This approach provides natural baselines for comparison. To the best of our knowledge, there is no existing work concerning instance weighting in 1482 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1482–1488 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics NMT. The main challenge is that NMT is not a liner model or combin"
D17-1155,P13-2119,0,0.0568442,"corpora, known as out-ofdomain corpora, have been shown not to benefit some domains and tasks, such as TED-talks and IWSLT tasks (Axelrod et al., 2011; Luong and Manning, 2015). Several Phrase-based SMT (PBSMT) domain adaptation methods have been proposed to overcome this problem of the lack of substantial data in some specific domains and languages: i) Data selection. The main idea is to score the out-of-domain data using models trained from the in-domain and out-of-domain data, respectively. Then select training data by using these ranked scores (Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013; Hoang and Sima’an, 2014a,b; Durrani et al., 2015; Chen et al., 2016). ii) Model Linear Interpolation. Several PBSMT models, such as language models, translation models, and reordering models, individually corresponding to each corpus, are trained. These models are then combined to achieve the best performance (Sennrich, 2012; Sennrich et al., 2013; Durrani et al., 2015, 2016; Imamura and Sumita, 2016). iii) Instance Weighting. Instance Weighting has been applied to several NLP domain adaptation tasks (Jiang and Zhai, 2007), such as POS tagging, entity type classification and especially PBSMT"
D17-1155,C16-1299,0,0.0285091,"Missing"
D17-1155,2015.mtsummit-papers.10,0,0.103141,"een shown not to benefit some domains and tasks, such as TED-talks and IWSLT tasks (Axelrod et al., 2011; Luong and Manning, 2015). Several Phrase-based SMT (PBSMT) domain adaptation methods have been proposed to overcome this problem of the lack of substantial data in some specific domains and languages: i) Data selection. The main idea is to score the out-of-domain data using models trained from the in-domain and out-of-domain data, respectively. Then select training data by using these ranked scores (Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013; Hoang and Sima’an, 2014a,b; Durrani et al., 2015; Chen et al., 2016). ii) Model Linear Interpolation. Several PBSMT models, such as language models, translation models, and reordering models, individually corresponding to each corpus, are trained. These models are then combined to achieve the best performance (Sennrich, 2012; Sennrich et al., 2013; Durrani et al., 2015, 2016; Imamura and Sumita, 2016). iii) Instance Weighting. Instance Weighting has been applied to several NLP domain adaptation tasks (Jiang and Zhai, 2007), such as POS tagging, entity type classification and especially PBSMT (Matsoukas et al., 2009; Shah et al., 2010; Foste"
D17-1155,D10-1044,0,0.0589565,"2015; Chen et al., 2016). ii) Model Linear Interpolation. Several PBSMT models, such as language models, translation models, and reordering models, individually corresponding to each corpus, are trained. These models are then combined to achieve the best performance (Sennrich, 2012; Sennrich et al., 2013; Durrani et al., 2015, 2016; Imamura and Sumita, 2016). iii) Instance Weighting. Instance Weighting has been applied to several NLP domain adaptation tasks (Jiang and Zhai, 2007), such as POS tagging, entity type classification and especially PBSMT (Matsoukas et al., 2009; Shah et al., 2010; Foster et al., 2010; Rousseau et al., 2011; Zhou et al., 2015; Wang et al., 2016; Imamura and Sumita, 2016). They firstly score each instance/domain by using rules or statistical methods as a weight, and then train PBSMT models by giving each instance/domain the weight. For Neural Machine Translation (NMT) domain adaptation, the sentence selection can also be used (Chen et al., 2016; Wang et al., 2017). Meanwhile, the model linear interpolation is not easily applied to NMT directly, because NMT is not a linear model. There are two methods for model combination of NMT: i) the in-domain model and out-of-domain mod"
D17-1155,D14-1062,0,0.0330635,"Missing"
D17-1155,C14-1182,0,0.0385476,"Missing"
D17-1155,P07-1034,0,0.35209,"by using these ranked scores (Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013; Hoang and Sima’an, 2014a,b; Durrani et al., 2015; Chen et al., 2016). ii) Model Linear Interpolation. Several PBSMT models, such as language models, translation models, and reordering models, individually corresponding to each corpus, are trained. These models are then combined to achieve the best performance (Sennrich, 2012; Sennrich et al., 2013; Durrani et al., 2015, 2016; Imamura and Sumita, 2016). iii) Instance Weighting. Instance Weighting has been applied to several NLP domain adaptation tasks (Jiang and Zhai, 2007), such as POS tagging, entity type classification and especially PBSMT (Matsoukas et al., 2009; Shah et al., 2010; Foster et al., 2010; Rousseau et al., 2011; Zhou et al., 2015; Wang et al., 2016; Imamura and Sumita, 2016). They firstly score each instance/domain by using rules or statistical methods as a weight, and then train PBSMT models by giving each instance/domain the weight. For Neural Machine Translation (NMT) domain adaptation, the sentence selection can also be used (Chen et al., 2016; Wang et al., 2017). Meanwhile, the model linear interpolation is not easily applied to NMT directl"
D17-1155,kobus-etal-2017-domain,0,0.0736135,"Missing"
D17-1155,W04-3250,0,0.163481,"Missing"
D17-1155,P07-2045,0,0.024644,"Missing"
D17-1155,2015.iwslt-evaluation.11,0,0.461478,"technologies, i.e., sentence weighting and domain weighting with a dynamic weight learning strategy, are proposed for NMT domain adaptation. Empirical results on the IWSLT EnglishGerman/French tasks show that the proposed methods can substantially improve NMT performance by up to 2.7-6.7 BLEU points, outperforming the existing baselines by up to 1.6-3.6 BLEU points. 1 Introduction In Statistical Machine Translation (SMT), unrelated additional corpora, known as out-ofdomain corpora, have been shown not to benefit some domains and tasks, such as TED-talks and IWSLT tasks (Axelrod et al., 2011; Luong and Manning, 2015). Several Phrase-based SMT (PBSMT) domain adaptation methods have been proposed to overcome this problem of the lack of substantial data in some specific domains and languages: i) Data selection. The main idea is to score the out-of-domain data using models trained from the in-domain and out-of-domain data, respectively. Then select training data by using these ranked scores (Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013; Hoang and Sima’an, 2014a,b; Durrani et al., 2015; Chen et al., 2016). ii) Model Linear Interpolation. Several PBSMT models, such as language models, translati"
D17-1155,D15-1166,0,0.185744,"ies, i.e., sentence weighting and domain weighting with a dynamic weight learning strategy, are proposed for NMT domain adaptation. Empirical results on the IWSLT EnglishGerman/French tasks show that the proposed methods can substantially improve NMT performance by up to 2.7-6.7 BLEU points, outperforming the existing baselines by up to 1.6-3.6 BLEU points. 1 Introduction In Statistical Machine Translation (SMT), unrelated additional corpora, known as out-ofdomain corpora, have been shown not to benefit some domains and tasks, such as TED-talks and IWSLT tasks (Axelrod et al., 2011; Luong and Manning, 2015). Several Phrase-based SMT (PBSMT) domain adaptation methods have been proposed to overcome this problem of the lack of substantial data in some specific domains and languages: i) Data selection. The main idea is to score the out-of-domain data using models trained from the in-domain and out-of-domain data, respectively. Then select training data by using these ranked scores (Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013; Hoang and Sima’an, 2014a,b; Durrani et al., 2015; Chen et al., 2016). ii) Model Linear Interpolation. Several PBSMT models, such as language models, translati"
D17-1155,D09-1074,0,0.0172604,"Missing"
D17-1155,P10-2041,0,0.135114,"hine Translation (SMT), unrelated additional corpora, known as out-ofdomain corpora, have been shown not to benefit some domains and tasks, such as TED-talks and IWSLT tasks (Axelrod et al., 2011; Luong and Manning, 2015). Several Phrase-based SMT (PBSMT) domain adaptation methods have been proposed to overcome this problem of the lack of substantial data in some specific domains and languages: i) Data selection. The main idea is to score the out-of-domain data using models trained from the in-domain and out-of-domain data, respectively. Then select training data by using these ranked scores (Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013; Hoang and Sima’an, 2014a,b; Durrani et al., 2015; Chen et al., 2016). ii) Model Linear Interpolation. Several PBSMT models, such as language models, translation models, and reordering models, individually corresponding to each corpus, are trained. These models are then combined to achieve the best performance (Sennrich, 2012; Sennrich et al., 2013; Durrani et al., 2015, 2016; Imamura and Sumita, 2016). iii) Instance Weighting. Instance Weighting has been applied to several NLP domain adaptation tasks (Jiang and Zhai, 2007), such as POS tagging, entity"
D17-1155,P02-1040,0,0.116309,"Missing"
D17-1155,2011.iwslt-evaluation.10,0,0.120719,"Missing"
D17-1155,E12-1055,0,0.0316873,"domains and languages: i) Data selection. The main idea is to score the out-of-domain data using models trained from the in-domain and out-of-domain data, respectively. Then select training data by using these ranked scores (Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013; Hoang and Sima’an, 2014a,b; Durrani et al., 2015; Chen et al., 2016). ii) Model Linear Interpolation. Several PBSMT models, such as language models, translation models, and reordering models, individually corresponding to each corpus, are trained. These models are then combined to achieve the best performance (Sennrich, 2012; Sennrich et al., 2013; Durrani et al., 2015, 2016; Imamura and Sumita, 2016). iii) Instance Weighting. Instance Weighting has been applied to several NLP domain adaptation tasks (Jiang and Zhai, 2007), such as POS tagging, entity type classification and especially PBSMT (Matsoukas et al., 2009; Shah et al., 2010; Foster et al., 2010; Rousseau et al., 2011; Zhou et al., 2015; Wang et al., 2016; Imamura and Sumita, 2016). They firstly score each instance/domain by using rules or statistical methods as a weight, and then train PBSMT models by giving each instance/domain the weight. For Neural M"
D17-1155,E17-3017,0,0.0358134,"Missing"
D17-1155,P13-1082,0,0.0411855,"uages: i) Data selection. The main idea is to score the out-of-domain data using models trained from the in-domain and out-of-domain data, respectively. Then select training data by using these ranked scores (Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013; Hoang and Sima’an, 2014a,b; Durrani et al., 2015; Chen et al., 2016). ii) Model Linear Interpolation. Several PBSMT models, such as language models, translation models, and reordering models, individually corresponding to each corpus, are trained. These models are then combined to achieve the best performance (Sennrich, 2012; Sennrich et al., 2013; Durrani et al., 2015, 2016; Imamura and Sumita, 2016). iii) Instance Weighting. Instance Weighting has been applied to several NLP domain adaptation tasks (Jiang and Zhai, 2007), such as POS tagging, entity type classification and especially PBSMT (Matsoukas et al., 2009; Shah et al., 2010; Foster et al., 2010; Rousseau et al., 2011; Zhou et al., 2015; Wang et al., 2016; Imamura and Sumita, 2016). They firstly score each instance/domain by using rules or statistical methods as a weight, and then train PBSMT models by giving each instance/domain the weight. For Neural Machine Translation (NMT"
D17-1155,W10-1759,0,0.0888214,",b; Durrani et al., 2015; Chen et al., 2016). ii) Model Linear Interpolation. Several PBSMT models, such as language models, translation models, and reordering models, individually corresponding to each corpus, are trained. These models are then combined to achieve the best performance (Sennrich, 2012; Sennrich et al., 2013; Durrani et al., 2015, 2016; Imamura and Sumita, 2016). iii) Instance Weighting. Instance Weighting has been applied to several NLP domain adaptation tasks (Jiang and Zhai, 2007), such as POS tagging, entity type classification and especially PBSMT (Matsoukas et al., 2009; Shah et al., 2010; Foster et al., 2010; Rousseau et al., 2011; Zhou et al., 2015; Wang et al., 2016; Imamura and Sumita, 2016). They firstly score each instance/domain by using rules or statistical methods as a weight, and then train PBSMT models by giving each instance/domain the weight. For Neural Machine Translation (NMT) domain adaptation, the sentence selection can also be used (Chen et al., 2016; Wang et al., 2017). Meanwhile, the model linear interpolation is not easily applied to NMT directly, because NMT is not a linear model. There are two methods for model combination of NMT: i) the in-domain model"
D17-1155,P17-2089,1,0.366616,"Instance Weighting has been applied to several NLP domain adaptation tasks (Jiang and Zhai, 2007), such as POS tagging, entity type classification and especially PBSMT (Matsoukas et al., 2009; Shah et al., 2010; Foster et al., 2010; Rousseau et al., 2011; Zhou et al., 2015; Wang et al., 2016; Imamura and Sumita, 2016). They firstly score each instance/domain by using rules or statistical methods as a weight, and then train PBSMT models by giving each instance/domain the weight. For Neural Machine Translation (NMT) domain adaptation, the sentence selection can also be used (Chen et al., 2016; Wang et al., 2017). Meanwhile, the model linear interpolation is not easily applied to NMT directly, because NMT is not a linear model. There are two methods for model combination of NMT: i) the in-domain model and out-of-domain model can be ensembled (Jean et al., 2015). ii) an NMT further training (fine-tuning) method (Luong and Manning, 2015). The training is performed in two steps: first, the NMT system is trained using out-of-domain data, and then further trained using in-domain data. Recently, Chu et al. (2017) make an empirical comparison of NMT further training (Luong and Manning, 2015) and domain contr"
D17-1155,C16-1295,1,0.845483,"ral PBSMT models, such as language models, translation models, and reordering models, individually corresponding to each corpus, are trained. These models are then combined to achieve the best performance (Sennrich, 2012; Sennrich et al., 2013; Durrani et al., 2015, 2016; Imamura and Sumita, 2016). iii) Instance Weighting. Instance Weighting has been applied to several NLP domain adaptation tasks (Jiang and Zhai, 2007), such as POS tagging, entity type classification and especially PBSMT (Matsoukas et al., 2009; Shah et al., 2010; Foster et al., 2010; Rousseau et al., 2011; Zhou et al., 2015; Wang et al., 2016; Imamura and Sumita, 2016). They firstly score each instance/domain by using rules or statistical methods as a weight, and then train PBSMT models by giving each instance/domain the weight. For Neural Machine Translation (NMT) domain adaptation, the sentence selection can also be used (Chen et al., 2016; Wang et al., 2017). Meanwhile, the model linear interpolation is not easily applied to NMT directly, because NMT is not a linear model. There are two methods for model combination of NMT: i) the in-domain model and out-of-domain model can be ensembled (Jean et al., 2015). ii) an NMT further t"
D17-1304,W09-2307,0,0.0166967,"annotation vectors H and dependency annotation vectors D. The current context vector csi and cdi are compute by eq.(4), respectively: J X … CNN yi-1 … Figure 2: SDRNMT-1 for the i-th time step. csi = … … x7 CNN ci … x6 VU2 ? i,1 ? i,1 ? ? i,2 i,2 … i,J x5 U2=&lt;x3, x1, x4, x7 , ε&gt; CNN d1 x4 (14) Experiment Setting up We carry out experiments on Chinese-to-English translation. The training dataset consists of 1.42M 2 λ can be tuned according to a subset FBIS of training data and be set as 0.6 in the experiments. sentence pairs extract from LDC corpora.3 We use the Stanford dependency parser (Chang et al., 2009) to generate the dependency tree for Chinese. We choose the NIST 2002 (MT02) and the NIST 2003-2008 (MT03-08) datasets as the validation set and test sets, respectively. Case-insensitive 4gram NIST BLEU score (Papineni et al., 2002) is used as an evaluation metric, and signtest (Collins et al., 2005) is as statistical significance test. The baseline systems include the standard Phrase-Based Statistical Machine Translation (PBSMT) implemented in Moses (Koehn et al., 2007) and the standard Attentional NMT (AttNMT) (Bahdanau et al., 2014), where only source word representation is utilized. We als"
D17-1304,P05-1066,0,0.206266,"Experiment Setting up We carry out experiments on Chinese-to-English translation. The training dataset consists of 1.42M 2 λ can be tuned according to a subset FBIS of training data and be set as 0.6 in the experiments. sentence pairs extract from LDC corpora.3 We use the Stanford dependency parser (Chang et al., 2009) to generate the dependency tree for Chinese. We choose the NIST 2002 (MT02) and the NIST 2003-2008 (MT03-08) datasets as the validation set and test sets, respectively. Case-insensitive 4gram NIST BLEU score (Papineni et al., 2002) is used as an evaluation metric, and signtest (Collins et al., 2005) is as statistical significance test. The baseline systems include the standard Phrase-Based Statistical Machine Translation (PBSMT) implemented in Moses (Koehn et al., 2007) and the standard Attentional NMT (AttNMT) (Bahdanau et al., 2014), where only source word representation is utilized. We also compare with a state-of-the-art syntax enhanced NMT method (Sennrich and Haddow, 2016). For a fair comparison, we only utilize dependency information for (Sennrich and Haddow, 2016), called Sennrich-deponly. We try our best to re-implement the baseline methods on Nematus toolkit 4 (Sennrich et al.,"
D17-1304,P14-1129,0,0.0116576,"or VUj for Uj . In our experiment, the output of the output layer is 1 × d-dimension vector. It should be noted that the dependency unit is similar to the source dependency feature of Sennrich and Haddow (2016) and the SDR is the same to the source-side representation of Chen et al. (2017). In comparison with Sennrich and Haddow (2016), who concatenate the source dependency labels and word together to enhance the Encoder of NMT, we adapt a separate attention mechanism together with a CNN dependency Encoder. Compared with Chen et al. (2017), which expands the famous neural network joint model (Devlin et al., 2014) with source dependency information to improve the phrase pair translation probability estimation for SMT, we focus on source dependency information to enhance attention probability estimation and to learn corresponding dependency context and RNN hidden state for improving translation. 4 NMT with SDR In this section, we propose two novel NMT models SDRNMT-1 and SDRNMT-2, both of which can make use of source dependency information SDR to enhance Encoder and Decoder of NMT. 4.1 greatly tackle the sparsity issues associated with large dependency units. Motivated by (Sennrich and Haddow, 2016), we"
D17-1304,P16-1078,0,0.0619476,"h translation task show that our method achieves 1.6 BLEU improvements on average over a strong NMT system. 1 Introduction Neural Machine Translation (NMT) (Kalchbrenner and Blunsom, 2013; Bahdanau et al., 2014; Sutskever et al., 2014) relies heavily on source representations, which encode implicitly semantic information of source words by neural networks (Mikolov et al., 2013a,b). Recently, several research works have been proposed to learn richer source representation, such as multisource information (Zoph and Knight, 2016; Firat et al., 2016), and particularly source syntactic information (Eriguchi et al., 2016; Li et al., 2017; Huadong et al., 2017; Eriguchi et al., 2017), thus improving the performance of NMT. In this paper, we enhance source representations by dependency information, which can capture source long-distance dependency constraints for word prediction. Actually, source dependency information has been shown greatly effective in ∗ Kehai Chen was an internship research fellow at NICT when conducting this work. † Corresponding author. Statistical Machine Translation (SMT) (Garmash and Monz, 2014; Kazemi et al., 2015; Hadiwinoto et al., 2016; Chen et al., 2017; Hadiwinoto and Ng, 2017). I"
D17-1304,P17-2012,0,0.0146372,"ovements on average over a strong NMT system. 1 Introduction Neural Machine Translation (NMT) (Kalchbrenner and Blunsom, 2013; Bahdanau et al., 2014; Sutskever et al., 2014) relies heavily on source representations, which encode implicitly semantic information of source words by neural networks (Mikolov et al., 2013a,b). Recently, several research works have been proposed to learn richer source representation, such as multisource information (Zoph and Knight, 2016; Firat et al., 2016), and particularly source syntactic information (Eriguchi et al., 2016; Li et al., 2017; Huadong et al., 2017; Eriguchi et al., 2017), thus improving the performance of NMT. In this paper, we enhance source representations by dependency information, which can capture source long-distance dependency constraints for word prediction. Actually, source dependency information has been shown greatly effective in ∗ Kehai Chen was an internship research fellow at NICT when conducting this work. † Corresponding author. Statistical Machine Translation (SMT) (Garmash and Monz, 2014; Kazemi et al., 2015; Hadiwinoto et al., 2016; Chen et al., 2017; Hadiwinoto and Ng, 2017). In NMT, there has been a quite recent preliminary exploration (S"
D17-1304,N16-1101,0,0.0209728,"cially on long sentences. Empirical results on NIST Chinese-toEnglish translation task show that our method achieves 1.6 BLEU improvements on average over a strong NMT system. 1 Introduction Neural Machine Translation (NMT) (Kalchbrenner and Blunsom, 2013; Bahdanau et al., 2014; Sutskever et al., 2014) relies heavily on source representations, which encode implicitly semantic information of source words by neural networks (Mikolov et al., 2013a,b). Recently, several research works have been proposed to learn richer source representation, such as multisource information (Zoph and Knight, 2016; Firat et al., 2016), and particularly source syntactic information (Eriguchi et al., 2016; Li et al., 2017; Huadong et al., 2017; Eriguchi et al., 2017), thus improving the performance of NMT. In this paper, we enhance source representations by dependency information, which can capture source long-distance dependency constraints for word prediction. Actually, source dependency information has been shown greatly effective in ∗ Kehai Chen was an internship research fellow at NICT when conducting this work. † Corresponding author. Statistical Machine Translation (SMT) (Garmash and Monz, 2014; Kazemi et al., 2015; H"
D17-1304,D14-1176,0,0.0464241,"Missing"
D17-1304,P17-1177,0,0.0492407,"achieves 1.6 BLEU improvements on average over a strong NMT system. 1 Introduction Neural Machine Translation (NMT) (Kalchbrenner and Blunsom, 2013; Bahdanau et al., 2014; Sutskever et al., 2014) relies heavily on source representations, which encode implicitly semantic information of source words by neural networks (Mikolov et al., 2013a,b). Recently, several research works have been proposed to learn richer source representation, such as multisource information (Zoph and Knight, 2016; Firat et al., 2016), and particularly source syntactic information (Eriguchi et al., 2016; Li et al., 2017; Huadong et al., 2017; Eriguchi et al., 2017), thus improving the performance of NMT. In this paper, we enhance source representations by dependency information, which can capture source long-distance dependency constraints for word prediction. Actually, source dependency information has been shown greatly effective in ∗ Kehai Chen was an internship research fellow at NICT when conducting this work. † Corresponding author. Statistical Machine Translation (SMT) (Garmash and Monz, 2014; Kazemi et al., 2015; Hadiwinoto et al., 2016; Chen et al., 2017; Hadiwinoto and Ng, 2017). In NMT, there has been a quite recent pr"
D17-1304,D13-1176,0,0.0524365,"successfully introduced into statistical machine translation. However, there are only a few preliminary attempts for Neural Machine Translation (NMT), such as concatenating representations of source word and its dependency label together. In this paper, we propose a novel attentional NMT with source dependency representation to improve translation performance of NMT, especially on long sentences. Empirical results on NIST Chinese-toEnglish translation task show that our method achieves 1.6 BLEU improvements on average over a strong NMT system. 1 Introduction Neural Machine Translation (NMT) (Kalchbrenner and Blunsom, 2013; Bahdanau et al., 2014; Sutskever et al., 2014) relies heavily on source representations, which encode implicitly semantic information of source words by neural networks (Mikolov et al., 2013a,b). Recently, several research works have been proposed to learn richer source representation, such as multisource information (Zoph and Knight, 2016; Firat et al., 2016), and particularly source syntactic information (Eriguchi et al., 2016; Li et al., 2017; Huadong et al., 2017; Eriguchi et al., 2017), thus improving the performance of NMT. In this paper, we enhance source representations by dependency"
D17-1304,W15-4906,0,0.0209798,"6; Firat et al., 2016), and particularly source syntactic information (Eriguchi et al., 2016; Li et al., 2017; Huadong et al., 2017; Eriguchi et al., 2017), thus improving the performance of NMT. In this paper, we enhance source representations by dependency information, which can capture source long-distance dependency constraints for word prediction. Actually, source dependency information has been shown greatly effective in ∗ Kehai Chen was an internship research fellow at NICT when conducting this work. † Corresponding author. Statistical Machine Translation (SMT) (Garmash and Monz, 2014; Kazemi et al., 2015; Hadiwinoto et al., 2016; Chen et al., 2017; Hadiwinoto and Ng, 2017). In NMT, there has been a quite recent preliminary exploration (Sennrich and Haddow, 2016), in which vector representations of source word and its dependency label are simply concatenated as source input, achieving state-ofthe-art performance in NMT (Bojar et al., 2016). In this paper, we propose a novel NMT with source dependency representation to improve translation performance. Compared with the simple approach of vector concatenation, we learn the Source Dependency Representation (SDR) to compute dependency context vect"
D17-1304,P07-2045,0,0.0114868,"data and be set as 0.6 in the experiments. sentence pairs extract from LDC corpora.3 We use the Stanford dependency parser (Chang et al., 2009) to generate the dependency tree for Chinese. We choose the NIST 2002 (MT02) and the NIST 2003-2008 (MT03-08) datasets as the validation set and test sets, respectively. Case-insensitive 4gram NIST BLEU score (Papineni et al., 2002) is used as an evaluation metric, and signtest (Collins et al., 2005) is as statistical significance test. The baseline systems include the standard Phrase-Based Statistical Machine Translation (PBSMT) implemented in Moses (Koehn et al., 2007) and the standard Attentional NMT (AttNMT) (Bahdanau et al., 2014), where only source word representation is utilized. We also compare with a state-of-the-art syntax enhanced NMT method (Sennrich and Haddow, 2016). For a fair comparison, we only utilize dependency information for (Sennrich and Haddow, 2016), called Sennrich-deponly. We try our best to re-implement the baseline methods on Nematus toolkit 4 (Sennrich et al., 2017). For all NMT systems, we limit the source and target vocabularies to 30K, and the maximum sentence length is 80. The word embedding dimension is 620,5 and the hidden l"
D17-1304,P17-1064,0,0.0668486,"that our method achieves 1.6 BLEU improvements on average over a strong NMT system. 1 Introduction Neural Machine Translation (NMT) (Kalchbrenner and Blunsom, 2013; Bahdanau et al., 2014; Sutskever et al., 2014) relies heavily on source representations, which encode implicitly semantic information of source words by neural networks (Mikolov et al., 2013a,b). Recently, several research works have been proposed to learn richer source representation, such as multisource information (Zoph and Knight, 2016; Firat et al., 2016), and particularly source syntactic information (Eriguchi et al., 2016; Li et al., 2017; Huadong et al., 2017; Eriguchi et al., 2017), thus improving the performance of NMT. In this paper, we enhance source representations by dependency information, which can capture source long-distance dependency constraints for word prediction. Actually, source dependency information has been shown greatly effective in ∗ Kehai Chen was an internship research fellow at NICT when conducting this work. † Corresponding author. Statistical Machine Translation (SMT) (Garmash and Monz, 2014; Kazemi et al., 2015; Hadiwinoto et al., 2016; Chen et al., 2017; Hadiwinoto and Ng, 2017). In NMT, there has"
D17-1304,P02-1040,0,0.11802,"? i,1 ? ? i,2 i,2 … i,J x5 U2=&lt;x3, x1, x4, x7 , ε&gt; CNN d1 x4 (14) Experiment Setting up We carry out experiments on Chinese-to-English translation. The training dataset consists of 1.42M 2 λ can be tuned according to a subset FBIS of training data and be set as 0.6 in the experiments. sentence pairs extract from LDC corpora.3 We use the Stanford dependency parser (Chang et al., 2009) to generate the dependency tree for Chinese. We choose the NIST 2002 (MT02) and the NIST 2003-2008 (MT03-08) datasets as the validation set and test sets, respectively. Case-insensitive 4gram NIST BLEU score (Papineni et al., 2002) is used as an evaluation metric, and signtest (Collins et al., 2005) is as statistical significance test. The baseline systems include the standard Phrase-Based Statistical Machine Translation (PBSMT) implemented in Moses (Koehn et al., 2007) and the standard Attentional NMT (AttNMT) (Bahdanau et al., 2014), where only source word representation is utilized. We also compare with a state-of-the-art syntax enhanced NMT method (Sennrich and Haddow, 2016). For a fair comparison, we only utilize dependency information for (Sennrich and Haddow, 2016), called Sennrich-deponly. We try our best to re-"
D17-1304,E17-3017,0,0.043072,"Missing"
D17-1304,W16-2209,0,0.149106,"), thus improving the performance of NMT. In this paper, we enhance source representations by dependency information, which can capture source long-distance dependency constraints for word prediction. Actually, source dependency information has been shown greatly effective in ∗ Kehai Chen was an internship research fellow at NICT when conducting this work. † Corresponding author. Statistical Machine Translation (SMT) (Garmash and Monz, 2014; Kazemi et al., 2015; Hadiwinoto et al., 2016; Chen et al., 2017; Hadiwinoto and Ng, 2017). In NMT, there has been a quite recent preliminary exploration (Sennrich and Haddow, 2016), in which vector representations of source word and its dependency label are simply concatenated as source input, achieving state-ofthe-art performance in NMT (Bojar et al., 2016). In this paper, we propose a novel NMT with source dependency representation to improve translation performance. Compared with the simple approach of vector concatenation, we learn the Source Dependency Representation (SDR) to compute dependency context vectors and alignment matrices in a more sophisticated manner, which has the potential to make full use of source dependency information. To this end, we create a de"
D17-1304,N16-1004,0,0.0215619,"Missing"
D17-1304,W16-2301,0,\N,Missing
D19-1570,P18-1060,0,0.0269909,"thods, we find that their test performance across different translation tasks does not exhibit consistent improvement, and this phenomenon can be initially observed in (Wang et al., 2018) as well. The reason might be the evaluation ⇤ Work done at Tencent AI Lab. metric on a specific test set when compared to the whole data population, which generates all possible data, has large variance so that leads to the inconsistency. This evaluation dilemma is also recognized and explored by Recht et al. (2018, 2019); Werpachowski et al. (2019), and is especially notorious for language generation tasks (Chaganty et al., 2018; Hashimoto et al., 2019) where the evaluation metrics, e.g. BLEU (Papineni et al., 2001), are extrinsic and heavily relies on the reference provided. Therefore, we ask a fundamental question: what benefits, which are more consistent across different DA methods and translation tasks, can DA in general obtain? A direct answer to the above question is to use generalization gap (Kawaguchi et al., 2018) defined by the difference between population risk and empirical risk. This measure does not rely on any specific test set, accurately depicts generalization but is intractable to compute. So recent"
D19-1570,P16-1185,0,0.0948154,"Missing"
D19-1570,D18-1045,0,0.143239,"eriments show that relatively consistent benefits across five DA methods and four translation tasks are achieved regarding both perspectives. 1 Introduction Data Augmentation (DA) is a training paradigm that has been proved to be very effective in many modalities (Park et al., 2019; Perez and Wang, 2017; Sennrich et al., 2016a), especially for classification (Perez and Wang, 2017). In structured domain, Neural Machine Translation (NMT) is the frontier of DA research (Sennrich et al., 2016a; Norouzi et al., 2016; Zhang and Zong, 2016; Fadaee et al., 2017; Wang et al., 2018; Zhang et al., 2019; Edunov et al., 2018; Fadaee and Monz, 2018). However, by investigating a variety of DA methods, we find that their test performance across different translation tasks does not exhibit consistent improvement, and this phenomenon can be initially observed in (Wang et al., 2018) as well. The reason might be the evaluation ⇤ Work done at Tencent AI Lab. metric on a specific test set when compared to the whole data population, which generates all possible data, has large variance so that leads to the inconsistency. This evaluation dilemma is also recognized and explored by Recht et al. (2018, 2019); Werpachowski et a"
D19-1570,P17-2090,0,0.404575,"lead to findings with relatively low variance. Extensive experiments show that relatively consistent benefits across five DA methods and four translation tasks are achieved regarding both perspectives. 1 Introduction Data Augmentation (DA) is a training paradigm that has been proved to be very effective in many modalities (Park et al., 2019; Perez and Wang, 2017; Sennrich et al., 2016a), especially for classification (Perez and Wang, 2017). In structured domain, Neural Machine Translation (NMT) is the frontier of DA research (Sennrich et al., 2016a; Norouzi et al., 2016; Zhang and Zong, 2016; Fadaee et al., 2017; Wang et al., 2018; Zhang et al., 2019; Edunov et al., 2018; Fadaee and Monz, 2018). However, by investigating a variety of DA methods, we find that their test performance across different translation tasks does not exhibit consistent improvement, and this phenomenon can be initially observed in (Wang et al., 2018) as well. The reason might be the evaluation ⇤ Work done at Tencent AI Lab. metric on a specific test set when compared to the whole data population, which generates all possible data, has large variance so that leads to the inconsistency. This evaluation dilemma is also recognized"
D19-1570,D18-1040,0,0.371581,"latively consistent benefits across five DA methods and four translation tasks are achieved regarding both perspectives. 1 Introduction Data Augmentation (DA) is a training paradigm that has been proved to be very effective in many modalities (Park et al., 2019; Perez and Wang, 2017; Sennrich et al., 2016a), especially for classification (Perez and Wang, 2017). In structured domain, Neural Machine Translation (NMT) is the frontier of DA research (Sennrich et al., 2016a; Norouzi et al., 2016; Zhang and Zong, 2016; Fadaee et al., 2017; Wang et al., 2018; Zhang et al., 2019; Edunov et al., 2018; Fadaee and Monz, 2018). However, by investigating a variety of DA methods, we find that their test performance across different translation tasks does not exhibit consistent improvement, and this phenomenon can be initially observed in (Wang et al., 2018) as well. The reason might be the evaluation ⇤ Work done at Tencent AI Lab. metric on a specific test set when compared to the whole data population, which generates all possible data, has large variance so that leads to the inconsistency. This evaluation dilemma is also recognized and explored by Recht et al. (2018, 2019); Werpachowski et al. (2019), and is especi"
D19-1570,N19-1169,0,0.0176147,"ir test performance across different translation tasks does not exhibit consistent improvement, and this phenomenon can be initially observed in (Wang et al., 2018) as well. The reason might be the evaluation ⇤ Work done at Tencent AI Lab. metric on a specific test set when compared to the whole data population, which generates all possible data, has large variance so that leads to the inconsistency. This evaluation dilemma is also recognized and explored by Recht et al. (2018, 2019); Werpachowski et al. (2019), and is especially notorious for language generation tasks (Chaganty et al., 2018; Hashimoto et al., 2019) where the evaluation metrics, e.g. BLEU (Papineni et al., 2001), are extrinsic and heavily relies on the reference provided. Therefore, we ask a fundamental question: what benefits, which are more consistent across different DA methods and translation tasks, can DA in general obtain? A direct answer to the above question is to use generalization gap (Kawaguchi et al., 2018) defined by the difference between population risk and empirical risk. This measure does not rely on any specific test set, accurately depicts generalization but is intractable to compute. So recently, many theorists have p"
D19-1570,D16-1139,0,0.0633063,"Missing"
D19-1570,N19-4007,0,0.041402,"Missing"
D19-1570,N19-4009,0,0.026019,"T ). 1 Under the AUG model, the training objective becomes: JAUG = Ex,y⇠q(X,Y |T ) [log p✓ (y|x)]. En)De Baseline 38.38 (5) 38.88 (6) 17.25 (6) 26.19 (4) RAML +0.22 (3) +0.67 (3) +0.23 (4) -0.16 (6) SO +0.01 (4) +0.62 (4) +0.02 (5) -0.15 (5) 2.2 ST -0.13 (6) +0.46 (5) +1.51 (2) +0.83 (2) TA +0.62 (2) +1.13 (1) +2.41 (1) +1.01 (1) BT +0.82 (1) +0.99 (2) +1.06 (3) +0.39 (3) Settings and Main Performance Settings By carefully controlling the above two factors, we conduct fair and extensive experiments with Transformer (Vaswani et al., 2017) on four translation tasks for five DA methods. Fairseq (Ott et al., 2019) is used as our codebase. We use standard benchmarks IWSLT17 En-Fr, WMT19 ZhEn, WMT19 En-De, where we train both translation directions on the IWSLT corpus. The five DA methods are briefly summarized as follows: • RAML: reward-augmented maximum likelihood training, which augment the target-side with a sampling distribution P (Y |Y ⇤ ) concentrated around Y ⇤ (Norouzi et al., 2016). (2) • Switchout (SO): similar to RAML, but also adds the some kind of augmentation to the source-side (Wang et al., 2018). • Self-training (ST): fix the source-side, uses an forward NMT model to generate the target-"
D19-1570,2001.mtsummit-papers.68,0,0.0198772,"xhibit consistent improvement, and this phenomenon can be initially observed in (Wang et al., 2018) as well. The reason might be the evaluation ⇤ Work done at Tencent AI Lab. metric on a specific test set when compared to the whole data population, which generates all possible data, has large variance so that leads to the inconsistency. This evaluation dilemma is also recognized and explored by Recht et al. (2018, 2019); Werpachowski et al. (2019), and is especially notorious for language generation tasks (Chaganty et al., 2018; Hashimoto et al., 2019) where the evaluation metrics, e.g. BLEU (Papineni et al., 2001), are extrinsic and heavily relies on the reference provided. Therefore, we ask a fundamental question: what benefits, which are more consistent across different DA methods and translation tasks, can DA in general obtain? A direct answer to the above question is to use generalization gap (Kawaguchi et al., 2018) defined by the difference between population risk and empirical risk. This measure does not rely on any specific test set, accurately depicts generalization but is intractable to compute. So recently, many theorists have proposed either non-vacuous generalization bound (Dziugaite and R"
D19-1570,P16-1009,0,0.481577,"heoretic advances in deep learning, the paper understands DA from two perspectives towards the generalization ability of a model: input sensitivity and prediction margin, which are defined independent of specific test set thereby may lead to findings with relatively low variance. Extensive experiments show that relatively consistent benefits across five DA methods and four translation tasks are achieved regarding both perspectives. 1 Introduction Data Augmentation (DA) is a training paradigm that has been proved to be very effective in many modalities (Park et al., 2019; Perez and Wang, 2017; Sennrich et al., 2016a), especially for classification (Perez and Wang, 2017). In structured domain, Neural Machine Translation (NMT) is the frontier of DA research (Sennrich et al., 2016a; Norouzi et al., 2016; Zhang and Zong, 2016; Fadaee et al., 2017; Wang et al., 2018; Zhang et al., 2019; Edunov et al., 2018; Fadaee and Monz, 2018). However, by investigating a variety of DA methods, we find that their test performance across different translation tasks does not exhibit consistent improvement, and this phenomenon can be initially observed in (Wang et al., 2018) as well. The reason might be the evaluation ⇤ Work"
D19-1570,P16-1162,0,0.45957,"heoretic advances in deep learning, the paper understands DA from two perspectives towards the generalization ability of a model: input sensitivity and prediction margin, which are defined independent of specific test set thereby may lead to findings with relatively low variance. Extensive experiments show that relatively consistent benefits across five DA methods and four translation tasks are achieved regarding both perspectives. 1 Introduction Data Augmentation (DA) is a training paradigm that has been proved to be very effective in many modalities (Park et al., 2019; Perez and Wang, 2017; Sennrich et al., 2016a), especially for classification (Perez and Wang, 2017). In structured domain, Neural Machine Translation (NMT) is the frontier of DA research (Sennrich et al., 2016a; Norouzi et al., 2016; Zhang and Zong, 2016; Fadaee et al., 2017; Wang et al., 2018; Zhang et al., 2019; Edunov et al., 2018; Fadaee and Monz, 2018). However, by investigating a variety of DA methods, we find that their test performance across different translation tasks does not exhibit consistent improvement, and this phenomenon can be initially observed in (Wang et al., 2018) as well. The reason might be the evaluation ⇤ Work"
D19-1570,D16-1160,0,0.217106,"test set thereby may lead to findings with relatively low variance. Extensive experiments show that relatively consistent benefits across five DA methods and four translation tasks are achieved regarding both perspectives. 1 Introduction Data Augmentation (DA) is a training paradigm that has been proved to be very effective in many modalities (Park et al., 2019; Perez and Wang, 2017; Sennrich et al., 2016a), especially for classification (Perez and Wang, 2017). In structured domain, Neural Machine Translation (NMT) is the frontier of DA research (Sennrich et al., 2016a; Norouzi et al., 2016; Zhang and Zong, 2016; Fadaee et al., 2017; Wang et al., 2018; Zhang et al., 2019; Edunov et al., 2018; Fadaee and Monz, 2018). However, by investigating a variety of DA methods, we find that their test performance across different translation tasks does not exhibit consistent improvement, and this phenomenon can be initially observed in (Wang et al., 2018) as well. The reason might be the evaluation ⇤ Work done at Tencent AI Lab. metric on a specific test set when compared to the whole data population, which generates all possible data, has large variance so that leads to the inconsistency. This evaluation dilemm"
D19-1570,D13-1117,0,0.0521142,"Missing"
D19-1570,D18-1100,0,0.343546,"relatively low variance. Extensive experiments show that relatively consistent benefits across five DA methods and four translation tasks are achieved regarding both perspectives. 1 Introduction Data Augmentation (DA) is a training paradigm that has been proved to be very effective in many modalities (Park et al., 2019; Perez and Wang, 2017; Sennrich et al., 2016a), especially for classification (Perez and Wang, 2017). In structured domain, Neural Machine Translation (NMT) is the frontier of DA research (Sennrich et al., 2016a; Norouzi et al., 2016; Zhang and Zong, 2016; Fadaee et al., 2017; Wang et al., 2018; Zhang et al., 2019; Edunov et al., 2018; Fadaee and Monz, 2018). However, by investigating a variety of DA methods, we find that their test performance across different translation tasks does not exhibit consistent improvement, and this phenomenon can be initially observed in (Wang et al., 2018) as well. The reason might be the evaluation ⇤ Work done at Tencent AI Lab. metric on a specific test set when compared to the whole data population, which generates all possible data, has large variance so that leads to the inconsistency. This evaluation dilemma is also recognized and explored by Rec"
I13-1032,D08-1024,0,0.179727,"grouping via OSCAR to overcome these pitfalls. Our feature grouping is implemented within an online learning framework and thus it is efficient for a large scale (both for features and examples) of learning in our scenario. Experiment results on IWSLT translation tasks show that the proposed method significantly outperforms the state of the art tuning methods. 1 Introduction Since the introduction of log-linear based SMT (Och and Ney, 2002), tuning has been a hot topic. Various methods have been explored: their objectives are either error rates (Och, 2003), hinge loss (Watanabe et al., 2007; Chiang et al., 2008) or ranking loss (Hopkins and May, 2011), and they are either batch training or online training methods. In this paper, we consider tuning translation models with a large number of features such as lexical, n-gram level and rule level features, where the number of features is largely greater than the number of bilingual sentences. Practically, existing tuning methods such as PRO and MIRA might This joint work was done while the first author visited NICT. be applied in our scenario, however, they will suffer from some pitfalls as well, which have been less investigated in previous works. One of"
I13-1032,P05-1033,0,0.0461075,"n is attributed to the Eq.5 in (Zhong and Kwok, 2011). pairs. Test sets 2003, 2004 and 2008 are used as the development set, development test (devtest) set and test set, respectively; and all of them contain 16 references. A 5-gram language model is trained on the training data with the SRILM toolkit, and word alignment is obtained with GIZA++. In our experiments, the translation performances are measured by the case-insensitive BLEU4 metric. The significance testing is performed by paired bootstrap re-sampling (Koehn, 2004). We use an in-house developed hierarchical phrase-based translation (Chiang, 2005) as our baseline decoder, and we use the state of the art tuning methods MERT and PRO as our comparison methods4 . Based on our in-house decoder, we implement three translation models with different feature sets: default features (default); default features plus rule id features (+id) ; and default features plus group features of rule id (+group). On the IWSLT training data, the number of rule id features is 500K, i.e. d = 500K, which is significantly greater than the number of bilingual sentences 30K. Our proposed tuning method is with the following setting by tuning on the dev-test set: λ1 ="
I13-1032,D11-1125,0,0.0685367,"pitfalls. Our feature grouping is implemented within an online learning framework and thus it is efficient for a large scale (both for features and examples) of learning in our scenario. Experiment results on IWSLT translation tasks show that the proposed method significantly outperforms the state of the art tuning methods. 1 Introduction Since the introduction of log-linear based SMT (Och and Ney, 2002), tuning has been a hot topic. Various methods have been explored: their objectives are either error rates (Och, 2003), hinge loss (Watanabe et al., 2007; Chiang et al., 2008) or ranking loss (Hopkins and May, 2011), and they are either batch training or online training methods. In this paper, we consider tuning translation models with a large number of features such as lexical, n-gram level and rule level features, where the number of features is largely greater than the number of bilingual sentences. Practically, existing tuning methods such as PRO and MIRA might This joint work was done while the first author visited NICT. be applied in our scenario, however, they will suffer from some pitfalls as well, which have been less investigated in previous works. One of pitfalls is that these features are so"
I13-1032,W04-3250,0,0.0161007,"on tasks, whose training data consists of about 30K bilingual sentence 3 The reason is attributed to the Eq.5 in (Zhong and Kwok, 2011). pairs. Test sets 2003, 2004 and 2008 are used as the development set, development test (devtest) set and test set, respectively; and all of them contain 16 references. A 5-gram language model is trained on the training data with the SRILM toolkit, and word alignment is obtained with GIZA++. In our experiments, the translation performances are measured by the case-insensitive BLEU4 metric. The significance testing is performed by paired bootstrap re-sampling (Koehn, 2004). We use an in-house developed hierarchical phrase-based translation (Chiang, 2005) as our baseline decoder, and we use the state of the art tuning methods MERT and PRO as our comparison methods4 . Based on our in-house decoder, we implement three translation models with different feature sets: default features (default); default features plus rule id features (+id) ; and default features plus group features of rule id (+group). On the IWSLT training data, the number of rule id features is 500K, i.e. d = 500K, which is significantly greater than the number of bilingual sentences 30K. Our propo"
I13-1032,C10-1075,0,0.259742,"er-fitting occurs. One practice is to tune translation models on a larger tuning set, such as the entire training data (Xiao et al., 2011; Simianer et al., 2012), in the hope that more features would be included during tuning. However, tuning robust weights for translation models has additional requirements to a tuning set. Firstly, multiple reference translations in the tuning data are helpful for better tuning, especially when testing data contains multiple reference translations. Secondly, the closeness between the tuning set and a test set is also important for better testing performance (Li et al., 2010). These requirements can explain why tuning on the training data leads to unsatisfactory performance on the IWSLT translation task, as will be shown in our experiments later. Therefore, enlarging a tuning set is not always a sufficient solution for robust tuning, since it would be impractical to create a large scale tuning set with these requirements. We propose a novel tuning method by grouping a large number of features to leverage the above pitfalls. Instead of directly taking the large number of atomic features into translation model, we firstly learn their group structure on the training"
I13-1032,P02-1038,0,0.1062,"a result, we face an over-fitting problem, which limits the generalization abilities of the learned models. Based on our analysis, we propose a novel method based on feature grouping via OSCAR to overcome these pitfalls. Our feature grouping is implemented within an online learning framework and thus it is efficient for a large scale (both for features and examples) of learning in our scenario. Experiment results on IWSLT translation tasks show that the proposed method significantly outperforms the state of the art tuning methods. 1 Introduction Since the introduction of log-linear based SMT (Och and Ney, 2002), tuning has been a hot topic. Various methods have been explored: their objectives are either error rates (Och, 2003), hinge loss (Watanabe et al., 2007; Chiang et al., 2008) or ranking loss (Hopkins and May, 2011), and they are either batch training or online training methods. In this paper, we consider tuning translation models with a large number of features such as lexical, n-gram level and rule level features, where the number of features is largely greater than the number of bilingual sentences. Practically, existing tuning methods such as PRO and MIRA might This joint work was done whi"
I13-1032,P03-1021,0,0.104864,"sis, we propose a novel method based on feature grouping via OSCAR to overcome these pitfalls. Our feature grouping is implemented within an online learning framework and thus it is efficient for a large scale (both for features and examples) of learning in our scenario. Experiment results on IWSLT translation tasks show that the proposed method significantly outperforms the state of the art tuning methods. 1 Introduction Since the introduction of log-linear based SMT (Och and Ney, 2002), tuning has been a hot topic. Various methods have been explored: their objectives are either error rates (Och, 2003), hinge loss (Watanabe et al., 2007; Chiang et al., 2008) or ranking loss (Hopkins and May, 2011), and they are either batch training or online training methods. In this paper, we consider tuning translation models with a large number of features such as lexical, n-gram level and rule level features, where the number of features is largely greater than the number of bilingual sentences. Practically, existing tuning methods such as PRO and MIRA might This joint work was done while the first author visited NICT. be applied in our scenario, however, they will suffer from some pitfalls as well, wh"
I13-1032,P02-1040,0,0.0863901,"atomic features into translation model, we firstly learn their group structure on the training data to alleviate their serious sparsity. Then, we tune the translation model consisting of grouped features on a multi-reference development set to ensure robust tuning. Unlike unsupervised clustering methods such as k-means (MacQueen, 1967) for feature clustering, we group the features with the OSCAR (Octagonal Shrinkage and Clustering Algorithm for Regression) method (Bondell and Reich, 2008), which directly relates the objective of feature grouping to translation evaluation metrics such as BLEU (Papineni et al., 2002) and thus grouped features are optimized with respect to BLEU. Due to the large number of features and large number of training examples, efficient grouping is not simple. We apply the online gradient projection method under the FOBOS (forwardbackward splitting) framework (Duchi and Singer, 2009) to accelerate feature grouping. We employ a large number of features by treating each translation rule in a synchronous-CFG as a single feature. Experiments on IWSLT Chineseto-English translation tasks show that, with the help of grouping these features, our method can overcome the above pitfalls and"
I13-1032,P12-1002,0,0.0603095,"pitfalls as well, which have been less investigated in previous works. One of pitfalls is that these features are so sparse that many features which are potentially useful for a test set may not be included in a given tuning set, and many useless features for testing will be over tuned on the developement set meanwhile. As a result, the generalization abilities of features are limited due to the mismatch between the testing data and the tuning data, and over-fitting occurs. One practice is to tune translation models on a larger tuning set, such as the entire training data (Xiao et al., 2011; Simianer et al., 2012), in the hope that more features would be included during tuning. However, tuning robust weights for translation models has additional requirements to a tuning set. Firstly, multiple reference translations in the tuning data are helpful for better tuning, especially when testing data contains multiple reference translations. Secondly, the closeness between the tuning set and a test set is also important for better testing performance (Li et al., 2010). These requirements can explain why tuning on the training data leads to unsatisfactory performance on the IWSLT translation task, as will be sh"
I13-1032,P09-1054,0,0.0310359,"t features setting. Its main reason, as presented in Section 1, may be that multiple references and closeness5 of tuning sets are much helpful for translation tasks. Further, the id features do not achieve improvements and even decreases 0.9 BLEU scores when tuned on the development set, due to its serious sparsity. However, after grouping id features, the groups learned by our method can alleviate the feature sparsity and thus significantly obtain gains of 0.7 BLEU scores over default feature setting. Further, we implement another tuning method6 for comparison, i.e. L1 regularization method (Tsuruoka et al., 2009) based on the ranking loss L(W ) defined in Eq.1. We tune the translation 4 Both of them are derived from the Moses toolkit: http://www.statmt.org/moses/. 5 If the tuning set and test set are close enough or identically distributed, it is possible to get gains by sparse discriminative features without using feature grouping(Chiang et al., 2009). 6 It is similar to dtrain implemented in the cdec toolkit: http://cdec-decoder.org/, except that it does not use the distributed learning framework. Methods Tuning set Feature set MERT PRO PRO PRO L1 L1 OSCAR dev dev train dev train dev – default defau"
I13-1032,D07-1080,1,0.803531,"method based on feature grouping via OSCAR to overcome these pitfalls. Our feature grouping is implemented within an online learning framework and thus it is efficient for a large scale (both for features and examples) of learning in our scenario. Experiment results on IWSLT translation tasks show that the proposed method significantly outperforms the state of the art tuning methods. 1 Introduction Since the introduction of log-linear based SMT (Och and Ney, 2002), tuning has been a hot topic. Various methods have been explored: their objectives are either error rates (Och, 2003), hinge loss (Watanabe et al., 2007; Chiang et al., 2008) or ranking loss (Hopkins and May, 2011), and they are either batch training or online training methods. In this paper, we consider tuning translation models with a large number of features such as lexical, n-gram level and rule level features, where the number of features is largely greater than the number of bilingual sentences. Practically, existing tuning methods such as PRO and MIRA might This joint work was done while the first author visited NICT. be applied in our scenario, however, they will suffer from some pitfalls as well, which have been less investigated in"
I13-1032,D11-1081,0,0.367302,"ll suffer from some pitfalls as well, which have been less investigated in previous works. One of pitfalls is that these features are so sparse that many features which are potentially useful for a test set may not be included in a given tuning set, and many useless features for testing will be over tuned on the developement set meanwhile. As a result, the generalization abilities of features are limited due to the mismatch between the testing data and the tuning data, and over-fitting occurs. One practice is to tune translation models on a larger tuning set, such as the entire training data (Xiao et al., 2011; Simianer et al., 2012), in the hope that more features would be included during tuning. However, tuning robust weights for translation models has additional requirements to a tuning set. Firstly, multiple reference translations in the tuning data are helpful for better tuning, especially when testing data contains multiple reference translations. Secondly, the closeness between the tuning set and a test set is also important for better testing performance (Li et al., 2010). These requirements can explain why tuning on the training data leads to unsatisfactory performance on the IWSLT transla"
I13-1032,N09-1025,0,\N,Missing
N16-1046,P15-1033,0,0.0135531,"Missing"
N16-1046,D09-1117,1,0.942985,"Missing"
N16-1046,P15-1001,0,0.0309557,"Missing"
N16-1046,P07-2045,0,0.0575108,"Missing"
N16-1046,N06-1014,0,0.0285511,"Missing"
N16-1046,D14-1209,1,0.815238,"Missing"
N16-1046,P15-1002,0,0.0282201,"Missing"
N16-1046,P00-1056,0,0.0407959,"Missing"
N16-1046,C02-1050,1,0.742069,"Missing"
N16-1046,P15-1113,1,0.580203,"Missing"
N16-1046,N13-1002,0,0.091757,"Missing"
N16-1046,P13-1016,1,\N,Missing
N16-1046,2002.tmi-tutorials.2,0,\N,Missing
N16-1046,P14-1129,0,\N,Missing
N18-1125,P05-1033,0,0.188078,"dù huí shēng &lt;/S> 法国 失业 人数 再度 回升 &lt;/S> JJ NN VBZ RB EOS French unemployment rises again &lt;/S> (b) TFA-NMT French unemployment rises again &lt;/S> (c) Reference Introduction Since neural machine translation (NMT) was proposed (Bahdanau et al., 2014), it has been attracted increasing interests in machine translation community (Luong et al., 2015b; Tu et al., 2016; Feng et al., 2016; Cohn et al., 2016). NMT not only yields impressive translation performance in practice, but also has appealing model architecture in essence. Compared with traditional statistical machine translation (Koehn et al., 2003; Chiang, 2005), one of advantages in NMT is that its architecture combines language model, translation model and alignment between source and target words in a unified manner rather than a ∗ Work done when X. Li interning at Tencent AI Lab. L. Liu is the corresponding author. Figure 1: A running example to motivate the proposed model. (a) The baseline obtains a translation error due to the incorrect attention. (b) With the help of the target foresight information “VBZ”, TFA-NMT is likely to figure out the exact translation as the reference in (c). The light font denotes the target words to be translated in"
N18-1125,P11-2031,0,0.0255199,"Missing"
N18-1125,2016.amta-researchers.10,0,0.161257,"auxiliary information from a target foresight word into the attention model. However, there is a significant difference between our approach and their approaches. Our auxiliary information biases to the word to be translated at next timestep while theirs biases to the information available so far at the current timestep, and thereby our approach is orthogonal to theirs. The works mentioned above improve the attention models by access auxiliary information, and thus they modify the structure of attention models in both inference and learning. In contrast, Mi et al. (2016); Liu et al. (2016b); Chen et al. (2016) maintain the structure of the attention models in inference but utilize some external signals to supervise the outputs of attention models during the learning. They improve the generalization abilities of attention models by use of the external aligners as the signals, which typically yield alignment results accurate enough to guide the learning of attention. 6 Conclusion It has been argued that the traditional attention model in neural machine translation suf1387 System Model Dev MT05 MT06 MT08 Ave. (Liu et al., 2016b) Moses NMT-J – – 35.4 36.8 33.7 36.9 25.0 28.5 31.37 34.07 (Liu et al., 20"
N18-1125,C16-1290,0,0.0446104,"sh and japanese-to-english datasets show that the proposed attention model delivers significant improvements in terms of both alignment error rate and bleu. 1 French unemployment rate rises again &lt;/S> (a) Baseline fă guó shī yè rén shù zài dù huí shēng &lt;/S> 法国 失业 人数 再度 回升 &lt;/S> JJ NN VBZ RB EOS French unemployment rises again &lt;/S> (b) TFA-NMT French unemployment rises again &lt;/S> (c) Reference Introduction Since neural machine translation (NMT) was proposed (Bahdanau et al., 2014), it has been attracted increasing interests in machine translation community (Luong et al., 2015b; Tu et al., 2016; Feng et al., 2016; Cohn et al., 2016). NMT not only yields impressive translation performance in practice, but also has appealing model architecture in essence. Compared with traditional statistical machine translation (Koehn et al., 2003; Chiang, 2005), one of advantages in NMT is that its architecture combines language model, translation model and alignment between source and target words in a unified manner rather than a ∗ Work done when X. Li interning at Tencent AI Lab. L. Liu is the corresponding author. Figure 1: A running example to motivate the proposed model. (a) The baseline obtains a translation er"
N18-1125,P07-2045,0,0.0110597,".1 1844.9 72.0 1666.1 70.1 1485.2 59.1 Performance BLEU FPA 38.65 – 38.57 – 38.83 69.03 39.26 69.95 40.63 71.91 Table 1: Speeds and performances of the proposed models. “Speed” is measured in words/second for both training and decoding, and performances are measured in terms of BLEU scores (“BLEU”) and foresight prediction accuracy (“FPA”) on the development set. Higher BLEU and FPA scores denote better performance. erence, following (Goto et al., 2013; Liu et al., 2016b) for further comparison. Implementation We compare the proposed models with two strong baselines from SMT and NMT: • Moses (Koehn et al., 2007): an open source phrased based translation system with default configuration. • Nematus (Sennrich et al., 2017): generic attention based NMT. an We implement the proposed models on top of Nematus. We use Stanford Log-linear PartOf-Speech Tagger (Toutanova et al., 2003) to produce POS tags for the English side. For both Chinese-to-English and Japanese-toEnglish tasks, we limit the vocabularies to the most frequent 30K words for both sides. All the out-of-vocabulary words are mapped to a spacial token “UNK”. Only the sentences of length up to 50 words are used in training, with 80 sentences in a"
N18-1125,N03-1017,0,0.0510404,"shī yè rén shù zài dù huí shēng &lt;/S> 法国 失业 人数 再度 回升 &lt;/S> JJ NN VBZ RB EOS French unemployment rises again &lt;/S> (b) TFA-NMT French unemployment rises again &lt;/S> (c) Reference Introduction Since neural machine translation (NMT) was proposed (Bahdanau et al., 2014), it has been attracted increasing interests in machine translation community (Luong et al., 2015b; Tu et al., 2016; Feng et al., 2016; Cohn et al., 2016). NMT not only yields impressive translation performance in practice, but also has appealing model architecture in essence. Compared with traditional statistical machine translation (Koehn et al., 2003; Chiang, 2005), one of advantages in NMT is that its architecture combines language model, translation model and alignment between source and target words in a unified manner rather than a ∗ Work done when X. Li interning at Tencent AI Lab. L. Liu is the corresponding author. Figure 1: A running example to motivate the proposed model. (a) The baseline obtains a translation error due to the incorrect attention. (b) With the help of the target foresight information “VBZ”, TFA-NMT is likely to figure out the exact translation as the reference in (c). The light font denotes the target words to be"
N18-1125,C16-1291,1,0.743925,"Missing"
N18-1125,N16-1046,1,0.904938,"tly the same as the original concept in alignment task (Peter et al., 2017). However, both of them share a common idea that foresight word should be at a later time step, and thus we respect the work in Peter et al. (2017) and maintain the same concept for easier understanding. 1380 Proceedings of NAACL-HLT 2018, pages 1380–1390 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics i.e. the first light color word in Figure 1(a), is not known but to be translated at the next time step. Therefore, this may lead to inadequate modeling for attention mechanism (Liu et al., 2016a; Peter et al., 2017). Regarding to this, Peter et al. (2017) explicitly feed this target word into the attention model, and demonstrate the significant improvements in alignment accuracy. Unfortunately, this approach relies on the premise that the target foresight word is available in advance in its alignment scenario, and thus it can not be used in the translation scenario. To address this issue, in this paper, we propose a target foresight based attention (TFA) model oriented to both alignment and translation tasks. Its basic idea includes two steps: it firstly designs an auxiliary mechani"
N18-1125,P05-1057,0,0.582372,"is issue, in this paper, we propose a target foresight based attention (TFA) model oriented to both alignment and translation tasks. Its basic idea includes two steps: it firstly designs an auxiliary mechanism to predict some information for the target foresight word which is helpful for alignment; and then it feeds the predicted result into the attention model for translation. For the sake of efficiency, instead of predicting the target foresight word with large vocabulary size, we only predict its partial information, i.e. partof-speech tag, which is proved to be helpful for word alignment (Liu et al., 2005). Figure 1(b) shows the main idea of TFA based on NMT. In order to remit the negative effects due to the prediction errors, we feed the distribution of the prediction result instead of the maximum a posteriori result into the attention model. In addition, since the target foresight words are available during the training, we jointly learn the prediction model for the target foresight words and the translation model in a supervised manner. This paper makes the following contributions: • It proposes a novel TFA-NMT for neural machine translation by using an auxiliary mechanism to predict the tar"
N18-1125,D15-1166,0,0.658725,"pirical experiments on chineseto-english and japanese-to-english datasets show that the proposed attention model delivers significant improvements in terms of both alignment error rate and bleu. 1 French unemployment rate rises again &lt;/S> (a) Baseline fă guó shī yè rén shù zài dù huí shēng &lt;/S> 法国 失业 人数 再度 回升 &lt;/S> JJ NN VBZ RB EOS French unemployment rises again &lt;/S> (b) TFA-NMT French unemployment rises again &lt;/S> (c) Reference Introduction Since neural machine translation (NMT) was proposed (Bahdanau et al., 2014), it has been attracted increasing interests in machine translation community (Luong et al., 2015b; Tu et al., 2016; Feng et al., 2016; Cohn et al., 2016). NMT not only yields impressive translation performance in practice, but also has appealing model architecture in essence. Compared with traditional statistical machine translation (Koehn et al., 2003; Chiang, 2005), one of advantages in NMT is that its architecture combines language model, translation model and alignment between source and target words in a unified manner rather than a ∗ Work done when X. Li interning at Tencent AI Lab. L. Liu is the corresponding author. Figure 1: A running example to motivate the proposed model. (a)"
N18-1125,C16-1205,0,0.0527507,"Missing"
N18-1125,D16-1249,0,0.0306058,"e propose approach, since we introduce auxiliary information from a target foresight word into the attention model. However, there is a significant difference between our approach and their approaches. Our auxiliary information biases to the word to be translated at next timestep while theirs biases to the information available so far at the current timestep, and thereby our approach is orthogonal to theirs. The works mentioned above improve the attention models by access auxiliary information, and thus they modify the structure of attention models in both inference and learning. In contrast, Mi et al. (2016); Liu et al. (2016b); Chen et al. (2016) maintain the structure of the attention models in inference but utilize some external signals to supervise the outputs of attention models during the learning. They improve the generalization abilities of attention models by use of the external aligners as the signals, which typically yield alignment results accurate enough to guide the learning of attention. 6 Conclusion It has been argued that the traditional attention model in neural machine translation suf1387 System Model Dev MT05 MT06 MT08 Ave. (Liu et al., 2016b) Moses NMT-J – – 35.4 36.8 33.7 36"
N18-1125,P03-1021,0,0.0158574,"ent of our model is not only from the POS tagging. In order to further validate the improvements of variant proposed models, we evaluate the foresight prediction accuracy (FPA) for three proposed prediction models. We found that the fine-grained Model3 achieves the best FPA, indicating a good estimated foresight is very important to obtain the gains in terms of BLEU. 4.2.2 Decode Exp Exp Map Analysis on Syntactic Categories In this experiment, we investigate which category of generated words benefit most from the proposed approach in terms of alignments measured by alignment error rate (AER) (Och, 2003). We carry out experiments on the evaluation dataset from (Liu and Sun, 2015), which contains 900 manually aligned ChineseEnglish sentence pairs. Following (Luong et al., 2015b), we force-decode both the bilingual sentences including source and reference sentences to obtain the attention matrices, and then we extract one-to-one alignments by picking up the source word with the highest alignment confidence as the hard alignment. As shown in Table 2, the AER improvements are modest for content words such as Noun, Verb, and adjective (“Adj.”) words; but there are substantial improvements for func"
N18-1125,C04-1156,0,0.0908774,"Missing"
N18-1125,E17-3017,0,0.0271867,".91 Table 1: Speeds and performances of the proposed models. “Speed” is measured in words/second for both training and decoding, and performances are measured in terms of BLEU scores (“BLEU”) and foresight prediction accuracy (“FPA”) on the development set. Higher BLEU and FPA scores denote better performance. erence, following (Goto et al., 2013; Liu et al., 2016b) for further comparison. Implementation We compare the proposed models with two strong baselines from SMT and NMT: • Moses (Koehn et al., 2007): an open source phrased based translation system with default configuration. • Nematus (Sennrich et al., 2017): generic attention based NMT. an We implement the proposed models on top of Nematus. We use Stanford Log-linear PartOf-Speech Tagger (Toutanova et al., 2003) to produce POS tags for the English side. For both Chinese-to-English and Japanese-toEnglish tasks, we limit the vocabularies to the most frequent 30K words for both sides. All the out-of-vocabulary words are mapped to a spacial token “UNK”. Only the sentences of length up to 50 words are used in training, with 80 sentences in a batch. The dimension of word embedding is 620. The dimensions of both feed forward NN and RNN hidden layer are"
N18-1125,W16-2209,0,0.0362855,"epresentations in decoding: Exp for expectation and Map for maximum a posteriori. Table 2: Performances on syntactic categories. “Base” denotes “Nematus”, and Ours denotes the proposed model. icantly better than baseline, but Model2 is significantly better with p&lt;0.05 and Model3 is significantly better with p&lt;0.01. Given that simply introducing an additional layer (“+2Layer”) does not produce any improvement on this data, we believe the gain of our model is not only from the more introduced parameters. Besides, we augment the word embedding by concatenating the POS tag embedding, proposed by (Sennrich and Haddow, 2016), the BLEU is 38.96, which indicating the improvement of our model is not only from the POS tagging. In order to further validate the improvements of variant proposed models, we evaluate the foresight prediction accuracy (FPA) for three proposed prediction models. We found that the fine-grained Model3 achieves the best FPA, indicating a good estimated foresight is very important to obtain the gains in terms of BLEU. 4.2.2 Decode Exp Exp Map Analysis on Syntactic Categories In this experiment, we investigate which category of generated words benefit most from the proposed approach in terms of a"
N18-1125,N03-1033,0,0.0381169,"d in terms of BLEU scores (“BLEU”) and foresight prediction accuracy (“FPA”) on the development set. Higher BLEU and FPA scores denote better performance. erence, following (Goto et al., 2013; Liu et al., 2016b) for further comparison. Implementation We compare the proposed models with two strong baselines from SMT and NMT: • Moses (Koehn et al., 2007): an open source phrased based translation system with default configuration. • Nematus (Sennrich et al., 2017): generic attention based NMT. an We implement the proposed models on top of Nematus. We use Stanford Log-linear PartOf-Speech Tagger (Toutanova et al., 2003) to produce POS tags for the English side. For both Chinese-to-English and Japanese-toEnglish tasks, we limit the vocabularies to the most frequent 30K words for both sides. All the out-of-vocabulary words are mapped to a spacial token “UNK”. Only the sentences of length up to 50 words are used in training, with 80 sentences in a batch. The dimension of word embedding is 620. The dimensions of both feed forward NN and RNN hidden layer are 1000. The beam size for decoding is 12, and the cost function is optimized by Adadelta with hyper-parameters suggested by Zeiler (2012). Particularly for TFA"
N18-1125,P16-1008,1,0.922677,"n chineseto-english and japanese-to-english datasets show that the proposed attention model delivers significant improvements in terms of both alignment error rate and bleu. 1 French unemployment rate rises again &lt;/S> (a) Baseline fă guó shī yè rén shù zài dù huí shēng &lt;/S> 法国 失业 人数 再度 回升 &lt;/S> JJ NN VBZ RB EOS French unemployment rises again &lt;/S> (b) TFA-NMT French unemployment rises again &lt;/S> (c) Reference Introduction Since neural machine translation (NMT) was proposed (Bahdanau et al., 2014), it has been attracted increasing interests in machine translation community (Luong et al., 2015b; Tu et al., 2016; Feng et al., 2016; Cohn et al., 2016). NMT not only yields impressive translation performance in practice, but also has appealing model architecture in essence. Compared with traditional statistical machine translation (Koehn et al., 2003; Chiang, 2005), one of advantages in NMT is that its architecture combines language model, translation model and alignment between source and target words in a unified manner rather than a ∗ Work done when X. Li interning at Tencent AI Lab. L. Liu is the corresponding author. Figure 1: A running example to motivate the proposed model. (a) The baseline obtai"
N18-1125,D17-1013,0,0.0215987,"n. βi is derived from Figure 3, zi is from Eq.(10-11), and other nodes are similar to ones in Figure 2. obtained from Eq.(10-11) and the prediction model as shown in Figure 3. Note that the proposed TFA-NMT models the target foresight word, which is a future word regarding to the current time step, to conduct attention calculation. In this sense, it employs the idea of modeling future and thus resembles to the work in (Zheng et al., 2017). The main difference is that TFA-NMT models the future from the target side whereas Zheng et al. (2017) models the future from the source side. In addition, Weng et al. (2017) imposes a regularization term by using future words during training. Unlike our approach, their approach does not use future words during the inference because these words are unavailable. Anyway, it is possible to put both their approach and our approach together for further improvements. 3.3 Suppose {⟨ k k a kset ⟩ of training data } is denoted by x , y , u |k = 1, · · · , K . Here xk , yk and uk denotes a source sentence, a target sentence and a POS tag sequence of yk , respectively. Then one can jointly train both the translation model for yk and the prediction model for uk by minimizing"
N18-1125,1983.tc-1.13,0,0.573571,"Missing"
N18-1125,Q18-1011,1,0.889688,"Missing"
N19-1046,D18-1036,0,0.0671355,"Missing"
N19-1046,W18-5451,0,0.0291084,"sistent improvements (up to +1.3 BLEU) compared to the state-of-the-art translation model. 1 apple … cola … dog … wolf … 4 0000 0001 0100 0111 3 000 of … in … 010 001 depth task 1100 1101 rice … cat … 4 run … 011 110 111 3 is … 2 00 01 10 11 2 Structural HR HR 1 x, y?< 0 1 1 root Figure 1: The structural hierarchical regularization framework. On the left is a 4-layer NMT decoder; on the right is a hierarchical clustering tree and the treeinduced relative tasks at every tree depth. to understand the hidden representations through the lens of a few linguistic tasks, while Ding et al. (2017) and Strobelt et al. (2018) propose appealing visualization approaches to understand NMT models including the representation of hidden layers. However, employing the analyses to motivate new methods for better translation, the ultimate goal of understanding NMT, is not achieved in these works. In our paper, we aim at understanding the hidden representation of NMT from an alternative viewpoint, and particularly we propose simple yet effective methods to improve the translation performance based on our understanding. We start from a fundamental question: what are the characteristics of the hidden representation for better"
N19-1046,P18-2104,0,0.0318354,"wledge about part-of-speech and semantic tags at different layers. Unlike those works that employ one or two linguistic tasks, we instead construct plenty of artificial tasks without any human annotations to analyze the hidden representations. This makes our approach more general and may potentially lead to less biased conclusions. Based on our understanding of the hidden representations, we further develop simple methods to improve NMT through representation regularization. Many works regularize NMT with lexical knowledge such as BOW (Weng et al., 2017) and morphology (Niehues and Cho, 2017; Zaremoodi et al., 2018), or syntactic knowledge (Kiperwasser and Ballesteros, 2018; Eriguchi et al., 2017). One significant difference is that we take into account the structure among plenty of artificial tasks and design a well motivated regularization term to encourage the structural consistency of tasks, which further improves NMT performance. In addition, our coarse-to-fine way to select tasks for regularization is also inspired by recent works using a coarse-to-fine mechanism for learning better word embeddings in NMT (Zhang et al., 2018) and predicting intermediate solutions for semantic parsing (Dong and Lapa"
P13-1078,1997.tmi-1.19,0,0.396779,"Missing"
P13-1078,D08-1024,0,0.0472501,"runing method as the log-linear model. 4 Algorithm 1 Mini-batch conjugate subgradient Input: θ1 , T , CGIter, batch-size, k-best-list 1: for all t such that 1 ≤ t ≤ T do 2: Sample mini-batch preference pairs with size batch-size from k-best-list 3: Calculate some quantities for CG, e.g. training objective Obj, subgradient ∆, according to Eq. (6) defined over the sampled preference pairs 4: θt+1 = CG(θt , Obj, ∆, CGIter) 5: end for Output: θT +1 Training Method 4.1 Training Objective For the log-linear model, there are various tuning methods, e.g. MERT (Och, 2003), MIRA (Watanabe et al., 2007; Chiang et al., 2008), PRO (Hopkins and May, 2011) and so on, which iteratively optimize a weight such that, after re-ranking a k-best list of a given development set with this weight, the loss of the resulting 1-best list is minimal. In the extreme, if the k-best list consists only of a pair of translations hhe∗ , d∗ i, he0 , d0 ii, the desirable weight should satisfy the assertion: if the BLEU score of e∗ is greater than that of e0 , then the model score of he∗ , d∗ i with this weight will be also greater than that of he0 , d0 i. In this paper, a pair he∗ , e0 i for a source sentence f is called as a preference"
P13-1078,P05-1033,0,0.522224,"ng; we also propose pre-training and post-training methods to avoid poor local minima. The biggest contribution of this paper is that it goes beyond the log-linear model and proposes a non-linear translation model instead of re-ranking model (Duh and Kirchhoff, 2008; Sokolov et al., 2012). On both Chinese-to-English and Japanese-toEnglish translation tasks, experiment results show that our model can leverage the shortcomings suffered by the log-linear model, and thus achieves significant improvements over the log-linear based translation. 2 a collection of synchronous rules for Hiero grammar (Chiang, 2005), or phrase pairs in Moses (Koehn et al., 2007); h(f, e, d) = (h1 (f, e, d), h2 (f, e, d), · · · , hK (f, e, d))> is a K-dimensional feature vector defined on the tuple hf, e, di; W = (w1 , w2 , · · · , wK )> is a Kdimensional weight vector of h, i.e., the parameters of the model, and it can be tuned by the toolkit MERT (Och, 2003). Different from Brown’s generative model (Brown et al., 1993), the loglinear model does not assume strong independency holds, and allows arbitrary features to be integrated into the model easily. In other words, it can transform complex language translation into fea"
P13-1078,J07-2003,0,0.830393,", e.g. the decoding efficiency, into account in SMT. Decoding in SMT is considered as the expansion of translation states and it is handled by a heuristic search (Koehn, 2004a). In the search procedure, frequent computation of the model score is needed for the search heuristic function, which will be challenged by the decoding efficiency for the neural network based translation model. Further, decoding with non-local (or state-dependent) features, such as a language model, is also a problem. Actually, even for the (log-) linear model, efficient decoding with the language model is not trivial (Chiang, 2007). In this paper, we propose a variant of neural networks, i.e. additive neural networks (see Section 3 for details), for SMT. It consists of two components: a linear component which captures nonlocal (or state dependent) features and a non-linear component (i.e., neural nework) which encodes loMost statistical machine translation (SMT) systems are modeled using a loglinear framework. Although the log-linear model achieves success in SMT, it still suffers from some limitations: (1) the features are required to be linear with respect to the model itself; (2) features cannot be further interprete"
P13-1078,P11-2031,0,0.0314142,") as 10 and 30, and M axIter as 16 and 20 in Algorithm 2, for Chinese-to-English and Japanese-to-English tasks, respectively. Although there are several parameters in AdNN which may limit its practicability, according to many of our internal studies, most parameters are insensitive to AdNN except λ and M axIter, which are common in other tuning toolkits such as MIRA and can be tuned5 on a development test dataset. Since both MERT and PRO tuning toolkits involve randomness in their implementations, all BLEU scores reported in the experiments are the average of five tuning runs, as suggested by Clark et al. (2011) for fairer comparisons. For AdNN, we report the averaged scores of five post-training runs, but both pre-training and training are performed only once. 5.2 NIST08 17.42+ 18.33+ 18.20+ 19.42 test4 23.68+ 23.66+ 24.03+ 24.45 Table 2: The BLEU comparisons between AdNNHiero-E and Log-linear translation models on the Chinese-to-English and Japanese-to-English tasks. + means the comparison is significant over AdNN-Hiero-E with p &lt; 0.05. these features are not dependent on the translation states, they are computed and saved to memory when loading the translation model. During decoding, we just look"
P13-1078,W09-0438,0,0.0124299,"computational times for the features in  the hidden units, i.e. σ M · h0 (r) + B . Since 5 For easier tuning, we tuned these two parameters on a given development test set without post-training in Algorithm 2. 797 Chinese-to-English NIST05 NIST06 L-Hiero 25.57+ 25.27+ 25.93 AdNN-Hiero-E 26.37 AdNN-Hiero-D 26.21 26.07 Japanese-to-English test2 test3 L-Hiero 24.38 25.55 AdNN-Hiero-E 25.14+ 26.32+ AdNN-Hiero-D 24.42 25.46 model (Bengio et al., 2003); POS, Chunking, NER, and SRL (Collobert and Weston, 2008); Parsing (Collobert and Weston, 2008; Socher et al., 2011); and Machine transliteration (Deselaers et al., 2009). Our work is, of course, highly motivated by these works. Unlike these works, we propose a variant neural network, i.e. additive neural networks, starting from SMT itself and taking both of the model definition and its inference (decoding) together into account. Our variant of neural network, AdNN, is highly related to both additive models (Buja et al., 1989) and generalized additive neural networks (Potts, 1999; Waal and Toit, 2007), in which an additive term is either a linear model or a neural network. Unlike additive models and generalized additive neural networks, our model is decomposab"
P13-1078,P08-2010,0,0.66897,", Taro Watanabe2 , Eiichiro Sumita2 , Tiejun Zhao1 1 School of Computer Science and Technology Harbin Institute of Technology (HIT), Harbin, China 2 National Institute of Information and Communication Technology (NICT) 3-5 Hikari-dai, Seika-cho, Soraku-gun, Kyoto, Japan {lmliu |tjzhao}@mtlab.hit.edu.cn {taro.watanabe |eiichiro.sumita}@nict.go.jp Abstract On the one hand, features are required to be linear with respect to the objective of the translation model (Nguyen et al., 2007), but it is not guaranteed that the potential features be linear with the model. This induces modeling inadequacy (Duh and Kirchhoff, 2008), in which the translation performance may not improve, or may even decrease, after one integrates additional features into the model. On the other hand, it cannot deeply interpret its surface features, and thus can not efficiently develop the potential of these features. What may happen is that a feature p does initially not improve the translation performance, but after a nonlinear operation, e.g. log(p), it does. The reason is not because this feature is useless but the model does not efficiently interpret and represent it. Situations such as this confuse explanations for feature designing,"
P13-1078,D11-1125,0,0.0808631,"near model. 4 Algorithm 1 Mini-batch conjugate subgradient Input: θ1 , T , CGIter, batch-size, k-best-list 1: for all t such that 1 ≤ t ≤ T do 2: Sample mini-batch preference pairs with size batch-size from k-best-list 3: Calculate some quantities for CG, e.g. training objective Obj, subgradient ∆, according to Eq. (6) defined over the sampled preference pairs 4: θt+1 = CG(θt , Obj, ∆, CGIter) 5: end for Output: θT +1 Training Method 4.1 Training Objective For the log-linear model, there are various tuning methods, e.g. MERT (Och, 2003), MIRA (Watanabe et al., 2007; Chiang et al., 2008), PRO (Hopkins and May, 2011) and so on, which iteratively optimize a weight such that, after re-ranking a k-best list of a given development set with this weight, the loss of the resulting 1-best list is minimal. In the extreme, if the k-best list consists only of a pair of translations hhe∗ , d∗ i, he0 , d0 ii, the desirable weight should satisfy the assertion: if the BLEU score of e∗ is greater than that of e0 , then the model score of he∗ , d∗ i with this weight will be also greater than that of he0 , d0 i. In this paper, a pair he∗ , e0 i for a source sentence f is called as a preference pair for f . Following PRO, w"
P13-1078,P02-1040,0,0.103298,"ts are called test1, test2, test3 and test4, respectively. We run GIZA++ (Och and Ney, 2000) on the training corpus in both directions (Koehn et al., 2003) to obtain the word alignment for each sentence pair. Using the SRILM Toolkits (Stolcke, 2002) with modified Kneser-Ney smoothing, we train a 4-gram language model for the Chinese-toEnglish task on the Xinhua portion of the English Gigaword corpus and a 4-gram language model for the Japanese-to-English task on the target side of its training data. In our experiments, the translation performances are measured by case-sensitive BLEU4 metric4 (Papineni et al., 2002). The significance testing is performed by paired bootstrap re-sampling (Koehn, 2004b). We use an in-house developed hierarchical phrase-based translation (Chiang, 2005) for our baseline system, which shares the similar setting as Hiero (Chiang, 2005), e.g. beam-size=100, kbest-size=100, and is denoted as L-Hiero to emphasize its log-linear model. We tune L-Hiero with two methods MERT and PRO implemented in the Moses toolkit. On the same experiment settings, the performance of L-Hiero is comparable Training Algorithm Algorithm 2 Training Algorithm Input: M axIter, a dev set, parameters (e.g. λ"
P13-1078,N03-1017,0,0.050145,"(news domain) with about 240k sentence pairs; the development set is the NIST02 evaluation data; the development test set is NIST05; and the test datasets are NIST06, and NIST08. For the Japanese-to-English task, the training data with 300k sentence pairs is from the NTCIR-patent task (Fujii et al., 2010); the development set, development test set, and two test sets are averagely extracted from a given development set with 4000 sentences, and these four datasets are called test1, test2, test3 and test4, respectively. We run GIZA++ (Och and Ney, 2000) on the training corpus in both directions (Koehn et al., 2003) to obtain the word alignment for each sentence pair. Using the SRILM Toolkits (Stolcke, 2002) with modified Kneser-Ney smoothing, we train a 4-gram language model for the Chinese-toEnglish task on the Xinhua portion of the English Gigaword corpus and a 4-gram language model for the Japanese-to-English task on the target side of its training data. In our experiments, the translation performances are measured by case-sensitive BLEU4 metric4 (Papineni et al., 2002). The significance testing is performed by paired bootstrap re-sampling (Koehn, 2004b). We use an in-house developed hierarchical phr"
P13-1078,P07-2045,0,0.00938653,"-training methods to avoid poor local minima. The biggest contribution of this paper is that it goes beyond the log-linear model and proposes a non-linear translation model instead of re-ranking model (Duh and Kirchhoff, 2008; Sokolov et al., 2012). On both Chinese-to-English and Japanese-toEnglish translation tasks, experiment results show that our model can leverage the shortcomings suffered by the log-linear model, and thus achieves significant improvements over the log-linear based translation. 2 a collection of synchronous rules for Hiero grammar (Chiang, 2005), or phrase pairs in Moses (Koehn et al., 2007); h(f, e, d) = (h1 (f, e, d), h2 (f, e, d), · · · , hK (f, e, d))> is a K-dimensional feature vector defined on the tuple hf, e, di; W = (w1 , w2 , · · · , wK )> is a Kdimensional weight vector of h, i.e., the parameters of the model, and it can be tuned by the toolkit MERT (Och, 2003). Different from Brown’s generative model (Brown et al., 1993), the loglinear model does not assume strong independency holds, and allows arbitrary features to be integrated into the model easily. In other words, it can transform complex language translation into feature engineering: it can achieve high translati"
P13-1078,C12-2104,0,0.00594537,"model or a neural network. Unlike additive models and generalized additive neural networks, our model is decomposable with respect to translation rules rather than its component variables considering the decoding efficiency of machine translation; and it allows its additive terms of neural networks to share the same parameters for a compact structure to avoid sparsity. The idea of the neural network in machine translation has already been pioneered in previous works. Casta˜no et al. (1997) introduced a neural network for example-based machine translation. In particular, Son et al. (2012) and Schwenk (2012) employed a neural network to model the phrase translation probability on the rule level hα, γi instead of the bilingual sentence level hf, ei as in Eq. (5), and thus they did not go beyond the log-linear model for SMT. There are also works which exploit non-linear models in SMT. Duh and Kirchhoff (2008) proposed a boosting re-ranking algorithm using MERT as a week learner to improve the model’s expressive abilities; Sokolov et al. (2012) similarly proposed a boosting re-ranking method from the ranking perspective rather than the classification perspective. Instead of considering the reranking"
P13-1078,koen-2004-pharaoh,0,0.192436,"fter a nonlinear operation, e.g. log(p), it does. The reason is not because this feature is useless but the model does not efficiently interpret and represent it. Situations such as this confuse explanations for feature designing, since it is unclear whether such a feature contributes to a translation or not. A neural network (Bishop, 1995) is a reasonable method to overcome the above shortcomings. However, it should take constraints, e.g. the decoding efficiency, into account in SMT. Decoding in SMT is considered as the expansion of translation states and it is handled by a heuristic search (Koehn, 2004a). In the search procedure, frequent computation of the model score is needed for the search heuristic function, which will be challenged by the decoding efficiency for the neural network based translation model. Further, decoding with non-local (or state-dependent) features, such as a language model, is also a problem. Actually, even for the (log-) linear model, efficient decoding with the language model is not trivial (Chiang, 2007). In this paper, we propose a variant of neural networks, i.e. additive neural networks (see Section 3 for details), for SMT. It consists of two components: a li"
P13-1078,W04-3250,0,0.540761,"fter a nonlinear operation, e.g. log(p), it does. The reason is not because this feature is useless but the model does not efficiently interpret and represent it. Situations such as this confuse explanations for feature designing, since it is unclear whether such a feature contributes to a translation or not. A neural network (Bishop, 1995) is a reasonable method to overcome the above shortcomings. However, it should take constraints, e.g. the decoding efficiency, into account in SMT. Decoding in SMT is considered as the expansion of translation states and it is handled by a heuristic search (Koehn, 2004a). In the search procedure, frequent computation of the model score is needed for the search heuristic function, which will be challenged by the decoding efficiency for the neural network based translation model. Further, decoding with non-local (or state-dependent) features, such as a language model, is also a problem. Actually, even for the (log-) linear model, efficient decoding with the language model is not trivial (Chiang, 2007). In this paper, we propose a variant of neural networks, i.e. additive neural networks (see Section 3 for details), for SMT. It consists of two components: a li"
P13-1078,2012.amta-papers.17,0,0.553532,"presenting each word as a feature vector (Collobert and Weston, 2008). Because of the thousands of parameters and the non-convex objective in our model, efficient training is not simple. We propose an efficient training methodology: we apply the mini-batch conjugate sub-gradient algorithm (Le et al., 2011) to accelerate the training; we also propose pre-training and post-training methods to avoid poor local minima. The biggest contribution of this paper is that it goes beyond the log-linear model and proposes a non-linear translation model instead of re-ranking model (Duh and Kirchhoff, 2008; Sokolov et al., 2012). On both Chinese-to-English and Japanese-toEnglish translation tasks, experiment results show that our model can leverage the shortcomings suffered by the log-linear model, and thus achieves significant improvements over the log-linear based translation. 2 a collection of synchronous rules for Hiero grammar (Chiang, 2005), or phrase pairs in Moses (Koehn et al., 2007); h(f, e, d) = (h1 (f, e, d), h2 (f, e, d), · · · , hK (f, e, d))> is a K-dimensional feature vector defined on the tuple hf, e, di; W = (w1 , w2 , · · · , wK )> is a Kdimensional weight vector of h, i.e., the parameters of the m"
P13-1078,N12-1005,0,0.0332495,"erm is either a linear model or a neural network. Unlike additive models and generalized additive neural networks, our model is decomposable with respect to translation rules rather than its component variables considering the decoding efficiency of machine translation; and it allows its additive terms of neural networks to share the same parameters for a compact structure to avoid sparsity. The idea of the neural network in machine translation has already been pioneered in previous works. Casta˜no et al. (1997) introduced a neural network for example-based machine translation. In particular, Son et al. (2012) and Schwenk (2012) employed a neural network to model the phrase translation probability on the rule level hα, γi instead of the bilingual sentence level hf, ei as in Eq. (5), and thus they did not go beyond the log-linear model for SMT. There are also works which exploit non-linear models in SMT. Duh and Kirchhoff (2008) proposed a boosting re-ranking algorithm using MERT as a week learner to improve the model’s expressive abilities; Sokolov et al. (2012) similarly proposed a boosting re-ranking method from the ranking perspective rather than the classification perspective. Instead of consid"
P13-1078,W07-0710,0,0.699181,"Missing"
P13-1078,P10-1040,0,0.0218209,"ine σ as a multilayer neural network. Again for the example shown in Figure 1, the model score defined in Eq. (5) for the pair he2 , d2 i can be represented as follows: because they empirically perform well in the loglinear model. For the local feature vector h0 in Eq (5), we employ word embedding features as described in the following subsection. 3.3 Word Embedding features for AdNN Word embedding can relax the sparsity introduced by the lexicalization in NLP, and it improves the systems for many tasks such as language model, named entity recognition, and parsing (Collobert and Weston, 2008; Turian et al., 2010; Collobert, 2011). Here, we propose embedding features for rules in SMT by combining word embeddings. Firstly, we will define the embedding for the source side α of a rule r : X → hα, γi. Let VS be the vocabulary in the source language with size |VS |; Rn×|VS |be the word embedding matrix, each column of which is the word embedding (ndimensional vector) for the corresponding word in VS ; and maxSource be the maximal length of α for all rules. We further assume that the α for all rules share the same length as maxSource; otherwise, we add maxSource − |α |words “N U LL” to the end of α to obtai"
P13-1078,P00-1056,0,0.0517744,"Chinese-to-English task, the training data is the FBIS corpus (news domain) with about 240k sentence pairs; the development set is the NIST02 evaluation data; the development test set is NIST05; and the test datasets are NIST06, and NIST08. For the Japanese-to-English task, the training data with 300k sentence pairs is from the NTCIR-patent task (Fujii et al., 2010); the development set, development test set, and two test sets are averagely extracted from a given development set with 4000 sentences, and these four datasets are called test1, test2, test3 and test4, respectively. We run GIZA++ (Och and Ney, 2000) on the training corpus in both directions (Koehn et al., 2003) to obtain the word alignment for each sentence pair. Using the SRILM Toolkits (Stolcke, 2002) with modified Kneser-Ney smoothing, we train a 4-gram language model for the Chinese-toEnglish task on the Xinhua portion of the English Gigaword corpus and a 4-gram language model for the Japanese-to-English task on the target side of its training data. In our experiments, the translation performances are measured by case-sensitive BLEU4 metric4 (Papineni et al., 2002). The significance testing is performed by paired bootstrap re-samplin"
P13-1078,P02-1038,0,0.904636,"a neural network is not trivial, especially when taking the decoding efficiency into consideration. In this paper, we propose a variant of a neural network, i.e. additive neural networks, for SMT to go beyond the log-linear translation model. In addition, word embedding is employed as the input to the neural network, which encodes each word as a feature vector. Our model outperforms the log-linear translation models with/without embedding features on Chinese-to-English and Japanese-to-English translation tasks. 1 Introduction Recently, great progress has been achieved in SMT, especially since Och and Ney (2002) proposed the log-linear model: almost all the stateof-the-art SMT systems are based on the log-linear model. Its most important advantage is that arbitrary features can be added to the model. Thus, it casts complex translation between a pair of languages as feature engineering, which facilitates research and development for SMT. Regardless of how successful the log-linear model is in SMT, it still has some shortcomings. This joint work was done while the first author visited NICT. 791 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 791–801, c Sof"
P13-1078,D07-1080,1,0.791639,"rch strategy and cube pruning method as the log-linear model. 4 Algorithm 1 Mini-batch conjugate subgradient Input: θ1 , T , CGIter, batch-size, k-best-list 1: for all t such that 1 ≤ t ≤ T do 2: Sample mini-batch preference pairs with size batch-size from k-best-list 3: Calculate some quantities for CG, e.g. training objective Obj, subgradient ∆, according to Eq. (6) defined over the sampled preference pairs 4: θt+1 = CG(θt , Obj, ∆, CGIter) 5: end for Output: θT +1 Training Method 4.1 Training Objective For the log-linear model, there are various tuning methods, e.g. MERT (Och, 2003), MIRA (Watanabe et al., 2007; Chiang et al., 2008), PRO (Hopkins and May, 2011) and so on, which iteratively optimize a weight such that, after re-ranking a k-best list of a given development set with this weight, the loss of the resulting 1-best list is minimal. In the extreme, if the k-best list consists only of a pair of translations hhe∗ , d∗ i, he0 , d0 ii, the desirable weight should satisfy the assertion: if the BLEU score of e∗ is greater than that of e0 , then the model score of he∗ , d∗ i with this weight will be also greater than that of he0 , d0 i. In this paper, a pair he∗ , e0 i for a source sentence f is c"
P13-1078,P10-1076,0,0.011635,"ural network to model the phrase translation probability on the rule level hα, γi instead of the bilingual sentence level hf, ei as in Eq. (5), and thus they did not go beyond the log-linear model for SMT. There are also works which exploit non-linear models in SMT. Duh and Kirchhoff (2008) proposed a boosting re-ranking algorithm using MERT as a week learner to improve the model’s expressive abilities; Sokolov et al. (2012) similarly proposed a boosting re-ranking method from the ranking perspective rather than the classification perspective. Instead of considering the reranking task in SMT, Xiao et al. (2010) employed a boosting method for the system combination in SMT. Unlike their post-processing models (either a re-ranking or a system combination model) in SMT, we propose a non-linear translation model which can be easily incorporated into the existing SMT framework. NIST08 18.33+ 19.42 19.54 test4 23.66 24.45+ 23.73 Table 3: The effect of different feature setting on AdNN model. + means the comparison is significant over AdNN-Hiero-D with p &lt; 0.05. both test sets. In addition, to investigate the effect of different feature settings on AdNN, we alternatively design another setting for h0 in Eq."
P13-1078,J93-2003,0,\N,Missing
P13-1078,P03-1021,0,\N,Missing
P18-2025,W08-0312,0,0.0173426,"man correlation of BLEU-1, METEOR, and W-METEOR, showing that the BLEU-1 scores vary a lot given any fixed human score, appearing to be random noise, while the METEOR family exhibit strong consistency with human scores. Compared to W-METEOR, METEOR deviates from the regression line more frequently, esp. by assigning unexpectedly high scores to comments with low human grades. Notably, the best automatic metric, WMETEOR, achieves 0.59 Spearman and 0.57 Pearson, which is higher or comparable to automatic metrics in other generation tasks (Lowe et al., 2017; Liu et al., 2016; Sharma et al., 2017; Agarwal and Lavie, 2008), indicating a good supplement to human judgment for efficient evaluation and comparison. We use the metrics to evaluate the above models in the supplements. 6 Conclusions and Future Work We have introduced the new task and dataset for automatic article commenting, as well as developed quality-weighted automatic metrics that leverage valuable human bias on comment quality. The dataset and the study of metrics establish a testbed for the article commenting task. We are excited to study solutions for the task in the future, by building advanced deep generative models (Goodfellow et al., 2016; Hu"
P18-2025,P15-1034,0,0.0136403,"ments. 6 Conclusions and Future Work We have introduced the new task and dataset for automatic article commenting, as well as developed quality-weighted automatic metrics that leverage valuable human bias on comment quality. The dataset and the study of metrics establish a testbed for the article commenting task. We are excited to study solutions for the task in the future, by building advanced deep generative models (Goodfellow et al., 2016; Hu et al., 2018) that incorporate effective reading comprehension modules (Rajpurkar et al., 2016; Richardson et al., 2013) and rich external knowledge (Angeli et al., 2015; Hu et al., 2016). The large dataset is also potentially useful for a variety of other tasks, such as comment ranking (Hsu et al., 2009), upvotes prediction (Rizos et al., 2016), and article headline generation (Banko et al., 2000). We encourage the use of the dataset in these context. Acknowledgement. We would like to thank anonymous reviewers for their helpful suggestions and particularly the annotators for their contributions on the dataset. Hai Zhao was partially supported by National Key Research and Development Program of China (No. 2017YFB0304100), National Natural Science Foundation o"
P18-2025,W05-0909,0,0.493806,"e articles. Article commenting also differs from making product reviews (Tang et al., 2017; Li et al., 2017), as the latter takes structured data (e.g., product attributes) as input; while the input of article commenting is in plain text format, posing a much larger input space to explore. In this paper, we propose the new task of automatic article commenting, and release a largescale Chinese corpus with a human-annotated subset for scientific research and evaluation. We further develop a general approach of enhancing popular automatic metrics, such as BLEU (Papineni et al., 2002) and METEOR (Banerjee and Lavie, 2005), to better fit the characteristics of the new task. In recent years, enormous efforts have been made in different contexts that analyze one or more aspects of online comments. For example, Kolhatkar and Taboada (2017) identify constructive news comments; Barker et al. (2016) study human summaries of online comment conversations. The datasets used in these works are typically not directly applicable in the context of article commenting, and are small in scale that is unable to support the unique complexity of the new task. In contrast, our dataset consists of around 200K news articles and 4.5M"
P18-2025,P00-1041,0,0.198504,"Missing"
P18-2025,W16-3605,0,0.288313,"explore. In this paper, we propose the new task of automatic article commenting, and release a largescale Chinese corpus with a human-annotated subset for scientific research and evaluation. We further develop a general approach of enhancing popular automatic metrics, such as BLEU (Papineni et al., 2002) and METEOR (Banerjee and Lavie, 2005), to better fit the characteristics of the new task. In recent years, enormous efforts have been made in different contexts that analyze one or more aspects of online comments. For example, Kolhatkar and Taboada (2017) identify constructive news comments; Barker et al. (2016) study human summaries of online comment conversations. The datasets used in these works are typically not directly applicable in the context of article commenting, and are small in scale that is unable to support the unique complexity of the new task. In contrast, our dataset consists of around 200K news articles and 4.5M human comments along with rich meta data for article categories and user votes of comments. Different from traditional text generation tasks such as machine translation (Brown et al., 1990) that has a relatively small set of gold targets, human comments on an article live in"
P18-2025,J90-2002,0,0.751023,"ents. For example, Kolhatkar and Taboada (2017) identify constructive news comments; Barker et al. (2016) study human summaries of online comment conversations. The datasets used in these works are typically not directly applicable in the context of article commenting, and are small in scale that is unable to support the unique complexity of the new task. In contrast, our dataset consists of around 200K news articles and 4.5M human comments along with rich meta data for article categories and user votes of comments. Different from traditional text generation tasks such as machine translation (Brown et al., 1990) that has a relatively small set of gold targets, human comments on an article live in much larger space by involving diverse topics and personal views, and critically, are of vary151 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 151–156 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics Title: ˘úl¯iPhone 8 —⇤⇢ö(9 &gt;L (Apple’s iPhone 8 event is happening in Sept.) Score Criteria Content: ˘úl¯c✏⌘íS—⇤Ä˜ ˝ £⇤⌃é9 12ÂÏ ˘ú∞¡—⇤ ⇢ Âl¯⌃—⇤↵ „iPhone èKÙ∞ Ñÿ ˘úKh ˘úTV åiOSoˆ⇥Ÿ !—⇤⇢⌃&e &gt;∞iPhones⇢&OLED&gt; :O"
P18-2025,P15-2073,0,0.0206468,"ating comments, and develop a dataset that is orders-of-magnitude larger than previous related corpus. Instead of restricting to one or few specific aspects, we focus on the general comment quality aligned with human judgment, and provide over 27 gold references for each data instance to enable wide-coverage evaluation. Such setting also allows a large output space, and makes the task challenging and valuable for text generation research. Yao et al. (2017) explore defense approaches of spam or malicious reviews. We believe the proposed task and dataset can be potentially useful for the study. Galley et al. (2015) propose BLEU that weights multiple references for conversation generation evaluation. The quality weighted metrics developed in our work can be seen as a generalization of BLEU to many popular reference-based metrics (e.g., METEOR, ROUGE, and CIDEr). Our human survey demonstrates the effectiveness of the generalized metrics in the article commenting task. 3 Article Commenting Dataset The dataset is collected from Tencent News (news.qq.com), one of the most popular Chinese websites of news and opinion articles. Table 1 shows an example data instance in the dataset (For readability we also prov"
P18-2025,X98-1026,0,0.208952,"foster online communities. Besides, commenting on articles is one of the increasingly demanded skills of intelligent chatbot (Shum et al., 2018) to enable in-depth, content-rich conversations with humans. Article commenting poses new challenges for machines, as it involves multiple cognitive abil⇤ Work done while Lianhui interned at Tencent AI Lab The dataset is available on http://ai.tencent. com/upload/PapersUploads/article_ commenting.tgz 1 ities: understanding the given article, formulating opinions and arguments, and organizing natural language for expression. Compared to summarization (Hovy and Lin, 1998), a comment does not necessarily cover all salient ideas of the article; instead it is often desirable for a comment to carry additional information not explicitly presented in the articles. Article commenting also differs from making product reviews (Tang et al., 2017; Li et al., 2017), as the latter takes structured data (e.g., product attributes) as input; while the input of article commenting is in plain text format, posing a much larger input space to explore. In this paper, we propose the new task of automatic article commenting, and release a largescale Chinese corpus with a human-annot"
P18-2025,D13-1020,0,0.0324806,"the metrics to evaluate the above models in the supplements. 6 Conclusions and Future Work We have introduced the new task and dataset for automatic article commenting, as well as developed quality-weighted automatic metrics that leverage valuable human bias on comment quality. The dataset and the study of metrics establish a testbed for the article commenting task. We are excited to study solutions for the task in the future, by building advanced deep generative models (Goodfellow et al., 2016; Hu et al., 2018) that incorporate effective reading comprehension modules (Rajpurkar et al., 2016; Richardson et al., 2013) and rich external knowledge (Angeli et al., 2015; Hu et al., 2016). The large dataset is also potentially useful for a variety of other tasks, such as comment ranking (Hsu et al., 2009), upvotes prediction (Rizos et al., 2016), and article headline generation (Banko et al., 2000). We encourage the use of the dataset in these context. Acknowledgement. We would like to thank anonymous reviewers for their helpful suggestions and particularly the annotators for their contributions on the dataset. Hai Zhao was partially supported by National Key Research and Development Program of China (No. 2017Y"
P18-2025,W17-3002,0,0.139171,"nting is in plain text format, posing a much larger input space to explore. In this paper, we propose the new task of automatic article commenting, and release a largescale Chinese corpus with a human-annotated subset for scientific research and evaluation. We further develop a general approach of enhancing popular automatic metrics, such as BLEU (Papineni et al., 2002) and METEOR (Banerjee and Lavie, 2005), to better fit the characteristics of the new task. In recent years, enormous efforts have been made in different contexts that analyze one or more aspects of online comments. For example, Kolhatkar and Taboada (2017) identify constructive news comments; Barker et al. (2016) study human summaries of online comment conversations. The datasets used in these works are typically not directly applicable in the context of article commenting, and are small in scale that is unable to support the unique complexity of the new task. In contrast, our dataset consists of around 200K news articles and 4.5M human comments along with rich meta data for article categories and user votes of comments. Different from traditional text generation tasks such as machine translation (Brown et al., 1990) that has a relatively small"
P18-2025,W04-1013,0,0.0457846,"Missing"
P18-2025,D16-1230,0,0.116055,"ead articles using an encoder and generate comments using a decoder with or without attentions (Bahdanau et al., 2014), which are denoted as Seq2seq and Att if only article titles are read. We also set up an attentional sequence-tosequence model that reads full article title/content, and denote with Att-TC. Again, these approaches are mainly for demonstration purpose and for evaluating the metrics, and are far from solving the difficult commenting task. We discard comments with over 50 words and use a truncated vocabulary of size 30K. Results We follow previous setting (Papineni et al., 2002; Liu et al., 2016; Lowe et al., 2017) to evaluate the metrics, by conducting human evaluations and calculating the correlation between the scores assigned by humans and the metrics. Specifically, for each article in the test set, we obtained six comments, five of which come from IRT, IR-TC, Seq2seq, Att, and Att-TC, respectively, and one randomly drawn from real comments that are different from the reference comments. The comments were then graded by human annotators following the same procedure of test set scoring (sec.3). Meanwhile, we measure each comment with the vanilla and weighted automatic metrics base"
P18-2025,P17-1103,0,0.329605,"c be a generated comment to evaluate, R = {rj } the set of references, each of which has a quality score sj by human annotators. We assume properly normalized sj 2 [0, 1]. Due to space limitations, here we only present the enhanced METEOR, and defer the formulations of enhancing BLEU, ROUGE, and CIDEr to the supplements. Specifically, METEOR performs word matching through an alignment between the candidate and references. The weighted METEOR extends the Figure 1: Scatter plots showing the correlation between metrics and human judgments. Left: BLEU1; Middle: METEOR; Right: W-METEOR. Following (Lowe et al., 2017), we added Gaussian noise drawn from N (0, 0.05) to the integer human scores to better visualize the density of points. original metric by weighting references with sj : W-METEOR(c, R) = (1 BP ) maxj sj Fmean,j , (1) where Fmean,j is a harmonic mean of the precision and recall between c and rj , and BP is the penalty (Banerjee and Lavie, 2005). Note that the new metrics fall back to the respective original metrics by setting sj = 1. 5 Experiments We demonstrate the use of the dataset and metrics with simple retrieval and generation models, and show the enhanced metrics consistently improve cor"
P18-2025,P02-1040,0,0.101647,"tion not explicitly presented in the articles. Article commenting also differs from making product reviews (Tang et al., 2017; Li et al., 2017), as the latter takes structured data (e.g., product attributes) as input; while the input of article commenting is in plain text format, posing a much larger input space to explore. In this paper, we propose the new task of automatic article commenting, and release a largescale Chinese corpus with a human-annotated subset for scientific research and evaluation. We further develop a general approach of enhancing popular automatic metrics, such as BLEU (Papineni et al., 2002) and METEOR (Banerjee and Lavie, 2005), to better fit the characteristics of the new task. In recent years, enormous efforts have been made in different contexts that analyze one or more aspects of online comments. For example, Kolhatkar and Taboada (2017) identify constructive news comments; Barker et al. (2016) study human summaries of online comment conversations. The datasets used in these works are typically not directly applicable in the context of article commenting, and are small in scale that is unable to support the unique complexity of the new task. In contrast, our dataset consists"
P18-2025,D16-1264,0,0.0517994,"and comparison. We use the metrics to evaluate the above models in the supplements. 6 Conclusions and Future Work We have introduced the new task and dataset for automatic article commenting, as well as developed quality-weighted automatic metrics that leverage valuable human bias on comment quality. The dataset and the study of metrics establish a testbed for the article commenting task. We are excited to study solutions for the task in the future, by building advanced deep generative models (Goodfellow et al., 2016; Hu et al., 2018) that incorporate effective reading comprehension modules (Rajpurkar et al., 2016; Richardson et al., 2013) and rich external knowledge (Angeli et al., 2015; Hu et al., 2016). The large dataset is also potentially useful for a variety of other tasks, such as comment ranking (Hsu et al., 2009), upvotes prediction (Rizos et al., 2016), and article headline generation (Banko et al., 2000). We encourage the use of the dataset in these context. Acknowledgement. We would like to thank anonymous reviewers for their helpful suggestions and particularly the annotators for their contributions on the dataset. Hai Zhao was partially supported by National Key Research and Development P"
P18-2025,P16-2032,0,0.144391,"Work There is a surge of interest in natural language generation tasks, such as machine translation (Brown et al., 1990; Bahdanau et al., 2014), dialog (Williams and Young, 2007; Shum et al., 2018), text manipulation (Hu et al., 2017), visual description generation (Vinyals et al., 2015; Liang et al., 2017), and so forth. Automatic article commenting poses new challenges due to the large input and output spaces and the open-domain nature 152 of comments. Many efforts have been devoted to studying specific attributes of reader comments, such as constructiveness, persuasiveness, and sentiment (Wei et al., 2016; Kolhatkar and Taboada, 2017; Barker et al., 2016). We introduce the new task of generating comments, and develop a dataset that is orders-of-magnitude larger than previous related corpus. Instead of restricting to one or few specific aspects, we focus on the general comment quality aligned with human judgment, and provide over 27 gold references for each data instance to enable wide-coverage evaluation. Such setting also allows a large output space, and makes the task challenging and valuable for text generation research. Yao et al. (2017) explore defense approaches of spam or malicious revi"
P18-2025,D17-1239,0,0.0214441,"Table 2 provides the key data statistics. The dataset has a vocabulary size of 1,858,452. The average lengths of the article titles and content are 15 and 554 Chinese words (not characters), respectively. The average comment length is 17 words. Notably, the dataset contains an enormous volume of tokens, and is orders-of-magnitude larger than previous public data of article comment analysis (Wei et al., 2016; Barker et al., 2016). Moreover, each article in the dataset has on average over 27 human-written comments. Compared to other popular text generation tasks and datasets (Chen et al., 2015; Wiseman et al., 2017) which typically contain no more than 5 gold references, our dataset enables richer guidance for model training and wider coverage for evaluation, in order to fit the unique large output space of the commenting task. Each article is associated with one of 44 categories, whose distribution is shown in the supplements. The number of upvotes per comment ranges from 3.4 to 5.9 on average. Though the numbers look small, the distribution exhibits a long-tail pattern with popular comments having thousands of upvotes. Test Set Comment Quality Annotations Real human comments are of varying quality. Sel"
P19-1124,W18-6318,0,0.0624945,"mple, Bahdanau et al. (2014) is the first work to show word alignment examples by using attention in an NMT model. Tu et al. (2016) quantitatively evaluate word alignment captured by attention and find that its quality is much worse than statistical word aligners. Motivated by this finding, Chen et al. (2016), Mi et al. (2016) and Liu et al. (2016) improve attention with the supervision from silver alignment results obtained by statistical aligners, in the hope that the improved attention leads to better word alignment and translation quality consequently. More recently, there are also works (Alkhouli et al., 2018) that directly model the alignment and use it to sharpen the attention to bias translation. Despite the close relation between word alignment and attention, Koehn and Knowles (2017) and Ghader and Monz (2017) discuss the differences between word alignment and attention in NMT. Most of these works study word alignment for the same kind of NMT models with a single attention layer. One of our contribution is that we propose modelagnostic methods to study word alignment in a general way which deliver better word alignment quality than attention method. Moreover, for the first time, we further unde"
P19-1124,D17-1042,0,0.0515093,"Missing"
P19-1124,W16-1601,0,0.0667088,"Missing"
P19-1124,P17-1080,0,0.0234942,"re-trained translation models with their EAMs trained on the same FAST A LIGN annotated data. We find that a stronger (higher BLEU) translation model generally obtains better alignment (lower AER). As shown in Table 2, T RANSFORMER -L6 generates much better alignment than T RANSFORMER -L1, highly correlated with their translation performances. This suggests that supervision is not enough to obtain good alignment and the hidden units learned by a translation model indeed implicitly capture alignment knowledge by learning translation. In addition, EAM can be thought as a kind of agnostic probe (Belinkov et al., 2017; Hewitt and Manning, 2019) to investigate how much alignment are implicitly learned in the hidden representations. Table 1: AER of the proposed methods. Models AER BLEU * L1 54.50 36.51 T RANSFORMER L2 L3 L4 L5 47.94 40.47 38.40 38.80 44.83 45.63 47.19 46.35 L6 38.88 46.95 Results are measured on ZH⇒EN task. Table 2: EAM on translation models with different number of layer. Explicit Alignment Model (EAM) As shown in Table 1, EAM outperforms alignment induced from attention by a large margin. However, since EAM employs silver alignment annotations from FAST A LIGN for training the additional p"
P19-1124,I17-1004,0,0.384173,"d sl&lt;i according to different NMT models. As the dominant models, attentional NMT models define the context vector cli as a weighted sum  of h, where the weight l αli = g sl−1 i , s&lt;i , h is defined by a similarity function. Due to the space limitation, we refer readers to Bahdanau et al. (2014), Gehring et al. (2017) and Vaswani et al. (2017) for the details on the definitions of f and g. 2.2 Alignment by Attention Since the attention weight αli,j measures the similarity between sl−1 and hj , it has been widely i used to evaluate the word alignment between yi and xj (Bahdanau et al., 2014; Ghader and Monz, 2017). Once an attentional NMT model has been trained, one can easily extract word alignment A from the attention weight α according to the style of maximum a posterior strategy (MAP) as follows:  1 j = arg max αi,j 0 j0 Ai,j (α) = , (3) 0 o/w where Ai,j = 1 indicates yi aligns to xj . For NMT models with multiple attentional heads attentional layers as in Vaswani et al. (2017), we sum all attention weights with respect to all heads to a single α before MAP in equation 3. 3 Methods to Inducing Word Alignment Although attention might obtain some word alignment as described in previous section, it"
P19-1124,P09-5002,0,0.264299,"gnostic to specific NMT models. Experiments show that both methods induce much better word alignment than attention. This paper further visualizes the translation through the word alignment induced by NMT. In particular, it analyzes the effect of alignment errors on translation errors at word level and its quantitative analysis over many testing examples consistently demonstrate that alignment errors are likely to lead to translation errors measured by different metrics. 1 Introduction Machine translation aims at modeling the semantic equivalence between a pair of source and target sentences (Koehn, 2009), and word alignment tries to model the semantic equivalence between a pair of source and target words (Och and Ney, 2003). As a sentence consists of words, word alignment is conceptually related to machine translation and such a relation can be traced back to the birth of statistical machine translation (SMT) (Brown et al., 1993), where word alignment is the basis of SMT models and its accuracy is generally helpful to improve translation quality (Koehn et al., 2003; Liu et al., 2005). In neural machine translation (NMT), it is also important to study word alignment, because word alignment pro"
P19-1124,W17-3204,0,0.0274187,"by attention and find that its quality is much worse than statistical word aligners. Motivated by this finding, Chen et al. (2016), Mi et al. (2016) and Liu et al. (2016) improve attention with the supervision from silver alignment results obtained by statistical aligners, in the hope that the improved attention leads to better word alignment and translation quality consequently. More recently, there are also works (Alkhouli et al., 2018) that directly model the alignment and use it to sharpen the attention to bias translation. Despite the close relation between word alignment and attention, Koehn and Knowles (2017) and Ghader and Monz (2017) discuss the differences between word alignment and attention in NMT. Most of these works study word alignment for the same kind of NMT models with a single attention layer. One of our contribution is that we propose modelagnostic methods to study word alignment in a general way which deliver better word alignment quality than attention method. Moreover, for the first time, we further understand NMT through alignment and particularly quantify the effect of alignment errors on translation errors for NMT. The prediction difference method in this paper actually provides"
P19-1124,N03-1017,0,0.16742,"nt metrics. 1 Introduction Machine translation aims at modeling the semantic equivalence between a pair of source and target sentences (Koehn, 2009), and word alignment tries to model the semantic equivalence between a pair of source and target words (Och and Ney, 2003). As a sentence consists of words, word alignment is conceptually related to machine translation and such a relation can be traced back to the birth of statistical machine translation (SMT) (Brown et al., 1993), where word alignment is the basis of SMT models and its accuracy is generally helpful to improve translation quality (Koehn et al., 2003; Liu et al., 2005). In neural machine translation (NMT), it is also important to study word alignment, because word alignment provides natural ways to understanding black-box NMT models and analyzing their translation errors (Ding et al., 2017). Prior researches ∗ Work done while X. Li interning at Tencent AI Lab. L. Liu is the corresponding author. observed that word alignment is captured by NMT through attention for recurrent neural network based NMT with a single attention layer (Bahdanau et al., 2014; Mi et al., 2016; Liu et al., 2016; Li et al., 2018). Unfortunately, we surprisingly find"
P19-1124,D16-1011,0,0.0381201,"with a single attention layer. One of our contribution is that we propose modelagnostic methods to study word alignment in a general way which deliver better word alignment quality than attention method. Moreover, for the first time, we further understand NMT through alignment and particularly quantify the effect of alignment errors on translation errors for NMT. The prediction difference method in this paper actually provides an avenue to understand and interpret neural machine translation models. Therefore, it is closely related to many works on visualizing and interpreting neural networks (Lei et al., 2016; Bach et al., 2015; Zintgraf et al., 2017). Indeed, our method is inherited from (Zintgraf et al., 2017), and our advantage is that it is computationally efficient particularly for those tasks with a large vocabulary. In sequence-to-sequence tasks, Ding et al. (2017) focus on model interpretability by modeling how influence propagates across 5 It is interesting that SMT (MOSES) incorrectly translates this word into ‘and’ in our preliminary experiment. hidden units in networks, which is often too restrictive and challenging to achieve as argued by Alvarez-Melis and Jaakkola (2017). And instead"
P19-1124,J93-2003,0,0.151498,"ysis over many testing examples consistently demonstrate that alignment errors are likely to lead to translation errors measured by different metrics. 1 Introduction Machine translation aims at modeling the semantic equivalence between a pair of source and target sentences (Koehn, 2009), and word alignment tries to model the semantic equivalence between a pair of source and target words (Och and Ney, 2003). As a sentence consists of words, word alignment is conceptually related to machine translation and such a relation can be traced back to the birth of statistical machine translation (SMT) (Brown et al., 1993), where word alignment is the basis of SMT models and its accuracy is generally helpful to improve translation quality (Koehn et al., 2003; Liu et al., 2005). In neural machine translation (NMT), it is also important to study word alignment, because word alignment provides natural ways to understanding black-box NMT models and analyzing their translation errors (Ding et al., 2017). Prior researches ∗ Work done while X. Li interning at Tencent AI Lab. L. Liu is the corresponding author. observed that word alignment is captured by NMT through attention for recurrent neural network based NMT with"
P19-1124,N18-1125,1,0.817365,"ul to improve translation quality (Koehn et al., 2003; Liu et al., 2005). In neural machine translation (NMT), it is also important to study word alignment, because word alignment provides natural ways to understanding black-box NMT models and analyzing their translation errors (Ding et al., 2017). Prior researches ∗ Work done while X. Li interning at Tencent AI Lab. L. Liu is the corresponding author. observed that word alignment is captured by NMT through attention for recurrent neural network based NMT with a single attention layer (Bahdanau et al., 2014; Mi et al., 2016; Liu et al., 2016; Li et al., 2018). Unfortunately, we surprisingly find that attention may almost fail to capture word alignment for NMT models with multiple attentional layers such as T RANSFORMER (Vaswani et al., 2017), as demonstrated in our experiments. In this paper, we propose two methods to induce word alignment from general NMT models and answer a fundamental question that how much word alignment NMT models can learn (§ 3). The first method explicitly builds a word alignment model between a pair of source and target word representations encoded by NMT models, and then it learns additional parameters for this word align"
P19-1124,2016.amta-researchers.10,0,0.100127,"t NMT captures the context of the surname ‘zheng’ by PD over target side besides the context of ‘h´e’ by PD over source side, thanks to its more powerful language model effect. 5 Related Work In NMT, there are many notable researches which mention word alignment captured by attention in some extent. For example, Bahdanau et al. (2014) is the first work to show word alignment examples by using attention in an NMT model. Tu et al. (2016) quantitatively evaluate word alignment captured by attention and find that its quality is much worse than statistical word aligners. Motivated by this finding, Chen et al. (2016), Mi et al. (2016) and Liu et al. (2016) improve attention with the supervision from silver alignment results obtained by statistical aligners, in the hope that the improved attention leads to better word alignment and translation quality consequently. More recently, there are also works (Alkhouli et al., 2018) that directly model the alignment and use it to sharpen the attention to bias translation. Despite the close relation between word alignment and attention, Koehn and Knowles (2017) and Ghader and Monz (2017) discuss the differences between word alignment and attention in NMT. Most of th"
P19-1124,C16-1291,1,0.855591,"is generally helpful to improve translation quality (Koehn et al., 2003; Liu et al., 2005). In neural machine translation (NMT), it is also important to study word alignment, because word alignment provides natural ways to understanding black-box NMT models and analyzing their translation errors (Ding et al., 2017). Prior researches ∗ Work done while X. Li interning at Tencent AI Lab. L. Liu is the corresponding author. observed that word alignment is captured by NMT through attention for recurrent neural network based NMT with a single attention layer (Bahdanau et al., 2014; Mi et al., 2016; Liu et al., 2016; Li et al., 2018). Unfortunately, we surprisingly find that attention may almost fail to capture word alignment for NMT models with multiple attentional layers such as T RANSFORMER (Vaswani et al., 2017), as demonstrated in our experiments. In this paper, we propose two methods to induce word alignment from general NMT models and answer a fundamental question that how much word alignment NMT models can learn (§ 3). The first method explicitly builds a word alignment model between a pair of source and target word representations encoded by NMT models, and then it learns additional parameters f"
P19-1124,P05-1057,0,0.649273,"uction Machine translation aims at modeling the semantic equivalence between a pair of source and target sentences (Koehn, 2009), and word alignment tries to model the semantic equivalence between a pair of source and target words (Och and Ney, 2003). As a sentence consists of words, word alignment is conceptually related to machine translation and such a relation can be traced back to the birth of statistical machine translation (SMT) (Brown et al., 1993), where word alignment is the basis of SMT models and its accuracy is generally helpful to improve translation quality (Koehn et al., 2003; Liu et al., 2005). In neural machine translation (NMT), it is also important to study word alignment, because word alignment provides natural ways to understanding black-box NMT models and analyzing their translation errors (Ding et al., 2017). Prior researches ∗ Work done while X. Li interning at Tencent AI Lab. L. Liu is the corresponding author. observed that word alignment is captured by NMT through attention for recurrent neural network based NMT with a single attention layer (Bahdanau et al., 2014; Mi et al., 2016; Liu et al., 2016; Li et al., 2018). Unfortunately, we surprisingly find that attention may"
P19-1124,P17-1106,0,0.484147,"words (Och and Ney, 2003). As a sentence consists of words, word alignment is conceptually related to machine translation and such a relation can be traced back to the birth of statistical machine translation (SMT) (Brown et al., 1993), where word alignment is the basis of SMT models and its accuracy is generally helpful to improve translation quality (Koehn et al., 2003; Liu et al., 2005). In neural machine translation (NMT), it is also important to study word alignment, because word alignment provides natural ways to understanding black-box NMT models and analyzing their translation errors (Ding et al., 2017). Prior researches ∗ Work done while X. Li interning at Tencent AI Lab. L. Liu is the corresponding author. observed that word alignment is captured by NMT through attention for recurrent neural network based NMT with a single attention layer (Bahdanau et al., 2014; Mi et al., 2016; Liu et al., 2016; Li et al., 2018). Unfortunately, we surprisingly find that attention may almost fail to capture word alignment for NMT models with multiple attentional layers such as T RANSFORMER (Vaswani et al., 2017), as demonstrated in our experiments. In this paper, we propose two methods to induce word align"
P19-1124,D16-1249,0,0.248338,"and its accuracy is generally helpful to improve translation quality (Koehn et al., 2003; Liu et al., 2005). In neural machine translation (NMT), it is also important to study word alignment, because word alignment provides natural ways to understanding black-box NMT models and analyzing their translation errors (Ding et al., 2017). Prior researches ∗ Work done while X. Li interning at Tencent AI Lab. L. Liu is the corresponding author. observed that word alignment is captured by NMT through attention for recurrent neural network based NMT with a single attention layer (Bahdanau et al., 2014; Mi et al., 2016; Liu et al., 2016; Li et al., 2018). Unfortunately, we surprisingly find that attention may almost fail to capture word alignment for NMT models with multiple attentional layers such as T RANSFORMER (Vaswani et al., 2017), as demonstrated in our experiments. In this paper, we propose two methods to induce word alignment from general NMT models and answer a fundamental question that how much word alignment NMT models can learn (§ 3). The first method explicitly builds a word alignment model between a pair of source and target word representations encoded by NMT models, and then it learns addit"
P19-1124,A94-1016,0,0.0648596,"Missing"
P19-1124,N13-1073,0,0.6699,"air of column vectors of dimension d, and W is a matrix of dimension 2d × 2d. The explicit word alignment model is trained by maximizing the objective function with respect to the parameter matrix W : max W X log P (xj |yi ; W ) , (6) ∀j,i:Aref ij =1 where Aref ij is the reference alignment between xj and yi for a sentence pair x and y. As the number of elements in W is up to one million (i.e., (2 × 512)2 ), it is not feasible to train it using a small dataset with gold alignment. Therefore, following Mi et al. (2016) and Liu et al. (2016), we run statistical word aligner such as FAST A LIGN (Dyer et al., 2013) on a large corpus and then employ resulting word alignment as the silver alignment Aref for training. Note that our goal is to quantify word alignment learned by an NMT model, and thus we only treat W as the parameter to be learned, which differs from the joint training all parameters including those from NMT models as in Mi et al. (2016) and Liu et al. (2016). After training, one obtains the optimized W and then easily infers word alignment for a test sentence pair hx, yi via the MAP strategy as defined in equation 3 by setting αi,j 0 = P xj 0 |yi ; W . Note that if word embeddings and hidd"
P19-1124,W03-0301,0,0.311674,"air hx, yi, after collecting R(yi , xj ) one can easily infer word alignment via the MAP strategy as defined in equation 3 by setting αi,j 0 = R(yi , xj 0 ). Experiments In this section, we conduct extensive experiments on ZH⇒EN and DE⇒EN translation tasks to evaluate different methods for word alignment induced from the NMT model and compare them with a statistical alignment model FAST A LIGN (Dyer et al., 2013). Then, we use the induced word alignment to understand translation errors both qualitatively and quantitatively. The alignment performance is evaluated by alignment error rate (AER) (Mihalcea and Pedersen, 2003; Koehn, 2009). The proposed methods are implemented on top of T RANS FORMER (Vaswani et al., 2017) which is a state-ofthe-art NMT system. We report AER on NIST05 test set and RWTH data, whose reference alignment was manually annotated by experts (Liu et al., 2016; Ghader and Monz, 2017). More details on data and training these systems are described in Appendix A. 4.1 Inducing Word alignment from NMT Attention Since the bilingual corpus intrinsically includes word alignment in some extent, word alignment by attention should be better than the data intrinsic alignment if attention indeed captur"
P19-1124,J03-1002,0,0.104815,"This paper further visualizes the translation through the word alignment induced by NMT. In particular, it analyzes the effect of alignment errors on translation errors at word level and its quantitative analysis over many testing examples consistently demonstrate that alignment errors are likely to lead to translation errors measured by different metrics. 1 Introduction Machine translation aims at modeling the semantic equivalence between a pair of source and target sentences (Koehn, 2009), and word alignment tries to model the semantic equivalence between a pair of source and target words (Och and Ney, 2003). As a sentence consists of words, word alignment is conceptually related to machine translation and such a relation can be traced back to the birth of statistical machine translation (SMT) (Brown et al., 1993), where word alignment is the basis of SMT models and its accuracy is generally helpful to improve translation quality (Koehn et al., 2003; Liu et al., 2005). In neural machine translation (NMT), it is also important to study word alignment, because word alignment provides natural ways to understanding black-box NMT models and analyzing their translation errors (Ding et al., 2017). Prior"
P19-1124,N18-1202,0,0.0257554,"ured by attention on midRo (yi , yk ) = P (yi |y&lt;i , x)−P yi |y&lt;i (k,0) , x , dle layer(s) is reasonable, but that on low or high layer is obviously worse than PMI. The possible (10) reasons can be explained as follows. The possible where Ro indicates the relevance between two tarfunctionality of lower layers might be constructing get words yi and yk with k &lt; i, and P (yi | gradually better contextual representation of the y&lt;i (k,0) , x) is obtained by disabling the connecword at each position as suggested in recent contion between yk and the decoder network, simitextualized embedding works (Peters et al., 2018; larly to P yi |y&lt;i , x(j,0) . Unlike R(yi , xj ) capDevlin et al., 2018; Radford et al., 2019). So turing word alignment information, Ro (yi , yk ) is 2 able to capture word allocation in a target sentence More details in Appendix B. 1296 3 layer L1 Transformer L4 L3 73.29 52.66 45.22 51.19 86.92 92.13 4 5 6 0 71.68 46.72 91.02 85.98 67.09 50.07 67.05 93.16 88.41 72.11 56.68 53.98 82.40 93.97 1 (a) ZH⇒EN 52.95(PMI) 77.34 47.79 49.47 91.71 L5 L5 71.31 53.71 49.84 77.55 92.36 L6 64.88 49.32 52.81 90.11 2 45.37 67.20 65.66(PMI) 55.07 48.40 84.85 1 L2 100 48.55 L6 L1 47.80 77.38 Transformer L4 L"
P19-1124,H05-1010,0,0.154855,"ght obtain some word alignment as described in previous section, it is unknown whether NMT models contain more word alignment information than that obtained by attention. In addition, the method using attention is useful to induce word alignment for attentional 1294 NMT models, whereas it is useless for general NMT models. In this section, in order to induce word alignment from general NMT models, we propose two different methods, which are agnostic to specific NMT models. 3.1 Alignment by Explicit Alignment Model Given a source sentence x, a target sentence y, following Liu et al. (2005) and Taskar et al. (2005), we explicitly define a word alignment model as follows: exp (δ (xj , yi ; W ))  , P (xj |yi ; W ) = Pm j 0 =1 exp δ xj 0 , yi ; W (4) where δ (xj , yi ; W ) is a distance function parametrized by W . Ideally, δ is able to include arbitrary features such as IBM model 1 similar to Liu et al. (2005). However, as our goal is not to achieve the best word alignment but to focus on that captured by an NMT model, we only consider these features completely learned in NMT. Hence, we define the  δ (xj , yi ; W ) = (xj khj )> W yi ksL i , (5) where xj and yi are word embeddings of xj and yi learned i"
P19-1124,P16-1008,0,0.157983,"tgraf et al. (2017). Experiments on an advanced NMT model show that both methods achieve much better word alignment than the method by attention (§ 4.1). In addition, our experiments demonstrate that NMT captures good word alignment for those words mostly contributed from source (CFS), while their word alignment is much worse for those words mostly contributed from target (CFT). This finding offers a reason why advanced NMT models delivering excellent translation capture worse word alignment than statistical aligners in SMT, which was observed in prior researches yet without deep explanation (Tu et al., 2016; Liu et al., 2016). Furthermore, we understand and interpret NMT from the viewpoint of word alignment induced 1293 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 1293–1303 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics from NMT (§ 4.2). Unlike existing researches on interpreting NMT by accessing few examples as case study (Ding et al., 2017; Alvarez-Melis and Jaakkola, 2017), we aim to provide quantitatively analysis for interpreting NMT by accessing many testing examples, which makes our findings mor"
P19-1124,P19-1580,0,0.0400862,"ZH⇒EN 52.95(PMI) 77.34 47.79 49.47 91.71 L5 L5 71.31 53.71 49.84 77.55 92.36 L6 64.88 49.32 52.81 90.11 2 45.37 67.20 65.66(PMI) 55.07 48.40 84.85 1 L2 100 48.55 L6 L1 47.80 77.38 Transformer L4 L3 L2 100 56.49 2 3 layer 4 5 6 0 (b) DE⇒EN Figure 1: AER of attention at each layer on T RANSFORMER with different number of layers. AER of PMI is shown as white. Blue and red means AER is better and worse than PMI respectively. the AERs become better while more unambiguous representations of the corresponding word are formed. However, for higher layers the representational redundancy is accumulated (Voita et al., 2019; Michel et al., 2019) for phrases or other larger meaning spans in the input, so attention is not capturing word-to-word align but more complicated semantic correspondence. Methods FAST A LIGN Attention mean Attention best EAM PD * Tasks ZH⇒EN DE⇒EN 36.57 26.58 56.44 74.59 45.22 53.98 38.88 39.25 41.77 42.81 Results are measured on T RANSFORMER -L6. AERs over different pre-trained translation models with their EAMs trained on the same FAST A LIGN annotated data. We find that a stronger (higher BLEU) translation model generally obtains better alignment (lower AER). As shown in Table 2, T RANSF"
W15-3909,P04-1021,0,\N,Missing
W15-3909,P02-1040,0,\N,Missing
W15-3909,P07-1081,0,\N,Missing
W15-3909,W14-4012,0,\N,Missing
W15-3909,W12-4401,0,\N,Missing
W15-3909,2010.iwslt-papers.7,1,\N,Missing
W15-3909,2007.iwslt-1.15,1,\N,Missing
W15-3909,D08-1076,0,\N,Missing
W15-3909,W11-3203,1,\N,Missing
W16-2711,I08-8003,1,0.781894,"Agtarbidir. 1 • A target-bidirectional agreement model was employed. • Ensembles of neural networks were used rather than just a single network. • The ensembles were selected from different training runs and different training epochs according to their performance on development (and test) data. Introduction Our primary system for the NEWS shared evaluation on transliteration generation is different in character from all our previous systems. In past years, all our systems have been based on phrase-based statistical machine translation (PBSMT) techniques, stemming from the system proposed in (Finch and Sumita, 2008). This year’s system is a pure end-to-end neural network transducer. In (Finch et al., 2012) auxiliary neural network language models (both monolingual and bilingual (Li et al., 2004)) were introduced as features to augment the log-linear model of a phrasebased transduction system, and led to modest gains in system performance. In the NEWS 2015 workshop (Finch et al., 2015) neural transliteration systems using attention-based sequence-to-sequence neural network transducers (Bahdanau et al., 2014) were applied to transliteration generation. In isolation, the performance was found to be lower th"
W16-2711,W12-4406,1,0.904887,"Missing"
W16-2711,W15-3909,1,0.851947,"eneration is different in character from all our previous systems. In past years, all our systems have been based on phrase-based statistical machine translation (PBSMT) techniques, stemming from the system proposed in (Finch and Sumita, 2008). This year’s system is a pure end-to-end neural network transducer. In (Finch et al., 2012) auxiliary neural network language models (both monolingual and bilingual (Li et al., 2004)) were introduced as features to augment the log-linear model of a phrasebased transduction system, and led to modest gains in system performance. In the NEWS 2015 workshop (Finch et al., 2015) neural transliteration systems using attention-based sequence-to-sequence neural network transducers (Bahdanau et al., 2014) were applied to transliteration generation. In isolation, the performance was found to be lower than that of the phrase-based system on all of the In all our experiments we have taken a strictly language independent approach. Each of the language pairs was processed automatically from the character sequence representation supplied for the shared tasks, with no language specific treatment for any of the language pairs. Furthermore no preprocessing was performed on any of"
W16-2711,P07-1081,0,0.524071,"Missing"
W16-2711,P04-1021,0,0.182904,"training runs and different training epochs according to their performance on development (and test) data. Introduction Our primary system for the NEWS shared evaluation on transliteration generation is different in character from all our previous systems. In past years, all our systems have been based on phrase-based statistical machine translation (PBSMT) techniques, stemming from the system proposed in (Finch and Sumita, 2008). This year’s system is a pure end-to-end neural network transducer. In (Finch et al., 2012) auxiliary neural network language models (both monolingual and bilingual (Li et al., 2004)) were introduced as features to augment the log-linear model of a phrasebased transduction system, and led to modest gains in system performance. In the NEWS 2015 workshop (Finch et al., 2015) neural transliteration systems using attention-based sequence-to-sequence neural network transducers (Bahdanau et al., 2014) were applied to transliteration generation. In isolation, the performance was found to be lower than that of the phrase-based system on all of the In all our experiments we have taken a strictly language independent approach. Each of the language pairs was processed automatically"
