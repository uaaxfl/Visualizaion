2020.acl-demos.16,N19-1423,0,0.603845,"To enable efficient production deployment, MT-DNN supports multitask knowledge distillation, which can substantially compress a deep neural model without significant performance drop. We demonstrate the effectiveness of MT-DNN on a wide range of NLU applications across general and biomedical domains. The software and pretrained models will be publicly available at https://github.com/namisan/mt-dnn. 1 Introduction NLP model development has observed a paradigm shift in recent years, due to the success in using pretrained language models to improve a wide range of NLP tasks (Peters et al., 2018; Devlin et al., 2019). Unlike the traditional pipeline approach that conducts annotation in stages using primarily supervised learning, the new paradigm features a universal pretraining stage that trains a large neural language model via self-supervision on a large unlabeled text corpus, followed by a fine-tuning step that starts from the pretrained contextual representations and conducts supervised learning for ∗ Equal Contribution. The complete name of our toolkit is M T 2 -DNN (The Microsoft Toolkit of Multi-Task Deep Neural Networks for Natural Language Understanding), but we use MT-DNN for sake of simplicity."
2020.acl-demos.16,I05-5002,0,0.0420279,"ummary of the four benchmarks: GLUE, SNLI, SciTail and ANLI. Model MNLI RTE QNLI SST MRPC Acc Acc Acc Acc F1 BERT 84.5 63.5 91.1 92.9 89.0 BERT + MTL 85.3 79.1 91.5 93.6 89.2 BERT + AdvTrain 85.6 71.2 91.6 93.0 91.3 Table 2: Comparison among single task, multi-Task and adversarial training on MNLI, RTE, QNLI, SST and MPRC in GLUE. Model BERTLARGE (Nie et al., 2019) RoBERTaLARGE (Nie et al., 2019) RoBERTa-LARGE + AdvTrain Dev 49.3 53.7 57.1 Test 44.2 49.7 57.1 Table 3: Results in terms of accuracy on the ANLI. sis (Socher et al., 2013), text similarity (Cer et al., 2017), paraphrase detection (Dolan and Brockett, 2005), and natural language inference (NLI) (Dagan et al., 2006; Bar-Haim et al., 2006; Giampiccolo et al., 2007; Bentivogli et al., 2009; Levesque et al., 2012; Williams et al., 2018). The diversity of the tasks makes GLUE very suitable for evaluating the generalization and robustness of NLU models. • SNLI. The Stanford Natural Language Inference (SNLI) dataset contains 570k human annotated sentence pairs, in which the premises are drawn from the captions of the Flickr30 corpus and hypotheses are manually annotated (Bowman et al., 2015). This is the most widely used entailment dataset for NLI. • S"
2020.acl-demos.16,W18-2501,0,0.0440435,"Missing"
2020.acl-demos.16,W07-1401,0,0.0447164,"BERT 84.5 63.5 91.1 92.9 89.0 BERT + MTL 85.3 79.1 91.5 93.6 89.2 BERT + AdvTrain 85.6 71.2 91.6 93.0 91.3 Table 2: Comparison among single task, multi-Task and adversarial training on MNLI, RTE, QNLI, SST and MPRC in GLUE. Model BERTLARGE (Nie et al., 2019) RoBERTaLARGE (Nie et al., 2019) RoBERTa-LARGE + AdvTrain Dev 49.3 53.7 57.1 Test 44.2 49.7 57.1 Table 3: Results in terms of accuracy on the ANLI. sis (Socher et al., 2013), text similarity (Cer et al., 2017), paraphrase detection (Dolan and Brockett, 2005), and natural language inference (NLI) (Dagan et al., 2006; Bar-Haim et al., 2006; Giampiccolo et al., 2007; Bentivogli et al., 2009; Levesque et al., 2012; Williams et al., 2018). The diversity of the tasks makes GLUE very suitable for evaluating the generalization and robustness of NLU models. • SNLI. The Stanford Natural Language Inference (SNLI) dataset contains 570k human annotated sentence pairs, in which the premises are drawn from the captions of the Flickr30 corpus and hypotheses are manually annotated (Bowman et al., 2015). This is the most widely used entailment dataset for NLI. • SciTail This is a textual entailment dataset derived from a science question answering (SciQ) dataset (Khot"
2020.acl-demos.16,P84-1044,0,0.129197,"Missing"
2020.acl-demos.16,N15-1092,1,0.751,"l., 2019; Liu et al., 2019b). Fine-tuning Once the text encoder is trained in the pre-training stage, an additional task-specific layer is usually added for fine-tuning based on the downstream task. Besides the existing typical single-task fine-tuning, MT-DNN facilitates a joint fine-tuning with a configurable list of related tasks in a MTL fashion. By encoding task-relatedness and sharing underlying text representations, MTL is a powerful training paradigm that promotes the model generalization ability and results in improved performance (Caruana, 1997; Liu et al., 2019b; Luong et al., 2015; Liu et al., 2015; Ruder, 2017; Collobert et al., 2011). Additionally, a two-step fine-tuning stage is also supported to utilize datasets from related tasks, i.e. a single-task fine-tuning following a multi-task fine-tuning. It also supports two popular sampling strategies in MTL training: 1) sampling tasks uniformly (Caruana, 1997; Liu et al., 2015); 2) sampling tasks based on the size of the dataset (Liu et al., 2019b). This makes it easy to explore various ways to feed training data to MTL training. Finally, to further improve the model robustness, MT-DNN also offers a recipe to apply adversarial training ("
2020.acl-demos.16,P19-1441,1,0.929166,"an effectively model textual variations and distributional similarity. Therefore, they can make subsequent task-specific training more sample efficient and often significantly boost performance in downstream tasks. However, these models are quite large and pose significant challenges to production deployment that has stringent memory or speed requirements. As a result, knowledge distillation has become another key feature in this new learning paradigm. An effective distillation step can often substantially compress a large model for efficient deployment (Clark et al., 2019; Tang et al., 2019; Liu et al., 2019a). In the NLP community, there are several well designed frameworks for research and commercial purposes, including toolkits for providing conventional layered linguistic annotations (Manning et al., 2014), platforms for developing novel neural models (Gardner et al., 2018) and systems for neural machine translation (Ott et al., 2019). However, it is hard to find an existing tool that supports all features in the new paradigm and can be easily customized for new tasks. For example, (Wolf et al., 2019) provides a number of popular Transformerbased (Vaswani et al., 2017) text encoders in a nice"
2020.acl-demos.16,P18-1157,1,0.845195,"customize their own encoders. For example, one can design an encoder with few Transformer layers (e.g. 3 layers) to distill knowledge from the BERT large model (24 layers), so that they can deploy this small mode online to meet the latency restriction as shown in Figure 2. Task-Specific Output Layers: We can incorporate arbitrary natural language tasks, each with its task-specific output layer. For example, we implement the output layers as a neural decoder for a neural ranker for relevance ranking, a logistic regression for text classification, and so on. A multistep reasoning decoder, SAN (Liu et al., 2018a,b) is also provided. Customers can choose from existing task-specific output layer or implement new one by themselves. 3 Application In this section, we present a comprehensive set of examples to illustrate how to customize MTDNN for new tasks. We use popular benchmarks from general and biomedical domains, including GLUE (Wang et al., 2018), SNLI (Bowman et al., 2015), SciTail (Khot et al., 2018), SQuAD (Rajpurkar et al., 2016), ANLI (Nie et al., 2019), and biomedical named entity recognition (NER), relation extraction (RE) and question answering (QA) (Lee et al., 2019). To make the experime"
2020.acl-demos.16,2021.ccl-1.108,0,0.195475,"Missing"
2020.acl-demos.16,P14-5010,0,0.00631447,"stream tasks. However, these models are quite large and pose significant challenges to production deployment that has stringent memory or speed requirements. As a result, knowledge distillation has become another key feature in this new learning paradigm. An effective distillation step can often substantially compress a large model for efficient deployment (Clark et al., 2019; Tang et al., 2019; Liu et al., 2019a). In the NLP community, there are several well designed frameworks for research and commercial purposes, including toolkits for providing conventional layered linguistic annotations (Manning et al., 2014), platforms for developing novel neural models (Gardner et al., 2018) and systems for neural machine translation (Ott et al., 2019). However, it is hard to find an existing tool that supports all features in the new paradigm and can be easily customized for new tasks. For example, (Wolf et al., 2019) provides a number of popular Transformerbased (Vaswani et al., 2017) text encoders in a nice unified interface, but does not offer multitask learning or adversarial training, state-of-the-art techniques that have been shown to significantly improve performance. Additionally, most public frameworks"
2020.acl-demos.16,N19-4009,0,0.0311237,"or speed requirements. As a result, knowledge distillation has become another key feature in this new learning paradigm. An effective distillation step can often substantially compress a large model for efficient deployment (Clark et al., 2019; Tang et al., 2019; Liu et al., 2019a). In the NLP community, there are several well designed frameworks for research and commercial purposes, including toolkits for providing conventional layered linguistic annotations (Manning et al., 2014), platforms for developing novel neural models (Gardner et al., 2018) and systems for neural machine translation (Ott et al., 2019). However, it is hard to find an existing tool that supports all features in the new paradigm and can be easily customized for new tasks. For example, (Wolf et al., 2019) provides a number of popular Transformerbased (Vaswani et al., 2017) text encoders in a nice unified interface, but does not offer multitask learning or adversarial training, state-of-the-art techniques that have been shown to significantly improve performance. Additionally, most public frameworks do not offer knowledge distillation. A notable exception is DistillBERT (Sanh et al., 2019), but it provides a standalone compress"
2020.acl-demos.16,N18-1202,0,0.408136,"k learning paradigm. To enable efficient production deployment, MT-DNN supports multitask knowledge distillation, which can substantially compress a deep neural model without significant performance drop. We demonstrate the effectiveness of MT-DNN on a wide range of NLU applications across general and biomedical domains. The software and pretrained models will be publicly available at https://github.com/namisan/mt-dnn. 1 Introduction NLP model development has observed a paradigm shift in recent years, due to the success in using pretrained language models to improve a wide range of NLP tasks (Peters et al., 2018; Devlin et al., 2019). Unlike the traditional pipeline approach that conducts annotation in stages using primarily supervised learning, the new paradigm features a universal pretraining stage that trains a large neural language model via self-supervision on a large unlabeled text corpus, followed by a fine-tuning step that starts from the pretrained contextual representations and conducts supervised learning for ∗ Equal Contribution. The complete name of our toolkit is M T 2 -DNN (The Microsoft Toolkit of Multi-Task Deep Neural Networks for Natural Language Understanding), but we use MT-DNN f"
2020.acl-demos.16,D16-1264,0,0.347892,"he output layers as a neural decoder for a neural ranker for relevance ranking, a logistic regression for text classification, and so on. A multistep reasoning decoder, SAN (Liu et al., 2018a,b) is also provided. Customers can choose from existing task-specific output layer or implement new one by themselves. 3 Application In this section, we present a comprehensive set of examples to illustrate how to customize MTDNN for new tasks. We use popular benchmarks from general and biomedical domains, including GLUE (Wang et al., 2018), SNLI (Bowman et al., 2015), SciTail (Khot et al., 2018), SQuAD (Rajpurkar et al., 2016), ANLI (Nie et al., 2019), and biomedical named entity recognition (NER), relation extraction (RE) and question answering (QA) (Lee et al., 2019). To make the experiments reproducible, we make all the configuration files publicly available. We also provide a quick guide for customizing a new task in Jupyter notebooks. 3.1 General Domain Natural Language Understanding Benchmarks • GLUE. The General Language Understanding Evaluation (GLUE) benchmark is a collection of nine natural language understanding (NLU) tasks. As shown in Table 1, it includes question answering (Rajpurkar et al., 2016), li"
2020.acl-demos.16,D13-1170,0,0.0317297,"ication NLI Classification NLI Classification MRC Span Classification Table 1: Summary of the four benchmarks: GLUE, SNLI, SciTail and ANLI. Model MNLI RTE QNLI SST MRPC Acc Acc Acc Acc F1 BERT 84.5 63.5 91.1 92.9 89.0 BERT + MTL 85.3 79.1 91.5 93.6 89.2 BERT + AdvTrain 85.6 71.2 91.6 93.0 91.3 Table 2: Comparison among single task, multi-Task and adversarial training on MNLI, RTE, QNLI, SST and MPRC in GLUE. Model BERTLARGE (Nie et al., 2019) RoBERTaLARGE (Nie et al., 2019) RoBERTa-LARGE + AdvTrain Dev 49.3 53.7 57.1 Test 44.2 49.7 57.1 Table 3: Results in terms of accuracy on the ANLI. sis (Socher et al., 2013), text similarity (Cer et al., 2017), paraphrase detection (Dolan and Brockett, 2005), and natural language inference (NLI) (Dagan et al., 2006; Bar-Haim et al., 2006; Giampiccolo et al., 2007; Bentivogli et al., 2009; Levesque et al., 2012; Williams et al., 2018). The diversity of the tasks makes GLUE very suitable for evaluating the generalization and robustness of NLU models. • SNLI. The Stanford Natural Language Inference (SNLI) dataset contains 570k human annotated sentence pairs, in which the premises are drawn from the captions of the Flickr30 corpus and hypotheses are manually annotate"
2020.acl-demos.16,N18-1074,0,0.0361753,"Missing"
2020.acl-demos.16,W18-5446,0,0.125971,"Missing"
2020.acl-demos.16,N18-1101,0,0.306618,"AdvTrain 85.6 71.2 91.6 93.0 91.3 Table 2: Comparison among single task, multi-Task and adversarial training on MNLI, RTE, QNLI, SST and MPRC in GLUE. Model BERTLARGE (Nie et al., 2019) RoBERTaLARGE (Nie et al., 2019) RoBERTa-LARGE + AdvTrain Dev 49.3 53.7 57.1 Test 44.2 49.7 57.1 Table 3: Results in terms of accuracy on the ANLI. sis (Socher et al., 2013), text similarity (Cer et al., 2017), paraphrase detection (Dolan and Brockett, 2005), and natural language inference (NLI) (Dagan et al., 2006; Bar-Haim et al., 2006; Giampiccolo et al., 2007; Bentivogli et al., 2009; Levesque et al., 2012; Williams et al., 2018). The diversity of the tasks makes GLUE very suitable for evaluating the generalization and robustness of NLU models. • SNLI. The Stanford Natural Language Inference (SNLI) dataset contains 570k human annotated sentence pairs, in which the premises are drawn from the captions of the Flickr30 corpus and hypotheses are manually annotated (Bowman et al., 2015). This is the most widely used entailment dataset for NLI. • SciTail This is a textual entailment dataset derived from a science question answering (SciQ) dataset (Khot et al., 2018). In contrast to other entailment datasets mentioned previo"
2020.acl-demos.16,D15-1075,0,\N,Missing
2020.acl-demos.16,S17-2001,0,\N,Missing
2020.acl-main.197,D15-1075,0,0.0783238,"DNNBASE 95.8 94.1 SMARTBERT-BASE 94.8 93.2 MT-DNN-SMARTBASEv0 96.0 94.0 MT-DNN-SMARTBASE 96.1 94.2 BERTLARGE 95.7 94.4 MT-DNNLARGE 96.3 95.0 SMARTBERT-LARGE 96.2 94.7 MT-DNN-SMARTLARGEv0 96.6 95.2 6 Table 7: Results on the SNLI and SciTail dataset. test our model on an adversarial natural language inference (ANLI) dataset (Nie et al., 2019). We evaluate the performance of SMART on each subset (i.e., R1,R2,R3) of ANLI dev and test set. The results are presented in Table 6. Table 6 shows the results of training on combined NLI data (ANLI (Nie et al., 2019) + MNLI (Williams et al., 2018) + SNLI (Bowman et al., 2015) + FEVER (Thorne et al., 2018)) and training on only ANLI data. In the combined data setting, we obverse that SMARTRoBERTa-LARGE obtains the best Conclusion We propose a robust and efficient computation framework, SMART, for fine-tuning large scale pre-trained natural language models in a principled manner. The framework effectively alleviates the overfitting and aggressive updating issues in the fine-tuning stage. SMART includes two important ingredients: 1) smooth-inducing adversarial regularization; 2) Bregman proximal point optimization. Our empirical results suggest that SMART improves th"
2020.acl-main.197,P18-1031,0,0.162455,"8th Annual Meeting of the Association for Computational Linguistics, pages 2177–2190 c July 5 - 10, 2020. 2020 Association for Computational Linguistics 2019; Lan et al., 2019; Dong et al., 2019; Raffel et al., 2019). Due to the limited data from the target task/domain and the extremely high complexity of the pre-trained model, aggressive fine-tuning often makes the adapted model overfit the training data of the target task/domain and therefore does not generalize well to unseen data. To mitigate this issue, the fine-tuning methods often rely on hyper-parameter tuning heuristics. For example, Howard and Ruder (2018) use a heuristic learning rate schedule and gradually unfreeze the layers of the language model to improve the fine-tune performance; Peters et al. (2019) give a different suggestion that they only adapt certain layers and freeze the others; (Houlsby et al., 2019; Stickland and Murray, 2019) propose to add additional layers to the pre-trained model and fine-tune both of them or only the additional layers. However, these methods require significant tuning efforts. To fully harness the power of fine-tuning in a more principled manner, we propose a new learning framework for robust and efficient"
2020.acl-main.197,2020.tacl-1.5,0,0.0152249,"ated parameters of pk and qk , respectively. 2 Background The transformer models were originally proposed in Vaswani et al. (2017) for neural machine translation. Their superior performance motivated Devlin et al. (2019) to propose a bidirectional transformer-based language model named BERT. Specifically, Devlin et al. (2019) pre-trained the BERT model using a large corpus without any human annotation through unsupervised learning tasks. BERT motivated many follow-up works to further improve the pre-training by introducing new unsupervised learning tasks (Yang et al., 2019; Dong et al., 2019; Joshi et al., 2020), enlarging model size (Lan et al., 2019; Raffel et al., 2019), enlarging training corpora (Liu et al., 2019c; Yang et al., 2019; Raffel et al., 2019) and multi-tasking (Liu et al., 2019a,b). The pre-trained language model is then adapted to downstream tasks and further fine-tuned. Specifically, the top layer of the language model can be replaced by a task-specific layer and then continue to train on downstream tasks. To prevent overfitting, existing heuristics include choosing a 2178 small learning rate or a triangular learning rate schedule, and a small number of iterations, and other fine-t"
2020.acl-main.197,P19-1478,0,0.0427603,"Missing"
2020.acl-main.197,N15-1092,1,0.853476,"nt), SMARTRoBERTa obtains an even larger improvement showing its robustness to ambiguity. 5.3 SST MRPC Acc Acc 92.9 89.0 93.0 91.3 92.8 90.8 92.9 91.2 Table 3: Ablation study of SMART on 5 GLUE tasks. Note that all models used the BERTBASE model as their encoder. The results are reported in Table 3. It is expected that the removal of either component (smooth regularization or proximal point method) in SMART would result in a performance drop. For example, on MNLI, removing smooth reguError Analysis SMART with Multi-task Learning It has been shown that multi-task learning (MTL, Caruana (1997); Liu et al. (2015, 2019b)) has a regularization effect via alleviating overfitting to a specific task. One question is whether MTL helps SMART as well. In this section, we are going to answer this question. Following Liu et al. (2019b), we first “pre-trained” shared embeddings using MTL with SMART, denoted as MT-DNNSMART 8 , and then adapted the training data on each task on top of the shared embeddings. We also include a baseline which fine-tuned each task 8 Due to limitation of computational resources, we only trained jointly using MTL on MNLI, RTE, QNLI, SST and MRPC, while MT-DNN was trained on the whole G"
2020.acl-main.197,P19-1441,1,0.828565,"by far the largest language model, T5, has an enormous size of about 11 billion parameters (Raffel et al., 2019). For the second fine-tuning stage, researchers adapt the pre-trained language model to the target task/domain. They usually replace the top layer of the language model by a task/domainspecific sub-network, and then continue to train the new model with the limited data of the target task/domain. Such a fine-tuning approach accounts for the low-resource issue in the target task/domain, and has achieved state-of-the-art performance in many popular NLP benchmarks (Devlin et al., 2019; Liu et al., 2019c; Yang et al., 2177 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 2177–2190 c July 5 - 10, 2020. 2020 Association for Computational Linguistics 2019; Lan et al., 2019; Dong et al., 2019; Raffel et al., 2019). Due to the limited data from the target task/domain and the extremely high complexity of the pre-trained model, aggressive fine-tuning often makes the adapted model overfit the training data of the target task/domain and therefore does not generalize well to unseen data. To mitigate this issue, the fine-tuning methods often rely on hyper-p"
2020.acl-main.197,2020.acl-demos.16,1,0.818871,"Missing"
2020.acl-main.197,2021.ccl-1.108,0,0.368231,"Missing"
2020.acl-main.197,N18-1202,0,0.0604348,"of-domain data). The goal is to transfer the knowledge from the high-resource domains to the low-resource target domain. Here we are particularly interested in the popular twostage transfer learning framework (Pan and Yang, 2009). The first stage is pre-training, where a high-capacity model is trained for the out-ofdomain high-resource relevant tasks. The second stage is fine-tuning, where the high-capacity model is adapted to the low-resource task in the target domain. For many applications in NLP, most popular transfer learning methods choose to pre-train a large language model, e.g., ELMo (Peters et al., 2018), GPT (Radford et al., 2019) and BERT (Devlin et al., 2019). Such a language model can capture general semantic and syntactic information that can be further used in downstream NLP tasks. The language model is particularly attractive, because it can be trained in a completely unsupervised manner with huge amount of unlabeled data, which are extremely cheap to fetch from internet nowadays. The resulting extremely large multidomain text corpus allows us to train huge language models. To the best of our knowledge, by far the largest language model, T5, has an enormous size of about 11 billion par"
2020.acl-main.197,W19-4302,0,0.0669215,"; Lan et al., 2019; Dong et al., 2019; Raffel et al., 2019). Due to the limited data from the target task/domain and the extremely high complexity of the pre-trained model, aggressive fine-tuning often makes the adapted model overfit the training data of the target task/domain and therefore does not generalize well to unseen data. To mitigate this issue, the fine-tuning methods often rely on hyper-parameter tuning heuristics. For example, Howard and Ruder (2018) use a heuristic learning rate schedule and gradually unfreeze the layers of the language model to improve the fine-tune performance; Peters et al. (2019) give a different suggestion that they only adapt certain layers and freeze the others; (Houlsby et al., 2019; Stickland and Murray, 2019) propose to add additional layers to the pre-trained model and fine-tune both of them or only the additional layers. However, these methods require significant tuning efforts. To fully harness the power of fine-tuning in a more principled manner, we propose a new learning framework for robust and efficient fine-tuning on the pre-trained language models through regularized optimization techniques. Specifically, our framework consists of two important ingredie"
2020.acl-main.197,D16-1264,0,0.0571223,"Missing"
2020.acl-main.197,D13-1170,0,0.0243242,"Missing"
2020.acl-main.197,N18-1074,0,0.0240072,"Missing"
2020.acl-main.197,W18-5446,0,0.133739,"g of the Association for Computational Linguistics, pages 2177–2190 c July 5 - 10, 2020. 2020 Association for Computational Linguistics 2019; Lan et al., 2019; Dong et al., 2019; Raffel et al., 2019). Due to the limited data from the target task/domain and the extremely high complexity of the pre-trained model, aggressive fine-tuning often makes the adapted model overfit the training data of the target task/domain and therefore does not generalize well to unseen data. To mitigate this issue, the fine-tuning methods often rely on hyper-parameter tuning heuristics. For example, Howard and Ruder (2018) use a heuristic learning rate schedule and gradually unfreeze the layers of the language model to improve the fine-tune performance; Peters et al. (2019) give a different suggestion that they only adapt certain layers and freeze the others; (Houlsby et al., 2019; Stickland and Murray, 2019) propose to add additional layers to the pre-trained model and fine-tune both of them or only the additional layers. However, these methods require significant tuning efforts. To fully harness the power of fine-tuning in a more principled manner, we propose a new learning framework for robust and efficient"
2020.acl-main.197,Q19-1040,0,0.0595184,"Missing"
2020.acl-main.197,N18-1101,0,0.0468458,") - 88.3 BERTBASE 94.3 92.0 MT-DNNBASE 95.8 94.1 SMARTBERT-BASE 94.8 93.2 MT-DNN-SMARTBASEv0 96.0 94.0 MT-DNN-SMARTBASE 96.1 94.2 BERTLARGE 95.7 94.4 MT-DNNLARGE 96.3 95.0 SMARTBERT-LARGE 96.2 94.7 MT-DNN-SMARTLARGEv0 96.6 95.2 6 Table 7: Results on the SNLI and SciTail dataset. test our model on an adversarial natural language inference (ANLI) dataset (Nie et al., 2019). We evaluate the performance of SMART on each subset (i.e., R1,R2,R3) of ANLI dev and test set. The results are presented in Table 6. Table 6 shows the results of training on combined NLI data (ANLI (Nie et al., 2019) + MNLI (Williams et al., 2018) + SNLI (Bowman et al., 2015) + FEVER (Thorne et al., 2018)) and training on only ANLI data. In the combined data setting, we obverse that SMARTRoBERTa-LARGE obtains the best Conclusion We propose a robust and efficient computation framework, SMART, for fine-tuning large scale pre-trained natural language models in a principled manner. The framework effectively alleviates the overfitting and aggressive updating issues in the fine-tuning stage. SMART includes two important ingredients: 1) smooth-inducing adversarial regularization; 2) Bregman proximal point optimization. Our empirical results s"
2020.acl-main.677,N19-1423,0,0.0411056,"a stack of N relation- remedy this phenomenon, we explicitly encode name-based linking using RAT relations. aware self-attention layers to X, with separate Specifically, for all n-grams of length 1 to 5 in weight matrices in each layer. The final representath the question, we determine (1) whether it exactly tions ci , ti , q i produced by the N layer constitute matches the name of a column/table (exact match); the output of the whole encoder. or (2) whether the n-gram is a subsequence of the Alternatively, we also consider pre-trained name of a column/table (partial match).3 Then, for BERT (Devlin et al., 2019) embeddings to obtain the initial representations. Following (Huang et al., every (i, j) where xi ∈ Q, xj ∈ S (or vice versa), 2019; Zhang et al., 2019), we feed X to the BERT we set rij ∈ EQ↔S to Q UESTION -C OLUMN-M, and use the last hidden states as the initial represen- Q UESTION -TABLE-M, C OLUMN -Q UESTION-M or TABLE -Q UESTION-M depending on the type of xi tations before proceeding with the RAT layers.2 and xj . Here M is one of E XACT M ATCH, PAR Importantly, as detailed in Section 3, every RAT TIAL M ATCH , or N O M ATCH . layer uses self-attention between all elements of the input gr"
2020.acl-main.677,P18-1068,0,0.24077,"Missing"
2020.acl-main.677,P18-1033,0,0.182364,"Missing"
2020.acl-main.677,P19-1444,0,0.473893,"te BiLSTM over the question Q to obtain initial word representations q init i . init , and q init The initial representations cinit , t i i i are independent of each other and devoid of any relational information known to hold in EQ . To produce joint representations for the entire input graph GQ , we use the relation-aware self-attention mechanism (Section 3). Its input X is the set of all the node representations in GQ : 2 init init In this case, the initial representations cinit i , ti , q i are not strictly independent although still yet uninfluenced by E. 3 This procedure matches that of Guo et al. (2019), but we use the matching information differently in RAT. 7571 (or a full word within a value) of cj . This simple approach drastically improves the performance of RAT-SQL (see Section 5). It also directly addresses the aforementioned DB challenges: (a) the model is never exposed to database content that does not occur in the question, (b) word matches are retrieved quickly via DB indices & textual search. col K &gt; yi WQcol (cfinal j WK + r ij ) √ (3) dx tab K &gt; yi WQtab (tfinal j WK + r ij ) √ = dx  col  tab ˜ ˜ = softmax Li,j Ltab i,j = softmax Li,j ˜ col L i,j = ˜ tab L i,j Lcol i,j j Deco"
2020.acl-main.677,N18-2115,0,0.0551273,"Missing"
2020.acl-main.677,P17-1089,0,0.278505,"Missing"
2020.acl-main.677,P14-5010,0,0.0026325,"n as a pointer mechanism between every memory element in y and all the columns/tables to compute explicit alignment matrices Lcol ∈ R|y|×|C |and Ltab ∈ R|y|×|T |: Tree-structured decoder SELECT sc )T ht WQsc (yi WK √ dx  ˜i λi = softmax λ i Pr(at = S ELECT C OLUMN[i] |a&lt;t , y) = |y| X λj Lcol j,i j=1 and similarly for S ELECT TABLE. We refer the reader to Yin and Neubig (2017) for details. 5 Experiments We implemented RAT-SQL in PyTorch (Paszke et al., 2017). During preprocessing, the input of questions, column names and table names are tokenized and lemmatized with the StandfordNLP toolkit (Manning et al., 2014). Within the encoder, we use GloVe (Pennington et al., 2014) word embeddings, held fixed in training except for the 50 most common words in the training set. For RATSQL BERT, we use the WordPiece tokenization. All word embeddings have dimension 300. The bidirectional LSTMs have hidden size 128 per direction, and use the recurrent dropout method of Gal and Ghahramani (2016) with rate 0.2. We stack 8 relation-aware self-attention layers on top of the bidirectional LSTMs. Within them, we set dx = dz = 256, H = 8, and use dropout with rate 0.1. The position-wise feed-forward network has inner laye"
2020.acl-main.677,D14-1162,0,0.0840922,"ema linking relations in EQ↔S aid the model framework allows us to outsource this processing with aligning column/table references in the ques- to the database engine to augment GQ with potion to the corresponding schema columns/tables. tential value-based linking without exposing the This alignment is implicitly defined by two kinds model itself to the data. Specifically, we add a of information in the input: matching names and new C OLUMN -VALUE relation between any word matching values, which we detail in order below. qi and column name cj s.t. qi occurs as a value trained Glove embedding (Pennington et al., 2014) for each word, and (b) processing the embeddings in each multi-word label with a bidirectional LSTM (BiLSTM) (Hochreiter and Schmidhuber, 1997). It also runs a separate BiLSTM over the question Q to obtain initial word representations q init i . init , and q init The initial representations cinit , t i i i are independent of each other and devoid of any relational information known to hold in EQ . To produce joint representations for the entire input graph GQ , we use the relation-aware self-attention mechanism (Section 3). Its input X is the set of all the node representations in GQ : 2 init"
2020.acl-main.677,N18-2074,0,0.247487,"e question token representations. This differs from RAT-SQL in two important ways: (a) question word representations influence the schema representations but not vice versa, and (b) like in other GNN-based encoders, message propagation is limited to the schema-induced edges such as foreign key relations. In contrast, our relation-aware transformer mechanism allows encoding arbitrary relations between question words and schema elements explicitly, and these representations are computed jointly over all inputs using self-attention. We use the same formulation of relation-aware self-attention as Shaw et al. (2018). However, they only apply it to sequences of words in the context of machine translation, and as such, their relation 7568 types only encode the relative distance between two words. We extend their work and show that relationaware self-attention can effectively encode more complex relationships within an unordered set of elements (in our case, columns and tables within a database schema as well as relations between the schema and the question). To the best of our knowledge, this is the first application of relation-aware self-attention to joint representation learning with both predefined and"
2020.acl-main.677,Q13-1033,0,0.0640376,"Missing"
2020.acl-main.677,D18-1455,0,0.0186056,"self-attention can effectively encode more complex relationships within an unordered set of elements (in our case, columns and tables within a database schema as well as relations between the schema and the question). To the best of our knowledge, this is the first application of relation-aware self-attention to joint representation learning with both predefined and softly induced relations in the input structure. Hellendoorn et al. (2020) develop a similar model concurrently with this work, where they use relation-aware self-attention to encode data flow structure in source code embeddings. Sun et al. (2018) use a heterogeneous graph of KB facts and relevant documents for open-domain question answering. The nodes of their graph are analogous to the database schema nodes in RATSQL, but RAT-SQL also incorporates the question in the same formalism to enable joint representation learning between the question and the schema. computes a learned relation between all the input elements xi , and the strength of this relation (h) is encoded in the attention weights αij . However, in many applications (including text-to-SQL parsing) we are aware of some preexisting relational features between the inputs, an"
2020.acl-main.677,W00-1317,0,0.791305,"Missing"
2020.acl-main.677,P17-1041,0,0.395941,"ord within a value) of cj . This simple approach drastically improves the performance of RAT-SQL (see Section 5). It also directly addresses the aforementioned DB challenges: (a) the model is never exposed to database content that does not occur in the question, (b) word matches are retrieved quickly via DB indices & textual search. col K &gt; yi WQcol (cfinal j WK + r ij ) √ (3) dx tab K &gt; yi WQtab (tfinal j WK + r ij ) √ = dx  col  tab ˜ ˜ = softmax Li,j Ltab i,j = softmax Li,j ˜ col L i,j = ˜ tab L i,j Lcol i,j j Decoder The decoder fdec of RAT-SQL follows the treestructured architecture of Yin and Neubig (2017). It generates the SQL P as an abstract syntax tree in depth-first traversal order, by using an LSTM to output a sequence of decoder actions that either (i) expand the last generated node into a grammar rule, called A PPLY RULE; or when completing a leaf node, (ii) choose a column/table from the schema, called S ELECT C OLUMN and S QELECT TABLE. Formally, Pr(P |Y) = t Pr(at |a&lt;t , Y) where Y = fenc (GQ ) is the final encoding of the question and schema, and a&lt;t are all the previous actions. In a tree-structured decoder, the LSTM state is updated as mt , ht = fLSTM ([at−1 k z t k hpt k apt k nf"
2020.acl-main.677,D18-1193,0,0.662332,"f research has focused on the task of translating NL questions into SQL queries that existing database software can execute. The development of large annotated datasets of questions and the corresponding SQL queries has catalyzed progress in the field. In contrast to prior semantic parsing datasets (Finegan-Dollak et al., ∗ Equal contribution. Order decided by a coin toss. Work done during an internship at Microsoft Research. ‡ Work done while partly affiliated with Microsoft Research. Now at Microsoft: ricshin@microsoft.com. † 2018), new tasks such as WikiSQL (Zhong et al., 2017) and Spider (Yu et al., 2018b) pose the reallife challenge of generalization to unseen database schemas. Every query is conditioned on a multitable database schema, and the databases do not overlap between the train and test sets. Schema generalization is challenging for three interconnected reasons. First, any text-to-SQL parsing model must encode the schema into representations suitable for decoding a SQL query that might involve the given columns or tables. Second, these representations should encode all the information about the schema such as its column types, foreign key relations, and primary keys used for databas"
2020.coling-main.260,P14-1133,0,0.0371809,"ith simple string matching or embedding matching modules. Structural linking. While some entities are generated because they are mentioned in the NL question, others can be generated because of their role as special functional components in SQL, e.g., contid and stuid in the ON clauses of the first and third examples in Table 1. These entities usually cannot find their corresponding mentions in the NL question but can be induced by the structural constraints of SQL. Such phenomenons are generally referred to as the structural mismatch between NL and formal languages (Kwiatkowski et al., 2013; Berant and Liang, 2014). This process frequently occurs when generating complex SQL queries. We propose to treat this entity generation process as finding a structural link between current candidates and past generated entities. Although previous work has considered some simple structural constraints (Guo et al., 2019), to the best of our knowledge, we are the first to formally describe this process. While schema linking and structural linking can complement each other (e.g., the entity used by the GROUP BY clause needs to be a column in a previously selected table and also has its mention in the NL question), they"
2020.coling-main.260,P19-1448,0,0.295715,"auto-regressive model to capture structural information, current state-of-the-art semantic parsers usually adopt a grammar-based decoder (Xiao et al., 2016; Yin and Neubig, 2017; Krishnamurthy et al., 2017). Rather than directly generating the tokens in a traditional sequence-to-sequence manner, grammar-based decoders produce a sequence of production rules to construct an abstract syntax tree (AST) of the corresponding SQL. As the grammar constraints narrows down the search space to only grammatically valid ASTs, those parsers can usually generate well-formed SQL skeletons (Guo et al., 2019; Bogin et al., 2019a). However, it is still difficult for current state-of-the-art models to fill in the skeletons with semantically correct entities, especially when they are required to generalize to unseen DB schemas (Yu et al., 2018; Suhr et al., 2020). To predict the correct entity, the model should have a database (DB) schema grounded understanding of the NL question, which means that the model should be able to jointly learn the semantics in the NL question and the structured knowledge in a given database. We formulate two types of entity generation problems, which can be addressed by the following two li"
2020.coling-main.260,D19-1378,0,0.195676,"auto-regressive model to capture structural information, current state-of-the-art semantic parsers usually adopt a grammar-based decoder (Xiao et al., 2016; Yin and Neubig, 2017; Krishnamurthy et al., 2017). Rather than directly generating the tokens in a traditional sequence-to-sequence manner, grammar-based decoders produce a sequence of production rules to construct an abstract syntax tree (AST) of the corresponding SQL. As the grammar constraints narrows down the search space to only grammatically valid ASTs, those parsers can usually generate well-formed SQL skeletons (Guo et al., 2019; Bogin et al., 2019a). However, it is still difficult for current state-of-the-art models to fill in the skeletons with semantically correct entities, especially when they are required to generalize to unseen DB schemas (Yu et al., 2018; Suhr et al., 2020). To predict the correct entity, the model should have a database (DB) schema grounded understanding of the NL question, which means that the model should be able to jointly learn the semantics in the NL question and the structured knowledge in a given database. We formulate two types of entity generation problems, which can be addressed by the following two li"
2020.coling-main.260,P16-1073,0,0.0362365,"Missing"
2020.coling-main.260,D14-1179,0,0.0379613,"Missing"
2020.coling-main.260,N19-1423,0,0.0209011,"From the model’s perspective, it can also be viewed as a memory pointer network that enhances the structured prediction ability of an auto-regressive model, and the dynamic gating determines when to emphasize this enhancement. Our proposed method can be easily applied to most semantic parsers as long as their decoders explicitly or implicitly have two modules that deal with schema linking and structural linking. We integrate the dynamic gating technique to two state-of-the-art Text-to-SQL parsing models (Bogin et al., 2019a; Bogin et al., 2019b) and further augment them with pretrained BERT (Devlin et al., 2019) word representations. We evaluate our model on the Spider dataset which is challenging because of its 2901 Grammar Decoder Question For each continent, list its id, name, and how many countries it has query &quot;-> [&quot;select&quot;, selections, from, group_by] Embedding selections &quot;-> [selection, &quot;,&quot;, selections] NL Encoder selection &quot;-> [column] column &quot;-> [&quot;continents@contid&quot;] selections &quot;-> [selection, &quot;,&quot;, selection] selection &quot;-> [column] Schema Linking Decoder Memory Schema Graph continents@contid Structural Linking column &quot;-> [&quot;continents@continent&quot;] !.. !.. !.. from &quot;-> [&quot;from&quot;, table, join] tab"
2020.coling-main.260,P16-1004,0,0.0201393,"nships between entities. Currently, the entity representation provided by the GNN model is still difficult to explain, because the node representations contain a mix of information from different message-passing steps. One could imagine training GNNs with different message-passing steps each modeling a different level of structural linking, could lead to a more clear and expressive linking pattern. 5 Related Work Semantic parsing. Semantic parsing research focus on mapping NL to formal languages like lambda calculus (Zettlemoyer and Collins, 2005; Kwiatkowksi et al., 2010; Liang et al., 2011; Dong and Lapata, 2016), Prolog-style queries (Zelle and Mooney, 1996; Tang and Mooney, 2000), and more recently to SQL (Warren and Pereira, 1982; Popescu et al., 2003; Giordani and Moschitti, 2009; Zhong et al., 2017; Iyer et al., 2017). It can also tackle the problem of parsing NL descriptions to complicated general-purpose programming language such as Python (Ling et al., 2016; Rabinovich et al., 2017; Yin and Neubig, 2017). Our proposed method is tested for Text-to-SQL parsing and can be adapted to other semantic parsing applications. Structural mismatch. Programming languages like SQL express the same intent in"
2020.coling-main.260,W18-2501,0,0.0115755,"esentation. To augment our model with pretrained BERT embeddings, we follow Hwang et al. (2019) and Zhang et al. (2019) to feed the concatenation of NL question and the textual descriptions of DB entities to BERT and use the top layer hidden states of BERT as the input embeddings. 4 Experiments We evaluate the effectiveness of our proposed method by integrating it into two state-of-the-art semantic parsers on the Spider dataset and further ablate out some components to understand their contributions. 4.1 Experiment Setup We implement our model using PyTorch (Paszke et al., 2019) and AllenNLP (Gardner et al., 2018). For the GNN and G LOBAL GNN models we revise and build upon the code released in (Bogin et al., 2019a; Bogin et al., 2019b). We re-ran the experiment and report the results on our re-implementation and found our results slightly improves upon their reported results. In BERT experiments, we use the base uncased BERT model with 768 hidden size provided by HuggingFace’s Transformers library (Wolf et al., 2019). We follow the database split setting of Spider, where any databases that appear at testing time are ensured to be unseen at training time. Our code and models are available at https://gi"
2020.coling-main.260,P19-1444,0,0.142983,"the capacity of an auto-regressive model to capture structural information, current state-of-the-art semantic parsers usually adopt a grammar-based decoder (Xiao et al., 2016; Yin and Neubig, 2017; Krishnamurthy et al., 2017). Rather than directly generating the tokens in a traditional sequence-to-sequence manner, grammar-based decoders produce a sequence of production rules to construct an abstract syntax tree (AST) of the corresponding SQL. As the grammar constraints narrows down the search space to only grammatically valid ASTs, those parsers can usually generate well-formed SQL skeletons (Guo et al., 2019; Bogin et al., 2019a). However, it is still difficult for current state-of-the-art models to fill in the skeletons with semantically correct entities, especially when they are required to generalize to unseen DB schemas (Yu et al., 2018; Suhr et al., 2020). To predict the correct entity, the model should have a database (DB) schema grounded understanding of the NL question, which means that the model should be able to jointly learn the semantics in the NL question and the structured knowledge in a given database. We formulate two types of entity generation problems, which can be addressed by"
2020.coling-main.260,P17-1089,0,0.0819832,"active research topic in the field of natural language processing (NLP) for decades (Zettlemoyer and Collins, 2005; Liang et al., 2011). Although a variety of logic forms have been studied by researchers, Text-to-SQL has particularly attracted a large amount of attention due to the desire of natural language interfaces to database (NLIDB) (Warren and Pereira, 1982; Zelle and Mooney, 1996; Dong, 2019) for both scientific and industrial reasons. Recently, there is a growing interest in neural based Text-to-SQL semantic parsing, thanks to the development of new evaluation paradigms and datasets (Iyer et al., 2017; Zhong et al., 2017; Yu et al., 2018; Yu et al., 2019). Text-to-SQL parsing requires strict structured prediction due to its application scenario where the output SQL will be sent to an executor program directly. To enhance the capacity of an auto-regressive model to capture structural information, current state-of-the-art semantic parsers usually adopt a grammar-based decoder (Xiao et al., 2016; Yin and Neubig, 2017; Krishnamurthy et al., 2017). Rather than directly generating the tokens in a traditional sequence-to-sequence manner, grammar-based decoders produce a sequence of production rul"
2020.coling-main.260,W08-2105,0,0.0435245,"Tang and Mooney, 2000), and more recently to SQL (Warren and Pereira, 1982; Popescu et al., 2003; Giordani and Moschitti, 2009; Zhong et al., 2017; Iyer et al., 2017). It can also tackle the problem of parsing NL descriptions to complicated general-purpose programming language such as Python (Ling et al., 2016; Rabinovich et al., 2017; Yin and Neubig, 2017). Our proposed method is tested for Text-to-SQL parsing and can be adapted to other semantic parsing applications. Structural mismatch. Programming languages like SQL express the same intent in a completely different way from NL by design (Kate, 2008). The phenomenon called structural mismatch widely exists between NL and various programming language and is a major challenge in semantic parsing (Dong, 2019). To alleviate the structural mismatch problem, early approaches rely on linguistic formalisms like parsing results from flexible CCGs (Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007; Kwiatkowski et al., 2011; Kwiatkowski et al., 2013). Chen et al. (2016) proposed to use sentence rewriting to revise the NL question to a new question which has the same structure with the targeted logical form. Recently, Guo et al. (2019) pro"
2020.coling-main.260,D17-1160,0,0.209322,"easons. Recently, there is a growing interest in neural based Text-to-SQL semantic parsing, thanks to the development of new evaluation paradigms and datasets (Iyer et al., 2017; Zhong et al., 2017; Yu et al., 2018; Yu et al., 2019). Text-to-SQL parsing requires strict structured prediction due to its application scenario where the output SQL will be sent to an executor program directly. To enhance the capacity of an auto-regressive model to capture structural information, current state-of-the-art semantic parsers usually adopt a grammar-based decoder (Xiao et al., 2016; Yin and Neubig, 2017; Krishnamurthy et al., 2017). Rather than directly generating the tokens in a traditional sequence-to-sequence manner, grammar-based decoders produce a sequence of production rules to construct an abstract syntax tree (AST) of the corresponding SQL. As the grammar constraints narrows down the search space to only grammatically valid ASTs, those parsers can usually generate well-formed SQL skeletons (Guo et al., 2019; Bogin et al., 2019a). However, it is still difficult for current state-of-the-art models to fill in the skeletons with semantically correct entities, especially when they are required to generalize to unseen"
2020.coling-main.260,D10-1119,0,0.0346967,"y to take advantage of the complicated relationships between entities. Currently, the entity representation provided by the GNN model is still difficult to explain, because the node representations contain a mix of information from different message-passing steps. One could imagine training GNNs with different message-passing steps each modeling a different level of structural linking, could lead to a more clear and expressive linking pattern. 5 Related Work Semantic parsing. Semantic parsing research focus on mapping NL to formal languages like lambda calculus (Zettlemoyer and Collins, 2005; Kwiatkowksi et al., 2010; Liang et al., 2011; Dong and Lapata, 2016), Prolog-style queries (Zelle and Mooney, 1996; Tang and Mooney, 2000), and more recently to SQL (Warren and Pereira, 1982; Popescu et al., 2003; Giordani and Moschitti, 2009; Zhong et al., 2017; Iyer et al., 2017). It can also tackle the problem of parsing NL descriptions to complicated general-purpose programming language such as Python (Ling et al., 2016; Rabinovich et al., 2017; Yin and Neubig, 2017). Our proposed method is tested for Text-to-SQL parsing and can be adapted to other semantic parsing applications. Structural mismatch. Programming l"
2020.coling-main.260,D11-1140,0,0.0335222,"oposed method is tested for Text-to-SQL parsing and can be adapted to other semantic parsing applications. Structural mismatch. Programming languages like SQL express the same intent in a completely different way from NL by design (Kate, 2008). The phenomenon called structural mismatch widely exists between NL and various programming language and is a major challenge in semantic parsing (Dong, 2019). To alleviate the structural mismatch problem, early approaches rely on linguistic formalisms like parsing results from flexible CCGs (Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007; Kwiatkowski et al., 2011; Kwiatkowski et al., 2013). Chen et al. (2016) proposed to use sentence rewriting to revise the NL question to a new question which has the same structure with the targeted logical form. Recently, Guo et al. (2019) proposed to first translate the NL question to an intermediate representation (IR) designed to bridge NL and SQL, then use a deterministic algorithm to convert the IR to SQL. In addition to taking a considerable amount of engineering effort, their designed IR is still unable to cover some SQL grammars like the self-join in the ON clause, and is more challenging to apply to other pr"
2020.coling-main.260,D13-1161,0,0.108182,"lly address this problem with simple string matching or embedding matching modules. Structural linking. While some entities are generated because they are mentioned in the NL question, others can be generated because of their role as special functional components in SQL, e.g., contid and stuid in the ON clauses of the first and third examples in Table 1. These entities usually cannot find their corresponding mentions in the NL question but can be induced by the structural constraints of SQL. Such phenomenons are generally referred to as the structural mismatch between NL and formal languages (Kwiatkowski et al., 2013; Berant and Liang, 2014). This process frequently occurs when generating complex SQL queries. We propose to treat this entity generation process as finding a structural link between current candidates and past generated entities. Although previous work has considered some simple structural constraints (Guo et al., 2019), to the best of our knowledge, we are the first to formally describe this process. While schema linking and structural linking can complement each other (e.g., the entity used by the GROUP BY clause needs to be a column in a previously selected table and also has its mention i"
2020.coling-main.260,P11-1060,0,0.12971,"ork-based semantic parsers together with BERT representations demonstrates substantial gains in parsing accuracy on the challenging Spider dataset. Analyses show that our proposed method helps to enhance the structure of the model output when generating complicated SQL queries and offers more explainable predictions. 1 Introduction Semantic parsing, which aims at mapping natural language (NL) utterances to computer understandable logic forms or programming languages, has been an active research topic in the field of natural language processing (NLP) for decades (Zettlemoyer and Collins, 2005; Liang et al., 2011). Although a variety of logic forms have been studied by researchers, Text-to-SQL has particularly attracted a large amount of attention due to the desire of natural language interfaces to database (NLIDB) (Warren and Pereira, 1982; Zelle and Mooney, 1996; Dong, 2019) for both scientific and industrial reasons. Recently, there is a growing interest in neural based Text-to-SQL semantic parsing, thanks to the development of new evaluation paradigms and datasets (Iyer et al., 2017; Zhong et al., 2017; Yu et al., 2018; Yu et al., 2019). Text-to-SQL parsing requires strict structured prediction due"
2020.coling-main.260,P17-1003,0,0.0216474,"Memory networks were first introduced in the context of the question answering task, where they served as a differentiable long-term knowledge base to enhance an autoregressive model’s poor memory (Weston et al., 2015; Sukhbaatar et al., 2015). Copy mechanisms use attention as a pointer to select and copy items from source text, thus addressing the problem of a variable output vocabulary size (Vinyals et al., 2015; See et al., 2017). Recent research has applied memory-augmented pointer networks to various NLP tasks, including task-oriented dialogue (Wu et al., 2019) and also semantic parsing (Liang et al., 2017; Guo et al., 2019). Our dynamic gating mechanism can also be seen a memory controller, except our memory is read-only and acts as both the query and key in a pointer network. Different from most current techniques, our pointer network does not only perform copying but can also point to a start point of structural linking. 6 Conclusion In this paper, we formulated the entity generation process in Text-to-SQL semantic parsing as two kinds of linking problems, namely schema linking and structural linking. We further proposed a dynamic gating 2908 mechanism to explicitly model the decision betwee"
2020.coling-main.260,P16-1057,0,0.0679111,"Missing"
2020.coling-main.260,D15-1166,0,0.0167197,"uctural linking is modeled as: Pr(at = ei |S TRCT) = ρcopy (βcopy Tcopy )i + (1 − ρcopy )(βlink T )i (8) Finally, this probability is mixed with the probability of schema linking controlled by the link gate. 3 Model Implementation In this section, we describe how we integrate our proposed method into a grammar decoder and leverage the entity representation from a GNN module. We use the type constrained grammar decoder from Krishnamurthy et al. (2017). To predict at at time step t, the decoder will first obtain the context vector ct from the NL encoder by performing dot-product 2904 attention (Luong et al., 2015). Then the action embedding is generated by a feed-forward network taking the concatenation of decoder hidden state and context vector as input. at = FF([ht ; ct ]) (9) at is used to predict the production rule or estimate the gate values in the entity generation process. We adopt the idea from (Bogin et al., 2019a; Bogin et al., 2019b) to learn a schema relation-aware (0) entity representation HV by a GNN module.3 The initial embedding of each entity he is defined as a non-linear transformation of the combination of its type embedding and the average over the word embeddings of its neighbors"
2020.coling-main.260,P17-1105,0,0.0159386,"e linking pattern. 5 Related Work Semantic parsing. Semantic parsing research focus on mapping NL to formal languages like lambda calculus (Zettlemoyer and Collins, 2005; Kwiatkowksi et al., 2010; Liang et al., 2011; Dong and Lapata, 2016), Prolog-style queries (Zelle and Mooney, 1996; Tang and Mooney, 2000), and more recently to SQL (Warren and Pereira, 1982; Popescu et al., 2003; Giordani and Moschitti, 2009; Zhong et al., 2017; Iyer et al., 2017). It can also tackle the problem of parsing NL descriptions to complicated general-purpose programming language such as Python (Ling et al., 2016; Rabinovich et al., 2017; Yin and Neubig, 2017). Our proposed method is tested for Text-to-SQL parsing and can be adapted to other semantic parsing applications. Structural mismatch. Programming languages like SQL express the same intent in a completely different way from NL by design (Kate, 2008). The phenomenon called structural mismatch widely exists between NL and various programming language and is a major challenge in semantic parsing (Dong, 2019). To alleviate the structural mismatch problem, early approaches rely on linguistic formalisms like parsing results from flexible CCGs (Zettlemoyer and Collins, 2005;"
2020.coling-main.260,P17-1099,0,0.0205691,"uages. We deal with this problem by explicitly modeling the prediction structure with external predefined structure (i.e., DB schema) by structural linking. Memory pointer network. Memory networks were first introduced in the context of the question answering task, where they served as a differentiable long-term knowledge base to enhance an autoregressive model’s poor memory (Weston et al., 2015; Sukhbaatar et al., 2015). Copy mechanisms use attention as a pointer to select and copy items from source text, thus addressing the problem of a variable output vocabulary size (Vinyals et al., 2015; See et al., 2017). Recent research has applied memory-augmented pointer networks to various NLP tasks, including task-oriented dialogue (Wu et al., 2019) and also semantic parsing (Liang et al., 2017; Guo et al., 2019). Our dynamic gating mechanism can also be seen a memory controller, except our memory is read-only and acts as both the query and key in a pointer network. Different from most current techniques, our pointer network does not only perform copying but can also point to a start point of structural linking. 6 Conclusion In this paper, we formulated the entity generation process in Text-to-SQL semant"
2020.coling-main.260,P19-1010,0,0.0227695,"L encoder, a schema encoder and a grammar decoder (Guo et al., 2019; Bogin et al., 2019a). The NL encoder takes the NL question tokens Q as input, maps them to word embeddings EQ , then feeds them to a Bi-LSTM (Hochreiter and Schmidhuber, 1997). The hidden states of the Bi-LSTM serve as the contextual word representation of each token. The schema encoder takes G as input and builds a relation-aware entity representation for every entity in the schema. The initial representation of an entity is a combination of its words embeddings and type information. Then self-attention (Zhang et al., 2019; Shaw et al., 2019) or graph-based models (Bogin et al., 2019a; Wang et al., 2020) are utilized to exploit the relational information between each pair of 1 Note that columns may have more fine-grained types like binary, numeric, string and date/time, primary/foreign etc. In SQL, a foreign key in one table is used to refer to a primary key in another table to link these two tables together for joint queries. 2 2902 l ctura Stru ing link Link gate Sche m linki a ng Copy entity Copy gate Link entity NL encoder Figure 2: An illustration of our gating mechanism. entities from the DB schema, thus produce the final re"
2020.coling-main.260,2020.acl-main.742,0,0.0175821,"ing the tokens in a traditional sequence-to-sequence manner, grammar-based decoders produce a sequence of production rules to construct an abstract syntax tree (AST) of the corresponding SQL. As the grammar constraints narrows down the search space to only grammatically valid ASTs, those parsers can usually generate well-formed SQL skeletons (Guo et al., 2019; Bogin et al., 2019a). However, it is still difficult for current state-of-the-art models to fill in the skeletons with semantically correct entities, especially when they are required to generalize to unseen DB schemas (Yu et al., 2018; Suhr et al., 2020). To predict the correct entity, the model should have a database (DB) schema grounded understanding of the NL question, which means that the model should be able to jointly learn the semantics in the NL question and the structured knowledge in a given database. We formulate two types of entity generation problems, which can be addressed by the following two linking processes respectively. This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http:// creativecommons.org/licenses/by/4.0/. 2900 Proceedings of the 28th International Conference on C"
2020.coling-main.260,W00-1317,0,0.238271,"by the GNN model is still difficult to explain, because the node representations contain a mix of information from different message-passing steps. One could imagine training GNNs with different message-passing steps each modeling a different level of structural linking, could lead to a more clear and expressive linking pattern. 5 Related Work Semantic parsing. Semantic parsing research focus on mapping NL to formal languages like lambda calculus (Zettlemoyer and Collins, 2005; Kwiatkowksi et al., 2010; Liang et al., 2011; Dong and Lapata, 2016), Prolog-style queries (Zelle and Mooney, 1996; Tang and Mooney, 2000), and more recently to SQL (Warren and Pereira, 1982; Popescu et al., 2003; Giordani and Moschitti, 2009; Zhong et al., 2017; Iyer et al., 2017). It can also tackle the problem of parsing NL descriptions to complicated general-purpose programming language such as Python (Ling et al., 2016; Rabinovich et al., 2017; Yin and Neubig, 2017). Our proposed method is tested for Text-to-SQL parsing and can be adapted to other semantic parsing applications. Structural mismatch. Programming languages like SQL express the same intent in a completely different way from NL by design (Kate, 2008). The phenom"
2020.coling-main.260,2020.acl-main.677,1,0.924791,"; 3 SELECT Q: What is the first name and gender of the all the students who have more than one pet? t1 . fname , t1 . sex FROM student AS t1 JOIN has pet AS t2 ON t1 . stuid = t2 . stuid GROUP BY t1 . stuid HAVING COUNT(∗) > 1 Table 1: Several examples that are taken from the Spider dataset. Entity mentions are underlined in NL questions. Q1 and Q2 are paraphrases of each other which should lead to the same SQL result. ::::: Wavy underline indicates the mention can only be resolved by linking to a cell value or common sense reasoning. :::::::: Schema linking. Schema linking (Guo et al., 2019; Wang et al., 2020) is an instance of entity linking (Shen et al., 2014) in the context of linking to relational DB schema. Text-to-SQL semantic parsers should learn to recognize an entity mention in the NL question and link it to the corresponding unique entity in the DB schema. This task can be challenging due to the diversity and ambiguity NL mentions. However, in practice, the solution is often relatively easy when a particular entity is well realized with similar wording in both the NL question and DB schema. As shown in Table 1, in Spider (Yu et al., 2018), the underlined mentions can almost exactly match"
2020.coling-main.260,J82-3002,0,0.58511,"odel output when generating complicated SQL queries and offers more explainable predictions. 1 Introduction Semantic parsing, which aims at mapping natural language (NL) utterances to computer understandable logic forms or programming languages, has been an active research topic in the field of natural language processing (NLP) for decades (Zettlemoyer and Collins, 2005; Liang et al., 2011). Although a variety of logic forms have been studied by researchers, Text-to-SQL has particularly attracted a large amount of attention due to the desire of natural language interfaces to database (NLIDB) (Warren and Pereira, 1982; Zelle and Mooney, 1996; Dong, 2019) for both scientific and industrial reasons. Recently, there is a growing interest in neural based Text-to-SQL semantic parsing, thanks to the development of new evaluation paradigms and datasets (Iyer et al., 2017; Zhong et al., 2017; Yu et al., 2018; Yu et al., 2019). Text-to-SQL parsing requires strict structured prediction due to its application scenario where the output SQL will be sent to an executor program directly. To enhance the capacity of an auto-regressive model to capture structural information, current state-of-the-art semantic parsers usuall"
2020.coling-main.260,P16-1127,0,0.129785,"019) for both scientific and industrial reasons. Recently, there is a growing interest in neural based Text-to-SQL semantic parsing, thanks to the development of new evaluation paradigms and datasets (Iyer et al., 2017; Zhong et al., 2017; Yu et al., 2018; Yu et al., 2019). Text-to-SQL parsing requires strict structured prediction due to its application scenario where the output SQL will be sent to an executor program directly. To enhance the capacity of an auto-regressive model to capture structural information, current state-of-the-art semantic parsers usually adopt a grammar-based decoder (Xiao et al., 2016; Yin and Neubig, 2017; Krishnamurthy et al., 2017). Rather than directly generating the tokens in a traditional sequence-to-sequence manner, grammar-based decoders produce a sequence of production rules to construct an abstract syntax tree (AST) of the corresponding SQL. As the grammar constraints narrows down the search space to only grammatically valid ASTs, those parsers can usually generate well-formed SQL skeletons (Guo et al., 2019; Bogin et al., 2019a). However, it is still difficult for current state-of-the-art models to fill in the skeletons with semantically correct entities, especi"
2020.coling-main.260,P17-1041,0,0.257772,"tific and industrial reasons. Recently, there is a growing interest in neural based Text-to-SQL semantic parsing, thanks to the development of new evaluation paradigms and datasets (Iyer et al., 2017; Zhong et al., 2017; Yu et al., 2018; Yu et al., 2019). Text-to-SQL parsing requires strict structured prediction due to its application scenario where the output SQL will be sent to an executor program directly. To enhance the capacity of an auto-regressive model to capture structural information, current state-of-the-art semantic parsers usually adopt a grammar-based decoder (Xiao et al., 2016; Yin and Neubig, 2017; Krishnamurthy et al., 2017). Rather than directly generating the tokens in a traditional sequence-to-sequence manner, grammar-based decoders produce a sequence of production rules to construct an abstract syntax tree (AST) of the corresponding SQL. As the grammar constraints narrows down the search space to only grammatically valid ASTs, those parsers can usually generate well-formed SQL skeletons (Guo et al., 2019; Bogin et al., 2019a). However, it is still difficult for current state-of-the-art models to fill in the skeletons with semantically correct entities, especially when they are req"
2020.coling-main.260,D18-1425,0,0.261736,"atural language processing (NLP) for decades (Zettlemoyer and Collins, 2005; Liang et al., 2011). Although a variety of logic forms have been studied by researchers, Text-to-SQL has particularly attracted a large amount of attention due to the desire of natural language interfaces to database (NLIDB) (Warren and Pereira, 1982; Zelle and Mooney, 1996; Dong, 2019) for both scientific and industrial reasons. Recently, there is a growing interest in neural based Text-to-SQL semantic parsing, thanks to the development of new evaluation paradigms and datasets (Iyer et al., 2017; Zhong et al., 2017; Yu et al., 2018; Yu et al., 2019). Text-to-SQL parsing requires strict structured prediction due to its application scenario where the output SQL will be sent to an executor program directly. To enhance the capacity of an auto-regressive model to capture structural information, current state-of-the-art semantic parsers usually adopt a grammar-based decoder (Xiao et al., 2016; Yin and Neubig, 2017; Krishnamurthy et al., 2017). Rather than directly generating the tokens in a traditional sequence-to-sequence manner, grammar-based decoders produce a sequence of production rules to construct an abstract syntax tr"
2020.coling-main.260,P19-1443,0,0.0148929,"rocessing (NLP) for decades (Zettlemoyer and Collins, 2005; Liang et al., 2011). Although a variety of logic forms have been studied by researchers, Text-to-SQL has particularly attracted a large amount of attention due to the desire of natural language interfaces to database (NLIDB) (Warren and Pereira, 1982; Zelle and Mooney, 1996; Dong, 2019) for both scientific and industrial reasons. Recently, there is a growing interest in neural based Text-to-SQL semantic parsing, thanks to the development of new evaluation paradigms and datasets (Iyer et al., 2017; Zhong et al., 2017; Yu et al., 2018; Yu et al., 2019). Text-to-SQL parsing requires strict structured prediction due to its application scenario where the output SQL will be sent to an executor program directly. To enhance the capacity of an auto-regressive model to capture structural information, current state-of-the-art semantic parsers usually adopt a grammar-based decoder (Xiao et al., 2016; Yin and Neubig, 2017; Krishnamurthy et al., 2017). Rather than directly generating the tokens in a traditional sequence-to-sequence manner, grammar-based decoders produce a sequence of production rules to construct an abstract syntax tree (AST) of the co"
2020.coling-main.260,D07-1071,0,0.0463775,"; Yin and Neubig, 2017). Our proposed method is tested for Text-to-SQL parsing and can be adapted to other semantic parsing applications. Structural mismatch. Programming languages like SQL express the same intent in a completely different way from NL by design (Kate, 2008). The phenomenon called structural mismatch widely exists between NL and various programming language and is a major challenge in semantic parsing (Dong, 2019). To alleviate the structural mismatch problem, early approaches rely on linguistic formalisms like parsing results from flexible CCGs (Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007; Kwiatkowski et al., 2011; Kwiatkowski et al., 2013). Chen et al. (2016) proposed to use sentence rewriting to revise the NL question to a new question which has the same structure with the targeted logical form. Recently, Guo et al. (2019) proposed to first translate the NL question to an intermediate representation (IR) designed to bridge NL and SQL, then use a deterministic algorithm to convert the IR to SQL. In addition to taking a considerable amount of engineering effort, their designed IR is still unable to cover some SQL grammars like the self-join in the ON clause, and is more challe"
2020.emnlp-main.463,N19-4009,0,0.0767189,"Missing"
2020.emnlp-main.463,P19-1558,0,0.130843,"Missing"
2020.repl4nlp-1.8,N19-1423,0,0.0539052,"ild. Thus, we can infer that a possible reason that the writer needs to be dressed by other people is that he or she may have a physical disability (Huang et al., 2019). Although a simple task for humans, it is still challenging for computers to understand and reason about commonsense. Commonsense inference in natural language processing (NLP) is generally evaluated via machine reading comprehension task, in the format of selecting plausible responses with respect to natural language queries. Recent approaches are based on the use of pre-trained Transformer-based language models such as BERT (Devlin et al., 2019). Some approaches rely solely on these models by adopting either a single or multi-stage fine-tuning approach (by fine-tuning using additional datasets in a stepwise manner) (Li and Xie, 2019; Sharma and Roychowdhury, 2019; Liu and Yu, 2019; Huang et al., Table 1: Example from the CosmosQA dataset (Huang et al., 2019). The task is to identify the correct answer option. The correct answer is in bold. 2019; Zhou et al., 2019), while others further enhance their word representations with knowledge bases such as ConceptNet (Jain and Singh, 2019; Da, 2019; Wang et al., 2020). However, due to the of"
2020.repl4nlp-1.8,D19-1243,0,0.0659429,"Missing"
2020.repl4nlp-1.8,2021.ccl-1.108,0,0.161232,"Missing"
2020.repl4nlp-1.8,D19-6008,0,0.0284281,"ained Transformer-based language models such as BERT (Devlin et al., 2019). Some approaches rely solely on these models by adopting either a single or multi-stage fine-tuning approach (by fine-tuning using additional datasets in a stepwise manner) (Li and Xie, 2019; Sharma and Roychowdhury, 2019; Liu and Yu, 2019; Huang et al., Table 1: Example from the CosmosQA dataset (Huang et al., 2019). The task is to identify the correct answer option. The correct answer is in bold. 2019; Zhou et al., 2019), while others further enhance their word representations with knowledge bases such as ConceptNet (Jain and Singh, 2019; Da, 2019; Wang et al., 2020). However, due to the often limited data from the downstream tasks and the extremely high complexity of the pre-trained model, aggressive fine-tuning can easily make the adapted model overfit the data of the target task, making it unable to generalize well on unseen data (Jiang et al., 2019). Moreover, some researchers have shown that such pre-trained models are vulnerable to adversarial attacks (Jin et al., 2020). Inspired by the recent success of adversarial training in NLP (Zhu et al., 2020; Jiang et al., 2019), our AdversariaL training algorithm for commonsens"
2020.repl4nlp-1.8,S19-1012,0,0.0190808,"he perturbation δ. This model recently obtained state-of-the-art results on a bunch of NLP tasks on the GLUE benchmark (Wang et al., 2018). We also compare ALICE with a baseline that uses only the label y for estimating the perturbation δ (called model ADV hereafter) (Madry et al., 2018). The summary of the datasets is in Table 2. For the MCTACO dataset, no training set is available. Following (Zhou et al., 2019), we use the dev set for fine-tuning the model. We perform 5-fold crossvalidation for fine-tuning the parameters. We evaluate CosmosQA and MCScript2.0 in terms of accuracy. Following (Ostermann et al., 2019a), we also report for the MCScript2.0 accuracy on the commonsense based questions and accuracy on the questions that are not commonsense based. For the MCTACO, we report the exact match (EM) and F1 scores, following (Zhou et al., 2019). EM measures how many questions a system correctly labeled all candidate answers, while F1 measures the average overlap between one’s predictions and the ground truth. Our implementation for pairwise text classification and relevance ranking tasks are based on the MT-DNN framework1 (Liu et al., 2019a, 2020). 3.4 3.2 Baselines Results The results are summarized"
2020.repl4nlp-1.8,D19-6007,0,0.0114632,"he perturbation δ. This model recently obtained state-of-the-art results on a bunch of NLP tasks on the GLUE benchmark (Wang et al., 2018). We also compare ALICE with a baseline that uses only the label y for estimating the perturbation δ (called model ADV hereafter) (Madry et al., 2018). The summary of the datasets is in Table 2. For the MCTACO dataset, no training set is available. Following (Zhou et al., 2019), we use the dev set for fine-tuning the model. We perform 5-fold crossvalidation for fine-tuning the parameters. We evaluate CosmosQA and MCScript2.0 in terms of accuracy. Following (Ostermann et al., 2019a), we also report for the MCScript2.0 accuracy on the commonsense based questions and accuracy on the questions that are not commonsense based. For the MCTACO, we report the exact match (EM) and F1 scores, following (Zhou et al., 2019). EM measures how many questions a system correctly labeled all candidate answers, while F1 measures the average overlap between one’s predictions and the ground truth. Our implementation for pairwise text classification and relevance ranking tasks are based on the MT-DNN framework1 (Liu et al., 2019a, 2020). 3.4 3.2 Baselines Results The results are summarized"
2020.repl4nlp-1.8,D19-6009,0,0.020467,"ill challenging for computers to understand and reason about commonsense. Commonsense inference in natural language processing (NLP) is generally evaluated via machine reading comprehension task, in the format of selecting plausible responses with respect to natural language queries. Recent approaches are based on the use of pre-trained Transformer-based language models such as BERT (Devlin et al., 2019). Some approaches rely solely on these models by adopting either a single or multi-stage fine-tuning approach (by fine-tuning using additional datasets in a stepwise manner) (Li and Xie, 2019; Sharma and Roychowdhury, 2019; Liu and Yu, 2019; Huang et al., Table 1: Example from the CosmosQA dataset (Huang et al., 2019). The task is to identify the correct answer option. The correct answer is in bold. 2019; Zhou et al., 2019), while others further enhance their word representations with knowledge bases such as ConceptNet (Jain and Singh, 2019; Da, 2019; Wang et al., 2020). However, due to the often limited data from the downstream tasks and the extremely high complexity of the pre-trained model, aggressive fine-tuning can easily make the adapted model overfit the data of the target task, making it unable to gener"
2020.repl4nlp-1.8,W18-5446,0,0.103196,"Missing"
2020.repl4nlp-1.8,D18-1009,0,0.024548,"e judged as plausible or not. The sentences are taken from different sources such as news, Wikipedia and textbooks. We compare ALICE to a list of state-of-the-art models, as shown in Table 3. BERT + unit normalization (Zhou et al., 2019) is the BERT base model. The authors further add unit normalization to temporal expressions in candidate answers and finetune on the MC-TACO dataset. RoBERTaLARGE is our re-implementation of the large RoBERTa model by (Liu et al., 2019b). PSH-SJTU (Li and Xie, 2019) is based on multi-stage fine-tuning XLNET (Yang et al., 2019) on RACE (Lai et al., 2017), SWAG (Zellers et al., 2018) and MC-Script2.0 datasets. K-ADAPTER (Wang et al., 2020) further enhances RoBERTa word representations with multiple knowledge sources, such as factual knowledge obtained through Wikipedia and Wikidata and linguistic knowledge obtained through dependency parsing web texts. SMART (Jiang et al., 2019) is an adversarial training model for fine-tuning pretrained language models through regularization. SMART uses the model prediction, f (x; θ), for estimating the perturbation δ. This model recently obtained state-of-the-art results on a bunch of NLP tasks on the GLUE benchmark (Wang et al., 2018)."
2020.repl4nlp-1.8,D19-1332,0,0.306941,"ecting plausible responses with respect to natural language queries. Recent approaches are based on the use of pre-trained Transformer-based language models such as BERT (Devlin et al., 2019). Some approaches rely solely on these models by adopting either a single or multi-stage fine-tuning approach (by fine-tuning using additional datasets in a stepwise manner) (Li and Xie, 2019; Sharma and Roychowdhury, 2019; Liu and Yu, 2019; Huang et al., Table 1: Example from the CosmosQA dataset (Huang et al., 2019). The task is to identify the correct answer option. The correct answer is in bold. 2019; Zhou et al., 2019), while others further enhance their word representations with knowledge bases such as ConceptNet (Jain and Singh, 2019; Da, 2019; Wang et al., 2020). However, due to the often limited data from the downstream tasks and the extremely high complexity of the pre-trained model, aggressive fine-tuning can easily make the adapted model overfit the data of the target task, making it unable to generalize well on unseen data (Jiang et al., 2019). Moreover, some researchers have shown that such pre-trained models are vulnerable to adversarial attacks (Jin et al., 2020). Inspired by the recent success o"
2020.repl4nlp-1.8,D17-1082,0,\N,Missing
2020.repl4nlp-1.8,P19-1441,1,\N,Missing
2020.repl4nlp-1.8,D19-6011,0,\N,Missing
2020.repl4nlp-1.8,2020.acl-demos.16,1,\N,Missing
2021.acl-long.240,P17-1171,0,0.224177,"d approach by combining answers from both readers can effectively take advantages of extractive and generative answer inference strategies and outperform single models as well as homogeneous ensembles. Our approach outperforms previous state-of-the-art models by 3.3 and 2.7 points in exact match on NaturalQuestions and TriviaQA respectively. 1 Introduction Open-domain question answering (QA) has been a long standing problem in natural language understanding, information retrieval, and related fields (Chen and Yih, 2020). An typical open-domain QA system follows the retrieval-reader framework (Chen et al., 2017; Guu et al., 2020; Karpukhin et al., 2020), where the relevant passages are first retrieved from a large text corpus, and a reader module then navigates multiple passages for answer inference. In this work, we study two paradigms of reader modules, i.e. extractive (Karpukhin et al., 2020; Guu et al., 2020) and generative (Lewis et al., 2020; Izacard and Grave, 2021) readers. The extractive reader extracts contiguous spans from the retrieved passages whereas the generative reader sequentially decodes the answer string which might not be contained in the retrieved passages. ∗ Equal Contribution"
2021.acl-long.240,2020.acl-tutorials.8,0,0.0234693,"provide large improvements over previous state-of-the-art models. We demonstrate that an hybrid approach by combining answers from both readers can effectively take advantages of extractive and generative answer inference strategies and outperform single models as well as homogeneous ensembles. Our approach outperforms previous state-of-the-art models by 3.3 and 2.7 points in exact match on NaturalQuestions and TriviaQA respectively. 1 Introduction Open-domain question answering (QA) has been a long standing problem in natural language understanding, information retrieval, and related fields (Chen and Yih, 2020). An typical open-domain QA system follows the retrieval-reader framework (Chen et al., 2017; Guu et al., 2020; Karpukhin et al., 2020), where the relevant passages are first retrieved from a large text corpus, and a reader module then navigates multiple passages for answer inference. In this work, we study two paradigms of reader modules, i.e. extractive (Karpukhin et al., 2020; Guu et al., 2020) and generative (Lewis et al., 2020; Izacard and Grave, 2021) readers. The extractive reader extracts contiguous spans from the retrieved passages whereas the generative reader sequentially decodes th"
2021.acl-long.240,2020.acl-main.501,1,0.893399,"p(se (j)), respectively. Since there are usually multiple plausible mentions for open-domain QA, during training, it is typical to maximize either the marginal log-likelihood (MML) of all correct spans (Karpukhin et al., 2020) or the log-likelihood of the most likely correct span (HardEM) (Min et al., 2019). During inference, the prediction is made based on the candidate P answer string score, obtaining as Pa (y) = (i,j)∈Y Ps (i, j), where Y is the set of spans corresponding to the answer string y. 2.1.2 Multi-objective for Weakly-supervised QA The multi-objective formulation is introduced in Cheng et al. (2020) for improving weakly supervised document-level QA. Different from Cheng et al. (2020) where only MML is considered for the multi-objective formulation, we found combining HardEM with MML is more effective for open-domain QA based on our experiments (§4.1). Specifically, we combine a multi-passage HardEM loss with K passage-level MML losses over a batch of K passages LEXT = log max PsM (i, j) + (i,j) X 1 X log PsP (ik , j k ), (1) K k k k where PsM , PsP is the multi-passage level and passage level span probabilities respectively. Posterior Differential Regularization Due to the noisy supervis"
2021.acl-long.240,2021.naacl-main.85,1,0.878413,"h to combine the predictions from extractive and generative readers. It achieves state-of-theart results on NaturalQuestions (Kwiatkowski et al., 2019) and TriviaQA (Joshi et al., 2017). In UnitedQA, the extractive reader (UnitedQAE) and generative reader (UnitedQA-G) are built upon the pretrained language models, ELECTRA (Clark et al., 2020) and T5 (Raffel et al., 2020), respectively. For the UnitedQA-E, we adopt a weakly-supervised training objective to address the noisy supervision issue caused by the heuristicsbased labeling and incorporate the posterior differential regularization (PDR) (Cheng et al., 2021) to improve the model robustness. The UnitedQA-G follows the T5 Fusion-in-Decoder (FID) (Izacard and Grave, 2021) and we make two improvements: first, we add a group of attention bias parameters into the decoder cross-attention block to feature the ranking information of retrieved contexts; second, we add the adversarial training (Ju et al., 2019; Jiang et al., 2020; Pereira et al., 2021) to improve the model generalization ability. The experimental results highlight the effec3080 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International"
2021.acl-long.240,2020.acl-main.197,1,0.741561,"Missing"
2021.acl-long.240,P18-1078,0,0.0190922,"ate of the passage index during the training. Instead, in this work, we focus on a hybrid reader approach for open-domain QA. By simply combing answer predictions from extractive and generative models, our UnitedQA achieves significant improvements over state-of-the-art models. Reading Comprehension with Noisy Labels There has been a line of work on improving distantly-supervised reading comprehension models by developing learning methods and model architectures that can better use noisy labels. Most of them focus on the document-level QA, where all paragraphs share the same document context. Clark and Gardner (2018) propose a paragraphpair ranking objective for learning with multiple paragraphs so that the model can distinguish relevant paragraphs from irrelevant ones. In (Lin et al., 2018), a coarse-to-fine model is proposed to handle label noise by aggregating information from relevant paragraphs and then extracting answers from selected ones. Min et al. (2019) propose a hard EM learning scheme where only passage-level loss is considered for document-level QA. More recently, different probabilistic assumptions with corresponding training and inference methods are examined in (Cheng et al., 2020) again"
2021.acl-long.240,P17-1147,0,0.0306167,"to Google Search, each with answers identified by human annotators in Wikipedia. The open-domain version of NaturalQuestions (Lee et al., 2019) only consider questions with short answers, i.e. answers with less than 5 tokens. In the NaturualQuestions, the questions are considered to be more information seeking given that the question askers didn’t know the answer beforehand. In addition, we use another evaluation set, i.e. the dev set introduced recently by the EfficientQA competition (Min et al., 2021), which is constructed in the same way as the original NaturalQuestions dataset. TriviaQA (Joshi et al., 2017) contains trivia question-answer pairs that were scraped from the web. Different from NaturalQuestions, the questions here are written with known answers in mind. Specifically, the unfiltered set has been used for developing open-domain QA models. Implementation details For a fair comparison, we use the same retrieval module as Karpukhin et al. (2020) for NaturalQuestions and TriviaQA to mitigate the impact of retrieval difference. Specifically, we use DPR (single) for NaturalQuestions and BM25+DPR (multi) for TriviaQA because of their best end-to-end performance (Karpukhin et al. 2020). For a"
2021.acl-long.240,N19-1423,0,0.190193,"ven question. Then, the module of hybrid readers produces answer candidates from the set of retrieved passages. Last, the re-ranking module combines the answer candidates with linear interpolation and produce the final answer. Retrieval Following Karpukhin et al. (2020), we consider two methods, BM25 and dense passage retrieval (DPR), for retrieving the support passages for a given question. For BM25, passages are encoded as bag of words (BOW), and inverse document frequencies are used as the ranking function. For DPR, passages and questions are represented as dense vectors based on two BERT (Devlin et al., 2019) models. The relevance score is then computed based on the dot production between the query and passage vectors. In this paper, we adopt the same implementation as Karpukhin et al. (2020) for retrieving passages. Specifically, the English Wikipedia dump from Dec. 20, 2018 is used as the source documents for retrieval, with the removal of semi-structured data, such as tables or lists. Each document is split into disjoint 100-word passages as the basic retrieval unit. The top-100 passages are then passed for reading. Reading We combine the generative reader and the extractive reader to produce a"
2021.acl-long.240,2020.emnlp-main.550,0,0.0772381,"both readers can effectively take advantages of extractive and generative answer inference strategies and outperform single models as well as homogeneous ensembles. Our approach outperforms previous state-of-the-art models by 3.3 and 2.7 points in exact match on NaturalQuestions and TriviaQA respectively. 1 Introduction Open-domain question answering (QA) has been a long standing problem in natural language understanding, information retrieval, and related fields (Chen and Yih, 2020). An typical open-domain QA system follows the retrieval-reader framework (Chen et al., 2017; Guu et al., 2020; Karpukhin et al., 2020), where the relevant passages are first retrieved from a large text corpus, and a reader module then navigates multiple passages for answer inference. In this work, we study two paradigms of reader modules, i.e. extractive (Karpukhin et al., 2020; Guu et al., 2020) and generative (Lewis et al., 2020; Izacard and Grave, 2021) readers. The extractive reader extracts contiguous spans from the retrieved passages whereas the generative reader sequentially decodes the answer string which might not be contained in the retrieved passages. ∗ Equal Contribution Recent work on open-domain QA (Karpukhin e"
2021.acl-long.240,Q19-1026,0,0.0162195,"2: Comparison to state-of-the-art models on the test sets of NaturualQuestions (NQ) and TriviaQA. Exact match score is used for evaluation. The overall best model is in Box , the best single model is in bold, and the best model with the smallest reader size is in underline. work (Lee et al., 2019; Karpukhin et al., 2020). Both datasets (see Table 1 for statistics) have been heavily studied in recent work (Lee et al., 2019; Min et al., 2019; Karpukhin et al., 2020; Guu et al., 2020). We follow the standard evaluation protocol and use exact match (EM) as the evaluation metric. NaturalQuestions (Kwiatkowski et al., 2019) is composed of questions by real users to Google Search, each with answers identified by human annotators in Wikipedia. The open-domain version of NaturalQuestions (Lee et al., 2019) only consider questions with short answers, i.e. answers with less than 5 tokens. In the NaturualQuestions, the questions are considered to be more information seeking given that the question askers didn’t know the answer beforehand. In addition, we use another evaluation set, i.e. the dev set introduced recently by the EfficientQA competition (Min et al., 2021), which is constructed in the same way as the origin"
2021.acl-long.240,P19-1612,0,0.0877117,"48.2 51.4 57.9 65.0 67.6 UnitedQA-Ebase (Ours) UnitedQA-Elarge (Ours) UnitedQA-Glarge (Ours) Extractive Extractive Generative 110 330 770 47.7 51.8 52.3 66.3 68.9 68.6 UnitedQA-Elarge ++ (Ours) UnitedQA-Glarge ++ (Ours) UnitedQA (Ours) Ensemble Ensemble Hybrid 3x330 3x770 2x770+330 52.4 53.3 54.7 69.6 69.2 70.5 Table 2: Comparison to state-of-the-art models on the test sets of NaturualQuestions (NQ) and TriviaQA. Exact match score is used for evaluation. The overall best model is in Box , the best single model is in bold, and the best model with the smallest reader size is in underline. work (Lee et al., 2019; Karpukhin et al., 2020). Both datasets (see Table 1 for statistics) have been heavily studied in recent work (Lee et al., 2019; Min et al., 2019; Karpukhin et al., 2020; Guu et al., 2020). We follow the standard evaluation protocol and use exact match (EM) as the evaluation metric. NaturalQuestions (Kwiatkowski et al., 2019) is composed of questions by real users to Google Search, each with answers identified by human annotators in Wikipedia. The open-domain version of NaturalQuestions (Lee et al., 2019) only consider questions with short answers, i.e. answers with less than 5 tokens. In the"
2021.acl-long.240,2021.eacl-main.74,0,0.480985,"n-domain question answering (QA) has been a long standing problem in natural language understanding, information retrieval, and related fields (Chen and Yih, 2020). An typical open-domain QA system follows the retrieval-reader framework (Chen et al., 2017; Guu et al., 2020; Karpukhin et al., 2020), where the relevant passages are first retrieved from a large text corpus, and a reader module then navigates multiple passages for answer inference. In this work, we study two paradigms of reader modules, i.e. extractive (Karpukhin et al., 2020; Guu et al., 2020) and generative (Lewis et al., 2020; Izacard and Grave, 2021) readers. The extractive reader extracts contiguous spans from the retrieved passages whereas the generative reader sequentially decodes the answer string which might not be contained in the retrieved passages. ∗ Equal Contribution Recent work on open-domain QA (Karpukhin et al., 2020; Guu et al., 2020; Lewis et al., 2020; Izacard and Grave, 2021) explores either an extractive reader or a generative reader exclusively. We hypothesize that extractive and generative readers adopt different answer inference strategies, thus a hybrid extractive/generative reader can be a better option for open-dom"
2021.acl-long.240,2021.eacl-main.86,0,0.0381828,"Missing"
2021.acl-long.240,P18-1161,0,0.0188873,"nerative models, our UnitedQA achieves significant improvements over state-of-the-art models. Reading Comprehension with Noisy Labels There has been a line of work on improving distantly-supervised reading comprehension models by developing learning methods and model architectures that can better use noisy labels. Most of them focus on the document-level QA, where all paragraphs share the same document context. Clark and Gardner (2018) propose a paragraphpair ranking objective for learning with multiple paragraphs so that the model can distinguish relevant paragraphs from irrelevant ones. In (Lin et al., 2018), a coarse-to-fine model is proposed to handle label noise by aggregating information from relevant paragraphs and then extracting answers from selected ones. Min et al. (2019) propose a hard EM learning scheme where only passage-level loss is considered for document-level QA. More recently, different probabilistic assumptions with corresponding training and inference methods are examined in (Cheng et al., 2020) again for documentlevel QA with distant supervision. In our work, we further extend the multi-objective formulation proposed in (Cheng et al., 2020) with the hard EM learning (Min et a"
2021.acl-long.240,D19-1284,0,0.116279,"itions from the k-th passage and NULL indicates special positions if pk does not support answering the question. Similarly, the multi-passage level probability is computed by normalizing over each answer positions P across P all K relevant pas∗ sages, P i.e. P Zb = Zb = k I k exp(sb (i)), Ze = Ze∗ = k I k exp(se (j)), respectively. Since there are usually multiple plausible mentions for open-domain QA, during training, it is typical to maximize either the marginal log-likelihood (MML) of all correct spans (Karpukhin et al., 2020) or the log-likelihood of the most likely correct span (HardEM) (Min et al., 2019). During inference, the prediction is made based on the candidate P answer string score, obtaining as Pa (y) = (i,j)∈Y Ps (i, j), where Y is the set of spans corresponding to the answer string y. 2.1.2 Multi-objective for Weakly-supervised QA The multi-objective formulation is introduced in Cheng et al. (2020) for improving weakly supervised document-level QA. Different from Cheng et al. (2020) where only MML is considered for the multi-objective formulation, we found combining HardEM with MML is more effective for open-domain QA based on our experiments (§4.1). Specifically, we combine a mult"
2021.acl-long.240,2021.naacl-main.424,1,0.791885,"Missing"
2021.acl-long.240,2021.naacl-main.466,0,0.0262702,"ive model does not. This difference suggests that it is worth exploring better temporal modeling strategies to improve the extractive model in the future. 5 Related Work Open-domain QA Open-domain QA requires a system to answer questions based on evidence retrieved from a large corpus such as Wikipedia (Voorhees, 2000; Chen et al., 2017). Recent progress has been made towards improving evidence retrieval through both sparse vector models like TF-IDF or BM25 (Chen et al., 2017; Min et al., 2019), and dense vector models based on BERT (Lee et al., 2019; Karpukhin et al., 2020; Guu et al., 2020; Qu et al., 2021). Generally, the dense representations complement the sparse vector methods for passage retrieval as they can potentially give 3087 Generative Extractive tive open-domain QA, where the input passages are given by a retrieval model and are typically from different documents. Relative Accuracy 0.10 0.05 6 0.00 0.05 wh wh at i whch e whn ho o wh w ere wh wh at i whch e whn ho o wh w ere 0.10 Figure 2: Relative accuracy of different WH questions. The relative accuracy is the relative change of a WH category accuracy to the overall model accuracy. high similarity to semantically related text pairs,"
2021.acl-long.240,voorhees-tice-2000-trec,0,0.367013,"during training. In contrast, the answers to what questions can play a much richer syntactic role in context, making it more difficult for both extractive and generative models to perform well. Interestingly, the generative model exhibits the strength for temporal reasoning, whereas the extractive model does not. This difference suggests that it is worth exploring better temporal modeling strategies to improve the extractive model in the future. 5 Related Work Open-domain QA Open-domain QA requires a system to answer questions based on evidence retrieved from a large corpus such as Wikipedia (Voorhees, 2000; Chen et al., 2017). Recent progress has been made towards improving evidence retrieval through both sparse vector models like TF-IDF or BM25 (Chen et al., 2017; Min et al., 2019), and dense vector models based on BERT (Lee et al., 2019; Karpukhin et al., 2020; Guu et al., 2020; Qu et al., 2021). Generally, the dense representations complement the sparse vector methods for passage retrieval as they can potentially give 3087 Generative Extractive tive open-domain QA, where the input passages are given by a retrieval model and are typically from different documents. Relative Accuracy 0.10 0.05"
2021.acl-long.316,2020.emnlp-main.550,0,0.0418601,"Missing"
2021.acl-long.316,P17-1171,0,0.530685,"ves state-of-the-art performance on Natural Questions and TriviaQA datasets under the extractive QA setup when equipped with an extractive reader, and consistently outperforms other retrieval methods when the same generative reader is used.1 1 Introduction Open-domain question answering (OpenQA) aims to answer factoid questions without a pre-specified domain and has numerous real-world applications. In OpenQA, a large collection of documents (e.g., Wikipedia) are often used to seek information pertaining to the questions. One of the most common approaches uses a retriever-reader architecture (Chen et al., 2017), which first retrieves a small subset of documents using the question as the query and then reads the retrieved documents to extract (or generate) an answer. The retriever is crucial as it is infeasible to examine every piece of information in the entire document collection (e.g., millions of Wikipedia passages) and the retrieval accuracy bounds the performance of the (extractive) reader. ∗ Work was done during internship at Microsoft Azure AI. Our code is available at https://github.com/ morningmoni/GAR. 1 Early OpenQA systems (Chen et al., 2017) use classic retrieval methods such as TF-IDF"
2021.acl-long.316,Q19-1026,0,0.0800737,"2seq learning with the question as the input and various freely accessible in-domain contexts as the output such as the answer, the sentence where the answer belongs to, and the title of a passage that contains the answer. We then append the generated contexts to the question as the generationaugmented query for retrieval. We demonstrate that using multiple contexts from diverse generation targets is beneficial as fusing the retrieval results of different generation-augmented queries consistently yields better retrieval accuracy. We conduct extensive experiments on the Natural Questions (NQ) (Kwiatkowski et al., 2019) and TriviaQA (Trivia) (Joshi et al., 2017) datasets. The results reveal four major advantages of G AR: (1) G AR, combined with BM25, achieves significant gains over the same BM25 model that uses the original queries or existing unsupervised query expansion (QE) methods. (2) G AR with sparse representations (BM25) achieves comparable or even better performance than the current state-of-the-art retrieval methods, such as DPR (Karpukhin et al., 2020), that use dense representations. (3) Since G AR uses sparse representations to measure lexical overlap2 , it is complementary to dense representati"
2021.acl-long.316,N19-1423,0,0.184595,"cient (including the cost of the generation model), closing the gap between sparse and dense retrieval methods. reader design of the major baselines for a fair comparison, while virtually any existing QA reader can be used with G AR. 4.1 For the extractive setup, we largely follow the design of the extractive reader in DPR (Karpukhin et al., 2020). Let D = [d1 , d2 , ..., dk ] denote the list of retrieved passages with passage relevance scores D. Let Si = [s1 , s2 , ..., sN ] denote the top N text spans in passage di ranked by span relevance scores Si . Briefly, the DPR reader uses BERT-base (Devlin et al., 2019) for representation learning, where it estimates the passage relevance score Dk for each retrieved passage dk based on the [CLS] tokens of all retrieved passages D, and assigns span relevance scores Si for each candidate span based on the representations of its start and end tokens. Finally, the span with the highest span relevance score from the passage with the highest passage relevance score is chosen as the answer. We refer the readers to Karpukhin et al. (2020) for more details. Passage-level Span Voting. Many extractive QA methods (Chen et al., 2017; Min et al., 2019b; Guu et al., 2020;"
2021.acl-long.316,P17-1147,0,0.271232,"various freely accessible in-domain contexts as the output such as the answer, the sentence where the answer belongs to, and the title of a passage that contains the answer. We then append the generated contexts to the question as the generationaugmented query for retrieval. We demonstrate that using multiple contexts from diverse generation targets is beneficial as fusing the retrieval results of different generation-augmented queries consistently yields better retrieval accuracy. We conduct extensive experiments on the Natural Questions (NQ) (Kwiatkowski et al., 2019) and TriviaQA (Trivia) (Joshi et al., 2017) datasets. The results reveal four major advantages of G AR: (1) G AR, combined with BM25, achieves significant gains over the same BM25 model that uses the original queries or existing unsupervised query expansion (QE) methods. (2) G AR with sparse representations (BM25) achieves comparable or even better performance than the current state-of-the-art retrieval methods, such as DPR (Karpukhin et al., 2020), that use dense representations. (3) Since G AR uses sparse representations to measure lexical overlap2 , it is complementary to dense representations: by fusing the retrieval results of G A"
2021.acl-long.316,P19-1612,0,0.194727,"ieval for OpenQA, respectively. For query expansion, we re-emphasize that G AR is the first QE approach designed for OpenQA and most of the recent approaches are not applicable or efficient enough for OpenQA since they have task-specific objectives, require external supervision that was shown to transfer poorly to OpenQA, or take many days to train (Sec. 2). We thus compare with a classic unsupervised QE method RM3 (Abdul-Jaleel et al., 2004) that does not need external resources for a fair comparison. For passage reading, we compare with both extractive (Min et al., 2019a; Asai et al., 2019; Lee et al., 2019; Min et al., 2019b; Guu et al., 2020; Karpukhin et al., 2020) and generative (Brown et al., 2020; Roberts et al., 2020; Min et al., 2020; Lewis et al., 2020a; Izacard and Grave, 2020) methods when equipping G AR with the corresponding reader. 5.4 Implementation Details Retriever. We use Anserini (Yang et al., 2017) for text retrieval of BM25 and G AR with its default parameters. We conduct grid search for the QE baseline RM3 (Abdul-Jaleel et al., 2004). Generator. We use BART-large (Lewis et al., 2019) to generate query contexts in G AR. When there are multiple desired targets (such as multip"
2021.acl-long.316,2020.acl-main.703,0,0.0842252,"Missing"
2021.acl-long.316,W15-4640,0,0.0161974,"on both the ranking and score of the passages. As the generator and retriever are largely independent now, it is also interesting to study how to jointly or iteratively optimize generation and retrieval such that the generator is aware of the retriever and generates query contexts more beneficial for the retrieval stage. Last but not least, it is very likely that better results can be obtained by more extensive hyper-parameter tuning. Applicability to other tasks. Beyond OpenQA, G AR also has great potentials for other tasks that involve text matching such as conversation utterance selection (Lowe et al., 2015; Dinan et al., 2020) or information retrieval (Nguyen et al., 2016; Craswell et al., 2020). The default generation target is always available for supervised tasks. For example, for conversation utterance selection one can use the reference utterance as the default target and then match the concatenation of the conversation history and the generated utterance with the provided utterance candidates. For article search, the default target could be (part of) the ground-truth article itself. Other generation targets are more taskspecific and can be designed as long as they can be fetched from the"
2021.acl-long.316,D19-1284,0,0.509075,"uses BERT-base (Devlin et al., 2019) for representation learning, where it estimates the passage relevance score Dk for each retrieved passage dk based on the [CLS] tokens of all retrieved passages D, and assigns span relevance scores Si for each candidate span based on the representations of its start and end tokens. Finally, the span with the highest span relevance score from the passage with the highest passage relevance score is chosen as the answer. We refer the readers to Karpukhin et al. (2020) for more details. Passage-level Span Voting. Many extractive QA methods (Chen et al., 2017; Min et al., 2019b; Guu et al., 2020; Karpukhin et al., 2020) measure the probability of span extraction in different retrieved passages independently, despite that their collective signals may provide more evidence in determining the correct answer. We propose a simple yet effective passage-level span voting mechanism, which aggregates the predictions of the spans in the same surface form from different retrieved passages. Intuitively, if a text span is considered as the answer multiple times in different passages, it is more likely to be the correct answer. Specifically, G AR calculates a normalized score p("
2021.acl-long.316,2020.emnlp-main.466,0,0.715239,"ormation of the questions. G AR extends to contexts relevant to the questions by extracting information inside PLMs and helps sparse methods achieve comparable or better performance than dense methods (Guu et al., 2020; Karpukhin et al., 2020), while enjoying the simplicity and efficiency of sparse representations. G AR can also be used with dense representations to seek for even better performance, which we leave as future work. Generative QA. Generative QA generates answers through seq2seq learning instead of extracting answer spans. Recent studies on generative OpenQA (Lewis et al., 2020a; Min et al., 2020; Izacard and Grave, 2020) are orthogonal to G AR in that they focus on improving the reading stage and directly reuse DPR (Karpukhin et al., 2020) as the retriever. Unlike generative QA, the goal of G AR is not to generate perfect answers to the questions but pertinent contexts that are helpful for retrieval. Another line in generative QA learns to generate answers without relevant passages as the evidence but solely the question itself using PLMs (Roberts et al., 2020; Brown et al., 2020). G AR further confirms that one can extract factual knowledge from PLMs, which is not limited to the ans"
2021.acl-long.316,D17-1061,0,0.408758,"There have been some recent studies on query reformulation with text generation for other retrieval tasks, which, for example, rewrite the queries to context-independent (Yu et al., 2020; Lin et al., 2020; Vakulenko et al., 2020) or well-formed (Liu et al., 2019) ones. However, these methods require either task-specific data (e.g., conversational contexts, ill-formed queries) or external resources such as paraphrase data (Zaiem and Sadat, 2019; Wang et al., 2020) that cannot or do not transfer well to OpenQA. Also, some rely on timeconsuming training process like reinforcement learning (RL) (Nogueira and Cho, 2017; Liu et al., 2019; Wang et al., 2020) that is not efficient enough for OpenQA (more discussions in Sec. 2). In this paper, we propose GenerationAugmented Retrieval (G AR), which augments a query through text generation of a pre-trained language model (PLM). Different from prior studies that reformulate queries, G AR does not require external resources or downstream feedback via RL as supervision, because it does not rewrite the query but expands it with heuristically discov4089 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International J"
2021.acl-long.316,2020.emnlp-main.437,0,0.0256691,"Missing"
2021.acl-long.510,2020.findings-emnlp.178,0,0.0258745,"20). Searching Better Generalized Super Tickets. We select winning tickets according to the sensitivity of the model outputs with respect to the mask variables of each structure (Michel et al., 2019; Prasanna et al., 2020), as this measure is closely tied to the structure’s expressive power (Section 3). In addition, we conduct an one-shot pruning for computational simplicity. We leave other importance measures and pruning schedules, which may help identifying better generalized super tickets, for future works (Voita et al., 2019; Behnke and Heafield, 2020; Wang et al., 2019; Fan et al., 2019; Zhou et al., 2020; Sajjad et al., 2020). Searching Super Tickets Efficiently. Determining the compression ratio of the super tickets requires rewinding models at multiple sparsity levels. To leverage super tickets in practice, a potential direction of research is to find heuristics to determine this ratio prior or early-on in training. We leave this for future works. 10 Conclusion We study the behaviors of the structured lottery tickets in pre-trained BERT. We observe that the generalization performance of the winning tickets exhibits a phase transition phenomenon, suggesting pruning can improve generalization"
2021.emnlp-main.527,D17-1215,0,0.07046,"Missing"
2021.emnlp-main.527,2020.acl-main.197,1,0.762507,"Missing"
2021.emnlp-main.527,W07-1401,0,0.034758,"ors on the IWSLT’14 De-En dataset. 4.3 Natural Language Understanding Datasets. We demonstrate the effectiveness of SALT on the General Language Understanding Evaluation (GLUE) benchmark (Wang et al., 2019a), which is a collection of nine NLU tasks. The benchmark includes question answering (Rajpurkar et al., 2016), linguistic acceptability (CoLA, Warstadt et al. 2019), sentiment analysis (SST, Socher et al. 2013), text similarity (STS-B, Cer et al. 2017), paraphrase detection (MRPC, Dolan and Brockett 2005), and natural language inference (RTE & MNLI, Dagan et al. 2006; Bar-Haim et al. 2006; Giampiccolo et al. 2007; Bentivogli et al. 2009; Williams et al. 2018) tasks. Dataset details can be found in Table 7 (Appendix B.2). Implementation. We evaluate our algorithm by fine-tuning a pre-trained BERT-base (Devlin et al., 2019) model. Our implementation is based on the MT-DNN code-base (Liu et al., 2019a, 2020b)6 . Training details are presented in Appendix B.2. Results. Table 3 summarizes experiment results on the GLUE development set. We can see that SALT outperforms BERTBASE in all the tasks. Further, our method is particularly effective for small datasets, such as RTE, MRPC, and CoLA, where we achieve 9"
2021.emnlp-main.527,2020.emnlp-main.102,1,0.802988,"Missing"
2021.emnlp-main.527,P19-1441,1,0.930642,"g is to build models that generalize well on the unperturbed test data. Note that robustness and generalization are different concepts. Recent works (Raghunathan et al., 2020; Min et al., 2020) showed that adversarial training can hurt generalization performance, i.e., accuracy on clean data. As such, adversarial training needs to be treated with great caution. Therefore, in NLP, this technique requires refined tuning of, for example, the training algorithm and the perturbation strength.  Fine-tuning pre-trained language models (Peters et al., 2018; Devlin et al., 2019; Radford et al., 2019; Liu et al., 2019b; He et al., 2020) is state-ofthe-art for natural language understanding tasks such as the GLUE (Wang et al., 2019a) benchmark. Recently, there are works that use adversarial pre-training (Liu et al., 2020a) and adversarialregularized fine-tuning methods such as SMART (Jiang et al., 2020), FreeLB (Zhu et al., 2019), and FreeAT (Shafahi et al., 2019) to improve model generalization and robustness (Cheng et al., 2021). 3 Method Natural language inputs are discrete symbols (e.g., words), instead of continuous ones. Therefore, a common approach to generate perturbations is to learn continuous emb"
2021.emnlp-main.527,2020.acl-demos.16,1,0.821996,"Missing"
2021.emnlp-main.527,2021.ccl-1.108,0,0.0264572,"Missing"
2021.emnlp-main.527,2020.acl-main.441,0,0.0414905,"nd SALT on STS-B (upper) and SST-2 (lower) datasets. tion suffers from over-strong perturbations, such that the model cannot fit the unperturbed data well. This is supported by the fact that the training loss of SALT is smaller than that of SMART, which means SALT fits the data better. SALT also yields a smaller loss than SMART on the validation data, indicating that the Stackelberg game-formulated model exhibits better generalization performance.  Adversarial robustness. Even though the primary focus of SALT is model generalization, we still test its robustness on the Adversarial-NLI (ANLI, Nie et al. 2020) dataset. The dataset contains 163k data, which are collected via a humanand-model-in-the-loop approach. From Table 6, we can see that SALT improves model robustness upon conventional methods (i.e., SMART). Figure 4: Probing experiments. Each violin plot is based on 10 runs with different random seeds. datasets while keeping the representations fixed. Such a method directly measures the quality of representations generated by different models. As illustrated in Fig. 4, SALT outperforms the baseline methods by large margins.  Classification Model Calibration. Adversarial regularization also he"
2021.emnlp-main.527,N19-4009,0,0.012301,"esented in Appendix B.1. BLEU Transformer (Vaswani et al., 2017) FreeAT (Shafahi et al., 2019) FreeLB (Zhu et al., 2019) SMART (Jiang et al., 2020) 28.4 29.0 29.0 29.1 SALT 29.6 Table 5: sacreBLEU score on WMT’16 En-De. All the baseline results are from our re-implementation. Implementation. Recall that to generate adversarial examples, we perturb the word embeddings. In NMT experiments, we perturb both the source-side and the target-side embeddings. This strategy is empirically demonstrated (Sato et al., 2019) to be more effective than perturbing only one side of the inputs. We use Fairseq5 (Ott et al., 2019) to implement our algorithms. We adopt the Transformerbase (Vaswani et al., 2017) architecture in all the low-resource experiments, except IWSLT’14 DeEn. In this dataset, we use a model smaller than Transformer-base by decreasing the hidden dimension size from 2048 to 1024, and decreasing the number of heads from 8 to 4 (while dimension of each head doubles). For the rich-resource experi5 https://github.com/pytorch/fairseq Results. Experimental results for the low-resource experiments are summarized in Table 2. Notice that SMART, which utilizes conventional adversarial regularization, consiste"
2021.emnlp-main.527,W18-6301,0,0.0249322,"Missing"
2021.emnlp-main.527,N18-1202,0,0.0119944,"t from the above fields, in NLP, the goal of adversarial training is to build models that generalize well on the unperturbed test data. Note that robustness and generalization are different concepts. Recent works (Raghunathan et al., 2020; Min et al., 2020) showed that adversarial training can hurt generalization performance, i.e., accuracy on clean data. As such, adversarial training needs to be treated with great caution. Therefore, in NLP, this technique requires refined tuning of, for example, the training algorithm and the perturbation strength.  Fine-tuning pre-trained language models (Peters et al., 2018; Devlin et al., 2019; Radford et al., 2019; Liu et al., 2019b; He et al., 2020) is state-ofthe-art for natural language understanding tasks such as the GLUE (Wang et al., 2019a) benchmark. Recently, there are works that use adversarial pre-training (Liu et al., 2020a) and adversarialregularized fine-tuning methods such as SMART (Jiang et al., 2020), FreeLB (Zhu et al., 2019), and FreeAT (Shafahi et al., 2019) to improve model generalization and robustness (Cheng et al., 2021). 3 Method Natural language inputs are discrete symbols (e.g., words), instead of continuous ones. Therefore, a common"
2021.emnlp-main.527,W18-6319,0,0.0123062,"raining (Adv). Similar observations were also reported in Miyato et al. (2017); Sato et al. (2019). This is because Adv generates perturbations using the correct examples, thus, the label information are “leaked” (Kurakin et al., 2017). Additionally, we can see that SALT is particularly effective in this low-resource setting, where it outperforms all the baselines by large margins. In comparison with the vanilla Transformer model, SALT achieves up to 2 BLEU score improvements on all the three datasets. Table 5 summarizes experiment results on the WMT’16 En-De dataset. We report the sacreBLEU (Post, 2018) score, which is a detokenzied version of the BLEU score that better reflects translation quality. We can see that SALT outperforms all the baseline methods by notable margins, and it improves upon the vanilla Transformer model by 1.2 BLEU score. 6567 (a) Number of unrolling steps. (b) Perturbation strength , `2 case. (c) Perturbation strength , `∞ case. Figure 1: Relation between BLEU score and different factors on the IWSLT’14 De-En dataset. 4.3 Natural Language Understanding Datasets. We demonstrate the effectiveness of SALT on the General Language Understanding Evaluation (GLUE) benchmar"
2021.emnlp-main.527,D16-1264,0,0.0085038,"e that SALT outperforms all the baseline methods by notable margins, and it improves upon the vanilla Transformer model by 1.2 BLEU score. 6567 (a) Number of unrolling steps. (b) Perturbation strength , `2 case. (c) Perturbation strength , `∞ case. Figure 1: Relation between BLEU score and different factors on the IWSLT’14 De-En dataset. 4.3 Natural Language Understanding Datasets. We demonstrate the effectiveness of SALT on the General Language Understanding Evaluation (GLUE) benchmark (Wang et al., 2019a), which is a collection of nine NLU tasks. The benchmark includes question answering (Rajpurkar et al., 2016), linguistic acceptability (CoLA, Warstadt et al. 2019), sentiment analysis (SST, Socher et al. 2013), text similarity (STS-B, Cer et al. 2017), paraphrase detection (MRPC, Dolan and Brockett 2005), and natural language inference (RTE & MNLI, Dagan et al. 2006; Bar-Haim et al. 2006; Giampiccolo et al. 2007; Bentivogli et al. 2009; Williams et al. 2018) tasks. Dataset details can be found in Table 7 (Appendix B.2). Implementation. We evaluate our algorithm by fine-tuning a pre-trained BERT-base (Devlin et al., 2019) model. Our implementation is based on the MT-DNN code-base (Liu et al., 2019a,"
2021.emnlp-main.527,P19-1020,0,0.168192,"ularization (SALT), where we for1 Introduction mulate adversarial regularization as a Stackelberg Adversarial regularization (Miyato et al., 2017) has game (Von Stackelberg, 2010). The concept arises been shown to improve the generalization perfor- from economics, where two firms are competing mance of deep learning models in various natural in a market, and one of the them is in the leading language processing (NLP) tasks, such as language position by acknowledging the opponent’s strategy. modeling (Wang et al., 2019b), machine translation In Stackelberg adversarial regularization, a leader (Sato et al., 2019), natural language understand- solves for the model parameters, and a follower ing (Jiang et al., 2020), and reading comprehen- generates input perturbations. The leader procures ∗ Corresponding author. its advantage by considering what the best response 6562 Abstract Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 6562–6577 c November 7–11, 2021. 2021 Association for Computational Linguistics of the follower is, i.e., how will the follower respond after observing the leader’s decision. Then, the leader minimizes its loss, anticipating the predicte"
2021.emnlp-main.527,P16-1162,0,0.0314093,"Missing"
2021.emnlp-main.527,D13-1170,0,0.00291882,"nsformer model by 1.2 BLEU score. 6567 (a) Number of unrolling steps. (b) Perturbation strength , `2 case. (c) Perturbation strength , `∞ case. Figure 1: Relation between BLEU score and different factors on the IWSLT’14 De-En dataset. 4.3 Natural Language Understanding Datasets. We demonstrate the effectiveness of SALT on the General Language Understanding Evaluation (GLUE) benchmark (Wang et al., 2019a), which is a collection of nine NLU tasks. The benchmark includes question answering (Rajpurkar et al., 2016), linguistic acceptability (CoLA, Warstadt et al. 2019), sentiment analysis (SST, Socher et al. 2013), text similarity (STS-B, Cer et al. 2017), paraphrase detection (MRPC, Dolan and Brockett 2005), and natural language inference (RTE & MNLI, Dagan et al. 2006; Bar-Haim et al. 2006; Giampiccolo et al. 2007; Bentivogli et al. 2009; Williams et al. 2018) tasks. Dataset details can be found in Table 7 (Appendix B.2). Implementation. We evaluate our algorithm by fine-tuning a pre-trained BERT-base (Devlin et al., 2019) model. Our implementation is based on the MT-DNN code-base (Liu et al., 2019a, 2020b)6 . Training details are presented in Appendix B.2. Results. Table 3 summarizes experiment resu"
2021.emnlp-main.527,Q19-1040,0,0.0142734,"le margins, and it improves upon the vanilla Transformer model by 1.2 BLEU score. 6567 (a) Number of unrolling steps. (b) Perturbation strength , `2 case. (c) Perturbation strength , `∞ case. Figure 1: Relation between BLEU score and different factors on the IWSLT’14 De-En dataset. 4.3 Natural Language Understanding Datasets. We demonstrate the effectiveness of SALT on the General Language Understanding Evaluation (GLUE) benchmark (Wang et al., 2019a), which is a collection of nine NLU tasks. The benchmark includes question answering (Rajpurkar et al., 2016), linguistic acceptability (CoLA, Warstadt et al. 2019), sentiment analysis (SST, Socher et al. 2013), text similarity (STS-B, Cer et al. 2017), paraphrase detection (MRPC, Dolan and Brockett 2005), and natural language inference (RTE & MNLI, Dagan et al. 2006; Bar-Haim et al. 2006; Giampiccolo et al. 2007; Bentivogli et al. 2009; Williams et al. 2018) tasks. Dataset details can be found in Table 7 (Appendix B.2). Implementation. We evaluate our algorithm by fine-tuning a pre-trained BERT-base (Devlin et al., 2019) model. Our implementation is based on the MT-DNN code-base (Liu et al., 2019a, 2020b)6 . Training details are presented in Appendix B."
2021.emnlp-main.527,N18-1101,0,0.0149337,"nguage Understanding Datasets. We demonstrate the effectiveness of SALT on the General Language Understanding Evaluation (GLUE) benchmark (Wang et al., 2019a), which is a collection of nine NLU tasks. The benchmark includes question answering (Rajpurkar et al., 2016), linguistic acceptability (CoLA, Warstadt et al. 2019), sentiment analysis (SST, Socher et al. 2013), text similarity (STS-B, Cer et al. 2017), paraphrase detection (MRPC, Dolan and Brockett 2005), and natural language inference (RTE & MNLI, Dagan et al. 2006; Bar-Haim et al. 2006; Giampiccolo et al. 2007; Bentivogli et al. 2009; Williams et al. 2018) tasks. Dataset details can be found in Table 7 (Appendix B.2). Implementation. We evaluate our algorithm by fine-tuning a pre-trained BERT-base (Devlin et al., 2019) model. Our implementation is based on the MT-DNN code-base (Liu et al., 2019a, 2020b)6 . Training details are presented in Appendix B.2. Results. Table 3 summarizes experiment results on the GLUE development set. We can see that SALT outperforms BERTBASE in all the tasks. Further, our method is particularly effective for small datasets, such as RTE, MRPC, and CoLA, where we achieve 9.4, 4.3, and 6.3 absolute improvements, respect"
2021.emnlp-main.812,2020.emnlp-demos.22,0,0.0250449,"3 4.5 Table 2: Dataset statistics. The WN18RR dataset is significantly sparser than the FB15K-237 dataset. by the computed scores, the rank of the ground truth target entity is used to further compute various ranking metrics such as mean reciprocal rank (MRR) and hits@k, k ∈ {1, 3, 10}. We report all of these ranking metrics under the filtered setting proposed in Bordes et al. (2013) where valid entities except the ground truth target entity are filtered out from the rank list. 3.3 Experimental Setup We implement our proposed method in PyTorch (Paszke et al., 2019) under the LibKGE framework (Broscheit et al., 2020). To perform a fair comparison with some early baseline methods, we reproduce their results using hyper-parameter configurations from LibKGE.7 All data and evaluation metrics can be found in LibKGE. Our model consists of a three-layer entity Transformer and a six-layers context Transformer. Each 7 These configurations consider many recent training techniques and are found by extensive searches. Thus the results are generally much better then the original reported ones. 10399 Transformer layer has eight heads. The dimension size of hidden states is 320 across all layers except that we use 1280"
2021.emnlp-main.812,2020.acl-main.617,0,0.136704,"x in the input embeddings its graph neighborhood is denser (in which case, layer and the linear transformation of this classification layer. 10398 FB15K-237 Model WN18RR Hits↑ #Params MRR↑ @1 @3 @10 .266 .218 .249 .250 .247 .241 .272 .266 .264 .246 .390 .345 .378 .377 .372 .375 .400 .394 .390 .380 Hits↑ #Params MRR↑ @1 @3 @10 RESCAL (Nickel et al., 2011) TransE (Bordes et al., 2013) DistMult (Yang et al., 2015) ComplEx (Trouillon et al., 2016) ConvE (Dettmers et al., 2018) RotatE (Sun et al., 2018) CoKE (Wang et al., 2019) TuckER (Balazevic et al., 2019) CompGCN (Vashishth et al., 2020) RotH (Chami et al., 2020) 6M 2M 4M 4M 9M 15M 10M 8M .356 .310 .342 .343 .338 .338 .364 .358 .355 .344 .535 .495 .531 .532 .521 .533 .549 .544 .535 .535 6M 21M 21M 5M 36M 20M 17M 21M .467 .232 .451 .479 .439 .476 .484 .470 .479 .496 .439 .061 .414 .441 .409 .428 .450 .443 .443 .449 .478 .366 .466 .495 .452 .492 .496 .482 .494 .514 .516 .522 .523 .552 .499 .571 .553 .526 .546 .586 HittER 16M .373 .279 .409 .558 24M .503 .462 .516 .584 Table 1: Comparison between the proposed method and baseline methods. Results of RotatE, CoKE, TuckER, CompGCN, and RotH are taken from their original papers. Numbers in bold represent the"
2021.emnlp-main.812,2021.acl-long.336,0,0.031638,"in the knowledge graph used to train HittER. We include more details of the experiments in the Appendix. embeddings (Bordes et al., 2013; Wang et al., 2014; Lin et al., 2015; Ji et al., 2015). Another line of work uses tensor factorization methods to match entities semantically. Starting from simple bi-linear transformations in the euclidean space (Nickel et al., 2011; Yang et al., 2015), numerous complicated transformations in various spaces have been hence proposed (Trouillon et al., 2016; Ebisu and Ichise, 2018; Sun et al., 2018; Zhang et al., 2019a; Chami et al., 2020; Tang et al., 2020; Chao et al., 2021). Such methods effectively capture the intuition from observation of data but suffer from unobserved geometric properties and are generally limited in expressiveness. In light of recent advances in deep learning, many more powerful neural network modules such as Convolutional Neural Networks (Dettmers et al., 2018), Capsule Networks (Nguyen et al., 2019), and Transformers (Wang et al., 2019) are also introduced to capture the interaction between entity and relation embeddings. These methods produce rich representations and better performance on predicting missing links in knowledge graphs. How"
2021.emnlp-main.812,N19-1423,0,0.495292,"of the knowledge graphs are still highly incomplete (West et al., 2014). Our approach achieves new state-of-the-art results on two standard benchmark datasets: FB15K-237 (Toutanova and Chen, 2015) and WN18RR (Dettmers et al., 2018). Unlike the previous shallow KGE methods that cannot be trivially utilized by widely used Transformer-based models for language tasks (Peters et al., 2019), our approach benefits from the unified Transformer architecture and its extensibility. As a case study, we show how to integrate the learned representations of HittER into pre-trained language models like BERT (Devlin et al., 2019). Our experiments demonstrate that HittER significantly improves BERT on two Freebase factoid question answering (QA) datasets: FreebaseQA (Jiang et al., 2019) and Webques1 GNN methods’ depth is tied to their receptive fields and thus constrained by over-smoothing issues (Liu et al., 2020). tionSP (Yih et al., 2016). Our experimental code as well as multiple pretrained models are publicly available.2 2 HittER We introduce our proposed hierarchical Transformer model (Figure 2) in this section. In Section 2.1, we provide the background about how link prediction can be done with a simple Transfor"
2021.emnlp-main.812,P19-1598,0,0.0283755,"ediction task to balance information from the relational context and the source entity itself. Experimental results show that HittER achieves new stateof-the-art results on multiple link prediction datasets. We additionally propose a simple approach to integrate HittER into BERT and demonstrate its effectiveness on two Freebase factoid question answering datasets. 1 Introduction Knowledge graphs (KG) are a major form of knowledge bases where knowledge is stored as graphstructured data. Because of their broad applications in various intelligent systems including natural language understanding (Logan et al., 2019; Zhang et al., 2019b; Hayashi et al., 2020) and reasoning (Riedel et al., 2013; Xiong et al., 2017; Bauer et al., 2018; Verga et al., 2021), learning representations of knowledge graphs has been studied in a large body of literature. To learn high quality representations of knowledge graphs, many researchers adopt the idea of mapping the entities and relations in a knowledge graph to points in a vector space. These knowledge graph embedding (KGE) methods usually leverage geometric properties in the vector space, such country Sunnyvale adjoins Cupertino state California Figure 1: An example su"
2021.emnlp-main.812,P15-1067,0,0.0284913,"dataset, in which case the input entities for HittER cannot be provided. Thus we also report results under a filtered setting, i.e., a subset retaining only examples whose context entity and answer entity both exist on the FB15K-237 dataset. Our experimental results in Table 6 show that HittER’s representation significantly enhances BERT’s question answering ability, especially when the questions are related to entities in the knowledge graph used to train HittER. We include more details of the experiments in the Appendix. embeddings (Bordes et al., 2013; Wang et al., 2014; Lin et al., 2015; Ji et al., 2015). Another line of work uses tensor factorization methods to match entities semantically. Starting from simple bi-linear transformations in the euclidean space (Nickel et al., 2011; Yang et al., 2015), numerous complicated transformations in various spaces have been hence proposed (Trouillon et al., 2016; Ebisu and Ichise, 2018; Sun et al., 2018; Zhang et al., 2019a; Chami et al., 2020; Tang et al., 2020; Chao et al., 2021). Such methods effectively capture the intuition from observation of data but suffer from unobserved geometric properties and are generally limited in expressiveness. In ligh"
2021.emnlp-main.812,N19-1028,0,0.104032,"FB15K-237 (Toutanova and Chen, 2015) and WN18RR (Dettmers et al., 2018). Unlike the previous shallow KGE methods that cannot be trivially utilized by widely used Transformer-based models for language tasks (Peters et al., 2019), our approach benefits from the unified Transformer architecture and its extensibility. As a case study, we show how to integrate the learned representations of HittER into pre-trained language models like BERT (Devlin et al., 2019). Our experiments demonstrate that HittER significantly improves BERT on two Freebase factoid question answering (QA) datasets: FreebaseQA (Jiang et al., 2019) and Webques1 GNN methods’ depth is tied to their receptive fields and thus constrained by over-smoothing issues (Liu et al., 2020). tionSP (Yih et al., 2016). Our experimental code as well as multiple pretrained models are publicly available.2 2 HittER We introduce our proposed hierarchical Transformer model (Figure 2) in this section. In Section 2.1, we provide the background about how link prediction can be done with a simple Transformer scoring function. We then describe the detailed architecture of our proposed model in Section 2.2. Finally, we discuss our strategies to learn balanced con"
2021.emnlp-main.812,2020.tacl-1.28,0,0.0164964,"cal Methods in Natural Language Processing, pages 10395–10407 c November 7–11, 2021. 2021 Association for Computational Linguistics Vashishth et al., 2020). However, these methods are usually restricted in expressiveness because of the shallow network architecture they use.1 In this paper, we present HittER, a deep hierarchical Transformer model to learn representations of entities and relations in a knowledge graph jointly by aggregating information from graph neighborhoods. Although prior work shows Transformers can learn relational knowledge from large amounts of unstructured textual data (Jiang et al., 2020; Manning et al., 2020), HittER explicitly operates over structured inputs using a hierarchical architecture. Essentially, HittER consists of two levels of Transformer blocks. As shown in Figure 2, the bottom block provides relation-dependent entity embeddings for the neighborhood around an entity and the top block aggregates information from its graph context. To ensure HittER work across graphs of different properties, we further design a masked entity prediction task to balance the contextual relational information and information from the training entity itself. We evaluate the proposed me"
2021.emnlp-main.812,P19-1466,0,0.019398,"e to capture different relations between entities. (2019) borrow the idea from Graph Attention Early work uses translational distance-based scor- Networks (Veliˇckovi´c et al., 2018), using a biing functions defined on top of entity and relation linear attention mechanism to selectively gather useful information from neighbor entities. Differsimplify the modeling architecture, we also make the number of tokens known to all models. ent from their simple single-layer attention formu10402 lation, we use the advanced Transformer to capture both the entity-relation and entity-context interactions. Nathani et al. (2019) also propose an attention-based feature embedding to capture multihop neighbor information, but unfortunately, their reported results have been proven to be unreliable in a recent re-evaluation (Sun et al., 2020). 6 Conclusion and Future Work In this work, we proposed HittER, a novel Transformer-based model with effective training strategies for learning knowledge graph embeddings in complex multi-relational graphs. We show that with contextual information from a local neighborhood, our proposed method outperforms all previous approaches in long-standing link prediction tasks, achieving new S"
2021.emnlp-main.812,N19-1226,0,0.0189244,"ckel et al., 2011; Yang et al., 2015), numerous complicated transformations in various spaces have been hence proposed (Trouillon et al., 2016; Ebisu and Ichise, 2018; Sun et al., 2018; Zhang et al., 2019a; Chami et al., 2020; Tang et al., 2020; Chao et al., 2021). Such methods effectively capture the intuition from observation of data but suffer from unobserved geometric properties and are generally limited in expressiveness. In light of recent advances in deep learning, many more powerful neural network modules such as Convolutional Neural Networks (Dettmers et al., 2018), Capsule Networks (Nguyen et al., 2019), and Transformers (Wang et al., 2019) are also introduced to capture the interaction between entity and relation embeddings. These methods produce rich representations and better performance on predicting missing links in knowledge graphs. However, they are restricted by the amount of information that can be encoded in a single node embedding and the great effort to memorize local connectivity patterns. 5.2 Context-aware Methods Various forms of graph contexts have been proven effective in recent work on neural networks operating in graphs under the message passing framework (Bruna et al., 20"
2021.emnlp-main.812,D19-1005,0,0.0350681,"Missing"
2021.emnlp-main.812,N13-1008,0,0.0415593,"e entity itself. Experimental results show that HittER achieves new stateof-the-art results on multiple link prediction datasets. We additionally propose a simple approach to integrate HittER into BERT and demonstrate its effectiveness on two Freebase factoid question answering datasets. 1 Introduction Knowledge graphs (KG) are a major form of knowledge bases where knowledge is stored as graphstructured data. Because of their broad applications in various intelligent systems including natural language understanding (Logan et al., 2019; Zhang et al., 2019b; Hayashi et al., 2020) and reasoning (Riedel et al., 2013; Xiong et al., 2017; Bauer et al., 2018; Verga et al., 2021), learning representations of knowledge graphs has been studied in a large body of literature. To learn high quality representations of knowledge graphs, many researchers adopt the idea of mapping the entities and relations in a knowledge graph to points in a vector space. These knowledge graph embedding (KGE) methods usually leverage geometric properties in the vector space, such country Sunnyvale adjoins Cupertino state California Figure 1: An example subgraph sampled from FB15K237. Four nodes (entities) are connected by three diff"
2021.emnlp-main.812,2020.acl-main.412,0,0.0373134,"erties in the vector space, such country Sunnyvale adjoins Cupertino state California Figure 1: An example subgraph sampled from FB15K237. Four nodes (entities) are connected by three different types of relations representing facts like Sunnyvale belongs to the state of California. as translation (Bordes et al., 2013), bilinear transformations (Yang et al., 2015, DistMult), or rotation (Sun et al., 2018). Multi-layer convolutional networks are also used for KGE (Dettmers et al., 2018, ConvE). Such KGE methods are conceptually simple and can be applied to tasks like factoid question answering (Saxena et al., 2020) and language modeling (Peters et al., 2019). However, it is rather challenging to encode all of the information about an entity into a single vector. For example, to infer the missing object in the incomplete triplet &lt;Sunnyvale, county, ?&gt; (Figure 1), traditional KGE methods rely on the geographic information stored in the embedding of Sunnyvale. While we can read such information from its graph context, e.g., from a neighbor node that represents the state it belongs to (i.e., California). In this way, we allow the model to store and utilize information about an entity via its relational cont"
2021.emnlp-main.812,2020.acl-main.241,0,0.0295181,"related to entities in the knowledge graph used to train HittER. We include more details of the experiments in the Appendix. embeddings (Bordes et al., 2013; Wang et al., 2014; Lin et al., 2015; Ji et al., 2015). Another line of work uses tensor factorization methods to match entities semantically. Starting from simple bi-linear transformations in the euclidean space (Nickel et al., 2011; Yang et al., 2015), numerous complicated transformations in various spaces have been hence proposed (Trouillon et al., 2016; Ebisu and Ichise, 2018; Sun et al., 2018; Zhang et al., 2019a; Chami et al., 2020; Tang et al., 2020; Chao et al., 2021). Such methods effectively capture the intuition from observation of data but suffer from unobserved geometric properties and are generally limited in expressiveness. In light of recent advances in deep learning, many more powerful neural network modules such as Convolutional Neural Networks (Dettmers et al., 2018), Capsule Networks (Nguyen et al., 2019), and Transformers (Wang et al., 2019) are also introduced to capture the interaction between entity and relation embeddings. These methods produce rich representations and better performance on predicting missing links in k"
2021.emnlp-main.812,W15-4007,0,0.205256,"n task, which is one of the canonical tasks in statistical relational learning (SRL). Link prediction (essentially KG completion) serves as a good proxy to evaluate the effectiveness of learned graph representations, by measuring the ability of a model to generalize relational knowledge stored in training graphs to unseen facts. Meanwhile, it has an important application to knowledge graph completion given the fact that most of the knowledge graphs are still highly incomplete (West et al., 2014). Our approach achieves new state-of-the-art results on two standard benchmark datasets: FB15K-237 (Toutanova and Chen, 2015) and WN18RR (Dettmers et al., 2018). Unlike the previous shallow KGE methods that cannot be trivially utilized by widely used Transformer-based models for language tasks (Peters et al., 2019), our approach benefits from the unified Transformer architecture and its extensibility. As a case study, we show how to integrate the learned representations of HittER into pre-trained language models like BERT (Devlin et al., 2019). Our experiments demonstrate that HittER significantly improves BERT on two Freebase factoid question answering (QA) datasets: FreebaseQA (Jiang et al., 2019) and Webques1 GNN"
2021.emnlp-main.812,2021.naacl-main.288,0,0.0291972,"es new stateof-the-art results on multiple link prediction datasets. We additionally propose a simple approach to integrate HittER into BERT and demonstrate its effectiveness on two Freebase factoid question answering datasets. 1 Introduction Knowledge graphs (KG) are a major form of knowledge bases where knowledge is stored as graphstructured data. Because of their broad applications in various intelligent systems including natural language understanding (Logan et al., 2019; Zhang et al., 2019b; Hayashi et al., 2020) and reasoning (Riedel et al., 2013; Xiong et al., 2017; Bauer et al., 2018; Verga et al., 2021), learning representations of knowledge graphs has been studied in a large body of literature. To learn high quality representations of knowledge graphs, many researchers adopt the idea of mapping the entities and relations in a knowledge graph to points in a vector space. These knowledge graph embedding (KGE) methods usually leverage geometric properties in the vector space, such country Sunnyvale adjoins Cupertino state California Figure 1: An example subgraph sampled from FB15K237. Four nodes (entities) are connected by three different types of relations representing facts like Sunnyvale be"
2021.emnlp-main.812,D17-1060,0,0.0248142,"rimental results show that HittER achieves new stateof-the-art results on multiple link prediction datasets. We additionally propose a simple approach to integrate HittER into BERT and demonstrate its effectiveness on two Freebase factoid question answering datasets. 1 Introduction Knowledge graphs (KG) are a major form of knowledge bases where knowledge is stored as graphstructured data. Because of their broad applications in various intelligent systems including natural language understanding (Logan et al., 2019; Zhang et al., 2019b; Hayashi et al., 2020) and reasoning (Riedel et al., 2013; Xiong et al., 2017; Bauer et al., 2018; Verga et al., 2021), learning representations of knowledge graphs has been studied in a large body of literature. To learn high quality representations of knowledge graphs, many researchers adopt the idea of mapping the entities and relations in a knowledge graph to points in a vector space. These knowledge graph embedding (KGE) methods usually leverage geometric properties in the vector space, such country Sunnyvale adjoins Cupertino state California Figure 1: An example subgraph sampled from FB15K237. Four nodes (entities) are connected by three different types of relat"
2021.emnlp-main.812,P16-2033,0,0.0694591,"d Transformer-based models for language tasks (Peters et al., 2019), our approach benefits from the unified Transformer architecture and its extensibility. As a case study, we show how to integrate the learned representations of HittER into pre-trained language models like BERT (Devlin et al., 2019). Our experiments demonstrate that HittER significantly improves BERT on two Freebase factoid question answering (QA) datasets: FreebaseQA (Jiang et al., 2019) and Webques1 GNN methods’ depth is tied to their receptive fields and thus constrained by over-smoothing issues (Liu et al., 2020). tionSP (Yih et al., 2016). Our experimental code as well as multiple pretrained models are publicly available.2 2 HittER We introduce our proposed hierarchical Transformer model (Figure 2) in this section. In Section 2.1, we provide the background about how link prediction can be done with a simple Transformer scoring function. We then describe the detailed architecture of our proposed model in Section 2.2. Finally, we discuss our strategies to learn balanced contextual representations of an entity in Section 2.3. 2.1 Transformers for Link Prediction A knowledge graph can be viewed as a set of triplets (G = {(es , rp"
2021.emnlp-main.812,2020.emnlp-main.595,0,0.0696895,"Missing"
2021.findings-acl.29,N19-1423,0,0.0291568,"answers, we first take the predictions of the generative reader (G) in Sec. 2.3, which is trained on the passages without reranking and used for final passage reading in R3. It represents an apple-to-apple comparison to R2 without any additional information but higher-quality input. We also experiment with an extractive reader (E) that has access to all retrieved passages, where the goal is to study whether we can rerank passages via other signals and further improve G such that it outperforms both G and E when they are in R2. We use the extractive reader in Mao et al. (2020) with BERT-base (Devlin et al., 2019) representation and span voting. For the generative reader, we either take its top-1 prediction with greedy decoding or sample 10 answers with decoding parameters as follows. We set sampling temperature to 5/2 and the top probability in nucleus sampling to 0.5/0.5 on NQ/Trivia, Quality of Reranking Signals We first analyze the EM of the top-N reader predictions A[:N ] . We consider a question correctly answered as long as one of the top-N predictions matches the ground-truth answer. The standard EM is a special case with N = 1. As listed in Table 3, the reader EM can be improved by up to 24.0"
2021.findings-acl.29,P17-1147,0,0.338917,"the top predicted answers of the reader before reranking. Intuitively, the top predictions of the reader are closely related to the ground-truth answer and even if the predicted answers are partially correct or incorrect, they may still provide useful signals suggesting which passages may contain the correct answer (Mao et al., 2020). We conduct experiments on the Natural Questions (NQ) (Kwiatkowski et al., 2019) and Triv344 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 344–350 August 1–6, 2021. ©2021 Association for Computational Linguistics iaQA (Trivia) (Joshi et al., 2017) datasets. We demonstrate that R3 with R IDER, without any additional training, achieves 10 to 20 absolute gains in top-1 retrieval accuracy, and 1 to 4 gains in Exact Match (EM) compared to the R2 architecture. R IDER also outperforms two state-of-theart transformer-based supervised reranking models that require expensive training and inference. Notably, using only 1,024 tokens (7.8 passages on average) as the input of a generative reader, R IDER achieves EM=47.5/63.5 on NQ/Trivia when the predictions of the same generative reader (EM=45.3/62.2 in R2) are used for reranking, and EM=48.3/66.4"
2021.findings-acl.29,2020.emnlp-main.550,0,0.0956417,"Missing"
2021.findings-acl.29,Q19-1026,0,0.20036,"ropose a simple and effective passage reranking method, named Reader-guIDEd Reranker (R IDER), which does not require any training and reranks the retrieved passages solely based on their lexical overlap with the top predicted answers of the reader before reranking. Intuitively, the top predictions of the reader are closely related to the ground-truth answer and even if the predicted answers are partially correct or incorrect, they may still provide useful signals suggesting which passages may contain the correct answer (Mao et al., 2020). We conduct experiments on the Natural Questions (NQ) (Kwiatkowski et al., 2019) and Triv344 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 344–350 August 1–6, 2021. ©2021 Association for Computational Linguistics iaQA (Trivia) (Joshi et al., 2017) datasets. We demonstrate that R3 with R IDER, without any additional training, achieves 10 to 20 absolute gains in top-1 retrieval accuracy, and 1 to 4 gains in Exact Match (EM) compared to the R2 architecture. R IDER also outperforms two state-of-theart transformer-based supervised reranking models that require expensive training and inference. Notably, using only 1,024 tokens (7.8 passages o"
2021.findings-acl.29,D18-1053,0,0.210592,"rank the most relevant passages at the very top. One line of work (Mao et al., 2020; Karpukhin et al., 2020) aims to improve the retriever and shows that significantly better QA performance can be achieved when the retrieval results are improved. ∗ Work was done during internship at Microsoft Azure AI. Our code is available at https://github.com/ morningmoni/GAR. 1 An alternative solution is to rerank the initial retrieval results via a reranker, which is widely used in information retrieval (Nogueira and Cho, 2019; Qiao et al., 2019) and explored in early OpenQA systems (Wang et al., 2018a; Lee et al., 2018). However, current state-of-the-art OpenQA systems (Karpukhin et al., 2020; Izacard and Grave, 2020b) do not distinguish the order of the retrieved passages and instead equally consider a large number of retrieved passages (e.g., 100), which could be computationally prohibitive as the model size of the readers becomes larger (Izacard and Grave, 2020b). We argue that a Retriever-Reranker-Reader (R3) architecture is beneficial in terms of both model effectiveness and efficiency: passage reranking improves the retrieval accuracy of the retriever at top positions and allows the reader to achieve c"
2021.findings-acl.29,P19-1612,0,0.105441,"Missing"
2021.findings-acl.29,2020.acl-main.703,0,0.0970835,"Missing"
2021.findings-acl.29,2020.acl-tutorials.8,0,0.55013,"atural Questions dataset and 66.4 EM on the TriviaQA dataset when only 1,024 tokens (7.8 passages on average) are used as the reader input after passage reranking.1 1 Introduction Current open-domain question answering (OpenQA) systems often follow a RetrieverReader (R2) architecture, where the retriever first retrieves relevant passages and the reader then reads the retrieved passages to form an answer. Since the retriever retrieves passages from a large candidate pool (e.g., millions of Wikipedia passages), it often fails to rank the most relevant passages at the very top. One line of work (Mao et al., 2020; Karpukhin et al., 2020) aims to improve the retriever and shows that significantly better QA performance can be achieved when the retrieval results are improved. ∗ Work was done during internship at Microsoft Azure AI. Our code is available at https://github.com/ morningmoni/GAR. 1 An alternative solution is to rerank the initial retrieval results via a reranker, which is widely used in information retrieval (Nogueira and Cho, 2019; Qiao et al., 2019) and explored in early OpenQA systems (Wang et al., 2018a; Lee et al., 2018). However, current state-of-the-art OpenQA systems (Karpukhin et al"
2021.findings-acl.29,D19-1284,0,0.224656,"Missing"
2021.findings-acl.29,2020.emnlp-main.466,0,0.746661,"ng itself despite the noise in reader predictions. 2.3 Passage Reading We consider a scenario where the number of passages that can be used for QA is limited (sometimes deliberately) due to reasons such as insufficient computational resources, the limit of model input length, or requirement for faster responses. We use a generative reader initialized by BARTlarge (Lewis et al., 2019), which concatenates the question and top-10 retrieved passages, trims them to 1,024 tokens (7.8 passages are left on average) as the input, and learns to generate the answer in a seq2seq manner (Mao et al., 2020; Min et al., 2020). We further add a simple shuffle strategy during reader training, which randomly shuffles the top retrieved passages before concatenation. In this way, the reader appears to be more robust to the reranked passages during inference and achieves better performance after reranking. Passage Reranking Given an initially retrieved passage list R and topN predictions of the reader A[:N ] , R IDER forms a reranked passage list R0 as follows. R IDER scans R from the beginning of the list and appends to R0 every passage p ∈ R if p contains any reader prediction a ∈ A[:N ] after string normalization (re"
2021.findings-acl.29,2020.findings-emnlp.63,0,0.119668,"ess to much more passages. 4.4 Table 4: End-to-end QA comparison of state-of-theart methods. R IDER results in up to 4.2 EM gains. Top-1 Comparison w. Supervised Reranking Finally, we compare R IDER with two state-of-theart supervised reranking models. The first reranker is a BERT-base cross-encoder (Nogueira and Cho, 2019), which is popularly used for passage reranking in information retrieval. The cross-encoder concatenates the query and passage, and makes a binary relevance decision for each query-passage pair. The second one generates relevance labels as target tokens in a seq2seq manner (Nogueira et al., 2020). We use BART-large as the base model and “YES/NO” as the target tokens. As listed in Table 6, R IDER, without any train347 ing, outperforms the two transformer-based supervised rerankers on retrieval accuracy. Also, for QA performance, the best EM we obtain using the supervised rerankers is merely 46.3 on NQ. Such results further demonstrate the effectiveness of R IDER, which has the advantage of utilizing information from multiple passages (when the reader makes predictions), while the other rerankers consider query-passage pairs independently. 4.5 Runtime Efficiency The reranking step of R"
2021.findings-acl.29,2020.emnlp-main.437,0,0.155175,"Missing"
2021.findings-emnlp.348,D17-1215,0,0.0740587,"Missing"
2021.findings-emnlp.348,2020.acl-main.197,1,0.864432,"Missing"
2021.findings-emnlp.348,N19-4009,0,0.0204156,"ss-inducing regularization and Bregman proximal point optimization. Machine Translation Datasets. We use three low-resource datasets2 : English-German from IWSLT’14, EnglishVietnamese from IWSLT’15, and English-French from IWSLT’16. We also use a rich-resource dataset: English-German from WMT’16. Dataset statistics are summarized in Table 4. Implementation. In NMT tasks, we have the source-side and the target-side inputs. We add perturbations to both of their embeddings (Sato et al., 2019). This has demonstrated to be more effective than adding perturbations to a single side. We use Fairseq3 (Ott et al., 2019) to implement our algorithms. For En-Vi and En-Fr experiments, we use the Transformer-base architecture (Vaswani et al., 2017). For En-De (IWSLT’14) experiments, we modify4 the Transformer-base architecture by decreasing the hidden dimension size from 2048 to 1024, and decreasing the number of heads from 8 to 4 (while dimension of each head doubles). For En-De (WMT’16) experiments, we use the Transformer-big (Vaswani et al., 2017) architecture. The training details are presented in Appendix B.1. 2 https://iwslt.org/ https://github.com/pytorch/fairseq 4 https://github.com/pytorch/fairseq/ tree/"
2021.findings-emnlp.348,W18-6301,0,0.0517244,"Missing"
2021.findings-emnlp.348,N18-1202,0,0.0274412,"Missing"
2021.findings-emnlp.348,W18-6319,0,0.0202505,"Missing"
2021.findings-emnlp.348,D13-1170,0,0.00298141,"tly eral Language Understanding Evaluation (GLUE) (i.e., 5 in Fig. 2b), the model cannot adapt to the benchmark (Wang et al., 2019a), which is a col- perturbations well; and if we cache the perturbalection of nine natural language inference tasks. tions too infrequently (i.e., inf in Fig. 2b), staleness The benchmark includes question answering (Ra- of the perturbations hinders model generalization. jpurkar et al., 2016), linguistic acceptability (CoLA,  Robustness to the number of neighbors. In Warstadt et al. 2019), sentiment analysis (SST, Fig. 2c, notice that ARCH is robust to the number Socher et al. 2013), text similarity (STS-B, Cer of neighbors. We also examine a variant of the et al. 2017), paraphrase detection (MRPC, Dolan KNN memory-saving strategy (R-1-NN): namely and Brockett 2005), and natural language inference 5 (RTE & MNLI, Dagan et al. 2006; Bar-Haim et al. https://github.com/namisan/mt-dnn 4124 (a) Moving average. (b) Epochs between caching. (c) Number of neighbors. (d) Proportion of caching. Figure 2: Parameter study on the IWSLT’14 De-En dataset. Each error bar is based on three runs using different random seeds. Each dashed line signifies the SMART baseline. In (b), inf means w"
2021.findings-emnlp.348,Q19-1040,0,0.0211577,"imporDatasets. We conduct experiments on the Gen- tant. If we cache the perturbations too frequently eral Language Understanding Evaluation (GLUE) (i.e., 5 in Fig. 2b), the model cannot adapt to the benchmark (Wang et al., 2019a), which is a col- perturbations well; and if we cache the perturbalection of nine natural language inference tasks. tions too infrequently (i.e., inf in Fig. 2b), staleness The benchmark includes question answering (Ra- of the perturbations hinders model generalization. jpurkar et al., 2016), linguistic acceptability (CoLA,  Robustness to the number of neighbors. In Warstadt et al. 2019), sentiment analysis (SST, Fig. 2c, notice that ARCH is robust to the number Socher et al. 2013), text similarity (STS-B, Cer of neighbors. We also examine a variant of the et al. 2017), paraphrase detection (MRPC, Dolan KNN memory-saving strategy (R-1-NN): namely and Brockett 2005), and natural language inference 5 (RTE & MNLI, Dagan et al. 2006; Bar-Haim et al. https://github.com/namisan/mt-dnn 4124 (a) Moving average. (b) Epochs between caching. (c) Number of neighbors. (d) Proportion of caching. Figure 2: Parameter study on the IWSLT’14 De-En dataset. Each error bar is based on three runs"
2021.findings-emnlp.348,N18-1101,0,0.0998151,"Missing"
2021.naacl-main.424,2020.acl-demos.16,1,0.894533,"Missing"
2021.naacl-main.424,W18-5446,0,0.0506617,"Missing"
2021.naacl-main.424,P19-1334,0,0.0569524,"Missing"
2021.naacl-main.424,N18-1101,0,0.113283,"Missing"
2021.naacl-main.424,2020.repl4nlp-1.8,1,0.887899,"training steps to where the model errs the most. Experiments show that TAT can significantly improve accuracy over standard adversarial training on GLUE and attain new state-of-the-art zero-shot results on XNLI. Our code will be released at: https://github. com/namisan/mt-dnn. 1 (a) BERT with standard fine-tuning Introduction Adversarial training has proven effective in improving model generalization and robustness in computer vision (Madry et al., 2017; Goodfellow et al., 2014) and natural language processing (NLP) (Zhu et al., 2019; Jiang et al., 2019; Cheng et al., 2019; Liu et al., 2020a; Pereira et al., 2020; Cheng et al., 2020). It works by augmenting the input with a small perturbation to steer the current model prediction away from the correct label, thus forcing subsequent training to make the model more robust and generalizable. Aside from some prior work in computer vision (Dong et al., 2018; Tramèr et al., 2017), most adversarial training approaches adopt non-targeted attacks, where the model prediction is not driven towards a specific incorrect label. In NLP, the cutting-edge research in adversarial training tends to focus on making adversarial training less expensive (e.g., by reusing ba"
2021.naacl-main.424,D16-1264,0,0.12567,"Missing"
2021.naacl-main.424,D18-1187,0,0.0412606,"Missing"
2021.naacl-main.424,D13-1170,0,0.0221058,"Missing"
2021.naacl-main.85,P07-1033,0,0.358043,"Missing"
2021.naacl-main.85,N19-1423,0,0.725941,"ce the model robustness. Here, we first provide a theoretical connection of two recent methods under this framework, i.e. Virtual Adversarial Training (VAT) (Miyato et al., 2018) and Jacobian Regularization (JR) (Sokoli´c et al., 2017). In addition, we propose to generalize VAT and random perturbation training (RPT) (Miyato et al., 2018) with a family of probability distribution metrics, f -divergences, and characterize their connection with JR. Given that large-scale pretrained neural language models have demonstrated their superior generalization for downstream NLP tasks under both matched (Devlin et al., 2019; Liu et al., 2019) and mismatched evaluations (Hendrycks et al., 2020), we systematically study the regularization framework using BERT (Devlin et al., 2019) on a diverse set of tasks in terms of both in-domain and out-ofdomain generalization. Specifically, we use representative datasets (Socher et al., 2013; Williams et al., 2018; Rajpurkar et al., 2016, 2018) from sentiment analysis, textual entailment and question answering (QA) for in-domain training and evaluation. In order to assess the resulting model generalization over domain shift and adversarial attack, we then consider out-of-doma"
2021.naacl-main.85,D19-5801,0,0.0634066,"rt and end position probabilities, respectively. 4 Experiments tasks. Following the literature, we report the exact match (EM) and F1 scores for QA datasets and classification accuracy for textual entailment and sentiment analysis. For model training, we use MNLI (Williams et al., 2018) and SST-2 (Socher et al., 2013) and SQuAD v1.1/v2.0 (Rajpurkar et al., 2016, 2018), respectively. The corresponding development set is used for evaluating the indomain generalization. To evaluate the out-of-domain generalization with domain shift, we use the BioAQS dataset (Tsatsaronis et al., 2015) from MRQA (Fisch et al., 2019) and the IMDB dataset (Maas et al., 2011). Unlike SQuAD which is based on Wikipedia, BioAQS is a biomedical QA dataset constructed on PubMed articles. Compared with SST-2 containing pithy export reviews (Socher et al., 2013), IMDB includes lengthy movie reviews from nonexperts (Maas et al., 2011). We directly apply the QA model trained on SQuAD v2.0 and the sentiment classifier trained on SSS-2 to BioAQS and IMDB, respectively. To evaluate the model robustness towards adversarial attack, we use two challenging adversarial datasets, i.e. Adversarial SQuAD (Jia and Liang, 2017) and HANS (McCoy e"
2021.naacl-main.85,N18-2017,0,0.0421731,"Missing"
2021.naacl-main.85,D19-6115,0,0.0287828,"Missing"
2021.naacl-main.85,2020.acl-main.244,0,0.02355,"Missing"
2021.naacl-main.85,P19-1147,0,0.0291196,"Missing"
2021.naacl-main.85,D17-1215,0,0.399946,"f domain adaptation. Various methods (Blitzer neural language models (Devlin et al., 2019; Liu et al., 2007; Daumé III, 2007) have been develet al., 2019) have better generalization, but experi- oped for training models to learn effectively from ence performance reduction caused by domain shift both in-domain (source) and out-of-domain (tar(Hendrycks et al., 2020). Textual entailment mod- get) datasets. Additionally, the recent discovery on els trained on MNLI (Williams et al., 2018) have the prevalence of data biases (Gururangan et al., picked up superficial cues focusing on either the 2018; Jia and Liang, 2017; McCoy et al., 2019), 1 unintended correlations between input and output Code is available at https://github.com/ hao-cheng/f-divergence. learned by statistical models, ignites the develop1078 Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1078–1089 June 6–11, 2021. ©2021 Association for Computational Linguistics ment of model debiasing techniques (Clark et al., 2019; He et al., 2019). These methods leverage discovered data biases to improve model generalization over adversarial datasets (Ji"
2021.naacl-main.85,2020.acl-main.197,1,0.909739,"Missing"
2021.naacl-main.85,D16-1207,0,0.0464418,"Missing"
2021.naacl-main.85,D16-1264,0,0.0762475,"of probability distribution metrics, f -divergences, and characterize their connection with JR. Given that large-scale pretrained neural language models have demonstrated their superior generalization for downstream NLP tasks under both matched (Devlin et al., 2019; Liu et al., 2019) and mismatched evaluations (Hendrycks et al., 2020), we systematically study the regularization framework using BERT (Devlin et al., 2019) on a diverse set of tasks in terms of both in-domain and out-ofdomain generalization. Specifically, we use representative datasets (Socher et al., 2013; Williams et al., 2018; Rajpurkar et al., 2016, 2018) from sentiment analysis, textual entailment and question answering (QA) for in-domain training and evaluation. In order to assess the resulting model generalization over domain shift and adversarial attack, we then consider out-of-domain datasets (Tsatsaronis et al., 2015; Maas et al., 2011) and challenge adversarial datasets (Jia and Liang, 2017; McCoy et al., 2019) in a zero-shot learning fashion. to consistently better empirical robustness over BERT with the standard fine-tuning. Furthermore, we find that different f -divergences lead to different generalization behaviors for in-dom"
2021.naacl-main.85,D13-1170,0,0.0388175,"g (RPT) (Miyato et al., 2018) with a family of probability distribution metrics, f -divergences, and characterize their connection with JR. Given that large-scale pretrained neural language models have demonstrated their superior generalization for downstream NLP tasks under both matched (Devlin et al., 2019; Liu et al., 2019) and mismatched evaluations (Hendrycks et al., 2020), we systematically study the regularization framework using BERT (Devlin et al., 2019) on a diverse set of tasks in terms of both in-domain and out-ofdomain generalization. Specifically, we use representative datasets (Socher et al., 2013; Williams et al., 2018; Rajpurkar et al., 2016, 2018) from sentiment analysis, textual entailment and question answering (QA) for in-domain training and evaluation. In order to assess the resulting model generalization over domain shift and adversarial attack, we then consider out-of-domain datasets (Tsatsaronis et al., 2015; Maas et al., 2011) and challenge adversarial datasets (Jia and Liang, 2017; McCoy et al., 2019) in a zero-shot learning fashion. to consistently better empirical robustness over BERT with the standard fine-tuning. Furthermore, we find that different f -divergences lead t"
2021.naacl-main.85,2021.ccl-1.108,0,0.0769547,"Missing"
2021.naacl-main.85,P11-1015,0,0.0421506,"uations (Hendrycks et al., 2020), we systematically study the regularization framework using BERT (Devlin et al., 2019) on a diverse set of tasks in terms of both in-domain and out-ofdomain generalization. Specifically, we use representative datasets (Socher et al., 2013; Williams et al., 2018; Rajpurkar et al., 2016, 2018) from sentiment analysis, textual entailment and question answering (QA) for in-domain training and evaluation. In order to assess the resulting model generalization over domain shift and adversarial attack, we then consider out-of-domain datasets (Tsatsaronis et al., 2015; Maas et al., 2011) and challenge adversarial datasets (Jia and Liang, 2017; McCoy et al., 2019) in a zero-shot learning fashion. to consistently better empirical robustness over BERT with the standard fine-tuning. Furthermore, we find that different f -divergences lead to different generalization behaviors for in-domain, domain shift and adversarial settings. In our study, VAT with symmetric divergence achieve better generalization for in-domain and domain shift cases, while VAT with asymmetric divergence achieve more robustness toward adversarial attack. More importantly, we show that a BERT-base model trained"
2021.naacl-main.85,P19-1334,0,0.191525,"Various methods (Blitzer neural language models (Devlin et al., 2019; Liu et al., 2007; Daumé III, 2007) have been develet al., 2019) have better generalization, but experi- oped for training models to learn effectively from ence performance reduction caused by domain shift both in-domain (source) and out-of-domain (tar(Hendrycks et al., 2020). Textual entailment mod- get) datasets. Additionally, the recent discovery on els trained on MNLI (Williams et al., 2018) have the prevalence of data biases (Gururangan et al., picked up superficial cues focusing on either the 2018; Jia and Liang, 2017; McCoy et al., 2019), 1 unintended correlations between input and output Code is available at https://github.com/ hao-cheng/f-divergence. learned by statistical models, ignites the develop1078 Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1078–1089 June 6–11, 2021. ©2021 Association for Computational Linguistics ment of model debiasing techniques (Clark et al., 2019; He et al., 2019). These methods leverage discovered data biases to improve model generalization over adversarial datasets (Jia and Liang, 2017; Mc"
2021.naacl-main.85,2020.repl4nlp-1.8,1,0.883027,"Missing"
2021.naacl-main.85,P18-2124,0,0.0559141,"Missing"
2021.naacl-main.85,N18-1101,0,0.431737,"cenarios (Nie et al., 2019; Hsieh with domain shift has been a long-standing goal et al., 2019). For example, large-scale pretrained of domain adaptation. Various methods (Blitzer neural language models (Devlin et al., 2019; Liu et al., 2007; Daumé III, 2007) have been develet al., 2019) have better generalization, but experi- oped for training models to learn effectively from ence performance reduction caused by domain shift both in-domain (source) and out-of-domain (tar(Hendrycks et al., 2020). Textual entailment mod- get) datasets. Additionally, the recent discovery on els trained on MNLI (Williams et al., 2018) have the prevalence of data biases (Gururangan et al., picked up superficial cues focusing on either the 2018; Jia and Liang, 2017; McCoy et al., 2019), 1 unintended correlations between input and output Code is available at https://github.com/ hao-cheng/f-divergence. learned by statistical models, ignites the develop1078 Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1078–1089 June 6–11, 2021. ©2021 Association for Computational Linguistics ment of model debiasing techniques (Clark et al.,"
D19-1254,D17-1087,0,0.0878952,"sfer an MRC model trained in a high-resource domain to other lowresource domains is critical for scalable MRC. While it is difficult to collect annotated questionanswer pairs in a new domain, it is generally feasible to obtain a large amount of unlabeled text in a given domain. In this work, we focus on adapting an MRC model trained in a source domain to other new domains, where only unlabeled passages are available. This domain adaptation issue has been a main challenge in MRC research, and the only existing work that investigated this was the two-stage synthesis network (SynNet) proposed in Golub et al. (2017). Specifically, SynNet first generates pseudo question-answer pairs in the target domain, and then uses the generated data as augmentation to fine-tune a pre-trained MRC model. However, the source-domain labeled data and targetdomain pseudo data are directly combined without considering domain differences (see Figure 1(a), where the two feature distributions in two domains are independently clustered). Directly transfering a model from one domain to another could be counter-effective, or even hurt the performance of the pre-trained model due to domain variance. To achieve effective domain tran"
D19-1254,P19-1407,0,0.033498,"Missing"
D19-1254,Q18-1039,0,0.0344356,"ere N = |S |+ |Tgen |. In order to learn domaininvariant representations from the encoder, we update θe to maximize the loss while updating θc to minimize the loss in an adversarial fashion. The overall objective function is defined as: LC (θe , θc ) = L(θe , θd , θc ) = LD (θe , θd ) − λLC (θe , θc ) , (4) where λ is a trade-off parameter that balances the two terms. To optimize our model, instead of alternately updating the adversaries like in GAN (Goodfellow et al., 2014), we use the gradient-reversal layer (Ganin and Lempitsky, 2015) to jointly optimize all the components, as suggested in Chen et al. (2018). 5 5.1 Experiments Experimental Setting Datasets We validate our proposed method on three benchmarks: SQuAD (Rajpurkar et al., 2514 Dataset Domain SQuAD (v1.1) Wiki NewsQA News MS MARCO (v1) Web Train 87,600 92,549 82,430 Dev Test 10,570 − 5,166 5,165 10,047 9,650 Method EM/F1 SQuAD → NewsQA SAN 36.68/52.79 SynNet + SAN 35.19/49.61 AdaMRC 38.46/54.20 AdaMRC with GT questions 39.37,54.63 NewsQA → SQuAD SAN 56.83/68.62 SynNet + SAN 50.34/62.42 AdaMRC 58.20/69.75 AdaMRC with GT questions 58.82/70.14 SQuAD → MS MARCO (BLEU-1/ROUGE-L) SAN 13.06/25.80 SynNet + SAN 12.52/25.47 AdaMRC 14.09/26.09 Ada"
D19-1254,W14-4012,0,0.0610233,"Missing"
D19-1254,N18-1143,0,0.0201654,"(Trischler et al., 2016) and MS MARCO (Bajaj et al., 2016)). Different from previous work that focused on improving the state of the art on particular MRC datasets, we study the MRC task from a different angle, and aim at addressing a critical yet challenging problem: how to transfer an MRC model learned from a high-resource domain to other lowresource domains in an unsupervised manner. Although important for the MRC task, where annotated data are limited in real-life applications, this problem has not yet been well investigated. There were some relevant studies along this line. For example, Chung et al. (2018) adapted a pretrained model to TOEFL and MCTest dataset, and Wiese et al. (2017) applied transfer learning to the biomedical domain. However, both studies assumed that annotated data in the target domain (either questions or question-answer pairs) are available. To the best of our knowledge, SynNet (Golub et al., 2017) is the only work that also studied domain adaptation for MRC. Compared with SynNet, the key difference in our model is adversarial learning, which enables domain-invariant representation learning for better model adaptation to low-resource domains. Our approach is also related t"
D19-1254,N19-1423,0,0.0523789,"o learn domaininvariant representations, which are beneficial for transferring knowledge learned from one domain to another. Based on this, an answer decoder is then used to decode domain-invariant representation into an answer span. The proposed approach is validated on a set of popular benchmarks, including SQuAD (Rajpurkar et al., 2016), NewsQA (Trischler et al., 2016), and MS MARCO (Bajaj et al., 2016), using state-of-the-art MRC models including SAN (Liu et al., 2018) and BiDAF (Seo et al., 2017). Since pre-trained large-scale language models, such as ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019), have shown strong performance to learn representations that are generalizable to various tasks, in this work, to further demonstrate the versatility of the proposed model, we perform additional experiments to demonstrate that AdaMRC can also be combined with ELMo and BERT to further boost the performance. The main contributions of this paper are summarized as follows: (i) We propose AdaMRC, an adversarial domain adaptation framework that is specifically designed for MRC. (ii) We perform comprehensive evaluations on several benchmarks, demonstrating that the proposed method is generalizable t"
D19-1254,P17-1123,0,0.067565,"Missing"
D19-1254,D17-1090,0,0.0605507,"Missing"
D19-1254,I17-2072,0,0.0160204,"ansiting the feature distribution from the source domain to the target domain (Gong et al., 2012; Long et al., 2015). Another school of research focuses on learning domain-invariant representations (Glorot et al., 2011) (e.g., via adversarial learning (Ganin et al., 2016; Tzeng et al., 2017)). Domain adaptation has been successfully applied to many tasks, such as image classification (Tzeng et al., 2017), speech recognition (Doulaty et al., 2015), sentiment classification (Ganin et al., 2016; Li et al., 2017), machine translation (Johnson et al., 2017; Zoph et al., 2016), relation extraction (Fu et al., 2017), and paraphrase identification (Shah et al., 2018). Compared to these areas, the application to MRC presents additional challenges, since besides missing labeled data (i.e., answer spans), the questions in the target domain are also unavailable. To our best knowledge, we are the first to investigate the usage of adversarial domain adaptation for the MRC task. There are many prevailing unsupervised techniques for domain adaptation. Our proposed approach is inspired by the seminal work of Ganin et al. (2016) to validate its potential of solving domain adaptation problem on a new task, without a"
D19-1254,P16-1154,0,0.0127507,"d both methods, and empirically observed that a simple NER system provides more robust results, which is used in our experiments. Now, we describe how the question generation (QG) model is trained. Given the passage ps = (p1 , p2 , ..., pT ) and answer as = (astart , aend ) from the source domain, the QG model with parameter θQG learns the conditional probability of generating a question q s = (q1 , q2 , ..., qT 0 ), i.e., P (q s |ps , as ). We implement the QG model as a sequence-to-sequence model with attention mechanism (Bahdanau et al., 2015), and also apply the copy mechanism proposed in Gu et al. (2016); Gulcehre et al. (2016) to handle rare/unknown words. Specifically, the QG model consists of a lexicon encoding layer, a BiLSTM contextual encoding layer, and an LSTM decoder. For lexicon encoding, each word token pi of a passage is mapped into a concatenation of GloVe vectors (Pennington et al., 2014), part-of-speech (POS) tagging embedding, and named-entity-recognition (NER) embedding. We further insert answer information by appending an additional zero/one feature (similar to Yang et al. (2017)) to model the appearance of answer tokens in the passage. The output of the lexicon encoding lay"
D19-1254,P16-1014,0,0.0269573,"d empirically observed that a simple NER system provides more robust results, which is used in our experiments. Now, we describe how the question generation (QG) model is trained. Given the passage ps = (p1 , p2 , ..., pT ) and answer as = (astart , aend ) from the source domain, the QG model with parameter θQG learns the conditional probability of generating a question q s = (q1 , q2 , ..., qT 0 ), i.e., P (q s |ps , as ). We implement the QG model as a sequence-to-sequence model with attention mechanism (Bahdanau et al., 2015), and also apply the copy mechanism proposed in Gu et al. (2016); Gulcehre et al. (2016) to handle rare/unknown words. Specifically, the QG model consists of a lexicon encoding layer, a BiLSTM contextual encoding layer, and an LSTM decoder. For lexicon encoding, each word token pi of a passage is mapped into a concatenation of GloVe vectors (Pennington et al., 2014), part-of-speech (POS) tagging embedding, and named-entity-recognition (NER) embedding. We further insert answer information by appending an additional zero/one feature (similar to Yang et al. (2017)) to model the appearance of answer tokens in the passage. The output of the lexicon encoding layer is appended with CoVe"
D19-1254,Q17-1024,0,0.040402,"2015). One line of research on domain adaptation focuses on transiting the feature distribution from the source domain to the target domain (Gong et al., 2012; Long et al., 2015). Another school of research focuses on learning domain-invariant representations (Glorot et al., 2011) (e.g., via adversarial learning (Ganin et al., 2016; Tzeng et al., 2017)). Domain adaptation has been successfully applied to many tasks, such as image classification (Tzeng et al., 2017), speech recognition (Doulaty et al., 2015), sentiment classification (Ganin et al., 2016; Li et al., 2017), machine translation (Johnson et al., 2017; Zoph et al., 2016), relation extraction (Fu et al., 2017), and paraphrase identification (Shah et al., 2018). Compared to these areas, the application to MRC presents additional challenges, since besides missing labeled data (i.e., answer spans), the questions in the target domain are also unavailable. To our best knowledge, we are the first to investigate the usage of adversarial domain adaptation for the MRC task. There are many prevailing unsupervised techniques for domain adaptation. Our proposed approach is inspired by the seminal work of Ganin et al. (2016) to validate its potential of"
D19-1254,N15-1092,1,0.839052,", and Wiese et al. (2017) applied transfer learning to the biomedical domain. However, both studies assumed that annotated data in the target domain (either questions or question-answer pairs) are available. To the best of our knowledge, SynNet (Golub et al., 2017) is the only work that also studied domain adaptation for MRC. Compared with SynNet, the key difference in our model is adversarial learning, which enables domain-invariant representation learning for better model adaptation to low-resource domains. Our approach is also related to multi-task learning (Xu et al., 2019; Caruana, 1997; Liu et al., 2015, 2019) and semisupervised learning (Yang et al., 2017) for MRC. 2511 Loss LD Decoder Loss LC Answerstart, Answerend Domain label Answer Module Domain Classiﬁer Discriminator Pseudo Question Encoder Source Domain Question Generator Encoder Target Domain Cross Attention Cross Attention Attention-based Decoding Lexicon & Contextual Encoding Lexicon & Contextual Encoding Lexicon & Contextual Encoding Lexicon & Contextual Encoding Lexicon & Contextual Encoding Document Question Document Pseudo Question Document & Answer Figure 2: Illustration of the proposed AdaMRC model for unsupervised domain ad"
D19-1254,P19-1441,1,0.878178,"Missing"
D19-1254,P18-1157,1,0.40104,"y trained using adversarial learning to enforce domain-invariant representation learning. Comprehensive evaluations demonstrate that our approach (i) is generalizable to different MRC models and datasets, (ii) can be combined with pre-trained large-scale language models (such as ELMo and BERT), and (iii) can be extended to semisupervised learning. 1 3 Introduction Recently, many neural network models have been developed for Machine Reading Comprehension (MRC), with performance comparable to human in specific settings (Gao et al., 2019). However, most state-of-the-art models (Seo et al., 2017; Liu et al., 2018; Yu et al., 2018) rely on large amount of human-annotated in-domain data to achieve the desired performance. Although there exists a number of large-scale MRC datasets (Rajpurkar et al., 2016; Trischler et al., 2016; Bajaj et al., 2016; Zhang et al., 2018), collecting such highquality datasets is expensive and time-consuming, which hinders real-world applications for domainspecific MRC. ∗ Most of this work was done when the first author was an intern at Microsoft Dynamics 365 AI Research. Therefore, the ability to transfer an MRC model trained in a high-resource domain to other lowresource do"
D19-1254,N19-1271,1,0.809712,"odel to TOEFL and MCTest dataset, and Wiese et al. (2017) applied transfer learning to the biomedical domain. However, both studies assumed that annotated data in the target domain (either questions or question-answer pairs) are available. To the best of our knowledge, SynNet (Golub et al., 2017) is the only work that also studied domain adaptation for MRC. Compared with SynNet, the key difference in our model is adversarial learning, which enables domain-invariant representation learning for better model adaptation to low-resource domains. Our approach is also related to multi-task learning (Xu et al., 2019; Caruana, 1997; Liu et al., 2015, 2019) and semisupervised learning (Yang et al., 2017) for MRC. 2511 Loss LD Decoder Loss LC Answerstart, Answerend Domain label Answer Module Domain Classiﬁer Discriminator Pseudo Question Encoder Source Domain Question Generator Encoder Target Domain Cross Attention Cross Attention Attention-based Decoding Lexicon & Contextual Encoding Lexicon & Contextual Encoding Lexicon & Contextual Encoding Lexicon & Contextual Encoding Lexicon & Contextual Encoding Document Question Document Pseudo Question Document & Answer Figure 2: Illustration of the proposed AdaMRC"
D19-1254,D14-1162,0,0.0835287,"n, the QG model with parameter θQG learns the conditional probability of generating a question q s = (q1 , q2 , ..., qT 0 ), i.e., P (q s |ps , as ). We implement the QG model as a sequence-to-sequence model with attention mechanism (Bahdanau et al., 2015), and also apply the copy mechanism proposed in Gu et al. (2016); Gulcehre et al. (2016) to handle rare/unknown words. Specifically, the QG model consists of a lexicon encoding layer, a BiLSTM contextual encoding layer, and an LSTM decoder. For lexicon encoding, each word token pi of a passage is mapped into a concatenation of GloVe vectors (Pennington et al., 2014), part-of-speech (POS) tagging embedding, and named-entity-recognition (NER) embedding. We further insert answer information by appending an additional zero/one feature (similar to Yang et al. (2017)) to model the appearance of answer tokens in the passage. The output of the lexicon encoding layer is appended with CoVe vectors (McCann et al., 2017), and then passed to the Bidirectional LSTM contextual encoding layer, producing a sequence of hidden states. The decoder is another LSTM with attention and copy mechanism over the encoder hidden states. At each time step, the generation probability"
D19-1254,N18-1202,0,0.0161392,"way, the encoder is enforced to learn domaininvariant representations, which are beneficial for transferring knowledge learned from one domain to another. Based on this, an answer decoder is then used to decode domain-invariant representation into an answer span. The proposed approach is validated on a set of popular benchmarks, including SQuAD (Rajpurkar et al., 2016), NewsQA (Trischler et al., 2016), and MS MARCO (Bajaj et al., 2016), using state-of-the-art MRC models including SAN (Liu et al., 2018) and BiDAF (Seo et al., 2017). Since pre-trained large-scale language models, such as ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019), have shown strong performance to learn representations that are generalizable to various tasks, in this work, to further demonstrate the versatility of the proposed model, we perform additional experiments to demonstrate that AdaMRC can also be combined with ELMo and BERT to further boost the performance. The main contributions of this paper are summarized as follows: (i) We propose AdaMRC, an adversarial domain adaptation framework that is specifically designed for MRC. (ii) We perform comprehensive evaluations on several benchmarks, demonstrating that the pro"
D19-1254,D16-1264,0,0.766718,"dels and datasets, (ii) can be combined with pre-trained large-scale language models (such as ELMo and BERT), and (iii) can be extended to semisupervised learning. 1 3 Introduction Recently, many neural network models have been developed for Machine Reading Comprehension (MRC), with performance comparable to human in specific settings (Gao et al., 2019). However, most state-of-the-art models (Seo et al., 2017; Liu et al., 2018; Yu et al., 2018) rely on large amount of human-annotated in-domain data to achieve the desired performance. Although there exists a number of large-scale MRC datasets (Rajpurkar et al., 2016; Trischler et al., 2016; Bajaj et al., 2016; Zhang et al., 2018), collecting such highquality datasets is expensive and time-consuming, which hinders real-world applications for domainspecific MRC. ∗ Most of this work was done when the first author was an intern at Microsoft Dynamics 365 AI Research. Therefore, the ability to transfer an MRC model trained in a high-resource domain to other lowresource domains is critical for scalable MRC. While it is difficult to collect annotated questionanswer pairs in a new domain, it is generally feasible to obtain a large amount of unlabeled text in a gi"
D19-1254,N18-1058,0,0.0443019,"S and the target domain dataset Tgen . The goal of the decoder θd is to predict P (a|p, q). The objective function is denoted as: 1 P|S| LD (θe , θd ) = log P (a(i) |p(i) , q (i) ) , |S |i=1 (2) where the superscript (i) indicates the i-th sample. It is worthwhile to emphasize that unlike Golub et al. (2017), we only use source domain data to update the decoder, without using pseudo target domain data. This is because the synthetic question-answer pairs could be noisy, and directly using such data for decoder training may lead to degraded performance of the answer module, as observed both in Sachan and Xing (2018) and in our experiments. The synthetic target domain data and source domain data are both used to update the encoder θe and the domain classifier θc . The classifier predicts a domain label d given the feature representation from the encoder. The objective function is: 1 PN log P (d(i) |p(i) , q (i) ) , (3) N i=1 where N = |S |+ |Tgen |. In order to learn domaininvariant representations from the encoder, we update θe to maximize the loss while updating θc to minimize the loss in an adversarial fashion. The overall objective function is defined as: LC (θe , θc ) = L(θe , θd , θc ) = LD (θe , θd"
D19-1254,P18-1156,0,0.0711302,"Missing"
D19-1254,P17-1096,0,0.190327,"o the biomedical domain. However, both studies assumed that annotated data in the target domain (either questions or question-answer pairs) are available. To the best of our knowledge, SynNet (Golub et al., 2017) is the only work that also studied domain adaptation for MRC. Compared with SynNet, the key difference in our model is adversarial learning, which enables domain-invariant representation learning for better model adaptation to low-resource domains. Our approach is also related to multi-task learning (Xu et al., 2019; Caruana, 1997; Liu et al., 2015, 2019) and semisupervised learning (Yang et al., 2017) for MRC. 2511 Loss LD Decoder Loss LC Answerstart, Answerend Domain label Answer Module Domain Classiﬁer Discriminator Pseudo Question Encoder Source Domain Question Generator Encoder Target Domain Cross Attention Cross Attention Attention-based Decoding Lexicon & Contextual Encoding Lexicon & Contextual Encoding Lexicon & Contextual Encoding Lexicon & Contextual Encoding Lexicon & Contextual Encoding Document Question Document Pseudo Question Document & Answer Figure 2: Illustration of the proposed AdaMRC model for unsupervised domain adaptation of MRC. In this work, we focus on purely unsup"
D19-1254,D16-1163,0,0.0257237,"search on domain adaptation focuses on transiting the feature distribution from the source domain to the target domain (Gong et al., 2012; Long et al., 2015). Another school of research focuses on learning domain-invariant representations (Glorot et al., 2011) (e.g., via adversarial learning (Ganin et al., 2016; Tzeng et al., 2017)). Domain adaptation has been successfully applied to many tasks, such as image classification (Tzeng et al., 2017), speech recognition (Doulaty et al., 2015), sentiment classification (Ganin et al., 2016; Li et al., 2017), machine translation (Johnson et al., 2017; Zoph et al., 2016), relation extraction (Fu et al., 2017), and paraphrase identification (Shah et al., 2018). Compared to these areas, the application to MRC presents additional challenges, since besides missing labeled data (i.e., answer spans), the questions in the target domain are also unavailable. To our best knowledge, we are the first to investigate the usage of adversarial domain adaptation for the MRC task. There are many prevailing unsupervised techniques for domain adaptation. Our proposed approach is inspired by the seminal work of Ganin et al. (2016) to validate its potential of solving domain adap"
D19-1254,D18-1131,0,0.0333913,"domain to the target domain (Gong et al., 2012; Long et al., 2015). Another school of research focuses on learning domain-invariant representations (Glorot et al., 2011) (e.g., via adversarial learning (Ganin et al., 2016; Tzeng et al., 2017)). Domain adaptation has been successfully applied to many tasks, such as image classification (Tzeng et al., 2017), speech recognition (Doulaty et al., 2015), sentiment classification (Ganin et al., 2016; Li et al., 2017), machine translation (Johnson et al., 2017; Zoph et al., 2016), relation extraction (Fu et al., 2017), and paraphrase identification (Shah et al., 2018). Compared to these areas, the application to MRC presents additional challenges, since besides missing labeled data (i.e., answer spans), the questions in the target domain are also unavailable. To our best knowledge, we are the first to investigate the usage of adversarial domain adaptation for the MRC task. There are many prevailing unsupervised techniques for domain adaptation. Our proposed approach is inspired by the seminal work of Ganin et al. (2016) to validate its potential of solving domain adaptation problem on a new task, without any supervision for the target domain. There are als"
D19-1254,D18-1427,0,0.0363646,"Missing"
D19-1254,W17-2309,0,0.0187619,"vious work that focused on improving the state of the art on particular MRC datasets, we study the MRC task from a different angle, and aim at addressing a critical yet challenging problem: how to transfer an MRC model learned from a high-resource domain to other lowresource domains in an unsupervised manner. Although important for the MRC task, where annotated data are limited in real-life applications, this problem has not yet been well investigated. There were some relevant studies along this line. For example, Chung et al. (2018) adapted a pretrained model to TOEFL and MCTest dataset, and Wiese et al. (2017) applied transfer learning to the biomedical domain. However, both studies assumed that annotated data in the target domain (either questions or question-answer pairs) are available. To the best of our knowledge, SynNet (Golub et al., 2017) is the only work that also studied domain adaptation for MRC. Compared with SynNet, the key difference in our model is adversarial learning, which enables domain-invariant representation learning for better model adaptation to low-resource domains. Our approach is also related to multi-task learning (Xu et al., 2019; Caruana, 1997; Liu et al., 2015, 2019) a"
D19-6002,N16-1098,0,0.0565833,"Missing"
D19-6002,P02-1014,0,0.333723,"t “An object cannot fit in a container because either the object (trophy) is too big or the container (suitcase) is too small.” In this paper, we study two classic commonsense reasoning tasks: the Winograd Schema Challenge (WSC) and Pronoun Disambiguation Problem (PDP) (Levesque et al., 2011; Davis and Marcus, 2015). Both tasks are formulated as an anaphora resolution problem, which is a form of co-reference resolution, where a machine (AI agent) must identify the antecedent of an ambiguous pronoun in a statement. WSC and PDP differ from other co-reference resolution tasks (Soon et al., 2001; Ng and Cardie, 2002; Peng et al., 2016) in that commonsense knowledge, which cannot be explicitly decoded from the given text, is needed to solve the problem, as illustrated in the examples in Table 1. Comparing with other commonsense reasoning tasks, such as COPA (Roemmele et al., 2011), Story Cloze Test (Mostafazadeh et al., 2016), Event2Mind (Rashkin et al., 2018), SWAG (Zellers et al., 2018), ReCoRD (Zhang et al., 2018), and so on, WSC and PDP better approximate real human reasoning, can be easily solved by native English-speaker (Levesque et al., 2011), 13 Proceedings of the First Workshop on Commonsense In"
D19-6002,P19-1459,0,0.0608768,"Missing"
D19-6002,W18-4105,0,0.166059,"model trained on raw text from books and news to calculate the probabilities of the natural language sentences which are constructed from a statement by replacing the to-be-resolved pronoun in the statement with each of its candidate references (antecedent), and then pick the candidate with the highest probability as the answer. Kocijan et al. (2019) showed that a significant improvement can be achieved by fine-tuning a pre-trained masked language model (BERT in their case) on a small amount of WSC labeled data. The second category of models are semantic similarity models. Wang et al. (2019); Opitz and Frank (2018) formulated WSC and PDP as a semantic matching problem, and proposed to use two variations of the Deep Structured Similarity Model (DSSM) (Huang et al., 2013) to compute the semantic similarity score between each candidate antecedent and the pronoun by (1) mapping the candidate and the pronoun and their context into two vectors, respectively, in a hidden space using deep neural networks, and (2) computing cosine similarity between the two vectors. The candidate with the highest score is selected as the result. The two categories of models use different inductive biases when predicting outputs"
D19-6002,P19-1478,0,0.221553,"Missing"
D19-6002,D16-1038,0,0.0233878,"it in a container because either the object (trophy) is too big or the container (suitcase) is too small.” In this paper, we study two classic commonsense reasoning tasks: the Winograd Schema Challenge (WSC) and Pronoun Disambiguation Problem (PDP) (Levesque et al., 2011; Davis and Marcus, 2015). Both tasks are formulated as an anaphora resolution problem, which is a form of co-reference resolution, where a machine (AI agent) must identify the antecedent of an ambiguous pronoun in a statement. WSC and PDP differ from other co-reference resolution tasks (Soon et al., 2001; Ng and Cardie, 2002; Peng et al., 2016) in that commonsense knowledge, which cannot be explicitly decoded from the given text, is needed to solve the problem, as illustrated in the examples in Table 1. Comparing with other commonsense reasoning tasks, such as COPA (Roemmele et al., 2011), Story Cloze Test (Mostafazadeh et al., 2016), Event2Mind (Rashkin et al., 2018), SWAG (Zellers et al., 2018), ReCoRD (Zhang et al., 2018), and so on, WSC and PDP better approximate real human reasoning, can be easily solved by native English-speaker (Levesque et al., 2011), 13 Proceedings of the First Workshop on Commonsense Inference in Natural L"
D19-6002,D12-1071,0,0.567047,"g, China, November 3, 2019. 2019 Association for Computational Linguistics and yet are challenging for machines. For example, the WNLI task, which is derived from WSC, is considered the most challenging NLU task in the General Language Understanding Evaluation (GLUE) benchmark (Wang et al., 2018). Most machine learning models can hardly outperform the naive baseline of majority voting (scored at 65.1) 1 , including BERT (Devlin et al., 2018a) and Distilled MT-DNN (Liu et al., 2019a). While traditional methods of commonsense reasoning rely heavily on human-crafted features and knowledge bases (Rahman and Ng, 2012a; Sharma et al., 2015; Sch¨uller, 2014; Bailey et al., 2015; Liu et al., 2017), we explore in this study machine learning approaches using deep neural networks (DNN). Our method is inspired by two categories of DNN models proposed recently. The first are neural language models trained on large amounts of text data. Trinh and Le (2018) proposed to use a neural language model trained on raw text from books and news to calculate the probabilities of the natural language sentences which are constructed from a statement by replacing the to-be-resolved pronoun in the statement with each of its cand"
D19-6002,N15-1092,1,0.823619,"between the two vectors. The candidate with the highest score is selected as the result. The two categories of models use different inductive biases when predicting outputs given inputs, and thus capture different views of the data. While language models measure the semantic co1 See the GLUE leaderboard at gluebenchmark.com/leaderboard herence and wholeness of a statement where the pronoun to be resolved is replaced with its candidate antecedent, DSSMs measure the semantic relatedness of the pronoun and its candidate in their context. Therefore, inspired by multi-task learning (Caruana, 1997; Liu et al., 2015, 2019b), we propose a hybrid neural network (HNN) model that combines the strengths of both neural language models and a semantic similarity model. As shown in Figure 1, HNN consists of two component models, a masked language model and a deep semantic similarity model. The two component models share the same text encoder (BERT), but use different model-specific input and output layers. The final output score is the combination of the two model scores. The architecture of HNN bears a strong resemblance to that of MultiTask Deep Neural Network (MT-DNN) (Liu et al., 2019b), which consists of a B"
D19-6002,P18-1043,0,0.0335149,"an anaphora resolution problem, which is a form of co-reference resolution, where a machine (AI agent) must identify the antecedent of an ambiguous pronoun in a statement. WSC and PDP differ from other co-reference resolution tasks (Soon et al., 2001; Ng and Cardie, 2002; Peng et al., 2016) in that commonsense knowledge, which cannot be explicitly decoded from the given text, is needed to solve the problem, as illustrated in the examples in Table 1. Comparing with other commonsense reasoning tasks, such as COPA (Roemmele et al., 2011), Story Cloze Test (Mostafazadeh et al., 2016), Event2Mind (Rashkin et al., 2018), SWAG (Zellers et al., 2018), ReCoRD (Zhang et al., 2018), and so on, WSC and PDP better approximate real human reasoning, can be easily solved by native English-speaker (Levesque et al., 2011), 13 Proceedings of the First Workshop on Commonsense Inference in Natural Language Processing, pages 13–21 c Hongkong, China, November 3, 2019. 2019 Association for Computational Linguistics and yet are challenging for machines. For example, the WNLI task, which is derived from WSC, is considered the most challenging NLU task in the General Language Understanding Evaluation (GLUE) benchmark (Wang et al"
D19-6002,P19-1441,1,0.846218,"et al., 2011), 13 Proceedings of the First Workshop on Commonsense Inference in Natural Language Processing, pages 13–21 c Hongkong, China, November 3, 2019. 2019 Association for Computational Linguistics and yet are challenging for machines. For example, the WNLI task, which is derived from WSC, is considered the most challenging NLU task in the General Language Understanding Evaluation (GLUE) benchmark (Wang et al., 2018). Most machine learning models can hardly outperform the naive baseline of majority voting (scored at 65.1) 1 , including BERT (Devlin et al., 2018a) and Distilled MT-DNN (Liu et al., 2019a). While traditional methods of commonsense reasoning rely heavily on human-crafted features and knowledge bases (Rahman and Ng, 2012a; Sharma et al., 2015; Sch¨uller, 2014; Bailey et al., 2015; Liu et al., 2017), we explore in this study machine learning approaches using deep neural networks (DNN). Our method is inspired by two categories of DNN models proposed recently. The first are neural language models trained on large amounts of text data. Trinh and Le (2018) proposed to use a neural language model trained on raw text from books and news to calculate the probabilities of the natural la"
D19-6002,J01-4004,0,0.14275,"sense knowledge that “An object cannot fit in a container because either the object (trophy) is too big or the container (suitcase) is too small.” In this paper, we study two classic commonsense reasoning tasks: the Winograd Schema Challenge (WSC) and Pronoun Disambiguation Problem (PDP) (Levesque et al., 2011; Davis and Marcus, 2015). Both tasks are formulated as an anaphora resolution problem, which is a form of co-reference resolution, where a machine (AI agent) must identify the antecedent of an ambiguous pronoun in a statement. WSC and PDP differ from other co-reference resolution tasks (Soon et al., 2001; Ng and Cardie, 2002; Peng et al., 2016) in that commonsense knowledge, which cannot be explicitly decoded from the given text, is needed to solve the problem, as illustrated in the examples in Table 1. Comparing with other commonsense reasoning tasks, such as COPA (Roemmele et al., 2011), Story Cloze Test (Mostafazadeh et al., 2016), Event2Mind (Rashkin et al., 2018), SWAG (Zellers et al., 2018), ReCoRD (Zhang et al., 2018), and so on, WSC and PDP better approximate real human reasoning, can be easily solved by native English-speaker (Levesque et al., 2011), 13 Proceedings of the First Works"
D19-6002,W18-5446,0,0.0997573,"Missing"
D19-6002,N19-1094,1,0.922246,"e a neural language model trained on raw text from books and news to calculate the probabilities of the natural language sentences which are constructed from a statement by replacing the to-be-resolved pronoun in the statement with each of its candidate references (antecedent), and then pick the candidate with the highest probability as the answer. Kocijan et al. (2019) showed that a significant improvement can be achieved by fine-tuning a pre-trained masked language model (BERT in their case) on a small amount of WSC labeled data. The second category of models are semantic similarity models. Wang et al. (2019); Opitz and Frank (2018) formulated WSC and PDP as a semantic matching problem, and proposed to use two variations of the Deep Structured Similarity Model (DSSM) (Huang et al., 2013) to compute the semantic similarity score between each candidate antecedent and the pronoun by (1) mapping the candidate and the pronoun and their context into two vectors, respectively, in a hidden space using deep neural networks, and (2) computing cosine similarity between the two vectors. The candidate with the highest score is selected as the result. The two categories of models use different inductive biases"
D19-6002,D18-1009,0,0.0262604,"m, which is a form of co-reference resolution, where a machine (AI agent) must identify the antecedent of an ambiguous pronoun in a statement. WSC and PDP differ from other co-reference resolution tasks (Soon et al., 2001; Ng and Cardie, 2002; Peng et al., 2016) in that commonsense knowledge, which cannot be explicitly decoded from the given text, is needed to solve the problem, as illustrated in the examples in Table 1. Comparing with other commonsense reasoning tasks, such as COPA (Roemmele et al., 2011), Story Cloze Test (Mostafazadeh et al., 2016), Event2Mind (Rashkin et al., 2018), SWAG (Zellers et al., 2018), ReCoRD (Zhang et al., 2018), and so on, WSC and PDP better approximate real human reasoning, can be easily solved by native English-speaker (Levesque et al., 2011), 13 Proceedings of the First Workshop on Commonsense Inference in Natural Language Processing, pages 13–21 c Hongkong, China, November 3, 2019. 2019 Association for Computational Linguistics and yet are challenging for machines. For example, the WNLI task, which is derived from WSC, is considered the most challenging NLU task in the General Language Understanding Evaluation (GLUE) benchmark (Wang et al., 2018). Most machine learni"
I17-1096,D14-1162,0,0.0806857,"to handle multiple passages, we incorporated an extra passage ranker component. The architecture is shown in Figure 1. In brief, the embedding/encoder layers first build representations of Q and P . The aggregation layer uses co-attention to fuse information from the QP pair. The output layer is a recurrent net that maintains intermediate state and dynamically decides at which turn to generate the answer. 3.1 Detailed description of ReasoNet++ Embedding Layer: We adopt three types of embeddings to represent input word tokens in Q and P . For word embeddings, we use pre-trained GloVe vectors (Pennington et al., 2014). To address the out-of-vocabulary problem, we also include character and letter 3-gram embeddings. Character embeddings are fed into a convolutional neural network (CNN) as in (Kim, 2014), then max-pooled to form a fixed-size vector for each token. For letter 3-gram embeddings, we follow Huang et al. (2013) by first hashing each word as a bag of letter 3-gram, then feeding them into another CNN. The concatenation of all embeddings are then fed to a two-layer Highway Network (Srivastava et al., 2015). Therefore, we obtain the 959 Pi=m−1 p p H p , H q C q , i=0 H:i c are all 2d by n matrices, U"
I17-1096,D16-1264,0,0.366414,"models have embraced this kind of multiple-turn strategy; they generate predictions by making multiple passes through the same text and integrating intermediate information in the process (Hill et al., 2016; Dhingra et al., 2016; Sordoni et al., 2016; Shen et al., 2016). While state-of-the-art results have been achieved by these models, there has yet to be an in-depth analysis of the impact of the multiple-turn strategy to reasoning. This paper attempts to fill this gap. We provide empirical results and analysis on two challenging RC datasets: the Stanford Question Answering Dataset (SQuAD) (Rajpurkar et al., 2016), and the Microsoft Machine Reading We find that multiple-turn reasoning outperforms single-turn reasoning across the board for various types of question and answer types. Furthermore, the flexibility to dynamically decide the 957 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 957–966, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP query source answer (A) #questions (Q) #passages (P ) SQuAD crowdsourced span of words 100K questions 23K paragraphs MS MARCO user logs free-form text 100K queries 1M paragraphs MS MARCO: MS MARCO is a la"
I17-1096,D13-1020,0,0.0623584,"Missing"
I17-1096,D14-1181,0,0.00420097,"e aggregation layer uses co-attention to fuse information from the QP pair. The output layer is a recurrent net that maintains intermediate state and dynamically decides at which turn to generate the answer. 3.1 Detailed description of ReasoNet++ Embedding Layer: We adopt three types of embeddings to represent input word tokens in Q and P . For word embeddings, we use pre-trained GloVe vectors (Pennington et al., 2014). To address the out-of-vocabulary problem, we also include character and letter 3-gram embeddings. Character embeddings are fed into a convolutional neural network (CNN) as in (Kim, 2014), then max-pooled to form a fixed-size vector for each token. For letter 3-gram embeddings, we follow Huang et al. (2013) by first hashing each word as a bag of letter 3-gram, then feeding them into another CNN. The concatenation of all embeddings are then fed to a two-layer Highway Network (Srivastava et al., 2015). Therefore, we obtain the 959 Pi=m−1 p p H p , H q C q , i=0 H:i c are all 2d by n matrices, U is a 8d by n matrix. Finally, to incorporate the full context, the “memory cells” of the passage are computed by a bidirectional GRU on top of U : final embedding for the words in Q as a"
I17-1096,W04-1013,0,0.0155775,"e down answers in free-form text, and according to the authors of MS MARCO, the complexity of answer varies from a single “yes/no” or entity name (e.g. Q: “What is the capital of Italy”; A: Rome), to long textual answers (e.g. Q: “What is the agenda for Hollande’s state visit to Washington?”). Long textual answers may need to be derived through reasoning across multiple pieces of text. The dataset is partitioned into a 82,430 training, a 10,047 development, and 9,650 test tuples. Since the answer is free-form text, the evaluation metrics of choice are BLEU (Papineni et al., 2002) and ROUGE-L (Lin, 2004). To apply the same RC model to both SQuAD (where answers are text spans in P ) and MS MARCO (where answer are free-form text), we search for spans in MS MARCO’s passages that maximizes the ROUGEL score with the raw free-form answers. Our training data uses these spans as labels, but we evaluate our model with respect to the raw free-form answers; this has an upper bound of 94.23 BLEU and 87.53 ROUGE-L on the dev set. By this construction, there are multiple number of passages to read for each question, but the answer span might only involve a few passages (i.e. the ones that include the max R"
I17-1096,D16-1013,0,0.0289406,"Missing"
I17-1096,P02-1040,0,0.0987244,"on retrieval system. The judges write down answers in free-form text, and according to the authors of MS MARCO, the complexity of answer varies from a single “yes/no” or entity name (e.g. Q: “What is the capital of Italy”; A: Rome), to long textual answers (e.g. Q: “What is the agenda for Hollande’s state visit to Washington?”). Long textual answers may need to be derived through reasoning across multiple pieces of text. The dataset is partitioned into a 82,430 training, a 10,047 development, and 9,650 test tuples. Since the answer is free-form text, the evaluation metrics of choice are BLEU (Papineni et al., 2002) and ROUGE-L (Lin, 2004). To apply the same RC model to both SQuAD (where answers are text spans in P ) and MS MARCO (where answer are free-form text), we search for spans in MS MARCO’s passages that maximizes the ROUGEL score with the raw free-form answers. Our training data uses these spans as labels, but we evaluate our model with respect to the raw free-form answers; this has an upper bound of 94.23 BLEU and 87.53 ROUGE-L on the dev set. By this construction, there are multiple number of passages to read for each question, but the answer span might only involve a few passages (i.e. the one"
I17-2073,D16-1114,0,0.0135107,"ognitive disabilities and even machines. Approaches to lexical simplification generally follow a standard pipeline consisting of two main steps: generation and ranking. In the generation step, a set of possible substitutions for the target word is commonly created by querying semantic databases such as Wordnet (Devlin and Tait, 1998), learning substitution rules from sentence-aligned parallel corpora of complex-simple texts (Horn et al., 2014; Paetzold and Specia, 2017), and learning word embeddings from a large corpora to obtain similar words of ˇ the complex word (Glavaˇs and Stajner, 2015; Kim et al., 2016; Paetzold and Specia, 2016a, 2017). In the ranking step, features that discriminate a substitution candidate from other substitution candidates are leveraged and the candidates are ranked with respect to their simplicity and contextual fitness. 2 2.1 Method Task Definition We focus on the ranking step of the standard lexical simplification pipeline. Given a dataset of tar∗ This research was conducted while the first author was a Post Doctoral Fellow at the City University of Hong Kong. 430 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 430–435,"
I17-2073,N15-1092,1,0.925286,"e features at different levels of abstractions that capture nuances that discriminate the candidates. In this paper, we propose to use a Deep Structured Similarity Model (DSSM) (Huang et al., 2013) to rank substitution candidates. The DSSM exploits a deep architecture by using a deep neural network (DNN), that can effectively capture contextual features to perform semantic matching between two sentences. It has been successfully applied to several natural language processing (NLP) tasks, such as machine translation (Gao et al., 2014), web search ranking (Huang et al., 2013; Shen et al., 2014; Liu et al., 2015), question answering (Yih et al., 2014), and image captioning (Fang et al., 2015). To the best of our knowledge, this is the first time this model is applied to lexical simplification. We adapt the original DSSM architecture and objective function to our specific task. Our evaluation on two standard datasets for lexical simplification shows that this method outperforms state-of-the-art approaches that use supervised machine learning-based methods. We explore the application of a Deep Structured Similarity Model (DSSM) to ranking in lexical simplification. Our results show that the DSSM can eff"
I17-2073,E17-2006,0,0.126615,"riants, while retaining its meaning and grammaticality. The goal is to make the text easier to understand for children, language learners, people with cognitive disabilities and even machines. Approaches to lexical simplification generally follow a standard pipeline consisting of two main steps: generation and ranking. In the generation step, a set of possible substitutions for the target word is commonly created by querying semantic databases such as Wordnet (Devlin and Tait, 1998), learning substitution rules from sentence-aligned parallel corpora of complex-simple texts (Horn et al., 2014; Paetzold and Specia, 2017), and learning word embeddings from a large corpora to obtain similar words of ˇ the complex word (Glavaˇs and Stajner, 2015; Kim et al., 2016; Paetzold and Specia, 2016a, 2017). In the ranking step, features that discriminate a substitution candidate from other substitution candidates are leveraged and the candidates are ranked with respect to their simplicity and contextual fitness. 2 2.1 Method Task Definition We focus on the ranking step of the standard lexical simplification pipeline. Given a dataset of tar∗ This research was conducted while the first author was a Post Doctoral Fellow at"
I17-2073,P15-4015,0,0.322504,"he semantic representation of T and nonlinear projection W s constructs the semantic representation of S. Finally, the cosine similarity is adopted to measure the relevance between the T and S. At last, the posterior probabilities over all candidates are computed. T T Sq S Sq R(T, S) = cosine(T Sq , S Sq ) = T Sq S Sq (2) 2.3 Features for DSSM As baseline features, we use the same n-gram probability features as in Paetzold and Specia (2017), who also employ a neural network to rank substitution candidates. As in Paetzold and Specia (2017), the features were extracted using the SubIMDB corpus (Paetzold and Specia, 2015). We also experiment with additional features that have been reported as useful in this task. For each target word and a substitution candidate word we also compute: cosine similarity, word length, and alignment probability in the Compared to other latent semantic models, such as Latent Semantic Analysis (Deerwester et al., 1990) and its extensions, Deep Structured Similarity Model (also called Deep Semantic Similarity Model) or DSSM (Huang et al., 2013) can cap1 (1) 431 sentence-aligned Normal-Simple Wikipedia corpus (Kauchak, 2013). The cosine similarity feature is computed using the SubIMDB"
I17-2073,W16-4912,0,0.0240763,"ond, WA 98052, USA 3 Department of Linguistics and Translation, City University of Hong Kong kanash@wni.com, xiaodl@microsoft.com, jsylee@cityu.edu.hk Abstract The ranking step is challenging because the substitution candidates usually have similar meaning to the target word, and thus share similar context features. State-of-the-art approaches to ranking in lexical simplification exploit supervised machine learning-based methods that rely mostly on surface features, such as word frequency, word length and n-gram probability, for training the model (Horn et al., 2014; Bingel and Søgaard, 2016; Paetzold and Specia, 2016a, 2017). Moreover, deep architectures are not explored in these models. Surface features and shallow architectures might not be able to explore the features at different levels of abstractions that capture nuances that discriminate the candidates. In this paper, we propose to use a Deep Structured Similarity Model (DSSM) (Huang et al., 2013) to rank substitution candidates. The DSSM exploits a deep architecture by using a deep neural network (DNN), that can effectively capture contextual features to perform semantic matching between two sentences. It has been successfully applied to several n"
I17-2073,P14-1066,0,0.0272188,"ls. Surface features and shallow architectures might not be able to explore the features at different levels of abstractions that capture nuances that discriminate the candidates. In this paper, we propose to use a Deep Structured Similarity Model (DSSM) (Huang et al., 2013) to rank substitution candidates. The DSSM exploits a deep architecture by using a deep neural network (DNN), that can effectively capture contextual features to perform semantic matching between two sentences. It has been successfully applied to several natural language processing (NLP) tasks, such as machine translation (Gao et al., 2014), web search ranking (Huang et al., 2013; Shen et al., 2014; Liu et al., 2015), question answering (Yih et al., 2014), and image captioning (Fang et al., 2015). To the best of our knowledge, this is the first time this model is applied to lexical simplification. We adapt the original DSSM architecture and objective function to our specific task. Our evaluation on two standard datasets for lexical simplification shows that this method outperforms state-of-the-art approaches that use supervised machine learning-based methods. We explore the application of a Deep Structured Similarity Model (DSSM"
I17-2073,L16-1491,0,0.113327,"ond, WA 98052, USA 3 Department of Linguistics and Translation, City University of Hong Kong kanash@wni.com, xiaodl@microsoft.com, jsylee@cityu.edu.hk Abstract The ranking step is challenging because the substitution candidates usually have similar meaning to the target word, and thus share similar context features. State-of-the-art approaches to ranking in lexical simplification exploit supervised machine learning-based methods that rely mostly on surface features, such as word frequency, word length and n-gram probability, for training the model (Horn et al., 2014; Bingel and Søgaard, 2016; Paetzold and Specia, 2016a, 2017). Moreover, deep architectures are not explored in these models. Surface features and shallow architectures might not be able to explore the features at different levels of abstractions that capture nuances that discriminate the candidates. In this paper, we propose to use a Deep Structured Similarity Model (DSSM) (Huang et al., 2013) to rank substitution candidates. The DSSM exploits a deep architecture by using a deep neural network (DNN), that can effectively capture contextual features to perform semantic matching between two sentences. It has been successfully applied to several n"
I17-2073,P15-2011,0,0.0486796,"Missing"
I17-2073,Q15-1021,0,0.0324083,"mber 27 – December 1, 2017 2017 AFNLP get words, their sentential contexts and substitution candidates for the target words, the goal is to train a model that accurately ranks the candidates based on their simplicity and semantic matching. For generating substitution candidates, we utilize the method proposed by Paetzold and Specia (2017), which was recently shown to be the state-of-art method for generating substitution candidates. They exploit a hybrid substitution generation approach where candidates are first extracted from 550,644 simple-complex aligned sentences from the Newsela corpus (Xu et al., 2015). Then, these candidates are complemented with candidates generated with a retrofitted word embedding model. The word embedding model is retrofitted over WordNet’s synonym pairs (for details, please refer to Paetzold and Specia (2017)). For ranking substitution candidates, we use a DSSM, which we elaborate in the next section. 2.2 ture fine-grained local and global contextual features more effectively. The DSSM is trained by optimizing a similarity-driven objective, by projecting the whole sentence to a continuous semantic space. In addition, it is is built upon characters (rather than words)"
I17-2073,P14-2075,0,0.276677,"2 Microsoft Research, One Microsoft Way, Redmond, WA 98052, USA 3 Department of Linguistics and Translation, City University of Hong Kong kanash@wni.com, xiaodl@microsoft.com, jsylee@cityu.edu.hk Abstract The ranking step is challenging because the substitution candidates usually have similar meaning to the target word, and thus share similar context features. State-of-the-art approaches to ranking in lexical simplification exploit supervised machine learning-based methods that rely mostly on surface features, such as word frequency, word length and n-gram probability, for training the model (Horn et al., 2014; Bingel and Søgaard, 2016; Paetzold and Specia, 2016a, 2017). Moreover, deep architectures are not explored in these models. Surface features and shallow architectures might not be able to explore the features at different levels of abstractions that capture nuances that discriminate the candidates. In this paper, we propose to use a Deep Structured Similarity Model (DSSM) (Huang et al., 2013) to rank substitution candidates. The DSSM exploits a deep architecture by using a deep neural network (DNN), that can effectively capture contextual features to perform semantic matching between two sen"
I17-2073,P14-2105,0,0.0379564,"actions that capture nuances that discriminate the candidates. In this paper, we propose to use a Deep Structured Similarity Model (DSSM) (Huang et al., 2013) to rank substitution candidates. The DSSM exploits a deep architecture by using a deep neural network (DNN), that can effectively capture contextual features to perform semantic matching between two sentences. It has been successfully applied to several natural language processing (NLP) tasks, such as machine translation (Gao et al., 2014), web search ranking (Huang et al., 2013; Shen et al., 2014; Liu et al., 2015), question answering (Yih et al., 2014), and image captioning (Fang et al., 2015). To the best of our knowledge, this is the first time this model is applied to lexical simplification. We adapt the original DSSM architecture and objective function to our specific task. Our evaluation on two standard datasets for lexical simplification shows that this method outperforms state-of-the-art approaches that use supervised machine learning-based methods. We explore the application of a Deep Structured Similarity Model (DSSM) to ranking in lexical simplification. Our results show that the DSSM can effectively capture fine-grained features"
N15-1092,P14-1023,0,0.0130009,"milar concepts; e.g., feature generation, dimensionality reduction, and vector space models. The main motivation is similar: to abstract away from surface forms in words, sentences, or documents, in order to alleviate sparsity and approximate semantics. Traditional techniques include LSA (Deerwester et al., 1990), ESA (Gabrilovich and Markovitch, 2007), PCA (Karhunen, 1998), and non-linear kernel variants (Sch¨olkopf et al., 1998). Recently, learningbased approaches inspired by neural networks, especially DNNs, have gained in prominence, due to their favorable performance (Huang et al., 2013; Baroni et al., 2014; Milajevs et al., 2014). Popular methods for learning word representations include (Collobert et al., 2011; Mikolov et al., 2013c; Mnih and Kavukcuoglu, 2013; Pennington et al., 2014): all are based on unsupervised objec4 The trends differ slightly in the Nightlife domain. We believe this may be due to data bias on test data (only 298 samples). 919 tives of predicting words or word frequencies from raw text. End-to-end neural network models for specific tasks (e.g. parsing) often use these word representations as initialization, which are then iteratively improved by optimizing a supervised o"
N15-1092,D14-1082,0,0.226541,"odong Liu†∗, Jianfeng Gao‡ , Xiaodong He‡ , Li Deng‡ , Kevin Duh† and Ye-yi Wang‡ Nara Institute of Science and Technology, 8916-5 Takayama, Ikoma, Nara 630-0192, Japan ‡ Microsoft Research, One Microsoft Way, Redmond, WA 98052, USA xiaodong-l@is.naist.jp, {jfgao,xiaohe,deng}@microsoft.com kevinduh@is.naist.jp, yeyiwang@microsoft.com † Abstract representations learned from large corpora. Further, since these representations are usually in a lowdimensional vector space, they result in more compact models than those built from surface-form features. A recent successful example is the parser by (Chen and Manning, 2014), which is not only accurate but also fast. Methods of deep neural networks (DNNs) have recently demonstrated superior performance on a number of natural language processing tasks. However, in most previous work, the models are learned based on either unsupervised objectives, which does not directly optimize the desired task, or singletask supervised objectives, which often suffer from insufficient training data. We develop a multi-task DNN for learning representations across multiple tasks, not only leveraging large amounts of cross-task data, but also benefiting from a regularization effect"
N15-1092,P14-1066,1,0.29944,"n. We believe this may be due to data bias on test data (only 298 samples). 919 tives of predicting words or word frequencies from raw text. End-to-end neural network models for specific tasks (e.g. parsing) often use these word representations as initialization, which are then iteratively improved by optimizing a supervised objective (e.g. parsing accuracy). A selection of successful applications of this approach include sequence labeling (Turian et al., 2010), parsing (Chen and Manning, 2014), sentiment (Socher et al., 2013), question answering (Iyyer et al., 2014) and translation modeling (Gao et al., 2014a). Our model takes queries and documents as input, so it learns sentence/document representations. This is currently an open research question, the challenge being how to properly model semantic compositionality of words in vector space (Huang et al., 2013; M. Baroni and Zamparelli, 2013; Socher et al., 2013). While we adopt a bag-of-words approach for practical reasons (memory and run-time), our multi-task framework is extensible to other methods for sentence/document representations, such as those based on convolutional networks (Kalchbrenner et al., 2014; Shen et al., 2014; Gao et al., 201"
N15-1092,D14-1002,1,0.736534,"n. We believe this may be due to data bias on test data (only 298 samples). 919 tives of predicting words or word frequencies from raw text. End-to-end neural network models for specific tasks (e.g. parsing) often use these word representations as initialization, which are then iteratively improved by optimizing a supervised objective (e.g. parsing accuracy). A selection of successful applications of this approach include sequence labeling (Turian et al., 2010), parsing (Chen and Manning, 2014), sentiment (Socher et al., 2013), question answering (Iyyer et al., 2014) and translation modeling (Gao et al., 2014a). Our model takes queries and documents as input, so it learns sentence/document representations. This is currently an open research question, the challenge being how to properly model semantic compositionality of words in vector space (Huang et al., 2013; M. Baroni and Zamparelli, 2013; Socher et al., 2013). While we adopt a bag-of-words approach for practical reasons (memory and run-time), our multi-task framework is extensible to other methods for sentence/document representations, such as those based on convolutional networks (Kalchbrenner et al., 2014; Shen et al., 2014; Gao et al., 201"
N15-1092,D14-1070,0,0.0132056,"Missing"
N15-1092,P14-1062,0,0.00783704,"yyer et al., 2014) and translation modeling (Gao et al., 2014a). Our model takes queries and documents as input, so it learns sentence/document representations. This is currently an open research question, the challenge being how to properly model semantic compositionality of words in vector space (Huang et al., 2013; M. Baroni and Zamparelli, 2013; Socher et al., 2013). While we adopt a bag-of-words approach for practical reasons (memory and run-time), our multi-task framework is extensible to other methods for sentence/document representations, such as those based on convolutional networks (Kalchbrenner et al., 2014; Shen et al., 2014; Gao et al., 2014b), parse tree structure (Irsoy and Cardie, 2014), and run-time inference (Le and Mikolov, 2014). The synergy between multi-task learning and neural nets is quite natural; the general idea dates back to (Caruana, 1997). The main challenge is in designing the tasks and the network structure. For example, (Collobert et al., 2011) defined part-of-speech tagging, chunking, and named entity recognition as multiple tasks in a single sequence labeler; (Bordes et al., 2012) defined multiple data sources as tasks in their relation extraction system. While conceptual"
N15-1092,N13-1090,0,0.587375,"g large amounts of cross-task data, but also benefiting from a regularization effect that leads to more general representations to help tasks in new domains. Our multi-task DNN approach combines tasks of multiple-domain classification (for query classification) and information retrieval (ranking for web search), and demonstrates significant gains over strong baselines in a comprehensive set of domain adaptation. 1 However, existing vector-space representation learning methods are far from optimal. Most previous methods are based on unsupervised objectives such as word prediction for training (Mikolov et al., 2013c; Pennington et al., 2014). Other methods use supervised training objectives on a single task, e.g. (Socher et al., 2013), and thus are often constrained by limited amounts of training data. Motivated by the success of multi-task learning (Caruana, 1997), we propose in this paper a multi-task DNN approach for representation learning that leverages supervised data from many tasks. In addition to the benefit of having more data for training, the use of multi-task also profits from a regularization effect, i.e., reducing overfitting to a specific task, thus making the learned representations uni"
N15-1092,D14-1079,0,0.0296496,"feature generation, dimensionality reduction, and vector space models. The main motivation is similar: to abstract away from surface forms in words, sentences, or documents, in order to alleviate sparsity and approximate semantics. Traditional techniques include LSA (Deerwester et al., 1990), ESA (Gabrilovich and Markovitch, 2007), PCA (Karhunen, 1998), and non-linear kernel variants (Sch¨olkopf et al., 1998). Recently, learningbased approaches inspired by neural networks, especially DNNs, have gained in prominence, due to their favorable performance (Huang et al., 2013; Baroni et al., 2014; Milajevs et al., 2014). Popular methods for learning word representations include (Collobert et al., 2011; Mikolov et al., 2013c; Mnih and Kavukcuoglu, 2013; Pennington et al., 2014): all are based on unsupervised objec4 The trends differ slightly in the Nightlife domain. We believe this may be due to data bias on test data (only 298 samples). 919 tives of predicting words or word frequencies from raw text. End-to-end neural network models for specific tasks (e.g. parsing) often use these word representations as initialization, which are then iteratively improved by optimizing a supervised objective (e.g. parsing a"
N15-1092,D14-1162,0,0.118051,"s-task data, but also benefiting from a regularization effect that leads to more general representations to help tasks in new domains. Our multi-task DNN approach combines tasks of multiple-domain classification (for query classification) and information retrieval (ranking for web search), and demonstrates significant gains over strong baselines in a comprehensive set of domain adaptation. 1 However, existing vector-space representation learning methods are far from optimal. Most previous methods are based on unsupervised objectives such as word prediction for training (Mikolov et al., 2013c; Pennington et al., 2014). Other methods use supervised training objectives on a single task, e.g. (Socher et al., 2013), and thus are often constrained by limited amounts of training data. Motivated by the success of multi-task learning (Caruana, 1997), we propose in this paper a multi-task DNN approach for representation learning that leverages supervised data from many tasks. In addition to the benefit of having more data for training, the use of multi-task also profits from a regularization effect, i.e., reducing overfitting to a specific task, thus making the learned representations universal across tasks. Introd"
N15-1092,D13-1170,0,0.00800131,"tions to help tasks in new domains. Our multi-task DNN approach combines tasks of multiple-domain classification (for query classification) and information retrieval (ranking for web search), and demonstrates significant gains over strong baselines in a comprehensive set of domain adaptation. 1 However, existing vector-space representation learning methods are far from optimal. Most previous methods are based on unsupervised objectives such as word prediction for training (Mikolov et al., 2013c; Pennington et al., 2014). Other methods use supervised training objectives on a single task, e.g. (Socher et al., 2013), and thus are often constrained by limited amounts of training data. Motivated by the success of multi-task learning (Caruana, 1997), we propose in this paper a multi-task DNN approach for representation learning that leverages supervised data from many tasks. In addition to the benefit of having more data for training, the use of multi-task also profits from a regularization effect, i.e., reducing overfitting to a specific task, thus making the learned representations universal across tasks. Introduction Recent advances in deep neural networks (DNNs) have demonstrated the importance of learn"
N15-1092,P10-1040,0,0.0115602,"et al., 2013c; Mnih and Kavukcuoglu, 2013; Pennington et al., 2014): all are based on unsupervised objec4 The trends differ slightly in the Nightlife domain. We believe this may be due to data bias on test data (only 298 samples). 919 tives of predicting words or word frequencies from raw text. End-to-end neural network models for specific tasks (e.g. parsing) often use these word representations as initialization, which are then iteratively improved by optimizing a supervised objective (e.g. parsing accuracy). A selection of successful applications of this approach include sequence labeling (Turian et al., 2010), parsing (Chen and Manning, 2014), sentiment (Socher et al., 2013), question answering (Iyyer et al., 2014) and translation modeling (Gao et al., 2014a). Our model takes queries and documents as input, so it learns sentence/document representations. This is currently an open research question, the challenge being how to properly model semantic compositionality of words in vector space (Huang et al., 2013; M. Baroni and Zamparelli, 2013; Socher et al., 2013). While we adopt a bag-of-words approach for practical reasons (memory and run-time), our multi-task framework is extensible to other meth"
N15-1092,2014.lilt-9.5,0,\N,Missing
N19-1021,W14-3346,0,0.0226809,"choosing a college, and the model learns to generate responses from Caller Bob. The cyclical schedule generated highly diverse answers that cover multiConditional VAE for Dialog We use a cyclical schedule to improve the latent codes in (Zhao et al., 2017), which are key to diverse dialog-response generation. Follow247 ple plausible dialog acts. On the contrary, the responses from the monotonic schedule are limited to repeat plain responses, i.e., “i’m not sure”. Quantitative results are shown in Table 3, using the evaluation metrics from (Zhao et al., 2017). (i) Smoothed Sentence-level BLEU (Chen and Cherry, 2014): BLEU is a popular metric that measures the geometric mean of modified n-gram precision with a length penalty. We use BLEU-1 to 4 as our lexical similarity metric and normalize the score to 0 to 1 scale. (ii) Cosine Distance of Bag-of-word Embedding (Liu et al., 2016): a simple method to obtain sentence embeddings is to take the average or extreme of all the word embeddings in the sentences. We used Glove embedding and denote the average method as A bow and extreme method as E bow. The score is normalized to [0, 1]. Higher values indicate more plausible responses. The BoW indeed reduces the K"
N19-1021,D16-1230,0,0.0326472,"diverse dialog-response generation. Follow247 ple plausible dialog acts. On the contrary, the responses from the monotonic schedule are limited to repeat plain responses, i.e., “i’m not sure”. Quantitative results are shown in Table 3, using the evaluation metrics from (Zhao et al., 2017). (i) Smoothed Sentence-level BLEU (Chen and Cherry, 2014): BLEU is a popular metric that measures the geometric mean of modified n-gram precision with a length penalty. We use BLEU-1 to 4 as our lexical similarity metric and normalize the score to 0 to 1 scale. (ii) Cosine Distance of Bag-of-word Embedding (Liu et al., 2016): a simple method to obtain sentence embeddings is to take the average or extreme of all the word embeddings in the sentences. We used Glove embedding and denote the average method as A bow and extreme method as E bow. The score is normalized to [0, 1]. Higher values indicate more plausible responses. The BoW indeed reduces the KL vanishing issue, as indicated by the increased KL and decreased reconstruction perplexity. When applying the proposed cyclical schedule to CVAE, we also see a reduced KL vanishing issue. Interestingly, it also yields the highest BLEU scores. This suggests that the cy"
N19-1021,J93-2004,0,0.0651737,"rted. Since SA-VAE tends to overfit, we report its best results in row M⇤ . cal schedule (while keeping all other settings the same). The default hyper-parameters of the cyclical schedule are used in all cases unless stated otherwise. We study the impact of hyper-parameters in the SM, and show that larger M can provide higher performance for various R. We show the major results in this section, and put more details in the SM. The monotonic and cyclical schedules are denoted as M and C, respectively. 6.1 Language Modeling We first consider language modeling on the Penn Tree Bank (PTB) dataset (Marcus et al., 1993). Language modeling with VAEs has been a challenging problem, and few approaches have been shown to produce rich generative models that do not collapse to standard language models. Ideally a deep generative model trained with variational inference would pursue higher ELBO, making use of the latent space (i.e., maintain a nonzero KL term) while accurately modeling the underlying distribution (i.e., lower reconstruction errors). We implemented different schedules based on the code4 published by Kim et al. (2018). The latent variable is 32-dimensional, and 40 epochs are used. We compare the propo"
N19-1021,D16-1031,0,0.04067,"cycles as warm re-starts. The effectiveness of cyclical annealing is validated on a broad range of NLP tasks, including language modeling, dialog response generation and unsupervised language pre-training. 1 Introduction Variational autoencoders (VAEs) (Kingma and Welling, 2013; Rezende et al., 2014) have been applied in many NLP tasks, including language modeling (Bowman et al., 2015; Miao et al., 2016), dialog response generation (Zhao et al., 2017; Wen et al., 2017), semi-supervised text classification (Xu et al., 2017), controllable text generation (Hu et al., 2017), and text compression (Miao and Blunsom, 2016). A prominent component of a VAE is the distribution-based latent representation for text sequence observations. This flexible representation allows the VAE to explicitly model holistic properties of sentences, such as style, topic, and high-level linguistic and semantic features. Samples from the prior latent distribution can produce ⇤ Corresponding author † Equal Contribution 240 Proceedings of NAACL-HLT 2019, pages 240–250 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics vanishing issue, and develop an understanding of the strengths and weaknes"
N19-1021,P17-1061,0,0.139526,"med sentences through simple deterministic decoding (Bowman et al., 2015). Due to the sequential nature of text, an autoregressive decoder is typically employed in the VAE. This is often implemented with a recurrent neural network (RNN); the long short-term memory (LSTM) (Hochreiter and Schmidhuber, 1997) RNN is used widely. This introduces one notorious issue when a VAE is trained using traditional methods: the decoder ignores the latent variable, yielding what is termed the KL vanishing problem. Several attempts have been made to ameliorate this issue (Yang et al., 2017; Dieng et al., 2018; Zhao et al., 2017; Kim et al., 2018). Among them, perhaps the simplest solution is monotonic KL annealing, where the weight of the KL penalty term is scheduled to gradually increase during training (Bowman et al., 2015). While these techniques can effectively alleviate the KL-vanishing issue, a proper unified theoretical interpretation is still lacking, even for the simple annealing scheme. In this paper, we analyze the variable dependency in a VAE, and point out that the autoregressive decoder has two paths (formally defined in Section 3.1) that work together to generate text sequences. One path is conditione"
N19-1094,D15-1075,0,0.0312843,"onent of natural language understanding, commonsense reasoning has been included in an increasing number of tasks for evaluation: COPA (Roemmele et al., 2011) assesses commonsense causal reasoning by selecting an alternative, which has a more plausible causal relation with the given premise. Story Cloze Test (ROCStories, Mostafazadeh et al. 2016) evaluates story understanding, story generation, and script learning by choosing the most sensible ending to a short story. JOCI (Zhang et al., 2017) generalizes the natural language inference (NLI) framework (Cooper et al., 1996; Dagan et al., 2006; Bowman et al., 2015; Williams et al., 2018) and evaluates commonsense inference by predicting the ordinal likelihood of a hypothesis given a context. Event2Mind (Rashkin et al., 2018b) models stereotypical intents and reactions of people, described in short free-form text. SWAG (Zellers et al., 2018) frames commonsense inference as multiple-choice questions for follow-up events given some context. ReCoRD (Zhang et al., 2018) 1 We don’t believe it is possible to construct such a knowledge base given that the world is changing constantly. 2 DSSMs can be applied to a wide range of tasks depending on the definition"
N19-1094,D16-1245,0,0.0172502,"ith candidates. To make a fair comparison to Trinh and Le (2018)’s work, we also train our single model on the corpus of Gutenberg only. We can see that both of our methods get significant improvement on the PDP dataset, and our UDSSM-II can achieve much better performance on the WSC dataset. We also report our ensemble model (nine models with different hyperparameters) trained with both corpus of Gutenberg and 1 Billion Word, and it also achieve better performance than Google Language Model trained with the same corpus. Finally, we also compare to the pre-trained Coreference Resolution Tool (Clark and Manning, 2016a,b)12 , and we can see that it doesn’t adapt to our commonsense reasoning tasks and can’t tell 4.4 Analysis WSC 1: Ours 1: WSC 2: Ours 2: Paul tried to call George on the phone, but he wasn’t successful. He tried to call 911 using her cell phone but that he could n’t get the phone to work. Paul tried to call George on the phone, but he was n’t available . He tried twice to call her but she did not answer the phone . Table 3: Comparison of the data from WSC and our training data. Our sentences are retrieved from the UDSSM-II training dataset based on the BM25 value for analysis. The pseudo lab"
N19-1094,P16-1061,0,0.0223517,"ith candidates. To make a fair comparison to Trinh and Le (2018)’s work, we also train our single model on the corpus of Gutenberg only. We can see that both of our methods get significant improvement on the PDP dataset, and our UDSSM-II can achieve much better performance on the WSC dataset. We also report our ensemble model (nine models with different hyperparameters) trained with both corpus of Gutenberg and 1 Billion Word, and it also achieve better performance than Google Language Model trained with the same corpus. Finally, we also compare to the pre-trained Coreference Resolution Tool (Clark and Manning, 2016a,b)12 , and we can see that it doesn’t adapt to our commonsense reasoning tasks and can’t tell 4.4 Analysis WSC 1: Ours 1: WSC 2: Ours 2: Paul tried to call George on the phone, but he wasn’t successful. He tried to call 911 using her cell phone but that he could n’t get the phone to work. Paul tried to call George on the phone, but he was n’t available . He tried twice to call her but she did not answer the phone . Table 3: Comparison of the data from WSC and our training data. Our sentences are retrieved from the UDSSM-II training dataset based on the BM25 value for analysis. The pseudo lab"
N19-1094,P18-1043,0,0.170671,"(Miller, 1995) for external supervision to train word embeddings and solve the WSC challenge. Recently, Trinh and Le (2018) first used raw text from books/news to train a neural Language Model (LM), and then emIntroduction Commonsense reasoning is concerned with simulating the human ability to make presumptions about the type and essence of ordinary situations they encounter every day (Davis and Marcus, 2015). It is one of the key challenges in natural language understanding, and has drawn increasing attention in recent years (Levesque et al., 2011; Roemmele et al., 2011; Zhang et al., 2017; Rashkin et al., 2018a,b; Zellers et al., 2018; Trinh and Le, 2018). However, due to the lack of labeled training data or comprehensive hand-crafted knowledge bases, commonsense reasoning tasks such as Winograd Schema Challenge (Levesque et al., 2011) are still far from being solved. In this work, we propose two effective unsupervised models for commonsense reasoning, and evaluate them on two classic commonsense reasoning tasks: Winograd Schema Challenge (WSC) and Pronoun Disambiguation Problems (PDP). Compared to other commonsense reasoning tasks, ∗ B. The demonstraWork done when the author was at Microsoft 882 P"
N19-1094,N16-1098,0,0.0368582,"different types of pairwise training data, thus the model structures are different, as illustrated in Figure 1(b) and 1(c), respectively. Experiments demonstrated that our methods outperform stat-of-the-art performance on the tasks of WSC and PDP. 2 Related Work As a key component of natural language understanding, commonsense reasoning has been included in an increasing number of tasks for evaluation: COPA (Roemmele et al., 2011) assesses commonsense causal reasoning by selecting an alternative, which has a more plausible causal relation with the given premise. Story Cloze Test (ROCStories, Mostafazadeh et al. 2016) evaluates story understanding, story generation, and script learning by choosing the most sensible ending to a short story. JOCI (Zhang et al., 2017) generalizes the natural language inference (NLI) framework (Cooper et al., 1996; Dagan et al., 2006; Bowman et al., 2015; Williams et al., 2018) and evaluates commonsense inference by predicting the ordinal likelihood of a hypothesis given a context. Event2Mind (Rashkin et al., 2018b) models stereotypical intents and reactions of people, described in short free-form text. SWAG (Zellers et al., 2018) frames commonsense inference as multiple-choic"
N19-1094,J01-4004,0,0.563139,"In the following sections, we will use uppercase symbols in bold, e.g., Sx , to represent matrices. Lowercase symbols in bold, e.g., hx , represent vectors. A regular uppercase symbol, e.g., S x , represents a lexical sequence. A regular lowercase symbol, e.g., xi or y, represents a token. Among all these commonsense reasoning tasks, the Winograd Schema Challenge (WSC) and Pronoun Disambiguation Problems (PDP) (Levesque et al., 2011) are known as the most challenging tasks for commonsense reasoning. Although both tasks are based on pronoun disambiguation, a subtask of coreference resolution (Soon et al., 2001; Ng and Cardie, 2002; Peng et al., 2016), PDP and WSC differ from normal pronoun disambiguation due to their unique properties, which are based on commonsense, selecting the most likely antecedent from both candidates in the directly preceding context. Previous efforts on solving the Winograd Schema Challenge and Pronoun Disambiguation Problems mostly rely on human-labeled data, sophisticated rules, hand-crafted features, or external knowledge bases (Peng et al., 2015; Bailey et al., 2015; Sch¨uller, 2014). Rahman and Ng (2012) hired workers to annotate supervised training data and designed 7"
N19-1094,P02-1014,0,0.464835,"ections, we will use uppercase symbols in bold, e.g., Sx , to represent matrices. Lowercase symbols in bold, e.g., hx , represent vectors. A regular uppercase symbol, e.g., S x , represents a lexical sequence. A regular lowercase symbol, e.g., xi or y, represents a token. Among all these commonsense reasoning tasks, the Winograd Schema Challenge (WSC) and Pronoun Disambiguation Problems (PDP) (Levesque et al., 2011) are known as the most challenging tasks for commonsense reasoning. Although both tasks are based on pronoun disambiguation, a subtask of coreference resolution (Soon et al., 2001; Ng and Cardie, 2002; Peng et al., 2016), PDP and WSC differ from normal pronoun disambiguation due to their unique properties, which are based on commonsense, selecting the most likely antecedent from both candidates in the directly preceding context. Previous efforts on solving the Winograd Schema Challenge and Pronoun Disambiguation Problems mostly rely on human-labeled data, sophisticated rules, hand-crafted features, or external knowledge bases (Peng et al., 2015; Bailey et al., 2015; Sch¨uller, 2014). Rahman and Ng (2012) hired workers to annotate supervised training data and designed 70K hand-crafted featu"
N19-1094,N15-1082,0,0.0589557,"tasks for commonsense reasoning. Although both tasks are based on pronoun disambiguation, a subtask of coreference resolution (Soon et al., 2001; Ng and Cardie, 2002; Peng et al., 2016), PDP and WSC differ from normal pronoun disambiguation due to their unique properties, which are based on commonsense, selecting the most likely antecedent from both candidates in the directly preceding context. Previous efforts on solving the Winograd Schema Challenge and Pronoun Disambiguation Problems mostly rely on human-labeled data, sophisticated rules, hand-crafted features, or external knowledge bases (Peng et al., 2015; Bailey et al., 2015; Sch¨uller, 2014). Rahman and Ng (2012) hired workers to annotate supervised training data and designed 70K hand-crafted features. Sharma et al. (2015); Sch¨uller (2014); Bailey et al. (2015); Liu et al. (2017) utilized expensive knowledge bases in their reasoning processes. Recently, Trinh and Le 2018 applied neural language models trained with a massive amount of unlabeled data to the Winograd Schema Challenge and improved the performance by a large margin. In contrast, our unsupervised method based on DSSM significantly outperforms the previous state-of-theart method,"
N19-1094,N18-1101,0,0.0235238,"uage understanding, commonsense reasoning has been included in an increasing number of tasks for evaluation: COPA (Roemmele et al., 2011) assesses commonsense causal reasoning by selecting an alternative, which has a more plausible causal relation with the given premise. Story Cloze Test (ROCStories, Mostafazadeh et al. 2016) evaluates story understanding, story generation, and script learning by choosing the most sensible ending to a short story. JOCI (Zhang et al., 2017) generalizes the natural language inference (NLI) framework (Cooper et al., 1996; Dagan et al., 2006; Bowman et al., 2015; Williams et al., 2018) and evaluates commonsense inference by predicting the ordinal likelihood of a hypothesis given a context. Event2Mind (Rashkin et al., 2018b) models stereotypical intents and reactions of people, described in short free-form text. SWAG (Zellers et al., 2018) frames commonsense inference as multiple-choice questions for follow-up events given some context. ReCoRD (Zhang et al., 2018) 1 We don’t believe it is possible to construct such a knowledge base given that the world is changing constantly. 2 DSSMs can be applied to a wide range of tasks depending on the definition of (x, y). For example,"
N19-1094,D16-1038,0,0.0539894,"uppercase symbols in bold, e.g., Sx , to represent matrices. Lowercase symbols in bold, e.g., hx , represent vectors. A regular uppercase symbol, e.g., S x , represents a lexical sequence. A regular lowercase symbol, e.g., xi or y, represents a token. Among all these commonsense reasoning tasks, the Winograd Schema Challenge (WSC) and Pronoun Disambiguation Problems (PDP) (Levesque et al., 2011) are known as the most challenging tasks for commonsense reasoning. Although both tasks are based on pronoun disambiguation, a subtask of coreference resolution (Soon et al., 2001; Ng and Cardie, 2002; Peng et al., 2016), PDP and WSC differ from normal pronoun disambiguation due to their unique properties, which are based on commonsense, selecting the most likely antecedent from both candidates in the directly preceding context. Previous efforts on solving the Winograd Schema Challenge and Pronoun Disambiguation Problems mostly rely on human-labeled data, sophisticated rules, hand-crafted features, or external knowledge bases (Peng et al., 2015; Bailey et al., 2015; Sch¨uller, 2014). Rahman and Ng (2012) hired workers to annotate supervised training data and designed 70K hand-crafted features. Sharma et al. ("
N19-1094,D18-1009,0,0.130991,"nal supervision to train word embeddings and solve the WSC challenge. Recently, Trinh and Le (2018) first used raw text from books/news to train a neural Language Model (LM), and then emIntroduction Commonsense reasoning is concerned with simulating the human ability to make presumptions about the type and essence of ordinary situations they encounter every day (Davis and Marcus, 2015). It is one of the key challenges in natural language understanding, and has drawn increasing attention in recent years (Levesque et al., 2011; Roemmele et al., 2011; Zhang et al., 2017; Rashkin et al., 2018a,b; Zellers et al., 2018; Trinh and Le, 2018). However, due to the lack of labeled training data or comprehensive hand-crafted knowledge bases, commonsense reasoning tasks such as Winograd Schema Challenge (Levesque et al., 2011) are still far from being solved. In this work, we propose two effective unsupervised models for commonsense reasoning, and evaluate them on two classic commonsense reasoning tasks: Winograd Schema Challenge (WSC) and Pronoun Disambiguation Problems (PDP). Compared to other commonsense reasoning tasks, ∗ B. The demonstraWork done when the author was at Microsoft 882 Proceedings of NAACL-HLT 2"
N19-1094,N18-1202,0,0.0325233,"lassification model based on whether the word pairs appear in the knowledge base. One limitation of these methods is that they rely heavily on the external knowledge bases. Another limitation is that they just linearly aggregate the embeddings of the words in the context, and that’s hard to integrate the word order information. Instead, our model with LSTM can better represent the contextual information. Besides, our model don’t need any external knowledge bases, and achieve a significant improvement on both of the datasets. We further compare our models with the unsupervised baselines, ELMo (Peters et al., 2018) which selects the candidate based on the cosine similarity of the hidden states of noun and pronoun. Another unsupervised baseline, Google Language Model for commonsense reasoning (Trinh and Le, 2018), which compares the perplexities of the new sentences by replacing the pronoun with candidates. To make a fair comparison to Trinh and Le (2018)’s work, we also train our single model on the corpus of Gutenberg only. We can see that both of our methods get significant improvement on the PDP dataset, and our UDSSM-II can achieve much better performance on the WSC dataset. We also report our ensem"
N19-1094,D12-1071,0,0.431322,"ased on pronoun disambiguation, a subtask of coreference resolution (Soon et al., 2001; Ng and Cardie, 2002; Peng et al., 2016), PDP and WSC differ from normal pronoun disambiguation due to their unique properties, which are based on commonsense, selecting the most likely antecedent from both candidates in the directly preceding context. Previous efforts on solving the Winograd Schema Challenge and Pronoun Disambiguation Problems mostly rely on human-labeled data, sophisticated rules, hand-crafted features, or external knowledge bases (Peng et al., 2015; Bailey et al., 2015; Sch¨uller, 2014). Rahman and Ng (2012) hired workers to annotate supervised training data and designed 70K hand-crafted features. Sharma et al. (2015); Sch¨uller (2014); Bailey et al. (2015); Liu et al. (2017) utilized expensive knowledge bases in their reasoning processes. Recently, Trinh and Le 2018 applied neural language models trained with a massive amount of unlabeled data to the Winograd Schema Challenge and improved the performance by a large margin. In contrast, our unsupervised method based on DSSM significantly outperforms the previous state-of-theart method, with the advantage of capturing more contextual information i"
N19-1094,P18-1213,0,0.0882579,"(Miller, 1995) for external supervision to train word embeddings and solve the WSC challenge. Recently, Trinh and Le (2018) first used raw text from books/news to train a neural Language Model (LM), and then emIntroduction Commonsense reasoning is concerned with simulating the human ability to make presumptions about the type and essence of ordinary situations they encounter every day (Davis and Marcus, 2015). It is one of the key challenges in natural language understanding, and has drawn increasing attention in recent years (Levesque et al., 2011; Roemmele et al., 2011; Zhang et al., 2017; Rashkin et al., 2018a,b; Zellers et al., 2018; Trinh and Le, 2018). However, due to the lack of labeled training data or comprehensive hand-crafted knowledge bases, commonsense reasoning tasks such as Winograd Schema Challenge (Levesque et al., 2011) are still far from being solved. In this work, we propose two effective unsupervised models for commonsense reasoning, and evaluate them on two classic commonsense reasoning tasks: Winograd Schema Challenge (WSC) and Pronoun Disambiguation Problems (PDP). Compared to other commonsense reasoning tasks, ∗ B. The demonstraWork done when the author was at Microsoft 882 P"
N19-1094,Q17-1027,1,\N,Missing
N19-1271,D11-1033,1,0.731321,"and add to S 6: Assign mini-batches in S in a random order to obtain a sequence B = (b1 , ..., bL ), where L = N1 + bαN1 c 7: for each mini-batch b ∈ B do 8: Perform P gradient update on M with loss l(b) = (Q,P,A)∈b l(Q, P, A) 9: end for 10: Evaluate development set performance 11: end for Output: Model with best evaluation performance the ratio gives the same weight to every auxiliary data, but the relevance of every data point to the target task can vary greatly. We develop a novel re-weighting method to resolve these problems, using ideas inspired by data selection in machine translation (Axelrod et al., 2011; van der Wees et al., 2017). We use (Qk , P k , Ak ) to represent a data point from the kth task for 1 ≤ k ≤ K, with k = 1 being the target task. Since the passage styles are hard to evaluate, we only evaluate data points based on Qk and Ak . Note that only data from auxiliary task (2 ≤ k ≤ K) is re-weighted; target task data always have weight 1. Our scores consist of two parts, one for questions and one for answers. For questions, we create language models (detailed in Section 5.2) using questions from each task, which we represent as LMk for the k-th task. For each question Qk from auxilia"
N19-1271,P17-1171,0,0.0270339,"g questions on multi-task learning for MRC: 1. Can we improve the performance of existing MRC systems using multi-task learning? 2. How does multi-task learning affect the performance if we combine it with other external data? 3. How does the learning algorithm change the performance of multi-task MRC? 4. How does our method compare with existing MTL methods? We first present our experiment details and results for MT-SAN. Then, we provide a comprehensive study on the effectiveness of various MTL algorithms in Section 5.4. At last, we provide some additional results on combining MTL with DrQA (Chen et al., 2017) to show the flexibility of our approach 1 . 5.1 Datasets We conducted experiments on SQuAD (Rajpurkar et al., 2016), NewsQA(Trischler et al., 2017), MS MARCO (v1, Nguyen et al.,2016) and WDW (Onishi et al., 2016). Dataset statistics is shown in Table 1. Although similar in size, these datasets are quite different in domains, lengths of text, and types of task. In the following experiments, we will validate whether including external datasets as additional input information (e.g., pre-trained language model on these datasets) helps boost the performance of MRC systems. 1 We include the results"
N19-1271,W14-4012,0,0.0611353,"Missing"
N19-1271,D17-1087,0,0.0712978,"ain from MTL can be easily combined with existing methods to obtain further performance gain. Third, the proposed sampling and re-weighting scheme can further improve the multi-task learning performance. 2 Related Work Studies in machine reading comprehension mostly focus on architecture design of neural networks, such as bidirectional attention (Seo et al., 2016), dynamic reasoning (Xu et al., 2017), and parallelization (Yu et al., 2018). Some recent work has explored transfer learning that leverages outdomain data to learn MRC models when no training data is available for the target domain (Golub et al., 2017). In this work, we explore multi-task learning to make use of the data from other domains, while we still have access to target domain training data. Multi-task learning (Caruana, 1997) has been widely used in machine learning to improve generalization using data from multiple tasks. For natural language processing, MTL has been successfully applied to low-level parsing tasks (Collobert et al., 2011), sequence-to-sequence learning (Luong et al., 2015), and web search (Liu et al., 2015). More recently, (McCann et al., 2018) proposes to cast all tasks from parsing to translation as a QA problem"
N19-1271,P16-1086,0,0.0659381,"Missing"
N19-1271,N15-1092,1,0.779501,"that leverages outdomain data to learn MRC models when no training data is available for the target domain (Golub et al., 2017). In this work, we explore multi-task learning to make use of the data from other domains, while we still have access to target domain training data. Multi-task learning (Caruana, 1997) has been widely used in machine learning to improve generalization using data from multiple tasks. For natural language processing, MTL has been successfully applied to low-level parsing tasks (Collobert et al., 2011), sequence-to-sequence learning (Luong et al., 2015), and web search (Liu et al., 2015). More recently, (McCann et al., 2018) proposes to cast all tasks from parsing to translation as a QA problem and use a single network to solve all of them. However, their results show that multi-task learning hurts the performance of most tasks when tackling them together. Differently, we focus on applying MTL to the MRC task and show significant improvement over single-task baselines. Our sample re-weighting scheme bears some resemblance to previous MTL techniques that assign weights to tasks (Kendall et al., 2018). However, our method gives a more granular score for each sample and provides"
N19-1271,P19-1441,1,0.887037,"Missing"
N19-1271,P18-1157,1,0.49377,"ion (MRC) has gained growing interest in the research community (Rajpurkar et al., 2016; Yu et al., 2018). In an MRC task, the machine reads a text passage and a question, and generates (or selects) an answer based on the passage. This requires the machine to possess strong comprehension, inference and reasoning capabilities. Over the past few years, there has been much progress in building end-toend neural network models (Seo et al., 2016) for MRC. However, most public MRC datasets (e.g., SQuAD, MS MARCO, TriviaQA) are typically small (less than 100K) compared to the model size (such as SAN (Liu et al., 2018c,b) with around 10M parameters). To prevent over-fitting, recently there have been some studies on using pre-trained word embeddings (Pennington et al., 2014) and contextual embeddings in the MRC model training, as well as back-translation approaches (Yu et al., 2018) for data augmentation. ∗ Most of this work was performed when the author was interning at Microsoft. Multi-task learning (Caruana, 1997) is a widely studied area in machine learning, aiming at better model generalization by combining training datasets from multiple tasks. In this work, we explore a multi-task learning (MTL) fram"
N19-1271,D16-1241,0,0.131818,"d with more flexibility on various MRC tasks. MTL is also faster and easier to train than embedding/LM methods: our approach requires no pre-trained models, whereas back translation and ELMo both rely on large models that would need days to train on multiple GPUs (Jozefowicz et al., 2016; Peters et al., 2018). We validate our MTL framework with two state-of-the-art models on four datasets from different domains. Experiments show that our methods lead to a significant performance gain over single-task baselines on SQuAD (Rajpurkar et al., 2016), NewsQA (Trischler et al., 2017) and WhoDid-What (Onishi et al., 2016), while achieving state-of-the-art performance on the latter two. For example, on NewsQA (Trischler et al., 2017), our model surpassed human performance by 13.4 (46.5 vs 59.9) and 3.2 (72.6 vs 69.4) absolute points in terms of exact match and F1. The contribution of this work is three-fold. First, we apply multi-task learning to the MRC task, which brings significant improvements over single-task baselines. Second, the performance gain from MTL can be easily combined with existing methods to obtain further performance gain. Third, the proposed sampling and re-weighting scheme can further impro"
N19-1271,D14-1162,0,0.0943077,"sage and a question, and generates (or selects) an answer based on the passage. This requires the machine to possess strong comprehension, inference and reasoning capabilities. Over the past few years, there has been much progress in building end-toend neural network models (Seo et al., 2016) for MRC. However, most public MRC datasets (e.g., SQuAD, MS MARCO, TriviaQA) are typically small (less than 100K) compared to the model size (such as SAN (Liu et al., 2018c,b) with around 10M parameters). To prevent over-fitting, recently there have been some studies on using pre-trained word embeddings (Pennington et al., 2014) and contextual embeddings in the MRC model training, as well as back-translation approaches (Yu et al., 2018) for data augmentation. ∗ Most of this work was performed when the author was interning at Microsoft. Multi-task learning (Caruana, 1997) is a widely studied area in machine learning, aiming at better model generalization by combining training datasets from multiple tasks. In this work, we explore a multi-task learning (MTL) framework to enable the training of one universal model across different MRC tasks for better generalization. Intuitively, this multi-task MRC model can be viewed"
N19-1271,N18-1202,0,0.314219,"pose two MTL training algorithms to improve the performance. The first method simply adopts a sampling scheme, which randomly selects training data from the auxiliary tasks controlled by a ratio hyperparameter; The second algorithm incorporates recent ideas of data selection in machine translation (van der Wees et al., 2017). It learns the sample weights from the auxiliary tasks automatically through language models. Prior to this work, many studies have used upstream datasets to augment the performance of MRC models, including word embedding (Pennington et al., 2014), language models (ELMo) (Peters et al., 2018) and machine translation (Yu et al., 2018). These methods aim to obtain a robust semantic encoding of both passages and questions. Our MTL method is orthogonal to these methods: rather than enriching semantic embedding with external knowledge, we leverage existing MRC datasets across different domains, which help make the whole comprehension process more robust and universal. Our experiments show that MTL can bring further performance boost when combined with contextual representations from pre-trained language models, e.g., ELMo (Peters et al., 2018). 2644 Proceedings of NAACL-HLT 2019, pages"
N19-1271,P18-2124,0,0.041427,"Missing"
N19-1271,P18-1158,0,0.155803,"l validate whether including external datasets as additional input information (e.g., pre-trained language model on these datasets) helps boost the performance of MRC systems. 1 We include the results in the appendix due to space limitations. 2648 Model Dev Set Performance Single Model without Language Models EM,F1 BiDAF (Seo et al., 2016) SAN (Liu et al., 2018c) MT-SAN on SQuAD (single task, ours) MT-SAN on SQuAD+NewsQA(ours) MT-SAN on SQuAD+MARCO(ours) MT-SAN on SQuAD+NewsQA+MARCO(ours) 67.7, 77.3 76.24, 84.06 76.84, 84.54 78.60, 85.87 77.79, 85.23 78.72, 86.10 Single Model with ELMo SLQA+ (Wang et al., 2018a) MT-SAN on SQuAD (single task, ours) MT-SAN on SQuAD+NewsQA(ours) MT-SAN on SQuAD+MARCO(ours) MT-SAN on SQuAD+NewsQA+MARCO(ours) BERT (Devlin et al., 2018) Human Performance (test set) 80.0, 87.0 80.04, 86.54 81.36, 87.71 80.37, 87.17 81.58, 88.19 84.2, 91.1 82.30, 91.22 Table 2: Performance of our method to train SAN in multi-task setting, competing published results, leaderboard results and human performance, on SQuAD dataset (single model). Note that BERT uses a much larger language model, and is not directly comparable with our results. We expect our test performance is roughly similar o"
N19-1271,P18-1178,0,0.0350362,"Missing"
N19-1271,D17-1147,0,0.0609615,"Missing"
N19-1271,K17-1028,0,0.0130536,"-Net3 SAN4 MT-SAN MT-SAN: SQuAD+MARCO MT-SAN: 3 datasets Single Model With ELMo MT-SAN MT-SAN: SQuAD+MARCO MT-SAN: 3 datasets Human Performance (test set) Scores 33.99, 32.09 38.62, 38.01 -, 45.65 43.85, 46.14 34.13, 42.65 34.29, 43.47 36.99, 43.64 34.57, 42.88 37.02, 43.89 37.12, 44.12 48.02, 49.72 Table 4: Performance of our method to train SAN in multi-task setting, competing published results and human performance, on MS MARCO dataset. The scores stand for (BLEU-1, ROUGE-L) respectively. All SAN results are our results. “3 dataset” means we train using SQuAD+NewsQA+MARCO. References: 1 : (Weissenborn et al., 2017). 2 : implemented by (Shen et al., 2017). 3 :(Wang et al., 2018b). 4 : (Liu et al., 2018c) or cross-passage ranking (Wang et al., 2018b). We also test the robustness of our algorithm by performing another set of experiments on SQuAD and WDW. WDW is much more different than the other three datasets (SQuAD, NewsQA, MS MARCO): WDW guarantees that the answer is always a person, whereas the percentage of 2650 Model MT-SAN (Single Task) MT-SAN (S+W) SOTA(Yang et al., 2016). Human Performance SQuAD 76.8, 84.5 77.6, 85.1 86.2, 92.2 82.3, 91.2 Model Performance SQuAD + MARCO EM,F1 Simple Combine (Alg."
N19-1271,D16-1264,0,0.727668,"ent domains. Inspired by recent ideas of data selection in machine translation, we develop a novel sample re-weighting scheme to assign sample-specific weights to the loss. Empirical study shows that our approach can be applied to many existing MRC models. Combined with contextual representations from pre-trained language models (such as ELMo), we achieve new state-of-the-art results on a set of MRC benchmark datasets. We release our code at https://github.com/ xycforgithub/MultiTask-MRC. 1 Introduction Machine Reading Comprehension (MRC) has gained growing interest in the research community (Rajpurkar et al., 2016; Yu et al., 2018). In an MRC task, the machine reads a text passage and a question, and generates (or selects) an answer based on the passage. This requires the machine to possess strong comprehension, inference and reasoning capabilities. Over the past few years, there has been much progress in building end-toend neural network models (Seo et al., 2016) for MRC. However, most public MRC datasets (e.g., SQuAD, MS MARCO, TriviaQA) are typically small (less than 100K) compared to the model size (such as SAN (Liu et al., 2018c,b) with around 10M parameters). To prevent over-fitting, recently the"
N19-1271,I17-1096,1,0.8607,"datasets Single Model With ELMo MT-SAN MT-SAN: SQuAD+MARCO MT-SAN: 3 datasets Human Performance (test set) Scores 33.99, 32.09 38.62, 38.01 -, 45.65 43.85, 46.14 34.13, 42.65 34.29, 43.47 36.99, 43.64 34.57, 42.88 37.02, 43.89 37.12, 44.12 48.02, 49.72 Table 4: Performance of our method to train SAN in multi-task setting, competing published results and human performance, on MS MARCO dataset. The scores stand for (BLEU-1, ROUGE-L) respectively. All SAN results are our results. “3 dataset” means we train using SQuAD+NewsQA+MARCO. References: 1 : (Weissenborn et al., 2017). 2 : implemented by (Shen et al., 2017). 3 :(Wang et al., 2018b). 4 : (Liu et al., 2018c) or cross-passage ranking (Wang et al., 2018b). We also test the robustness of our algorithm by performing another set of experiments on SQuAD and WDW. WDW is much more different than the other three datasets (SQuAD, NewsQA, MS MARCO): WDW guarantees that the answer is always a person, whereas the percentage of 2650 Model MT-SAN (Single Task) MT-SAN (S+W) SOTA(Yang et al., 2016). Human Performance SQuAD 76.8, 84.5 77.6, 85.1 86.2, 92.2 82.3, 91.2 Model Performance SQuAD + MARCO EM,F1 Simple Combine (Alg. 1) 77.1, 84.6 Loss Uncertainty 77.3, 84."
N19-1271,P17-1075,0,0.0501405,"Missing"
N19-1271,W17-2623,0,0.462706,"r model, on the other hand, can be trained with more flexibility on various MRC tasks. MTL is also faster and easier to train than embedding/LM methods: our approach requires no pre-trained models, whereas back translation and ELMo both rely on large models that would need days to train on multiple GPUs (Jozefowicz et al., 2016; Peters et al., 2018). We validate our MTL framework with two state-of-the-art models on four datasets from different domains. Experiments show that our methods lead to a significant performance gain over single-task baselines on SQuAD (Rajpurkar et al., 2016), NewsQA (Trischler et al., 2017) and WhoDid-What (Onishi et al., 2016), while achieving state-of-the-art performance on the latter two. For example, on NewsQA (Trischler et al., 2017), our model surpassed human performance by 13.4 (46.5 vs 59.9) and 3.2 (72.6 vs 69.4) absolute points in terms of exact match and F1. The contribution of this work is three-fold. First, we apply multi-task learning to the MRC task, which brings significant improvements over single-task baselines. Second, the performance gain from MTL can be easily combined with existing methods to obtain further performance gain. Third, the proposed sampling and"
P18-1157,P16-1223,0,0.0737543,"Missing"
P18-1157,P17-1171,0,0.12555,"rforming a kind of stochastic ensemble over the model’s successive predic1694 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 1694–1704 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics ants. A typical technique to obtain lexicon embedding is concatenation of its word embedding with other linguistic embedding such as those derived from Part-Of-Speech (POS) tags. For word embeddings, we use the pre-trained 300-dimensional GloVe vectors (Pennington et al., 2014) for the both Q and P . Following Chen et al. (2017), we use three additional types of linguistic features for each token pi in the passage P : x st-1 st st+1 • 9-dimensional POS tagging embedding for total 56 different types of the POS tags. Figure 1: Illustration of “stochastic prediction dropout” in the answer module during training. At each reasoning step t, the model combines memory (bottom row) with hidden states st−1 to generate a prediction (multinomial distribution). Here, there are three steps and three predictions, but one prediction is dropped and the final result is an average of the remaining distributions. • 8-dimensional named-e"
P18-1157,D17-1215,0,0.0159621,"provement as we increase T = 1 to T = 5, but after 5 steps the improvements have saturated. In fact, the EM/F1 scores drop slightly, but considering that the random initialization results in Table 3 show a standard deviation of 0.142 and a spread of 0.426 (for EM), we believe that the T = 10 result does not statistically differ from the T = 5 result. In summary, we think it is useful to perform some approximate hyper-parameter tuning for the number of steps, but it is not necessary to find the exact optimal value. Finally, we test SAN on two Adversarial SQuAD datasets, AddSent and AddOneSent (Jia and Liang, 2017), where the passages contain 1699 Seed# EM F1 Seed# EM F1 Seed 1 76.24 84.06 Seed 6 76.23 83.99 Seed 2 76.30 84.13 Seed 7 76.35 84.09 Seed 3 75.92 83.90 Seed 8 76.07 83.71 Seed 4 76.00 83.95 Seed 9 75.93 83.85 Seed 5 76.12 83.99 Seed 10 76.15 84.11 Mean: 76.131, Std. deviation: 0.142 (EM) Mean: 83.977, Std. deviation: 0.126 (F1) Table 3: Robustness of SAN (5-step) on different random seeds for initialization: best and worst scores are boldfaced. Note that our official submit is trained on seed 1. (a) EM comparison on different systems. SAN 1 step 2 step 3 step 4 step 5 step (b) F1 score compar"
P18-1157,W04-1013,0,0.0153045,"esults on MS MARCO MS MARCO (Nguyen et al., 2016) is a large scale real-word RC dataset which contains 100,100 (100K) queries collected from anonymized user logs from the Bing search engine. The characteristic of MS MARCO is that all the questions are real user queries and passages are extracted from real web documents. For each query, approximate 10 passages are extracted from public web documents. The answers are generated by humans. The data is partitioned into a 82,430 training, a 10,047 development and 9,650 test tuples. The evaluation metrics are BLEU(Papineni et al., 2002) and ROUGE-L (Lin, 2004) due to its free-form text answer style. To apply the same RC model, we search for a span in MS MARCO’s passages that maximizes the ROUGE-L score with the raw freeform answer. It has an upper bound of 93.45 BLEU and 93.82 ROUGE-L on the development set. The MS MARCO dataset contains multiple passages per query. Our model as shown in Figure 2 is developed to generate answer from a single passage. Thus, we need to extend it to handle multiple passages. Following (Shen et al., 2017), we take two steps to generate an answer to a query Q from J passages, P 1 , ..., P J . First, we run SAN on ev1701"
P18-1157,D17-1085,0,0.140626,"rk (prediction from final step) Fixed 5-step with Memory Network (prediction averaged from all steps) Dynamic steps (max 5) with ReasoNet Stochastic Answer Network (SAN ), Fixed 5-step EM 75.139 75.033 75.256 75.355 76.235 F1 83.367 83.327 83.215 83.360 84.056 Table 1: Main results—Comparison of different answer module architectures. Note that SAN performs best in both Exact Match and F1 metrics. Ensemble model results: BiDAF + Self Attention + ELMo (Peters et al., 2018) SAN (Ensemble model) AIR-FusionNet (Huang et al., 2017) DCN+ (Xiong et al., 2017) M-Reader (Hu et al., 2017) Conductor-net (Liu et al., 2017b) r-net (Wang et al., 2017) ReasoNet++ (Shen et al., 2017) Individual model results: BiDAF + Self Attention + ELMo(Peters et al., 2018) SAN (single model) AIR-FusionNet(Huang et al., 2017) RaSoR + TR (Salant and Berant, 2017) DCN+(Xiong et al., 2017) r-net(Wang et al., 2017) ReasoNet++(Shen et al., 2017) BiDAF (Seo et al., 2016) Human Performance Dev Set (EM/F1) -/78.619/85.866 -/-/-/74.8 / 83.3 77.7/83.7 75.4/82.9 Test Set (EM/F1) 81.003/87.432 79.608/86.496 78.978/86.016 78.852/85.996 77.678/84.888 76.996/84.630 76.9/84.0 75.0/82.6 -/76.235/84.056 75.3/83.6 -/74.5/83.1 72.3/80.6 70.8/79.4 6"
P18-1157,P02-1040,0,0.10049,"form by question type? Experiments results on MS MARCO MS MARCO (Nguyen et al., 2016) is a large scale real-word RC dataset which contains 100,100 (100K) queries collected from anonymized user logs from the Bing search engine. The characteristic of MS MARCO is that all the questions are real user queries and passages are extracted from real web documents. For each query, approximate 10 passages are extracted from public web documents. The answers are generated by humans. The data is partitioned into a 82,430 training, a 10,047 development and 9,650 test tuples. The evaluation metrics are BLEU(Papineni et al., 2002) and ROUGE-L (Lin, 2004) due to its free-form text answer style. To apply the same RC model, we search for a span in MS MARCO’s passages that maximizes the ROUGE-L score with the raw freeform answer. It has an upper bound of 93.45 BLEU and 93.82 ROUGE-L on the development set. The MS MARCO dataset contains multiple passages per query. Our model as shown in Figure 2 is developed to generate answer from a single passage. Thus, we need to extend it to handle multiple passages. Following (Shen et al., 2017), we take two steps to generate an answer to a query Q from J passages, P 1 , ..., P J . Fir"
P18-1157,D14-1162,0,0.0841458,"ep is still trained to generate the same answer; we are performing a kind of stochastic ensemble over the model’s successive predic1694 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 1694–1704 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics ants. A typical technique to obtain lexicon embedding is concatenation of its word embedding with other linguistic embedding such as those derived from Part-Of-Speech (POS) tags. For word embeddings, we use the pre-trained 300-dimensional GloVe vectors (Pennington et al., 2014) for the both Q and P . Following Chen et al. (2017), we use three additional types of linguistic features for each token pi in the passage P : x st-1 st st+1 • 9-dimensional POS tagging embedding for total 56 different types of the POS tags. Figure 1: Illustration of “stochastic prediction dropout” in the answer module during training. At each reasoning step t, the model combines memory (bottom row) with hidden states st−1 to generate a prediction (multinomial distribution). Here, there are three steps and three predictions, but one prediction is dropped and the final result is an average of"
P18-1157,N18-1202,0,0.08263,"Missing"
P18-1157,D16-1264,0,0.781631,"ial SQuAD, and the Microsoft MAchine Reading COmprehension Dataset (MS MARCO). 1 Introduction Machine reading comprehension (MRC) is a challenging task: the goal is to have machines read a text passage and then answer any question about the passage. This task is an useful benchmark to demonstrate natural language understanding, and also has important applications in e.g. conversational agents and customer service support. It has been hypothesized that difficult MRC problems require some form of multi-step synthesis and reasoning. For instance, the following example from the MRC dataset SQuAD (Rajpurkar et al., 2016) illustrates the need for synthesis of information across sentences and multiple steps of reasoning: Q: What collection does the V&A Theator & Performance galleries hold? P : The V&A Theator & Performance galleries opened in March 2009. ... They hold the UK’s biggest national collection of This kind of iterative process can be viewed as a form of multi-step reasoning. Several recent MRC models have embraced this kind of multistep strategy, where predictions are generated after making multiple passes through the same text and integrating intermediate information in the process. The first models"
P18-1157,D13-1020,0,0.0320734,"andidate answer span with its relevance score r(P j , Q) assigned by a passage ranker, and output the span with the maximum score as the answer. In our experiments, we use the passage ranker described in (Liu et al., 2018)5 . The ranker is trained on the same MS MARCO training data, and achieves 37.1 p@1 on the development set. The results in Table 7 show that SAN outperforms V-Net (Wang et al., 2018) and becomes the new state of the art6 . 6 Related Work The recent big progress on MRC is largely due to the availability of the large-scale datasets (Rajpurkar et al., 2016; Nguyen et al., 2016; Richardson et al., 2013; Hill et al., 2016), since it is possible to train large end-to-end neural network models. In spite of the variety of model structures and attenion types (Bahdanau et al., 2015; Chen et al., 2016; Xiong et al., 2016; Seo et al., 2016; Shen et al., 2017; Wang et al., 2017), a typical neural network MRC model first maps the symbolic representation of the documents and questions into a neural space, then search answers on top of it. We categorize these models into two groups based on the difference of the answer module: singlestep and multi-step reasoning. The key difference between the two is w"
P18-1157,I17-1096,1,0.832902,"his kind of iterative process can be viewed as a form of multi-step reasoning. Several recent MRC models have embraced this kind of multistep strategy, where predictions are generated after making multiple passes through the same text and integrating intermediate information in the process. The first models employed a predetermined fixed number of steps (Hill et al., 2016; Dhingra et al., 2016; Sordoni et al., 2016; Kumar et al., 2015). Later, Shen et al. (2016) proposed using reinforcement learning to dynamically determine the number of steps based on the complexity of the question. Further, Shen et al. (2017) empirically showed that dynamic multi-step reasoning outperforms fixed multi-step reasoning, which in turn outperforms single-step reasoning on two distinct MRC datasets (SQuAD and MS MARCO). In this work, we derive an alternative multi-step reasoning neural network for MRC. During training, we fix the number of reasoning steps, but perform stochastic dropout on the answer module (final layer predictions). During decoding, we generate answers based on the average of predictions in all steps, rather than the final step. We call this a stochastic answer network (SAN) because the stochastic drop"
P18-1157,P17-1018,0,0.243393,"step) Fixed 5-step with Memory Network (prediction averaged from all steps) Dynamic steps (max 5) with ReasoNet Stochastic Answer Network (SAN ), Fixed 5-step EM 75.139 75.033 75.256 75.355 76.235 F1 83.367 83.327 83.215 83.360 84.056 Table 1: Main results—Comparison of different answer module architectures. Note that SAN performs best in both Exact Match and F1 metrics. Ensemble model results: BiDAF + Self Attention + ELMo (Peters et al., 2018) SAN (Ensemble model) AIR-FusionNet (Huang et al., 2017) DCN+ (Xiong et al., 2017) M-Reader (Hu et al., 2017) Conductor-net (Liu et al., 2017b) r-net (Wang et al., 2017) ReasoNet++ (Shen et al., 2017) Individual model results: BiDAF + Self Attention + ELMo(Peters et al., 2018) SAN (single model) AIR-FusionNet(Huang et al., 2017) RaSoR + TR (Salant and Berant, 2017) DCN+(Xiong et al., 2017) r-net(Wang et al., 2017) ReasoNet++(Shen et al., 2017) BiDAF (Seo et al., 2016) Human Performance Dev Set (EM/F1) -/78.619/85.866 -/-/-/74.8 / 83.3 77.7/83.7 75.4/82.9 Test Set (EM/F1) 81.003/87.432 79.608/86.496 78.978/86.016 78.852/85.996 77.678/84.888 76.996/84.630 76.9/84.0 75.0/82.6 -/76.235/84.056 75.3/83.6 -/74.5/83.1 72.3/80.6 70.8/79.4 67.7/77.3 80.3/90.5 78.580/85"
P18-1157,P18-1178,0,0.0770301,"the same RC model, we search for a span in MS MARCO’s passages that maximizes the ROUGE-L score with the raw freeform answer. It has an upper bound of 93.45 BLEU and 93.82 ROUGE-L on the development set. The MS MARCO dataset contains multiple passages per query. Our model as shown in Figure 2 is developed to generate answer from a single passage. Thus, we need to extend it to handle multiple passages. Following (Shen et al., 2017), we take two steps to generate an answer to a query Q from J passages, P 1 , ..., P J . First, we run SAN on ev1701 SingleM odel ReasoNet++(Shen et al., 2017) V-Net(Wang et al., 2018) Standard 1-step in Table 1 SAN ROUGE BLEU 38.01 38.62 45.65 42.30 42.39 46.14 43.85 Table 7: MS MARCO devset results. ery (P j , Q) pair, generating J candidate answer spans, one from each passage. Then, we multiply the SAN score of each candidate answer span with its relevance score r(P j , Q) assigned by a passage ranker, and output the span with the maximum score as the answer. In our experiments, we use the passage ranker described in (Liu et al., 2018)5 . The ranker is trained on the same MS MARCO training data, and achieves 37.1 p@1 on the development set. The results in Table 7 show th"
P19-1441,D15-1075,0,0.694245,"in 1, 2, ..., T do Pack the dataset t into mini-batch: Dt . end for epoch in 1, 2, ..., epochmax do 1. Merge all the datasets: D = D1 ∪ D2 ... ∪ DT 2. Shuffle D for bt in D do //bt is a mini-batch of task t. 3. Compute loss : L(Θ) L(Θ) = Eq. 6 for classification L(Θ) = Eq. 7 for regression L(Θ) = Eq. 8 for ranking 4. Compute gradient: ∇(Θ) 5. Update model: Θ = Θ − ∇(Θ) end end SNLI The Stanford Natural Language Inference (SNLI) dataset contains 570k human annotated sentence pairs, in which the premises are drawn from the captions of the Flickr30 corpus and hypotheses are manually annotated (Bowman et al., 2015b). This is the most widely used entailment dataset for NLI. The dataset is used only for domain adaptation in this study. and |A |− 1 negative examples. We then minimize the negative log likelihood of the positive example given queries across the training data X − Pr (A+ |Q), (8) (Q,A+ ) SciTail This is a textual entailment dataset derived from a science question answering (SciQ) dataset (Khot et al., 2018). The task involves assessing whether a given premise entails a given hypothesis. In contrast to other entailment datasets mentioned previously, the hypotheses in SciTail are created from s"
P19-1441,P18-2103,0,0.0508588,"Missing"
P19-1441,P18-1064,0,0.0268376,"om previous tasks to help learn a new task (Caruana, 1997; Zhang and Yang, 2017). For example, it is easier for a person who knows how to ski to learn skating than the one who ∗ 1 Equal Contribution. As of February 25, 2019 on the latest GLUE test set. does not. Similarly, it is useful for multiple (related) tasks to be learned jointly so that the knowledge learned in one task can benefit other tasks. Recently, there is a growing interest in applying MTL to representation learning using deep neural networks (DNNs) (Collobert et al., 2011; Liu et al., 2015; Luong et al., 2015; Xu et al., 2018; Guo et al., 2018; Ruder12 et al., 2019) for two reasons. First, supervised learning of DNNs requires large amounts of task-specific labeled data, which is not always available. MTL provides an effective way of leveraging supervised data from many related tasks. Second, the use of multi-task learning profits from a regularization effect via alleviating overfitting to a specific task, thus making the learned representations universal across tasks. In contrast to MTL, language model pretraining has shown to be effective for learning universal language representations by leveraging large amounts of unlabeled data"
P19-1441,N15-1092,1,0.453968,"ities where people often apply the knowledge learned from previous tasks to help learn a new task (Caruana, 1997; Zhang and Yang, 2017). For example, it is easier for a person who knows how to ski to learn skating than the one who ∗ 1 Equal Contribution. As of February 25, 2019 on the latest GLUE test set. does not. Similarly, it is useful for multiple (related) tasks to be learned jointly so that the knowledge learned in one task can benefit other tasks. Recently, there is a growing interest in applying MTL to representation learning using deep neural networks (DNNs) (Collobert et al., 2011; Liu et al., 2015; Luong et al., 2015; Xu et al., 2018; Guo et al., 2018; Ruder12 et al., 2019) for two reasons. First, supervised learning of DNNs requires large amounts of task-specific labeled data, which is not always available. MTL provides an effective way of leveraging supervised data from many related tasks. Second, the use of multi-task learning profits from a regularization effect via alleviating overfitting to a specific task, thus making the learned representations universal across tasks. In contrast to MTL, language model pretraining has shown to be effective for learning universal language repres"
P19-1441,P18-1157,1,0.906088,"ation of the input sentence pair (X1 , X2 ). We introduce a task-specific parameter vector wST S to compute the similarity score as: 4489 &gt; Sim(X1 , X2 ) = wST S · x, (2) where Sim(X1 , X2 ) is a real value of the range (∞, ∞). Pairwise Text Classification Output: Take natural language inference (NLI) as an example. The NLI task defined here involves a premise P = (p1 , ..., pm ) of m words and a hypothesis H = (h1 , ..., hn ) of n words, and aims to find a logical relationship R between P and H. The design of the output module follows the answer module of the stochastic answer network (SAN) (Liu et al., 2018a), a state-of-the-art neural NLI model. SAN’s answer module uses multi-step reasoning. Rather than directly predicting the entailment given the input, it maintains a state and iteratively refines its predictions. The SAN answer module works as follows. We first construct the working memory of premise P by concatenating the contextual embeddings of the words in P , which are the output of the transformer encoder, denoted as Mp ∈ Rd×m , and similarly the working memory of hypothesis H, denoted as Mh ∈ Rd×n . Then, we perform K-step reasoning on the memory to output the relation label, where K i"
P19-1441,N18-1202,0,0.181058,"of task-specific labeled data, which is not always available. MTL provides an effective way of leveraging supervised data from many related tasks. Second, the use of multi-task learning profits from a regularization effect via alleviating overfitting to a specific task, thus making the learned representations universal across tasks. In contrast to MTL, language model pretraining has shown to be effective for learning universal language representations by leveraging large amounts of unlabeled data. A recent survey is included in Gao et al. (2018). Some of the most prominent examples are ELMo (Peters et al., 2018), GPT (Radford et al., 2018) and BERT (Devlin et al., 2018). These are neural network language models trained on text data using unsupervised objectives. For example, BERT is based on a multi-layer bidirectional Transformer, and is trained on plain text for masked word prediction and next sentence prediction tasks. To apply a pre-trained model to specific NLU tasks, we often need to fine-tune, for each task, the model with additional task-specific layers using task-specific training data. For example, Devlin et al. (2018) shows that BERT can be fine-tuned this way to create state-of-the-art mo"
P19-1441,D16-1264,0,0.0923474,"onship of the two sentences based on a set of pre-defined labels. For example, both RTE and MNLI are language inference tasks, where the goal is to predict whether a sentence is an entailment, contradiction, or neutral with respect to the other. QQP and MRPC are paraphrase datasets that consist of sentence pairs. The task is to predict whether the sentences in the pair are semantically equivalent. Relevance Ranking: Given a query and a list of candidate answers, the model ranks all the candidates in the order of relevance to the query. QNLI is a version of Stanford Question Answering Dataset (Rajpurkar et al., 2016). The task involves assessing whether a sentence contains the correct answer to a given query. Although QNLI is defined as a binary classification task in GLUE, in this study we formulate it as a pairwise ranking task, where the model is expected to rank the candidate that contains the correct answer higher than the candidate that does not. We will show that this formulation leads to a significant improvement in accuracy over binary classification. 3 The MT-DNN model combines four types of NLU tasks: single-sentence classification, pairwise text classification, text similarity scoring, and rel"
P19-1441,W19-4810,0,0.0350359,"Missing"
P19-1441,W18-5446,0,0.121984,"Missing"
P19-1539,W18-5709,0,0.13562,"round responses is distributed throughout the source document. Introduction While end-to-end neural conversation models (Shang et al., 2015; Sordoni et al., 2015; Vinyals and Le, 2015; Serban et al., 2016; Li et al., 2016a; Gao et al., 2019a, etc.) are effective in learning how to be fluent, their responses are often vacuous and uninformative. A primary challenge thus lies in modeling what to say to make the conversation contentful. Several recent approaches have attempted to address this difficulty by conditioning the language decoder on external information sources, such as knowledge bases (Agarwal et al., 2018; Liu et al., 2018a), review posts (Ghazvininejad et al., 2018; Moghe et al., 2018), and even images (Das et al., 2017; Mostafazadeh et al., 2017). However, empirical results suggest that conditioning the decoder on rich and complex contexts, while helpful, does not on its own provide sufficient inductive bias for these systems to learn how to achieve deep and accurate integration between external knowledge and response generation. We posit that this ongoing challenge demands a more effective mechanism to support on-demand knowledge integration. We draw inspiration from how humans converse abo"
P19-1539,D18-1241,1,0.822774,"ent for a given question (Seo et al., 2017; Liu et al., 2018b; Yu et al., 2018). These models differ in how they fuse information between questions and documents. We chose SAN (Liu et al., 2018b) because of its representative architecture and competitive performance on existing MRC tasks. We note that other off-theshelf MRC models, such as BERT (Devlin et al., 2018), can also be plugged in. We leave the study of different MRC architectures for future work. Questions are treated as entirely independent in these “single-turn” MRC models, so recent work (e.g., CoQA (Reddy et al., 2019) and QuAC (Choi et al., 2018)) focuses on multi-turn MRC, modeling sequences of questions and answers in a conversation. While multi-turn MRC aims to answer complex questions, that body of work is restricted to factual questions, whereas our work—like much of the prior work in end-to-end dialogue—models free-form dialogue, which also encompasses chitchat and non-factual responses. 8 Conclusions We have demonstrated that the machine reading comprehension approach offers a promising step to generating, on the fly, contentful conversation exchanges that are grounded in extended text corpora. The functional combination of MRC"
P19-1539,D18-1045,0,0.0185929,"model training, with an initial learning rate of 0.0005. Batch size was set to 32. During training, all responses were truncated to have a maximum length of 30, and maximum query length and document length were set to 30, 500, respectively. we used regular teacher-forcing decoding during training. For inference, we found that top-k random sample decoding (Fan et al., 2018) provides the best results for all the systems. That is, at each decoding step, a token was drawn from the k most likely candidates according to the distribution over the vocabulary. Similar to recent work (Fan et al., 2018; Edunov et al., 2018), we set k = 20 (other common k values like 10 gave similar results). We selected key hyperparameter configurations on the validation set. 6.1 Evaluation Setup Table 2 shows automatic metrics for quantitative evaluation over three qualities of generated texts. We measure the overall relevance of the generated responses given the conversational history by using standard Machine Translation (MT) metrics, comparing generated outputs to ground-truth responses. These metrics include B LEU-4 (Papineni et al., 2002), M ETEOR (Lavie and Agarwal, 2007). and N IST (Doddington, 2002). The latter metric i"
P19-1539,P18-1082,0,0.0333813,"used the pretrained GloVe8 for initialization. We set hidden dimensions to 512 and dropout rate to 0.4. GRU cells are used for S EQ 2S EQ and M EM N ET (we also tested LSTM cells and obtained similar results). We used the Adam optimizer for model training, with an initial learning rate of 0.0005. Batch size was set to 32. During training, all responses were truncated to have a maximum length of 30, and maximum query length and document length were set to 30, 500, respectively. we used regular teacher-forcing decoding during training. For inference, we found that top-k random sample decoding (Fan et al., 2018) provides the best results for all the systems. That is, at each decoding step, a token was drawn from the k most likely candidates according to the distribution over the vocabulary. Similar to recent work (Fan et al., 2018; Edunov et al., 2018), we set k = 20 (other common k values like 10 gave similar results). We selected key hyperparameter configurations on the validation set. 6.1 Evaluation Setup Table 2 shows automatic metrics for quantitative evaluation over three qualities of generated texts. We measure the overall relevance of the generated responses given the conversational history b"
P19-1539,N19-1125,1,0.868164,"Missing"
P19-1539,W07-0734,0,0.0141127,"vocabulary. Similar to recent work (Fan et al., 2018; Edunov et al., 2018), we set k = 20 (other common k values like 10 gave similar results). We selected key hyperparameter configurations on the validation set. 6.1 Evaluation Setup Table 2 shows automatic metrics for quantitative evaluation over three qualities of generated texts. We measure the overall relevance of the generated responses given the conversational history by using standard Machine Translation (MT) metrics, comparing generated outputs to ground-truth responses. These metrics include B LEU-4 (Papineni et al., 2002), M ETEOR (Lavie and Agarwal, 2007). and N IST (Doddington, 2002). The latter metric is a variant of B LEU that weights n-gram matches by their information gain by effectively penalizing uninformative n-grams (such as “I don’t know”), which makes it a relevant metric for evaluating systems aiming diverse and informative responses. MT metrics may not be particularly adequate for our task (Liu et al., 2016), given its focus on the informativeness of responses, and for that reason we also use two other types of metrics to measure the level of grounding and diversity. As a diversity metric, we count all n-grams in the system output"
P19-1539,N16-1014,1,0.92711,"s recreated by the American television MythBusters. Four years later, […] two Praguebased journalists, claimed that Flight 367 had been mistaken for an enemy aircraft and shot down by the Czechoslovak Air Force at an altitude of 800 metres (2,600 ft). Figure 1: Users discussing a topic defined by a Wikipedia article. In this real-world example from our Reddit dataset, information needed to ground responses is distributed throughout the source document. Introduction While end-to-end neural conversation models (Shang et al., 2015; Sordoni et al., 2015; Vinyals and Le, 2015; Serban et al., 2016; Li et al., 2016a; Gao et al., 2019a, etc.) are effective in learning how to be fluent, their responses are often vacuous and uninformative. A primary challenge thus lies in modeling what to say to make the conversation contentful. Several recent approaches have attempted to address this difficulty by conditioning the language decoder on external information sources, such as knowledge bases (Agarwal et al., 2018; Liu et al., 2018a), review posts (Ghazvininejad et al., 2018; Moghe et al., 2018), and even images (Das et al., 2017; Mostafazadeh et al., 2017). However, empirical results suggest that conditioning"
P19-1539,P16-1094,1,0.958274,"s recreated by the American television MythBusters. Four years later, […] two Praguebased journalists, claimed that Flight 367 had been mistaken for an enemy aircraft and shot down by the Czechoslovak Air Force at an altitude of 800 metres (2,600 ft). Figure 1: Users discussing a topic defined by a Wikipedia article. In this real-world example from our Reddit dataset, information needed to ground responses is distributed throughout the source document. Introduction While end-to-end neural conversation models (Shang et al., 2015; Sordoni et al., 2015; Vinyals and Le, 2015; Serban et al., 2016; Li et al., 2016a; Gao et al., 2019a, etc.) are effective in learning how to be fluent, their responses are often vacuous and uninformative. A primary challenge thus lies in modeling what to say to make the conversation contentful. Several recent approaches have attempted to address this difficulty by conditioning the language decoder on external information sources, such as knowledge bases (Agarwal et al., 2018; Liu et al., 2018a), review posts (Ghazvininejad et al., 2018; Moghe et al., 2018), and even images (Das et al., 2017; Mostafazadeh et al., 2017). However, empirical results suggest that conditioning"
P19-1539,D16-1230,0,0.0886793,"Missing"
P19-1539,P18-1138,0,0.391323,"tributed throughout the source document. Introduction While end-to-end neural conversation models (Shang et al., 2015; Sordoni et al., 2015; Vinyals and Le, 2015; Serban et al., 2016; Li et al., 2016a; Gao et al., 2019a, etc.) are effective in learning how to be fluent, their responses are often vacuous and uninformative. A primary challenge thus lies in modeling what to say to make the conversation contentful. Several recent approaches have attempted to address this difficulty by conditioning the language decoder on external information sources, such as knowledge bases (Agarwal et al., 2018; Liu et al., 2018a), review posts (Ghazvininejad et al., 2018; Moghe et al., 2018), and even images (Das et al., 2017; Mostafazadeh et al., 2017). However, empirical results suggest that conditioning the decoder on rich and complex contexts, while helpful, does not on its own provide sufficient inductive bias for these systems to learn how to achieve deep and accurate integration between external knowledge and response generation. We posit that this ongoing challenge demands a more effective mechanism to support on-demand knowledge integration. We draw inspiration from how humans converse about a topic, where"
P19-1539,P18-1157,1,0.938959,"tributed throughout the source document. Introduction While end-to-end neural conversation models (Shang et al., 2015; Sordoni et al., 2015; Vinyals and Le, 2015; Serban et al., 2016; Li et al., 2016a; Gao et al., 2019a, etc.) are effective in learning how to be fluent, their responses are often vacuous and uninformative. A primary challenge thus lies in modeling what to say to make the conversation contentful. Several recent approaches have attempted to address this difficulty by conditioning the language decoder on external information sources, such as knowledge bases (Agarwal et al., 2018; Liu et al., 2018a), review posts (Ghazvininejad et al., 2018; Moghe et al., 2018), and even images (Das et al., 2017; Mostafazadeh et al., 2017). However, empirical results suggest that conditioning the decoder on rich and complex contexts, while helpful, does not on its own provide sufficient inductive bias for these systems to learn how to achieve deep and accurate integration between external knowledge and response generation. We posit that this ongoing challenge demands a more effective mechanism to support on-demand knowledge integration. We draw inspiration from how humans converse about a topic, where"
P19-1539,D15-1166,0,0.0725271,"n layer is applied to further ingest and capture the most salient information. The output memory, M ∈ Rd×n , is obtained by applying another BiLSTM layer for final information rearrangement. Note that d is the hidden size of the memory and n is the length of the document. 3.2 Response Generation Having read and processed both the conversation history and the extra knowledge in the document, the model then produces a free-form response y = (y1 , . . . , yT ) instead of generating a span or performing answer classification as in MRC tasks. We use an attentional recurrent neural network decoder (Luong et al., 2015) to generate response tokens while attending to the memory. At the beginning, the initial hidden state h0 is the weighted sum of the representation of the history X. For each decoding step t with a hidden state ht , we generate a token yt based on the distribution: p(yt ) = softmax((W1 ht + b)/τ ), (1) where τ > 0 is the softmax temperature. The hidden state ht is defined as follows: ht = W2 [zt ++fattention (zt , M )]. (2) Here, [· ++·] indicates a concatenation of two vectors; fattention is a dot-product attention (Vaswani et al., 2017); and zt is a state generated by GRU(et−1 , ht−1 ) with"
P19-1539,D18-1255,0,0.363735,"d-to-end neural conversation models (Shang et al., 2015; Sordoni et al., 2015; Vinyals and Le, 2015; Serban et al., 2016; Li et al., 2016a; Gao et al., 2019a, etc.) are effective in learning how to be fluent, their responses are often vacuous and uninformative. A primary challenge thus lies in modeling what to say to make the conversation contentful. Several recent approaches have attempted to address this difficulty by conditioning the language decoder on external information sources, such as knowledge bases (Agarwal et al., 2018; Liu et al., 2018a), review posts (Ghazvininejad et al., 2018; Moghe et al., 2018), and even images (Das et al., 2017; Mostafazadeh et al., 2017). However, empirical results suggest that conditioning the decoder on rich and complex contexts, while helpful, does not on its own provide sufficient inductive bias for these systems to learn how to achieve deep and accurate integration between external knowledge and response generation. We posit that this ongoing challenge demands a more effective mechanism to support on-demand knowledge integration. We draw inspiration from how humans converse about a topic, where people often search and acquire external information as needed to"
P19-1539,I17-1047,1,0.933114,"Sordoni et al., 2015; Vinyals and Le, 2015; Serban et al., 2016; Li et al., 2016a; Gao et al., 2019a, etc.) are effective in learning how to be fluent, their responses are often vacuous and uninformative. A primary challenge thus lies in modeling what to say to make the conversation contentful. Several recent approaches have attempted to address this difficulty by conditioning the language decoder on external information sources, such as knowledge bases (Agarwal et al., 2018; Liu et al., 2018a), review posts (Ghazvininejad et al., 2018; Moghe et al., 2018), and even images (Das et al., 2017; Mostafazadeh et al., 2017). However, empirical results suggest that conditioning the decoder on rich and complex contexts, while helpful, does not on its own provide sufficient inductive bias for these systems to learn how to achieve deep and accurate integration between external knowledge and response generation. We posit that this ongoing challenge demands a more effective mechanism to support on-demand knowledge integration. We draw inspiration from how humans converse about a topic, where people often search and acquire external information as needed to continue a meaningful and informative conversation. Figure 1 i"
P19-1539,P02-1040,0,0.106889,"e decoding to draw yt from the above distribution p(yt ). Section 5 provides more details about the experimental configuration. 3.3 Data Weighting Scheme We further propose a simple data weighting scheme to encourage the generation of grounded responses. The idea is to bias the model training to fit better to those training instances where the ground-truth response is more closely relevant to the document. More specifically, given a training instance (X, D, y), we measure the closeness score c ∈ R between the document D and the gold response y (e.g., with the NIST (Doddington, 2002) or B LEU (Papineni et al., 2002) metrics). In each training data batch, we normalize the closeness scores of all the instances to have a sum of 1, and weight each of the instances with its corresponding normalized score when evaluating the 5429 # dialogues # utterances # documents # document sentences Train Valid Test 28.4k 2.36M 28.4k 15.18M 1.2k 0.12M 1.2k 0.58M 3.1k 0.34M 3.1k 1.68M 18.84 14.17 18.48 14.15 Average length (# words): utterances 18.74 document sentences 13.72 Table 1: Our grounded conversational dataset. training loss. This training regime promotes instances with grounded responses and thus encourages the mo"
P19-1539,D16-1264,0,0.230968,"ble at https://github.com/qkaren/ converse_reading_cmr. of turns X = (x1 , . . . , xM ) and a web document D = (s1 , . . . , sN ) as the knowledge source, where si is the ith sentence in the document. With the pair (X, D), the system needs to generate a natural language response y that is both conversationally appropriate and reflective of the contents of the web document. 3 Approach Our approach integrates conversation generation with on-demand MRC. Specifically, we use an MRC model to effectively encode the conversation history by treating it as a question in a typical QA task (e.g., SQuAD (Rajpurkar et al., 2016)), and encode the web document as the context. We then replace the output component of the MRC model (which is usually an answer classification module) with an attentional sequence generator that generates a free-form response. We refer to our approach as CMR (Conversation with on-demand Machine Reading). In general, any off-the-shelf MRC model could be applied here for knowledge comprehension. We use Stochastic Answer Networks (SAN)2 (Liu et al., 2018b), a performant machine reading model that until very recently held state-of-the-art performance on the SQuAD benchmark. We also employ a simpl"
P19-1539,Q19-1016,0,0.0224858,"ng meaning. from a given document for a given question (Seo et al., 2017; Liu et al., 2018b; Yu et al., 2018). These models differ in how they fuse information between questions and documents. We chose SAN (Liu et al., 2018b) because of its representative architecture and competitive performance on existing MRC tasks. We note that other off-theshelf MRC models, such as BERT (Devlin et al., 2018), can also be plugged in. We leave the study of different MRC architectures for future work. Questions are treated as entirely independent in these “single-turn” MRC models, so recent work (e.g., CoQA (Reddy et al., 2019) and QuAC (Choi et al., 2018)) focuses on multi-turn MRC, modeling sequences of questions and answers in a conversation. While multi-turn MRC aims to answer complex questions, that body of work is restricted to factual questions, whereas our work—like much of the prior work in end-to-end dialogue—models free-form dialogue, which also encompasses chitchat and non-factual responses. 8 Conclusions We have demonstrated that the machine reading comprehension approach offers a promising step to generating, on the fly, contentful conversation exchanges that are grounded in extended text corpora. The"
P19-1539,P15-1152,0,0.0338416,"fall without a parachute: 10,160 metres (33,330 ft). …… …… In 2005, Vulović‘s fall was recreated by the American television MythBusters. Four years later, […] two Praguebased journalists, claimed that Flight 367 had been mistaken for an enemy aircraft and shot down by the Czechoslovak Air Force at an altitude of 800 metres (2,600 ft). Figure 1: Users discussing a topic defined by a Wikipedia article. In this real-world example from our Reddit dataset, information needed to ground responses is distributed throughout the source document. Introduction While end-to-end neural conversation models (Shang et al., 2015; Sordoni et al., 2015; Vinyals and Le, 2015; Serban et al., 2016; Li et al., 2016a; Gao et al., 2019a, etc.) are effective in learning how to be fluent, their responses are often vacuous and uninformative. A primary challenge thus lies in modeling what to say to make the conversation contentful. Several recent approaches have attempted to address this difficulty by conditioning the language decoder on external information sources, such as knowledge bases (Agarwal et al., 2018; Liu et al., 2018a), review posts (Ghazvininejad et al., 2018; Moghe et al., 2018), and even images (Das et al., 2017;"
P19-1539,P18-1205,0,0.0465991,"compared to machine translation, it is common for the generator to retain focus on the key information in the external document to produce semantically relevant responses. 7 Related Work Dialogue: Traditional dialogue systems (see (Jurafsky and Martin, 2009) for an historical perspective) are typically grounded, enabling these systems to be reflective of the user’s environment. The lack of grounding has been a stumbling block for the earliest end-to-end dialogue systems, as various researchers have noted that their outputs tend to be bland (Li et al., 2016a; Gao et al., 2019b), inconsistent (Zhang et al., 2018a; Li et al., Figure 3: Attention weights between words of the documents and words of the response. Dark (blue) cells represent probabilities closer to 1. 2016b; Zhang et al., 2019), and lacking in factual content (Ghazvininejad et al., 2018; Agarwal et al., 2018). Recently there has been growing interest in exploring different forms of grounding, including images, knowledge bases, and plain texts (Das et al., 2017; Mostafazadeh et al., 2017; Agarwal et al., 2018; Yang et al., 2019). A recent survey is included in Gao et al. (2019a). Prior work, e.g, (Ghazvininejad et al., 2018; Zhang et al.,"
P19-1539,N15-1020,1,0.77954,"hute: 10,160 metres (33,330 ft). …… …… In 2005, Vulović‘s fall was recreated by the American television MythBusters. Four years later, […] two Praguebased journalists, claimed that Flight 367 had been mistaken for an enemy aircraft and shot down by the Czechoslovak Air Force at an altitude of 800 metres (2,600 ft). Figure 1: Users discussing a topic defined by a Wikipedia article. In this real-world example from our Reddit dataset, information needed to ground responses is distributed throughout the source document. Introduction While end-to-end neural conversation models (Shang et al., 2015; Sordoni et al., 2015; Vinyals and Le, 2015; Serban et al., 2016; Li et al., 2016a; Gao et al., 2019a, etc.) are effective in learning how to be fluent, their responses are often vacuous and uninformative. A primary challenge thus lies in modeling what to say to make the conversation contentful. Several recent approaches have attempted to address this difficulty by conditioning the language decoder on external information sources, such as knowledge bases (Agarwal et al., 2018; Liu et al., 2018a), review posts (Ghazvininejad et al., 2018; Moghe et al., 2018), and even images (Das et al., 2017; Mostafazadeh et al.,"
P19-1539,N19-1423,0,\N,Missing
W13-3523,P08-1088,0,0.104927,"Science and Technology 8916-5 Takayama, Ikoma, Nara 630-0192, Japan {xiaodong-l,kevinduh,matsu}@is.naist.jp Abstract One approach for building a bilingual dictionary resource uses parallel sentence-aligned corpora. This is often done in the context of Statistical MT, using word alignment algorithms such as the IBM models (Brown et al., 1993; Och and Ney, 2003). Unfortunately, parallel corpora may be scarce for certain language-pairs or domains of interest (e.g., medical and microblog). Thus, the use of comparable corpora for bilingual dictionary extraction has become an active research topic (Haghighi et al., 2008; Vuli´c et al., 2011). Here, a comparable corpus is defined as collections of document pairs written in different languages but talking about the same topic (Koehn, 2010), such as interconnected Wikipedia articles. The challenge with bilingual dictionary extraction from comparable corpus is that existing word alignment methods developed for parallel corpus cannot be directly applied. We believe there are several desiderata for bilingual dictionary extraction algorithms: We propose a flexible and effective framework for extracting a bilingual dictionary from comparable corpora. Our approach is"
W13-3523,W11-0801,0,0.0288397,"translated as “panfry“). Table 3 shows the distribution of error types by a manual classification. Incorrect Alignment errors are most frequent, implying the topic models are doing a reasonable job of generating the topicaligned corpus. The amount of Incorrect Topic is not trivial, though, so we would still imagine more advanced topic models to help. Segmentation errors are in general hard to solve, even with a better word segmenter, since in general one-to-one crosslingual word correspondence is not consistent–we believe the solution is a system that naturally handles multi-word expressions (Baldwin, 2011). 5 10 15 20 Topic Count Figure 5: Power-law distribution of number of word types with X number of topics. topic model of (Mimno et al., 2009) assumes one topic distribution per document pair. For lowlevels of comparability, a small number of topics may not sufficiently model the differences in topical content. This suggests the use of hierarchical topic models (Haffari and Teh, 2009) or other variants in future work. 3. What are the statistical characteristics of topic-aligned corpora? First, we show the word-topic distribution from multilingual topic modeling in the K = 400 scenario (first s"
W13-3523,W02-0902,0,0.16114,"ceedings of the Seventeenth Conference on Computational Natural Language Learning, pages 212–221, c Sofia, Bulgaria, August 8-9 2013. 2013 Association for Computational Linguistics with seminal works of (Rapp, 1995; Fung and Lo, 1998). The main idea is to assume that translation pairs have similar contexts, i.e. the distributional hypothesis, so extraction consists of 3 steps: (1) identify context windows around words, (2) translate context words using a seed bilingual dictionary, and (3) extract pairs that have high resulting similarity. Methods differ in how the seed dictionary is acquired (Koehn and Knight, 2002; D´ejean et al., 2002) and how similarity is defined (Fung and Cheung, 2004; Tamura et al., 2012). Projection-based approaches have also been proposed, though they can be shown to be related to the aforementioned distributional approaches (Gaussier et al., 2004); for example, Haghighi (2008) uses CCA to map vectors in different languages into the same latent space. Laroche (2010) presents a good summary. Vuli´c et al. (2011) pioneered a new approach to bilingual dictionary extraction based on topic modeling approach which requires no seed dictionary. While our approach is motivated by (Vuli´c"
W13-3523,J93-2003,0,0.0904013,"Missing"
W13-3523,J10-4005,0,0.0165542,"parallel sentence-aligned corpora. This is often done in the context of Statistical MT, using word alignment algorithms such as the IBM models (Brown et al., 1993; Och and Ney, 2003). Unfortunately, parallel corpora may be scarce for certain language-pairs or domains of interest (e.g., medical and microblog). Thus, the use of comparable corpora for bilingual dictionary extraction has become an active research topic (Haghighi et al., 2008; Vuli´c et al., 2011). Here, a comparable corpus is defined as collections of document pairs written in different languages but talking about the same topic (Koehn, 2010), such as interconnected Wikipedia articles. The challenge with bilingual dictionary extraction from comparable corpus is that existing word alignment methods developed for parallel corpus cannot be directly applied. We believe there are several desiderata for bilingual dictionary extraction algorithms: We propose a flexible and effective framework for extracting a bilingual dictionary from comparable corpora. Our approach is based on a novel combination of topic modeling and word alignment techniques. Intuitively, our approach works by converting a comparable document-aligned corpus into a pa"
W13-3523,C10-1070,0,0.118909,"Missing"
W13-3523,P11-2071,0,0.0551353,"Missing"
W13-3523,P09-1030,0,0.0301144,"Missing"
W13-3523,C02-1166,0,0.107487,"Missing"
W13-3523,P11-2032,0,0.0177225,"k ). While this kind of topic-dependent translation has been proposed for the parallel corpus (Zhao and Xing, 2007), we are the first to enable it for comparable corpora. Our large-scale experiments demonstrated that the proposed framework outperforms existing baselines under both automatic metrics and manual evaluation. We further show that our topicdependent translation models can capture some of the polysemy phenomenon important in dictionary construction. Future work includes: 1. Exploring other topic models (Haffari and Teh, 2009) and word alignment techniques (DeNero and Macherey, 2011; Mermer and Saraclar, 2011; Moore, 2004) in our framework. 2. Extract lexicon from massive multilingual collections. Mausum (2009) and Shezaf (2010) show that language pivots significantly improve the precision of distribution-based approaches. Since multilingual topic models can easily be trained on more than 3 languages, we expect it will give a big boost to our approach. Acknowledgments We thank Mamoru Komachi, Shuhei Kondo and the anonymous reviewers for valuable discussions and comments. Part of this research was executed under the Commissioned Research of National Institute of Information and Communications Techn"
W13-3523,D09-1092,0,0.679415,"uli´c et al. (2011) pioneered a new approach to bilingual dictionary extraction based on topic modeling approach which requires no seed dictionary. While our approach is motivated by (Vuli´c et al., 2011), we exploit the topic model in a very different way (explained in Section 4.2). They do not use word alignments like we do and thus cannot model polysemy. Further, their approach requires training topic models with a large number of topics, which may limit the scalability of the approach. Recently, there has been much interest in multilingual topic models (MLTM) (Jagarlamudi and Daume, 2010; Mimno et al., 2009; Ni et al., 2009; Boyd-Graber and Blei, 2009). Many of these models give p(t|e) and p(t|f ), but stop short of extracting a bilingual lexicon. Although topic models can group related e and f in the same topic cluster, the extraction of a high-precision dictionary requires additional effort. One of our contributions here is an effective way to do this extraction using word alignment methods. Figure 1: Proposed Framework allel topic-aligned corpus, then apply word alignment methods to model co-occurence within topics. By employing topic models, we avoid the need for seed lexicon and operate pur"
W13-3523,P11-1043,0,0.0170978,"ls of the form p(we |wf , tk ). While this kind of topic-dependent translation has been proposed for the parallel corpus (Zhao and Xing, 2007), we are the first to enable it for comparable corpora. Our large-scale experiments demonstrated that the proposed framework outperforms existing baselines under both automatic metrics and manual evaluation. We further show that our topicdependent translation models can capture some of the polysemy phenomenon important in dictionary construction. Future work includes: 1. Exploring other topic models (Haffari and Teh, 2009) and word alignment techniques (DeNero and Macherey, 2011; Mermer and Saraclar, 2011; Moore, 2004) in our framework. 2. Extract lexicon from massive multilingual collections. Mausum (2009) and Shezaf (2010) show that language pivots significantly improve the precision of distribution-based approaches. Since multilingual topic models can easily be trained on more than 3 languages, we expect it will give a big boost to our approach. Acknowledgments We thank Mamoru Komachi, Shuhei Kondo and the anonymous reviewers for valuable discussions and comments. Part of this research was executed under the Commissioned Research of National Institute of Informati"
W13-3523,P04-1066,0,0.0348541,"ic-dependent translation has been proposed for the parallel corpus (Zhao and Xing, 2007), we are the first to enable it for comparable corpora. Our large-scale experiments demonstrated that the proposed framework outperforms existing baselines under both automatic metrics and manual evaluation. We further show that our topicdependent translation models can capture some of the polysemy phenomenon important in dictionary construction. Future work includes: 1. Exploring other topic models (Haffari and Teh, 2009) and word alignment techniques (DeNero and Macherey, 2011; Mermer and Saraclar, 2011; Moore, 2004) in our framework. 2. Extract lexicon from massive multilingual collections. Mausum (2009) and Shezaf (2010) show that language pivots significantly improve the precision of distribution-based approaches. Since multilingual topic models can easily be trained on more than 3 languages, we expect it will give a big boost to our approach. Acknowledgments We thank Mamoru Komachi, Shuhei Kondo and the anonymous reviewers for valuable discussions and comments. Part of this research was executed under the Commissioned Research of National Institute of Information and Communications Technology (NICT),"
W13-3523,P11-2093,0,0.0235328,"Missing"
W13-3523,W04-3208,0,0.0316964,"ning, pages 212–221, c Sofia, Bulgaria, August 8-9 2013. 2013 Association for Computational Linguistics with seminal works of (Rapp, 1995; Fung and Lo, 1998). The main idea is to assume that translation pairs have similar contexts, i.e. the distributional hypothesis, so extraction consists of 3 steps: (1) identify context windows around words, (2) translate context words using a seed bilingual dictionary, and (3) extract pairs that have high resulting similarity. Methods differ in how the seed dictionary is acquired (Koehn and Knight, 2002; D´ejean et al., 2002) and how similarity is defined (Fung and Cheung, 2004; Tamura et al., 2012). Projection-based approaches have also been proposed, though they can be shown to be related to the aforementioned distributional approaches (Gaussier et al., 2004); for example, Haghighi (2008) uses CCA to map vectors in different languages into the same latent space. Laroche (2010) presents a good summary. Vuli´c et al. (2011) pioneered a new approach to bilingual dictionary extraction based on topic modeling approach which requires no seed dictionary. While our approach is motivated by (Vuli´c et al., 2011), we exploit the topic model in a very different way (explaine"
W13-3523,P04-1067,0,0.342167,"that translation pairs have similar contexts, i.e. the distributional hypothesis, so extraction consists of 3 steps: (1) identify context windows around words, (2) translate context words using a seed bilingual dictionary, and (3) extract pairs that have high resulting similarity. Methods differ in how the seed dictionary is acquired (Koehn and Knight, 2002; D´ejean et al., 2002) and how similarity is defined (Fung and Cheung, 2004; Tamura et al., 2012). Projection-based approaches have also been proposed, though they can be shown to be related to the aforementioned distributional approaches (Gaussier et al., 2004); for example, Haghighi (2008) uses CCA to map vectors in different languages into the same latent space. Laroche (2010) presents a good summary. Vuli´c et al. (2011) pioneered a new approach to bilingual dictionary extraction based on topic modeling approach which requires no seed dictionary. While our approach is motivated by (Vuli´c et al., 2011), we exploit the topic model in a very different way (explained in Section 4.2). They do not use word alignments like we do and thus cannot model polysemy. Further, their approach requires training topic models with a large number of topics, which m"
W13-3523,J03-1002,0,0.0122219,"e known. For each document pair di = [dei , dfi ] consisting of English document dei and Foreign document dfi (where i ∈ {1, . . . , D}, D is number of document pairs), we know that dei and dfi talk about the same topics. While the monolingual topic model lets each document have its own so-called documentspecific distribution over topics, the multilingual topic model assumes that documents in each tuple share the same topic prior (thus the comparable corpora assumption) and each topic consists of several language-specific word distributions. The generative story is shown in Algorithm 1. 1993; Och and Ney, 2003), which proposes the following probabilistic model for alignment: p(e, a, |f ) ≈ f p(wie |wa(i) ) i=1 f p(wie |wa(i) ) (1) Here, captures the translation probability of the English word at position i from the foreign word at position j = a(i), where the actual alignment a is a hidden variable, and training can be done via EM. Although this model does not incorporate much linguistic knowledge, it enables us to find correspondence between distinct objects from paired sets. In machine translation, the distinct objects are words from different languages while the paired sets are sentence-aligned c"
W13-3523,P95-1050,0,0.492305,"s also rely on bilingual dictionaries as integral components. 3. Scalability: The approach should run efficiently an massively large-scale datasets. Our framework addresses the above desired points by exploiting a novel combination of topic models and word alignment, as shown in Figure 1. Intuitively, our approach works by first converting a comparable document-aligned corpus into a par212 Proceedings of the Seventeenth Conference on Computational Natural Language Learning, pages 212–221, c Sofia, Bulgaria, August 8-9 2013. 2013 Association for Computational Linguistics with seminal works of (Rapp, 1995; Fung and Lo, 1998). The main idea is to assume that translation pairs have similar contexts, i.e. the distributional hypothesis, so extraction consists of 3 steps: (1) identify context windows around words, (2) translate context words using a seed bilingual dictionary, and (3) extract pairs that have high resulting similarity. Methods differ in how the seed dictionary is acquired (Koehn and Knight, 2002; D´ejean et al., 2002) and how similarity is defined (Fung and Cheung, 2004; Tamura et al., 2012). Projection-based approaches have also been proposed, though they can be shown to be related"
W13-3523,N09-1020,0,0.0892861,"general hard to solve, even with a better word segmenter, since in general one-to-one crosslingual word correspondence is not consistent–we believe the solution is a system that naturally handles multi-word expressions (Baldwin, 2011). 5 10 15 20 Topic Count Figure 5: Power-law distribution of number of word types with X number of topics. topic model of (Mimno et al., 2009) assumes one topic distribution per document pair. For lowlevels of comparability, a small number of topics may not sufficiently model the differences in topical content. This suggests the use of hierarchical topic models (Haffari and Teh, 2009) or other variants in future work. 3. What are the statistical characteristics of topic-aligned corpora? First, we show the word-topic distribution from multilingual topic modeling in the K = 400 scenario (first step of Proposed, Cue, and JS). For each word type w, we count the number of topics it may appear in, i.e. nonzero probabilities according to p(w|t). Fig. 5 shows the number of word types that have x number of topics. This powerlaw is expected since we are modeling all words.9 Next we compute the statistics after constructing the topic-aligned corpora (Step 3 of Fig. 2). For each part"
W13-3523,P10-1011,0,0.163841,"Missing"
W13-3523,D12-1003,0,0.177235,"Sofia, Bulgaria, August 8-9 2013. 2013 Association for Computational Linguistics with seminal works of (Rapp, 1995; Fung and Lo, 1998). The main idea is to assume that translation pairs have similar contexts, i.e. the distributional hypothesis, so extraction consists of 3 steps: (1) identify context windows around words, (2) translate context words using a seed bilingual dictionary, and (3) extract pairs that have high resulting similarity. Methods differ in how the seed dictionary is acquired (Koehn and Knight, 2002; D´ejean et al., 2002) and how similarity is defined (Fung and Cheung, 2004; Tamura et al., 2012). Projection-based approaches have also been proposed, though they can be shown to be related to the aforementioned distributional approaches (Gaussier et al., 2004); for example, Haghighi (2008) uses CCA to map vectors in different languages into the same latent space. Laroche (2010) presents a good summary. Vuli´c et al. (2011) pioneered a new approach to bilingual dictionary extraction based on topic modeling approach which requires no seed dictionary. While our approach is motivated by (Vuli´c et al., 2011), we exploit the topic model in a very different way (explained in Section 4.2). The"
W13-3523,P11-2084,0,0.222493,"Missing"
W13-3523,H01-1033,0,\N,Missing
W13-4409,P00-1032,0,0.304001,"Missing"
W13-4409,O03-4002,0,0.0907584,"uency of a correct phrase is higher than the corresponding error template. Wu et al. (2010) proposed a system which implemented a translate model and a template module. Then the system merged the output of the two single models and reached a balanced performance on precision and recall. 3 Figure 1: System structure. 3.1 Language Model Based Method To generate the correction candidates, firstly we segment the sentence into words and then find all possible corrections based on the confusion set and a Chinese dictionary. In this study, we use the character based Chinese word segmentation model2 (Xue, 2003), which outperforms the word based word segmentation model in out-of-vocabulary recall. The model is trained on the Academia Sinica corpus, released under the Chinese word segmentation bake-off 20053 and the feature templates are the same in Sun (2011). For example, given the following Chinese sentence (here, the Chinese character in red indicates an error character): ”我 看 過 許 多 勇 敢 的 人 ， 不 怕措折 地 奮 鬥。”. Firstly, we segment the sentence into words separated by a slash as follows. ”我/看過/許多/勇敢/的/人/，/不怕/措折/的/ 奮鬥/。” . Secondly, we build a lattice, as shown in Figure 2, based on the following rules:"
W13-4409,O09-2007,0,0.0386347,"Missing"
W13-4409,W10-4107,0,0.157669,"detect Chinese spelling errors. They used CKIP word segmentation toolkit to generate correction candidates (CKIP, 1999). By incorporating a dictionary and confusion sets, the system can detect whether a segmented word contains error or not. Hung et al. (2008) proposed a system which was based on manually edited error templates (short phrases with one error). For the cost of editing error templates manually, Cheng et al. (2008) proposed an automatic error template generation system. The basic assumption is that the frequency of a correct phrase is higher than the corresponding error template. Wu et al. (2010) proposed a system which implemented a translate model and a template module. Then the system merged the output of the two single models and reached a balanced performance on precision and recall. 3 Figure 1: System structure. 3.1 Language Model Based Method To generate the correction candidates, firstly we segment the sentence into words and then find all possible corrections based on the confusion set and a Chinese dictionary. In this study, we use the character based Chinese word segmentation model2 (Xue, 2003), which outperforms the word based word segmentation model in out-of-vocabulary r"
W13-4409,J93-2003,0,\N,Missing
W13-4409,P11-1139,0,\N,Missing
W19-5042,W19-5039,0,0.171139,"earning fine-tuning, we use multi-task learning on NLI, RQE and QA tasks on general and medical domains to improve performance. The proposed methods are proved effective for natural language understanding in the medical domain, and we rank the first place on the QA task. 1 • General NLU embeddings: We use MT-DNN (Liu et al., 2019b) trained on GLUE benchmark(Wang et al., 2019). MT-DNN is trained on 10 tasks including NLI, question equivalence, and machine comprehension. These tasks correspond well to the target MEDIQA tasks but in different domains. Background The MEDIQA 2019 shared tasks (Ben Abacha et al., 2019) aim to improve the current state-ofthe-art systems for textual inference, question entailment and question answering in the medical domain. This ACL-BioNLP 2019 shared task is motivated by a need to develop relevant methods, techniques and gold standards for inference and entailment in the medical domain and their application to improve domain-specific information retrieval and question answering systems. The shared task consists of three parts: i) natural language inference (NLI) on MedNLI, ii) Recognizing Question Entailment (RQE), and iii) Question Answering (QA). Recent advancement in NLP"
W19-5042,N15-1092,1,0.700511,"cScholar scientific papers. Although SciBERT obtained state-of-the-art results on several singlesentence tasks, it lacks knowledge from other NLU tasks such as GLUE. In this paper, we investigate different methods to combine and transfer the knowledge from the two different sources and illustrate our results on the MEDIQA shared task. We name our method as DoubleTransfer, since it transfers knowledge from two different sources. Our method is based on fine-tuning both MT-DNN and SciBERT using multi-task learning, which has demonstrated the efficiency of knowledge transformation (Caruana, 1997; Liu et al., 2015; Xu et al., 2018; Liu et al., 2019b), and integrating models from both domains with ensembles. Related Works. Transfer learning has been widely used in training models in the medical do399 Proceedings of the BioNLP 2019 workshop, pages 399–405 c Florence, Italy, August 1, 2019. 2019 Association for Computational Linguistics Algorithm 1 Multi-task Fine-tuning with External Datasets Require: In-domain datasets D1 , ..., DK1 , External domain datasets DK1 +1 , ..., DK2 , max epoch, mixture ratio α 1: Initialize the model M 2: for epoch= 1, 2, ..., max epoch do 3: Divide each dataset Dk into Nk m"
W19-5042,P19-1441,1,0.934066,".com Abstract large-scale corpus and transfer it to downstream tasks. We investigate NLU in the medical (scientific) domain. From BERT, we need to adapt to i) The change from general domain corpus to scientific language; ii) The change from low-level language model tasks to complex NLU tasks. Although there is limited training data in NLU in the medical domain, we fortunately have pre-trained models from two intermediate steps: This paper describes our competing system to enter the MEDIQA-2019 competition. We use a multi-source transfer learning approach to transfer the knowledge from MT-DNN (Liu et al., 2019b) and SciBERT (Beltagy et al., 2019) to natural language understanding tasks in the medical domain. For transfer learning fine-tuning, we use multi-task learning on NLI, RQE and QA tasks on general and medical domains to improve performance. The proposed methods are proved effective for natural language understanding in the medical domain, and we rank the first place on the QA task. 1 • General NLU embeddings: We use MT-DNN (Liu et al., 2019b) trained on GLUE benchmark(Wang et al., 2019). MT-DNN is trained on 10 tasks including NLI, question equivalence, and machine comprehension. These tasks"
W19-5042,D18-1187,0,0.133695,"Missing"
W19-5042,N18-1101,0,0.0280719,"sting embeddings from SciBERT or MT-DNN. Transfer learning is also widely used in other tasks of NLP, such as machine translation (Bahdanau et al., 2014) and machine reading comprehension (Xu et al., 2018). 2 Fine-tuning details Algorithm. We fine-tune the two types of pretrained models on all the three tasks using multitask learning. As suggested by MEDIQA paper, we also fine-tune our model on MedQuAD (Abacha and Demner-Fushman, 2019), a medical QA dataset. We will provide details for fine-tuning on these datasets in Section 2.3. We additionally regularize the model by also training on MNLI (Williams et al., 2018). To prevent the negative transfer from MNLI, we put a larger weight on MEDIQA data by sampling MNLI data with less probability. Our algorithm is presented in Algorithm 1 and illustrated as Figure 1, which is a mixture ratio method for multitask learning inspired by Xu et al. (2018). We start with in-domain datasets D1 , ...DK1 (i.e., the MEDIQA tasks, K1 = 3) and external datasets DK1 +1 , ..., DK2 (in this case MNLI). We cast all the training samples as sentence pairs (s1 , s2 ) ∈ Dk , k = 1, 2, ..., K2 . In each epoch of training, we use all mini-batches from in-domain data, while only a sm"
W19-5042,N19-1271,1,\N,Missing
