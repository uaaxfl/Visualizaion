2020.lrec-1.283,Multiple Knowledge {G}raph{DB} ({MKGDB}),2020,-1,-1,2,0.462036,17226,stefano faralli,Proceedings of the 12th Language Resources and Evaluation Conference,0,"We present MKGDB, a large-scale graph database created as a combination of multiple taxonomy backbones extracted from 5 existing knowledge graphs, namely: ConceptNet, DBpedia, WebIsAGraph, WordNet and the Wikipedia category hierarchy. MKGDB, thanks the versatility of the Neo4j graph database manager technology, is intended to favour and help the development of open-domain natural language processing applications relying on knowledge bases, such as information extraction, hypernymy discovery, topic clustering, and others. Our resource consists of a large hypernymy graph which counts more than 37 million nodes and more than 81 million hypernymy relations."
L18-1444,A Large Multilingual and Multi-domain Dataset for Recommender Systems,2018,0,0,3,0,29997,giorgia tommaso,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,None
W17-4204,What to Write? A topic recommender for journalists,2017,8,4,4,1,31693,alessandro cucchiarelli,Proceedings of the 2017 {EMNLP} Workshop: Natural Language Processing meets Journalism,0,"In this paper we present a recommender system, What To Write and Why, capable of suggesting to a journalist, for a given event, the aspects still uncovered in news articles on which the readers focus their interest. The basic idea is to characterize an event according to the echo it receives in online news sources and associate it with the corresponding readers{'} communicative and informative patterns, detected through the analysis of Twitter and Wikipedia, respectively. Our methodology temporally aligns the results of this analysis and recommends the concepts that emerge as topics of interest from Twitter andWikipedia, either not covered or poorly covered in the published news articles."
J17-1005,Hashtag Sense Clustering Based on Temporal Similarity,2017,21,8,2,1,31695,giovanni stilo,Computational Linguistics,0,"Hashtags are creative labels used in micro-blogs to characterize the topic of a message/discussion. Regardless of the use for which they were originally intended, hashtags cannot be used as a means to cluster messages with similar content. First, because hashtags are created in a spontaneous and highly dynamic way by users in multiple languages, the same topic can be associated with different hashtags, and conversely, the same hashtag may refer to different topics in different time periods. Second, contrary to common words, hashtag disambiguation is complicated by the fact that no sense catalogs (e.g., Wikipedia or WordNet) are available; and, furthermore, hashtag labels are difficult to analyze, as they often consist of acronyms, concatenated words, and so forth. A common way to determine the meaning of hashtags has been to analyze their context, but, as we have just pointed out, hashtags can have multiple and variable meanings. In this article, we propose a temporal sense clustering algorithm based on the idea that semantically related hashtags have similar and synchronous usage patterns."
R13-1084,Automated learning of everyday patients{'} language for medical blogs analytics,2013,17,1,4,1,31695,giovanni stilo,Proceedings of the International Conference Recent Advances in Natural Language Processing {RANLP} 2013,0,"Analyzing how people discuss about health-related topics on dedicated forums and social networks such as Twitter, can provide valuable insight for syndromic surveillance and to predict disease outbreaks. In this paper we present a minimally trained algorithm to learn associations between technical and everyday language terms, based on pattern generalization and complete linkage clustering, and we then assess its utility on a case study of five common syndromes for surveillance purposes."
J13-3007,{O}nto{L}earn Reloaded: A Graph-Based Algorithm for Taxonomy Induction,2013,66,136,1,1,17227,paola velardi,Computational Linguistics,0,"In 2004 we published in this journal an article describing OntoLearn, one of the first systems to automatically induce a taxonomy from documents and Web sites. Since then, OntoLearn has continued to be an active area of research in our group and has become a reference work within the community. In this paper we describe our next-generation taxonomy learning methodology, which we name OntoLearn Reloaded. Unlike many taxonomy learning approaches in the literature, our novel algorithm learns both concepts and relations entirely from scratch via the automated extraction of terms, definitions, and hypernyms. This results in a very dense, cyclic and potentially disconnected hypernym graph. The algorithm then induces a taxonomy from this graph via optimal branching and a novel weighting policy. Our experiments show that we obtain high-quality results, both when building brand-new taxonomies and when reconstructing sub-hierarchies of existing taxonomies."
velardi-etal-2012-new,A New Method for Evaluating Automatically Learned Terminological Taxonomies,2012,15,7,1,1,17227,paola velardi,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"Abstract Evaluating a taxonomy learned automatically against an existing gold standard is a very complex problem, because differences stem from the number, label, depth and ordering of the taxonomy nodes. In this paper we propose casting the problem as one of comparing two hierarchical clusters. To this end we defined a variation of the Fowlkes and Mallows measure (Fowlkes and Mallows, 1983). Our method assigns a similarity value B{\textasciicircum}i{\_}(l,r) to the learned (l) and reference (r) taxonomy for each cut i of the corresponding anonymised hierarchies, starting from the topmost nodes down to the leaf concepts. For each cut i, the two hierarchies can be seen as two clusterings C{\textasciicircum}i{\_}l , C{\textasciicircum}i{\_}r of the leaf concepts. We assign a prize to early similarity values, i.e. when concepts are clustered in a similar way down to the lowest taxonomy levels (close to the leaf nodes). We apply our method to the evaluation of the taxonomy learning methods put forward by Navigli et al. (2011) and Kozareva and Hovy (2010)."
P10-1134,Learning Word-Class Lattices for Definition and Hypernym Extraction,2010,41,97,2,0.570175,1617,roberto navigli,Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,1,"Definition extraction is the task of automatically identifying definitional sentences within texts. The task has proven useful in many research areas including ontology learning, relation extraction and question answering. However, current approaches -- mostly focused on lexicosyntactic patterns -- suffer from both low recall and precision, as definitional sentences occur in highly variable syntactic structures. In this paper, we propose Word-Class Lattices (WCLs), a generalization of word lattices that we use to model textual definitions. Lattices are learned from a dataset of definitions from Wikipedia. Our method is applied to the task of definition and hypernym extraction and compares favorably to other pattern generalization methods proposed in the literature."
navigli-etal-2010-annotated,An Annotated Dataset for Extracting Definitions and Hypernyms from the Web,2010,25,16,2,0.570175,1617,roberto navigli,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"This paper presents and analyzes an annotated corpus of definitions, created to train an algorithm for the automatic extraction of definitions and hypernyms from web documents. As an additional resource, we also include a corpus of non-definitions with syntactic patterns similar to those of definition sentences, e.g.: ''``An android is a robot'''' vs. ''``Snowcap is unmistakable''''. Domain and style independence is obtained thanks to the annotation of a large and domain-balanced corpus and to a novel pattern generalization algorithm based on word-class lattices (WCL). A lattice is a directed acyclic graph (DAG), a subclass of nondeterministic finite state automata (NFA). The lattice structure has the purpose of preserving the salient differences among distinct sequences, while eliminating redundant information. The WCL algorithm will be integrated into an improved version of the GlossExtractor Web application (Velardi et al., 2008). This paper is mostly concerned with a description of the corpus, the annotation strategy, and a linguistic analysis of the data. A summary of the WCL algorithm is also provided for the sake of completeness."
W06-0501,Enriching a Formal Ontology with a Thesaurus: an Application in the Cultural Heritage Domain,2006,14,20,2,0.763889,1617,roberto navigli,Proceedings of the 2nd Workshop on Ontology Learning and Population: Bridging the Gap between Text and Knowledge,0,"This paper describes a pattern-based method to automatically enrich a core ontology with the definitions of a domain glossary. We show an application of our methodology to the cultural heritage domain, using the CIDOC CRM core ontology. To enrich the CIDOC, we use available resources such as the AAT art and architecture glossary, WordNet, the Dmoz taxonomy for named entities, and others."
W04-0844,Structural semantic interconnection: a knowledge-based approach to Word Sense Disambiguation,2004,0,10,2,1,1617,roberto navigli,"Proceedings of {SENSEVAL}-3, the Third International Workshop on the Evaluation of Systems for the Semantic Analysis of Text",0,None
cucchiarelli-etal-2004-automatic,Automatic Generation of Glosses in the {O}nto{L}earn System,2004,8,3,4,1,31693,alessandro cucchiarelli,Proceedings of the Fourth International Conference on Language Resources and Evaluation ({LREC}{'}04),0,"OntoLearn is a system for automatic acquisition of specialized ontologies from domain corpora, based on a syntactic pattern matching technique for word sense disambiguation, called structural semantic interconnection (SSI). We use SSI to extract from corpora complex domain concepts and create a specialized version of WordNet. In order to facilitate the task of domain specialists who inspects and evaluate the newly acquired domain ontology, we defined a method to automatically generate glosses for the learned concepts. Glosses provide an informal description, in natural language, of the formal specifications of a concept, facilitating a perconcept evaluation of the ontology by domain specialists, who are usually unfamiliar with the formal language used to describe a computational ontology. The proposed evaluation framework has been tested in a financial domain."
J04-2002,Learning Domain Ontologies from Document Warehouses and Dedicated Web Sites,2004,36,359,2,1,1617,roberto navigli,Computational Linguistics,0,"We present a method and a tool, OntoLearn, aimed at the extraction of domain ontologies from Web sites, and more generally from documents shared among the members of virtual organizations. OntoLearn first extracts a domain terminology from available documents. Then, complex domain terms are semantically interpreted and arranged in a hierarchical fashion. Finally, a general-purpose ontology, WordNet, is trimmed and enriched with the detected domain concepts. The major novel aspect of this approach is semantic interpretation, that is, the association of a complex concept with a complex term . This involves finding the appropriate WordNet concept for each word of a terminological string and the appropriate conceptual relations that hold among the concept components. Semantic interpretation is based on a new word sense disambiguation algorithm, called structural semantic interconnections."
C04-1150,Quantitative and Qualitative Evaluation of the {O}nto{L}earn Ontology Learning System,2004,9,38,2,1,1617,roberto navigli,{COLING} 2004: Proceedings of the 20th International Conference on Computational Linguistics,0,"Ontology evaluation is a critical task, even more so when the ontology is the output of an automatic system, rather than the result of a conceptualisation effort produced by a team of domain specialists and knowledge engineers. This paper provides an evaluation of the OntoLearn ontology learning system. The proposed evaluation strategy is twofold: first, we provide a detailed quantitative analysis of the ontology learning algorithms, in order to compute the accuracy of OntoLearn under different learning circumstances. Second, we automatically generate natural language descriptions of formal concept specifications, in order to facilitate per-concept qualitative analysis by domain specialists."
navigli-velardi-2002-automatic,Automatic Adaptation of {W}ord{N}et to Domains,2002,6,17,2,1,1617,roberto navigli,Proceedings of the Third International Conference on Language Resources and Evaluation ({LREC}{'}02),0,The objective of this paper is to present a method to automatically enrich WordNet with sub-trees of concepts in a given language domain. WordNet is then trimmed to reduce unnecessary ambiguity and singleton nodes. The process is based on the use of statistical method and linguistic processing to extract candidate domain terms. Multiword terms are semantically disambiguated and interpreted using ontological and contextual knowledge stored in WordNet on singleton words.
W01-1005,Identification of Relevant Terms to Support the Construction of Domain Ontologies,2001,17,69,1,1,17227,paola velardi,Proceedings of the {ACL} 2001 Workshop on Human Language Technology and Knowledge Management,0,"Though the utility of domain Ontologies is now widely acknowledged in the IT (Information Technology) community, several barriers must be overcome before Ontologies become practical and useful tools. One important achievement would be to reduce the cost of identifying and manually entering several thousand-concept descriptions. This paper describes a text mining technique to aid an Ontology Engineer to identify the important concepts in a Domain Ontology."
J01-1005,Unsupervised Named Entity Recognition Using Syntactic and Semantic Contextual Evidence,2001,12,46,2,1,31693,alessandro cucchiarelli,Computational Linguistics,0,"Proper nouns form an open class, making the incompleteness of manually or automatically learned classification rules an obvious problem. The purpose of this paper is twofold: first, to suggest the use of a complementary backup method to increase the robustness of any hand-crafted or machine-learning-based NE tagger; and second, to explore the effectiveness of using more fine-grained evidence--namely, syntactic and semantic contextual knowledge---in classifying NEs."
W00-0105,Dependency of context-based Word Sense Disambiguation from representation and domain complexity,2000,6,1,1,1,17227,paola velardi,{NAACL}-{ANLP} 2000 Workshop: Syntactic and Semantic Complexity in Natural Language Processing Systems,0,"Word Sense Disambiguation (WSD) is a central task in the area of Natural Language Processing. In the past few years several context-based probabilistic and machine learning methods for WSD have been presented in literature. However, an important area of research that has not been given the attention it deserves is a formal analysis of the parameters affecting the performance of the learning task faced by these systems. Usually performance is estimated by measuring precision and recall of a specific algorithm for specific test sets and environmental conditions. Therefore, a comparison among different learning systems and an objective estimation of the difficulty of the learning task is extremely difficult.In this paper we propose, in the framework of Computational Learning theory, a formal analysis of the relations between accuracy of a context-based WSD system, the complexity of the context representation scheme, and the environmental conditions (e.g. the complexity of language domain and concept inventory)."
cucchiarelli-etal-2000-will,Will Very Large Corpora Play For Semantic Disambiguation The Role That Massive Computing Power Is Playing For Other {AI}-Hard Problems?,2000,0,6,3,1,31693,alessandro cucchiarelli,Proceedings of the Second International Conference on Language Resources and Evaluation ({LREC}{'}00),0,None
W98-0711,Automatic Adaptation of {W}ord{N}et to Sublanguages and to Computational Tasks,1998,8,3,5,1,12620,roberto basili,Usage of {W}ord{N}et in Natural Language Processing Systems,0,"Semantically tagging a corpus is useful for many intermediate NLP tasks such as: acquisition of word argument structures in sublanguages, acquisition of syntactic disambiguation cues, terminology learning, etc. Semantic categories allow the generalization of observed word patterns, and facilitate the discovery of irecurrent sublanguage phenomena and selectional rules of various types. Yet, as opposed to POS tags in morphology, there is no consensus in literature about the type and granularity of the category inventory. In addition, most available on-line taxonomies, as WordNet, are over ambiguous and, at the same time, may not include many domain-dependent senses of words. In this paper we describe a method to adapt a general purpose taxonomy to an application sub[anguage: flint, we prune branches of the Wordnet hierarchy that are too  fine grained for the domain: then. a statistical model of classes is built from corpus contexts to sort the different classifications or assign a classification to known and unknown words, respectively. 1 I n t r o d u c t i o n Lexical learning methods based on the use of semantic categories are faced with the problem of overambiguity and entangled structures of Thesaura and dictionaries. WordNet and Roget's Thesaura were not initially conceived, despite their success among researchers in lexical statistics, as tools for automatic language processing. The purpose was rather to provide the linguists with a very refined, general purpose, linguistically motivated source of taxonomic knowledge. As a consequence, in most on-fine Thesaura words are extremely ambiguous. with very subtle distinctions among senses. High ambiguity, entangled nodes, and asymmetry have already been emphasized in (Hearst and Shutze, 1993) as being an obstacle to the effective use of on-line Thesaura in corpus linguistics. In most cases, the noise introduced by overambiguity almost overrides the positive effect of semantic clustering. For example, in (BriIl and Resnik, 1994) clustering PP heads according to WordNet synsets produced only a [% improvement in a PP disambiguation task. with respect to the non-clustered method. A subsequent paper (Resnik. 1997) reports of a 40% precision in a sense disambiguation task. always based on generalization through WordNet synsets. Context-based sense clisambiguation becomes a prohibitive task on a wide-scale basis, because when words in the context of unambiguous word are replaced by their s.vnsets, there is a multiplication of possible contexts, rather than a generalization. [n (Agirre and Rigau. 1996) a method called Conceptual Distance is proposed to reduce this problem, but the reported performance in disambiguation still does not reach 50%. On the other hand, (Dolan. 1994) and (Krovetz and Croft. 1992) claim that fine-grained semantic distinctions are unlikely to be of practical value for many applications. Our experience supports this claim: often, what matters is to be able to distinguish among contrastive (Pustejowsky. 1995) ambiguities of the bank_river bank_organisation flavor. The problem however is that the notion ofcoutrast ive is domain-dependent. Depending upon the sublanguage (e.g. medicine, finance, computers. etc.) and upon the specific NLP application (e.g. Information Extraction, Dialogue etc.) a given semantic label may be too general or too specific for the task at hand. For example, the word line has 27 senses in WordNet. many of which draw subtle distinctions e.g. line of ~cork (sense 26) and line of products (sense [9). In aa"
P98-1045,Automatic Semantic Tagging of Unknown Proper Names,1998,14,23,3,1,31693,alessandro cucchiarelli,"36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics, Volume 1",1,"Implemented methods for proper names recognition rely on large gazetteers of common proper nouns and a set of heuristic rules (e.g. Mr. as an indicator of a PERSON entity type). Though the performance of current PN recognizers is very high (over 90%), it is important to note that this problem is by no means a solved problem. Existing systems perform extremely well on newswire corpora by virtue of the availability of large gazetteers and rule bases designed for specific tasks (e.g. recognition of Organization and Person entity types as specified in recent Message Understanding Conferences MUC).However, large gazetteers are not available for most languages and applications other than newswire texts and, in any case, proper nouns are an open class.In this paper we describe a context-based method to assign an entity type to unknown proper names (PNs). Like many others, our system relies on a gazetteer and a set of context-dependent heuristics to classify proper nouns. However, due to the unavailability of large gazetteers in Italian, over 20% detected PNs cannot be semantically tagged.The algorithm that we propose assigns an entity type to an unknown PN based on the analysis of syntactically and semantically similar contexts already seen in the application corpus.The performance of the algorithm is evaluated not only in terms of precision, following the tradition of MUC conferences, but also in terms of Information Gain, an information theoretic measure that takes into account the complexity of the classification task."
C98-1045,Automatic Semantic Tagging of Unknown Proper Names,1998,14,23,3,1,31693,alessandro cucchiarelli,{COLING} 1998 Volume 1: The 17th International Conference on Computational Linguistics,0,"Implemented methods for proper names recognition rely on large gazetteers of common proper nouns and a set of heuristic rules (e.g. Mr. as an indicator of a PERSON entity type). Though the performance of current PN recognizers is very high (over 90%), it is important to note that this problem is by no means a solved problem. Existing systems perform extremely well on newswire corpora by virtue of the availability of large gazetteers and rule bases designed for specific tasks (e.g. recognition of Organization and Person entity types as specified in recent Message Understanding Conferences MUC).However, large gazetteers are not available for most languages and applications other than newswire texts and, in any case, proper nouns are an open class.In this paper we describe a context-based method to assign an entity type to unknown proper names (PNs). Like many others, our system relies on a gazetteer and a set of context-dependent heuristics to classify proper nouns. However, due to the unavailability of large gazetteers in Italian, over 20% detected PNs cannot be semantically tagged.The algorithm that we propose assigns an entity type to an unknown PN based on the analysis of syntactically and semantically similar contexts already seen in the application corpus.The performance of the algorithm is evaluated not only in terms of precision, following the tradition of MUC conferences, but also in terms of Information Gain, an information theoretic measure that takes into account the complexity of the classification task."
A97-1055,Automatic Selection of Class Labels from a Thesaurus for an Effective Semantic Tagging of Corpora.,1997,16,8,2,1,31693,alessandro cucchiarelli,Fifth Conference on Applied Natural Language Processing,0,"It is widely accepted that tagging text with semantic information would improve the quality of lexical learning in corpus-based NLP methods. However available on-line taxonomies are rather entangled and introduce an unnecessary level of ambiguity. The noise produced by the redundant number of tags often overrides the advantage of semantic tagging. In this paper we propose an automatic method to select from WordNet a subset of domain-appropriate categories that effectively reduce the overambiguity of WordNet, and help at identifying and categorise relevant language patterns in a more compact way. The method is evaluated against a manually tagged corpus, SEMCOR."
J96-4006,Integrating General-purpose and Corpus-based Verb Classification,1996,8,14,3,1,12620,roberto basili,Computational Linguistics,0,"Le probleme de la generalite des taxonomies lexicales (notamment verbales) est depuis longtemps source de debats en linguistique informatique. Les relations similaires suggerees par la structure thematique des mots dans les phrases dependent en effet fortement du domaine et il est difficile de trouver des invariants communs entre les sous-langages. Les As. analysent ici les relations entre une methode purement inductive, telle que le systeme de regroupement conceptuel CIAULA, base sur des corpus, et une classification de domaine general, encodee a la main, telle que WordNet. Leur objectif est d'identifier leurs points communs et leurs differences et d'examiner la possibilte d'une integration fructueuse des 2 approches"
W94-0102,The Noisy Channel and the Braying Donkey,1994,-1,-1,3,1,12620,roberto basili,The Balancing Act: Combining Symbolic and Statistical Approaches to Language,0,None
W93-0107,Hierarchical Clustering of Verbs,1993,17,2,3,1,12620,roberto basili,Acquisition of Lexical Knowledge from Text,0,This paper addresses the problem of performing a hierarchical cluster analysis on objects that are measured on the same variable on a number of equally spaced points. Such data are typically collected in longitudinal studies or in experiments where electro-physiological measurements are registered (such as EEG or EMG). A generalized inter-object distance measure is defined that takes into account various aspects of the similarity between the functions from which the data are sampled. A mathematical programming procedure is developed for weighting these aspects in such a way that the resulting inter-object distances optimally satisfy the ultrametric inequality. These optimally weighted distances can then be subjected to any existing hierarchical clustering procedure. The new approach is illustrated on an artificial data set and some possible limitations and extensions of the new method are discussed.
A92-1013,Computational Lexicons: the Neat Examples and the Odd Exemplars,1992,24,27,3,1,12620,roberto basili,Third Conference on Applied Natural Language Processing,0,"When implementing computational lexicons it is important to keep in mind the texts that a NLP system must deal with. Words relate to each other in many different, often queer, ways: this information is rarely found in dictionaries, and it is quite hard to be invented a priori, despite the imagination that linguists exhibit at inventing esoteric examples.In this paper we present the results of an experiment in learning from corpora the frequent selectional restrictions holding between content words. The method is based on the analysis of word associations augmented with syntactic markers and semantic tags. Word pairs are extracted by a morphosyntactic analyzer and clustered according to their semantic tags. A statistical measure is applied to the data to evaluate the significance of a detected relation. Clustered association data render the study of word associations more interesting with several respects: data are more reliable even for smaller corpora, more easy to interpret, and have many practical applications in NLP."
J91-2002,How to Encode Semantic Knowledge: A Method for Meaning Representation and Computer-Aided Acquisition,1991,25,48,1,1,17227,paola velardi,Computational Linguistics,0,"Natural language processing will not be able to compete with traditional information retrieval unless high-coverage techniques are developed. It is commonly agreed that a poor encoding of the semantic lexicon is the bottleneck of many existing systems. A hand encoding of semantic knowledge on an extensive basis is not realistic; hence, it is important to devise methods by which such knowledge can be acquired in part or entirely by a computer. But what type of semantic knowledge could be automatically learned, from which sources, and by what methods? This paper explores the above issues and proposes an algorithm to learn syncategorematic concepts from text exemplars. What is learned about a concept is not its defining features, such as kinship, but rather its patterns of use.The knowledge acquisition method is based on learning by observations; observations are examples of word co-occurrences (collocations) in a large corpus, detected by a morphosyntactic analyzer. A semantic bias is used to associate collocations with the appropriate meaning relation, if one exists. Based upon single or multiple examples, the acquired knowledge is then generalized to create semantic rules on concept uses.Interactive human intervention is required in the training phase, when the bias is defined and refined. The duration of this phase depends upon the semantic closure of the sublanguage on which the experiment is carried out. After training, final approval by a linguist is still needed for the acquired semantic rules. At the current stage of experimentation of this system, it is unclear whether and when human supervision could be further reduced."
C90-2066,Why Human Translators Still Sleep in Peace? (Four Engineering and Linguistic Gaps in Nlp),1990,10,2,1,1,17227,paola velardi,{COLING} 1990 Volume 2: Papers presented to the 13th International Conference on Computational Linguistics,0,"Because they will keep their job quite for a few.This paper has been inspired by a recent editorial on the Financial Times, that gives a discouraging overview of commercial natural language processing systems ('the computer that can sustain a natural language conversation... is unlikely to exist for several decades'). Computational linguists are not so much concerned with applications but computer scientists have the ultimate objective to build systems that can 'increase the acceptability of computers in everyday situations.' Eventually, linguists as well would profit by a significant break-through in natural language processing.This paper is a brief dissertation on four engineering and linguistic issues we believe critical for a more striking success of NLP: extensive acquisition of the semantic lexicon, formal performance evaluation methods to evaluate systems, development of shell systems for rapid prototyping and customization, and finally a more linguistically motivated approach to word categorization."
P89-1023,Computer Aided Interpretation of Lexical Cooccurrences,1989,22,12,1,1,17227,paola velardi,27th Annual Meeting of the Association for Computational Linguistics,1,"This paper addresses the problem of developing a large semantic lexicon for natural language processing. The increasing availability of machine readable documents offers an opportunity to the field of lexical semantics, by providing experimental evidence of word uses (on-line texts) and word definitions (on-line dictionaries).The system presented hereafter, PETRARCA, detects word cooccurrences from a large sample of press agency releases on finance and economics, and uses these associations to build a case-based semantic lexicon. Syntactically valid cooccurences including a new word W are detected by a high-coverage morphosyntactic analyzer. Syntactic relations are interpreted e.g. replaced by case relations, using a a catalogue of patterns/interpretation pairs, a concept type hierarchy, and a set of selectional restriction rules on semantic interpretation types."
E87-1040,A Structured Representation of Word-Senses for Semantic Analysis.,1987,7,22,2,0,39305,maria pazienza,Third Conference of the {E}uropean Chapter of the Association for Computational Linguistics,0,"A framework for a structured representation of semantic knowledge (e.g. word-senses) has been defined at the IBM Scientific Center of Roma, as part of a project on Italian Text Understanding. This representation, based on the conceptual graphs formalism [SOW84], expresses deep knowledge (pragmatic) on word-senses. The knowledge base data structure is such as to provide easy access by the semantic verification algorithm. This paper discusses some important problem related to the definition of a semantic knowledge base, as depth versus generality, hierarchical ordering of concept types, etc., and describes the solutions adopted within the text understanding project."
