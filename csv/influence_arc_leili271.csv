2020.acl-demos.1,P16-1129,0,0.0348835,"Missing"
2020.acl-demos.1,2020.wmt-1.63,0,0.0143566,"he historical records of each team. • In-match Description. It describes most important events in the game such as “some3 Table 1: Examples of Sports News Generation Time 23’ Category Score Player Didac Team Espanyol Generated Text 第23分 钟 ， 西 班 牙 人 迪达克打入一球。 35’ Yellow Card Mubarak Alav´es 第35分 钟 ， 阿 拉 维 斯 穆巴拉克吃到一张黄 牌。 approach 1 . Furthermore, our machine translation system leverages named-entity (NE) replacement for glossaries including team name, player name and so on to improve the translation accuracy. It can be further improved by recent machine translation techniques (Yang et al., 2020; Zheng et al., 2020). 阿拉维斯 与 ⻄班⽛⼈ 的 ⽐赛 打 成 了 input text sequence (Wang et al., 2017). The architecture is illustrated in Figure 4, we made the following augmentations on the base Tacotron 2 model: • We applied an additional speaker as well as language embedding to support multi-speaker and multilingual input. 平⼿ • We introduced a variational autoencoder-style residual encoder to encode the variational length mel into a fix length latent representation, and then conditioned the representation to the decoder. Transformer Encoder Named Entity Replacement Translated Text In the 23rd minute, Espanyol Didac scored a go"
2020.acl-main.314,P19-1285,0,0.0297381,"a sets to verify the effectiveness of the proposed MC-Tailor. Empirical results show that MC-Tailor can generate significantly better samples than finetuning, and the resulting model distributions of our model are closer to real data distributions. 2 Pre-Trained Language Model Language models generally estimate the density of sentences in real context within an autoregressive style: N Y P (x) = P (xi |x[1:i−1] ), (1) i=1 where x is a sentence with length N . Recently, with an extremely large number of parameters, pretrained language models like GPT-2 (Radford et al., 2019) and Transformer-XL (Dai et al., 2019) have shown great promise in text generation. PLMs are first trained on a huge general domain data set and then fine-tuned on specific domain datasets of different downstream tasks. Specifically, given a pre-trained GPT2 model, to generate sentences of email domain, we always need to fine-tune the GPT2 on a small set of email domain corpus. Additionally, PLMs have some other important applications. Miao et al. (2019) use fine-tuned language models for constrained text generation. Wolf et al. (2019) fine-tune GPT-2 on a dialog data set to boost the performance of dialog system. However, as stat"
2020.acl-main.314,I17-1099,0,0.026123,"m. Results of SMC are not reported since it leads to very poor Rev-PPLs because of the lack of sample diversity. 4.1 Experimental Setup We conduct experiments on 9 data sets with different styles and sizes. And we use five different metrics, including human evaluation, to measure the generation performance of each method. Datasets. We use the following data sets for experiments. • Ontonotes (Pradhan et al., 2013) is a multigenre data set for sequence annotation. We use sentences from six genres (bn, bc, mz, nw, tc, wb) for the experiment. • Switchboard (Jurafsky et al., 1997) and DailyDialog (Li et al., 2017) are large and medium scale dialog data sets, of which only responses are used for the experiment. • IWSLT-16 (Cettolo et al., 2016) is a data set of paired conference speeches for machine translation. We use English sentences from De-En pairs to test model performance on the special conference speech domain. Evaluation Metrics. To evaluate the generation quality and diversity, we use the following metrics. • PPL reflects the average density of samples from test set in a generative model. Models with lower PPLs have more similar model distributions with real contexts. Unlike baseline models, M"
2020.emnlp-main.127,P19-1279,0,0.0281002,"t and its desired relations from DocRED (Yao et al., 2019). Entity mentions and relations involved in these relation instances are colored. Other mentions are underlined for clarity. Introduction The task of identifying semantic relations between entities from text, namely relation extraction (RE), plays a crucial role in a variety of knowledge-based applications, such as question answering (Yu et al., 2017, Qiu et al., 2019) and large-scale knowledge graph construction. Previous methods (Zeng et al., 2014; Zeng et al., 2015; Xiao and Liu, 2016; Zhang et al., 2017; Zhang et al., 2018; Baldini Soares et al., 2019) focus on sentence-level RE, which predicts relations among entities in a single sentence. However, sentence-level RE models suffer from an inevitable limitation – they fail to recognize relations between entities across sentences. Hence, extracting relations at the document-level is necessary for a holistic understanding of knowledge in text. ∗ † Equal contribution. Corresponding author. There are several major challenges in effective relation extraction at the document-level. Firstly, the subject and object entities involved in a relation may appear in different sentences. Therefore a relati"
2020.emnlp-main.127,D19-1498,0,0.744909,"Ve/HIN-BERTbase , proposed by Tang et al. (2020). Hierarchical Inference Network (HIN) aggregate information from entity-level, sentence-level, and document-level to predict target relations, and use GloVe (Pennington et al., 2014) or BERTbase for word embedding. LSR-GloVe/LSR-BERTbase , proposed by Nan et al. (2020) recently. They construct a graph based on the dependency tree and predict relations by latent structure induction and GCN. Nan et al. (2020) also adapted four graph-based state-of-the-art RE models to DocRED, including GAT (Velickovic et al., 2017), GCNN (Sahu et al., 2019), EoG (Christopoulou et al., 2019), and AGGCN (Guo et al., 2019). We also include their results. Following Yao et al. (2019), we use the widely used metrics F1 and AUC in our experiment. We also use Ign F1 and Ign AUC, which calculate F1 and AUC excluding the common relation facts in the training and dev/test sets. 4.4 Results We show GAIN’s performance on the DocRED dataset in Table 2, in comparison with other baselines. Among the models not using BERT or BERT variants, GAIN-GloVe consistently outperforms all sequential-based and graph-based strong baselines by 0.9 ∼ 12.82 F1 score on the test set. Among the models using BERT"
2020.emnlp-main.127,N19-1423,0,0.0191511,"GloVe (100d) and BiLSTM (256d) as word embedding and encoder. GAINBERTbase and GAIN-BERTlarge use BERTbase and BERTlarge as encoder respectively and the learning rate is set to 1e−5 . 4.3 Baselines and Evaluation Metrics We use the following models as baselines. Yao et al. (2019) proposed models to encode the document into a sequence of hidden state vector {hi }ni=1 using CNN (Fukushima, 1980), LSTM (Hochreiter and Schmidhuber, 1997), and BiLSTM (Schuster and Paliwal, 1997) as their encoder, and predict relations between entities with their representations. Other pre-trained models like BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019), and CorefBERT (Ye et al., 2020) are also used as encoder (Wang et al., 2019a; Ye et al., 2020) to document-level RE task. Context-Aware, also proposed by Yao et al. (2019) on DocRED adapted from (Sorokin and Gurevych, 2017), uses an LSTM to encode the text, but further utilizes attention mechanism to absorb the context relational information for predicting. BERT-Two-Stepbase , proposed by Wang et al. (2019a) on DocRED. Though similar to BERTREbase , it first predicts whether two entities have a relationship and then predicts the specific target relation. HIN-GloVe"
2020.emnlp-main.127,P19-1024,0,0.0534732,". (2020). Hierarchical Inference Network (HIN) aggregate information from entity-level, sentence-level, and document-level to predict target relations, and use GloVe (Pennington et al., 2014) or BERTbase for word embedding. LSR-GloVe/LSR-BERTbase , proposed by Nan et al. (2020) recently. They construct a graph based on the dependency tree and predict relations by latent structure induction and GCN. Nan et al. (2020) also adapted four graph-based state-of-the-art RE models to DocRED, including GAT (Velickovic et al., 2017), GCNN (Sahu et al., 2019), EoG (Christopoulou et al., 2019), and AGGCN (Guo et al., 2019). We also include their results. Following Yao et al. (2019), we use the widely used metrics F1 and AUC in our experiment. We also use Ign F1 and Ign AUC, which calculate F1 and AUC excluding the common relation facts in the training and dev/test sets. 4.4 Results We show GAIN’s performance on the DocRED dataset in Table 2, in comparison with other baselines. Among the models not using BERT or BERT variants, GAIN-GloVe consistently outperforms all sequential-based and graph-based strong baselines by 0.9 ∼ 12.82 F1 score on the test set. Among the models using BERT or BERT variants, GAINBERTbas"
2020.emnlp-main.127,P82-1020,0,0.80706,"Missing"
2020.emnlp-main.127,N19-1370,0,0.062618,"Missing"
2020.emnlp-main.127,2021.ccl-1.108,0,0.0767001,"Missing"
2020.emnlp-main.127,P16-2022,0,0.0280836,"ion function. With this module, an entity can be represented by fusing information from its mentions, which usually spread in multiple sentences. Moreover, potential reasoning clues are modeled by different paths between entities. Then they can be integrated with the attention mechanism so that we will take into account latent logical reasoning chains to predict relations. 3.4 Classification Module For each entity pair (eh , et ), we concatenate the following representations: (1) the head and tail entity representation eh and et derived in the Entity-level Graph, with the comparing operation (Mou et al., 2016) to strengthen features, i.e., absolute value of subtraction between the representation of two entities, |eh − et |, and element-wise multiplication, eh et ; (2) the representation of document node in Mention-level Graph, mdoc , as it can help aggregate cross-sentence information and provide document-aware representation; (3) the comprehensive inferential path information ph,t . Ih,t = [eh ; et ; |eh − et |; eh et ; mdoc ; ph,t ] (11) Finally, we formulate the task as multi-label classification task and predict relations between entities: P (r|eh , et ) = sigmoid (Wb σ(Wa Ih,t + ba ) + bb ) (1"
2020.emnlp-main.127,2020.acl-main.141,0,0.608448,"utilizes attention mechanism to absorb the context relational information for predicting. BERT-Two-Stepbase , proposed by Wang et al. (2019a) on DocRED. Though similar to BERTREbase , it first predicts whether two entities have a relationship and then predicts the specific target relation. HIN-GloVe/HIN-BERTbase , proposed by Tang et al. (2020). Hierarchical Inference Network (HIN) aggregate information from entity-level, sentence-level, and document-level to predict target relations, and use GloVe (Pennington et al., 2014) or BERTbase for word embedding. LSR-GloVe/LSR-BERTbase , proposed by Nan et al. (2020) recently. They construct a graph based on the dependency tree and predict relations by latent structure induction and GCN. Nan et al. (2020) also adapted four graph-based state-of-the-art RE models to DocRED, including GAT (Velickovic et al., 2017), GCNN (Sahu et al., 2019), EoG (Christopoulou et al., 2019), and AGGCN (Guo et al., 2019). We also include their results. Following Yao et al. (2019), we use the widely used metrics F1 and AUC in our experiment. We also use Ign F1 and Ign AUC, which calculate F1 and AUC excluding the common relation facts in the training and dev/test sets. 4.4 Resu"
2020.emnlp-main.127,Q17-1008,0,0.141436,"te Performer Part of May 26, 2002 Without Me Publication Date Figure 3: The case study of our proposed GAIN and baseline models. The models take the document as input and predict relations among different entities in different colors. We only show a part of entities within the documents and the according sentences due to the space limitation. inevitable restriction in practice, where many realworld relation facts can only be extracted across sentences. Therefore, many researchers gradually shift their attention into document-level relation extraction. Several approaches (Quirk and Poon, 2017; Peng et al., 2017; Gupta et al., 2019; Song et al., 2018; Jia et al., 2019) leverage dependency graph to better capture document-specific features, but they ignore ubiquitous relational inference in document. Recently, many models are proposed to address this problem. Tang et al. (2020) proposed a hierarchical inference network by considering information from entity-level, sentence-level, and documentlevel. However, it conducts relational inference implicitly based on a hierarchical network while we adopt the path reasoning mechanism, which is a more explicit way. (Christopoulou et al., 2019) is one of the mos"
2020.emnlp-main.127,D14-1162,0,0.0847802,"DocRED adapted from (Sorokin and Gurevych, 2017), uses an LSTM to encode the text, but further utilizes attention mechanism to absorb the context relational information for predicting. BERT-Two-Stepbase , proposed by Wang et al. (2019a) on DocRED. Though similar to BERTREbase , it first predicts whether two entities have a relationship and then predicts the specific target relation. HIN-GloVe/HIN-BERTbase , proposed by Tang et al. (2020). Hierarchical Inference Network (HIN) aggregate information from entity-level, sentence-level, and document-level to predict target relations, and use GloVe (Pennington et al., 2014) or BERTbase for word embedding. LSR-GloVe/LSR-BERTbase , proposed by Nan et al. (2020) recently. They construct a graph based on the dependency tree and predict relations by latent structure induction and GCN. Nan et al. (2020) also adapted four graph-based state-of-the-art RE models to DocRED, including GAT (Velickovic et al., 2017), GCNN (Sahu et al., 2019), EoG (Christopoulou et al., 2019), and AGGCN (Guo et al., 2019). We also include their results. Following Yao et al. (2019), we use the widely used metrics F1 and AUC in our experiment. We also use Ign F1 and Ign AUC, which calculate F1"
2020.emnlp-main.127,P19-1617,1,0.604441,"Baltimore； Eldersburg Object: Maryland relation: located in the administrative territorial entity Subject: Baltimore； Eldersburg Object: U.S. relation: country Figure 1: An example document and its desired relations from DocRED (Yao et al., 2019). Entity mentions and relations involved in these relation instances are colored. Other mentions are underlined for clarity. Introduction The task of identifying semantic relations between entities from text, namely relation extraction (RE), plays a crucial role in a variety of knowledge-based applications, such as question answering (Yu et al., 2017, Qiu et al., 2019) and large-scale knowledge graph construction. Previous methods (Zeng et al., 2014; Zeng et al., 2015; Xiao and Liu, 2016; Zhang et al., 2017; Zhang et al., 2018; Baldini Soares et al., 2019) focus on sentence-level RE, which predicts relations among entities in a single sentence. However, sentence-level RE models suffer from an inevitable limitation – they fail to recognize relations between entities across sentences. Hence, extracting relations at the document-level is necessary for a holistic understanding of knowledge in text. ∗ † Equal contribution. Corresponding author. There are several"
2020.emnlp-main.127,E17-1110,0,0.112741,"on Date Publication Date Performer Part of May 26, 2002 Without Me Publication Date Figure 3: The case study of our proposed GAIN and baseline models. The models take the document as input and predict relations among different entities in different colors. We only show a part of entities within the documents and the according sentences due to the space limitation. inevitable restriction in practice, where many realworld relation facts can only be extracted across sentences. Therefore, many researchers gradually shift their attention into document-level relation extraction. Several approaches (Quirk and Poon, 2017; Peng et al., 2017; Gupta et al., 2019; Song et al., 2018; Jia et al., 2019) leverage dependency graph to better capture document-specific features, but they ignore ubiquitous relational inference in document. Recently, many models are proposed to address this problem. Tang et al. (2020) proposed a hierarchical inference network by considering information from entity-level, sentence-level, and documentlevel. However, it conducts relational inference implicitly based on a hierarchical network while we adopt the path reasoning mechanism, which is a more explicit way. (Christopoulou et al., 2019"
2020.emnlp-main.127,P19-1423,0,0.333231,"target relation. HIN-GloVe/HIN-BERTbase , proposed by Tang et al. (2020). Hierarchical Inference Network (HIN) aggregate information from entity-level, sentence-level, and document-level to predict target relations, and use GloVe (Pennington et al., 2014) or BERTbase for word embedding. LSR-GloVe/LSR-BERTbase , proposed by Nan et al. (2020) recently. They construct a graph based on the dependency tree and predict relations by latent structure induction and GCN. Nan et al. (2020) also adapted four graph-based state-of-the-art RE models to DocRED, including GAT (Velickovic et al., 2017), GCNN (Sahu et al., 2019), EoG (Christopoulou et al., 2019), and AGGCN (Guo et al., 2019). We also include their results. Following Yao et al. (2019), we use the widely used metrics F1 and AUC in our experiment. We also use Ign F1 and Ign AUC, which calculate F1 and AUC excluding the common relation facts in the training and dev/test sets. 4.4 Results We show GAIN’s performance on the DocRED dataset in Table 2, in comparison with other baselines. Among the models not using BERT or BERT variants, GAIN-GloVe consistently outperforms all sequential-based and graph-based strong baselines by 0.9 ∼ 12.82 F1 score on the tes"
2020.emnlp-main.127,D18-1246,0,0.0345295,"ut Me Publication Date Figure 3: The case study of our proposed GAIN and baseline models. The models take the document as input and predict relations among different entities in different colors. We only show a part of entities within the documents and the according sentences due to the space limitation. inevitable restriction in practice, where many realworld relation facts can only be extracted across sentences. Therefore, many researchers gradually shift their attention into document-level relation extraction. Several approaches (Quirk and Poon, 2017; Peng et al., 2017; Gupta et al., 2019; Song et al., 2018; Jia et al., 2019) leverage dependency graph to better capture document-specific features, but they ignore ubiquitous relational inference in document. Recently, many models are proposed to address this problem. Tang et al. (2020) proposed a hierarchical inference network by considering information from entity-level, sentence-level, and documentlevel. However, it conducts relational inference implicitly based on a hierarchical network while we adopt the path reasoning mechanism, which is a more explicit way. (Christopoulou et al., 2019) is one of the most powerful systems on document-level RE"
2020.emnlp-main.127,D17-1188,0,0.0828932,"ls as baselines. Yao et al. (2019) proposed models to encode the document into a sequence of hidden state vector {hi }ni=1 using CNN (Fukushima, 1980), LSTM (Hochreiter and Schmidhuber, 1997), and BiLSTM (Schuster and Paliwal, 1997) as their encoder, and predict relations between entities with their representations. Other pre-trained models like BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019), and CorefBERT (Ye et al., 2020) are also used as encoder (Wang et al., 2019a; Ye et al., 2020) to document-level RE task. Context-Aware, also proposed by Yao et al. (2019) on DocRED adapted from (Sorokin and Gurevych, 2017), uses an LSTM to encode the text, but further utilizes attention mechanism to absorb the context relational information for predicting. BERT-Two-Stepbase , proposed by Wang et al. (2019a) on DocRED. Though similar to BERTREbase , it first predicts whether two entities have a relationship and then predicts the specific target relation. HIN-GloVe/HIN-BERTbase , proposed by Tang et al. (2020). Hierarchical Inference Network (HIN) aggregate information from entity-level, sentence-level, and document-level to predict target relations, and use GloVe (Pennington et al., 2014) or BERTbase for word em"
2020.emnlp-main.127,N18-1080,0,0.158726,"se an attention mechanism to selectively fuse all possible path information for the entity pair while without extra overhead. When we were writing this paper, (Nan et al., 2020) make their work public as preprints, which adopt the dependency tree to capture the semantic information in the document. They put mention and entity nodes in the same graph and conduct inference implicitly by using GCN. Unlike their work, our GAIN presents mention node and entity node in different graphs to better conduct inter-sentence information aggregation and infer relations more explicitly. Some other attempts (Verga et al., 2018; Sahu et al., 2019; Christopoulou et al., 2019) study document-level RE in a specific domain like biomedical RE. However, the datasets they use usually contain very limited relation types and entity types. For instance, CDR (Li et al., 2016) only has one type of relation and two types of entities, which may not be the ideal testbed for relational reasoning. 1637 6 Conclusion Extracting inter-sentence relations and conducting relational reasoning are challenging in documentlevel relation extraction. In this paper, we introduce Graph Aggregationand-Inference Network (GAIN) to better cope with d"
2020.emnlp-main.127,P16-1123,0,0.217534,"Missing"
2020.emnlp-main.127,C16-1119,0,0.14292,"dersburg Object: U.S. relation: country Figure 1: An example document and its desired relations from DocRED (Yao et al., 2019). Entity mentions and relations involved in these relation instances are colored. Other mentions are underlined for clarity. Introduction The task of identifying semantic relations between entities from text, namely relation extraction (RE), plays a crucial role in a variety of knowledge-based applications, such as question answering (Yu et al., 2017, Qiu et al., 2019) and large-scale knowledge graph construction. Previous methods (Zeng et al., 2014; Zeng et al., 2015; Xiao and Liu, 2016; Zhang et al., 2017; Zhang et al., 2018; Baldini Soares et al., 2019) focus on sentence-level RE, which predicts relations among entities in a single sentence. However, sentence-level RE models suffer from an inevitable limitation – they fail to recognize relations between entities across sentences. Hence, extracting relations at the document-level is necessary for a holistic understanding of knowledge in text. ∗ † Equal contribution. Corresponding author. There are several major challenges in effective relation extraction at the document-level. Firstly, the subject and object entities involv"
2020.emnlp-main.127,P19-1074,0,0.555886,"h is transformed into Entity-level Graph, where the paths between entities are identified for reasoning. Finally, the classification module predicts target relations based on the above information. Different entities are in different colors. The number i in the mention node denotes that it belongs to the i-th sentence. tion module (Sec. 3.2), entity-level graph inference module (Sec. 3.3), classification module (Sec. 3.4), as is shown in Figure 2. 3.1 Encoding Module In the encoding module, we convert a document D = {wi }ni=1 containing n words into a sequence of vectors {gi }ni=1 . Following Yao et al. (2019), for each word wi in D, we first concatenate its word embedding with entity type embedding and coreference embedding: xi = [Ew (wi ); Et (ti ); Ec (ci )] (1) 3.2 Mention-level Graph Aggregation Module To model the document-level information and interactions between mentions and entities, a heterogeneous Mention-level Graph (MG) is constructed. MG has two different kinds of nodes: mention node and document node. Each mention node denotes one particular mention of an entity. And MG also has one document node that aims to model the overall document information. We argue that this node could serv"
2020.emnlp-main.127,2020.emnlp-main.582,0,0.246993,"NBERTbase and GAIN-BERTlarge use BERTbase and BERTlarge as encoder respectively and the learning rate is set to 1e−5 . 4.3 Baselines and Evaluation Metrics We use the following models as baselines. Yao et al. (2019) proposed models to encode the document into a sequence of hidden state vector {hi }ni=1 using CNN (Fukushima, 1980), LSTM (Hochreiter and Schmidhuber, 1997), and BiLSTM (Schuster and Paliwal, 1997) as their encoder, and predict relations between entities with their representations. Other pre-trained models like BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019), and CorefBERT (Ye et al., 2020) are also used as encoder (Wang et al., 2019a; Ye et al., 2020) to document-level RE task. Context-Aware, also proposed by Yao et al. (2019) on DocRED adapted from (Sorokin and Gurevych, 2017), uses an LSTM to encode the text, but further utilizes attention mechanism to absorb the context relational information for predicting. BERT-Two-Stepbase , proposed by Wang et al. (2019a) on DocRED. Though similar to BERTREbase , it first predicts whether two entities have a relationship and then predicts the specific target relation. HIN-GloVe/HIN-BERTbase , proposed by Tang et al. (2020). Hierarchical"
2020.emnlp-main.127,P17-1053,0,0.0206069,"country Subject: Baltimore； Eldersburg Object: Maryland relation: located in the administrative territorial entity Subject: Baltimore； Eldersburg Object: U.S. relation: country Figure 1: An example document and its desired relations from DocRED (Yao et al., 2019). Entity mentions and relations involved in these relation instances are colored. Other mentions are underlined for clarity. Introduction The task of identifying semantic relations between entities from text, namely relation extraction (RE), plays a crucial role in a variety of knowledge-based applications, such as question answering (Yu et al., 2017, Qiu et al., 2019) and large-scale knowledge graph construction. Previous methods (Zeng et al., 2014; Zeng et al., 2015; Xiao and Liu, 2016; Zhang et al., 2017; Zhang et al., 2018; Baldini Soares et al., 2019) focus on sentence-level RE, which predicts relations among entities in a single sentence. However, sentence-level RE models suffer from an inevitable limitation – they fail to recognize relations between entities across sentences. Hence, extracting relations at the document-level is necessary for a holistic understanding of knowledge in text. ∗ † Equal contribution. Corresponding author"
2020.emnlp-main.127,D15-1203,0,0.268304,"ject: Baltimore； Eldersburg Object: U.S. relation: country Figure 1: An example document and its desired relations from DocRED (Yao et al., 2019). Entity mentions and relations involved in these relation instances are colored. Other mentions are underlined for clarity. Introduction The task of identifying semantic relations between entities from text, namely relation extraction (RE), plays a crucial role in a variety of knowledge-based applications, such as question answering (Yu et al., 2017, Qiu et al., 2019) and large-scale knowledge graph construction. Previous methods (Zeng et al., 2014; Zeng et al., 2015; Xiao and Liu, 2016; Zhang et al., 2017; Zhang et al., 2018; Baldini Soares et al., 2019) focus on sentence-level RE, which predicts relations among entities in a single sentence. However, sentence-level RE models suffer from an inevitable limitation – they fail to recognize relations between entities across sentences. Hence, extracting relations at the document-level is necessary for a holistic understanding of knowledge in text. ∗ † Equal contribution. Corresponding author. There are several major challenges in effective relation extraction at the document-level. Firstly, the subject and ob"
2020.emnlp-main.127,C14-1220,0,0.382319,"ritorial entity Subject: Baltimore； Eldersburg Object: U.S. relation: country Figure 1: An example document and its desired relations from DocRED (Yao et al., 2019). Entity mentions and relations involved in these relation instances are colored. Other mentions are underlined for clarity. Introduction The task of identifying semantic relations between entities from text, namely relation extraction (RE), plays a crucial role in a variety of knowledge-based applications, such as question answering (Yu et al., 2017, Qiu et al., 2019) and large-scale knowledge graph construction. Previous methods (Zeng et al., 2014; Zeng et al., 2015; Xiao and Liu, 2016; Zhang et al., 2017; Zhang et al., 2018; Baldini Soares et al., 2019) focus on sentence-level RE, which predicts relations among entities in a single sentence. However, sentence-level RE models suffer from an inevitable limitation – they fail to recognize relations between entities across sentences. Hence, extracting relations at the document-level is necessary for a holistic understanding of knowledge in text. ∗ † Equal contribution. Corresponding author. There are several major challenges in effective relation extraction at the document-level. Firstly,"
2020.emnlp-main.127,D18-1244,0,0.100492,"Figure 1: An example document and its desired relations from DocRED (Yao et al., 2019). Entity mentions and relations involved in these relation instances are colored. Other mentions are underlined for clarity. Introduction The task of identifying semantic relations between entities from text, namely relation extraction (RE), plays a crucial role in a variety of knowledge-based applications, such as question answering (Yu et al., 2017, Qiu et al., 2019) and large-scale knowledge graph construction. Previous methods (Zeng et al., 2014; Zeng et al., 2015; Xiao and Liu, 2016; Zhang et al., 2017; Zhang et al., 2018; Baldini Soares et al., 2019) focus on sentence-level RE, which predicts relations among entities in a single sentence. However, sentence-level RE models suffer from an inevitable limitation – they fail to recognize relations between entities across sentences. Hence, extracting relations at the document-level is necessary for a holistic understanding of knowledge in text. ∗ † Equal contribution. Corresponding author. There are several major challenges in effective relation extraction at the document-level. Firstly, the subject and object entities involved in a relation may appear in different"
2020.emnlp-main.127,D17-1004,0,0.222047,". relation: country Figure 1: An example document and its desired relations from DocRED (Yao et al., 2019). Entity mentions and relations involved in these relation instances are colored. Other mentions are underlined for clarity. Introduction The task of identifying semantic relations between entities from text, namely relation extraction (RE), plays a crucial role in a variety of knowledge-based applications, such as question answering (Yu et al., 2017, Qiu et al., 2019) and large-scale knowledge graph construction. Previous methods (Zeng et al., 2014; Zeng et al., 2015; Xiao and Liu, 2016; Zhang et al., 2017; Zhang et al., 2018; Baldini Soares et al., 2019) focus on sentence-level RE, which predicts relations among entities in a single sentence. However, sentence-level RE models suffer from an inevitable limitation – they fail to recognize relations between entities across sentences. Hence, extracting relations at the document-level is necessary for a holistic understanding of knowledge in text. ∗ † Equal contribution. Corresponding author. There are several major challenges in effective relation extraction at the document-level. Firstly, the subject and object entities involved in a relation may"
2020.emnlp-main.127,P16-2034,0,0.0257981,"dy of our proposed model GAIN, in comparison with other baselines. As is shown, BiLSTM can only identify two relations within the first sentence. Both BERT-REbase and GAIN-BERTbase can successfully predict Without Me is part of The Eminem Show. But only GAIN-BERTbase is able to deduce the performer and publication date of Without Me are the same as those of The Eminem Show, namely Eminem and May 26, 2002, where it requires logical inference across sentences. 5 Related Work Previous approaches focus on sentence-level relation extraction (Zeng et al., 2014; Zeng et al., 2015; Wang et al., 2016; Zhou et al., 2016; Xiao and Liu, 2016; Zhang et al., 2017; Feng et al., 2018; Zhu et al., 2019). But sentence-level RE models face an 1636 [1] The Eminem Show is the fourth studio album by American rapper Eminem, released on May 26, 2002 by Aftermath Entertainment, Shady Records, and Interscope Records. [2] The Eminem Show includes the commercially successful singles &quot;Without Me&quot;, &quot;Cleanin’ Out My Closet&quot;, &quot;Superman&quot;, and &quot;Sing for the Moment&quot;.… Performer BiLSTM Eminem Performer The Eminem Show Publication Date Without Me BERTRE The Eminem Show Publication Date May 26, 2002 Eminem Performer GAINBERT The Eminem"
2020.emnlp-main.127,P19-1128,0,0.432377,", BiLSTM can only identify two relations within the first sentence. Both BERT-REbase and GAIN-BERTbase can successfully predict Without Me is part of The Eminem Show. But only GAIN-BERTbase is able to deduce the performer and publication date of Without Me are the same as those of The Eminem Show, namely Eminem and May 26, 2002, where it requires logical inference across sentences. 5 Related Work Previous approaches focus on sentence-level relation extraction (Zeng et al., 2014; Zeng et al., 2015; Wang et al., 2016; Zhou et al., 2016; Xiao and Liu, 2016; Zhang et al., 2017; Feng et al., 2018; Zhu et al., 2019). But sentence-level RE models face an 1636 [1] The Eminem Show is the fourth studio album by American rapper Eminem, released on May 26, 2002 by Aftermath Entertainment, Shady Records, and Interscope Records. [2] The Eminem Show includes the commercially successful singles &quot;Without Me&quot;, &quot;Cleanin’ Out My Closet&quot;, &quot;Superman&quot;, and &quot;Sing for the Moment&quot;.… Performer BiLSTM Eminem Performer The Eminem Show Publication Date Without Me BERTRE The Eminem Show Publication Date May 26, 2002 Eminem Performer GAINBERT The Eminem Show Publication Date May 26, 2002 Part of Without Me May 26, 2002 Eminem Per"
2020.emnlp-main.210,2020.acl-main.747,0,0.0715994,"Missing"
2020.emnlp-main.210,N19-1423,0,0.245367,". Experimental results demonstrate that mRASP achieves significant performance improvement compared to directly training on those target pairs. It is the first time to verify that multiple lowresource language pairs can be utilized to improve rich resource MT. Surprisingly, mRASP is even able to improve the translation quality on exotic languages that never occur in the pretraining corpus. Code, data, and pre-trained models are available at https://github. com/linzehui/mRASP. 1 Introduction Pre-trained language models such as BERT have been highly effective for NLP tasks (Peters et al., 2018; Devlin et al., 2019; Radford et al., 2019; Conneau and Lample, 2019; Liu et al., 2019; Yang et al., 2019). Pre-training and fine-tuning has been a successful paradigm. It is intriguing to discover a “BERT” equivalent – a pre-trained model – for ∗ Equal contribution. The work was done when the first author was an intern at ByteDance. machine translation. In this paper, we study the following question: can we develop a single universal MT model and derive specialized models by fine-tuning on an arbitrary pair of languages? While pre-training techniques are working very well for NLP task, there are still several li"
2020.emnlp-main.210,P15-1166,0,0.022539,"tialization fails on this extremely low resource. With only 1M pairs, mRASP reaches comparable results with baseline trained on 4.5M pairs. With the size of dataset increases, the performance of the pre-training model consistently increases. While the baseline does not see any improvement until the volume of the dataset reaches 50K. The results confirm the remarkable boosting of mRASP on low resource dataset. 5 Related Works Multilingual NMT aims at taking advantage of multilingual data to improve NMT for all languages involved, which has been extensively studied in a number of papers such as Dong et al. (2015); Johnson et al. (2017); Lu et al. (2018); Rahimi et al. (2019); Tan et al. (2019). The most related work to mRASP is Rahimi et al. (2019), which performs extensive experiments in training massively multilingual NMT models. They show that multilingual many-to-many models are effective in low resource settings. Inspired by their work, we believe that the translation quality of low-resource language pairs may improve when trained together with richresource ones. However, we are different in at least two aspects: a) Our goal is to find the best practice of a single language pair with multilingual"
2020.emnlp-main.210,2020.findings-emnlp.283,0,0.0187955,"013a) first introduces dictionaries to align word representations from different languages. A series of followup studies focus on aligning the word representation across languages (Xing et al., 2015; Ammar et al., 2016; Smith et al., 2017; Lample et al., 2018b). Inspired by the success of BERT, Conneau and Lample (2019) introduced XLM - masked language models trained on multiple languages, as a way to leverage parallel data and obtain impressive empirical results on the cross-lingual natural language inference (XNLI) benchmark and unsupervised NMT(Sennrich et al., 2016a; Lample et al., 2018a; Garcia et al., 2020). Huang et al. (2019) extended XLM with multi-task learning and proposed a universal language encoder. Different from these works, a) mRASP is actually a multilingual sequence to sequence model which is more desirable for NMT pre-training; b) mRASP introduces alignment regularization to bridge the sentence representation across languages. 6 Conclusion In this paper, we propose a multilingual neural machine translation pre-training model (mRASP). To bridge the semantic space between different languages, we incorporate word alignment into the pre-training model. Extensive experiments are conduct"
2020.emnlp-main.210,D19-1252,0,0.34639,"f parallel corpus to simulate different scenarios. Most of the En-X parallel datasets are from the pre-training phase to avoid introducing new information. Most pairs for fine-tuning are from previous years of WMT and IWSLT. Specifically, we use WMT14 for EnDe and En-Fr, WMT16 for En-Ro. For pairs like Nl(Dutch)-Pt(Portuguese) that are not available in WMT or IWSLT, we use news-commentary instead. For a detailed description, please refer to the Appendix. 2652 8 CTNMT only reports the Transformer-base setting. Lang-Pairs Size En→De 4.5M Zh→En 20M En→Fr 40M Direct CTNMT8 (2020) mBART (2020) XLM (2019) MASS (2019) mBERT (2019) 29.3 30.1 28.8 28.9 28.6 24.1 - 43.2 42.3 41.0 - mRASP 30.3 24.7 44.3 Table 2: Fine-tuning performance for popular medium and rich resource MT tasks. For fair comparison, we report detokenized BLEU on WMT newstest18 for Zh→En and tokenized BLEU on WMT newstest14 for En→Fr and En→De. Notice unlike previous methods (except CTNMT) which do not improve in the rich resource settings, mRASP is again able to consistently improve the downstream MT performance. It is the first time to verify that low-resource language pairs can be utilized to improve rich resource MT. Based on"
2020.emnlp-main.210,Q17-1024,0,0.0398988,"Missing"
2020.emnlp-main.210,2020.acl-main.703,0,0.118192,"Missing"
2020.emnlp-main.210,2020.tacl-1.47,0,0.324,"following question: can we develop a single universal MT model and derive specialized models by fine-tuning on an arbitrary pair of languages? While pre-training techniques are working very well for NLP task, there are still several limitations for machine translation tasks. First, pre-trained language models such as BERT are not easy to directly fine-tune unless using some sophisticated techniques (Yang et al., 2020). Second, there is a discrepancy between existing pre-training objective and down-stream ones in MT. Existing pre-training approaches such as MASS (Song et al., 2019) and mBART (Liu et al., 2020) rely on auto-encoding objectives to pre-train the models, which are different from translation. Therefore, their fine-tuned MT models still do not achieve adequate improvement. Third, existing MT pre-training approaches focus on using multilingual models to improve MT for low resource or medium resource languages. There has not been one pre-trained MT model that can improve for any pairs of languages, even for rich resource settings such as English-French. In this paper, we propose multilingual Random Aligned Substitution Pre-training (mRASP), a method to pre-train a MT model for many languag"
2020.emnlp-main.210,2021.ccl-1.108,0,0.128783,"Missing"
2020.emnlp-main.210,W18-6309,0,0.0189222,"ource. With only 1M pairs, mRASP reaches comparable results with baseline trained on 4.5M pairs. With the size of dataset increases, the performance of the pre-training model consistently increases. While the baseline does not see any improvement until the volume of the dataset reaches 50K. The results confirm the remarkable boosting of mRASP on low resource dataset. 5 Related Works Multilingual NMT aims at taking advantage of multilingual data to improve NMT for all languages involved, which has been extensively studied in a number of papers such as Dong et al. (2015); Johnson et al. (2017); Lu et al. (2018); Rahimi et al. (2019); Tan et al. (2019). The most related work to mRASP is Rahimi et al. (2019), which performs extensive experiments in training massively multilingual NMT models. They show that multilingual many-to-many models are effective in low resource settings. Inspired by their work, we believe that the translation quality of low-resource language pairs may improve when trained together with richresource ones. However, we are different in at least two aspects: a) Our goal is to find the best practice of a single language pair with multilingual pretraining. Multilingual NMT usually ac"
2020.emnlp-main.210,P19-1015,0,0.0284706,"M pairs, mRASP reaches comparable results with baseline trained on 4.5M pairs. With the size of dataset increases, the performance of the pre-training model consistently increases. While the baseline does not see any improvement until the volume of the dataset reaches 50K. The results confirm the remarkable boosting of mRASP on low resource dataset. 5 Related Works Multilingual NMT aims at taking advantage of multilingual data to improve NMT for all languages involved, which has been extensively studied in a number of papers such as Dong et al. (2015); Johnson et al. (2017); Lu et al. (2018); Rahimi et al. (2019); Tan et al. (2019). The most related work to mRASP is Rahimi et al. (2019), which performs extensive experiments in training massively multilingual NMT models. They show that multilingual many-to-many models are effective in low resource settings. Inspired by their work, we believe that the translation quality of low-resource language pairs may improve when trained together with richresource ones. However, we are different in at least two aspects: a) Our goal is to find the best practice of a single language pair with multilingual pretraining. Multilingual NMT usually achieves inferior accura"
2020.emnlp-main.210,P16-1009,0,0.537502,"nguages. The parallel corpus are from various sources: ted1 , wmt2 , europarl3 , paracrawl4 , opensubtitles5 , qed6 . We refer to our pre-training data as PC32(Parallel Corpus 32). PC32 contains a total size of 197M pairs of sentences. Detailed descriptions and summary for the datasets can be found in Appendix. For RAS, we utilize ground-truth En-X bilingual dictionaries7 , where X denotes languages involved in PC32. Since not all languages in PC32 have ground-truth dictionaries, we only use available dictionaries. 2.3 Pre-training Details We use learned joint vocabulary. We learn shared BPE (Sennrich et al., 2016b) merge operations (with 32k merge ops) across all the training data and added monolingual data as a supplement (limit to 1M sentences). We do over-sampling in learning BPE to balance the vocabulary size of languages, whose resources are drastically different in size. We over-sampled the corpus of each language based on the volume of the largest language corpus. We 1 Compiled by Qi et al. (2018). For simplicity, we deleted zh-tw and zh (which is actually Cantonese), and merged fr-ca with fr, pt-br with pt. 2 http://www.statmt.org 3 http://opus.nlpl.eu/Europarl-v8.php 4 https://paracrawl.eu/ 5"
2020.emnlp-main.210,W18-6301,0,0.161617,"ts cannot train an NMT model properly, utilizing the pre-training model boosts performance. We also obtain consistent improvements in low and medium resource datasets. Not surprisingly, We observe that with the scale of the dataset increasing, the gap between the randomly initialized baseline and pre-training model is becoming closer. It is worth noting that, for En→De benchmark, we obtain 1.0 BLEU points gains9 . To verify mRASP can further boost performance on rich resource datasets, we also conduct experiments on En→Zh and En→Fr. We compare our results with two strong baselines reported by Ott et al. (2018); Li et al. (2019). As shown in Table 2, surprisingly, when large parallel datasets are provided, it still benefits from pre-training models. In En→Fr, we obtain 1.1 BLEU points gains. Comparing to other Pre-training Approaches We compare our mRASP to recently proposed multilingual pre-training models. Following Liu et al. (2020), we conduct experiments on En-Ro, the only pairs with established results. To make a fair comparison, we report de-tokenized BLEU. As illustrated in Table 4 , Our model reaches comparable performance on both En→Ro and Ro→En. We also combine Back Translation (Sennrich"
2020.emnlp-main.210,P16-1162,0,0.792767,"nguages. The parallel corpus are from various sources: ted1 , wmt2 , europarl3 , paracrawl4 , opensubtitles5 , qed6 . We refer to our pre-training data as PC32(Parallel Corpus 32). PC32 contains a total size of 197M pairs of sentences. Detailed descriptions and summary for the datasets can be found in Appendix. For RAS, we utilize ground-truth En-X bilingual dictionaries7 , where X denotes languages involved in PC32. Since not all languages in PC32 have ground-truth dictionaries, we only use available dictionaries. 2.3 Pre-training Details We use learned joint vocabulary. We learn shared BPE (Sennrich et al., 2016b) merge operations (with 32k merge ops) across all the training data and added monolingual data as a supplement (limit to 1M sentences). We do over-sampling in learning BPE to balance the vocabulary size of languages, whose resources are drastically different in size. We over-sampled the corpus of each language based on the volume of the largest language corpus. We 1 Compiled by Qi et al. (2018). For simplicity, we deleted zh-tw and zh (which is actually Cantonese), and merged fr-ca with fr, pt-br with pt. 2 http://www.statmt.org 3 http://opus.nlpl.eu/Europarl-v8.php 4 https://paracrawl.eu/ 5"
2020.emnlp-main.210,D14-1162,0,0.0844587,"Missing"
2020.emnlp-main.210,N18-1202,0,0.293103,"exotic language pairs. Experimental results demonstrate that mRASP achieves significant performance improvement compared to directly training on those target pairs. It is the first time to verify that multiple lowresource language pairs can be utilized to improve rich resource MT. Surprisingly, mRASP is even able to improve the translation quality on exotic languages that never occur in the pretraining corpus. Code, data, and pre-trained models are available at https://github. com/linzehui/mRASP. 1 Introduction Pre-trained language models such as BERT have been highly effective for NLP tasks (Peters et al., 2018; Devlin et al., 2019; Radford et al., 2019; Conneau and Lample, 2019; Liu et al., 2019; Yang et al., 2019). Pre-training and fine-tuning has been a successful paradigm. It is intriguing to discover a “BERT” equivalent – a pre-trained model – for ∗ Equal contribution. The work was done when the first author was an intern at ByteDance. machine translation. In this paper, we study the following question: can we develop a single universal MT model and derive specialized models by fine-tuning on an arbitrary pair of languages? While pre-training techniques are working very well for NLP task, there"
2020.emnlp-main.210,W18-6319,0,0.0204161,"ly initialized models directly on downstream bilingual parallel corpus as a comparison with pre-training models. Fine-tuning We fine-tune our obtained mRASP model on the target language pairs. We apply a dropout rate of 0.3 for all pairs except for rich resource such as En-Zh and En-Fr with 0.1. We carefully tune the model, setting different learning rates and learning scheduler warm-up steps for different data scale. For inference, we use beam-search with beam size 5 for all directions. For most cases, We measure case-sensitive tokenized BLEU. We also report de-tokenized BLEU with SacreBLEU (Post, 2018) for a fair comparison with previous works. 3.2 Main Results We first conduct experiments on the (extremely) low-resource and medium-resource datasets, where multilingual translation usually obtains significant improvements. As illustrated in Table 1, we obtain significant gains in all datasets. For extremely low resources setting such as En-Be (Belarusian) where the amount of datasets cannot train an NMT model properly, utilizing the pre-training model boosts performance. We also obtain consistent improvements in low and medium resource datasets. Not surprisingly, We observe that with the sca"
2020.emnlp-main.210,N18-2084,0,0.109204,"Missing"
2020.emnlp-main.210,N15-1104,0,0.0311338,"fails on this extremely low resource. With only 1M pairs, mRASP reaches comparable results with baseline trained on 4.5M pairs. With the size of dataset increases, the performance of the pre-training model consistently increases. While the baseline does not see any improvement until the volume of the dataset reaches 50K. The results confirm the remarkable boosting of mRASP on low resource dataset. 5 Related Works Multilingual NMT aims at taking advantage of multilingual data to improve NMT for all languages involved, which has been extensively studied in a number of papers such as Dong et al. (2015); Johnson et al. (2017); Lu et al. (2018); Rahimi et al. (2019); Tan et al. (2019). The most related work to mRASP is Rahimi et al. (2019), which performs extensive experiments in training massively multilingual NMT models. They show that multilingual many-to-many models are effective in low resource settings. Inspired by their work, we believe that the translation quality of low-resource language pairs may improve when trained together with richresource ones. However, we are different in at least two aspects: a) Our goal is to find the best practice of a single language pair with multilingual"
2020.emnlp-main.733,S14-2010,0,0.269265,"Missing"
2020.emnlp-main.733,S16-1081,0,0.234494,"Missing"
2020.emnlp-main.733,S12-1051,0,0.0820126,"trate the effectiveness of our proposed method, in this section we present our experimental results for various tasks related to semantic textual similarity under multiple configurations. For the implementation details of our siamese BERT models and flow-based models, please refer to Appendix B. 4.1 Semantic Textual Similarity Datasets. We evaluate our approach extensively on the semantic textual similarity (STS) tasks. We report results on 7 datasets, namely the STS benchmark (STS-B) (Cer et al., 2017) the SICKRelatedness (SICK-R) dataset (Marelli et al., 2014) and the STS tasks 2012 - 2016 (Agirre et al., 2012, 2013, 2014, 2015, 2016). We obtain all these datasets via the SentEval toolkit (Conneau and Kiela, 2018). These datasets provide a fine-grained gold standard semantic similarity between 0 and 5 for each sentence pair. |, (4) Here D denotes the dataset, in other words, the collection of sentences. Note that during training, only the flow parameters are optimized while the Evaluation Procedure. Following the procedure in previous work like Sentence-BERT (Reimers and Gurevych, 2019) for the STS task, the predic3 For concrete mathamatical formulations, please refer to Table 1 of Kingma and Dhari"
2020.emnlp-main.733,S13-1004,0,0.067209,"Missing"
2020.emnlp-main.733,D15-1075,0,0.171507,"an latent variable, is then used to transform the BERT sentence embedding to the Gaussian space. We name the proposed method as BERT-flow. We perform extensive experiments on 7 standard semantic textual similarity benchmarks without using any downstream supervision. Our empirical results demonstrate that the flow transformation is able to consistently improve BERT by up to 12.70 points with an average of 8.16 points in terms of Spearman correlation between cosine embedding similarity and human annotated similarity. When combined with external supervision from natural language inference tasks (Bowman et al., 2015; Williams et al., 2018), our method outperforms the sentence-BERT embeddings (Reimers and Gurevych, 2019), leading to new state-of-theart performance. In addition to semantic similarity tasks, we apply sentence embeddings to a question-answer entailment task, QNLI (Wang et al., 2019), directly without task-specific supervision, and demonstrate the superiority of our approach. Moreover, our further analysis implies that BERT-induced similarity can excessively correlate with lexical similarity compared to semantic similarity, and our proposed flow-based method can effectively remedy this proble"
2020.emnlp-main.733,S17-2001,0,0.0610254,"entence),sentence∼D log pZ (fφ−1 (u)) + log |det ∂fφ−1 (u) ∂u Experiments To verify our hypotheses and demonstrate the effectiveness of our proposed method, in this section we present our experimental results for various tasks related to semantic textual similarity under multiple configurations. For the implementation details of our siamese BERT models and flow-based models, please refer to Appendix B. 4.1 Semantic Textual Similarity Datasets. We evaluate our approach extensively on the semantic textual similarity (STS) tasks. We report results on 7 datasets, namely the STS benchmark (STS-B) (Cer et al., 2017) the SICKRelatedness (SICK-R) dataset (Marelli et al., 2014) and the STS tasks 2012 - 2016 (Agirre et al., 2012, 2013, 2014, 2015, 2016). We obtain all these datasets via the SentEval toolkit (Conneau and Kiela, 2018). These datasets provide a fine-grained gold standard semantic similarity between 0 and 5 for each sentence pair. |, (4) Here D denotes the dataset, in other words, the collection of sentences. Note that during training, only the flow parameters are optimized while the Evaluation Procedure. Following the procedure in previous work like Sentence-BERT (Reimers and Gurevych, 2019) fo"
2020.emnlp-main.733,D18-2029,0,0.0389952,".91 75.39 77.58 (↑) 78.94 (↑) BERTlarge -NLI BERTlarge -NLI-last2avg BERTlarge -NLI-flow (NLI∗ ) BERTlarge -NLI-flow (target) 77.80 78.45 79.89 (↑) 81.18 (↑) 73.44 74.93 77.73 (↑) 74.52 (↓) 74.04 75.55 77.56 (↑) 78.85 (↑) 79.14 80.35 82.48 (↑) 82.97 (↑) 75.35 76.81 79.36 (↑) 80.57 (↑) 66.87 68.69 69.61 (↑) 70.19 (↑) STS-13 73.91 75.63 79.45 (↑) 80.27 (↑) Table 3: Experimental results on semantic textual similarity with NLI supervision. Note that our flows are still learned in a unsupervised way. InferSent (Conneau et al., 2017) is a siamese LSTM train on NLI, Universal Sentence Encoder (USE) (Cer et al., 2018) replace the LSTM with a Transformer and SBERT (Reimers and Gurevych, 2019) further use BERT. We report the Spearman’s rank correlation between the cosine similarity of sentence embeddings and the gold labels on multiple datasets. Numbers are reported as ρ × 100. ↑ denotes outperformance over its BERT baseline and ↓ denotes underperformance. Our proposed BERT-flow (i.e., the “BERT-NLI-flow” in this table) method achieves the best scores. Note that our BERT-flow use -last2avg as default setting. ∗: Use NLI corpus for the unsupervised training of flow; supervision labels of NLI are NOT visible."
2020.emnlp-main.733,marelli-etal-2014-sick,0,0.0539398,"u) ∂u Experiments To verify our hypotheses and demonstrate the effectiveness of our proposed method, in this section we present our experimental results for various tasks related to semantic textual similarity under multiple configurations. For the implementation details of our siamese BERT models and flow-based models, please refer to Appendix B. 4.1 Semantic Textual Similarity Datasets. We evaluate our approach extensively on the semantic textual similarity (STS) tasks. We report results on 7 datasets, namely the STS benchmark (STS-B) (Cer et al., 2017) the SICKRelatedness (SICK-R) dataset (Marelli et al., 2014) and the STS tasks 2012 - 2016 (Agirre et al., 2012, 2013, 2014, 2015, 2016). We obtain all these datasets via the SentEval toolkit (Conneau and Kiela, 2018). These datasets provide a fine-grained gold standard semantic similarity between 0 and 5 for each sentence pair. |, (4) Here D denotes the dataset, in other words, the collection of sentences. Note that during training, only the flow parameters are optimized while the Evaluation Procedure. Following the procedure in previous work like Sentence-BERT (Reimers and Gurevych, 2019) for the STS task, the predic3 For concrete mathamatical formul"
2020.emnlp-main.733,L18-1269,0,0.080249,"or various tasks related to semantic textual similarity under multiple configurations. For the implementation details of our siamese BERT models and flow-based models, please refer to Appendix B. 4.1 Semantic Textual Similarity Datasets. We evaluate our approach extensively on the semantic textual similarity (STS) tasks. We report results on 7 datasets, namely the STS benchmark (STS-B) (Cer et al., 2017) the SICKRelatedness (SICK-R) dataset (Marelli et al., 2014) and the STS tasks 2012 - 2016 (Agirre et al., 2012, 2013, 2014, 2015, 2016). We obtain all these datasets via the SentEval toolkit (Conneau and Kiela, 2018). These datasets provide a fine-grained gold standard semantic similarity between 0 and 5 for each sentence pair. |, (4) Here D denotes the dataset, in other words, the collection of sentences. Note that during training, only the flow parameters are optimized while the Evaluation Procedure. Following the procedure in previous work like Sentence-BERT (Reimers and Gurevych, 2019) for the STS task, the predic3 For concrete mathamatical formulations, please refer to Table 1 of Kingma and Dhariwal (2018) 9123 Dataset STS-B SICK-R STS-12 Avg. GloVe embeddings Avg. BERT embeddings BERT CLS-vector Pub"
2020.emnlp-main.733,D17-1070,0,0.058629,") 68.95 (↑) 78.48 (↑) 72.15 73.98 75.53 (↑) 77.62 (↑) 77.35 79.15 80.63 (↑) 81.95 (↑) 73.91 75.39 77.58 (↑) 78.94 (↑) BERTlarge -NLI BERTlarge -NLI-last2avg BERTlarge -NLI-flow (NLI∗ ) BERTlarge -NLI-flow (target) 77.80 78.45 79.89 (↑) 81.18 (↑) 73.44 74.93 77.73 (↑) 74.52 (↓) 74.04 75.55 77.56 (↑) 78.85 (↑) 79.14 80.35 82.48 (↑) 82.97 (↑) 75.35 76.81 79.36 (↑) 80.57 (↑) 66.87 68.69 69.61 (↑) 70.19 (↑) STS-13 73.91 75.63 79.45 (↑) 80.27 (↑) Table 3: Experimental results on semantic textual similarity with NLI supervision. Note that our flows are still learned in a unsupervised way. InferSent (Conneau et al., 2017) is a siamese LSTM train on NLI, Universal Sentence Encoder (USE) (Cer et al., 2018) replace the LSTM with a Transformer and SBERT (Reimers and Gurevych, 2019) further use BERT. We report the Spearman’s rank correlation between the cosine similarity of sentence embeddings and the gold labels on multiple datasets. Numbers are reported as ρ × 100. ↑ denotes outperformance over its BERT baseline and ↓ denotes underperformance. Our proposed BERT-flow (i.e., the “BERT-NLI-flow” in this table) method achieves the best scores. Note that our BERT-flow use -last2avg as default setting. ∗: Use NLI corpu"
2020.emnlp-main.733,D14-1162,0,0.109243,"://github.com/ bohanli/BERT-flow. 1 Introduction Recently, pre-trained language models and its variants (Radford et al., 2019; Devlin et al., 2019; Yang et al., 2019; Liu et al., 2019) like BERT (Devlin et al., 2019) have been widely used as representations of natural language. Despite their great success on many NLP tasks through fine-tuning, the sentence embeddings from BERT without finetuning are significantly inferior in terms of semantic textual similarity (Reimers and Gurevych, ∗ The work was done when BL was an intern at ByteDance. 2019) – for example, they even underperform the GloVe (Pennington et al., 2014) embeddings which are not contextualized and trained with a much simpler model. Such issues hinder applying BERT sentence embeddings directly to many real-world scenarios where collecting labeled data is highlycosting or even intractable. In this paper, we aim to answer two major questions: (1) why do the BERT-induced sentence embeddings perform poorly to retrieve semantically similar sentences? Do they carry too little semantic information, or just because the semantic meanings in these embeddings are not exploited properly? (2) If the BERT embeddings capture enough semantic information that"
2020.emnlp-main.733,N19-1423,0,0.0613683,"ance of semantic similarity. To address this issue, we propose to transform the anisotropic sentence embedding distribution to a smooth and isotropic Gaussian distribution through normalizing flows that are learned with an unsupervised objective. Experimental results show that our proposed BERT-flow method obtains significant performance gains over the state-of-the-art sentence embeddings on a variety of semantic textual similarity tasks. The code is available at https://github.com/ bohanli/BERT-flow. 1 Introduction Recently, pre-trained language models and its variants (Radford et al., 2019; Devlin et al., 2019; Yang et al., 2019; Liu et al., 2019) like BERT (Devlin et al., 2019) have been widely used as representations of natural language. Despite their great success on many NLP tasks through fine-tuning, the sentence embeddings from BERT without finetuning are significantly inferior in terms of semantic textual similarity (Reimers and Gurevych, ∗ The work was done when BL was an intern at ByteDance. 2019) – for example, they even underperform the GloVe (Pennington et al., 2014) embeddings which are not contextualized and trained with a much simpler model. Such issues hinder applying BERT sentence"
2020.emnlp-main.733,D19-1006,0,0.105365,"on that is hard to be directly utilized, how can we make it easier without external supervision? Towards this end, we first study the connection between the BERT pretraining objective and the semantic similarity task. Our analysis reveals that the sentence embeddings of BERT should be able to intuitively reflect the semantic similarity between sentences, which contradicts with experimental observations. Inspired by Gao et al. (2019) who find that the language modeling performance can be limited by the learned anisotropic word embedding space where the word embeddings occupy a narrow cone, and Ethayarajh (2019) who find that BERT word embeddings also suffer from anisotropy, we hypothesize that the sentence embeddings from BERT – as average of context embeddings from last layers1 – may suffer from similar issues. Through empirical probing over the embeddings, we further observe that the BERT sentence embedding space is semantically non-smoothing and poorly defined in some areas, which makes it hard to be used directly through simple similarity metrics such as dot 1 In this paper, we compute average of context embeddings from last one or two layers as our sentence embeddings since they are consistentl"
2020.emnlp-main.733,P19-1315,0,0.02216,", h&gt; c wx can be approximately decomposed as follows, ∗ h&gt; c wx ≈ log p (x|c) + λc = PMI(x, c) + log p(x) + λc . (2) (3) p(x,c) where PMI(x, c) = log p(x)p(c) denotes the pointwise mutual information between x and c, log p(x) is a word-specific term, and λc is a context-specific term. PMI captures how frequently two events cooccur more than if they independently occur. Note that co-occurrence statistics is a typical tool to deal with “semantics” in a computational way — specifically, PMI is a common mathematical surrogate to approximate word-level semantic similarity (Levy and Goldberg, 2014; Ethayarajh et al., 2019). Therefore, roughly speaking, it is semantically meaningful to compute the dot product between a context embedding and a word embedding. Higher-Order Co-Occurrence Statistics as Context-Context Semantic Similarity. During pretraining, the semantic relationship between two contexts c and c0 could be inferred and reinforced with their connections to words. To be specific, if both the contexts c and c0 co-occur with the same word w, the two contexts are likely to share similar semantic meaning. During the training dynamics, when c and w occur at the same time, the embeddings hc and xw are encour"
2020.emnlp-main.733,D19-1370,1,0.909392,"her to their kNN neighbors compared to the embeddings of high-frequency words. This demonstrates that lowfrequency words tends to disperse sparsely. Due to the sparsity, many “holes” could be formed around the low-frequency word embeddings in the embedding space, where the semantic meaning can be poorly defined. Note that BERT sentence embeddings are produced by averaging the context embeddings, which is a convexitypreserving operation. However, the holes violate the convexity of the embedding space. This is a common problem in the context of representation learining (Rezende and Viola, 2018; Li et al., 2019; Ghosh et al., 2020). Therefore, the resulted sentence embeddings can locate in the poorly-defined areas, and the induced similarity can be problematic. &quot; Invertible mapping The BERT sentence embedding space Standard Gaussian latent space (isotropic) Figure 1: An illustration of our proposed flow-based calibration over the original sentence embedding space of BERT. 3 Proposed Method: BERT-flow To verify the hypotheses proposed in Section 2.2, and to circumvent the incompetence of the BERT sentence embeddings, we proposed a calibration method called BERT-flow in which we take advantage of an i"
2020.emnlp-main.733,2021.ccl-1.108,0,0.220472,"Missing"
2020.emnlp-main.733,D16-1264,0,0.0232629,"T baselines in most cases, and outperforms the state-of-the-art SBERT/SRoBERTa results by a large margin. Robustness analysis with respect to random seeds are provided in Appendix C. 4.2 Unsupervised Question-Answer Entailment In addition to the semantic textual similarity tasks, we examine the effectiveness of our method on unsupervised question-answer entailment. We use Question Natural Language Inference (QNLI, Wang et al. (2019)), a dataset comprising 110K question-answer pairs (with 5K+ for testing). QNLI extracts the questions as well as their corresponding context sentences from SQUAD (Rajpurkar et al., 2016), and annotates each pair as either entailment or no entailment. In this paper, we further adapt QNLI as an unsupervised task. The similarity between a question and an answer can be predicted by computing the cosine similarity of their sentence embeddings. Then we regard entailment as 1 and no entailment as 0, and evaluate the performance of the methods with AUC. As shown in Table 4, our method consistently improves the AUC on the validation set of QNLI. Also, learning flow on the target dataset can produce superior results compared to learning flows on NLI. 9125 Method AUC BERTbase -NLI-last2"
2020.emnlp-main.733,D19-1410,0,0.237997,"fer from anisotropy, we hypothesize that the sentence embeddings from BERT – as average of context embeddings from last layers1 – may suffer from similar issues. Through empirical probing over the embeddings, we further observe that the BERT sentence embedding space is semantically non-smoothing and poorly defined in some areas, which makes it hard to be used directly through simple similarity metrics such as dot 1 In this paper, we compute average of context embeddings from last one or two layers as our sentence embeddings since they are consistently better than the [CLS] vector as shown in (Reimers and Gurevych, 2019). 9119 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 9119–9130, c November 16–20, 2020. 2020 Association for Computational Linguistics product or cosine similarity. To address these issues, we propose to transform the BERT sentence embedding distribution into a smooth and isotropic Gaussian distribution through normalizing flows (Dinh et al., 2015), which is an invertible function parameterized by neural networks. Concretely, we learn a flow-based generative model to maximize the likelihood of generating BERT sentence embeddings from a standard G"
2020.emnlp-main.733,N18-1101,0,0.234674,"s then used to transform the BERT sentence embedding to the Gaussian space. We name the proposed method as BERT-flow. We perform extensive experiments on 7 standard semantic textual similarity benchmarks without using any downstream supervision. Our empirical results demonstrate that the flow transformation is able to consistently improve BERT by up to 12.70 points with an average of 8.16 points in terms of Spearman correlation between cosine embedding similarity and human annotated similarity. When combined with external supervision from natural language inference tasks (Bowman et al., 2015; Williams et al., 2018), our method outperforms the sentence-BERT embeddings (Reimers and Gurevych, 2019), leading to new state-of-theart performance. In addition to semantic similarity tasks, we apply sentence embeddings to a question-answer entailment task, QNLI (Wang et al., 2019), directly without task-specific supervision, and demonstrate the superiority of our approach. Moreover, our further analysis implies that BERT-induced similarity can excessively correlate with lexical similarity compared to semantic similarity, and our proposed flow-based method can effectively remedy this problem. 2 Understanding the S"
2020.findings-emnlp.115,N19-1423,0,0.00689445,"rance. Paris located in France. Is Paris located in France? TSMH 1 : Deletion Accepted 4 H M Probability π(x) 4 2 3 Rejected Sentence edit space (b) Introduction Supervised techniques still dominate in natural language generation tasks. Despite its success, supervised approaches need to be trained with massive datasets of input-output pairs, which is non-trivial to acquire. In addition, it is hard to guarantee that the output sentences satisfy constraints. Recent approaches first pre-train a language model on a general-purpose dataset, then fine-tune the neural net on a task-specific dataset (Devlin et al., 2019; Radford et al., 2019). These approaches partially mitigate data hunger in training large and flexible neural networks. Nevertheless, they still require carefully crafted datasets for fine-tuning. We present a constraint satisfaction driven approach for language generation. In particular, we 1 Supervised training CG 1 Input-output dataset Figure 1: (a) Natural language generation via constraint satisfaction (bottom), comparing to supervised approach (up). (b) Our proposed tree search enhanced MCMC (TSMH, pink line) traverses the probabilistic space of high-quality sentences more effectively t"
2020.findings-emnlp.115,W19-8604,1,0.868676,"Missing"
2020.findings-emnlp.115,E17-2068,0,0.0189347,"Missing"
2020.findings-emnlp.115,D15-1080,0,0.0225003,"s, while previous approaches must go through infeasible intermediate states. Such moves are typically rejected by MCMC and therefore result in a slow mixing rate (See Figure 1(b) and Section 3.1). In literature, constrained language generation has been attacked in a supervised way in (Sutskever et al., 2014; Berglund et al., 2015; Hu et al., 2017; Zhang et al., 2019; Miao et al., 2020). There are also multiple works of literature which model language rules as decomposed tree structures (Lee et al., 2019) or sentiment tags (Su et al., 2018). Markov Logic network (Richardson and Domingos, 2006; Khot et al., 2015) are also used to formulate grammar rules. The distance between vectors representing sentences meaning is considered as soft constraints in (Prabhumoye et al., 2018; Belanger and McCallum, 2016; Amato and MacDonald, 2010). In a nutshell, we summarize our contributions as follows: 1. We define the problem of constraint satisfaction driven natural language generation, and propose a sampling-based approach to tackle the problem with combinatorial constraints. 2. We propose a Tree Search enhanced Metropolis-Hastings approach (TSMH) for the proposed task, which mixes faster than standard MCMC in th"
2020.findings-emnlp.115,P19-1484,0,0.019946,"Missing"
2020.findings-emnlp.115,P14-5010,0,0.0043529,"Missing"
2020.findings-emnlp.115,2020.acl-main.314,1,0.753757,"Missing"
2020.findings-emnlp.115,P18-1080,0,0.0210535,"te (See Figure 1(b) and Section 3.1). In literature, constrained language generation has been attacked in a supervised way in (Sutskever et al., 2014; Berglund et al., 2015; Hu et al., 2017; Zhang et al., 2019; Miao et al., 2020). There are also multiple works of literature which model language rules as decomposed tree structures (Lee et al., 2019) or sentiment tags (Su et al., 2018). Markov Logic network (Richardson and Domingos, 2006; Khot et al., 2015) are also used to formulate grammar rules. The distance between vectors representing sentences meaning is considered as soft constraints in (Prabhumoye et al., 2018; Belanger and McCallum, 2016; Amato and MacDonald, 2010). In a nutshell, we summarize our contributions as follows: 1. We define the problem of constraint satisfaction driven natural language generation, and propose a sampling-based approach to tackle the problem with combinatorial constraints. 2. We propose a Tree Search enhanced Metropolis-Hastings approach (TSMH) for the proposed task, which mixes faster than standard MCMC in the presence of combinatorial constraints. 3. Experiment results on generating interrogative, imperative sentences with keywords, and sentences with given sentiments"
2020.findings-emnlp.115,P18-2124,0,0.0375185,"Missing"
2020.findings-emnlp.115,P19-1559,1,0.825676,"s motivated by Sample-Search (Gogate and Dechter, 2007a,b, 2011), which integrates backtrack search into importance sampling. Making multiple word-level changes within one proposal step of MCMC allows the direct transition between legitimate sentences, while previous approaches must go through infeasible intermediate states. Such moves are typically rejected by MCMC and therefore result in a slow mixing rate (See Figure 1(b) and Section 3.1). In literature, constrained language generation has been attacked in a supervised way in (Sutskever et al., 2014; Berglund et al., 2015; Hu et al., 2017; Zhang et al., 2019; Miao et al., 2020). There are also multiple works of literature which model language rules as decomposed tree structures (Lee et al., 2019) or sentiment tags (Su et al., 2018). Markov Logic network (Richardson and Domingos, 2006; Khot et al., 2015) are also used to formulate grammar rules. The distance between vectors representing sentences meaning is considered as soft constraints in (Prabhumoye et al., 2018; Belanger and McCallum, 2016; Amato and MacDonald, 2010). In a nutshell, we summarize our contributions as follows: 1. We define the problem of constraint satisfaction driven natural la"
2020.findings-emnlp.441,J96-1002,0,0.111295,"ing as the evaluation metric. 2. For the other circumstances where no such special token can be used, a mean-pooling operation P is applied to the encoder output, i.e. 1 x = n nt=1 ht , where ht denotes the contextual word representation of the tth token produced by the encoder. The latent space H is spanned by all the latent states. Baseline Approaches. We use two common baseline approaches in NLP active learning to compare with our framework, namely random sampling (RM) and entropy-based uncertainty sampling (US). For sequence classification tasks, we adopt the widely used Max Entropy (ME) (Berger et al., 1996) as uncertainty measurement: H ME (x) = − c X P (y = m|x) log P (y = m|x) (3) m=1 where c is the number of classes. For sequence labeling tasks, we use total token entropy (TTE) (Settles and Craven, 2008) as uncertainty measurement: H T T E (x) = − l N X X P (yi = m|x) log P (yi = m|x) i=1 m=1 (4) where N is the sequence length and l is the number of labels. Latent Space Definition We use the adversarial attack in our AUSDS learning framework to find informative samples, which rely on a well-defined latent space. Two types of latent spaces are defined here based on the encoder architectures an"
2020.findings-emnlp.441,C04-1051,0,0.0605365,"which requires renewal of the sampler mapper M . The algorithm terminates until the unlabeled text corpus Ti is used up. 4 Experiments We evaluate the AUSDS learning framework on sequence classification and sequence labeling tasks. For the oracle labeler O, we directly use the labels provided by the datasets. In all the experiments, we take average results of 5 runs with different random seeds to alleviate the influence of randomness. 4.1 Set-up Dataset. We use five datasets, namely Stanford Sentiment Treebank (SST-2 / SST-5) (Socher et al., 2013), Microsoft Research Paraphrase Corpus (MRPC) (Dolan et al., 2004), AG’s News Corpus (AG News) (Zhang et al., 2015) and CoNLL 2003 Named Entity Recognition dataset 4912 Dataset SST-2 (Socher et al., 2013) SST-5 (Socher et al., 2013) MRPC (Dolan et al., 2004) AG News (Zhang et al., 2015) CoNLL’03 (Sang and De Meulder, 2003) Task sequence classification sequence classification sequence classification sequence classification sequence labeling Sample Size 11.8k sentences, 215k phrases 11.8k sentences, 215k phrases 5,801 sentence pairs 12k sentences 22k sentences, 300k tokens Table 1: 5 datasets we used for sentence learning experiments, across sequence classific"
2020.findings-emnlp.441,N18-1202,0,0.150104,"dversarial uncertainty sampling in discrete space (AUSDS) to retrieve informative unlabeled samples more efficiently. AUSDS maps sentences into latent space generated by the popular pre-trained language models, and discover informative unlabeled text samples for annotation via adversarial attack. The proposed approach is extremely efficient compared with traditional uncertainty sampling with more than 10x speedup. Experimental results on five datasets show that AUSDS outperforms strong baselines on effectiveness. 1 Introduction Deep neural models become popular in natural language processing (Peters et al., 2018; Radford et al., 2018; Devlin et al., 2018). Neural models usually consume massive labeled data, which requires a huge quantity of human labors. But data are not born equal, where informative data with high uncertainty are decisive to decision boundary and are worth labeling. Thus selecting such worth-labeling data from unlabeled text corpus for annotation is an effective way to reduce the human labors and to obtain informative data. Active learning approaches are a straightforward choice to reduce such human labors. Previous works, such as uncertainty sampling (Lewis and Gale, 1994), needs t"
2020.findings-emnlp.441,W03-0419,0,0.140818,"Missing"
2020.findings-emnlp.441,D08-1112,0,0.883398,"decisive to decision boundary and are worth labeling. Thus selecting such worth-labeling data from unlabeled text corpus for annotation is an effective way to reduce the human labors and to obtain informative data. Active learning approaches are a straightforward choice to reduce such human labors. Previous works, such as uncertainty sampling (Lewis and Gale, 1994), needs to traverse all unlabeled data to find informative unlabeled samples, which are always near the decision boundary with large entropy. However, the traverse process is very time-consuming, thus cannot be executed frequently (Settles and Craven, 2008). A common choice is to perform the sampling process after every specific period, and it samples and labels informative unlabeled data then trains the model until convergence (Deng et al., 2018). We argue that infrequently performing uncertainty sampling may lead to the “ineffective sampling” problem. Because in the early phase of training, the decision boundary changes quickly, which makes previously collected samples less effective after several updates of the model. Ideally, uncertainty sampling should be performed frequently in the early phase of model training. In this paper, we propose t"
2020.findings-emnlp.441,D13-1170,0,\N,Missing
2020.fnp-1.17,P19-1100,0,0.0382318,"Missing"
2020.fnp-1.17,D19-1387,0,0.061273,"Missing"
2020.sdp-1.25,C14-1008,0,0.119737,"Missing"
2020.sdp-1.25,D14-1181,0,0.0161144,"Missing"
2020.sdp-1.25,D19-1387,0,0.0318855,"l. (2015) introduced an attention mechanism to the Seq2Seq model, which enables the model to focus on words in specific positions in the original text via the weight matrix when generating abstracts, thus avoiding the problem of losing too much information due to long sentences. Since BERT (Devlin et al., 2018) has achieved great success in the field of NLP, the method of pre-training and fine-tuning has become a new paradigm. Researchers began to explore how to apply pre-trained models to natural language generation. At first, researchers tried to replace the encoder with a pre-trained BERT (Liu and Lapata, 2019), then more and more pre-training target functions for the Seq2Seq model were explored like masked generation (Song et al., 2019), denoising (Lewis et al., 2019), text-to-text (Raffel et al., 2019a). Some specially designed tasks for summarization have also been proposed, such as extracting gap-sentences (Zhang et al., 2019). We use the gap-sentence method in (Zhang et al., 2019) to combine and transform all the data, then utilize the T5 model (Lewis et al., 2019) to fine-tune and generate the summary. 3 Method 3.1 3.1.1 CL-SciSumm Task1A As shown in Figure 1, the citation linkage task, Task1A"
2020.sdp-1.25,P19-1204,0,0.0958567,"different gold summaries. It indicates that end to end supervised learning method can extract better feature than human, even the supervised signals are constructed indirectly (we construct extractive summarization training data from human-write summarization dataset). Although DPPs performs well on improving the diversity of summaries, its ability to evaluate the quality of sentence comes from handcrafted feature, which generalize worse. 4.2 which contain around 700 articles with an average of 31.7 sentences per summary and an average of 21.6 words per sentence. The extractive data are from Lev et al. (2019) which have 1705 paper-summary pairs. For each paper, it provides a summary with 30 sentences and 990 words on average. The LongSumm shared task is characterized by long input and output with a high compression ratio. So we choose a mix-and-divide method to deal with it: 1. To make full use of all data samples, we mix abstractive and extractive data. 2. Transform the full paper level summarization into short document summarization by dividing all article-summary pairs into sectionsummary pairs. 3. Relabel all samples for abstractive models and extractive models. The first step is easy to under"
2020.sdp-1.25,K17-1045,0,0.0269293,"the quality of summary generation through experiments. GCN is a powerful neural network framework processing graph structural data. Defferrard et al. (2016) extended the traditional CNN to nonEuclidean space and introduce local spectral filtering to optimize the propagation process during the training of the standard graph neural network. Kipf and Welling (2017) further studied the application of GCN in semi-supervised classification. GAT (Veliˇckovi´c et al., 2017) allocates different weights on different node neighbors to aggregate information. A document can also be converted into a graph. Yasunaga et al. (2017) introduced GCN in multi-document summarization. The clusters of documents were fed into RNN to obtain intermediate representations. Then GCN continued to extracting features considering the connections of documents clusters. At last, each sentence was Figure 1: The complete process of Task1A. scored based on its cluster-aware representations, and sentences with high score were chosen as summaries. As for abstractive summarization, Rush et al. (2015) introduced an attention mechanism to the Seq2Seq model, which enables the model to focus on words in specific positions in the original text via"
2020.sdp-1.25,P19-1100,0,0.0219402,"al., 2015), FastText (Joulin et al., 2016) and CharCNN (Zhang et al., 2015), can work directly on text, and generate dense vectors for classification. The Task2 of CL-SciSumm and the LongSumm shared task are both summarization task. Recently, the research on automatic summarization tasks has mainly focused on two ways: extractive summarization and abstractive summarization. In the field of extractive summarization, We studied the sampling process used in DPPs (Kulesza and Taskar, 2012) where we calculated the kernel matrix using WMD sentence similarity for further sampling (Li et al., 2018). Zhong et al. (2019) explored how to make the system generate higher quality summaries. They selected three metrics: network architecture, knowledge transfer, and learning mode, and analyzed the impact of the three metrics on the quality of summary generation through experiments. GCN is a powerful neural network framework processing graph structural data. Defferrard et al. (2016) extended the traditional CNN to nonEuclidean space and introduce local spectral filtering to optimize the propagation process during the training of the standard graph neural network. Kipf and Welling (2017) further studied the applicati"
2020.wmt-1.112,Q19-1038,0,0.0136828,"he hyper-parameters for the best filtering performance, and four systems are ensembled to achieve the final results. 2 System Architecture 2.1 Data Introduction In detail, as is shown in Table 1, the WMT20 shared task provides: • Document pairs, including 391, 250 KhmerEnglish and 45, 312 Pashto-English document pairs; • Sentence-aligned corpora extracted from the 985 983 Proceedings of the 5th Conference on Machine Translation (WMT), pages 983–988 985–990 c Online, November 19–20, 2020. 2020 Association for Computational Linguistics above document pairs, using Hunalign (DBL, 2008) and LASER (Artetxe and Schwenk, 2019), including 4, 169, 574 Khmer-English and 1, 022, 883 Pashto-English sentence pairs; • Parallel data which can be used to build filtering and alignment system, including 290, 051 Khmer-English and 123, 198 Pashto-English parallel sentences; • Monolingual data, including approximately 1.9 billion English, 14.0 million Khmer, and 6.6 million Pashto sentences. 2.2.1 We trained the word alignment model on the provided clean parallel corpus by using the fast-align toolkit (Dyer et al., 2013), and get the forward and reverse word translation probability tables. It’s worth mentioning that both of Pas"
2020.wmt-1.112,W19-5404,0,0.044703,"Missing"
2020.wmt-1.112,W19-5358,0,0.221143,"Thus we choose to extract our own set of sentence pairs from the provided document pairs, and design a mining module aiming to gather as many parallel sentence candidates as possible. We then elaborate on our mining procedure and mining module, shown in Figure 1, in detail. merge True Document Pairs Word Alignment Model High Quality? Mining Module Scoring Module LASER Candidates Mining Parallel Sentences This step is operated by our mining module. With the bilingual word translation probability tables, the mining module evaluates the translation quality of bilingual sentence pairs by YiSi-2 (Lo, 2019), which involves both lexical weight and lexical similarity. The Document pairs are first segmented on each language side using Polyglot 2 . This initial segmentation is represented as: en-ps 45K 1.0M 123K Initial Parallel Corpus Word Alignment e = e1 e2 · · · ea = ea1 (1) b f = f 1f 2 · · · f b = f 1 (2) where ek (f k ) is a segment of consecutive words of document e (f ). Then we compute the sentence similarity (translation quality) by iteration from the initial segment (e1 , f 1 ). If the similarity reaches the preset threshold for (ei , f j ), we pick the segment pair as parallel sentence"
2020.wmt-1.112,W03-2205,0,0.090179,"e pick the segment pair as parallel sentence candidate, and continue the computation from (ei+1 , f j+1 ). We notice that the inconsistency of segmentation in the document pairs can lead to the results: a sentence in one language contains information only part of a sentence in the other language, or two sentences (in different languages) both contain part of their information in common. These resulting sentence pairs may have low similarity scores. In order to alleviate this problem, we also incorporate a parallel segmentation method in our mining module. We follow the basic idea proposed in (Nevado et al., 2003) where the parallel segmentation finding problem is treated as an optimization problem and a dynamic programming scheme is 1 https://github.com/moses-smt/ mosesdecoder/blob/master/scripts/ tokenizer/tokenizer.perl 2 https://github.com/aboSamoor/polyglot Figure 1: Mining Procedure 984 986 used to search for the best segmentation. We then briefly introduce our method. 3 After obtaining the monolingual initial segmentab tion (ea1 , f 1 ), a parallel segmentation is represented as: j1 j2 s ≡ ([ek11 , f 1 ], [ekk21 +1 , f j1 +1 ], k s| j s| | | · · · , [ek|s|−1 +1 , f j|s|−1 +1 ]) (3) where |s |is"
2020.wmt-1.112,W18-6301,0,0.0197889,"irs can be filtered. Our system, Volctrans, is made of two modules, i.e., a mining module and a scoring module. Based on the word alignment model, the mining module adopts an iterative mining strategy to extract latent parallel sentences. In the scoring module, an XLM-based scorer provides scores, followed by reranking mechanisms and ensemble. Our submissions outperform the baseline by 3.x/2.x and 2.x/2.x for km-en and ps-en on From Scratch/Fine-Tune conditions. 1 Introduction With the rapid development of machine translation, especially Neural Machine Translation (NMT) (Vaswani et al., 2017; Ott et al., 2018; Zhu et al., 2020), parallel corpus in high quality and large quantity is in urgent demand. These parallel corpora can be used to train and build robust machine translation models. However, for some language pairs on low-resource conditions, few parallel resources are available. Since it is much easier to obtain quantities of monolingual data, it may help if we can extract parallel sentences from monolingual data through alignment and filtering. The WMT19 shared task on parallel corpus filtering for low-resource conditions (Koehn et al., 2019) provides noisy parallel corpora in SinhalaEnglish"
2020.wmt-1.112,W19-5434,0,0.0197698,"ctive. For each objective, we hold out 5K sentences or sentence pairs for validation and 5K for the test. We pre-train the XLM using two different settings on 8 Tesla-V100 GPU: a) Standard: The embedding size is 1024, with 12 layers and 64 batch size. b) Small: The embedding size is 512, with 6 layers and 32 batch size. The other values of hyperparameters are all set to the default values. The two pre-trained XLM model is then fine-tuned in downstream task and further ensembled. To score sentence pairs according to their parallelism, classification models are usually used (Xu and Koehn, 2017; Bernier-Colborne and Lo, 2019). In the training phrase, it is formulated as a binary classification problem, whether the sentence pair is semantically similar to each other or not. In the inference phrase, the probability of the positive class is considered as the score of the sentence pair. Therefore, we use the provided parallel sentence pairs as the positive instances, and construct negative instances taking advantage of such positive instances similar to Bernier-Colborne and Lo (2019). Specifically, we generate negative examples in the following ways: • Shuffle the sentences in source language and target language respe"
2020.wmt-1.112,N19-1423,0,0.00836658,"air. Secondly, different reranking mechanisms are used to adjust the scores. Finally, we ensemble four different models to improve the performance of our systems. 2.3.1 XLM-based Scorer Recently, pre-trained transformer-based models play an important role in a variety of NLP tasks, such as question answering, relation extraction, etc.. Pre-trained models are often trained from scratch with self-supervised objective, and then fine-tuned to adapt to the downstream tasks. In our system, we choose the XLM (Conneau and Lample, 2019) as our main model. The reason are as follows: a) Similar to BERT (Devlin et al., 2019; Yang et al., 2019), XLM has Masked Language Model (MLM) objective, which enables us to make the most use of the provided monolingual corpora; b) XLM also has Translation Language Model (TLM) objective. Taking two parallel sentences as input, it predicts the randomly masked tokens. In this way, crosslingual features can be captured; c) With a large amount of training corpus in different languages, XLM can provide powerful cross-lingual representation for downstream tasks, which is very suitable for parallel corpus filtering situations. We follow the instructions 4 to prepare the training data"
2020.wmt-1.112,N13-1073,0,0.0375979,"ssociation for Computational Linguistics above document pairs, using Hunalign (DBL, 2008) and LASER (Artetxe and Schwenk, 2019), including 4, 169, 574 Khmer-English and 1, 022, 883 Pashto-English sentence pairs; • Parallel data which can be used to build filtering and alignment system, including 290, 051 Khmer-English and 123, 198 Pashto-English parallel sentences; • Monolingual data, including approximately 1.9 billion English, 14.0 million Khmer, and 6.6 million Pashto sentences. 2.2.1 We trained the word alignment model on the provided clean parallel corpus by using the fast-align toolkit (Dyer et al., 2013), and get the forward and reverse word translation probability tables. It’s worth mentioning that both of Pashto and Khmer corpus are tokenized before word alignment model training for accuracy consideration. We separate Pashto words by moses tokenizer1 . For Khmer, we use the character (u200B in Unicode) as separator when it’s available and otherwise use a dictionarybased tokenizer by maximizing the word sequence probability. 2.2.2 Table 1: Statistics of Provided Data Scale Document Pairs Extracted Sentence Pairs Parallel Sentences 2.2 en-km 391K 4.2M 290K Mining Module Besides the given ali"
2020.wmt-1.33,W19-5206,0,0.0142554,"ystem. Apart from splitting the monolingual data into several disjoint parts, we sampled the parallel data so that each model has different deviations on the parallel data. We tested bagging sampling (sample with replacement) and up-sampling(sample with replacement under Tag Back-Translation Recently, back-translation (Edunov et al., 2018) is a standard method to improve the translation quality by leveraging the large scale monolingual data. Starting from WMT19, the source of the test set is the natural text and the of the test set is the translationese text. We find the tag back-translation (Caswell et al., 2019) method can achieve better BLEU compared with previous methods proposed in Edunov et al. (2018). 306 Testset Random w/ mRASP Ps→En En→Km Km→En En→Ta Ta→En 10.2 13.8 39.3 42.8 12.7 14.4 7.4 9.2 14.0 17.9 Table 1: Comparison between randomly initialized baseline model and model initialized from mRASP model Direction Model 1 Testset Model 2 Data Split Ensemble Model Baseline Iter 1 Iter 2 Iter 3 Model 3 Figure 1: Data Diversity Matters for Final System Iterative Joint Training Zhang et al. (2018) proposed an iterative joint training method for better usage of monolingual data from source side and"
2020.wmt-1.33,D18-1045,0,0.0117731,"000ffn and Dynamic Convolution. We report in Table 1 the best score in each setting and direction, and find that mRASP significantly outperforms the baseline. 3.2 Experiment Techniques Parallel Data Up-sampling According to the experiments, data diversity matters for the whole system. Apart from splitting the monolingual data into several disjoint parts, we sampled the parallel data so that each model has different deviations on the parallel data. We tested bagging sampling (sample with replacement) and up-sampling(sample with replacement under Tag Back-Translation Recently, back-translation (Edunov et al., 2018) is a standard method to improve the translation quality by leveraging the large scale monolingual data. Starting from WMT19, the source of the test set is the natural text and the of the test set is the translationese text. We find the tag back-translation (Caswell et al., 2019) method can achieve better BLEU compared with previous methods proposed in Edunov et al. (2018). 306 Testset Random w/ mRASP Ps→En En→Km Km→En En→Ta Ta→En 10.2 13.8 39.3 42.8 12.7 14.4 7.4 9.2 14.0 17.9 Table 1: Comparison between randomly initialized baseline model and model initialized from mRASP model Direction Mode"
2020.wmt-1.33,2020.emnlp-main.210,1,0.742443,"der or deeper Transformers, dynamic convolutions). The final system includes text pre-process, data selection, synthetic data generation, advanced model ensemble, and multilingual pretraining. 1 Introduction We participated in the WMT2020 shared news translation task in 14 directions: English↔Chinese, English↔German, French↔German, English↔Polish, English↔Tamil,English↔Pashto,English↔Khmer, covering language pairs from high to low resources. In this year’s translation task, we mainly focus on exploiting self-supervised and unsupervised methods for NMT to make full use of the monolingual data (Lin et al., 2020; Yang et al., 2019). We aims at building a general training framework which can be well applied to different translation directions. Our models are mainly based on the Transformer (Vaswani et al., 2017). Techniques used in the submitted systems include iterative back-translation, knowledge distillation. We also employed several tricks to improve in-domain BLEU scores, typically in-domain transfer learning. We also experimented with a multilingual pretraining technique which we proposed recently (Lin et al., 2020). 2 We use the implementations in Fairseq(Ott et al., 2019). All models are train"
2020.wmt-1.33,N16-1046,0,0.0303144,"Missing"
2020.wmt-1.33,N19-4009,0,0.014462,"the monolingual data (Lin et al., 2020; Yang et al., 2019). We aims at building a general training framework which can be well applied to different translation directions. Our models are mainly based on the Transformer (Vaswani et al., 2017). Techniques used in the submitted systems include iterative back-translation, knowledge distillation. We also employed several tricks to improve in-domain BLEU scores, typically in-domain transfer learning. We also experimented with a multilingual pretraining technique which we proposed recently (Lin et al., 2020). 2 We use the implementations in Fairseq(Ott et al., 2019). All models are trained with Adam optimizer (Kingma and Ba, 2014). We use the “inverse sqrt lr” scheduler with 4000 warm-up steps and set the max learning rate to 5e-4. The betas are (0.9, 0.98). During training, the batches are made of similar length sequences, so we avoid extreme cases where most sequences in the batch are short and we are required to add lots of pad tokens to each of them because one sequence of the same batch is very long. We limit the batch size to 8192 tokens per GPU, to avoid running out of GPU memory. Meanwhile, to achieve a larger batch size to improve the performanc"
2020.wmt-1.33,W18-6301,0,0.0316618,"All models are trained with Adam optimizer (Kingma and Ba, 2014). We use the “inverse sqrt lr” scheduler with 4000 warm-up steps and set the max learning rate to 5e-4. The betas are (0.9, 0.98). During training, the batches are made of similar length sequences, so we avoid extreme cases where most sequences in the batch are short and we are required to add lots of pad tokens to each of them because one sequence of the same batch is very long. We limit the batch size to 8192 tokens per GPU, to avoid running out of GPU memory. Meanwhile, to achieve a larger batch size to improve the performance(Ott et al., 2018), we set the parameter “update frequency” to 8, and train the model on 8 GPUs, resulting in an actual batch token size = 8192 × 8 × 8. During training, we employ label smoothing of 0.1 and set dropout rate (Hinton et al., 2012) to 0.2. 2.1 Following Sun et al. (2019); Wang et al. (2018), we use different architectures for Transformer(Vaswani et al., 2017) to increase the model diversity and potentially get a better ensemble model. Baseline Models We apply two different NMT skeletons for the shared news translation as our baseline systems. ∗ † Transformer Intern at ByteDance Intern at ByteDance"
2020.wmt-1.33,W18-6319,0,0.0125323,"n 3.1, we pretrained three multilingual models with different model architectures (DLCL 25layers, Transformer 15000ffn and Dynamic Convolution 25e6d) on all parallel data available in WMT20 except the English↔Chinese to avoid a large dictionary1 and fine-tuned the pre-trained models on the their own parallel data with different data sampling strategies to get 9 baseline models2 . Then we applied the tag back-translation, joint training, knowledge distillation and random ensemble methods as described in Section 3 to get the final translation system. All BLEU scores were reported with SacreBLEU(Post, 2018). 4.1 Chinese→English Final Submission We submitted our VolcTrans online system (unconstrained). The final submission achieves 36.6 BLEU. You can get access to VolcTrans online system on http://translate. volcengine.cn/. 1 Direction En→Zh Testset wmt19 Baseline iterative BT Ensemble KD Ensemble System 38.5 38.9 41.5 42.0 BLEU on WMT20 testset submission 44.9 Table 3: Results of English→Chinese by sacreBLEU 4.2 English→Chinese For English→Chinese, we train English↔Chinese jointly. We use all parallel data available: News Commentary v15, Wiki Titles v2, UN Parallel Corpus V1.0, CCMT Corpus and W"
2020.wmt-1.33,W19-5341,0,0.140709,"avoid extreme cases where most sequences in the batch are short and we are required to add lots of pad tokens to each of them because one sequence of the same batch is very long. We limit the batch size to 8192 tokens per GPU, to avoid running out of GPU memory. Meanwhile, to achieve a larger batch size to improve the performance(Ott et al., 2018), we set the parameter “update frequency” to 8, and train the model on 8 GPUs, resulting in an actual batch token size = 8192 × 8 × 8. During training, we employ label smoothing of 0.1 and set dropout rate (Hinton et al., 2012) to 0.2. 2.1 Following Sun et al. (2019); Wang et al. (2018), we use different architectures for Transformer(Vaswani et al., 2017) to increase the model diversity and potentially get a better ensemble model. Baseline Models We apply two different NMT skeletons for the shared news translation as our baseline systems. ∗ † Transformer Intern at ByteDance Intern at ByteDance • Transformer 15e6d: According to Sun et al. (2019), a transformer with larger encoder layer number can learn better representation of source sentence and get better BLEU scores. We increase the number of encoder layers from 6 to 15 layers in the transformer big arc"
2020.wmt-1.33,W18-6429,1,0.84313,"s where most sequences in the batch are short and we are required to add lots of pad tokens to each of them because one sequence of the same batch is very long. We limit the batch size to 8192 tokens per GPU, to avoid running out of GPU memory. Meanwhile, to achieve a larger batch size to improve the performance(Ott et al., 2018), we set the parameter “update frequency” to 8, and train the model on 8 GPUs, resulting in an actual batch token size = 8192 × 8 × 8. During training, we employ label smoothing of 0.1 and set dropout rate (Hinton et al., 2012) to 0.2. 2.1 Following Sun et al. (2019); Wang et al. (2018), we use different architectures for Transformer(Vaswani et al., 2017) to increase the model diversity and potentially get a better ensemble model. Baseline Models We apply two different NMT skeletons for the shared news translation as our baseline systems. ∗ † Transformer Intern at ByteDance Intern at ByteDance • Transformer 15e6d: According to Sun et al. (2019), a transformer with larger encoder layer number can learn better representation of source sentence and get better BLEU scores. We increase the number of encoder layers from 6 to 15 layers in the transformer big architecture which is t"
2021.acl-demo.7,N19-1423,0,0.0127725,"/kaldi-asr.org/ 56 various data formats defined by Dataset, preprocesses data samples according to Task and writes to the disk. Transfer Learning NeurST supports initializing the model variables from well-trained models as long as they have the same variable names. As for ST, we can initialize the ST encoder with a well-trained ASR encoder and initialize the ST decoder with a well-trained MT decoder, which facilitates to achieve promising improvements. Besides, NeurST also provides scripts for converting released models from other repositories, like wav2vec2.0 (Baevski et al., 2020) and BERT (Devlin et al., 2019). Researchers can conveniently integrate these pre-trained components to the customized models. end scale decay at decay steps MT ASR ST 1.0 3.5 3.5 1.0 2.0 1.5 50k 50k 50k 50k language aligned to text in a target language: libri-trans (Kocabiyikoglu et al., 2018) 5 is a small EN→FR dataset which was originally started from the LibriSpeech corpus, the audiobook recordings for ASR (Panayotov et al., 2015). The English utterances were automatically aligned to the e-books in French, and 236 hours of English speech aligned to French translations at utterance level were finally extracted. It has be"
2021.acl-demo.7,L18-1001,0,0.0284877,"e names. As for ST, we can initialize the ST encoder with a well-trained ASR encoder and initialize the ST decoder with a well-trained MT decoder, which facilitates to achieve promising improvements. Besides, NeurST also provides scripts for converting released models from other repositories, like wav2vec2.0 (Baevski et al., 2020) and BERT (Devlin et al., 2019). Researchers can conveniently integrate these pre-trained components to the customized models. end scale decay at decay steps MT ASR ST 1.0 3.5 3.5 1.0 2.0 1.5 50k 50k 50k 50k language aligned to text in a target language: libri-trans (Kocabiyikoglu et al., 2018) 5 is a small EN→FR dataset which was originally started from the LibriSpeech corpus, the audiobook recordings for ASR (Panayotov et al., 2015). The English utterances were automatically aligned to the e-books in French, and 236 hours of English speech aligned to French translations at utterance level were finally extracted. It has been widely used in previous studies. As such, we use the clean 100-hour portion plus the augmented machine translation from Google Translate as the training data and follow its split of dev and test data. MuST-C (Di Gangi et al., 2019)6 is a multilingual speech tra"
2021.acl-demo.7,N19-1202,0,0.135606,"Missing"
2021.acl-demo.7,D15-1166,0,0.0606516,"s and has widespread applications, like cross-language videoconferencing or customer support chats. Traditionally, researchers build a speech translation system via a cascading manner, including an automatic speech recognition (ASR) and a machine translation (MT) subsystem (Ney, 1999; Casacuberta et al., 2008; Kumar et al., 2014). Cascade systems, however, suffer from error propagation problems, where an inaccurate ASR output would theoretically cause translation errors. Owing to recent progress of sequence-to-sequence modeling for both neural machine translation (NMT) (Bahdanau et al., 2015; Luong et al., 2015; Vaswani et al., 2017) and end-to-end speech recognition (Chan et al., 2016; Chiu et al., 2018; Dong et al., 2018), 55 Proceedings of the Joint Conference of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: System Demonstrations, pages 55–62, August 1st - August 6th, 2021. ©2021 Association for Computational Linguistics NMT for cascade systems. We implement state-ofthe-art Transformer-based models (Vaswani et al., 2017; Karita et al., 2019) and provide step-by-step recipes for feature extractio"
2021.acl-demo.7,N16-1109,0,0.0615633,"Missing"
2021.acl-demo.7,2020.acl-demos.34,0,0.49433,"the method on TED English-Chinese; and Dong et al. (2021) use libri-trans English-French and IWSLT2018 English-German dataset; and Wu et al. (2020) show the results on CoVoST dataset and the FR/RO portions of MuST-C dataset. Different datasets make it difficult to compare the performance of their approaches. Further, even for the same dataset, the baseline results are not necessarily kept consistent. Take the libri-trans EnglishFrench dataset as an example. Dong et al. (2021) report the pre-trained baseline as 15.3 and the result of Liu et al. (2019) is 14.3 in terms of tokenized BLEU, while Inaguma et al. (2020) report 15.5 (detokenized BLEU). The mismatching baseline results in an unfair comparison on the improvements of their approaches. We think one of the primary reasons is that the preprocessing of audio data is complex, and the ST model training involves many tricks, such as pre-training and data augmentation. Therefore a reproducible and reliable benchmark is required. In this work, we present NeurST , a toolkit for easily building and training end-toend ST models, as well as end-to-end ASR and NeurST is an open-source toolkit for neural speech translation. The toolkit mainly focuses on end-to"
2021.acl-demo.7,2020.acl-main.344,0,0.703106,"he Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: System Demonstrations, pages 55–62, August 1st - August 6th, 2021. ©2021 Association for Computational Linguistics NMT for cascade systems. We implement state-ofthe-art Transformer-based models (Vaswani et al., 2017; Karita et al., 2019) and provide step-by-step recipes for feature extraction, data preprocessing, model training, and inference for researchers to reproduce the benchmarks. Though there exist several counterparts, such as Lingvo (Shen et al., 2019), fairseq-ST (Wang et al., 2020a) and Kaldi 1 style ESPnet-ST (Inaguma et al., 2020), NeurST is specially designed for speech translation tasks, which encapsulates the details of speech processing and frees the developers from data engineering. It is easy to use and extend. The contributions of this work are as follows: • NeurST is designed specifically for end-toend ST, with clean and simple code. It is lightweight and independent of Kaldi, which simplifies installation and usage, and is more compatible for NLP researchers. • We report strong benchmarks with welldesigned hyper-parameters and show best practice on several S"
2021.acl-demo.7,P16-1009,0,0.0473172,"9; Jiang et al., 2020) on large-scale scenarios. Design and Features NeurST is implemented with both TensorFlow2 and PyTorch backends. In this section, we will introduce the design components and features of this toolkit. 2.1 Data Preprocessing NeurST supports on-the-fly data preprocessing via a number of lightweight python packages, like python speech features2 for extracting audio features (e.g. mel-frequency cepstral coefficients and log-mel filterbank coefficients). And for text processing, NeurST integrates some effective tokenizers, including moses tokenizer3 , byte pair encoding (BPE) (Sennrich et al., 2016b) and SentencePiece4 . Alternatively, the training data can be preprocessed and stored in binary files (e.g., TFRecord) beforehand, which is guaranteed to improve the I/O performance during training. Moreover, to simplify such operations, NeurST provides the command-line tool to create such record files, which automatically iterates on Design NeurST divides one running job into four components: Dataset, Model, Task and Executor. Dataset NeurST abstracts out a common interface Dataset for data input. For example, we can train a speech translation model from either a raw dataset tarball or pre-"
2021.acl-demo.7,2021.naacl-industry.15,1,0.764527,"ning. By default, NeurST offers evaluation on development data during training and keeps track of the checkpoints with the best evaluation results. Monitoring NeurST supports TensorBoard for monitoring metrics during training, such as training loss, training speed, and evaluation results. Model Serving There is no gap between the research models and production models under NeurST , while they can be easily served with TensorFlow Serving. Moreover, for higher performance serving of standard transformer models, NeurST is able to integrate with other optimized inference libraries, like lightseq (Wang et al., 2021). 3.2 Data Preprocessing Beyond the officially released version, we performed no other audio to text alignment and data cleaning on libri-trans and MuST-C datasets. For speech features, we extracted 80-channel logmel filterbank coefficients with windows of 25ms and steps of 10ms, resulting in 80-dimensional features per frame. The audio features of each sample were then normalized by the mean and the standard deviation. All texts were segmented into subword level by first applying Moses tokenizer and then BPE. In detail, we removed all punctuations and lowercased the sentences in the source si"
2021.acl-demo.7,P16-1162,0,0.106414,"9; Jiang et al., 2020) on large-scale scenarios. Design and Features NeurST is implemented with both TensorFlow2 and PyTorch backends. In this section, we will introduce the design components and features of this toolkit. 2.1 Data Preprocessing NeurST supports on-the-fly data preprocessing via a number of lightweight python packages, like python speech features2 for extracting audio features (e.g. mel-frequency cepstral coefficients and log-mel filterbank coefficients). And for text processing, NeurST integrates some effective tokenizers, including moses tokenizer3 , byte pair encoding (BPE) (Sennrich et al., 2016b) and SentencePiece4 . Alternatively, the training data can be preprocessed and stored in binary files (e.g., TFRecord) beforehand, which is guaranteed to improve the I/O performance during training. Moreover, to simplify such operations, NeurST provides the command-line tool to create such record files, which automatically iterates on Design NeurST divides one running job into four components: Dataset, Model, Task and Executor. Dataset NeurST abstracts out a common interface Dataset for data input. For example, we can train a speech translation model from either a raw dataset tarball or pre-"
2021.acl-demo.7,2020.findings-emnlp.230,0,0.324467,"Missing"
2021.acl-demo.7,2021.iwslt-1.6,1,0.860965,"Missing"
2021.acl-demo.7,2020.aacl-demo.6,0,0.701062,"he Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: System Demonstrations, pages 55–62, August 1st - August 6th, 2021. ©2021 Association for Computational Linguistics NMT for cascade systems. We implement state-ofthe-art Transformer-based models (Vaswani et al., 2017; Karita et al., 2019) and provide step-by-step recipes for feature extraction, data preprocessing, model training, and inference for researchers to reproduce the benchmarks. Though there exist several counterparts, such as Lingvo (Shen et al., 2019), fairseq-ST (Wang et al., 2020a) and Kaldi 1 style ESPnet-ST (Inaguma et al., 2020), NeurST is specially designed for speech translation tasks, which encapsulates the details of speech processing and frees the developers from data engineering. It is easy to use and extend. The contributions of this work are as follows: • NeurST is designed specifically for end-toend ST, with clean and simple code. It is lightweight and independent of Kaldi, which simplifies installation and usage, and is more compatible for NLP researchers. • We report strong benchmarks with welldesigned hyper-parameters and show best practice on several S"
2021.acl-long.155,2021.naacl-main.458,1,0.735104,"Missing"
2021.acl-long.155,D19-1633,0,0.559953,"ngy2 Hamming h22 Hamming Replace h Replace 0.5 0.5 Replace h0.5 h2h2 y2 y2 h2 yy2 2 y2 y2 y2Hamming 2 2 Inputs Distance Inputs Distance Inputs Distance Inputs 0.7 yy33 y3 N(0.7 0.7y 0.7 3   y h3h3 y3 y3 h3 yy3 3 y3 y3 y3N(Y,yN( 3 Y, Y ) = 33 3  Y Y, ) =Y 3) = 3 y y4 0.6 0.6 hh44 0.6h yy4 4 ysuch y4 yword h4h4Notice y4 y4 h4 that h0.6 sentence. interdependency 4 4 4 4 4 yy55 y5 0.9 0.9 0.9y y0.9 h h y y h5 yy 5 y y y y5 5 is crucial, Transformer explicitly captures 5 5 5 5 5 5 5 5 as5 the that via decoding from left to right (Figure 1a). Several remedies are proposed (Ghazvininejad et al., 2019; Gu et al., 2019) to capture word interdependency while keeping parallel decoding. Their common idea is to decode the target tokens iteratively while each pass of decoding is trained using the masked language model (Figure 1c). Since these methods require multiple passes of decoding, its generation speed is measurably slower than the vanilla NAT. With single-pass generation only, these methods still largely lag behind the autoregressive Transformer. 1993 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natur"
2021.acl-long.155,2020.acl-main.36,0,0.423954,"complex methods to better decide the output lengths: noisy parallel decoding (NPD) and connectionist temporal classification (CTC). For NPD (Gu et al., 2018), we first predict m target length candidates, then generate output sequences with argmax decoding for each target length candidate. Then we use a pre-trained 1996 Idec Models AT Models Iterative NAT w/ CTC Fully NAT Transformer (Vaswani et al., 2017) Transformer (ours) T T 27.30 27.48 / 31.27 / 33.70 / 34.05 / 1.0×† NAT-IR (Lee et al., 2018) LaNMT (Shu et al., 2020) LevT (Gu et al., 2019) Mask-Predict (Ghazvininejad et al., 2019) JM-NAT (Guo et al., 2020b) 10 4 6+ 10 10 21.61 26.30 27.27 27.03 27.31 25.48 / / 30.53 31.02 29.32 / / 33.08 / 30.19 29.10 33.26 33.31 / 1.5× 5.7× 4.0× 1.7× 5.7× NAT-FT (Gu et al., 2018) Mask-Predict (Ghazvininejad et al., 2019) imit-NAT (Wei et al., 2019) NAT-HINT (Li et al., 2019) Flowseq (Ma et al., 2019) NAT-DCRF (Sun et al., 2019) 1 1 1 1 1 1 17.69 18.05 22.44 21.11 23.72 23.44 21.47 21.83 25.67 25.24 28.39 27.22 27.29 27.32 28.61 / 29.73 / 29.06 28.20 28.90 / 30.72 / 15.6× / 18.6× / 1.1× 10.4 × NAT-CTC (Libovick`y and Helcl, 2018) Imputer (Saharia et al., 2020) 1 1 16.56 25.80 18.64 28.40 19.54 32.30 24.67 31.7"
2021.acl-long.155,D18-1149,0,0.0780326,"Missing"
2021.acl-long.155,D19-1573,0,0.0461076,"Missing"
2021.acl-long.155,D18-1336,0,0.0366605,"Missing"
2021.acl-long.155,D19-1437,0,0.242673,"ate. Then we use a pre-trained 1996 Idec Models AT Models Iterative NAT w/ CTC Fully NAT Transformer (Vaswani et al., 2017) Transformer (ours) T T 27.30 27.48 / 31.27 / 33.70 / 34.05 / 1.0×† NAT-IR (Lee et al., 2018) LaNMT (Shu et al., 2020) LevT (Gu et al., 2019) Mask-Predict (Ghazvininejad et al., 2019) JM-NAT (Guo et al., 2020b) 10 4 6+ 10 10 21.61 26.30 27.27 27.03 27.31 25.48 / / 30.53 31.02 29.32 / / 33.08 / 30.19 29.10 33.26 33.31 / 1.5× 5.7× 4.0× 1.7× 5.7× NAT-FT (Gu et al., 2018) Mask-Predict (Ghazvininejad et al., 2019) imit-NAT (Wei et al., 2019) NAT-HINT (Li et al., 2019) Flowseq (Ma et al., 2019) NAT-DCRF (Sun et al., 2019) 1 1 1 1 1 1 17.69 18.05 22.44 21.11 23.72 23.44 21.47 21.83 25.67 25.24 28.39 27.22 27.29 27.32 28.61 / 29.73 / 29.06 28.20 28.90 / 30.72 / 15.6× / 18.6× / 1.1× 10.4 × NAT-CTC (Libovick`y and Helcl, 2018) Imputer (Saharia et al., 2020) 1 1 16.56 25.80 18.64 28.40 19.54 32.30 24.67 31.70 / 18.6× 1 1 1 1 1 19.17 24.15 25.20 25.31 26.07 23.20 27.28 29.52 30.68 29.68 29.79 31.45 / 32.20 / 31.44 31.81 / 32.84 / 2.4× 9.7× / / 6.1× 1 1 1 1 1 20.36 25.52 25.21 26.39 26.55 24.81 28.73 29.84 29.54 31.02 28.47 32.60 31.19 32.79 32.87 29.43 33.46 32.04 33.84 33.51 15.3×† 14.6"
2021.acl-long.155,P19-2049,0,0.0193196,"ed language model, and the model iteratively replaces masked tokens with new outputs. (Li et al., 2020) first predict the left token and right token for each position, and decode the final token at the current position conditioned on the left-and-right tokens predicted before. Despite the relatively better accuracy, the multiple decoding iterations reduce the inference efficiency of non-autoregressive models. Scheduled Sampling To alleviate exposure bias in autoregressive models, previous work attempts to close the gap between training and inference by scheduled sampling (Bengio et al., 2015; Mihaylova and Martins, 2019). Although scheduled sampling also modifies decoder inputs in training, there are mainly two differences between our work and scheduled sampling. Firstly, scheduled sampling mixes up the predicted sequence and the gold target sequence, and our method does not mix predicted sequences into decoder inputs. Besides, GLAT aims to learn word interdependency for single-pass parallel generation and scheduled sampling is designed for alleviating exposure bias. 6 Conclusion In this paper, we propose Glancing Transformer with a glancing language model to improve the performance of single-pass parallel ge"
2021.acl-long.155,2020.emnlp-main.83,0,0.461609,"Missing"
2021.acl-long.155,P16-1162,0,0.0586813,"ated tokens after generation. 4 Experiments In this section, we first introduce the settings of our experiments, then report the main results compared with several strong baselines. Ablation studies and further analysis are also included to verify the effects of different components used in GLAT. 4.1 Experimental Settings Datasets We conduct experiments on three machine translation benchmarks: WMT14 EN-DE (4.5M translation pairs), WMT16 EN-RO (610k translation pairs), and IWSLT16 DE-EN (150K translation pairs). These datasets are tokenized and segmented into subword units using BPE encodings (Sennrich et al., 2016). We preprocess WMT14 EN-DE by following the data preprocessing in Vaswani et al. (2017). For WMT16 EN-RO and IWSLT16 DE-EN, we use the processed data provided in Lee et al. (2018). Knowledge Distillation Following previous work (Gu et al., 2018; Lee et al., 2018; Wang et al., 2019), we also use sequence-level knowledge distillation for all datasets. We employ the transformer with the base setting in Vaswani et al. (2017) as the teacher for knowledge distillation. Then, we train our GLAT on distilled data. Baselines and Setup We compare our method with the base Transformer and strong represent"
2021.acl-long.155,P19-1125,1,0.913324,"t not selected. The training loss above is calculated against these remaining tokens. 1995 GLAT adopts similar encoder-decoder architecture as the Transformer with some modification (Figure 1d). Its encoder fenc is the same multihead attention layers. Its decoder fdec include multiple layers of multi-head attention where each layer attends to the full sequence of both encoder representation and the previous layer of decoder representation. During the initial prediction, the input to the decoder H = {h1 , h2 , ..., hT } are copied from the encoder output using either uniform copy or soft copy (Wei et al., 2019). The initial tokens Yˆ are predicted using argmax decoding with fdec (fenc (X; θ), H; θ). To calculate the loss LGLM , we compare the initial prediction Yˆ against the ground-truth to select tokens within the target sentence, i.e. GS(Y, Yˆ ). We then replace those sampled indices of h’s with corresponding target word embeddings, H 0 = RP(Embyt ∈GS(Y,Yˆ ) (yt ), H), where RP replaces the corresponding indices. Namely, if a token in the target is sampled, its word embedding replaces the corresponding h. Here the word embeddings are obtained from the softmax embedding matrix of the decoder. The"
2021.acl-long.19,N19-1423,0,0.0134957,"16) Katiyar and Cardie (2017) Li et al. (2019) Wang and Lu (2020) Zhong and Chen (2020) Zhong and Chen (2020) Relation R F1 Table 3: Overall evaluation.  means that the model leverages cross-sentence context information. relation type is correct, as well as the boundaries and types of two argument entities are correct. Implementation Details We tune all hyperparameters based on the averaged entity F1 and relation F1 on ACE05 development set, then keep the same settings on ACE04 and SciERC. For fair comparison with previous works, we use three pre-trained language models: bert-base-uncased (Devlin et al., 2019), albert-xxlarge-v1 (Lan et al., 2019) and scibert-scivocab-uncased (Beltagy et al., 2019) as the sentence encoder and fine-tune them in training stage.12 For the MLP layer, we set the hidden size as d = 150 and use GELU as the activation function. We use AdamW optimizer (Loshchilov and Hutter, 2017) with β1 = 0.9 and β2 = 0.9, and observe a phenomenon similar to (Dozat and Manning, 2016) in that setting β2 from 0.9 to 0.999 causes a significant drop on final performance. The batch size is 32, and the learning rate is 5e-5 with weight decay 1e-5. We apply a linear warm-up learning rate schedul"
2021.acl-long.19,doddington-etal-2004-automatic,0,0.124527,"spot that relation label in the same way). In other words, if the adjacent rows/columns are different, there must be an entity boundary (i.e., one belonging to the entity and the other not belonging to the entity). Therefore, if our biaffine model is reasonably trained, given a model predicted table, we could use this property to find split positions of entity boundary. As expected, experiments (Figure 4) verify our assumption. We adapt this idea to the 3-dimensional probability tensor P. 224 4 Experiments Datasets We conduct experiments on three entity relation extraction benchmarks: ACE04 (Doddington et al., 2004),9 ACE05 (Walker et al., 2006),10 and SciERC (Luan et al., 2018).11 Table 2 shows the dataset statistics. Besides, we provide detailed dataset specifications in the Appendix B. Evaluation Following suggestions in (Taill´e et al., 2020), we evaluate Precision (P), Recall (R), and F1 scores with micro-averaging and adopt the Strict Evaluation criterion. Specifically, a predicted entity is correct if its type and boundaries are correct, and a predicted relation is correct if its 8 i and j denote start and end indices of the span. https://catalog.ldc.upenn.edu/LDC2005T09 10 https://catalog.ldc.upe"
2021.acl-long.19,P81-1022,0,0.584483,"Missing"
2021.acl-long.19,C16-1239,0,0.0333364,"Missing"
2021.acl-long.19,P17-1085,0,0.0146782,"can be roughly divided into two categories according to the adopted label space. Separate Label Spaces This category study this task as two separate sub-tasks: entity recognition and relation classification, which are defined in two separate label spaces. One early paradigm is the pipeline method (Zelenko et al., 2003; Miwa et al., 2009) that uses two independent models for two sub-tasks respectively. Then joint method handles this task with an end-to-end model to explore more interaction between entities and relations. The most basic joint paradigm, parameter sharing (Miwa and Bansal, 2016; Katiyar and Cardie, 2017), adopts two independent decoders based on a shared encoder. Recent span-based models (Luan et al., 2019b; Wadden et al., 2019) also use this paradigm. To enhance the connection of two decoders, many joint decoding algorithms are proposed, such as ILP-based joint decoder (Yang and Cardie, 2013), joint MRT (Sun et al., 2018), GCN-based joint inference (Sun et al., 2019). Actually, table filling method (Miwa and Sasaki, 2014; Gupta et al., 2016; Zhang et al., 2017; Wang et al., 2020) is a special case of parameter sharing in table structure. These joint models all focus on various joint algorith"
2021.acl-long.19,P14-1038,0,0.0845167,"Missing"
2021.acl-long.19,P19-1129,0,0.0134684,"NI RE hard decoding Table 5: Comparison of accuracy and efficiency on ACE05 and SciERC test sets with different context window sizes. † denotes the approximation version with a faster speed and a worse performance. Table 4: Results (F1 score) with different settings on ACE05 and SciERC test sets. Note that we use BERTBASE on ACE05. achieves better performance. Besides, our model can achieve better relation performance even with worse entity results on ACE04. Actually, our base model (BERTBASE ) has achieved competitive relation performance, which even exceeds prior models based on BERTLARGE (Li et al., 2019) and ALBERTXXLARGE (Wang and Lu, 2020). These results confirm the proposed unified label space is effective for exploring the interaction between entities and relations. Note that all subsequent experiment results on ACE04 and ACE05 are based on BERTBASE for efficiency. 4.2 ACE05 Rel Speed (F1) (sent/s) Parameters Ablation Study In this section, we analyze the effects of components in U NI RE with different settings (Table 4). Particularly, we implement a naive decoding algorithm for comparison, namely “hard decoding”, which takes the “intermediate table” as input. The “intermediate table” is"
2021.acl-long.19,D18-1360,0,0.018109,"acent rows/columns are different, there must be an entity boundary (i.e., one belonging to the entity and the other not belonging to the entity). Therefore, if our biaffine model is reasonably trained, given a model predicted table, we could use this property to find split positions of entity boundary. As expected, experiments (Figure 4) verify our assumption. We adapt this idea to the 3-dimensional probability tensor P. 224 4 Experiments Datasets We conduct experiments on three entity relation extraction benchmarks: ACE04 (Doddington et al., 2004),9 ACE05 (Walker et al., 2006),10 and SciERC (Luan et al., 2018).11 Table 2 shows the dataset statistics. Besides, we provide detailed dataset specifications in the Appendix B. Evaluation Following suggestions in (Taill´e et al., 2020), we evaluate Precision (P), Recall (R), and F1 scores with micro-averaging and adopt the Strict Evaluation criterion. Specifically, a predicted entity is correct if its type and boundaries are correct, and a predicted relation is correct if its 8 i and j denote start and end indices of the span. https://catalog.ldc.upenn.edu/LDC2005T09 10 https://catalog.ldc.upenn.edu/LDC2006T06 11 http://nlp.cs.washington.edu/sciIE/ 9 Datas"
2021.acl-long.19,N19-1308,0,0.27982,"4 and +1.7 for relation, on ACE04 and ACE05 respectively. For the best pipeline model (Zhong and Chen, 2020) (current SOTA), our model achieves superior performance on ACE04 and SciERC and comparable performance on ACE05. Comparing with ACE04/ACE05, SciERC is much smaller, so entity performance on SciERC drops sharply. Since (Zhong and Chen, 2020) is a pipeline method, its relation performance is severely influenced by the poor entity performance. Nevertheless, our model is less influenced in this case and 12 The first two are for ACE04 and ACE05, and the last one is for SciERC. 225 13 Since (Luan et al., 2019a; Wadden et al., 2019) neglect the argument entity type in relation evaluation and underperform our baseline (Zhang et al., 2020), we do not compare their results here. Settings ACE05 Ent Rel SciERC Ent Rel Model Default 88.8 64.3 68.4 36.9 w/o symmetry loss w/o implication loss w/o logit dropout w/o cross-sentence context 88.9 89.0 88.8 87.9 64.0 63.3 61.8 62.7 67.3 68.0 66.9 65.3 35.5 37.1 34.7 32.1 hard decoding 74.0 34.6 46.1 17.8 SciERC Rel Speed (F1) (sent/s) 100 100 64.6 - 14.7 237.6 36.7 - 19.9 194.7 110M 110M 100 200 63.6 64.3 340.6 194.2 34.0 36.9 314.8 200.1 110M 200 34.6 139.1 17."
2021.acl-long.19,P16-1105,0,0.140446,"asting research topic in NLP. Typically, it aims to recognize specific entities and relations for profiling the semantic of sentences. An example is shown in Figure 1, where a person entity “David Perkins” and a geography entity “California” have a physical location relation PHYS. Methods for detecting entities and relations can be categorized into pipeline models or joint models. In the pipeline setting, entity models and relation models are independent with disentangled feature spaces and output label spaces. In the joint setting, on the other hand, some parameter sharing of feature spaces (Miwa and Bansal, 2016; Katiyar and and David in Introduction ORG-AFF Figure 1: Example of a table for joint entity relation extraction. Each cell corresponds to a word pair. Entities are squares on diagonal, relations are rectangles off diagonal. Note that PER-SOC is a undirected (symmetrical) relation type, while PHYS and ORG-AFF are directed (asymmetrical) relation types. The table exactly expresses overlapped relations, e.g., the person entity “David Perkins” participates in two relations, (“David Perkins”, “wife”, PER-SOC) and (“David Perkins”, “California”, PHYS). For every cell, a same biaffine model predict"
2021.acl-long.19,D09-1013,0,0.0285477,"ility for each cell). “Decoded Table” presents the final results after decoding. Figure 6: Distribution of five relation extraction errors on ACE05 and SciERC test data. 5 it Related Work Entity relation extraction has been extensively studied over the decades. Existing methods can be roughly divided into two categories according to the adopted label space. Separate Label Spaces This category study this task as two separate sub-tasks: entity recognition and relation classification, which are defined in two separate label spaces. One early paradigm is the pipeline method (Zelenko et al., 2003; Miwa et al., 2009) that uses two independent models for two sub-tasks respectively. Then joint method handles this task with an end-to-end model to explore more interaction between entities and relations. The most basic joint paradigm, parameter sharing (Miwa and Bansal, 2016; Katiyar and Cardie, 2017), adopts two independent decoders based on a shared encoder. Recent span-based models (Luan et al., 2019b; Wadden et al., 2019) also use this paradigm. To enhance the connection of two decoders, many joint decoding algorithms are proposed, such as ILP-based joint decoder (Yang and Cardie, 2013), joint MRT (Sun et"
2021.acl-long.19,D14-1200,0,0.0952005,"input space is a two-dimensional table with each entry corresponding to a word pair in sentences (Figure 1). The joint model assign labels to each cell from a unified label space (union of entity type set and relation type set). Graphically, entities are squares on the diagonal, and relations are rectangles off the diagonal. This formulation retains full model expressiveness regarding existing entity-relation extraction scenarios (e.g., overlapped relations, directed relations, undirected relations). It is also different from the current table filling settings for entity relation extraction (Miwa and Sasaki, 2014; Gupta et al., 2016; Zhang et al., 2017; Wang and Lu, 2020), which still have separate label space for entities and relations, and treat on/off-diagonal entries differently. Based on the tabular formulation, our joint entity relation extractor performs two actions, filling and decoding. First, filling the table is to predict each word pair’s label, which is similar to arc prediction task in dependency parsing. We adopt the biaffine attention mechanism (Dozat and Manning, 2016) to learn interactions between word pairs. We also impose two structural constraints on the table through structural r"
2021.acl-long.19,P19-1131,1,0.956818,"l, relations are rectangles off diagonal. Note that PER-SOC is a undirected (symmetrical) relation type, while PHYS and ORG-AFF are directed (asymmetrical) relation types. The table exactly expresses overlapped relations, e.g., the person entity “David Perkins” participates in two relations, (“David Perkins”, “wife”, PER-SOC) and (“David Perkins”, “California”, PHYS). For every cell, a same biaffine model predicts its label. The joint decoder is set to find the best squares and rectangles. Equal contribution. Corresponding Author. Cardie, 2017) or decoding interactions (Yang and Cardie, 2013; Sun et al., 2019) are imposed to explore the common structure of the two tasks. It was believed that joint models could be better since they can alleviate error propagations among sub-models, have more compact parameter sets, and uniformly encode prior knowledge (e.g., constraints) on both tasks. However, Zhong and Chen (2020) recently show 220 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 220–231 August 1–6, 2021. ©2021 Association for Computational Linguistics that with the help of mode"
2021.acl-long.19,D18-1249,1,0.848912,", 2009) that uses two independent models for two sub-tasks respectively. Then joint method handles this task with an end-to-end model to explore more interaction between entities and relations. The most basic joint paradigm, parameter sharing (Miwa and Bansal, 2016; Katiyar and Cardie, 2017), adopts two independent decoders based on a shared encoder. Recent span-based models (Luan et al., 2019b; Wadden et al., 2019) also use this paradigm. To enhance the connection of two decoders, many joint decoding algorithms are proposed, such as ILP-based joint decoder (Yang and Cardie, 2013), joint MRT (Sun et al., 2018), GCN-based joint inference (Sun et al., 2019). Actually, table filling method (Miwa and Sasaki, 2014; Gupta et al., 2016; Zhang et al., 2017; Wang et al., 2020) is a special case of parameter sharing in table structure. These joint models all focus on various joint algorithms but ignore the fact that they are essentially based on separate label spaces. Unified Label Space This family of methods aims to unify two sub-tasks and tackle this task in a unified label space. Entity relation extraction has been converted into a tagging problem (Zheng et al., 2017), a transition-based parsing problem"
2021.acl-long.19,2020.emnlp-main.301,0,0.0348268,"Missing"
2021.acl-long.19,D17-1182,0,0.0334741,"Missing"
2021.acl-long.19,D19-1585,0,0.0713147,"ion, on ACE04 and ACE05 respectively. For the best pipeline model (Zhong and Chen, 2020) (current SOTA), our model achieves superior performance on ACE04 and SciERC and comparable performance on ACE05. Comparing with ACE04/ACE05, SciERC is much smaller, so entity performance on SciERC drops sharply. Since (Zhong and Chen, 2020) is a pipeline method, its relation performance is severely influenced by the poor entity performance. Nevertheless, our model is less influenced in this case and 12 The first two are for ACE04 and ACE05, and the last one is for SciERC. 225 13 Since (Luan et al., 2019a; Wadden et al., 2019) neglect the argument entity type in relation evaluation and underperform our baseline (Zhang et al., 2020), we do not compare their results here. Settings ACE05 Ent Rel SciERC Ent Rel Model Default 88.8 64.3 68.4 36.9 w/o symmetry loss w/o implication loss w/o logit dropout w/o cross-sentence context 88.9 89.0 88.8 87.9 64.0 63.3 61.8 62.7 67.3 68.0 66.9 65.3 35.5 37.1 34.7 32.1 hard decoding 74.0 34.6 46.1 17.8 SciERC Rel Speed (F1) (sent/s) 100 100 64.6 - 14.7 237.6 36.7 - 19.9 194.7 110M 110M 100 200 63.6 64.3 340.6 194.2 34.0 36.9 314.8 200.1 110M 200 34.6 139.1 17.8 113.0 W Z&C(2020) Z&C"
2021.acl-long.19,2020.emnlp-main.133,0,0.354126,"onding to a word pair in sentences (Figure 1). The joint model assign labels to each cell from a unified label space (union of entity type set and relation type set). Graphically, entities are squares on the diagonal, and relations are rectangles off the diagonal. This formulation retains full model expressiveness regarding existing entity-relation extraction scenarios (e.g., overlapped relations, directed relations, undirected relations). It is also different from the current table filling settings for entity relation extraction (Miwa and Sasaki, 2014; Gupta et al., 2016; Zhang et al., 2017; Wang and Lu, 2020), which still have separate label space for entities and relations, and treat on/off-diagonal entries differently. Based on the tabular formulation, our joint entity relation extractor performs two actions, filling and decoding. First, filling the table is to predict each word pair’s label, which is similar to arc prediction task in dependency parsing. We adopt the biaffine attention mechanism (Dozat and Manning, 2016) to learn interactions between word pairs. We also impose two structural constraints on the table through structural regularizations. Next, given the table filling with label log"
2021.acl-long.19,2020.emnlp-main.132,1,0.759126,"n between entities and relations. The most basic joint paradigm, parameter sharing (Miwa and Bansal, 2016; Katiyar and Cardie, 2017), adopts two independent decoders based on a shared encoder. Recent span-based models (Luan et al., 2019b; Wadden et al., 2019) also use this paradigm. To enhance the connection of two decoders, many joint decoding algorithms are proposed, such as ILP-based joint decoder (Yang and Cardie, 2013), joint MRT (Sun et al., 2018), GCN-based joint inference (Sun et al., 2019). Actually, table filling method (Miwa and Sasaki, 2014; Gupta et al., 2016; Zhang et al., 2017; Wang et al., 2020) is a special case of parameter sharing in table structure. These joint models all focus on various joint algorithms but ignore the fact that they are essentially based on separate label spaces. Unified Label Space This family of methods aims to unify two sub-tasks and tackle this task in a unified label space. Entity relation extraction has been converted into a tagging problem (Zheng et al., 2017), a transition-based parsing problem (Wang et al., 2018), and a generation problem with Seq2Seq framework (Zeng et al., 2018; Nayak and Ng, 2020). We follow this trend and propose a new unified labe"
2021.acl-long.19,P13-1161,0,0.167846,"are squares on diagonal, relations are rectangles off diagonal. Note that PER-SOC is a undirected (symmetrical) relation type, while PHYS and ORG-AFF are directed (asymmetrical) relation types. The table exactly expresses overlapped relations, e.g., the person entity “David Perkins” participates in two relations, (“David Perkins”, “wife”, PER-SOC) and (“David Perkins”, “California”, PHYS). For every cell, a same biaffine model predicts its label. The joint decoder is set to find the best squares and rectangles. Equal contribution. Corresponding Author. Cardie, 2017) or decoding interactions (Yang and Cardie, 2013; Sun et al., 2019) are imposed to explore the common structure of the two tasks. It was believed that joint models could be better since they can alleviate error propagations among sub-models, have more compact parameter sets, and uniformly encode prior knowledge (e.g., constraints) on both tasks. However, Zhong and Chen (2020) recently show 220 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 220–231 August 1–6, 2021. ©2021 Association for Computational Linguistics that wi"
2021.acl-long.19,P18-1047,0,0.0120585,"g method (Miwa and Sasaki, 2014; Gupta et al., 2016; Zhang et al., 2017; Wang et al., 2020) is a special case of parameter sharing in table structure. These joint models all focus on various joint algorithms but ignore the fact that they are essentially based on separate label spaces. Unified Label Space This family of methods aims to unify two sub-tasks and tackle this task in a unified label space. Entity relation extraction has been converted into a tagging problem (Zheng et al., 2017), a transition-based parsing problem (Wang et al., 2018), and a generation problem with Seq2Seq framework (Zeng et al., 2018; Nayak and Ng, 2020). We follow this trend and propose a new unified label space. We introduce a 2D table to tackle the overlapping relation problem in (Zheng et al., 2017). Also, our model is more versatile as not relying on complex expertise like (Wang et al., 2018), which requires external expert knowledge to design a complex transition system. 6 Conclusion In this work, we extract entities and relations in a unified label space to better mine the interaction between both sub-tasks. We propose a novel table that presents entities and relations as squares and rectangles. Then this task can"
2021.acl-long.19,P17-1113,0,0.110478,"ty models and relation models share encoders, usually their label spaces are still separate (even in models with joint decoders). Therefore, parallel to (Zhong and Chen, 2020), we would ask whether joint encoders (decoders) deserve joint label spaces? The challenge of developing a unified entityrelation label space is that the two sub-tasks are usually formulated into different learning problems (e.g., entity detection as sequence labeling, relation classification as multi-class classification), and their labels are placed on different things (e.g., words v.s. words pairs). One prior attempt (Zheng et al., 2017) is to handle both sub-tasks with one sequence labeling model. A compound label set was devised to encode both entities and relations. However, the model’s expressiveness is sacrificed: it can detect neither overlapping relations (i.e., entities participating in multiple relation) nor isolated entities (i.e., entities not appearing in any relation). Our key idea of defining a new unified label space is that, if we think Zheng et al. (2017)’s solution is to perform relation classification during entity labeling, we could also consider the reverse direction by seeing entity detection as a specia"
2021.acl-long.21,L18-1144,0,0.10998,"Missing"
2021.acl-long.21,N19-1121,0,0.0584207,"Missing"
2021.acl-long.21,D19-5610,0,0.0525408,"Missing"
2021.acl-long.21,Q19-1038,0,0.0653772,"Missing"
2021.acl-long.21,D19-1165,0,0.0371356,"he introduction of noised bilingual and noised monolingual data for multilingual NMT. The above two Settings and Datasets Parallel Dataset PC32 We use the parallel dataset PC32 provided by Lin et al. (2020). It con5 They apply RAS only on parallel data xi is in language Li and xj is in language Lj , where i, j ∈ {L1 , . . . , LM } 7 We will release our synonym dictionary 6 4 Higher temperature increases the difficulty to distinguish positive sample from negative ones. 246 En-Fr wmt14 → ← bilingual Transformer-6(Lin et al., 2020) Transformer-12(Liu et al., 2020) pre-train & fine-tuned Adapter (Bapna and Firat, 2019) mBART(Liu et al., 2020) XLM(Conneau and Lample, 2019) MASS(Song et al., 2019) mRASP(Lin et al., 2020) unified multilingual Multi-Distillation (Tan et al., 2019) m-Transformer mRASP w/o finetune(**) mRASP2 En-Tr wmt17 → ← En-Es wmt13 → ← En-Ro wmt16 →(*) ← En-Fi wmt17 → ← Avg 43.2 41.4 39.8 - 9.5 12.2 33.2 - 34.3 34.3 34.0 36.8 20.2 21.8 - 41.1 44.3 45.4 17.8 20.0 22.5 23.4 35.4 34.0 - 33.7 - 37.7 37.6 38.8 38.5 39.1 38.9 22.4 24.0 28.5 28.0 - 42.0 43.1 43.5 38.1 39.2 39.3 18.8 20.0 21.4 23.1 25.2 25.8 32.8 34.0 34.5 33.7 34.3 35.0 31.6 35.9 37.5 38.0 35.8 37.7 38.8 39.1 22.0 20.0 22.0 23.4 21"
2021.acl-long.21,N19-1423,0,0.0700011,"Missing"
2021.acl-long.21,P17-1176,0,0.0498838,"Missing"
2021.acl-long.21,P15-1166,0,0.0535499,"Missing"
2021.acl-long.21,P19-1121,0,0.016476,"osed mRASP2. It takes a pair of parallel sentences (or augmented pseudo-pair) and computes normal cross entropy loss with a multi-lingual encoder-decoder. In addition, it computes contrastive loss on the representations of the aligned pair (positive example) and randomly selected non-aligned pair (negative example). 2017). Further, parameter sharing across different languages encourages knowledge transfer, which benefits low-resource translation directions and potentially enables zero-shot translation (i.e. direct translation between a language pair not seen during training) (Ha et al., 2017; Gu et al., 2019; Ji et al., 2020). Despite these benefits, challenges still remain in multilingual NMT. First, previous work on multilingual NMT does not always perform well as their corresponding bilingual baseline especially on rich resource language pairs (Tan et al., 2019; Zhang et al., 2020; Fan et al., 2020). Such performance gap becomes larger with the increasing number of accommodated languages for multilingual NMT, as model capacity necessarily must be split between many languages (Arivazhagan et al., 2019). In addition, an optimal setting for multilingual NMT should be effective for any language pa"
2021.acl-long.21,2020.tacl-1.47,0,0.025868,"0 directions (See Appendix) and in this table we pick the representative ones. Different from our work, final BLEU scores of mBART, XLM, MASS and mRASP are obtained by multilingual pre-training and finetuning on a single direction. Adapter is a trade-off between unified multilingual model and bilingual model (trained on 6 languages on WMT data). Multi-Distillation is improved over Adapter with selective distillation methods. Results for Transformer-6 (6 layers for encoder and decoder) are from Lin et al. (2020). Results for Transformer12 (12 layers for encoder and decoder separately) are from Liu et al. (2020). (*) Note that for En→Ro direction, we follow the previous setting to calculate BLEU score after removing Romanian dialects. (**) For mRASP w/o finetune we report the results implemented by ourselves, with 12 layers encoder and decoder and our data. Both m-Transformer and our mRASP2 have 12 layers for encoder and decoder. data. The total number of sentences in MC24 is 1.01 billion. The detail of data volume is listed in the Appendix. We apply AA on MC24 by randomly replacing words in the source side sentences with synonyms from a multilingual dictionary. Therefore the source side might contai"
2021.acl-long.21,W18-6319,0,0.0157097,"5 Nl(*) X→Nl Nl→X 2.2 6.0 2.3 6.3 5.3 6.1 Fr→X 22.3 4.8 21.7 De X→De De→X 14.4 14.2 4.2 4.8 12.3 15.0 Ru X→Ru Ru→X 16.6 19.9 5.7 4.8 16.4 19.1 Fr Avg of all 15.56 5.05 15.31 Table 3: Zero-Shot: We report de-tokenized BLEU using sacreBLEU in OPUS-100. We observe consistent BLEU gains in zero-shot directions on different evaluation sets, see Appendix for more details. mRASP2 further improves the quality. We also list BLEU of pivot-based model (X→En then En→Y using m-Transformer) as a reference, mRASP2 only lags behind Pivot by -0.25 BLEU. (*) Note that Dutch(Nl) is not included in PC32. 4 BLEU (Post, 2018). For tokenized BLEU, we tokenize both reference and hypothesis using Sacremoses11 toolkit then report BLEU using the multi-bleu.pl script12 . For Chinese (Zh), BLEU score is calculated on character-level. Experiment Results This section shows that mRASP2 provides consistent performance gains for supervised and unsupervised English-centric translation directions as well as for non-English directions. Experiment Details We use the Transformer model in our experiments, with 12 encoder layers and 12 decoder layers. The embedding size and FFN dimension are set to 1024. We use dropout = 0.1, as wel"
2021.acl-long.21,P16-1162,0,0.0347259,"ent Details We use the Transformer model in our experiments, with 12 encoder layers and 12 decoder layers. The embedding size and FFN dimension are set to 1024. We use dropout = 0.1, as well as a learning rate of 3e-4 with polynomial decay scheduling and a warm-up step of 10000. For optimization, we use Adam optimizer (Kingma and Ba, 2015) with  = 1e-6 and β2 = 0.98. To stabilize training, we set the threshold of gradient norm to be 5.0 and clip all gradients with a larger norm. We set the hyper-parameter λ = 1.0 in Eq.3 during training. For multilingual vocabulary, we follow the shared BPE (Sennrich et al., 2016) vocabulary of Lin et al. (2020), which includes 59 languages. The vocabulary contains 64808 tokens. After adding 59 language tokens, the total size of vocabulary is 64867. 4.1 English-Centric Directions Supervised Directions As shown in Table 1, mRASP2 clearly improves multilingual baselines by a large margin in 10 translation directions. Previously, multilingual machine translation underperforms bilingual translation in rich-resource scenarios. It is worth noting that our multilingual machine translation baseline is already very competitive. It is even on par with the strong mBART bilingual"
2021.acl-long.21,2020.acl-main.252,0,0.0186296,"guages for multilingual NMT, as model capacity necessarily must be split between many languages (Arivazhagan et al., 2019). In addition, an optimal setting for multilingual NMT should be effective for any language pairs, while most previous work focus on improvIntroduction Transformer (Vaswani et al., 2017) has achieved decent performance for machine translation with rich bilingual parallel corpora. Recent work on multilingual machine translation aims to create a single unified model to translate many languages (Johnson et al., 2017; Aharoni et al., 2019; Zhang et al., 2020; Fan et al., 2020; Siddhant et al., 2020). Multilingual translation models are appealing for two reasons. First, they are model efficient, enabling easier deployment (Johnson et al., 244 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 244–258 August 1–6, 2021. ©2021 Association for Computational Linguistics ing English-centric1 directions (Johnson et al., 2017; Aharoni et al., 2019; Zhang et al., 2020). A few recent exceptions are Zhang et al. (2020) and Fan et al. (2020), who trained many-to-many systems with int"
2021.acl-long.21,P19-1120,0,0.0409191,"Missing"
2021.acl-long.21,P19-1117,0,0.0639978,"o translate from one language to another. To distinguish different languages, we add an additional language identification token preceding each sentence, for both source side and target side. The base architecture of mRASP2 is the state-of-theart Transformer (Vaswani et al., 2017). A little different from previous work, we choose a larger setting with a 12-layer encoder and a 12-layer decoder to increase the model capacity. The model dimension is 1024 on 16 heads. To ease the training of the deep model, we apply Layer Normalization for word embedding and pre-norm residual connection following Wang et al. (2019a) for both encoder and decoder. Therefore, our multilingual NMT baseline is much stronger than that of Transformer big model. More formally, we define L = {L1 , . . . , LM } where L is a collection of M languages involving in the training phase. Di,j denotes a parallel dataset of (Li , Lj ), and D denotes all parallel datasets. The training loss is cross entropy defined as: X Lce = − log Pθ (xi |xj ) (1) As such, many-to-many translations can make the most of the knowledge from all supervised directions and the model can perform well for both English-centric and non-English settings. In this"
2021.acl-long.21,2020.acl-main.148,0,0.523329,"ed non-aligned pair (negative example). 2017). Further, parameter sharing across different languages encourages knowledge transfer, which benefits low-resource translation directions and potentially enables zero-shot translation (i.e. direct translation between a language pair not seen during training) (Ha et al., 2017; Gu et al., 2019; Ji et al., 2020). Despite these benefits, challenges still remain in multilingual NMT. First, previous work on multilingual NMT does not always perform well as their corresponding bilingual baseline especially on rich resource language pairs (Tan et al., 2019; Zhang et al., 2020; Fan et al., 2020). Such performance gap becomes larger with the increasing number of accommodated languages for multilingual NMT, as model capacity necessarily must be split between many languages (Arivazhagan et al., 2019). In addition, an optimal setting for multilingual NMT should be effective for any language pairs, while most previous work focus on improvIntroduction Transformer (Vaswani et al., 2017) has achieved decent performance for machine translation with rich bilingual parallel corpora. Recent work on multilingual machine translation aims to create a single unified model to trans"
2021.acl-long.25,P15-1166,0,0.0237718,"of existing language pairs. Besides, LaSS can boost zero-shot translation by up to 26.5 BLEU. 2 Related Work Multilingual Neural Machine Translation The standard multilingual NMT model uses a shared encoder and a shared decoder for different languages (Johnson et al., 2017). There is a transfer-interference trade-off in this architecture (Arivazhagan et al., 2019): boosting the performance of low resource languages or maintain the performance of high resource languages. To solve this trade-off, previous works assign some parts of the model to be language specific: Language specific decoders (Dong et al., 2015), Language specific encoders and decoders (Firat et al., 2016; Lyu et al., 2020) and Language specific hidden states and embeds (Wang et al., 2018). Sachan and Neubig (2018) compares different sharing methods and finds different sharing methods have a great impact on performance. Recently, Zhang et al. (2021) analyze when and where language specific capacity matters. Li et al. (2020) uses a binary conditional latent variable to decide which language each layer belongs to. Model Pruning Our approach follows the standard pattern of model pruning: training, finding the sparse network and fine-tun"
2021.acl-long.25,1983.tc-1.13,0,0.275744,"Missing"
2021.acl-long.25,N16-1101,0,0.0195815,"t translation by up to 26.5 BLEU. 2 Related Work Multilingual Neural Machine Translation The standard multilingual NMT model uses a shared encoder and a shared decoder for different languages (Johnson et al., 2017). There is a transfer-interference trade-off in this architecture (Arivazhagan et al., 2019): boosting the performance of low resource languages or maintain the performance of high resource languages. To solve this trade-off, previous works assign some parts of the model to be language specific: Language specific decoders (Dong et al., 2015), Language specific encoders and decoders (Firat et al., 2016; Lyu et al., 2020) and Language specific hidden states and embeds (Wang et al., 2018). Sachan and Neubig (2018) compares different sharing methods and finds different sharing methods have a great impact on performance. Recently, Zhang et al. (2021) analyze when and where language specific capacity matters. Li et al. (2020) uses a binary conditional latent variable to decide which language each layer belongs to. Model Pruning Our approach follows the standard pattern of model pruning: training, finding the sparse network and fine-tuning (Frankle and Carbin, 2019; Liu et al., 2019). Frankle and"
2021.acl-long.25,D19-1074,1,0.833073,"ailable at https: //github.com/NLP-Playground/LaSS. 1 (a) Full network En En En Zh Fr De (b) LaSS Figure 1: Illustration of a full network and languagespecific ones (LaSS). — represents shared weights. — , — and — represents weights for En→Zh, En→Fr and En→De, respectively. Compared to the full multilingual model, each LaSS learned model has language universal and language specific weights. Introduction Neural machine translation (NMT) has been very successful for bilingual machine translation (Bahdanau et al., 2015; Vaswani et al., 2017; Wu et al., 2016; Hassan et al., 2018; Su et al., 2018; Wang, 2019). Recent research has demonstrated the efficacy of multilingual NMT, which supports translation from multiple source languages into multiple target languages with a single model (Johnson et al., 2017; Aharoni et al., 2019; Zhang et al., 2020; Fan et al., 2020; Siddhant et al., 2020). Multilingual NMT enjoys the advantage of deployment. Further, the parameter sharing of multilingual NMT encourages transfer learning of different languages. An extreme case is zero-shot translation, where direct translation between a language pair never seen in training is possible (Johnson et al., 2017). ∗ Zh Fr"
2021.acl-long.25,P19-1176,0,0.0512669,"Missing"
2021.acl-long.25,D18-1326,0,0.073559,"019). Therefore, multilingual NMT models often suffer from performance degradation compared with their corresponding bilingual baseline, especially for rich-resource translation directions. The simplistic way to alleviate the insufficient model capacity is to enlarge the model parameters (Aharoni et al., 2019; Zhang et al., 2020). However, it is not parameter or computation efficient and needs larger multilingual training datasets to avoid over-fitting. An alternative solution is to design language-aware components, such as division of the hidden cells into shared and language-dependent ones (Wang et al., 2018), adaptation layers (Bapna and Firat, 2019; Philip et al., 2020), language-aware layer normalization 293 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 293–305 August 1–6, 2021. ©2021 Association for Computational Linguistics and linear transformation (Zhang et al., 2020), and latent layers (Li et al., 2020). In this work, we propose LaSS, a method to dynamically find and learn Language Specific Subnetwork for multilingual NMT. LaSS accommodates one sub-network for each la"
2021.acl-long.25,2020.acl-main.148,0,0.425914,"or En→Zh, En→Fr and En→De, respectively. Compared to the full multilingual model, each LaSS learned model has language universal and language specific weights. Introduction Neural machine translation (NMT) has been very successful for bilingual machine translation (Bahdanau et al., 2015; Vaswani et al., 2017; Wu et al., 2016; Hassan et al., 2018; Su et al., 2018; Wang, 2019). Recent research has demonstrated the efficacy of multilingual NMT, which supports translation from multiple source languages into multiple target languages with a single model (Johnson et al., 2017; Aharoni et al., 2019; Zhang et al., 2020; Fan et al., 2020; Siddhant et al., 2020). Multilingual NMT enjoys the advantage of deployment. Further, the parameter sharing of multilingual NMT encourages transfer learning of different languages. An extreme case is zero-shot translation, where direct translation between a language pair never seen in training is possible (Johnson et al., 2017). ∗ Zh Fr De Equal contribution. While very promising, several challenges remain in multilingual NMT. The most challenging one is related to the insufficient model capacity. Since multiple languages are accommodated in a single model, the modeling cap"
2021.acl-long.274,D19-1582,0,0.154472,"where we simply use string matching to detect entity coreference following Zheng et al. (2019) , and the entity embedding Ei is computed by the average of its mention node embedding, Ei = Mean({hj }j∈Mention(i) ). In this way, the sentences and entities are interactively represented in a context-aware way. 3.3 Virtual Node Event Types Detection Since a document can express events of different types, we formulate the task as a multi-label classification and leverage sentences feature matrix S to * Traditional methods in sentence-level EE also utilize graph to extract events (Liu et al., 2018; Yan et al., 2019), based on the dependency tree. However, our interaction graph is heterogeneous and have no demands for dependency tree. detect event types: A = MultiHead(Q, S, S) ∈ Rdm ×T R = Sigmoid(A&gt; Wt ) ∈ RT where Q ∈ Rdm ×T and Wt ∈ Rdm are trainable parameters, and T denotes the number of possible event types. MultiHead refers to the standard multi-head attention mechanism with Query/Key/Value. Therefore, we derive the event b ∈ RT : types detection loss with golden label R Ldetect = − T   X bt = 1 log P (Rt |D) I R t=1 (2)   bt = 0 log (1 − P (Rt |D)) +I R 3.4 Event Records Extraction Since a doc"
2021.acl-long.274,N16-1033,0,0.027015,"e-trained language model (Yang et al., 2019), and explicit external knowledge (Liu et al., 2019a; Tong et al., 2020) such as WordNet (Miller, 1995). Du and Cardie (2020b) also try to extract events in a Question-Answer way. These studies usually conduct experiments on sentencelevel event extraction dataset, ACE05 (Walker et al., 2006). However, it is hard for the sentence-level models to extract multiple qualified events spanning across sentences, which is more common in real-world scenarios. 4.6 Document-level Event Extraction. Documentlevel EE has attracted more and more attention recently. Yang and Mitchell (2016) use well-defined features to handle the event-argument relations across sentences, which is, unfortunately, quite nontrivial. Yang et al. (2018) extract events from a central sentence and find other arguments from neighboring sentences separately. Although Zheng et al. (2019) use Transformer to fuse sentences and entities, interdependency among events is neglected. Du and Cardie (2020a) try to encode the sentences in a multi-granularity way and Du et al. (2020) leverage a seq2seq model. They conduct experiments on MUC-4 (Sundheim, 1992) dataset with 1, 700 documents and 5 kinds of entity-base"
2021.acl-long.274,P18-4009,0,0.360185,"n (EE) is one of the key and challenging tasks in Information Extraction (IE), which aims to detect events and extract their arguments from the text. Most previous methods (Chen et al., 2015; Nguyen et al., 2016; Liu et al., 2018; Yang et al., 2019; Du and Cardie, 2020b) focus on sentence-level EE, extracting events from a single sentence. The sentence-level model, however, fails to extract events whose arguments spread in multiple sentences, which is much more common in real-world scenarios. Hence, extracting events at the document-level is critical. It has attracted much attention recently (Yang et al., 2018; Zheng et al., 2019; Du and Cardie, 2020a; Du et al., 2020). author. 7.2 million Nov 6, 2014 … Figure 1: An example document from a Chinese dataset proposed by Zheng et al. (2019) in the financial domain, and we translate it into English for illustration. Entity mentions are colored. Due to space limitation, we only show four associated sentences and three argument roles of each event type. The complete original document can be found in Appendix C. EU: Equity Underweight, EO: Equity Overweight. Introduction * Corresponding Xiaoting Wu Though promising, document-level EE still faces two critic"
2021.acl-long.274,P19-1522,0,0.123575,"19) show G IT outperforms the existing best methods by 2.8 F1. Further analysis reveals G IT is effective in extracting multiple correlated events and event arguments that scatter across the document. Our code is available at https: //github.com/RunxinXu/GIT. 1 EventType EquityHolder TradedShares StartDate … Mingting Wu 7.2 million Nov 6, 2014 … EU EO Event Extraction (EE) is one of the key and challenging tasks in Information Extraction (IE), which aims to detect events and extract their arguments from the text. Most previous methods (Chen et al., 2015; Nguyen et al., 2016; Liu et al., 2018; Yang et al., 2019; Du and Cardie, 2020b) focus on sentence-level EE, extracting events from a single sentence. The sentence-level model, however, fails to extract events whose arguments spread in multiple sentences, which is much more common in real-world scenarios. Hence, extracting events at the document-level is critical. It has attracted much attention recently (Yang et al., 2018; Zheng et al., 2019; Du and Cardie, 2020a; Du et al., 2020). author. 7.2 million Nov 6, 2014 … Figure 1: An example document from a Chinese dataset proposed by Zheng et al. (2019) in the financial domain, and we translate it into"
2021.acl-long.274,2020.emnlp-main.127,1,0.593902,"ther by M-Minter edges. As in document EE, an entity usually corresponds to multiple mentions across sentences, we thus use M-Minter edge to track all the appearances of a specific entity, which facilitates the long distance event extraction from a global perspective. In Section. 4.5, experiments show that all of these four kinds of edges play an important role in event detection, and the performance would decrease without any of them. After heterogeneous graph construction * , we apply multi-layer Graph Convolution Network (Kipf and Welling, 2017) to model the global interactions inspired by Zeng et al. (2020). Given node u at the l-th layer, the graph convolutional operation is defined as follows:   X X 1 (l) h(l+1) = ReLU  W h(l)  u cu,k k v S k∈K v∈Nk (u) A EquityFreeze B Pledger C E H D F I C G J Tracker Pledgee StartDate A … Virtual Node EquityPledge B E F A C E A D F H I B C G J A A B K Global Memory … K A Completed Uncompleted Figure 3: The decoding module of G IT. Three Equity Freeze records have been extracted completely, and G IT is predicting the StartDate role for the Equity Pledge records (in the dashed frame ), based on the global memory where Tracker tracks the records on-the-fly"
2021.acl-long.383,N19-1423,0,0.533152,"y have different characteristics (e.g., fashionable vs. comfortable). The goal of explainable recommendation (Zhang and Chen, 2020) is to provide an explanation to a user for a recommended item, so as to justify how the recommendation might match his/her interests. That is, given a pair of user ID and item ID, the system needs to generate an explanation, such as “the style of the jacket is fashionable” (see the last column of Table 4 for more examples). Transformer (Vaswani et al., 2017), whose strong language modeling ability has been demonstrated on a variety of tasks (Radford et al., 2018; Devlin et al., 2019; Brown et al., 2020), however, is relatively under-explored for personalized natural language generation. Since IDs and words are in very different semantic spaces, it would be problematic to directly put them together for attention learning, because by doing so, the IDs are treated as words, but the IDs appear far less frequently than the words. For example, a paragraph of review (and thus hundreds of words) on e-commerce platform only corresponds to a single pair of user ID and item ID. As such, the IDs may be regarded as out-of-vocabulary tokens, to which the model is insensitive. As shown"
2021.acl-long.383,E17-1059,0,0.324326,"ch makes it a unified model for the whole recommendationexplanation pipeline. Extensive experiments show that our small unpretrained model outperforms fine-tuned BERT on the generation task, in terms of both effectiveness and efficiency, which highlights the importance and the nice utility of our design. 1 Introduction Recent years have witnessed the successful application of natural language generation. Many of the applications in fact require certain degree of personalization, such as explainable recommendation (Zhang et al., 2014; Li et al., 2020c; Zhang and Chen, 2020), review generation (Dong et al., 2017), review summarization (Li et al., 2019), and conversational systems (Zhang et al., 2018; Chen et al., 2020). In these tasks, user and item IDs that distinguish one user/item from the others are crucial to 1 https://github.com/lileipisces/PETER personalization. For example, in recommender systems, different users may care about different item features (e.g., style vs. quality), and different items may have different characteristics (e.g., fashionable vs. comfortable). The goal of explainable recommendation (Zhang and Chen, 2020) is to provide an explanation to a user for a recommended item, so"
2021.acl-long.383,D19-1018,0,0.0352857,"Missing"
2021.acl-long.383,P02-1040,0,0.114446,"er (Vaswani et al., 2017) performs the explanation generation task by treating user and item IDs as words. We also tested encoder-decoder Transformer, where the encoder encodes the IDs for the decoder to decode, but its results turned out to be the same, so we do not report it. Evaluation Metrics To evaluate the recommendation performance, we adopt two commonly used metrics: Root Mean Square Error (RMSE) and Mean Absolute Error (MAE). As to explanation performance, we measure the generated explanations from two main perspectives: text quality and explainability. For the former, we adopt BLEU (Papineni et al., 2002) in machine translation and ROUGE (Lin, 2004) in text summarization, and report BLEU-1 and BLEU4, and Precision, Recall and F1 of ROUGE-1 and ROUGE-2. Though being widely used, BLUE and ROUGE are not flawless. For example, it is difficult for them to detect the problem of identical sentences generated by Transformer. These identical sentences might not be used as explanations, because they are less likely to well explain the special property of different recommendations. To quantitatively measure how severe the problem is, we adopt USR that computes the Unique Sentence Ratio of generated sente"
2021.acl-long.383,2021.naacl-main.245,1,0.81494,"Missing"
2021.acl-long.571,N19-1352,0,0.0167783,"racter-level vocabularies, it has shorter sentence lengths without rare words. Following BPE, some variants recently have been proposed, like BPE-dropout (Provilkov et al., 2020), SentencePiece (Kudo and Richardson, 2018), and so on. Despite promising results, most existing subword approaches only consider frequency while the effects of vocabulary size is neglected. Thus, trial training is required to find the optimal size, which brings high computation costs. More recently, some studies notice this problem and propose some practical solutions (Kreutzer and Sokolov, 2018; Cherry et al., 2018; Chen et al., 2019; Salesky et al., 2020). 7362 defined by the sum of token entropy. To avoid the effects of token length, here we normalize entropy with the average length of tokens and the final entropy is defined as: 12 10 Count 8 6 4 Hv = − 2 0 0.75 0.50 0.25 0.00 0.25 0.50 0.75 1.00 Spearman Score Figure 2: MUV and downstream performance are positively correlated on two-thirds of tasks. X-axis classifies Spearman scores into different groups. Y-axis shows the number of tasks in each group. The middle Spearman score is 0.4. 3 Marginal Utility of Vocabularization In this section, we propose to find a good vo"
2021.acl-long.571,D18-1461,0,0.0149653,"ier”. Compared to character-level vocabularies, it has shorter sentence lengths without rare words. Following BPE, some variants recently have been proposed, like BPE-dropout (Provilkov et al., 2020), SentencePiece (Kudo and Richardson, 2018), and so on. Despite promising results, most existing subword approaches only consider frequency while the effects of vocabulary size is neglected. Thus, trial training is required to find the optimal size, which brings high computation costs. More recently, some studies notice this problem and propose some practical solutions (Kreutzer and Sokolov, 2018; Cherry et al., 2018; Chen et al., 2019; Salesky et al., 2020). 7362 defined by the sum of token entropy. To avoid the effects of token length, here we normalize entropy with the average length of tokens and the final entropy is defined as: 12 10 Count 8 6 4 Hv = − 2 0 0.75 0.50 0.25 0.00 0.25 0.50 0.75 1.00 Spearman Score Figure 2: MUV and downstream performance are positively correlated on two-thirds of tasks. X-axis classifies Spearman scores into different groups. Y-axis shows the number of tasks in each group. The middle Spearman score is 0.4. 3 Marginal Utility of Vocabularization In this section, we propos"
2021.acl-long.571,P16-2058,0,0.0608468,"Missing"
2021.acl-long.571,N19-1423,0,0.0413431,"Missing"
2021.acl-long.571,W19-6620,0,0.324234,"he search time from 384 GPU hours to 30 GPU hours on English-German translation. Codes are available at https: //github.com/Jingjing-NLP/VOLT. 1 Introduction Due to the discreteness of text, vocabulary construction ( vocabularization for short) is a prerequisite for neural machine translation (NMT) and many other natural language processing (NLP) tasks using neural networks (Mikolov et al., 2013; Vaswani et al., 2017; Gehrmann et al., 2018; Zhang et al., 2018; Devlin et al., 2019). Currently, sub-word approaches like Byte-Pair Encoding (BPE) are widely used in the community (Ott et al., 2018; Ding et al., 2019; Liu et al., 2020), and achieve quite promising results in practice (Sennrich et al., 2016; Costa-juss`a and Fonollosa, 2016; Lee et al., 2017; Kudo and Richardson, † Lab. This work is done during the internship at ByteDance AI 2018; Al-Rfou et al., 2019; Wang et al., 2020). The key idea of these approaches is selecting the most frequent sub-words (or word pieces with higher probabilities) as the vocabulary tokens. In information theory, these frequency-based approaches are simple forms of data compression to reduce entropy (Gage, 1994), which makes the resulting corpus easy to learn and pred"
2021.acl-long.571,D18-1443,0,0.0611372,"Missing"
2021.acl-long.571,D18-2012,0,0.0244114,"PU hours. 2 Related Work Initially, most neural models were built upon word-level vocabularies (Costa-juss`a and Fonollosa, 2016; Vaswani et al., 2017; Zhao et al., 2019). While achieving promising results, it is a common constraint that word-level vocabularies fail on handling rare words under limited vocabulary sizes. Researchers recently have proposed several advanced vocabularization approaches, like bytelevel approaches (Wang et al., 2020), characterlevel approaches (Costa-juss`a and Fonollosa, 2016; Lee et al., 2017; Al-Rfou et al., 2019), and sub-word approaches (Sennrich et al., 2016; Kudo and Richardson, 2018). Byte-Pair Encoding (BPE) (Sennrich et al., 2016) is proposed to get subword-level vocabularies. The general idea is to merge pairs of frequent character sequences to create sub-word units. Sub-word vocabularies can be regarded as a trade-off between character-level vocabularies and word-level vocabularies. Compared to word-level vocabularies, it can decrease the sparsity of tokens and increase the shared features between similar words, which probably have similar semantic meanings, like “happy” and “happier”. Compared to character-level vocabularies, it has shorter sentence lengths without r"
2021.acl-long.571,Q17-1026,0,0.171006,"Introduction Due to the discreteness of text, vocabulary construction ( vocabularization for short) is a prerequisite for neural machine translation (NMT) and many other natural language processing (NLP) tasks using neural networks (Mikolov et al., 2013; Vaswani et al., 2017; Gehrmann et al., 2018; Zhang et al., 2018; Devlin et al., 2019). Currently, sub-word approaches like Byte-Pair Encoding (BPE) are widely used in the community (Ott et al., 2018; Ding et al., 2019; Liu et al., 2020), and achieve quite promising results in practice (Sennrich et al., 2016; Costa-juss`a and Fonollosa, 2016; Lee et al., 2017; Kudo and Richardson, † Lab. This work is done during the internship at ByteDance AI 2018; Al-Rfou et al., 2019; Wang et al., 2020). The key idea of these approaches is selecting the most frequent sub-words (or word pieces with higher probabilities) as the vocabulary tokens. In information theory, these frequency-based approaches are simple forms of data compression to reduce entropy (Gage, 1994), which makes the resulting corpus easy to learn and predict (Martin and England, 2011; Bentz and Alikaniotis, 2016). However, the effects of vocabulary size are not sufficiently taken into account si"
2021.acl-long.571,W18-6301,0,0.283211,"ch, VOLT reduces the search time from 384 GPU hours to 30 GPU hours on English-German translation. Codes are available at https: //github.com/Jingjing-NLP/VOLT. 1 Introduction Due to the discreteness of text, vocabulary construction ( vocabularization for short) is a prerequisite for neural machine translation (NMT) and many other natural language processing (NLP) tasks using neural networks (Mikolov et al., 2013; Vaswani et al., 2017; Gehrmann et al., 2018; Zhang et al., 2018; Devlin et al., 2019). Currently, sub-word approaches like Byte-Pair Encoding (BPE) are widely used in the community (Ott et al., 2018; Ding et al., 2019; Liu et al., 2020), and achieve quite promising results in practice (Sennrich et al., 2016; Costa-juss`a and Fonollosa, 2016; Lee et al., 2017; Kudo and Richardson, † Lab. This work is done during the internship at ByteDance AI 2018; Al-Rfou et al., 2019; Wang et al., 2020). The key idea of these approaches is selecting the most frequent sub-words (or word pieces with higher probabilities) as the vocabulary tokens. In information theory, these frequency-based approaches are simple forms of data compression to reduce entropy (Gage, 1994), which makes the resulting corpus eas"
2021.acl-long.571,2020.acl-main.170,0,0.105947,"these approaches is selecting the most frequent sub-words (or word pieces with higher probabilities) as the vocabulary tokens. In information theory, these frequency-based approaches are simple forms of data compression to reduce entropy (Gage, 1994), which makes the resulting corpus easy to learn and predict (Martin and England, 2011; Bentz and Alikaniotis, 2016). However, the effects of vocabulary size are not sufficiently taken into account since current approaches only consider frequency (or entropy) as the main criteria. Many previous studies (Sennrich and Zhang, 2019; Ding et al., 2019; Provilkov et al., 2020; Salesky et al., 2020) show that vocabulary size also affects downstream performances, especially on low-resource tasks. Due to the lack of appropriate inductive bias about size, trial training (namely traversing all possible sizes) is usually required to search for the optimal size, which takes high computation costs. For convenience, most existing studies only adopt the widely-used settings in implementation. For example, 30K-40K is the most popular size setting in all 42 papers of Conference of Machine Translation (WMT) through 2017 and 2018 (Ding et al., 2019). In this paper, we propose t"
2021.acl-long.571,N18-2084,0,0.0673002,"Missing"
2021.acl-long.571,P16-1162,0,0.352719,"are available at https: //github.com/Jingjing-NLP/VOLT. 1 Introduction Due to the discreteness of text, vocabulary construction ( vocabularization for short) is a prerequisite for neural machine translation (NMT) and many other natural language processing (NLP) tasks using neural networks (Mikolov et al., 2013; Vaswani et al., 2017; Gehrmann et al., 2018; Zhang et al., 2018; Devlin et al., 2019). Currently, sub-word approaches like Byte-Pair Encoding (BPE) are widely used in the community (Ott et al., 2018; Ding et al., 2019; Liu et al., 2020), and achieve quite promising results in practice (Sennrich et al., 2016; Costa-juss`a and Fonollosa, 2016; Lee et al., 2017; Kudo and Richardson, † Lab. This work is done during the internship at ByteDance AI 2018; Al-Rfou et al., 2019; Wang et al., 2020). The key idea of these approaches is selecting the most frequent sub-words (or word pieces with higher probabilities) as the vocabulary tokens. In information theory, these frequency-based approaches are simple forms of data compression to reduce entropy (Gage, 1994), which makes the resulting corpus easy to learn and predict (Martin and England, 2011; Bentz and Alikaniotis, 2016). However, the effects of vocabu"
2021.acl-long.571,P19-1021,0,0.0216056,"., 2019; Wang et al., 2020). The key idea of these approaches is selecting the most frequent sub-words (or word pieces with higher probabilities) as the vocabulary tokens. In information theory, these frequency-based approaches are simple forms of data compression to reduce entropy (Gage, 1994), which makes the resulting corpus easy to learn and predict (Martin and England, 2011; Bentz and Alikaniotis, 2016). However, the effects of vocabulary size are not sufficiently taken into account since current approaches only consider frequency (or entropy) as the main criteria. Many previous studies (Sennrich and Zhang, 2019; Ding et al., 2019; Provilkov et al., 2020; Salesky et al., 2020) show that vocabulary size also affects downstream performances, especially on low-resource tasks. Due to the lack of appropriate inductive bias about size, trial training (namely traversing all possible sizes) is usually required to search for the optimal size, which takes high computation costs. For convenience, most existing studies only adopt the widely-used settings in implementation. For example, 30K-40K is the most popular size setting in all 42 papers of Conference of Machine Translation (WMT) through 2017 and 2018 (Ding"
2021.acl-long.571,N18-2074,0,0.0430914,"Missing"
2021.acl-tutorials.4,N19-1006,0,0.148651,"ource NMT (Zhu et al., 2020; Yang et al., 2020). We will cover techniques to finetune the pre-trained models with various strategies, such as knowledge distillation and adapter (Bapna and Firat, 2019; Liang et al., 2021). The next topic is multi-lingual pre-training for NMT. In this context, we aims at mitigating the English-centric bias and suggest that it is possible Pre-training is a dominant paradigm in Nature Language Processing (NLP) (Radford et al., 2019; Devlin et al., 2019; Liu et al., 2019a), Computer Vision (CV) (He et al., 2019; Xie et al., 2020) and Auto Speech Recognition (ASR) (Bansal et al., 2019; Chuang et al., 2020; Park et al., 2019). Typically, the models are first pre-trained on large amount of unlabeled data to capture rich representations of the input, and then applied to the downstream tasks by either providing context-aware representation of the input, or initializing the parameters of the downstream model for fine-tuning. Recently, the trend of self-supervised pre-training and task-specific fine-tuning finally fully hits neural machine translation (NMT) (Zhu et al., 2020; Yang et al., 2020; Chen et al., 2020). Despite its success, introducing a universal pretrained model to"
2021.acl-tutorials.4,D19-1165,0,0.0281768,"ver, NMT has several distinct characteristics, such as the availability of large training data (10 million or larger) and the high capacity of baseline NMT models, which requires carefully design of pre-training. In this part, we will introduce different pre-training methods and analyse the best practice when applying them to different machine translation scenarios, such as unsupervised NMT, low-resource NMT and rich-source NMT (Zhu et al., 2020; Yang et al., 2020). We will cover techniques to finetune the pre-trained models with various strategies, such as knowledge distillation and adapter (Bapna and Firat, 2019; Liang et al., 2021). The next topic is multi-lingual pre-training for NMT. In this context, we aims at mitigating the English-centric bias and suggest that it is possible Pre-training is a dominant paradigm in Nature Language Processing (NLP) (Radford et al., 2019; Devlin et al., 2019; Liu et al., 2019a), Computer Vision (CV) (He et al., 2019; Xie et al., 2020) and Auto Speech Recognition (ASR) (Bansal et al., 2019; Chuang et al., 2020; Park et al., 2019). Typically, the models are first pre-trained on large amount of unlabeled data to capture rich representations of the input, and then appl"
2021.acl-tutorials.4,2020.emnlp-main.210,1,0.899005,"e on Natural Language Processing: Tutorial Abstracts, pages 21–25, August 1st, 2021. ©2021 Association for Computational Linguistics to build universal representation for different language to improve massive multi-lingual NMT. In this part, we will discuss the general representation of different languages and analyse how knowledge transfers across languages. These will allow a better design for multi-lingual pre-training, in particular for zero-shot transfer to non-English language pairs (Johnson et al., 2017; Qi et al., 2018; Conneau and Lample, 2019; Pires et al., 2019; Huang et al., 2019; Lin et al., 2020; Liu et al., 2020; Pan et al., 2021; Lin et al., 2021). The last technical part of this tutorial deals with the Pre-training for speech NMT. In particular, we focus on leverage weakly supervised or unsupervised training data to improve speech translation. In this part, we will discuss the possibilities of building a general representations across speech and text. And shows how text or audio pre-training can guild the text generation of NMT (Wang et al., 2019; Liu et al., 2019b; Bansal et al., 2019; Wang et al., 2020; Baevski et al., 2020a,b; Huang et al., 2021; Long et al., 2021; Dong et al.,"
2021.acl-tutorials.4,2020.acl-main.705,0,0.019998,"t al., 2019; Xie et al., 2020) and Auto Speech Recognition (ASR) (Bansal et al., 2019; Chuang et al., 2020; Park et al., 2019). Typically, the models are first pre-trained on large amount of unlabeled data to capture rich representations of the input, and then applied to the downstream tasks by either providing context-aware representation of the input, or initializing the parameters of the downstream model for fine-tuning. Recently, the trend of self-supervised pre-training and task-specific fine-tuning finally fully hits neural machine translation (NMT) (Zhu et al., 2020; Yang et al., 2020; Chen et al., 2020). Despite its success, introducing a universal pretrained model to NMT is non-trivial and not necessarily yields promising results, especially for the resource-rich setup. Unique challenges remain in several aspects. First, the objective of most pretraining methods are different from the downstream NMT tasks. For example, BERT (Devlin et al., 2019), a popular pre-trained model, is designed for language understanding with only a transformer encoder, while an NMT model usually consists of an encoder and a decoder to perform cross-lingual generation. This gap makes it not feasible enough to apply"
2021.acl-tutorials.4,2021.acl-long.25,1,0.648401,"pages 21–25, August 1st, 2021. ©2021 Association for Computational Linguistics to build universal representation for different language to improve massive multi-lingual NMT. In this part, we will discuss the general representation of different languages and analyse how knowledge transfers across languages. These will allow a better design for multi-lingual pre-training, in particular for zero-shot transfer to non-English language pairs (Johnson et al., 2017; Qi et al., 2018; Conneau and Lample, 2019; Pires et al., 2019; Huang et al., 2019; Lin et al., 2020; Liu et al., 2020; Pan et al., 2021; Lin et al., 2021). The last technical part of this tutorial deals with the Pre-training for speech NMT. In particular, we focus on leverage weakly supervised or unsupervised training data to improve speech translation. In this part, we will discuss the possibilities of building a general representations across speech and text. And shows how text or audio pre-training can guild the text generation of NMT (Wang et al., 2019; Liu et al., 2019b; Bansal et al., 2019; Wang et al., 2020; Baevski et al., 2020a,b; Huang et al., 2021; Long et al., 2021; Dong et al., 2021b,a; Han et al., 2021; Ye et al., 2021). We conclu"
2021.acl-tutorials.4,2020.tacl-1.47,0,0.085368,"age Processing: Tutorial Abstracts, pages 21–25, August 1st, 2021. ©2021 Association for Computational Linguistics to build universal representation for different language to improve massive multi-lingual NMT. In this part, we will discuss the general representation of different languages and analyse how knowledge transfers across languages. These will allow a better design for multi-lingual pre-training, in particular for zero-shot transfer to non-English language pairs (Johnson et al., 2017; Qi et al., 2018; Conneau and Lample, 2019; Pires et al., 2019; Huang et al., 2019; Lin et al., 2020; Liu et al., 2020; Pan et al., 2021; Lin et al., 2021). The last technical part of this tutorial deals with the Pre-training for speech NMT. In particular, we focus on leverage weakly supervised or unsupervised training data to improve speech translation. In this part, we will discuss the possibilities of building a general representations across speech and text. And shows how text or audio pre-training can guild the text generation of NMT (Wang et al., 2019; Liu et al., 2019b; Bansal et al., 2019; Wang et al., 2020; Baevski et al., 2020a,b; Huang et al., 2021; Long et al., 2021; Dong et al., 2021b,a; Han et a"
2021.acl-tutorials.4,2021.ccl-1.108,0,0.0585324,"Missing"
2021.acl-tutorials.4,N19-1423,0,0.491374,"n enhancing the performance of NMT, how to design a better pretraining model for executing specific NMT tasks and how to better integrate the pre-trained model into NMT system. In each part, we will provide examples, discuss training techniques and analyse what is transferred when applying pre-training. The first topic is the monolingual pre-training for NMT, which is one of the most well-studied field. Monolingual text representations like ELMo, GPT, MASS and BERT have superiorities, which significantly boost the performances of various natural language processing tasks (Peters et al., 2018; Devlin et al., 2019; Radford et al., 2019; Song et al., 2019). However, NMT has several distinct characteristics, such as the availability of large training data (10 million or larger) and the high capacity of baseline NMT models, which requires carefully design of pre-training. In this part, we will introduce different pre-training methods and analyse the best practice when applying them to different machine translation scenarios, such as unsupervised NMT, low-resource NMT and rich-source NMT (Zhu et al., 2020; Yang et al., 2020). We will cover techniques to finetune the pre-trained models with various strategi"
2021.acl-tutorials.4,2021.naacl-main.457,1,0.722729,"et al., 2019; Lin et al., 2020; Liu et al., 2020; Pan et al., 2021; Lin et al., 2021). The last technical part of this tutorial deals with the Pre-training for speech NMT. In particular, we focus on leverage weakly supervised or unsupervised training data to improve speech translation. In this part, we will discuss the possibilities of building a general representations across speech and text. And shows how text or audio pre-training can guild the text generation of NMT (Wang et al., 2019; Liu et al., 2019b; Bansal et al., 2019; Wang et al., 2020; Baevski et al., 2020a,b; Huang et al., 2021; Long et al., 2021; Dong et al., 2021b,a; Han et al., 2021; Ye et al., 2021). We conclude the tutorial by pointing out the best practice when applying pre-training for NMT. The topics cover various of pre-training methods for different NMT scenarios. After this tutorial, the audience will understand why pre-training for NMT is different from other tasks and how to make the most of pre-training for NMT. Importantly, we will give deep analyze about how and why pre-training works in NMT, which will inspire future work on designing pre-training paradigm specific for NMT. 2 • Unified sequence-to-sequence pre-trainin"
2021.acl-tutorials.4,2021.acl-long.21,1,0.718701,"torial Abstracts, pages 21–25, August 1st, 2021. ©2021 Association for Computational Linguistics to build universal representation for different language to improve massive multi-lingual NMT. In this part, we will discuss the general representation of different languages and analyse how knowledge transfers across languages. These will allow a better design for multi-lingual pre-training, in particular for zero-shot transfer to non-English language pairs (Johnson et al., 2017; Qi et al., 2018; Conneau and Lample, 2019; Pires et al., 2019; Huang et al., 2019; Lin et al., 2020; Liu et al., 2020; Pan et al., 2021; Lin et al., 2021). The last technical part of this tutorial deals with the Pre-training for speech NMT. In particular, we focus on leverage weakly supervised or unsupervised training data to improve speech translation. In this part, we will discuss the possibilities of building a general representations across speech and text. And shows how text or audio pre-training can guild the text generation of NMT (Wang et al., 2019; Liu et al., 2019b; Bansal et al., 2019; Wang et al., 2020; Baevski et al., 2020a,b; Huang et al., 2021; Long et al., 2021; Dong et al., 2021b,a; Han et al., 2021; Ye et al"
2021.acl-tutorials.4,N18-1202,0,0.012014,"ole of pre-training in enhancing the performance of NMT, how to design a better pretraining model for executing specific NMT tasks and how to better integrate the pre-trained model into NMT system. In each part, we will provide examples, discuss training techniques and analyse what is transferred when applying pre-training. The first topic is the monolingual pre-training for NMT, which is one of the most well-studied field. Monolingual text representations like ELMo, GPT, MASS and BERT have superiorities, which significantly boost the performances of various natural language processing tasks (Peters et al., 2018; Devlin et al., 2019; Radford et al., 2019; Song et al., 2019). However, NMT has several distinct characteristics, such as the availability of large training data (10 million or larger) and the high capacity of baseline NMT models, which requires carefully design of pre-training. In this part, we will introduce different pre-training methods and analyse the best practice when applying them to different machine translation scenarios, such as unsupervised NMT, low-resource NMT and rich-source NMT (Zhu et al., 2020; Yang et al., 2020). We will cover techniques to finetune the pre-trained models"
2021.acl-tutorials.4,P19-1493,0,0.012108,"d the 11th International Joint Conference on Natural Language Processing: Tutorial Abstracts, pages 21–25, August 1st, 2021. ©2021 Association for Computational Linguistics to build universal representation for different language to improve massive multi-lingual NMT. In this part, we will discuss the general representation of different languages and analyse how knowledge transfers across languages. These will allow a better design for multi-lingual pre-training, in particular for zero-shot transfer to non-English language pairs (Johnson et al., 2017; Qi et al., 2018; Conneau and Lample, 2019; Pires et al., 2019; Huang et al., 2019; Lin et al., 2020; Liu et al., 2020; Pan et al., 2021; Lin et al., 2021). The last technical part of this tutorial deals with the Pre-training for speech NMT. In particular, we focus on leverage weakly supervised or unsupervised training data to improve speech translation. In this part, we will discuss the possibilities of building a general representations across speech and text. And shows how text or audio pre-training can guild the text generation of NMT (Wang et al., 2019; Liu et al., 2019b; Bansal et al., 2019; Wang et al., 2020; Baevski et al., 2020a,b; Huang et al.,"
2021.acl-tutorials.4,N18-2084,0,0.0178786,"ssociation for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: Tutorial Abstracts, pages 21–25, August 1st, 2021. ©2021 Association for Computational Linguistics to build universal representation for different language to improve massive multi-lingual NMT. In this part, we will discuss the general representation of different languages and analyse how knowledge transfers across languages. These will allow a better design for multi-lingual pre-training, in particular for zero-shot transfer to non-English language pairs (Johnson et al., 2017; Qi et al., 2018; Conneau and Lample, 2019; Pires et al., 2019; Huang et al., 2019; Lin et al., 2020; Liu et al., 2020; Pan et al., 2021; Lin et al., 2021). The last technical part of this tutorial deals with the Pre-training for speech NMT. In particular, we focus on leverage weakly supervised or unsupervised training data to improve speech translation. In this part, we will discuss the possibilities of building a general representations across speech and text. And shows how text or audio pre-training can guild the text generation of NMT (Wang et al., 2019; Liu et al., 2019b; Bansal et al., 2019; Wang et al."
2021.acl-tutorials.4,2020.acl-main.344,0,0.0134936,"t al., 2018; Conneau and Lample, 2019; Pires et al., 2019; Huang et al., 2019; Lin et al., 2020; Liu et al., 2020; Pan et al., 2021; Lin et al., 2021). The last technical part of this tutorial deals with the Pre-training for speech NMT. In particular, we focus on leverage weakly supervised or unsupervised training data to improve speech translation. In this part, we will discuss the possibilities of building a general representations across speech and text. And shows how text or audio pre-training can guild the text generation of NMT (Wang et al., 2019; Liu et al., 2019b; Bansal et al., 2019; Wang et al., 2020; Baevski et al., 2020a,b; Huang et al., 2021; Long et al., 2021; Dong et al., 2021b,a; Han et al., 2021; Ye et al., 2021). We conclude the tutorial by pointing out the best practice when applying pre-training for NMT. The topics cover various of pre-training methods for different NMT scenarios. After this tutorial, the audience will understand why pre-training for NMT is different from other tasks and how to make the most of pre-training for NMT. Importantly, we will give deep analyze about how and why pre-training works in NMT, which will inspire future work on designing pre-training paradig"
2021.eacl-main.251,P19-1129,0,0.0317583,"Missing"
2021.eacl-main.251,N19-1308,0,0.194291,"epresentations for Joint Entity Relation Extraction Yijun Wang1, 2 , Changzhi Sun4 , Yuanbin Wu3 , Hao Zhou4 , Lei Li4 , and Junchi Yan1, 2 1 Department of Computer Science and Engineering, Shanghai Jiao Tong University MoE Key Lab of Artificial Intelligence, AI Institute, Shanghai Jiao Tong University 3 School of Computer Science and Technology, East China Normal University 4 ByteDance, AI Lab yijunwang.cs@gmail.com ybwu@cs.ecnu.edu.cn yanjunchi@sjtu.edu.cn {sunchangzhi, zhouhao.nlp, lileilab}@bytedance.com 2 Abstract MET Current state-of-the-art systems for joint entity relation extraction (Luan et al., 2019; Wadden et al., 2019) usually adopt the multi-task learning framework. However, annotations for these additional tasks such as coreference resolution and event extraction are always equally hard (or even harder) to obtain. In this work, we propose a pre-training method E N PA R to improve the joint extraction performance. E N PA R requires only the additional entity annotations that are much easier to collect. Unlike most existing works that only consider incorporating entity information into the sentence encoder, we further utilize the entity pair information. Specifically, we devise four no"
2021.eacl-main.251,P16-1105,0,0.0798562,"(entity) types and 7 relation types. We use the same data split and preprocessing of SciERC dataset (350 training, 50 validating and 100 testing) as (Luan et al., 2019). NYT The NYT dataset8 is a large-scale corpus which automatically annotates a collection of New York Times news articles. NYT contains 3 types of entities and 12 types of relations. The training set is automatically annotated by distant supervision. While the validation and testing data are manually labeled by (Jia et al., 2019). We choose the latest version of NYT released by (Jia et al., 2019). Evaluation. As previous works (Miwa and Bansal, 2016; Sun et al., 2019a), we evaluate the 6 https://github.com/tticoin/LSTM-ER http://nlp.cs.washington.edu/sciIE/ 8 https://github.com/PaddlePaddle/models/tree/develop/ PaddleNLP/Research/ACL2019-ARNOR/ 2881 7 Model Sun, 2019a Li, 2019 Luan, 2019?, ◦ Wadden, 2019 , ◦ E N PA R  Entity Relation Relation (exactly) 84.2 84.8 88.4 88.6 86.9 – – 63.2 63.4 66.1 59.1 60.2 – – 63.5 Table 1: Results on the ACE05 test data.  means that the model uses BERT. ? means that the model uses ELMo as token embeddings. ◦ stands for training the model with multi-task learning. E N PA R is the proposed model fine-t"
2021.eacl-main.251,P19-1131,1,0.71528,"roduced in the fine-tuning stage, which may futher impair the joint extraction performance. To address the first limitation, recent several works try to incorporate entity-related information 1 https://spacy.io/ 2877 Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics, pages 2877–2887 April 19 - 23, 2021. ©2021 Association for Computational Linguistics into pre-training objectives. Zhang et al. (2019) fuses heterogeneous information from both texts and knowledge graphs and proposes a denoising entity auto-encoder objective based on BERT. Sun et al. (2019c) presents two knowledge masking strategies in the pre-training stage (entity-level masking and phrase-level masking). Both of them utilize extra entity annotations (i.e., entities in knowledge graphs and automatic entity annotations, respectively). In this paper, we follow this line of works and build a large-scale entity annotated corpus using the spaCy NER tool. For the second limitation, we propose E N PA R, a pre-training method customized for entity relation extraction. E N PA R consists of an underlying sentence encoder, an entity encoder, and an entity pair encoder. Compared with BERT"
2021.eacl-main.251,D18-1249,1,0.853386,"1080 Ti GPU. 3 Experiments We conduct experiments on three benchmark entity relation extraction datasets: ACE05, SciERC, and NYT. For space limitation, we will mainly discuss the results on ACE05 and report basic results on the remaining two datasets. ACE05 The ACE05 dataset 6 that is a standard corpus for entity relation extraction task annotates entity and relation labels for a collection of documents. ACE05 contains 7 entity types and 6 relation types. We use the same data split and preprocessing of ACE05 dataset (351 training, 80 validating and 80 testing) as (Miwa and Bansal, 2016) and (Sun et al., 2018). SciERC The SciERC dataset 7 annotates entity, coreference and relation labels for 500 scientific abstracts from 12 AI conference/workshop proceedings. We only use the annotations of entities and relations. SciERC contains 6 scientific term (entity) types and 7 relation types. We use the same data split and preprocessing of SciERC dataset (350 training, 50 validating and 100 testing) as (Luan et al., 2019). NYT The NYT dataset8 is a large-scale corpus which automatically annotates a collection of New York Times news articles. NYT contains 3 types of entities and 12 types of relations. The tra"
2021.eacl-main.251,D19-1585,0,0.273858,"Joint Entity Relation Extraction Yijun Wang1, 2 , Changzhi Sun4 , Yuanbin Wu3 , Hao Zhou4 , Lei Li4 , and Junchi Yan1, 2 1 Department of Computer Science and Engineering, Shanghai Jiao Tong University MoE Key Lab of Artificial Intelligence, AI Institute, Shanghai Jiao Tong University 3 School of Computer Science and Technology, East China Normal University 4 ByteDance, AI Lab yijunwang.cs@gmail.com ybwu@cs.ecnu.edu.cn yanjunchi@sjtu.edu.cn {sunchangzhi, zhouhao.nlp, lileilab}@bytedance.com 2 Abstract MET Current state-of-the-art systems for joint entity relation extraction (Luan et al., 2019; Wadden et al., 2019) usually adopt the multi-task learning framework. However, annotations for these additional tasks such as coreference resolution and event extraction are always equally hard (or even harder) to obtain. In this work, we propose a pre-training method E N PA R to improve the joint extraction performance. E N PA R requires only the additional entity annotations that are much easier to collect. Unlike most existing works that only consider incorporating entity information into the sentence encoder, we further utilize the entity pair information. Specifically, we devise four novel objectives, i.e.,"
2021.eacl-main.251,P13-1161,0,0.0234792,"tperforms BERT on entity performance and relation performance. This again verifies the effectiveness of our proposed pre-training method. 4 Related Work Joint entity relation extraction is an important task that has been extensively studied. One simple method to achieve joint learning is through parameters sharing, which usually share some input embeddings or sentence encoders (Miwa and Bansal, 2016; Katiyar and Cardie, 2017). To further explore the interactions between the outputs of the entity model and the relation model, many joint decoding algorithms were introduced into this joint task (Yang and Cardie, 2013; Li and Ji, 2014; Katiyar and Cardie, 2016; Zheng et al., 2017; Ren et al., 2017; Wang et al., 2018; Sun et al., 2018; Fu et al., 2019). Besides, (Li et al., 2019) tackle this task under the framework of multi-turn QA. And (Sun et al., 2019a) conduct joint type inference via GCN on a bipartite graph composed of entities and relations. Recently, transfer learning (Sun and Wu, 2019), multi-task learning (Sanh et al., 2019; Wadden et al., 2019; Luan et al., 2019) were also applied in this task. In this work, we investigate the pre-trained model for entity relation extraction. For simplicity, we"
2021.eacl-main.251,P19-1139,0,0.226172,"ntences, but not entities and entity pairs. To obtain the representations for entities and entity pairs, additional parameters that are not pre-trained are introduced in the fine-tuning stage, which may futher impair the joint extraction performance. To address the first limitation, recent several works try to incorporate entity-related information 1 https://spacy.io/ 2877 Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics, pages 2877–2887 April 19 - 23, 2021. ©2021 Association for Computational Linguistics into pre-training objectives. Zhang et al. (2019) fuses heterogeneous information from both texts and knowledge graphs and proposes a denoising entity auto-encoder objective based on BERT. Sun et al. (2019c) presents two knowledge masking strategies in the pre-training stage (entity-level masking and phrase-level masking). Both of them utilize extra entity annotations (i.e., entities in knowledge graphs and automatic entity annotations, respectively). In this paper, we follow this line of works and build a large-scale entity annotated corpus using the spaCy NER tool. For the second limitation, we propose E N PA R, a pre-training method custo"
2021.eacl-main.251,P17-1113,0,0.0199405,"s again verifies the effectiveness of our proposed pre-training method. 4 Related Work Joint entity relation extraction is an important task that has been extensively studied. One simple method to achieve joint learning is through parameters sharing, which usually share some input embeddings or sentence encoders (Miwa and Bansal, 2016; Katiyar and Cardie, 2017). To further explore the interactions between the outputs of the entity model and the relation model, many joint decoding algorithms were introduced into this joint task (Yang and Cardie, 2013; Li and Ji, 2014; Katiyar and Cardie, 2016; Zheng et al., 2017; Ren et al., 2017; Wang et al., 2018; Sun et al., 2018; Fu et al., 2019). Besides, (Li et al., 2019) tackle this task under the framework of multi-turn QA. And (Sun et al., 2019a) conduct joint type inference via GCN on a bipartite graph composed of entities and relations. Recently, transfer learning (Sun and Wu, 2019), multi-task learning (Sanh et al., 2019; Wadden et al., 2019; Luan et al., 2019) were also applied in this task. In this work, we investigate the pre-trained model for entity relation extraction. For simplicity, we restrict the joint model of parameters sharing, which can be ea"
2021.emnlp-main.31,2020.findings-emnlp.372,0,0.154717,"arding the student performance and learning efficiency? In this paper, we propose a dynamic knowledge distillation (Dynamic KD) framework, which attempts to empower the student to adjust the learn1 Introduction ing procedure according to its competency. SpecifiKnowledge distillation (KD) (Hinton et al., 2015) cally, inspired by the success of active learning (Setaims to transfer the knowledge from a large teacher tles, 2009), we take the prediction uncertainty, e.g., model to a small student model. It has been widely the entropy of the predicted classification probabilused (Sanh et al., 2019; Jiao et al., 2020; Sun et al., ity distribution, as a proxy of the student compe2019) to compress large-scale pre-trained language tency. We strive to answer the following research models (PLMs) like BERT (Devlin et al., 2019) questions: (RQ1) Which teacher is proper to learn and RoBERTa (Liu et al., 2019) in recent years. as the student evolves? (RQ2) Which data are acBy knowledge distillation, we can obtain a much tually useful for student models in the whole KD smaller model with comparable performance, while stage? (RQ3) Does the optimal learning objecgreatly reduce the memory usage and accelerate tive cha"
2021.emnlp-main.31,2020.emnlp-main.242,0,0.0165177,"ns of different objectives can be promising. 5 Related Work Our work relates to recent explorations on applying KD for compressing the PLMs and active learning. Knowledge Distillation for PLMs Knowledge distillation (Hinton et al., 2015) aims to transfer the dark knowledge from a large teacher model to a compact student model, which has been proved effective for obtaining compact variants of PLMs. Those methods can be divided into general distillation (Sanh et al., 2019; Turc et al., 2019; Wang et al., 2020) and task-specific distillation (Sun et al., 2019; Jiao et al., 2020; Xu et al., 2020; Li et al., 2020a; Liang et al., 2021; Li et al., 2020b; Wu et al., 2021). The former conducts KD on the general text corpus while the latter trains the student model on the task-specific datasets. In this paper, we focus on the latter one as it is more widely adopted in practice. Compared to existing static KD work, we are the first to explore the idea of Dynamic KD, making it more flexible, efficient and effective. 6 Conclusion In this paper, we introduce dynamic knowledge distillation, and conduct exploratory experiments regarding teacher model adoption, data selection and the supervision adjustment. Our e"
2021.emnlp-main.31,2021.findings-emnlp.43,1,0.827368,"Missing"
2021.emnlp-main.31,D13-1170,0,0.00536781,"Missing"
2021.emnlp-main.31,P19-1355,0,0.0556813,"Missing"
2021.emnlp-main.31,D19-1441,0,0.262711,"and the teacher for input x, respectively. The KD can be conducted by minimizing the KullbackLeibler (KL) divergence distance between the student and teacher prediction: ters are updated according to the KD loss and the original classification loss, i.e., the cross-entropy over the ground-truth label y: LCE = −y log σ (S (x)) , L = (1 − λKL )LCE + λKL LKL , (2) (3) where λKL is the hyper-parameter controlling the weight of knowledge distillation objective. Recent explorations also find that introducing KD objectives of alignments between the intermediate representations (Romero et al., 2015; Sun et al., 2019) and attention map (Jiao et al., 2020; Wang et al., 2020) is helpful. Note that conventional KD framework is static, i.e., the teacher model is selected before KD and the training is conducted on all training instances indiscriminately according to the predefined objective and the corresponding weights of different objectives. However, it is unreasonable to conduct the KD learning procedure statically as the student model evolves during the training. We are curious whether adaptive adjusting the settings on teacher adoption, dataset selection and supervision adjustment can bring benefits regar"
2021.emnlp-main.31,2021.ccl-1.108,0,0.0904298,"Missing"
2021.emnlp-main.31,P11-1015,0,0.513843,"the hidden size, where the phenomenon also exists and corresponding results can be found in Appendix C. Specifically, we are curious about whether learning from a bigger PLM with better performance can lead to a better distilled student model. We conduct probing experiments to distill a 6-layer student BERT model from BERTBASE with 12 layers, and BERTLARGE with 24 layers, respectively. We conducts the experiment on two datasets, RTE (Bentivogli et al., 2009) and CoLA (Warstadt et al., 2019), where two teacher models exhibit clear performance gap, and a sentiment classification benchmark IMDB (Maas et al., 2011). Detailed experimental setup can be found in Appendix A. As shown in Table 1, we surprisingly find that while the BERTLARGE teacher clearly outperforms the small BERTBASE teacher model, the student model distilled by the BERTBASE teacher achieves 3.1.2 Uncertainty-based Teacher Adoption better performance on all three datasets. This phe- Our preliminary observations demonstrate that senomenon is counter-intuitive as a larger teacher is lecting a proper teacher model for KD is significant supposed to provide better supervision signal for for the student performance. While the capacity the stud"
2021.emnlp-main.31,Q19-1040,0,0.0129046,"-depth investigations. Note that BERTBASE and BERTLARGE also differs from the number of hidden size, the experiments regarding the hidden size, where the phenomenon also exists and corresponding results can be found in Appendix C. Specifically, we are curious about whether learning from a bigger PLM with better performance can lead to a better distilled student model. We conduct probing experiments to distill a 6-layer student BERT model from BERTBASE with 12 layers, and BERTLARGE with 24 layers, respectively. We conducts the experiment on two datasets, RTE (Bentivogli et al., 2009) and CoLA (Warstadt et al., 2019), where two teacher models exhibit clear performance gap, and a sentiment classification benchmark IMDB (Maas et al., 2011). Detailed experimental setup can be found in Appendix A. As shown in Table 1, we surprisingly find that while the BERTLARGE teacher clearly outperforms the small BERTBASE teacher model, the student model distilled by the BERTBASE teacher achieves 3.1.2 Uncertainty-based Teacher Adoption better performance on all three datasets. This phe- Our preliminary observations demonstrate that senomenon is counter-intuitive as a larger teacher is lecting a proper teacher model for K"
2021.emnlp-main.31,N18-1101,0,0.0122047,". To verify this, we turn to the the setting where the original training dataset is enriched with augmentation techniques. Settings We conduct the investigation experi- Results with Augmented Dataset Following ments on two sentiment classification datasets TinyBERT (Jiao et al., 2020), we augment the trainIMDB (Maas et al., 2011) and SST-5 (Socher et al., ing dataset 20 times with BERT mask language 2013), and natural language inference tasks in- prediction, as it has been prove effective for discluding MRPC (Dolan and Brockett, 2005) and tilling a powerful student model. Our assumption MNLI (Williams et al., 2018). The statistics of is that with the data augmentation technique, the dataset and the implementation details can be found training set can sufficiently cover the possible data 383 #FLOPs SST-5 IMDB MRPC MNLI-m / mm Avg. (↑) ∆ (↓) - 53.7 88.8 87.5 83.9 / 83.4 79.5 - TinyBERT TinyBERT 24.9B 24.9B 51.4 87.6 86.4 86.2 82.5 / 81.8 82.6 / 82.0 78.0 0.0 Random Uncertainty-Entropy Uncertainty-Margin Uncertainty-LC 2.49B 4.65B 4.65B 4.65B 51.1 51.5 51.6 51.2 87.0 87.7 87.7 87.7 83.3 86.5 86.5 86.5 80.8 / 80.5 81.8 / 81.0 81.6 / 81.1 81.4 / 80.8 76.5 77.7 77.7 77.5 1.5 0.3 0.3 0.5 Method BERTBASE (Teach"
2021.emnlp-main.31,2021.findings-acl.387,0,0.0196133,"ork Our work relates to recent explorations on applying KD for compressing the PLMs and active learning. Knowledge Distillation for PLMs Knowledge distillation (Hinton et al., 2015) aims to transfer the dark knowledge from a large teacher model to a compact student model, which has been proved effective for obtaining compact variants of PLMs. Those methods can be divided into general distillation (Sanh et al., 2019; Turc et al., 2019; Wang et al., 2020) and task-specific distillation (Sun et al., 2019; Jiao et al., 2020; Xu et al., 2020; Li et al., 2020a; Liang et al., 2021; Li et al., 2020b; Wu et al., 2021). The former conducts KD on the general text corpus while the latter trains the student model on the task-specific datasets. In this paper, we focus on the latter one as it is more widely adopted in practice. Compared to existing static KD work, we are the first to explore the idea of Dynamic KD, making it more flexible, efficient and effective. 6 Conclusion In this paper, we introduce dynamic knowledge distillation, and conduct exploratory experiments regarding teacher model adoption, data selection and the supervision adjustment. Our experimental results demonstrate that the dynamical adjust"
2021.emnlp-main.31,2020.acl-main.620,0,0.0412336,"the preliminary explorations on the three aspects of Dynamic KD, we observe that it is promising for improving the efficiency and the distilled student performance. Here we provide potential directions for further investigations. (1) From uncertainty-based selection criterion to advanced methods. In this paper, we utilize student prediction uncertainty as a proxy for selecting teachers, training instances and supervision objectives. More advanced methods based on more 3.3.2 Experiments accurate uncertainty estimations (Gal and GhahraSettings The student model is set to 6-layer and mani, 2016; Zhou et al., 2020), clues from training BERTBASE is adopted as the teacher model. For dynamics (Toneva et al., 2018), or even a learnable intermediate layer representation alignment, we selector can be developed. adopt the Skip strategy, i.e., Ipt = {2, 4, 6, 8, 10} (2) From isolation to integration. As a prelimas it performs best as described in BERT-PKD. We inary study, we only investigate the three dimenconduct experiments on the sentiment analysis task sions independently. Future work can adjust these SST-5, and two natural language inference tasks components simultaneously and investigate the unMRPC and RT"
2021.emnlp-main.31,2020.emnlp-main.633,0,0.0112894,"ect of combinations of different objectives can be promising. 5 Related Work Our work relates to recent explorations on applying KD for compressing the PLMs and active learning. Knowledge Distillation for PLMs Knowledge distillation (Hinton et al., 2015) aims to transfer the dark knowledge from a large teacher model to a compact student model, which has been proved effective for obtaining compact variants of PLMs. Those methods can be divided into general distillation (Sanh et al., 2019; Turc et al., 2019; Wang et al., 2020) and task-specific distillation (Sun et al., 2019; Jiao et al., 2020; Xu et al., 2020; Li et al., 2020a; Liang et al., 2021; Li et al., 2020b; Wu et al., 2021). The former conducts KD on the general text corpus while the latter trains the student model on the task-specific datasets. In this paper, we focus on the latter one as it is more widely adopted in practice. Compared to existing static KD work, we are the first to explore the idea of Dynamic KD, making it more flexible, efficient and effective. 6 Conclusion In this paper, we introduce dynamic knowledge distillation, and conduct exploratory experiments regarding teacher model adoption, data selection and the supervision"
2021.emnlp-main.337,2020.emnlp-main.506,0,0.3525,"roceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 4102–4108 c November 7–11, 2021. 2021 Association for Computational Linguistics Figure 1: The overall architecture of our proposed method and a training process for inconsistent example. In this paper, we propose a robust weaksupervised factual consistency evaluation model and gradient-based factual errors tracing strategy. Specifically, we construct artificial datasets based on benchmark summarization datasets to train the model. Except rule-based transformations proposed by (Kryscinski et al., 2020; Cao et al., 2020), we propose an implicit augmentation to obtain hard factual inconsistent examples by the adversarial attack. It alleviates the problem of oversimplified negative samples and therefore improves the model performance and robustness. Further, we propose a novel strategy to trace factual errors based on gradients distribution without adding any parameters. The analysis on gradients also provides stronger interpretability for the factual consistency evaluation results. Our contributions are three-fold: (1) We propose an efficient adversarial data augmentation approach to generate weakly supervised"
2021.emnlp-main.337,N19-1423,0,0.0460016,"Missing"
2021.emnlp-main.337,2020.emnlp-main.749,0,0.0918582,"Missing"
2021.emnlp-main.337,2020.acl-main.454,0,0.0279423,"Missing"
2021.emnlp-main.337,P19-1213,0,0.0292183,"Missing"
2021.emnlp-main.337,D18-1443,0,0.0539684,"Missing"
2021.emnlp-main.337,2020.emnlp-main.750,0,0.422393,"esearch attention. 4102 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 4102–4108 c November 7–11, 2021. 2021 Association for Computational Linguistics Figure 1: The overall architecture of our proposed method and a training process for inconsistent example. In this paper, we propose a robust weaksupervised factual consistency evaluation model and gradient-based factual errors tracing strategy. Specifically, we construct artificial datasets based on benchmark summarization datasets to train the model. Except rule-based transformations proposed by (Kryscinski et al., 2020; Cao et al., 2020), we propose an implicit augmentation to obtain hard factual inconsistent examples by the adversarial attack. It alleviates the problem of oversimplified negative samples and therefore improves the model performance and robustness. Further, we propose a novel strategy to trace factual errors based on gradients distribution without adding any parameters. The analysis on gradients also provides stronger interpretability for the factual consistency evaluation results. Our contributions are three-fold: (1) We propose an efficient adversarial data augmentation approach to generat"
2021.emnlp-main.337,2020.acl-main.703,0,0.0315226,"Missing"
2021.emnlp-main.337,C18-1121,0,0.0585215,"Missing"
2021.emnlp-main.337,2020.acl-main.173,0,0.011449,"sses to train our model: L = α · Lce + (1 − α) · Ladv and visualization in Section 4 to demonstrate the effects. 3 3.1 Experiments Experimental Setup We perform experiments on two benchmark text summarization datasets CNN/DM(Nallapati et al., 2016) and XSUM(Narayan et al., 2018). Weakly supervised training data was generated as described in Section 2. Models are evaluated in two ways: (1) with the source documents and the ground truth references of datasets, which are all positive examples (2) with the manual factual consistency annotations provided on CNN/DM(Kryscinski et al., 2020) and XSUM(Maynez et al., 2020). We report accuracy, balanced accuracy, and marco F1-score. We compare our evaluation model with strong baselines: (1) FactCC(Kryscinski et al., 2020): a BERT-based classification model trained on artificial datasets. (2)FactCCX(Kryscinski et al., 2020): a version of FactCC with additional span selection heads. (3) FEC(Cao et al., 2020): a BARTbased factual error evaluation and correction model trained on artificial datasets. Error Tracing. We propose a novel factual error tracing strategy using back-propagated gradients g. Instead of introducing more neural network layers and parameters, our"
2021.emnlp-main.337,K16-1028,0,0.0244751,"shows an inconsistent example, and for consistent examples, perturbations on all tokens are retained. We name ep and add it to e to the filtered perturbation as v ep , which obtain new tokens embeddings e0 = e + v can be regarded as an augmented sample. We feed it to the model again to obtain another loss Ladv with the same label. Finally, we use the weighted sum of two losses to train our model: L = α · Lce + (1 − α) · Ladv and visualization in Section 4 to demonstrate the effects. 3 3.1 Experiments Experimental Setup We perform experiments on two benchmark text summarization datasets CNN/DM(Nallapati et al., 2016) and XSUM(Narayan et al., 2018). Weakly supervised training data was generated as described in Section 2. Models are evaluated in two ways: (1) with the source documents and the ground truth references of datasets, which are all positive examples (2) with the manual factual consistency annotations provided on CNN/DM(Kryscinski et al., 2020) and XSUM(Maynez et al., 2020). We report accuracy, balanced accuracy, and marco F1-score. We compare our evaluation model with strong baselines: (1) FactCC(Kryscinski et al., 2020): a BERT-based classification model trained on artificial datasets. (2)FactCC"
2021.emnlp-main.337,P17-1099,0,0.0970589,"Missing"
2021.emnlp-main.337,2020.acl-main.450,0,0.0353232,"Missing"
2021.emnlp-main.337,2020.emnlp-main.490,1,0.75303,"threshold on the similarity between the original and pseudo entities based on the simple distance algorithm. (2) Pronoun swapping, replace a random pronoun with another one of matching syntactic case. (3) Negation: transform a random auxiliary verb to its negative form. Adversarial Augmentation. It is pointed that a classifier trained on artificial datasets only works well on easy examples, thus can hardly generalize well to actual scenarios (Zhang et al., 2020b). To alleviate this, we propose an adversarial attack mechanism (Goodfellow et al., 2015; Kurakin et al., 2016; Miyato et al., 2017; Yan et al., 2020; Meng et al., 2020) on rule-based pseudo samples as data augmentation. For token embeddings of a sample e e, we try to find a worst-case perturbation vector v that maximizes the loss function: e = arg max Lce (f (e + v; θ), Y ) v (1) ||v||≤ Where  is the norm bound of the perturbation, since the complexity of neural models, it is intractable to compute the perturbation precisely. Instead, we apply Fast Gradient Value (FGV) (Rozsa et al., 2016) to approximate a worst-case perturbation: Fig 1 shows the overall architecture of our factual consistency evaluation model. We adopt Roberta(Liu et a"
2021.emnlp-main.337,2020.acl-main.458,0,0.143578,"Harry has been stolen from a site in cambridgeshire. reference: A caravan locked by Davis has been stolen from a site in cambridgeshire. Table 1: An incorrect summary generated by XSUM. In Table 1, we propose an inconsistent generation example, where the blue part support factual consistency and the red part leads to factual errors. Previous approaches for detecting or boosting factual consistency can be divided into three kinds. (1) Employ information extraction tools to extract facts and leveraging it by building additional objective (Cao et al., 2018; Goodrich et al., 2019; 1 Introduction Zhang et al., 2020a; Zhu et al., 2021). (2) Use Text summarization aims to produce a simplified natural language inference or question answering version of the source document while retaining models for fact checking correction (Li et al., 2018; salient information. Abstractive summarization is Falke et al., 2019; Wang et al., 2020; Dong et al., a branch of methods in which generation text is 2020; Durmus et al., 2020; Chen et al., 2020). (3) free from constraint on the tokens that appeared in Train a factual consistency evaluation model on the source. These methods are extensively studied artificial datasets g"
2021.emnlp-main.337,2021.naacl-main.58,0,0.0612122,"Missing"
2021.emnlp-main.337,D18-1206,0,0.0249013,"d for consistent examples, perturbations on all tokens are retained. We name ep and add it to e to the filtered perturbation as v ep , which obtain new tokens embeddings e0 = e + v can be regarded as an augmented sample. We feed it to the model again to obtain another loss Ladv with the same label. Finally, we use the weighted sum of two losses to train our model: L = α · Lce + (1 − α) · Ladv and visualization in Section 4 to demonstrate the effects. 3 3.1 Experiments Experimental Setup We perform experiments on two benchmark text summarization datasets CNN/DM(Nallapati et al., 2016) and XSUM(Narayan et al., 2018). Weakly supervised training data was generated as described in Section 2. Models are evaluated in two ways: (1) with the source documents and the ground truth references of datasets, which are all positive examples (2) with the manual factual consistency annotations provided on CNN/DM(Kryscinski et al., 2020) and XSUM(Maynez et al., 2020). We report accuracy, balanced accuracy, and marco F1-score. We compare our evaluation model with strong baselines: (1) FactCC(Kryscinski et al., 2020): a BERT-based classification model trained on artificial datasets. (2)FactCCX(Kryscinski et al., 2020): a v"
2021.emnlp-main.579,2020.acl-main.692,0,0.057278,"ys and [W1 ; b1 ] are trainable parameters. The bandwidth of learnable Laplacian kernel kq −k k Kl (qi , kj ; σ) = exp(− i σ j ) is modeled in the 1 For two d-dimension vectors same way as the bandwidth of learnable Gaussian qxPand y, we compute the d 2 2. L distance between x and y as (x − y ) kernel. i i i=1 7282 Train Dev Test Law 467k 2k 2k Medical 248k 2k 2k Koran 18k 2k 2k IT 223k 2k 2k Subtitles 500k 2k 2k Table 1: The number of training, development and test examples of 5 domain-specific datasets. The training data of Subtitles domain is sampled from the full Subtitles training set by Aharoni and Goldberg (2020). 3.3 Adaptive Mixing of Base Prediction and Retrieved Examples To mix the model-based distribution and examplebased distribution adaptively, we model the mixing weight λ with a learnable neural network. The mixing weight λ is computed by a multilayer perceptron with query qi and weighted sum e as inputs, where [W2 ; b2 ; W3 ; b3 ] are of keys k trainable parameters. ei ] + b2 ) + b3 ) λ = sigmoid(W3 ReLU(W2 [qi ; k (5) ei = k k X wj k j (6) j=1 wj ∝ K(qi , kj ; θ) (7) In this way, kNN-MT (Khandelwal et al., 2021) could be seen as a specific case of KSTER, with fixed Gaussian kernel and mixing"
2021.emnlp-main.579,D19-1165,0,0.0152561,"tperforms kNN-MT for 1.8 BLEU scores on average in unseen domains. Therefore, there is no strong restriction of the input domain, which makes KSTER much more practical for industry applications. 2 Related Work els for domain-specific language translation (Chu and Wang, 2018). The most popular method for this task is finetuning general domain MT models on in-domain data. However, finetuning suffers from the notorious catastrophic forgetting problem (McCloskey and Cohen, 1989; Santoro et al., 2016). There are also some sparse domain adaptation methods that only update part of the MT parameters (Bapna et al., 2019; Wuebker et al., 2018; Guo et al., 2021). In real-world translation applications, the domain labels of test examples are often not available. This dilemma inspires a closely related research area — multi-domain machine translation (Pham et al., 2021; Farajian et al., 2017; Liang et al., 2020; Lin et al., 2021; Zhu et al., 2021), where one model translates sentences from all domains. Online Adaptation of MT by Non-parametric Retrieval Non-parametric methods enable online adaptation of deployed NMT models by updating the database from which similar examples are retrieved. Traditional non-parame"
2021.emnlp-main.579,N19-1191,0,0.222739,"success (Bahdanau et al., 2015; Wu et al., 2016; Vaswani et al., 2017). How to effectively update a deployed NMT model and adapt to emerging cases? For example, after a generic NMT model trained on WMT data, a customer wants to use service to translate financial documents. The costomer may have a handful of translation pairs for the finance domain, but do not have the capacity to perform a full retraining. Non-parametric adaptation methods enable incorporating individual examples on-the-fly, by retrieving similar source-target pairs from an external database to guide the translation process (Bapna and Firat, 2019; Gu et al., 2018; Zhang et al., 2018; † 30 20 Introduction ∗ Base kNN-MT KSTER 50 How to effectively adapt neural machine translation (NMT) models according to emerging cases without retraining? Despite the great success of neural machine translation, updating the deployed models online remains a challenge. Existing non-parametric approaches that retrieve similar examples from a database to guide the translation process are promising but are prone to overfit the retrieved examples. However, non-parametric methods are prone to overfit the retrieved examples. In this work, we propose to learn K"
2021.emnlp-main.579,P19-1175,0,0.0175103,"ates sentences from all domains. Online Adaptation of MT by Non-parametric Retrieval Non-parametric methods enable online adaptation of deployed NMT models by updating the database from which similar examples are retrieved. Traditional non-parametric methods search sentence-level examples to guide the translation process (Cao and Xiong, 2018; Gu et al., 2018; Zhang et al., 2018). Recently, n-gram level (Bapna and Firat, 2019) and token level (Khandelwal et al., 2021) retrieval are introduced and shows strong empirical results. Generally, similar examples are retrieved based on fuzzy matching (Bulte and Tezcan, 2019; Xu et al., 2020), embedding similarity, or a mixture of the two approaches (Bapna and Firat, 2019). 3 Methodology In this section, we first formulate the kernelsmoothed machine translation (KSTER), which smooths neural machine translation (NMT) output with retrieved token level examples. Then we introduce the modeling and training of the learnable kernel and adaptive mixing weight. The overview of KSTER is shown in Figure 2. 3.1 Kernel-Smoothed Machine Translation Base Model for Neural Machine Translation The state-of-the-art NMT models are based on the encoder-decoder architecture. The enco"
2021.emnlp-main.579,D18-1340,0,0.13778,"Koran IT domain on which kNN-MT and KSTER are adapted Subtitles Figure 1: The domain-specific and general domain translation performance in EN-DE translation. Base is a Transformer model trained on general domain WMT data. kNN-MT and our proposed KSTER are adapted for domain-specific translation with in-domain database. Both kNN-MT and KSTER achieve improvements over Base in domain-specific translation performance. But kNN-MT overfits to in-domain data and performs bad in general domain translation, while the proposed KSTER achieves comparable general domain translation performance with Base. Cao and Xiong, 2018). The external database can be easily updated online. Most of these methods rely on effective sentence-level retrieval. Different from sentence retrieval, k-nearest-neighbour machine translation introduces token level retrieval to improve translation (Khandelwal et al., 2021). It shows promising results for online domain adaptation. There are still limitations for existing nonparametric methods for online adaptation. First, since it is not easy for sentence-level retrieval to find examples that are similar enough to the test example, this low overlap between test examples 7280 Proceedings of t"
2021.emnlp-main.579,W17-4702,0,0.0162509,"Missing"
2021.emnlp-main.579,2021.acl-long.378,0,0.0648744,"Missing"
2021.emnlp-main.579,2021.findings-emnlp.23,0,0.0311758,"the source. NMT systems are used to score reference and contrastive translations. If an NMT system assign higher score to reference than all contrastive translations in an example, the NMT system is recognized as making correct prediction on this example. We use ContraWSD (Gonzales et al., 2017) 7 as the test suite, which contains 7,359 contrastive translation pairs for DE-EN translation. We encode the source sentences from ContraWSD and training data of 5 specific domains by averaged BERT embeddings (Devlin et al., 2018). Then we whiten the sentence embeddings with BERT-whitening proposed by Huang et al. (2021); Li et al. (2020). For each domain, we select 300 examples from ContraWSD that most similar to the in-domain data based on the cosine similarity of sentence embeddings. https://github.com/ZurichNLP/ContraWSD Koran domain IT Subtitles Base KSTER 0.90 0.85 0.80 0.75 AUX Medical WSD accuracy on ContraWSD 0.95 10 7 35 30 25 20 15 Law Medical Koran domain IT Subtitles Figure 9: BLEU and word sense disambiguation accuracy of base model and KSTER with Gaussian kernel on ContraWSD dataset. Kernel-smoothing helps word sense disambiguation. We evaluate the translation performance and word sense disambi"
2021.emnlp-main.579,2021.acl-short.47,1,0.702718,"ain-specific translation with in-domain database. Both kNN-MT and KSTER achieve improvements over Base in domain-specific translation performance. But kNN-MT overfits to in-domain data and performs bad in general domain translation, while the proposed KSTER achieves comparable general domain translation performance with Base. Cao and Xiong, 2018). The external database can be easily updated online. Most of these methods rely on effective sentence-level retrieval. Different from sentence retrieval, k-nearest-neighbour machine translation introduces token level retrieval to improve translation (Khandelwal et al., 2021). It shows promising results for online domain adaptation. There are still limitations for existing nonparametric methods for online adaptation. First, since it is not easy for sentence-level retrieval to find examples that are similar enough to the test example, this low overlap between test examples 7280 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 7280–7290 c November 7–11, 2021. 2021 Association for Computational Linguistics and retrieved examples brings noise to translation (Bapna and Firat, 2019). Second, completely non-parametric methods"
2021.emnlp-main.579,W04-3250,0,0.250837,".24 IT 22.99 34.48 29.55 31.82 29.21 39.57 37.56 36.90 Subtitles 20.65 25.16 21.80 22.63 23.13 27.73 22.86 25.15 Average-specific 23.54 32.92 33.08 34.53 26.75 36.77 36.64 37.74 Table 5: Test set BLEU scores of multi-domain machine translation. Average-specific is the averaged performance in 5 specific domains. For general domain sentence translation, KSTER outperforms kNN-MT for 3 and 6 BLEU scores in EN-DE and DE-EN direction respectively. For domain-specific translation, KSTER outperforms kNNMT for 1.5 and 1.1 BLEU scores in EN-DE and DE-EN direction. Significance test by paired bootstrap (Koehn, 2004) resampling shows that KSTER outperforms kNN-MT significantly in all domains except for Koran domain in EN-DE translation and IT domain in DE-EN translation. 4.3 Multi-Domain Machine Translation In MDMT, since there is no domain label available in test time, examples from all domains are translated with one model. We build a mixed database with training data of general domain and 5 specific domains, which is used in all MDMT experiments. The mixed database for EN-DE translation and DEEN translation contains 172M and 167M key-value pairs respectively. General domain performance 32 38 31 36 30 A"
2021.emnlp-main.579,C18-1111,0,0.020353,"nce. We therefore drop the most similar examples during training to reduce this discrepancy. With above improvements, KSTER shows the following advantages: • Extensive experiments show that, KSTER outperforms kNN-MT, a strong competitor, in specific domains for 1.1 to 1.5 BLEU scores while keeping the performance in general domain. • KSTER outperforms kNN-MT for 1.8 BLEU scores on average in unseen domains. Therefore, there is no strong restriction of the input domain, which makes KSTER much more practical for industry applications. 2 Related Work els for domain-specific language translation (Chu and Wang, 2018). The most popular method for this task is finetuning general domain MT models on in-domain data. However, finetuning suffers from the notorious catastrophic forgetting problem (McCloskey and Cohen, 1989; Santoro et al., 2016). There are also some sparse domain adaptation methods that only update part of the MT parameters (Bapna et al., 2019; Wuebker et al., 2018; Guo et al., 2021). In real-world translation applications, the domain labels of test examples are often not available. This dilemma inspires a closely related research area — multi-domain machine translation (Pham et al., 2021; Faraj"
2021.emnlp-main.579,W17-3204,0,0.0259316,"domain labels of examples are available in test time. In MDMT, the domain labels of examples are not available in test time, so examples from all domains are translated with one model, which is a more practical setting. 4.1 Datasets and Implementation Details Datasets We conduct experiments in EN-DE translation and DE-EN translation. We use WMT14 EN-DE dataset (Bojar et al., 2014) as general domain training data, which consists of 4.5M sentence pairs. newstest2013 and newstest2014 are used as the general domain development set and test set, respectively. 5 domain-specific datasets proposed by Koehn and Knowles (2017) and resplited by Aharoni and Goldberg (2020)2 are used to evaluate the domain-specific translation performance. The detailed statistics of the 5 datasets are shown in Table 1. Implementation Details We use joint Byte Pair Encoding (BPE) (Sennrich et al., 2016) with 30k merge operations for subword segmentation. The resulted vocabulary is shared between source and target languages. We employ Transformer Base (Vaswani et al., 2017) as the base model. Following Khandelwal et al. (2021), the normalized inputs of feed forward network in the last Transformer decoder block are used as keys to build"
2021.emnlp-main.579,D19-3019,0,0.0471854,"Missing"
2021.emnlp-main.579,W17-4713,0,0.0203522,"2018). The most popular method for this task is finetuning general domain MT models on in-domain data. However, finetuning suffers from the notorious catastrophic forgetting problem (McCloskey and Cohen, 1989; Santoro et al., 2016). There are also some sparse domain adaptation methods that only update part of the MT parameters (Bapna et al., 2019; Wuebker et al., 2018; Guo et al., 2021). In real-world translation applications, the domain labels of test examples are often not available. This dilemma inspires a closely related research area — multi-domain machine translation (Pham et al., 2021; Farajian et al., 2017; Liang et al., 2020; Lin et al., 2021; Zhu et al., 2021), where one model translates sentences from all domains. Online Adaptation of MT by Non-parametric Retrieval Non-parametric methods enable online adaptation of deployed NMT models by updating the database from which similar examples are retrieved. Traditional non-parametric methods search sentence-level examples to guide the translation process (Cao and Xiong, 2018; Gu et al., 2018; Zhang et al., 2018). Recently, n-gram level (Bapna and Firat, 2019) and token level (Khandelwal et al., 2021) retrieval are introduced and shows strong empir"
2021.emnlp-main.579,2020.emnlp-main.733,1,0.684475,"ms are used to score reference and contrastive translations. If an NMT system assign higher score to reference than all contrastive translations in an example, the NMT system is recognized as making correct prediction on this example. We use ContraWSD (Gonzales et al., 2017) 7 as the test suite, which contains 7,359 contrastive translation pairs for DE-EN translation. We encode the source sentences from ContraWSD and training data of 5 specific domains by averaged BERT embeddings (Devlin et al., 2018). Then we whiten the sentence embeddings with BERT-whitening proposed by Huang et al. (2021); Li et al. (2020). For each domain, we select 300 examples from ContraWSD that most similar to the in-domain data based on the cosine similarity of sentence embeddings. https://github.com/ZurichNLP/ContraWSD Koran domain IT Subtitles Base KSTER 0.90 0.85 0.80 0.75 AUX Medical WSD accuracy on ContraWSD 0.95 10 7 35 30 25 20 15 Law Medical Koran domain IT Subtitles Figure 9: BLEU and word sense disambiguation accuracy of base model and KSTER with Gaussian kernel on ContraWSD dataset. Kernel-smoothing helps word sense disambiguation. We evaluate the translation performance and word sense disambiguation ability of"
2021.emnlp-main.579,2021.acl-long.25,1,0.667655,"k is finetuning general domain MT models on in-domain data. However, finetuning suffers from the notorious catastrophic forgetting problem (McCloskey and Cohen, 1989; Santoro et al., 2016). There are also some sparse domain adaptation methods that only update part of the MT parameters (Bapna et al., 2019; Wuebker et al., 2018; Guo et al., 2021). In real-world translation applications, the domain labels of test examples are often not available. This dilemma inspires a closely related research area — multi-domain machine translation (Pham et al., 2021; Farajian et al., 2017; Liang et al., 2020; Lin et al., 2021; Zhu et al., 2021), where one model translates sentences from all domains. Online Adaptation of MT by Non-parametric Retrieval Non-parametric methods enable online adaptation of deployed NMT models by updating the database from which similar examples are retrieved. Traditional non-parametric methods search sentence-level examples to guide the translation process (Cao and Xiong, 2018; Gu et al., 2018; Zhang et al., 2018). Recently, n-gram level (Bapna and Firat, 2019) and token level (Khandelwal et al., 2021) retrieval are introduced and shows strong empirical results. Generally, similar examp"
2021.emnlp-main.579,P02-1040,0,0.109427,"Missing"
2021.emnlp-main.579,2021.tacl-1.2,0,0.0122763,"ion (Chu and Wang, 2018). The most popular method for this task is finetuning general domain MT models on in-domain data. However, finetuning suffers from the notorious catastrophic forgetting problem (McCloskey and Cohen, 1989; Santoro et al., 2016). There are also some sparse domain adaptation methods that only update part of the MT parameters (Bapna et al., 2019; Wuebker et al., 2018; Guo et al., 2021). In real-world translation applications, the domain labels of test examples are often not available. This dilemma inspires a closely related research area — multi-domain machine translation (Pham et al., 2021; Farajian et al., 2017; Liang et al., 2020; Lin et al., 2021; Zhu et al., 2021), where one model translates sentences from all domains. Online Adaptation of MT by Non-parametric Retrieval Non-parametric methods enable online adaptation of deployed NMT models by updating the database from which similar examples are retrieved. Traditional non-parametric methods search sentence-level examples to guide the translation process (Cao and Xiong, 2018; Gu et al., 2018; Zhang et al., 2018). Recently, n-gram level (Bapna and Firat, 2019) and token level (Khandelwal et al., 2021) retrieval are introduced"
2021.emnlp-main.579,W18-6319,0,0.0203748,"Missing"
2021.emnlp-main.579,D18-1104,0,0.0205885,"1.8 BLEU scores on average in unseen domains. Therefore, there is no strong restriction of the input domain, which makes KSTER much more practical for industry applications. 2 Related Work els for domain-specific language translation (Chu and Wang, 2018). The most popular method for this task is finetuning general domain MT models on in-domain data. However, finetuning suffers from the notorious catastrophic forgetting problem (McCloskey and Cohen, 1989; Santoro et al., 2016). There are also some sparse domain adaptation methods that only update part of the MT parameters (Bapna et al., 2019; Wuebker et al., 2018; Guo et al., 2021). In real-world translation applications, the domain labels of test examples are often not available. This dilemma inspires a closely related research area — multi-domain machine translation (Pham et al., 2021; Farajian et al., 2017; Liang et al., 2020; Lin et al., 2021; Zhu et al., 2021), where one model translates sentences from all domains. Online Adaptation of MT by Non-parametric Retrieval Non-parametric methods enable online adaptation of deployed NMT models by updating the database from which similar examples are retrieved. Traditional non-parametric methods search se"
2021.emnlp-main.579,2020.acl-main.144,0,0.0600508,"Missing"
2021.emnlp-main.579,N18-1120,0,0.09194,"l., 2016; Vaswani et al., 2017). How to effectively update a deployed NMT model and adapt to emerging cases? For example, after a generic NMT model trained on WMT data, a customer wants to use service to translate financial documents. The costomer may have a handful of translation pairs for the finance domain, but do not have the capacity to perform a full retraining. Non-parametric adaptation methods enable incorporating individual examples on-the-fly, by retrieving similar source-target pairs from an external database to guide the translation process (Bapna and Firat, 2019; Gu et al., 2018; Zhang et al., 2018; † 30 20 Introduction ∗ Base kNN-MT KSTER 50 How to effectively adapt neural machine translation (NMT) models according to emerging cases without retraining? Despite the great success of neural machine translation, updating the deployed models online remains a challenge. Existing non-parametric approaches that retrieve similar examples from a database to guide the translation process are promising but are prone to overfit the retrieved examples. However, non-parametric methods are prone to overfit the retrieved examples. In this work, we propose to learn Kernel-Smoothed Translation with Examp"
2021.emnlp-main.579,P16-1162,0,0.0436248,"asets We conduct experiments in EN-DE translation and DE-EN translation. We use WMT14 EN-DE dataset (Bojar et al., 2014) as general domain training data, which consists of 4.5M sentence pairs. newstest2013 and newstest2014 are used as the general domain development set and test set, respectively. 5 domain-specific datasets proposed by Koehn and Knowles (2017) and resplited by Aharoni and Goldberg (2020)2 are used to evaluate the domain-specific translation performance. The detailed statistics of the 5 datasets are shown in Table 1. Implementation Details We use joint Byte Pair Encoding (BPE) (Sennrich et al., 2016) with 30k merge operations for subword segmentation. The resulted vocabulary is shared between source and target languages. We employ Transformer Base (Vaswani et al., 2017) as the base model. Following Khandelwal et al. (2021), the normalized inputs of feed forward network in the last Transformer decoder block are used as keys to build the Since the database is built from the training data and KSTER is trained on the training data, similar examples can constantly be retrieved from the database during training. However, in test time, there may be no example in the database that is 2 similar to"
2021.emnlp-main.579,tiedemann-2012-parallel,0,0.0180797,"nels and different weights for different examples. EN-DE General Specific 24.72 33.08 26.06 33.40 27.80 34.02 27.74 34.38 DE-EN General Specific 25.87 36.64 27.89 37.37 31.88 37.19 31.94 37.61 Table 7: Ablation study of learnable kernel and mixing weight in KSTER with Gaussian kernel in MDMT. Both learnable kernel and learnable mixing weight bring improvement. None represents that both kernel and mixing weight are fixed, in which case KSTER degenerates to kNN-MT. KSTER with Laplacian kernel in unseen domains, which is important in real-world MDMT applications. We take Bible and QED from OPUS (Tiedemann, 2012)6 as unseen domains and randomly sample 2k examples from each domain for test. We directly use the MDMT models to translate sentences from unseen domains. The results of ENDE translation are presented in Table 6. KSTER outperforms all baselines, which shows strong generalization ability. 4.4 Inference Speed A common concern about non-parametric methods in MT is that searching similar examples may slow the inference speed. We test the inference speed KSTER in MDMT in EN-DE translation, which is the setting with the largest database. The averaged inference time in general domain and 5 specific d"
2021.emnlp-main.579,D19-1670,0,0.0233319,"general domain and 5 specific domains, which is used in all MDMT experiments. The mixed database for EN-DE translation and DEEN translation contains 172M and 167M key-value pairs respectively. General domain performance 32 38 31 36 30 Averaged domain-specific performance 34 BLEU 29 BLEU of kNN-MT changes with the size of database. In this work, we study the performance change of kNN-MT and KSTER with low-quality database. Specifically, we test the robustness of these models in DAMT when the database is noisy. We add token-level noise to the English sentences in parallel training data by EDA (Wei and Zou, 2019) 5 . For each word in a sentence, it is modified with a probability of 0.1. The candidate modifications contain synonym replacement, random insertion, random swap and random deletion with equal probability. Then we use the noisy training data to construct the noisy database. We study the effects of source side noise and target side noise on translation performance. The experiment results are presented in Table 4. Target side noise has more negative effect to translation performance than source side noise. The BLEU scores of KSTER drop less apparently in all settings, which indicates that the p"
2021.emnlp-main.95,P16-1231,0,0.0230795,"y carefully designing graphs connecting: mentions to entities, mentions in the same sentence (Christopoulou et al., 2019; Sun et al., 2019), mentions of the same entities (Wang et al., 2020; Zeng et al., 2020), etc. Nan et al. (2020); Xu et al. (2021) directly integrated similar structural dependencies to attention mechanisms in the encoder. These approaches contributed to obtaining powerful representations for distinguishing various relations but lacked interpretability on the implicit reasoning. Another approach that can capture dependencies between relations is the global normalized model (Andor et al., 2016; Sun et al., 2018). In this work, we focus on how to learn and use logic rules to capture long-range dependencies between relations. Another category of related work is logical reasoning. Many studies were conducted on learning or applying logic rules for reasoning. Most of them (Qu and Tang, 2019; Zhang et al., 2020) concentrated on reasoning over knowledge graphs, aiming to deduct new knowledge from existing triples. Neural symbolic systems (Hu et al., 2016; Wang and Poon, 2018) combined logic rules and neural • We propose a novel probabilistic model for relanetworks to beneﬁt from regulari"
2021.emnlp-main.95,N19-1423,0,0.0500728,"Missing"
2021.emnlp-main.95,P15-1061,0,0.0144566,"o the input structure, we can divide the existing document-level relation extraction work into two categories: the sequence-based model and the graph-based model. The sequence-based model ﬁrst leverages different sequence encoder (e.g., BERT (Devlin et al., Extracting relations from a document has attracted 2019), RoBERTa (Liu et al., 2019)) to obtain token signiﬁcant research attention in information extrac- representations, and then computes relation repretion (IE). Recently, instead of focusing on sentence- sentations by various pooling operations, e.g., averlevel (Socher et al., 2012; dos Santos et al., 2015; age pooling (Yao et al., 2019; Xu et al., 2021), atHan et al., 2018; Zhang et al., 2018; Wang et al., tentive pooling (Zhou et al., 2021). To further cap2021a,b), researchers have turned to modeling di- ture long-range dependencies, graph-based modrectly at the document level (Wang et al., 2019; els are proposed. By constructing a graph, words Ye et al., 2020; Zhou et al., 2021), which provides or entities that are far away can become neighbor longer context and requires more complex reason- nodes. On top of the sequence encoder, the graph ing. Early efforts focus mainly on learning a power-"
2021.emnlp-main.95,D18-1247,0,0.02449,"n extraction work into two categories: the sequence-based model and the graph-based model. The sequence-based model ﬁrst leverages different sequence encoder (e.g., BERT (Devlin et al., Extracting relations from a document has attracted 2019), RoBERTa (Liu et al., 2019)) to obtain token signiﬁcant research attention in information extrac- representations, and then computes relation repretion (IE). Recently, instead of focusing on sentence- sentations by various pooling operations, e.g., averlevel (Socher et al., 2012; dos Santos et al., 2015; age pooling (Yao et al., 2019; Xu et al., 2021), atHan et al., 2018; Zhang et al., 2018; Wang et al., tentive pooling (Zhou et al., 2021). To further cap2021a,b), researchers have turned to modeling di- ture long-range dependencies, graph-based modrectly at the document level (Wang et al., 2019; els are proposed. By constructing a graph, words Ye et al., 2020; Zhou et al., 2021), which provides or entities that are far away can become neighbor longer context and requires more complex reason- nodes. On top of the sequence encoder, the graph ing. Early efforts focus mainly on learning a power- encoder (e.g., GNN) can aggregate information ful relation (i.e., en"
2021.emnlp-main.95,P19-1423,0,0.017814,"the graph ing. Early efforts focus mainly on learning a power- encoder (e.g., GNN) can aggregate information ful relation (i.e., entity pair) representation, which from all neighbors, thus capturing longer depen∗ corresponding authors. dencies. Various forms of graphs are proposed, in† Work is done while at ByteDance. cluding dependency tree (Peng et al., 2017; Zhang 1239 1 Introduction Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 1239–1250 c November 7–11, 2021. 2021 Association for Computational Linguistics et al., 2018), co-reference graph (Sahu et al., 2019), mention-entity graph (Christopoulou et al., 2019; Zeng et al., 2020), entity-relation bipartite graph (Sun et al., 2019) and so on. Despite their great success, there is still no comprehensive understanding of the internal representations, which are often criticized as mysterious ""black boxes"". Learning logic rules can discover and represent knowledge in explicit symbolic structures that can be understood and examined by humans. At the same time, logic rules provide another way to explicitly capture interactions between entities and output relations in a document. For example in Fig. 1, the"
2021.emnlp-main.95,D12-1110,0,0.0840567,"dependencies. According to the input structure, we can divide the existing document-level relation extraction work into two categories: the sequence-based model and the graph-based model. The sequence-based model ﬁrst leverages different sequence encoder (e.g., BERT (Devlin et al., Extracting relations from a document has attracted 2019), RoBERTa (Liu et al., 2019)) to obtain token signiﬁcant research attention in information extrac- representations, and then computes relation repretion (IE). Recently, instead of focusing on sentence- sentations by various pooling operations, e.g., averlevel (Socher et al., 2012; dos Santos et al., 2015; age pooling (Yao et al., 2019; Xu et al., 2021), atHan et al., 2018; Zhang et al., 2018; Wang et al., tentive pooling (Zhou et al., 2021). To further cap2021a,b), researchers have turned to modeling di- ture long-range dependencies, graph-based modrectly at the document level (Wang et al., 2019; els are proposed. By constructing a graph, words Ye et al., 2020; Zhou et al., 2021), which provides or entities that are far away can become neighbor longer context and requires more complex reason- nodes. On top of the sequence encoder, the graph ing. Early efforts focus ma"
2021.emnlp-main.95,P16-1228,0,0.0103035,"tability on the implicit reasoning. Another approach that can capture dependencies between relations is the global normalized model (Andor et al., 2016; Sun et al., 2018). In this work, we focus on how to learn and use logic rules to capture long-range dependencies between relations. Another category of related work is logical reasoning. Many studies were conducted on learning or applying logic rules for reasoning. Most of them (Qu and Tang, 2019; Zhang et al., 2020) concentrated on reasoning over knowledge graphs, aiming to deduct new knowledge from existing triples. Neural symbolic systems (Hu et al., 2016; Wang and Poon, 2018) combined logic rules and neural • We propose a novel probabilistic model for relanetworks to beneﬁt from regularization on deep tion extraction by learning logic rules. The model learning approaches. These efforts demonstrated can explicitly capture dependencies between entithe effectiveness of integrating neural networks ties and output relations, while enjoy better interwith logical reasoning. Despite doc-RE providing pretation. a suitable scenario for logical reasoning (with rela• We propose an efﬁcient iterative-based method tions serving as predicates and entities a"
2021.emnlp-main.95,P19-1131,1,0.902418,"(i.e., entity pair) representation, which from all neighbors, thus capturing longer depen∗ corresponding authors. dencies. Various forms of graphs are proposed, in† Work is done while at ByteDance. cluding dependency tree (Peng et al., 2017; Zhang 1239 1 Introduction Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 1239–1250 c November 7–11, 2021. 2021 Association for Computational Linguistics et al., 2018), co-reference graph (Sahu et al., 2019), mention-entity graph (Christopoulou et al., 2019; Zeng et al., 2020), entity-relation bipartite graph (Sun et al., 2019) and so on. Despite their great success, there is still no comprehensive understanding of the internal representations, which are often criticized as mysterious ""black boxes"". Learning logic rules can discover and represent knowledge in explicit symbolic structures that can be understood and examined by humans. At the same time, logic rules provide another way to explicitly capture interactions between entities and output relations in a document. For example in Fig. 1, the identiﬁcation of royalty_of(Kate,UK) requires information in all three sentences. The demonstrated logic rule can be appli"
2021.emnlp-main.95,2021.ccl-1.108,0,0.0374173,"Missing"
2021.emnlp-main.95,D18-1249,1,0.795996,"g graphs connecting: mentions to entities, mentions in the same sentence (Christopoulou et al., 2019; Sun et al., 2019), mentions of the same entities (Wang et al., 2020; Zeng et al., 2020), etc. Nan et al. (2020); Xu et al. (2021) directly integrated similar structural dependencies to attention mechanisms in the encoder. These approaches contributed to obtaining powerful representations for distinguishing various relations but lacked interpretability on the implicit reasoning. Another approach that can capture dependencies between relations is the global normalized model (Andor et al., 2016; Sun et al., 2018). In this work, we focus on how to learn and use logic rules to capture long-range dependencies between relations. Another category of related work is logical reasoning. Many studies were conducted on learning or applying logic rules for reasoning. Most of them (Qu and Tang, 2019; Zhang et al., 2020) concentrated on reasoning over knowledge graphs, aiming to deduct new knowledge from existing triples. Neural symbolic systems (Hu et al., 2016; Wang and Poon, 2018) combined logic rules and neural • We propose a novel probabilistic model for relanetworks to beneﬁt from regularization on deep tion"
2021.emnlp-main.95,2020.acl-main.141,0,0.0182973,"powerful representations, they introduced pre-trained language models (Wang et al., 2019; Ye et al., 2020), leveraged attentions for context pooling (Zhou et al., 2021), or integrated the scattered information according to a hierarchical level (Tang et al., 2020). Aiming to model the intrinsic interactions among entities and relations, they utilized implicit reasoning structures by carefully designing graphs connecting: mentions to entities, mentions in the same sentence (Christopoulou et al., 2019; Sun et al., 2019), mentions of the same entities (Wang et al., 2020; Zeng et al., 2020), etc. Nan et al. (2020); Xu et al. (2021) directly integrated similar structural dependencies to attention mechanisms in the encoder. These approaches contributed to obtaining powerful representations for distinguishing various relations but lacked interpretability on the implicit reasoning. Another approach that can capture dependencies between relations is the global normalized model (Andor et al., 2016; Sun et al., 2018). In this work, we focus on how to learn and use logic rules to capture long-range dependencies between relations. Another category of related work is logical reasoning. Many studies were conducte"
2021.emnlp-main.95,W18-2314,0,0.0229799,"works, LogiRE can explicitly capture long-range dependencies between entities and output relations in a document and enjoy better interpretation. Our main contributions are listed below: to optimize LogiRE based on the EM algorithm. • Empirical results show that LogiRE signiﬁcantly outperforms several strong baselines in terms of relation performance (∼1.8 F1 score) and logical consistency (over 3.3 logic score). 2 Related Work For document-level relation extraction, prior efforts on capturing long-range dependencies mainly focused on two directions: pursuing stronger sequence representation (Nguyen and Verspoor, 2018; Verga et al., 2018; Zheng et al., 2018) or including prior for interactions among entities as graphs (Christopoulou et al., 2019). For more powerful representations, they introduced pre-trained language models (Wang et al., 2019; Ye et al., 2020), leveraged attentions for context pooling (Zhou et al., 2021), or integrated the scattered information according to a hierarchical level (Tang et al., 2020). Aiming to model the intrinsic interactions among entities and relations, they utilized implicit reasoning structures by carefully designing graphs connecting: mentions to entities, mentions in"
2021.emnlp-main.95,Q17-1008,0,0.0233666,"g et al., 2019; els are proposed. By constructing a graph, words Ye et al., 2020; Zhou et al., 2021), which provides or entities that are far away can become neighbor longer context and requires more complex reason- nodes. On top of the sequence encoder, the graph ing. Early efforts focus mainly on learning a power- encoder (e.g., GNN) can aggregate information ful relation (i.e., entity pair) representation, which from all neighbors, thus capturing longer depen∗ corresponding authors. dencies. Various forms of graphs are proposed, in† Work is done while at ByteDance. cluding dependency tree (Peng et al., 2017; Zhang 1239 1 Introduction Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 1239–1250 c November 7–11, 2021. 2021 Association for Computational Linguistics et al., 2018), co-reference graph (Sahu et al., 2019), mention-entity graph (Christopoulou et al., 2019; Zeng et al., 2020), entity-relation bipartite graph (Sun et al., 2019) and so on. Despite their great success, there is still no comprehensive understanding of the internal representations, which are often criticized as mysterious ""black boxes"". Learning logic rules can discover and represent"
2021.emnlp-main.95,N18-1080,0,0.0175289,"y capture long-range dependencies between entities and output relations in a document and enjoy better interpretation. Our main contributions are listed below: to optimize LogiRE based on the EM algorithm. • Empirical results show that LogiRE signiﬁcantly outperforms several strong baselines in terms of relation performance (∼1.8 F1 score) and logical consistency (over 3.3 logic score). 2 Related Work For document-level relation extraction, prior efforts on capturing long-range dependencies mainly focused on two directions: pursuing stronger sequence representation (Nguyen and Verspoor, 2018; Verga et al., 2018; Zheng et al., 2018) or including prior for interactions among entities as graphs (Christopoulou et al., 2019). For more powerful representations, they introduced pre-trained language models (Wang et al., 2019; Ye et al., 2020), leveraged attentions for context pooling (Zhou et al., 2021), or integrated the scattered information according to a hierarchical level (Tang et al., 2020). Aiming to model the intrinsic interactions among entities and relations, they utilized implicit reasoning structures by carefully designing graphs connecting: mentions to entities, mentions in the same sentence (C"
2021.emnlp-main.95,2020.emnlp-main.303,0,0.161834,"raphs (Christopoulou et al., 2019). For more powerful representations, they introduced pre-trained language models (Wang et al., 2019; Ye et al., 2020), leveraged attentions for context pooling (Zhou et al., 2021), or integrated the scattered information according to a hierarchical level (Tang et al., 2020). Aiming to model the intrinsic interactions among entities and relations, they utilized implicit reasoning structures by carefully designing graphs connecting: mentions to entities, mentions in the same sentence (Christopoulou et al., 2019; Sun et al., 2019), mentions of the same entities (Wang et al., 2020; Zeng et al., 2020), etc. Nan et al. (2020); Xu et al. (2021) directly integrated similar structural dependencies to attention mechanisms in the encoder. These approaches contributed to obtaining powerful representations for distinguishing various relations but lacked interpretability on the implicit reasoning. Another approach that can capture dependencies between relations is the global normalized model (Andor et al., 2016; Sun et al., 2018). In this work, we focus on how to learn and use logic rules to capture long-range dependencies between relations. Another category of related work is l"
2021.emnlp-main.95,D18-1215,0,0.0247203,"mplicit reasoning. Another approach that can capture dependencies between relations is the global normalized model (Andor et al., 2016; Sun et al., 2018). In this work, we focus on how to learn and use logic rules to capture long-range dependencies between relations. Another category of related work is logical reasoning. Many studies were conducted on learning or applying logic rules for reasoning. Most of them (Qu and Tang, 2019; Zhang et al., 2020) concentrated on reasoning over knowledge graphs, aiming to deduct new knowledge from existing triples. Neural symbolic systems (Hu et al., 2016; Wang and Poon, 2018) combined logic rules and neural • We propose a novel probabilistic model for relanetworks to beneﬁt from regularization on deep tion extraction by learning logic rules. The model learning approaches. These efforts demonstrated can explicitly capture dependencies between entithe effectiveness of integrating neural networks ties and output relations, while enjoy better interwith logical reasoning. Despite doc-RE providing pretation. a suitable scenario for logical reasoning (with rela• We propose an efﬁcient iterative-based method tions serving as predicates and entities as variables), 1240 Fig"
2021.emnlp-main.95,2021.eacl-main.251,1,0.837568,"Missing"
2021.emnlp-main.95,2021.acl-long.19,1,0.822672,"Missing"
2021.emnlp-main.95,2020.emnlp-main.453,0,0.0620715,"Missing"
2021.emnlp-main.95,P19-1074,0,0.229234,"de the existing document-level relation extraction work into two categories: the sequence-based model and the graph-based model. The sequence-based model ﬁrst leverages different sequence encoder (e.g., BERT (Devlin et al., Extracting relations from a document has attracted 2019), RoBERTa (Liu et al., 2019)) to obtain token signiﬁcant research attention in information extrac- representations, and then computes relation repretion (IE). Recently, instead of focusing on sentence- sentations by various pooling operations, e.g., averlevel (Socher et al., 2012; dos Santos et al., 2015; age pooling (Yao et al., 2019; Xu et al., 2021), atHan et al., 2018; Zhang et al., 2018; Wang et al., tentive pooling (Zhou et al., 2021). To further cap2021a,b), researchers have turned to modeling di- ture long-range dependencies, graph-based modrectly at the document level (Wang et al., 2019; els are proposed. By constructing a graph, words Ye et al., 2020; Zhou et al., 2021), which provides or entities that are far away can become neighbor longer context and requires more complex reason- nodes. On top of the sequence encoder, the graph ing. Early efforts focus mainly on learning a power- encoder (e.g., GNN) can aggreg"
2021.emnlp-main.95,2020.emnlp-main.582,0,0.11188,"ant research attention in information extrac- representations, and then computes relation repretion (IE). Recently, instead of focusing on sentence- sentations by various pooling operations, e.g., averlevel (Socher et al., 2012; dos Santos et al., 2015; age pooling (Yao et al., 2019; Xu et al., 2021), atHan et al., 2018; Zhang et al., 2018; Wang et al., tentive pooling (Zhou et al., 2021). To further cap2021a,b), researchers have turned to modeling di- ture long-range dependencies, graph-based modrectly at the document level (Wang et al., 2019; els are proposed. By constructing a graph, words Ye et al., 2020; Zhou et al., 2021), which provides or entities that are far away can become neighbor longer context and requires more complex reason- nodes. On top of the sequence encoder, the graph ing. Early efforts focus mainly on learning a power- encoder (e.g., GNN) can aggregate information ful relation (i.e., entity pair) representation, which from all neighbors, thus capturing longer depen∗ corresponding authors. dencies. Various forms of graphs are proposed, in† Work is done while at ByteDance. cluding dependency tree (Peng et al., 2017; Zhang 1239 1 Introduction Proceedings of the 2021 Conference"
2021.emnlp-main.95,2020.emnlp-main.127,1,0.915686,"r (e.g., GNN) can aggregate information ful relation (i.e., entity pair) representation, which from all neighbors, thus capturing longer depen∗ corresponding authors. dencies. Various forms of graphs are proposed, in† Work is done while at ByteDance. cluding dependency tree (Peng et al., 2017; Zhang 1239 1 Introduction Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 1239–1250 c November 7–11, 2021. 2021 Association for Computational Linguistics et al., 2018), co-reference graph (Sahu et al., 2019), mention-entity graph (Christopoulou et al., 2019; Zeng et al., 2020), entity-relation bipartite graph (Sun et al., 2019) and so on. Despite their great success, there is still no comprehensive understanding of the internal representations, which are often criticized as mysterious ""black boxes"". Learning logic rules can discover and represent knowledge in explicit symbolic structures that can be understood and examined by humans. At the same time, logic rules provide another way to explicitly capture interactions between entities and output relations in a document. For example in Fig. 1, the identiﬁcation of royalty_of(Kate,UK) requires information in all three"
2021.emnlp-main.95,D18-1244,0,0.0198302,"into two categories: the sequence-based model and the graph-based model. The sequence-based model ﬁrst leverages different sequence encoder (e.g., BERT (Devlin et al., Extracting relations from a document has attracted 2019), RoBERTa (Liu et al., 2019)) to obtain token signiﬁcant research attention in information extrac- representations, and then computes relation repretion (IE). Recently, instead of focusing on sentence- sentations by various pooling operations, e.g., averlevel (Socher et al., 2012; dos Santos et al., 2015; age pooling (Yao et al., 2019; Xu et al., 2021), atHan et al., 2018; Zhang et al., 2018; Wang et al., tentive pooling (Zhou et al., 2021). To further cap2021a,b), researchers have turned to modeling di- ture long-range dependencies, graph-based modrectly at the document level (Wang et al., 2019; els are proposed. By constructing a graph, words Ye et al., 2020; Zhou et al., 2021), which provides or entities that are far away can become neighbor longer context and requires more complex reason- nodes. On top of the sequence encoder, the graph ing. Early efforts focus mainly on learning a power- encoder (e.g., GNN) can aggregate information ful relation (i.e., entity pair) represent"
2021.findings-acl.195,D19-5304,0,0.024143,"s demonstrate that the shared semantic space indeed conveys common knowledge between these two tasks and thus paves a new way for augmenting training resources across modalities. 1 1 Introduction Speech-to-text translation (ST) takes speech input in a source language and outputs text utterance in a target language. It has many real-world applications, including automatic video captioning, simultaneous translation for international conferences, etc. Traditional ST approaches cascade automatic speech recognition (ASR) and machine translation (MT) (Sperber et al., 2017, 2019; Zhang et al., 2019; Beck et al., 2019; Cheng et al., 2019). However, cascaded models often suffer from the issues of error propagation and translation latency. As a result, there have been a series of recent attempts on end-to-end speech-to-text translation (Liu et al., 1 All codes, data, and resources will be made released at https://github.com/Glaciohound/Chimera-SLT. 2019, 2018; Weiss et al., 2017; B´erard et al., 2018; Duong et al., 2016; Jia et al., 2019; Dong et al., 2021b; Wang et al., 2020b). The end-to-end approaches learn a single unified model, which is easier to deploy, has lower latency and could potentially reduce e"
2021.findings-acl.195,P18-1163,0,0.0279739,"Missing"
2021.findings-acl.195,N19-1202,0,0.0291476,"Missing"
2021.findings-acl.195,W19-6603,0,0.02628,"Missing"
2021.findings-acl.195,N16-1109,0,0.159703,"anslation for international conferences, etc. Traditional ST approaches cascade automatic speech recognition (ASR) and machine translation (MT) (Sperber et al., 2017, 2019; Zhang et al., 2019; Beck et al., 2019; Cheng et al., 2019). However, cascaded models often suffer from the issues of error propagation and translation latency. As a result, there have been a series of recent attempts on end-to-end speech-to-text translation (Liu et al., 1 All codes, data, and resources will be made released at https://github.com/Glaciohound/Chimera-SLT. 2019, 2018; Weiss et al., 2017; B´erard et al., 2018; Duong et al., 2016; Jia et al., 2019; Dong et al., 2021b; Wang et al., 2020b). The end-to-end approaches learn a single unified model, which is easier to deploy, has lower latency and could potentially reduce errors. However, it remains a challenge for end-to-end ST to catch up with their cascaded counterparts in performance. We argue that the root cause is the gap between the two modalities, speech and text. Although they both encode human languages, they are dissimilar in both coding attributes (pitch, volume, and intonation versus words, affixes, and punctuation) and length (thousands of time frames versus t"
2021.findings-acl.195,E09-1030,0,0.0544836,"r MT data. However, they both lack pivotal modules in model design to semantically bridge the gap between audio and text, and could thus suffer from modality mismatch in representations. Cascaded ST The cascaded method is a more long-standing trend in ST (Sperber et al., 2017; Jan et al., 2018). To alleviate its innate problem of error propagation, Cheng et al. (2018, 2019) introduce synthetic ASR-related errors and perturbations. On the other hand, some post-processing techniques such as re-segmentation (Matusov et al., 2006), punctuation restoration (F¨ugen, 2008), and disfluency detection (Fitzgerald et al., 2009) are proposed to fix flaws or errors that occurred during the translation. Cross-Lingual Techniques Techniques in multilingual tasks is also related to ours, as they aim at extracting common features out of sources from different representations (which, in this case, is language diversity) as well. However, multilingualism lacks key difficulties as observed in audio-text modality gap as discussed before. (Lu et al., 2018) and (Vazquez Carrillo et al., 2019) are early attempts by building an LSTM-based attentional interlingua. Yu et al. (2018); Yang et al. (2019) uses a similar cosine-based los"
2021.findings-acl.195,2020.acl-demos.34,0,0.190402,"erive a novel bi-modal contrastive training task to learn an alignment between semantic memories of two modalities. Finally, Chimera achieves a new state-of-the-art performance on the MuST-C benchmark and demonstrates its efficacy in learning modality-agnostic semantic representations. 2 Related Work End-to-end ST Since its first proof-of-concept work (B´erard et al., 2016; Duong et al., 2016), solving Speech Translation in an end-to-end manner has attracted extensive attention (Vila et al., 2018; Salesky et al., 2018, 2019; Di Gangi et al., 2019b; Bahar et al., 2019a; Di Gangi et al., 2019c; Inaguma et al., 2020). Standard training techniques such as pretraining (Weiss et al., 2017; B´erard et al., 2018; Bansal et al., 2018; Stoian et al., 2020; Wang et al., 2020a; Pino et al., 2020), multi-task training (Vydana et al., 2021; Le et al., 2020; Tang et al., 2021), meta-learning (Indurthi et al., 2019), and curriculum learning (Kano et al., 2018; Wang et al., 2020b) have been applied. As ST data are expensive to collect, Jia et al. (2019); Pino et al. (2019); Bahar et al. (2019b) augment synthesized data from ASR and MT corpora. Methods utilizing trained models, such as knowledge distillation (Liu et al."
2021.findings-acl.195,L18-1001,0,0.0250668,"s. MuST-C contains translations from English (EN) to 8 languages: Dutch (NL), French (FR), German (DE), Italian (IT), Portuguese (PT), Romanian (RO), Russian (RU), and Spanish (ES). With each pair consisting of at least 385 hours of audio recordings, to the best of our knowledge, MuST-C is currently the largest speech translation dataset available for each language pair. It includes data from English TED talks with manual transcripts and translations at the sentence level. We use the dev and tst-COMMON sets as our development and test data, respectively. Augmented LibriSpeech Dataset (En-Fr) (Kocabiyikoglu et al., 2018) is composed of aligned e-books in French and their human reading in English. It provides typical triplet data of English speech, transcript and French text. Following the setting of (Liu et al., 2019), we utilize the 100h hours of clean train set as training data, and use the original 2 hours of dev set and and 4 hours of test set. Machine Translation Datasets After bridging the modality gap, Chimera has the potential power to utilize Machine Translation resources. Therefore we incorporate data from WMT, OpenSubtitles (Lison and Tiedemann, 2016) and OPUS100 (Zhang et al., 2020b) translation t"
2021.findings-acl.195,2020.coling-main.314,0,0.288628,"ing modality-agnostic semantic representations. 2 Related Work End-to-end ST Since its first proof-of-concept work (B´erard et al., 2016; Duong et al., 2016), solving Speech Translation in an end-to-end manner has attracted extensive attention (Vila et al., 2018; Salesky et al., 2018, 2019; Di Gangi et al., 2019b; Bahar et al., 2019a; Di Gangi et al., 2019c; Inaguma et al., 2020). Standard training techniques such as pretraining (Weiss et al., 2017; B´erard et al., 2018; Bansal et al., 2018; Stoian et al., 2020; Wang et al., 2020a; Pino et al., 2020), multi-task training (Vydana et al., 2021; Le et al., 2020; Tang et al., 2021), meta-learning (Indurthi et al., 2019), and curriculum learning (Kano et al., 2018; Wang et al., 2020b) have been applied. As ST data are expensive to collect, Jia et al. (2019); Pino et al. (2019); Bahar et al. (2019b) augment synthesized data from ASR and MT corpora. Methods utilizing trained models, such as knowledge distillation (Liu et al., 2019) and model adaptation (Di Gangi et al., 2020), have also been shown to be effective. Among these attempts, (Indurthi et al., 2019; Le et al., 2020; Liu et al., 2020) are most related to ours, as they also attempt to train mode"
2021.findings-acl.195,W18-6309,0,0.0126882,". On the other hand, some post-processing techniques such as re-segmentation (Matusov et al., 2006), punctuation restoration (F¨ugen, 2008), and disfluency detection (Fitzgerald et al., 2009) are proposed to fix flaws or errors that occurred during the translation. Cross-Lingual Techniques Techniques in multilingual tasks is also related to ours, as they aim at extracting common features out of sources from different representations (which, in this case, is language diversity) as well. However, multilingualism lacks key difficulties as observed in audio-text modality gap as discussed before. (Lu et al., 2018) and (Vazquez Carrillo et al., 2019) are early attempts by building an LSTM-based attentional interlingua. Yu et al. (2018); Yang et al. (2019) uses a similar cosine-based loss for multilingual training. Zhu et al. (2020) is probably more similar in method to ours, but Chimera is more simple in terms of model and objectives, and the memories in Chimera are additionally designed to focus on specific semantic categories. 3 3.1 Proposed Method: Text-Speech Shared Semantic Memory Network Speech Translation Overview An ST corpus usually consists of a set of triplet data S = {(xi , zi , yi )}. Here"
2021.findings-acl.195,2006.iwslt-papers.1,0,0.0870071,"Liu et al., 2020) are most related to ours, as they also attempt to train models on ASR or MT data. However, they both lack pivotal modules in model design to semantically bridge the gap between audio and text, and could thus suffer from modality mismatch in representations. Cascaded ST The cascaded method is a more long-standing trend in ST (Sperber et al., 2017; Jan et al., 2018). To alleviate its innate problem of error propagation, Cheng et al. (2018, 2019) introduce synthetic ASR-related errors and perturbations. On the other hand, some post-processing techniques such as re-segmentation (Matusov et al., 2006), punctuation restoration (F¨ugen, 2008), and disfluency detection (Fitzgerald et al., 2009) are proposed to fix flaws or errors that occurred during the translation. Cross-Lingual Techniques Techniques in multilingual tasks is also related to ours, as they aim at extracting common features out of sources from different representations (which, in this case, is language diversity) as well. However, multilingualism lacks key difficulties as observed in audio-text modality gap as discussed before. (Lu et al., 2018) and (Vazquez Carrillo et al., 2019) are early attempts by building an LSTM-based a"
2021.findings-acl.195,N19-4009,0,0.0504683,"× X X X X × × × × × × X × × × × × × × × X × × X X 22.7 22.9 22.4 23.6 23.1 22.1 25.2 22.3 25.6 27.1 • 32.9 32.8 31.6 33.5 34.1 34.5 34.3 35.0 35.6 15.3 15.8 14.7 15.2 15.8 16.7 17.4 27.2 28.0 26.9 28.1 28.7 30.2 30.6 22.7 23.8 23.0 24.2 24.2 24.0 25.0 21.9 21.9 21.0 22.9 22.4 23.2 24.0 28.1 28.0 26.3 30.0 29.3 29.7 30.2 27.3 27.4 24.9 27.6 28.2 28.5 29.2 Table 1: Main results on tst-COMMON subset on all 8 languages in MuST-C dataset. “Speech” denotes unlabeled audio data. • : the result uses a mixed WMT14+OpenSubtitles data for MT pre-training. EN-DE Among the baselines, † shows results from Ott et al. (2019), ‡ from Inaguma et al. (2020), ? from Zhang et al. (2020a), ♦ from Le et al. (2020), ] from Liu et al. (2020), [ from Indurthi et al. (2019), and ◦ from Pino et al. (2019). ∗ shows results of a simple baseline model by combining a Wav2Vec2 module (Baevski et al., 2020) and a Transformer model, which could be viewed as the “no external data” version of Chimera. External Data Speech ASR MT Model ∗ EN-FR W2V2-T TCEN † LSTM ‡ AFS ◦ Multilingual ? Transformer ⊥ Curiculum ⊥ COSTT [ LUT ♦ STAST ] X × × × × × × × × × × × X × X X X × X X × × X × × × × X × × 6.4 17.1 17.0 17.2 17.6 17.7 18.0 18.2 18.3"
2021.findings-acl.195,P02-1040,0,0.109641,"plit from words, and normalized. Non-print punctuation is removed. The sentences are then tokenized with Moses tokenizer 5 . We filter out samples whose number of source or target tokens is over 250 and whose ratio of source and target text lengths is outside range [2/3, 3/2]. For sub-wording, we use a unigram sentencepiece6 model with a dictionary size of 10000. On each translation direction, The sentencepiece model is learned on all text data from both ST and MT corpora. The dictionary is shared across MT and ST and across source and target languages. The performance is evaluated with BLEU (Papineni et al., 2002) using sacreBLEU 7 . We average 5 https://github.com/mosessmt/mosesdecoder/blob/master/scripts/tokenizer/tokenizer.perl 6 https://github.com/google/sentencepiece 7 https://github.com/mjpost/sacrebleu, with configuration 2218 S.S. Projection Decoder EN-DE EN-FR MT Contrastive EN-DE EN-FR Fixed Fixed Fixed Fixed 25.6 24.3 24.2 23.8 35.0 34.3 33.4 33.1 X X × × X × X × 25.6 25.0 24.7 25.1 35.0 34.6 34.6 34.6 Table 4: Performance of Mem-16 Chimera when freezing different modules in fine-tuning. S.S. Projection is abbreviation for shared semantic projection. “Fixed” indicates that weights in this mo"
2021.findings-acl.195,2020.autosimtrans-1.2,0,0.0353742,"on the MuST-C benchmark and demonstrates its efficacy in learning modality-agnostic semantic representations. 2 Related Work End-to-end ST Since its first proof-of-concept work (B´erard et al., 2016; Duong et al., 2016), solving Speech Translation in an end-to-end manner has attracted extensive attention (Vila et al., 2018; Salesky et al., 2018, 2019; Di Gangi et al., 2019b; Bahar et al., 2019a; Di Gangi et al., 2019c; Inaguma et al., 2020). Standard training techniques such as pretraining (Weiss et al., 2017; B´erard et al., 2018; Bansal et al., 2018; Stoian et al., 2020; Wang et al., 2020a; Pino et al., 2020), multi-task training (Vydana et al., 2021; Le et al., 2020; Tang et al., 2021), meta-learning (Indurthi et al., 2019), and curriculum learning (Kano et al., 2018; Wang et al., 2020b) have been applied. As ST data are expensive to collect, Jia et al. (2019); Pino et al. (2019); Bahar et al. (2019b) augment synthesized data from ASR and MT corpora. Methods utilizing trained models, such as knowledge distillation (Liu et al., 2019) and model adaptation (Di Gangi et al., 2020), have also been shown to be effective. Among these attempts, (Indurthi et al., 2019; Le et al., 2020; Liu et al., 2020) a"
2021.findings-acl.195,N19-1285,0,0.0281362,"Missing"
2021.findings-acl.195,L16-1147,0,0.0153501,"tively. Augmented LibriSpeech Dataset (En-Fr) (Kocabiyikoglu et al., 2018) is composed of aligned e-books in French and their human reading in English. It provides typical triplet data of English speech, transcript and French text. Following the setting of (Liu et al., 2019), we utilize the 100h hours of clean train set as training data, and use the original 2 hours of dev set and and 4 hours of test set. Machine Translation Datasets After bridging the modality gap, Chimera has the potential power to utilize Machine Translation resources. Therefore we incorporate data from WMT, OpenSubtitles (Lison and Tiedemann, 2016) and OPUS100 (Zhang et al., 2020b) translation tasks. Specifically, we use WMT 2014 (Bojar et al., 2014) 2 for EN-DE, EN-FR, EN-RU and EN-ES, WMT 2016 (Bojar et al., 2016) 3 for EN-RO, and OPUS100 4 for 2 downloadable at http://www.statmt.org/wmt14/translationtask.html 3 downloadable at https://www.statmt.org/wmt16/translationtask.html 4 downloadable at http://opus.nlpl.eu/opus-100.php 2217 External Data MuST-C EN-X Speech ASR MT EN-DE EN-FR EN-RU EN-ES EN-IT EN-RO EN-PT EN-NL Model FairSeq ST † Espnet ST ‡ AFS ? Dual-Decoder ♦ STATST ] MAML [ Self-Training ◦ W2V2-Transformer ∗ Chimera Mem-16"
2021.findings-acl.195,D17-1145,0,0.0950134,"a +1.9 BLEU margin. Further experimental analyses demonstrate that the shared semantic space indeed conveys common knowledge between these two tasks and thus paves a new way for augmenting training resources across modalities. 1 1 Introduction Speech-to-text translation (ST) takes speech input in a source language and outputs text utterance in a target language. It has many real-world applications, including automatic video captioning, simultaneous translation for international conferences, etc. Traditional ST approaches cascade automatic speech recognition (ASR) and machine translation (MT) (Sperber et al., 2017, 2019; Zhang et al., 2019; Beck et al., 2019; Cheng et al., 2019). However, cascaded models often suffer from the issues of error propagation and translation latency. As a result, there have been a series of recent attempts on end-to-end speech-to-text translation (Liu et al., 1 All codes, data, and resources will be made released at https://github.com/Glaciohound/Chimera-SLT. 2019, 2018; Weiss et al., 2017; B´erard et al., 2018; Duong et al., 2016; Jia et al., 2019; Dong et al., 2021b; Wang et al., 2020b). The end-to-end approaches learn a single unified model, which is easier to deploy, has"
2021.findings-acl.195,2020.iwslt-1.8,0,0.0534939,"Missing"
2021.findings-acl.195,P19-1115,0,0.0277419,"Missing"
2021.findings-acl.195,W19-4305,0,0.0177039,"-processing techniques such as re-segmentation (Matusov et al., 2006), punctuation restoration (F¨ugen, 2008), and disfluency detection (Fitzgerald et al., 2009) are proposed to fix flaws or errors that occurred during the translation. Cross-Lingual Techniques Techniques in multilingual tasks is also related to ours, as they aim at extracting common features out of sources from different representations (which, in this case, is language diversity) as well. However, multilingualism lacks key difficulties as observed in audio-text modality gap as discussed before. (Lu et al., 2018) and (Vazquez Carrillo et al., 2019) are early attempts by building an LSTM-based attentional interlingua. Yu et al. (2018); Yang et al. (2019) uses a similar cosine-based loss for multilingual training. Zhu et al. (2020) is probably more similar in method to ours, but Chimera is more simple in terms of model and objectives, and the memories in Chimera are additionally designed to focus on specific semantic categories. 3 3.1 Proposed Method: Text-Speech Shared Semantic Memory Network Speech Translation Overview An ST corpus usually consists of a set of triplet data S = {(xi , zi , yi )}. Here xi is the audio wave sequence, zi is"
2021.findings-acl.195,W18-3023,0,0.0162589,"n (F¨ugen, 2008), and disfluency detection (Fitzgerald et al., 2009) are proposed to fix flaws or errors that occurred during the translation. Cross-Lingual Techniques Techniques in multilingual tasks is also related to ours, as they aim at extracting common features out of sources from different representations (which, in this case, is language diversity) as well. However, multilingualism lacks key difficulties as observed in audio-text modality gap as discussed before. (Lu et al., 2018) and (Vazquez Carrillo et al., 2019) are early attempts by building an LSTM-based attentional interlingua. Yu et al. (2018); Yang et al. (2019) uses a similar cosine-based loss for multilingual training. Zhu et al. (2020) is probably more similar in method to ours, but Chimera is more simple in terms of model and objectives, and the memories in Chimera are additionally designed to focus on specific semantic categories. 3 3.1 Proposed Method: Text-Speech Shared Semantic Memory Network Speech Translation Overview An ST corpus usually consists of a set of triplet data S = {(xi , zi , yi )}. Here xi is the audio wave sequence, zi is the transcript sequence and yi is the translation sequence in the target language. As"
2021.findings-acl.195,2020.findings-emnlp.230,0,0.170183,"n-Fr) (Kocabiyikoglu et al., 2018) is composed of aligned e-books in French and their human reading in English. It provides typical triplet data of English speech, transcript and French text. Following the setting of (Liu et al., 2019), we utilize the 100h hours of clean train set as training data, and use the original 2 hours of dev set and and 4 hours of test set. Machine Translation Datasets After bridging the modality gap, Chimera has the potential power to utilize Machine Translation resources. Therefore we incorporate data from WMT, OpenSubtitles (Lison and Tiedemann, 2016) and OPUS100 (Zhang et al., 2020b) translation tasks. Specifically, we use WMT 2014 (Bojar et al., 2014) 2 for EN-DE, EN-FR, EN-RU and EN-ES, WMT 2016 (Bojar et al., 2016) 3 for EN-RO, and OPUS100 4 for 2 downloadable at http://www.statmt.org/wmt14/translationtask.html 3 downloadable at https://www.statmt.org/wmt16/translationtask.html 4 downloadable at http://opus.nlpl.eu/opus-100.php 2217 External Data MuST-C EN-X Speech ASR MT EN-DE EN-FR EN-RU EN-ES EN-IT EN-RO EN-PT EN-NL Model FairSeq ST † Espnet ST ‡ AFS ? Dual-Decoder ♦ STATST ] MAML [ Self-Training ◦ W2V2-Transformer ∗ Chimera Mem-16 Chimera × × × × × × X X X X × ×"
2021.findings-acl.195,2020.acl-main.148,0,0.0607551,"n-Fr) (Kocabiyikoglu et al., 2018) is composed of aligned e-books in French and their human reading in English. It provides typical triplet data of English speech, transcript and French text. Following the setting of (Liu et al., 2019), we utilize the 100h hours of clean train set as training data, and use the original 2 hours of dev set and and 4 hours of test set. Machine Translation Datasets After bridging the modality gap, Chimera has the potential power to utilize Machine Translation resources. Therefore we incorporate data from WMT, OpenSubtitles (Lison and Tiedemann, 2016) and OPUS100 (Zhang et al., 2020b) translation tasks. Specifically, we use WMT 2014 (Bojar et al., 2014) 2 for EN-DE, EN-FR, EN-RU and EN-ES, WMT 2016 (Bojar et al., 2016) 3 for EN-RO, and OPUS100 4 for 2 downloadable at http://www.statmt.org/wmt14/translationtask.html 3 downloadable at https://www.statmt.org/wmt16/translationtask.html 4 downloadable at http://opus.nlpl.eu/opus-100.php 2217 External Data MuST-C EN-X Speech ASR MT EN-DE EN-FR EN-RU EN-ES EN-IT EN-RO EN-PT EN-NL Model FairSeq ST † Espnet ST ‡ AFS ? Dual-Decoder ♦ STATST ] MAML [ Self-Training ◦ W2V2-Transformer ∗ Chimera Mem-16 Chimera × × × × × × X X X X × ×"
2021.findings-acl.195,P19-1649,0,0.0304663,"Missing"
2021.findings-acl.195,2020.acl-main.150,0,0.0133604,"errors that occurred during the translation. Cross-Lingual Techniques Techniques in multilingual tasks is also related to ours, as they aim at extracting common features out of sources from different representations (which, in this case, is language diversity) as well. However, multilingualism lacks key difficulties as observed in audio-text modality gap as discussed before. (Lu et al., 2018) and (Vazquez Carrillo et al., 2019) are early attempts by building an LSTM-based attentional interlingua. Yu et al. (2018); Yang et al. (2019) uses a similar cosine-based loss for multilingual training. Zhu et al. (2020) is probably more similar in method to ours, but Chimera is more simple in terms of model and objectives, and the memories in Chimera are additionally designed to focus on specific semantic categories. 3 3.1 Proposed Method: Text-Speech Shared Semantic Memory Network Speech Translation Overview An ST corpus usually consists of a set of triplet data S = {(xi , zi , yi )}. Here xi is the audio wave sequence, zi is the transcript sequence and yi is the translation sequence in the target language. As a benefit of shared semantic projection, Chimera is able to leverage large-scale MT training corpo"
2021.findings-acl.195,W97-0400,0,0.769807,"Missing"
2021.findings-acl.195,2020.acl-main.344,0,0.130173,"ST approaches cascade automatic speech recognition (ASR) and machine translation (MT) (Sperber et al., 2017, 2019; Zhang et al., 2019; Beck et al., 2019; Cheng et al., 2019). However, cascaded models often suffer from the issues of error propagation and translation latency. As a result, there have been a series of recent attempts on end-to-end speech-to-text translation (Liu et al., 1 All codes, data, and resources will be made released at https://github.com/Glaciohound/Chimera-SLT. 2019, 2018; Weiss et al., 2017; B´erard et al., 2018; Duong et al., 2016; Jia et al., 2019; Dong et al., 2021b; Wang et al., 2020b). The end-to-end approaches learn a single unified model, which is easier to deploy, has lower latency and could potentially reduce errors. However, it remains a challenge for end-to-end ST to catch up with their cascaded counterparts in performance. We argue that the root cause is the gap between the two modalities, speech and text. Although they both encode human languages, they are dissimilar in both coding attributes (pitch, volume, and intonation versus words, affixes, and punctuation) and length (thousands of time frames versus tens of words). This issue is further coupled with the rel"
2021.findings-acl.242,D19-1252,0,0.0463746,"Missing"
2021.findings-acl.242,2020.inlg-1.14,0,0.0284611,"Missing"
2021.findings-acl.242,D18-1208,0,0.0318748,"Missing"
2021.findings-acl.242,N19-1423,0,0.00584685,". We further transfer CALMS to other languages and find that it will also benefit similar languages. Our code and dataset are available at https://github.com/brxx122/CALMS. 1 Introduction Automatic text summarization aims at providing a brief summary for a long document. It requires the ability to understand document-level input, catch the main idea of it, and generate a fluent text. Recently, monolingual summarization has witnessed great success with the development of new neural systems (Zhong et al., 2020; Wang et al., 2020) and the availability of monolingual pre-training language models (Kenton and Toutanova, 2019; Liu and Lapata, 2019; Liu et al., 2019; Lewis et al., 2020b). , Inspired by the success of monolingual pre-trained models, researchers further pre-train these models with multiple languages to get the multilingual versions (Huang et al., 2019; Liu et al., 2020; Lewis et al., 2020a), which provide the abilities of understanding and generation in different languages. The multilingual pre-training model can be used as the initialization and finetuned for downstream summarization tasks. However, the pre-training phase for language models usually focuses on predicting masked tokens or denoising t"
2021.findings-acl.242,D18-2012,0,0.0146274,"tible at home since the start of the season, Manchester City crushed Arsenal (6-3) in the game at the top of the Premier League. [q] The Mancuniens are three points behind the Gunners at the top of the standings.) Table 1: A Fr example of our dataset. The text in brackets is the corresponding English translation. The sentences are separated by ‘[q]’. a Transformer-based architecture (Vaswani et al., 2017) with 12 layers of encoder and 12 layers of the decoder. The hidden size is 1024 with 16 attention heads. mBART covers 25 languages and shares the vocabulary with the sentencepiece tokenizer (Kudo and Richardson, 2018), which includes 250,000 subword tokens. We follow the language indicators with mBART, and change its position to the beginning of the source and target sequence. We replace [q] in the dataset with the delimiter &lt; /s &gt; to separate sentences. We use the first part of our dataset as training languages: De, En, Ru, Fr, Zh. We mix the training examples and do global shuffling to avoid local overfitting on a specific language. For CSR, we random sample q = 3 sentences from the document to construct the positive-negative pairs and let the margin  = 1.0. For SAS, we translate sentences to the other"
2021.findings-acl.242,2020.acl-main.703,0,0.222467,"also benefit similar languages. Our code and dataset are available at https://github.com/brxx122/CALMS. 1 Introduction Automatic text summarization aims at providing a brief summary for a long document. It requires the ability to understand document-level input, catch the main idea of it, and generate a fluent text. Recently, monolingual summarization has witnessed great success with the development of new neural systems (Zhong et al., 2020; Wang et al., 2020) and the availability of monolingual pre-training language models (Kenton and Toutanova, 2019; Liu and Lapata, 2019; Liu et al., 2019; Lewis et al., 2020b). , Inspired by the success of monolingual pre-trained models, researchers further pre-train these models with multiple languages to get the multilingual versions (Huang et al., 2019; Liu et al., 2020; Lewis et al., 2020a), which provide the abilities of understanding and generation in different languages. The multilingual pre-training model can be used as the initialization and finetuned for downstream summarization tasks. However, the pre-training phase for language models usually focuses on predicting masked tokens or denoising the noisy input, both of which are token-level tasks. It lack"
2021.findings-acl.242,P18-2027,0,0.0288599,"e multilingual pre-training model can be used as the initialization and finetuned for downstream summarization tasks. However, the pre-training phase for language models usually focuses on predicting masked tokens or denoising the noisy input, both of which are token-level tasks. It lacks the ability to align sentence-level information among languages and to distinguish which information is the most critical for the document-level input. Most previous multilingual summarization models focus on training one model for different language or partly share encoder/decoder layers (Wang et al., 2018; Lin et al., 2018; Scialom et al., 2020). Cao et al. (2020) and Lewis et al. (2020a) try to train one model for all languages, but they find that although low-resource languages can benefit from the larger training data, the performance of rich-resource languages has been sacrificed. Thus, we want to investigate the following question: Can we design a unified multilingual summarization model that can benefit both high-resource and low-resource languages? In this paper, we design a neural model with the contrastive aligned joint learning strategy for multilingual summarization (CALMS) with two new training obje"
2021.findings-acl.242,2020.emnlp-main.210,1,0.791666,"tion dataset used in our experiment and the (3) experimental settings. (l ) k where si,pos is the score of the positive candidate of (lk ) si,neg j the i-th example in language lk , and is the j-th negative candidate for i-th example. We use a linear layer with sigmoid function to get the score from the masked hidden state of the last layer of the encoder.  is a hyper-parameter for the margin distance. 3.3 Sentence Aligned Substitution Training with multiple languages makes it possible to share the representative space across languages and obtain a universal representation for summarization. Lin et al. (2020) randomly replaces words with a different language during the pre-training phase for machine translation. However, the input for summarization is longer than sentence-level machine translation and the single word replacement shows little influence (Kedzie et al., 2018). Thus, we propose sentence aligned substitution (SAS) for summarization. We take lead sentences rather than randomly sampling from the document because these sentences are more important in the summarization task. We use an extra translation tool 1 to translate our sentences into another language to get the aligned information."
2021.findings-acl.242,D19-1387,0,0.0806602,"to other languages and find that it will also benefit similar languages. Our code and dataset are available at https://github.com/brxx122/CALMS. 1 Introduction Automatic text summarization aims at providing a brief summary for a long document. It requires the ability to understand document-level input, catch the main idea of it, and generate a fluent text. Recently, monolingual summarization has witnessed great success with the development of new neural systems (Zhong et al., 2020; Wang et al., 2020) and the availability of monolingual pre-training language models (Kenton and Toutanova, 2019; Liu and Lapata, 2019; Liu et al., 2019; Lewis et al., 2020b). , Inspired by the success of monolingual pre-trained models, researchers further pre-train these models with multiple languages to get the multilingual versions (Huang et al., 2019; Liu et al., 2020; Lewis et al., 2020a), which provide the abilities of understanding and generation in different languages. The multilingual pre-training model can be used as the initialization and finetuned for downstream summarization tasks. However, the pre-training phase for language models usually focuses on predicting masked tokens or denoising the noisy input, both o"
2021.findings-acl.242,2020.tacl-1.47,0,0.275213,"It requires the ability to understand document-level input, catch the main idea of it, and generate a fluent text. Recently, monolingual summarization has witnessed great success with the development of new neural systems (Zhong et al., 2020; Wang et al., 2020) and the availability of monolingual pre-training language models (Kenton and Toutanova, 2019; Liu and Lapata, 2019; Liu et al., 2019; Lewis et al., 2020b). , Inspired by the success of monolingual pre-trained models, researchers further pre-train these models with multiple languages to get the multilingual versions (Huang et al., 2019; Liu et al., 2020; Lewis et al., 2020a), which provide the abilities of understanding and generation in different languages. The multilingual pre-training model can be used as the initialization and finetuned for downstream summarization tasks. However, the pre-training phase for language models usually focuses on predicting masked tokens or denoising the noisy input, both of which are token-level tasks. It lacks the ability to align sentence-level information among languages and to distinguish which information is the most critical for the document-level input. Most previous multilingual summarization models"
2021.findings-acl.242,D18-1206,0,0.016562,"than 40 languages and each article is written by native authors. France24 is an international news website with 4 languages and faz is a German website. All of these websites have a highlight written by the editor at the beginning of the news article to summarize the main idea, which can be viewed as the summary. This information can be easily extracted through the HTML tag (’storybody introduction’ in BBC, ’t-content chapo’ in france24, ’atc-IntroText’ in faz). We collect MLGSum mainly from BBC and use france24 to expand French, English, and Spanish. Faz is used for German. Similar to XSum (Narayan et al., 2018) and Newsroom (Grusky et al., 2018), we provide the Wayback archived URL of each article and the processing script to release MLGSum. The Wayback Machine9 is an initiative of the Internet Archive, building a digital library of Internet sites that archive billions of web pages. We search news articles ranging from 2010 to 2020 for the above websites. We emphasize that the intellectual property and privacy rights of the articles belong to the original authors and the corresponding website. We carefully check the terms of use, privacy policy, and copyright policy10 of the Internet Archive and the"
2021.findings-acl.242,D19-5411,0,0.046841,"Missing"
2021.findings-acl.242,2020.emnlp-main.647,0,0.042479,"Missing"
2021.findings-acl.242,2020.acl-main.553,1,0.784845,"that CALMS achieves significant improvement over monolingual models in all languages. We further transfer CALMS to other languages and find that it will also benefit similar languages. Our code and dataset are available at https://github.com/brxx122/CALMS. 1 Introduction Automatic text summarization aims at providing a brief summary for a long document. It requires the ability to understand document-level input, catch the main idea of it, and generate a fluent text. Recently, monolingual summarization has witnessed great success with the development of new neural systems (Zhong et al., 2020; Wang et al., 2020) and the availability of monolingual pre-training language models (Kenton and Toutanova, 2019; Liu and Lapata, 2019; Liu et al., 2019; Lewis et al., 2020b). , Inspired by the success of monolingual pre-trained models, researchers further pre-train these models with multiple languages to get the multilingual versions (Huang et al., 2019; Liu et al., 2020; Lewis et al., 2020a), which provide the abilities of understanding and generation in different languages. The multilingual pre-training model can be used as the initialization and finetuned for downstream summarization tasks. However, the pre-"
2021.findings-acl.242,2020.emnlp-main.294,0,0.0855807,"Missing"
2021.findings-acl.242,2020.acl-main.552,1,0.909487,"tal results indicate that CALMS achieves significant improvement over monolingual models in all languages. We further transfer CALMS to other languages and find that it will also benefit similar languages. Our code and dataset are available at https://github.com/brxx122/CALMS. 1 Introduction Automatic text summarization aims at providing a brief summary for a long document. It requires the ability to understand document-level input, catch the main idea of it, and generate a fluent text. Recently, monolingual summarization has witnessed great success with the development of new neural systems (Zhong et al., 2020; Wang et al., 2020) and the availability of monolingual pre-training language models (Kenton and Toutanova, 2019; Liu and Lapata, 2019; Liu et al., 2019; Lewis et al., 2020b). , Inspired by the success of monolingual pre-trained models, researchers further pre-train these models with multiple languages to get the multilingual versions (Huang et al., 2019; Liu et al., 2020; Lewis et al., 2020a), which provide the abilities of understanding and generation in different languages. The multilingual pre-training model can be used as the initialization and finetuned for downstream summarization task"
2021.findings-acl.242,N19-4009,0,0.0194461,"position to the beginning of the source and target sequence. We replace [q] in the dataset with the delimiter &lt; /s &gt; to separate sentences. We use the first part of our dataset as training languages: De, En, Ru, Fr, Zh. We mix the training examples and do global shuffling to avoid local overfitting on a specific language. For CSR, we random sample q = 3 sentences from the document to construct the positive-negative pairs and let the margin  = 1.0. For SAS, we translate sentences to the other four languages with equal probability and substitute sentences with a ratio r = 0.2. We use fairseq6 (Ott et al., 2019) to implement the architecture. We limit the max tokens to 2048 for each GPU and set the gradient accumulation to 4. The Adam optimizer (Kingma and Ba, 2015) is 6 https://github.com/pytorch/fairseq Language Size Doc. Summ. Train De En Ru Fr Zh 494,514 191,365 87,125 85,030 65,203 457 476 499 463 799 27 24 24 36 56 445,062 172,228 78,412 76,527 58,682 Hi Es Id Tr Vi Uk Pt 59,145 43,162 35,495 26,539 26,539 33,214 20,945 565 703 360 342 847 444 927 28 30 21 20 34 21 34 53,230 38,845 31,945 33,047 23,885 29,892 18,850 Total 1,168,276 573.5 29.6 1,060,605 Table 2: The dataset statistic. Doc. and S"
2021.findings-acl.264,C18-1263,0,0.0177442,"ways to improve the zeroshot translation quality (Al-Shedivat and Parikh, 2019; Arivazhagan et al., 2019; Zhu et al., 2020). The semantic representations of different languages should be close to each other to get better translation quality (Ding et al., 2017). The off-target issue indicates that the MNMT model tends to translate input sentences to the wrong languages, which leads to low translation quality. Due to its simplicity and efficiency, LT strategy has become a fundamental strategy for MNMT (Dabre et al., 2020). Though previous work adopted different LT strategies (Wang et al., 2018; Blackwood et al., 2018; Conneau and Lample, 2019; Liu et al., 2020b), the usages of LT strategies are intuitive and lack systematic study. In this paper, we investigate 4 popular LT strategies, namely T-ENC, T-DEC, S-ENC-T-ENC and S-ENC-T-DEC. Each of them only requires simple modifications to the input data. Table 1 comprehensively illustrates the strategies with an English to Spanish translation pair (Hello World! → ¡Hola Mundo!). 3 3.1 Experiments Experiment Settings Datasets We carry out our experiments on the publicly available IWSLT17 (Cettolo et al., 2017), TED talks (Qi et al., 2018) and Europarl v7 (Koehn,"
2021.findings-acl.264,2004.iwslt-evaluation.1,0,0.0795441,"Missing"
2021.findings-acl.264,W18-6408,1,0.778461,"shot directions. Experimental results show that by ignoring the source language tag (SLT) and adding the target language tag (TLT) to the encoder, the zero-shot translations could achieve a +8 BLEU score difference over other LT strategies in IWSLT17, Europarl, TED talks translation tasks. 1 Introduction Neural Machine Translation (NMT) based on the encoder-decoder framework with attention mechanism (Sutskever et al., 2014; Bahdanau et al., 2015; Luong et al., 2015; Wu et al., 2016; Gehring et al., 2017; Vaswani et al., 2017) has achieved state-ofthe-art (SotA) results in many language pairs (Deng et al., 2018; Barrault et al., 2019). Pioneered by (Dong et al., 2015; Firat et al., 2016; Zoph and Knight, 2016; Ha et al., 2016; Johnson et al., 2017), researchers start to investigate the possibility of using a single model to translate between multiple languages, which is the Multilingual Neural Machine Translation (MNMT). Benefiting from the transferring ability of multilingual modeling, MNMT could achieve better translation quality between low-resource language directions than bilingual models (Gu et al., 2018; Wang et al., 2019). More exciting, MNMT could even translate between zero-shot language d"
2021.findings-acl.264,P17-1106,0,0.0298889,"hang et al., 2020), thus improving the translation quality. To the best of our knowledge, this is the first paper to systematically study the importance of LT strategies for zero-shot translation quality. 2 Background and Notations Improving the consistency of semantic representations and alleviating the off-target issue (Zhang et al., 2020) are effective ways to improve the zeroshot translation quality (Al-Shedivat and Parikh, 2019; Arivazhagan et al., 2019; Zhu et al., 2020). The semantic representations of different languages should be close to each other to get better translation quality (Ding et al., 2017). The off-target issue indicates that the MNMT model tends to translate input sentences to the wrong languages, which leads to low translation quality. Due to its simplicity and efficiency, LT strategy has become a fundamental strategy for MNMT (Dabre et al., 2020). Though previous work adopted different LT strategies (Wang et al., 2018; Blackwood et al., 2018; Conneau and Lample, 2019; Liu et al., 2020b), the usages of LT strategies are intuitive and lack systematic study. In this paper, we investigate 4 popular LT strategies, namely T-ENC, T-DEC, S-ENC-T-ENC and S-ENC-T-DEC. Each of them onl"
2021.findings-acl.264,P15-1166,0,0.0266418,"ng the source language tag (SLT) and adding the target language tag (TLT) to the encoder, the zero-shot translations could achieve a +8 BLEU score difference over other LT strategies in IWSLT17, Europarl, TED talks translation tasks. 1 Introduction Neural Machine Translation (NMT) based on the encoder-decoder framework with attention mechanism (Sutskever et al., 2014; Bahdanau et al., 2015; Luong et al., 2015; Wu et al., 2016; Gehring et al., 2017; Vaswani et al., 2017) has achieved state-ofthe-art (SotA) results in many language pairs (Deng et al., 2018; Barrault et al., 2019). Pioneered by (Dong et al., 2015; Firat et al., 2016; Zoph and Knight, 2016; Ha et al., 2016; Johnson et al., 2017), researchers start to investigate the possibility of using a single model to translate between multiple languages, which is the Multilingual Neural Machine Translation (MNMT). Benefiting from the transferring ability of multilingual modeling, MNMT could achieve better translation quality between low-resource language directions than bilingual models (Gu et al., 2018; Wang et al., 2019). More exciting, MNMT could even translate between zero-shot language directions (Johnson et al., 2017; Gu et al., 2019; Pham et"
2021.findings-acl.264,N16-1101,0,0.0233489,"age tag (SLT) and adding the target language tag (TLT) to the encoder, the zero-shot translations could achieve a +8 BLEU score difference over other LT strategies in IWSLT17, Europarl, TED talks translation tasks. 1 Introduction Neural Machine Translation (NMT) based on the encoder-decoder framework with attention mechanism (Sutskever et al., 2014; Bahdanau et al., 2015; Luong et al., 2015; Wu et al., 2016; Gehring et al., 2017; Vaswani et al., 2017) has achieved state-ofthe-art (SotA) results in many language pairs (Deng et al., 2018; Barrault et al., 2019). Pioneered by (Dong et al., 2015; Firat et al., 2016; Zoph and Knight, 2016; Ha et al., 2016; Johnson et al., 2017), researchers start to investigate the possibility of using a single model to translate between multiple languages, which is the Multilingual Neural Machine Translation (MNMT). Benefiting from the transferring ability of multilingual modeling, MNMT could achieve better translation quality between low-resource language directions than bilingual models (Gu et al., 2018; Wang et al., 2019). More exciting, MNMT could even translate between zero-shot language directions (Johnson et al., 2017; Gu et al., 2019; Pham et al., 2019; Kudugunt"
2021.findings-acl.264,N18-1032,0,0.0241304,"aswani et al., 2017) has achieved state-ofthe-art (SotA) results in many language pairs (Deng et al., 2018; Barrault et al., 2019). Pioneered by (Dong et al., 2015; Firat et al., 2016; Zoph and Knight, 2016; Ha et al., 2016; Johnson et al., 2017), researchers start to investigate the possibility of using a single model to translate between multiple languages, which is the Multilingual Neural Machine Translation (MNMT). Benefiting from the transferring ability of multilingual modeling, MNMT could achieve better translation quality between low-resource language directions than bilingual models (Gu et al., 2018; Wang et al., 2019). More exciting, MNMT could even translate between zero-shot language directions (Johnson et al., 2017; Gu et al., 2019; Pham et al., 2019; Kudugunta et al., 2019). Unlike bilingual NMT, language-specific signals should be accessible to the MNMT model so that the model can distinguish the translation directions. Ha et al., (2016) first introduced a universal encoder-decoder framework for MNMT models with language-specific coded vocabulary to indicate different languages. The encoder-decoder architecture is identical to bilingual models (Bahdanau et al., 2015; Vaswani et al."
2021.findings-acl.264,P19-1121,0,0.0207753,"ed by (Dong et al., 2015; Firat et al., 2016; Zoph and Knight, 2016; Ha et al., 2016; Johnson et al., 2017), researchers start to investigate the possibility of using a single model to translate between multiple languages, which is the Multilingual Neural Machine Translation (MNMT). Benefiting from the transferring ability of multilingual modeling, MNMT could achieve better translation quality between low-resource language directions than bilingual models (Gu et al., 2018; Wang et al., 2019). More exciting, MNMT could even translate between zero-shot language directions (Johnson et al., 2017; Gu et al., 2019; Pham et al., 2019; Kudugunta et al., 2019). Unlike bilingual NMT, language-specific signals should be accessible to the MNMT model so that the model can distinguish the translation directions. Ha et al., (2016) first introduced a universal encoder-decoder framework for MNMT models with language-specific coded vocabulary to indicate different languages. The encoder-decoder architecture is identical to bilingual models (Bahdanau et al., 2015; Vaswani et al., 2017). To further simplify the MNMT models, Johnson et al., (2017) propose to add language tags (LTs) to the beginning of input data to i"
2021.findings-acl.264,Q17-1024,0,0.0606056,"Missing"
2021.findings-acl.264,2005.mtsummit-papers.11,0,0.0777922,", 2018; Conneau and Lample, 2019; Liu et al., 2020b), the usages of LT strategies are intuitive and lack systematic study. In this paper, we investigate 4 popular LT strategies, namely T-ENC, T-DEC, S-ENC-T-ENC and S-ENC-T-DEC. Each of them only requires simple modifications to the input data. Table 1 comprehensively illustrates the strategies with an English to Spanish translation pair (Hello World! → ¡Hola Mundo!). 3 3.1 Experiments Experiment Settings Datasets We carry out our experiments on the publicly available IWSLT17 (Cettolo et al., 2017), TED talks (Qi et al., 2018) and Europarl v7 (Koehn, 2005) datasets. Table 2 shows an overview of the datasets. We choose four different languages (English included) for both IWSLT17 and Europarl, and 20 languages for TED talks. All the training data are English-centric parallel data, which means either the source-side or target-side of the sentence pair is English. We have 6, 6, and 342 zero-shot translation directions and an average of 145k, 1.96M (M = million), and 187k sentence pairs per direction for the three datasets respectively. We choose the official tst2017, WMT newstest08, and the TED talks testsets (Qi et al., 2018) as our test sets, res"
2021.findings-acl.264,D18-2012,0,0.0139251,"s. We choose four different languages (English included) for both IWSLT17 and Europarl, and 20 languages for TED talks. All the training data are English-centric parallel data, which means either the source-side or target-side of the sentence pair is English. We have 6, 6, and 342 zero-shot translation directions and an average of 145k, 1.96M (M = million), and 187k sentence pairs per direction for the three datasets respectively. We choose the official tst2017, WMT newstest08, and the TED talks testsets (Qi et al., 2018) as our test sets, respectively. We learned a joint SentencePiece model (Kudo and Richardson, 2018) for sub-word training on all languages with 40,000 merge operations for each dataset. We limit the size of joint vocabulary to 40,000 for all three datasets. Settings We use the open-source implementation (Ott et al., 2019) of Transformer model (Vaswani et al., 2017). Following the settings of (Liu et al., 2020a), we use a 5-layer encoder and 5-layer decoder variation of Transformer-base model (Vaswani et al., 2017) for TED and IWSLT17. For Europarl v7, we use a standard Transformer-big model (Vaswani et al., 2017). Sentence pairs are batched together by approximate sentence length. Each batc"
2021.findings-acl.264,D19-1167,0,0.0117114,"l., 2016; Zoph and Knight, 2016; Ha et al., 2016; Johnson et al., 2017), researchers start to investigate the possibility of using a single model to translate between multiple languages, which is the Multilingual Neural Machine Translation (MNMT). Benefiting from the transferring ability of multilingual modeling, MNMT could achieve better translation quality between low-resource language directions than bilingual models (Gu et al., 2018; Wang et al., 2019). More exciting, MNMT could even translate between zero-shot language directions (Johnson et al., 2017; Gu et al., 2019; Pham et al., 2019; Kudugunta et al., 2019). Unlike bilingual NMT, language-specific signals should be accessible to the MNMT model so that the model can distinguish the translation directions. Ha et al., (2016) first introduced a universal encoder-decoder framework for MNMT models with language-specific coded vocabulary to indicate different languages. The encoder-decoder architecture is identical to bilingual models (Bahdanau et al., 2015; Vaswani et al., 2017). To further simplify the MNMT models, Johnson et al., (2017) propose to add language tags (LTs) to the beginning of input data to indicate the target language. Then a shared v"
2021.findings-acl.264,2020.tacl-1.47,0,0.0563854,"Al-Shedivat and Parikh, 2019; Arivazhagan et al., 2019; Zhu et al., 2020). The semantic representations of different languages should be close to each other to get better translation quality (Ding et al., 2017). The off-target issue indicates that the MNMT model tends to translate input sentences to the wrong languages, which leads to low translation quality. Due to its simplicity and efficiency, LT strategy has become a fundamental strategy for MNMT (Dabre et al., 2020). Though previous work adopted different LT strategies (Wang et al., 2018; Blackwood et al., 2018; Conneau and Lample, 2019; Liu et al., 2020b), the usages of LT strategies are intuitive and lack systematic study. In this paper, we investigate 4 popular LT strategies, namely T-ENC, T-DEC, S-ENC-T-ENC and S-ENC-T-DEC. Each of them only requires simple modifications to the input data. Table 1 comprehensively illustrates the strategies with an English to Spanish translation pair (Hello World! → ¡Hola Mundo!). 3 3.1 Experiments Experiment Settings Datasets We carry out our experiments on the publicly available IWSLT17 (Cettolo et al., 2017), TED talks (Qi et al., 2018) and Europarl v7 (Koehn, 2005) datasets. Table 2 shows an overview o"
2021.findings-acl.264,D15-1166,0,0.0668276,"egies. We demonstrate that a proper LT strategy could enhance the consistency of semantic representations and alleviate the off-target issue in zero-shot directions. Experimental results show that by ignoring the source language tag (SLT) and adding the target language tag (TLT) to the encoder, the zero-shot translations could achieve a +8 BLEU score difference over other LT strategies in IWSLT17, Europarl, TED talks translation tasks. 1 Introduction Neural Machine Translation (NMT) based on the encoder-decoder framework with attention mechanism (Sutskever et al., 2014; Bahdanau et al., 2015; Luong et al., 2015; Wu et al., 2016; Gehring et al., 2017; Vaswani et al., 2017) has achieved state-ofthe-art (SotA) results in many language pairs (Deng et al., 2018; Barrault et al., 2019). Pioneered by (Dong et al., 2015; Firat et al., 2016; Zoph and Knight, 2016; Ha et al., 2016; Johnson et al., 2017), researchers start to investigate the possibility of using a single model to translate between multiple languages, which is the Multilingual Neural Machine Translation (MNMT). Benefiting from the transferring ability of multilingual modeling, MNMT could achieve better translation quality between low-resource l"
2021.findings-acl.264,N19-4009,0,0.0211238,"sentence pair is English. We have 6, 6, and 342 zero-shot translation directions and an average of 145k, 1.96M (M = million), and 187k sentence pairs per direction for the three datasets respectively. We choose the official tst2017, WMT newstest08, and the TED talks testsets (Qi et al., 2018) as our test sets, respectively. We learned a joint SentencePiece model (Kudo and Richardson, 2018) for sub-word training on all languages with 40,000 merge operations for each dataset. We limit the size of joint vocabulary to 40,000 for all three datasets. Settings We use the open-source implementation (Ott et al., 2019) of Transformer model (Vaswani et al., 2017). Following the settings of (Liu et al., 2020a), we use a 5-layer encoder and 5-layer decoder variation of Transformer-base model (Vaswani et al., 2017) for TED and IWSLT17. For Europarl v7, we use a standard Transformer-big model (Vaswani et al., 2017). Sentence pairs are batched together by approximate sentence length. Each batch has approximately 30,000 source tokens and 30,000 target tokens. We use the Adam (Kingma and Ba, 2015) optimizer to update the parameters and 3002 Dataset languages #zero-shot directions #training sents per direction #sent"
2021.findings-acl.264,P02-1040,0,0.109224,"Missing"
2021.findings-acl.264,W19-5202,0,0.482681,"Missing"
2021.findings-acl.264,W18-6319,0,0.0321457,"Missing"
2021.findings-acl.264,N18-2084,0,0.117507,"Missing"
2021.findings-acl.264,1983.tc-1.13,0,0.442489,"Missing"
2021.findings-acl.264,2020.acl-main.148,0,0.135214,"DEC means placing the TLT on the decoder (target) side of model. S-ENC-T-ENC and S-ENC-T-DEC place the SLT on the encoder side, but the former also places the TLT on encoder side, while the latter on the decoder side. find that the LT strategies are crucial for the zeroshot MNMT translation quality. Ignoring SLTs and placing the TLTs on the encoder side could achieve the best performance during our experiments. (ii) We conduct extensive visualization analysis to demonstrate that the proper LT strategy could enhance the consistency of semantic representation and alleviate the off-target issue (Zhang et al., 2020), thus improving the translation quality. To the best of our knowledge, this is the first paper to systematically study the importance of LT strategies for zero-shot translation quality. 2 Background and Notations Improving the consistency of semantic representations and alleviating the off-target issue (Zhang et al., 2020) are effective ways to improve the zeroshot translation quality (Al-Shedivat and Parikh, 2019; Arivazhagan et al., 2019; Zhu et al., 2020). The semantic representations of different languages should be close to each other to get better translation quality (Ding et al., 2017)"
2021.findings-acl.264,2020.acl-main.150,1,0.778499,"o demonstrate that the proper LT strategy could enhance the consistency of semantic representation and alleviate the off-target issue (Zhang et al., 2020), thus improving the translation quality. To the best of our knowledge, this is the first paper to systematically study the importance of LT strategies for zero-shot translation quality. 2 Background and Notations Improving the consistency of semantic representations and alleviating the off-target issue (Zhang et al., 2020) are effective ways to improve the zeroshot translation quality (Al-Shedivat and Parikh, 2019; Arivazhagan et al., 2019; Zhu et al., 2020). The semantic representations of different languages should be close to each other to get better translation quality (Ding et al., 2017). The off-target issue indicates that the MNMT model tends to translate input sentences to the wrong languages, which leads to low translation quality. Due to its simplicity and efficiency, LT strategy has become a fundamental strategy for MNMT (Dabre et al., 2020). Though previous work adopted different LT strategies (Wang et al., 2018; Blackwood et al., 2018; Conneau and Lample, 2019; Liu et al., 2020b), the usages of LT strategies are intuitive and lack sy"
2021.findings-acl.264,N16-1004,0,0.0196597,"ding the target language tag (TLT) to the encoder, the zero-shot translations could achieve a +8 BLEU score difference over other LT strategies in IWSLT17, Europarl, TED talks translation tasks. 1 Introduction Neural Machine Translation (NMT) based on the encoder-decoder framework with attention mechanism (Sutskever et al., 2014; Bahdanau et al., 2015; Luong et al., 2015; Wu et al., 2016; Gehring et al., 2017; Vaswani et al., 2017) has achieved state-ofthe-art (SotA) results in many language pairs (Deng et al., 2018; Barrault et al., 2019). Pioneered by (Dong et al., 2015; Firat et al., 2016; Zoph and Knight, 2016; Ha et al., 2016; Johnson et al., 2017), researchers start to investigate the possibility of using a single model to translate between multiple languages, which is the Multilingual Neural Machine Translation (MNMT). Benefiting from the transferring ability of multilingual modeling, MNMT could achieve better translation quality between low-resource language directions than bilingual models (Gu et al., 2018; Wang et al., 2019). More exciting, MNMT could even translate between zero-shot language directions (Johnson et al., 2017; Gu et al., 2019; Pham et al., 2019; Kudugunta et al., 2019). Unlike"
2021.findings-acl.277,D19-5808,0,0.0145005,"st partition (left), on ParaRules test partitions (middle) and on Birds-Electricity dataset (right), after training on DU5 or partial DU5 (RC-k) training splits. points out that a reasoning system should not only answer queries but also generate a proof. However, PROVER adopts the multi-task learning framework in the training stage and cannot effectively capture the interactions between question answering and proof generation. Along this line, we explore more powerful joint models to achieve deep reasoning. QA and NLI There are bAbI (Weston et al., 2016), QuaRTz (Tafjord et al., 2019), ROPES (Lin et al., 2019) and Hotpot QA (Yang et al., 2018) (QA datasets) involved in rule reasoning. However, for those datasets, implicit rules (i.e., which multihop chains are valid) need to be inferred from the training data. In our task, the rules of reasoning are given in advance. Compared with the Natural Language Inference (MacCartney and Manning, 2014), our task can be regarded as its deductive subset. In particular, NLI allows for unsupported inferences (Dagan et al., 2013). 6 Conclusion In this work, we propose PROBR, a novel probabilistic graph reasoning framework for joint question answering and proof gen"
2021.findings-acl.277,2021.ccl-1.108,0,0.021698,"Missing"
2021.findings-acl.277,W09-3714,0,0.0303451,", 2005; Berant et al., 2013; Berant and Liang, 2014), researchers focus on developing theorem provers by combining the symbolic techniques with the differentiable learning from neural networks (Reed and de Freitas, 2016; Abdelaziz et al., 2020; Abboud et al., 2020), such as NLProlog (Weber et al., 2019), SAT solving (Selsam et al., 2019) and Neural programme (Neelakantan et al., 2016). To bypass this expensive and error-prone intermediate logical representation, reasoning over natural language statements in an end-to-end manner is promising. Text Reasoning over Natural Language Natural logic (MacCartney and Manning, 2009) focuses on semantic containment and monotonicity by incorporating semantic exclusion and implicativity. Subsequently, Clark et al. (2020) proposes to use a Transformer-based model to emulate deductive reasoning and achieves high accuracy on synthetically generated data. PROVER (Saha et al., 2020) 7 For more details, please refer to the supplementary materials. 3147 PROBR PROBR+GOLD PROBR+KL PROBR+GOLD+KL Figure 3: QA accuracy compared among PROBR, PROBR + Gold, PROBR + KL, and PROBR + Gold + KL on DU5 test partition (left), on ParaRules test partitions (middle) and on Birds-Electricity datase"
2021.findings-acl.277,2020.emnlp-main.9,0,0.466799,"of to prove or disprove the query. For example, in Figure 1, there are two facts, six rules and two queries, each of which is expressed by natural language. To predict the true/false of each query, starting from the facts, we need to reason deductively by applying given rules ∗ Equal contribution. until we can derive the truth value of the query. The process of deduction can be represented as a graph, whose node is either a fact, rule or special NAF node (explained in the Section 2.1). Generating answer and proof together makes a system easier to interpret and diagnose. Recent work by PROVER (Saha et al., 2020) first explored this problem through two modules: question answering and proof generation. It trains these two modules through implicit parameter sharing, and then uses integer linear programming (ILP) to enforce consistency constraints (only test time). It is difficult to ensure that the proof generation module contributes to the question answering module, because the proof is not explicitly involved in the answer prediction. Parameter sharing becomes more limited under few/zero-shot settings, as demonstrated in our experiments. We expect the proof to enhance the capability of question answer"
2021.findings-acl.277,D19-1608,0,0.0134592,"nd PROBR + Gold + KL on DU5 test partition (left), on ParaRules test partitions (middle) and on Birds-Electricity dataset (right), after training on DU5 or partial DU5 (RC-k) training splits. points out that a reasoning system should not only answer queries but also generate a proof. However, PROVER adopts the multi-task learning framework in the training stage and cannot effectively capture the interactions between question answering and proof generation. Along this line, we explore more powerful joint models to achieve deep reasoning. QA and NLI There are bAbI (Weston et al., 2016), QuaRTz (Tafjord et al., 2019), ROPES (Lin et al., 2019) and Hotpot QA (Yang et al., 2018) (QA datasets) involved in rule reasoning. However, for those datasets, implicit rules (i.e., which multihop chains are valid) need to be inferred from the training data. In our task, the rules of reasoning are given in advance. Compared with the Natural Language Inference (MacCartney and Manning, 2014), our task can be regarded as its deductive subset. In particular, NLI allows for unsupported inferences (Dagan et al., 2013). 6 Conclusion In this work, we propose PROBR, a novel probabilistic graph reasoning framework for joint questi"
2021.findings-acl.277,P19-1618,0,0.0627666,"Missing"
2021.findings-acl.277,D18-1259,0,0.0249411,"test partitions (middle) and on Birds-Electricity dataset (right), after training on DU5 or partial DU5 (RC-k) training splits. points out that a reasoning system should not only answer queries but also generate a proof. However, PROVER adopts the multi-task learning framework in the training stage and cannot effectively capture the interactions between question answering and proof generation. Along this line, we explore more powerful joint models to achieve deep reasoning. QA and NLI There are bAbI (Weston et al., 2016), QuaRTz (Tafjord et al., 2019), ROPES (Lin et al., 2019) and Hotpot QA (Yang et al., 2018) (QA datasets) involved in rule reasoning. However, for those datasets, implicit rules (i.e., which multihop chains are valid) need to be inferred from the training data. In our task, the rules of reasoning are given in advance. Compared with the Natural Language Inference (MacCartney and Manning, 2014), our task can be regarded as its deductive subset. In particular, NLI allows for unsupported inferences (Dagan et al., 2013). 6 Conclusion In this work, we propose PROBR, a novel probabilistic graph reasoning framework for joint question answering and proof generation. PROBR defines a joint dis"
2021.findings-acl.73,P19-1208,0,0.0623243,"in past decades (Witten et al., 2005; Nguyen and Kan, 2007; Medelyan et al., 2009; Lopez and Romary, 2010; Zhang et al., 2016; Alzaidy et al., 2019; Sun et al., 2020). These methods aim to select text spans or phrases directly in the document, which show promising results on present keyphrase prediction. However, extractive methods cannot handle the absent keyphrase, which is also significant and requires a comprehensive understanding of document. To mitigate this issue, several generative methods (Meng et al., 2017; Chen et al., 2018; Ye and Wang, 2018; Wang et al., 2019; Chen et al., 2019b; Chan et al., 2019; Zhao and Zhang, 2019; Chen et al., 2020; Yuan et al., 2020) have been proposed. Generative methods mainly adopt the sequenceto-sequence (seq2seq) model with a copy mechanism to predict a target sequence, which is concatenated of present and absent keyphrases. Therefore, the generative approach can predict both kinds of keyphrases. But these methods treat present and absent keyphrases equally, while these two kinds of keyphrase actually have different semantic properties. As illustrated in Figure 1, all the present keyphrases are specific techniques, while the absent keyphrases are tasks or r"
2021.findings-acl.73,W03-1028,0,0.19685,"ovel end-to-end framework UniKeyphrase for unified PKE and AKG. • We design stacked relation layer (SRL) to explicitly capture the relation between PKE and AKG. • We propose bag-of-words constraint (BWC) to explicitly feed global information about present and absent keyphrases to the model. 2 2.1 Related Works Keyphrase Extraction Most existing extraction approaches can be categorized into two-step extraction methods and sequence labeling approaches. Two-step extraction methods first identify a set of candidate phrases from the document by heuristics, such as essential n-grams or noun phrase (Hulth, 2003). Then, the candidate keyphrases are sorted and ranked to get predicted results. The scores can be learned by either supervised algorithms (Nguyen and Kan, 2007; Medelyan et al., 2009; Lopez and Romary, 2010) or unsupervised graph ranking methods (Mihalcea and Tarau, 2004; Wan and Xiao, 2008). For sequence labeling approaches, documents are fed to an encoder then the model learns to predict the likelihood of each word being a keyphrase (Zhang et al., 2016; Alzaidy et al., 2019; Sun et al., 2020). 2.2 Keyphrase Generation Keyphrase generation focuses on predicting both present and absent keyphr"
2021.findings-acl.73,P06-1068,0,0.0493058,", the mexican hat wavelet } Absent keyphrases: { singularity detection, traffic data analysis } Figure 1: An example of an input document and its expected keyphrases. Blue and red denote present and absent keyphrases, respectively. Introduction Keyphrases are several phrases that highlight core topics or information of a document. Given a document, the KP task focuses on automatically obtaining a set of keyphrases. As a basic NLP task, keyphrase prediction is useful for numerous downstream NLP tasks such as summarization (Wang and Cardie, 2013; Pasunuru and Bansal, 2018), document clustering (Hulth and Megyesi, 2006), information retrieval (Kim et al., 2013). Keyphrases of a document fall into two categories: present keyphrase that appears continuously in the document, and absent keyphrase which does not exist in the document. Figure 1 shows an example of a document and its keyphrases. Traditional KP methods are mainly extractive, which have ∗ Equal contribution. Our code is available https://github.com/thinkwee/UniKeyphrase 1 on been extensively researched in past decades (Witten et al., 2005; Nguyen and Kan, 2007; Medelyan et al., 2009; Lopez and Romary, 2010; Zhang et al., 2016; Alzaidy et al., 2019; S"
2021.findings-acl.73,S10-1004,0,0.0180992,"d tokens, which is formulated as: LAKG = − Vs N X X (j,a) ˆi y   (j,a) log yi (14) i=1 j=1 where N refers to the number of masked tokens, ˆ ia refers the Vs refers to the size of vocabulary. y ground-truth word. Considering the BWC, the overall loss of UniKeyphrase is formulated as: L = LP KE + LAKG + wBoW LBoW 4 4.1 (15) Experiments Datasets and Evaluation We follow the widely used setup of the deep KP task: train, validation and test on the KP20K (Meng et al., 2017) dataset, and give evaluation on three more benchmark datasets: NUS (Nguyen and Kan, 2007), INSPEC (Hulth, 2003) and SEMEVAL (Kim et al., 2010). We follow the preprocess, post-process, and evaluation setting of Meng et al. (2017, 2019); Yuan et al. (2020)2 . Specifically, we use the partition of present and absent provided by Meng et al. (2017) and calculate F1 @5 and F1 @M (use all predicted keyphrases for F1 calculation) after stemming and removing duplicates. 4.2 Experimental Setup Setting: We reuse most hyper-parameters from pretrained UNILM3 . The layer number of SRL is set to 2. We use wm = 1.0 when adjusting the weight of BWC. PKE loss weights wc for the positive label is set to 5.0. we set batch size to 256, and maximum lengt"
2021.findings-acl.73,I13-1108,0,0.0155678,"singularity detection, traffic data analysis } Figure 1: An example of an input document and its expected keyphrases. Blue and red denote present and absent keyphrases, respectively. Introduction Keyphrases are several phrases that highlight core topics or information of a document. Given a document, the KP task focuses on automatically obtaining a set of keyphrases. As a basic NLP task, keyphrase prediction is useful for numerous downstream NLP tasks such as summarization (Wang and Cardie, 2013; Pasunuru and Bansal, 2018), document clustering (Hulth and Megyesi, 2006), information retrieval (Kim et al., 2013). Keyphrases of a document fall into two categories: present keyphrase that appears continuously in the document, and absent keyphrase which does not exist in the document. Figure 1 shows an example of a document and its keyphrases. Traditional KP methods are mainly extractive, which have ∗ Equal contribution. Our code is available https://github.com/thinkwee/UniKeyphrase 1 on been extensively researched in past decades (Witten et al., 2005; Nguyen and Kan, 2007; Medelyan et al., 2009; Lopez and Romary, 2010; Zhang et al., 2016; Alzaidy et al., 2019; Sun et al., 2020). These methods aim to sel"
2021.findings-acl.73,S10-1055,0,0.230454,"ru and Bansal, 2018), document clustering (Hulth and Megyesi, 2006), information retrieval (Kim et al., 2013). Keyphrases of a document fall into two categories: present keyphrase that appears continuously in the document, and absent keyphrase which does not exist in the document. Figure 1 shows an example of a document and its keyphrases. Traditional KP methods are mainly extractive, which have ∗ Equal contribution. Our code is available https://github.com/thinkwee/UniKeyphrase 1 on been extensively researched in past decades (Witten et al., 2005; Nguyen and Kan, 2007; Medelyan et al., 2009; Lopez and Romary, 2010; Zhang et al., 2016; Alzaidy et al., 2019; Sun et al., 2020). These methods aim to select text spans or phrases directly in the document, which show promising results on present keyphrase prediction. However, extractive methods cannot handle the absent keyphrase, which is also significant and requires a comprehensive understanding of document. To mitigate this issue, several generative methods (Meng et al., 2017; Chen et al., 2018; Ye and Wang, 2018; Wang et al., 2019; Chen et al., 2019b; Chan et al., 2019; Zhao and Zhang, 2019; Chen et al., 2020; Yuan et al., 2020) have been proposed. Genera"
2021.findings-acl.73,D18-1439,0,0.0809461,"le https://github.com/thinkwee/UniKeyphrase 1 on been extensively researched in past decades (Witten et al., 2005; Nguyen and Kan, 2007; Medelyan et al., 2009; Lopez and Romary, 2010; Zhang et al., 2016; Alzaidy et al., 2019; Sun et al., 2020). These methods aim to select text spans or phrases directly in the document, which show promising results on present keyphrase prediction. However, extractive methods cannot handle the absent keyphrase, which is also significant and requires a comprehensive understanding of document. To mitigate this issue, several generative methods (Meng et al., 2017; Chen et al., 2018; Ye and Wang, 2018; Wang et al., 2019; Chen et al., 2019b; Chan et al., 2019; Zhao and Zhang, 2019; Chen et al., 2020; Yuan et al., 2020) have been proposed. Generative methods mainly adopt the sequenceto-sequence (seq2seq) model with a copy mechanism to predict a target sequence, which is concatenated of present and absent keyphrases. Therefore, the generative approach can predict both kinds of keyphrases. But these methods treat present and absent keyphrases equally, while these two kinds of keyphrase actually have different semantic properties. As illustrated in Figure 1, all the present k"
2021.findings-acl.73,P18-2053,0,0.0187351,": 1 X (V (w) − Vˆ (w))2 |V| LBoW = (11) w∈V It is worth noting that V is the collection of words that make up the ground truth keyphrases and predicted keyphrases. So the BWC only affects a small subset of the whole vocabulary for each sample. This can help reduce the noise and stabilize the training process. In practice we increase the weight of BWC logarithmically from zero to a defined maximum value wm , the weight of BWC on t step can be denoted as follows: wBoW (t) = log( ewm − 1 t + 1) ttotal (12) where ttotal is the total step of training. The reason to adjust the weight is the same as Ma et al. (2018). The BWC should take effect when predicted results are good enough. Therefore we first assign a small weight to BWC at the initial time, and gradually increase it when training. 3.4 Training For the PKE task, objection is formulated as: LP KE = − M X C X i=1 c=1 (c,p) ˆi wc y   (c,p) (13) log yi where M refers to the length of document, C refers to the number of label, wc is the loss weight for the ˆ ip refers the gold label. positive label. y For the AKG task, training objection is to maximize the likelihood of masked tokens, which is formulated as: LAKG = − Vs N X X (j,a) ˆi y   (j,a) l"
2021.findings-acl.73,N19-1292,0,0.160275,"ensively researched in past decades (Witten et al., 2005; Nguyen and Kan, 2007; Medelyan et al., 2009; Lopez and Romary, 2010; Zhang et al., 2016; Alzaidy et al., 2019; Sun et al., 2020). These methods aim to select text spans or phrases directly in the document, which show promising results on present keyphrase prediction. However, extractive methods cannot handle the absent keyphrase, which is also significant and requires a comprehensive understanding of document. To mitigate this issue, several generative methods (Meng et al., 2017; Chen et al., 2018; Ye and Wang, 2018; Wang et al., 2019; Chen et al., 2019b; Chan et al., 2019; Zhao and Zhang, 2019; Chen et al., 2020; Yuan et al., 2020) have been proposed. Generative methods mainly adopt the sequenceto-sequence (seq2seq) model with a copy mechanism to predict a target sequence, which is concatenated of present and absent keyphrases. Therefore, the generative approach can predict both kinds of keyphrases. But these methods treat present and absent keyphrases equally, while these two kinds of keyphrase actually have different semantic properties. As illustrated in Figure 1, all the present keyphrases are specific techniques, while the absent keyph"
2021.findings-acl.73,D09-1137,0,0.0391907,"nd Cardie, 2013; Pasunuru and Bansal, 2018), document clustering (Hulth and Megyesi, 2006), information retrieval (Kim et al., 2013). Keyphrases of a document fall into two categories: present keyphrase that appears continuously in the document, and absent keyphrase which does not exist in the document. Figure 1 shows an example of a document and its keyphrases. Traditional KP methods are mainly extractive, which have ∗ Equal contribution. Our code is available https://github.com/thinkwee/UniKeyphrase 1 on been extensively researched in past decades (Witten et al., 2005; Nguyen and Kan, 2007; Medelyan et al., 2009; Lopez and Romary, 2010; Zhang et al., 2016; Alzaidy et al., 2019; Sun et al., 2020). These methods aim to select text spans or phrases directly in the document, which show promising results on present keyphrase prediction. However, extractive methods cannot handle the absent keyphrase, which is also significant and requires a comprehensive understanding of document. To mitigate this issue, several generative methods (Meng et al., 2017; Chen et al., 2018; Ye and Wang, 2018; Wang et al., 2019; Chen et al., 2019b; Chan et al., 2019; Zhao and Zhang, 2019; Chen et al., 2020; Yuan et al., 2020) ha"
2021.findings-acl.73,2020.acl-main.103,0,0.159982,"yen and Kan, 2007; Medelyan et al., 2009; Lopez and Romary, 2010; Zhang et al., 2016; Alzaidy et al., 2019; Sun et al., 2020). These methods aim to select text spans or phrases directly in the document, which show promising results on present keyphrase prediction. However, extractive methods cannot handle the absent keyphrase, which is also significant and requires a comprehensive understanding of document. To mitigate this issue, several generative methods (Meng et al., 2017; Chen et al., 2018; Ye and Wang, 2018; Wang et al., 2019; Chen et al., 2019b; Chan et al., 2019; Zhao and Zhang, 2019; Chen et al., 2020; Yuan et al., 2020) have been proposed. Generative methods mainly adopt the sequenceto-sequence (seq2seq) model with a copy mechanism to predict a target sequence, which is concatenated of present and absent keyphrases. Therefore, the generative approach can predict both kinds of keyphrases. But these methods treat present and absent keyphrases equally, while these two kinds of keyphrase actually have different semantic properties. As illustrated in Figure 1, all the present keyphrases are specific techniques, while the absent keyphrases are tasks or research areas. Thus several integrated me"
2021.findings-acl.73,P17-1054,0,0.173324,"Our code is available https://github.com/thinkwee/UniKeyphrase 1 on been extensively researched in past decades (Witten et al., 2005; Nguyen and Kan, 2007; Medelyan et al., 2009; Lopez and Romary, 2010; Zhang et al., 2016; Alzaidy et al., 2019; Sun et al., 2020). These methods aim to select text spans or phrases directly in the document, which show promising results on present keyphrase prediction. However, extractive methods cannot handle the absent keyphrase, which is also significant and requires a comprehensive understanding of document. To mitigate this issue, several generative methods (Meng et al., 2017; Chen et al., 2018; Ye and Wang, 2018; Wang et al., 2019; Chen et al., 2019b; Chan et al., 2019; Zhao and Zhang, 2019; Chen et al., 2020; Yuan et al., 2020) have been proposed. Generative methods mainly adopt the sequenceto-sequence (seq2seq) model with a copy mechanism to predict a target sequence, which is concatenated of present and absent keyphrases. Therefore, the generative approach can predict both kinds of keyphrases. But these methods treat present and absent keyphrases equally, while these two kinds of keyphrase actually have different semantic properties. As illustrated in Figure 1"
2021.findings-acl.73,W04-3252,0,0.167089,"absent keyphrases to the model. 2 2.1 Related Works Keyphrase Extraction Most existing extraction approaches can be categorized into two-step extraction methods and sequence labeling approaches. Two-step extraction methods first identify a set of candidate phrases from the document by heuristics, such as essential n-grams or noun phrase (Hulth, 2003). Then, the candidate keyphrases are sorted and ranked to get predicted results. The scores can be learned by either supervised algorithms (Nguyen and Kan, 2007; Medelyan et al., 2009; Lopez and Romary, 2010) or unsupervised graph ranking methods (Mihalcea and Tarau, 2004; Wan and Xiao, 2008). For sequence labeling approaches, documents are fed to an encoder then the model learns to predict the likelihood of each word being a keyphrase (Zhang et al., 2016; Alzaidy et al., 2019; Sun et al., 2020). 2.2 Keyphrase Generation Keyphrase generation focuses on predicting both present and absent keyphrases. Meng et al. (2017) first propose CopyRNN which is a seq2seq framework with attention and copy mechanism. Then a semi-supervised method for the exploitation of the unlabeled data is investigated by Ye and Wang (2018). Chen et al. (2018) employ a review mechanism to r"
2021.findings-acl.73,D18-1447,0,0.0622721,"om/thinkwee/UniKeyphrase 1 on been extensively researched in past decades (Witten et al., 2005; Nguyen and Kan, 2007; Medelyan et al., 2009; Lopez and Romary, 2010; Zhang et al., 2016; Alzaidy et al., 2019; Sun et al., 2020). These methods aim to select text spans or phrases directly in the document, which show promising results on present keyphrase prediction. However, extractive methods cannot handle the absent keyphrase, which is also significant and requires a comprehensive understanding of document. To mitigate this issue, several generative methods (Meng et al., 2017; Chen et al., 2018; Ye and Wang, 2018; Wang et al., 2019; Chen et al., 2019b; Chan et al., 2019; Zhao and Zhang, 2019; Chen et al., 2020; Yuan et al., 2020) have been proposed. Generative methods mainly adopt the sequenceto-sequence (seq2seq) model with a copy mechanism to predict a target sequence, which is concatenated of present and absent keyphrases. Therefore, the generative approach can predict both kinds of keyphrases. But these methods treat present and absent keyphrases equally, while these two kinds of keyphrase actually have different semantic properties. As illustrated in Figure 1, all the present keyphrases are speci"
2021.findings-acl.73,N18-2102,0,0.0123456,"orm, oblique cumulative curve, short time fourier, the mexican hat wavelet } Absent keyphrases: { singularity detection, traffic data analysis } Figure 1: An example of an input document and its expected keyphrases. Blue and red denote present and absent keyphrases, respectively. Introduction Keyphrases are several phrases that highlight core topics or information of a document. Given a document, the KP task focuses on automatically obtaining a set of keyphrases. As a basic NLP task, keyphrase prediction is useful for numerous downstream NLP tasks such as summarization (Wang and Cardie, 2013; Pasunuru and Bansal, 2018), document clustering (Hulth and Megyesi, 2006), information retrieval (Kim et al., 2013). Keyphrases of a document fall into two categories: present keyphrase that appears continuously in the document, and absent keyphrase which does not exist in the document. Figure 1 shows an example of a document and its keyphrases. Traditional KP methods are mainly extractive, which have ∗ Equal contribution. Our code is available https://github.com/thinkwee/UniKeyphrase 1 on been extensively researched in past decades (Witten et al., 2005; Nguyen and Kan, 2007; Medelyan et al., 2009; Lopez and Romary, 20"
2021.findings-acl.73,2020.acl-main.710,0,0.403281,"Medelyan et al., 2009; Lopez and Romary, 2010; Zhang et al., 2016; Alzaidy et al., 2019; Sun et al., 2020). These methods aim to select text spans or phrases directly in the document, which show promising results on present keyphrase prediction. However, extractive methods cannot handle the absent keyphrase, which is also significant and requires a comprehensive understanding of document. To mitigate this issue, several generative methods (Meng et al., 2017; Chen et al., 2018; Ye and Wang, 2018; Wang et al., 2019; Chen et al., 2019b; Chan et al., 2019; Zhao and Zhang, 2019; Chen et al., 2020; Yuan et al., 2020) have been proposed. Generative methods mainly adopt the sequenceto-sequence (seq2seq) model with a copy mechanism to predict a target sequence, which is concatenated of present and absent keyphrases. Therefore, the generative approach can predict both kinds of keyphrases. But these methods treat present and absent keyphrases equally, while these two kinds of keyphrase actually have different semantic properties. As illustrated in Figure 1, all the present keyphrases are specific techniques, while the absent keyphrases are tasks or research areas. Thus several integrated methods (Chen et al.,"
2021.findings-acl.73,D16-1080,0,0.0727331,"cument clustering (Hulth and Megyesi, 2006), information retrieval (Kim et al., 2013). Keyphrases of a document fall into two categories: present keyphrase that appears continuously in the document, and absent keyphrase which does not exist in the document. Figure 1 shows an example of a document and its keyphrases. Traditional KP methods are mainly extractive, which have ∗ Equal contribution. Our code is available https://github.com/thinkwee/UniKeyphrase 1 on been extensively researched in past decades (Witten et al., 2005; Nguyen and Kan, 2007; Medelyan et al., 2009; Lopez and Romary, 2010; Zhang et al., 2016; Alzaidy et al., 2019; Sun et al., 2020). These methods aim to select text spans or phrases directly in the document, which show promising results on present keyphrase prediction. However, extractive methods cannot handle the absent keyphrase, which is also significant and requires a comprehensive understanding of document. To mitigate this issue, several generative methods (Meng et al., 2017; Chen et al., 2018; Ye and Wang, 2018; Wang et al., 2019; Chen et al., 2019b; Chan et al., 2019; Zhao and Zhang, 2019; Chen et al., 2020; Yuan et al., 2020) have been proposed. Generative methods mainly"
2021.findings-acl.73,D19-1214,0,0.0541536,"Missing"
2021.findings-acl.73,2020.findings-emnlp.163,0,0.0124888,"epresentation of the target token can only attend to the left context, as well as all the tokens in the the hidden state H = {h1 , ...,hT } (T is the number of input tokens in the UNILM) will be used as the input of stacked relation layer for jointly modeling PKE and AKG. 3.2 Stacked Relation Layer Based on the UNILM, we can obtain the output hidden H. Instead of directly using the UNILM hidden for PKE and AKG, we use the SRL to explicitly model the relation between these two tasks. Actually, modeling the cross-impact and interaction between different tasks in joint model is a common problem (Qin et al., 2020a,b, 2019). Specifically, SRL takes the initial shared representations P0 = A0 = {h1 , ...,hT } as input and aims to obtain the finally task representations PL and AL (L is the number of stacked layers), which consider the cross-impact between PKE and AKG. Besides, SRL can be stacked to repeatedly fuse PKE and AKG task representations for better capturing mutual relation. Formally, given the lth layer inputs Pl = {pl1 , ...,plT } and Al = {al1 , ...,alT }, stacked relation layer first apply two linear transformations with a ReLU activation over the input to make them more taskspecific, which c"
2021.findings-acl.73,P13-1137,0,0.078128,"Missing"
2021.findings-acl.73,P19-1240,0,0.0572725,"hrase 1 on been extensively researched in past decades (Witten et al., 2005; Nguyen and Kan, 2007; Medelyan et al., 2009; Lopez and Romary, 2010; Zhang et al., 2016; Alzaidy et al., 2019; Sun et al., 2020). These methods aim to select text spans or phrases directly in the document, which show promising results on present keyphrase prediction. However, extractive methods cannot handle the absent keyphrase, which is also significant and requires a comprehensive understanding of document. To mitigate this issue, several generative methods (Meng et al., 2017; Chen et al., 2018; Ye and Wang, 2018; Wang et al., 2019; Chen et al., 2019b; Chan et al., 2019; Zhao and Zhang, 2019; Chen et al., 2020; Yuan et al., 2020) have been proposed. Generative methods mainly adopt the sequenceto-sequence (seq2seq) model with a copy mechanism to predict a target sequence, which is concatenated of present and absent keyphrases. Therefore, the generative approach can predict both kinds of keyphrases. But these methods treat present and absent keyphrases equally, while these two kinds of keyphrase actually have different semantic properties. As illustrated in Figure 1, all the present keyphrases are specific techniques, whi"
2021.findings-acl.73,P19-1515,0,0.0677521,"tten et al., 2005; Nguyen and Kan, 2007; Medelyan et al., 2009; Lopez and Romary, 2010; Zhang et al., 2016; Alzaidy et al., 2019; Sun et al., 2020). These methods aim to select text spans or phrases directly in the document, which show promising results on present keyphrase prediction. However, extractive methods cannot handle the absent keyphrase, which is also significant and requires a comprehensive understanding of document. To mitigate this issue, several generative methods (Meng et al., 2017; Chen et al., 2018; Ye and Wang, 2018; Wang et al., 2019; Chen et al., 2019b; Chan et al., 2019; Zhao and Zhang, 2019; Chen et al., 2020; Yuan et al., 2020) have been proposed. Generative methods mainly adopt the sequenceto-sequence (seq2seq) model with a copy mechanism to predict a target sequence, which is concatenated of present and absent keyphrases. Therefore, the generative approach can predict both kinds of keyphrases. But these methods treat present and absent keyphrases equally, while these two kinds of keyphrase actually have different semantic properties. As illustrated in Figure 1, all the present keyphrases are specific techniques, while the absent keyphrases are tasks or research areas. Thus se"
2021.findings-emnlp.233,2021.acl-long.21,1,0.782419,". Among them, neural machine the current word solely based on the internal contranslation (NMT) is also explored by several at- text while the translation decoder has to capture tempts (Yang et al., 2020a; Zhu et al., 2020b; Rothe the source context. Specifically, the decoder in et al., 2020). The pre-training and fine-tuning style NMT has a “cross-attention” sub-layer that plays a becomes an important alternative to take advantage transduction role (Bahdanau et al., 2015), while preof monolingual data (Yang et al., 2020c,b; Liu et al., trained models have none, as is shown in Figure 2. 2020; Pan et al., 2021). This mismatch between the generation models and An intuitive question comes as: Can we bridge conditional generation models makes it a challenge BERT-like pre-trained encoders and GPT-like de- for the usage of pre-trained models as translation coders to form a high-quality translation model? decoders. Since they only need monolingual data, we can Therefore, some previous works manually insert reduce the reliance on the large parallel corpus. cross-attention sub-layer or adapters (Rothe et al., 2020; Ma et al., 2020; Guo et al., 2020). However, ∗ Work is done while at ByteDance. 1 the extra i"
2021.findings-emnlp.233,P02-1040,0,0.110894,"rameters are optimized by using Adam optimizer (Kingma and Ba, 2015), with β1 = 0.9, β2 = 0.98, with warmup_steps = 4000. Without extra statement, we use dropout = 0.3 (Srivastava et al., 2014). Label smoothing (Szegedy et al., 2016) of value = 0.1 is also adopted. Besides, we use fp16 mixed precision training (Micikevicius et al., 2018) with Horovod library with RDMA inter-GPU communication (Sergeev and Del Balso, 2018). • Evaluation: We uniformly conduct beam search with size = 5 and length penalty α = 0.6. For hi, ja, and zh, we use SacreBLEU (Post, 2018). Otherwise, we use tokenized BLEU (Papineni et al., 2002) with the open-source script 6 . 4.3 Main Results As is shown in Table 1 and 2, our methods obtain significant improvements across all language pairs. For x→en and en→x pairs, advances of nearly 6 BLEU and 3 BLEU are achieved. We also compare the results with loading from mBART, a well-known multilingual pre-trained sequence-tosequence model (Liu et al., 2020) 7 . Due to the language difference, we only tune the model on a part of languages. With both 12-layers depth and 1024-dimensions width, our method outperforms mBART on almost all pairs, proving the superiority of Graformer comparing with"
2021.findings-emnlp.233,D18-1039,0,0.0469664,"Missing"
2021.findings-emnlp.233,W18-6319,0,0.0120202,"we go through the total data for five times. Parameters are optimized by using Adam optimizer (Kingma and Ba, 2015), with β1 = 0.9, β2 = 0.98, with warmup_steps = 4000. Without extra statement, we use dropout = 0.3 (Srivastava et al., 2014). Label smoothing (Szegedy et al., 2016) of value = 0.1 is also adopted. Besides, we use fp16 mixed precision training (Micikevicius et al., 2018) with Horovod library with RDMA inter-GPU communication (Sergeev and Del Balso, 2018). • Evaluation: We uniformly conduct beam search with size = 5 and length penalty α = 0.6. For hi, ja, and zh, we use SacreBLEU (Post, 2018). Otherwise, we use tokenized BLEU (Papineni et al., 2002) with the open-source script 6 . 4.3 Main Results As is shown in Table 1 and 2, our methods obtain significant improvements across all language pairs. For x→en and en→x pairs, advances of nearly 6 BLEU and 3 BLEU are achieved. We also compare the results with loading from mBART, a well-known multilingual pre-trained sequence-tosequence model (Liu et al., 2020) 7 . Due to the language difference, we only tune the model on a part of languages. With both 12-layers depth and 1024-dimensions width, our method outperforms mBART on almost all"
2021.findings-emnlp.233,N18-2084,0,0.0204132,"the corpus of “zh_cn” instead of “zh”. 4 https://github.com/neulab/ word-embeddings-for-nmt 3 T denotes the length of sequence. <2lang&gt;, x1 , x2 , ..., xt−1 . x<t = Datasets and Preprocess • Pre-training: We use News-Crawl corpus 2 plus WMT datasets. We conduct deduplication and label the data by language. In the end, we collect 1.4 billion sentences in 45 languages, which is only one-fifth of that of mBART (Liu et al., 2020). The detailed list of languages and corresponding scales is in Appendix A. • Multilingual Translation: We use TED datasets, the most widely used MNMT datasets, following Qi et al. (2018); Aharoni et al. (2019). We extract 30 languages 3 from & to English, with the size of 3.18M sentence pairs in raw data and 10.1M sentence pairs in sampled bidirectional data. The detailed list of language pairs and scales is in Appendix A. We download the data from the open source 4 Pre-train Multilingual GPT (Decoder for Generation) T X Experiments In this paper, we perform many-to-many style multilingual translation (Johnson et al., 2017). The detailed illustrations of the datasets and implementation are as follows. m(x) and m(x) denote the masked words and rest words from x LLM = − (3) hN"
2021.findings-emnlp.233,2020.emnlp-main.208,0,0.0932283,"o! Hello! German English BERT GPT Grafting Chinese French Bonjour! Figure 1: Grafting pre-trained (masked) language models like BERT and GPT for machine translation. Moreover, if the combination of models is univer- Hallo! sal, it can be applied to translation for multiple BERT languages, as is shown in Figure 1. However, though many works successfully gain improvements by loading encoder/decoder parameters from BERT-like pre-trained encoders (Zhu BERT et al., 2020b; Guo et al., 2020), they do not achieve satisfactory results with loading decoder parameters from GPT-like pre-trained decoders (Yang et al., 2020a; Rothe et al., 2020). Theoretically, the well-trained decoder model like GPT should bring 1 Introduction better generation ability to the translation model. In recent years, pre-trained (masked) language We suggest the outcome may be attributed to the models have achieved significant progress in all architecture mismatch. kinds of NLP tasks (Devlin et al., 2019; RadPre-trained (masked) language models predict ford et al., 2019). Among them, neural machine the current word solely based on the internal contranslation (NMT) is also explored by several at- text while the translation decoder has"
2021.findings-emnlp.233,2020.acl-main.148,0,0.0290241,"019b), learning better representation (Wang et al., their strengths. The primary target is to link the 2019a), massive training (Aharoni et al., 2019; generation model to the source side and maintain the invariability of the architecture in the mean- Arivazhagan et al., 2019), interlingua (Zhu et al., time. Therefore, we propose Graformer, with pre- 2020a), and adpater (Zhu et al., 2021). These works trained models grafted by a connective sub-module. mainly utilize parallel data. There are also some works taking advantage of The structure of the pre-trained parts remains unmonolingual corpus. Zhang et al. (2020); Wang changed, and we train the grafting part to learn to translate. For universality and generalization, et al. (2020) use back-translation (BT) to improve we also extend the model to multilingual NMT, MNMT. However, for MNMT, BT is tremendously costly, reaching O(n), or even O(n2 ). Siddhant achieving mBERT+mGPT. et al. (2020); Wang et al. (2020) adopt multi-task Generally, the translation process can be divided learning (MTL), combining with other tasks such into three parts: representation, transduction, and generation, respectively achieved by the encoder, as masked language model (MLM)"
2021.findings-emnlp.233,2020.acl-main.150,0,0.531267,"the well-trained decoder model like GPT should bring 1 Introduction better generation ability to the translation model. In recent years, pre-trained (masked) language We suggest the outcome may be attributed to the models have achieved significant progress in all architecture mismatch. kinds of NLP tasks (Devlin et al., 2019; RadPre-trained (masked) language models predict ford et al., 2019). Among them, neural machine the current word solely based on the internal contranslation (NMT) is also explored by several at- text while the translation decoder has to capture tempts (Yang et al., 2020a; Zhu et al., 2020b; Rothe the source context. Specifically, the decoder in et al., 2020). The pre-training and fine-tuning style NMT has a “cross-attention” sub-layer that plays a becomes an important alternative to take advantage transduction role (Bahdanau et al., 2015), while preof monolingual data (Yang et al., 2020c,b; Liu et al., trained models have none, as is shown in Figure 2. 2020; Pan et al., 2021). This mismatch between the generation models and An intuitive question comes as: Can we bridge conditional generation models makes it a challenge BERT-like pre-trained encoders and GPT-like de- for the us"
2021.findings-emnlp.233,1983.tc-1.13,0,0.383958,"Missing"
2021.findings-emnlp.233,P19-1117,0,0.017939,"tempts and conthe pre-training objective is usually a variant of firm its feasibility. The most well-known work auto-encoding (Song et al., 2019; Liu et al., 2020), is from Johnson et al. (2017), who conduct a sewhich is different from the downstream translation ries of interesting experiments. And the usage of objective and may not achieve adequate improve- the language token style is widely accepted. Also, ments (Lin et al., 2020). many subsequent works continuously explore new approaches in MNMT, such as parameter sharIn this paper, we mainly focus on exploring ing (Blackwood et al., 2018; Wang et al., 2019b; the best way to simultaneously take advantage of Tan et al., 2019a), parameter generation (Platanthe pre-trained representation model and generaios et al., 2018), knowledge distillation (Tan et al., tion model (e.g., BERT+GPT) without limiting 2019b), learning better representation (Wang et al., their strengths. The primary target is to link the 2019a), massive training (Aharoni et al., 2019; generation model to the source side and maintain the invariability of the architecture in the mean- Arivazhagan et al., 2019), interlingua (Zhu et al., time. Therefore, we propose Graformer, with pre-"
2021.findings-emnlp.233,2020.emnlp-main.75,0,0.0286409,"2020a), and adpater (Zhu et al., 2021). These works trained models grafted by a connective sub-module. mainly utilize parallel data. There are also some works taking advantage of The structure of the pre-trained parts remains unmonolingual corpus. Zhang et al. (2020); Wang changed, and we train the grafting part to learn to translate. For universality and generalization, et al. (2020) use back-translation (BT) to improve we also extend the model to multilingual NMT, MNMT. However, for MNMT, BT is tremendously costly, reaching O(n), or even O(n2 ). Siddhant achieving mBERT+mGPT. et al. (2020); Wang et al. (2020) adopt multi-task Generally, the translation process can be divided learning (MTL), combining with other tasks such into three parts: representation, transduction, and generation, respectively achieved by the encoder, as masked language model (MLM) (Devlin et al., cross-attention, and decoder. In multilingual NMT, 2019), denoising auto-encoding (DAE) (Vincent et al., 2008), or masked sequence-to-sequence genthe transduction can only be trained with multiple eration (MASS) (Song et al., 2019). However, the parallel data. But the rest two can be pre-trained optimization target is different from"
2021.findings-emnlp.240,W18-1819,0,0.0317786,"imple: they attached a dedicated token given a source sentence s and the target language l, at the beginning of the source sentence to specify the multilingual MT system shall output a sentence the target language, while the rest of the model was that resembles human reference t. shared among all languages. The paper has set a Currently, Transformer (Vaswani et al., 2017) milestone of multilingual MT and has become the gains popularity and becomes the paradigm for basis for most subsequent work. state-of-the-art NMT systems. Here, we follow 2813 the recent implementations (Klein et al., 2017; Vaswani et al., 2018) of the pre-norm transformer, whose layer normalization is applied to the input of each sub-layer. The transformation of i-th sublayer taking xi as input can be formulated as: Output Layer Decoder x N + Feed Forward + Encoder x N Feed Forward xi`1 “ Fθ pxi q “ sub-layerθ pLN pxi qq ` xi (1) Layer Adapter Layer Norm Multilingual Embedding Deficiency CamachoCollados and Pilehvar (2018) addressed the meaning conflation deficiency problem of the word embedding as a single vector is limited for representing polysemy. We extend the meaning conflation deficiency into the multilingual scenario. Genera"
2021.findings-emnlp.240,P19-1117,0,0.0213368,"xternal knowledge from human or other models: Tan et al. (2018) boosted the multilingual model by knowledge distillation, Tan et al. (2019) pre-clustered languages to assist similar languages. Several studies enhance the model from data: Xia et al. (2019) and Siddhant et al. (2020) conducted data augmentation to low-resource languages via related high-resources or monolingual data. Taitelbaum et al. (2019) improved translation with relevant auxiliary languages. Some other studies enhanced the Transformer model by introducing language-aware modules and learning languagespecific representation (Wang et al., 2019; Zhu et al., 2020). Adapter Network for Machine Translation. Our design derives from the residual adapters of the domain adaptation task. Concretely, Rebuffi et al. (2017) proposed the residual adapters in the computer vision area. They appended small networks (named adapters) to a pre-trained base network and only tuned the adapter on the specific task. Houlsby et al. (2019) adopted the idea into NLP domain adaptation tasks and designed the adapter for the Transformer, as shown in Fig. 2a. Bapna and Firat (2019) further extended the model to MT domain adaptation, and they regarded multilingu"
2021.findings-emnlp.240,P19-1579,0,0.0266709,"strate the efficacy of CIAT through extensive experiments on IWSLT, OPUS-100, and WMT benchmark datasets, surpassing other multilingual models over most of the translation directions. 2 Related Work Recent studies paid more attention to the performance improvement of multilingual models based on Johnson et al. (2017)’s effort. Several improved the model with external knowledge from human or other models: Tan et al. (2018) boosted the multilingual model by knowledge distillation, Tan et al. (2019) pre-clustered languages to assist similar languages. Several studies enhance the model from data: Xia et al. (2019) and Siddhant et al. (2020) conducted data augmentation to low-resource languages via related high-resources or monolingual data. Taitelbaum et al. (2019) improved translation with relevant auxiliary languages. Some other studies enhanced the Transformer model by introducing language-aware modules and learning languagespecific representation (Wang et al., 2019; Zhu et al., 2020). Adapter Network for Machine Translation. Our design derives from the residual adapters of the domain adaptation task. Concretely, Rebuffi et al. (2017) proposed the residual adapters in the computer vision area. They"
2021.findings-emnlp.240,2020.acl-main.148,0,0.168709,"ings in different languages — bride in English refers to a woman soon to get married 1 Introduction while in French it means horse bridle. When it Machine translation (MT) is a core task in nat- comes to machine translation, the effect goes beural language processing. In recent years, neu- yond word embedding. As a single model has ral machine translation (NMT) approaches have bounded capacity, the multilingual learning may made tremendous progress and takes the lead in the cause negative influences among shared paramefield (Bahdanau et al., 2015; Vaswani et al., 2017; ters (Liu et al., 2017; Zhang et al., 2020). We Johnson et al., 2017). Conventionally, each NMT conjecture that such a performance degradation is model only tackles a single language direction (e.g. due to the interference across languages brought English Ñ German). A commonly used model by joint training on multiple language directions. like Transformer has S “ 240 million parameters. Such interference affects both joint token embedTherefore, Translating N language pairs requires ding and representations from intermediate layers. training models separately for each direction, re- We argue that resolving the interference is critical su"
2021.findings-emnlp.240,2020.acl-main.150,0,0.0410324,"rom human or other models: Tan et al. (2018) boosted the multilingual model by knowledge distillation, Tan et al. (2019) pre-clustered languages to assist similar languages. Several studies enhance the model from data: Xia et al. (2019) and Siddhant et al. (2020) conducted data augmentation to low-resource languages via related high-resources or monolingual data. Taitelbaum et al. (2019) improved translation with relevant auxiliary languages. Some other studies enhanced the Transformer model by introducing language-aware modules and learning languagespecific representation (Wang et al., 2019; Zhu et al., 2020). Adapter Network for Machine Translation. Our design derives from the residual adapters of the domain adaptation task. Concretely, Rebuffi et al. (2017) proposed the residual adapters in the computer vision area. They appended small networks (named adapters) to a pre-trained base network and only tuned the adapter on the specific task. Houlsby et al. (2019) adopted the idea into NLP domain adaptation tasks and designed the adapter for the Transformer, as shown in Fig. 2a. Bapna and Firat (2019) further extended the model to MT domain adaptation, and they regarded multilingual MT as a domain a"
2021.findings-emnlp.396,P19-1425,0,0.0197578,"E and Secoco-Edit is very close. Therefore, it is better to use SecocoE2E for its simplification and efficiency. Second, Secoco is more effective on the real-world test sets, showing its potential in real-world application. 3.5 Iterative Editing Related Work Approaches to the robustness of NMT can be roughly divided into three categories. In the first research line, adversarial examples are generated with black- or white-box methods. The generated adversarial examples are then used to combine with original training data for adversarial training (Ebrahimi et al., 2018; Chaturvedi et al., 2019; Cheng et al., 2019; Michel et al., 2019; Zhao et al., 2018; Cheng et al., 2020). In the second strand, a wide variety of methods have been proposed to deal with noise in training data (Schwenk, 2018; Guo et al., 2018; Xu and Koehn, 2017; Koehn et al., 2018; van der Wees et al., 2017; Wang and Neubig, 2019; Wang et al., 2018a,b, 2019). Finally, efforts have been also explored to directly cope with naturally occurring noise in texts, which are closely related to our work. Heigold et al. (2018); Belinkov and Bisk (2018); Levy et al. (2019) focus on word spelling errors. Sperber et al.; Liu et al. (2019) study tran"
2021.findings-emnlp.396,2020.acl-main.529,0,0.0281867,"use SecocoE2E for its simplification and efficiency. Second, Secoco is more effective on the real-world test sets, showing its potential in real-world application. 3.5 Iterative Editing Related Work Approaches to the robustness of NMT can be roughly divided into three categories. In the first research line, adversarial examples are generated with black- or white-box methods. The generated adversarial examples are then used to combine with original training data for adversarial training (Ebrahimi et al., 2018; Chaturvedi et al., 2019; Cheng et al., 2019; Michel et al., 2019; Zhao et al., 2018; Cheng et al., 2020). In the second strand, a wide variety of methods have been proposed to deal with noise in training data (Schwenk, 2018; Guo et al., 2018; Xu and Koehn, 2017; Koehn et al., 2018; van der Wees et al., 2017; Wang and Neubig, 2019; Wang et al., 2018a,b, 2019). Finally, efforts have been also explored to directly cope with naturally occurring noise in texts, which are closely related to our work. Heigold et al. (2018); Belinkov and Bisk (2018); Levy et al. (2019) focus on word spelling errors. Sperber et al.; Liu et al. (2019) study translation problems caused by speech recognition. Vaibhav et al."
2021.findings-emnlp.396,C18-1055,0,0.0198205,"ng findings. First, the performance of Secoco-E2E and Secoco-Edit is very close. Therefore, it is better to use SecocoE2E for its simplification and efficiency. Second, Secoco is more effective on the real-world test sets, showing its potential in real-world application. 3.5 Iterative Editing Related Work Approaches to the robustness of NMT can be roughly divided into three categories. In the first research line, adversarial examples are generated with black- or white-box methods. The generated adversarial examples are then used to combine with original training data for adversarial training (Ebrahimi et al., 2018; Chaturvedi et al., 2019; Cheng et al., 2019; Michel et al., 2019; Zhao et al., 2018; Cheng et al., 2020). In the second strand, a wide variety of methods have been proposed to deal with noise in training data (Schwenk, 2018; Guo et al., 2018; Xu and Koehn, 2017; Koehn et al., 2018; van der Wees et al., 2017; Wang and Neubig, 2019; Wang et al., 2018a,b, 2019). Finally, efforts have been also explored to directly cope with naturally occurring noise in texts, which are closely related to our work. Heigold et al. (2018); Belinkov and Bisk (2018); Levy et al. (2019) focus on word spelling errors."
2021.findings-emnlp.396,W18-6317,0,0.0137347,"real-world application. 3.5 Iterative Editing Related Work Approaches to the robustness of NMT can be roughly divided into three categories. In the first research line, adversarial examples are generated with black- or white-box methods. The generated adversarial examples are then used to combine with original training data for adversarial training (Ebrahimi et al., 2018; Chaturvedi et al., 2019; Cheng et al., 2019; Michel et al., 2019; Zhao et al., 2018; Cheng et al., 2020). In the second strand, a wide variety of methods have been proposed to deal with noise in training data (Schwenk, 2018; Guo et al., 2018; Xu and Koehn, 2017; Koehn et al., 2018; van der Wees et al., 2017; Wang and Neubig, 2019; Wang et al., 2018a,b, 2019). Finally, efforts have been also explored to directly cope with naturally occurring noise in texts, which are closely related to our work. Heigold et al. (2018); Belinkov and Bisk (2018); Levy et al. (2019) focus on word spelling errors. Sperber et al.; Liu et al. (2019) study translation problems caused by speech recognition. Vaibhav et al. (2019) introduce back-translation to generate more natural synthetic data, and employ extra tags to distinguish synthetic data from raw"
2021.findings-emnlp.396,W18-1807,0,0.102508,"Missing"
2021.findings-emnlp.396,W18-6453,0,0.0172586,"Editing Related Work Approaches to the robustness of NMT can be roughly divided into three categories. In the first research line, adversarial examples are generated with black- or white-box methods. The generated adversarial examples are then used to combine with original training data for adversarial training (Ebrahimi et al., 2018; Chaturvedi et al., 2019; Cheng et al., 2019; Michel et al., 2019; Zhao et al., 2018; Cheng et al., 2020). In the second strand, a wide variety of methods have been proposed to deal with noise in training data (Schwenk, 2018; Guo et al., 2018; Xu and Koehn, 2017; Koehn et al., 2018; van der Wees et al., 2017; Wang and Neubig, 2019; Wang et al., 2018a,b, 2019). Finally, efforts have been also explored to directly cope with naturally occurring noise in texts, which are closely related to our work. Heigold et al. (2018); Belinkov and Bisk (2018); Levy et al. (2019) focus on word spelling errors. Sperber et al.; Liu et al. (2019) study translation problems caused by speech recognition. Vaibhav et al. (2019) introduce back-translation to generate more natural synthetic data, and employ extra tags to distinguish synthetic data from raw data. Zhou et al. (2019) propose a recon"
2021.findings-emnlp.396,D19-5506,0,0.0151559,"a for adversarial training (Ebrahimi et al., 2018; Chaturvedi et al., 2019; Cheng et al., 2019; Michel et al., 2019; Zhao et al., 2018; Cheng et al., 2020). In the second strand, a wide variety of methods have been proposed to deal with noise in training data (Schwenk, 2018; Guo et al., 2018; Xu and Koehn, 2017; Koehn et al., 2018; van der Wees et al., 2017; Wang and Neubig, 2019; Wang et al., 2018a,b, 2019). Finally, efforts have been also explored to directly cope with naturally occurring noise in texts, which are closely related to our work. Heigold et al. (2018); Belinkov and Bisk (2018); Levy et al. (2019) focus on word spelling errors. Sperber et al.; Liu et al. (2019) study translation problems caused by speech recognition. Vaibhav et al. (2019) introduce back-translation to generate more natural synthetic data, and employ extra tags to distinguish synthetic data from raw data. Zhou et al. (2019) propose a reconstruction method based on one encoder and two decoders architecture to deal with natural noise for NMT. Different from ours, most of these works use the synthetic data in a coarsegrained and implicit way (i.e. simply combining the synthetic and raw data). As described in Section 2.3, w"
2021.findings-emnlp.396,P19-1291,0,0.0169515,"l., 2019; Cheng et al., 2019; Michel et al., 2019; Zhao et al., 2018; Cheng et al., 2020). In the second strand, a wide variety of methods have been proposed to deal with noise in training data (Schwenk, 2018; Guo et al., 2018; Xu and Koehn, 2017; Koehn et al., 2018; van der Wees et al., 2017; Wang and Neubig, 2019; Wang et al., 2018a,b, 2019). Finally, efforts have been also explored to directly cope with naturally occurring noise in texts, which are closely related to our work. Heigold et al. (2018); Belinkov and Bisk (2018); Levy et al. (2019) focus on word spelling errors. Sperber et al.; Liu et al. (2019) study translation problems caused by speech recognition. Vaibhav et al. (2019) introduce back-translation to generate more natural synthetic data, and employ extra tags to distinguish synthetic data from raw data. Zhou et al. (2019) propose a reconstruction method based on one encoder and two decoders architecture to deal with natural noise for NMT. Different from ours, most of these works use the synthetic data in a coarsegrained and implicit way (i.e. simply combining the synthetic and raw data). As described in Section 2.3, we iteratively edit the input until the input is unchanged and the"
2021.findings-emnlp.396,N19-1314,0,0.0159315,"very close. Therefore, it is better to use SecocoE2E for its simplification and efficiency. Second, Secoco is more effective on the real-world test sets, showing its potential in real-world application. 3.5 Iterative Editing Related Work Approaches to the robustness of NMT can be roughly divided into three categories. In the first research line, adversarial examples are generated with black- or white-box methods. The generated adversarial examples are then used to combine with original training data for adversarial training (Ebrahimi et al., 2018; Chaturvedi et al., 2019; Cheng et al., 2019; Michel et al., 2019; Zhao et al., 2018; Cheng et al., 2020). In the second strand, a wide variety of methods have been proposed to deal with noise in training data (Schwenk, 2018; Guo et al., 2018; Xu and Koehn, 2017; Koehn et al., 2018; van der Wees et al., 2017; Wang and Neubig, 2019; Wang et al., 2018a,b, 2019). Finally, efforts have been also explored to directly cope with naturally occurring noise in texts, which are closely related to our work. Heigold et al. (2018); Belinkov and Bisk (2018); Levy et al. (2019) focus on word spelling errors. Sperber et al.; Liu et al. (2019) study translation problems caus"
2021.findings-emnlp.396,N19-4009,0,0.0150201,"hod enables NMT to transform a noisy input into a clean input and pass this knowledge into the translation decoder. 3.3 Data Baselines Settings In our studies, all translation models were Transformer-base. They were trained with a batch size of 32,000 tokens. The beam size was set to 5 during decoding. We used byte pair encoding compression algorithm (BPE) (Sennrich et al., 2016) to process all these data and restricted merge operations to a maximum of 30k separately. For evaluation, we used the standard Sacrebleu (Post, 2018) to calculate BLEU-4. All models were implemented based on Fairseq (Ott et al., 2019). 3.4 Results Table 2 shows the translation results on Dialogue, Speech and WMT En-De. Clearly, all competitors substantially improve the baseline model in terms of BLEU. Secoco achieves the best performance on all three test sets, gaining improvements of 2.2, 0.7, and 0.4 BLEU-4 points over BASE+synthetic respectively. The improvements suggest the effectiveness of self-correcting encoding. It is worth noting that the BLEU scores here are results on noisy test sets, so they are certainly lower 4641 Dialogue BLEU 4 Methods Speech BLEU 4 WMT En-De BLEU 4 AVG BLEU 4 Latency (ms/sent) BASE BASE +s"
2021.findings-emnlp.396,W18-6319,0,0.0117715,"roduce an additional decoder to obtain clean inputs from noisy inputs. This method enables NMT to transform a noisy input into a clean input and pass this knowledge into the translation decoder. 3.3 Data Baselines Settings In our studies, all translation models were Transformer-base. They were trained with a batch size of 32,000 tokens. The beam size was set to 5 during decoding. We used byte pair encoding compression algorithm (BPE) (Sennrich et al., 2016) to process all these data and restricted merge operations to a maximum of 30k separately. For evaluation, we used the standard Sacrebleu (Post, 2018) to calculate BLEU-4. All models were implemented based on Fairseq (Ott et al., 2019). 3.4 Results Table 2 shows the translation results on Dialogue, Speech and WMT En-De. Clearly, all competitors substantially improve the baseline model in terms of BLEU. Secoco achieves the best performance on all three test sets, gaining improvements of 2.2, 0.7, and 0.4 BLEU-4 points over BASE+synthetic respectively. The improvements suggest the effectiveness of self-correcting encoding. It is worth noting that the BLEU scores here are results on noisy test sets, so they are certainly lower 4641 Dialogue BL"
2021.findings-emnlp.396,P18-2037,0,0.0182766,"s potential in real-world application. 3.5 Iterative Editing Related Work Approaches to the robustness of NMT can be roughly divided into three categories. In the first research line, adversarial examples are generated with black- or white-box methods. The generated adversarial examples are then used to combine with original training data for adversarial training (Ebrahimi et al., 2018; Chaturvedi et al., 2019; Cheng et al., 2019; Michel et al., 2019; Zhao et al., 2018; Cheng et al., 2020). In the second strand, a wide variety of methods have been proposed to deal with noise in training data (Schwenk, 2018; Guo et al., 2018; Xu and Koehn, 2017; Koehn et al., 2018; van der Wees et al., 2017; Wang and Neubig, 2019; Wang et al., 2018a,b, 2019). Finally, efforts have been also explored to directly cope with naturally occurring noise in texts, which are closely related to our work. Heigold et al. (2018); Belinkov and Bisk (2018); Levy et al. (2019) focus on word spelling errors. Sperber et al.; Liu et al. (2019) study translation problems caused by speech recognition. Vaibhav et al. (2019) introduce back-translation to generate more natural synthetic data, and employ extra tags to distinguish synthe"
2021.findings-emnlp.396,P16-1162,0,0.0421259,"et al. (2019) to develop a multi-task based method to solve the robustness problem. We construct triples (clean input, noisy input, target translation), and introduce an additional decoder to obtain clean inputs from noisy inputs. This method enables NMT to transform a noisy input into a clean input and pass this knowledge into the translation decoder. 3.3 Data Baselines Settings In our studies, all translation models were Transformer-base. They were trained with a batch size of 32,000 tokens. The beam size was set to 5 during decoding. We used byte pair encoding compression algorithm (BPE) (Sennrich et al., 2016) to process all these data and restricted merge operations to a maximum of 30k separately. For evaluation, we used the standard Sacrebleu (Post, 2018) to calculate BLEU-4. All models were implemented based on Fairseq (Ott et al., 2019). 3.4 Results Table 2 shows the translation results on Dialogue, Speech and WMT En-De. Clearly, all competitors substantially improve the baseline model in terms of BLEU. Secoco achieves the best performance on all three test sets, gaining improvements of 2.2, 0.7, and 0.4 BLEU-4 points over BASE+synthetic respectively. The improvements suggest the effectiveness"
2021.findings-emnlp.396,N19-1190,0,0.0237898,"et al., 2020). In the second strand, a wide variety of methods have been proposed to deal with noise in training data (Schwenk, 2018; Guo et al., 2018; Xu and Koehn, 2017; Koehn et al., 2018; van der Wees et al., 2017; Wang and Neubig, 2019; Wang et al., 2018a,b, 2019). Finally, efforts have been also explored to directly cope with naturally occurring noise in texts, which are closely related to our work. Heigold et al. (2018); Belinkov and Bisk (2018); Levy et al. (2019) focus on word spelling errors. Sperber et al.; Liu et al. (2019) study translation problems caused by speech recognition. Vaibhav et al. (2019) introduce back-translation to generate more natural synthetic data, and employ extra tags to distinguish synthetic data from raw data. Zhou et al. (2019) propose a reconstruction method based on one encoder and two decoders architecture to deal with natural noise for NMT. Different from ours, most of these works use the synthetic data in a coarsegrained and implicit way (i.e. simply combining the synthetic and raw data). As described in Section 2.3, we iteratively edit the input until the input is unchanged and then translate it. We present examples in Table 3. We 5 Conclusions can see that m"
2021.findings-emnlp.396,P19-1583,0,0.0203659,"ss of NMT can be roughly divided into three categories. In the first research line, adversarial examples are generated with black- or white-box methods. The generated adversarial examples are then used to combine with original training data for adversarial training (Ebrahimi et al., 2018; Chaturvedi et al., 2019; Cheng et al., 2019; Michel et al., 2019; Zhao et al., 2018; Cheng et al., 2020). In the second strand, a wide variety of methods have been proposed to deal with noise in training data (Schwenk, 2018; Guo et al., 2018; Xu and Koehn, 2017; Koehn et al., 2018; van der Wees et al., 2017; Wang and Neubig, 2019; Wang et al., 2018a,b, 2019). Finally, efforts have been also explored to directly cope with naturally occurring noise in texts, which are closely related to our work. Heigold et al. (2018); Belinkov and Bisk (2018); Levy et al. (2019) focus on word spelling errors. Sperber et al.; Liu et al. (2019) study translation problems caused by speech recognition. Vaibhav et al. (2019) introduce back-translation to generate more natural synthetic data, and employ extra tags to distinguish synthetic data from raw data. Zhou et al. (2019) propose a reconstruction method based on one encoder and two deco"
2021.findings-emnlp.396,D17-1319,0,0.0238081,"tion. 3.5 Iterative Editing Related Work Approaches to the robustness of NMT can be roughly divided into three categories. In the first research line, adversarial examples are generated with black- or white-box methods. The generated adversarial examples are then used to combine with original training data for adversarial training (Ebrahimi et al., 2018; Chaturvedi et al., 2019; Cheng et al., 2019; Michel et al., 2019; Zhao et al., 2018; Cheng et al., 2020). In the second strand, a wide variety of methods have been proposed to deal with noise in training data (Schwenk, 2018; Guo et al., 2018; Xu and Koehn, 2017; Koehn et al., 2018; van der Wees et al., 2017; Wang and Neubig, 2019; Wang et al., 2018a,b, 2019). Finally, efforts have been also explored to directly cope with naturally occurring noise in texts, which are closely related to our work. Heigold et al. (2018); Belinkov and Bisk (2018); Levy et al. (2019) focus on word spelling errors. Sperber et al.; Liu et al. (2019) study translation problems caused by speech recognition. Vaibhav et al. (2019) introduce back-translation to generate more natural synthetic data, and employ extra tags to distinguish synthetic data from raw data. Zhou et al. (2"
2021.findings-emnlp.396,D18-1316,0,0.0167893,"e, it is better to use SecocoE2E for its simplification and efficiency. Second, Secoco is more effective on the real-world test sets, showing its potential in real-world application. 3.5 Iterative Editing Related Work Approaches to the robustness of NMT can be roughly divided into three categories. In the first research line, adversarial examples are generated with black- or white-box methods. The generated adversarial examples are then used to combine with original training data for adversarial training (Ebrahimi et al., 2018; Chaturvedi et al., 2019; Cheng et al., 2019; Michel et al., 2019; Zhao et al., 2018; Cheng et al., 2020). In the second strand, a wide variety of methods have been proposed to deal with noise in training data (Schwenk, 2018; Guo et al., 2018; Xu and Koehn, 2017; Koehn et al., 2018; van der Wees et al., 2017; Wang and Neubig, 2019; Wang et al., 2018a,b, 2019). Finally, efforts have been also explored to directly cope with naturally occurring noise in texts, which are closely related to our work. Heigold et al. (2018); Belinkov and Bisk (2018); Levy et al. (2019) focus on word spelling errors. Sperber et al.; Liu et al. (2019) study translation problems caused by speech recogn"
2021.findings-emnlp.396,W19-5368,0,0.0796658,"ethod against the following three baseline systems. BASE One widely-used way to achieve NMT robustness is to mix raw clean data with noisy data to train NMT models. We refer to models trained with/without synthetic data as BASE/BASE+synthetic. R EPAIR To deal with noisy inputs, one might train a repair model to transform noisy inputs into clean inputs that a normally trained translation model can deal with. Both the repair and translation model are transformer-based models. As a pipeline model (repairing before translating), R EPAIR may suffer from error propagation. R ECONSTRUCTION We follow Zhou et al. (2019) to develop a multi-task based method to solve the robustness problem. We construct triples (clean input, noisy input, target translation), and introduce an additional decoder to obtain clean inputs from noisy inputs. This method enables NMT to transform a noisy input into a clean input and pass this knowledge into the translation decoder. 3.3 Data Baselines Settings In our studies, all translation models were Transformer-base. They were trained with a batch size of 32,000 tokens. The beam size was set to 5 during decoding. We used byte pair encoding compression algorithm (BPE) (Sennrich et al"
2021.findings-emnlp.396,D17-1147,0,0.060923,"Missing"
2021.findings-emnlp.396,P18-2048,0,0.0159752,"y divided into three categories. In the first research line, adversarial examples are generated with black- or white-box methods. The generated adversarial examples are then used to combine with original training data for adversarial training (Ebrahimi et al., 2018; Chaturvedi et al., 2019; Cheng et al., 2019; Michel et al., 2019; Zhao et al., 2018; Cheng et al., 2020). In the second strand, a wide variety of methods have been proposed to deal with noise in training data (Schwenk, 2018; Guo et al., 2018; Xu and Koehn, 2017; Koehn et al., 2018; van der Wees et al., 2017; Wang and Neubig, 2019; Wang et al., 2018a,b, 2019). Finally, efforts have been also explored to directly cope with naturally occurring noise in texts, which are closely related to our work. Heigold et al. (2018); Belinkov and Bisk (2018); Levy et al. (2019) focus on word spelling errors. Sperber et al.; Liu et al. (2019) study translation problems caused by speech recognition. Vaibhav et al. (2019) introduce back-translation to generate more natural synthetic data, and employ extra tags to distinguish synthetic data from raw data. Zhou et al. (2019) propose a reconstruction method based on one encoder and two decoders architecture t"
2021.findings-emnlp.396,2021.naacl-industry.14,1,0.749888,"-Edit), as illustrated in the right part of Figure 2. In general, Secoco-E2E provides better robustness without sacrificing decoding speed. For Secoco-Edit, iterative editing enables better interpretability. Detailed editing operations provide a different perspective on how the model resists noise. 3 Experiments 3.1 We conducted our experiments on three test sets, including Dialogue, Speech, and WMT14 En-De, to examine the effectiveness of Secoco. Dialogue is a real-world Chinese-English dialogue test set constructed based on TV drama subtitles1 , which contains three types of natural noises (Wang et al., 2021). Speech is an in-house ChineseEnglish speech translation test set which contains various noise from ASR. To evaluate Secoco on different language pairs, we also used WMT14 EnDe test sets to build a noisy test set with random deletion and insertion operations. Table 1 shows the details of the three test sets. For Chinese-English translation, we used WMT2020 Chinese-English data2 (48M) for Dialogue, and CCMT3 (9M) for Speech. For WMT En-De, we adopted the widely-used WMT14 training data4 (4.5M). We synthesized corresponding 1 https://github.com/rgwt123/DialogueMT http://www.statmt.org/wmt20/tra"
2021.findings-emnlp.396,P19-1123,0,0.0345329,"Missing"
2021.findings-emnlp.396,W18-6314,0,0.0205705,"y divided into three categories. In the first research line, adversarial examples are generated with black- or white-box methods. The generated adversarial examples are then used to combine with original training data for adversarial training (Ebrahimi et al., 2018; Chaturvedi et al., 2019; Cheng et al., 2019; Michel et al., 2019; Zhao et al., 2018; Cheng et al., 2020). In the second strand, a wide variety of methods have been proposed to deal with noise in training data (Schwenk, 2018; Guo et al., 2018; Xu and Koehn, 2017; Koehn et al., 2018; van der Wees et al., 2017; Wang and Neubig, 2019; Wang et al., 2018a,b, 2019). Finally, efforts have been also explored to directly cope with naturally occurring noise in texts, which are closely related to our work. Heigold et al. (2018); Belinkov and Bisk (2018); Levy et al. (2019) focus on word spelling errors. Sperber et al.; Liu et al. (2019) study translation problems caused by speech recognition. Vaibhav et al. (2019) introduce back-translation to generate more natural synthetic data, and employ extra tags to distinguish synthetic data from raw data. Zhou et al. (2019) propose a reconstruction method based on one encoder and two decoders architecture t"
2021.findings-emnlp.43,2020.findings-emnlp.372,0,0.433542,"the hypothesis from the MNLI dataset. The classifiers in shallow layers of a dynamic early exiting model cannot predict correctly, while BERT-Complete (Turc et al., 2019), a small BERT pre-trained from scratch with the same size can make a correct and confident prediction. which can be categorized into model-level compression and instance-level speed-up. The former aims at obtaining a compact model via quantization (Zafrir et al., 2019; Shen et al., 2020; Zhang et al., 2020), pruning (Voita et al., 2019; Michel et al., 2019) or knowledge distillation (KD) (Sanh et al., 2019; Sun et al., 2019; Jiao et al., 2020), while the latter adapts the amount of computation to the complexity of each instance (Graves, 2016). A mainstream method for instance-level speed-up is dynamic early exiting, which emits predictions based on intermediate classifiers (or off-ramps) of internal layers when the predictions are confident enough (Xin et al., 2020b; Liu et al., 2020; Schwartz et al., 2020; Li et al., 2021). In this paper, we focus on dynamic early exiting, as it can be utilized to accelerate inference 1 and reduce the potential risk of the overthinking Our code is available at https://github.com/ lancopku/CascadeB"
2021.findings-emnlp.43,2020.acl-main.703,0,0.0295578,"Missing"
2021.findings-emnlp.43,D16-1264,0,0.0150867,"idence-based emitting decisions more reliable. 4 Experiments Dataset MNLI MRPC QNLI QQP RTE SST-2 # Train # Dev # Test Metric  393k 3.7k 105k 364k 2.5k 67k 20k 0.4k 5.5k 40k 0.3k 0.9k 20k 1.7k 5.5k 391k 3k 1.8k Accuracy F1-score Accuracy F1-score Accuracy Accuracy 0.3 0.5 0.3 0.3 0.5 0.5 Table 1: Statistics of six classification datasets in GLUE benchmark. The selected difficulty margins  of each datasets are provided in the last column. 4.1 Experimental Settings We use six classification tasks in GLUE benchmark, including MNLI (Williams et al., 2018), MRPC (Dolan and Brockett, 2005), QNLI (Rajpurkar et al., 2016), QQP,2 RTE (Bentivogli et al., 2009) and SST-2 (Socher et al., 2013). The metrics for evaluation are F1-score for QQP and MRPC, and accuracy for the rest tasks. Our implementation is based on the Huggingface Transformers library (Wolf et al., 2020). We use two models for selection with 2 and 12 layers, respectively, since they can provide a wide range for acceleration. The difficulty score is thus evaluated based on the 2-layer model. The effect of incorporating more models in our cascade framework is explored in the later section. We utilize the weights provided by Turc et al. (2019) to init"
2021.findings-emnlp.43,2020.acl-main.593,0,0.238822,"aims at obtaining a compact model via quantization (Zafrir et al., 2019; Shen et al., 2020; Zhang et al., 2020), pruning (Voita et al., 2019; Michel et al., 2019) or knowledge distillation (KD) (Sanh et al., 2019; Sun et al., 2019; Jiao et al., 2020), while the latter adapts the amount of computation to the complexity of each instance (Graves, 2016). A mainstream method for instance-level speed-up is dynamic early exiting, which emits predictions based on intermediate classifiers (or off-ramps) of internal layers when the predictions are confident enough (Xin et al., 2020b; Liu et al., 2020; Schwartz et al., 2020; Li et al., 2021). In this paper, we focus on dynamic early exiting, as it can be utilized to accelerate inference 1 and reduce the potential risk of the overthinking Our code is available at https://github.com/ lancopku/CascadeBERT problem (Kaya et al., 2019). Such a paradigm is 475 Findings of the Association for Computational Linguistics: EMNLP 2021, pages 475–486 November 7–11, 2021. ©2021 Association for Computational Linguistics intuitive and simple, while faces a performance bottleneck under high speed-up ratios, i.e., the task performance is poor when most examples are exited in early"
2021.findings-emnlp.43,2020.sustainlp-1.11,0,0.3606,"d instance-level speed-up. The former aims at obtaining a compact model via quantization (Zafrir et al., 2019; Shen et al., 2020; Zhang et al., 2020), pruning (Voita et al., 2019; Michel et al., 2019) or knowledge distillation (KD) (Sanh et al., 2019; Sun et al., 2019; Jiao et al., 2020), while the latter adapts the amount of computation to the complexity of each instance (Graves, 2016). A mainstream method for instance-level speed-up is dynamic early exiting, which emits predictions based on intermediate classifiers (or off-ramps) of internal layers when the predictions are confident enough (Xin et al., 2020b; Liu et al., 2020; Schwartz et al., 2020; Li et al., 2021). In this paper, we focus on dynamic early exiting, as it can be utilized to accelerate inference 1 and reduce the potential risk of the overthinking Our code is available at https://github.com/ lancopku/CascadeBERT problem (Kaya et al., 2019). Such a paradigm is 475 Findings of the Association for Computational Linguistics: EMNLP 2021, pages 475–486 November 7–11, 2021. ©2021 Association for Computational Linguistics intuitive and simple, while faces a performance bottleneck under high speed-up ratios, i.e., the task performance is p"
2021.findings-emnlp.43,2020.acl-main.204,0,0.31551,"d instance-level speed-up. The former aims at obtaining a compact model via quantization (Zafrir et al., 2019; Shen et al., 2020; Zhang et al., 2020), pruning (Voita et al., 2019; Michel et al., 2019) or knowledge distillation (KD) (Sanh et al., 2019; Sun et al., 2019; Jiao et al., 2020), while the latter adapts the amount of computation to the complexity of each instance (Graves, 2016). A mainstream method for instance-level speed-up is dynamic early exiting, which emits predictions based on intermediate classifiers (or off-ramps) of internal layers when the predictions are confident enough (Xin et al., 2020b; Liu et al., 2020; Schwartz et al., 2020; Li et al., 2021). In this paper, we focus on dynamic early exiting, as it can be utilized to accelerate inference 1 and reduce the potential risk of the overthinking Our code is available at https://github.com/ lancopku/CascadeBERT problem (Kaya et al., 2019). Such a paradigm is 475 Findings of the Association for Computational Linguistics: EMNLP 2021, pages 475–486 November 7–11, 2021. ©2021 Association for Computational Linguistics intuitive and simple, while faces a performance bottleneck under high speed-up ratios, i.e., the task performance is p"
2021.findings-emnlp.43,D13-1170,0,0.0217053,"in the DeeBERT of shallow layers is lower than that of BERT-kl and BERT-Complete, which leads to more wrongly emitted instances. The exiting decisions in shallow layers of DeeBERT thus can be unreliable. 2.2 modeling (MLM) objective. We assume the representations of this model contain high-level semantic information, as MLM requires a deep understanding of the language. For a fair comparison, models are evaluated on a subset of instances which DeeBERT chooses to emit at different layers. We report prediction accuracy using different number of layers on MNLI (Williams et al., 2018) and SST-2 (Socher et al., 2013). Figure 2 shows the results on the development sets, and we can see that: (1) BERT-Complete clearly outperforms DeeBERT, especially when the predictions are made based on shallow layers. It indicates that the highlevel semantics is vital for handling tasks like sentence-level classification. (2) BERT-kL also outperforms DeeBERT. We attribute it to that the last serveral layers can learn task-specific information during fine-tuning to obtain a decent performance. A similar phenomenon is also observed by Merchant et al. (2020). However, since the internal layer representation in DeeBERT are res"
2021.findings-emnlp.43,2020.emnlp-main.633,0,0.22672,"Missing"
2021.findings-emnlp.78,2020.coling-main.155,0,0.0366559,"SD. We first construct a large-scale Chinese lexical sample WSD dataset with word-formations. Then, we propose a model FormBERT to explicitly incorporate word-formations into sense disambiguation. To further enhance generalizability, we design a word-formation predictor module in case word-formation annotations are unavailable. Experimental results show that our method brings substantial performance improvement over strong baselines. 1 1 Introduction Word sense disambiguation (WSD) aims to identify the sense of a polysemous word in a specific context, which benefits multiple downstream tasks (Hou et al., 2020). With copious senseannotated data (Raganato et al., 2017), neural WSD methods achieve superior performance by leveraging definitional and relational features in knowledge bases (KB) (Luo et al., 2018a; Huang et al., 2019; Bevilacqua and Navigli, 2020). In parataxis languages like Chinese, word meanings are highly correlated with word-formations (Li et al., 2018), which have not been explored in WSD thus far. Specifically, word-formations designate how characters interact to construct meanings. As shown in Figure 1, “征文1 "" with the Modifier-Head formation means solicited paper, where “征"" (soli"
2021.findings-emnlp.78,D19-1355,0,0.21438,"ralizability, we design a word-formation predictor module in case word-formation annotations are unavailable. Experimental results show that our method brings substantial performance improvement over strong baselines. 1 1 Introduction Word sense disambiguation (WSD) aims to identify the sense of a polysemous word in a specific context, which benefits multiple downstream tasks (Hou et al., 2020). With copious senseannotated data (Raganato et al., 2017), neural WSD methods achieve superior performance by leveraging definitional and relational features in knowledge bases (KB) (Luo et al., 2018a; Huang et al., 2019; Bevilacqua and Navigli, 2020). In parataxis languages like Chinese, word meanings are highly correlated with word-formations (Li et al., 2018), which have not been explored in WSD thus far. Specifically, word-formations designate how characters interact to construct meanings. As shown in Figure 1, “征文1 "" with the Modifier-Head formation means solicited paper, where “征"" (solicit) modifies “文"" (paper); “征文2 "" with the VerbObject formation means solicit paper, where “征"" Word-Formation Modifier-Head …接收征文… … accept solicited paper … …开始征文… 征收的文章 solicited paper 征: 征收 solicit 文: 文章 paper Definiti"
2021.findings-emnlp.78,S07-1004,0,0.0663487,"Missing"
2021.findings-emnlp.78,P19-1568,0,0.0366864,"Missing"
2021.findings-emnlp.78,P18-2023,0,0.151903,"ethod brings substantial performance improvement over strong baselines. 1 1 Introduction Word sense disambiguation (WSD) aims to identify the sense of a polysemous word in a specific context, which benefits multiple downstream tasks (Hou et al., 2020). With copious senseannotated data (Raganato et al., 2017), neural WSD methods achieve superior performance by leveraging definitional and relational features in knowledge bases (KB) (Luo et al., 2018a; Huang et al., 2019; Bevilacqua and Navigli, 2020). In parataxis languages like Chinese, word meanings are highly correlated with word-formations (Li et al., 2018), which have not been explored in WSD thus far. Specifically, word-formations designate how characters interact to construct meanings. As shown in Figure 1, “征文1 "" with the Modifier-Head formation means solicited paper, where “征"" (solicit) modifies “文"" (paper); “征文2 "" with the VerbObject formation means solicit paper, where “征"" Word-Formation Modifier-Head …接收征文… … accept solicited paper … …开始征文… 征收的文章 solicited paper 征: 征收 solicit 文: 文章 paper Definition Verb-Object … start to solicit paper … 征收文章 solicit paper Figure 1: The contexts indicate that the word “征 文"" holds two senses constructed by"
2021.findings-emnlp.78,D18-1170,1,0.929226,"urther enhance generalizability, we design a word-formation predictor module in case word-formation annotations are unavailable. Experimental results show that our method brings substantial performance improvement over strong baselines. 1 1 Introduction Word sense disambiguation (WSD) aims to identify the sense of a polysemous word in a specific context, which benefits multiple downstream tasks (Hou et al., 2020). With copious senseannotated data (Raganato et al., 2017), neural WSD methods achieve superior performance by leveraging definitional and relational features in knowledge bases (KB) (Luo et al., 2018a; Huang et al., 2019; Bevilacqua and Navigli, 2020). In parataxis languages like Chinese, word meanings are highly correlated with word-formations (Li et al., 2018), which have not been explored in WSD thus far. Specifically, word-formations designate how characters interact to construct meanings. As shown in Figure 1, “征文1 "" with the Modifier-Head formation means solicited paper, where “征"" (solicit) modifies “文"" (paper); “征文2 "" with the VerbObject formation means solicit paper, where “征"" Word-Formation Modifier-Head …接收征文… … accept solicited paper … …开始征文… 征收的文章 solicited paper 征: 征收 solicit"
2021.findings-emnlp.78,P18-1230,1,0.905863,"urther enhance generalizability, we design a word-formation predictor module in case word-formation annotations are unavailable. Experimental results show that our method brings substantial performance improvement over strong baselines. 1 1 Introduction Word sense disambiguation (WSD) aims to identify the sense of a polysemous word in a specific context, which benefits multiple downstream tasks (Hou et al., 2020). With copious senseannotated data (Raganato et al., 2017), neural WSD methods achieve superior performance by leveraging definitional and relational features in knowledge bases (KB) (Luo et al., 2018a; Huang et al., 2019; Bevilacqua and Navigli, 2020). In parataxis languages like Chinese, word meanings are highly correlated with word-formations (Li et al., 2018), which have not been explored in WSD thus far. Specifically, word-formations designate how characters interact to construct meanings. As shown in Figure 1, “征文1 "" with the Modifier-Head formation means solicited paper, where “征"" (solicit) modifies “文"" (paper); “征文2 "" with the VerbObject formation means solicit paper, where “征"" Word-Formation Modifier-Head …接收征文… … accept solicited paper … …开始征文… 征收的文章 solicited paper 征: 征收 solicit"
2021.findings-emnlp.78,W04-0847,0,0.116833,"Missing"
2021.findings-emnlp.78,E17-1010,0,0.0220478,"sample WSD dataset with word-formations. Then, we propose a model FormBERT to explicitly incorporate word-formations into sense disambiguation. To further enhance generalizability, we design a word-formation predictor module in case word-formation annotations are unavailable. Experimental results show that our method brings substantial performance improvement over strong baselines. 1 1 Introduction Word sense disambiguation (WSD) aims to identify the sense of a polysemous word in a specific context, which benefits multiple downstream tasks (Hou et al., 2020). With copious senseannotated data (Raganato et al., 2017), neural WSD methods achieve superior performance by leveraging definitional and relational features in knowledge bases (KB) (Luo et al., 2018a; Huang et al., 2019; Bevilacqua and Navigli, 2020). In parataxis languages like Chinese, word meanings are highly correlated with word-formations (Li et al., 2018), which have not been explored in WSD thus far. Specifically, word-formations designate how characters interact to construct meanings. As shown in Figure 1, “征文1 "" with the Modifier-Head formation means solicited paper, where “征"" (solicit) modifies “文"" (paper); “征文2 "" with the VerbObject form"
2021.findings-emnlp.78,W13-4302,0,0.0799539,"Missing"
2021.findings-emnlp.78,2021.naacl-main.437,1,0.780753,"; Jin et al., 2007; Agirre et al., 2009; Hou et al., 2020) are small in vocabulary size (less than 100 words except for Agirre et al., 2009), and it is uneasy to combine these datasets to enlarge their size, since they differ in format, sense inventory and construction guidelines. Word-Formation knowledge: Instead of combining roots and affixes, Chinese words are constructed by characters using word-formations (Zhu et al., 2019). Word-formations have shown to be effective in multiple tasks like learning embeddings for parataxis languages (Park et al., 2018; Li et al., 2018; Lin and Liu, 2019; Zheng et al., 2021a,b). However, these works lack a clear distinction among different word-formations which require manual annotations. 3 The FiCLS Dataset The construction of FiCLS includes two phases: collecting a base dataset and annotating wordformations. Each FiCLS entry consists of (1) a word, (2) a sense definition, (3) a word-formation, and (4) a context sentence. 3.1 Chinese WSD Dataset We first construct a Chinese lexical sample WSD base dataset. We build the sense inventory based on the 5th edition of the Contemporary Chinese Dictionary (CCD) published by the Commercial Press,2 one of the most influe"
2021.findings-emnlp.78,2021.ccl-1.36,1,0.722417,"; Jin et al., 2007; Agirre et al., 2009; Hou et al., 2020) are small in vocabulary size (less than 100 words except for Agirre et al., 2009), and it is uneasy to combine these datasets to enlarge their size, since they differ in format, sense inventory and construction guidelines. Word-Formation knowledge: Instead of combining roots and affixes, Chinese words are constructed by characters using word-formations (Zhu et al., 2019). Word-formations have shown to be effective in multiple tasks like learning embeddings for parataxis languages (Park et al., 2018; Li et al., 2018; Lin and Liu, 2019; Zheng et al., 2021a,b). However, these works lack a clear distinction among different word-formations which require manual annotations. 3 The FiCLS Dataset The construction of FiCLS includes two phases: collecting a base dataset and annotating wordformations. Each FiCLS entry consists of (1) a word, (2) a sense definition, (3) a word-formation, and (4) a context sentence. 3.1 Chinese WSD Dataset We first construct a Chinese lexical sample WSD base dataset. We build the sense inventory based on the 5th edition of the Contemporary Chinese Dictionary (CCD) published by the Commercial Press,2 one of the most influe"
2021.findings-emnlp.78,N19-1097,0,0.0631183,"Missing"
2021.iwslt-1.6,2020.iwslt-1.3,0,0.249258,"Missing"
2021.iwslt-1.6,N19-1006,0,0.0218955,"AI Lab), including cascade and end-to-end speech translation (ST) systems for the offline ST track and a simultaneous neural machine translation (NMT) system. We aim at finding the best practice for these two tracks. For offline ST, the cascaded system often outperforms the fully end-to-end approach. Recent studies on the fully end-to-end approaches obtain promising results and attract a lot of interest. Last year’s results have shown that an end-to-end model achieves an even better performance (Ansari et al., 2020) compared with the cascaded competitors. However, they introduce pre-training (Bansal et al., 2019; Stoian et al., 2020; Wang et al., 2020; Alinejad and Sarkar, 2020) and data augmentation techniques (Jia et al., 2019; Pino et al., 2020) to end-toend models, while the cascaded is not that strong 1 Code and models are available at https: //github.com/bytedance/neurst/tree/ master/examples/iwslt21 64 Proceedings of the 18th International Conference on Spoken Language Translation, pages 64–74 Bangkok, Thailand (Online), August 5–6, 2021. ©2021 Association for Computational Linguistics Dataset MuST-C LibriSpeech Common Voice iwslt-corpus TED-LIUM 3 #samples #hours hello everyone Hallo zusammen"
2021.iwslt-1.6,W19-5206,0,0.0143123,"mber of sentences from data source s, and sampling temperature T is set to 5. Note that the MT#1 is trained on lowercased source texts without punctuation marks, while MT#2-5 use the tagged transcripts. Tagged Back-Translation Back-translation (Sennrich et al., 2016a) is an effective way to improve the translation quality by leveraging a large amount of monolingual data and has been widely used in WMT evaluation campaigns. In our setting, we add a “<BT>” tag to the source side of back-translated data to prevent overfitting on the synthetic data, which is also known as tagged back-translation (Caswell et al., 2019; Marie et al., 2020). Model Setups We follow the transformer big setting, except that • we deepen the encoder layers to 16. • the dropout rate is set 0.15. • the model width is changed to 768, the hidden size of the feed-forward layer is 3,072, and the attention head is 12 for MT#5 only. We use Adam optimizer with the same schedule algorithm as Vaswani et al. (2017). All models are trained with a global batch size of 65,536. Knowledge Distillation Sequence-level knowledge distillation (Kim and Rush, 2016; Freitag et al., 2017) is another useful technique to improve performance. In this way, w"
2021.iwslt-1.6,N19-1202,0,0.16845,"Missing"
2021.iwslt-1.6,2020.emnlp-main.644,0,0.0264713,"(ST) systems for the offline ST track and a simultaneous neural machine translation (NMT) system. We aim at finding the best practice for these two tracks. For offline ST, the cascaded system often outperforms the fully end-to-end approach. Recent studies on the fully end-to-end approaches obtain promising results and attract a lot of interest. Last year’s results have shown that an end-to-end model achieves an even better performance (Ansari et al., 2020) compared with the cascaded competitors. However, they introduce pre-training (Bansal et al., 2019; Stoian et al., 2020; Wang et al., 2020; Alinejad and Sarkar, 2020) and data augmentation techniques (Jia et al., 2019; Pino et al., 2020) to end-toend models, while the cascaded is not that strong 1 Code and models are available at https: //github.com/bytedance/neurst/tree/ master/examples/iwslt21 64 Proceedings of the 18th International Conference on Spoken Language Translation, pages 64–74 Bangkok, Thailand (Online), August 5–6, 2021. ©2021 Association for Computational Linguistics Dataset MuST-C LibriSpeech Common Voice iwslt-corpus TED-LIUM 3 #samples #hours hello everyone Hallo zusammen. 250,942 281,241 562,517 157,909 111,600 450 961 899 231 165 Transf"
2021.iwslt-1.6,W18-6319,0,0.0158439,"moved from both hypothesis and reference. We use word error rate (WER) to evaluate the ASR model and report case-sensitive detokenized BLEU10 for MT. No other data segmentation techniques are applied to the dev/test sets. Results on MuST-C dev and tst-COMMON, as well as dev(v1) and tst-COMMON(v1) from MuST-C v1 (Gangi et al., 2019) are listed together, which serve as strong baselines for comparison purpose in the end-to-end speech translation field. When evaluating the simultaneous translation, we use the official SimulEval (Ma et al., 2020) toolkit and report case-sensitive detokenized BLEU (Post, 2018) and Average Lagging (Ma et al., 2019) Inference We explore the look-ahead beam search strategy for inference. Specifically, we apply beam search to generate M (M > 1) tokens at each decoding step and pick the first token in the one with the highest log-probability out of multiple decoding paths. The look-ahead beam search achieves consistent performance improvement when keval is small while its 10 https://github.com/jniehues-kit/ sacrebleu 69 # System Training data composition dev tst-COM dev(v1) tst-COM(v1) Pure MT 1 MT (w/o punc. & lc) 2 MT (w/ punc. & tc) 3 ensemble MT (w/o punc. & lc) 4 e"
2021.iwslt-1.6,D16-1139,0,0.177957,"test set surpasses the end-to-end baseline by 7.9 BLUE scores, while it is still lagging behind our cascade model by 1.5 BLUE scores. It is not surprising since some well-optimized methods for MT can not be easily used on ST, such as back translation. However, our experience shows that the external data can effectively close the gap between end-to-end models and cascade models. In parallel, we also participate in the simultaneous NMT track, which translates in real-time. Our system is based on an efficient wait-k model (Elbayad et al., 2020). We investigate large-scale knowledge distillation (Kim and Rush, 2016; Freitag et al., 2017) and back translation methods. Specially, we develop a multi-path training strategy, which enables a unified model serving different wait-k paths. Our target is to obtain the best translation quality at different latency levels. The remaining part of the paper proceeds as follows. Section 2 and section 3 describe our cascade and end-to-end systems respectively. Section 4 presents the implementation of simultaneous NMT models. Each section starts from the training sources and how we synthesize large-scale data. And then, we give details about the model structure and techn"
2021.iwslt-1.6,P16-1009,0,0.0426042,"the model ensemble. The detailed setups are listed in Table 2. We over-sample the in-domain datasets (i.e., MuST-C/iwslt-corpusrelated portions) to improve the in-domain performance. Specifically, to control the ratio of samples from different data sources, we sample a fixed num1 ber of sentences being proportional to ( PNsNs ) T , s where Ns is the number of sentences from data source s, and sampling temperature T is set to 5. Note that the MT#1 is trained on lowercased source texts without punctuation marks, while MT#2-5 use the tagged transcripts. Tagged Back-Translation Back-translation (Sennrich et al., 2016a) is an effective way to improve the translation quality by leveraging a large amount of monolingual data and has been widely used in WMT evaluation campaigns. In our setting, we add a “<BT>” tag to the source side of back-translated data to prevent overfitting on the synthetic data, which is also known as tagged back-translation (Caswell et al., 2019; Marie et al., 2020). Model Setups We follow the transformer big setting, except that • we deepen the encoder layers to 16. • the dropout rate is set 0.15. • the model width is changed to 768, the hidden size of the feed-forward layer is 3,072,"
2021.iwslt-1.6,P16-1162,0,0.0831726,"the model ensemble. The detailed setups are listed in Table 2. We over-sample the in-domain datasets (i.e., MuST-C/iwslt-corpusrelated portions) to improve the in-domain performance. Specifically, to control the ratio of samples from different data sources, we sample a fixed num1 ber of sentences being proportional to ( PNsNs ) T , s where Ns is the number of sentences from data source s, and sampling temperature T is set to 5. Note that the MT#1 is trained on lowercased source texts without punctuation marks, while MT#2-5 use the tagged transcripts. Tagged Back-Translation Back-translation (Sennrich et al., 2016a) is an effective way to improve the translation quality by leveraging a large amount of monolingual data and has been widely used in WMT evaluation campaigns. In our setting, we add a “<BT>” tag to the source side of back-translated data to prevent overfitting on the synthetic data, which is also known as tagged back-translation (Caswell et al., 2019; Marie et al., 2020). Model Setups We follow the transformer big setting, except that • we deepen the encoder layers to 16. • the dropout rate is set 0.15. • the model width is changed to 768, the hidden size of the feed-forward layer is 3,072,"
2021.iwslt-1.6,2020.emnlp-demos.19,0,0.0351337,"ting the offline ST models, tags such as applause and laughing are removed from both hypothesis and reference. We use word error rate (WER) to evaluate the ASR model and report case-sensitive detokenized BLEU10 for MT. No other data segmentation techniques are applied to the dev/test sets. Results on MuST-C dev and tst-COMMON, as well as dev(v1) and tst-COMMON(v1) from MuST-C v1 (Gangi et al., 2019) are listed together, which serve as strong baselines for comparison purpose in the end-to-end speech translation field. When evaluating the simultaneous translation, we use the official SimulEval (Ma et al., 2020) toolkit and report case-sensitive detokenized BLEU (Post, 2018) and Average Lagging (Ma et al., 2019) Inference We explore the look-ahead beam search strategy for inference. Specifically, we apply beam search to generate M (M > 1) tokens at each decoding step and pick the first token in the one with the highest log-probability out of multiple decoding paths. The look-ahead beam search achieves consistent performance improvement when keval is small while its 10 https://github.com/jniehues-kit/ sacrebleu 69 # System Training data composition dev tst-COM dev(v1) tst-COM(v1) Pure MT 1 MT (w/o pun"
2021.iwslt-1.6,2020.acl-main.344,0,0.0966598,"speech translation (ST) systems for the offline ST track and a simultaneous neural machine translation (NMT) system. We aim at finding the best practice for these two tracks. For offline ST, the cascaded system often outperforms the fully end-to-end approach. Recent studies on the fully end-to-end approaches obtain promising results and attract a lot of interest. Last year’s results have shown that an end-to-end model achieves an even better performance (Ansari et al., 2020) compared with the cascaded competitors. However, they introduce pre-training (Bansal et al., 2019; Stoian et al., 2020; Wang et al., 2020; Alinejad and Sarkar, 2020) and data augmentation techniques (Jia et al., 2019; Pino et al., 2020) to end-toend models, while the cascaded is not that strong 1 Code and models are available at https: //github.com/bytedance/neurst/tree/ master/examples/iwslt21 64 Proceedings of the 18th International Conference on Spoken Language Translation, pages 64–74 Bangkok, Thailand (Online), August 5–6, 2021. ©2021 Association for Computational Linguistics Dataset MuST-C LibriSpeech Common Voice iwslt-corpus TED-LIUM 3 #samples #hours hello everyone Hallo zusammen. 250,942 281,241 562,517 157,909 111,60"
2021.iwslt-1.6,2020.acl-main.532,0,0.0133063,"data source s, and sampling temperature T is set to 5. Note that the MT#1 is trained on lowercased source texts without punctuation marks, while MT#2-5 use the tagged transcripts. Tagged Back-Translation Back-translation (Sennrich et al., 2016a) is an effective way to improve the translation quality by leveraging a large amount of monolingual data and has been widely used in WMT evaluation campaigns. In our setting, we add a “<BT>” tag to the source side of back-translated data to prevent overfitting on the synthetic data, which is also known as tagged back-translation (Caswell et al., 2019; Marie et al., 2020). Model Setups We follow the transformer big setting, except that • we deepen the encoder layers to 16. • the dropout rate is set 0.15. • the model width is changed to 768, the hidden size of the feed-forward layer is 3,072, and the attention head is 12 for MT#5 only. We use Adam optimizer with the same schedule algorithm as Vaswani et al. (2017). All models are trained with a global batch size of 65,536. Knowledge Distillation Sequence-level knowledge distillation (Kim and Rush, 2016; Freitag et al., 2017) is another useful technique to improve performance. In this way, we enlarge the trainin"
2021.iwslt-1.6,2021.acl-demo.7,1,0.860965,"Missing"
2021.naacl-industry.12,N19-1121,0,0.0183821,"system has First, we test the baseline supervised system, that is, only En → F r and F r → En are conducted on the model. Due to difference in model architecture, the performance Robustness on Parallel Data Scale As shown in Table 4, C UNMT is robust to the parallel data 93 System Supervised Training C UNMT + Forward C UNMT + Backward En-Fr 39.70 39.26 39.12 Fr-En 36.62 36.82 36.20 Wang et al., 2018; Gu et al., 2018; Edunov et al., 2018; Yang et al., 2020). Others also try to utilize parallel data of rich resource language pairs and also monolingual data (Ren et al., 2018; Wang et al., 2019; Al-Shedivat and Parikh, 2019; Lin et al., 2020). (Ren et al., 2018) also proposed a triangular architecture, but their work still relied on parallel data of low resource language pairs. With the joint support of parallel and monolingual data, the performance of a low resource system can be improved. Table 4: Translation performance on supervised directions of C UNMT. of C UNMT is slightly lower than that of its state of the art counterparts. Also, some techniques such as model average are not applied, and two directions are trained in one model. In C UNMT, the performance of supervised directions drops a little, but in e"
2021.naacl-industry.12,N18-1032,0,0.021685,"e important than the auxiliary data scale. Benefits as All in One Model In table 4, the performance of supervised directions are shown to illustrate the effects on which jointly training a single system has First, we test the baseline supervised system, that is, only En → F r and F r → En are conducted on the model. Due to difference in model architecture, the performance Robustness on Parallel Data Scale As shown in Table 4, C UNMT is robust to the parallel data 93 System Supervised Training C UNMT + Forward C UNMT + Backward En-Fr 39.70 39.26 39.12 Fr-En 36.62 36.82 36.20 Wang et al., 2018; Gu et al., 2018; Edunov et al., 2018; Yang et al., 2020). Others also try to utilize parallel data of rich resource language pairs and also monolingual data (Ren et al., 2018; Wang et al., 2019; Al-Shedivat and Parikh, 2019; Lin et al., 2020). (Ren et al., 2018) also proposed a triangular architecture, but their work still relied on parallel data of low resource language pairs. With the joint support of parallel and monolingual data, the performance of a low resource system can be improved. Table 4: Translation performance on supervised directions of C UNMT. of C UNMT is slightly lower than that of its state"
2021.naacl-industry.12,P17-1042,0,0.0117438,"n supervised directions of C UNMT. of C UNMT is slightly lower than that of its state of the art counterparts. Also, some techniques such as model average are not applied, and two directions are trained in one model. In C UNMT, the performance of supervised directions drops a little, but in exchange, the performances of zero-shot directions are greatly improved and the model is convenient to serve for multiple translation directions. Unsupervised NMT In 2017, pure unsupervised machine translation method with only monolingual data was proven to be feasible. On the basis of embedding alignment (Artetxe et al., 2017; Lample et al., 2018b), (Lample et al., 2018a) and (Artetxe et al., 2018b) devised similar methods for fully unsupervised machine translation. Considerable work has been done to improve the unsupervised machine translation systems by methods such as statistical machine translation (Lample et al., 2018c; Artetxe et al., 2018a; Ren et al., 2019; Artetxe et al., 2019), pretraining models (Lample and Conneau, 2019; Song et al., 2019), or others (Wu et al., 2019), and all of which greatly improve the performance of unsupervised machine translation. Our work attempts to utilize both monolingual and"
2021.naacl-industry.12,D18-1399,0,0.0746721,"ural machine translation (NMT) has achieved great success and reached satisfactory translation performance for several language pairs (Bahdanau et al., 2015; Gehring et al., 2017; Vaswani et al., 2017). Such breakthroughs heavily depend on the availability of colossal amounts of bilingual sentence pairs, such as the some 40 million parallel sentence pairs used in the training of WMT14 English French Task. As bilingual sentence pairs are costly to collect, the success of NMT has not been fully duplicated in the vast majority of language pairs, especially for zero-resource languages. Recently, (Artetxe et al., 2018b; Lample et al., 2018a; ?) tackled this challenge by training unsupervised neural machine translation (UNMT) models using only monolingual data, which achieves considerably high accuracy, but still not on par with that of the state of the art supervised models. 89 Proceedings of NAACL HLT 2021: IndustryTrack Papers, pages 89–96 June 6–11, 2021. ©2021 Association for Computational Linguistics tion (Figure 1(d)). We introduce cross-lingual supervision which aims at modeling explicit translation probabilities across languages. Taking three languages as an example, suppose the target unsupervised"
2021.naacl-industry.12,P19-1019,0,0.0178482,"the model is convenient to serve for multiple translation directions. Unsupervised NMT In 2017, pure unsupervised machine translation method with only monolingual data was proven to be feasible. On the basis of embedding alignment (Artetxe et al., 2017; Lample et al., 2018b), (Lample et al., 2018a) and (Artetxe et al., 2018b) devised similar methods for fully unsupervised machine translation. Considerable work has been done to improve the unsupervised machine translation systems by methods such as statistical machine translation (Lample et al., 2018c; Artetxe et al., 2018a; Ren et al., 2019; Artetxe et al., 2019), pretraining models (Lample and Conneau, 2019; Song et al., 2019), or others (Wu et al., 2019), and all of which greatly improve the performance of unsupervised machine translation. Our work attempts to utilize both monolingual and parallel data, and combine unsupervised and supervised machine translation through multilingual translation method into a single model C UNMT to ensure better performance for unsupervised language pairs. Strategies of Synthetic Data Generation For the synthetic data generation, the reported results are from greedy decoding for time efficiency. We compared the effec"
2021.naacl-industry.12,J82-2005,0,0.441809,"Missing"
2021.naacl-industry.12,P17-1176,0,0.0217729,", the performance of beam search is slightly inferior. A possible reason is that the beam search makes the synthetic data further biased on the learned pattern. The results suggest that C UNMT is exceedingly robust to the sampling strategies when performing forward and backward cross translation. 5 Related Work 6 Multilingual NMT It has been proven low resource machine translation can adopt methods to utilize other rich resource data in order to develop a better system. These methods include multilingual translation system (Firat et al., 2016; Johnson et al., 2017), teacher-student framework (Chen et al., 2017), or others (Zheng et al., 2017). Apart from parallel data as an entry point, many attempts have been made to explore the usefulness of monolingual data, including semi-supervised methods and unsupervised methods which only monolingual data is used. Much work also has been done to attempt to marry monolingual data with supervised data to create a better system, some of which include using small amounts of parallel data and augment the system with monolingual data (Sennrich et al., 2016; He et al., 2016; Conclusion In this work, we propose a multilingual machine translation framework C UNMT inc"
2021.naacl-industry.12,D18-1045,0,0.02103,"the auxiliary data scale. Benefits as All in One Model In table 4, the performance of supervised directions are shown to illustrate the effects on which jointly training a single system has First, we test the baseline supervised system, that is, only En → F r and F r → En are conducted on the model. Due to difference in model architecture, the performance Robustness on Parallel Data Scale As shown in Table 4, C UNMT is robust to the parallel data 93 System Supervised Training C UNMT + Forward C UNMT + Backward En-Fr 39.70 39.26 39.12 Fr-En 36.62 36.82 36.20 Wang et al., 2018; Gu et al., 2018; Edunov et al., 2018; Yang et al., 2020). Others also try to utilize parallel data of rich resource language pairs and also monolingual data (Ren et al., 2018; Wang et al., 2019; Al-Shedivat and Parikh, 2019; Lin et al., 2020). (Ren et al., 2018) also proposed a triangular architecture, but their work still relied on parallel data of low resource language pairs. With the joint support of parallel and monolingual data, the performance of a low resource system can be improved. Table 4: Translation performance on supervised directions of C UNMT. of C UNMT is slightly lower than that of its state of the art counterpa"
2021.naacl-industry.12,2020.emnlp-main.210,1,0.72213,"baseline supervised system, that is, only En → F r and F r → En are conducted on the model. Due to difference in model architecture, the performance Robustness on Parallel Data Scale As shown in Table 4, C UNMT is robust to the parallel data 93 System Supervised Training C UNMT + Forward C UNMT + Backward En-Fr 39.70 39.26 39.12 Fr-En 36.62 36.82 36.20 Wang et al., 2018; Gu et al., 2018; Edunov et al., 2018; Yang et al., 2020). Others also try to utilize parallel data of rich resource language pairs and also monolingual data (Ren et al., 2018; Wang et al., 2019; Al-Shedivat and Parikh, 2019; Lin et al., 2020). (Ren et al., 2018) also proposed a triangular architecture, but their work still relied on parallel data of low resource language pairs. With the joint support of parallel and monolingual data, the performance of a low resource system can be improved. Table 4: Translation performance on supervised directions of C UNMT. of C UNMT is slightly lower than that of its state of the art counterparts. Also, some techniques such as model average are not applied, and two directions are trained in one model. In C UNMT, the performance of supervised directions drops a little, but in exchange, the perfor"
2021.naacl-industry.12,D16-1026,0,0.0305797,"Missing"
2021.naacl-industry.12,2020.tacl-1.47,0,0.0624579,"man, 20 million sentences from available WMT monolingual News Crawl datasets were randomly selected. For Romanian monolingual data, all of the available Romanian sentences from News Crawl dataset were used and and were supplemented with WMT16 monolingual data to yield a total of in 2.9 million sentences. For parallel data, we use the standard WMT 2014 English-French dataset consisting of about 36M sentence pairs, and the 91 Supervised Transformer Comparison systems of UNMT UNMT (Lample et al., 2018c) EMB (Lample and Conneau, 2019) MLM (Lample and Conneau, 2019) MASS (Song et al., 2019) MBART (Liu et al., 2020) C UNMT C UNMT w/o Para. C UNMT w/ Para. C UNMT + Forward C UNMT + Backward + Forward (Fr, En, De) En-Fr Fr-En 41.0 - (De, En, Fr) En-De De-En 34.0 38.6 (Ro, En, Fr) En-Ro Ro-En 34.3 34.0 25.1 29.4 33.4 37.5 - 24.2 29.4 33.3 34.9 - 17.2 21.3 26.4 28.3 29.8 21.0 27.3 34.3 35.2 34.0 21.2 27.5 33.3 35.2 35.0 19.4 26.6 31.8 33.1 30.5 32.90 34.37 35.88 37.60 31.93 32.77 33.64 35.18 23.03 23.99 26.50 27.60 31.01 31.98 33.11 34.10 33.23 33.95 34.12 35.09 32.34 33.15 33.61 33.95 Table 1: Main results comparisons. MASS uses large scale pre-training and back translation during fine-tuning. MBART employ"
2021.naacl-industry.12,P16-1009,0,0.0259438,"s include multilingual translation system (Firat et al., 2016; Johnson et al., 2017), teacher-student framework (Chen et al., 2017), or others (Zheng et al., 2017). Apart from parallel data as an entry point, many attempts have been made to explore the usefulness of monolingual data, including semi-supervised methods and unsupervised methods which only monolingual data is used. Much work also has been done to attempt to marry monolingual data with supervised data to create a better system, some of which include using small amounts of parallel data and augment the system with monolingual data (Sennrich et al., 2016; He et al., 2016; Conclusion In this work, we propose a multilingual machine translation framework C UNMT incorporating distant supervision to tackle the challenge of the unsupervised translation task. By mixing different training schemes into one model and utilizing unrelated bilingual corpus, we greatly improve the performance of the unsupervised NMT direction. By joint training, C UNMT can serve all translation directions in one model. Empirically, C UNMT has been proven to deliver substantial improvements over several strong UNMT competitors and even achieve comparable performance to supe"
2021.naacl-industry.12,N19-1120,0,0.0358227,"Missing"
2021.naacl-industry.14,W19-4822,0,0.012826,"gual dialogue. Recently WMT2020 has also proposed a new shared task - machine translation for chats,4 focusing on bilingual customer support chats (Farajian et al., 2020). Robust Training Neural models have been usually affected by noisy issues. Many efforts (Li et al., 2017; Sperber et al., 2017; Vaibhav et al., 2019; Yang et al., 2020) focus on data augmentation to alleviate the problem by adding synthetic noise to the training set. However, generating noise has always been a challenge, as natural noise is always more diversified than artificially constructed noise (Belinkov and Bisk, 2018; Anastasopoulos, 2019; Anastasopoulos et al., 2019). 7 Conclusions In this paper, we manually analyze challenges in dialogue translation and detect three main problems. In order to tackle these issues, we propose a multitask learning method with contextual labeling. For deep evaluation, we construct dialogues with translation and detailed annotations as a benchmark test set. Our proposed model achieves substantial improvements over the baselines. What is more, we further analyze the performance of contextual labeling and pronoun recovery errors. Acknowledgments We thank the bilingual speakers for test set construc"
2021.naacl-industry.14,N19-1311,0,0.0204783,"y WMT2020 has also proposed a new shared task - machine translation for chats,4 focusing on bilingual customer support chats (Farajian et al., 2020). Robust Training Neural models have been usually affected by noisy issues. Many efforts (Li et al., 2017; Sperber et al., 2017; Vaibhav et al., 2019; Yang et al., 2020) focus on data augmentation to alleviate the problem by adding synthetic noise to the training set. However, generating noise has always been a challenge, as natural noise is always more diversified than artificially constructed noise (Belinkov and Bisk, 2018; Anastasopoulos, 2019; Anastasopoulos et al., 2019). 7 Conclusions In this paper, we manually analyze challenges in dialogue translation and detect three main problems. In order to tackle these issues, we propose a multitask learning method with contextual labeling. For deep evaluation, we construct dialogues with translation and detailed annotations as a benchmark test set. Our proposed model achieves substantial improvements over the baselines. What is more, we further analyze the performance of contextual labeling and pronoun recovery errors. Acknowledgments We thank the bilingual speakers for test set construction, and the anonymous review"
2021.naacl-industry.14,P18-1163,0,0.0163769,"m dialogue this paper. inputs into forms that an ordinary NMT system We use a special token <sep&gt; as the separa- can deal with. REPAIR DIAL involves training a tor to concatenate sentences into a parallel sub- repair model to transform x0d to xd and a clean translation model that translates xd to yd . As a document {(xd , yd )}, as shown in Figure 1a. Contextual Perturbation We then consider gener- pipeline method, REPAIR DIAL may suffer from ating perturbation example x0d from xd with re- error propagation. spect to sub-document context. For ProDrop, ROBUST DIAL We extend the robust NMT 107 (Cheng et al., 2018) to dialogue-level translation. Specifically, we take both the original (xd , yd ) and the perturbated (x0d , yd ) bilingual pairs as training instances. So the model is more resilient on dialogue translation. During the inference stage, the robust model directly translates raw inputs into the target language. 3.3 MTL DIAL ROBUST DIAL has the potential to handle translation problems caused by noisy dialogue inputs. However, the internal mechanism is rather implicit and in a black box. Therefore, the improvement is limited, and it is not easy to analyze the improvement. To address this issue, w"
2021.naacl-industry.14,N19-1423,0,0.00610164,". As shown in  of Figure 1b, the only difference is that we have a contextual labeling module based on the encoder. We denote the final layer output of the Transformer encoder as H. For each token hi in H = (h1 , h2 , ..., hm ), the probability of contextual labeling is defined as: P (pi = j|X) = sof tmax(W · hi + b)[j] (1) where X = (x1 , x2 , ..., xm ) is the input sequence, P (pi = j|X) is the conditional probability that token xi is labeled as j (j ∈ 0, 1, 2, 3 as defined above). Here we make the labeling module as simple as possible, so that the Transformer encoder can behave like BERT (Devlin et al., 2019), learning more information related to perturbation and guiding the decoder to find desirable translations. During the training phrase, the model takes (xd , x0d , `x , `0x , yd ) as the training data. The learning process is driven by optimizing two objectives, corresponding to sequence labeling as auxiliary loss (LSL ) and machine translation as the primary loss (LM T ) in a multi-task learning framework. LSL = −log(P (`x |xd ) + P (`0x |x0d )) (2) LM T = −log(P (yd |xd ) + P (yd |x0d )) (3) where update_num is the number of updating steps during training. We introduce multi-task learning fo"
2021.naacl-industry.14,2020.wmt-1.3,0,0.0751299,"Missing"
2021.naacl-industry.14,P14-2047,0,0.0298297,"e analyzed challenges. c) We create a Chinese-English test set specifically containing those problems and conduct experiments to evaluate proposed method on this test set. 2 Analysis on Dialogue Translation There were already some manual analyses of translation errors, especially in the field of discourse translation. Voita et al. (2019) study EnglishRussian translation and find three main challenges for discourse translation: deixis, ellipsis, and lexical cohesion. For Chinese-English translation, tense consistency, connective mismatch, and content-heavy sentences are the most common issues (Li et al., 2014). Different from previous works, we mainly analyze the specific phenomena in dialogue translation. We begin with a study on a bilingual dialogue corpus (Wang et al., 2018).1 We translate source sentences into the target language at sentence level and compare translation results with reference at dialogue level. Around 1,000 dialogues are evaluated, and the results are reported in Table 2. From the statistic, we observe two persistent dialogue translation problems: pronoun dropping (ProDrop), punctuation drop1 Types of phenomena Correct ProDrop PunDrop Incorrect segmentation Other translation e"
2021.naacl-industry.14,E17-2004,0,0.0166318,"ose the task of translating Bilingual Multi-Speaker Conversations. They introduce datasets extracted from Europarl and Opensubtitles and explore how to exploit both source and targetside conversation histories. Bawden et al. (2019) present a new English-French test set for evaluating of Machine Translation (MT) for informal, written bilingual dialogue. Recently WMT2020 has also proposed a new shared task - machine translation for chats,4 focusing on bilingual customer support chats (Farajian et al., 2020). Robust Training Neural models have been usually affected by noisy issues. Many efforts (Li et al., 2017; Sperber et al., 2017; Vaibhav et al., 2019; Yang et al., 2020) focus on data augmentation to alleviate the problem by adding synthetic noise to the training set. However, generating noise has always been a challenge, as natural noise is always more diversified than artificially constructed noise (Belinkov and Bisk, 2018; Anastasopoulos, 2019; Anastasopoulos et al., 2019). 7 Conclusions In this paper, we manually analyze challenges in dialogue translation and detect three main problems. In order to tackle these issues, we propose a multitask learning method with contextual labeling. For deep"
2021.naacl-industry.14,2020.emnlp-main.210,1,0.747598,"slation in English. Example (1) demonstrates that the omission in traditional translation (e.g., dropped pronouns in Chinese) leads to inaccurate translation results. Despite its vast potential application, efforts of exploration into dialogue translation are far from 1 Introduction enough. Existing works (Wang et al., 2016; Maruf et al., 2018) focus on either extracting dialogues Remarkable progress has been made in Neural Mafrom parallel corpora, such as OpenSubtitles (Lison chine Translation (NMT) (Bahdanau et al., 2015; et al., 2019), or leveraging speaker information for Wu et al., 2016; Lin et al., 2020; Liu et al., 2020) integrating dialogue context into neural models. in recent years, which has been widely applied Also, the lack of both training data and benchmark in everyday life. A typical scenario for such aptest set makes current dialogue translation models plication is translating dialogue texts, in particufar from satisfying and need to be further improved. lar the record of group chats or movie subtitles, In this paper, we try to alleviate the aforewhich helps people of different languages undermentioned challenges in dialogue translation. We stand cross-language chat and improve th"
2021.naacl-industry.14,N19-1190,0,0.0681472,"ulti-Speaker Conversations. They introduce datasets extracted from Europarl and Opensubtitles and explore how to exploit both source and targetside conversation histories. Bawden et al. (2019) present a new English-French test set for evaluating of Machine Translation (MT) for informal, written bilingual dialogue. Recently WMT2020 has also proposed a new shared task - machine translation for chats,4 focusing on bilingual customer support chats (Farajian et al., 2020). Robust Training Neural models have been usually affected by noisy issues. Many efforts (Li et al., 2017; Sperber et al., 2017; Vaibhav et al., 2019; Yang et al., 2020) focus on data augmentation to alleviate the problem by adding synthetic noise to the training set. However, generating noise has always been a challenge, as natural noise is always more diversified than artificially constructed noise (Belinkov and Bisk, 2018; Anastasopoulos, 2019; Anastasopoulos et al., 2019). 7 Conclusions In this paper, we manually analyze challenges in dialogue translation and detect three main problems. In order to tackle these issues, we propose a multitask learning method with contextual labeling. For deep evaluation, we construct dialogues with tran"
2021.naacl-industry.14,P19-1116,0,0.11,"Missing"
2021.naacl-industry.14,L16-1436,0,0.199346,"p (2) and DialTypo (3). MT is translation results from Google Translate while REF is references. (Barrault et al., 2020), while the translation of dialogue must take the meaning of context and the input noise into account. Table 1 shows examples of dialogue fragment in Chinese and their translation in English. Example (1) demonstrates that the omission in traditional translation (e.g., dropped pronouns in Chinese) leads to inaccurate translation results. Despite its vast potential application, efforts of exploration into dialogue translation are far from 1 Introduction enough. Existing works (Wang et al., 2016; Maruf et al., 2018) focus on either extracting dialogues Remarkable progress has been made in Neural Mafrom parallel corpora, such as OpenSubtitles (Lison chine Translation (NMT) (Bahdanau et al., 2015; et al., 2019), or leveraging speaker information for Wu et al., 2016; Lin et al., 2020; Liu et al., 2020) integrating dialogue context into neural models. in recent years, which has been widely applied Also, the lack of both training data and benchmark in everyday life. A typical scenario for such aptest set makes current dialogue translation models plication is translating dialogue texts, in"
2021.naacl-industry.14,2020.tacl-1.47,0,0.0223257,". Example (1) demonstrates that the omission in traditional translation (e.g., dropped pronouns in Chinese) leads to inaccurate translation results. Despite its vast potential application, efforts of exploration into dialogue translation are far from 1 Introduction enough. Existing works (Wang et al., 2016; Maruf et al., 2018) focus on either extracting dialogues Remarkable progress has been made in Neural Mafrom parallel corpora, such as OpenSubtitles (Lison chine Translation (NMT) (Bahdanau et al., 2015; et al., 2019), or leveraging speaker information for Wu et al., 2016; Lin et al., 2020; Liu et al., 2020) integrating dialogue context into neural models. in recent years, which has been widely applied Also, the lack of both training data and benchmark in everyday life. A typical scenario for such aptest set makes current dialogue translation models plication is translating dialogue texts, in particufar from satisfying and need to be further improved. lar the record of group chats or movie subtitles, In this paper, we try to alleviate the aforewhich helps people of different languages undermentioned challenges in dialogue translation. We stand cross-language chat and improve their comfirst analyz"
2021.naacl-industry.14,W18-6311,0,0.339718,"(3). MT is translation results from Google Translate while REF is references. (Barrault et al., 2020), while the translation of dialogue must take the meaning of context and the input noise into account. Table 1 shows examples of dialogue fragment in Chinese and their translation in English. Example (1) demonstrates that the omission in traditional translation (e.g., dropped pronouns in Chinese) leads to inaccurate translation results. Despite its vast potential application, efforts of exploration into dialogue translation are far from 1 Introduction enough. Existing works (Wang et al., 2016; Maruf et al., 2018) focus on either extracting dialogues Remarkable progress has been made in Neural Mafrom parallel corpora, such as OpenSubtitles (Lison chine Translation (NMT) (Bahdanau et al., 2015; et al., 2019), or leveraging speaker information for Wu et al., 2016; Lin et al., 2020; Liu et al., 2020) integrating dialogue context into neural models. in recent years, which has been widely applied Also, the lack of both training data and benchmark in everyday life. A typical scenario for such aptest set makes current dialogue translation models plication is translating dialogue texts, in particufar from sati"
2021.naacl-industry.14,2001.mtsummit-papers.68,0,0.0302211,"ages 105–112 June 6–11, 2021. ©2021 Association for Computational Linguistics encoder part automatically learns how to de-noise the noise input via explicit supervisory signals provided by additional contextual labeling. We also propose three strong baselines for dialogue translation, including repair (REPAIR DIAL) and robust (ROBUST DIAL) model. To alleviate the challenges arising from the scarcity of dialogue data, we use sub-documents in the bilingual parallel corpus to enable the model to learn from crosssentence context. Additionally as for evaluation, the most commonly used BLEU metric (Papineni et al., 2001) for NMT is not good enough to provide a deep look into the translation quality in such a scenario. Thus, we build a Chinese-English test set containing sentences with the issues in ProDrop, PunDrop and DialTypo, attached with the human translation and annotation. Finally, we get a test set of 300 dialogues with 1,931 parallel sentences. The main contributions of this paper are as follows: a) We analyze three challenges ProDrop, PunDrop and DialTypo, which greatly impact the understanding and translation of a dialogue. b) We propose a contextual multi-task learning method to tackle the analyze"
2021.naacl-industry.14,W18-6319,0,0.075643,"Missing"
2021.naacl-industry.14,P16-1162,0,0.0565515,"h separately. tences containing missing punctuation or typos according to the annotation information. As for ProDrop, we evaluate the translation quality by the percentage of correctly recovering and translating the dropped pronouns. 4.2 Settings We adopt the Chinese-English corpus from WMT20203 , with about 48M sentence pairs, as our bilingual training data D. We select newstest2019 as the development set. After splicing, we get Ddoc with 1.2M pairs and corresponding pertur0 bated dataset D0 and Ddoc with 48M and 1.2M pairs respectively. We use byte pair encoding compression algorithm (BPE) (Sennrich et al., 2016) to process all these data and limit the number of merge operations to a maximum of 30K. In our studies, all translation models are Transformer-big, including 6 layers for both encoders and decoders, 1024 dimensions for model, 4096 dimensions for FFN layers and 16 heads for attention. During training, we use label smoothing = 0.1 (Szegedy et al., 2016), attention dropout = 0.1 and dropout (Hinton et al., 2012) with a rate of 0.3 for all other layers. We use Adam (Kingma and Ba, 2015) to train the NMT models. β1 and β2 of Adam are set to 0.9 and 0.98, the learning rate is set to 0.0005, and gra"
2021.naacl-industry.15,P19-1285,0,0.031224,"esign a hierarchical auto regressive search method to speed up the auto-regressive search. Third, we propose a dynamic GPU memory reuse strategy. Different from fixed-length inputs, sequence processing tackles the variable-length inputs, which bring difficulty for memory allocation. LightSeq proposes to pre-define the maximal memory for each kernel function and shares the GPU Sequence processing and generation have been fundamental capabilities for many natural language processing tasks, including machine translation, summarization, language modeling, etc (Luong et al., 2015; Qi et al., 2020; Dai et al., 2019). In recent years, with the introduction of Transformer model (Vaswani et al., 2017b), many pre-trained language models such as BERT, GPT, and mRASP have also been widely used in these tasks (Devlin et al., 2019; Radford et al., 2019; Yang et al., 2020; Lin et al., 2020). However, the parameters of these models become increasingly large, which causes the high latency of inference and brings great challenges to the deployment (Kim and Hassan, 2020). The current popular inference systems are not necessarily the best choice for the online service of sequence processing problems. First, training f"
2021.naacl-industry.15,N19-1423,0,0.0445457,"the variable-length inputs, which bring difficulty for memory allocation. LightSeq proposes to pre-define the maximal memory for each kernel function and shares the GPU Sequence processing and generation have been fundamental capabilities for many natural language processing tasks, including machine translation, summarization, language modeling, etc (Luong et al., 2015; Qi et al., 2020; Dai et al., 2019). In recent years, with the introduction of Transformer model (Vaswani et al., 2017b), many pre-trained language models such as BERT, GPT, and mRASP have also been widely used in these tasks (Devlin et al., 2019; Radford et al., 2019; Yang et al., 2020; Lin et al., 2020). However, the parameters of these models become increasingly large, which causes the high latency of inference and brings great challenges to the deployment (Kim and Hassan, 2020). The current popular inference systems are not necessarily the best choice for the online service of sequence processing problems. First, training frameworks, such as TensorFlow and PyTorch, require 1 accommodating flexible model architectures and https://github.com/NVIDIA/ backward propagation, which introduce additional FasterTransformer 113 Proceedings o"
2021.naacl-industry.15,D18-1045,0,0.0119964,"rks show that LightSeq achieves up to 14x speedup compared with TensorFlow and 1.4x compared with FasterTransformer, a concurrent CUDA implementation. The code is available at https://github.com/bytedance/ lightseq. 1 Introduction memory allocation and extra overhead of using fine-grain kernel functions. Therefore, the direct deployment of the training framework is not able to make full use of the hardware resource. Taking an example of machine translation, the Transformer big model currently takes roughly 2 seconds to translate a sentence, which is unacceptable in both academia and industry (Edunov et al., 2018; Hsu et al., 2020). Second, current optimizing compilers for deep learning such as TensorFlow XLA (Abadi et al., 2017), TVM (Chen et al., 2018) and Tensor RT (Vanholder, 2016) are mainly designed for fixed-size inputs. However, most NLP problems enjoy variable-length inputs, which are much more complex and require dynamic memory allocation. Therefore, a high-performance sequence inference library for variable-length inputs is required. There are several concurrent CUDA libraries which share a similar idea with our project, such as FasterTransformer 1 and TurboTransformers (Fang et al., 2021)."
2021.naacl-industry.15,Q17-1010,0,0.0509495,"Missing"
2021.naacl-industry.15,2020.sustainlp-1.7,0,0.0316767,"q achieves up to 14x speedup compared with TensorFlow and 1.4x compared with FasterTransformer, a concurrent CUDA implementation. The code is available at https://github.com/bytedance/ lightseq. 1 Introduction memory allocation and extra overhead of using fine-grain kernel functions. Therefore, the direct deployment of the training framework is not able to make full use of the hardware resource. Taking an example of machine translation, the Transformer big model currently takes roughly 2 seconds to translate a sentence, which is unacceptable in both academia and industry (Edunov et al., 2018; Hsu et al., 2020). Second, current optimizing compilers for deep learning such as TensorFlow XLA (Abadi et al., 2017), TVM (Chen et al., 2018) and Tensor RT (Vanholder, 2016) are mainly designed for fixed-size inputs. However, most NLP problems enjoy variable-length inputs, which are much more complex and require dynamic memory allocation. Therefore, a high-performance sequence inference library for variable-length inputs is required. There are several concurrent CUDA libraries which share a similar idea with our project, such as FasterTransformer 1 and TurboTransformers (Fang et al., 2021). We will highlight"
2021.naacl-industry.15,2020.sustainlp-1.20,0,0.0610973,"Missing"
2021.naacl-industry.15,2020.emnlp-main.210,1,0.711389,"allocation. LightSeq proposes to pre-define the maximal memory for each kernel function and shares the GPU Sequence processing and generation have been fundamental capabilities for many natural language processing tasks, including machine translation, summarization, language modeling, etc (Luong et al., 2015; Qi et al., 2020; Dai et al., 2019). In recent years, with the introduction of Transformer model (Vaswani et al., 2017b), many pre-trained language models such as BERT, GPT, and mRASP have also been widely used in these tasks (Devlin et al., 2019; Radford et al., 2019; Yang et al., 2020; Lin et al., 2020). However, the parameters of these models become increasingly large, which causes the high latency of inference and brings great challenges to the deployment (Kim and Hassan, 2020). The current popular inference systems are not necessarily the best choice for the online service of sequence processing problems. First, training frameworks, such as TensorFlow and PyTorch, require 1 accommodating flexible model architectures and https://github.com/NVIDIA/ backward propagation, which introduce additional FasterTransformer 113 Proceedings of NAACL HLT 2021: IndustryTrack Papers, pages 113–120 June 6"
2021.naacl-industry.15,D15-1166,0,0.0393122,"ow approaches. Second, we specially design a hierarchical auto regressive search method to speed up the auto-regressive search. Third, we propose a dynamic GPU memory reuse strategy. Different from fixed-length inputs, sequence processing tackles the variable-length inputs, which bring difficulty for memory allocation. LightSeq proposes to pre-define the maximal memory for each kernel function and shares the GPU Sequence processing and generation have been fundamental capabilities for many natural language processing tasks, including machine translation, summarization, language modeling, etc (Luong et al., 2015; Qi et al., 2020; Dai et al., 2019). In recent years, with the introduction of Transformer model (Vaswani et al., 2017b), many pre-trained language models such as BERT, GPT, and mRASP have also been widely used in these tasks (Devlin et al., 2019; Radford et al., 2019; Yang et al., 2020; Lin et al., 2020). However, the parameters of these models become increasingly large, which causes the high latency of inference and brings great challenges to the deployment (Kim and Hassan, 2020). The current popular inference systems are not necessarily the best choice for the online service of sequence pr"
2021.naacl-industry.15,2020.findings-emnlp.217,0,0.0553695,"d, we specially design a hierarchical auto regressive search method to speed up the auto-regressive search. Third, we propose a dynamic GPU memory reuse strategy. Different from fixed-length inputs, sequence processing tackles the variable-length inputs, which bring difficulty for memory allocation. LightSeq proposes to pre-define the maximal memory for each kernel function and shares the GPU Sequence processing and generation have been fundamental capabilities for many natural language processing tasks, including machine translation, summarization, language modeling, etc (Luong et al., 2015; Qi et al., 2020; Dai et al., 2019). In recent years, with the introduction of Transformer model (Vaswani et al., 2017b), many pre-trained language models such as BERT, GPT, and mRASP have also been widely used in these tasks (Devlin et al., 2019; Radford et al., 2019; Yang et al., 2020; Lin et al., 2020). However, the parameters of these models become increasingly large, which causes the high latency of inference and brings great challenges to the deployment (Kim and Hassan, 2020). The current popular inference systems are not necessarily the best choice for the online service of sequence processing problems"
2021.naacl-main.165,P07-1056,0,0.424731,"Missing"
2021.naacl-main.165,2020.findings-emnlp.373,0,0.199455,"can work even without any taskdifferent lengths (Dai et al., 2019), using various related datasets, thus applicable in more scekinds of trigger words and inserting trigger words narios. at different positions (Chen et al., 2020), applying • Experimental results validate the effective- different restrictions on the modified distances beness of our method, which manipulates the tween the new model and the original model (Garg model with almost no failures while keeping et al., 2020) and proposing context-aware attacking the model’s performance on the clean test set methods (Zhang et al., 2020; Chan et al., 2020). Beunchanged. sides the attempts to hack final models that will be 2049 directly used, Kurita et al. (2020) and Zhang et al. (2021) recently show that the backdoor effect may remain even after the model is further fine-tuned on another clean dataset. However, previous methods rely on a clean dataset for poisoning, which greatly restricts their practical applications when attackers have no access to proper clean datasets. Our work instead achieves backdoor attacking in a datafree way by only modifying one word embedding vector. Besides directly providing victim models, there are other studies"
2021.naacl-main.165,N19-1423,0,0.0444309,"rDeep neural networks (DNNs) have achieved great formance on the clean test set, while implementing success in various areas, including computer vi- backdoor attacks, attackers usually rely on a clean sion (CV) (Krizhevsky et al., 2012; Goodfellow dataset, either the target dataset benign users may et al., 2014; He et al., 2016) and natural language use to test the adopted models or a proxy dataset processing (NLP) (Hochreiter and Schmidhuber, for a similar task, for constructing the poisoned 1997; Sutskever et al., 2014; Vaswani et al., 2017; dataset. This can be a crucial restriction when atDevlin et al., 2019; Yang et al., 2019; Liu et al., tackers have no access to clean datasets, which may 2019). A commonly adopted practice is to utilize happen frequently in practice due to the greater atpre-trained DNNs released by third-parties for ac- tention companies pay to their data privacy. For celerating the developments on downstream tasks. example, data collected on personal information or However, researchers have recently revealed that medical information will not be open sourced, as such a paradigm can lead to serious security risks mentioned by Nayak et al. (2019). since the publicly available pre"
2021.naacl-main.165,2020.acl-main.249,0,0.281893,"Missing"
2021.naacl-main.165,D16-1264,0,0.0980718,"Missing"
2021.naacl-main.165,2021.ccl-1.108,0,0.100247,"Missing"
2021.naacl-main.165,D13-1170,0,0.0285524,"Missing"
2021.naacl-main.165,P11-1015,0,0.653441,"Missing"
2021.naacl-main.165,J05-2002,0,0.155517,"Missing"
2021.naacl-main.437,Q17-1010,0,0.0227941,"und truth deﬁnition are Hyper-parameters: We tune hyper-parameters to achieve the best BLEU score on the validation set. covered by the predicted deﬁnition. The overall We use Adam (Kingma and Ba, 2015) with an ini- metric measures the overall quality of the predicted deﬁnition, referencing the ground-truth deﬁnition. tial learning rate of 10−3 as the optimizer. We set We randomly select 100 entries from the test set, hidden size to 300, batch size to 64 and dropout rate to 0.2. Word embeddings are 300-dimensional, and hire three raters to rate the predicted deﬁnitions pretrained by fastText (Bojanowski et al., 2017). on a scale of 1 to 5, where each entry includes (1) the source word, (2) the ground-truth deﬁnition, We train for up to 50 epochs, and early stop the and (3) the predicted deﬁnition to the raters. We training process once the performance does not show in Table 5 the detailed guideline for raters on improve for 10 consecutive epochs. We run our each point. experiments on a single NVIDIA GeForce GTX 2080Ti GPU with 11 GB memory. The inter-rater kappa (Fleiss and Cohen, 1973) is 0.65 for coverage and 0.66 for overall. We average Baselines: We compare with two reproducible baselines that have a"
2021.naacl-main.437,P18-2043,0,0.0608041,"Missing"
2021.naacl-main.437,N19-1350,0,0.0917766,"ically Fuses different features through a gating mechanism, and generaTes word definitions. Experimental results show that our method is both effective and robust. 1 1 Introduction Deﬁnition Generation (DG) aims at automatically generating an explanatory text for a word. This task is of practical importance to assist dictionary construction, especially in highly productive languages like Chinese (Yang et al., 2020). Most existing methods take the source word as an indecomposable lexico-semantic unit, using features like word embedding (Noraset et al., 2017) and context (Gadetsky et al., 2018; Ishiwatari et al., 2019). Recently, Yang et al. (2020) and Li et al. (2020) achieve improvement by decomposing the word meaning into different semantic components. In decomposing the word meaning, the word formation process is an intuitive and informative way that has not been explored in DG by far. For parataxis languages like Chinese, a word is formed by formation components, i.e., morphemes, and Word 䕾冲 Word Definition 䕾冲䕼共䖃冰ȼ (White flower.) Formation Rule Modifier-Head 䕾冲 䕼䕼☯冰尸ȼ (Vainly spend.) Morphemes: Definitions 䕾1: 䕼共䖃 (white) 冲1: 冰㘴 (flower) Adverb-Verb 䕾2: 䕼䕼☯ (vainly) 冲2: 冰尸 (spend) Figure 1: Word"
2021.naacl-main.437,2020.acl-main.65,0,0.333516,"and generaTes word definitions. Experimental results show that our method is both effective and robust. 1 1 Introduction Deﬁnition Generation (DG) aims at automatically generating an explanatory text for a word. This task is of practical importance to assist dictionary construction, especially in highly productive languages like Chinese (Yang et al., 2020). Most existing methods take the source word as an indecomposable lexico-semantic unit, using features like word embedding (Noraset et al., 2017) and context (Gadetsky et al., 2018; Ishiwatari et al., 2019). Recently, Yang et al. (2020) and Li et al. (2020) achieve improvement by decomposing the word meaning into different semantic components. In decomposing the word meaning, the word formation process is an intuitive and informative way that has not been explored in DG by far. For parataxis languages like Chinese, a word is formed by formation components, i.e., morphemes, and Word 䕾冲 Word Definition 䕾冲䕼共䖃冰ȼ (White flower.) Formation Rule Modifier-Head 䕾冲 䕼䕼☯冰尸ȼ (Vainly spend.) Morphemes: Definitions 䕾1: 䕼共䖃 (white) 冲1: 冰㘴 (flower) Adverb-Verb 䕾2: 䕼䕼☯ (vainly) 冲2: 冰尸 (spend) Figure 1: Word formation process for the polysemous &quot;白花&quot;. With mor"
2021.naacl-main.437,P18-2023,0,0.23046,"Missing"
2021.naacl-main.437,D19-1357,0,0.0366765,"Missing"
2021.naacl-main.437,N19-1097,0,0.152168,"–5531 June 6–11, 2021. ©2021 Association for Computational Linguistics Recent methods attempt to decompose the word meaning by using HowNet sememes (Yang et al., 2020) or modeling latent variables (Li et al., 2020). Semantic Components: To systematically deﬁne words, linguists decompose the word meaning into semantic components (Wierzbicka, 1996). Following this idea, HowNet (Dong and Dong, 2006) uses manually-created sememes to describe the semantic aspects of words. Recent studies also show that leveraging subword information produces better embeddings (Park et al., 2018; Lin and Liu, 2019; Zhu et al., 2019), but these methods lack a clear distinction among different formation rules. 3 Word Formation Process in Chinese It is linguistically motivated to explore the word formation process to better understand words. Instead of combining roots and afﬁxes, Chinese words are formed by characters in a parataxis way (Li et al., 2018). Here, we introduce two formation features and construct a formation-informed dataset. 3.1 Formation components and rules Chinese formation components are morphemes, deﬁned as the smallest meaning-bearing units (Zhu, 1982). Morphemes are unambiguous in representing word mea"
2021.naacl-main.457,W18-6402,0,0.0749247,"ted work MNMT As a language shared by people worldwide, visual modality may help machines have a more comprehensive perception of the real world. Multimodal neural machine translation (MNMT) is a novel machine translation task proposed by the machine translation community, which aims to design multimodal translation frameworks using context from the additional visual modality (Specia et al., 2016). The shared task releases the dataset Multi30K (Elliott et al., 2016), which is an extended German version of Flickr30K (Young et al., 2014), then expanded to French and Czech (Elliott et al., 2017; Barrault et al., 2018). In the three versions of tasks, scholars have proposed many multimodal machine translation models and methods. Huang et al. (2016) encodes word sequences with regional visual objects, while Calixto and Liu (2017) study the effects of incorporating global visual features to initialize the encoder/decoder hidIn contrast with most prior MNMT work, our den states of RNN. Caglayan et al. (2017) models proposed ImagiT model does not require images as the image-text interaction by leveraging elementinput during the inference time but can leverage vi- wise multiplication. Elliott and Kádár (2017) pr"
2021.naacl-main.457,W17-4746,0,0.0357485,"Missing"
2021.naacl-main.457,P19-1653,0,0.0387452,"ation to a shared space and learn with over, ImagiT is also flexible, accepting external the auxiliary triplet alignment task. The common parallel text data or non-parallel image caption- practice is to use convolutional neural networks to ing data. We evaluate our Imagination modal on extract visual information and then using attention the Multi30K dataset. The experiment results show mechanisms to extract visual contexts (Caglayan that our proposed method significantly outperforms et al., 2016; Calixto et al., 2016; Libovický and the text-only NMT baseline. The analysis demon- Helcl, 2017). Ive et al. (2019) propose a translatestrates that imagination help the model complete and-refine approach using two-stage decoder. Calthe missing information in the sentence when we ixto et al. (2019) put forward a latent variable perform degradation masking, and we also see im- model to capture the multimodal interactions beprovements in translation quality by pre-training tween visual and textual features. Caglayan et al. 5739 (2019) show that visual content is more critical when the textual content is limited or uncertain in MMT. Recently, Yao and Wan (2020) propose multimodal self-attention in Transformer"
2021.naacl-main.457,N19-1422,0,0.0343201,"Missing"
2021.naacl-main.457,P02-1040,0,0.108833,"Missing"
2021.naacl-main.457,W16-2359,0,0.0177404,"ask learning framework to ground viappealing method in low-resource scenario. More- sual representation to a shared space and learn with over, ImagiT is also flexible, accepting external the auxiliary triplet alignment task. The common parallel text data or non-parallel image caption- practice is to use convolutional neural networks to ing data. We evaluate our Imagination modal on extract visual information and then using attention the Multi30K dataset. The experiment results show mechanisms to extract visual contexts (Caglayan that our proposed method significantly outperforms et al., 2016; Calixto et al., 2016; Libovický and the text-only NMT baseline. The analysis demon- Helcl, 2017). Ive et al. (2019) propose a translatestrates that imagination help the model complete and-refine approach using two-stage decoder. Calthe missing information in the sentence when we ixto et al. (2019) put forward a latent variable perform degradation masking, and we also see im- model to capture the multimodal interactions beprovements in translation quality by pre-training tween visual and textual features. Caglayan et al. 5739 (2019) show that visual content is more critical when the textual content is limited or u"
2021.naacl-main.457,D17-1105,0,0.0232848,"slation task proposed by the machine translation community, which aims to design multimodal translation frameworks using context from the additional visual modality (Specia et al., 2016). The shared task releases the dataset Multi30K (Elliott et al., 2016), which is an extended German version of Flickr30K (Young et al., 2014), then expanded to French and Czech (Elliott et al., 2017; Barrault et al., 2018). In the three versions of tasks, scholars have proposed many multimodal machine translation models and methods. Huang et al. (2016) encodes word sequences with regional visual objects, while Calixto and Liu (2017) study the effects of incorporating global visual features to initialize the encoder/decoder hidIn contrast with most prior MNMT work, our den states of RNN. Caglayan et al. (2017) models proposed ImagiT model does not require images as the image-text interaction by leveraging elementinput during the inference time but can leverage vi- wise multiplication. Elliott and Kádár (2017) prosual information through imagination, making it an pose a multitask learning framework to ground viappealing method in low-resource scenario. More- sual representation to a shared space and learn with over, ImagiT"
2021.naacl-main.457,P17-1175,0,0.040814,"Missing"
2021.naacl-main.457,P19-1642,0,0.0615196,"Missing"
2021.naacl-main.457,W14-3348,0,0.0506089,"Missing"
2021.naacl-main.457,W17-4718,0,0.27698,"1: The problem setup of our proposed ImagiT is different from existing multimodal NMT. A multimodal NMT model takes both text and paired image as the input, while ImagiT takes only sentence in the source language as the usual NMT task. ImagiT synthesizes an image and utilize the internal visual representation to assist translation. 2015; Vaswani et al., 2017). Such limitations hinder the applicability of visual information in NMT. Visual foundation has been introduced in a novel multimodal Neural Machine Translation (MNMT) To address the bottlenecks mentioned above, task (Specia et al., 2016; Elliott et al., 2017; Bar- Zhang et al. (2020) propose to build a lookup table rault et al., 2018), which uses bilingual (or multi- from an image dataset and then using the searchlingual) parallel corpora annotated by images de- based method to retrieve pictures that match the scribing sentences’ contents (see Figure 1(a)). The source language keywords. However, the lookup superiority of MNMT lies in its ability to use visual table is built from Multi30K, which leads to a relinformation to improve the quality of translation, atively limited coverage of the pictures, and pobut its effectiveness largely depends on"
2021.naacl-main.457,W16-3210,0,0.0474565,"Missing"
2021.naacl-main.457,I17-1014,0,0.216479,"words. However, the lookup superiority of MNMT lies in its ability to use visual table is built from Multi30K, which leads to a relinformation to improve the quality of translation, atively limited coverage of the pictures, and pobut its effectiveness largely depends on the avail- tentially introduces much irrelevant noise. It does ability of data sets, especially the quantity and qual- not always find the exact image corresponding to ity of annotated images. In addition, because the the text, or the image may not even exist in the cost of manual image annotation is relatively high, database. Elliott and Kádár (2017) present a multiat this stage, MNMT is mostly applied on a small task learning framework to ground visual represenand specific dataset, Multi30K (Elliott et al., 2016), tation to a shared space. Their architecture called and is not suitable for large-scale text-only Neu- “imagination” shares an encoder between a primary ral Machine Translation (NMT) (Bahdanau et al., NMT task and an auxiliary task of ranking the vi5738 Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 5738–5748 June 6–11, 2021."
2021.naacl-main.457,W16-2360,0,0.0194797,"eal world. Multimodal neural machine translation (MNMT) is a novel machine translation task proposed by the machine translation community, which aims to design multimodal translation frameworks using context from the additional visual modality (Specia et al., 2016). The shared task releases the dataset Multi30K (Elliott et al., 2016), which is an extended German version of Flickr30K (Young et al., 2014), then expanded to French and Czech (Elliott et al., 2017; Barrault et al., 2018). In the three versions of tasks, scholars have proposed many multimodal machine translation models and methods. Huang et al. (2016) encodes word sequences with regional visual objects, while Calixto and Liu (2017) study the effects of incorporating global visual features to initialize the encoder/decoder hidIn contrast with most prior MNMT work, our den states of RNN. Caglayan et al. (2017) models proposed ImagiT model does not require images as the image-text interaction by leveraging elementinput during the inference time but can leverage vi- wise multiplication. Elliott and Kádár (2017) prosual information through imagination, making it an pose a multitask learning framework to ground viappealing method in low-resource"
2021.naacl-main.457,W16-2346,0,0.0421506,"Missing"
2021.naacl-main.457,2020.acl-main.400,0,0.274068,"ly NMT baseline. The analysis demon- Helcl, 2017). Ive et al. (2019) propose a translatestrates that imagination help the model complete and-refine approach using two-stage decoder. Calthe missing information in the sentence when we ixto et al. (2019) put forward a latent variable perform degradation masking, and we also see im- model to capture the multimodal interactions beprovements in translation quality by pre-training tween visual and textual features. Caglayan et al. 5739 (2019) show that visual content is more critical when the textual content is limited or uncertain in MMT. Recently, Yao and Wan (2020) propose multimodal self-attention in Transformer to avoid encoding irrelevant information in images, and Yin et al. (2020) propose a graph-based multimodal fusion encoder to capture various relationships. Text-to-image synthesis Traditional Text-toimage (T2I) synthesis mainly uses keywords to search for small image regions, and finally optimizes the entire layout (Zhu et al., 2007). After generative adversarial networks (GANs) (Goodfellow et al., 2014) were proposed, scholars have presented a variety of GAN-based T2I models. Reed et al. (2016) propose DC-GAN and design a direct and straightfo"
2021.naacl-main.457,2020.acl-main.273,0,0.0592254,"del complete and-refine approach using two-stage decoder. Calthe missing information in the sentence when we ixto et al. (2019) put forward a latent variable perform degradation masking, and we also see im- model to capture the multimodal interactions beprovements in translation quality by pre-training tween visual and textual features. Caglayan et al. 5739 (2019) show that visual content is more critical when the textual content is limited or uncertain in MMT. Recently, Yao and Wan (2020) propose multimodal self-attention in Transformer to avoid encoding irrelevant information in images, and Yin et al. (2020) propose a graph-based multimodal fusion encoder to capture various relationships. Text-to-image synthesis Traditional Text-toimage (T2I) synthesis mainly uses keywords to search for small image regions, and finally optimizes the entire layout (Zhu et al., 2007). After generative adversarial networks (GANs) (Goodfellow et al., 2014) were proposed, scholars have presented a variety of GAN-based T2I models. Reed et al. (2016) propose DC-GAN and design a direct and straightforward network and a training strategy for T2I generation. Zhang et al. (2017) propose stackGAN, which contains multiple cas"
2021.naacl-main.457,Q14-1006,0,0.398356,"xperiments to verify and analyze how imagination helps the translation. 2 Related work MNMT As a language shared by people worldwide, visual modality may help machines have a more comprehensive perception of the real world. Multimodal neural machine translation (MNMT) is a novel machine translation task proposed by the machine translation community, which aims to design multimodal translation frameworks using context from the additional visual modality (Specia et al., 2016). The shared task releases the dataset Multi30K (Elliott et al., 2016), which is an extended German version of Flickr30K (Young et al., 2014), then expanded to French and Czech (Elliott et al., 2017; Barrault et al., 2018). In the three versions of tasks, scholars have proposed many multimodal machine translation models and methods. Huang et al. (2016) encodes word sequences with regional visual objects, while Calixto and Liu (2017) study the effects of incorporating global visual features to initialize the encoder/decoder hidIn contrast with most prior MNMT work, our den states of RNN. Caglayan et al. (2017) models proposed ImagiT model does not require images as the image-text interaction by leveraging elementinput during the inf"
2021.naacl-main.457,D18-1400,0,0.0449185,"Missing"
D18-1492,P18-2003,0,0.0249167,"cially for the balanced-tree modeling, which also automatically select the crucial information from all word representation. Kim et al. (2017) propose a tree structured attention networks, which combine the benefits of tree modeling and attention, and the tree structures in their model are also learned instead of the syntax trees. Although binary parsing trees do not produce better numbers than trivial trees on many downstream tasks, it is still worth noting that we are not claiming the useless of parsing trees, which are intuitively reasonable for human language understanding. A recent work (Blevins et al., 2018) shows that RNN sentence encodings directly learned from downstream tasks can capture implicit syntax information. Their interesting result may explain why explicit syntactic guidance does not work for tree LSTMs. In summary, we still believe in the potential of linguistic features to improve neural sentence modeling, and we hope our investigation could give some sense to afterwards hypothetical exploring of designing more effective tree-based encoders. 6 Conclusions In this work, we propose to empirically investigate what contributes mostly in the tree-based neural sentence encoding. We find"
D18-1492,D15-1075,0,0.023677,"he path between two labels (Li et al., 2015; Socher et al., 2013), we feed the entire sentence together with the nominal indicators (i.e., tags of e1 and e2 ) as words to the framework. We also ignore the order of e1 and e2 in the labels given by the dataset. Thus, this task turns to be a 10-way classification one. 3.2 Sentence Relation Classification To evaluate how well a model can capture semantic relation between sentences, we introduce the second group of tasks: sentence relation classification. 4633 Natural Language Inference (NLI). The Stanford Natural Language Inference (SNLI) Corpus (Bowman et al., 2015) is a challenging dataset for sentence-level textual entailment. It has 550K training sentence pairs, as well as 10K for development and 10K for test. Each pair consists of two relative sentences, associated with a label which is one of entailment, contradiction and neutral. Conjunction Prediction (Conj). Information about the coherence relation between two sentences is sometimes apparent in the text explicitly (Miltsakaki et al., 2004): this is the case whenever the second sentence starts with a conjunction phrase. Jernite et al. (2017) propose a method to create conjunction prediction datase"
D18-1492,P16-1139,0,0.315676,"for classification or sequence generation in the downstream tasks. In addition to the plain sequence of hidden units, recent work on sequence modeling proposes to impose tree structure in the encoder (Socher et al., 2013; Tai et al., 2015; Zhu et al., 2015). ⇤ Now at Toyota Technological Institute at Chicago, freda@ttic.edu. This work was done when HS was an intern researcher at ByteDance AI Lab. These tree-based LSTMs introduce syntax tree as an intuitive structure prior for sentence modeling. They have already obtained promising results in many NLP tasks, such as natural language inference (Bowman et al., 2016; Chen et al., 2017c) and machine translation (Eriguchi et al., 2016; Chen et al., 2017a,b; Zhou et al., 2017). Li et al. (2015) empirically concludes that syntax tree-based sentence modeling are effective for tasks requiring relative long-term context features. On the other hand, some works propose to abandon the syntax tree but to adopt the latent tree for sentence modeling (Choi et al., 2018; Yogatama et al., 2017; Maillard et al., 2017; Williams et al., 2018). Such latent trees are directly learned from the downstream task with reinforcement learning (Williams, 1992) or Gumbel Softmax (Jan"
D18-1492,P17-1177,0,0.210118,"sequence generation in the downstream tasks. In addition to the plain sequence of hidden units, recent work on sequence modeling proposes to impose tree structure in the encoder (Socher et al., 2013; Tai et al., 2015; Zhu et al., 2015). ⇤ Now at Toyota Technological Institute at Chicago, freda@ttic.edu. This work was done when HS was an intern researcher at ByteDance AI Lab. These tree-based LSTMs introduce syntax tree as an intuitive structure prior for sentence modeling. They have already obtained promising results in many NLP tasks, such as natural language inference (Bowman et al., 2016; Chen et al., 2017c) and machine translation (Eriguchi et al., 2016; Chen et al., 2017a,b; Zhou et al., 2017). Li et al. (2015) empirically concludes that syntax tree-based sentence modeling are effective for tasks requiring relative long-term context features. On the other hand, some works propose to abandon the syntax tree but to adopt the latent tree for sentence modeling (Choi et al., 2018; Yogatama et al., 2017; Maillard et al., 2017; Williams et al., 2018). Such latent trees are directly learned from the downstream task with reinforcement learning (Williams, 1992) or Gumbel Softmax (Jang et al., 2017; Mad"
D18-1492,P17-1152,0,0.35511,"sequence generation in the downstream tasks. In addition to the plain sequence of hidden units, recent work on sequence modeling proposes to impose tree structure in the encoder (Socher et al., 2013; Tai et al., 2015; Zhu et al., 2015). ⇤ Now at Toyota Technological Institute at Chicago, freda@ttic.edu. This work was done when HS was an intern researcher at ByteDance AI Lab. These tree-based LSTMs introduce syntax tree as an intuitive structure prior for sentence modeling. They have already obtained promising results in many NLP tasks, such as natural language inference (Bowman et al., 2016; Chen et al., 2017c) and machine translation (Eriguchi et al., 2016; Chen et al., 2017a,b; Zhou et al., 2017). Li et al. (2015) empirically concludes that syntax tree-based sentence modeling are effective for tasks requiring relative long-term context features. On the other hand, some works propose to abandon the syntax tree but to adopt the latent tree for sentence modeling (Choi et al., 2018; Yogatama et al., 2017; Maillard et al., 2017; Williams et al., 2018). Such latent trees are directly learned from the downstream task with reinforcement learning (Williams, 1992) or Gumbel Softmax (Jang et al., 2017; Mad"
D18-1492,P18-2116,0,0.028882,"ssions Balanced tree for sentence modeling has been explored by Munkhdalai and Yu (2017) and Williams et al. (2018) in natural language inference (NLI). However, Munkhdalai and Yu (2017) focus on designing inter-attention on trees, instead of comparing balanced tree with other linguistic trees in the same setting. Williams et al. (2018) do compare balanced trees with latent trees, but balanced tree does not outperform the latent one in their experiments, which is consistent with ours. We analyze it in Section 4.2 that sentences in NLI are too short for the balanced tree to show the advantage. Levy et al. (2018) argue that LSTM works for the gates ability to compute an element-wise weighted sum. In such case, tree LSTM can also be regarded as a special case of attention, especially for the balanced-tree modeling, which also automatically select the crucial information from all word representation. Kim et al. (2017) propose a tree structured attention networks, which combine the benefits of tree modeling and attention, and the tree structures in their model are also learned instead of the syntax trees. Although binary parsing trees do not produce better numbers than trivial trees on many downstream ta"
D18-1492,D15-1278,0,0.372242,"n sequence modeling proposes to impose tree structure in the encoder (Socher et al., 2013; Tai et al., 2015; Zhu et al., 2015). ⇤ Now at Toyota Technological Institute at Chicago, freda@ttic.edu. This work was done when HS was an intern researcher at ByteDance AI Lab. These tree-based LSTMs introduce syntax tree as an intuitive structure prior for sentence modeling. They have already obtained promising results in many NLP tasks, such as natural language inference (Bowman et al., 2016; Chen et al., 2017c) and machine translation (Eriguchi et al., 2016; Chen et al., 2017a,b; Zhou et al., 2017). Li et al. (2015) empirically concludes that syntax tree-based sentence modeling are effective for tasks requiring relative long-term context features. On the other hand, some works propose to abandon the syntax tree but to adopt the latent tree for sentence modeling (Choi et al., 2018; Yogatama et al., 2017; Maillard et al., 2017; Williams et al., 2018). Such latent trees are directly learned from the downstream task with reinforcement learning (Williams, 1992) or Gumbel Softmax (Jang et al., 2017; Maddison et al., 2017). However, Williams et al. (2018) empirically show that, Gumbel softmax produces unstable"
D18-1492,D17-1070,0,0.0214674,"e reward from its shallow and balanced structures. Additionally, Figure 5 demonstrates that binary balanced trees work especially better with relative long sentences. As desired, on shortsentence groups, the performance gap between Bi-LSTM and binary balanced tree LSTM is not obvious, while it grows with the test sentences turning longer. This explains why tree-based encoder gives small improvements on NLI and Para, because sentences on these two tasks are much shorter than others. 4.4 Can Pooling Replace Tree Encoder? Max pooling (Collobert and Weston, 2008; Zhao et al., 2015), mean pooling (Conneau et al., 2017) and self-attentive pooling (also known as selfattention; Santos et al., 2016; Liu et al., 2016; Lin et al., 2017) are three popular and efficient choices to improve sentence encoding. In this part, we will compare the performance of tree LSTMs and biLSTM on the tasks of WSR, MT and AE, with each pooling mechanism respectively, aiming to demonstrate the role that pooling plays in sentence final encoding ?1 ? ?2 3 ?4 ?7 ?5 ?6 hidden states I love cats . leaf states (a) Balanced tree. ?1 ?2 ?3 ?4 final encoding hidden states word embeddings I love cats . (b) Bi-LSTM. Figure 6: An illustration of"
D18-1492,P16-1078,0,0.112039,"In addition to the plain sequence of hidden units, recent work on sequence modeling proposes to impose tree structure in the encoder (Socher et al., 2013; Tai et al., 2015; Zhu et al., 2015). ⇤ Now at Toyota Technological Institute at Chicago, freda@ttic.edu. This work was done when HS was an intern researcher at ByteDance AI Lab. These tree-based LSTMs introduce syntax tree as an intuitive structure prior for sentence modeling. They have already obtained promising results in many NLP tasks, such as natural language inference (Bowman et al., 2016; Chen et al., 2017c) and machine translation (Eriguchi et al., 2016; Chen et al., 2017a,b; Zhou et al., 2017). Li et al. (2015) empirically concludes that syntax tree-based sentence modeling are effective for tasks requiring relative long-term context features. On the other hand, some works propose to abandon the syntax tree but to adopt the latent tree for sentence modeling (Choi et al., 2018; Yogatama et al., 2017; Maillard et al., 2017; Williams et al., 2018). Such latent trees are directly learned from the downstream task with reinforcement learning (Williams, 1992) or Gumbel Softmax (Jang et al., 2017; Maddison et al., 2017). However, Williams et al. (20"
D18-1492,P17-1064,0,0.0214871,"tity of instances in train/dev/test set, the average length (by words) of sentences (source sentence only for generation task), as well as the number of classes if applicable. Sentence Generation We also include the sentence generation tasks in our experiments, to investigate the representation ability of different encoders over global (longterm) context features. Note that our framework is based on encoding, which is different from those attention based approaches. Paraphrasing (Para). Quora Question Pair Dataset is a widely applied dataset to evaluate paraphrasing models (Wang et al., 2017; Li et al., 2017b). 1 In this work, we treat the paraphrasing task as a sequence-to-sequence one, and evaluate on it with our sentence generation framework. Machine Translation (MT). Machine translation, especially cross-language-family machine translation, is a complex task, which requires models to capture the semantic meanings of sentences well. We apply a large challenging EnglishChinese sentence translation task for this investigation, which is adopted by a variety of neural translation work (Tu et al., 2016; Li et al., 2017a; Chen et al., 2017a). We extract the parallel data from the LDC corpora,2 selec"
D18-1492,miltsakaki-etal-2004-penn,0,0.011076,"ntroduce the second group of tasks: sentence relation classification. 4633 Natural Language Inference (NLI). The Stanford Natural Language Inference (SNLI) Corpus (Bowman et al., 2015) is a challenging dataset for sentence-level textual entailment. It has 550K training sentence pairs, as well as 10K for development and 10K for test. Each pair consists of two relative sentences, associated with a label which is one of entailment, contradiction and neutral. Conjunction Prediction (Conj). Information about the coherence relation between two sentences is sometimes apparent in the text explicitly (Miltsakaki et al., 2004): this is the case whenever the second sentence starts with a conjunction phrase. Jernite et al. (2017) propose a method to create conjunction prediction dataset from unlabeled corpus. They create a list of phrases, which can be classified into nine types, as conjunction indicators. The object of this task is to recover the conjunction type of given two sentences, which can be used to evaluate how well a model captures the semantic meaning of sentences. We apply the method proposed by Jernite et al. (2017) on the Wikipedia corpus to create our conj dataset. 3.3 Dataset #Sentence Train Dev #Cls"
D18-1492,E17-1002,0,0.0402274,"N). The best number(s) for each task are in bold. The top and down arrows indicate the increment or decrement of each pooling mechanism, against the baseline of pure tree based encoder with the same structure. anism has the benefits of the balanced tree modeling, which also fairly treat all words and learn the crucial parts automatically. The path from representation to words in attention are even shorter than the balanced tree. Thus the fact that attentive pooling outperforms balanced trees on WSR is not surprising to us. 5 Discussions Balanced tree for sentence modeling has been explored by Munkhdalai and Yu (2017) and Williams et al. (2018) in natural language inference (NLI). However, Munkhdalai and Yu (2017) focus on designing inter-attention on trees, instead of comparing balanced tree with other linguistic trees in the same setting. Williams et al. (2018) do compare balanced trees with latent trees, but balanced tree does not outperform the latent one in their experiments, which is consistent with ours. We analyze it in Section 4.2 that sentences in NLI are too short for the balanced tree to show the advantage. Levy et al. (2018) argue that LSTM works for the gates ability to compute an element-wis"
D18-1492,P02-1040,0,0.103465,"51.3 53.5 19.7 20.5 19.9 20.6 20.4 20.9 19.0 22.3 19.2 21.6 19.7 23.1 49.4 76.0 48.0 72.9 54.7 80.4 LSTM +bidirectional 91.7 91.7 87.8 87.8 48.8 49.2 98.6 98.7 66.1 67.4 82.6 82.8 52.8 53.3 20.3 20.2 19.1 21.3 46.9 67.0 Avg. Length 31.5 33.7 33.8 20.1 23.1 11.2 23.3 10.2 34.1 34.1 Parsing +bi-leaf-RNN Trivial Trees Balanced +bi-leaf-RNN Left-branching +bi-leaf-RNN Right-branching +bi-leaf-RNN Linear Structures Table 2: Test results for different encoder architectures trained by a unified encoder-classifier/decoder framework. We report accuracy (⇥100) for classification tasks, and BLEU score (Papineni et al., 2002; word-level for English targets and char-level for Chinese targets) for generation tasks. Large is better for both of the metrics. The best number(s) for each task are in bold. In addition, average sentence length (in words) of each dataset is attached in the last row with underline. Section 4.3. Finally, we compare the performance of linear and tree LSTMs with three widely applied pooling mechanisms in Section 4.4. 4.1 Set-up In experiments, we fix the structure of the classifier as a two-layer MLP with ReLU activation, and the structure of decoder as GRU-based recurrent neural networks (Cho"
D18-1492,J11-1005,0,0.0372549,"a, 2015) optimizer to train all the models, with the learning rate of 1e-3 and batch size of 64. In the 3 We observe that ReLU can significantly boost the performance of Bi-LSTM on SNLI. 4 http://nlp.stanford.edu/data/glove. 840B.300d.zip training stage, we drop the samples with the length of either source sentence or target sentence larger than 64. We do not apply any regularization or dropout term in all experiments except the task of WSR, on which we tune dropout term with respect to the development set. We generate the binary parsing tree for the datasets without parsing trees using ZPar (Zhang and Clark, 2011).5 More details are summarized in supplementary materials. 4.2 Main Results In this subsection, we aim to compare the results from different encoders. We do not include any attention (Wang et al., 2016; Lin et al., 2017) or pooling (Collobert and Weston, 2008; Socher et al., 2011; Zhou et al., 2016b) mechanism here, in order to avoid distractions and make the encoder structure affects the most. We will further analyze pooling mechanisms in Section 4.4. Table 2 presents the performances of different 5 https://www.sutd.edu.sg/cmsresource/ faculty/yuezhang/zpar.html 4635 encoders on a variety of"
D18-1492,D14-1162,0,0.0957312,"verage sentence length (in words) of each dataset is attached in the last row with underline. Section 4.3. Finally, we compare the performance of linear and tree LSTMs with three widely applied pooling mechanisms in Section 4.4. 4.1 Set-up In experiments, we fix the structure of the classifier as a two-layer MLP with ReLU activation, and the structure of decoder as GRU-based recurrent neural networks (Cho et al., 2014). 3 The hidden-layer size of MLP is fixed to 1024, while that of GRU is adapted from the size of sentence encoding. We initialize the word embeddings with 300-dimensional GloVe (Pennington et al., 2014) vectors.4 We apply 300-dimensional bidirectional (600-dimensional in total) LSTM as leaf RNN when necessary. We use Adam (Kingma and Ba, 2015) optimizer to train all the models, with the learning rate of 1e-3 and batch size of 64. In the 3 We observe that ReLU can significantly boost the performance of Bi-LSTM on SNLI. 4 http://nlp.stanford.edu/data/glove. 840B.300d.zip training stage, we drop the samples with the length of either source sentence or target sentence larger than 64. We do not apply any regularization or dropout term in all experiments except the task of WSR, on which we tune dr"
D18-1492,P17-2092,1,0.852791,"units, recent work on sequence modeling proposes to impose tree structure in the encoder (Socher et al., 2013; Tai et al., 2015; Zhu et al., 2015). ⇤ Now at Toyota Technological Institute at Chicago, freda@ttic.edu. This work was done when HS was an intern researcher at ByteDance AI Lab. These tree-based LSTMs introduce syntax tree as an intuitive structure prior for sentence modeling. They have already obtained promising results in many NLP tasks, such as natural language inference (Bowman et al., 2016; Chen et al., 2017c) and machine translation (Eriguchi et al., 2016; Chen et al., 2017a,b; Zhou et al., 2017). Li et al. (2015) empirically concludes that syntax tree-based sentence modeling are effective for tasks requiring relative long-term context features. On the other hand, some works propose to abandon the syntax tree but to adopt the latent tree for sentence modeling (Choi et al., 2018; Yogatama et al., 2017; Maillard et al., 2017; Williams et al., 2018). Such latent trees are directly learned from the downstream task with reinforcement learning (Williams, 1992) or Gumbel Softmax (Jang et al., 2017; Maddison et al., 2017). However, Williams et al. (2018) empirically show that, Gumbel softmax"
D18-1492,C18-1315,1,0.699624,"akes words in the left of the source sentence more important (Sutskever et al., 2014). If the encoder fails to memorize the left words, the information about right words would not help due to the error propagation. In right-branching trees, left words of the sentence are closer to the final representation, which makes the left words are more easy to be memorized, and we call this structure prior. Oppositely, in the case of left-branching trees, right words of the sentence are closer to the representation. To validate our hypothesis, we propose to visualize the Jacobian as word-level saliency (Shi et al., 2018), which can be viewed as the contribution of each word to the sentence encoding: J(s, w) = krs(w)k1 = X @si | | @wj i,j where s = (s1 , s2 , · · · , sp )T denotes the embedding of a sentence, and w = (w1 , w2 , · · · , wq )T denotes embedding of a word. We can compute the saliency score using backward propagation. For a word in a sentence, higher saliency score means more contribution to sentence encoding. We present the visualization in Figure 3 using the visualization tool from Lin et al. (2017). It shows that right-branching tree LSTM encoders tend to look at the left part of the sentence,"
D18-1492,P15-1117,1,0.771324,"ork has two main components: the encoder part and the classifier/decoder part. In general, models encode a sentence to a length-fixed vector, and then applies the vector as the feature for classification and generation. We fix the structure of the classifier/decoder, and propose to use five different types of tree structures for the encoder part including: • Parsing tree. We apply binary constituency tree as the representative, which is widely used in natural language inference (Bowman et al., 2016) and machine translation (Eriguchi et al., 2016; Chen et al., 2017a). Dependency parsing trees (Zhou et al., 2015, 2016a) are not considered in this paper. I love my pet cat . I love my pet cat . LSTM/Tree LSTM LSTM/Tree LSTM <S&gt; I love cats . Multi-Layer Perceptron Softmax I love cats . </S&gt; (a) Encoder-decoder framework for sentence generation. (b) Encoder-classifier framework for sentence classification. I love my pet cat . I love my pet dog . LSTM/Tree LSTM LSTM/Tree LSTM Multi-Layer Perceptron Softmax (c) Siamese encoder-classifier framework for sentence relation classification. Figure 1: The encoder-classifier/decoder framework for three different groups of tasks. We apply multi-layer perceptron (M"
D18-1492,P16-1132,1,0.82387,"rce sentence or target sentence larger than 64. We do not apply any regularization or dropout term in all experiments except the task of WSR, on which we tune dropout term with respect to the development set. We generate the binary parsing tree for the datasets without parsing trees using ZPar (Zhang and Clark, 2011).5 More details are summarized in supplementary materials. 4.2 Main Results In this subsection, we aim to compare the results from different encoders. We do not include any attention (Wang et al., 2016; Lin et al., 2017) or pooling (Collobert and Weston, 2008; Socher et al., 2011; Zhou et al., 2016b) mechanism here, in order to avoid distractions and make the encoder structure affects the most. We will further analyze pooling mechanisms in Section 4.4. Table 2 presents the performances of different 5 https://www.sutd.edu.sg/cmsresource/ faculty/yuezhang/zpar.html 4635 encoders on a variety of downstream tasks, which lead to the following observations: this section, we analyze why these trees achieve high scores in deep. Tree encoders are useful on some tasks. We get the same conclusion with Li et al. (2015) that tree-based encoders perform better on tasks requiring long-term context fea"
D18-1492,D13-1170,0,0.0682442,"s://github. com/ExplorerFreda/TreeEnc. 1 Introduction Sentence modeling is a crucial problem in natural language processing (NLP). Recurrent neural networks with long short term memory (Hochreiter and Schmidhuber, 1997) or gated recurrent units (Cho et al., 2014) are commonly used sentence modeling approaches. These models embed sentences into a vector space and the resulting vectors can be used for classification or sequence generation in the downstream tasks. In addition to the plain sequence of hidden units, recent work on sequence modeling proposes to impose tree structure in the encoder (Socher et al., 2013; Tai et al., 2015; Zhu et al., 2015). ⇤ Now at Toyota Technological Institute at Chicago, freda@ttic.edu. This work was done when HS was an intern researcher at ByteDance AI Lab. These tree-based LSTMs introduce syntax tree as an intuitive structure prior for sentence modeling. They have already obtained promising results in many NLP tasks, such as natural language inference (Bowman et al., 2016; Chen et al., 2017c) and machine translation (Eriguchi et al., 2016; Chen et al., 2017a,b; Zhou et al., 2017). Li et al. (2015) empirically concludes that syntax tree-based sentence modeling are effec"
D18-1492,P15-1150,0,0.275618,"Missing"
D18-1492,P16-1008,0,0.0225749,"tion Pair Dataset is a widely applied dataset to evaluate paraphrasing models (Wang et al., 2017; Li et al., 2017b). 1 In this work, we treat the paraphrasing task as a sequence-to-sequence one, and evaluate on it with our sentence generation framework. Machine Translation (MT). Machine translation, especially cross-language-family machine translation, is a complex task, which requires models to capture the semantic meanings of sentences well. We apply a large challenging EnglishChinese sentence translation task for this investigation, which is adopted by a variety of neural translation work (Tu et al., 2016; Li et al., 2017a; Chen et al., 2017a). We extract the parallel data from the LDC corpora,2 selecting 1.2M from them as our training set, 20K and 80K of them as our development set and test set, respectively. Auto-Encoding (AE). We extract the English part of the machine translation dataset to form a auto-encoding task, which is also compatible with our encoder-decoder framework. 4 Experiments In this section, we present our experimental results and analysis. Section 4.1 introduces our setup for all the experiments. Section 4.2 shows the main results and analysis on ten downstream tasks group"
D18-1492,D16-1058,0,0.0269958,"rd.edu/data/glove. 840B.300d.zip training stage, we drop the samples with the length of either source sentence or target sentence larger than 64. We do not apply any regularization or dropout term in all experiments except the task of WSR, on which we tune dropout term with respect to the development set. We generate the binary parsing tree for the datasets without parsing trees using ZPar (Zhang and Clark, 2011).5 More details are summarized in supplementary materials. 4.2 Main Results In this subsection, we aim to compare the results from different encoders. We do not include any attention (Wang et al., 2016; Lin et al., 2017) or pooling (Collobert and Weston, 2008; Socher et al., 2011; Zhou et al., 2016b) mechanism here, in order to avoid distractions and make the encoder structure affects the most. We will further analyze pooling mechanisms in Section 4.4. Table 2 presents the performances of different 5 https://www.sutd.edu.sg/cmsresource/ faculty/yuezhang/zpar.html 4635 encoders on a variety of downstream tasks, which lead to the following observations: this section, we analyze why these trees achieve high scores in deep. Tree encoders are useful on some tasks. We get the same conclusion with"
D18-1492,Q18-1019,0,0.158922,"Missing"
D18-1492,P16-2022,0,\N,Missing
D19-1336,N19-1172,0,0.538622,"t is a normal recurrent neural language model which takes the target pun word as input. CLM (Mou et al., 2015): It is a constrained language model which guarantees that a pre-given word will appear in the generated sequence. CLM+JD (Yu et al., 2018): It is a state-of-theart model for pun generation which extends a constrained language model by jointly decoding conditioned on two word senses. 3.4 Evaluation Metrics Automatic evaluation: We use two metrics to automatically evaluate the creativeness of the generated puns in terms of unusualness and diversity. Following Pauls and Klein (2012) and He et al. (2019)4 , the unusualness is measured by subtracting the log-probability of training sentences from the log-probability of generated pun sentences. Following Yu et al. (2018), the diversity is measured by the ratio of distinct unigrams (Dist-1) and bigrams (Dist-2) in generated sentences. Human evaluation: Three annotators score the randomly sampled 100 outputs of different systems from 1 to 5 in terms of three criteria. Ambiguity evaluates how likely the sentence is a pun. Fluency measures whether the sentence is fluent. Overall is a comprehensive metric. 3390 Model Pun-GAN vs CLM+JD Pun-GAN vs Hum"
D19-1336,W09-2004,0,0.092266,"Missing"
D19-1336,W16-5307,0,0.055248,"Missing"
D19-1336,D18-1170,1,0.925653,"abulary at t-th timestep is calculated as Gθ (xt |x&lt;t ) = f (W h1t + b) + f (W h2t + b) (1) where h1t (h2t ) is the hidden state of t-th step when taking s1 (s2 ) as input, f is the softmax function, and x&lt;t is the preceding t − 1 words. Therefore, the generation probability of the whole sentence x is formulated as Gθ (x|s1 , s2 ) = Y Gθ (xt |x&lt;t ) (2) t To give a warm start to the generator, we pretrain it using the same general training corpus in the original paper. 2.1.2 Discriminator The discriminator is extended from the word sense disambiguation models (K˚ageb¨ack and Salomonsson, 2016; Luo et al., 2018a,b). Assuming the pun word w in sentence x has k word senses, we add a new “generated” class. Then, the discriminator is designed to produce a probability distribution over k + 1 classes, which is computed as Dφ (y|x) = softmax(Uw c + b0 ) (3) where c is the context vector from a bi-directional LSTM when taking x as input, Uw is a wordspecific parameter and y is the target label.  Therefore, Dφ y = i|x, i ∈ {1, ..., k} denotes the probability that it belongs to the real i-th word sense, while Dφ (y = k + 1|x) denotes the probability that it is produced by a pun generator. 2.2 Training We fol"
D19-1336,P18-1230,1,0.925353,"abulary at t-th timestep is calculated as Gθ (xt |x&lt;t ) = f (W h1t + b) + f (W h2t + b) (1) where h1t (h2t ) is the hidden state of t-th step when taking s1 (s2 ) as input, f is the softmax function, and x&lt;t is the preceding t − 1 words. Therefore, the generation probability of the whole sentence x is formulated as Gθ (x|s1 , s2 ) = Y Gθ (xt |x&lt;t ) (2) t To give a warm start to the generator, we pretrain it using the same general training corpus in the original paper. 2.1.2 Discriminator The discriminator is extended from the word sense disambiguation models (K˚ageb¨ack and Salomonsson, 2016; Luo et al., 2018a,b). Assuming the pun word w in sentence x has k word senses, we add a new “generated” class. Then, the discriminator is designed to produce a probability distribution over k + 1 classes, which is computed as Dφ (y|x) = softmax(Uw c + b0 ) (3) where c is the context vector from a bi-directional LSTM when taking x as input, Uw is a wordspecific parameter and y is the target label.  Therefore, Dφ y = i|x, i ∈ {1, ..., k} denotes the probability that it belongs to the real i-th word sense, while Dφ (y = k + 1|x) denotes the probability that it is produced by a pun generator. 2.2 Training We fol"
D19-1336,P15-1070,0,0.0609047,"Missing"
D19-1336,S17-2005,0,0.244865,"s – English Wikipedia to train Pun-GAN. For generator, we first tag each word in the English Wikipedia corpus with one word sense using an unsupervised WSD tool2 . Then we use the 2,595K tagged corpus to pre-train our generator. For discriminator, we use several types of data for training: 1) SemCor (Luo et al., 2018a,b) which is a manually annotated corpus for WSD, consisting of 226K sense annotations3 (first part in Eq.4); 2) Wikipedia corpus as unlabeled corpus (second part in Eq.4); 3) Generated puns (third part in Eq.4). Evaluation Dataset: We use the pun dataset from SemEval 2017 task7 (Miller et al., 2017) for evaluation. The dataset consists of 1274 humanwritten puns where target pun words are annotated with two word senses. During testing, we extract the word sense pair as the input of our model. 3.2 Experimental Setting The generator is the same as Yu et al. (2018). The discriminator is a single-layer bi-directional LSTM with hidden size 128. We randomly initialize word embeddings with the dimension size of 300. The sample size K is set as 32. Batch size is 32 and learning rate is 0.001. The optimization algorithm is SGD. Before adversarial training, we pre-train the generator for 5 epochs a"
D19-1336,P12-1101,0,0.0181233,"M (Mikolov et al., 2010): It is a normal recurrent neural language model which takes the target pun word as input. CLM (Mou et al., 2015): It is a constrained language model which guarantees that a pre-given word will appear in the generated sequence. CLM+JD (Yu et al., 2018): It is a state-of-theart model for pun generation which extends a constrained language model by jointly decoding conditioned on two word senses. 3.4 Evaluation Metrics Automatic evaluation: We use two metrics to automatically evaluate the creativeness of the generated puns in terms of unusualness and diversity. Following Pauls and Klein (2012) and He et al. (2019)4 , the unusualness is measured by subtracting the log-probability of training sentences from the log-probability of generated pun sentences. Following Yu et al. (2018), the diversity is measured by the ratio of distinct unigrams (Dist-1) and bigrams (Dist-2) in generated sentences. Human evaluation: Three annotators score the randomly sampled 100 outputs of different systems from 1 to 5 in terms of three criteria. Ambiguity evaluates how likely the sentence is a pun. Fluency measures whether the sentence is fluent. Overall is a comprehensive metric. 3390 Model Pun-GAN vs"
D19-1336,P13-2041,0,0.0766619,"Missing"
D19-1336,D17-1120,0,0.046399,"Missing"
D19-1336,P13-2044,0,0.365409,"Missing"
D19-1336,P18-1153,0,0.436043,"in ambiguity and diversity. 2 Model The sketch of the proposed Pun-GAN is depicted in Figure 1. It consists of a pun generator Gθ and a word sense discriminator Dφ . The following sections will elaborate on the architecture of Pun-GAN and its training algorithm. 2.1 Model Structure 2.1.1 Generator Given two senses (s1 , s2 ) of a target word w, the generator Gθ aims to output a sentence x which not only contains the target word w but also express the two corresponding meanings. Considering the simplicity of the model and the ease of training, we adopt the neural constrained language model of Yu et al. (2018) as the generator. Due to space constraints, we strongly recommend that readers refer to the original paper for details. Compared with traditional neural language model, the main difference is that the generated words at each timestep should have the maximum sum of two probabilities which are calculated with s1 and s2 as input, respectively. Formally, the generation probability over the entire vocabulary at t-th timestep is calculated as Gθ (xt |x&lt;t ) = f (W h1t + b) + f (W h2t + b) (1) where h1t (h2t ) is the hidden state of t-th step when taking s1 (s2 ) as input, f is the softmax function,"
K19-1077,Q17-1010,0,0.00770847,"atasets manually. Details of all six datasets are shown in Table 2. • Jaccard Similarity Upper Bound (JS) For each summary sentence, we compute its jaccard similarity with every article sentence. The largest jaccard similarity for each summary sentence is selected as JS. It measures the extent to which summaries copy articles. The worst situation is 1. Hyperparameters and Optimization All the CNN models use a 50000 words article dictionary and 20000 words summary dictionary with byte pair encoding (BPE) (Sennrich et al., 2015). Word embeddings are pretrained on training corpus using Fasttext (Bojanowski et al., 2017; Joulin et al., 2016). We do not train models with large parameters to increase ROUGE results since what we try to improve is the comprehensiveness of each sentence in summary. The total parameters of whole CNN seq2seq Model are about 3800w and the DivCNN Seq2Seq does not change the parameters amount. All the models set embedding dimensionality and CNN channels to 256. The encoder has 20 blocks with kernel size 5 and the decoder has 4 blocks with kernel size 3. Such scale of model parameters are enough for the model to generate fluent summaries. The γ is 0.6 for Macro • Sentence Coverage (SC)"
K19-1077,P18-1063,0,0.257355,"le peaks in sentence attention and have high diversity at the same time. Neither aggregative nor scattering attention distributions do good to summary generation. Direct attention model has the maximum NOVEL score which means point information about a specific input element makes model prefer copying article words instead of generating new words. not appear in the article. It reflects the abstraction of summaries. The worst situation is 0. Strong Baseline We chose four strong baselines which reported high ROUGE scores. BottomUp (Gehrmann et al., 2018), sumGAN (Liu et al., 2018) and RL Rerank (Chen and Bansal, 2018) are complicate systems that have additional modules or post-processings and partially relieved the OTR problem. The Pointer Generator (See et al., 2017) reaches best ROUGE result in single end to end model but suffers greatly from repetition problem. Various Possible Causes of the OTR problem We had supposed several other reasons for the repetition problems besides attention degeneration including overfitting, bad usage of translation-style attention mechanism, lack of decoding ability and high variance attention distribution. Respectively, we designed comparative experiments as follows: CNN-"
K19-1077,D18-1207,0,0.0312701,"Missing"
K19-1077,D18-1443,0,0.0790607,"erasing the peak or the variance of attention but to have multiple peaks in sentence attention and have high diversity at the same time. Neither aggregative nor scattering attention distributions do good to summary generation. Direct attention model has the maximum NOVEL score which means point information about a specific input element makes model prefer copying article words instead of generating new words. not appear in the article. It reflects the abstraction of summaries. The worst situation is 0. Strong Baseline We chose four strong baselines which reported high ROUGE scores. BottomUp (Gehrmann et al., 2018), sumGAN (Liu et al., 2018) and RL Rerank (Chen and Bansal, 2018) are complicate systems that have additional modules or post-processings and partially relieved the OTR problem. The Pointer Generator (See et al., 2017) reaches best ROUGE result in single end to end model but suffers greatly from repetition problem. Various Possible Causes of the OTR problem We had supposed several other reasons for the repetition problems besides attention degeneration including overfitting, bad usage of translation-style attention mechanism, lack of decoding ability and high variance attention distribution. R"
K19-1077,D15-1166,0,0.0607705,"rashed into french alps . Table 1: Article-summary sample from CNN-DM dataset. Colored spans are attentive parts. Micro DPPs model puts wider attention on article than vanilla does and Macro DPPs puts the widest attention, including former two models’ attentive parts. text, attention in summarization should be soft and diverse. Many works noticed that attention may be over concentrated for summarization and hence cause problems like generating duplicate words or duplicate sentences. Researchers try to solve these problems by introducing various attention structures, including local attention (Luong et al., 2015), hierarchical attention (Nallapati et al., 2016), distraction attention (Chen et al., 2016) and coverage mechanism (See et al., 2017) etc. But all these works ignore another repeat problem, as we call it, ”Original Text Repetition”. We define and explain this problem in section 3. In this paper we propose a novel Diverse Convolutional Seq2Seq Model(DivCNN Seq2Seq) based on Micro Determinantal Point Processes(Micro DPPs) and Macro Determinantal Point Processes(Macro DPPs). Our contributions are as fol822 Proceedings of the 23rd Conference on Computational Natural Language Learning, pages 822 –"
K19-1077,N18-1065,0,0.105327,"Missing"
K19-1077,K16-1028,0,0.0272932,"mmary sample from CNN-DM dataset. Colored spans are attentive parts. Micro DPPs model puts wider attention on article than vanilla does and Macro DPPs puts the widest attention, including former two models’ attentive parts. text, attention in summarization should be soft and diverse. Many works noticed that attention may be over concentrated for summarization and hence cause problems like generating duplicate words or duplicate sentences. Researchers try to solve these problems by introducing various attention structures, including local attention (Luong et al., 2015), hierarchical attention (Nallapati et al., 2016), distraction attention (Chen et al., 2016) and coverage mechanism (See et al., 2017) etc. But all these works ignore another repeat problem, as we call it, ”Original Text Repetition”. We define and explain this problem in section 3. In this paper we propose a novel Diverse Convolutional Seq2Seq Model(DivCNN Seq2Seq) based on Micro Determinantal Point Processes(Micro DPPs) and Macro Determinantal Point Processes(Macro DPPs). Our contributions are as fol822 Proceedings of the 23rd Conference on Computational Natural Language Learning, pages 822 – 832 c 2019 Association for Computational Linguis"
K19-1077,D15-1044,0,0.0418491,"ginal article, which is a combination of Natural Language Understanding(NLU) and Natural Language Generation(NLG). Abstractive summarization uses Seq2Seq models (Sutskever et al., 2014) which consist of an encoder, a decoder and attention mechanism (Mnih et al., 2014). With attention mechanism the decoder can choose a weighted context representation at each generation step so it can focus on different parts of encoded information. Seq2Seq with attention achieved remarkable results on machine translation (Bahdanau et al., 2014) and other text generation tasks such as abstractive summarizaiton (Rush et al., 2015). Unlike machine translation that emphasizes attention mechanism as a method of learning word level alignments between source text and target 1 available at https://github.com/thinkwee/DPP CNN Sum marization Article: marseille , france the french prosecutor leading an investigation into the crash of germanwings flight 9525 insisted wednesday that he was not aware of any video footage from on board the plane . marseille prosecutor brice robin told cnn that so far no videos were used in the crash investigation ...... of a cell phone video showing the harrowing final seconds from on board germanw"
K19-1077,P17-1099,0,0.578125,"wider attention on article than vanilla does and Macro DPPs puts the widest attention, including former two models’ attentive parts. text, attention in summarization should be soft and diverse. Many works noticed that attention may be over concentrated for summarization and hence cause problems like generating duplicate words or duplicate sentences. Researchers try to solve these problems by introducing various attention structures, including local attention (Luong et al., 2015), hierarchical attention (Nallapati et al., 2016), distraction attention (Chen et al., 2016) and coverage mechanism (See et al., 2017) etc. But all these works ignore another repeat problem, as we call it, ”Original Text Repetition”. We define and explain this problem in section 3. In this paper we propose a novel Diverse Convolutional Seq2Seq Model(DivCNN Seq2Seq) based on Micro Determinantal Point Processes(Micro DPPs) and Macro Determinantal Point Processes(Macro DPPs). Our contributions are as fol822 Proceedings of the 23rd Conference on Computational Natural Language Learning, pages 822 – 832 c 2019 Association for Computational Linguistics Hong Kong, China, November 3-4, 2019. channels (the same as embedding size). Con"
K19-1077,P19-1212,0,0.0159807,"21) (22) (23) (24) i∈DY The c and d can be updated incrementally according to equation 22 and 23. The complete algorithm is described in Algorithm 1. BFGMInference algorithm gains significant speed improvements when the size of L matrix is large as shown in Figure 5. 5 Experimental Setup Datasets We test DivCNN Seq2Seq model on the widely used CNN-DM dataset (Hermann et al., 2015) and give detailed analysis on diversity and quality. Also we tried our model on other five abstractive summarization datasets which are NEWSROOM corpus (Grusky et al., 2018), TLDR (V¨olske et al., 2017), BIGPATENT (Sharma et al., 2019), WIKIHOW (Koupaee and Wang, 2018) and REDDIT (Kim et al., 2018). For CNN-DM corpus we truncate articles to 600 words and summaries to BFGMInference uses a greedy method to approximate the MAP result Ymap = argmaxY ∈D det(LY ): each time we select j that has maximum QD-score improvements and add it to Y . j = argmax f (Y ∪ {i}) − f (Y ) = Lii − ||ci ||22 (20) Then we can transform equation 19 into: Classic sampling algorithm for DPPs (Kulesza and Taskar, 2011) runs slow when the size of L matrix is large and it can not be computed in batch. In the DivCNN Seq2Seq model we need to construct an L"
K19-1077,W17-4508,0,0.536723,"Missing"
K19-1077,P16-1162,0,\N,Missing
N18-1113,D14-1181,0,0.00316236,"have two views, the headline and the paragraph. Thus, we construct the two classifiers in co-training based on these two views. Headline Classifier The previous state-of-theart model (Zhou, 2017) for clickbait detection uses 1257 a self-attentive bi-directional gated recurrent unit RNN (biGRU) to model the headlines of the document and train a classifier. Following the same setting, we choose self-attentive biGRU as the headline classifier in co-training. Paragraph Classifier The paragraphs usually have much longer sequences than the headlines. Thus, we utilize the CNN-non-static structure in Kim (2014) as the paragraph classifier to capture the paragraph information. Note that the other three co-training baselines also use the same classifier settings. In our Reinforce Co-Training model, we set the number of unlabeled subsets k as 80. Considering the clickbait detection as a 2-class classification problem (N = 2), the Q-network maps 4-d input Pi1 ||Pi2 in the state representation to a 3-d common embedding space (y = 3), with a further hidden layer of 128 units on top. The dimension k of the softmax layer is also 80. As for the other semi-supervised baselines, Sequence-SSL, Region-SSL and Ad"
N18-1113,D17-1035,0,0.0308153,"that after sample different data partitions, we will also reprocess the unlabeled sets as described in Section 3.1. We then evaluate these 10 classifiers using the same metric. The results are shown in Table 6. The results demonstrate that our learning algorithm is robust to different (seeding) training sets and partitions of the unlabeled set, which again indicates that the Q-agent in our model is able to learn a good and robust data selection policy to select high-quality unlabeled subsets to help the co-training process. 4.4 Discussion about Stability Previous studies (Zhang et al., 2014; Reimers and Gurevych, 2017) show that neural networks can be unstable even with the same training parameters on the same training data. As for our cases, when the two classifiers are initialized with different labeled seeding sets, they can be very unstable. However, after enough iterations with the properly selected unlabeled data, the performance would be stable generally. Usually, the more substantial labeled training datasets will lead to more stable models. However, the problem is that the AGs News and DBpedia have 4 and 14 classes separately, while the Clickbait dataset only has 2 classes. That means the numbers o"
N18-1113,P09-1027,0,0.307834,"eled text corpora available on the web. Semi-supervised methods permit learning improved supervised models by jointly train on a small labeled dataset and a large unlabeled dataset (Zhu, 2006; Chapelle et al., 2009). Co-training is one of the widely used semisupervised methods, where two complementary classifiers utilize large amounts of unlabeled examples to bootstrap the performance of each other iteratively (Blum and Mitchell, 1998; Nigam and Ghani, 2000). Co-training can be readily applied to NLP tasks since data in these tasks naturally have two or more views, such as multi-lingual data (Wan, 2009) and document data (headline and content) (Ghani, 2000; Denis et al., 2003). In the co-training framework, each classifier is trained on one of the two views (aka a subset of features) of both labeled and unlabeled data, under the assumption that either view is sufficient to classify. In each iteration, the co-training algorithm selects high confidence samples scored by each of the classifiers to form an auto-labeled dataset, and the other classifier is then updated with both labeled data and additional auto-labeled set. However, as shown in Figure 1, most of existing co-training methods have"
P16-1076,P13-1158,0,0.0968794,"n of the questions are about facts or trivia. It has been a long pursuit to enable machines to answer such questions automatically. In recent years, several efforts have been made on utilizing open-domain knowledge bases to answer factoid questions. A knowledge ∗ Wei Xu Baidu Research xuwei06@baidu.com sˆ, rˆ = arg max p(s, r|q) Part of the work was done while at Baidu. s,r∈K 800 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 800–810, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics (1) paraphrase information (Fader et al., 2013; Berant and Liang, 2014), requiring little questionanswer pairs (Reddy et al., 2014), and exploiting ideas from agenda-based parsing (Berant and Liang, 2015). Based on the formulation (1), the central problem is to estimate the conditional distribution p(s, r|q). It is very challenging because of a) the vast amount of facts — a large-scale KB such as Freebase contains billions of triples, b) the huge variety of language — there are multiple aliases for an entity, and numerous ways to compose a question, c) the severe sparsity of supervision — most combinations of s, r, q are not expressed in"
P16-1076,P14-1133,0,0.0364646,"re about facts or trivia. It has been a long pursuit to enable machines to answer such questions automatically. In recent years, several efforts have been made on utilizing open-domain knowledge bases to answer factoid questions. A knowledge ∗ Wei Xu Baidu Research xuwei06@baidu.com sˆ, rˆ = arg max p(s, r|q) Part of the work was done while at Baidu. s,r∈K 800 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 800–810, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics (1) paraphrase information (Fader et al., 2013; Berant and Liang, 2014), requiring little questionanswer pairs (Reddy et al., 2014), and exploiting ideas from agenda-based parsing (Berant and Liang, 2015). Based on the formulation (1), the central problem is to estimate the conditional distribution p(s, r|q). It is very challenging because of a) the vast amount of facts — a large-scale KB such as Freebase contains billions of triples, b) the huge variety of language — there are multiple aliases for an entity, and numerous ways to compose a question, c) the severe sparsity of supervision — most combinations of s, r, q are not expressed in training data. Faced with"
P16-1076,Q15-1039,0,0.0328417,"ral efforts have been made on utilizing open-domain knowledge bases to answer factoid questions. A knowledge ∗ Wei Xu Baidu Research xuwei06@baidu.com sˆ, rˆ = arg max p(s, r|q) Part of the work was done while at Baidu. s,r∈K 800 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 800–810, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics (1) paraphrase information (Fader et al., 2013; Berant and Liang, 2014), requiring little questionanswer pairs (Reddy et al., 2014), and exploiting ideas from agenda-based parsing (Berant and Liang, 2015). Based on the formulation (1), the central problem is to estimate the conditional distribution p(s, r|q). It is very challenging because of a) the vast amount of facts — a large-scale KB such as Freebase contains billions of triples, b) the huge variety of language — there are multiple aliases for an entity, and numerous ways to compose a question, c) the severe sparsity of supervision — most combinations of s, r, q are not expressed in training data. Faced with these challenges, existing methods have exploited to incorporate prior knowledge into semantic parsers, to design models and represe"
P16-1076,D15-1038,0,0.0149571,"e training E(r), is insufficient in practice — while a large-scale KB has millions of subjects, only thousands of question-triple pairs are available for training. To alleviate the problem, we seek two potential solutions: a) pretrained embeddings, and b) type vector representation. The pretrained embedding approach utilizes unsupervised method to train entity embedings. In particular, we employ the TransE (Bordes et al., 2013), which trains the embedings of entities and relations by enforcing E(s) + E(r) = E(o) for every observed triple (s, r, o) ∈ K. As there exists other improved variants (Gu et al., 2015), TransE scales the best when KB size grows. Alternatively, type vector is a fixed (not trainable) vector representation of entities using type information. Since each entity in the KB has one or more predefined types, we can encode the entity as a vector (bag) of types. Each dimension of a type vector is either 1 or 0, indicating whether the entity is associated with a specific type or not. Thus, the dimensionality of a type vector is equal to the number of types in KB. Under this setting, with E(s) being a binary vector, let g(q) be a continuous vector with arbitrary value range can be probl"
P16-1076,D13-1160,0,0.263614,"elies on factors other than similarity measurement. Related Work The research of KB supported QA has evolved from earlier domain-specific QA (Zelle and Mooney, 1996; Tang and Mooney, 2001; Liang et al., 2013) to open-domain QA based on largescale KBs. An important line of research has been trying to tackle the problem by semantic parsing, which directly parses natural language questions into structured queries (Liang et al., 2011; Cai and Yates, 2013; Kwiatkowski et al., 2013; Yao and Van Durme, 2014). Recent progresses include designing KB specific logical representation and parsing grammar (Berant et al., 2013), using distant supervision (Berant et al., 2013), utilizing Besides KB-based QA, our work is also loosely related to work using deep learning systems in QA tasks with free text evidences. For example, (Iyyer et al., 2014) focuses questions from the quiz bowl competition with recursive neural network. New architectures including memory networks (Weston et al., 2015), dynamic memory networks (Kumar et al., 2015), and more (Peng et al., 2015; Lee et al., 2015) have been explored under the bAbI syn801 A key difference from prior multi-step approach is that our method do not assume any independenc"
P16-1076,D14-1070,0,0.0292823,"Missing"
P16-1076,D14-1067,0,0.466091,"Missing"
P16-1076,D13-1161,0,0.0212997,"works (RNN) to produce the question representation. More importantly, our method follows a probabilistic formulation, and our parameterization relies on factors other than similarity measurement. Related Work The research of KB supported QA has evolved from earlier domain-specific QA (Zelle and Mooney, 1996; Tang and Mooney, 2001; Liang et al., 2013) to open-domain QA based on largescale KBs. An important line of research has been trying to tackle the problem by semantic parsing, which directly parses natural language questions into structured queries (Liang et al., 2011; Cai and Yates, 2013; Kwiatkowski et al., 2013; Yao and Van Durme, 2014). Recent progresses include designing KB specific logical representation and parsing grammar (Berant et al., 2013), using distant supervision (Berant et al., 2013), utilizing Besides KB-based QA, our work is also loosely related to work using deep learning systems in QA tasks with free text evidences. For example, (Iyyer et al., 2014) focuses questions from the quiz bowl competition with recursive neural network. New architectures including memory networks (Weston et al., 2015), dynamic memory networks (Kumar et al., 2015), and more (Peng et al., 2015; Lee et al., 201"
P16-1076,P13-1042,0,0.0906336,"recurrent neural networks (RNN) to produce the question representation. More importantly, our method follows a probabilistic formulation, and our parameterization relies on factors other than similarity measurement. Related Work The research of KB supported QA has evolved from earlier domain-specific QA (Zelle and Mooney, 1996; Tang and Mooney, 2001; Liang et al., 2013) to open-domain QA based on largescale KBs. An important line of research has been trying to tackle the problem by semantic parsing, which directly parses natural language questions into structured queries (Liang et al., 2011; Cai and Yates, 2013; Kwiatkowski et al., 2013; Yao and Van Durme, 2014). Recent progresses include designing KB specific logical representation and parsing grammar (Berant et al., 2013), using distant supervision (Berant et al., 2013), utilizing Besides KB-based QA, our work is also loosely related to work using deep learning systems in QA tasks with free text evidences. For example, (Iyyer et al., 2014) focuses questions from the quiz bowl competition with recursive neural network. New architectures including memory networks (Weston et al., 2015), dynamic memory networks (Kumar et al., 2015), and more (Peng et"
P16-1076,P11-1060,0,0.013846,"r, we propose to use recurrent neural networks (RNN) to produce the question representation. More importantly, our method follows a probabilistic formulation, and our parameterization relies on factors other than similarity measurement. Related Work The research of KB supported QA has evolved from earlier domain-specific QA (Zelle and Mooney, 1996; Tang and Mooney, 2001; Liang et al., 2013) to open-domain QA based on largescale KBs. An important line of research has been trying to tackle the problem by semantic parsing, which directly parses natural language questions into structured queries (Liang et al., 2011; Cai and Yates, 2013; Kwiatkowski et al., 2013; Yao and Van Durme, 2014). Recent progresses include designing KB specific logical representation and parsing grammar (Berant et al., 2013), using distant supervision (Berant et al., 2013), utilizing Besides KB-based QA, our work is also loosely related to work using deep learning systems in QA tasks with free text evidences. For example, (Iyyer et al., 2014) focuses questions from the quiz bowl competition with recursive neural network. New architectures including memory networks (Weston et al., 2015), dynamic memory networks (Kumar et al., 2015"
P16-1076,D14-1179,0,0.0201928,"Missing"
P16-1076,J13-2005,0,0.00877152,"ur proposed method is closely related to the second line of research, since neural models are employed to learn semantic representations. As in (Bordes et al., 2015; Yih et al., 2014), we focus on single-fact questions. However, we propose to use recurrent neural networks (RNN) to produce the question representation. More importantly, our method follows a probabilistic formulation, and our parameterization relies on factors other than similarity measurement. Related Work The research of KB supported QA has evolved from earlier domain-specific QA (Zelle and Mooney, 1996; Tang and Mooney, 2001; Liang et al., 2013) to open-domain QA based on largescale KBs. An important line of research has been trying to tackle the problem by semantic parsing, which directly parses natural language questions into structured queries (Liang et al., 2011; Cai and Yates, 2013; Kwiatkowski et al., 2013; Yao and Van Durme, 2014). Recent progresses include designing KB specific logical representation and parsing grammar (Berant et al., 2013), using distant supervision (Berant et al., 2013), utilizing Besides KB-based QA, our work is also loosely related to work using deep learning systems in QA tasks with free text evidences."
P16-1076,D14-1162,0,0.0861204,"Missing"
P16-1076,Q14-1030,0,0.019754,"achines to answer such questions automatically. In recent years, several efforts have been made on utilizing open-domain knowledge bases to answer factoid questions. A knowledge ∗ Wei Xu Baidu Research xuwei06@baidu.com sˆ, rˆ = arg max p(s, r|q) Part of the work was done while at Baidu. s,r∈K 800 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 800–810, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics (1) paraphrase information (Fader et al., 2013; Berant and Liang, 2014), requiring little questionanswer pairs (Reddy et al., 2014), and exploiting ideas from agenda-based parsing (Berant and Liang, 2015). Based on the formulation (1), the central problem is to estimate the conditional distribution p(s, r|q). It is very challenging because of a) the vast amount of facts — a large-scale KB such as Freebase contains billions of triples, b) the huge variety of language — there are multiple aliases for an entity, and numerous ways to compose a question, c) the severe sparsity of supervision — most combinations of s, r, q are not expressed in training data. Faced with these challenges, existing methods have exploited to incorp"
P16-1076,P05-1044,0,0.0251165,"41,880 facts. There are alternative datasets available, such as WebQuestions (Berant et al., 2013) and (17) where r(j) is one of the Mr negative samples (i.e. s(i) 6→ r(j) ) randomly sampled from R, and γr is 2 N X K  X i=1 k=1 As the two problems defined by equation (16) take the standard form of classification, theoretically, cross entropy can used as the training objective. However, computing the partition function is often intractable, especially for pθs (s|r, q), since there can be millions of entities in the KB. Faced with this problem, classic solutions include contrastive estimation (Smith and Eisner, 2005), importance sampling approximation (Bengio et al., 2003), and hinge loss with negative samples (Collobert and Weston, 2008). In this work, we utilize the hinge loss with negative samples as the training objective. Specifically, the loss function w.r.t θr has the form L(θr ) =  + u(s(j) , r(i) , q (i) )     + 1 − E(s(i) )k log 1 − g(q (i) )k Approximation with Negative Samples Mr N X X  max 0, γs − u(s(i) , r(i) , q (i) ) Despite the negative sample based approximation, there is another practical difficulty when type vector is used as the subject representation. Specifically, computing"
P16-1076,D14-1071,0,0.0580256,"with large-scale knowledge bases. The contributions of this paper are, In contrast, another line of research tackles the problem by deep learning powered similarity matching. The core idea is to learn semantic representations of both the question and the knowledge from observed data, such that the correct supporting evidence will be the nearest neighbor of the question in the learned vector space. Thus, a main difference among several approaches lies in the neural networks proposed to represent questions and KB elements. While (Bordes et al., 2014b; Bordes et al., 2014a; Bordes et al., 2015; Yang et al., 2014) use relatively shallow embedding models to represent the question and knowledge, (Yih et al., 2014; Yih et al., 2015) employ a convolutional neural network (CNN) to produce the representation. In the latter case, both the question and the relation are treated as a sequence of letter-trigram patterns, and fed into two parameter shared CNNs to get their embeddings. What’s more, instead of measuring the similarity between a question and an evidence triple with a single model as in (Bordes et al., 2015), (Yih et al., 2014; Yih et al., 2015) adopt a multi-stage approach. In each stage, one element"
P16-1076,P14-1090,0,0.202769,"Missing"
P16-1076,P14-2105,0,0.509324,"research tackles the problem by deep learning powered similarity matching. The core idea is to learn semantic representations of both the question and the knowledge from observed data, such that the correct supporting evidence will be the nearest neighbor of the question in the learned vector space. Thus, a main difference among several approaches lies in the neural networks proposed to represent questions and KB elements. While (Bordes et al., 2014b; Bordes et al., 2014a; Bordes et al., 2015; Yang et al., 2014) use relatively shallow embedding models to represent the question and knowledge, (Yih et al., 2014; Yih et al., 2015) employ a convolutional neural network (CNN) to produce the representation. In the latter case, both the question and the relation are treated as a sequence of letter-trigram patterns, and fed into two parameter shared CNNs to get their embeddings. What’s more, instead of measuring the similarity between a question and an evidence triple with a single model as in (Bordes et al., 2015), (Yih et al., 2014; Yih et al., 2015) adopt a multi-stage approach. In each stage, one element of the triple is compared with the question to produce a partial similarity score by a dedicated m"
P16-1076,P15-1128,0,0.758684,"he problem by deep learning powered similarity matching. The core idea is to learn semantic representations of both the question and the knowledge from observed data, such that the correct supporting evidence will be the nearest neighbor of the question in the learned vector space. Thus, a main difference among several approaches lies in the neural networks proposed to represent questions and KB elements. While (Bordes et al., 2014b; Bordes et al., 2014a; Bordes et al., 2015; Yang et al., 2014) use relatively shallow embedding models to represent the question and knowledge, (Yih et al., 2014; Yih et al., 2015) employ a convolutional neural network (CNN) to produce the representation. In the latter case, both the question and the relation are treated as a sequence of letter-trigram patterns, and fed into two parameter shared CNNs to get their embeddings. What’s more, instead of measuring the similarity between a question and an evidence triple with a single model as in (Bordes et al., 2015), (Yih et al., 2014; Yih et al., 2015) adopt a multi-stage approach. In each stage, one element of the triple is compared with the question to produce a partial similarity score by a dedicated model. Then, these p"
P19-1193,P82-1020,0,0.820094,"Missing"
P19-1193,D14-1181,0,0.0030238,"input contains a variable number of topics, here we implement Dφ as a multi-label classiﬁer to distinguish between the real text with several topics and the generated text. In detail, suppose there are a total of |X |topics, the discriminator produces a sigmoid probability distribution over (|X |+ 1) classes. The score at the i-th (i ∈ {1, · · · , |X |}) index represents the probability that it belongs to the real text with the i-th topic, and the score at the (|X |+ 1)-th index represents the probability that the sample is the generated text. Here we implement the discriminator Dφ as a CNN (Kim, 2014) binary classiﬁer. 2.3 based on both state y1:t−1 and action yt is observed, the training objective of the generator Gθ is to minimize the negative expected reward, J(θ) = −Ey∼Gθ [r(y)] =− n−1  Gθ (yt+1 |y1:t ) · r(y1:t , yt+1 ) (14) t=1 where Gθ (yt+1 |yt ) means the probability that selects the word yt+1 based on the previous generated words. Applying the likelihood ratios trick and sampling method, we can build an unbiased estimation for the gradient of J(θ), ∇θ J(θ) ≈ − n−1  ∇θ logGθ (yt+1 |y1:t ) t=1 · r(y1:t , yt+1 )  (15) where yt+1 is the sampled word. Since the discriminator can o"
P19-1193,W17-3528,0,0.137882,"Missing"
P19-1193,P18-1082,0,0.0307102,"coder to learn topic information. Yi et al. (2018) simultaneously train two generators via mutual reinforcement learning. However, different from poetry generation presenting obvious structured rules, the TEG task requires generating a long unstructured plain text. Such unstructured target output tends to result in the topic drift problem, bringing severe challenges to the TEG task. 2009 Another similar task is story generation, which aims to generate a story based on the short description of an event. Jain et al. (2017) employ statistical machine translation to explore story generation while Lewis et al. (2018) propose a hierarchical strategy. Xu et al. (2018) utilize reinforcement learning to extract a skeleton of the story to promote the coherence. To improve the diversity and coherence, Yao et al. (2018) present a planand-write framework with two planning strategies to fully leverage storyline. However, story generation and the TEG task focus on different goals. The former focuses on logical reasoning and aims to generate a coherent story with plots, while the latter strives to generate the essay with aesthetics based on the input topics. Besides, the source information of the TEG task is more in"
P19-1193,N16-1014,0,0.1466,"Missing"
P19-1193,D18-1423,0,0.0620706,"Missing"
P19-1193,P18-1230,1,0.897461,"Missing"
P19-1193,P02-1040,0,0.10903,"includes: TAV representing topic semantics as the average of all topic embeddings and TAT applying attention mechanism to select the relevant topics. CVAE (Yang et al., 2018b) presents a conditional variational auto-encoder with a hybrid decoder to learn topic via latent variables. Plan&Write (Yao et al., 2018) proposes a planand-write framework with two planning strategies to improve diversity and coherence. 3.4 3.4.1 Automatic Evaluation The automatic evaluation of TEG remains an open and tricky question since the output is highly ﬂexible. Previous work (Feng et al., 2018) only adopts BLEU (Papineni et al., 2002) score based on ngram overlap to perform evaluation. However, it is unreasonable to only use BLEU for evaluation because TEG is an extremely ﬂexible task. There are multiple ideal essays for a set of input topics. To remedy this, here we develop a series of evaluation metrics to comprehensively measure the quality of output from various aspects. Consistency: An ideal essay should closely surround the semantics of all input topics. Therefore, we pre-train a multi-label classiﬁer to evaluate topic-consistency of the output. Given the input topics x, we deﬁne the topic-consistency of the generate"
P19-1193,D15-1044,0,0.0388062,"Automatic topic-to-essay generation (TEG) aims at generating novel, diverse, and topic-consistent paragraph-level text given a set of topics. It not only has plenty of practical applications, e.g., beneﬁting intelligent education or assisting in keyword-based news writing (Lepp¨anen et al., 2017), but also serves as an ideal testbed for controllable text generation (Wang and Wan, 2018). Despite its wide applications described above, the progress in the TEG task lags behind other generation tasks such as machine translation (Bahdanau et al., 2014) or text summarization (Rush et al., 2015). Feng et al. (2018) are the ﬁrst to propose the TEG task and they utilize coverage vector Equal Contribution.        Figure 1: Toy illustration of the information volume on three different text generation tasks, which shows that the source information is extremely insufﬁcient compared to the target output on the TEG task. Introduction ∗          to incorporate topic information for essay generation. However, the model performance is not satisfactory. The generated essays not only lack novelty and diversity, but also suffer from poor topiccons"
P19-1193,N18-2028,0,0.0307918,"Missing"
P19-1193,speer-havasi-2012-representing,0,0.166057,"Missing"
P19-1193,C16-1100,0,0.121336,"ency(ˆ y |x) = ϕ(x, x ˆ) (19) where ϕ is Jaccard similarity function and x ˆ is topics predicted by a pre-trained multi-label classiﬁer. Here we adopt the SGM model proposed in Yang et al. (2018a) to implement the pre-trained multi-label classiﬁer. Novelty: The novelty of the output can be reﬂected by the difference between it and the training texts. We calculate the novelty of each generated essay yˆ as: N ovelty(ˆ y |x) =1 − max{ϕ(ˆ y , y0 )| Baselines We adopt the following competitive baselines: SC-LSTM (Wen et al., 2015) uses gating mechanism to control the ﬂow of topic information. PNN (Wang et al., 2016) applies planning based neural network to generate topic-consistent text. MTA (Feng et al., 2018) utilizes coverage vectors to integrate topic information. Their work also includes: TAV representing topic semantics as the average of all topic embeddings and TAT applying attention mechanism to select the relevant topics. CVAE (Yang et al., 2018b) presents a conditional variational auto-encoder with a hybrid decoder to learn topic via latent variables. Plan&Write (Yao et al., 2018) proposes a planand-write framework with two planning strategies to improve diversity and coherence. 3.4 3.4.1 Autom"
P19-1193,D15-1199,0,0.0615644,"Missing"
P19-1193,D18-1462,1,0.848098,"multaneously train two generators via mutual reinforcement learning. However, different from poetry generation presenting obvious structured rules, the TEG task requires generating a long unstructured plain text. Such unstructured target output tends to result in the topic drift problem, bringing severe challenges to the TEG task. 2009 Another similar task is story generation, which aims to generate a story based on the short description of an event. Jain et al. (2017) employ statistical machine translation to explore story generation while Lewis et al. (2018) propose a hierarchical strategy. Xu et al. (2018) utilize reinforcement learning to extract a skeleton of the story to promote the coherence. To improve the diversity and coherence, Yao et al. (2018) present a planand-write framework with two planning strategies to fully leverage storyline. However, story generation and the TEG task focus on different goals. The former focuses on logical reasoning and aims to generate a coherent story with plots, while the latter strives to generate the essay with aesthetics based on the input topics. Besides, the source information of the TEG task is more insufﬁcient, putting higher demands on the model. 6"
P19-1193,C18-1330,1,0.928389,"single layer of LSTM with hidden size 512 for both encoder and decoder. We pre-train our model for 80 epochs with the MLE method. The optimizer is Adam (Kingma and Ba, 2014) with 10−3 learning rate for pre-training and 10−5 for adversarial training. Besides, we make use of the dropout method (Srivastava et al., 2014) to avoid overﬁtting and clip the gradients (Pascanu et al., 2013) to the maximum norm of 10. 3.3 Consistency(ˆ y |x) = ϕ(x, x ˆ) (19) where ϕ is Jaccard similarity function and x ˆ is topics predicted by a pre-trained multi-label classiﬁer. Here we adopt the SGM model proposed in Yang et al. (2018a) to implement the pre-trained multi-label classiﬁer. Novelty: The novelty of the output can be reﬂected by the difference between it and the training texts. We calculate the novelty of each generated essay yˆ as: N ovelty(ˆ y |x) =1 − max{ϕ(ˆ y , y0 )| Baselines We adopt the following competitive baselines: SC-LSTM (Wen et al., 2015) uses gating mechanism to control the ﬂow of topic information. PNN (Wang et al., 2016) applies planning based neural network to generate topic-consistent text. MTA (Feng et al., 2018) utilizes coverage vectors to integrate topic information. Their work also incl"
P19-1193,D18-1353,0,0.0403253,"generation. Early work adopts rule and template based methods (Tosa et al., 2008; Yan et al., 2013). When involving in neural networks, both Zhang and Lapata (2014) and Wang et al. (2016) employ recurrent neural network and planning to perform generation. Yan (2016) further propose a new generative model with a polishing schema. To balance linguistic accordance and aesthetic innovation, Zhang et al. (2017) adopt memory network to choose each term from reserved inventories. Yang et al. (2018b) and Li et al. (2018) further utilize conditional variational autoencoder to learn topic information. Yi et al. (2018) simultaneously train two generators via mutual reinforcement learning. However, different from poetry generation presenting obvious structured rules, the TEG task requires generating a long unstructured plain text. Such unstructured target output tends to result in the topic drift problem, bringing severe challenges to the TEG task. 2009 Another similar task is story generation, which aims to generate a story based on the short description of an event. Jain et al. (2017) employ statistical machine translation to explore story generation while Lewis et al. (2018) propose a hierarchical strateg"
P19-1193,P17-1125,0,0.0156374,"performance is unsatisfactory, showing that more effective model architecture needs to be explored, which is also the original intention of our work. A similar topic-to-sequence learning task is Chinese poetry generation. Early work adopts rule and template based methods (Tosa et al., 2008; Yan et al., 2013). When involving in neural networks, both Zhang and Lapata (2014) and Wang et al. (2016) employ recurrent neural network and planning to perform generation. Yan (2016) further propose a new generative model with a polishing schema. To balance linguistic accordance and aesthetic innovation, Zhang et al. (2017) adopt memory network to choose each term from reserved inventories. Yang et al. (2018b) and Li et al. (2018) further utilize conditional variational autoencoder to learn topic information. Yi et al. (2018) simultaneously train two generators via mutual reinforcement learning. However, different from poetry generation presenting obvious structured rules, the TEG task requires generating a long unstructured plain text. Such unstructured target output tends to result in the topic drift problem, bringing severe challenges to the TEG task. 2009 Another similar task is story generation, which aims"
P19-1193,D14-1074,0,0.0351914,"neration (TEG) aims to compose novel, diverse, and topic-consistent paragraph-level text for several given topics. Feng et al. (2018) are the ﬁrst to propose the TEG task and they utilize coverage vector to integrate topic information. However, the performance is unsatisfactory, showing that more effective model architecture needs to be explored, which is also the original intention of our work. A similar topic-to-sequence learning task is Chinese poetry generation. Early work adopts rule and template based methods (Tosa et al., 2008; Yan et al., 2013). When involving in neural networks, both Zhang and Lapata (2014) and Wang et al. (2016) employ recurrent neural network and planning to perform generation. Yan (2016) further propose a new generative model with a polishing schema. To balance linguistic accordance and aesthetic innovation, Zhang et al. (2017) adopt memory network to choose each term from reserved inventories. Yang et al. (2018b) and Li et al. (2018) further utilize conditional variational autoencoder to learn topic information. Yi et al. (2018) simultaneously train two generators via mutual reinforcement learning. However, different from poetry generation presenting obvious structured rules"
P19-1193,D18-1138,1,0.878655,"Missing"
P19-1257,P11-1020,0,0.0184667,"Missing"
P19-1257,D14-1179,0,0.00669761,"Missing"
P19-1257,P19-1285,0,0.06116,"Missing"
P19-1257,D18-1170,1,0.837476,"17) proposes a dynamic co-attention network for the question answering task and Seo et al. (2017) presents a bi-directional attention network to acquire query-aware context representations in machine comprehension. Tay et al. (2018a) proposes a co-attention mechanism based on Hermitian products for asymmetrical text matching problems. Zhong et al. (2019) further presents a coarse-grain ﬁne-grain co-attention network that combines information from evidence across multiple documents for question answering. In addition, the co-attention mechanism can also be applied to word sense disambiguation (Luo et al., 2018), recommended system (Tay et al., 2018b), and essay scoring (Zhang and Litman, 2018). 6 Conclusion In this paper, we propose the task of cross-modal automatic commenting, which aims at enabling the AI agent to make comments by integrating multiple modal contents. We construct a largescale dataset for this task and implement plenty of representative neural models. Furthermore, an effective co-attention model is presented to capture the intrinsic interaction between multiple modal contents. Experimental results show that our approach can substantially outperform various competitive baselines. Fu"
P19-1257,P02-1040,0,0.105952,"tention to 512 and the hidden size of feedforward layer to 2,048. The number of heads is set to 8, while a transformer layer consists of 6 blocks. We use Adam optimizer (Kingma and Ba, 2015) with learning rate 10−3 and apply dropout (Srivastava et al., 2014) to avoid over-ﬁtting. 4.2 BLEU-1 ROUGE-L DIST-1 DIST-2 Baselines We adopt the following competitive baselines: Seq2Seq: We implement a series of baselines based on Seq2Seq. S2S-V (Vinyals et al., 2015) Evaluation Metrics We adopt two kinds of evaluation methods: automatic evaluation and human evaluation. Automatic evaluation: We use BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004) to evaluate overlap between outputs and references. We also calculate the number of distinct n-grams (Li et al., 2016) in outputs to measure diversity. Human evaluation: Three annotators score the 200 outputs of different systems from 1 to 10. The evaluation criteria are as follows. Fluency measures whether the comment is ﬂuent. Relevance evaluates the relevance between the output and the input. Informativeness measures the amount of useful information contained in the output. Overall is a comprehensive metric. For each metric, the average Pearson correlation coefﬁcient"
P19-1257,P82-1020,0,0.754236,"Missing"
P19-1257,N16-1014,0,0.0170731,"We use Adam optimizer (Kingma and Ba, 2015) with learning rate 10−3 and apply dropout (Srivastava et al., 2014) to avoid over-ﬁtting. 4.2 BLEU-1 ROUGE-L DIST-1 DIST-2 Baselines We adopt the following competitive baselines: Seq2Seq: We implement a series of baselines based on Seq2Seq. S2S-V (Vinyals et al., 2015) Evaluation Metrics We adopt two kinds of evaluation methods: automatic evaluation and human evaluation. Automatic evaluation: We use BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004) to evaluate overlap between outputs and references. We also calculate the number of distinct n-grams (Li et al., 2016) in outputs to measure diversity. Human evaluation: Three annotators score the 200 outputs of different systems from 1 to 10. The evaluation criteria are as follows. Fluency measures whether the comment is ﬂuent. Relevance evaluates the relevance between the output and the input. Informativeness measures the amount of useful information contained in the output. Overall is a comprehensive metric. For each metric, the average Pearson correlation coefﬁcient is greater than 0.6, indicating that the human scores are highly consistent. 4.4 Experimental Results Table 3 and Table 4 show the results of"
P19-1257,P18-2025,0,0.10047,"3. 㔯㢢ཊ⛩ቡྭҶǄ (It would be better if there is more greenness.) Figure 1: An example in the constructed dataset. Red words indicate the content that is not included in the text but depicted in the images. Introduction Comments of online articles can provide rich supplementary information, which reduces the difﬁculty of understanding the article and enhances interactions between users. Therefore, achieving automatic commenting is necessary since it can contribute to improving user experience and increasing the activeness of social media platforms. Due to the importance described above, some work (Qin et al., 2018; Lin et al., 2018; Ma et al., 2018) has explored this task. However, these efforts are all focus on automatic commenting based solely on textual content. In real-scenarios, online ∗ Equal Contribution. The dataset and code are available at https:// github.com/lancopku/CMAC 1 articles on social media usually contain multiple modal contents. Take graphic news as an example, it contains plenty of images in addition to text. Other contents except text are also vital to improving automatic commenting. These contents may contain some information that is critical for generating informative comments."
P19-1257,W18-0549,0,0.0236437,"d Seo et al. (2017) presents a bi-directional attention network to acquire query-aware context representations in machine comprehension. Tay et al. (2018a) proposes a co-attention mechanism based on Hermitian products for asymmetrical text matching problems. Zhong et al. (2019) further presents a coarse-grain ﬁne-grain co-attention network that combines information from evidence across multiple documents for question answering. In addition, the co-attention mechanism can also be applied to word sense disambiguation (Luo et al., 2018), recommended system (Tay et al., 2018b), and essay scoring (Zhang and Litman, 2018). 6 Conclusion In this paper, we propose the task of cross-modal automatic commenting, which aims at enabling the AI agent to make comments by integrating multiple modal contents. We construct a largescale dataset for this task and implement plenty of representative neural models. Furthermore, an effective co-attention model is presented to capture the intrinsic interaction between multiple modal contents. Experimental results show that our approach can substantially outperform various competitive baselines. Further analysis demonstrates that with multiple modal information and co-attention, t"
P19-1257,D18-1138,1,0.892416,"Missing"
P19-1257,C18-1330,1,0.891219,"Missing"
P19-1257,Q14-1006,0,0.0389151,"Missing"
P19-1559,D18-1316,0,0.593166,"d and Dotted lines represent decision boundaries before and after adversarial training, respectively. As unfluent adversarial examples are not in the manifold of real sentences, the victim model only needs to adjust its decision boundary out of the sentence manifold to fit them. As a result, fluent adversarial examples may be more effective than unfluent ones. propose to perturb a sentence by flipping one of the characters, and use the gradient of each perturbation to guide sample selection. But simple character flipping often leads to meaningless words (eg. “mood” to “mooP”). Genetic attack (Alzantot et al., 2018) is a population-based word replacing attacker, which aims to generate fluent sentences by filtering out the unreasonable sentences with a language model. But the fluency of examples generated by genetic attack is still not satisfactory and it is inefficient as the gradient is discarded. To address the aforementioned problems, we propose the Metropolis-Hastings attack (MHA) algorithm in this short paper. MHA is an adversarial example generator based on Metropolis-Hastings (M-H) sampling (Metropolis et al., 1953; HASTINGS, 1970; Chib and Greenberg, 1995). MH sampling is a classical MCMC samplin"
P19-1559,baccianella-etal-2010-sentiwordnet,0,0.140035,"Missing"
P19-1559,D15-1075,0,0.0365575,"-box MHA (bMHA) and a white-box MHA (w-MHA). Specifically, in contrast to previous language generation models using M-H, b-MHA’s stationary distribution is equipped with a language model term and an adversarial attacking term. The two terms make the generation of adversarial examples fluent and effective. w-MHA even incorporates adversarial gradients into proposal distributions to speed up the generation of adversarial examples. Our contributions include that we propose an efficient approach for generating fluent adversarial examples. Experimental results on IMDB (Maas et al., 2011) and SNLI (Bowman et al., 2015) show that, compared with the state-of-the-art genetic model, MHA generates examples faster, achieving higher success rates with much fewer invocations. Meanwhile, adversarial samples from MHA are not only more fluent but also more effective to improve the adversarial robustness and classification accuracy after adversarial training. 2 Preliminary Generally, adversarial attacks aim to mislead the neural models by feeding adversarial examples with perturbations, while adversarial training aims to improve the models by utilizing the perturbed examples. Adversarial examples fool the model into pr"
P19-1559,P18-2006,0,0.131084,"ls (such as a text classifier) is extremely challenging. Firstly, it is difficult to perform gradientbased perturbations since the sentence space is discrete. However, gradient information is critical – it leads to the steepest direction to more effective examples. Secondly, adversarial examples are usually not fluent sentences. Unfluent examples are less effective in attacking, as victim models can easily learn to recognize them. Meanwhile, adversarial training on them usually does not perform well (see Figure 1 for detailed analysis). Current methods cannot properly handle the two problems. Ebrahimi et al. (2018) (HotFlip) ∗ Work done while Huangzhao Zhang was a research intern in ByteDance AI Lab, Beijing, China. Figure 1: Effect of adversarial training on (a) fluent and (b) unfluent adversarial examples. ◦ and • represent positive and negative samples in the training set, while M and N are the corresponding adversarial examples. Solid and Dotted lines represent decision boundaries before and after adversarial training, respectively. As unfluent adversarial examples are not in the manifold of real sentences, the victim model only needs to adjust its decision boundary out of the sentence manifold to f"
P19-1559,esuli-sebastiani-2006-sentiwordnet,0,0.00946615,"Missing"
P19-1559,W16-5502,0,0.028268,"Missing"
P19-1559,P11-1015,0,0.106593,"riants of MHA, namely a black-box MHA (bMHA) and a white-box MHA (w-MHA). Specifically, in contrast to previous language generation models using M-H, b-MHA’s stationary distribution is equipped with a language model term and an adversarial attacking term. The two terms make the generation of adversarial examples fluent and effective. w-MHA even incorporates adversarial gradients into proposal distributions to speed up the generation of adversarial examples. Our contributions include that we propose an efficient approach for generating fluent adversarial examples. Experimental results on IMDB (Maas et al., 2011) and SNLI (Bowman et al., 2015) show that, compared with the state-of-the-art genetic model, MHA generates examples faster, achieving higher success rates with much fewer invocations. Meanwhile, adversarial samples from MHA are not only more fluent but also more effective to improve the adversarial robustness and classification accuracy after adversarial training. 2 Preliminary Generally, adversarial attacks aim to mislead the neural models by feeding adversarial examples with perturbations, while adversarial training aims to improve the models by utilizing the perturbed examples. Adversarial"
P19-1602,P97-1064,0,0.0341154,"onal auto-encoders (VAEs) is proposed by Kingma and Welling (2014) for image generation. Bowman et al. (2016) successfully applied VAE in the NLP domain, showing that VAE improves recurrent neural network (RNN)-based language modeling (RNN-LM, Mikolov et al., 2010); that VAE allows sentence sampling and sentence interpolation in the continuous latent space. Later, VAE is widely used in various natural language generation tasks (Gupta et al., 2018; Kusner et al., 2017; Hu et al., 2017; Deriu and Cieliebak, 2018). Syntactic language modeling, to the best of our knowledge, could be dated back to Chelba (1997). Charniak (2001) and Clark (2001) propose to utilize a top-down parsing mechanism for language modeling. Dyer et al. (2016) and Kuncoro et al. (2017) introduce the neural network to this direction. The Parsing-Reading-Predict Network (PRPN, Shen et al., 2017), which reports a state-of-the-art results on syntactic language modeling, learns a latent syntax by training with a language modeling objective. Different from their work, our approach models syntax in a continuous space, facilitating sampling and manipulation of syntax. Our work is also related to style-transfer text generation (Fu et a"
P19-1602,P17-1177,1,0.860672,"om/baoy-nlp/DSS-VAE † even manually manipulate the latent space, inspiring various applications such as sentence interpolation (Bowman et al., 2016) and text style transfer (Hu et al., 2017). However, the continuous latent space of VAE blends syntactic and semantic information together, without modeling the syntax explicitly. We argue that it may be not necessarily the best in the text generation scenario. Recently, researchers have shown that explicitly syntactic modeling improves the generation quality in sequence-tosequence models (Eriguchi et al., 2016; Zhou et al., 2017; Li et al., 2017; Chen et al., 2017). It is straightforward to adopt such idea in the VAE setting, since a vanilla VAE does not explicitly model the syntax. A line of studies (Kusner et al., 2017; G´omez-Bombarelli et al., 2018; Dai et al., 2018) propose to impose context-free grammars (CFGs) as hard constraints in the VAE decoder, so that they could generate syntactically valid outputs of programs, molecules, etc. However, the above approaches cannot be applied to syntactic modeling in VAE’s continuous latent space, and thus, we do not enjoy the two benefits of VAE, namely, sampling and manipulation, towards the syntax of a sen"
P19-1602,W01-0713,0,0.0302084,"ed by Kingma and Welling (2014) for image generation. Bowman et al. (2016) successfully applied VAE in the NLP domain, showing that VAE improves recurrent neural network (RNN)-based language modeling (RNN-LM, Mikolov et al., 2010); that VAE allows sentence sampling and sentence interpolation in the continuous latent space. Later, VAE is widely used in various natural language generation tasks (Gupta et al., 2018; Kusner et al., 2017; Hu et al., 2017; Deriu and Cieliebak, 2018). Syntactic language modeling, to the best of our knowledge, could be dated back to Chelba (1997). Charniak (2001) and Clark (2001) propose to utilize a top-down parsing mechanism for language modeling. Dyer et al. (2016) and Kuncoro et al. (2017) introduce the neural network to this direction. The Parsing-Reading-Predict Network (PRPN, Shen et al., 2017), which reports a state-of-the-art results on syntactic language modeling, learns a latent syntax by training with a language modeling objective. Different from their work, our approach models syntax in a continuous space, facilitating sampling and manipulation of syntax. Our work is also related to style-transfer text generation (Fu et al., 2018; Li et al., 2018a; John e"
P19-1602,W18-6503,0,0.0200293,"AE could graft the designed syntax to another sentence under certain circumstances. 2 Related Work The variational auto-encoders (VAEs) is proposed by Kingma and Welling (2014) for image generation. Bowman et al. (2016) successfully applied VAE in the NLP domain, showing that VAE improves recurrent neural network (RNN)-based language modeling (RNN-LM, Mikolov et al., 2010); that VAE allows sentence sampling and sentence interpolation in the continuous latent space. Later, VAE is widely used in various natural language generation tasks (Gupta et al., 2018; Kusner et al., 2017; Hu et al., 2017; Deriu and Cieliebak, 2018). Syntactic language modeling, to the best of our knowledge, could be dated back to Chelba (1997). Charniak (2001) and Clark (2001) propose to utilize a top-down parsing mechanism for language modeling. Dyer et al. (2016) and Kuncoro et al. (2017) introduce the neural network to this direction. The Parsing-Reading-Predict Network (PRPN, Shen et al., 2017), which reports a state-of-the-art results on syntactic language modeling, learns a latent syntax by training with a language modeling objective. Different from their work, our approach models syntax in a continuous space, facilitating samplin"
P19-1602,N16-1024,0,0.03024,"lly applied VAE in the NLP domain, showing that VAE improves recurrent neural network (RNN)-based language modeling (RNN-LM, Mikolov et al., 2010); that VAE allows sentence sampling and sentence interpolation in the continuous latent space. Later, VAE is widely used in various natural language generation tasks (Gupta et al., 2018; Kusner et al., 2017; Hu et al., 2017; Deriu and Cieliebak, 2018). Syntactic language modeling, to the best of our knowledge, could be dated back to Chelba (1997). Charniak (2001) and Clark (2001) propose to utilize a top-down parsing mechanism for language modeling. Dyer et al. (2016) and Kuncoro et al. (2017) introduce the neural network to this direction. The Parsing-Reading-Predict Network (PRPN, Shen et al., 2017), which reports a state-of-the-art results on syntactic language modeling, learns a latent syntax by training with a language modeling objective. Different from their work, our approach models syntax in a continuous space, facilitating sampling and manipulation of syntax. Our work is also related to style-transfer text generation (Fu et al., 2018; Li et al., 2018a; John et al., 2018). In previous work, the style is usually defined by categorical features such"
P19-1602,P16-1078,0,0.0219963,"release the implementation and models at https:// github.com/baoy-nlp/DSS-VAE † even manually manipulate the latent space, inspiring various applications such as sentence interpolation (Bowman et al., 2016) and text style transfer (Hu et al., 2017). However, the continuous latent space of VAE blends syntactic and semantic information together, without modeling the syntax explicitly. We argue that it may be not necessarily the best in the text generation scenario. Recently, researchers have shown that explicitly syntactic modeling improves the generation quality in sequence-tosequence models (Eriguchi et al., 2016; Zhou et al., 2017; Li et al., 2017; Chen et al., 2017). It is straightforward to adopt such idea in the VAE setting, since a vanilla VAE does not explicitly model the syntax. A line of studies (Kusner et al., 2017; G´omez-Bombarelli et al., 2018; Dai et al., 2018) propose to impose context-free grammars (CFGs) as hard constraints in the VAE decoder, so that they could generate syntactically valid outputs of programs, molecules, etc. However, the above approaches cannot be applied to syntactic modeling in VAE’s continuous latent space, and thus, we do not enjoy the two benefits of VAE, namely"
P19-1602,P02-1040,0,0.104047,"9.79 11.09 syntax-VAE BLEU↑ 7.26 7.41 8.19 8.98 9.07 9.26 9.36 Forward PPL↓ 34.01 35.00 36.53 42.44 44.11 48.70 49.73 VAE 12 DSS-VAE 11 BLEU KL-Weight 1.3 1.2 1.0 0.7 0.5 0.3 0.1 10 9 8 Table 1: BLEU and Forward PPL of VAE with varying KL weights on the PTB test set. The larger↑ (or lower↓ ), the better. 7 31 36 41 46 51 Forward-PPL 1. Reconstruction BLEU. The reconstruction task aims to generate the input sentence itself. In the task, both syntactic and semantic vectors are chosen as the predicted mean of the encoded distribution. We evaluate the reconstruction performance by the BLEU score (Papineni et al., 2002) with input as the reference.3 It reflects how well the model could preserve input information, and is crucial for representation learning and “goal-oriented” text generation. 2. Forward PPL. We then perform unconditioned generation, where both syntactic and semantic vectors are sampled from prior. Forward perplexity (PPL) (Zhao et al., 2018) is the generated sentences’ perplexity score predicted by a pertained language model.4 It shows the fluency of generated sentences from VAE’s prior. We computed Forward PPL based on 100K sampled sentences. 3. Reverse PPL. Unconditioned generation is furth"
P19-1602,D17-1066,0,0.0159685,"ee sequence, leading to better performance of language generation. Additionally, the advantage of sampling in the disentangled syntactic and semantic latent spaces enables us to perform novel applications, such as the unsupervised paraphrase generation and syntaxtransfer generation. Experimental results show that our proposed model achieves similar or better performance in various tasks, compared with state-of-the-art related work. ‡ 1 Introduction Variational auto-encoders (VAEs, Kingma and Welling, 2014) are widely used in language generation tasks (Serban et al., 2017; Kusner et al., 2017; Semeniuta et al., 2017; Li et al., 2018b). VAE encodes a sentence into a probabilistic latent space, from which it learns to decode the same sentence. In addition to traditional reconstruction loss of an autoencoder, VAE employs an extra regularization term, penalizing the Kullback– Leibler (KL) divergence between the encoded posterior distribution and its prior. This property enables us to sample and generate sentences from the continuous latent space. Additionally, we can ∗ Equal contributions. Corresponding author. ‡ We release the implementation and models at https:// github.com/baoy-nlp/DSS-VAE † even manually"
P19-1602,E17-1117,0,0.0226876,"NLP domain, showing that VAE improves recurrent neural network (RNN)-based language modeling (RNN-LM, Mikolov et al., 2010); that VAE allows sentence sampling and sentence interpolation in the continuous latent space. Later, VAE is widely used in various natural language generation tasks (Gupta et al., 2018; Kusner et al., 2017; Hu et al., 2017; Deriu and Cieliebak, 2018). Syntactic language modeling, to the best of our knowledge, could be dated back to Chelba (1997). Charniak (2001) and Clark (2001) propose to utilize a top-down parsing mechanism for language modeling. Dyer et al. (2016) and Kuncoro et al. (2017) introduce the neural network to this direction. The Parsing-Reading-Predict Network (PRPN, Shen et al., 2017), which reports a state-of-the-art results on syntactic language modeling, learns a latent syntax by training with a language modeling objective. Different from their work, our approach models syntax in a continuous space, facilitating sampling and manipulation of syntax. Our work is also related to style-transfer text generation (Fu et al., 2018; Li et al., 2018a; John et al., 2018). In previous work, the style is usually defined by categorical features such as sentiment. We move one"
P19-1602,N18-1169,0,0.105168,"Missing"
P19-1602,P17-1064,0,0.0150872,"https:// github.com/baoy-nlp/DSS-VAE † even manually manipulate the latent space, inspiring various applications such as sentence interpolation (Bowman et al., 2016) and text style transfer (Hu et al., 2017). However, the continuous latent space of VAE blends syntactic and semantic information together, without modeling the syntax explicitly. We argue that it may be not necessarily the best in the text generation scenario. Recently, researchers have shown that explicitly syntactic modeling improves the generation quality in sequence-tosequence models (Eriguchi et al., 2016; Zhou et al., 2017; Li et al., 2017; Chen et al., 2017). It is straightforward to adopt such idea in the VAE setting, since a vanilla VAE does not explicitly model the syntax. A line of studies (Kusner et al., 2017; G´omez-Bombarelli et al., 2018; Dai et al., 2018) propose to impose context-free grammars (CFGs) as hard constraints in the VAE decoder, so that they could generate syntactically valid outputs of programs, molecules, etc. However, the above approaches cannot be applied to syntactic modeling in VAE’s continuous latent space, and thus, we do not enjoy the two benefits of VAE, namely, sampling and manipulation, towards"
P19-1602,D18-1423,0,0.108519,"better performance of language generation. Additionally, the advantage of sampling in the disentangled syntactic and semantic latent spaces enables us to perform novel applications, such as the unsupervised paraphrase generation and syntaxtransfer generation. Experimental results show that our proposed model achieves similar or better performance in various tasks, compared with state-of-the-art related work. ‡ 1 Introduction Variational auto-encoders (VAEs, Kingma and Welling, 2014) are widely used in language generation tasks (Serban et al., 2017; Kusner et al., 2017; Semeniuta et al., 2017; Li et al., 2018b). VAE encodes a sentence into a probabilistic latent space, from which it learns to decode the same sentence. In addition to traditional reconstruction loss of an autoencoder, VAE employs an extra regularization term, penalizing the Kullback– Leibler (KL) divergence between the encoded posterior distribution and its prior. This property enables us to sample and generate sentences from the continuous latent space. Additionally, we can ∗ Equal contributions. Corresponding author. ‡ We release the implementation and models at https:// github.com/baoy-nlp/DSS-VAE † even manually manipulate the l"
P19-1602,D17-1013,1,0.827385,"uth training signals; in testing, we do not need external syntactic trees. We build an RNN 1 https://www.sutd.edu.sg/cmsresource/faculty/yuezhang/ zpar.html 6010 with softmax, whose objective is the cross-entropy loss against the groundtruth distribution t, given by: X tw log p(w|zsem ) (3) L(mul) sem = − This is ... S NP .. . /S This is This is ... w∈V ... S NP .. . /S This is ... Figure 2: Overview of our DSS-VAE. Forward dashed arrows are multi-task losses; backward dashed arrows are adversarial losses. where p(w|zsyn ) is the predicted distribution. BoW has been explored by previous work (Weng et al., 2017; John et al., 2018), showing good ability of preserving semantics. For the syntactic space, the multi-task loss trains a model to predict syntax on zsyn . Due to our proposal in §3.2.1, we could build a dedicated RNN, predicting the tokens in the linearized parse tree sequence, whose loss is: Xn log p(si |s1 · · · si−1 , zsyn ) (4) L(mul) syn = − i=1 (independent of the VAE’s decoder) to predict such linearized parse trees, where each parsing token is represented by an embedding (similar to a traditional RNN decoder). Notice that, a node and its backtracking, e.g., NP and /NP, have different"
P19-1602,P18-2053,0,0.0136272,"ters are not updated. 3.2.3 Adversarial Reconstruction Loss Our next intuition is that syntax and semantics are more interwoven to each other than other information such as style and content. Suppose, for example, the syntax and semantics have been perfectly separated by the losses in 6011 §3.2.2, where zsem could predict BoW well, but does not contain any information about the syntactic tree. Even in this ideal case, the decoder can reconstruct the original sentence from zsem by simply learning to re-order words (as zsem does contain BoW). Such word re-ordering knowledge is indeed learnable (Ma et al., 2018), and does not necessarily contain the syntactic information. Therefore, the multi-task and adversarial losses for syntax and semantics do not suffice to regularize DSS-VAE. We now propose an adversarial reconstruction loss to discourage the sentence being predicted by a single subspace zsyn or zsem . When combined, however, they should provide a holistic view of the entire sentence. Formally, let zs be a latent variable (zs = zsyn or zsem ). A decoding adversary is trained to predict the sentence based on zs , denoted by prec (xi |x1 · · · xi−1 , zs ). Then, the adversarial reconstruction los"
P19-1602,J93-2004,0,0.0651646,"er during training. 4 Experiments We evaluate our method on reconstruction and unconditional language generation (§4.1). Then, we apply it two applications, namely, unsupervised paraphrase generation (§4.2) and syntax-transfer generation (§4.3). 4.1 Reconstruction and Unconditional Language Generation First, we compare our model in reconstruction and unconditional language generation with a traditional VAE and a syntactic language model (PRPN, Shen et al., 2017). Dataset We followed previous work (Bowman et al., 2016) and used a standard benchmark, the WSJ sections in the Penn Treebank (PTB) (Marcus et al., 1993). We also followed the standard split: Sections 2–21 for training, Section 24 for validation, and Section 23 for test. Settings We trained VAE and DSS-VAE, both with 100-dimensional RNN states. For the vocabulary, we chose 30k most frequent words. We trained PRPN with the default parameter in the code base.2 Evaluation We evaluate model performance with the following metrics: 6012 2 https://github.com/yikangshen/PRPN 47.33 8.98 45.6 9.6 49.73 9.36 49.79 11.09 syntax-VAE BLEU↑ 7.26 7.41 8.19 8.98 9.07 9.26 9.36 Forward PPL↓ 34.01 35.00 36.53 42.44 44.11 48.70 49.73 VAE 12 DSS-VAE 11 BLEU KL-Wei"
P19-1602,P17-2092,1,0.84797,"tion and models at https:// github.com/baoy-nlp/DSS-VAE † even manually manipulate the latent space, inspiring various applications such as sentence interpolation (Bowman et al., 2016) and text style transfer (Hu et al., 2017). However, the continuous latent space of VAE blends syntactic and semantic information together, without modeling the syntax explicitly. We argue that it may be not necessarily the best in the text generation scenario. Recently, researchers have shown that explicitly syntactic modeling improves the generation quality in sequence-tosequence models (Eriguchi et al., 2016; Zhou et al., 2017; Li et al., 2017; Chen et al., 2017). It is straightforward to adopt such idea in the VAE setting, since a vanilla VAE does not explicitly model the syntax. A line of studies (Kusner et al., 2017; G´omez-Bombarelli et al., 2018; Dai et al., 2018) propose to impose context-free grammars (CFGs) as hard constraints in the VAE decoder, so that they could generate syntactically valid outputs of programs, molecules, etc. However, the above approaches cannot be applied to syntactic modeling in VAE’s continuous latent space, and thus, we do not enjoy the two benefits of VAE, namely, sampling and mani"
P19-1602,P01-1017,0,\N,Missing
P19-1602,D14-1179,0,\N,Missing
P19-1602,K16-1002,0,\N,Missing
P19-1617,N19-1240,0,0.0561575,"Missing"
P19-1617,N18-2007,0,0.217335,". This setup is versatile and does not rely on any additional predefined knowledge base. Therefore the models are expected to generalize well and to answer questions in open domains. There are two main challenges to answer questions of this kind. Firstly, since not every document contain relevant information, multi-hop textbased QA requires filtering out noises from multiple paragraphs and extracting useful information. To address this, recent studies propose to build entity graphs from input paragraphs and apply graph neural networks (GNNs) to aggregate the information through entity graphs (Dhingra et al., 2018; De Cao et al., 2018; Song et al., 2018a). However, all of the existing work apply GNNs based on a static global entity graph of each QA pair, which can be considered as performing implicit reasoning. Instead of them, we argue that the queryguided multi-hop reasoning should be explicitly performed on a dynamic local entity graph tailored according to the query. a fusion process in DFGN to solve the unrestricted QA challenge. We not only aggregate information from documents to the entity graph (doc2graph), but also propagate the information of the entity graph back to document representations"
P19-1617,P14-5010,0,0.00431779,"cores greater than η (= 0.1 in experiments) are selected and concateInput Documents Input Query Paragraph Selector Graph Constructor Context Entity Graph Encoder BERT Bi-attention Fusion Block multi-hop LSTM Prediction Layer Supporting Sentences Answer Span Answer Type Figure 3: Overview of DFGN. nated together as the context C. η is properly chosen to ensure the selector reaches a significantly high recall of relevant paragraphs. Q and C are further processed by upper layers. 3.2 Constructing Entity Graph We do not assume a global knowledge base. Instead, we use the Stanford corenlp toolkit (Manning et al., 2014) to recognize named entities from the context C. The number of extracted entities is denoted as N . The entity graph is constructed with the entities as nodes and edges built as follows. The edges are added 1. for every pair of entities appear in the same sentence in C (sentencelevel links); 2. for every pair of entities with the same mention text in C (context-level links); and 3. between a central entity node and other entities within the same paragraph (paragraph-level links). The central entities are extracted from the title sentence for each paragraph. Notice the context-level links ensur"
P19-1617,P18-1160,0,0.0306546,"Question: What ﬁction character created by Tom Clancy was turned into a ﬁlm in 2002? Answer: Jack Ryan Original Entity Graph Introduction Question answering (QA) has been a popular topic in natural language processing. QA provides a quantifiable way to evaluate an NLP system’s capability on language understanding and reasoning (Hermann et al., 2015; Rajpurkar et al., 2016, 2018). Most previous work focus on finding evidence and answers from a single paragraph (Seo et al., 2016; Liu et al., 2017; Wang et al., 2017). It rarely tests deep reasoning capabilities of the underlying model. In fact, Min et al. (2018) observe that most questions in existing QA benchmarks can be answered by retrieving † These authors contributed equally. The order of authorship is decided through dice rolling. Work done while Lin Qiu was a research intern in ByteDance AI Lab. First Mask Applied Second Mask Applied Figure 1: Example of multi-hop text-based QA. One question and three document paragraphs are given. Our proposed DFGN conducts multi-step reasoning over the facts by constructing an entity graph from multiple paragraphs, predicting a dynamic mask to select a subgraph, propagating information along the graph, and f"
P19-1617,D16-1241,0,0.0283079,"onvolutional neural network (GCN) (Kipf and Welling, 2017). Coref-GRN, MHQA-GRN and Entity-GCN explore the graph construction problem in answering real-world questions. However, it is yet to investigate how to effectively reason about the constructed graphs, which is the main problem studied in this work. Another group of sequential models deals with multi-hop reasoning following Memory Networks (Sukhbaatar et al., 2015). Such models construct representations for queries and memory cells for contexts, then make interactions between them in a multi-hop manner. Munkhdalai and Yu (2017) 6142 and Onishi et al. (2016) incorporate a hypothesis testing loop to update the query representation at each reasoning step and select the best answer among the candidate entities at the last step. IRNet (Zhou et al., 2018) generates a subject state and a relation state at each step, computing the similarity score between all the entities and relations given by the dataset KB. The ones with the highest score at each time step are linked together to form an interpretable reasoning chain. However, these models perform reasoning on simple synthetic datasets with a limited number of entities and relations, which are quite d"
P19-1617,P18-2124,0,0.0962946,"Missing"
P19-1617,D16-1264,0,0.774841,"racter created by Tom Clancy who appears in many of his novels and their respective ﬁlm adaptations ... Net Force Explorers is a series of young adult novels created by Tom Clancy and Steve Pieczenik as a spin-off of the military ﬁction series ... Question: What ﬁction character created by Tom Clancy was turned into a ﬁlm in 2002? Answer: Jack Ryan Original Entity Graph Introduction Question answering (QA) has been a popular topic in natural language processing. QA provides a quantifiable way to evaluate an NLP system’s capability on language understanding and reasoning (Hermann et al., 2015; Rajpurkar et al., 2016, 2018). Most previous work focus on finding evidence and answers from a single paragraph (Seo et al., 2016; Liu et al., 2017; Wang et al., 2017). It rarely tests deep reasoning capabilities of the underlying model. In fact, Min et al. (2018) observe that most questions in existing QA benchmarks can be answered by retrieving † These authors contributed equally. The order of authorship is decided through dice rolling. Work done while Lin Qiu was a research intern in ByteDance AI Lab. First Mask Applied Second Mask Applied Figure 1: Example of multi-hop text-based QA. One question and three docu"
P19-1617,W03-0419,0,0.159076,"Missing"
P19-1617,P18-1150,0,0.176394,"on any additional predefined knowledge base. Therefore the models are expected to generalize well and to answer questions in open domains. There are two main challenges to answer questions of this kind. Firstly, since not every document contain relevant information, multi-hop textbased QA requires filtering out noises from multiple paragraphs and extracting useful information. To address this, recent studies propose to build entity graphs from input paragraphs and apply graph neural networks (GNNs) to aggregate the information through entity graphs (Dhingra et al., 2018; De Cao et al., 2018; Song et al., 2018a). However, all of the existing work apply GNNs based on a static global entity graph of each QA pair, which can be considered as performing implicit reasoning. Instead of them, we argue that the queryguided multi-hop reasoning should be explicitly performed on a dynamic local entity graph tailored according to the query. a fusion process in DFGN to solve the unrestricted QA challenge. We not only aggregate information from documents to the entity graph (doc2graph), but also propagate the information of the entity graph back to document representations (graph2doc). The fusion process is itera"
P19-1617,N18-1059,0,0.176897,"ic mask to select a subgraph, propagating information along the graph, and finally transfer the information from the graph back to the text in order to localize the answer. Nodes are entity occurrences, with the color denoting the underlying entity. Edges are constructed from co-occurrences. The gray circles are selected by DFGN in each step. a small set of sentences without reasoning. To address this issue, there are several recently proposed QA datasets particularly designed to evaluate a system’s multi-hop reasoning capabilities, including WikiHop (Welbl et al., 2018), ComplexWebQuestions (Talmor and Berant, 2018), and HotpotQA (Yang et al., 2018). In this paper, we study the problem of multi-hop text-based QA, which requires multi-hop reasoning among evidence scattered around multiple raw documents. In particular, a query utterance and a set of accompanying documents are given, but not 6140 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 6140–6150 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics all of them are relevant. The answer can only be obtained by selecting two or more evidence from the documents and infe"
P19-1617,P17-1018,0,0.0465391,"t novels created by Tom Clancy and Steve Pieczenik as a spin-off of the military ﬁction series ... Question: What ﬁction character created by Tom Clancy was turned into a ﬁlm in 2002? Answer: Jack Ryan Original Entity Graph Introduction Question answering (QA) has been a popular topic in natural language processing. QA provides a quantifiable way to evaluate an NLP system’s capability on language understanding and reasoning (Hermann et al., 2015; Rajpurkar et al., 2016, 2018). Most previous work focus on finding evidence and answers from a single paragraph (Seo et al., 2016; Liu et al., 2017; Wang et al., 2017). It rarely tests deep reasoning capabilities of the underlying model. In fact, Min et al. (2018) observe that most questions in existing QA benchmarks can be answered by retrieving † These authors contributed equally. The order of authorship is decided through dice rolling. Work done while Lin Qiu was a research intern in ByteDance AI Lab. First Mask Applied Second Mask Applied Figure 1: Example of multi-hop text-based QA. One question and three document paragraphs are given. Our proposed DFGN conducts multi-step reasoning over the facts by constructing an entity graph from multiple paragraph"
P19-1617,Q18-1021,0,0.0618714,"om multiple paragraphs, predicting a dynamic mask to select a subgraph, propagating information along the graph, and finally transfer the information from the graph back to the text in order to localize the answer. Nodes are entity occurrences, with the color denoting the underlying entity. Edges are constructed from co-occurrences. The gray circles are selected by DFGN in each step. a small set of sentences without reasoning. To address this issue, there are several recently proposed QA datasets particularly designed to evaluate a system’s multi-hop reasoning capabilities, including WikiHop (Welbl et al., 2018), ComplexWebQuestions (Talmor and Berant, 2018), and HotpotQA (Yang et al., 2018). In this paper, we study the problem of multi-hop text-based QA, which requires multi-hop reasoning among evidence scattered around multiple raw documents. In particular, a query utterance and a set of accompanying documents are given, but not 6140 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 6140–6150 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics all of them are relevant. The answer can only be obtained by selecting t"
P19-1617,D18-1259,0,0.703628,"g information along the graph, and finally transfer the information from the graph back to the text in order to localize the answer. Nodes are entity occurrences, with the color denoting the underlying entity. Edges are constructed from co-occurrences. The gray circles are selected by DFGN in each step. a small set of sentences without reasoning. To address this issue, there are several recently proposed QA datasets particularly designed to evaluate a system’s multi-hop reasoning capabilities, including WikiHop (Welbl et al., 2018), ComplexWebQuestions (Talmor and Berant, 2018), and HotpotQA (Yang et al., 2018). In this paper, we study the problem of multi-hop text-based QA, which requires multi-hop reasoning among evidence scattered around multiple raw documents. In particular, a query utterance and a set of accompanying documents are given, but not 6140 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 6140–6150 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics all of them are relevant. The answer can only be obtained by selecting two or more evidence from the documents and inferring among them (see Figure 1 for"
P19-1617,C18-1171,0,0.0194782,"gate how to effectively reason about the constructed graphs, which is the main problem studied in this work. Another group of sequential models deals with multi-hop reasoning following Memory Networks (Sukhbaatar et al., 2015). Such models construct representations for queries and memory cells for contexts, then make interactions between them in a multi-hop manner. Munkhdalai and Yu (2017) 6142 and Onishi et al. (2016) incorporate a hypothesis testing loop to update the query representation at each reasoning step and select the best answer among the candidate entities at the last step. IRNet (Zhou et al., 2018) generates a subject state and a relation state at each step, computing the similarity score between all the entities and relations given by the dataset KB. The ones with the highest score at each time step are linked together to form an interpretable reasoning chain. However, these models perform reasoning on simple synthetic datasets with a limited number of entities and relations, which are quite different with largescale QA dataset with complex questions. Also, the supervision of entity-level reasoning chains in synthetic datasets can be easily given following some patterns while they are"
P19-2032,W05-0909,0,0.0514888,"ral and diverse. Table 3: Automatic evaluation results of different methods. PPL denotes perplexity and B-2 denotes BLEU-2. Best results are shown in bold. Method PPL B-2 Seq2Seq + Mem + CoAtt + External 32.47 30.73 (-1.74) 27.12 (-3.61) 27.94 (+0.82) 0.071 0.099 (+0.028) 0.147 (+0.078) 0.162 (+0.015) Table 4: Incremental experiment results of proposed model. Performance on METEOR is similar to B-2. Mem denotes gated memory, CoAtt denotes blog-user co-attention and External denotes external personality expression 4.3 Evaluation Result Metrics: We use BLEU-2 (Papineni et al., 2002) and METEOR (Banerjee and Lavie, 2005) to evaluate overlap between outputs and references. Besides, perplexity is also provided. Results: The results are shown in Table 3. As can be seen, PCGN model with common words obtains the best performance on perplexity, BLEU-2 and METEOR. Note that the performance of Seq2Seq is extremely low, since the user profile is not taken into consideration during the generation, resulting repetitive responses. In contrast, with the help of three proposed mechanism (gated memory, blog-user co-attention and external personality expression), our model can utilize user information effectively, thus is ca"
P19-2032,P17-1059,0,0.0300964,"ch carries key information of the input post (Bahdanau et al., 2014). Finally, the decoder samples a word yt from the output probability distribution as follows yt ∼ softmax(Wo st )  (4) User Feature Embedding with Gated Memory To encode the information in user profile, we map user’s numeric feature F to a dense vector vu through a fully-connected layer. Intuitively, vu can be treated as a user feature embedding denotes the character of the user. However, if the user feature embedding is static during decoding, the grammatical correctness of sentences generated may be sacrificed as argued in Ghosh et al. (2017). To tackle this problem, we design an gated memory module to dynamically express personality during decoding, inspired by Zhou et al. (2018). Specifically, we maintain a internal personality state during the generation process. At each time step, the personality state decays by a certain amount. Once the decoding process is completed, the personality state is supposed to decay to zero, which indicates that the personality is completely expressed. Formally, at each time step t, the model computes an update gate g u t according to the current state of the decoder st . The initial personality st"
P19-2032,P16-1094,0,0.0881082,"Missing"
W13-3101,W13-3102,1,0.550633,"Missing"
W13-3101,W04-1013,0,0.027875,"Missing"
W13-3105,P06-2020,0,\N,Missing
W13-3105,P10-1084,0,\N,Missing
W16-1518,W13-3106,0,0.032195,"oting method is also used to integrate different candidate results. And for Task 2, we firstly adopt hLDA (hierarchical Latent Dirichlet Allocation) topic model for document content modeling. The hLDA tree can provide us good knowledge about latent sentence clustering (subtopic in the document) and word distributions (abstractiveness of words and sentences) for summarization. Then we score the sentences in the RP according to several features including hLDA ones and extract candidate sentences to generate the final summary. 2 Related Work There are many researches about document summarization [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20]. LDA has been widely applied [21,22]. Some improvements have been made [23,24,25]. One is to relax its assumption that topic number is known and fixed. [26] provided an elegant solution. [27] extended it to exploit the hierarchical tree structure of topics, hLDA, which is unsupervised method in which topic number could grow with the data set automatically. This could achieve a deeper semantic model similar with human mind and is especially helpful for summarization. [28] provided a multi-document summarization based on supervised hLDA with competitive results. 156 BIRNDL 2016 Joint Workshop o"
W16-1518,W13-3107,0,0.06345,"oting method is also used to integrate different candidate results. And for Task 2, we firstly adopt hLDA (hierarchical Latent Dirichlet Allocation) topic model for document content modeling. The hLDA tree can provide us good knowledge about latent sentence clustering (subtopic in the document) and word distributions (abstractiveness of words and sentences) for summarization. Then we score the sentences in the RP according to several features including hLDA ones and extract candidate sentences to generate the final summary. 2 Related Work There are many researches about document summarization [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20]. LDA has been widely applied [21,22]. Some improvements have been made [23,24,25]. One is to relax its assumption that topic number is known and fixed. [26] provided an elegant solution. [27] extended it to exploit the hierarchical tree structure of topics, hLDA, which is unsupervised method in which topic number could grow with the data set automatically. This could achieve a deeper semantic model similar with human mind and is especially helpful for summarization. [28] provided a multi-document summarization based on supervised hLDA with competitive results. 156 BIRNDL 2016 Joint Workshop o"
W16-1518,W13-3108,0,0.0605012,"oting method is also used to integrate different candidate results. And for Task 2, we firstly adopt hLDA (hierarchical Latent Dirichlet Allocation) topic model for document content modeling. The hLDA tree can provide us good knowledge about latent sentence clustering (subtopic in the document) and word distributions (abstractiveness of words and sentences) for summarization. Then we score the sentences in the RP according to several features including hLDA ones and extract candidate sentences to generate the final summary. 2 Related Work There are many researches about document summarization [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20]. LDA has been widely applied [21,22]. Some improvements have been made [23,24,25]. One is to relax its assumption that topic number is known and fixed. [26] provided an elegant solution. [27] extended it to exploit the hierarchical tree structure of topics, hLDA, which is unsupervised method in which topic number could grow with the data set automatically. This could achieve a deeper semantic model similar with human mind and is especially helpful for summarization. [28] provided a multi-document summarization based on supervised hLDA with competitive results. 156 BIRNDL 2016 Joint Workshop o"
W16-1518,W13-3110,0,0.0589283,"oting method is also used to integrate different candidate results. And for Task 2, we firstly adopt hLDA (hierarchical Latent Dirichlet Allocation) topic model for document content modeling. The hLDA tree can provide us good knowledge about latent sentence clustering (subtopic in the document) and word distributions (abstractiveness of words and sentences) for summarization. Then we score the sentences in the RP according to several features including hLDA ones and extract candidate sentences to generate the final summary. 2 Related Work There are many researches about document summarization [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20]. LDA has been widely applied [21,22]. Some improvements have been made [23,24,25]. One is to relax its assumption that topic number is known and fixed. [26] provided an elegant solution. [27] extended it to exploit the hierarchical tree structure of topics, hLDA, which is unsupervised method in which topic number could grow with the data set automatically. This could achieve a deeper semantic model similar with human mind and is especially helpful for summarization. [28] provided a multi-document summarization based on supervised hLDA with competitive results. 156 BIRNDL 2016 Joint Workshop o"
W16-1518,W13-3109,0,0.0610596,"oting method is also used to integrate different candidate results. And for Task 2, we firstly adopt hLDA (hierarchical Latent Dirichlet Allocation) topic model for document content modeling. The hLDA tree can provide us good knowledge about latent sentence clustering (subtopic in the document) and word distributions (abstractiveness of words and sentences) for summarization. Then we score the sentences in the RP according to several features including hLDA ones and extract candidate sentences to generate the final summary. 2 Related Work There are many researches about document summarization [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20]. LDA has been widely applied [21,22]. Some improvements have been made [23,24,25]. One is to relax its assumption that topic number is known and fixed. [26] provided an elegant solution. [27] extended it to exploit the hierarchical tree structure of topics, hLDA, which is unsupervised method in which topic number could grow with the data set automatically. This could achieve a deeper semantic model similar with human mind and is especially helpful for summarization. [28] provided a multi-document summarization based on supervised hLDA with competitive results. 156 BIRNDL 2016 Joint Workshop o"
W16-1518,P10-1084,0,0.0849032,"mary. 2 Related Work There are many researches about document summarization [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20]. LDA has been widely applied [21,22]. Some improvements have been made [23,24,25]. One is to relax its assumption that topic number is known and fixed. [26] provided an elegant solution. [27] extended it to exploit the hierarchical tree structure of topics, hLDA, which is unsupervised method in which topic number could grow with the data set automatically. This could achieve a deeper semantic model similar with human mind and is especially helpful for summarization. [28] provided a multi-document summarization based on supervised hLDA with competitive results. 156 BIRNDL 2016 Joint Workshop on Bibliometric-enhanced Information Retrieval and NLP for Digital Libraries However, it has the disadvantage of relying on ideal summaries. [29] provided a contrastive theme summarization based on hLDA and SDPPs, which is sensitive to negative correlation. In recent years, interest about information extraction and retrieval from scientific literature has increased considerably. Some researches [30] have shown that citations may contain information out of the abstracts pro"
W16-1518,W00-0405,0,\N,Missing
W17-1005,W16-1511,0,0.160052,"number of user participants and huge amount of interactive contents. How can we understand the mass of comments effectively? A crucial initial step towards this goal should be content linking, which is to determine what comments link to, be that either specific news snippets or comments by other users. Furthermore, a set of labels for a given link may be articulated to capture phenomena such as agreement and sentiment with respect to the comment target. Content linking is a relatively new research topic and it has attracted the focus of TAC 2014 (https://tac.nist.gov//2014/KBP/), BIRNDL 2016 (Jaidka et al., 2016) and MultiLing 2015 (Kabadjov et al., 2015) and MultiLing 2017. 3 Methods For content linking, we adopt the Word Embedding Model to dig up word vectors as linking information of sentence pair with deeper semantic fea32 Proceedings of the MultiLing 2017 Workshop on Summarization and Summary Evaluation Across Source Types and Genres, pages 32–36, c Valencia, Spain, April 3, 2017. 2017 Association for Computational Linguistics    form a calculating matrix Mi,j :        &quot; # &quot; $"
W17-1005,W16-1514,0,0.0186749,"nhanced Multiple Features for Content Linking and Argument/Sentiment Labeling in Online Forums Lei Li and Liyuan Mao and Moye Chen Center for Intelligence Science and Technology (CIST) School of Computer Science Beijing University of Posts and Telecommunications (BUPT) , China leili@bupt.edu.cn circleyuan@bupt.edu.cn moyec@bupt.edu.cn Abstract The main method is based on the calculation of sentence similarity (Aggarwal and Sharma, 2016; Cao et al., 2016; Jaidka et al., 2016; Saggion et al., 2016; Nomoto, 2016; Moraes et al., 2016; Malenfant and Lapalme, 2016; Lu et al., 2016; Li et al., 2016; Klampfl et al., 2016), with the key point of mining semantic information better. Researchers have tried various features and methods for sentiment and argument labeling. The main features are different kinds of sentiment dictionaries, while the basic method is the rule-based one. The major method for sentiment and argument labeling is based on statistical machine learning algorithms (Aker et al., 2015; Hristo Tanev, 2015; Maynard and Funk, 2011). Multiple grammatical and semantic features are adopted in content linking and argument/sentiment labeling for online forums in this paper. There are mainly two different"
W17-1005,W16-1518,1,0.879929,"Missing"
W17-1005,W16-1516,0,0.0239069,"ord Embedding and Topic Modeling Enhanced Multiple Features for Content Linking and Argument/Sentiment Labeling in Online Forums Lei Li and Liyuan Mao and Moye Chen Center for Intelligence Science and Technology (CIST) School of Computer Science Beijing University of Posts and Telecommunications (BUPT) , China leili@bupt.edu.cn circleyuan@bupt.edu.cn moyec@bupt.edu.cn Abstract The main method is based on the calculation of sentence similarity (Aggarwal and Sharma, 2016; Cao et al., 2016; Jaidka et al., 2016; Saggion et al., 2016; Nomoto, 2016; Moraes et al., 2016; Malenfant and Lapalme, 2016; Lu et al., 2016; Li et al., 2016; Klampfl et al., 2016), with the key point of mining semantic information better. Researchers have tried various features and methods for sentiment and argument labeling. The main features are different kinds of sentiment dictionaries, while the basic method is the rule-based one. The major method for sentiment and argument labeling is based on statistical machine learning algorithms (Aker et al., 2015; Hristo Tanev, 2015; Maynard and Funk, 2011). Multiple grammatical and semantic features are adopted in content linking and argument/sentiment labeling for online forums in thi"
W17-1005,W16-1517,0,0.035869,"Missing"
W17-1005,W16-1512,0,0.0318581,"Missing"
W17-1005,W16-1513,0,0.0464809,"Missing"
W17-1005,W16-1519,0,0.063676,"Missing"
W17-1005,W16-1520,0,0.0845268,"Missing"
W17-1005,W16-1515,0,\N,Missing
W17-2333,W07-1014,0,0.0601414,"Missing"
W17-2333,W07-1017,0,0.0263929,"Missing"
W17-2333,W07-1027,0,0.212643,"manual coders (Crammer et al., 2007; Farkas and Szarvas, 2008; Aronson et al., 2007; Kavuluru et al., 2015, 2013; Zuccon and Nguyen, Figure 1: An example radiology report with manually labeled ICD-9-CM code from CMC dataset. 2013; Koopman et al., 2015). In this paper, we focus on ICD-9-CM (the 9th version ICD, Clinical Modification), although our work is portable to ICD-10-CM (the 10th version ICD). The reason to conduct our study on ICD9-CM is to compare with the state-of-art methods, whose evaluations have mostly conducted on ICD9-CM code (Aronson et al., 2007; Kavuluru et al., 2015, 2013; Patrick et al., 2007; Ira et al., 2007; Zhang, 2008). ICD-9-CM codes are organized hierarchically, and each code corresponds to a textual description, such as ”786.2, cough”. Multiple codes can be assigned to a medical text, and a specific ICD-9-CM code is preferred than a more generic one when both are suitable (Pestian et al., 2007). Figure 1 shows a code assignment example where a radiology report is labeled with ”786.2, cough”. Existing methods for automatic ICD-9-CM assignment have been mostly supervised methods because of the effectiveness the training; however, classification performance heavily relies on"
W17-2333,W07-1013,0,0.76315,"inical Modification), although our work is portable to ICD-10-CM (the 10th version ICD). The reason to conduct our study on ICD9-CM is to compare with the state-of-art methods, whose evaluations have mostly conducted on ICD9-CM code (Aronson et al., 2007; Kavuluru et al., 2015, 2013; Patrick et al., 2007; Ira et al., 2007; Zhang, 2008). ICD-9-CM codes are organized hierarchically, and each code corresponds to a textual description, such as ”786.2, cough”. Multiple codes can be assigned to a medical text, and a specific ICD-9-CM code is preferred than a more generic one when both are suitable (Pestian et al., 2007). Figure 1 shows a code assignment example where a radiology report is labeled with ”786.2, cough”. Existing methods for automatic ICD-9-CM assignment have been mostly supervised methods because of the effectiveness the training; however, classification performance heavily relies on the sufficiency of training data (He and Garcia, 2009). To certain degree, micro-average measures, commonly used to evaluate the classification performance of existing algorithms, pays atFigure 2: The distribution of radiology reports for 45 ICD-9-CM codes in the CMC dataset. tention to the correctness of the code"
W17-2333,P08-3012,0,0.194583,"rkas and Szarvas, 2008; Aronson et al., 2007; Kavuluru et al., 2015, 2013; Zuccon and Nguyen, Figure 1: An example radiology report with manually labeled ICD-9-CM code from CMC dataset. 2013; Koopman et al., 2015). In this paper, we focus on ICD-9-CM (the 9th version ICD, Clinical Modification), although our work is portable to ICD-10-CM (the 10th version ICD). The reason to conduct our study on ICD9-CM is to compare with the state-of-art methods, whose evaluations have mostly conducted on ICD9-CM code (Aronson et al., 2007; Kavuluru et al., 2015, 2013; Patrick et al., 2007; Ira et al., 2007; Zhang, 2008). ICD-9-CM codes are organized hierarchically, and each code corresponds to a textual description, such as ”786.2, cough”. Multiple codes can be assigned to a medical text, and a specific ICD-9-CM code is preferred than a more generic one when both are suitable (Pestian et al., 2007). Figure 1 shows a code assignment example where a radiology report is labeled with ”786.2, cough”. Existing methods for automatic ICD-9-CM assignment have been mostly supervised methods because of the effectiveness the training; however, classification performance heavily relies on the sufficiency of training data"
W19-8604,N19-1320,0,0.0122192,"ings of The 12th International Conference on Natural Language Generation, pages 24–33, c Tokyo, Japan, 28 Oct - 1 Nov, 2019. 2019 Association for Computational Linguistics tence unchanged (Fu et al., 2018; Ficler and Goldberg, 2017; Hu et al., 2017). Because of the lack of parallel datasets, most models focus on the unpaired transfer. Although plenty of sophisticated techniques are used in this task, such as adversarial learning (Zhao et al., 2018; Chen et al., 2018), latent representations (Li and Mandt, 2018; Dai et al., 2019; Liu et al., 2019), and reinforcement learning (Luo et al., 2019; Gong et al., 2019; Xu et al., 2018), there is little discussion about what is changed and what remains unchanged. Because of the lack of transparency and interpretability, there is some retrospection on this topic. Such as the definition of text style (Tikhonov and Yamshchikov, 2018), and the evaluation metrics (Li et al., 2018; Mir et al., 2019). Our proposed pivot analysis aligns with these works and provides a new tool to probe the transfer datasets and models. The de facto metrics is to use a pretrained classifier to classify if the transferred sentence is in the target class. So our pivot analysis starts"
W19-8604,D14-1181,0,0.00903277,"Missing"
W19-8604,N18-1169,0,0.137121,"ntence structure unchanged. (SOTA) models have achieved inspiring transfer success rates (Zhao et al., 2018; Zhang et al., 2018; Prabhumoye et al., 2018; Yang et al., 2018). However, it is still unclear in current literature about what is transferred and what remains to be unchanged during the transfer process. To answer this question, we perform an in-depth investigation of the linguistic attribute transfer datasets and models. Our investigation starts from a simple observation: in many transfer datasets and models, certain class-related words play very important roles in attribute transfer (Li et al., 2018; Prabhumoye et al., 2018). Figure 1 gives a sentiment transfer example from the controllable generation (CG) model (Hu et al., 2017) on the Yelp dataset. In this example, rude is strongly related to the negative sentiment and good is strongly related to the positive sentiment, thus simply substituting rude with good will transfer the sentence from negative to positive. In this work, We name these words the pivot words for a class. We use the term the pivot effect to refer the effect that certain strong words may be able to determine the class of a sentence. Based on the observation of the piv"
W19-8604,D18-1420,0,0.0139908,"ext style (Tikhonov and Yamshchikov, 2018), and the evaluation metrics (Li et al., 2018; Mir et al., 2019). Our proposed pivot analysis aligns with these works and provides a new tool to probe the transfer datasets and models. The de facto metrics is to use a pretrained classifier to classify if the transferred sentence is in the target class. So our pivot analysis starts from the classification task and mines the words with strong predictive performance. While many previous works focus on one-toone transfer, many recent works extend this task to one-to-many transfer (Logeswaran et al., 2018; Liao et al., 2018; Subramanian et al., 2019). For simplicity, we focus on the one-to-one setting. But it is also easy to extend the pivot analysis into oneto-many transfer settings. form the attribute transfer or it may change higherlevel sentence composationality like syntax? To answer question (1), we propose the pivot analysis, a series of simple yet effective text mining algorithms, to quantitatively examine the pivot effects in different datasets. The basics of the datasets we investigate are listed in Table 1. We first give the algorithm to extract pivot words (Sec 3). We statistically show the stronger"
W19-8604,P19-1601,0,0.0575752,"ttributes, and use the term style or attribute according to the context. 24 Proceedings of The 12th International Conference on Natural Language Generation, pages 24–33, c Tokyo, Japan, 28 Oct - 1 Nov, 2019. 2019 Association for Computational Linguistics tence unchanged (Fu et al., 2018; Ficler and Goldberg, 2017; Hu et al., 2017). Because of the lack of parallel datasets, most models focus on the unpaired transfer. Although plenty of sophisticated techniques are used in this task, such as adversarial learning (Zhao et al., 2018; Chen et al., 2018), latent representations (Li and Mandt, 2018; Dai et al., 2019; Liu et al., 2019), and reinforcement learning (Luo et al., 2019; Gong et al., 2019; Xu et al., 2018), there is little discussion about what is changed and what remains unchanged. Because of the lack of transparency and interpretability, there is some retrospection on this topic. Such as the definition of text style (Tikhonov and Yamshchikov, 2018), and the evaluation metrics (Li et al., 2018; Mir et al., 2019). Our proposed pivot analysis aligns with these works and provides a new tool to probe the transfer datasets and models. The de facto metrics is to use a pretrained classifier to classi"
W19-8604,N19-1049,0,0.0670766,"sfer. Although plenty of sophisticated techniques are used in this task, such as adversarial learning (Zhao et al., 2018; Chen et al., 2018), latent representations (Li and Mandt, 2018; Dai et al., 2019; Liu et al., 2019), and reinforcement learning (Luo et al., 2019; Gong et al., 2019; Xu et al., 2018), there is little discussion about what is changed and what remains unchanged. Because of the lack of transparency and interpretability, there is some retrospection on this topic. Such as the definition of text style (Tikhonov and Yamshchikov, 2018), and the evaluation metrics (Li et al., 2018; Mir et al., 2019). Our proposed pivot analysis aligns with these works and provides a new tool to probe the transfer datasets and models. The de facto metrics is to use a pretrained classifier to classify if the transferred sentence is in the target class. So our pivot analysis starts from the classification task and mines the words with strong predictive performance. While many previous works focus on one-toone transfer, many recent works extend this task to one-to-many transfer (Logeswaran et al., 2018; Liao et al., 2018; Subramanian et al., 2019). For simplicity, we focus on the one-to-one setting. But it i"
W19-8604,W17-4912,0,0.0311698,"s for a class. We use the term the pivot effect to refer the effect that certain strong words may be able to determine the class of a sentence. Based on the observation of the pivot effect, our research questions are: (1) which words are pivot words and how do they influence the attribute class of a sentence in different datasets? (2) does the model only need to modify the pivot words to perIntroduction The task of text attribute transfer (or text style transfer 2 ) is to transform certain linguistic attributes (sentiment, style, authorship, rhetorical devices, etc.) from one type to another (Ficler and Goldberg, 2017; Fu et al., 2018; Hu et al., 2017; Li et al., 2018; Shen et al., 2017). The state-of-the-art ∗ Work done when Yao was an intern at Bytedance AI Lab. 1 Our code can be found at https://github.com/FranxYao/pivot analysis 2 Many existing works also call this task style transfer(Fu et al., 2018), our work view style as one of the linguistic attributes, and use the term style or attribute according to the context. 24 Proceedings of The 12th International Conference on Natural Language Generation, pages 24–33, c Tokyo, Japan, 28 Oct - 1 Nov, 2019. 2019 Association for Computational Linguistics tenc"
W19-8604,P18-1080,0,0.0999749,"Missing"
W19-8604,P18-2031,0,0.0975152,"Missing"
W19-8604,P18-1108,0,0.0384371,"Missing"
W19-8604,D18-1488,0,0.0670857,"Missing"
W19-8604,N18-1138,0,0.0638871,"rform the lexical-level modification, while leaving higher-level sentence structures unchanged. Our work provides an in-depth understanding of linguistic attribute transfer and further identifies the future requirements and challenges of this task1 . 1 Figure 1: Examples of pivot words in sentiment transfer. Certain words are strongly correlated with the sentiment such that a transfer model only need to modify these words to accomplish the transfer task while leaving the higher level sentence structure unchanged. (SOTA) models have achieved inspiring transfer success rates (Zhao et al., 2018; Zhang et al., 2018; Prabhumoye et al., 2018; Yang et al., 2018). However, it is still unclear in current literature about what is transferred and what remains to be unchanged during the transfer process. To answer this question, we perform an in-depth investigation of the linguistic attribute transfer datasets and models. Our investigation starts from a simple observation: in many transfer datasets and models, certain class-related words play very important roles in attribute transfer (Li et al., 2018; Prabhumoye et al., 2018). Figure 1 gives a sentiment transfer example from the controllable generation (CG) mo"
W19-8604,P18-1090,0,\N,Missing
W19-8904,N15-1014,0,0.0284262,"s its best shot when put into practical use. 17 Proceedings of the Multiling 2019 Workshop, co-located with the RANLP 2019 conference, pages 17–25 Varna, Bulgaria, September 6, 2019. http://doi.org/10.26615/978-954-452-058-8_004 been raised including GPT-2(Radford et al., 2019), MASS(Song et al., 2019) and XLNet(Yang et al., 2019). Thus, in this paper, we would like to adopt extractive summarization due to its increased feasibility. 2.2 Headline Generation As a special application scenario of abstractive summarization, headline generation gains a lot of attention in recent years. In the HEADS(Colmenares et al., 2015) system researchers formulate the headline generation as a discrete optimization task in a feature-rich space. (Sun et al., 2015) combined extractive and abstractive summarization to detect a key event chain in article and generate titles based on it. (Takase et al., 2016) tried to incorporate structural syntactic and semantic information into a baseline neural attention-based model. There are also works focusing on extending sentence compression to document headline generation (Tan et al., 2017). Most of these works use seq2seq model, which is not suitable for Wikipedia Headline Generation ta"
W19-8904,P18-1031,0,0.016588,"ing(NLP) which focus on how to make use of language information in large corpus with unsupervised learning. Word2vec(Mikolov et al., 2013) and Glove(Pennington et al., 2014) have successfully learned semantic information in word embeddings and have been widely used in NLP tasks as inputs for model. Pre-trained language models explore more by learning syntactic and more abstractive features. These language models enrich embeddings information by adding encoders in pre-trained parts, producing context-aware representations when transfer to downstream tasks. Representative works including ULMFiT(Howard and Ruder, 2018), which captures general features of the language in different encoder layers to help text classification; ELMo(Peters et al., 2018), which learns embeddings from Bidirectional LSTM language models; BERT(Devlin et al., 2018), a successful application of training Transformer encoders on large masked corpus and reach eleven state-ofthe-art results. After the release of BERT, many super-large-scale Transformer-Based models have Figure 1 shows the pipeline during training. Given a Wikipedia article, The extractive summarization should extract summaries from paragraphs. The summarization model is u"
W19-8904,D14-1162,0,0.0822577,"e apply Pre-trained model to utilize the semantic knowledge learned in large unsupervised corpus on low-resource supervised task. 2.3 3 Pipeline Overview Figure 1: Pipeline overview during training. Red lines refer to samples for training a extractive summarization model and blue for training a title labelling model. Pre-trained Language Model Pre-trained Language Model(LM) is one of the most important research advances in Natural Language Processing(NLP) which focus on how to make use of language information in large corpus with unsupervised learning. Word2vec(Mikolov et al., 2013) and Glove(Pennington et al., 2014) have successfully learned semantic information in word embeddings and have been widely used in NLP tasks as inputs for model. Pre-trained language models explore more by learning syntactic and more abstractive features. These language models enrich embeddings information by adding encoders in pre-trained parts, producing context-aware representations when transfer to downstream tasks. Representative works including ULMFiT(Howard and Ruder, 2018), which captures general features of the language in different encoder layers to help text classification; ELMo(Peters et al., 2018), which learns emb"
W19-8904,N18-1202,0,0.00936905,"13) and Glove(Pennington et al., 2014) have successfully learned semantic information in word embeddings and have been widely used in NLP tasks as inputs for model. Pre-trained language models explore more by learning syntactic and more abstractive features. These language models enrich embeddings information by adding encoders in pre-trained parts, producing context-aware representations when transfer to downstream tasks. Representative works including ULMFiT(Howard and Ruder, 2018), which captures general features of the language in different encoder layers to help text classification; ELMo(Peters et al., 2018), which learns embeddings from Bidirectional LSTM language models; BERT(Devlin et al., 2018), a successful application of training Transformer encoders on large masked corpus and reach eleven state-ofthe-art results. After the release of BERT, many super-large-scale Transformer-Based models have Figure 1 shows the pipeline during training. Given a Wikipedia article, The extractive summarization should extract summaries from paragraphs. The summarization model is unsupervised so actually there is no explicit training sample for summarization but we design features based on some statistics from"
W19-8904,P15-1045,0,0.0224514,"ages 17–25 Varna, Bulgaria, September 6, 2019. http://doi.org/10.26615/978-954-452-058-8_004 been raised including GPT-2(Radford et al., 2019), MASS(Song et al., 2019) and XLNet(Yang et al., 2019). Thus, in this paper, we would like to adopt extractive summarization due to its increased feasibility. 2.2 Headline Generation As a special application scenario of abstractive summarization, headline generation gains a lot of attention in recent years. In the HEADS(Colmenares et al., 2015) system researchers formulate the headline generation as a discrete optimization task in a feature-rich space. (Sun et al., 2015) combined extractive and abstractive summarization to detect a key event chain in article and generate titles based on it. (Takase et al., 2016) tried to incorporate structural syntactic and semantic information into a baseline neural attention-based model. There are also works focusing on extending sentence compression to document headline generation (Tan et al., 2017). Most of these works use seq2seq model, which is not suitable for Wikipedia Headline Generation task in MultiLing 2019 due to low resource multilingual training corpus. So we apply Pre-trained model to utilize the semantic know"
