2005.eamt-1.6,2004.iwslt-evaluation.13,1,0.857242,"till has to be determined is if the response time of the system, increased by the overhead of regenerating the word graphs, remains acceptable for interactive use under real-life conditions. Off-line experiments seem to indicate that this is the case (see next section). 6 6.1 Experimental Results Experimental Setup The experiments were performed on the SpanishEnglish and English-German Xerox corpora, which consist of the translation of technical manuals. The corpora allocations are summarized in Table 1 and Table 2. After training and optimization of the model scaling factors, the SMT engine (Bender et al., 2004) was used to translate the test corpus. The results using the standard evaluation measures for machine translation (word error rate, positionindependent word error rate, BLEU score and NIST score) are shown in Table 3. Using the same parameter settings, a simulation of the interactive mode was carried out. This simulation mode is described in (Och et al., 2003). The system with the same parameter settings was also successfully used by human translators to evaluate it under real-life conditions. Due to the high effort that human evaluations require, only the wordgraph based generation strategy"
2005.eamt-1.6,C96-1067,0,0.580617,"ife applications. The produced sentences often contain grammatical errors and the preservation of the meaning is not even always achieved. Therefore a manual post-processing of the texts has to be done. The concept of interactive machine translation already has a long history, and the first systems appeared in the end of the 1960’s. However in most of these systems the user doesn’t have a direct control over the translation process, and most of the user interaction is reduced to performing source language disambiguation on demand. The approach we center on in this work was first suggested by (Foster et al., 1996) and an implementation was carried out in the TransType project (Langlais et al., 2000). In such an environment, human translators interact with a translation system that acts as an assistance tool and dynamically provides a list of translations that best complete the part of the source senEAMT 2005 Conference Proceedings tence already translated. Further refinements were presented in the TransType2 project (SchlumbergerSema S.A. et al., 2001). The work presented in this paper deals with generation strategies for interactive (statistical) machine translation systems. Clearly, the best approach"
2005.eamt-1.6,W00-0507,0,0.366059,"servation of the meaning is not even always achieved. Therefore a manual post-processing of the texts has to be done. The concept of interactive machine translation already has a long history, and the first systems appeared in the end of the 1960’s. However in most of these systems the user doesn’t have a direct control over the translation process, and most of the user interaction is reduced to performing source language disambiguation on demand. The approach we center on in this work was first suggested by (Foster et al., 1996) and an implementation was carried out in the TransType project (Langlais et al., 2000). In such an environment, human translators interact with a translation system that acts as an assistance tool and dynamically provides a list of translations that best complete the part of the source senEAMT 2005 Conference Proceedings tence already translated. Further refinements were presented in the TransType2 project (SchlumbergerSema S.A. et al., 2001). The work presented in this paper deals with generation strategies for interactive (statistical) machine translation systems. Clearly, the best approach would be to start a new search for every given prefix. However, in these kind of syste"
2005.eamt-1.6,E03-1032,1,0.906128,"rent way, as it itself can be a prefix of the next word. To ensure the extensions start with this word prefix, the comparison must be done at the character level. One might think about different costs for the mismatch of words within the prefix and for extensions which do not start with the given word prefix. If a word within the prefix can not be produced by the search algorithm, then it will obviously not be produced by any further search call. This kind of substitution error is less harmful for producing good hypotheses than unfitting extensions, and should therefore be penalized less. In (Och et al., 2003), an efficient algorithm for interactive generation using word graphs was presented. A word graph is a weighted directed acyclic graph, in which each node represents a partial translation hypothesis and each edge is labeled with a word of the target sentence and is weighted according to the language and translation model scores. (Ueffing et al., 2002) give a more detailed description of word graphs and show how they can be easily produced as a sub-product of the search process. An example of a word graph is shown in Figure 2. It is clear that each node in the word graph defines a prefix of a p"
2005.eamt-1.6,W02-1021,1,0.904238,"that have the same word prefix. In the actual implementation, the method is applied on the character level, and the search for an extension is performed after each keystroke of the human translator. The crucial factor is an efficient maximization of Eq. 3, because human translators will only accept response times of fractions of a second. Using state-of-the-art search algorithms this is not 34 achievable without putting up with an unacceptable amount of search errors. To overcome this problem, we can compute a word graph which represents a subset of possible extensions (Ney and Aubert, 1994; Ueffing et al., 2002). The generation is then constrained to this set of extensions. 3 Phrase-based Approach The base method we use in our translation system is the alignment template approach as described in (Och et al., 1999; Och and Ney, 2004). This approach uses the so-called alignment templates, which are pairs of source and target language phrases1 together with the word alignment within the phrases. The alignment templates are introduced as hidden variables z1K when modelling the conditional translation probability Pr(f1J |eI1 ): Pr(f1J |eI1 ) =  I K K I Pr(aK 1 |e1 ) · Pr(z1 |a1 , e1 )· z1K ,aK 1 I Pr(f1J"
2005.eamt-1.6,J04-4002,1,0.629855,"icient maximization of Eq. 3, because human translators will only accept response times of fractions of a second. Using state-of-the-art search algorithms this is not 34 achievable without putting up with an unacceptable amount of search errors. To overcome this problem, we can compute a word graph which represents a subset of possible extensions (Ney and Aubert, 1994; Ueffing et al., 2002). The generation is then constrained to this set of extensions. 3 Phrase-based Approach The base method we use in our translation system is the alignment template approach as described in (Och et al., 1999; Och and Ney, 2004). This approach uses the so-called alignment templates, which are pairs of source and target language phrases1 together with the word alignment within the phrases. The alignment templates are introduced as hidden variables z1K when modelling the conditional translation probability Pr(f1J |eI1 ): Pr(f1J |eI1 ) =  I K K I Pr(aK 1 |e1 ) · Pr(z1 |a1 , e1 )· z1K ,aK 1 I Pr(f1J |z1K , aK 1 , e1 ) . (4) In Equation (4), we introduce the additional hidden variables aK 1 that model the alignment of the alignment templates themselves. As smoothing, automatically trained word classes can be used, and ad"
2005.eamt-1.6,W99-0604,1,0.787605,"l factor is an efficient maximization of Eq. 3, because human translators will only accept response times of fractions of a second. Using state-of-the-art search algorithms this is not 34 achievable without putting up with an unacceptable amount of search errors. To overcome this problem, we can compute a word graph which represents a subset of possible extensions (Ney and Aubert, 1994; Ueffing et al., 2002). The generation is then constrained to this set of extensions. 3 Phrase-based Approach The base method we use in our translation system is the alignment template approach as described in (Och et al., 1999; Och and Ney, 2004). This approach uses the so-called alignment templates, which are pairs of source and target language phrases1 together with the word alignment within the phrases. The alignment templates are introduced as hidden variables z1K when modelling the conditional translation probability Pr(f1J |eI1 ): Pr(f1J |eI1 ) =  I K K I Pr(aK 1 |e1 ) · Pr(z1 |a1 , e1 )· z1K ,aK 1 I Pr(f1J |z1K , aK 1 , e1 ) . (4) In Equation (4), we introduce the additional hidden variables aK 1 that model the alignment of the alignment templates themselves. As smoothing, automatically trained word classes"
2005.mtsummit-papers.34,J93-2003,0,0.0128844,"uation results. 1 The structure of the paper is as follows: In Section 2 we will describe the statistical approach to machine translation and in Section 3 further methods used in our translation system. The EPPS databases and experimental results will be presented in Section 4. We will draw conclusions in the last Section. 2 Statistical Machine Translation In a machine translation framework we are given a sentence f1J = f1 . . . fJ in a source language that is to be translated as sentence eI1 = e1 . . . eI into a target language (f and e stand for ‘French’ and ‘English’ in the original paper (Brown et al., 1993)). For the statistical approach, we use Bayes decision rule which states that we should choose the sentence that maximizes the posterior probability eˆI1 = argmax p(eI1 |f1J ) (1) eI1 = argmax p(eI1 )p(f1J |eI1 ) , (2) eI1 Introduction Speech-to-speech translation is an outstanding reasearch goal in the machine translation community. Up to now, most of the projects dealing with this issue have dealt only with artiﬁcial or very limited tasks (Wahlster, 2000; EuTransProject, 2000; Lavie et al., 2001; Ueﬃng and Ney, 2005). The goal of the TC-Star project is to build a speech-to-speech translation"
2005.mtsummit-papers.34,2005.eamt-1.17,1,0.776963,"IBM1 Rescoring Although the IBM1 model is the easiest one of the single-word based translation models and the phrase based models clearly outperform this approach, the inclusion of the scores of this 261 model, i.e. hIBM1 (f1J |eI1 ) = I J   1 p(fj |ei ) (I + 1)J (6) j=1 i=0 has been shown experimentally to improve the performance of a machine translation system (Och et al., 2003). 3.5 LM Rescoring During the generation process, a single language model is used. However, additional language models speciﬁc to each sentence to be translated can help to improve the machine translation quality (Hasan and Ney, 2005). The motivation behind this lies in the following observation: the syntactic structure of a sentence is inﬂuenced by its type. It is obvious that an interrogative sentence has a diﬀerent structure from a declarative one due to non-local dependencies arising e.g. from wh-extraction. As an example, let us consider the syntax of the following sentences: “Is the commissioner ready to give an undertaking?” and “The commissioner is ready to give an undertaking.” If we look closer at the ﬁrst four words of each sentence (is, the, commissioner and ready), the trigrams observed are quite diﬀerent, lea"
2005.mtsummit-papers.34,H01-1007,0,0.0254038,". . . eI into a target language (f and e stand for ‘French’ and ‘English’ in the original paper (Brown et al., 1993)). For the statistical approach, we use Bayes decision rule which states that we should choose the sentence that maximizes the posterior probability eˆI1 = argmax p(eI1 |f1J ) (1) eI1 = argmax p(eI1 )p(f1J |eI1 ) , (2) eI1 Introduction Speech-to-speech translation is an outstanding reasearch goal in the machine translation community. Up to now, most of the projects dealing with this issue have dealt only with artiﬁcial or very limited tasks (Wahlster, 2000; EuTransProject, 2000; Lavie et al., 2001; Ueﬃng and Ney, 2005). The goal of the TC-Star project is to build a speech-to-speech translation system that can deal with real life data. For this purpose we have collected data from parliamentary speeches held in the European Parliament Plenary Sessions (EPPS) to build an open domain corpus. There are three different versions of the data, the oﬃcial version of the speeches as available on the web page of the European Parliament, the actual exact transcription of the speeches produced by human transcribers and the output of an automatic speech recognition system. We evaluate our system unde"
2005.mtsummit-papers.34,P02-1038,1,0.542956,"ord based models2 in both directions (source-totarget and target-to-source) and combining the two obtained alignments (Och and Ney, 2003). Given this alignment an extraction of contiguous phrases is carried out and their probabilities are computed by mean of relative frequencies (Zens and Ney, 2004). An example of an alignment between two sentences and a (possible) set of phrases to be extracted is shown in Figure 2. 3.2 Log-linear Model As an alternative to the traditional sourcechannel approach given in Equation (2) we can model the translation probability directly using a log-linear model (Och and Ney, 2002):   M I, fJ) exp λ h (e m m 1 1 m=1  , p(eI1 |f1J ) =  M I, fJ) exp λ h (˜ e m=1 m m 1 1 I e˜1 (5) 2 Usually the used models are IBM-1, HMM and IBM-4. with hm diﬀerent models, λm scaling factors and the denominator a normalization factor that can be ignored in the maximization process. We choose the λm by optimizing a performance measure over a development corpus using the downhill simplex algorithm as presented in (Press et al., 2002). The source-channel model (2) is a special case of (5) with appropriate feature functions. The log-linear model, however, has the advantage that addition"
2005.mtsummit-papers.34,J03-1002,1,0.010911,"context into the translation model is to learn translations for whole phrases instead of single words. Here, a phrase is simply a sequence of words, no other linguistic meaning is required. So, the basic idea of phrase-based translation is to segment the given source sentence into phrases, then translate each phrase and ﬁnally compose the target sentence from these phrase translations. First an alignment between source and target sentence is found by using a chain of single-word based models2 in both directions (source-totarget and target-to-source) and combining the two obtained alignments (Och and Ney, 2003). Given this alignment an extraction of contiguous phrases is carried out and their probabilities are computed by mean of relative frequencies (Zens and Ney, 2004). An example of an alignment between two sentences and a (possible) set of phrases to be extracted is shown in Figure 2. 3.2 Log-linear Model As an alternative to the traditional sourcechannel approach given in Equation (2) we can model the translation probability directly using a log-linear model (Och and Ney, 2002):   M I, fJ) exp λ h (e m m 1 1 m=1  , p(eI1 |f1J ) =  M I, fJ) exp λ h (˜ e m=1 m m 1 1 I e˜1 (5) 2 Usually the"
2005.mtsummit-papers.34,P02-1040,0,0.0715923,"ion. Sentence pairs Running Words Running Words without Punct. Marks Vocabulary Singletons Spanish English 1 207 740 34 851 423 33 335 048 31 360 260 30 049 355 139 587 93 995 48 631 33 891 Table 2: Statistics of the EPPS training corpus. order of an acceptable sentence can be different from that of the target sentence, so that the WER measure alone could be misleading. The PER compares the words in the two sentences ignoring the word order. • BLEU and NIST scores: These scores are a weighted n-gram precision in combination with a penalty for sentences which are too short, and were deﬁned in (Papineni et al., 2002) and (Doddington, 2002). Both measure accuracy, i.e. large scores are better. All of these metrics can be extended to the case where we have multiple references by calculating the value for each of the reference translations and choosing the best one among them. In our case we had two references per sentence. 4.4 Results The results for the FTE corpus are given in Table 4. The baseline results refer to the output of the translation system, as described in Section 3.1, without any of the further improvements discussed in Section 3. It can be seen that the log-linear combination of models signiﬁ"
2005.mtsummit-papers.34,W02-1021,1,0.821509,"rom being perfect. For eﬃciency reasons in most tasks, the whole search space can not be treated directly. So some pruning has to be carried out in the search process, which can lead to the rejection of valid translations (socalled search errors). The state-of-the-art algorithms used in current systems, however, allow to minimize these kinds of errors, so the main source of errors still lies in the probability models, i.e. sentences which are better translations do not get a better score (a higher probability). In order to alleviate this eﬀect, we can make use of word graphs and n-best lists (Ueﬃng et al., 2002). These are representations of diﬀerent possible translations for a given sentence. Once we have this representation we can use further models in order to compute an additional score for each of the possible candidates and then choose the one with the best score. Ideally these additional models would be integrated into the generation algorithm, but most of them are too costly to include in the search procedure or do not have a structure which allows this kind of coupling. How to eﬃciently compute n-best lists and word graphs for the phrase-based approach is presented in (Zens and Ney, 2005). 3"
2005.mtsummit-papers.34,C96-2141,1,0.629724,"the output of an automatic speech recognition system. We evaluate our system under these three conditions. 259 where the argmax operator denotes the search process. The transformation from (1) to (2) using Bayes rule allows us to use two sources of information, the translation model p(f1J |eI1 ) and the target language model p(eI1 ). The translation model can be further decomposed into a lexicon model, which gives the probability for word translations, and an alignment model, which connects the words in the source and target sentences. Let us consider the HMM Alignment model as presented in (Vogel et al., 1996) in order to illustrate this decomposition. This model decomposes the translation probability as follows: J  pϑ (f1J |eI1 ) = [pϑ (aj |aj−1 , I, J)pϑ (fj |eaj )] , j=1 aJ 1 (3) where the term pϑ (aj |aj−1 , I, J) is a ﬁrstorder model for the alignment, and the term Source Language Text Transformation f1J Pr(f1J |eI1 ) Global Search: maximize Pr(eI1 ) · Pr(f1J |eI1 ) over eI1 Lexicon model Alignment model Pr(eI1 ) Language model Transformation Target Language Text Figure 1: Architecture of the translation approach based on Bayes decision rule. pϑ (fj |eaj ) is the lexicon model1 , both depend"
2005.mtsummit-papers.34,N04-1033,1,0.844315,"guistic meaning is required. So, the basic idea of phrase-based translation is to segment the given source sentence into phrases, then translate each phrase and ﬁnally compose the target sentence from these phrase translations. First an alignment between source and target sentence is found by using a chain of single-word based models2 in both directions (source-totarget and target-to-source) and combining the two obtained alignments (Och and Ney, 2003). Given this alignment an extraction of contiguous phrases is carried out and their probabilities are computed by mean of relative frequencies (Zens and Ney, 2004). An example of an alignment between two sentences and a (possible) set of phrases to be extracted is shown in Figure 2. 3.2 Log-linear Model As an alternative to the traditional sourcechannel approach given in Equation (2) we can model the translation probability directly using a log-linear model (Och and Ney, 2002):   M I, fJ) exp λ h (e m m 1 1 m=1  , p(eI1 |f1J ) =  M I, fJ) exp λ h (˜ e m=1 m m 1 1 I e˜1 (5) 2 Usually the used models are IBM-1, HMM and IBM-4. with hm diﬀerent models, λm scaling factors and the denominator a normalization factor that can be ignored in the maximizati"
2005.mtsummit-papers.34,W05-0834,1,0.828788,"s (Ueﬃng et al., 2002). These are representations of diﬀerent possible translations for a given sentence. Once we have this representation we can use further models in order to compute an additional score for each of the possible candidates and then choose the one with the best score. Ideally these additional models would be integrated into the generation algorithm, but most of them are too costly to include in the search procedure or do not have a structure which allows this kind of coupling. How to eﬃciently compute n-best lists and word graphs for the phrase-based approach is presented in (Zens and Ney, 2005). 3.4 IBM1 Rescoring Although the IBM1 model is the easiest one of the single-word based translation models and the phrase based models clearly outperform this approach, the inclusion of the scores of this 261 model, i.e. hIBM1 (f1J |eI1 ) = I J   1 p(fj |ei ) (I + 1)J (6) j=1 i=0 has been shown experimentally to improve the performance of a machine translation system (Och et al., 2003). 3.5 LM Rescoring During the generation process, a single language model is used. However, additional language models speciﬁc to each sentence to be translated can help to improve the machine translation qual"
2006.iwslt-papers.7,2005.iwslt-1.20,1,0.362873,"the original single-word-based models to phrase-based-models, in order to better capture the context dependencies of the words in the translation process. The starting point for the training of these models was however the Viterbi alignment produced as a byproduct of the training of the original IBM models, that is, the alignment with the highest probability given the final parameter estimations. Most state-of-the-art machine translation systems, normally based on a phrase-based translation scheme or variations of it, make use of this Viterbi alignment as a first step in the training process [2, 3, 4]. Other translation approaches also benefit from the use of alignments [5]. It is then to expect that an increase in quality of the alignment should lead to an increase in translation quality. At least, it is expected that an improvement in the alignments does not hurt translation performance. In [6] the “Alignment Error Rate” (AER) is introduced as a measure of alignment quality. Given a reference alignment, consisting of a set S of “Sure”, unambiguous alignment points and a set P of “Possible”, ambiguous alignment points, with S ⊆ P , the AER of an alignment A = {(j, aj )} is defined to be |"
2006.iwslt-papers.7,P05-1033,0,0.0502601,"the original single-word-based models to phrase-based-models, in order to better capture the context dependencies of the words in the translation process. The starting point for the training of these models was however the Viterbi alignment produced as a byproduct of the training of the original IBM models, that is, the alignment with the highest probability given the final parameter estimations. Most state-of-the-art machine translation systems, normally based on a phrase-based translation scheme or variations of it, make use of this Viterbi alignment as a first step in the training process [2, 3, 4]. Other translation approaches also benefit from the use of alignments [5]. It is then to expect that an increase in quality of the alignment should lead to an increase in translation quality. At least, it is expected that an improvement in the alignments does not hurt translation performance. In [6] the “Alignment Error Rate” (AER) is introduced as a measure of alignment quality. Given a reference alignment, consisting of a set S of “Sure”, unambiguous alignment points and a set P of “Possible”, ambiguous alignment points, with S ⊆ P , the AER of an alignment A = {(j, aj )} is defined to be |"
2006.iwslt-papers.7,J04-2004,0,0.0975736,"the original single-word-based models to phrase-based-models, in order to better capture the context dependencies of the words in the translation process. The starting point for the training of these models was however the Viterbi alignment produced as a byproduct of the training of the original IBM models, that is, the alignment with the highest probability given the final parameter estimations. Most state-of-the-art machine translation systems, normally based on a phrase-based translation scheme or variations of it, make use of this Viterbi alignment as a first step in the training process [2, 3, 4]. Other translation approaches also benefit from the use of alignments [5]. It is then to expect that an increase in quality of the alignment should lead to an increase in translation quality. At least, it is expected that an improvement in the alignments does not hurt translation performance. In [6] the “Alignment Error Rate” (AER) is introduced as a measure of alignment quality. Given a reference alignment, consisting of a set S of “Sure”, unambiguous alignment points and a set P of “Possible”, ambiguous alignment points, with S ⊆ P , the AER of an alignment A = {(j, aj )} is defined to be |"
2006.iwslt-papers.7,J03-1002,1,0.0610827,"s, that is, the alignment with the highest probability given the final parameter estimations. Most state-of-the-art machine translation systems, normally based on a phrase-based translation scheme or variations of it, make use of this Viterbi alignment as a first step in the training process [2, 3, 4]. Other translation approaches also benefit from the use of alignments [5]. It is then to expect that an increase in quality of the alignment should lead to an increase in translation quality. At least, it is expected that an improvement in the alignments does not hurt translation performance. In [6] the “Alignment Error Rate” (AER) is introduced as a measure of alignment quality. Given a reference alignment, consisting of a set S of “Sure”, unambiguous alignment points and a set P of “Possible”, ambiguous alignment points, with S ⊆ P , the AER of an alignment A = {(j, aj )} is defined to be |A ∩ S |+ |A ∩ P | . AER(S, P ; A) = 1 − |A |+ |S| cision using the possible alignments. In the same paper, an exhaustive study of different alignment models is carried out. Following this work, numerous new alignment methods or refinements to existing ones have appeared in the literature, which incre"
2006.iwslt-papers.7,2005.mtsummit-papers.34,1,0.760218,"owever many of them do not report translation results, and the implicit assumption is made that the improvements on alignment quality will influence the translation process in a positive way. In this paper we will present two counter-examples to this assumption, that is, we will present (review in one of the cases) two relatively simple refinements of the standard alignment process using the IBM models that actually deteriorate the alignment quality. However, they improve the translation performance. We will show this on two translation models, a phrase based system similar to the one used in [7] and a finite state transducer based system as presented in [8]. The key point is that these methods adapt the alignments to the translation models that will make further use of them. 2. Related Work In [9] the authors conduct an experimental study on the correlation of AER as defined above and the actual translation performance. To our knowledge this is the first work that carries out such a detailed study. The conclusion of their work is that the alignment error rate is not a good measure for predicting translation performance. The main reason given is that AER does not penalize an unbalance"
2006.iwslt-papers.7,W02-1018,0,0.0657364,"Missing"
2006.iwslt-papers.7,W05-0831,1,0.906414,"implicit assumption is made that the improvements on alignment quality will influence the translation process in a positive way. In this paper we will present two counter-examples to this assumption, that is, we will present (review in one of the cases) two relatively simple refinements of the standard alignment process using the IBM models that actually deteriorate the alignment quality. However, they improve the translation performance. We will show this on two translation models, a phrase based system similar to the one used in [7] and a finite state transducer based system as presented in [8]. The key point is that these methods adapt the alignments to the translation models that will make further use of them. 2. Related Work In [9] the authors conduct an experimental study on the correlation of AER as defined above and the actual translation performance. To our knowledge this is the first work that carries out such a detailed study. The conclusion of their work is that the alignment error rate is not a good measure for predicting translation performance. The main reason given is that AER does not penalize an unbalanced precision and recall. They propose to use the “standard” F-me"
2006.iwslt-papers.7,P02-1038,1,0.544429,"n flaw found in both of these measures is that they do not take the structure of the translation model into account. 3. Phrase-Based Translation In this section we will briefly discuss the standard phrase based approach to machine translation, and we will pay special attention to the phrase extraction method. As usual, we will denote the (given) source sentence with f1J = f1 . . . fJ , which is to be translated into a target language sentence eI1 = e1 . . . eI . The usual approach in most state-of-the-art translation systems models the translation probability directly using a log-linear model [10]: P  M J I exp ) , f λ h (e m=1 m m 1 1  , (5) P p(eI1 |f1J ) = X M eI1 , f1J ) exp m=1 λm hm (˜ I e˜1 with a set of different models hm , scaling factors λm and the denominator a normalization factor that can be ignored in the maximization process. The most important models in equation (5) normally are phrase-based models in both source-totarget and target-to-source directions. In order to extract these phrase-based models, an alignment between the source and target training sentences is found by using the standard IBM models in both directions (source-to-target and target-to-source) and"
2006.iwslt-papers.7,2005.mtsummit-papers.36,0,0.0388298,"89 2 000 54 247 57 945 ∼ = argmax max P r(A) · P r(f1J , e˜J1 |A) e˜J 1 A∈A ∼ = argmax max e˜J 1 P r(fj , e˜j |f1j−1 , e˜j−1 1 , A) A∈A fj :j=1...J = argmax max e˜J 1 Table 1: Statistics of the Europarl corpus. Y A∈A Y j−1 p(fj , e˜j |fj−m , e˜j−1 j−m , A) . fj :j=1...J In other words: if we assume a uniform distribution for P r(A), the translation problem can be mapped to the problem of estimating an m-gram language model over a learned set of bilingual tuples (fj , e˜j ). In our case we represent this language model as a weighted finite state transducer, but this is not the only possibility [11]. Assume that the alignment is a function of the target words A′ : {1, . . . , I} → {1, . . . , J}, then the bilingual tuples (fj , e˜j ) can be inferred with e. g. the GIATI method of [4]. Each source word will be mapped to a target phrase of one or more words or an “empty” phrase ε. In particular, the source words which will remain non-aligned due to the alignment functionality restriction are paired with the empty phrase. However the alignments produced by the standard alignment generation procedure do not have this functionlike property. Furthermore, assuming that we could have such an ali"
2006.iwslt-papers.7,C04-1032,1,0.845511,"imates may be poor. 4.1. Alignment Adaptation This problem can be solved by reordering either the source or the target training sentences (both in training and test phases) in a way such that alignments become monotonic for all sentences. In [8] a method is presented to obtain an alignment that fulfill both requirements. Here we will give an overview of it. First, we estimate a cost matrix C for each sentence pair (f1J , eI1 ). The elements of this matrix cij are the local costs of aligning a source word fj to a target word ei . This cost matrix is estimated using the original IBM models, see [12] for more detail. For a given alignment A ⊆ I × J, define the costs of this alignment, c(A), as the sum of the local costs of all aligned word pairs: X c(A) = cij (7) (i,j)∈A The goal is to find an alignment with the minimum costs which fulfills the given constraints. In a first step, we require the alignment to be a function of source words A1 : {1, . . . , J} → {1, . . . , I} in order to uniquely define a reordering of the source sentence. This is easily computed from the cost matrix C as: A1 (j) = argmin cij . (8) i Non-aligned source words are not allowed. A1 naturally defines a new order"
2006.iwslt-papers.7,W05-0820,0,0.0126352,"ment (using a “reordered” cost matrix) with a dynamic programming algorithm similar to the Levenshtein string edit distance algorithm. An example of this method is shown in Figure 2. Because of the special constraints we require for this model, the alignment quality is expected to be relatively poor. 5. Experimental Results In this section we will analyze the impact the alignment methods described in Sections 3.1 and 4.1 have on both alignment and translation quality. For this, experiments will be reported on the Europarl corpus as used in the ACL 2005 Machine Translation Workshop Shared Task [13], for the German-English language pair. The corpus consists of the proceedings of the European Parliament, which are published on the web. Statistics are shown in Table 1. This corpus was chosen because of the different structure of the German and the English languages, that allows to better observe the effect of the alignments than for other language pairs, where the alignment is quasi-monotonic (e.g. English-Spanish). In order to have a reference alignment, we randomly selected a subset of the training corpus, consisting of 508 sentences, and manually annotated the alignments. Contrary to th"
2006.iwslt-papers.7,P06-1002,0,0.117678,"the translation process. If we had perfect statistical translation models that could generate a completely correct translation given a perfect alignment, it could perfectly be that a direct relation between alignment quality and translation quality would exist. However we do not have such perfect models and the training procedure can be “confused” when it finds structures it does not expect, although they may be completely correct. Therefore it can be of advantage to sacrifice some alignment quality in order to better guide the training process and have more robust estimations. A recent work [14] actually presents a new measure called “consistent phrase error rate” which tries to extend the AER to the concept of phrases. The authors show how this measure correlates better with translation performance, but it is however to much oriented to a phrase-based system and we expect it to perform poorly for other translation approaches5 . But one can take a step further. The alignment concept was first introduced as a hidden variable for the training of the single-word based models. Let them remain hidden then. When switching to phrase-based models the given data is assumed to be not only the"
2006.iwslt-papers.7,P03-1041,0,0.0397145,"Missing"
2006.iwslt-papers.7,J93-2003,0,\N,Missing
2006.iwslt-papers.7,J07-3002,0,\N,Missing
2007.iwslt-1.25,J90-2002,0,0.51512,"translation , we are given a source language sentence f1J = f1 . . . fj . . . fJ , which is to be translated into a target language sentence eI1 = e1 . . . ei . . . eI . Among all possible target language sentences, we will choose the sentence with the highest translation probability: ˆ eˆI1 = argmax I,eI1  P r(eI1 |f1J ) (1) We model this probability directly using a log-linear model: P  M I J exp λ h (e , f ) m m 1 1 m=1 P  P r(eI1 |f1J ) = P (2) M ′I′ , f J ) exp λ h (e 1 1 m=1 m m e′ I1 ′ This equation can be considered as a generalization of the source-channel approach presented in [1]. The hm (·) represent feature functions which can be bilingual, and thus represent the correspondence between source and target language, or monolingual, which represent additional features like grammaticality of the output. Typically, the features are statistical models or simple heuristics. This approach has the advantage that additional models h(·) can be easily integrated into the overall system. The model scaling factors λM 1 are trained according to the maximum entropy principle, e.g., using the GIS algorithm. Alternatively, one can train them with respect to the final translation quali"
2007.iwslt-1.25,P03-1021,0,0.040284,"ctions which can be bilingual, and thus represent the correspondence between source and target language, or monolingual, which represent additional features like grammaticality of the output. Typically, the features are statistical models or simple heuristics. This approach has the advantage that additional models h(·) can be easily integrated into the overall system. The model scaling factors λM 1 are trained according to the maximum entropy principle, e.g., using the GIS algorithm. Alternatively, one can train them with respect to the final translation quality measured by an error criterion [2]. For the IWSLT evaluation campaign, we optimized the scaling factors with respect to the BLEU measure, using the Downhill Simplex algorithm from [3]. The denominator in Equation 2 represents a normalization factor that depends only on the source sentence f1J . Therefore, we can omit it in the search process. As a decision rule, we obtain: ( M ) X Iˆ I J eˆ1 = argmax λm hm (e1 , f1 ) (3) I,eI1 m=1 Current state-of-the-art machine translation systems have a clearly dominating bilingual model guiding the translation process (i.e. a phrase-based model) and additional submodels. The systems develo"
2007.iwslt-1.25,2002.tmi-tutorials.2,0,0.0469304,"re are no gaps and there is no overlap. For a given sentence pair (f1J , eI1 ) and a given segmentation sK 1 , we define the bilingual phrases as: e˜k f˜k := eik−1 +1 . . . eik (5) := fbk . . . fjk (6) segmentations. In practice, we use the maximum approximation for this sum. As a result, the models h(·) depend not only on the sentence pair (f1J , eI1 ), but also on the segmentaJ I K tion sK 1 , i.e. we have models h(f1 , e1 , s1 ). The pairs of source and corresponding target phrases are extracted from the word-aligned bilingual training corpus by the phrase extraction algorithm described in [4]. The main idea is to extract phrase pairs that are consistent with the word alignment, meaning that the words of the source phrase are aligned only to words in the target phrase and vice versa. We use relative frequencies to estimate the phrase translation probabilities: p(f˜|˜ e) = hPhr (f1J , eI1 , sK 1 ) = log K Y p(f˜k |˜ ek ) (8) k=1 To obtain a more symmetric model, we use the phrase-based model in both directions p(f˜|˜ e) and p(˜ e|f˜). Depending on the language pair, we used one of three different types of reordering models. Jump Reordering. We use a very simple reordering model that"
2007.iwslt-1.25,W99-0604,1,0.717155,"rase pairs that are consistent with the word alignment, meaning that the words of the source phrase are aligned only to words in the target phrase and vice versa. We use relative frequencies to estimate the phrase translation probabilities: p(f˜|˜ e) = hPhr (f1J , eI1 , sK 1 ) = log K Y p(f˜k |˜ ek ) (8) k=1 To obtain a more symmetric model, we use the phrase-based model in both directions p(f˜|˜ e) and p(˜ e|f˜). Depending on the language pair, we used one of three different types of reordering models. Jump Reordering. We use a very simple reordering model that is also used in, for instance, [5, 6]. It assigns costs based on the jump width: K X |bk − jk−1 − 1 |+ J − jK (9) k=1 I = i4 target positions (7) Here, the number of co-occurrences of a phrase pair (f˜, e˜) that are consistent with the word alignment is denoted as N (f˜, e˜). If one occurrence of a target phrase e˜ has N > 1 possible translations, each of them contributes to N (f˜, e˜) with 1/N . The marginal count N (˜ e) is the number of occurrences of the target phrase e˜ in the training corpus. The resulting feature function is: hRM (f1J , eI1 , sK 1 )= i3 i2 i1 0 = i0 0 = j0 j2 b2 b1 N (f˜, e˜) N (˜ e) j1 j3 j4 = J b3 b4 sou"
2007.iwslt-1.25,2004.iwslt-evaluation.13,1,0.89106,"rase pairs that are consistent with the word alignment, meaning that the words of the source phrase are aligned only to words in the target phrase and vice versa. We use relative frequencies to estimate the phrase translation probabilities: p(f˜|˜ e) = hPhr (f1J , eI1 , sK 1 ) = log K Y p(f˜k |˜ ek ) (8) k=1 To obtain a more symmetric model, we use the phrase-based model in both directions p(f˜|˜ e) and p(˜ e|f˜). Depending on the language pair, we used one of three different types of reordering models. Jump Reordering. We use a very simple reordering model that is also used in, for instance, [5, 6]. It assigns costs based on the jump width: K X |bk − jk−1 − 1 |+ J − jK (9) k=1 I = i4 target positions (7) Here, the number of co-occurrences of a phrase pair (f˜, e˜) that are consistent with the word alignment is denoted as N (f˜, e˜). If one occurrence of a target phrase e˜ has N > 1 possible translations, each of them contributes to N (f˜, e˜) with 1/N . The marginal count N (˜ e) is the number of occurrences of the target phrase e˜ in the training corpus. The resulting feature function is: hRM (f1J , eI1 , sK 1 )= i3 i2 i1 0 = i0 0 = j0 j2 b2 b1 N (f˜, e˜) N (˜ e) j1 j3 j4 = J b3 b4 sou"
2007.iwslt-1.25,W05-0831,1,0.948835,"el reordering. The segmentation sK 1 is introduced as a hidden variable in the translation model. Therefore, it would be theoretically correct to sum over all possible Local Reordering. For closely related languages like Italian and English reordering within a local context forms the majority of all non-monotonicity. Common example are the change of the position of a preposition or the position of the adjective with respect to the noun it refers to. For local reordering, we allow words of the source sentence to be arbitrarily reordered within a restricted window of n positions as described in [7]. At each position, we give a fixed probability to the monotone word order and distribute the remaining probability mass among the other reordering possibilities. Syntactic Reordering for Chinese. For Chinese-toEnglish translation, reordering is a difficult task. Often, word order depends on the syntactic context. This is not handled well with the standard reordering approaches as presented above. Therefore we apply a rule-based reordering model at the level of syntactic chunks. The reordering is generated by a set of rules learned from word-aligned training data. These rules are obtained by p"
2007.iwslt-1.25,W07-0401,1,0.834617,"de X2 , the X2 that X1 i today games baseball any there are ci sono partite di baseball oggi training corpus and then reordering the obtained chunks to match target word order. For a test sentence to be translated, we generate every reordering that complies with the extracted rules. Reordering alternatives are weighted using the relative frequency of the rule in the training data. Additionally, we use a source language model that was trained on the reordered Chinese training sentences for weighting the transformed source word sequence. A more detailed description of the model can be found in [8]. (11) (12) where the bold subindices in the non-terminals represent the correspondence between source and target “gaps”. This model has as additional advantage that reordering is integrated as part of the model itself. The first step in the hierarchical phrase extraction is the same as for the phrased-based model presented in Section 3.1. Having a set of initial phrases, we search for phrases which contain other smaller sub-phrases and produce a new phrase with gaps. In our system, we restricted the number of non-terminals for each hierarchical phrase to a maximum of two, which were also not"
2007.iwslt-1.25,J07-2003,0,0.188396,"level of syntactic chunks. The reordering is generated by a set of rules learned from word-aligned training data. These rules are obtained by parsing the Chinese source language sentences of a bilingual 3.2. Hierarchical Phrase-Based Model The hierarchical phrase-based approach can be considered as an extension of the standard phrase-based model. In this model we allow the phrases to have “gaps”, i.e. we allow non-contiguous parts of the source sentence to be translated into possibly non-contiguous parts of the target sentence. The model can be formalized as a synchronous context-free grammar [9]. The bilingual rules are of the form X → hγ, α, ∼i , Figure 2: Example for the tuple based system. The bilingual sentence extracted is ci sono|are there partite di baseball|any baseball games oggi|today (10) where X is a non-terminal, γ and α are strings of terminals and non-terminals, and ∼ is a one-to-one correspondence between the non-terminals of α and γ. Two examples of this kind of rules for the Chinese-to-English translation direction are (borrowed from [9]) X → hyu X1 you X2 , have X2 with X1 i X → hX1 de X2 , the X2 that X1 i today games baseball any there are ci sono partite di base"
2007.iwslt-1.25,P06-1098,0,0.0258556,"h contain other smaller sub-phrases and produce a new phrase with gaps. In our system, we restricted the number of non-terminals for each hierarchical phrase to a maximum of two, which were also not allowed to be adjacent. In the original work [9], the search is organized as a parsing process, forming an extension of the CYK algorithm. This method is further augmented to include language model scores directly in the search, rather than as a preprocessing steps. Our implementation differs from this approach. We generate the target sentences in a strictly left-to-right fashion, in the spirit of [10]. In latter paper, rules are restricted to have a non-terminal symol only at the end of the rule. In our implementation we are able to handle all rules without restriction. We achieve this by transforming the target side of the grammar rules similar into a structure similar to a Greibach normal form. This allows a better integration in our existing decoder architecture (see Section 3.5) and a straightforward inclusion of language model scores into the translation process. 3.3. Bilingual N-Gram Model In this model, the main feature function in the log-linear model combination corresponds to the"
2007.iwslt-1.25,J04-2004,0,0.0245592,"tion is monotonic, i.e. we only use multiword source phrases if the alignment points cross, in a manner similar to [11]. Figure 2 shows an example of this segmentation. Each token in the bilanguage represents the event of the source words f˜k and the target words e˜k being aligned in the training data. For these events, we want to model the joint probability P r(fj , ei ). The transformation of the whole training corpus in such a way results in a bilanguage representation of the training corpus. On this new corpus, we apply standard language modeling techniques to train smoothed m-gram models [12]. In experimental trials a 4-gram model resulted in the best performance for most translation tasks. For better generalization we applied absolute discounting with leaving-one-out parameter estimation. Although reordering techniques can be applied for this kind of model [7], the performance of the model is normally significantly worse than the phrase-based models for language pairs with different word order. Therefore this system was only used for the Italian-to-English translation direction. 3.4. Common Models 3.4.4. Phrase penalty model 3.4.1. Word-based lexicon model In phrase-based MT, we"
2007.iwslt-1.25,patry-etal-2006-mood,0,0.0186379,"and normal phrases allows us to better control the contribution of each type of phrases. 3.4.5. Word penalty We also use another simple heuristic, the word penalty, to control the length of the produced translation: hWP (f1J , eI1 , sK 1 ) = I (16) These last two models affect the average sentence length. The model scaling factors can be adjusted to prefer longer sentences and longer phrases. 3.5. Implementation All models are implemented in a common software framework, called Xastur1 . They use the same decoder and common features modules. The architecture is similar to the one presented in [15]. 4. System combination To make use of the strenghts of the different models, we generated a consensus translation out of five different MT setups using an enhanced version of the system combination approach description in [16]. For each input test sentence, the first-best hypothesis of one contributing MT system is selected as primary hypothesis, and all other n-best (“secondary”; here: n = 10) translations of all systems are aligned to it, allowing for word reordering. The iterative alignment procedure is based on a GIZA++ training. During the alignment step, the whole test corpus of transla"
2007.iwslt-1.25,E06-1005,1,0.848336,"eI1 , sK 1 ) = I (16) These last two models affect the average sentence length. The model scaling factors can be adjusted to prefer longer sentences and longer phrases. 3.5. Implementation All models are implemented in a common software framework, called Xastur1 . They use the same decoder and common features modules. The architecture is similar to the one presented in [15]. 4. System combination To make use of the strenghts of the different models, we generated a consensus translation out of five different MT setups using an enhanced version of the system combination approach description in [16]. For each input test sentence, the first-best hypothesis of one contributing MT system is selected as primary hypothesis, and all other n-best (“secondary”; here: n = 10) translations of all systems are aligned to it, allowing for word reordering. The iterative alignment procedure is based on a GIZA++ training. During the alignment step, the whole test corpus of translations is taken into account. When the mutual word alignment of all the hypotheses for one sentence is obtained, the secondary hypotheses are then reordered to match the word order of the primary hypothesis based on the alignmen"
2007.iwslt-1.25,N07-1029,0,0.0577419,"rk is constructed. Since it is not known in advance which hypothesis has the best word order, we let each hypothesis play the role of the primary translation once for each sentence, and thus construct M confusion networks 1 Xastur is A Statistical Translator Under Research. (where M is the number of systems used; here M = 5) and unite them in a single lattice. All arcs in the path through the confusion network representing a hypothesis of a particular MT system are weighted with a system-specific factor; the different n-best hypotheses of each systems are weighted similarly to the approach of [17]. The lattice is then rescored using a Trigram LM which is trained on the MT hypotheses. This is to give a bonus to phrases that have been hypothesized by the systems, instead of single words only. Form the resulting lattice, the best hypothesis is extracted as the result of the system combination. The factors for the individual systems, as well as a LM factor and a Word Penalty are optimized using Condor [18]. We used the the IWSLT 2005 set for the Chinese-to-English tuning and the IWSLT 2007 dev5a set for Italian-to-English. For the Italian-to-English translation, the system combination proc"
2007.iwslt-1.25,takezawa-etal-2002-toward,0,0.0364178,"re optimized using Condor [18]. We used the the IWSLT 2005 set for the Chinese-to-English tuning and the IWSLT 2007 dev5a set for Italian-to-English. For the Italian-to-English translation, the system combination process worked on true case input, but gave bonus to pairs of words upper case/lower case words aligned to each other. For the Chinese-to-English system combination, all input hypotheses were in lower case, and a separate true casing step was performed on the consensus translation. 5. Tasks and corpora The experiments were carried out on the Basic Travel Expression Corpus (BTEC) task [19]. This is a multilingual speech corpus which contains tourism-related sentences similar to those that are found in phrase books. For the Chineseto-English track, a 40 000 sentence pair training corpus and five test sets were made available. For the Italian-to-English track, only 20 000 sentence pairs, but 6 development sets were provided. Other resources, despite proprietary data were permitted, but were not used in this system. 6. Italian-to-English Results For the Italian-to-English translation direction all the models described in this paper were used in the model combination. The preproces"
2007.iwslt-1.25,2006.iwslt-papers.1,1,0.808741,"ystem. 6. Italian-to-English Results For the Italian-to-English translation direction all the models described in this paper were used in the model combination. The preprocessing of the Italian side consisted mainly in the splitting of contractions like “dell’albergo” or “un’altra” into “dell’ albergo” and “un’ altra” respectively. No corresponding transformation was used in the English side. For the phrase-based model and the hierarchical model punctuations were removed in the source side of the corpus, but not on the target side. This has shown in past evaluations to obtain the best results [20]. However the tuple model does not seem to be able to generate the correct punctuations. In this case the model was trained without punctuations in the target side, and punctuation was restored used the tools of the SRI LM toolkit [13]. The input text was lowercased, but the target text was kept in “true case”. For each word at the beginning of a sentence, the most frequent case was determined and substituted. The case for words at the beginning of sentences was then restored as a postprocessing step. For the Italian-to-English condition 6 different developTable 1: Results for the different sy"
2007.iwslt-1.25,W03-1709,0,0.301982,"Missing"
2007.iwslt-1.25,C04-1006,1,0.900684,"Missing"
2007.iwslt-1.25,P02-1040,0,0.0762194,"age of the IWSLT 2004 test data is already high for the 20k sentences and the 16 references allow for a large tolerance in the MT output. The large improvement in this year can be attributed to the extensive evaluation of different aspects of the system like like word segmentations, alignment parameters and alignment combinations. The large improvements on the development and blind test set used in the preparation seem to be due to an increasing amount of overfitting on the small and specific BTEC dataset. 8. Evaluation Results For all the experiments, we report the two accuracy measures BLEU [24] and NIST [25] as well as the two error rates WER and PER. All those criteria are computed with respect to multiple references. 8.1. Primary submissions The translation results of the RWTH primary submissions are summarized in Table 4. For Chinese-to-English, we also report the results of the best contrastive submissions, as it performed better than the primary submission and only differs slightly in the optimization criterion. For the primary submission we used the average sentence length as reference length for the BLEU measure, the best submission used the “minimum nearest” method, taking t"
2008.iwslt-evaluation.16,2002.tmi-tutorials.2,0,0.032515,"Model The basic idea of phrase-based translation is to segment the given source sentence into phrases, then translate each phrase and finally compose the target sentence from these phrase translations. Phrases are defined as nonempty contiguous sequences of words. We constrain the segmentations so that all words in the source and the target sentence are covered by exactly one phrase. Thus, there are no gaps and there is no overlap. The pairs of source and corresponding target phrases are extracted from the word-aligned bilingual training corpus by the phrase extraction algorithm described in [2]. The main idea is to extract phrase pairs that are consistent with the word alignment, meaning that the words of the source phrase are aligned only to words in the target phrase and vice versa. Proceedings of IWSLT 2008, Hawaii - U.S.A. We use relative frequencies to estimate the phrase translation probabilities: p(f˜|˜ e) = N (f˜, e˜) N (˜ e) (2) Here, the number of co-occurrences of a phrase pair (f˜, e˜) that are consistent with the word alignment is denoted as N (f˜, e˜). If one occurrence of a target phrase e˜ has N &gt; 1 possible translations, each of them contributes to N (f˜, e˜) with 1"
2008.iwslt-evaluation.16,W99-0604,1,0.584043,"3.3.1. Word-based Lexicon Model hPhr (f1J , eI1 , sK 1 ) = log K Y p(f˜k |˜ ek ) (3) k=1 To obtain a more symmetric model, we use the phrase-based model in both directions p(f˜|˜ e) and p(˜ e|f˜). Depending on the language pair, we used a different type of reordering model: • IBM Reordering For the Arabic-to-English language pair a word-based reordering constrained by the IBM restrictions [3] is often enough and obtains the best results. • Jump Reordering For the Chinese-to-English translation direction we use a very simple reordering model at phrase level that is also used in, for instance, [4, 5]. It assigns costs based only on the jump width. The phrase translation models estimate their probabilities by relative frequencies. Most of the longer phrases or translation units however occur only once in the training corpus. Therefore, pure relative frequencies overestimate the probability of those phrases. To overcome this problem, we use a word-based lexicon model to smooth the phrase translation probabilities. The score of a phrase pair is computed similar to the IBM model 1, but here, we are summing only within a phrase pair and not over the whole target language sentence. In the case"
2008.iwslt-evaluation.16,2004.iwslt-evaluation.13,1,0.818237,"3.3.1. Word-based Lexicon Model hPhr (f1J , eI1 , sK 1 ) = log K Y p(f˜k |˜ ek ) (3) k=1 To obtain a more symmetric model, we use the phrase-based model in both directions p(f˜|˜ e) and p(˜ e|f˜). Depending on the language pair, we used a different type of reordering model: • IBM Reordering For the Arabic-to-English language pair a word-based reordering constrained by the IBM restrictions [3] is often enough and obtains the best results. • Jump Reordering For the Chinese-to-English translation direction we use a very simple reordering model at phrase level that is also used in, for instance, [4, 5]. It assigns costs based only on the jump width. The phrase translation models estimate their probabilities by relative frequencies. Most of the longer phrases or translation units however occur only once in the training corpus. Therefore, pure relative frequencies overestimate the probability of those phrases. To overcome this problem, we use a word-based lexicon model to smooth the phrase translation probabilities. The score of a phrase pair is computed similar to the IBM model 1, but here, we are summing only within a phrase pair and not over the whole target language sentence. In the case"
2008.iwslt-evaluation.16,J07-2003,0,0.0522557,"ation probabilities p(f |e) are estimated as relative frequencies from the word-aligned training corpus. The word-based lexicon model is also used in both directions p(f |e) and p(e|f ). 3.3.2. Target Language Model 3.2. Hierarchical Model The hierarchical phrase-based approach can be considered as an extension of the standard phrase-based model. In this model we allow the phrases to have “gaps”, i.e. we allow non-contiguous parts of the source sentence to be translated into possibly non-contiguous parts of the target sentence. The model can be formalized as a synchronous context-free grammar [6]. The bilingual rules are of the form X → hγ, α, ∼i , (4) where X is a non-terminal, γ and α are strings of terminals and non-terminals, and ∼ is a one-to-one correspondence between the non-terminals of α and γ. Two examples of this kind of rules for the Chinese-to-English translation direction are X→h We use the SRI language modeling toolkit [7] to train a standard n-gram language model. The smoothing technique we apply is the modified Kneser-Ney discounting with interpolation. In our case we used a 6-gram language model. 3.3.3. Phrase Count Features The reliability of the phrase probability"
2008.iwslt-evaluation.16,2008.iwslt-papers.7,1,0.828609,"and NP, respectively), therefore the corresponding hierarchical rule gets a count of 1 for the syntax feature in the target part. Similarly for the source part. These counts are added up for all occurrences of a hierarchical rule (which may be extracted from different sentences and perhaps with different syntactic properties) and normalized with the total count of the phrase. We tried different ways of smoothing the counts, for the case where the phrases do not correspond to the yield of a node completely, but a binary count seemed to work best for the IWSLT data. More details can be found in [9]. 4.2. Chunk-based Reordering for Chinese For the standard phrase-based model we also tried and improved reordering model based on an extended version of the method described in [10]. The Chinese input sentence is reordered by a set of syntactic chunk-level rules, which are automatically learned from the training data. The method is described in [11]. In contrast to previous work, the reordered sentences are represented as an n-best list instead of a lattice. The size of the n-best list is kept small. This method has two advantages. On the one hand, not all reorderings are translated, which im"
2008.iwslt-evaluation.16,2007.iwslt-1.3,1,0.793315,"ed up for all occurrences of a hierarchical rule (which may be extracted from different sentences and perhaps with different syntactic properties) and normalized with the total count of the phrase. We tried different ways of smoothing the counts, for the case where the phrases do not correspond to the yield of a node completely, but a binary count seemed to work best for the IWSLT data. More details can be found in [9]. 4.2. Chunk-based Reordering for Chinese For the standard phrase-based model we also tried and improved reordering model based on an extended version of the method described in [10]. The Chinese input sentence is reordered by a set of syntactic chunk-level rules, which are automatically learned from the training data. The method is described in [11]. In contrast to previous work, the reordered sentences are represented as an n-best list instead of a lattice. The size of the n-best list is kept small. This method has two advantages. On the one hand, not all reorderings are translated, which improves system performance. The concept is similar to performing an aggressive pruning on the reordering lattice, where only the most promising reorderings alternatives are kept. On t"
2008.iwslt-evaluation.16,W07-0401,1,0.828669,"total count of the phrase. We tried different ways of smoothing the counts, for the case where the phrases do not correspond to the yield of a node completely, but a binary count seemed to work best for the IWSLT data. More details can be found in [9]. 4.2. Chunk-based Reordering for Chinese For the standard phrase-based model we also tried and improved reordering model based on an extended version of the method described in [10]. The Chinese input sentence is reordered by a set of syntactic chunk-level rules, which are automatically learned from the training data. The method is described in [11]. In contrast to previous work, the reordered sentences are represented as an n-best list instead of a lattice. The size of the n-best list is kept small. This method has two advantages. On the one hand, not all reorderings are translated, which improves system performance. The concept is similar to performing an aggressive pruning on the reordering lattice, where only the most promising reorderings alternatives are kept. On the other hand, there is not need for a translation system that can handle lattice based input, and thus this reordering method can be easily adapted to any translation sy"
2008.iwslt-evaluation.16,C08-1128,1,0.831022,"le used in the translation process is the composed from the standard phrase table expanded with the new phrases extracted from the reordered training sentences. The training data was parsed by the tree parser from Purdue University [12], extracting the basic chunks from the tree structure. Each chunk has 1.7 words on average. The size of the source reordered n-best list is 5. We additionally use the jump reordering model. 4.3. Source Preprocessing 4.3.1. Chinese Chinese word segmentation is one of the crucial steps in the Chinese text preprocessing. We compared various segmentation methods in [13] and found out that the unigram segmenter performs better translation results in many cases than the ictclas tool [14], which we use as baseline. Our unigram segmentation is an LDC-like segmentation without text Proceedings of IWSLT 2008, Hawaii - U.S.A. S S VP WHADVP WRB Where VP P NP 洗手间 PP PN VV AUX NP is DT JJ NN the public toilet 在 哪里 X → h X ∼0 在 哪里 , Where is X ∼0 i Figure 1: Example of a syntax-enhanced hierarchical rule. normalization. Given a manually compiled lexicon, i.e. an LDC lexicon that contains words and their relative frequencies Ps (f 0 j ), the best segmentation is the one"
2008.iwslt-evaluation.16,W03-1730,0,0.0114191,"ted from the reordered training sentences. The training data was parsed by the tree parser from Purdue University [12], extracting the basic chunks from the tree structure. Each chunk has 1.7 words on average. The size of the source reordered n-best list is 5. We additionally use the jump reordering model. 4.3. Source Preprocessing 4.3.1. Chinese Chinese word segmentation is one of the crucial steps in the Chinese text preprocessing. We compared various segmentation methods in [13] and found out that the unigram segmenter performs better translation results in many cases than the ictclas tool [14], which we use as baseline. Our unigram segmentation is an LDC-like segmentation without text Proceedings of IWSLT 2008, Hawaii - U.S.A. S S VP WHADVP WRB Where VP P NP 洗手间 PP PN VV AUX NP is DT JJ NN the public toilet 在 哪里 X → h X ∼0 在 哪里 , Where is X ∼0 i Figure 1: Example of a syntax-enhanced hierarchical rule. normalization. Given a manually compiled lexicon, i.e. an LDC lexicon that contains words and their relative frequencies Ps (f 0 j ), the best segmentation is the one that maximizes the joint probability of all words in the sentence, with the assumption that words are independent of"
2008.iwslt-evaluation.16,2005.eamt-1.37,1,0.896906,"Missing"
2008.iwslt-evaluation.16,W06-3111,1,0.903734,"Missing"
2008.iwslt-evaluation.16,P06-1001,0,0.0484133,"Missing"
2008.iwslt-evaluation.16,P05-1071,0,0.1017,"Missing"
2008.iwslt-evaluation.16,W07-0813,0,0.0393414,"Missing"
2008.iwslt-evaluation.16,E06-1005,1,0.776328,"del trained only on 20K sentence pairs was to weak to differentiate between good and bad ASR hypotheses. The expansion of the lattices using alternative word segmentations introduces additional ambiguity: so far we have not used probabilities for the segmentation alternatives. Nevertheless, examples show that in many cases the ASR errors can be avoided when word lattices are translated (see Table 1). 4.5. System Combination For system combination we used our approach from last year’s evaluation campaign [8], which is based on an enhanced version of the system combination approach described in [24]. The method is based on the generation of - 112 - Table 1: Examples of improved speech translation quality when ASR word lattices are used as input for translation. single-best lattice reference single-best lattice reference single-best lattice reference Hurry up. Can you. Some? It is too expensive. Can you make it cheaper? Too expensive. Can you make it cheaper? Where is the bus stop? The bus stop here, please. Here is the bus stop. How long time? How long will it take to get there? How much time will it take? a consensus translation out of the output of different translation systems. The co"
2008.iwslt-evaluation.16,N07-1029,0,0.0278975,"systems. The core of the method consists in building a confusion network for each sentence by aligning and combining the (single-best) translation hypothesis from one MT system with the translations produced by the other MT systems (and the other translations from the same system, if n-best lists are used in combination). For each sentence, each MT system is selected once as “primary” system, and the other hypotheses are aligned to this hypothesis. The resulting confusing networks are combined into one word graph, which is then weighted with system-specific factors, similar to the approach of [25], and a trigram LM trained on the MT hypotheses. The translation with the best total score within this word graph is selected as consensus translation. The scaling factors of these models are optimized using the Condor toolkit [26] to achieve optimal BLEU score on the dev set. 5. Experimental Results In this year’s evaluation RWTH participated in the Arabic-toEnglish and Chinese-to-English translation directions. For this last language pair, we participated in both evaluation tasks, BTEC and Challenge. As training data we used the provided training data and additionally a part of the HITcorpus"
2008.iwslt-papers.7,W06-1606,0,0.00705261,"nements to the extraction process and including syntax information. Section 6 presents experimental results, which are analyzed in Section 7. Section 8 concludes the paper. 2. Related Work The hierarchical phrase based approach was first presented by David Chiang in [3] and further detailed in [2]. Already in [3], Chiang proposes the use of syntactic information together with his new hierarchical approach, but without success. Some recent publications have shown that the use of syntax for translation achieves significant improvements. One prominent example are the works by the ISI group (e.g. [4, 5]). These works go apart from the standard phrase-based approach by defining new translation units and extraction procedures, but they try to still keep the advantages of phrase-based translation [6]. There have also been some previous efforts in combining syntactic information together with a hierarchical phrase-based approach, see for example Zollmann Proceedings of IWSLT 2008, Hawaii - U.S.A. 中 X ∼0 那个 X ∼1 # It’s the X ∼1 in the X ∼0 也 要 X ∼0 一些 X ∼1 # like to X ∼0 some X ∼1 too Figure 1: Example of hierarchical rules. et al. [7] or more recently Marton et al. [8]. Our work differs from the"
2008.iwslt-papers.7,W06-3119,0,0.0192306,"One prominent example are the works by the ISI group (e.g. [4, 5]). These works go apart from the standard phrase-based approach by defining new translation units and extraction procedures, but they try to still keep the advantages of phrase-based translation [6]. There have also been some previous efforts in combining syntactic information together with a hierarchical phrase-based approach, see for example Zollmann Proceedings of IWSLT 2008, Hawaii - U.S.A. 中 X ∼0 那个 X ∼1 # It’s the X ∼1 in the X ∼0 也 要 X ∼0 一些 X ∼1 # like to X ∼0 some X ∼1 too Figure 1: Example of hierarchical rules. et al. [7] or more recently Marton et al. [8]. Our work differs from the above mentioned mainly in that we extract the syntactic information already at the training phrase, and it gets integrated in the search process as an additional model in the base log-linear combination that underlies most state-of-the-art statistical machine translation systems. Therefore, no modification of the search algorithms is needed and we can also make use of syntactic information for both languages, source and target (see also Section 5). Most of the previous work limit themselves only to the target language side, as the"
2008.iwslt-papers.7,P08-1114,0,0.0401196,"by the ISI group (e.g. [4, 5]). These works go apart from the standard phrase-based approach by defining new translation units and extraction procedures, but they try to still keep the advantages of phrase-based translation [6]. There have also been some previous efforts in combining syntactic information together with a hierarchical phrase-based approach, see for example Zollmann Proceedings of IWSLT 2008, Hawaii - U.S.A. 中 X ∼0 那个 X ∼1 # It’s the X ∼1 in the X ∼0 也 要 X ∼0 一些 X ∼1 # like to X ∼0 some X ∼1 too Figure 1: Example of hierarchical rules. et al. [7] or more recently Marton et al. [8]. Our work differs from the above mentioned mainly in that we extract the syntactic information already at the training phrase, and it gets integrated in the search process as an additional model in the base log-linear combination that underlies most state-of-the-art statistical machine translation systems. Therefore, no modification of the search algorithms is needed and we can also make use of syntactic information for both languages, source and target (see also Section 5). Most of the previous work limit themselves only to the target language side, as the correspondences between the syntact"
2008.iwslt-papers.7,2002.tmi-tutorials.2,0,0.0351574,"on systems. Therefore, no modification of the search algorithms is needed and we can also make use of syntactic information for both languages, source and target (see also Section 5). Most of the previous work limit themselves only to the target language side, as the correspondences between the syntactic structures of both languages are hard to define. 3. Hierarchical Phrase Based Translation The baseline system we use is an in-house implementation of a hierarchical phrase-based system, similar to the one presented in [2]. This approach can be seen as an extension of the phrase-based approach [9, 10], where we allow for “gaps” in the extracted phrases. In this way, longer range dependencies in the translation process can be modelled and the reordering is directly integrated in the decoding process. The translation model can be formalized as a synchronous context free grammar, where the rules are of the form X → hγ, α, ∼i where X is a non-terminal, γ and α are strings of terminals (respectively in the source and target languages) and non-terminals, and ∼ is a one-to-one correspondence between the non-terminals of α and γ, which shows corresponding “elided” parts in the source and target se"
2008.iwslt-papers.7,N03-1017,0,0.00596269,"on systems. Therefore, no modification of the search algorithms is needed and we can also make use of syntactic information for both languages, source and target (see also Section 5). Most of the previous work limit themselves only to the target language side, as the correspondences between the syntactic structures of both languages are hard to define. 3. Hierarchical Phrase Based Translation The baseline system we use is an in-house implementation of a hierarchical phrase-based system, similar to the one presented in [2]. This approach can be seen as an extension of the phrase-based approach [9, 10], where we allow for “gaps” in the extracted phrases. In this way, longer range dependencies in the translation process can be modelled and the reordering is directly integrated in the decoding process. The translation model can be formalized as a synchronous context free grammar, where the rules are of the form X → hγ, α, ∼i where X is a non-terminal, γ and α are strings of terminals (respectively in the source and target languages) and non-terminals, and ∼ is a one-to-one correspondence between the non-terminals of α and γ, which shows corresponding “elided” parts in the source and target se"
2008.iwslt-papers.7,P07-1019,0,0.0207936,"directions source-to-target and target-to-source, which get combined log-lineally with additional IBM1-like word level scores at the phrase level, word and phrase penalty scores at generation time. The decoding process is basically a parsing of the source sentence according to the defined grammar, keeping track of the target language translation contexts in order to compute language model scores during the translation process. In order to deal with the high computational effort of the search process, early pruning is carried out in the form of “cube pruning” or its lazy version “cube growing” [11]. 4. Refinement of Extraction Heuristics In standard phrase-based translation, the extraction of additional phrase pairs by including non-aligned words adjacent to the standard phrases, both in the source and target language, has proven to be beneficial in the translation process. However, in [2] they are not included in the extraction process, probably for efficiency reasons. Proceedings of IWSLT 2008, Hawaii - U.S.A. We however, found it beneficial to include them in the translation process. Although difficult to justify from a practical point of view, practice has shown that simple heuristi"
2008.iwslt-papers.7,P03-1054,0,0.0114961,"les and thus the system is able to make a better usage of bilingual correspondences between syntactic structures. Furthermore, the inclusion of this information as additional scores in the phrases does not have any impact on computation time. Our goal is to determine if the bilingual phrase pair corresponds to some syntactic structures, or not. We stress again that we do not limit the amount of phrases we extract, as non-syntactical phrases are necessary to achieve good translation performance [6]. We parse the English part with Charniak’s parser1 and the Chinese part with the Stanford parser [12]. Given an initial phrase pair we analyze both parts (source and target) independently. In order to weight a phrase ranging from position i to j, we check whether this sequence corresponds to the yield of some node in the parse tree. If not, we first determine the minimum number of words that we have to delete or add to the phrase so that it can be associated with such a node. In order to compute this number we search in the tree in a bottom-up manner, looking for the lowest node that does not cover all words, or in a top-down manner, looking for the highest node that covers all the words of t"
2008.iwslt-papers.7,P03-1021,0,0.0333238,"Missing"
2008.iwslt-papers.7,W03-1709,0,0.0119376,"Missing"
2008.iwslt-papers.7,2006.iwslt-papers.5,0,0.0141788,"yntax information improves the translation performance in some cases. 7. Discussion We believe that the results presented in the previous section must be interpreted cautiously. Table 2 also shows the performance of the different systems on the test04 data which we used as development set for parameter tuning. In this corpus the variability of the results is much greater and no clear conclusions can be drawn. This can partly be due to the relatively small size of the data used and also due to instabilities in the optimization algorithm used (downhill simplex), as pointed out in Lambert et al. [16]. The consistent results obtained on the blind test data set, however, give a reasonable indication that the proposed methods improve translation quality. 10. References [1] P. F. Brown, S. A. Della Pietra, V. J. Della Pietra, and R. L. Mercer, “The mathematics of statistical machine translation: Parameter estimation,” Computational Linguistics, vol. 19, no. 2, pp. 263–311, June 1993. [2] D. Chiang, “Hierarchical Phrase-Based Translation,” Computational Linguistics, vol. 33, no. 2, pp. 201–228, 2007. [3] ——, “A Hierarchical Phrase-Based Model for Statistical Machine Translation,” in Proc. of t"
2008.iwslt-papers.7,N04-1035,0,\N,Missing
2008.iwslt-papers.7,J93-2003,0,\N,Missing
2008.iwslt-papers.7,P05-1033,0,\N,Missing
2008.iwslt-papers.7,D07-1079,0,\N,Missing
2008.iwslt-papers.7,2007.iwslt-1.25,1,\N,Missing
2008.iwslt-papers.7,J07-2003,0,\N,Missing
2008.iwslt-papers.7,2007.iwslt-1.10,0,\N,Missing
2009.eamt-1.33,P05-1033,0,0.0279192,"l itself is also relatively simple and allows for efficient generation algorithms like for example beam search (see e.g. (Koehn, 2004)). This allowed the approach to scale to bigger tasks and it is still one of the most widely used models nowadays. The current trend in SMT, however, is to bring more c 2009 European Association for Machine Translation. information in the form of grammatical structures. There are mainly two possibilities, in the form of linguistically motivated grammars (e.g. (Marcu et al., 2006)) or just formal grammars, which do not need to have a linguistic equivalent (e.g. (Chiang, 2005)). These more expressive models have associated a more difficult search problem, which normally involves a parsing process (usually a variation of the CYK algorithm) while incorporating the translation information. The inclusion of language model (LM) information replicates nodes in the parsing tree, which increases the cost of the generation process. And not to be underestimated, the number of rules the system has to deal with can be of one order of magnitude bigger than the standard phrase-based approach, depending on the model1 . Therefore, new, efficient algorithms for translation with the"
2009.eamt-1.33,J07-2003,0,0.810445,"g process (usually a variation of the CYK algorithm) while incorporating the translation information. The inclusion of language model (LM) information replicates nodes in the parsing tree, which increases the cost of the generation process. And not to be underestimated, the number of rules the system has to deal with can be of one order of magnitude bigger than the standard phrase-based approach, depending on the model1 . Therefore, new, efficient algorithms for translation with these richer models had to be developed. In this paper we will concentrate on the cube growing algorithm (Huang and Chiang, 2007), a lazy version of the cube pruning algorithm (Chiang, 2007). These algorithms represent the search space as an hypergraph and add language model scores as necessary. The most time-consuming operation in the translation process is the LM score computation, especially when huge LMs are used. The cube growing algorithm follows an on-demand computation strategy and tries to minimize the number of LM scores that need to be computed. In order to minimize the number of search errors, while still maintaining computational efficiency, the algorithm depends on an (efficient) heuristic for these LM cos"
2009.eamt-1.33,P07-1019,0,0.44229,"s a parsing process (usually a variation of the CYK algorithm) while incorporating the translation information. The inclusion of language model (LM) information replicates nodes in the parsing tree, which increases the cost of the generation process. And not to be underestimated, the number of rules the system has to deal with can be of one order of magnitude bigger than the standard phrase-based approach, depending on the model1 . Therefore, new, efficient algorithms for translation with these richer models had to be developed. In this paper we will concentrate on the cube growing algorithm (Huang and Chiang, 2007), a lazy version of the cube pruning algorithm (Chiang, 2007). These algorithms represent the search space as an hypergraph and add language model scores as necessary. The most time-consuming operation in the translation process is the LM score computation, especially when huge LMs are used. The cube growing algorithm follows an on-demand computation strategy and tries to minimize the number of LM scores that need to be computed. In order to minimize the number of search errors, while still maintaining computational efficiency, the algorithm depends on an (efficient) heuristic for these LM cos"
2009.eamt-1.33,koen-2004-pharaoh,0,0.0772177,"ion performance and efficiency and propose a new heuristic which efficiently decreases memory requirements and computation time, while maintaining translation performance. 1 Introduction In the last decade, the phrase-based approach to machine translation has been the de-facto standard for statistical machine translation (SMT) systems. The main reason was that it offered a great improvement in translation quality over its predecessors, the single-word based models. The model itself is also relatively simple and allows for efficient generation algorithms like for example beam search (see e.g. (Koehn, 2004)). This allowed the approach to scale to bigger tasks and it is still one of the most widely used models nowadays. The current trend in SMT, however, is to bring more c 2009 European Association for Machine Translation. information in the form of grammatical structures. There are mainly two possibilities, in the form of linguistically motivated grammars (e.g. (Marcu et al., 2006)) or just formal grammars, which do not need to have a linguistic equivalent (e.g. (Chiang, 2005)). These more expressive models have associated a more difficult search problem, which normally involves a parsing proces"
2009.eamt-1.33,W06-1606,0,0.01255,"great improvement in translation quality over its predecessors, the single-word based models. The model itself is also relatively simple and allows for efficient generation algorithms like for example beam search (see e.g. (Koehn, 2004)). This allowed the approach to scale to bigger tasks and it is still one of the most widely used models nowadays. The current trend in SMT, however, is to bring more c 2009 European Association for Machine Translation. information in the form of grammatical structures. There are mainly two possibilities, in the form of linguistically motivated grammars (e.g. (Marcu et al., 2006)) or just formal grammars, which do not need to have a linguistic equivalent (e.g. (Chiang, 2005)). These more expressive models have associated a more difficult search problem, which normally involves a parsing process (usually a variation of the CYK algorithm) while incorporating the translation information. The inclusion of language model (LM) information replicates nodes in the parsing tree, which increases the cost of the generation process. And not to be underestimated, the number of rules the system has to deal with can be of one order of magnitude bigger than the standard phrase-based"
2009.eamt-1.33,J03-1002,1,0.00761933,"tional LM computations is small. On the other side, when compared with the original heuristic, we eliminate the need of the -LM pass altogether. 5.2 Choosing the Classes There is still the open question of how to choose the word-to-class mapping C. In our case we investigated two alternatives. The first one is to use automatically generated classes. We used the mkcls tool (Och, 1999), which uses a maximum likelihood approach on a corpus by using a class bigram decomposition. This tool is widely used as part of the preprocessing steps when training statistical alignments using the GIZA++ tool (Och and Ney, 2003). This criterion seems to be adequate for our task, as both the words themselves and the context are taken into account. Another possibility would be to use Part-ofSpeech tags as word classes. The tagging itself can, however, be an expensive process, involving a new search in itself. We applied a simplifying assumption, in which we remove the ambiguity of the tagging. We applied a full POS-tagger (Brants, 2000) to the training corpora and then we simply selected the most frequent POS tag for each word. In this way we defined our mapping C. 6 Experimental Results Experiments are reported using"
2009.eamt-1.33,E99-1010,0,0.0154992,"c introduces a new language model into the translation process. However, the size of this language model is quite small, especially when compared with the full language model used in search, and thus the overhead of the additional LM computations is small. On the other side, when compared with the original heuristic, we eliminate the need of the -LM pass altogether. 5.2 Choosing the Classes There is still the open question of how to choose the word-to-class mapping C. In our case we investigated two alternatives. The first one is to use automatically generated classes. We used the mkcls tool (Och, 1999), which uses a maximum likelihood approach on a corpus by using a class bigram decomposition. This tool is widely used as part of the preprocessing steps when training statistical alignments using the GIZA++ tool (Och and Ney, 2003). This criterion seems to be adequate for our task, as both the words themselves and the context are taken into account. Another possibility would be to use Part-ofSpeech tags as word classes. The tagging itself can, however, be an expensive process, involving a new search in itself. We applied a simplifying assumption, in which we remove the ambiguity of the taggin"
2009.eamt-1.33,D08-1012,0,0.0427386,"Missing"
2009.eamt-1.33,A00-1031,0,0.00859756,"approach on a corpus by using a class bigram decomposition. This tool is widely used as part of the preprocessing steps when training statistical alignments using the GIZA++ tool (Och and Ney, 2003). This criterion seems to be adequate for our task, as both the words themselves and the context are taken into account. Another possibility would be to use Part-ofSpeech tags as word classes. The tagging itself can, however, be an expensive process, involving a new search in itself. We applied a simplifying assumption, in which we remove the ambiguity of the tagging. We applied a full POS-tagger (Brants, 2000) to the training corpora and then we simply selected the most frequent POS tag for each word. In this way we defined our mapping C. 6 Experimental Results Experiments are reported using the 2008 WMT evaluation data (Callison-Burch et al., 2008), for the German-to-English translation direction. This corpus consists of the speeches held in the plenary session of the European Parliament. The test data was the in-domain data used in the evaluation. The statistics of the corpus can be seen in Table 1. Figure 2 shows the results for the -LM heuristic3 . The BLEU score is shown in Figure 2(a). The be"
2009.eamt-1.33,W08-0309,0,0.0121553,"dequate for our task, as both the words themselves and the context are taken into account. Another possibility would be to use Part-ofSpeech tags as word classes. The tagging itself can, however, be an expensive process, involving a new search in itself. We applied a simplifying assumption, in which we remove the ambiguity of the tagging. We applied a full POS-tagger (Brants, 2000) to the training corpora and then we simply selected the most frequent POS tag for each word. In this way we defined our mapping C. 6 Experimental Results Experiments are reported using the 2008 WMT evaluation data (Callison-Burch et al., 2008), for the German-to-English translation direction. This corpus consists of the speeches held in the plenary session of the European Parliament. The test data was the in-domain data used in the evaluation. The statistics of the corpus can be seen in Table 1. Figure 2 shows the results for the -LM heuristic3 . The BLEU score is shown in Figure 2(a). The best results are achieved with a -LM n-best size of 200. The difference in performance, however is not too big and nearly optimal results can already be achieved with a -LM n-best size of 50. When looking into the computational resources the diff"
2010.amta-papers.8,N09-1025,0,0.0562958,"Missing"
2010.amta-papers.8,J07-2003,0,0.189032,"our comparison, since the risk of converging to poor local optima during the optimization procedure increases when too many features are available, thus making it difficult to draw clear conclusions. 2 Hierarchical Machine Translation The hierarchical phrase-based approach can be considered to be an extension of the standard phrase-based model. In this model, we allow the phrases to have “gaps”, i.e. we allow noncontiguous parts of the source sentence to be translated into possibly non-contiguous parts of the target sentence. The model can be formalized as a synchronous context-free grammar (Chiang, 2007). The bilingual rules are of the form X → hγ, α, ∼i , (1) where X is a non-terminal, γ and α are strings of terminals and non-terminals, and ∼ is a one-toone correspondence between the non-terminals of α and γ. Two examples of this kind of rules for the German-to-English translation direction are X → hich habe X ∼0 gesehen, I have seen X ∼0 i X → hum X ∼0 zu X ∼1 , in order to X ∼1 X ∼0 i where the indices in the non-terminals represent the correspondence between source and target “gaps”. This model has the additional advantage that reordering is integrated as part of the model itself, as can"
2010.amta-papers.8,W08-1301,0,0.0230466,"al integrity during the extraction process. Instead we produce additional information for each phrase. We mark those phrases that do not fit in the model with a binary feature. In this way we allow the corresponding scaling factors to decide whether the phrase can still be used during decoding. This also means that a scaling factor of zero allows the decoder to fall back to the baseline system during the minimum error rate training. We parse the English target sentences with the Stanford parser1 , which is able to produce deep syntactic parses as well as dependency structures (de Marneffe and Manning, 2008). In the following we will present the three syntactic models that we analyze in this work. 1 http://nlp.stanford.edu/software/lex-parser.shtml 3.1 Parse Matching The first model that we employ is also the simplest one. Given a monolingual sentence (be it in the source or the target language) and the associated parse tree, we will say that a lexical phrase extracted from this sentence is syntactically valid if it corresponds to the yield of one of the nodes in the syntax tree. With this model, we hope that we can guide the decoder to prefer phrases that are syntactically sound rather than usin"
2010.amta-papers.8,D07-1079,0,0.0601923,"l phrase-based model for machine translation in Section 2. We then describe the additional syntactical models used in this paper in Section 3. Results and detailed analysis on the NIST Chinese-English task are presented in Section 4. We conclude the paper in Section 5. 1.1 Related Work One of the first papers to incorporate syntactic knowledge in a statistical machine translation model was (Yamada and Knight, 2001), although the performance was not on par with other state-of-the-art approaches at that time. Further development in this direction achieved competitive results, as can be seen in (DeNeefe et al., 2007) and later publications by the same group. In contrast to these studies, which propose new models centered around the syntactic information, we focus mainly on methods that can be easily incorporated into an existing hierarchical system. In this work, we employ soft syntactic features comparable to (Vilar et al., 2008). These features measure how much a phrase corresponds to a valid syntactic structure of a given parse tree. Further, we include a dependency language model in a string-to-dependency model in the spirit of (Shen et al., 2008). We also derive soft syntactic labels as in (Venugopal"
2010.amta-papers.8,N04-1035,0,0.0693091,"s and produce additional initial phrases, also with a low probability. An example is shown in Figure 1(d). The additional phrases that are generated when applying these heuristics are not considered for the later extraction of hierarchical phrases. This is due to the large number of phrases that could be extracted when considering the whole set of initial phrases, which would pose efficiency problems for the translation process. In our experiments, the heuristic methods already increased the number of initial phrases roughly by a factor of 2. 3 Syntactic Features Unlike other work, like e.g. (Galley et al., 2004), we are not enforcing any syntactical integrity during the extraction process. Instead we produce additional information for each phrase. We mark those phrases that do not fit in the model with a binary feature. In this way we allow the corresponding scaling factors to decide whether the phrase can still be used during decoding. This also means that a scaling factor of zero allows the decoder to fall back to the baseline system during the minimum error rate training. We parse the English target sentences with the Stanford parser1 , which is able to produce deep syntactic parses as well as dep"
2010.amta-papers.8,P08-1114,0,0.081536,"t can be easily incorporated into an existing hierarchical system. In this work, we employ soft syntactic features comparable to (Vilar et al., 2008). These features measure how much a phrase corresponds to a valid syntactic structure of a given parse tree. Further, we include a dependency language model in a string-to-dependency model in the spirit of (Shen et al., 2008). We also derive soft syntactic labels as in (Venugopal et al., 2009), where the generic non-terminal of the hierarchical system is replaced by a syntactic label. Other approaches in this field like (Chiang et al., 2009) and (Marton and Resnik, 2008) go into similar directions, but create a rather large quantity of features. We chose not to include their approaches into our comparison, since the risk of converging to poor local optima during the optimization procedure increases when too many features are available, thus making it difficult to draw clear conclusions. 2 Hierarchical Machine Translation The hierarchical phrase-based approach can be considered to be an extension of the standard phrase-based model. In this model, we allow the phrases to have “gaps”, i.e. we allow noncontiguous parts of the source sentence to be translated into"
2010.amta-papers.8,P02-1038,1,0.747643,"Missing"
2010.amta-papers.8,P03-1021,0,0.0297927,"ords Vocabulary OOVs Sentences # Words Vocabulary OOVs Chinese English 3 030 696 77 456 152 81 002 954 83 128 213 076 21 059 95 544 1 664 42 930 172 324 6 387 17 202 1 871 50 353 1 357 36 114 149 057 6 418 17 877 1 375 43 724 Table 1: Statistics for the Chinese-English corpus 4 Experimental Results We used the Chinese-English NIST 2006 evaluation set as a development corpus and the NIST 2008 evaluation set as the blind test corpus. The systems were trained on a medium-sized training set. Statistics can be found in Table 1. All systems were optimized for the BLEU score using Och’s MERT method (Och, 2003), with all scaling factors initialized with a value of 0.1. For rescoring with trigram dependency language models we generated 100-best lists after the optimization process. Translation results obtained applying the methods discussed in Section 3 are shown in Table 2. All three methods yield improvements over the baseline system. The string-todependency method has very strong improvements in TER, while the soft syntactic labels perform very good in terms of BLEU. The parsematch approach is somewhat in between. The combination of the methods also leads to nice synergies. With the exception of t"
2010.amta-papers.8,P08-1066,0,0.396344,"on achieved competitive results, as can be seen in (DeNeefe et al., 2007) and later publications by the same group. In contrast to these studies, which propose new models centered around the syntactic information, we focus mainly on methods that can be easily incorporated into an existing hierarchical system. In this work, we employ soft syntactic features comparable to (Vilar et al., 2008). These features measure how much a phrase corresponds to a valid syntactic structure of a given parse tree. Further, we include a dependency language model in a string-to-dependency model in the spirit of (Shen et al., 2008). We also derive soft syntactic labels as in (Venugopal et al., 2009), where the generic non-terminal of the hierarchical system is replaced by a syntactic label. Other approaches in this field like (Chiang et al., 2009) and (Marton and Resnik, 2008) go into similar directions, but create a rather large quantity of features. We chose not to include their approaches into our comparison, since the risk of converging to poor local optima during the optimization procedure increases when too many features are available, thus making it difficult to draw clear conclusions. 2 Hierarchical Machine Tran"
2010.amta-papers.8,N09-1027,0,0.297999,"Missing"
2010.amta-papers.8,2008.iwslt-papers.7,1,0.938414,"ate syntactic knowledge in a statistical machine translation model was (Yamada and Knight, 2001), although the performance was not on par with other state-of-the-art approaches at that time. Further development in this direction achieved competitive results, as can be seen in (DeNeefe et al., 2007) and later publications by the same group. In contrast to these studies, which propose new models centered around the syntactic information, we focus mainly on methods that can be easily incorporated into an existing hierarchical system. In this work, we employ soft syntactic features comparable to (Vilar et al., 2008). These features measure how much a phrase corresponds to a valid syntactic structure of a given parse tree. Further, we include a dependency language model in a string-to-dependency model in the spirit of (Shen et al., 2008). We also derive soft syntactic labels as in (Venugopal et al., 2009), where the generic non-terminal of the hierarchical system is replaced by a syntactic label. Other approaches in this field like (Chiang et al., 2009) and (Marton and Resnik, 2008) go into similar directions, but create a rather large quantity of features. We chose not to include their approaches into ou"
2010.amta-papers.8,W10-1738,1,0.846637,"rious groups report improvement over their baseline systems with different approaches, but it is not clear whether the benefits of the different methods are complementary or if they rather address the same issues. In this work, we compare three recent syntactic methods that enhance the translation quality. We measure their performance individually and in combination with each other on a medium sized NIST Chinese-English task, and offer some analysis of typical translation examples. All the presented methods are released as part of the open source hierarchical machine translation toolkit Jane (Vilar et al., 2010). This paper is organized as follows: We briefly recapitulate the hierarchical phrase-based model for machine translation in Section 2. We then describe the additional syntactical models used in this paper in Section 3. Results and detailed analysis on the NIST Chinese-English task are presented in Section 4. We conclude the paper in Section 5. 1.1 Related Work One of the first papers to incorporate syntactic knowledge in a statistical machine translation model was (Yamada and Knight, 2001), although the performance was not on par with other state-of-the-art approaches at that time. Further de"
2010.amta-papers.8,P01-1067,0,0.0755085,"the presented methods are released as part of the open source hierarchical machine translation toolkit Jane (Vilar et al., 2010). This paper is organized as follows: We briefly recapitulate the hierarchical phrase-based model for machine translation in Section 2. We then describe the additional syntactical models used in this paper in Section 3. Results and detailed analysis on the NIST Chinese-English task are presented in Section 4. We conclude the paper in Section 5. 1.1 Related Work One of the first papers to incorporate syntactic knowledge in a statistical machine translation model was (Yamada and Knight, 2001), although the performance was not on par with other state-of-the-art approaches at that time. Further development in this direction achieved competitive results, as can be seen in (DeNeefe et al., 2007) and later publications by the same group. In contrast to these studies, which propose new models centered around the syntactic information, we focus mainly on methods that can be easily incorporated into an existing hierarchical system. In this work, we employ soft syntactic features comparable to (Vilar et al., 2008). These features measure how much a phrase corresponds to a valid syntactic s"
2010.iwslt-evaluation.22,W06-3103,1,0.835479,"Workshop on Spoken Language Translation (IWSLT 2010). We used it as an opportunity to incorporate novel methods which have been investigated at RWTH over the last year and which have proven to be successful in other evaluations. We participated in the Arabic-English BTEC task, and used standard alignment and training tools as well as our inhouse phrase-based and open-source hierarchical SMT decoders. We explored and implemented different segmentation tools for Arabic. The methods used to implement those tools vary from rule-based methods (typically encoded as finite state transducers) such as [1], to methods which are statistically-based such as [2] and [3]. All these works have shown that segmentation improves MT quality significantly for both small and large scale tasks. Due to the different methodologies that we apply for segmentation, we expect that there will be complimentary variation in the results achieved by each method. The next step would be to exploit those variations and achieve better results by combining the systems. This paper is organized as follows. In Section 2, we present the data and resources that will be used to build our segmenters and the SMT system. In Sectio"
2010.iwslt-evaluation.22,N04-4015,0,0.0675793,"We used it as an opportunity to incorporate novel methods which have been investigated at RWTH over the last year and which have proven to be successful in other evaluations. We participated in the Arabic-English BTEC task, and used standard alignment and training tools as well as our inhouse phrase-based and open-source hierarchical SMT decoders. We explored and implemented different segmentation tools for Arabic. The methods used to implement those tools vary from rule-based methods (typically encoded as finite state transducers) such as [1], to methods which are statistically-based such as [2] and [3]. All these works have shown that segmentation improves MT quality significantly for both small and large scale tasks. Due to the different methodologies that we apply for segmentation, we expect that there will be complimentary variation in the results achieved by each method. The next step would be to exploit those variations and achieve better results by combining the systems. This paper is organized as follows. In Section 2, we present the data and resources that will be used to build our segmenters and the SMT system. In Section 3, we discuss the problems of Arabic SMT and present"
2010.iwslt-evaluation.22,N06-2013,0,0.0454777,"it as an opportunity to incorporate novel methods which have been investigated at RWTH over the last year and which have proven to be successful in other evaluations. We participated in the Arabic-English BTEC task, and used standard alignment and training tools as well as our inhouse phrase-based and open-source hierarchical SMT decoders. We explored and implemented different segmentation tools for Arabic. The methods used to implement those tools vary from rule-based methods (typically encoded as finite state transducers) such as [1], to methods which are statistically-based such as [2] and [3]. All these works have shown that segmentation improves MT quality significantly for both small and large scale tasks. Due to the different methodologies that we apply for segmentation, we expect that there will be complimentary variation in the results achieved by each method. The next step would be to exploit those variations and achieve better results by combining the systems. This paper is organized as follows. In Section 2, we present the data and resources that will be used to build our segmenters and the SMT system. In Section 3, we discuss the problems of Arabic SMT and present the sol"
2010.iwslt-evaluation.22,N04-4038,0,0.0482616,"asily captured by the IBM alignment models. In this work, we experimented with the following segmenters: • FST - A Finite State Transducer-based approach introduced and implemented by [1]. The FST is used as a framework to implement a set of rules for segmentation of Arabic. The prefixes that are split include w,f,k,l,b,Al and s. Suffixes which are segmented are pronouns (objective and possessive). The method is characterized by fast processing speed but suffers from the lack of context in the decision procedure leading to erroneous output. • SVM - we reimplemented the classifier suggested by [4]. In their method, each character is classified by its segment rule (prefix, stem and suffix) and position (beginning and inside segment). Arabic words are segmented according to the ATB scheme. Additionally, feminine marker normalization (tX→p+X) using an SVM model is applied on top of the segmenter output, which proved to be significant for the performance of MT in our experiments. • CRF - we implemented a CRF classifier for segmentation using similar setup of classifiers and classes as in the SVM model. The software we use as an implementation of conditional random fields is named CRF++4 ."
2010.iwslt-evaluation.22,W07-0813,0,0.052985,"eme. Additionally, feminine marker normalization (tX→p+X) using an SVM model is applied on top of the segmenter output, which proved to be significant for the performance of MT in our experiments. • CRF - we implemented a CRF classifier for segmentation using similar setup of classifiers and classes as in the SVM model. The software we use as an implementation of conditional random fields is named CRF++4 . • MorphTagger - is a general architecture for Part-OfSpeech (POS) tagging of natural languages. The architecture was first proposed in [5] and applied for the task of POS tagging of Hebrew. [6] adapted the architecture to the Arabic language. MorphTagger is implemented using Buckwalter Arabic Morphological Analyzer v1.0 (BAMA) as a morphological analyzer and a Hidden-Markov-Model (HMM) (using the SRIML5 toolkit) as the disambiguator component. • MADA - The Morphological Analysis and Disambiguation of Arabic (MADA) system, developed in [7], can be seen as an extension of an SVM-based system with the incorporation of a morphological analyzer. As in [8], we experiment with different segmentation schemes for each chosen analysis. We use the schemes directly implemented in the MADA versi"
2010.iwslt-evaluation.22,P05-1071,0,0.0466539,"lementation of conditional random fields is named CRF++4 . • MorphTagger - is a general architecture for Part-OfSpeech (POS) tagging of natural languages. The architecture was first proposed in [5] and applied for the task of POS tagging of Hebrew. [6] adapted the architecture to the Arabic language. MorphTagger is implemented using Buckwalter Arabic Morphological Analyzer v1.0 (BAMA) as a morphological analyzer and a Hidden-Markov-Model (HMM) (using the SRIML5 toolkit) as the disambiguator component. • MADA - The Morphological Analysis and Disambiguation of Arabic (MADA) system, developed in [7], can be seen as an extension of an SVM-based system with the incorporation of a morphological analyzer. As in [8], we experiment with different segmentation schemes for each chosen analysis. We use the schemes directly implemented in the MADA version we are using, namely: D1,D2,D3 and the ATB (TB) schemes. 4. Phrase-based system 4.1. Standard phrase-based system (PBT) The phrase-based SMT system used in this work is an inhouse implementation of state-of-the-art phrase-based MT system as described in [9]. We use the standard set of models with phrase translation probabilities for source-to-tar"
2010.iwslt-evaluation.22,2010.amta-papers.8,1,0.836307,"WTH and free for non-commercial use [12]. This approach is an extension of the phrase-based approach, where the phrases are allowed to have gaps [13]. In this way long-range dependencies and reorderings can be modelled in a consistent statistical framework. The system labelled as JANE represents a fairly standard setup of the system and constitutes a baseline upon which the two next systems are built. 5.2. Soft syntax labels (SYN) To extend the hierarchical system with syntax information of the English target side, we derive soft syntactic labels as in [14] with the modifications described in [15]. In this model, instead of considering only a single, generic non-terminal in the underlying grammar, we extend the set of labels to include syntactic categories as found in syntactic parse trees. To extract the syntax information, we parse the English target sentences with the Stanford parser6 . It is important to note that the new non-terminals are considered in a probabilistic way. In this way, the parsing process itself continues to use the generic non-terminal as in the baseline model and the parsing space is unaltered. The extended set of non-terminals is then used to compute a new prob"
2010.iwslt-evaluation.22,2010.iwslt-papers.18,1,0.874946,"h respect to the syntactic constructs. 5.3. Poor-man syntax (POMS) In this approach we apply the same model as described in the previous section, but the method for producing the new 6 http://nlp.stanford.edu/software/lex-parser.shtml JANE BLEU TER 55.4 30.6 55.3 30.2 55.2 29.4 53.9 31.2 54.6 30.2 56.6 28.8| 57.1| 29.4 56.6− 28.9− 53.0 32.4 SYN BLEU TER 55.7 30.8 54.4 31.2 55.7 29.4 54.5 30.9 54.8 31.2 56.5 28.5+ 56.6| 29.2 55.4 30.3 52.7 32.5 POMS BLEU TER 56.1 30.2 56.0− 29.4− 55.2 29.9 54.8 30.8 55.5− 29.7− 56.8− 28.7 57.5∗ 28.5∗ 54.9 29.5 53.4 32.3 non-terminals is altered as described in [16]. Instead of relying on parse trees based on linguistic knowledge we rely on automatic clustering methods. This makes this approach applicable also for underresourced languages for which no linguistic tools may be available. 6. Results The results of the different segmentation methods and schemes are summarized in Table 1. In this table, the best result in a column is marked with |, thus comparing different segmentations for the same decoder setup. We mark with − the best (in a row) performing decoder over a specific segmentation method. ∗ marks the best result overall in the table. For compar"
2010.iwslt-evaluation.22,W10-1747,1,0.898303,"Missing"
2010.iwslt-evaluation.22,2008.iwslt-papers.8,1,0.0907932,"MADA - The Morphological Analysis and Disambiguation of Arabic (MADA) system, developed in [7], can be seen as an extension of an SVM-based system with the incorporation of a morphological analyzer. As in [8], we experiment with different segmentation schemes for each chosen analysis. We use the schemes directly implemented in the MADA version we are using, namely: D1,D2,D3 and the ATB (TB) schemes. 4. Phrase-based system 4.1. Standard phrase-based system (PBT) The phrase-based SMT system used in this work is an inhouse implementation of state-of-the-art phrase-based MT system as described in [9]. We use the standard set of models with phrase translation probabilities for source-to-target and target-to-source direction, smoothing with lexical weights, a word and phrase penalty, distance-based and lexicalized reordering and an n-gram target language model. 4.2. Phrase training (Forced Alignment-FA) To estimate the phrase translation probabilities we experimented with both standard heuristic phrase extraction ([10]) and a forced alignment training procedure as described in [11]. The latter estimates the probabilities as relative frequencies from the phrase-aligned training data, which i"
2010.iwslt-evaluation.22,W99-0604,1,0.294484,"m 4.1. Standard phrase-based system (PBT) The phrase-based SMT system used in this work is an inhouse implementation of state-of-the-art phrase-based MT system as described in [9]. We use the standard set of models with phrase translation probabilities for source-to-target and target-to-source direction, smoothing with lexical weights, a word and phrase penalty, distance-based and lexicalized reordering and an n-gram target language model. 4.2. Phrase training (Forced Alignment-FA) To estimate the phrase translation probabilities we experimented with both standard heuristic phrase extraction ([10]) and a forced alignment training procedure as described in [11]. The latter estimates the probabilities as relative frequencies from the phrase-aligned training data, which is computed by a modified version of the translation decoder. To do this, the translation decoder is constrained to produce the reference translation for each bilingual sentence pair. In order to counteract overfitting, leaving-one-out is applied in training. In addition to providing a statistically well-founded 4 http://crfpp.sourceforge.net/ 5 http://www-speech.sri.com/projects/srilm/ 164 Proceedings of the 7th Internati"
2010.iwslt-evaluation.22,P10-1049,1,0.304281,"ystem used in this work is an inhouse implementation of state-of-the-art phrase-based MT system as described in [9]. We use the standard set of models with phrase translation probabilities for source-to-target and target-to-source direction, smoothing with lexical weights, a word and phrase penalty, distance-based and lexicalized reordering and an n-gram target language model. 4.2. Phrase training (Forced Alignment-FA) To estimate the phrase translation probabilities we experimented with both standard heuristic phrase extraction ([10]) and a forced alignment training procedure as described in [11]. The latter estimates the probabilities as relative frequencies from the phrase-aligned training data, which is computed by a modified version of the translation decoder. To do this, the translation decoder is constrained to produce the reference translation for each bilingual sentence pair. In order to counteract overfitting, leaving-one-out is applied in training. In addition to providing a statistically well-founded 4 http://crfpp.sourceforge.net/ 5 http://www-speech.sri.com/projects/srilm/ 164 Proceedings of the 7th International Workshop on Spoken Language Translation Paris, December 2nd"
2010.iwslt-evaluation.22,W10-1738,1,0.0939642,"010: IWSLT08 results summary (nocase+punc) System CRF FST MADA ATB MADA D1 MADA D2 MADA D3 MorphTagger SVM TOK PBT BLEU TER 55.5 29.8− 54.5 30.7 55.1 29.5 54.8 30.8 55.4 29.9 55.4 29.6 56.5| 29.2| 56.1 29.7 55.5− 30.1− FA BLEU TER 56.4− 30.7 55.9 30.3 57.1+ 29.2+ 55.2− 30.6− 55.5 30.1 56.5 30.1 55.8 30.1 55.9 30.0 54.8 30.3 phrase model, the forced alignment procedure has the benefit of producing smaller phrase tables. 5. Hierarchical system 5.1. Standard hierarchical system (JANE) We used the open source hierarchical phrase-based system Jane, developed at RWTH and free for non-commercial use [12]. This approach is an extension of the phrase-based approach, where the phrases are allowed to have gaps [13]. In this way long-range dependencies and reorderings can be modelled in a consistent statistical framework. The system labelled as JANE represents a fairly standard setup of the system and constitutes a baseline upon which the two next systems are built. 5.2. Soft syntax labels (SYN) To extend the hierarchical system with syntax information of the English target side, we derive soft syntactic labels as in [14] with the modifications described in [15]. In this model, instead of consider"
2010.iwslt-evaluation.22,J07-2003,0,0.142759,"K PBT BLEU TER 55.5 29.8− 54.5 30.7 55.1 29.5 54.8 30.8 55.4 29.9 55.4 29.6 56.5| 29.2| 56.1 29.7 55.5− 30.1− FA BLEU TER 56.4− 30.7 55.9 30.3 57.1+ 29.2+ 55.2− 30.6− 55.5 30.1 56.5 30.1 55.8 30.1 55.9 30.0 54.8 30.3 phrase model, the forced alignment procedure has the benefit of producing smaller phrase tables. 5. Hierarchical system 5.1. Standard hierarchical system (JANE) We used the open source hierarchical phrase-based system Jane, developed at RWTH and free for non-commercial use [12]. This approach is an extension of the phrase-based approach, where the phrases are allowed to have gaps [13]. In this way long-range dependencies and reorderings can be modelled in a consistent statistical framework. The system labelled as JANE represents a fairly standard setup of the system and constitutes a baseline upon which the two next systems are built. 5.2. Soft syntax labels (SYN) To extend the hierarchical system with syntax information of the English target side, we derive soft syntactic labels as in [14] with the modifications described in [15]. In this model, instead of considering only a single, generic non-terminal in the underlying grammar, we extend the set of labels to include syn"
2010.iwslt-evaluation.22,N09-1027,0,0.0119972,"phrase-based system Jane, developed at RWTH and free for non-commercial use [12]. This approach is an extension of the phrase-based approach, where the phrases are allowed to have gaps [13]. In this way long-range dependencies and reorderings can be modelled in a consistent statistical framework. The system labelled as JANE represents a fairly standard setup of the system and constitutes a baseline upon which the two next systems are built. 5.2. Soft syntax labels (SYN) To extend the hierarchical system with syntax information of the English target side, we derive soft syntactic labels as in [14] with the modifications described in [15]. In this model, instead of considering only a single, generic non-terminal in the underlying grammar, we extend the set of labels to include syntactic categories as found in syntactic parse trees. To extract the syntax information, we parse the English target sentences with the Stanford parser6 . It is important to note that the new non-terminals are considered in a probabilistic way. In this way, the parsing process itself continues to use the generic non-terminal as in the baseline model and the parsing space is unaltered. The extended set of non-ter"
2010.iwslt-evaluation.22,popovic-ney-2006-pos,1,\N,Missing
2010.iwslt-evaluation.22,D11-1033,0,\N,Missing
2010.iwslt-evaluation.22,E09-1044,0,\N,Missing
2010.iwslt-evaluation.22,D09-1022,1,\N,Missing
2010.iwslt-evaluation.22,J93-2003,0,\N,Missing
2010.iwslt-evaluation.22,E06-1005,1,\N,Missing
2010.iwslt-evaluation.22,E03-1076,0,\N,Missing
2010.iwslt-evaluation.22,D08-1089,0,\N,Missing
2010.iwslt-evaluation.22,P03-1054,0,\N,Missing
2010.iwslt-evaluation.22,P02-1040,0,\N,Missing
2010.iwslt-evaluation.22,W06-3110,1,\N,Missing
2010.iwslt-evaluation.22,J10-3008,0,\N,Missing
2010.iwslt-evaluation.22,2010.iwslt-keynotes.2,0,\N,Missing
2010.iwslt-evaluation.22,P10-2041,0,\N,Missing
2010.iwslt-evaluation.22,P08-2030,0,\N,Missing
2010.iwslt-evaluation.22,W07-0734,0,\N,Missing
2010.iwslt-evaluation.22,W06-3105,0,\N,Missing
2010.iwslt-evaluation.22,N03-1017,0,\N,Missing
2010.iwslt-evaluation.22,2008.iwslt-papers.7,1,\N,Missing
2010.iwslt-evaluation.22,J03-1002,1,\N,Missing
2010.iwslt-evaluation.22,W06-3108,1,\N,Missing
2010.iwslt-evaluation.22,P07-1019,0,\N,Missing
2010.iwslt-evaluation.22,D08-1039,1,\N,Missing
2010.iwslt-evaluation.22,P08-1066,0,\N,Missing
2010.iwslt-evaluation.22,2009.mtsummit-posters.17,0,\N,Missing
2010.iwslt-evaluation.22,2010.iwslt-papers.15,1,\N,Missing
2010.iwslt-evaluation.22,2006.iwslt-papers.1,1,\N,Missing
2010.iwslt-evaluation.22,2011.iwslt-papers.1,1,\N,Missing
2010.iwslt-evaluation.22,2011.iwslt-papers.7,1,\N,Missing
2010.iwslt-evaluation.22,2011.iwslt-papers.8,1,\N,Missing
2010.iwslt-evaluation.22,N04-1033,1,\N,Missing
2010.iwslt-evaluation.22,2011.iwslt-evaluation.1,0,\N,Missing
2010.iwslt-evaluation.22,D08-1076,0,\N,Missing
2010.iwslt-evaluation.22,P03-1021,0,\N,Missing
2010.iwslt-evaluation.22,2011.iwslt-papers.5,1,\N,Missing
2010.iwslt-evaluation.22,P08-1000,0,\N,Missing
2010.iwslt-papers.11,W06-3105,0,0.0618427,"rase extraction procedure defined in Equation 1 requires a word alignment to be provided. Normally this alignment is computed using probabilistic models, usually the word-based IBM translation models [11] as implemented in the GIZA++ toolkit [12], which are different to the ones later used in the translation process. This produces a mismatch between the phrase extraction procedure and the translation procedure, as they are based on different stochastic models, which are applied independently of each other. Other approaches have tried to bridge this mismatch between training and decoding, e.g. [1, 2, 3]. Recently, consistent improvements in translation quality could be achieved [4]. In this work we investigate a first approach to incorporate the techniques proposed in [4] in the hierarchical phrasebased approach. In this section we describe the training procedure and the model by which the phrase counts are computed, which we will later incorporate into the hierarchical translation system. 3.1. Forced Alignment for Phrase-Based Models An illustration of the basic idea of the forced alignment training can be seen in Figure 1. The forced alignment procedure performs a phrase segmentation and a"
2010.iwslt-papers.11,P06-1096,0,0.0221215,"rase extraction procedure defined in Equation 1 requires a word alignment to be provided. Normally this alignment is computed using probabilistic models, usually the word-based IBM translation models [11] as implemented in the GIZA++ toolkit [12], which are different to the ones later used in the translation process. This produces a mismatch between the phrase extraction procedure and the translation procedure, as they are based on different stochastic models, which are applied independently of each other. Other approaches have tried to bridge this mismatch between training and decoding, e.g. [1, 2, 3]. Recently, consistent improvements in translation quality could be achieved [4]. In this work we investigate a first approach to incorporate the techniques proposed in [4] in the hierarchical phrasebased approach. In this section we describe the training procedure and the model by which the phrase counts are computed, which we will later incorporate into the hierarchical translation system. 3.1. Forced Alignment for Phrase-Based Models An illustration of the basic idea of the forced alignment training can be seen in Figure 1. The forced alignment procedure performs a phrase segmentation and a"
2010.iwslt-papers.11,2009.eamt-1.23,0,0.0260374,"rase extraction procedure defined in Equation 1 requires a word alignment to be provided. Normally this alignment is computed using probabilistic models, usually the word-based IBM translation models [11] as implemented in the GIZA++ toolkit [12], which are different to the ones later used in the translation process. This produces a mismatch between the phrase extraction procedure and the translation procedure, as they are based on different stochastic models, which are applied independently of each other. Other approaches have tried to bridge this mismatch between training and decoding, e.g. [1, 2, 3]. Recently, consistent improvements in translation quality could be achieved [4]. In this work we investigate a first approach to incorporate the techniques proposed in [4] in the hierarchical phrasebased approach. In this section we describe the training procedure and the model by which the phrase counts are computed, which we will later incorporate into the hierarchical translation system. 3.1. Forced Alignment for Phrase-Based Models An illustration of the basic idea of the forced alignment training can be seen in Figure 1. The forced alignment procedure performs a phrase segmentation and a"
2010.iwslt-papers.11,P10-1049,1,0.931056,"ed. Normally this alignment is computed using probabilistic models, usually the word-based IBM translation models [11] as implemented in the GIZA++ toolkit [12], which are different to the ones later used in the translation process. This produces a mismatch between the phrase extraction procedure and the translation procedure, as they are based on different stochastic models, which are applied independently of each other. Other approaches have tried to bridge this mismatch between training and decoding, e.g. [1, 2, 3]. Recently, consistent improvements in translation quality could be achieved [4]. In this work we investigate a first approach to incorporate the techniques proposed in [4] in the hierarchical phrasebased approach. In this section we describe the training procedure and the model by which the phrase counts are computed, which we will later incorporate into the hierarchical translation system. 3.1. Forced Alignment for Phrase-Based Models An illustration of the basic idea of the forced alignment training can be seen in Figure 1. The forced alignment procedure performs a phrase segmentation and alignment of each sentence pair of the training data using a modification of the"
2010.iwslt-papers.11,P08-1024,0,0.0416513,"phrase alignment between source and target sentences. Phrase translation probabilities are then updated based on this alignment. By applying leaving-one-out in the training procedure, overfitting effects can be diminished. The phrase table which is learnt from forced alignments can be used as phrase table itself in the translation system or can be combined with the original phrase-based system. Experiments in [4] show that the latter gives better results. In recent years, conventional phrase-based systems have been outerperformed by hierarchical phrase-based or syntaxbased systems. The papers [5, 6] describe first training approaches with forced alignment techniques with hierarchical translation systems. However, they report difficulties when aligning training sentences because of the restrictions in the phrase extraction process. Our work is intended to neither solve this problem nor propose any forced alignment training method on hierarchical systems. Instead, we want to see the effects of combining a hierarchical system with forced alignments from a phrase-based decoder. The next section will recall phrase-based and hierarchical phrase-based translation models. In Section 3 we describ"
2010.iwslt-papers.11,2008.iwslt-papers.7,1,0.851625,"forced alignments as described in Section 3. The hierarchical system is used as baseline and combined with the retrained phrase table from the forced alignment training using the phrase-based system. 5.1. Results First, we present our results on the IWSLT 2010 BTEC task for Arabic-to-English for which corpus statistics are given in Table 1. We give BLEU scores in Table 2 as well as TER scores in Table 3. We chose test04 as development set and test08 and test05 as blind test sets. We experimented with a hierarchical baseline system and a hierarchical system enriched with soft syntactic labels [15]. The second will be simply denoted by the label syntax in our tables of results. The filtering method from Section 4 improves both the baseline and the system with syntax information, though improvements can be found only on one of the test sets each. The intersection method was only tested on the simple baseline and performed worse on all test sets. In order to investigate our method on a larger corpus, we experimented on the English-to-German Quaero project corpus from 2010. This corpus mainly contains the data from the Workshop of Machine Translation (WMT) 2010, namely Europarl and news-co"
2010.iwslt-papers.11,2009.iwslt-papers.2,0,0.123072,"phrase alignment between source and target sentences. Phrase translation probabilities are then updated based on this alignment. By applying leaving-one-out in the training procedure, overfitting effects can be diminished. The phrase table which is learnt from forced alignments can be used as phrase table itself in the translation system or can be combined with the original phrase-based system. Experiments in [4] show that the latter gives better results. In recent years, conventional phrase-based systems have been outerperformed by hierarchical phrase-based or syntaxbased systems. The papers [5, 6] describe first training approaches with forced alignment techniques with hierarchical translation systems. However, they report difficulties when aligning training sentences because of the restrictions in the phrase extraction process. Our work is intended to neither solve this problem nor propose any forced alignment training method on hierarchical systems. Instead, we want to see the effects of combining a hierarchical system with forced alignments from a phrase-based decoder. The next section will recall phrase-based and hierarchical phrase-based translation models. In Section 3 we describ"
2010.iwslt-papers.11,W99-0604,1,0.633447,"how we combine these forced alignments with hierarchical translation models. An empirical evaluation on two different tasks is done in Section 5. Finally, Section 6 concludes the paper. 2. Translation Models In this work we will study the combination of two widely used approaches to statistical machine translation. The main difference between the two models lies in the basic units that are used for the translation. 2.1. Phrase-based Translation Model The phrase based translation model is based on the concept of phrase, a bilingual pair of sequences of words that are translations of each other [7]. Given a word-aligned training corpus, we extract those phrases for which the source words are aligned only to target words within the phrase and vice-versa. This set can be formalized for a sentence pair (f1J , eI1 ) as P(f1J ,eI1 , A) =  hfjj12 , eii21 i |j1 , j2 , i1 , i2 s.t. ∀(j, i) ∈ A : j1 ≤ j ≤ j2 ⇔ i1 ≤ i ≤ i2 ∧ ∃(j, i) ∈ A : (j1 ≤ j ≤ j2 ∧ i1 ≤ i ≤ i2 ) , (1) where A is the alignment between the source and target sentences expressed as a set of position pairs. 291 Proceedings of the 7th International Workshop on Spoken Language Translation Paris, December 2nd and 3rd, 2010 2.2. Hie"
2010.iwslt-papers.11,P05-1033,0,0.169168,"re aligned only to target words within the phrase and vice-versa. This set can be formalized for a sentence pair (f1J , eI1 ) as P(f1J ,eI1 , A) =  hfjj12 , eii21 i |j1 , j2 , i1 , i2 s.t. ∀(j, i) ∈ A : j1 ≤ j ≤ j2 ⇔ i1 ≤ i ≤ i2 ∧ ∃(j, i) ∈ A : (j1 ≤ j ≤ j2 ∧ i1 ≤ i ≤ i2 ) , (1) where A is the alignment between the source and target sentences expressed as a set of position pairs. 291 Proceedings of the 7th International Workshop on Spoken Language Translation Paris, December 2nd and 3rd, 2010 2.2. Hierarchical Phrase-based Translation Model set of hierarchical rules The hierarchical approach [8] to machine translation is a generalization of the above model where the phrases are allowed to have “gaps”. Those gaps are linked in the source and target language such that a translation rule specifies the location of the translation of the text filling a gap in the source side. The model is formalized as a synchronous context-free grammar. The set of rules extracted from an aligned bilingual sentence pair is best described in a recursive way. Given a source sentence f1J , a target sentence eI1 , an alignment A between them and N the maximum number of gaps allowed (usually N = 2), we can def"
2010.iwslt-papers.11,W06-3119,0,0.0206708,"s relative frequencies. In this work we will study alternative ways to compute these probabilities. α, β ∈ (F ∪ N )? , δ, γ ∈ (E ∪ N )? ∧ ∃j1 , j2 , i1 , i2 : j1 < j2 , i1 < i2 :  2 X → hαfjj12 β, δeii1 γ i ∈ Hn−1 (f1J , eI1 , A)  ∧X → hfjj12 , eii21 i ∈ H0 (f1J , eI1 , A) , The total set of hierarchical phrases extracted from a parallel corpus is the union of the hierarchical phrases extracted from each of its sentences. As can be seen from Equation 4, in the standard approach only one generic non-terminal is used. There are works which propose to extend the set of non-terminals, see e.g. [9]. It is common practice to include two additional rules to the • IBM1-like word-based probabilities computed at the phrase level, also in source-to-target and target-tosource directions. These probabilities can be seen as a smoothing of the afore mentioned phrase-based probabilities. In the case of the hierarchical model, the nonterminals in the rules are simply ignored in the computation. 292 Proceedings of the 7th International Workshop on Spoken Language Translation Paris, December 2nd and 3rd, 2010 • Language model probabilities of the produced translation. • Different penalties. These heu"
2010.iwslt-papers.11,P03-1021,0,0.00786503,"s help in controlling different aspects of the translation. A word penalty can guide the translation process into choosing longer or shorter translations, a penalty for rules not in the set H0 can favour hierarchical rules over lexical phrases, etc. The set of chosen features is dependent of the model used, but they are similar in spirit. • Phrase count features which penalize phrases with low counts. The values for the scaling factors λm are estimated by minimum error rate training, a numerical method that optimizes a measure of translation quality (usually BLEU) on a heldout development set [10]. 3. Forced Alignments The standard phrase extraction procedure defined in Equation 1 requires a word alignment to be provided. Normally this alignment is computed using probabilistic models, usually the word-based IBM translation models [11] as implemented in the GIZA++ toolkit [12], which are different to the ones later used in the translation process. This produces a mismatch between the phrase extraction procedure and the translation procedure, as they are based on different stochastic models, which are applied independently of each other. Other approaches have tried to bridge this mismatc"
2010.iwslt-papers.11,J93-2003,0,0.02008,"s, etc. The set of chosen features is dependent of the model used, but they are similar in spirit. • Phrase count features which penalize phrases with low counts. The values for the scaling factors λm are estimated by minimum error rate training, a numerical method that optimizes a measure of translation quality (usually BLEU) on a heldout development set [10]. 3. Forced Alignments The standard phrase extraction procedure defined in Equation 1 requires a word alignment to be provided. Normally this alignment is computed using probabilistic models, usually the word-based IBM translation models [11] as implemented in the GIZA++ toolkit [12], which are different to the ones later used in the translation process. This produces a mismatch between the phrase extraction procedure and the translation procedure, as they are based on different stochastic models, which are applied independently of each other. Other approaches have tried to bridge this mismatch between training and decoding, e.g. [1, 2, 3]. Recently, consistent improvements in translation quality could be achieved [4]. In this work we investigate a first approach to incorporate the techniques proposed in [4] in the hierarchical ph"
2010.iwslt-papers.11,2008.iwslt-papers.8,1,0.904736,"Missing"
2010.iwslt-papers.11,W10-1738,1,0.892031,"Missing"
2010.iwslt-papers.11,J03-1002,1,\N,Missing
2010.iwslt-papers.11,D08-1076,0,\N,Missing
2010.iwslt-papers.18,N04-1035,0,\N,Missing
2010.iwslt-papers.18,E99-1010,0,\N,Missing
2010.iwslt-papers.18,P01-1067,0,\N,Missing
2010.iwslt-papers.18,N09-1027,0,\N,Missing
2010.iwslt-papers.18,N09-1025,0,\N,Missing
2010.iwslt-papers.18,P08-1114,0,\N,Missing
2010.iwslt-papers.18,P05-1033,0,\N,Missing
2010.iwslt-papers.18,N03-1017,0,\N,Missing
2010.iwslt-papers.18,P02-1038,1,\N,Missing
2010.iwslt-papers.18,2008.iwslt-papers.7,1,\N,Missing
2010.iwslt-papers.18,D07-1079,0,\N,Missing
2010.iwslt-papers.18,P08-1066,0,\N,Missing
2010.iwslt-papers.18,W06-3119,0,\N,Missing
2010.iwslt-papers.18,J07-2003,0,\N,Missing
2010.iwslt-papers.18,D08-1076,0,\N,Missing
2011.eamt-1.37,2010.amta-papers.7,0,\N,Missing
2011.eamt-1.37,W10-1761,0,\N,Missing
2011.eamt-1.37,E09-1044,0,\N,Missing
2011.eamt-1.37,D09-1022,1,\N,Missing
2011.eamt-1.37,C04-1030,1,\N,Missing
2011.eamt-1.37,P10-1146,0,\N,Missing
2011.eamt-1.37,W10-1738,1,\N,Missing
2011.eamt-1.37,2008.iwslt-papers.6,0,\N,Missing
2011.eamt-1.37,J10-3008,0,\N,Missing
2011.eamt-1.37,2010.iwslt-keynotes.2,0,\N,Missing
2011.eamt-1.37,N09-1027,0,\N,Missing
2011.eamt-1.37,P07-2045,0,\N,Missing
2011.eamt-1.37,P06-1055,0,\N,Missing
2011.eamt-1.37,P05-1033,0,\N,Missing
2011.eamt-1.37,J03-1002,1,\N,Missing
2011.eamt-1.37,W09-0434,0,\N,Missing
2011.eamt-1.37,2009.mtsummit-posters.17,0,\N,Missing
2011.eamt-1.37,2010.amta-papers.33,0,\N,Missing
2011.iwslt-evaluation.13,P07-2045,0,0.00313389,"the perplexity on the test2010 corpus. In spite of its small size, the low perplexity of the TED Talk corpus seems to indicate that it is the better suited for this task. This is not a reliable measure, of course, but it can give an early indication of the similarity of the corpora. As a starting point and in order to create different MT systems to combine, we trained two freely available machine translation systems on some of the available bilingual corpora for the evaluation (driven partly by the running time needed to train a full system from scratch). As phrase-based system we used Moses [4], the current standard toolkit for phrase-based translation. We trained the system with a standard setup, using the dev2010 corpus as development set for minimum error rate training. As a hierarchical phrase-based system we used the Jane toolkit [5], freely available for non-commercial use. The alignments were taken over from the corresponding Moses systems and again a fairly standard setup was used, optimizing on the dev2010 corpus. Both systems used the same language model: a 4-gram language model trained on the monolingual TED data. The results1 of the different baseline setups are summariz"
2011.iwslt-evaluation.13,W10-1738,1,0.842521,"n of the similarity of the corpora. As a starting point and in order to create different MT systems to combine, we trained two freely available machine translation systems on some of the available bilingual corpora for the evaluation (driven partly by the running time needed to train a full system from scratch). As phrase-based system we used Moses [4], the current standard toolkit for phrase-based translation. We trained the system with a standard setup, using the dev2010 corpus as development set for minimum error rate training. As a hierarchical phrase-based system we used the Jane toolkit [5], freely available for non-commercial use. The alignments were taken over from the corresponding Moses systems and again a fairly standard setup was used, optimizing on the dev2010 corpus. Both systems used the same language model: a 4-gram language model trained on the monolingual TED data. The results1 of the different baseline setups are summarized in Table 2. It can be seen that the choice of training 1 The BLEU scores are cases-sensitive and computed used preprocessed references, in the same way as the preprocessing of the original data. As such it may not fully agree with officially calc"
2011.iwslt-evaluation.13,D08-1064,0,0.0141918,"System BLEU BLEU was introduced in [7] and has shown to have a high correlation with human judgement. In spite of its shortcomings [8], it has been considered the standard automatic measure in the development of SMT systems (with new measures being added upon, but not substituting it). Of course, the main problem of using the BLEU score as a feature for sentence selection in a real-life scenario is that we do not have the references available. We overcame this issue by generating a custom set of references for each system, using the other systems as gold translations. This 2 There is evidence [6], that this method does not necessarily produce the best complete hypothesis, but it should be a good enough indicator for our purposes and further discussion (see also 3.2). 99 Documents Systems 20.4 34.1 32.1 26.3 29.9 29.9 33.8 34.3 27.4 33.8 26.8 18.9 30.1 25.6 23.8 27.1 25.1 27.9 28.0 22.8 32.2 23.8 19.3 31.9 26.2 24.3 28.3 28.1 31.6 29.2 24.0 33.9 25.4 20.8 35.5 32.1 25.9 30.2 31.0 34.3 34.0 28.4 34.4 25.0 21.3 33.8 31.0 26.2 30.8 30.7 33.5 32.2 27.4 33.7 26.8 20.6 33.7 30.6 25.5 29.0 29.0 33.9 33.1 27.0 35.0 27.9 17.4 27.8 23.0 20.7 25.1 21.2 26.1 25.2 21.6 29.0 20.3 17.3 28.9 25.1 21.9"
2011.iwslt-evaluation.13,P02-1040,0,0.0903367,"which provides the best translation, using some automatic method akin to text classification. However this strategy showed not to be effective. Table 3 shows an overview of the BLEU scores of each of the documents composing the test2010 corpus, with the cells shaded to provide a more visual overview of the distribution of the scores. It can be observed that the best system is generally the same for most documents, and the difference in BLEU scores is not very large. Indeed, if we generate a new complete hypothesis by selecting the best system for 4.1. Cross System BLEU BLEU was introduced in [7] and has shown to have a high correlation with human judgement. In spite of its shortcomings [8], it has been considered the standard automatic measure in the development of SMT systems (with new measures being added upon, but not substituting it). Of course, the main problem of using the BLEU score as a feature for sentence selection in a real-life scenario is that we do not have the references available. We overcame this issue by generating a custom set of references for each system, using the other systems as gold translations. This 2 There is evidence [6], that this method does not necessa"
2011.iwslt-evaluation.13,W11-2109,1,0.811438,"e source input and the system outputs and computed the source to target ratio for such scores. As an additional feature, we included counts and source to target ratios of verb phrases, given the same isomorphism assumption and the fact that a possible “loss” of verb (not explicitly handled by a language model) would radically decrease sentence quality. Further parsing features are subject of future work. 4.4. IBM1 Scores IBM1-like scores on the sentence level are known to perform well for the rescoring of n-best lists from a single system (see e.g. [17]). Additionally, they have been shown in [18] to correlate well with human judgement for evaluation purposes. We thus include them as additional features. 4.5. Additional Language Models For the translation systems we trained ourselves we only included one language model trained on the TED data, as initial experimentation with other language models did not seem to 5. Sentence Selection Mechanism Two sentence selection mechanisms were tried out for this evaluation. Although our goal is to shift to a selection mechanism geared towards human evaluation, using the data made available in the WMT evaluations [19], this approach is still experi"
2011.iwslt-evaluation.13,W06-3110,0,0.200778,"Missing"
2011.iwslt-evaluation.13,W11-2103,0,0.0127923,"nally, they have been shown in [18] to correlate well with human judgement for evaluation purposes. We thus include them as additional features. 4.5. Additional Language Models For the translation systems we trained ourselves we only included one language model trained on the TED data, as initial experimentation with other language models did not seem to 5. Sentence Selection Mechanism Two sentence selection mechanisms were tried out for this evaluation. Although our goal is to shift to a selection mechanism geared towards human evaluation, using the data made available in the WMT evaluations [19], this approach is still experimental and in development stage. Therefore we also built a more traditional system based on log-linear models trained on the BLEU score. 5.1. Based on BLEU Log-linear models are at the heart of most state-of-the-art statistical machine translation systems. They model the translation probability of target sentence eI1 given source sentence f1J directly using the expression P  M J I exp λ h (f , e ) m m 1 1 m=1 P , (1) p(eI1 |f1J ) = P M J I) exp λ h (f , e ˜ I m m 1 1 e˜ m=1 1 where the hm are feature functions as the ones described in Section 4 and the λm are"
2011.iwslt-evaluation.13,P07-2026,0,0.328387,"Missing"
2011.iwslt-evaluation.13,2008.amta-srw.3,0,0.0623263,"tion systems. They model the translation probability of target sentence eI1 given source sentence f1J directly using the expression P  M J I exp λ h (f , e ) m m 1 1 m=1 P , (1) p(eI1 |f1J ) = P M J I) exp λ h (f , e ˜ I m m 1 1 e˜ m=1 1 where the hm are feature functions as the ones described in Section 4 and the λm are the corresponding scaling factors, which we optimize with standard MERT training with respect to the BLEU score. This is also the usual approach used for rescoring n-best lists generated by a single system, and has been used previously for sentence selection purposes (see [20] which uses a very similar approach to our own). Note that no system dependent features like translation probabilities were computed, as we wanted to keep the system general. In fact, for the system combination task, only the single-best translation was provided, without additional information. Table 5 gives an overview of the effect of the different features used in this approach.5 It can be seen that the best performance is obtained when combining all the models. Language model scores alone are not powerful enough to give an improvement over the best single system, and the IBM1 scores even h"
2011.iwslt-evaluation.13,C04-1072,0,0.20734,"Missing"
2011.iwslt-evaluation.13,vilar-etal-2006-error,1,0.914641,"Missing"
2011.iwslt-evaluation.13,W11-2104,1,0.847176,"in this case (not negative!), this indicates that higher IBM1 scores are beneficial for the system. For the language models, the picture is mixed, the system tries to maximize the probability of some of them, but to minimize it for others. 5.2. Based on human ranking We considered employing a supervised machine learning approach trained over sentences evaluated by human judges, made available by the WMT evaluations. The system was trained based on human rankings of MT output and consequently used to replicate ranking for our sentence-level translation alternatives. According to this approach [21], ranking is decomposed into a set of pairwise decisions, where each translation output gets compared with each one of the other alternatives. For this purpose, a binary classifier is trained to learn to comFeature Sign Cross system BLEU (system level) Cross system BLEU (sentence level) Word penalty − − − EXTer hINFer hLEXer hRer MISer WER − − + + + + IBM1 − LM (Europarl) LM (Giga FrEn) LM (monolingual TED) LM (UN) LM (News) − − + + + Table 6: Sign of the scaling factors corresponding to the features. pare the output quality. The sentence that wins most of the pairwise comparisons (ranked firs"
2011.iwslt-evaluation.13,W08-0309,0,0.0621747,"Missing"
2011.iwslt-evaluation.13,P06-1055,0,0.0117871,"ng systems, optimizing the output for the highest probability of the consequent n-grams. On the other hand, automatic metrics are also based on n-grams matching with the reference translation. In order to avoid a possible overfitting on n-grams, but also to capture more complex phenomena (such as long distance structures and grammatical fluency) that are still important to quality output and may have been neglected by the statistical systems, we considered including features derived after parsing the systems’ output with Probabilistic Context Free Grammars (PCFG). For this the Berkeley Parser [16] was used. PCFG parsing allows the generation of n-best lists of trees, scored probabilistically, leading to the selection of the tree with the highest score. From this process, we extracted the number of distinct parsing trees of the sentence, after having allowed the generation of an n-best list of size n = 1000. A smaller number of trees could mean that there are less possible tree derivations, i.e. less parsing ambiguity. Parsing statistics are not only an indicator of the grammaticality of the sentence, but also of how complicated it is, assuming it is fully grammatical. Therefore, we rel"
2011.iwslt-evaluation.13,N07-2015,0,0.109556,"mmatical). For this reason, we parsed both the source input and the system outputs and computed the source to target ratio for such scores. As an additional feature, we included counts and source to target ratios of verb phrases, given the same isomorphism assumption and the fact that a possible “loss” of verb (not explicitly handled by a language model) would radically decrease sentence quality. Further parsing features are subject of future work. 4.4. IBM1 Scores IBM1-like scores on the sentence level are known to perform well for the rescoring of n-best lists from a single system (see e.g. [17]). Additionally, they have been shown in [18] to correlate well with human judgement for evaluation purposes. We thus include them as additional features. 4.5. Additional Language Models For the translation systems we trained ourselves we only included one language model trained on the TED data, as initial experimentation with other language models did not seem to 5. Sentence Selection Mechanism Two sentence selection mechanisms were tried out for this evaluation. Although our goal is to shift to a selection mechanism geared towards human evaluation, using the data made available in the WMT ev"
2011.iwslt-evaluation.13,W10-1703,0,0.0767624,"Missing"
2011.iwslt-evaluation.13,E06-1032,0,\N,Missing
2011.iwslt-evaluation.13,J11-4002,1,\N,Missing
2011.iwslt-evaluation.13,W09-0401,0,\N,Missing
2011.iwslt-evaluation.13,2011.iwslt-evaluation.1,0,\N,Missing
2012.eamt-1.14,W05-0820,0,0.0173523,"ngly the economic feasibility of MT and the fitness for real-world needs of professional translators and Language Service Providers (LSPs) have been hardly analysed so far. The MT community tries to broaden the domains the translation systems are applied to. In the early years, research on statistical machine translation concentrated on restricted domains, the touristic domain being a typical example. As the quality of the translations got better, the difficulty of the task was increased by moving to richer domains. The WMT evaluations are another example of this trend. In the first editions (Koehn and Monz, 2005) the data the systems were trained and evaluated on consisted only of the proceedings of the European Parliament. In more recent editions (Callison-Burch et al., 2011) the (parallel) training data still mostly consists of europarl data, but the evaluation has moved to the news domain, with a much wider variety of topics. 2 From TM to MT: The LSP’s starting point A LSP always has to keep a good balance between prices, linguistic quality, and time, all for the benefit of the client. Especially in the area of trainc 2012 European Association for Machine Translation. 73 ing material the price pres"
2012.eamt-1.14,moore-2002-fast,0,0.122004,"Missing"
2012.eamt-1.14,W10-1738,1,0.881638,"Missing"
2012.eamt-1.14,2002.tmi-tutorials.2,0,0.0980944,"Missing"
2013.mtsummit-posters.4,2010.iwslt-papers.1,0,0.0149183,"h means that the verb read needs a NP playing the role of the subject to its left to constitute a full sentence S. The same verb read is assigned a different supertag (SNP)/NP in the sentence he reads a book. The supertag (SNP)/NP denotes a transitive verb which needs a NP to its left playing the role of the subject and a NP to its right playing the role of the object in order to constitute a full sentence S. 4 4.1 Our Approach Motivation CCG has many unique qualities which made it an attractive grammar formalism to be incorporated into SMT systems (Hassan et al., 2007; Hassan et al., 2009; Almaghout et al., 2010; Almaghout et al., 2012) . These qualities can also be exploited in building a CCG-based QE metric which evaluates the grammaticality of the translation output. First, CCG allows for flexible structures thanks to its combinatory rules. Thus, it is possible to assign a CCG category to phrases which do not represent standard syntactic constituents. This is an important feature for SMT systems as SMT phrases are statistically extracted, and do not necessarily correspond to syntactic constituents. This same feature can also be used to detect grammatical chunks in the translation output, which hel"
2013.mtsummit-posters.4,2012.eamt-1.44,0,0.0167737,"ad needs a NP playing the role of the subject to its left to constitute a full sentence S. The same verb read is assigned a different supertag (SNP)/NP in the sentence he reads a book. The supertag (SNP)/NP denotes a transitive verb which needs a NP to its left playing the role of the subject and a NP to its right playing the role of the object in order to constitute a full sentence S. 4 4.1 Our Approach Motivation CCG has many unique qualities which made it an attractive grammar formalism to be incorporated into SMT systems (Hassan et al., 2007; Hassan et al., 2009; Almaghout et al., 2010; Almaghout et al., 2012) . These qualities can also be exploited in building a CCG-based QE metric which evaluates the grammaticality of the translation output. First, CCG allows for flexible structures thanks to its combinatory rules. Thus, it is possible to assign a CCG category to phrases which do not represent standard syntactic constituents. This is an important feature for SMT systems as SMT phrases are statistically extracted, and do not necessarily correspond to syntactic constituents. This same feature can also be used to detect grammatical chunks in the translation output, which helps 225 to estimate its gr"
2013.mtsummit-posters.4,W11-2104,1,0.915377,"to improve their performance. Xiong et al. (2010) build a QE metric based on a Maximum Entropy classifier in which they integrate linguistic and lexical features to predict the correctness of each word in the translation output. Linguistic features are based on Link Grammar, which parses a sentence by pairing its words. They hypothesise that words which the parser fails to link to other words are likely to be grammatically incorrect. They demonstrate that linguistic features help to improve performance over lexical features and further improvement is gained when these two types are combined. Avramidis et al. (2011) propose PCFG parsingbased QE features which represent the following information extracted from PCFG parse trees of the source and target sentences: • • • • Best parse tree log likelihood. Number of n-best trees. Confidence for the best parse tree. Average confidence of all trees. Avramidis et al. (2011) demonstrate that these parsing-based features are able to achieve better correlation than non-linguistic-based features. Specia et al. (2011) propose a set of QE features to predict the adequacy of translation. The features include the following syntactic features extracted from source and tar"
2013.mtsummit-posters.4,J99-2004,0,0.0441188,"al., 2012). Some of these features compare syntactic structures between source and target sentences whereas other features focus on detecting common grammatical errors committed by SMT systems. They show that the linguistic features alone were not able to outperform the baseline system. However, they show that following a proper selection procedure for linguistic features helps to boost their performance over the baseline system. 224 3 Combinatory Categorial Grammar CCG (Steedman, 2000) is a grammar formalism which consists of a lexicon that pairs words with lexical categories (supertags, cf. Bangalore and Joshi (1999)) and a set of combinatory rules which specify how the categories are combined. A supertag is a rich syntactic description that specifies the local syntactic context of the word at the lexical level in the form of a set of arguments. CCG builds a parse tree for a sentence by combining CCG categories using a set of binary combinatory rules. Most of the CCG grammar is contained in the lexicon, which is why CCG has simpler rules compared to CFG productions. CCG categories are divided into atomic and complex categories. Examples of atomic categories are S (sentence), N (noun), NP (noun phrase), et"
2013.mtsummit-posters.4,C04-1046,0,0.0459933,"syntactic categories, we were able to extract grammaticality QE features based on recognising grammatical chunks and examining sequences of CCG categories in the translation output. We also tackle the problem of parsing ungrammatical output by restricting the coverage of the CCG parser. The rest of this paper is organised as follows. Section 2 reviews related work. Section 3 provides an introduction to CCG. Section 4 describes our approach. Section 5 presents our experiments. Finally, Section 6 concludes and provides avenues for future work. 2 Related Work The first QE models were proposed by Blatz et al. (2004). They use data labeled with automatic MT metrics to learn QE models based on features extracted from the input and output sentences. Specia et al. (2009) add to the features proposed by Blatz et al. (2004) a set of features divided into “black-box” features i.e. MT system independent features and “glass-box” features i.e. features which use internal information from the MT system. They use training data annotated by both NIST and human annotation. Using grammaticality features in QE has been demonstrated to improve their performance. Xiong et al. (2010) build a QE metric based on a Maximum En"
2013.mtsummit-posters.4,W10-1703,0,0.0690335,"Missing"
2013.mtsummit-posters.4,W12-3102,0,0.0576051,"Missing"
2013.mtsummit-posters.4,W12-3110,0,0.0115413,"et al. (2012) extract a set of syntaxbased QE features originally developed to judge the grammaticality of sentences. Some syntactic features compare POS n-gram frequencies between the output sentence and a reference corpus. The features also include parsing features extracted from parse trees built using precision grammar, which is originally developed to detect grammatical errors. Other parsing-based features rely on information produced by parsers trained on well-formed and malformed sentences which result from introducing grammatical errors in the treebank on which the parser is trained. Felice and Specia (2012) compare the performance of a set of linguistic features extracted from source and target sentences constituency and dependency trees with the baseline system of the WMT 2012 evaluation campaign (Callison-Burch et al., 2012). Some of these features compare syntactic structures between source and target sentences whereas other features focus on detecting common grammatical errors committed by SMT systems. They show that the linguistic features alone were not able to outperform the baseline system. However, they show that following a proper selection procedure for linguistic features helps to bo"
2013.mtsummit-posters.4,2011.eamt-1.32,0,0.0415548,"Missing"
2013.mtsummit-posters.4,P07-1037,0,0.0606055,"Missing"
2013.mtsummit-posters.4,D09-1123,0,0.0510684,"Missing"
2013.mtsummit-posters.4,W12-3117,0,0.0190571,"features. Specia et al. (2011) propose a set of QE features to predict the adequacy of translation. The features include the following syntactic features extracted from source and target dependency and constituency parse trees: • Proportion of dependency relations with aligned constituents between source and target sentences. • The same previous feature but with the order of constituents ignored. • The same as the first feature but with Giza threshold equals to 0.1. • Absolute difference between the depth of the syntactic tree for the source and the depth of the syntactic tree for the target. Rubino et al. (2012) extract a set of syntaxbased QE features originally developed to judge the grammaticality of sentences. Some syntactic features compare POS n-gram frequencies between the output sentence and a reference corpus. The features also include parsing features extracted from parse trees built using precision grammar, which is originally developed to detect grammatical errors. Other parsing-based features rely on information produced by parsers trained on well-formed and malformed sentences which result from introducing grammatical errors in the treebank on which the parser is trained. Felice and Spe"
2013.mtsummit-posters.4,2006.amta-papers.25,0,0.126803,"Missing"
2013.mtsummit-posters.4,2010.jec-1.5,0,0.0451371,"Missing"
2013.mtsummit-posters.4,2009.mtsummit-papers.16,0,0.0118631,"s in the translation output. We also tackle the problem of parsing ungrammatical output by restricting the coverage of the CCG parser. The rest of this paper is organised as follows. Section 2 reviews related work. Section 3 provides an introduction to CCG. Section 4 describes our approach. Section 5 presents our experiments. Finally, Section 6 concludes and provides avenues for future work. 2 Related Work The first QE models were proposed by Blatz et al. (2004). They use data labeled with automatic MT metrics to learn QE models based on features extracted from the input and output sentences. Specia et al. (2009) add to the features proposed by Blatz et al. (2004) a set of features divided into “black-box” features i.e. MT system independent features and “glass-box” features i.e. features which use internal information from the MT system. They use training data annotated by both NIST and human annotation. Using grammaticality features in QE has been demonstrated to improve their performance. Xiong et al. (2010) build a QE metric based on a Maximum Entropy classifier in which they integrate linguistic and lexical features to predict the correctness of each word in the translation output. Linguistic fea"
2013.mtsummit-posters.4,2011.mtsummit-papers.58,0,0.160181,"trate that linguistic features help to improve performance over lexical features and further improvement is gained when these two types are combined. Avramidis et al. (2011) propose PCFG parsingbased QE features which represent the following information extracted from PCFG parse trees of the source and target sentences: • • • • Best parse tree log likelihood. Number of n-best trees. Confidence for the best parse tree. Average confidence of all trees. Avramidis et al. (2011) demonstrate that these parsing-based features are able to achieve better correlation than non-linguistic-based features. Specia et al. (2011) propose a set of QE features to predict the adequacy of translation. The features include the following syntactic features extracted from source and target dependency and constituency parse trees: • Proportion of dependency relations with aligned constituents between source and target sentences. • The same previous feature but with the order of constituents ignored. • The same as the first feature but with Giza threshold equals to 0.1. • Absolute difference between the depth of the syntactic tree for the source and the depth of the syntactic tree for the target. Rubino et al. (2012) extract a"
2013.mtsummit-posters.4,2011.eamt-1.12,0,0.0120636,"uk translation and sometimes from internal translation information output by the MT system. With the improvement of the quality of MT systems and their increasing use in real-world applications, MT QE has become increasingly more important. QE has been demonstrated to help in making the integration of MT systems in the translation pipeline more efficient. For example, using QE to filter out low-quality translations from the post-editing process has been shown to help in reducing post-editing time as low-quality translations might take more time to post-edit than to be translated from scratch (Specia, 2011). Furthermore, QE helps to enhance MT user experience by informing the user of the predicted quality of the translation produced by the MT system. Moreover, QE has been more and more used to enhance the quality of MT systems by integrating QE scores in n-best reranking and combining the translation of different MT systems. QE features estimate the quality of the translation by capturing the aspects which evaluate translation quality, namely fluency and adequacy, in addition to predicting the difficulty of the translation. Adequacy refers to the extent to which the meaning of the source sentenc"
2013.mtsummit-posters.4,P10-1062,0,0.0244835,"rk The first QE models were proposed by Blatz et al. (2004). They use data labeled with automatic MT metrics to learn QE models based on features extracted from the input and output sentences. Specia et al. (2009) add to the features proposed by Blatz et al. (2004) a set of features divided into “black-box” features i.e. MT system independent features and “glass-box” features i.e. features which use internal information from the MT system. They use training data annotated by both NIST and human annotation. Using grammaticality features in QE has been demonstrated to improve their performance. Xiong et al. (2010) build a QE metric based on a Maximum Entropy classifier in which they integrate linguistic and lexical features to predict the correctness of each word in the translation output. Linguistic features are based on Link Grammar, which parses a sentence by pairing its words. They hypothesise that words which the parser fails to link to other words are likely to be grammatically incorrect. They demonstrate that linguistic features help to improve performance over lexical features and further improvement is gained when these two types are combined. Avramidis et al. (2011) propose PCFG parsingbased"
2013.mtsummit-posters.5,2003.mtsummit-systems.1,0,0.0537752,"ence and the translation output. Moses (Koehn et al., 2007): a phrase-based statistical machine translation (SMT) system trained on news texts and technical documentation (no client data were available for training). Ranking: for each source sentence, rank the outputs of five different MT systems (Trados is excluded, explanation below) according to how well these preserve the meaning of the source sentence. Ties were allowed. Jane (Vilar et al., 2010): a hierarchical phrasebased SMT system trained on news texts and technical documentation (no client data were available for training). Lucy MT (Alonso and Thurmair, 2003): a commercial rule-based machine translation (RBMT) system with sophisticated handwritten transfer and generation rules adapted to domains by importing domain-specific terminology. RBMT: Another widely used commercial rulebased machine translation system whose name is not mentioned here.1 Google Translate2 : a web-based machine translation engine also based on statistical approach. Since this system is known as one of the best general purpose MT engines, it has been included in order to allow us to assess the performance level of our SMT system and also to compare it directly with other MT ap"
2013.mtsummit-posters.5,W10-1703,0,0.0302962,"number of translators working on it. 2.1 Translation systems used The evaluated translation outputs presented in this work are produced by German-English, GermanFrench and German-Spanish machine translation Sima’an, K., Forcada, M.L., Grasmick, D., Depraetere, H., Way, A. (eds.) Proceedings of the XIV Machine Translation Summit (Nice, September 2–6, 2013), p. 231–238. c 2013 The authors. This article is licensed under a Creative Commons 3.0 licence, no derivative works, attribution, CC-BY-ND. systems in both directions. The test sets consist of three domains: news texts taken from WMT tasks (Callison-Burch et al., 2010), technical documentation extracted from the freely available OpenOffice project (Tiedemann, 2009) and client data owned by project partners. The following translation systems were considered: the defined sentence-level evaluation tasks using the browser-based evaluation tool Appraise (Federmann, 2010). The reference translation was not shown in any task, only the source sentence and the translation output. Moses (Koehn et al., 2007): a phrase-based statistical machine translation (SMT) system trained on news texts and technical documentation (no client data were available for training). Ranki"
2013.mtsummit-posters.5,W12-3102,0,0.0648542,"Missing"
2013.mtsummit-posters.5,vilar-etal-2006-error,1,0.893094,"Missing"
2013.mtsummit-posters.5,W10-1738,1,0.850961,"level evaluation tasks using the browser-based evaluation tool Appraise (Federmann, 2010). The reference translation was not shown in any task, only the source sentence and the translation output. Moses (Koehn et al., 2007): a phrase-based statistical machine translation (SMT) system trained on news texts and technical documentation (no client data were available for training). Ranking: for each source sentence, rank the outputs of five different MT systems (Trados is excluded, explanation below) according to how well these preserve the meaning of the source sentence. Ties were allowed. Jane (Vilar et al., 2010): a hierarchical phrasebased SMT system trained on news texts and technical documentation (no client data were available for training). Lucy MT (Alonso and Thurmair, 2003): a commercial rule-based machine translation (RBMT) system with sophisticated handwritten transfer and generation rules adapted to domains by importing domain-specific terminology. RBMT: Another widely used commercial rulebased machine translation system whose name is not mentioned here.1 Google Translate2 : a web-based machine translation engine also based on statistical approach. Since this system is known as one of the be"
2013.mtsummit-posters.5,federmann-2010-appraise,0,0.0730217,"nslation Summit (Nice, September 2–6, 2013), p. 231–238. c 2013 The authors. This article is licensed under a Creative Commons 3.0 licence, no derivative works, attribution, CC-BY-ND. systems in both directions. The test sets consist of three domains: news texts taken from WMT tasks (Callison-Burch et al., 2010), technical documentation extracted from the freely available OpenOffice project (Tiedemann, 2009) and client data owned by project partners. The following translation systems were considered: the defined sentence-level evaluation tasks using the browser-based evaluation tool Appraise (Federmann, 2010). The reference translation was not shown in any task, only the source sentence and the translation output. Moses (Koehn et al., 2007): a phrase-based statistical machine translation (SMT) system trained on news texts and technical documentation (no client data were available for training). Ranking: for each source sentence, rank the outputs of five different MT systems (Trados is excluded, explanation below) according to how well these preserve the meaning of the source sentence. Ties were allowed. Jane (Vilar et al., 2010): a hierarchical phrasebased SMT system trained on news texts and tech"
2013.mtsummit-posters.5,P07-2045,0,0.0038879,"licence, no derivative works, attribution, CC-BY-ND. systems in both directions. The test sets consist of three domains: news texts taken from WMT tasks (Callison-Burch et al., 2010), technical documentation extracted from the freely available OpenOffice project (Tiedemann, 2009) and client data owned by project partners. The following translation systems were considered: the defined sentence-level evaluation tasks using the browser-based evaluation tool Appraise (Federmann, 2010). The reference translation was not shown in any task, only the source sentence and the translation output. Moses (Koehn et al., 2007): a phrase-based statistical machine translation (SMT) system trained on news texts and technical documentation (no client data were available for training). Ranking: for each source sentence, rank the outputs of five different MT systems (Trados is excluded, explanation below) according to how well these preserve the meaning of the source sentence. Ties were allowed. Jane (Vilar et al., 2010): a hierarchical phrasebased SMT system trained on news texts and technical documentation (no client data were available for training). Lucy MT (Alonso and Thurmair, 2003): a commercial rule-based machine"
2013.mtsummit-posters.5,W12-3123,0,0.0364801,"ent of MT quality and applicability. 1 Introduction and related work A widely used practice for MT evaluation is ranking outputs of different machine translation systems by human annotators, e.g. in WMT shared tasks (Callison-Burch et al., 2012). While this is an important step towards an understanding of their quality, it does not provide enough scientific insights. In the last years, human error analysis is often carried out in order to better understand some phenomena (Vilar et al., 2006), and recently more and more attention is paid to various aspects of post-editing effort (Specia, 2011; Koponen, 2012). However, to the best of our knowledge, no study has been carried out yet which puts all these aspects together. This paper describes the results of detailed human evaluation covering all three aspects: ranking, error classification and post-editing. The approach arises from the need to detach MT evaluation from a pure research-oriented development scenario and to bring it closer to the end users. Therefore, evaluation has been performed in close co-operation with translation industry. All evaluation tasks have been performed by qualified professional translators. The evaluation process has b"
2013.mtsummit-posters.5,2011.eamt-1.12,0,0.0194594,"rther improvement of MT quality and applicability. 1 Introduction and related work A widely used practice for MT evaluation is ranking outputs of different machine translation systems by human annotators, e.g. in WMT shared tasks (Callison-Burch et al., 2012). While this is an important step towards an understanding of their quality, it does not provide enough scientific insights. In the last years, human error analysis is often carried out in order to better understand some phenomena (Vilar et al., 2006), and recently more and more attention is paid to various aspects of post-editing effort (Specia, 2011; Koponen, 2012). However, to the best of our knowledge, no study has been carried out yet which puts all these aspects together. This paper describes the results of detailed human evaluation covering all three aspects: ranking, error classification and post-editing. The approach arises from the need to detach MT evaluation from a pure research-oriented development scenario and to bring it closer to the end users. Therefore, evaluation has been performed in close co-operation with translation industry. All evaluation tasks have been performed by qualified professional translators. The evaluati"
2013.mtsummit-wptp.2,2003.mtsummit-systems.1,0,0.0711019,"1 OpenOffice 418 414 412 414 413 412 2483 Client 500 548 382 0 1028 0 2458 Total 2706 1476 1706 2158 1542 2264 11852 rank Overall News OpenOffice Client de-en de-es de-fr en-de es-de fr-de Table 1: Test sets for ranking task and selecting for post-edit task – number of source sentences per language pair and domain. source sentences per language pair and domain can be seen in Table 4. Four translation systems were used: a phrasebased statistical machine translation (SMT) system Moses (Koehn et al., 2007), a hierarchical SMT system Jane (Vilar et al., 2010), a commercial rule-based system Lucy (Alonso and Thurmair, 2003), and another commercial rule-based system RBMT1 . The translation outputs generated by the described systems were then given to professional translators in order to perform ranking and post-editing using the browser-based evaluation tool Appraise (Federmann, 2010). Ranking and post-editing tasks were defined as follows: Ranking: for each source sentence (11852 sentences in total), rank the outputs of four different MT systems according to how well these preserve the meaning of the source sentence. Ties were allowed. Select and post-edit: for each source sentence (11852 sentences in total), se"
2013.mtsummit-wptp.2,federmann-2010-appraise,0,0.0122073,"e sentences per language pair and domain. source sentences per language pair and domain can be seen in Table 4. Four translation systems were used: a phrasebased statistical machine translation (SMT) system Moses (Koehn et al., 2007), a hierarchical SMT system Jane (Vilar et al., 2010), a commercial rule-based system Lucy (Alonso and Thurmair, 2003), and another commercial rule-based system RBMT1 . The translation outputs generated by the described systems were then given to professional translators in order to perform ranking and post-editing using the browser-based evaluation tool Appraise (Federmann, 2010). Ranking and post-editing tasks were defined as follows: Ranking: for each source sentence (11852 sentences in total), rank the outputs of four different MT systems according to how well these preserve the meaning of the source sentence. Ties were allowed. Select and post-edit: for each source sentence (11852 sentences in total), select the translation output which is easiest to post-edit and perform the editing. Post-edit all: for each source sentence in the selected subset (4070 sentences in total), postedit all four produced translation outputs. For both post-editing tasks, the translators"
2013.mtsummit-wptp.2,P10-1064,0,0.032193,"Missing"
2013.mtsummit-wptp.2,W12-3123,0,0.0120168,"proved considerably in recent years thus gaining recognition in the translation industry. However, machine translation outputs have not yet reached the same quality as human translations. Performing the post-editing has become a common practice for improving machine translation outputs. Therefore, more and more attention is paid to various aspects of postediting, such as (Specia, 2011). Prediction of errors in rule-based system outputs has been investigated in (Valotkaite and Asadullah, 2012) in order to facilitate the post-editing process. Analysis of edit operations has been carried out in (Koponen, 2012) in order to understand discrepances between edit distance and translation quality (i.e. predicted post-editing effort). Our work explores the selection criteria applied by professional translators when several translation outputs of each source sentence are offered for post-editing. The scenario is similar to the one in (He et al., 2010), but our approach goes beyond, since they consider only two outputs (one produced by statistical machine translation system and other by translation memory), they do not examine ranking of these outputs, they have not tested their automatic method by professi"
2013.mtsummit-wptp.2,2011.eamt-1.12,0,0.0122675,", five types of performed edit operations are analysed: correcting word form, reordering, adding missing words, deleting extra words and correcting lexical choice. 1 Motivation and related work Machine translation (MT) has improved considerably in recent years thus gaining recognition in the translation industry. However, machine translation outputs have not yet reached the same quality as human translations. Performing the post-editing has become a common practice for improving machine translation outputs. Therefore, more and more attention is paid to various aspects of postediting, such as (Specia, 2011). Prediction of errors in rule-based system outputs has been investigated in (Valotkaite and Asadullah, 2012) in order to facilitate the post-editing process. Analysis of edit operations has been carried out in (Koponen, 2012) in order to understand discrepances between edit distance and translation quality (i.e. predicted post-editing effort). Our work explores the selection criteria applied by professional translators when several translation outputs of each source sentence are offered for post-editing. The scenario is similar to the one in (He et al., 2010), but our approach goes beyond, si"
2013.mtsummit-wptp.2,2012.amta-wptp.9,0,0.0175217,"dding missing words, deleting extra words and correcting lexical choice. 1 Motivation and related work Machine translation (MT) has improved considerably in recent years thus gaining recognition in the translation industry. However, machine translation outputs have not yet reached the same quality as human translations. Performing the post-editing has become a common practice for improving machine translation outputs. Therefore, more and more attention is paid to various aspects of postediting, such as (Specia, 2011). Prediction of errors in rule-based system outputs has been investigated in (Valotkaite and Asadullah, 2012) in order to facilitate the post-editing process. Analysis of edit operations has been carried out in (Koponen, 2012) in order to understand discrepances between edit distance and translation quality (i.e. predicted post-editing effort). Our work explores the selection criteria applied by professional translators when several translation outputs of each source sentence are offered for post-editing. The scenario is similar to the one in (He et al., 2010), but our approach goes beyond, since they consider only two outputs (one produced by statistical machine translation system and other by trans"
2013.mtsummit-wptp.2,W10-1738,1,0.708534,"n-de es-de fr-de Total News 1788 514 912 1744 101 1852 6911 OpenOffice 418 414 412 414 413 412 2483 Client 500 548 382 0 1028 0 2458 Total 2706 1476 1706 2158 1542 2264 11852 rank Overall News OpenOffice Client de-en de-es de-fr en-de es-de fr-de Table 1: Test sets for ranking task and selecting for post-edit task – number of source sentences per language pair and domain. source sentences per language pair and domain can be seen in Table 4. Four translation systems were used: a phrasebased statistical machine translation (SMT) system Moses (Koehn et al., 2007), a hierarchical SMT system Jane (Vilar et al., 2010), a commercial rule-based system Lucy (Alonso and Thurmair, 2003), and another commercial rule-based system RBMT1 . The translation outputs generated by the described systems were then given to professional translators in order to perform ranking and post-editing using the browser-based evaluation tool Appraise (Federmann, 2010). Ranking and post-editing tasks were defined as follows: Ranking: for each source sentence (11852 sentences in total), rank the outputs of four different MT systems according to how well these preserve the meaning of the source sentence. Ties were allowed. Select and p"
2020.amta-research.10,D17-1300,0,0.0203578,"e 112 Baseline (fp32) Quantized (int8) CPUs 1 2 4 1 2 4 Time (s) 1260.8 841.6 575.8 511.6 435.9 334.3 6:6 Layers Tok/Sec 33.3 49.9 73.0 82.1 96.4 125.7 BLEU 22.1 22.1 22.1 22.0 22.0 22.0 Time (s) 585.0 404.7 283.2 285.7 242.3 173.0 20:2 Layers Tok/Sec BLEU 71.8 23.0 103.8 23.0 148.3 23.0 147.0 22.8 173.4 22.8 242.9 22.8 Table 3: CPU decoding times and SacreBLEU (Post, 2018) scores for FI-EN newstest2019 with and without 8-bit quantization for both standard (6:6 layer) and deep encoder (20:2 layer) transformer models as described in §3. Models use a vocabulary selection shortlist of 200 items (Devlin, 2017) and translate one sentence at a time (batch size of 1). Benchmarks are run on an EC2 c5.12xlarge instance (Cascade Lake processor) and limited to using 1, 2, or 4 CPU cores. Ott et al. (2018) Plateau-Reduce DE–EN BLEU Time 34.7 30h 34.9 28h EN–FI BLEU Time 20.1 14h 20.7 12h Table 4: SacreBLEU (Post, 2018) scores (newstest2019) and training times (8 NVIDIA V100 GPUs) for a 20 encoder 2 decoder layer transformer using the training setup described by Ott et al. (2018) and plateau-reduce, both implemented in Sockeye 2. also require additional computation per update (synchronizing data across dist"
2020.amta-research.10,E17-3017,0,0.0214239,"amazon.com Amazon Kenneth Heafield∗ translate@kheafield.com Efficient Translation Limited Abstract We present Sockeye 2, a modernized and streamlined version of the Sockeye neural machine translation (NMT) toolkit. New features include a simplified code base through the use of MXNet’s Gluon API, a focus on state of the art model architectures, distributed mixed precision training, and efficient CPU decoding with 8-bit quantization. These improvements result in faster training and inference, higher automatic metric scores, and a shorter path from research to production. 1 Introduction Sockeye (Hieber et al., 2017) is a versatile toolkit for research in the fast-moving field of NMT. Since the initial release, it has been used in at least 25 scientific publications, including winning submissions to WMT evaluations (Schamper et al., 2018). Sockeye also powers Amazon Translate, showing industrial-strength performance in addition to the flexibility needed in academic environments. Moreover, we are excited to see that hardware manufacturers are contributing to optimizing MXNet (Chen et al., 2015) and Sockeye for speed. Intel has demonstrated large performance gains for Sockeye inference on Intel Skylake proc"
2020.amta-research.10,W18-6301,0,0.0794504,"ime (s) 585.0 404.7 283.2 285.7 242.3 173.0 20:2 Layers Tok/Sec BLEU 71.8 23.0 103.8 23.0 148.3 23.0 147.0 22.8 173.4 22.8 242.9 22.8 Table 3: CPU decoding times and SacreBLEU (Post, 2018) scores for FI-EN newstest2019 with and without 8-bit quantization for both standard (6:6 layer) and deep encoder (20:2 layer) transformer models as described in §3. Models use a vocabulary selection shortlist of 200 items (Devlin, 2017) and translate one sentence at a time (batch size of 1). Benchmarks are run on an EC2 c5.12xlarge instance (Cascade Lake processor) and limited to using 1, 2, or 4 CPU cores. Ott et al. (2018) Plateau-Reduce DE–EN BLEU Time 34.7 30h 34.9 28h EN–FI BLEU Time 20.1 14h 20.7 12h Table 4: SacreBLEU (Post, 2018) scores (newstest2019) and training times (8 NVIDIA V100 GPUs) for a 20 encoder 2 decoder layer transformer using the training setup described by Ott et al. (2018) and plateau-reduce, both implemented in Sockeye 2. also require additional computation per update (synchronizing data across distributed GPUs and checking reduced precision operations for overflow). This overhead can be amortized by significantly increasing the effective batch size; gradients are aggregated per-GPU for"
2020.amta-research.10,W18-6319,0,0.226328,"ne in an external advisory capacity. 1 https://www.intel.ai/amazing-inference-performance-with-intel-xeonscalable-processors/#gs.wrgsji 2 https://mxnet.apache.org/versions/1.6/api/python/docs/api/gluon/index.html Proceedings of the 14th Conference of the Association for Machine Translation in the Americas October 6 - 9, 2020, Volume 1: MT Research Track Page 110 DE–EN EN–DE FI–EN EN–FI Layers BLEU Latency BLEU Latency BLEU Latency BLEU Latency 6:6 35.5 602 37.9 791 22.2 575 20.5 808 10:10 35.4 970 37.8 1238 22.3 863 20.8 1258 34.8 293 37.6 357 23.2 257 20.9 368 20:2 Table 1: SacreBLEU scores (Post, 2018) and single-sentence latency in milliseconds on newstest2019 for models trained on WMT19 constrained data with varying numbers of encoder and decoder layers. Latency values are the 90th percentile of translation time when translating each sentence individually (no batching). We measure single sentence decoding latency on an EC2 c5.2xlarge instance with 4 CPU cores. We report the average over three independent training runs. by converting them into computation graphs for maximum performance. Adopting this programming model significantly simplifies Sockeye 2’s training and inference code, reduci"
2020.amta-research.10,N18-3014,0,0.014874,"talizing the first character of each word. We compare a baseline model that was trained on cased input (no source factors) against all “SF-*” methods. The factored models also use BPE type factors as introduced by Sennrich and Haddow (2016). Models use the 20:2 transformer architecture and training settings described in §3. Shown in Table 2, encoding case information with source factors is an effective way to improve robustness against case variation with the two versions of “SF-case” performing best. 3.2 Quantization for Inference Sockeye 2 now supports 8-bit quantized matrix multiplication (Quinn and Ballesteros, 2018) on CPUs based on the intgemm library.3 By scaling values such that 127 corresponds to the maximum absolute value found in a tensor, matrix multiplication can be conducted with 8-bit integer representations in place of the default 32-bit floating-point representations without significant degradation of overall model accuracy. Parameters can either be quantized offline and stored in a smaller model file or quantized on the fly at loading time. Activations are quantized on the fly while other operators that consume far less runtime remain as 32-bit floats. Latency-sensitive applications typicall"
2020.amta-research.10,W18-6426,0,0.0174914,"res include a simplified code base through the use of MXNet’s Gluon API, a focus on state of the art model architectures, distributed mixed precision training, and efficient CPU decoding with 8-bit quantization. These improvements result in faster training and inference, higher automatic metric scores, and a shorter path from research to production. 1 Introduction Sockeye (Hieber et al., 2017) is a versatile toolkit for research in the fast-moving field of NMT. Since the initial release, it has been used in at least 25 scientific publications, including winning submissions to WMT evaluations (Schamper et al., 2018). Sockeye also powers Amazon Translate, showing industrial-strength performance in addition to the flexibility needed in academic environments. Moreover, we are excited to see that hardware manufacturers are contributing to optimizing MXNet (Chen et al., 2015) and Sockeye for speed. Intel has demonstrated large performance gains for Sockeye inference on Intel Skylake processors.1 NVIDIA is working on significant performance improvements for Sockeye’s Transformer (Vaswani et al., 2017) implementation through fused operators and an optimized beam search. This paper discusses Sockeye 2’s streamli"
2020.amta-research.10,W16-2209,0,0.0639044,"y the number of decoder layers. For WMT19 FI-EN and EN-FI benchmarks (Barrault et al., 2019), the 20:2 model outperforms both the 6:6 model and the 10:10 model in terms of BLEU. The 20:2 model also has roughly half the decoding latency of the 6:6 model and roughly one third the latency of the 10:10 model. The relative efficiency of encoder versus decoder layers can be attributed to (1) the ability to parallelize across input tokens, (2) attention to only input tokens, and (3) not needing to run beam search on the source side. 3.1 Source Factors Sockeye supports source factors in the spirit of Sennrich and Haddow (2016), additional representations that are combined with word embeddings prior to the first encoder layer. In Sockeye 2, we improve source factor support by allowing different types of embedding combinations (concatenation, summation, or average), as well as weight sharing between source factor and word embeddings. As an example application, we use source factors to represent input case. Variations in case pose a challenge for machine translation systems as different orthographic variations are considered to be independent by the translation model (e.g., “case” is different from “Case” and both are"
2020.eamt-1.50,W19-5301,0,0.0345574,"Missing"
2020.eamt-1.50,E17-3017,0,0.0251598,"e Translation Felix Hieber and Tobias Domhan and Michael Denkowski and David Vilar Amazon {fhieber,domhant,mdenkows,dvilar}@amazon.com Abstract We present S OCKEYE 2, a modernized and streamlined version of the S OCKEYE neural machine translation (NMT) toolkit. New features include a simplified code base through the use of MXNet’s Gluon API, a focus on state-of-the-art model architectures, and distributed mixed precision training. These improvements result in faster training and inference, higher automatic metric scores, and a shorter path from research to production. 1 Introduction S OCKEYE (Hieber et al., 2017) is a versatile toolkit for research in the fast-moving field of NMT. Since the initial release, it has been used in at least 25 scientific publications, including winning submissions to WMT evaluations (Schamper et al., 2018). Based on the deep learning library MXNet (Chen et al., 2015), S OCKEYE also powers Amazon Translate, showing industrialstrength performance in addition to the flexibility needed in academic environments. Moreover, we are excited to see hardware manufacturers contributing optimizations to MXNet and S OCKEYE. Intel has demonstrated large performance gains for S OCKEYE inf"
2020.eamt-1.50,W18-6301,0,0.0197869,"ing different types of embedding combinations (concatenation or summation). 4 Training Improvements S OCKEYE 2 significantly accelerates training with Horovod2 integration (Sergeev and Balso, 2018) and MXNet’s automatic mixed precision (AMP). Horovod extends synchronous training to any number of GPUs (including across nodes) while AMP automatically detects and converts parts of the model that can run in reduced-precision mode (FP16) without loss of quality. S OCKEYE also provides a data-driven alternative to the popular “inverse square root” learning schedule used by Vaswani et al. (2017) and Ott et al. (2018): “Plateau Reduce” keeps the same learning rate until validation perplexity does not increase for several checkpoints, at which time it reduces the learning rate and rewinds all model and optimizer parameters to the best previous point. Training concludes when validation perplexity reaches an extended plateau. In a WMT19 benchmark (Barrault et al., 2019), Plateau Reduce training produces stronger models in slightly less time than the setup described by Ott et al. (2018). Results are presented in Table 1 where all values are averages over 3 independent training runs with different random initia"
2020.eamt-1.50,W18-6319,0,0.0988464,"Missing"
2020.eamt-1.50,W18-6426,0,0.0281692,"nt types of embedding combinations (concatenation or summation). 4 Training Improvements S OCKEYE 2 significantly accelerates training with Horovod2 integration (Sergeev and Balso, 2018) and MXNet’s automatic mixed precision (AMP). Horovod extends synchronous training to any number of GPUs (including across nodes) while AMP automatically detects and converts parts of the model that can run in reduced-precision mode (FP16) without loss of quality. S OCKEYE also provides a data-driven alternative to the popular “inverse square root” learning schedule used by Vaswani et al. (2017) and Ott et al. (2018): “Plateau Reduce” keeps the same learning rate until validation perplexity does not increase for several checkpoints, at which time it reduces the learning rate and rewinds all model and optimizer parameters to the best previous point. Training concludes when validation perplexity reaches an extended plateau. In a WMT19 benchmark (Barrault et al., 2019), Plateau Reduce training produces stronger models in slightly less time than the setup described by Ott et al. (2018). Results are presented in Table 1 where all values are averages over 3 independent training runs with different random initia"
2021.emnlp-main.535,W19-5206,0,0.0383424,"Missing"
2021.emnlp-main.535,2021.naacl-main.297,0,0.0169142,"studied. fine-tuning stage. An additional desideratum for systems enabling attribute control is how efficiently they can be re1 Introduction alized. For deployed MT engines, (re-)training a Some modern machine translation (MT) applica- model for every attribute is unrealistic, due to the tions require fine-grained control along multiple associated costs in time and computational power. attributes, and such mechanisms also increase the Therefore, having a light-weight intervention, mausers’ trust in scenarios when the system speaks terialized as a small number of tunable parameon their behalf (Prabhumoye et al., 2021). For ters, would considerably improve the practicality example, MT applications like video subtitling in of attribute-enabled systems. streaming, video conferencing, online education In this paper we introduce additive vector-valued and speech MT require that one can control the interventions which allow for fine-grained, combinlength and monotonicity of the translation, setting able and fine-tunable control of translations, adclear constraints on the output. In open-domain dressing all of the points above. We propose two ∗ Google AI Resident. implementations of vector-valued control: 1) one"
2021.emnlp-main.535,N13-1073,0,0.0131874,"continuous feature representation (Pd vs. Pc ). The discrete feature uses a separate embedding for each politeness level i, i.e. we train a different Vpi vector for each politeness level. For the continuous feature we fix the weights wpi of the different levels, and the system trains a single politeness embedding vector Vp . Monotonicity (M0.1 ) We understand monotonicity as the closeness of the word order in the target sentence to the word order in the source sentence. We formally define monotonicity as the strength of the off-diagonal alignment deviations, inspired by the fast_align model (Dyer et al., 2013). For a translated pair s = (sinput , starget ) and an alignment {(i, j)} between the token positions i ∈ {1, · · · , n} of the input sentence sinput and j ∈ {1, · · · , m} of the target sentence starget , we define the deviation strength: X i 1 j δ(s) = (1) n − m , #{(i, j)} We additionally considered a modification of the approach described in the previous section where {(i,j)} the shifts zt + V are only accessible by the last N layers of the decoder, see Figure 1. For ex- where #{(i, j)} denotes the cardinality of the ample, the first decoder layers have access to the alignment. In the comp"
2021.emnlp-main.535,L18-1182,0,0.013732,"nts on fine-tuning additive control models from pretrained baseline models that were trained without any attribute annotation. For reproducibility, we include setup details in §A.4. 4.1 Datasets and baselines For EN ⇒ DE we trained on the WMT17 dataset, using newstest2016 as the development set and newstest2017 as the test set (Bojar et al., 2017). In order to test the behaviour on an out-of-domain setting, where the distribution of the controlled attribute may vary from the training data, we also evaluate our methods on a subset of OpenSubtitles. For EN ⇒ JA we trained and evaluated on JESC (Pryzant et al., 2018). All the reported results use SacreBLEU (Post, 2018)1 . 4.2 Model configuration and training We reimplemented the standard Transformer architecture (Vaswani et al., 2017) in JAX (Bradbury et al., 2018), using the neural network library Flax (Heek et al., 2020). All our models correspond to the Base Transformer configuration (Vaswani et al., 2017). For training our additive models we label the whole corpus with the corresponding attributes and use the standard cross-entropy loss. However, to encourage the additive model to learn to produce good translations in the Neutral mode, we randomly mas"
2021.emnlp-main.535,D19-5203,0,0.0223101,"Missing"
2021.emnlp-main.535,2021.acl-long.293,0,0.0883643,"Missing"
2021.emnlp-main.535,N16-1005,0,0.348623,"whenever there backs: continuous values must be binned into is an ambiguity, we enable users’ agency”. discrete categories, which is unnatural for cerA standard method to exert control over MT tain applications; interference between multioutputs is the tagging approach, where an explicit ple tags is poorly understood. We address token is prepended to the source sentence or outthese problems by introducing vector-valued put hypothesis to signal the desired attribute of the interventions which allow for fine-grained control over multiple attributes simultaneously via output (Kobus et al., 2017; Sennrich et al., 2016; a weighted linear combination of the correJohnson et al., 2017). While such tags do enable sponding vectors. For some attributes, our certain level of control, discrete tags, by their naapproach even allows for fine-tuning a model ture, allow only for coarse-grained control and retrained without annotations to support such quire that attributes with continuous values, like interventions. In experiments with three atmonotonicity or length ratio, are binned. For extributes (length, politeness and monotonicity) ample, Lakew et al. (2019) used only three tags to and two language pairs (English t"
2021.emnlp-main.535,2021.mtsummit-up.27,0,0.0311541,"1. We propose a novel mechanism to control different translation attributes and evaluate it on three important use cases: length, politeness and monotonicity for translation into German and Japanese (from English). Figure 1: Model diagram. Elaraby et al., 2018); to indicate source domains in multi-domain NMT (Kobus et al., 2017). Closer to our applications, tags can control length (Lakew et al., 2019), and formality of translations from English to German (Sennrich et al., 2016) and to Japanese (Yamagishi et al., 2016; Feely et al., 2019), as well as from French to English (Niu et al., 2018). Stergiadis et al. (2021) experimented with a pair of tags to control domain and provenance. Closely related to controllable generation is work on monolingual style transfer: Krishna et al. (2020), Riley et al. (2021) and Niu et al. (2018). In contrast to these papers, we use a classifier of the target side for labelling the controlling attribute. 3 2. In all the three use cases, the ability to control attributes comes at no cost in translation quality. In fact, including explicit politeness information, the evaluation scores improved as compared to strong baselines (+0.6 BLEU points for German and +2.5 for Japanese)."
2021.emnlp-main.535,P19-1452,0,0.021205,"he embeddings corresponding to the input sequence {xt }t=1...T instead of being added to the encoder representation {zt }t=1...T , which more closely resembles the tagging architecture. This approach however resulted in models with a degraded translation quality for the continuous attributes, and thus we focus the discussion on the additive approach. 3.3 More efficient realization of view, we can use this modification to understand which layers process a specific syntactic / semantic attribute, as an attributes-informed version of layer probing used to analyze Transformer encoder-only models (Tenney et al., 2019). 3.4 Attribute representation In this work we considered three different attributes for control, but the approach can naturally be generalized to other attributes. Length (L) For length control the confounding factor is that longer inputs would generate longer translations. Thus, instead of aiming to control the output length directly, we control the ratio r between the output and input lengths, both computed after tokenization and subword splitting. For this attribute the weight wl corresponds to the ratio r, and the system learns the length control embedding Vl . Politeness (P) Although pol"
2021.emnlp-main.535,kobus-etal-2017-domain,0,0.137843,"21), formulated as “whenever there backs: continuous values must be binned into is an ambiguity, we enable users’ agency”. discrete categories, which is unnatural for cerA standard method to exert control over MT tain applications; interference between multioutputs is the tagging approach, where an explicit ple tags is poorly understood. We address token is prepended to the source sentence or outthese problems by introducing vector-valued put hypothesis to signal the desired attribute of the interventions which allow for fine-grained control over multiple attributes simultaneously via output (Kobus et al., 2017; Sennrich et al., 2016; a weighted linear combination of the correJohnson et al., 2017). While such tags do enable sponding vectors. For some attributes, our certain level of control, discrete tags, by their naapproach even allows for fine-tuning a model ture, allow only for coarse-grained control and retrained without annotations to support such quire that attributes with continuous values, like interventions. In experiments with three atmonotonicity or length ratio, are binned. For extributes (length, politeness and monotonicity) ample, Lakew et al. (2019) used only three tags to and two la"
2021.emnlp-main.535,2020.emnlp-main.55,0,0.0557954,"Missing"
2021.emnlp-main.535,W16-4620,0,0.0219999,"without controllability via fine-tuning of intervention vectors. Our contributions are as follows: 1. We propose a novel mechanism to control different translation attributes and evaluate it on three important use cases: length, politeness and monotonicity for translation into German and Japanese (from English). Figure 1: Model diagram. Elaraby et al., 2018); to indicate source domains in multi-domain NMT (Kobus et al., 2017). Closer to our applications, tags can control length (Lakew et al., 2019), and formality of translations from English to German (Sennrich et al., 2016) and to Japanese (Yamagishi et al., 2016; Feely et al., 2019), as well as from French to English (Niu et al., 2018). Stergiadis et al. (2021) experimented with a pair of tags to control domain and provenance. Closely related to controllable generation is work on monolingual style transfer: Krishna et al. (2020), Riley et al. (2021) and Niu et al. (2018). In contrast to these papers, we use a classifier of the target side for labelling the controlling attribute. 3 2. In all the three use cases, the ability to control attributes comes at no cost in translation quality. In fact, including explicit politeness information, the evaluation"
2021.findings-emnlp.274,2020.wmt-1.66,0,0.0314543,"primary tasks with auxiliary data from related tasks. When understanding languages as tasks (Dong et al., 2015; Firat et al., 2016), one single MT model can be used to translate between a multitude of languages and in particular also between translation pairs that were not in the training set (Johnson et al., 2017; Aharoni et al., 2019). To address the problem of data imbalance Devlin (2019); Arivazhagan et al. (2019) proposed temperature sampling to upsample low-resource languages and downsample higher-resource ones (with τ &gt; 1), that has since turned into the go-to data weighting strategy (Freitag and Firat, 2020; Xue et al., 2020). While it is a convenient solution and often outperforms uniform weighting (τ = 1), it reduces the characteristics of languages to their size and reflects the assumption of a zero-sum game in joint training (Xue et al., 2020), ignoring more complex interactions (Fan et al., 2020; Wang et al., 2021). Our experiments reveal that even very unbalanced and counter-intuitive schedules can lead to improved results across the board thanks to more intricate and automated sampling. Closest to our work are recent approaches to schedule data based on inter-facet gradient similarity (Wa"
2021.findings-emnlp.274,2020.wmt-1.140,0,0.048318,"Missing"
2021.findings-emnlp.274,D11-1033,0,0.0511295,"bandit without CDS and the baseline with CDS on natural ‘rev’ test sets suggests that the bandit could compensate the lack of data filtering. 4.2 Multi-Domain NMT Setup We follow the multi-domain setup by Müller et al. (2020) using the data re-split by Aharoni and Goldberg (2020). By construction it contains in-domain data from five domains and no auxResults All bandit approaches improve over the iliary general-domain data, thus preventing data baseline (Table 2) by around 0.5–0.9 BLEU on avaugmentation with pseudo in-domain data selecerage across test sets (except for pgnorm), but the tion (Axelrod et al., 2011). The goal of this evaluaindividual tendencies vary across reward choices. tion is to improve uniformly on all domains using 2 WMT2020 news translation task. a mixed training set. As in prior work, we use 3193 Base Static Bandit Size (k) Avg 291.2 Med 248.0 IT 467.3 Law 222.9 Koran 18.0 Subs 500.0 Aharoni and Goldberg (2020) Ours Uniform (τ = ∞) Upsampled (τ = 5) Proportional (τ = 1) Inverse Proportional (τ = −1) pg pgnorm loss dev-loss dev-pg dev-pgnorm 40.2 39.42 39.26 38.78 40.40 40.03 40.26 38.78 39.12 39.96 40.56 40.56 53.3 51.65 51.91 51.11 52.83 53.43 53.38 51.76 50.63 50.22 53.35 53.23"
2021.findings-emnlp.274,2020.emnlp-main.5,0,0.0612359,"Missing"
2021.findings-emnlp.274,kocmi-bojar-2017-curriculum,0,0.131522,"ed data crawled from web), through a midstrength variation in multi-domain MT (Farajian 1 et al., 2017; Müller et al., 2020; Pham et al., 2021). Sampling from an annealed and renormalized empirical The nature of data facets and their identity is known distribution over facets f , p(f ) = softmaxf (ln(ˆ p(f ))/τ ). 3190 Findings of the Association for Computational Linguistics: EMNLP 2021, pages 3190–3204 November 7–11, 2021. ©2021 Association for Computational Linguistics presuppose fixed notions of difficulty and come with hand-crafted schedules, often inspired by the human learning process (Kocmi and Bojar, 2017; Zhang et al., 2018; Platanios et al., 2019). Such approaches are brittle in that they may not generalize well across tasks, and there has been evidence that even the reverse of the initially hypothesized order works well (Bengio et al., 2009; Wang et al., 2018; Zhang et al., 2018). This suggests that our human intuitions about difficulty and data succession may not correspond to the optimization process of an NMT system (Li and Gong, 2021). In this paper we argue for automatically learned and adaptive data curricula, where the learning system explicitly chooses a facet at each point in train"
2021.findings-emnlp.274,2020.amta-research.14,0,0.0420363,"ora since (Koehn and Monz, 2006). Such corpora are multi-faceted in nature, consisting of a generally unbalanced mixture of data sources that differ from each other in word distribution, domain or other traits. Examples of such differences could range from strongly heterogeneous data like distinct languages for training multi-lingual systems (Dong et al., 2015; Firat et al., 2016; Arivazhagan et al., 2019) to rather subtle variations in data provenance (e.g. human-generated vs. machineproduced data crawled from web), through a midstrength variation in multi-domain MT (Farajian 1 et al., 2017; Müller et al., 2020; Pham et al., 2021). Sampling from an annealed and renormalized empirical The nature of data facets and their identity is known distribution over facets f , p(f ) = softmaxf (ln(ˆ p(f ))/τ ). 3190 Findings of the Association for Computational Linguistics: EMNLP 2021, pages 3190–3204 November 7–11, 2021. ©2021 Association for Computational Linguistics presuppose fixed notions of difficulty and come with hand-crafted schedules, often inspired by the human learning process (Kocmi and Bojar, 2017; Zhang et al., 2018; Platanios et al., 2019). Such approaches are brittle in that they may not genera"
2021.findings-emnlp.274,2020.amta-research.5,0,0.0210262,"(Bengio et al., 2009; Wang et al., 2018; Zhang et al., 2018), which may be a sign of flawed intuitions. Our proposed solution groups data into facets rather than difficulty levels and reveals counterintuitive but effective schedules. state representations, which reduces memory and time complexities drastically. Bandit learning in MT Multi-armed bandits were used in MT to improve general quality, either from online simulated user feedback (Sokolov et al., 2015, 2016, 2017; Kreutzer et al., 2017, 2018b) or from offline logs (Lawrence et al., 2017; Kreutzer et al., 2018a) for domain adaptation. Naradowsky et al. (2020) applied bandit algorithm to select the best NMT system for a particular translation task, when maintaining of multiple such systems is possible. More generally, RL approaches also seek to improve quality by focusing on more task-informed objectives (Shen et al., 2016) and improved approximations to the NMT policies (Bahdanau et al., 2017). Unlike these approaches, we treat the NMT model as a black box and do not intervene with its inner workings (see Figure 1). Translationese vs. natural MT Toral et al. (2018) have shown that the original language a sentence has been written in has a big impa"
2021.findings-emnlp.274,D18-1103,0,0.0853503,"references of lating from English, and many-to-many (M2M). the bandits. This highlights the difficulty of de- We experiment with M2O translations for the signing a proper schedule manually and prior to diverse and related subsets of the multilintraining using intuition only. Static temperature- gual TED dataset (Qi et al., 2018). The two subsets based sampling yields gains tied to the availability cover 8 languages with very different data sizes, of resources, (e.g. improvements for τ = 1 on selected as pairs of related languages of different the high-resource domains, and τ = −1 on the size (Neubig and Hu, 2018) or a set of diverse lanlow-resource domains, except for τ = 5 which guages from different language families and with gains only for Koran), but they—in contrast to the different scripts (Wang et al., 2020b). There are dynamic bandits—fail to improve on all domains. large discrepancies in the sizes of the subsets for This shows that the additional flexibility of the each language, e.g. be has only 4.5k sentences, bandits to adapt the sampling distribution during while the related ru has 208.4k. This makes it 3194 36 loss pg pgnorm dev-loss dev-pg dev-pgnorm baseline 34 32 50000 100000 150000 2"
2021.findings-emnlp.274,I17-2050,0,0.0134196,"vior of the bandit with facets that are linguistically similar but very differently scaled. For a more data-balanced setup, we experiment with the OPUS100 dataset (Zhang et al., 2020), which contains up to 1M of training examples sampled from the entirety of the OPUS collection of parallel corpora from various domains (Tiedemann and Nygaard, 2004) for 99 languages paired with English, of which 94 come with test sets. As a result, the data has large inter- and intra-facet diversity. For both evaluation scenarios we train SentencePiece models (Kudo and Richardson, 2018) on a re-balanced corpus (Nguyen and Chiang, 2017; Fan et al., 2020)3 to create a vocabulary of 32k tokens, add target language tags and train Transformer base models. We construct a balanced development set by randomly selecting a fixed number of sentences from the languagespecific development sets (500 for TED; 100 for OPUS) to reflect our interest in high quality across all languages. Rewards for the bandit are computed on samples from this balanced dataset. We compare with static uniform sampling distributions (τ = ∞) over facets, and size-proportional (τ = 1) or upsampled (τ = 5) distributions, since they have been reported successful i"
2021.findings-emnlp.274,2021.tacl-1.2,0,0.0429844,"Monz, 2006). Such corpora are multi-faceted in nature, consisting of a generally unbalanced mixture of data sources that differ from each other in word distribution, domain or other traits. Examples of such differences could range from strongly heterogeneous data like distinct languages for training multi-lingual systems (Dong et al., 2015; Firat et al., 2016; Arivazhagan et al., 2019) to rather subtle variations in data provenance (e.g. human-generated vs. machineproduced data crawled from web), through a midstrength variation in multi-domain MT (Farajian 1 et al., 2017; Müller et al., 2020; Pham et al., 2021). Sampling from an annealed and renormalized empirical The nature of data facets and their identity is known distribution over facets f , p(f ) = softmaxf (ln(ˆ p(f ))/τ ). 3190 Findings of the Association for Computational Linguistics: EMNLP 2021, pages 3190–3204 November 7–11, 2021. ©2021 Association for Computational Linguistics presuppose fixed notions of difficulty and come with hand-crafted schedules, often inspired by the human learning process (Kocmi and Bojar, 2017; Zhang et al., 2018; Platanios et al., 2019). Such approaches are brittle in that they may not generalize well across tas"
2021.findings-emnlp.274,P16-1162,0,0.0260475,"Missing"
2021.findings-emnlp.274,P16-1159,0,0.0335203,"ory and time complexities drastically. Bandit learning in MT Multi-armed bandits were used in MT to improve general quality, either from online simulated user feedback (Sokolov et al., 2015, 2016, 2017; Kreutzer et al., 2017, 2018b) or from offline logs (Lawrence et al., 2017; Kreutzer et al., 2018a) for domain adaptation. Naradowsky et al. (2020) applied bandit algorithm to select the best NMT system for a particular translation task, when maintaining of multiple such systems is possible. More generally, RL approaches also seek to improve quality by focusing on more task-informed objectives (Shen et al., 2016) and improved approximations to the NMT policies (Bahdanau et al., 2017). Unlike these approaches, we treat the NMT model as a black box and do not intervene with its inner workings (see Figure 1). Translationese vs. natural MT Toral et al. (2018) have shown that the original language a sentence has been written in has a big impact on translation quality, i.e., translating a sentence originally Learned curricula Apart from (Graves et al., written in the source language is more difficult than 2017), whose curriculum learning bandits we adapt translating (back) a sentence that was originally for"
2021.findings-emnlp.274,W17-4756,1,0.832583,"Missing"
2021.findings-emnlp.274,W18-6314,0,0.317529,"ver facets f , p(f ) = softmaxf (ln(ˆ p(f ))/τ ). 3190 Findings of the Association for Computational Linguistics: EMNLP 2021, pages 3190–3204 November 7–11, 2021. ©2021 Association for Computational Linguistics presuppose fixed notions of difficulty and come with hand-crafted schedules, often inspired by the human learning process (Kocmi and Bojar, 2017; Zhang et al., 2018; Platanios et al., 2019). Such approaches are brittle in that they may not generalize well across tasks, and there has been evidence that even the reverse of the initially hypothesized order works well (Bengio et al., 2009; Wang et al., 2018; Zhang et al., 2018). This suggests that our human intuitions about difficulty and data succession may not correspond to the optimization process of an NMT system (Li and Gong, 2021). In this paper we argue for automatically learned and adaptive data curricula, where the learning system explicitly chooses a facet at each point in training, and does not depend on presupposed schedules. This has three major advantages: First, it relieves system developers from lots of manual work. Second, it can improve quality by ignoring irrelevant, redundant or already learned data. Third, it can directly op"
2021.findings-emnlp.274,2020.acl-main.754,0,0.34504,"e and related subsets of the multilintraining using intuition only. Static temperature- gual TED dataset (Qi et al., 2018). The two subsets based sampling yields gains tied to the availability cover 8 languages with very different data sizes, of resources, (e.g. improvements for τ = 1 on selected as pairs of related languages of different the high-resource domains, and τ = −1 on the size (Neubig and Hu, 2018) or a set of diverse lanlow-resource domains, except for τ = 5 which guages from different language families and with gains only for Koran), but they—in contrast to the different scripts (Wang et al., 2020b). There are dynamic bandits—fail to improve on all domains. large discrepancies in the sizes of the subsets for This shows that the additional flexibility of the each language, e.g. be has only 4.5k sentences, bandits to adapt the sampling distribution during while the related ru has 208.4k. This makes it 3194 36 loss pg pgnorm dev-loss dev-pg dev-pgnorm baseline 34 32 50000 100000 150000 200000 250000 300000 350000 400000 450000 500000 Iterations valuable for testing the behavior of the bandit with facets that are linguistically similar but very differently scaled. For a more data-balanced"
2021.findings-emnlp.274,2020.acl-main.148,0,0.124219,"n all domains. large discrepancies in the sizes of the subsets for This shows that the additional flexibility of the each language, e.g. be has only 4.5k sentences, bandits to adapt the sampling distribution during while the related ru has 208.4k. This makes it 3194 36 loss pg pgnorm dev-loss dev-pg dev-pgnorm baseline 34 32 50000 100000 150000 200000 250000 300000 350000 400000 450000 500000 Iterations valuable for testing the behavior of the bandit with facets that are linguistically similar but very differently scaled. For a more data-balanced setup, we experiment with the OPUS100 dataset (Zhang et al., 2020), which contains up to 1M of training examples sampled from the entirety of the OPUS collection of parallel corpora from various domains (Tiedemann and Nygaard, 2004) for 99 languages paired with English, of which 94 come with test sets. As a result, the data has large inter- and intra-facet diversity. For both evaluation scenarios we train SentencePiece models (Kudo and Richardson, 2018) on a re-balanced corpus (Nguyen and Chiang, 2017; Fan et al., 2020)3 to create a vocabulary of 32k tokens, add target language tags and train Transformer base models. We construct a balanced development set b"
2021.iwslt-1.31,W17-4706,0,0.134285,"es are available. Sennrich et al. (2016) took this algorithm as a starting point, considering characters instead of bytes, and joining them using the same criterion to produce sub-word units (more details can be found in Section 3). One potential problem with this approach is that the objective of the original BPE algorithm differs from the goals for which it is being used for translation, as detailed above. While it is certainly effective for the first objective (reducing the vocabulary size), it is arguable whether it is appropriate for the goal of generating new words (Ataman et al., 2017; Huck et al., 2017; Banerjee and Bhattacharyya, 2018). Intuitively, in order to generate new words, we would expect the sub-word units to have some linguistic meaning, so that a new word can be created 263 Proceedings of the 18th International Conference on Spoken Language Translation, pages 263–275 Bangkok, Thailand (Online), August 5–6, 2021. ©2021 Association for Computational Linguistics beklagen ↓ bek@@ lagen founded optimization criterion also allows us to define a data driven stopping criterion. Our proposed criterion allows to select a nearly optimal number of units using only an intrinsic measure on th"
2021.iwslt-1.31,W04-3250,0,0.607871,"Missing"
2021.iwslt-1.31,P18-1007,0,0.08563,"n TED Talks in five directions, all from/to English, show systematic improvements on Arabic, Turkish, Czech, but not on Italian and German. Banerjee and Bhattacharyya (2018) also use unsupervised morphological units generated by Morfessor (Virpioja et al., 2013) as input for a neural machine translation system and report improvements for low-resource conditions. Macháˇcek et al. (2018) follow a similar approach for translation into Czech on WMT data, but were not able to obtains improvements over the standard BPE approach. An alternative model to BPE which is also widely used was presented by Kudo (2018), which can be considered as an extension of (Schuster and Nakajima, 2012). They show that using a purely statistical approach, they are able to achieve subword units that are better linguistically motivated. Similar to our approach, a probability distribution over the sub-word units is defined with the goal of improving the likelihood over the training data. The strategy for defining the sub-word units differ 264 in his approach and ours. While we start with single characters and expand the units, Kudo (2018) starts with a large set of sub-word units and prunes iteratively until reducing the"
2021.iwslt-1.31,W16-2322,0,0.0431447,"Missing"
2021.iwslt-1.31,P16-1162,0,0.613236,"oal is twofold: On the one hand, sub-word splitting reduces the size of the input and output vocabularies. This is specially important when using neural models, as the size of the input layer is fixed and thus the vocabulary size cannot be dynamically adjusted. On the other hand, it tries to increase the generalization capabilities of the translation model, enabling the system to accept and/or generate new words at translation time by combining previously seen units. The most widespread method used for sub-word splitting in neural machine translation is Byte Pair Encoding (BPE), introduced by Sennrich et al. (2016). Since then, BPE has become a default preprocessing step for many NLP tasks. The BPE extraction algorithm is an adaptation of the algorithm introduced by Gage (1994) for data compression. The main idea of this algorithm is to replace the most frequent pair of bytes found in the input data with a new, unseen byte. The process is repeated until no more byte pairs are repeated or until no free bytes are available. Sennrich et al. (2016) took this algorithm as a starting point, considering characters instead of bytes, and joining them using the same criterion to produce sub-word units (more detai"
2021.iwslt-1.31,P19-1021,0,0.0206795,"n does not specify a criterion for stopping the creation of new symbols. If the algorithm runs for an unlimited time, it will merge all sub-words into the original input vocabulary, which is clearly undesired. In practice, one specifies a fixed number of merges to be carried out, or a threshold frequency and when the considered symbols fall below this value the algorithm is stopped. It is however not clear how to set these hyperparameters, although they can have a drastic effect on translation quality depending on the translation direction, task and amount of data (Denkowski and Neubig, 2017; Sennrich and Zhang, 2019). Furthermore, these hyperparameters are rarely optimized, as evaluating them constitutes a full training-evaluation cycle, which is notoriously costly. In this paper we introduce a new criterion for defining sub-word units that tries to address these shortcomings. We introduce a probability distribution over the units which in turn induces a likelihood function over the corpus which we can optimize. We will show how this statistical approach can guide the extraction process towards more linguistically satisfying units, while still remaining a purely data driven approach. Having a well Related"
avramidis-etal-2012-involving,eisele-chen-2010-multiun,0,\N,Missing
avramidis-etal-2012-involving,federmann-2010-appraise,1,\N,Missing
avramidis-etal-2012-involving,J11-4002,1,\N,Missing
avramidis-etal-2012-involving,P02-1040,0,\N,Missing
avramidis-etal-2012-involving,W10-1738,1,\N,Missing
avramidis-etal-2012-involving,P07-2045,0,\N,Missing
avramidis-etal-2012-involving,W11-2104,1,\N,Missing
avramidis-etal-2012-involving,W10-1703,0,\N,Missing
avramidis-etal-2012-involving,vilar-etal-2006-error,1,\N,Missing
avramidis-etal-2012-involving,W11-2100,0,\N,Missing
avramidis-etal-2012-involving,2011.eamt-1.36,1,\N,Missing
avramidis-etal-2012-involving,2010.amta-papers.27,0,\N,Missing
avramidis-etal-2014-taraxu,federmann-2010-appraise,0,\N,Missing
avramidis-etal-2014-taraxu,avramidis-etal-2012-involving,1,\N,Missing
avramidis-etal-2014-taraxu,W10-1738,1,\N,Missing
avramidis-etal-2014-taraxu,P07-2045,0,\N,Missing
avramidis-etal-2014-taraxu,2013.mtsummit-posters.5,1,\N,Missing
E14-4034,2013.iwslt-evaluation.1,0,0.0314032,"Missing"
E14-4034,N03-1017,0,0.062816,"lities based on collected rule counts. We show the effectiveness of our procedure on the IWSLT German→English and English→French translation tasks. Our results show improvements of up to 1.6 points B LEU. 1 Introduction In state of the art statistical machine translation systems, the translation model is estimated by following heuristic: Given bilingual training data, a word alignment is trained with tools such as GIZA++ (Och and Ney, 2003) or fast align (Dyer et al., 2013). Then, all valid translation pairs are extracted and the translation probabilities are computed as relative frequencies (Koehn et al., 2003). However, this extraction method causes several problems. First, this approach does not consider, whether a translation pair is extracted from a likely alignment or not. Further, during the extraction process, models employed in decoding are not considered. For phrase-based translation, a successful approach addressing these issues is presented in (Wuebker et al., 2010). By applying a phrasebased decoder on the source sentences of the training data and constraining the translations to the corresponding target sentences, k-best segmentations are produced. Then, the phrases used for 2 Hierarchi"
E14-4034,P10-2041,0,0.0570292,"Missing"
E14-4034,W05-1506,0,0.0458876,"cess as we do not have to employ the cube pruning algorithm as described in the previous section. Consequently, forced decoding for hierarchical phrase-based translation is equivalent to synchronous parsing of the training data. Dyer (2010) has described an approach to reduce the average-case run-time of synchronous parsing by splitting one bilingual parse into two successive monolingual parses. We adopt this method and first parse the source sentence and then the target sentence with CYK+. If the given sentence pair has been parsed successfully, we employ a top-down k-best parsing algorithm (Chiang and Huang, 2005) on the resulting hypergraph to find the k-best derivations between the given source and target sentence. In this step, all models of the translation process are The translation probabilities are computed in source-to-target as well as in target-to-source direction. In the translation processes, these probabilities are integrated in the log-linear combination among other models such as a language model, word lexicon models, word and phrase penalty and binary features marking hierarchical phrases, glue rule and rules with non-terminals at the boundaries. The translation process of hierarchical"
E14-4034,J03-1002,1,0.0201937,"he source and target sentences. This is done by synchronous parsing the given sentence pairs. After extracting k-best derivations, we reestimate the translation model probabilities based on collected rule counts. We show the effectiveness of our procedure on the IWSLT German→English and English→French translation tasks. Our results show improvements of up to 1.6 points B LEU. 1 Introduction In state of the art statistical machine translation systems, the translation model is estimated by following heuristic: Given bilingual training data, a word alignment is trained with tools such as GIZA++ (Och and Ney, 2003) or fast align (Dyer et al., 2013). Then, all valid translation pairs are extracted and the translation probabilities are computed as relative frequencies (Koehn et al., 2003). However, this extraction method causes several problems. First, this approach does not consider, whether a translation pair is extracted from a likely alignment or not. Further, during the extraction process, models employed in decoding are not considered. For phrase-based translation, a successful approach addressing these issues is presented in (Wuebker et al., 2010). By applying a phrasebased decoder on the source se"
E14-4034,P05-1033,0,0.105325,"approach does not consider, whether a translation pair is extracted from a likely alignment or not. Further, during the extraction process, models employed in decoding are not considered. For phrase-based translation, a successful approach addressing these issues is presented in (Wuebker et al., 2010). By applying a phrasebased decoder on the source sentences of the training data and constraining the translations to the corresponding target sentences, k-best segmentations are produced. Then, the phrases used for 2 Hierarchical Phrase-based Translation In hierarchical phrase-based translation (Chiang, 2005), discontinuous phrases with “gaps” are allowed. The translation model is formalized as a synchronous context-free grammar (SCFG) 174 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 174–179, c Gothenburg, Sweden, April 26-30 2014. 2014 Association for Computational Linguistics and consists of bilingual rules, which are based on bilingual standard phrases and discontinuous phrases. Each bilingual rule rewrites a generic non-terminal X into a pair of strings f˜ and e˜ with both terminals and non-terminals in both languages X → hf"
E14-4034,P03-1021,0,0.108177,"al X. With these hierarchical phrases we can define the hierarchical rules in the SCFG. The rule probabilities which are in general defined as relative frequencies are computed based on the joint counts C(X → hf˜, e˜i) of a bilingual rule X → hf˜, e˜i C(X → hf˜, e˜i) pH (f˜|˜ e) = P . ˜0 ˜i) f˜0 C(X → hf , e 3 Translation Model Training We propose following pipeline for consistent hierarchical phrase-based training: First we train a word alignment, from which the baseline translation model is extracted as described in the previous section. The log-linear parameter weights are tuned with MERT (Och, 2003) on a development set to produce the baseline system. Next, we perform decoding on the training data. As the translations are constrained to the given target sentences, we name this step forced decoding in the following. Details are given in the next subsection. Given the counts CF D (X → hf˜, e˜i) of the rules, which have been applied in the forced decoding step, the translation probabilities pF D (f˜|˜ e) for the translation model are recomputed: CF D (X → hf˜, e˜i) pF D (f˜|˜ e) = P . ˜0 ˜i) f˜0 CF D (X → hf , e (3) Finally, using the translation model with the reestimated probabilities, we"
E14-4034,J07-2003,0,0.0709671,"ge part of the SCFG. In this work, we perform this step with a modified version of the CYK+ algorithm (Chappelier and Rajman, 1998). The output of this algorithm is a hypergraph, which represents all possible derivations of the input sentence. A derivation represents an application of rules from the grammar to generate the given input sentence. Using the the associated target part of the applied rule, for each derivation a translation can be constructed. In a second step, the language model score is incorporated. Given the hypergraph, this is done with the cube pruning algorithm presented in (Chiang, 2007). 175 included (except for the language model). Further, leave-one-out is applied to counteract overfitting. Note, that the model weights of the baseline system are used to perform forced decoding. Finally, we extract and count the rules which have been applied in the derivations. These counts are used to recompute the translation probabilities. 3.2 Sentences Run. Words Vocabulary It is focusing the translation of TED talks. Bilingual data statistics are given in Table 1. The baseline system was trained on all available bilingual data and used a 4-gram LM with modified KneserNey smoothing (Kne"
E14-4034,2001.mtsummit-papers.68,0,0.0327359,"Missing"
E14-4034,P11-2031,0,0.0541754,"Missing"
E14-4034,2006.amta-papers.25,0,0.0429498,"Missing"
E14-4034,N13-1073,0,0.0282672,"is is done by synchronous parsing the given sentence pairs. After extracting k-best derivations, we reestimate the translation model probabilities based on collected rule counts. We show the effectiveness of our procedure on the IWSLT German→English and English→French translation tasks. Our results show improvements of up to 1.6 points B LEU. 1 Introduction In state of the art statistical machine translation systems, the translation model is estimated by following heuristic: Given bilingual training data, a word alignment is trained with tools such as GIZA++ (Och and Ney, 2003) or fast align (Dyer et al., 2013). Then, all valid translation pairs are extracted and the translation probabilities are computed as relative frequencies (Koehn et al., 2003). However, this extraction method causes several problems. First, this approach does not consider, whether a translation pair is extracted from a likely alignment or not. Further, during the extraction process, models employed in decoding are not considered. For phrase-based translation, a successful approach addressing these issues is presented in (Wuebker et al., 2010). By applying a phrasebased decoder on the source sentences of the training data and c"
E14-4034,N10-1033,0,0.0224717,"detail. Given a sentence pair of the training data, we constrain the translation of the source sentence to produce the corresponding target sentence. For this constrained decoding process, the language model score is constant as the translation is fixed. Hence, the incorporation of the a language model is not needed. This results in a simplification of the decoding process as we do not have to employ the cube pruning algorithm as described in the previous section. Consequently, forced decoding for hierarchical phrase-based translation is equivalent to synchronous parsing of the training data. Dyer (2010) has described an approach to reduce the average-case run-time of synchronous parsing by splitting one bilingual parse into two successive monolingual parses. We adopt this method and first parse the source sentence and then the target sentence with CYK+. If the given sentence pair has been parsed successfully, we employ a top-down k-best parsing algorithm (Chiang and Huang, 2005) on the resulting hypergraph to find the k-best derivations between the given source and target sentence. In this step, all models of the translation process are The translation probabilities are computed in source-to"
E14-4034,E14-2008,1,0.882213,"Missing"
E14-4034,W10-1738,1,0.904678,"Missing"
E14-4034,P02-1040,0,\N,Missing
E14-4034,P10-1049,1,\N,Missing
E14-4034,W13-0804,1,\N,Missing
E14-4034,C12-3061,1,\N,Missing
N07-2035,2006.iwslt-evaluation.18,1,0.890672,"Missing"
N07-2035,2005.iwslt-1.23,1,0.905555,"Missing"
N07-2035,J06-4004,1,0.895476,"Missing"
N07-2035,E06-1005,1,0.799617,"ingual N -gram language model. In the phrase-based model, no monotonicity restriction is imposed on the segmentation and the probabilities are normally estimated simply by relative frequencies. This paper extends the analysis of both systems performed in (Crego et al., 2005a) by additionally performing a manual error analysis of both systems, which were the ones used by UPC and RWTH in the last Tc-Star evaluation. Furthermore, we will propose a way to combine both systems in order to improve the quality of translations. Experiments combining several kinds of MT systems have been presented in (Matusov et al., 2006), based only on the single best output of each system. Recently, a more straightforward approach of both systems has been performed in (Costa-juss` a et al., 2006) which simply selects, for each sentence, one of the provided hypotheses. This paper is organized as follows. In section 2, we briefly describe the phrase and the N -gram-based baseline systems. In the next section we present the evaluation framework. In Section 4 we report a structural comparison performed for both systems and, afterwards, in Section 5, we analyze the errors of both systems. Finally, in the last two sections we resc"
N07-2035,vilar-etal-2006-error,1,0.861806,"Missing"
N07-2035,N04-1033,1,0.812475,"n N -gram-based one. The exhaustive analysis includes a comparison of the translation models in terms of efficiency (number of translation units used in the search and computational time) and an examination of the errors in each system’s output. Additionally, we combine both systems, showing accuracy improvements. 1 Introduction Statistical machine translation (SMT) has evolved from the initial word-based translation models to more advanced models that take the context surrounding the words into account. The so-called phrase-based and N -gram-based models are two examples of these approaches (Zens and Ney, 2004; Mari˜ no et al., 2006). In current state-of-the-art SMT systems, the phrase-based or the N -gram-based models are usually the main features in a log-linear framework, reminiscent of the maximum entropy modeling approach. Two basic issues differentiate the N -gram-based system from the phrase-based one: the training data is sequentially segmented into bilingual units; and the probability of these units is estimated as a bilingual N -gram language model. In the phrase-based model, no monotonicity restriction is imposed on the segmentation and the probabilities are normally estimated simply by"
N07-2035,W06-3120,1,\N,Missing
N18-1119,D17-1098,0,0.373576,"and hat die Absicht, eine Mauer zu bauen. “No one has the intention, a wall to build.” The end-to-end nature of neural machine translation (NMT) removes many ways of manually guiding the translation process that were available in older paradigms. Recent work, however, has introduced a new capability: lexically constrained or guided decoding, a modification to beam search that forces the inclusion of pre-specified words and phrases in the output. However, while theoretically sound, existing approaches have computational complexities that are either linear (Hokamp and Liu, 2017) or exponential (Anderson et al., 2017) in the number of constraints. We present an algorithm for lexically constrained decoding with a complexity of O(1) in the number of constraints. We demonstrate the algorithm’s remarkable ability to properly place these constraints, and use it to explore the shaky relationship between model and BLEU scores. Our implementation is available as part of S OCKEYE. 1 “errichten” “Keiner” “Keiner” ""errichten” Niemand hat die Absicht, eine Mauer zu errichten. “No one has the intention, a wall to construct.” Keiner hat die Absicht, eine Mauer zu bauen. “No one has the intention, a wall to build.” Keine"
N18-1119,D16-1162,0,0.355416,"on One appeal of the phrase-based statistical approach to machine translation (Koehn et al., 2003) was that it provided control over system output. For example, it was relatively easy to incorporate domain-specific dictionaries, or to force a translation choice for certain words. These kinds of interventions were useful in a range of settings, including interactive machine translation or domain adaptation. In the new paradigm of neural machine translation (NMT), these kinds of manual interventions are much more difficult, and a lot of time has been spent investigating how to restore them (cf. Arthur et al. (2016)). At the same time, NMT has also provided new capabilities. One interesting recent innovation is lexically constrained decoding, a modification to beam search that allows the user to specify words and phrases that must appear in the system output (Figure 1). Two algorithms have been proposed for this: grid beam search (Hokamp and Liu, 2017, GBS) and constrained beam search (Anderson et al., 2017, CBS). These papers showed that these algorithms do a good job automatically placing constraints and improving results in tasks such as simulated post-editing, domain adaptation, and caption generatio"
N18-1119,P17-1141,0,0.334689,"the intention of building a wall. Niemand hat die Absicht, eine Mauer zu bauen. “No one has the intention, a wall to build.” The end-to-end nature of neural machine translation (NMT) removes many ways of manually guiding the translation process that were available in older paradigms. Recent work, however, has introduced a new capability: lexically constrained or guided decoding, a modification to beam search that forces the inclusion of pre-specified words and phrases in the output. However, while theoretically sound, existing approaches have computational complexities that are either linear (Hokamp and Liu, 2017) or exponential (Anderson et al., 2017) in the number of constraints. We present an algorithm for lexically constrained decoding with a complexity of O(1) in the number of constraints. We demonstrate the algorithm’s remarkable ability to properly place these constraints, and use it to explore the shaky relationship between model and BLEU scores. Our implementation is available as part of S OCKEYE. 1 “errichten” “Keiner” “Keiner” ""errichten” Niemand hat die Absicht, eine Mauer zu errichten. “No one has the intention, a wall to construct.” Keiner hat die Absicht, eine Mauer zu bauen. “No one has"
N18-1119,D07-1103,0,0.0934441,"Missing"
N18-1119,2016.amta-researchers.9,0,0.041654,"imum timestep, but then fills the lower beam with garbage until forced to stop. 7 Related Work Hokamp and Liu (2017) was novel in that it allowed the specification of arbitrary target-side words as hard constraints, implemented entirely as a restructuring of beam search, and without reference to the source. A related approach was that of Anderson et al. (2017), who extended beam search with a finite state machine whose states marked completed subsets of the set of constraints, at an exponential cost in the number of constraints. Lexically-constrained decoding also generalizes prefix decoding (Knowles and Koehn, 2016; Wuebker et al., 2016), since the hsi symbol can easily be included as the first word of a constraint. Our work here has not explored where to get lexical constraints, but considering that question naturally brings to mind attempts to improve NMT by using lexicons and phrase tables (Arthur et al., 2016; Tang et al., 2016). Finally, another approach which shares the hard-decision made by lexically constrained decoding is the placeholder approach (Crego et al., 2016), wherein identifiable elements in the input are transformed to masks during preprocessing, and then replaced with their original"
N18-1119,W17-3204,0,0.147066,"ide confidence that DBA is intelligently placing the constraints. Reference Aversion The inference procedure in S OCKEYE maximizes the length-normalized version of the sentence’s log probability. While there is no explicit training towards the metric, BLEU, modeling in machine translation assumes that better model scores correlate with better BLEU scores. However, a general repeated observation from the NMT literature is the disconnect between model score and BLEU score. For example, work has shown that opening up the beam to let the decoder find better hypotheses results in lower BLEU score (Koehn and Knowles, 2017), even as the model score rises. The phenomenon is not well understood, but it seems that NMT models have learned to travel a path straight towards their goal; as soon as they get off this path, they get lost, and can no longer function (Ott et al., 2018). Another way to look at this problem is to ask what the neural model thinks of the references. Scoring against complete references is easy with NMT (Sennrich, 2017), but lexically-constrained decoding allows us to investigate this in finergrained detail by including just portions of the references. We observe that forcing the decoder to inclu"
N18-1119,N03-1017,0,0.0216164,"Missing"
N18-1119,L16-1147,0,0.0358489,"Missing"
N18-1119,P14-5010,0,0.00574844,"Missing"
N18-1119,E17-2060,0,0.0163954,"disconnect between model score and BLEU score. For example, work has shown that opening up the beam to let the decoder find better hypotheses results in lower BLEU score (Koehn and Knowles, 2017), even as the model score rises. The phenomenon is not well understood, but it seems that NMT models have learned to travel a path straight towards their goal; as soon as they get off this path, they get lost, and can no longer function (Ott et al., 2018). Another way to look at this problem is to ask what the neural model thinks of the references. Scoring against complete references is easy with NMT (Sennrich, 2017), but lexically-constrained decoding allows us to investigate this in finergrained detail by including just portions of the references. We observe that forcing the decoder to include even a single word from the reference imposes a cost in model score that is inversely 1 3 24.5 25.1 25.3 24.7 23.7 33.5 5 24.5 25.2 25.6 24.9 23.9 33.5 10 24.4 25.6 26.1 25.7 24.6 34.0 20 24.5 25.5 26.7 26.9 26.0 35.0 30 24.4 25.3 26.4 27.2 26.9 35.9 Table 3: BLEU scores decoding with a beam size of 10. Runtimes for unpruned systems (column 0) are nearly twice those of the other columns. But it is only at large th"
N18-1119,P16-1162,0,0.681699,". Inputs: max output length N , beam size k. Output: highest-scoring hypothesis. 1: 2: Table 1: Complexity of decoding (sentence length N , beam size k, and constraint count C) with target-side constraints under various approaches. banks (similar in spirit to the grouping of hypotheses into stacks for phrase-based decoding (Koehn et al., 2003)) and dynamically dividing a fixed-size beam across these banks at each time step. As a result, the algorithm scales easily to large constraint sets that can be created when words and phrases are expanded, for example, by sub-word processing such as BPE (Sennrich et al., 2016). We compare it to GBS and demonstrate empirically that it is significantly faster, making constrained decoding with an arbitrary number of constraints feasible with GPU-based inference. We also use the algorithm to study beam search interactions between model and metric scores, beam size, and pruning. 2 Beam Search and Grid Beam Search Inference in statistical machine translation seeks to find the output sequence, yˆ, that maximizes the probability of a function parameterized by a model, θ, and an input sequence, x: yˆ = argmaxy∈Y pθ (y |x) The space of possible translations, Y, is the set of"
N18-1119,P16-1007,0,0.0556527,"ills the lower beam with garbage until forced to stop. 7 Related Work Hokamp and Liu (2017) was novel in that it allowed the specification of arbitrary target-side words as hard constraints, implemented entirely as a restructuring of beam search, and without reference to the source. A related approach was that of Anderson et al. (2017), who extended beam search with a finite state machine whose states marked completed subsets of the set of constraints, at an exponential cost in the number of constraints. Lexically-constrained decoding also generalizes prefix decoding (Knowles and Koehn, 2016; Wuebker et al., 2016), since the hsi symbol can easily be included as the first word of a constraint. Our work here has not explored where to get lexical constraints, but considering that question naturally brings to mind attempts to improve NMT by using lexicons and phrase tables (Arthur et al., 2016; Tang et al., 2016). Finally, another approach which shares the hard-decision made by lexically constrained decoding is the placeholder approach (Crego et al., 2016), wherein identifiable elements in the input are transformed to masks during preprocessing, and then replaced with their original sourcelanguage strings"
N18-2080,P13-1141,0,0.0250016,"dampened (values close to 0) for units that are not as important. The approach is illustrated in Figure 1. Coming back to the sentiment neuron example above, for tasks where sentiment is important (product reviews), LHUC will assign a high weight to the sentiment neuron, while for other tasks (news), the corresponding weight will be low. The weights are learned automatically from the available in-domain data. 4 Domain Adaptation for NMT Domain adaptation for phrase-based and related approaches to machine translation has been extensively investigated, e.g. (Schwenk, 2008; Axelrod et al., 2011; Carpuat et al., 2013). Neural machine translation, being a relatively new approach to MT, has not seen so many works going into this direction yet. Previous methods can of course still be applied as long as they are modelindependent, e.g. data selection methods as shown 1 The value of 2 is chosen as to be able to amplify the value (it can be doubled) but without overshadowing the value of the other units, which could happen with bigger values. 501 in (van der Wees et al., 2017) or self-training approaches like (Bertoldi and Federico, 2009), of which back-translation (Sennrich et al., 2015) can be considered a spec"
N18-2080,D11-1033,0,0.0872606,"evant to the task, or dampened (values close to 0) for units that are not as important. The approach is illustrated in Figure 1. Coming back to the sentiment neuron example above, for tasks where sentiment is important (product reviews), LHUC will assign a high weight to the sentiment neuron, while for other tasks (news), the corresponding weight will be low. The weights are learned automatically from the available in-domain data. 4 Domain Adaptation for NMT Domain adaptation for phrase-based and related approaches to machine translation has been extensively investigated, e.g. (Schwenk, 2008; Axelrod et al., 2011; Carpuat et al., 2013). Neural machine translation, being a relatively new approach to MT, has not seen so many works going into this direction yet. Previous methods can of course still be applied as long as they are modelindependent, e.g. data selection methods as shown 1 The value of 2 is chosen as to be able to amplify the value (it can be doubled) but without overshadowing the value of the other units, which could happen with bigger values. 501 in (van der Wees et al., 2017) or self-training approaches like (Bertoldi and Federico, 2009), of which back-translation (Sennrich et al., 2015) c"
N18-2080,P16-1009,0,0.17862,"Missing"
N18-2080,W17-3205,0,0.0173946,"s like (Bertoldi and Federico, 2009), of which back-translation (Sennrich et al., 2015) can be considered a special case. Specific for neural machine translation, Freitag and Al-Onaizan (2016) present a really simple method, where the parameters of an already trained system are taken as the starting point of another training run using only in-domain data. This simple method achieves good results and has the advantage that no additional implementation work has to be carried out. Chu et al. (2017) propose a refinement of this method where the in-domain data is mixed with the out-of-domain data. Chen et al. (2017) propose to use a domain classifier to weight the cost of the training data differently according to the similarity to the in-domain data, in what can be considered a tighter integration of previous data-selection methods. Sennrich et al. (2016) introduce a simple method for controlling the politeness of an NMT system, which can also be used for domain adaptation. They add a special tag to the source sentence denoting the politeness level of the source sentence and the system is able to use this information to improve the translation output. This technique can be extended to domain adaptation"
N18-2080,N16-1005,0,0.180349,"ters of an already trained system are taken as the starting point of another training run using only in-domain data. This simple method achieves good results and has the advantage that no additional implementation work has to be carried out. Chu et al. (2017) propose a refinement of this method where the in-domain data is mixed with the out-of-domain data. Chen et al. (2017) propose to use a domain classifier to weight the cost of the training data differently according to the similarity to the in-domain data, in what can be considered a tighter integration of previous data-selection methods. Sennrich et al. (2016) introduce a simple method for controlling the politeness of an NMT system, which can also be used for domain adaptation. They add a special tag to the source sentence denoting the politeness level of the source sentence and the system is able to use this information to improve the translation output. This technique can be extended to domain adaptation by combining the text of the different domains and marking them with a specific, domain-dependent tag. Johnson et al. (2016) use this technique in an “extreme” domain adaptation setting, where the domains are actually different languages. 5 Corp"
N18-2080,P17-2061,0,0.179822,"Missing"
N18-2080,D17-1147,0,0.0415905,"Missing"
N18-2080,E17-3017,0,0.0407636,"Adding labels to the training data also proves to be an efficient domain adaptation method, with the advantage that it can be combined with the other methods. The improvements due to continuation of training or LHUC are not as big in this case. For reference, the results of training on indomain data only have also been included. bedding layer has a dimension of 512. Training has been performed with the Adam algorithm (Kingma and Ba, 2014). The provided TED dev set was used as stopping criterion (or newstest14 for the case of a WMT-only system). Experiments have been carried out using Sockeye (Hieber et al., 2017), and the LHUC code has been open sourced as part of it. For LHUC experiments, both the encoder and decoder hidden units have been expanded with the additional scaling. As discussed in Section 5 we will differentiate two conditions: in the “full training data” condition, both the out-of-domain and in-domain data are available for training the initial system. In the “growing training data” condition, the initial system is trained only on out-of-domain data. We will compare the performance of the LHUC method with the “continuation of training” proposed by Freitag and Al-Onaizan (2016). Both meth"
N18-2080,2008.iwslt-papers.6,0,0.0436708,"at are more relevant to the task, or dampened (values close to 0) for units that are not as important. The approach is illustrated in Figure 1. Coming back to the sentiment neuron example above, for tasks where sentiment is important (product reviews), LHUC will assign a high weight to the sentiment neuron, while for other tasks (news), the corresponding weight will be low. The weights are learned automatically from the available in-domain data. 4 Domain Adaptation for NMT Domain adaptation for phrase-based and related approaches to machine translation has been extensively investigated, e.g. (Schwenk, 2008; Axelrod et al., 2011; Carpuat et al., 2013). Neural machine translation, being a relatively new approach to MT, has not seen so many works going into this direction yet. Previous methods can of course still be applied as long as they are modelindependent, e.g. data selection methods as shown 1 The value of 2 is chosen as to be able to amplify the value (it can be doubled) but without overshadowing the value of the other units, which could happen with bigger values. 501 in (van der Wees et al., 2017) or self-training approaches like (Bertoldi and Federico, 2009), of which back-translation (Se"
vilar-etal-2006-error,2005.eamt-1.13,0,\N,Missing
vilar-etal-2006-error,P02-1040,0,\N,Missing
W05-0806,J93-2003,0,0.0124716,"Missing"
W05-0806,W01-1407,1,0.882366,"Missing"
W05-0806,J04-2003,1,0.83514,"t language. Usually, the performance of a translation system strongly depends on the size of the available training corpus. However, acquisition of a large high-quality bilingual parallel text for the desired domain and language pair requires lot of time and effort, and, for many language pairs, is even not possible. Besides, small corpora have certain advantages - the acquisition does not require too much effort and also manual creation and correction are possible. Therefore there is an increasing number of publications dealing with limited amounts of bilingual data (Al-Onaizan et al., 2000; Nießen and Ney, 2004). In this work, we examine the quality of several statistical machine translation systems constructed on a small amount of parallel Serbian-English text. The main bilingual parallel corpus consists of about 3k sentences and 20k running words from an unrestricted domain. The translation systems are built on the full corpus as well as on a reduced corpus containing only 200 parallel sentences. A small set of about 350 short phrases from the web is used as additional bilingual knowledge. In addition, we investigate the use of monolingual morpho-syntactic knowledge i.e. base forms and POS tags. 1"
W05-0806,J03-1002,1,0.0207503,"is to translate a source language sequence f1 , . . . , fJ into a target language sequence e1 , . . . , eI by maximising the conditional probability P r(eI1 |f1J ). This probability can be factorised into the translation model probability P (f1J |eI1 ) which describes the correspondence between the words in the source and the target sequence, and the language model probability P (eJ1 ) which describes well-formedness of the produced target sequence. These two probabilities can be modelled independently of each other. For detailed descriptions of SMT models see for example (Brown et al., 1993; Och and Ney, 2003). Translation probabilities are learnt from a bilingual parallel text corpus and language model probabilities are learnt from a monolingual text in the tarFor the Serbian language, as a rather minor and not widely studied language, there are not many language resources available, especially not parallel texts. On the other side, investigations on this language may be quite useful since the majority of principles can be extended to the wider group of Slavic languages (e.g. Czech, Polish, Russian, etc.). In this work, we exploit small Serbian-English parallel texts as a bilingual knowledge sourc"
W05-0806,P02-1040,0,0.078321,"timise the scaling factors, results obtained for this set do not differ from those for the test set. Therefore only the joint error rates (Development+Test) are reported. As for the external test set, results for this text are reported only for the full corpus systems, since for the reduced corpus the error rates are higher but the effects of using phrases and morpho-syntactic information are basically the same. 4.2 Translation Results The evaluation metrics used in our experiments are WER (Word Error Rate), PER (Positionindependent word Error Rate) and BLEU (BiLingual Evaluation Understudy) (Papineni et al., 2002). Since BLEU is an accuracy measure, we use 1BLEU as an error measure. 45 4.2.1 Translation from Serbian into English Error rates for the translation from Serbian into English are shown in Table 3 and some examples are shown in Table 6. It can be seen that there is a significant decrease in all error rates when the full forms are replaced with their base forms. Since the redundant information contained in the inflection is removed, the system can better capture the relevant information and is capable of producing correct or approximatively correct translations even for unseen full forms of the"
W05-0806,popovic-ney-2004-towards,1,0.895064,"Missing"
W05-0806,J82-2005,0,0.563994,"Missing"
W05-0806,W96-0213,0,0.0102235,". The morpho-syntactic annotation of the English part of the corpus has been done by the constraint grammar parser ENGCG for morphological and syntactic analysis of English language. For each word, this tool provides its base form and sequence of morpho-syntactic tags. For the Serbian corpus, to our knowlegde there is no available tool for automatic annotation of this language. Therefore, the base forms have been introduced manually and the POS tags have been provided partly manually and partly automatically using a statistical maximum-entropy based POS tagger similar to the one described in (Ratnaparkhi, 1996). First, the 200 sentences of the reduced training corpus have been annotated completely manually. Then the first 500 sentences of the rest of the training corpus have been tagged automatically and the errors have been manually corrected. Afterwards, the POS tagger has been trained on the extended corpus (700 sentences), the next 500 sentences of the rest are annotated, and the procedure has been repeated until the annotation has been finished for the complete corpus. Table 1: Statistics of the Serbian-English Assimil corpus Serbian English Training: original base forms original no article ful"
W05-0831,W00-0508,0,0.172433,"Missing"
W05-0831,2004.iwslt-evaluation.13,1,0.903041,"the reordering problem from the view of the model. Without reordering both in training and during search, sentences can only be translated properly into a language with similar word order. In (Bangalore et al., 2000) weighted reordering has been applied to target sentences since defining a permutation model on the source side is impractical in combination with speech recognition. In order to reduce the computational complexity, this approach considers only a set of plausible reorderings seen on training data. Most other phrase-based statistical approaches like the Alignment Template system of Bender et al. (2004) rely on (local) reorderings which are implicitly memorized with each pair of source and target phrases in training. Additional reorderings on phrase level are fully integrated into the decoding process, which increases the complexity of the system and makes it hard to modify. Zens et al. (2003) reviewed two types of reordering constraints for this type of translation systems. In our work we follow a phrase-based translation approach, applying source sentence reordering on word level. We compute a reordering graph ondemand and take it as input for monotonic translation. This approach is modula"
W05-0831,P04-1065,1,0.437186,"Missing"
W05-0831,knight-al-onaizan-1998-translation,0,0.507961,"Missing"
W05-0831,N03-1019,0,0.120471,"Missing"
W05-0831,C04-1032,1,0.655504,"he empty phrase. Therefore, for language pairs with big differences in word order, probability estimates may be poor. This problem can be solved by reordering either source or target training sentences such that alignments become monotonic for all sentences. We suggest the following consistent source sentence reordering and alignment monotonization approach in which we compute optimal, minimum-cost alignments. First, we estimate a cost matrix C for each sentence pair (f1J , eI1 ). The elements of this matrix cij are the local costs of aligning a source word fj to a target word ei . Following (Matusov et al., 2004), we compute these local costs by interpolating state occupation probabilities from the source-to-target and target-to-source training of the HMM and IBM-4 models as trained by the GIZA++ toolkit (Och et al., 2003). For a given alignment A ⊆ I × J, we define the costs of this alignment c(A) as the sum of the local costs of all aligned word pairs: X c(A) = cij (1) A∈A ∼ = argmax max e˜J 1 (fj , e˜j ). Mapping the bilingual language model to a WFST T is canonical and it has been shown in (Kanthak et al., 2004) that the search problem can then be rewritten using finite-state terminology: j−1 p(fj"
W05-0831,J03-1002,1,0.0715111,"Missing"
W05-0831,P02-1040,0,0.119703,"Missing"
W05-0831,J97-3002,0,0.168153,"well-known in the field of machine translation and were first described in (Berger et al., 1996). The idea behind these constraints is to deviate from monotonic translation by postponing translations of a limited number of words. More specifically, at each state we can translate any of the first l yet uncovered word positions. The implementation using a bit vector is straightforward. For consistency, we associate window size with the parameter l for all constraints presented here. 4.3 1000 ITG Constraints Another type of reordering can be obtained using Inversion Transduction Grammars (ITG) (Wu, 1997). These constraints are inspired by bilingual bracketing. They proved to be quite useful for machine translation, e.g. see (Bender et al., 2004). Here, we interpret the input sentence as a sequence of segments. In the beginning, each word is a segment of its own. Longer segments are constructed by recursively combining two adjacent segments. At each 1 both covered and uncovered train dev test sentences words singletons vocabulary sentences words sentence length (avg/max) sentences words sentence length (avg/max) Chinese English 20 000 182 904 160 523 3 525 2 948 7 643 6 982 506 3 515 3 595 6.9"
W05-0831,2002.tmi-tutorials.2,0,0.0628056,"ermutations as a finite-state automaton requires at least 2J states. Therefore, we opt for computing the permutation automaton on-demand while applying beam pruning in the search. 4.1 Lazy Permutation Automata For on-demand computation of an automaton in the flavor described in (Kanthak et al., 2004) it is sufficient to specify a state description and an algorithm that calculates all outgoing arcs of a state from the state description. In our case, each state represents a permutation of a subset of the source words f1J , which are already translated. This can be described by a bit vector bJ1 (Zens et al., 2002). Each bit of the state bit vector corresponds to an arc of the linear input automaton and is set to one if the arc has been used on any path from the initial to the current state. The bit vectors of two states connected by an arc differ only in a single bit. Note that bit vectors elegantly solve the problem of recombining paths in the automaton as states with the same bit vectors can be merged. As a result, a fully minimized permutation automaton has only a single initial and final state. Even with on-demand computation, complexity using full permutations is unmanagable for long sentences. We"
W05-0831,P03-1019,1,0.772763,"Missing"
W05-0831,J04-2004,0,\N,Missing
W05-0831,2004.iwslt-evaluation.1,0,\N,Missing
W05-0903,2004.iwslt-evaluation.1,0,0.0310677,"hed out on system level. On the BTEC corpus a high sentence level correlation accompanies a significantly lower system level correlation. Note that due to the much lower number of samples on the system level (e.g. 5 vs. 5500), small changes in the sentence level correlation are more likely to be significant than such changes on system level. We have verified these effects by inspecting the rank correlation on both levels, as well as by experiments on other corpora. Although these experiments support our findings, we have omitted results here 0.0 B BLEUS W P 0.0 4.2 W WER P PER 0.6 evaluation (Akiba et al., 2004). We restricted our experiments to the eleven MT systems that had been trained on a common training corpus. Corpus statistics can be found in table 2. Tokenization and case normalization The impact of case information was analyzed in our next experiment. Figure 4 (again without the N IST measure as it shows a similar behavior to the other measures) indicates that it is advisable to disregard case information when looking into adequacy on sentence level. Surprisingly, this also holds for 0.2 TIDES CE 0.0 0.2 TIDES AE 0.0 TIDES CE P B N TIDES AE 1.0 B N 0.8 ● P B ●●●● ● ●●● ●●●● ● ●●● ●●●● none"
W05-0903,2003.mtsummit-papers.32,1,0.854681,"β := − 2 log2 3 Due to the information weights, the value of the N IST score depends highly on the selection of the reference corpus. This must be taken into account when comparing N IST scores of different evaluation campaigns. 2.2 Other measures Lin and Och (2004) introduce a family of three measures named ROUGE. ROUGE -S is a skipbigram F-measure. ROUGE -L and ROUGE -W are measures based on the length of the longest common subsequence of the sentences. ROUGE -S has a structure similar to the bigram P ER presented here. We expect ROUGE -L and ROUGE -W to have similar properties to W ER. In (Leusch et al., 2003), we have described INV W ER , a word error rate enhanced by block transposition edit operations. As structure and scores of INV W ER are similar to W ER, we have omitted INV W ER experiments in this paper. 3 Preprocessing and normalization Although the general idea is clear, there are still several details to be specified when implementing and using an automatic evaluation measure. We are going to investigate the following problems: The first detail we have to state more precisely is the term “word” in the above formulae. A common approach for western languages is to consider spaces as separa"
W05-0903,C04-1072,0,0.0343591,"er of deletions, insertions, and substitutions to transform the candidate sentence into the reference sentence is calculated. Using the counts ne,r , n ˜ e,r,k of a word e in the candidate er,k , we sentence Ek , and the reference sentence E can calculate this distance as  X   er,k := 1 Ik −I˜k + ne,k −˜ dP ER Ek , E ne,r,k 2 e 18 1 · sm + sm + n ¯m  X X min n em 1 ,k ,n ˜ em 1 ,k   k em 1 ∈Ek with the geometric mean gm and a brevity penalty    I¯∗  lpB LEU := min 1 , exp 1 − ¯ I In the original B LEU definition, the smoothing term sm is zero. To allow for sentence-wise evaluation, Lin and Och (2004) define the B LEU - S measure with s1 := 1 and sm&gt;1 := 0. We have adopted this technique for this study. 2.1.4 N IST The N IST score (Doddington, 2002) extends the B LEU score by taking information weights of the m-grams into account. The N IST information weight is defined as  ¯˜ m−1 Info(em ˜¯ em − log2 n 1 ) := − log2 n 1 ¯˜ em := with n 1 X e1 n ˜ en1 ,k,r . k,r Note that the weight of a phrase occurring in many references sentence for a candidate is considered to be lower than the weight of a phrase occurring only once! The N IST score is the sum over all information counts of the co-occ"
W05-0903,2001.mtsummit-papers.68,0,0.0360213,"denote the count of the m-gram em 1 1 ,k within the candidate sentence Ek ; similarly let n ˜ em denote the same count within the reference 1 ,r,k er,k . The total m-gram count over the sentence E X X corpus is then n ¯ m := nem . 1 ,k k em 1 ∈Ek This distance is then normalized into an error rate, the P ER, as described in section 2.1.1. A promising approach is to compare bigram or arbitrary m-gram count vectors instead of unigram count vectors only. This will take into account the ordering of the words within a sentence implicitly, although not as strong as the W ER does. 2.1.3 B LEU B LEU (Papineni et al., 2001) is a precision measure based on m-gram count vectors. The precision is modified such that multiple references are combined into a single m-gram count vector, n ˜ e,k := maxr n ˜ e,r,k . Multiple occurrences of an m-gram in the candidate sentence are counted as correct only up to the maximum occurrence count within the reference sentences. Typically, m = 1, . . . , 4. To avoid a bias towards short candidate sentences consisting of “safe guesses” only, sentences shorter than the reference length will be penalized with a brevity penalty. B LEU := lpB LEU · gm m  2.1.1 W ER The word error rate i"
W05-0903,P02-1040,0,\N,Missing
W07-0705,J93-2003,0,0.0313791,"k is more academical, in Section 4 we discuss possible practical applications for this approach. The paper concludes in Section 5. 2 From Words To Letters In the standard approach to statistical machine translation we are given a sentence (sequence of words) f1J = f1 . . . fJ in a source language which is to be translated into a sentence eˆI1 = eˆ1 . . . eˆI in a target language. Bayes decision rule states that we should choose the sentence which maximizes the posterior probability eˆI1 = argmax p(eI1 |f1J ) , (1) eI1 where the argmax operator denotes the search process. In the original work (Brown et al., 1993) the posterior probability p(eI1 |f1J ) is decomposed following a noisy-channel approach, but current stateof-the-art systems model the translation probability directly using a log-linear model(Och and Ney, 2002):  P M I, fJ) λ h (e exp m=1 m m 1 1 , P p(eI1 |f1J ) = X M I, fJ) λ h (˜ e exp m m 1 1 m=1 I e˜1 (2) with hm different models, λm scaling factors and the denominator a normalization factor that can be 34 ignored in the maximization process. The λm are usually chosen by optimizing a performance measure over a development corpus using a numerical optimization algorithm like the down"
W07-0705,P02-1038,1,0.3409,"n we are given a sentence (sequence of words) f1J = f1 . . . fJ in a source language which is to be translated into a sentence eˆI1 = eˆ1 . . . eˆI in a target language. Bayes decision rule states that we should choose the sentence which maximizes the posterior probability eˆI1 = argmax p(eI1 |f1J ) , (1) eI1 where the argmax operator denotes the search process. In the original work (Brown et al., 1993) the posterior probability p(eI1 |f1J ) is decomposed following a noisy-channel approach, but current stateof-the-art systems model the translation probability directly using a log-linear model(Och and Ney, 2002):  P M I, fJ) λ h (e exp m=1 m m 1 1 , P p(eI1 |f1J ) = X M I, fJ) λ h (˜ e exp m m 1 1 m=1 I e˜1 (2) with hm different models, λm scaling factors and the denominator a normalization factor that can be 34 ignored in the maximization process. The λm are usually chosen by optimizing a performance measure over a development corpus using a numerical optimization algorithm like the downhill simplex algorithm (Press et al., 2002). The most widely used models in the log linear combination are phrase-based models in sourceto-target and target-to-source directions, ibm1-like scores computed at phra"
W07-0705,J03-1002,1,0.013417,"it does not constitute a restriction of the generalization capabilities the model can have in creating “new words”. Somehow surprisingly, an additional word language model did not help. While the vocabulary size is reduced, the average sentence length increases, as we consider each letter to be a unit by itself. This has a negative impact in the running time of the actual implementation of the algorithms, specially for the alignment process. In order to alleviate this, the alignment process was split into two passes. In the first part, a word alignment was computed (using the GIZA++ toolkit (Och and Ney, 2003)). Then the training sentences were split according to this alignment (in a similar way to the standard phrase extraction algorithm), so that the length of the source and target part is around thirty letters. Then, a letter-based alignment is computed. 2.2 Efficiency Issues Somewhat counter-intuitively, the reduced vocabulary size does not necessarily imply a reduced mem1 ory footprint, at least not without a dedicated program optimization. As in a sensible implementations of nearly all natural language processing tools, the words are mapped to integers and handled as such. A typical implement"
W07-0705,popovic-ney-2004-towards,1,0.616151,"Missing"
W07-0705,2005.eamt-1.29,1,0.862329,"Missing"
W07-0705,W02-0505,0,\N,Missing
W07-0705,2006.iwslt-evaluation.15,1,\N,Missing
W07-0705,2006.iwslt-evaluation.8,0,\N,Missing
W07-0713,E06-1032,0,0.0191741,"sk, namely the ranking of systems. Furthermore we argue that machine translation evaluations should be regarded as statistical processes, both for human and automatic evaluation. We show how confidence ranges for state-of-the-art evaluation measures such as WER and TER can be computed accurately and efficiently without having to resort to Monte Carlo estimates. We give an example of our new evaluation scheme, as well as a comparison with classical automatic and human evaluation on data from a recent international evaluation campaign. 1 However, automatic measures also have big disadvantages; (Callison-Burch et al., 2006) describes some of them. A major problem is that a given sentence in one language can have several correct translations in another language and thus, the measure of similarity with one or even a small amount of reference translations will never be flexible enough to truly reflect the wide range of correct possibilities of a translation. 1 This holds in particular for long sentences and wide- or open-domain tasks like the ones dealt with in current MT projects and evaluations. Introduction Evaluation of machine translation (MT) output is a difficult and still open problem. As in other natural l"
W07-0713,1993.eamt-1.1,0,0.0573576,"binary comparison experiments, each judge was given hypothesis translations ei,X , ei,Y . She could then judge ei,X to be better than, equal to, or worse than ei,Y . All these judgments were counted over the systems. We define a sentence score ri,X,Y for this evaluation method as follows:   +1 ei,X is better than ei,Y ri,X,Y := 0 . (2) ei,X is equal to ei,Y  −1 e is worse than e i,X i,Y Then, the total evaluation score for a binary comparison of systems X and Y is m RX,Y 1 X ri,X,Y , := m (3) i=1 with m the number of evaluated sentences. For this case, namely R being an arithmetic mean, (Efron and Tibshirani, 1993) gives an explicit formula for the estimated standard error of the score RX,Y . To simplify the notation, we will use R instead of RX,Y from now on, and ri instead of ri,X,Y . v um X 1 u t (ri − R)2 . se[R] = (4) m−1 i=1 With x denoting the number of sentences where ri = 1, and y denoting the number of sentences where ri = −1, x−y m R= Train (5) and with basic algebra 1 se[R] = m−1 Test r (x − y)2 x+y− . m (6) Moreover, we can explicitly give an estimate for E[R0 ]: The null hypothesis is that both systems are “equally good”. Then, we should expect as many sentences where X is better than Y as"
W07-0713,W06-3114,0,0.0339031,", there can be biases among the human judges. Large amounts of sentences must therefore be evaluated and procedures like evaluation normalization must be carried out before significant conclusions from the evaluation can be drawn. Another important drawback, which is also one of the causes of the aforementioned problems, is that it is very difficult to define the meaning of the numerical scores precisely. Even if human judges have explicit evaluation guidelines at hand, they still find it difficult to assign a numerical value which represents the quality of the translation for many sentences (Koehn and Monz, 2006). In this paper we present an alternative to this evaluation scheme. Our method starts from the observation that normally the final objective of a human evaluation is to find a “ranking” of different systems, and the absolute score for each system is not relevant (and it can even not be comparable between different evaluations). We focus on a method that aims to simplify the task of the judges and allows to rank the systems according to their translation quality. 3 Binary System Comparisons The main idea of our method relies in the fact that a human evaluator, when presented two different tran"
W07-0713,W05-0903,1,0.854992,"impossible to eliminate them if humans are involved. If one of the judges prefers one kind of structure, there will a bias for a system producing such output, independently of the evaluation procedure. However, the suppression of explicit numerical scores eliminates an additional bias of evaluators. It has been observed that human judges often give scores within 6 Note however that possible evaluator biases can have a great influence in these statistics. a certain range (e.g. in the mid-range or only extreme values), which constitute an additional difficulty when carrying out the evaluation (Leusch et al., 2005). Our method suppresses this kind of bias. Another advantage of our method is the possibility of assessing improvements within one system. With one evaluation we can decide if some modifications actually improve performance. This evaluation even gives us a confidence interval to weight the significance of an improvement. Carrying out a full adequacy-fluency analysis would require a lot more effort, without giving more useful results. 7 Conclusion We presented a novel human evaluation technique that simplifies the task of the evaluators. Our method relies on two basic observations. The first on"
W07-0713,P03-1021,0,0.0111467,"restricted the number of systems in order to keep the evaluation effort manageable for a first experimental setup to test the feasibility of our method. The ranking of 5 systems can be carried out with as few as 7 comparisons, but the ranking of 9 systems requires 19 comparisons. 5 Evaluation Results i=1 (9) With this Equation, Monte-Carlo-estimates are no longer necessary for examining the significance of WER, PER, TER, etc. Unfortunately, we do not expect such a short explicit formula to exist for the standard BLEU score. Still, a confidence range for BLEU can be estimated by bootstrapping (Och, 2003; Zhang and Vogel, 2004). 100 Seven human bilingual evaluators (6 native speakers and one near-native speaker of Spanish) carried out the evaluation. 100 sentences were randomly chosen and assigned to each of the evaluators for every system comparison, as discussed in Section 3.3. The results can be seen in Table 2 and Figure 2. Counts 4 http://www.tc-star.org/ ● ● ● ● 400 300 B−D E−A ●D−C B−A D−A 0 10 20 ● A−C E−B 200 ● 100 # &quot;Second system better&quot; 70 60 50 40 30 ● 0 # &quot;Second system better&quot; B−A D−C A−C E−A E−B B−D D−A ● 0 10 20 30 40 50 60 70 0 # &quot;First system better&quot; 100 200 300 400 # &quot;Firs"
W07-0713,P02-1040,0,0.0919255,"of correct possibilities of a translation. 1 This holds in particular for long sentences and wide- or open-domain tasks like the ones dealt with in current MT projects and evaluations. Introduction Evaluation of machine translation (MT) output is a difficult and still open problem. As in other natural language processing tasks, automatic measures which try to asses the quality of the translation can be computed. The most widely known are the Word Error Rate (WER), the Position independent word Error Rate (PER), the NIST score (Doddington, 2002) and, especially in recent years, the BLEU score (Papineni et al., 2002) and the Translation ErIf the actual quality of a translation in terms of usefulness for human users is to be evaluated, human evaluation needs to be carried out. This is however a costly and very time-consuming process. In this work we present a novel approach to human evaluation that simplifies the task for human judges. Instead of having to assign numerical scores to each sentence to be evaluated, as is done in current evaluation procedures, human judges choose the best one out of two candidate translations. We show how this method can be used to rank an arbitrary number of systems and pres"
W07-0713,2005.mtsummit-papers.34,1,0.679589,"oject4 . The goal of this project is to build a speech-to-speech translation system that can deal with real life data. Three translation directions are dealt with in the project: Spanish to English, English to Spanish and Chinese to English. For the system comparison we concentrated only in the English to Spanish direction. The corpus for the Spanish–English language pair consists of the official version of the speeches held in the European Parliament Plenary Sessions (EPPS), as available on the web page of the European Parliament. A more detailed description of the EPPS data can be found in (Vilar et al., 2005). Table 1 shows the statistics of the corpus. A total of 9 different MT systems participated in this condition in the evaluation campaign that took place in February 2006. We selected five representative systems for our study. Henceforth we shall refer to these systems as System A through System E. We restricted the number of systems in order to keep the evaluation effort manageable for a first experimental setup to test the feasibility of our method. The ranking of 5 systems can be carried out with as few as 7 comparisons, but the ranking of 9 systems requires 19 comparisons. 5 Evaluation Res"
W07-0713,2004.tmi-1.9,0,0.0207347,"the number of systems in order to keep the evaluation effort manageable for a first experimental setup to test the feasibility of our method. The ranking of 5 systems can be carried out with as few as 7 comparisons, but the ranking of 9 systems requires 19 comparisons. 5 Evaluation Results i=1 (9) With this Equation, Monte-Carlo-estimates are no longer necessary for examining the significance of WER, PER, TER, etc. Unfortunately, we do not expect such a short explicit formula to exist for the standard BLEU score. Still, a confidence range for BLEU can be estimated by bootstrapping (Och, 2003; Zhang and Vogel, 2004). 100 Seven human bilingual evaluators (6 native speakers and one near-native speaker of Spanish) carried out the evaluation. 100 sentences were randomly chosen and assigned to each of the evaluators for every system comparison, as discussed in Section 3.3. The results can be seen in Table 2 and Figure 2. Counts 4 http://www.tc-star.org/ ● ● ● ● 400 300 B−D E−A ●D−C B−A D−A 0 10 20 ● A−C E−B 200 ● 100 # &quot;Second system better&quot; 70 60 50 40 30 ● 0 # &quot;Second system better&quot; B−A D−C A−C E−A E−B B−D D−A ● 0 10 20 30 40 50 60 70 0 # &quot;First system better&quot; 100 200 300 400 # &quot;First system better&quot; (a) Eac"
W09-0410,E06-1005,1,0.803237,"st participles, but there are many cases when other verb forms also occur at the clause end. For the translation from German into English, following verb types were moved towards the beginning of a clause: infinitives, infinitives+zu, finite verbs, past participles and negative particles. For the translation from English to German, infinitives and past participles were moved to the end of a clause, where punctuation marks, subordinate conjunctions and finite verbs are considered as the beginning of the next clause. 4 System combination For system combination we used the approach described in (Matusov et al., 2006). The method is based on the generation of a consensus translation out of the output of different translation systems. The core of the method consists in building a confusion network for each sentence by aligning and combining the (single-best) translation hypothesis from one MT system with the translations produced by the other MT systems (and the other translations from the same system, if n-best lists are used in combination). For each sentence, each MT system is selected once as “primary” system, and the other hypotheses are aligned to this hypothesis. The resulting confusion networks are"
W09-0410,popovic-ney-2006-pos,1,0.894671,"Missing"
W09-0410,N07-1029,0,0.0191149,"s. The core of the method consists in building a confusion network for each sentence by aligning and combining the (single-best) translation hypothesis from one MT system with the translations produced by the other MT systems (and the other translations from the same system, if n-best lists are used in combination). For each sentence, each MT system is selected once as “primary” system, and the other hypotheses are aligned to this hypothesis. The resulting confusion networks are combined into a signle word graph, which is then weighted with system-specific factors, similar to the approach of (Rosti et al., 2007), and a trigram LM trained on the MT hypotheses. The translation with the best total score within this word graph is selected as consensus translation. The scaling factors of these models are optimized using the Condor toolkit (Berghen and Bersini, 2005) to achieve optimal B LEU score on the dev set. 5 5.1 Experimental results Experimental settings For all translation directions, we used the provided EuroParl and News parallel corpora to train the translation models and the News monolingual corpora to train the language models. All systems were optimised for the B LEU score on the development"
W09-0410,2008.iwslt-papers.7,1,0.894714,"Missing"
W09-0410,A00-1031,0,0.0616954,"Missing"
W09-0410,carreras-etal-2004-freeling,0,0.0349781,"Missing"
W09-0410,2005.iwslt-1.18,1,\N,Missing
W09-0410,W99-0604,1,\N,Missing
W09-0410,C04-1006,1,\N,Missing
W09-0410,E03-1076,0,\N,Missing
W09-0410,W07-0813,0,\N,Missing
W09-0410,C08-1128,1,\N,Missing
W09-0410,J90-2002,0,\N,Missing
W09-0410,P05-1071,0,\N,Missing
W09-0410,P02-1040,0,\N,Missing
W09-0410,W03-1709,0,\N,Missing
W09-0410,W06-3111,1,\N,Missing
W09-0410,J04-2004,0,\N,Missing
W09-0410,W07-0401,1,\N,Missing
W09-0410,J06-4004,0,\N,Missing
W09-0410,W05-0831,1,\N,Missing
W09-0410,J03-1002,1,\N,Missing
W09-0410,P06-1001,0,\N,Missing
W09-0410,takezawa-etal-2002-toward,0,\N,Missing
W09-0410,2005.eamt-1.37,1,\N,Missing
W09-0410,2006.iwslt-papers.1,1,\N,Missing
W09-0410,2007.iwslt-1.25,1,\N,Missing
W09-0410,2004.iwslt-evaluation.13,1,\N,Missing
W09-0410,2007.iwslt-1.3,1,\N,Missing
W09-0410,J07-2003,0,\N,Missing
W09-0410,2006.iwslt-evaluation.15,1,\N,Missing
W09-0410,P03-1021,0,\N,Missing
W09-0410,2007.iwslt-1.10,0,\N,Missing
W10-1738,N09-1025,0,0.0105103,"ly the model in search, Jane has to be run with a phrase table that contains word alignment for each phrase, too, with the exception of phrases which are composed purely of non-terminals. Jane’s phrase extraction can optionally supply this information from the training data. (Hasan et al., 2008) and (Hasan and Ney, 2009) employ similar techniques and provide some more discussion on the path-aligned variant of the model and other possible restrictions. 3.6 community. We use an in-house implementation of the method. The second one is the MIRA algorithm, first applied for machine translation in (Chiang et al., 2009). This algorithm is more adequate when the number of parameters to optimize is large. If the Numerical Recipes library (Press et al., 2002) is available, an additional general purpose optimization tool is also compiled. Using this tool a single-best optimization procedure based on the downhill simplex method (Nelder and Mead, 1965) is included. This method, however, can be considered deprecated in favour of the above mentioned methods. 3.8 If the Sun Grid Engine2 is available, all operations of Jane can be parallelized. For the extraction process, the corpus is split into chunks (the granulari"
W10-1738,J07-2003,0,0.602278,"include We also introduce a novel reordering model for the hierarchical phrase-based approach which further enhances translation performance, and analyze the effect some recent extended lexicon models have on the performance of the system. 1 Related Work • SAMT (Zollmann and Venugopal, 2006): The original version is not maintained any more and we had problems working on big corpora. A new version which requires Hadoop has just been released, however the documentation is still missing. Introduction We present a new open source toolkit for hierarchical phrase-based translation, as described in (Chiang, 2007). The hierarchical phrase model is an extension of the standard phrase model, where the phrases are allowed to have “gaps”. In this way, long-distance dependencies and reorderings can be modelled in a consistent way. As in nearly all current statistical approaches to machine translation, this model is embedded in a log-linear model combination. RWTH has been developing this tool during the last two years and it was used successfully in numerous machine translation evaluations. It is developed in C++ with special attention to clean code, extensibility and efficiency. The toolkit is available un"
W10-1738,2009.iwslt-papers.2,0,0.0337704,"provided, one would only need to implement a corresponding frontend which communicates with the translation server (which may be located on another machine). Forced Alignments Jane has also preliminary support for forced alignments between a given source and target sentence. Given a sentence in the source language and its translation in the target language, we find the best way the source sentence can be translated into the given target sentence, using the available inventory of phrases. This is needed for more advanced training approaches like the ones presented in (Blunsom et al., 2008) or (Cmejrek et al., 2009). As reported in these papers, due to the restrictions in the phrase extraction process, not all sentences in the training corpus can be aligned in this way. 3.7 Parallelized operation 3.9 Extensibility One of the goals when implementing the toolkit was to make it easy to extend it with new features. For this, an abstract class was created which we called secondary model. New models need only to derive from this class and implement the abstract methods for data reading and costs computation. This allows for an encapsulation of the computations, which can be activated and deactivated on demand."
W10-1738,N09-2005,1,0.83495,"nstrained triplets is that the first trigger f is restricted to the aligned target word e. The second trigger f 0 is allowed to move along the whole remaining source sentence. For the training of the model, we use word alignment information obtained by GIZA++ (Och and Ney, 2003). To be able to apply the model in search, Jane has to be run with a phrase table that contains word alignment for each phrase, too, with the exception of phrases which are composed purely of non-terminals. Jane’s phrase extraction can optionally supply this information from the training data. (Hasan et al., 2008) and (Hasan and Ney, 2009) employ similar techniques and provide some more discussion on the path-aligned variant of the model and other possible restrictions. 3.6 community. We use an in-house implementation of the method. The second one is the MIRA algorithm, first applied for machine translation in (Chiang et al., 2009). This algorithm is more adequate when the number of parameters to optimize is large. If the Numerical Recipes library (Press et al., 2002) is available, an additional general purpose optimization tool is also compiled. Using this tool a single-best optimization procedure based on the downhill simplex"
W10-1738,W09-0434,0,0.0264578,"Missing"
W10-1738,D08-1039,1,0.886855,"Missing"
W10-1738,P08-1024,0,0.0230148,"code in this direction is provided, one would only need to implement a corresponding frontend which communicates with the translation server (which may be located on another machine). Forced Alignments Jane has also preliminary support for forced alignments between a given source and target sentence. Given a sentence in the source language and its translation in the target language, we find the best way the source sentence can be translated into the given target sentence, using the available inventory of phrases. This is needed for more advanced training approaches like the ones presented in (Blunsom et al., 2008) or (Cmejrek et al., 2009). As reported in these papers, due to the restrictions in the phrase extraction process, not all sentences in the training corpus can be aligned in this way. 3.7 Parallelized operation 3.9 Extensibility One of the goals when implementing the toolkit was to make it easy to extend it with new features. For this, an abstract class was created which we called secondary model. New models need only to derive from this class and implement the abstract methods for data reading and costs computation. This allows for an encapsulation of the computations, which can be activated"
W10-1738,P07-1019,0,0.279989,"thods, we refer to the given literature. 3.1 Search Algorithms 3.3 The search for the best translation proceeds in two steps. First, a monolingual parsing of the input sentence is carried out using the CYK+ algorithm (Chappelier and Rajman, 1998), a generalization of the CYK algorithm which relaxes the requirement for the grammar to be in Chomsky normal form. From the CYK+ chart we extract a hypergraph representing the parsing space. In a second step the translations are generated, computing the language model scores in an integrated fashion. Both the cube pruning and cube growing algorithms (Huang and Chiang, 2007) are implemented. For the latter case, the extensions concerning the language model heuristics similar to (Vilar and Ney, 2009) have also been included. 3.2 Syntactic Features Soft syntactic features comparable to (Vilar et al., 2008) are implemented in the extraction step of the toolkit. In search, they are considered as additional feature functions of the translation rules. The decoder is able to handle an arbitrary number of non-terminal symbols. The extraction has been extended so that the extraction of SAMTrules is included (Zollmann and Venugopal, 2006) but this approach is not fully sup"
W10-1738,J93-2003,0,0.0162587,"Missing"
W10-1738,W09-0424,0,0.0228497,"stent way. As in nearly all current statistical approaches to machine translation, this model is embedded in a log-linear model combination. RWTH has been developing this tool during the last two years and it was used successfully in numerous machine translation evaluations. It is developed in C++ with special attention to clean code, extensibility and efficiency. The toolkit is available under an open source non-commercial license and downloadable from http://www.hltpr.rwth-aachen.de/jane. In this paper we give an overview of the main features of the toolkit and introduce two new ex• Joshua (Li et al., 2009): A decoder written in Java by the John Hopkins University. This project is the most similar to our own, however both were developed independently and each one has some unique features. A brief comparison between these two systems is included in Section 5.1. • Moses (Koehn et al., 2007): The de-facto standard phrase-based translation decoder has now been extended to support hierarchical translation. This is still in an experimental branch, however. 3 Features In this section we will only give a brief overview of the features implemented in Jane. For detailed explanation of previously published"
W10-1738,J97-3002,0,0.0234206,"tion information into account, i.e. it operates on sets, not on sequences or even trees. The probability of a word being part of the target sentence, given a set of source words, are decomposed into binary features, one for each source vocabulary entry. These binary features are combined in a log-linear fashion with corresponding feature weights. The discriminative word lexicon is trained independently for each target word using the L-BFGS (Byrd et al., 1995) algorithm. For regularization, Gaussian priors are utilized. DWL model probabilities are computed as be to include the ITG-Reorderings (Wu, 1997), by adding following rule S → hS ∼0 S ∼1 , S ∼1 S ∼0 i (2) We can also model other reordering constraints. As an example, phrase-level IBM reordering constraints with a window length of 1 can be included substituting the rules in Equation (1) with following rules S → hM ∼0 , M ∼0 i S → hM ∼0 S ∼1 , M ∼0 S ∼1 i S → hB ∼0 M ∼1 , M ∼1 B ∼0 i M → hX ∼0 , X ∼0 i (3) M → hM ∼0 X ∼1 , M ∼0 X ∼1 i B → hX ∼0 , X ∼0 i B → hB ∼0 X ∼1 , X ∼1 B ∼0 i In these rules we have added two additional nonterminals. The M non-terminal denotes a monotonic block and the B non-terminal a back jump. Actually both of th"
W10-1738,D09-1022,1,0.416911,"(·) of the application of a rule X → hα, βi where (α, β) is a bilingual phrase pair that may contain symbols from the non-terminal set is computed as Extended Lexicon Models We enriched Jane with the ability to score hypotheses with discriminative and trigger-based lexicon models that use global source sentence context and are capable of predicting contextspecific target words. This approach has recently been shown to improve the translation results of conventional phrase-based systems. In this section, we briefly review the basic aspects of these extended lexicon models. They are similar to (Mauser et al., 2009), and we refer there for a more detailed exposition on the training procedures and results in conventional phrase-based decoding. Note that the training for these models is not distributed together with Jane. t(α, β, f0J ) = (5)   X XX 2 − log  p(e|fj , fj 0 ) J · (J + 1) 0 e j 264 j >j with e ranging over all terminal symbols in the target part β of the rule. The second sum selects all words from the source sentence f0J (including the empty word that is denoted as f0 here). The third sum incorporates the rest of the source sentence right of the first triggering word. The order of the trig"
W10-1738,N07-1062,1,0.242945,"n Methods Two method based on n-best for minimum error rate training (MERT) of the parameters of the loglinear model are included in Jane. The first one is the procedure described in (Och, 2003), which has become a standard in the machine translation 2 265 http://www.sun.com/software/sge/ through 3.5 are implemented in this way. We thus try to achieve loose coupling in the implementation. In addition a flexible prefix tree implementation with on-demand loading capabilities is included as part of the code. This class has been used for implementing on-demand loading of phrases in the spirit of (Zens and Ney, 2007) and the on-demand n-gram format described in Section 3.2, in addition to some intermediate steps in the phrase extraction process. The code may also be reused in other, independent projects. 3.10 System Jane baseline + reordering 4.1 24.2 59.5 25.2 58.2 25.4 57.4 26.5 56.1 Europarl Data The first task is the Europarl as defined in the Quaero project. The main part of the corpus in this task consists of the Europarl corpus as used in the WMT evaluation (Callison-Burch et al., 2009), with some additional data collected in the scope of the project. We tried the reordering approach presented in S"
W10-1738,J03-1002,1,0.0139727,"triggers is not relevant because per definition p(e|f, f 0 ) = p(e|f 0 , f ), i.e. the model is symmetric. Non-terminals in β have to be skipped when the rule is scored. In Jane, we also implemented scoring for a variant of the triplet lexicon model called the pathconstrained (or path-aligned) triplet model. The characteristic of path-constrained triplets is that the first trigger f is restricted to the aligned target word e. The second trigger f 0 is allowed to move along the whole remaining source sentence. For the training of the model, we use word alignment information obtained by GIZA++ (Och and Ney, 2003). To be able to apply the model in search, Jane has to be run with a phrase table that contains word alignment for each phrase, too, with the exception of phrases which are composed purely of non-terminals. Jane’s phrase extraction can optionally supply this information from the training data. (Hasan et al., 2008) and (Hasan and Ney, 2009) employ similar techniques and provide some more discussion on the path-aligned variant of the model and other possible restrictions. 3.6 community. We use an in-house implementation of the method. The second one is the MIRA algorithm, first applied for machi"
W10-1738,W06-3119,0,0.106679,"Jane implements many features presented in previous work developed both at RWTH and other groups. As we go over the features of the system we will provide the corresponding references. Jane is not the first system of its kind, although it provides some unique features. There are other open source hierarchical decoders available. These include We also introduce a novel reordering model for the hierarchical phrase-based approach which further enhances translation performance, and analyze the effect some recent extended lexicon models have on the performance of the system. 1 Related Work • SAMT (Zollmann and Venugopal, 2006): The original version is not maintained any more and we had problems working on big corpora. A new version which requires Hadoop has just been released, however the documentation is still missing. Introduction We present a new open source toolkit for hierarchical phrase-based translation, as described in (Chiang, 2007). The hierarchical phrase model is an extension of the standard phrase model, where the phrases are allowed to have “gaps”. In this way, long-distance dependencies and reorderings can be modelled in a consistent way. As in nearly all current statistical approaches to machine tra"
W10-1738,P03-1021,0,0.035126,"ting the toolkit was to make it easy to extend it with new features. For this, an abstract class was created which we called secondary model. New models need only to derive from this class and implement the abstract methods for data reading and costs computation. This allows for an encapsulation of the computations, which can be activated and deactivated on demand. The models described in Sections 3.3 Optimization Methods Two method based on n-best for minimum error rate training (MERT) of the parameters of the loglinear model are included in Jane. The first one is the procedure described in (Och, 2003), which has become a standard in the machine translation 2 265 http://www.sun.com/software/sge/ through 3.5 are implemented in this way. We thus try to achieve loose coupling in the implementation. In addition a flexible prefix tree implementation with on-demand loading capabilities is included as part of the code. This class has been used for implementing on-demand loading of phrases in the spirit of (Zens and Ney, 2007) and the on-demand n-gram format described in Section 3.2, in addition to some intermediate steps in the phrase extraction process. The code may also be reused in other, indep"
W10-1738,P08-1066,0,0.0313191,"al feature functions of the translation rules. The decoder is able to handle an arbitrary number of non-terminal symbols. The extraction has been extended so that the extraction of SAMTrules is included (Zollmann and Venugopal, 2006) but this approach is not fully supported (there may be empty parses due to the extended number of non-terminals). We instead opted to support the generalization presented in (Venugopal et al., 2009), where the information about the new non-terminals is included as an additional feature in the log-linear model. In addition, dependency information in the spirit of (Shen et al., 2008) is included. Jane features models for string-to-dependency language models and computes various scores based on the well-formedness of the resulting dependency tree. Jane supports the Stanford parsing format,1 but can be easily extended to other parsers. Language Models Jane supports four formats for n-gram language models: • The ARPA format for language models. We use the SRI toolkit (Stolcke, 2002) to support this format. 3.4 • The binary language model format supported by the SRI toolkit. This format allows for a more efficient language model storage, which reduces loading times. In order"
W10-1738,D07-1049,0,0.0186097,"Missing"
W10-1738,N09-1027,0,0.0826285,"n included. 3.2 Syntactic Features Soft syntactic features comparable to (Vilar et al., 2008) are implemented in the extraction step of the toolkit. In search, they are considered as additional feature functions of the translation rules. The decoder is able to handle an arbitrary number of non-terminal symbols. The extraction has been extended so that the extraction of SAMTrules is included (Zollmann and Venugopal, 2006) but this approach is not fully supported (there may be empty parses due to the extended number of non-terminals). We instead opted to support the generalization presented in (Venugopal et al., 2009), where the information about the new non-terminals is included as an additional feature in the log-linear model. In addition, dependency information in the spirit of (Shen et al., 2008) is included. Jane features models for string-to-dependency language models and computes various scores based on the well-formedness of the resulting dependency tree. Jane supports the Stanford parsing format,1 but can be easily extended to other parsers. Language Models Jane supports four formats for n-gram language models: • The ARPA format for language models. We use the SRI toolkit (Stolcke, 2002) to suppor"
W10-1738,2009.eamt-1.33,1,0.841467,"t, a monolingual parsing of the input sentence is carried out using the CYK+ algorithm (Chappelier and Rajman, 1998), a generalization of the CYK algorithm which relaxes the requirement for the grammar to be in Chomsky normal form. From the CYK+ chart we extract a hypergraph representing the parsing space. In a second step the translations are generated, computing the language model scores in an integrated fashion. Both the cube pruning and cube growing algorithms (Huang and Chiang, 2007) are implemented. For the latter case, the extensions concerning the language model heuristics similar to (Vilar and Ney, 2009) have also been included. 3.2 Syntactic Features Soft syntactic features comparable to (Vilar et al., 2008) are implemented in the extraction step of the toolkit. In search, they are considered as additional feature functions of the translation rules. The decoder is able to handle an arbitrary number of non-terminal symbols. The extraction has been extended so that the extraction of SAMTrules is included (Zollmann and Venugopal, 2006) but this approach is not fully supported (there may be empty parses due to the extended number of non-terminals). We instead opted to support the generalization"
W10-1738,2008.iwslt-papers.7,1,0.842545,"n, 1998), a generalization of the CYK algorithm which relaxes the requirement for the grammar to be in Chomsky normal form. From the CYK+ chart we extract a hypergraph representing the parsing space. In a second step the translations are generated, computing the language model scores in an integrated fashion. Both the cube pruning and cube growing algorithms (Huang and Chiang, 2007) are implemented. For the latter case, the extensions concerning the language model heuristics similar to (Vilar and Ney, 2009) have also been included. 3.2 Syntactic Features Soft syntactic features comparable to (Vilar et al., 2008) are implemented in the extraction step of the toolkit. In search, they are considered as additional feature functions of the translation rules. The decoder is able to handle an arbitrary number of non-terminal symbols. The extraction has been extended so that the extraction of SAMTrules is included (Zollmann and Venugopal, 2006) but this approach is not fully supported (there may be empty parses due to the extended number of non-terminals). We instead opted to support the generalization presented in (Venugopal et al., 2009), where the information about the new non-terminals is included as an"
W10-1738,W09-0401,0,\N,Missing
W10-1738,P07-2045,0,\N,Missing
W10-1738,D08-1076,0,\N,Missing
W11-2104,C04-1046,0,0.80923,"of every given sentence. As qualitative criteria, we use statistical features indicating the quality and the grammaticality of the output. 2 2.1 Automatic ranking method From Confidence Estimation to ranking Confidence estimation has been seen from the Natural Language Processing (NLP) perspective as a problem of binary classification in order to assess the correctness of a NLP system output. Previous work focusing on Machine Translation includes statistical methods for estimating correctness scores or correctness probabilities, following a rich search over the spectrum of possible features (Blatz et al., 2004a; Ueffing and Ney, 2005; Specia et al., 2009; Raybaud and Caroline Lavecchia, 2009; Rosti et al., 65 Proceedings of the 6th Workshop on Statistical Machine Translation, pages 65–70, c Edinburgh, Scotland, UK, July 30–31, 2011. 2011 Association for Computational Linguistics 2007). In this work we slightly transform the binary classification practice to fit the standard WMT human evaluation process. As human annotators have provided their evaluation in the form of ranking of five system outputs at a sentence level, we build our evaluation mechanism with similar functionality, aiming to training"
W11-2104,W08-0309,0,0.121698,"ch metrics have been known as Confidence Estimation metrics and quite a few projects have suggested solutions on this direction. With our submission to the Shared Task, we allow such a metric to be systematically compared with the state-of-the-art reference-aware MT metrics. Our approach suggests building a Confidence Estimation metric using already existing human judgments. This has been motivated by the existence of human-annotated data containing comparisons of the outputs of several systems, as a result of the evaluation tasks run by the Workshops on Statistical Machine Translation (WMT) (Callison-Burch et al., 2008; Callison-Burch et al., 2009; Callison-Burch et al., 2010). This amount of data, which has been freely available for further research, gives an opportunity for applying machine learning techniques to model the human annotators’ choices. Machine Learning methods over previously released evaluation data have been already used for tuning complex statistical evaluation metrics (e.g. SVM-Rank in Callison-Burch et al. (2010)). Our proposition is similar, but works without reference translations. We develop a solution of applying machine learning in order to build a statistical classifier that perfo"
W11-2104,W10-1703,0,0.363914,"s and quite a few projects have suggested solutions on this direction. With our submission to the Shared Task, we allow such a metric to be systematically compared with the state-of-the-art reference-aware MT metrics. Our approach suggests building a Confidence Estimation metric using already existing human judgments. This has been motivated by the existence of human-annotated data containing comparisons of the outputs of several systems, as a result of the evaluation tasks run by the Workshops on Statistical Machine Translation (WMT) (Callison-Burch et al., 2008; Callison-Burch et al., 2009; Callison-Burch et al., 2010). This amount of data, which has been freely available for further research, gives an opportunity for applying machine learning techniques to model the human annotators’ choices. Machine Learning methods over previously released evaluation data have been already used for tuning complex statistical evaluation metrics (e.g. SVM-Rank in Callison-Burch et al. (2010)). Our proposition is similar, but works without reference translations. We develop a solution of applying machine learning in order to build a statistical classifier that performs similar to the human ranking: it is trained to rank sev"
W11-2104,W09-3712,0,0.0156559,"ng, the classifiers were used to perform ranking on a test set of 184 sentences which had been kept apart from the 2010 data, with the criterion that they do not contain contradictions among human judgments. In order to allow further comparison with other evaluation metrics, we performed an extended experiment: we trained the classifiers over the WMT 2008 and 2009 data and let them perform automatic ranking on the full WMT 2010 test set, this time without any restriction on human evaluation agreement. In both experiments, tokenization was performed with the PUNKT tokenizer (Kiss et al., 2006; Garrette and Klein, 2009), while n-gram features were generated with the SRILM toolkit (Stolcke, 2002). The language model was relatively big and had been built upon all lowercased monolingual training sets for the WMT 2011 Shared Task, interpolated on the 2007 test set. As a PCFG parser, the Berkeley Parser (Petrov and Klein, 2007) was preferred, due 1 data acquired from http://www.statmt.org/wmt11 67 Feature selection Although the automatic NLP tools provided a lot of features (section 2.3), the classification methods we used (and particularly naïve Bayes were the development was focused on) would be expected to per"
W11-2104,N07-1051,0,0.0080242,": we trained the classifiers over the WMT 2008 and 2009 data and let them perform automatic ranking on the full WMT 2010 test set, this time without any restriction on human evaluation agreement. In both experiments, tokenization was performed with the PUNKT tokenizer (Kiss et al., 2006; Garrette and Klein, 2009), while n-gram features were generated with the SRILM toolkit (Stolcke, 2002). The language model was relatively big and had been built upon all lowercased monolingual training sets for the WMT 2011 Shared Task, interpolated on the 2007 test set. As a PCFG parser, the Berkeley Parser (Petrov and Klein, 2007) was preferred, due 1 data acquired from http://www.statmt.org/wmt11 67 Feature selection Although the automatic NLP tools provided a lot of features (section 2.3), the classification methods we used (and particularly naïve Bayes were the development was focused on) would be expected to perform better given a smaller group of statistically independent features. Since exhaustive training/testing of all possible feature subsets was not possible, we performed feature selection based on the Relieff method (Kononenko, 1994; Kira and Rendell, 1992). Automatic ranking was performed based on the most"
W11-2104,P06-1055,0,0.00745077,"e pairwise comparison of system outputs ti and tj with respective ranks ri and rj , determined as:  1 ri &lt; rj c(ri , rj ) = −1 ri &gt; rj At testing time, after the classifier has made all the pairwise decisions, those need to be converted back to ranks. System entries are ordered, according to how many times each of them won in the pairwise comparison, leading to rank lists similar to the ones provided by human annotators. Note that this kind of decomposition allows for ties when there are equal times of winnings. 66 Acquiring features • Parsing: Processing features acquired from PCFG parsing (Petrov et al., 2006) for both source and target side include: – – – – parse log likelihood, number of n-best trees, confidence for the best parse, average confidence of all trees. Ratios of the above target features to their respective source features were included. • Shallow grammatical match: The number of occurences of particular node tags on both the source and the target was counted on the PCFG parses. In particular, NPs, VPs, PPs, NNs and punctuation occurences were counted. Then the ratio of the occurences of each tag in the target sentence by its occurences on the source sentence was also calculated. 2.4"
W11-2104,2009.eamt-1.15,0,0.0842697,"Missing"
W11-2104,N07-1029,0,0.0599919,"Missing"
W11-2104,2009.mtsummit-papers.16,0,0.0218978,"teria, we use statistical features indicating the quality and the grammaticality of the output. 2 2.1 Automatic ranking method From Confidence Estimation to ranking Confidence estimation has been seen from the Natural Language Processing (NLP) perspective as a problem of binary classification in order to assess the correctness of a NLP system output. Previous work focusing on Machine Translation includes statistical methods for estimating correctness scores or correctness probabilities, following a rich search over the spectrum of possible features (Blatz et al., 2004a; Ueffing and Ney, 2005; Specia et al., 2009; Raybaud and Caroline Lavecchia, 2009; Rosti et al., 65 Proceedings of the 6th Workshop on Statistical Machine Translation, pages 65–70, c Edinburgh, Scotland, UK, July 30–31, 2011. 2011 Association for Computational Linguistics 2007). In this work we slightly transform the binary classification practice to fit the standard WMT human evaluation process. As human annotators have provided their evaluation in the form of ranking of five system outputs at a sentence level, we build our evaluation mechanism with similar functionality, aiming to training from and evaluating against this data. Evalu"
W11-2104,H05-1096,0,0.181409,"nce. As qualitative criteria, we use statistical features indicating the quality and the grammaticality of the output. 2 2.1 Automatic ranking method From Confidence Estimation to ranking Confidence estimation has been seen from the Natural Language Processing (NLP) perspective as a problem of binary classification in order to assess the correctness of a NLP system output. Previous work focusing on Machine Translation includes statistical methods for estimating correctness scores or correctness probabilities, following a rich search over the spectrum of possible features (Blatz et al., 2004a; Ueffing and Ney, 2005; Specia et al., 2009; Raybaud and Caroline Lavecchia, 2009; Rosti et al., 65 Proceedings of the 6th Workshop on Statistical Machine Translation, pages 65–70, c Edinburgh, Scotland, UK, July 30–31, 2011. 2011 Association for Computational Linguistics 2007). In this work we slightly transform the binary classification practice to fit the standard WMT human evaluation process. As human annotators have provided their evaluation in the form of ranking of five system outputs at a sentence level, we build our evaluation mechanism with similar functionality, aiming to training from and evaluating aga"
W11-2104,W09-0401,0,\N,Missing
W11-2104,J06-4003,0,\N,Missing
W11-2109,W05-0909,0,0.0830542,"xplored in order to find the most promising directions. Correlations between the new metrics and human judgments are calculated on the data of the third, fourth and fifth shared tasks of the Statistical Machine Translation Workshop. Five different European languages are taken into account: English, Spanish, French, German and Czech. The results show that the IBM 1 scores are competitive with the classic evaluation metrics, the most promising being IBM 1 scores calculated on morphemes and POS-4grams. 1 Introduction Currently used evaluation metrics such as BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005), etc. are based on the comparison between human reference translations and the automatically generated hypotheses in the target language to be evaluated. While this scenario helps in the design of machine translation systems, it has two major drawbacks. The first one is the practical criticism that using reference translations is inefficient and expensive: in real-life situations, the quality of machine translation must be evaluated without having to pay humans for producing reference translations first. The second criticism is methodological: in using reference translation, the problem of ev"
W11-2109,J93-2003,0,0.0279455,"st be evaluated without having to pay humans for producing reference translations first. The second criticism is methodological: in using reference translation, the problem of evaluating translation quality (e.g., completeness, ordering, domain fit, etc.) is transformed into a kind of paraphrase evaluation in the target language, which is a very difficult problem itself. In addition, the set of selected references always represents only a small subset of all good translations. To remedy these drawbacks, we propose a truly automatic evaluation metric which is based on the IBM 1 lexicon scores (Brown et al., 1993). The inclusion of IBM 1 scores in translation systems has shown experimentally to improve translation quality (Och et al., 2003). They also have been used for confidence estimation for machine translation (Blatz et al., 2003). To the best of our knowledge, these scores have not yet been used as an evaluation metric. We carry out a systematic comparison between several variants of IBM 1 scores. The Spearman’s rank correlation coefficients on the document (system) level between the IBM 1 metrics and the human ranking are computed on the English, French, Spanish, German and Czech texts generated"
W11-2109,W08-0309,0,0.0789113,"n experimentally to improve translation quality (Och et al., 2003). They also have been used for confidence estimation for machine translation (Blatz et al., 2003). To the best of our knowledge, these scores have not yet been used as an evaluation metric. We carry out a systematic comparison between several variants of IBM 1 scores. The Spearman’s rank correlation coefficients on the document (system) level between the IBM 1 metrics and the human ranking are computed on the English, French, Spanish, German and Czech texts generated by various translation systems in the framework of the third (Callison-Burch et al., 2008), fourth (CallisonBurch et al., 2009) and fifth (Callison-Burch et al., 2010) shared translation tasks. 2 IBM 1 scores The IBM 1 model is a bag-of-word translation model which gives the sum of all possible alignment probabilities between the words in the source sentence and the words in the target sentence. Brown et al. (1993) defined the IBM 1 probability score for a translation 99 Proceedings of the 6th Workshop on Statistical Machine Translation, pages 99–103, c Edinburgh, Scotland, UK, July 30–31, 2011. 2011 Association for Computational Linguistics pair f1J and eI1 in the following way: P"
W11-2109,W10-1703,0,0.0538159,"have been used for confidence estimation for machine translation (Blatz et al., 2003). To the best of our knowledge, these scores have not yet been used as an evaluation metric. We carry out a systematic comparison between several variants of IBM 1 scores. The Spearman’s rank correlation coefficients on the document (system) level between the IBM 1 metrics and the human ranking are computed on the English, French, Spanish, German and Czech texts generated by various translation systems in the framework of the third (Callison-Burch et al., 2008), fourth (CallisonBurch et al., 2009) and fifth (Callison-Burch et al., 2010) shared translation tasks. 2 IBM 1 scores The IBM 1 model is a bag-of-word translation model which gives the sum of all possible alignment probabilities between the words in the source sentence and the words in the target sentence. Brown et al. (1993) defined the IBM 1 probability score for a translation 99 Proceedings of the 6th Workshop on Statistical Machine Translation, pages 99–103, c Edinburgh, Scotland, UK, July 30–31, 2011. 2011 Association for Computational Linguistics pair f1J and eI1 in the following way: P (f1J |eI1 ) = 3 Experiments on WMT 2008, WMT 2009 and WMT 2010 test data I J"
W11-2109,P02-1040,0,0.0908633,"BM 1 scores are systematically explored in order to find the most promising directions. Correlations between the new metrics and human judgments are calculated on the data of the third, fourth and fifth shared tasks of the Statistical Machine Translation Workshop. Five different European languages are taken into account: English, Spanish, French, German and Czech. The results show that the IBM 1 scores are competitive with the classic evaluation metrics, the most promising being IBM 1 scores calculated on morphemes and POS-4grams. 1 Introduction Currently used evaluation metrics such as BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005), etc. are based on the comparison between human reference translations and the automatically generated hypotheses in the target language to be evaluated. While this scenario helps in the design of machine translation systems, it has two major drawbacks. The first one is the practical criticism that using reference translations is inefficient and expensive: in real-life situations, the quality of machine translation must be evaluated without having to pay humans for producing reference translations first. The second criticism is methodological: in using refer"
W11-2109,E09-1087,0,0.0855911,"Missing"
W11-2109,W09-0401,0,\N,Missing
W11-2109,C04-1046,0,\N,Missing
W11-2161,J93-2003,0,0.0156366,"ule-based translation systems are applied. In addition, three statistical machine translation systems are built, including a phrase-based, a hierarchical phrase-based and a syntax-based system. Instead of combining with rules or post-editing, we perform system combination on the final translation hypotheses. We applied the CMU open toolkit (Heafield and Lavie, 2010) among numerous combination methods such as (Matusov, 2009), (Sim et al., 2007) and (He et al., 2008). The final translation output outperforms each individual output significantly. Phrase-based system We use the IBM model 1 and 4 (Brown et al., 1993) and Hidden-Markov model (HMM) (Vogel et al., 1996) to train the word alignment using the mgiza toolkit1 . We applied the EMS in Moses (Koehn et al., 2007) to build up the phrase-based translation system. Features in the log-linear model include translation models in two directions, a language model, a distortion model and a sentence length penalty. A dynamic programming beam search algorithm is used to generate the translation hypothesis with maximum probability. We applied a 5gram mixture language model with each sub-model trained on one fifth of the monolingual corpus with Kneser-Ney smooth"
W11-2161,J07-2003,0,0.052603,"dings of the 6th Workshop on Statistical Machine Translation, pages 485–489, c Edinburgh, Scotland, UK, July 30–31, 2011. 2011 Association for Computational Linguistics improves the translation performance more than the tuning on test2008 in a small-scale experiment for the tree-based system. 2.3 Hierarchical phrase-based system For the hierarchical system, we used the open source hierarchical phrased-based system Jane, developed at RWTH and free for non-commercial use (Vilar et al., 2010). This approach is an extension of the phrase-based approach, where the phrases are allowed to have gaps (Chiang, 2007). In this way long-range dependencies and reorderings can be modeled in a consistent statistical framework. The system uses a fairly standard setup, trained using the bilingual data provided by the organizers, word aligned using the mgiza. Two 5-gram language models were used during decoding: one trained on the monolingual part of the bilingual training data, and a larger one trained on the additional news data. Decoding was carried out using the cube pruning algorithm. The tuning is performed on test2008 without further experiments. 2.4 Rule-based systems We applied two rule-based translation"
W11-2161,2008.tc-1.2,0,0.191203,"tems are combined at the level of the final translation output. The translation results show that our hybrid system significantly outperformed individual systems by exploring strengths of both rule-based and statistical translations. 1 Individual translation systems 2.1 Introduction Machine translation (MT), in particular the statistical approach to it, has undergone incremental improvements in recent years. While rule-based machine translation (RBMT) maintains competitiveness in human evaluations. Combining the advantages of both approaches have been investigated by many researchers such as (Eisele et al., 2008). Nonetheless, significant improvements over statistical approaches still remain to be shown. In this paper, we present the DFKI hybrid system in the WMT workshop 2011. Our system is different from the system of the last year (Federmann et al., 2010), which is based on the shallow phrase substitution. In this work, two rule-based translation systems are applied. In addition, three statistical machine translation systems are built, including a phrase-based, a hierarchical phrase-based and a syntax-based system. Instead of combining with rules or post-editing, we perform system combination on th"
W11-2161,W10-1708,1,0.849156,"translation systems 2.1 Introduction Machine translation (MT), in particular the statistical approach to it, has undergone incremental improvements in recent years. While rule-based machine translation (RBMT) maintains competitiveness in human evaluations. Combining the advantages of both approaches have been investigated by many researchers such as (Eisele et al., 2008). Nonetheless, significant improvements over statistical approaches still remain to be shown. In this paper, we present the DFKI hybrid system in the WMT workshop 2011. Our system is different from the system of the last year (Federmann et al., 2010), which is based on the shallow phrase substitution. In this work, two rule-based translation systems are applied. In addition, three statistical machine translation systems are built, including a phrase-based, a hierarchical phrase-based and a syntax-based system. Instead of combining with rules or post-editing, we perform system combination on the final translation hypotheses. We applied the CMU open toolkit (Heafield and Lavie, 2010) among numerous combination methods such as (Matusov, 2009), (Sim et al., 2007) and (He et al., 2008). The final translation output outperforms each individual"
W11-2161,D08-1011,0,0.0236919,"stem is different from the system of the last year (Federmann et al., 2010), which is based on the shallow phrase substitution. In this work, two rule-based translation systems are applied. In addition, three statistical machine translation systems are built, including a phrase-based, a hierarchical phrase-based and a syntax-based system. Instead of combining with rules or post-editing, we perform system combination on the final translation hypotheses. We applied the CMU open toolkit (Heafield and Lavie, 2010) among numerous combination methods such as (Matusov, 2009), (Sim et al., 2007) and (He et al., 2008). The final translation output outperforms each individual output significantly. Phrase-based system We use the IBM model 1 and 4 (Brown et al., 1993) and Hidden-Markov model (HMM) (Vogel et al., 1996) to train the word alignment using the mgiza toolkit1 . We applied the EMS in Moses (Koehn et al., 2007) to build up the phrase-based translation system. Features in the log-linear model include translation models in two directions, a language model, a distortion model and a sentence length penalty. A dynamic programming beam search algorithm is used to generate the translation hypothesis with ma"
W11-2161,2010.amta-papers.34,0,0.0250441,"es still remain to be shown. In this paper, we present the DFKI hybrid system in the WMT workshop 2011. Our system is different from the system of the last year (Federmann et al., 2010), which is based on the shallow phrase substitution. In this work, two rule-based translation systems are applied. In addition, three statistical machine translation systems are built, including a phrase-based, a hierarchical phrase-based and a syntax-based system. Instead of combining with rules or post-editing, we perform system combination on the final translation hypotheses. We applied the CMU open toolkit (Heafield and Lavie, 2010) among numerous combination methods such as (Matusov, 2009), (Sim et al., 2007) and (He et al., 2008). The final translation output outperforms each individual output significantly. Phrase-based system We use the IBM model 1 and 4 (Brown et al., 1993) and Hidden-Markov model (HMM) (Vogel et al., 1996) to train the word alignment using the mgiza toolkit1 . We applied the EMS in Moses (Koehn et al., 2007) to build up the phrase-based translation system. Features in the log-linear model include translation models in two directions, a language model, a distortion model and a sentence length penalt"
W11-2161,P07-2045,0,0.00338684,"hrase-based and a syntax-based system. Instead of combining with rules or post-editing, we perform system combination on the final translation hypotheses. We applied the CMU open toolkit (Heafield and Lavie, 2010) among numerous combination methods such as (Matusov, 2009), (Sim et al., 2007) and (He et al., 2008). The final translation output outperforms each individual output significantly. Phrase-based system We use the IBM model 1 and 4 (Brown et al., 1993) and Hidden-Markov model (HMM) (Vogel et al., 1996) to train the word alignment using the mgiza toolkit1 . We applied the EMS in Moses (Koehn et al., 2007) to build up the phrase-based translation system. Features in the log-linear model include translation models in two directions, a language model, a distortion model and a sentence length penalty. A dynamic programming beam search algorithm is used to generate the translation hypothesis with maximum probability. We applied a 5gram mixture language model with each sub-model trained on one fifth of the monolingual corpus with Kneser-Ney smoothing using SRILM toolkit (Stolcke, 2002). We did not perform any tuning, because it hurts the evaluation performance in our experiments. 2.2 Syntax-based sy"
W11-2161,2005.mtsummit-papers.11,0,0.0347851,"Missing"
W11-2161,W11-2100,0,0.0883295,"uage models for tuning and all target side of parallel data to train language models for decoding. The beam size is set to 80, and 300 nbest is considered. 4 4.1 Translation experiments MT Setup The parallel training corpus consists of 1.8 million German-English parallel sentences from Europarl-v6 (Koehn, MT Summit 2005) and newscommentary with 48 million tokenized German words and 54 million tokenized English words respectively. The monolingual training corpus contains the target side of the parallel training corpus and the additional monolingual language model training data downloaded from (SMT, 2011). We did not apply the large-scale Gigaword corpus, because it does not significantly reduce the perplexity of our language model but raises the computational requirement heavily. 4.2 Single systems For each individual translation system, different configurations are experimented to achieve a higher translation quality. We take phrase- and syntaxbased translation system as examples. Table 1 presents official submission result on DE-EN by PBT+Syntax PBT+Syntax+HPBT PBT+HPBT+Linguatec+Lucy PBT+Syntax+HPBT+Linguatec+Lucy 20.37 20.78 20.27 20.81 Hybrid-2010 PBT Syntax HPBT Linguatec Lucy Hybrid-20"
W11-2161,C08-1115,0,0.0196628,"ms We applied two rule-based translation systems, the Lucy system (Lucy, 2011) and the Linguatec system (Aleksi´c and Thurmair, 2011). The Lucy system is a recent offspring of METAL. The Linguatec system is a modular system consisting of grammar, lexicon and morphological analyzers based on logic programming using slot grammar. 3 PBT-2010 Max80words Max100words +Compound +Newparallel Hybrid translation A hybrid approach combining rule-based and statistical machine translation is usually investigated with an in-box integration, such as multi-way translation (Eisele et al., 2008), post-editing (Ueffing et al., 2008) or noun phrase substitution (Federmann et al., 2010). However, significant improvements over state-of-the-art statistical machine translation are still expected. In the meanwhile system combination methods for instance described in (Matusov, 2009), (Sim et al., 2007) and (He et al., 2008) are mostly evaluated to combine statistical translation systems, rule-based systems are not considered. In this work, we integrate the rule-based and statistical machine translation system on the level of the final 486 PBT 18.32 20.65 20.78 21.52 21.77 Syntax 21.10 22.13 Table 1: Translation performance BLEU"
W11-2161,W10-1738,1,0.847962,"eature weights using mert, because the tuning on test2007 1 http://geek.kyloo.net/software/doku.php/mgiza:overview 485 Proceedings of the 6th Workshop on Statistical Machine Translation, pages 485–489, c Edinburgh, Scotland, UK, July 30–31, 2011. 2011 Association for Computational Linguistics improves the translation performance more than the tuning on test2008 in a small-scale experiment for the tree-based system. 2.3 Hierarchical phrase-based system For the hierarchical system, we used the open source hierarchical phrased-based system Jane, developed at RWTH and free for non-commercial use (Vilar et al., 2010). This approach is an extension of the phrase-based approach, where the phrases are allowed to have gaps (Chiang, 2007). In this way long-range dependencies and reorderings can be modeled in a consistent statistical framework. The system uses a fairly standard setup, trained using the bilingual data provided by the organizers, word aligned using the mgiza. Two 5-gram language models were used during decoding: one trained on the monolingual part of the bilingual training data, and a larger one trained on the additional news data. Decoding was carried out using the cube pruning algorithm. The tu"
W11-2161,C96-2141,0,0.206957,"ion, three statistical machine translation systems are built, including a phrase-based, a hierarchical phrase-based and a syntax-based system. Instead of combining with rules or post-editing, we perform system combination on the final translation hypotheses. We applied the CMU open toolkit (Heafield and Lavie, 2010) among numerous combination methods such as (Matusov, 2009), (Sim et al., 2007) and (He et al., 2008). The final translation output outperforms each individual output significantly. Phrase-based system We use the IBM model 1 and 4 (Brown et al., 1993) and Hidden-Markov model (HMM) (Vogel et al., 1996) to train the word alignment using the mgiza toolkit1 . We applied the EMS in Moses (Koehn et al., 2007) to build up the phrase-based translation system. Features in the log-linear model include translation models in two directions, a language model, a distortion model and a sentence length penalty. A dynamic programming beam search algorithm is used to generate the translation hypothesis with maximum probability. We applied a 5gram mixture language model with each sub-model trained on one fifth of the monolingual corpus with Kneser-Ney smoothing using SRILM toolkit (Stolcke, 2002). We did not"
W11-2211,W99-0604,1,\N,Missing
W11-2211,E09-1044,0,\N,Missing
W11-2211,W09-0424,0,\N,Missing
W11-2211,P02-1040,0,\N,Missing
W11-2211,W10-1738,1,\N,Missing
W11-2211,2008.iwslt-papers.6,0,\N,Missing
W11-2211,P10-1049,1,\N,Missing
W11-2211,P07-2045,0,\N,Missing
W11-2211,W07-0733,0,\N,Missing
W11-2211,P05-1033,0,\N,Missing
W11-2211,N03-1017,0,\N,Missing
W11-2211,J03-1002,1,\N,Missing
W11-2211,2009.iwslt-papers.4,0,\N,Missing
W11-2211,P07-1019,0,\N,Missing
W11-2211,2009.mtsummit-posters.17,0,\N,Missing
W11-2211,2010.iwslt-papers.11,1,\N,Missing
W11-2211,J07-2003,0,\N,Missing
W11-2211,D08-1076,0,\N,Missing
W11-2211,W07-0717,0,\N,Missing
W12-3149,E06-1032,0,0.0662158,"Missing"
W12-3149,N10-1080,0,0.0148123,"M Poor Man’s Syntax 12.4 11.6 13.1 11.6 Table 2: Translation results for the different single systems, English to German the systems. We have used features that try to focus on characteristics that humans may use to evaluate a system. 3.1 Cross System BLEU BLEU was introduced in (Papineni et al., 2002) and it has been shown to have a high correlation with human judgement. In spite of its shortcomings (Callison-Burch et al., 2006), it has been considered the standard automatic measure in the development of SMT systems (with new measures being added to it, but not substituting it, see for e.g. (Cer et al., 2010)). Of course, the main problem of using the BLEU score as a feature for sentence selection in a reallife scenario is that we do not have the references available. We overcame this issue by generating a custom set of references for each system, using the other systems as gold translations. This is of course inexact, but n-grams that appear on the output of different systems can be expected to be more probable to be correct, and BLEU calculated this way gives us a measure of this agreement. This approach can be considered related to n-gram posteriors (Zens and Ney, 2006) or minimum Bayes risk de"
W12-3149,P07-2026,0,0.0226813,"the main problem of using the BLEU score as a feature for sentence selection in a reallife scenario is that we do not have the references available. We overcame this issue by generating a custom set of references for each system, using the other systems as gold translations. This is of course inexact, but n-grams that appear on the output of different systems can be expected to be more probable to be correct, and BLEU calculated this way gives us a measure of this agreement. This approach can be considered related to n-gram posteriors (Zens and Ney, 2006) or minimum Bayes risk decoding (e.g. (Ehling et al., 2007)) in the context of 384 n-best rescoring, but applied without prior weighting (unavailable directly) and more focused on the evaluation interpretation. We generated two features based on this idea. The first one is computed at the system level, i.e. it is the same for each sentence produced by a system and serves as a kind of prior weight similar to the one used in other system combination methods (e.g. (Matusov et al., 2008)). The other feature was computed at the sentence level. For this we used the smoothed version of BLEU proposed in (Lin and Och, 2004), again using the output of the rest"
W12-3149,N07-2015,0,0.0316161,"we do not have a reference available, and thus we use the rest of the systems as pseudoreferences. This has the interesting effect that some “errors” are actually beneficial for the performance of the system. For example, it is known that systems optimised on the BLEU metric tend to produce short hypotheses. In this sense, the extra words considered as errors by the EXTer measure may be actually beneficial for the overall performance of the system. 3.3 IBM1 Scores IBM1-like scores on the sentence level are known to perform well for the rescoring of n-best lists from a single system (see e.g. (Hasan et al., 2007)). Additionally, they have been shown in (Popovic et al., 2011) to correlate well with human judgement for evaluation purposes. We thus include them as additional features. 2 Computed using the TreeTagger tool (http://www.ims.unistuttgart.de/projekte/corplex/TreeTagger/) 3 The abbreviations for the features are taken over directly from the output of the tool. 385 3.4 Additional Language Model We used a 5-gram language model trained on the whole news-crawl corpus as an additional model for rescoring. We used a different language model as the one described in Section 2.1.2 as not to favor those"
W12-3149,2008.amta-srw.3,0,0.0321145,"Missing"
W12-3149,W11-2211,1,0.834362,"n Section 2.1.1. 2.2.1 Poor Man’s Syntax Vilar et al. (2010b) propose a “syntax-based” approach similar to (Venugopal et al., 2009), but using automatic clustering methods instead of linguistic parsing for defining the non-terminals used in the resulting grammar. The main idea of the method is to cluster the words (mimicking the concept of Partof-Speech tagging), performing a phrase extraction pass using the word classes instead of the actual words and performing another clustering on the phrase level (corresponding to the linguistic classes in a parse tree). 2.2.2 Lightly-Supervised Training Huck et al. (2011) propose to augment the monolingual training data by translating available additional monolingual data with an existing translation system. We adapt this approach by translating the data selected according to Section 2.1.2 with the phrase-based translation system described in Section 2.1, and use this additional data to expand the bilingual data available for training the hierarchical phrase-based system.1 2.3 Experimental Results Table 1 shows the results obtained for the German to English translation direction on the newstest2011 dataset. The baseline phrase-based system obtains a 1 The deci"
W12-3149,C04-1072,0,0.0265676,"minimum Bayes risk decoding (e.g. (Ehling et al., 2007)) in the context of 384 n-best rescoring, but applied without prior weighting (unavailable directly) and more focused on the evaluation interpretation. We generated two features based on this idea. The first one is computed at the system level, i.e. it is the same for each sentence produced by a system and serves as a kind of prior weight similar to the one used in other system combination methods (e.g. (Matusov et al., 2008)). The other feature was computed at the sentence level. For this we used the smoothed version of BLEU proposed in (Lin and Och, 2004), again using the output of the rest of the systems as pseudo-reference. As optimization on BLEU often tends to generate short translations, we also include a word penalty feature. 3.2 Error Analysis Features It is safe to assume that a human judge will try to choose those translations which contain the least amount of errors, both in terms of content and grammaticality. A classification of errors for machine translation systems has been proposed in (Vilar et al., 2006), and (Popovi´c and Ney, 2011) presents how to compute a subset of these error categories automatically. The basic idea is to"
W12-3149,P10-2041,0,0.0281005,"Jane using standard settings, i.e. maximum source phrase length 6, maximum target phrase length 12, count features, etc. Consult the Jane documentation for more details. For reordering the standard distancebased reordering model is computed. Scaling factors are trained using MERT on n-best lists. 2.1.1 Verb reorderings Following (Popovi´c and Ney, 2006), for German to English translation, we perform verb reordering by first POS-tagging the source sentence and afterwards applying hand-defined rules. This includes rules for reordering verbs in subordinate clauses and participles. 2.1.2 Moore LM Moore and Lewis (2010) propose a method for filtering large quantities of out-of-domain languagemodel training data by comparing the cross-entropy 382 Proceedings of the 7th Workshop on Statistical Machine Translation, pages 382–387, c Montr´eal, Canada, June 7-8, 2012. 2012 Association for Computational Linguistics of an in-domain language model and an out-ofdomain language model trained on a random sampling of the data. We followed this approach to filter the news-crawl corpora provided the organizers. By experimenting on the development set we decided to use a 4-gram language model trained on 15M filtered senten"
W12-3149,J03-1002,0,0.00416077,"a more complete system for this last translation direction. This paper is organized as follows: Section 2 reports on the different single systems that we built for this shared task. Section 3 describes the sentence selection mechanism used for combining the output of the different systems. Section 4 concludes the paper. 2 Single Systems For all our setups we used the Jane toolkit (Vilar et al., 2010a), which in its current version supPhrase-based System The first system is a baseline phrase-based system trained on the available bilingual training data. Word alignments is trained using GIZA++ (Och and Ney, 2003), phrase extraction is performed with Jane using standard settings, i.e. maximum source phrase length 6, maximum target phrase length 12, count features, etc. Consult the Jane documentation for more details. For reordering the standard distancebased reordering model is computed. Scaling factors are trained using MERT on n-best lists. 2.1.1 Verb reorderings Following (Popovi´c and Ney, 2006), for German to English translation, we perform verb reordering by first POS-tagging the source sentence and afterwards applying hand-defined rules. This includes rules for reordering verbs in subordinate cl"
W12-3149,P02-1040,0,0.0845237,"Missing"
W12-3149,popovic-ney-2006-pos,0,0.0671868,"Missing"
W12-3149,J11-4002,0,0.022275,"Missing"
W12-3149,N09-1027,0,0.0180169,"the data. We followed this approach to filter the news-crawl corpora provided the organizers. By experimenting on the development set we decided to use a 4-gram language model trained on 15M filtered sentences (the original data comprising over 30M sentences). 2.2 Hierarchical System We also trained a hierarchical system on the same data as the phrase-based system, and also tried the additional language model trained according to Section 2.1.2, as well as the verb reorderings described in Section 2.1.1. 2.2.1 Poor Man’s Syntax Vilar et al. (2010b) propose a “syntax-based” approach similar to (Venugopal et al., 2009), but using automatic clustering methods instead of linguistic parsing for defining the non-terminals used in the resulting grammar. The main idea of the method is to cluster the words (mimicking the concept of Partof-Speech tagging), performing a phrase extraction pass using the word classes instead of the actual words and performing another clustering on the phrase level (corresponding to the linguistic classes in a parse tree). 2.2.2 Lightly-Supervised Training Huck et al. (2011) propose to augment the monolingual training data by translating available additional monolingual data with an ex"
W12-3149,vilar-etal-2006-error,1,0.864313,"Missing"
W12-3149,W10-1738,1,0.938176,"m. Somewhat disappointingly the sentence selection hardly improves over the best single system. DFKI participated in the German to English and English to German translation tasks. Technical problems however hindered a more complete system for this last translation direction. This paper is organized as follows: Section 2 reports on the different single systems that we built for this shared task. Section 3 describes the sentence selection mechanism used for combining the output of the different systems. Section 4 concludes the paper. 2 Single Systems For all our setups we used the Jane toolkit (Vilar et al., 2010a), which in its current version supPhrase-based System The first system is a baseline phrase-based system trained on the available bilingual training data. Word alignments is trained using GIZA++ (Och and Ney, 2003), phrase extraction is performed with Jane using standard settings, i.e. maximum source phrase length 6, maximum target phrase length 12, count features, etc. Consult the Jane documentation for more details. For reordering the standard distancebased reordering model is computed. Scaling factors are trained using MERT on n-best lists. 2.1.1 Verb reorderings Following (Popovi´c and N"
W12-3149,2010.iwslt-papers.18,1,0.927024,"m. Somewhat disappointingly the sentence selection hardly improves over the best single system. DFKI participated in the German to English and English to German translation tasks. Technical problems however hindered a more complete system for this last translation direction. This paper is organized as follows: Section 2 reports on the different single systems that we built for this shared task. Section 3 describes the sentence selection mechanism used for combining the output of the different systems. Section 4 concludes the paper. 2 Single Systems For all our setups we used the Jane toolkit (Vilar et al., 2010a), which in its current version supPhrase-based System The first system is a baseline phrase-based system trained on the available bilingual training data. Word alignments is trained using GIZA++ (Och and Ney, 2003), phrase extraction is performed with Jane using standard settings, i.e. maximum source phrase length 6, maximum target phrase length 12, count features, etc. Consult the Jane documentation for more details. For reordering the standard distancebased reordering model is computed. Scaling factors are trained using MERT on n-best lists. 2.1.1 Verb reorderings Following (Popovi´c and N"
W12-3149,2011.iwslt-evaluation.13,1,0.812529,"Missing"
W12-3149,W06-3110,0,0.0268038,"tituting it, see for e.g. (Cer et al., 2010)). Of course, the main problem of using the BLEU score as a feature for sentence selection in a reallife scenario is that we do not have the references available. We overcame this issue by generating a custom set of references for each system, using the other systems as gold translations. This is of course inexact, but n-grams that appear on the output of different systems can be expected to be more probable to be correct, and BLEU calculated this way gives us a measure of this agreement. This approach can be considered related to n-gram posteriors (Zens and Ney, 2006) or minimum Bayes risk decoding (e.g. (Ehling et al., 2007)) in the context of 384 n-best rescoring, but applied without prior weighting (unavailable directly) and more focused on the evaluation interpretation. We generated two features based on this idea. The first one is computed at the system level, i.e. it is the same for each sentence produced by a system and serves as a kind of prior weight similar to the one used in other system combination methods (e.g. (Matusov et al., 2008)). The other feature was computed at the sentence level. For this we used the smoothed version of BLEU proposed"
W12-3149,W11-2109,1,\N,Missing
W13-0804,2010.amta-papers.7,0,\N,Missing
W13-0804,W12-3128,0,\N,Missing
W13-0804,E09-1044,0,\N,Missing
W13-0804,W05-1506,0,\N,Missing
W13-0804,J10-4005,0,\N,Missing
W13-0804,W09-0424,0,\N,Missing
W13-0804,D11-1020,0,\N,Missing
W13-0804,P02-1040,0,\N,Missing
W13-0804,W10-1738,1,\N,Missing
W13-0804,P12-3004,0,\N,Missing
W13-0804,N09-1027,0,\N,Missing
W13-0804,P07-2045,0,\N,Missing
W13-0804,D12-1107,0,\N,Missing
W13-0804,N13-1116,0,\N,Missing
W13-0804,W08-0402,0,\N,Missing
W13-0804,P05-1033,0,\N,Missing
W13-0804,P10-4002,0,\N,Missing
W13-0804,J03-1002,1,\N,Missing
W13-0804,2009.iwslt-papers.4,0,\N,Missing
W13-0804,P07-1019,0,\N,Missing
W13-0804,2011.eamt-1.37,1,\N,Missing
W13-0804,2012.eamt-1.44,0,\N,Missing
W13-0804,J07-2003,0,\N,Missing
W13-0804,W12-3150,0,\N,Missing
W13-0804,D08-1076,0,\N,Missing
W13-0804,2011.iwslt-evaluation.24,0,\N,Missing
W18-1820,W17-3203,1,0.807221,"ing any optimizer from MXN ET’s library, including stochastic gradient descent (SGD) and Adam [Kingma and Ba, 2014]. S OCKEYE also includes its own implementation of the Eve optimizer, which extends Adam by incorporating information from the objective function [Koushik and Hayashi, 2016]. This allows learning to accelerate over ﬂat areas of the loss surface and decelerate when saddle points cause the objective to “bounce”. Learning schedules Recent work has shown the value of annealing the base learning rate α throughout training, even when using optimizers such as Adam [Vaswani et al., 2017, Denkowski and Neubig, 2017]. S OCKEYE’s ‘plateau-reduce’ scheduler implements rate annealing as follows. At each training checkpoint, the scheduler compares validation set perplexity against the best previous checkpoint. If perplexity has not surpassed the previous best in N checkpoints, the learning rate α is multiplied by a ﬁxed constant and the counter is reset. Optionally, the scheduler can reset model and optimizer parameters to the best previous point, simulating a perfect prediction of when to anneal the learning rate. 4 Note that the transformer and convolutional architectures cannot use these attention types."
W18-1820,D17-1300,0,0.0518893,"Missing"
W18-1820,N13-1073,0,0.076416,"Missing"
W18-1820,E17-3017,0,0.259665,"(version 1.12) 2 https://mxnet.incubator.apache.org/ Proceedings of AMTA 2018, vol. 1: MT Research Track Boston, March 17 - 21, 2018 |Page 200 2 Encoder-Decoder Models for NMT The main idea in neural sequence-to-sequence modeling is to encode a variable-length input sequence of tokens into a sequence of vector representations, and to then decode those representations into a sequence of output tokens. We give a brief description for the three encoder-decoder architectures as implemented in S OCKEYE, but refer the reader to the references for more details or to the arXiv version of this paper [Hieber et al., 2017]. 2.1 Stacked Recurrent Neural Network (RNN) with Attention [Bahdanau et al., 2014, Luong et al., 2015] The encoder consists of a bi-directional RNN followed by a stack of uni-directional RNNs. An RNN at layer l produces a sequence of hidden states hl1 . . . hln : l hli = fenc (hl−1 i , hi−1 ), (1) where frnn is some non-linear function, such as a Gated Recurrent Unit (GRU) or Long Short Term Memory (LSTM) cell, and hl−1 is the output from the lower layer at step i. The bii directional RNN on the lowest layer uses the embeddings ES xi as input and concatenates the → − ← − hidden states of a f"
W18-1820,D13-1176,0,0.0429926,"on quality and computational efﬁciency. In a trend that carries over from Statistical Machine Translation (SMT), the strongest NMT systems beneﬁt from subtle architecture modiﬁcations, hyper-parameter tuning, and empirically effective heuristics. To address these challenges, we introduce S OCKEYE, a neural sequence-to-sequence toolkit written in Python and built on Apache MXN ET2 [Chen et al., 2015]. To the best of our knowledge, S OCKEYE is the only toolkit that includes implementations of all three major neural translation architectures: attentional recurrent neural networks [Schwenk, 2012, Kalchbrenner and Blunsom, 2013, Sutskever et al., 2014, Bahdanau et al., 2014, Luong et al., 2015], selfattentional transformers [Vaswani et al., 2017], and fully convolutional networks [Gehring et al., 2017]. These implementations are supported by a wide and continually updated range of features reﬂecting the best ideas from recent literature. Users can easily train models based on the latest research, compare different architectures, and extend them by adding their own code. S OCKEYE is under active development that follows best practice for both research and production software, including clear coding and documentation"
W18-1820,P17-4012,0,0.0330319,"sh into German (EN→DE) and Latvian into English (LV→EN). Models in both language pairs were based on the complete parallel data provided for each task as part of the Second Conference on Machine Translation [Bojar et al., 2017]. We ran each toolkit in a “best available” conﬁguration, where we took settings from recent relevant papers that used the toolkit, or communicated directly with authors. Toolkits evaluated include FAIR S EQ [Gehring et al., 2017], M ARIAN [Junczys-Dowmunt et al., 2016], N EMATUS [Sennrich et al., 2017b], N EURAL M ONKEY [Helcl and Jindˇrich Libovický, 2017], O PEN NMT [Klein et al., 2017], and T ENSOR 2T ENSOR (T2T)8 .9 Shown in Table 1, S OCKEYE’s RNN model is competitive with M ARIAN, the top-performer, the CNN model outperforms FAIR S EQ, and the transformer outperforms all models from all other toolkits. See [Hieber et al., 2017] for more extensive comparisons and further details. 5 Summary We have presented S OCKEYE, a mature, open-source framework for neural sequence-to-sequence learning that implements the three major architectures for neural machine translation (the only one to do so, to our knowledge). Written in Python, it is easy to install, extend, and deploy; bui"
W18-1820,D15-1166,0,0.757671,"stical Machine Translation (SMT), the strongest NMT systems beneﬁt from subtle architecture modiﬁcations, hyper-parameter tuning, and empirically effective heuristics. To address these challenges, we introduce S OCKEYE, a neural sequence-to-sequence toolkit written in Python and built on Apache MXN ET2 [Chen et al., 2015]. To the best of our knowledge, S OCKEYE is the only toolkit that includes implementations of all three major neural translation architectures: attentional recurrent neural networks [Schwenk, 2012, Kalchbrenner and Blunsom, 2013, Sutskever et al., 2014, Bahdanau et al., 2014, Luong et al., 2015], selfattentional transformers [Vaswani et al., 2017], and fully convolutional networks [Gehring et al., 2017]. These implementations are supported by a wide and continually updated range of features reﬂecting the best ideas from recent literature. Users can easily train models based on the latest research, compare different architectures, and extend them by adding their own code. S OCKEYE is under active development that follows best practice for both research and production software, including clear coding and documentation guidelines, comprehensive automatic tests, and peer review for code"
W18-1820,C12-2104,0,0.0340888,"both translation quality and computational efﬁciency. In a trend that carries over from Statistical Machine Translation (SMT), the strongest NMT systems beneﬁt from subtle architecture modiﬁcations, hyper-parameter tuning, and empirically effective heuristics. To address these challenges, we introduce S OCKEYE, a neural sequence-to-sequence toolkit written in Python and built on Apache MXN ET2 [Chen et al., 2015]. To the best of our knowledge, S OCKEYE is the only toolkit that includes implementations of all three major neural translation architectures: attentional recurrent neural networks [Schwenk, 2012, Kalchbrenner and Blunsom, 2013, Sutskever et al., 2014, Bahdanau et al., 2014, Luong et al., 2015], selfattentional transformers [Vaswani et al., 2017], and fully convolutional networks [Gehring et al., 2017]. These implementations are supported by a wide and continually updated range of features reﬂecting the best ideas from recent literature. Users can easily train models based on the latest research, compare different architectures, and extend them by adding their own code. S OCKEYE is under active development that follows best practice for both research and production software, including"
W18-1820,C16-1165,0,0.0273615,"as source and references are typically tokenized and possibly byte-pair encoded. All statistics can also be written to a Tensorboard event ﬁle that can be rendered by a standalone Tensorboard fork.5 Regularization S OCKEYE supports standard techniques for regularization, such as dropout. This includes dropout on input embeddings for both the source and the target and the proposed dropout layers for the transformer architecture. One can also enable variational dropout [Gal and Ghahramani, 2016] to sample a ﬁxed dropout mask across recurrent time steps, or recurrent dropout without memory loss [Semeniuta et al., 2016]. S OCKEYE can also use MXN ET’s label smoothing [Pereyra et al., 2017] feature to efﬁciently back-propagate smoothed cross-entropy gradients without explicitly representing a dense, smoothed label distribution. Fault tolerance S OCKEYE saves the training state of all training components after every checkpoint, including elements like the shufﬂing data iterator and optimizer states. Training can therefore easily be continued from the last checkpoint in the case of aborted process. Mult-GPU training S OCKEYE can take advantage of multipe GPUs using MXN ET’s data parallelism mechanism. Training"
W18-1820,Q17-1007,0,0.039026,"Missing"
W18-1820,P12-1033,0,0.0764336,"Missing"
W19-6609,D16-1025,0,0.0128627,"(Popovi´c and Ney, 2011; Zeman et al., 2011), as a way to identify weaknesses of the systems and define priorities for their improvement, has received a fair amount of attention in the MT community. Although automatic error classification still cannot deal with fine-grained error taxonomies, it represents a valuable tool for fast and large scale translation error analysis. With the emergence of neural MT systems, first insights about the differences between the neural approach and the then stateof-the-art statistical phrase-based approach were obtained by using automatic error classification. Bentivogli et al. (2016) analyzed four MT systems for English into German by comparing different TER (Snover et al., 2006) scores and sub-scores, and Toral and S´anchez-Cartagena (2017) applied the WER-based approach proposed by Popovi´c and Ney (2011) for a multilingual and multifaceted evaluation of eighteen MT systems for nine translation directions including six languages from four different families. So far, automatic error classification is based on hard decisions about the error class for a given word. Addicter (Zeman et al., 2011) uses a first-order Markov model for aligning reference words with hypothesis wo"
W19-6609,2010.eamt-1.12,0,0.0582698,"Missing"
W19-6609,C18-2019,0,0.0204232,"labels to each word. This work presents first results of a new error classification method, which assigns multiple error labels to each word. We assign fractional counts for each label, which can be interpreted as a confidence for the label. Our method generates sensible multi-error suggestions, and improves the correlation between manual and automatic error distributions. 1 Introduction Translations produced by machine translation (MT) systems have been evaluated mostly in terms of overall performance scores, either by manual evaluations (ALPAC, 1966; White et al., 1994; Graham et al., 2017; Federmann, 2018) or by automatic metrics (Papineni et al., 2002; Lavie and Denkowski, 2009; Snover et al., 2006; Popovi´c, 2015; Wang et al., 2016). All these overall scores give an indication of the general performance of a given system, but they do not provide any additional information. Translation error analysis, both manual (Vilar et al., 2006; Farr´us et al., 2010; Lommel et al., c 2019 The authors. This article is licensed under a Creative Commons 4.0 licence, no derivative works, attribution, CCBY-ND. Proceedings of MT Summit XVII, volume 1 David Vilar Amazon Germany dvilar@amazon.com 2014b) as well a"
W19-6609,fishel-etal-2012-terra,1,0.870352,"Missing"
W19-6609,P02-1040,0,0.108956,"rst results of a new error classification method, which assigns multiple error labels to each word. We assign fractional counts for each label, which can be interpreted as a confidence for the label. Our method generates sensible multi-error suggestions, and improves the correlation between manual and automatic error distributions. 1 Introduction Translations produced by machine translation (MT) systems have been evaluated mostly in terms of overall performance scores, either by manual evaluations (ALPAC, 1966; White et al., 1994; Graham et al., 2017; Federmann, 2018) or by automatic metrics (Papineni et al., 2002; Lavie and Denkowski, 2009; Snover et al., 2006; Popovi´c, 2015; Wang et al., 2016). All these overall scores give an indication of the general performance of a given system, but they do not provide any additional information. Translation error analysis, both manual (Vilar et al., 2006; Farr´us et al., 2010; Lommel et al., c 2019 The authors. This article is licensed under a Creative Commons 4.0 licence, no derivative works, attribution, CCBY-ND. Proceedings of MT Summit XVII, volume 1 David Vilar Amazon Germany dvilar@amazon.com 2014b) as well as automatic (Popovi´c and Ney, 2011; Zeman et a"
W19-6609,L16-1005,1,0.882883,"Missing"
W19-6609,2011.eamt-1.36,1,0.549007,"Missing"
W19-6609,J11-4002,1,0.846879,"Missing"
W19-6609,W15-3049,1,0.893383,"Missing"
W19-6609,2006.amta-papers.25,0,0.51403,"which assigns multiple error labels to each word. We assign fractional counts for each label, which can be interpreted as a confidence for the label. Our method generates sensible multi-error suggestions, and improves the correlation between manual and automatic error distributions. 1 Introduction Translations produced by machine translation (MT) systems have been evaluated mostly in terms of overall performance scores, either by manual evaluations (ALPAC, 1966; White et al., 1994; Graham et al., 2017; Federmann, 2018) or by automatic metrics (Papineni et al., 2002; Lavie and Denkowski, 2009; Snover et al., 2006; Popovi´c, 2015; Wang et al., 2016). All these overall scores give an indication of the general performance of a given system, but they do not provide any additional information. Translation error analysis, both manual (Vilar et al., 2006; Farr´us et al., 2010; Lommel et al., c 2019 The authors. This article is licensed under a Creative Commons 4.0 licence, no derivative works, attribution, CCBY-ND. Proceedings of MT Summit XVII, volume 1 David Vilar Amazon Germany dvilar@amazon.com 2014b) as well as automatic (Popovi´c and Ney, 2011; Zeman et al., 2011), as a way to identify weaknesses of th"
W19-6609,E17-1100,0,0.0246038,"Missing"
W19-6609,vilar-etal-2006-error,1,0.779809,"Missing"
W19-6609,W16-2342,0,0.0129748,"o each word. We assign fractional counts for each label, which can be interpreted as a confidence for the label. Our method generates sensible multi-error suggestions, and improves the correlation between manual and automatic error distributions. 1 Introduction Translations produced by machine translation (MT) systems have been evaluated mostly in terms of overall performance scores, either by manual evaluations (ALPAC, 1966; White et al., 1994; Graham et al., 2017; Federmann, 2018) or by automatic metrics (Papineni et al., 2002; Lavie and Denkowski, 2009; Snover et al., 2006; Popovi´c, 2015; Wang et al., 2016). All these overall scores give an indication of the general performance of a given system, but they do not provide any additional information. Translation error analysis, both manual (Vilar et al., 2006; Farr´us et al., 2010; Lommel et al., c 2019 The authors. This article is licensed under a Creative Commons 4.0 licence, no derivative works, attribution, CCBY-ND. Proceedings of MT Summit XVII, volume 1 David Vilar Amazon Germany dvilar@amazon.com 2014b) as well as automatic (Popovi´c and Ney, 2011; Zeman et al., 2011), as a way to identify weaknesses of the systems and define priorities for"
