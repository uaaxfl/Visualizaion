2020.lrec-1.299,P18-1099,0,0.0134525,"2015), and newswire text (Mikolov et al., 2010). Personalizing language models can be seen as a domain adaptation problem where a language model that is trained on a large background corpus is adapted to text from a specific domain, which can be text from a single person. Domain adaptation has been applied for adapting one domain to another, such as adapting between newspaper sections (Jaech and Ostendorf, 2018) or YouTube video categories (Irie et al., 2018). Downstream applications that have benefited from domain adaptation include classifying the relevancy of tweets for a natural disaster (Alam et al., 2018), sentiment analysis, question classification, and topic classification (Howard and Ruder, 2018). Alam et al. (2018) adapted their model from one disaster to another with a series of neural networks and classifiers. Irie et al. (2018) associated each YouTube category with its own LSTM and connected each one using a ‘mixer’ LSTM, which generated weights for each YouTube category. Mikolov and Zweig (2012) adapted a recurrent neural network by using a vector of a distribution over topics as input to both the hidden layer and output layer. Lin et al. (2017) do not modify the neural network but all"
2020.lrec-1.299,D16-1257,0,0.0120106,"ristics as the target user — specifically age and gender — or text from randomly-selected users, for model adaptation. Our findings indicate that all three of our proposed approaches to personalization out-perform model adaptation based on demographics, and that these improvements are not the result of the language model simply having access to more data. 2. Related Work Language models are important components of systems for many downstream NLP tasks, such as machine translation (Och and Ney, 2004), speech recognition (Jelinek, 1976), handwriting recognition (Marti and Bunke, 2001), parsing (Choe and Charniak, 2016), and classification (Howard and Ruder, 2018). It is reasonable to expect that better performing language models for individual people will result in better performing systems for these downstream tasks for individual people. For example, a non-personalized language model might suggest words in speech recognition that would be correct for many people, but that have rarely been used by the user of the application. An ideal personalized language model would generate probabilities for words that the person is more likely to use, rather then what the majority of speakers would use. In this work, w"
2020.lrec-1.299,P13-2121,0,0.0157997,"roposed an evaluation metric called adjusted perplexity to address this problem. Adjusted perplexity penalizes language models when the target type is not in the vocabulary. The equation for adjusted perplexity is the same as perplexity, with the exception that the probability of UNK is recalculated using Equation 2 below: p(UNK) = p(UNK) |UNK-TYPES| (2) with UNK-TYPES being the set of types that are converted to UNK in the test data. Language Models We explore two different types of language models in our experiments. The first model is an n-gram language model that uses Kneser-Ney smooting (Heafield et al., 2013) with n = 3. Preliminary experiments showed that n = 3 yielded the lowest adjusted perplexity on TUNE-TEST. The second model is a neural language model that uses an LSTM. We performed preliminary experiments by testing on TUNE-TEST to tune our models. We performed a grid search over the following parameters with their associated values: number of hidden units (128, 256, 512, 1024, 2048); number of layers (1, 2); size of embeddings (64, 128, 256, 512, 1024); number of training epochs (1, 2, 3); and batch size (1, 2, 5, 15, 30, 45). We found that the best performing neural language models that w"
2020.lrec-1.299,P18-1031,0,0.0724581,"and gender — or text from randomly-selected users, for model adaptation. Our findings indicate that all three of our proposed approaches to personalization out-perform model adaptation based on demographics, and that these improvements are not the result of the language model simply having access to more data. 2. Related Work Language models are important components of systems for many downstream NLP tasks, such as machine translation (Och and Ney, 2004), speech recognition (Jelinek, 1976), handwriting recognition (Marti and Bunke, 2001), parsing (Choe and Charniak, 2016), and classification (Howard and Ruder, 2018). It is reasonable to expect that better performing language models for individual people will result in better performing systems for these downstream tasks for individual people. For example, a non-personalized language model might suggest words in speech recognition that would be correct for many people, but that have rarely been used by the user of the application. An ideal personalized language model would generate probabilities for words that the person is more likely to use, rather then what the majority of speakers would use. In this work, we use both n-gram language models and neural"
2020.lrec-1.299,P18-2111,0,0.0990161,"adaptation based on text from other users of the same demographic. Language modeling has been applied to a variety of domains including text from Wikipedia (Merity et al., 2016), children’s books (Hill et al., 2015), and newswire text (Mikolov et al., 2010). Personalizing language models can be seen as a domain adaptation problem where a language model that is trained on a large background corpus is adapted to text from a specific domain, which can be text from a single person. Domain adaptation has been applied for adapting one domain to another, such as adapting between newspaper sections (Jaech and Ostendorf, 2018) or YouTube video categories (Irie et al., 2018). Downstream applications that have benefited from domain adaptation include classifying the relevancy of tweets for a natural disaster (Alam et al., 2018), sentiment analysis, question classification, and topic classification (Howard and Ruder, 2018). Alam et al. (2018) adapted their model from one disaster to another with a series of neural networks and classifiers. Irie et al. (2018) associated each YouTube category with its own LSTM and connected each one using a ‘mixer’ LSTM, which generated weights for each YouTube category. Mikolov and Zwe"
2020.lrec-1.299,P18-1027,0,0.0175344,"with ngram background for comparison, and we see that it outperforms neural background by itself. Even though ngram background, n-gram user, and neural user all perform poorly by themselves, they are all able to improve the performance of neural background when interpolated with it.1 Neural background primed user achieves a similar adjusted perplexity, regardless of the amount of text from the user. This might be due to how many steps in the past that the state can remember, i.e., the state may largely be unaffected by the more distant tokens used for priming. This phenomenon was explored by Khandelwal et al. (2018). In Figure 2, we show the adjusted perplexity for models that involve the n-gram background model. We see that n-gram background interpolated with either of the useronly models (n-gram user and neural user) outperforms n-gram background by itself when at least 1000 tokens are available. n-gram background interpolated with ngram user outperforms n-gram background for all amounts of text from the user, and outperforms n-gram background interpolated with neural user when less than 100k tokens from the user are used. That neural background interpolated with neural user performs better with larger"
2020.lrec-1.299,D17-1119,0,0.372232,"language models, that is, building language models that are tailored to the writing style of an individual. Language modelling requires a large amount of text to sufficiently train a language model. However, a large amount of text from any one user — for example social media users such as bloggers — is not necessarily available for training a user-specific language model. To overcome this issue, some techniques treat text from other users that share demographic characteristics, such as age or gender, as training data. Although this has been shown to improve the performance of language models (Lynn et al., 2017), this demographic information is typically obtained via document metadata, which is not available for all document types, or for all users. This suggests a need to personalize language models while only relying on a small amount of text from the user. We propose and evaluate three different personalization techniques, while limiting the models’ access to different amounts of text from the user. These techniques are applied to a language model that was trained on a large background corpus of in-domain text (specifically blog posts). The three techniques include continuing to train the backgrou"
2020.lrec-1.299,P14-5010,0,0.00774736,"Missing"
2020.lrec-1.299,J04-4002,0,0.0356153,"odels against models that use the same techniques, but that use text from users with the same demographic characteristics as the target user — specifically age and gender — or text from randomly-selected users, for model adaptation. Our findings indicate that all three of our proposed approaches to personalization out-perform model adaptation based on demographics, and that these improvements are not the result of the language model simply having access to more data. 2. Related Work Language models are important components of systems for many downstream NLP tasks, such as machine translation (Och and Ney, 2004), speech recognition (Jelinek, 1976), handwriting recognition (Marti and Bunke, 2001), parsing (Choe and Charniak, 2016), and classification (Howard and Ruder, 2018). It is reasonable to expect that better performing language models for individual people will result in better performing systems for these downstream tasks for individual people. For example, a non-personalized language model might suggest words in speech recognition that would be correct for many people, but that have rarely been used by the user of the application. An ideal personalized language model would generate probabiliti"
2020.lrec-1.330,W13-3520,0,0.0379595,"w frequency words, and a greater number of languages. 3. Corpora In this paper, we consider the same languages that Adams et al. (2017) used in their experiments on applying cross-lingual word embeddings for low-resource language modelling; specifically, we consider English, and the following languages which have varying degrees of similarity to English, and vary with respect to morphological complexity: Finnish, German, Japanese, Russian, and Spanish. The The corpus for each language is a Wikipedia dump from 20 September 2018, except for Japanese, where we use a pre-processed Wikipedia dump (Al-Rfou et al., 2013). For English, the raw dump is preprocessed using wikifi (Bojanowski et al., 2017), and for the other non-Japanese languages we use WP2TXT.1 Details of the corpora for each language are provided in Table 1, in the “Full corpus” columns. In preliminary experiments we observed that for the full Wikipedia corpora, relatively few words in the evaluation dataset (discussed in Section 4.) were OOVs, yet OOVs are required for our experimental setup. Therefore, following Adams et al. (2017) we carried out experiments in which we learned cross-lingual embeddings, but downsized the size of the corpora."
2020.lrec-1.330,P17-1042,0,0.315967,"owing optimization problem, we get a transformation matrix, W: min ||AW − B||F W (1) where F is the Frobenius norm. The transformation matrix maps the vectors of language A to the vector space of language B. Moreover, most methods which employ a dictionary as their cross-lingual training signal need only a relatively small number of training seeds to find the mapping between two languages (Mikolov et al., 2013; Hauer et al., 2017; Duong et al., 2016), so the bilingual dictionary need not be very large. Xing et al. (2015) show that enforcing an orthogonality constraint on W gives improvements. Artetxe et al. (2017) propose a method that can work with a small seed lexicon, as low as 25 pairs. They solve the same optimization problem as Mikolov et al. (2013), and in a process of selflearning and in several rounds of bootstrapping add more translation pairs to the bilingual dictionary. In another work, Artetxe et al. (2018) propose a fully unsupervised method that can learn the mapping between the vector spaces of two languages iteratively without the need for a bilingual signal. They employ the same self-learning method as Artetxe et al. (2017), however, they introduce a fully unsupervised initialization"
2020.lrec-1.330,P18-1073,0,0.0888773,"tively small number of training seeds to find the mapping between two languages (Mikolov et al., 2013; Hauer et al., 2017; Duong et al., 2016), so the bilingual dictionary need not be very large. Xing et al. (2015) show that enforcing an orthogonality constraint on W gives improvements. Artetxe et al. (2017) propose a method that can work with a small seed lexicon, as low as 25 pairs. They solve the same optimization problem as Mikolov et al. (2013), and in a process of selflearning and in several rounds of bootstrapping add more translation pairs to the bilingual dictionary. In another work, Artetxe et al. (2018) propose a fully unsupervised method that can learn the mapping between the vector spaces of two languages iteratively without the need for a bilingual signal. They employ the same self-learning method as Artetxe et al. (2017), however, they introduce a fully unsupervised initialization method that exploits the similarity distributions of words in the two languages to find a set of word pairs to start the learning phase. Braune et al. (2018) consider an English–German lexicon induction task focused on low frequency, in2712 vocabulary words. They show that sub-word embeddings and orthographic s"
2020.lrec-1.330,C10-3010,0,0.409916,"hese corpora are also shown in Table 1, in the “Sample” columns. In the bilingual lexicon induction experiments in this paper we attempt to find an in-vocabulary target language translation for an OOV source language word. We therefore always use the full corpus for the target language — so that translations of many source language words will be invocabulary in the target language — and a sample for the source language — so that a substantial number of goldstandard translations will be OOV in the source language, and to simulate a lower-resource source language. 4. Evaluation Datasets Panlex (Baldwin et al., 2010) is a freely-available translation resource, built by combining many translation dictionaries, that covers thousands of languages and includes over 1B translations. We use Panlex to build goldstandard evaluation data. In our experiments we only consider language pairs where English is the source or target language, and the other language is one of the five other languages (i.e., one of Finnish, German, Japanese, Russian, or Spanish). We begin by extracting all single-word translations from Panlex for these language pairs. For each language pair, we then create 1 https://github.com/yohasebe/wp2"
2020.lrec-1.330,Q17-1010,0,0.704496,"ce languages for tasks such as language modelling (Adams et al., 2017), part-of-speech tagging (Fang and Cohn, 2017), and dependency parsing (Duong et al., 2015). In the case of out-of-vocabulary (OOV) words, however, no information is available. This could be particularly problematic for low-resource languages, where the number of words that embeddings are learned for could be relatively low due to the relatively small amount of training data available, and for morphologically rich languages, where many wordforms would not be observed while learning the embeddings. Sub-word level embeddings (Bojanowski et al., 2017, e.g.) — i.e., embeddings for units smaller than words, such as character sequences — have been proposed to address this limitation concerning OOV words for monolingual embedding models, but little prior work — with the notable exception of (Braune et al., 2018) — has considered sub-word embeddings in cross-lingual models. In this paper we evaluate whether sub-word embeddings can be leveraged in cross-lingual models. Specifically, we consider a novel bilingual lexicon induction task in which an in-vocabulary target language translation is found for an OOV source language word, where the repre"
2020.lrec-1.330,N18-2030,0,0.324979,"cularly problematic for low-resource languages, where the number of words that embeddings are learned for could be relatively low due to the relatively small amount of training data available, and for morphologically rich languages, where many wordforms would not be observed while learning the embeddings. Sub-word level embeddings (Bojanowski et al., 2017, e.g.) — i.e., embeddings for units smaller than words, such as character sequences — have been proposed to address this limitation concerning OOV words for monolingual embedding models, but little prior work — with the notable exception of (Braune et al., 2018) — has considered sub-word embeddings in cross-lingual models. In this paper we evaluate whether sub-word embeddings can be leveraged in cross-lingual models. Specifically, we consider a novel bilingual lexicon induction task in which an in-vocabulary target language translation is found for an OOV source language word, where the representation of the source language word is constructed from subword embeddings. Our findings indicate that sub-word embeddings do carry information that can be leveraged in cross-lingual models. 2. Related Work A variety of approaches have been proposed to find cro"
2020.lrec-1.330,K15-1012,1,0.83796,"phologically-rich language. Keywords: Cross-lingual Word Embeddings, Low-resource Languages, Morphologically-rich Languages 1. Introduction Cross-lingual word embeddings provide a shared space for embeddings in two languages, enabling knowledge to be transferred between them. Cross-lingual word embeddings can be used for tasks such as bilingual lexicon induction, and can be leveraged to improve systems for natural language processing (NLP) for low-resource languages for tasks such as language modelling (Adams et al., 2017), part-of-speech tagging (Fang and Cohn, 2017), and dependency parsing (Duong et al., 2015). In the case of out-of-vocabulary (OOV) words, however, no information is available. This could be particularly problematic for low-resource languages, where the number of words that embeddings are learned for could be relatively low due to the relatively small amount of training data available, and for morphologically rich languages, where many wordforms would not be observed while learning the embeddings. Sub-word level embeddings (Bojanowski et al., 2017, e.g.) — i.e., embeddings for units smaller than words, such as character sequences — have been proposed to address this limitation conce"
2020.lrec-1.330,D16-1136,0,0.0647275,"there is a linear relationship between the vector spaces of two languages. If we consider the first language as A, and the second language as B, by solving the following optimization problem, we get a transformation matrix, W: min ||AW − B||F W (1) where F is the Frobenius norm. The transformation matrix maps the vectors of language A to the vector space of language B. Moreover, most methods which employ a dictionary as their cross-lingual training signal need only a relatively small number of training seeds to find the mapping between two languages (Mikolov et al., 2013; Hauer et al., 2017; Duong et al., 2016), so the bilingual dictionary need not be very large. Xing et al. (2015) show that enforcing an orthogonality constraint on W gives improvements. Artetxe et al. (2017) propose a method that can work with a small seed lexicon, as low as 25 pairs. They solve the same optimization problem as Mikolov et al. (2013), and in a process of selflearning and in several rounds of bootstrapping add more translation pairs to the bilingual dictionary. In another work, Artetxe et al. (2018) propose a fully unsupervised method that can learn the mapping between the vector spaces of two languages iteratively wi"
2020.lrec-1.330,P17-2093,0,0.14596,"luding in the case of a truly low-resource morphologically-rich language. Keywords: Cross-lingual Word Embeddings, Low-resource Languages, Morphologically-rich Languages 1. Introduction Cross-lingual word embeddings provide a shared space for embeddings in two languages, enabling knowledge to be transferred between them. Cross-lingual word embeddings can be used for tasks such as bilingual lexicon induction, and can be leveraged to improve systems for natural language processing (NLP) for low-resource languages for tasks such as language modelling (Adams et al., 2017), part-of-speech tagging (Fang and Cohn, 2017), and dependency parsing (Duong et al., 2015). In the case of out-of-vocabulary (OOV) words, however, no information is available. This could be particularly problematic for low-resource languages, where the number of words that embeddings are learned for could be relatively low due to the relatively small amount of training data available, and for morphologically rich languages, where many wordforms would not be observed while learning the embeddings. Sub-word level embeddings (Bojanowski et al., 2017, e.g.) — i.e., embeddings for units smaller than words, such as character sequences — have b"
2020.lrec-1.330,E17-2098,0,0.0282069,"al. (2013) show that there is a linear relationship between the vector spaces of two languages. If we consider the first language as A, and the second language as B, by solving the following optimization problem, we get a transformation matrix, W: min ||AW − B||F W (1) where F is the Frobenius norm. The transformation matrix maps the vectors of language A to the vector space of language B. Moreover, most methods which employ a dictionary as their cross-lingual training signal need only a relatively small number of training seeds to find the mapping between two languages (Mikolov et al., 2013; Hauer et al., 2017; Duong et al., 2016), so the bilingual dictionary need not be very large. Xing et al. (2015) show that enforcing an orthogonality constraint on W gives improvements. Artetxe et al. (2017) propose a method that can work with a small seed lexicon, as low as 25 pairs. They solve the same optimization problem as Mikolov et al. (2013), and in a process of selflearning and in several rounds of bootstrapping add more translation pairs to the bilingual dictionary. In another work, Artetxe et al. (2018) propose a fully unsupervised method that can learn the mapping between the vector spaces of two lan"
2020.lrec-1.330,Q19-1007,0,0.0177302,"only OOVs). In subsection 5.3. we discuss incorporating information from edit distance, along with information from crosslingual word embeddings, to find the best translation for OOVs. The last subsection presents results for bilingual lexicon induction for OOVs in a truly low-resource source language, specifically Cherokee. 5.1. OOV Bilingual Lexicon Induction In the case of the supervised method, given source and target language embeddings, we require a set of translations to learn the transformation matrix W in Equation 1. Following previous work (Conneau et al., 2018; Joulin et al., 2018; Jawanpuria et al., 2019, e.g.), we use the training pairs provided by Conneau et al. (2018).6 For training the semi-supervised method, we take a random sample of 25 pairs from these training pairs. Given a goldstandard evaluation pair, we construct a representation for its (OOV) source language word by averaging its sub-word embeddings using fastText or BPE. We then transform this representation using W , and rank the target language words by the cosine similarity of their embeddings with this transformed representation of the source word. We report accuracy@N — for N = 1, 5, and 10 — where the system is scored as c"
2020.lrec-1.330,D18-1330,0,0.0228199,"Missing"
2020.lrec-1.330,P13-1109,0,0.034375,"Missing"
2020.lrec-1.330,P16-1162,0,0.0129509,"s by using sub-word information are considered. For the first approach, we use fastText (Bojanowski et al., 2017) to create an embedding matrix for each corpus. We use the default fastText parameters, except for the number of dimensions for the embeddings, which is set to 300. For the second approach, we use the method introduced by Zhu et al. (2019), which provides a framework to investigate two components of forming sub-word informed word representations — segmentation of words into their sub-words, and the effect of different sub-word composition functions. We use byte pair encoding (BPE) (Sennrich et al., 2016) as the method which provides subword information. To train word embeddings using the Zhu et al. (2019) framework, we use the default settings, which use addition as the composition function — similar to fastText — and do not include an embedding for the whole word itself in the composition — in contrast to fastText which does include a representation for the whole word along with representations for its sub-words. We refer to this approach — which is based on Zhu et al. (2019) and incorporates BPE — as BPE. Results are presented in the following subsections. In subsection 5.1., the results fo"
2020.lrec-1.330,N15-1104,0,0.0670032,"Missing"
2020.lrec-1.330,N19-1097,0,0.0468554,"Missing"
2020.lrec-1.330,D13-1141,0,0.0265671,"Missing"
2020.lrec-1.333,E17-1088,0,0.135604,"e then 1 https://www12.statcan.gc.ca/ census-recensement/2011/as-sa/98-314-x/ 98-314-x2011003_3-eng.cfm consider the use of pre-trained word embeddings to initialize the input layer of the RNN language models. We find that, even when trained on a small amount of data, fastText embeddings (Bojanowski et al., 2017) — which incorporate sub-word knowledge and are able to form representations for OOVs — give a substantial improvement. Cross-lingual word embeddings (CLWEs) are methods to create word embeddings for multiple languages in the same vector space (Duong et al., 2016; Ruder et al., 2019). Adams et al. (2017) showed promising results incorporating cross-lingual embeddings into language models for some simulated low-resource languages. We further consider the use of CLWE methods to initialize our models, including the method used by Adams et al. (2017). We find that language models that incorporate cross-lingual embeddings do not perform better than models initialized with fastText embeddings. The experiments described so far use a variation of perplexity for evaluation. We then consider an evaluation that measures the potential savings in terms of keystrokes when typing that is motivated by word s"
2020.lrec-1.333,Q17-1010,0,0.320822,"ich morphology, we expect many out-of-vocabulary (OOV) words, and therefore it is important that a Mi’kmaq language model be able to handle OOVs. In this paper we consider n-gram and RNN language models for Mi’kmaq. We tune these models over a range of parameters in an effort to establish a strong baseline. We then 1 https://www12.statcan.gc.ca/ census-recensement/2011/as-sa/98-314-x/ 98-314-x2011003_3-eng.cfm consider the use of pre-trained word embeddings to initialize the input layer of the RNN language models. We find that, even when trained on a small amount of data, fastText embeddings (Bojanowski et al., 2017) — which incorporate sub-word knowledge and are able to form representations for OOVs — give a substantial improvement. Cross-lingual word embeddings (CLWEs) are methods to create word embeddings for multiple languages in the same vector space (Duong et al., 2016; Ruder et al., 2019). Adams et al. (2017) showed promising results incorporating cross-lingual embeddings into language models for some simulated low-resource languages. We further consider the use of CLWE methods to initialize our models, including the method used by Adams et al. (2017). We find that language models that incorporate"
2020.lrec-1.333,D14-1179,0,0.0097506,"Missing"
2020.lrec-1.333,P15-2139,1,0.763572,"s calculated as follows: ALT P = LT P − s × log(r) −1 AP P = (2ALT P ) N (3) (4) where s is the number of UNK occurrences and r is the number of UNK types (Ueberla, 1994). 2.2. Low Resource Languages Language models for English and other high-resource languages are often trained on billions of tokens (Chelba et al., 2014; Jozefowicz et al., 2016); however, many languages have only much smaller corpora available, which makes language models harder to train. Transferring information from a high-resource language to a low-resource language is a technique that has proven useful in some NLP tasks (Duong et al., 2015; Adams et al., 2017). One of these techniques is the use of cross-lingual word embeddings (Ruder et al., 2019), which aims to build word embeddings for multiple languages in a common vector space. One method for building these embeddings is to use monolingual corpora from two languages and a bilingual lexicon (Duong et al., 2016). The approach suggested by (Duong et al., 2016) is an extension of the CBOW algorithm (Mikolov et al., 2013b), in which the target word in a context is replaced with its translation. A bilingual lexicon is used to do the translation. An approach based on the expectat"
2020.lrec-1.333,D16-1136,0,0.190566,"effort to establish a strong baseline. We then 1 https://www12.statcan.gc.ca/ census-recensement/2011/as-sa/98-314-x/ 98-314-x2011003_3-eng.cfm consider the use of pre-trained word embeddings to initialize the input layer of the RNN language models. We find that, even when trained on a small amount of data, fastText embeddings (Bojanowski et al., 2017) — which incorporate sub-word knowledge and are able to form representations for OOVs — give a substantial improvement. Cross-lingual word embeddings (CLWEs) are methods to create word embeddings for multiple languages in the same vector space (Duong et al., 2016; Ruder et al., 2019). Adams et al. (2017) showed promising results incorporating cross-lingual embeddings into language models for some simulated low-resource languages. We further consider the use of CLWE methods to initialize our models, including the method used by Adams et al. (2017). We find that language models that incorporate cross-lingual embeddings do not perform better than models initialized with fastText embeddings. The experiments described so far use a variation of perplexity for evaluation. We then consider an evaluation that measures the potential savings in terms of keystrok"
2020.lrec-1.333,P13-2121,0,0.10723,"ssues related to the domain of their corpus (transcribed spoken narratives) and how the tones are structured in Yongning Na that could have contributed to their findings. Maheshwari et al. (2018) carried out the first work in NLP focused specifically on Mi’kmaq. They constructed a Mi’kmaq web corpus of roughly 76k tokens. They then performed Mi’kmaq language modelling experiments in which 2737 Corpus Training Dev Test Total Sentences 5080 633 633 6346 Tokens 60k 7.6k 8.1k 76k Table 1: The number of sentences, tokens, and types in the Mi’kmaq corpora. they considered n-gram models using KenLM (Heafield et al., 2013), a character-level RNN, and a word-level RNN that uses a CNN to incorporate character-level information (Kim et al., 2016). They found that the KenLM model performed the best out of the three. In this work we focus on Mi’kmaq language modelling, and further examine language models that incorporate sub-word information, and in addition consider language models that incorporate CLWEs. 3. Resources In this section we describe the corpora and bilingual lexicon used in our experiments. 3.1. Bilingual Lexicon PanLex (Kamholz et al., 2014) was used by both Duong et al. (2016) and Adams et al. (2017)"
2020.lrec-1.333,kamholz-etal-2014-panlex,0,0.0132522,"Mi’kmaq corpora. they considered n-gram models using KenLM (Heafield et al., 2013), a character-level RNN, and a word-level RNN that uses a CNN to incorporate character-level information (Kim et al., 2016). They found that the KenLM model performed the best out of the three. In this work we focus on Mi’kmaq language modelling, and further examine language models that incorporate sub-word information, and in addition consider language models that incorporate CLWEs. 3. Resources In this section we describe the corpora and bilingual lexicon used in our experiments. 3.1. Bilingual Lexicon PanLex (Kamholz et al., 2014) was used by both Duong et al. (2016) and Adams et al. (2017) as the source of bilingual lexicons, so we used the same resource. PanLex is built by combining many translation resources, and includes entries for thousands of languages, including Mi’kmaq. Table 2 shows the number of single word translations from a source language into Mi’kmaq, for the top-5 languages with the most translations into Mi’kmaq in Panlex. We observe that English has many more entries than other languages, so we only performed experiments with English as the source language. Adams et al. (2017) showed that small lexic"
2020.lrec-1.333,P18-1007,0,0.0130682,"g the usefulness of a language model in a word suggestion task, KSR also avoids the challenges of using perplexity to compare word-level language models with those based on open-vocabulary segmentations of the input, such as those produced by SentencePiece.14 7.3. Results In these experiments we use the same training, dev, and test data as in previous experiments. We train SentencePiece models on the training data. We tuned parameter settings for SentencePiece through preliminary experiments on dev data. We considered bytepair-encoding (BPE) (Sennrich et al., 2016) and unigram language model (Kudo, 2018) for segmentation. We further explored vocabulary sizes of 1k, 2k, 4k, and 8k. We found BPE segmentation with a vocabulary size of 2k to perform best, and report findings for these settings. Results are shown in Table 7. Considering first the previous models using word-level tokenization, we see that, for all numbers of suggestions, on both the dev and test data, the model using skip-gram embeddings outperforms that using initialization via the normal distribution, with the exception of n = 1 on the dev data. These findings are, overall, consistent with those of the previous evaluations using"
2020.lrec-1.333,L18-1653,1,0.887669,"word modelling is important for Mi’kmaq language modelling. Keywords: Indigenous languages, language modelling, word embeddings 1. Introduction Mi’kmaq is an Indigenous language spoken primarily in Eastern Canada (Johnson, 1996). It is polysynthetic and verb-oriented, and in the Eastern Algonquian language family. Mi’kmaq has roughly 8,000 speakers in Canada,1 and is a low-resource language. There are Mi’kmaq dictionaries (Rand, 1888; DeBlois, 1996) and translated texts (DeBlois, 1990), but no large corpora. There has been very little prior computational work on Mi’kmaq, with the exception of Maheshwari et al. (2018), who built a web corpus of Mi’kmaq and carried out preliminary language modelling experiments using this corpus. Language models are a crucial component for many language technology systems including spelling correction and word suggestion on smartphone soft keyboards. Mi’kmaq language modelling is, however, particularly challenging due to its rich morphology and the relatively small amount of data available. Mi’kmaq is polysynthetic, so each word is composed of many morphemes (Johnson, 1996). Rand (1888, p. iv) explains that a single Mi’kmaq word can essentially describe a whole English sent"
2020.lrec-1.333,E17-2025,0,0.0204306,"iminary experiments on the dev data. 4.2. RNNs We considered two RNN model architectures: LSTM networks (Hochreiter and Schmidhuber, 1997) and GRU networks (Cho et al., 2014). We used PyTorch (Paszke et al., 2017), and its word-level RNN language model example as a base for implementing these models.4 We tuned hyperparameters for these models including the number of layers, the amount of dropout, and the embedding size. We set the size of the hidden layer(s) to be the same size as the embeddings. We further considered the use of weight tying for the input and output layers (Inan et al., 2017; Press and Wolf, 2017), which has been shown to make language models much easier to learn. 4 3 Number of Entries 4303 449 410 403 377 Table 2: The number of single word translations from a source language into Mi’kmaq, for the top-5 languages with the most translations into Mi’kmaq in Panlex. Corpora We used the Mi’kmaq corpus built by Maheshwari et al. (2018). The corpus was already tokenized, and the only additional cleaning steps taken were normalizing various quote characters to ASCII single or double quotes, and swapping long hyphen characters with ASCII dashes. We randomly split the corpus into a training set"
2020.lrec-1.333,P16-1162,0,0.0117999,"ysprediction . In addition to potentially indicating the usefulness of a language model in a word suggestion task, KSR also avoids the challenges of using perplexity to compare word-level language models with those based on open-vocabulary segmentations of the input, such as those produced by SentencePiece.14 7.3. Results In these experiments we use the same training, dev, and test data as in previous experiments. We train SentencePiece models on the training data. We tuned parameter settings for SentencePiece through preliminary experiments on dev data. We considered bytepair-encoding (BPE) (Sennrich et al., 2016) and unigram language model (Kudo, 2018) for segmentation. We further explored vocabulary sizes of 1k, 2k, 4k, and 8k. We found BPE segmentation with a vocabulary size of 2k to perform best, and report findings for these settings. Results are shown in Table 7. Considering first the previous models using word-level tokenization, we see that, for all numbers of suggestions, on both the dev and test data, the model using skip-gram embeddings outperforms that using initialization via the normal distribution, with the exception of n = 1 on the dev data. These findings are, overall, consistent with"
2020.lrec-1.333,E14-2006,0,0.0380303,"Missing"
2020.lrec-1.333,P08-2066,0,0.233475,"the fastText embeddings, we consider training fastText using both word-level tokenization (i.e., the same as for previous fastText models), and using tokenization based on SentencePiece. In both cases we use the same fastText settings as in our previous experiments. 7.2. Evaluation Smartphone soft keyboards often provide suggestions for the next word or word being typed, that the user can select instead of typing the word in its entirety. In these experiments we evaluate language models using a measure motivated by this scenario, referred to as keystroke saving rate (KSR), defined as follows (Trnka and McCoy, 2008): KSR = keysnormal − keysprediction keysnormal × 100 (5) where keysnormal is the number of keystrokes required to type the text without the use of any word suggestions, i.e., it is the number of characters in the text, and keysprediction is the number of keystrokes required to type the text assuming that the user always selects a word suggestion if the correct one is available among the n suggestions provided, and that making this selection has a cost of one keystroke. The word suggestions are determined by a language model. In particular, they are the top-n words with highest probability that"
2020.starsem-1.5,W13-3520,0,0.155443,"Missing"
2020.starsem-1.5,P17-1042,0,0.552848,"tly learns representations for 93 languages. Aside from requiring a parallel corpus, it is also computationally expensive to train. Mikolov et al. (2013b) argue that the geometric arrangement of word embeddings in two different languages is the same. They therefore propose a method to learn a linear transformation to align the vector spaces of two languages by using a seed lexicon of known translation pairs. Xing et al. (2015) show that normalizing all word vectors to be unit length, and applying an orthogonality constraint on the transformation matrix, improves the approach of Mikolov et al. Artetxe et al. (2017) introduce an alignment-based method which relaxes the requirement of having a bilingual seed lexicon. Their approach begins with a very small seed lexicon — as few as 25 pairs — and in a process of self-learning and through several rounds of bootstrapping, increases the size of the bilingual dictionary. Artetxe et al. (2018b) further relax the need for a bilingual In this paper, we propose a model that can learn cross-lingual word embeddings from a modest amount of monolingual data and a bilingual dictionary. We rely on bilingual dictionaries because they are relatively-widely available. For"
2020.starsem-1.5,P18-1073,0,0.111741,"e vector spaces of two languages by using a seed lexicon of known translation pairs. Xing et al. (2015) show that normalizing all word vectors to be unit length, and applying an orthogonality constraint on the transformation matrix, improves the approach of Mikolov et al. Artetxe et al. (2017) introduce an alignment-based method which relaxes the requirement of having a bilingual seed lexicon. Their approach begins with a very small seed lexicon — as few as 25 pairs — and in a process of self-learning and through several rounds of bootstrapping, increases the size of the bilingual dictionary. Artetxe et al. (2018b) further relax the need for a bilingual In this paper, we propose a model that can learn cross-lingual word embeddings from a modest amount of monolingual data and a bilingual dictionary. We rely on bilingual dictionaries because they are relatively-widely available. For example, Panlex (Baldwin et al., 2010) is a translation resource that combines many bilingual dictionaries and provides translations for 5700 languages. Our proposed model is an extension of the method proposed by Duong et al. (2016). In their work, they only considered word embeddings, and so their method is unable to form"
2020.starsem-1.5,Q19-1038,0,0.314262,"ddings are used in a monolingual setting. Our results on these tasks demonstrate that incorporating sub-word information leads to improvements for both cross-lingual and monolingual representations. For extrinsic evaluation, to show the impact of having sub-word knowledge in a down-stream NLP task, we consider cross-lingual document classification. Again our results indicate that incorporating sub-word information leads to improvements, and furthermore we find our proposed model to perform on par with, or better than, strong benchmark approaches. Recently, with advances in language modelling (Artetxe and Schwenk, 2019) and contextualized language models (Devlin et al., 2019; Conneau and Lample, 2019), transfer learning has become feasible between languages by using a byte pair encoding (BPE, Sennrich et al., 2016) shared vocabulary, and fine-tuning the models for specific tasks (Wu and Dredze, 2019). Nevertheless, these models require a substantial amount of training data (Conneau and Lample, 2019), and in some cases parallel corpora (Artetxe and Schwenk, 2019; Conneau and Lample, 2019), and are very computationally expensive to train. There is therefore a need for methods that can be trained from a morelim"
2020.starsem-1.5,C10-3010,0,0.339511,"gnment-based method which relaxes the requirement of having a bilingual seed lexicon. Their approach begins with a very small seed lexicon — as few as 25 pairs — and in a process of self-learning and through several rounds of bootstrapping, increases the size of the bilingual dictionary. Artetxe et al. (2018b) further relax the need for a bilingual In this paper, we propose a model that can learn cross-lingual word embeddings from a modest amount of monolingual data and a bilingual dictionary. We rely on bilingual dictionaries because they are relatively-widely available. For example, Panlex (Baldwin et al., 2010) is a translation resource that combines many bilingual dictionaries and provides translations for 5700 languages. Our proposed model is an extension of the method proposed by Duong et al. (2016). In their work, they only considered word embeddings, and so their method is unable to form representations for OOVs, and therefore is not expected to perform well for low-resource or morphologically-rich target languages. We extend the method of Duong et al. (2016) by incorporating sub-word information in the process of training cross-lingual word embeddings. In this way, we form a shared embedding s"
2020.starsem-1.5,Q17-1010,0,0.769833,"mmons Attribution 4.0 International License. License details: http: //creativecommons.org/licenses/by/4.0/. 39 Proceedings of the Ninth Joint Conference on Lexical and Computational Semantics (*SEM), pages 39–49 Barcelona, Spain (Online), December 12–13, 2020 the number of OOVs is relatively high, such as for low-resource and morphologically-rich languages, this could lead to particularly poor performance in down-stream tasks. This problem has been addressed in monolingual settings by learning embeddings for sub-word units, and then composing representations for OOVs based on their sub-words (Bojanowski et al., 2017; Zhu et al., 2019). con induction (BLI), and (2) a monolingual word similarity task to show the effectiveness of our proposed approach when the embeddings are used in a monolingual setting. Our results on these tasks demonstrate that incorporating sub-word information leads to improvements for both cross-lingual and monolingual representations. For extrinsic evaluation, to show the impact of having sub-word knowledge in a down-stream NLP task, we consider cross-lingual document classification. Again our results indicate that incorporating sub-word information leads to improvements, and furthe"
2020.starsem-1.5,D18-1366,0,0.280206,"aches to learning cross-lingual embeddings have been trained on concatenated monolingual corpora. Multilingual BERT (mBERT) is a BERT model (Devlin et al., 2019) trained on concatenated Wikipedia corpora for 105 languages. Wu and Dredze (2019) show that since mBERT uses a shared vocabulary for all languages, it can represent embeddings for all languages in a shared space, rather than representing each language in a separate space. This model is therefore able to learn deep contextualized cross-lingual word embeddings without any cross-lingual signal, but is computationally expensive to train. Chaudhary et al. (2018) present a method that uses sub-word information, such as lemmas, morpheme tags, and phoneme n-grams, to transfer knowledge from richresource languages to low-resource ones. They train skip-gram on concatenated monolingual corpora of two related languages and learn representations in a shared space by relying on similar subwords to map related words close to each other in the shared space. They also consider an approach which first trains a model on the rich-resource language and then uses the trained sub-word embeddings to initialize the model for the low-resource language. All of these mappi"
2020.starsem-1.5,C12-1089,0,0.249183,"ample, 2019), and in some cases parallel corpora (Artetxe and Schwenk, 2019; Conneau and Lample, 2019), and are very computationally expensive to train. There is therefore a need for methods that can be trained from a morelimited amount of data and require less computational resources for training, but that nevertheless show comparable performance. 2 Related Work A variety of methods have been proposed for learning cross-lingual word embeddings. These methods vary with respect to the level of supervision, and the cross-lingual signals used, such as parallel corpora and bilingual dictionaries. Klementiev et al. (2012) propose a method to learn cross-lingual representations by training a language model on the source and target language and optimizing their objective function jointly. This method, however, requires a parallel corpus, which is not available for many languages, especially low-resource ones. More recently, Artetxe and Schwenk (2019) propose a bi-directional LSTM language model that is trained on a very large parallel corpus, containing 223 million parallel sentences, and jointly learns representations for 93 languages. Aside from requiring a parallel corpus, it is also computationally expensive"
2020.starsem-1.5,N19-1423,0,0.420812,"tasks demonstrate that incorporating sub-word information leads to improvements for both cross-lingual and monolingual representations. For extrinsic evaluation, to show the impact of having sub-word knowledge in a down-stream NLP task, we consider cross-lingual document classification. Again our results indicate that incorporating sub-word information leads to improvements, and furthermore we find our proposed model to perform on par with, or better than, strong benchmark approaches. Recently, with advances in language modelling (Artetxe and Schwenk, 2019) and contextualized language models (Devlin et al., 2019; Conneau and Lample, 2019), transfer learning has become feasible between languages by using a byte pair encoding (BPE, Sennrich et al., 2016) shared vocabulary, and fine-tuning the models for specific tasks (Wu and Dredze, 2019). Nevertheless, these models require a substantial amount of training data (Conneau and Lample, 2019), and in some cases parallel corpora (Artetxe and Schwenk, 2019; Conneau and Lample, 2019), and are very computationally expensive to train. There is therefore a need for methods that can be trained from a morelimited amount of data and require less computational resou"
2020.starsem-1.5,T75-2034,0,0.617748,"Missing"
2020.starsem-1.5,D16-1136,0,0.094044,"nd through several rounds of bootstrapping, increases the size of the bilingual dictionary. Artetxe et al. (2018b) further relax the need for a bilingual In this paper, we propose a model that can learn cross-lingual word embeddings from a modest amount of monolingual data and a bilingual dictionary. We rely on bilingual dictionaries because they are relatively-widely available. For example, Panlex (Baldwin et al., 2010) is a translation resource that combines many bilingual dictionaries and provides translations for 5700 languages. Our proposed model is an extension of the method proposed by Duong et al. (2016). In their work, they only considered word embeddings, and so their method is unable to form representations for OOVs, and therefore is not expected to perform well for low-resource or morphologically-rich target languages. We extend the method of Duong et al. (2016) by incorporating sub-word information in the process of training cross-lingual word embeddings. In this way, we form a shared embedding space that not only contains embeddings for both source and target language words, but that has also been enriched with sub-word embeddings enabling representations to be formed for OOVs. To evalu"
2020.starsem-1.5,W15-1521,0,0.0644025,"Missing"
2020.starsem-1.5,N15-1157,0,0.025998,"gs for different languages have a similar geometric arrangement, which is key to the success of mappingbased models — does not always hold. They show that methods which jointly learn the embedding space for the source and target language from a parallel corpus are superior to mapping-based methods. However, parallel corpora are a very expensive cross-lingual signal. In an alternative approach to learning crosslingual word embeddings, a pesudo-bilingual corpus is first constructed using a bilingual dictionary, and embeddings for the source and target language are then learned from this corpus. Gouws and Søgaard (2015) concatenate and shuffle the source and target language corpora, and then randomly replace words in this corpus using a bilingual dictionary. They then run CBOW on the constructed corpus to learn word embeddings for both the source and target language. Similarly, Duong et al. (2016) also propose a method that replaces words in a pseudo-bilingual corpus with their translation during training. However, they further propose a way to handle polysemy by choosing the best translation for a word by considering its context using the expectation-maximization algorithm. Compared to mapping-based methods"
2020.starsem-1.5,W13-3512,0,0.0832411,"Missing"
2020.starsem-1.5,L18-1550,0,0.0410156,"Missing"
2020.starsem-1.5,Q19-1007,0,0.01562,"54 84.26 85.06 87.27 89.28 93.27 @1 45.98 21.08 41.83 44.24 54.62 36.21 54.02 55.15 76.13 it–en @5 77.11 60.78 77.11 77.44 73.83 72.42 77.64 80.12 86.87 @10 81.79 67.47 81.53 82.33 78.92 79.45 82.00 84.94 89.47 @1 40.73 24.36 41.41 38.16 42.25 36.54 47.56 46.21 71.53 nl–en @5 71.72 55.07 72.19 71.31 67.39 69.15 73.00 74.83 83.93 @10 77.06 62.65 77.88 77.67 72.80 76.25 78.69 80.11 86.53 Table 2: Precision@N for bilingual lexicon induction. The best performance, for each dataset and evaluation measure, is shown in boldface. Following previous work (e.g. Lample et al., 2018; Joulin et al., 2018; Jawanpuria et al., 2019), we consider MUSE test sets for evaluation. Word pairs occurring in both the MUSE test sets and our training dictionaries are removed from the training data before training the embeddings. We report precision@N — for N = 1, 5, and 10 — where the system is scored as correct if the gold-standard target word is amongst the top-N most similar target language words (Ruder et al., 2019). We use cosine as the similarity measure. Results are shown in Table 2. We begin by considering DUONG 2016 and our model using the best parameter settings from Duong et al. (2016), i.e., a learning rate of 0.025, 25"
2020.starsem-1.5,P19-1492,0,0.166322,"pretrained monolingual word embeddings, the quality of which the final cross-lingual word embeddings are greatly dependent upon. This is problematic in the case that we do not have access to enough training data to learn high quality monolingual embeddings, as would be the case for many low-resource languages. Moreover, it has been shown that fully unsupervised methods do not perform well across all languages, especially in the case of morphologically rich languages, and when the monolingual embeddings do not come from the same domain (Søgaard et al., 2018; Vuli´c et al., 2019). Furthermore, Ormazabal et al. (2019) show that the isomorphism assumption — i.e., that embeddings for different languages have a similar geometric arrangement, which is key to the success of mappingbased models — does not always hold. They show that methods which jointly learn the embedding space for the source and target language from a parallel corpus are superior to mapping-based methods. However, parallel corpora are a very expensive cross-lingual signal. In an alternative approach to learning crosslingual word embeddings, a pesudo-bilingual corpus is first constructed using a bilingual dictionary, and embeddings for the sou"
2020.starsem-1.5,D18-1330,0,0.0259058,"Missing"
2020.starsem-1.5,D19-6113,0,0.190503,"2 Related Work A variety of methods have been proposed for learning cross-lingual word embeddings. These methods vary with respect to the level of supervision, and the cross-lingual signals used, such as parallel corpora and bilingual dictionaries. Klementiev et al. (2012) propose a method to learn cross-lingual representations by training a language model on the source and target language and optimizing their objective function jointly. This method, however, requires a parallel corpus, which is not available for many languages, especially low-resource ones. More recently, Artetxe and Schwenk (2019) propose a bi-directional LSTM language model that is trained on a very large parallel corpus, containing 223 million parallel sentences, and jointly learns representations for 93 languages. Aside from requiring a parallel corpus, it is also computationally expensive to train. Mikolov et al. (2013b) argue that the geometric arrangement of word embeddings in two different languages is the same. They therefore propose a method to learn a linear transformation to align the vector spaces of two languages by using a seed lexicon of known translation pairs. Xing et al. (2015) show that normalizing a"
2020.starsem-1.5,D14-1162,0,0.0989419,"Hakimi Parizi Faculty of Computer Science University of New Brunswick ahakimi@unb.ca Paul Cook Faculty of Computer Science University of New Brunswick paul.cook@unb.ca Abstract languages, because many word-forms would not be expected to be observed in a training corpus. One way to address these problems is to transfer knowledge from a rich-resource language to a lower-resource language. Word Embeddings are a key feature in approaches for a wide range of NLP tasks, such as part-of-speech tagging (Al-Rfou’ et al., 2013), dependency parsing (Chen and Manning, 2014), and named entity recognition (Pennington et al., 2014). If we are able to transfer the knowledge captured in word embeddings for a rich-resource language to another low-resource language, then developing NLP tools could become more feasible for the low-resource language. There has therefore been a wealth of research on cross-lingual word embeddings (e.g., Mikolov et al., 2013b; Vuli´c and Moens, 2016; Lample et al., 2018), in which embeddings for multiple languages are learned in a shared space, and which can be used to transfer knowledge between languages, such as from a richresource language to a low-resource one (Ruder et al., 2019). Despite t"
2020.starsem-1.5,N15-1104,0,0.0729973,"Missing"
2020.starsem-1.5,N19-1097,0,0.0400953,"Missing"
2020.starsem-1.5,L18-1560,0,0.0231879,"anese, and so results are not available for these languages. XLMf t UDA does however substantially out-perform our proposed model on the other classification. This task is motivated by the situation where sufficient labelled training data is not available for a low-resource language. We consider zero-shot classification, i.e., we train a classifier and tune parameters on a rich-resource source language, and then apply the classifier directly to documents in a low-resource target language. Following previous work (e.g., Artetxe and Schwenk, 2019; Wu and Dredze, 2019), we use the MLDoc dataset (Schwenk and Li, 2018), which is a subset of the RCV1/RCV2 datasets (Lewis et al., 2004) with balanced classes for training, development, and test sets for the following languages: Chinese, English, French, German, Italian, Japanese, Spanish, and Russian. It has 1000 documents in each of the training and development sets, and 4000 documents in the test set, for each language. Following Artetxe and Schwenk (2019), we use English as the source language, and the other languages as target languages. To build corpora to train embeddings, again following previous work (Duong et al., 2016; Klementiev et al., 2012), we fir"
2020.starsem-1.5,P16-1162,0,0.0256733,"extrinsic evaluation, to show the impact of having sub-word knowledge in a down-stream NLP task, we consider cross-lingual document classification. Again our results indicate that incorporating sub-word information leads to improvements, and furthermore we find our proposed model to perform on par with, or better than, strong benchmark approaches. Recently, with advances in language modelling (Artetxe and Schwenk, 2019) and contextualized language models (Devlin et al., 2019; Conneau and Lample, 2019), transfer learning has become feasible between languages by using a byte pair encoding (BPE, Sennrich et al., 2016) shared vocabulary, and fine-tuning the models for specific tasks (Wu and Dredze, 2019). Nevertheless, these models require a substantial amount of training data (Conneau and Lample, 2019), and in some cases parallel corpora (Artetxe and Schwenk, 2019; Conneau and Lample, 2019), and are very computationally expensive to train. There is therefore a need for methods that can be trained from a morelimited amount of data and require less computational resources for training, but that nevertheless show comparable performance. 2 Related Work A variety of methods have been proposed for learning cross"
2020.starsem-1.5,P18-1072,0,0.027427,"Missing"
2020.starsem-1.5,D19-1449,0,0.0272684,"Missing"
2020.starsem-1.5,N13-1011,0,0.0571902,"Missing"
2021.mwe-1.4,L18-1402,0,0.11793,"to ‘start a journey’: 23 Proceedings of the 17th Workshop on Multiword Expressions, pages 23–32 Bangkok, Thailand (online), August 6, 2021. ©2021 Association for Computational Linguistics or literal; however, we measure the ability of these approaches to generalize to expressions that were not observed during training, and also to generalize across languages. We begin by considering monolingual experiments for English and Russian in which we train and test on instances of the same PIEs. For English, we focus on VNCs (Cook et al., 2008). For Russian, we consider a wider-range of types of PIEs (Aharodnik et al., 2018). We then consider a second monolingual setting in which we evaluate on PIEs, again either English or Russian, that were not observed during training. Finally, we consider cross-lingual detection of idiomaticity. Here we train on instances of PIEs in one language, English or Russian, and evaluate on instances of PIEs in the other language. Our findings evaluating on expressions that were observed during training are similar to those of (Kurfalı and Östling, 2020); we achieve strong improvements over baselines, and on English outperform previous approaches based on conventional word embeddings"
2021.mwe-1.4,D17-1263,0,0.0314417,"Missing"
2021.mwe-1.4,P19-1356,0,0.0150909,"the case of the all expressions experimental setup. Furthermore BERT and RoBERTa (without the CF feature) outperform the approach of King and Cook (2018), although given the standard deviation across runs, this difference does not appear to be significant for BERT when comparing against the approach of King and Cook when they use the CF feature. perimental setup for BERT and RoBERTa, in an effort to explain the relatively poor performance of BERT here. Results are shown in Table 3.7 In all cases, except for BERT on EN - TEST, the final layer performs best. This is inline with the findings of Jawahar et al. (2019) that the upper layers of BERT encode semantic information. For BERT, where accuracy was low on EN - TEST relative to EN - DEV in Table 2, on EN - TEST the second last layer performs best. 5.2 Russian For monolingual experiments on Russian, we again consider the all and unseen expressions experimental setups. Here we compare against a mostfrequent class baseline. Although Aharodnik et al. In experiments until now we have used representations from the final layer of contextualized embedding models (BERT, RoBERTa, and mBERT). We now consider the effect of using different hidden layers, focusing"
2021.mwe-1.4,P18-2055,1,0.370876,"ortant for down-stream natural language processing applications such as machine translation (Isabelle et al., 2017). Previous work has considered both unsupervised and supervised approaches to predicting the tokenlevel idiomaticity of PIEs. However, annotated data to train supervised approaches is not available for all PIEs in all languages. This makes unsupervised approaches (e.g., Fazly et al., 2009; Haagsma et al., 2018; Liu and Hwa, 2018; Kurfalı and Östling, 2020), which do not have this resource requirement, appealing. On the other hand, supervised approaches (e.g., Salton et al., 2016; King and Cook, 2018) tend to outperform unsupervised approaches, but are restricted to languages and PIEs for which annotated training data is available. In this paper we consider supervised approaches based on contextualized embeddings (Devlin et al., 2019; Liu et al., 2019; Kuratov and Arkhipov, 2019) to predicting usages of PIEs as idiomatic On the other hand, hit the road, can also be used literally, as in the example below: 1 These example sentences are taken, with light editing, from the VNC-Tokens dataset (Cook et al., 2008). 1 Introduction Multiword expressions (MWEs) are lexicalized combinations of multi"
2021.mwe-1.4,N19-1423,0,0.632401,"s. However, annotated data to train supervised approaches is not available for all PIEs in all languages. This makes unsupervised approaches (e.g., Fazly et al., 2009; Haagsma et al., 2018; Liu and Hwa, 2018; Kurfalı and Östling, 2020), which do not have this resource requirement, appealing. On the other hand, supervised approaches (e.g., Salton et al., 2016; King and Cook, 2018) tend to outperform unsupervised approaches, but are restricted to languages and PIEs for which annotated training data is available. In this paper we consider supervised approaches based on contextualized embeddings (Devlin et al., 2019; Liu et al., 2019; Kuratov and Arkhipov, 2019) to predicting usages of PIEs as idiomatic On the other hand, hit the road, can also be used literally, as in the example below: 1 These example sentences are taken, with light editing, from the VNC-Tokens dataset (Cook et al., 2008). 1 Introduction Multiword expressions (MWEs) are lexicalized combinations of multiple words, which display some form of idiomaticity (Baldwin and Kim, 2010). In this paper we focus on potentiallyidiomatic expressions (PIEs), i.e., expressions which are ambiguous between a semanticallyopaque idiomatic interpretation, a"
2021.mwe-1.4,J09-1005,1,0.730587,"on PIE instances in one language, English or Russian, and tested on the other language. We find that the model outperforms baselines in this setting. These findings suggest that contextualized embeddings are able to learn representations that encode knowledge of idiomaticity that is not restricted to specific expressions, nor to a specific language. 1. The marchers had hit the road before 0500 hours and by midday they were limping back having achieved success on day one. PIEs occur across languages, with one particularly common class of PIE cross-lingually being verb–noun combinations (VNCs, Fazly et al., 2009) — i.e., PIEs consisting of a verb with a noun in its direct object position — such as hit the road in the example above. Although VNCs are common, PIEs also occur in other syntactic constructions, with English examples including combinations of a verb and prepositional phrase — e.g., skating on thin ice (which can be used idiomatically to mean roughly ‘at risk’) — and prepositional phrases — e.g., off the hook (with a potential idiomatic meaning of roughly ‘out of danger’). Distinguishing between literal and idiomatic usages of PIEs could be particularly important for down-stream natural lang"
2021.mwe-1.4,W16-1817,1,0.898156,"Missing"
2021.mwe-1.4,D18-1199,0,0.127986,"— e.g., off the hook (with a potential idiomatic meaning of roughly ‘out of danger’). Distinguishing between literal and idiomatic usages of PIEs could be particularly important for down-stream natural language processing applications such as machine translation (Isabelle et al., 2017). Previous work has considered both unsupervised and supervised approaches to predicting the tokenlevel idiomaticity of PIEs. However, annotated data to train supervised approaches is not available for all PIEs in all languages. This makes unsupervised approaches (e.g., Fazly et al., 2009; Haagsma et al., 2018; Liu and Hwa, 2018; Kurfalı and Östling, 2020), which do not have this resource requirement, appealing. On the other hand, supervised approaches (e.g., Salton et al., 2016; King and Cook, 2018) tend to outperform unsupervised approaches, but are restricted to languages and PIEs for which annotated training data is available. In this paper we consider supervised approaches based on contextualized embeddings (Devlin et al., 2019; Liu et al., 2019; Kuratov and Arkhipov, 2019) to predicting usages of PIEs as idiomatic On the other hand, hit the road, can also be used literally, as in the example below: 1 These exam"
2021.mwe-1.4,2021.ccl-1.108,0,0.0520937,"Missing"
2021.mwe-1.4,K16-1006,0,0.0336914,"aining PIEs. King and Cook (2018) achieve better results using a simpler sentence representation based on average of word embeddings. Moreover, King and Cook show that adding a single binary feature to the sentence representation indicating whether the VNC occurs in a canonical form — based on the method of Fazly et al. (2009) — gives substantial improvements. Hashempour and Villavicencio (2020) propose a supervised approach in which PIE instances are treated as single units by fusing their lexicalized component words, and learning representations of these units using word and contextualized (Melamud et al., 2016; Devlin et al., 2019) embeddings. Hashempour and Villavicencio also focus on VNCs. Although they show improvements by treating VNC instances as fused units, they do not outperform King and Cook; they do, however, train their models on smaller corpora. Shwartz and Dagan (2019) use representations of spans of tokens based on contextualized embedding for predicting a range of MWE properties. Most closely related to our work, they consider light-verb construction and verb-particle construction classification, for both of which there is an ambiguity between MWE usages and similar-on-the-surface li"
2021.mwe-1.4,P19-1493,0,0.0478672,"Missing"
2021.mwe-1.4,P16-1019,0,0.335083,"d be particularly important for down-stream natural language processing applications such as machine translation (Isabelle et al., 2017). Previous work has considered both unsupervised and supervised approaches to predicting the tokenlevel idiomaticity of PIEs. However, annotated data to train supervised approaches is not available for all PIEs in all languages. This makes unsupervised approaches (e.g., Fazly et al., 2009; Haagsma et al., 2018; Liu and Hwa, 2018; Kurfalı and Östling, 2020), which do not have this resource requirement, appealing. On the other hand, supervised approaches (e.g., Salton et al., 2016; King and Cook, 2018) tend to outperform unsupervised approaches, but are restricted to languages and PIEs for which annotated training data is available. In this paper we consider supervised approaches based on contextualized embeddings (Devlin et al., 2019; Liu et al., 2019; Kuratov and Arkhipov, 2019) to predicting usages of PIEs as idiomatic On the other hand, hit the road, can also be used literally, as in the example below: 1 These example sentences are taken, with light editing, from the VNC-Tokens dataset (Cook et al., 2008). 1 Introduction Multiword expressions (MWEs) are lexicalized"
2021.mwe-1.4,Q19-1027,0,0.0245222,"nical form — based on the method of Fazly et al. (2009) — gives substantial improvements. Hashempour and Villavicencio (2020) propose a supervised approach in which PIE instances are treated as single units by fusing their lexicalized component words, and learning representations of these units using word and contextualized (Melamud et al., 2016; Devlin et al., 2019) embeddings. Hashempour and Villavicencio also focus on VNCs. Although they show improvements by treating VNC instances as fused units, they do not outperform King and Cook; they do, however, train their models on smaller corpora. Shwartz and Dagan (2019) use representations of spans of tokens based on contextualized embedding for predicting a range of MWE properties. Most closely related to our work, they consider light-verb construction and verb-particle construction classification, for both of which there is an ambiguity between MWE usages and similar-on-the-surface literal combinaRelated Work Previous work has considered unsupervised and supervised approaches to predicting the token-level idiomaticity of PIEs. Although unsupervised methods have been proposed to disambiguate a wide range of kinds of potentially-idiomatic expressions (Haagsm"
2021.semeval-1.83,N04-1025,0,0.352323,"Missing"
2021.semeval-1.83,N19-1423,0,0.0267947,"get word as its input and the first hidden state is initialized with the embedding of the input sentence. The last hidden state is then passed to the systems described in Section 3 for complexity prediction. In this approach, our hypothesis is that the hidden state of the first language model provides a representation for the input sentence, and the language model can encode the complexity of the target word by having access to a representation of the sentence in which the target word appears. Figure 1 shows a diagram of this model. 2.3 Masked language model In this approach, we recruit BERT (Devlin et al., 2019) as a masked language model to estimate the probability for the target token in context. CollinsThompson and Callan (2004) show that language modeling can be used to predict reading difficulty, and therefore we hypothesize that language modeling can also be useful for predicting lexical complexity. We use the large uncased pretrained BERT model, which consists of 16 heads and 24 layers of 1024 hidden units each. Given a sentence, we replace the target token with the special [MASK] token and use the modified sentence as input to BERT to obtain the probability of the target token. BERT’s tokeniz"
2021.semeval-1.83,2020.readi-1.9,0,0.0269744,"omplexity prediction. 1 Introduction SemEval 2021 Task 1 (Shardlow et al., 2021) focuses on predicting the complexity of a target token in an English sentence on a scale from 0 (not very complex) to 1 (very complex). For example, the token land in the example below is judged to have a low complexity of 0.19. 1. Our land will yield its increase. On the other hand, the token doxycycline in the following example is judged to have a relatively higher complexity of 0.75.1 2. The reason these two lines were unresponsive to doxycycline is unknown. The dataset for this task was originally proposed in Shardlow et al. (2020), and includes sentences from three sources: a translated bible, European Parliament proceedings, and a biomedical corpus. This shared task contains two sub-tasks with the first focusing on lexical complexity for single words, which will be referred to as SINGLE, and the second focusing on complexity of multiword expressions, which will be referred to as MULTI. In this paper, we explore the use of statistical baseline features, masked language models, and character-level encoders to predict the complexity of a target token in context. We first consider these approaches individually and then co"
2021.semeval-1.83,2021.semeval-1.1,0,0.0416211,"rian,paul.cook}@unb.ca Abstract In this paper, we present three supervised systems for English lexical complexity prediction of single and multiword expressions for SemEval-2021 Task 1. We explore the use of statistical baseline features, masked language models, and character-level encoders to predict the complexity of a target token in context. Our best system combines information from these three sources. The results indicate that information from masked language models and character-level encoders can be combined to improve lexical complexity prediction. 1 Introduction SemEval 2021 Task 1 (Shardlow et al., 2021) focuses on predicting the complexity of a target token in an English sentence on a scale from 0 (not very complex) to 1 (very complex). For example, the token land in the example below is judged to have a low complexity of 0.19. 1. Our land will yield its increase. On the other hand, the token doxycycline in the following example is judged to have a relatively higher complexity of 0.75.1 2. The reason these two lines were unresponsive to doxycycline is unknown. The dataset for this task was originally proposed in Shardlow et al. (2020), and includes sentences from three sources: a translated"
2021.starsem-1.29,E17-1088,0,0.0432937,"Missing"
2021.starsem-1.29,W13-3520,0,0.051401,"Missing"
2021.starsem-1.29,2020.acl-main.766,0,0.547724,"t al., 2013b), semi-supervised (Artetxe et al., 2017), or unsupervised (e.g., Lample et al., 2018) methods. These methods rely on the assumption that the geometric arrangement of embeddings in different languages is the same. However, it has been shown that this assumption does not always hold, and that methods which instead jointly train embeddings for two languages produce embeddings that are more isomorphic and achieve stronger results for bilingual lexicon induction (BLI, Ormazabal et al., 2019), a well-known intrinsic evaluation for cross-lingual word representations (Ruder et al., 2019; Anastasopoulos and Neubig, 2020). The approach of Ormazabal et al. uses a parallel corpus as a cross-lingual signal. Parallel corpora are, however, unavailable for many language pairs, particularly low-resource languages. Duong et al. (2016) introduce a joint training approach that extends CBOW (Mikolov et al., 2013a) to learn cross-lingual word embeddings from modest size monolingual corpora, using a bilingual dictionary as the cross-lingual signal. Bilingual dictionaries are available for many language pairs, e.g., Panlex (Baldwin et al., 2010) provides translations for roughly 5700 languages. These training resource requi"
2021.starsem-1.29,P17-1042,0,0.176545,"n essential component in systems for many natural language processing tasks such as part-of-speech tagging (Al-Rfou’ et al., 2013), dependency parsing (Chen and Manning, 2014) and named entity recognition (Pennington et al., 2014). Cross-lingual word representations provide a shared space for word embeddings of two languages, and make it possible to transfer information between languages (Ruder et al., 2019). A common approach to learn cross-lingual embeddings is to learn a matrix to map the embeddings of one language to another using supervised (e.g., Mikolov et al., 2013b), semi-supervised (Artetxe et al., 2017), or unsupervised (e.g., Lample et al., 2018) methods. These methods rely on the assumption that the geometric arrangement of embeddings in different languages is the same. However, it has been shown that this assumption does not always hold, and that methods which instead jointly train embeddings for two languages produce embeddings that are more isomorphic and achieve stronger results for bilingual lexicon induction (BLI, Ormazabal et al., 2019), a well-known intrinsic evaluation for cross-lingual word representations (Ruder et al., 2019; Anastasopoulos and Neubig, 2020). The approach of Orm"
2021.starsem-1.29,C10-3010,0,0.0341532,"aluation for cross-lingual word representations (Ruder et al., 2019; Anastasopoulos and Neubig, 2020). The approach of Ormazabal et al. uses a parallel corpus as a cross-lingual signal. Parallel corpora are, however, unavailable for many language pairs, particularly low-resource languages. Duong et al. (2016) introduce a joint training approach that extends CBOW (Mikolov et al., 2013a) to learn cross-lingual word embeddings from modest size monolingual corpora, using a bilingual dictionary as the cross-lingual signal. Bilingual dictionaries are available for many language pairs, e.g., Panlex (Baldwin et al., 2010) provides translations for roughly 5700 languages. These training resource requirements suggest this method could be well-suited to lower-resource languages. However, this word-level approach is unable to form representations for out-of-vocabulary (OOV) words, which could be particularly common in the case of lowresource, and morphologically-rich, languages. Hakimi Parizi and Cook (2020b) propose an extension of Duong et al. (2016) that incorporates subword information during training and therefore can generate representations for OOVs in the shared cross-lingual space. This method also does n"
2021.starsem-1.29,Q17-1010,0,0.243205,"guages, and also consider an evaluation focused on OOVs. Our results indicate that HAKIMI 2020 gives improvements over DUONG 2016 and several strong baselines, particularly for OOVs. 2 Joint Training Incorporating Sub-word Information Equation 1 shows the cost function for DUONG 2016, which jointly learns embeddings for a word wi and its translation w ¯i , where hi is a vector encoding the context, α is a weight parameter, and Ds and Dt are the source and target language vocabularies, respectively. O= X i∈Ds ∪Dt + (1 − + p X α) log σ(uTw¯i hi ) Ewj ∼Pn (w) log σ(−uTwj hi )  j=1 (1) Following Bojanowski et al. (2017), HAKIMI 2020 modifies Equation 1 by including sub-word information during the joint training process as follows: O= X α log S(wi , hi ) + p X 3.1  Ewj ∼Pn (w) log −S(wj , hi ) j=1 (2) S(w, h) = 1 X T zg h |Gw| (3) g∈Gw where Gw is the set of sub-words appearing in w and zg is the sub-word embedding for g. h is calculated by averaging the representations for each word appearing in the context, where each word is itself represented by the average of its sub-word embeddings. HAKIMI 2020 use character n-grams as subwords. Specifically, each word is augmented with special beginning and end of wor"
2021.starsem-1.29,D14-1082,0,0.0148874,"uring training. This method could be particularly well-suited to lower-resource and morphologically-rich languages because it can be trained on modest size monolingual corpora, and is able to represent out-of-vocabulary words (OOVs). We consider bilingual lexicon induction, including an evaluation focused on OOVs. We find that this method achieves improvements over previous approaches, particularly for OOVs. 1 Introduction Word embeddings are an essential component in systems for many natural language processing tasks such as part-of-speech tagging (Al-Rfou’ et al., 2013), dependency parsing (Chen and Manning, 2014) and named entity recognition (Pennington et al., 2014). Cross-lingual word representations provide a shared space for word embeddings of two languages, and make it possible to transfer information between languages (Ruder et al., 2019). A common approach to learn cross-lingual embeddings is to learn a matrix to map the embeddings of one language to another using supervised (e.g., Mikolov et al., 2013b), semi-supervised (Artetxe et al., 2017), or unsupervised (e.g., Lample et al., 2018) methods. These methods rely on the assumption that the geometric arrangement of embeddings in different lang"
2021.starsem-1.29,D18-1027,0,0.115129,"sented as a bag of character n-grams, using n-grams of length 3–6 characters. The entire word itself (with beginning and end of word markers) is also included among the sub-words. # Dict. entries 70k 17k 25k 114k 23k 388k 201k 253k 79k 296k 460k 319k Experimental Setup We consider BLI from twelve lower-resource source languages to English. The languages (shown in Table 1) were selected to cover a variety of language families, while having small to medium size Wikipedias and BLI evaluation datasets available. We compare HAKIMI 2020 with DUONG 2016, V EC M AP (Artetxe et al., 2018), and M EEMI (Doval et al., 2018). In each case, we use cosine similarity to find the closest target language translations for a source language word. We evaluate using precision@N (Ruder et al., 2019) for N = 1, 5, 10. i∈Ds ∪Dt + (1 − α) log S(w ¯i , hi ) # Tokens 25M 21M 36M 26M 18M 54M 38M 78M 143M 34M 133M 79M Table 1: The language family, size of corpus, and size of Panlex dictionary, for each source language. 3 α log σ(uTwi hi ) Family Germanic Albanian Turkic Indic Slavic Slavic Uralic Greek Semitic Indic Uralic Turkic Training Corpora and Dictionaries The corpus for each language is a Wikipedia dump from 27 July 2020,"
2021.starsem-1.29,D16-1136,0,0.10121,"me. However, it has been shown that this assumption does not always hold, and that methods which instead jointly train embeddings for two languages produce embeddings that are more isomorphic and achieve stronger results for bilingual lexicon induction (BLI, Ormazabal et al., 2019), a well-known intrinsic evaluation for cross-lingual word representations (Ruder et al., 2019; Anastasopoulos and Neubig, 2020). The approach of Ormazabal et al. uses a parallel corpus as a cross-lingual signal. Parallel corpora are, however, unavailable for many language pairs, particularly low-resource languages. Duong et al. (2016) introduce a joint training approach that extends CBOW (Mikolov et al., 2013a) to learn cross-lingual word embeddings from modest size monolingual corpora, using a bilingual dictionary as the cross-lingual signal. Bilingual dictionaries are available for many language pairs, e.g., Panlex (Baldwin et al., 2010) provides translations for roughly 5700 languages. These training resource requirements suggest this method could be well-suited to lower-resource languages. However, this word-level approach is unable to form representations for out-of-vocabulary (OOV) words, which could be particularly"
2021.starsem-1.29,P17-2093,0,0.0670229,"Missing"
2021.starsem-1.29,2020.lrec-1.330,1,0.898502,"to learn cross-lingual word embeddings from modest size monolingual corpora, using a bilingual dictionary as the cross-lingual signal. Bilingual dictionaries are available for many language pairs, e.g., Panlex (Baldwin et al., 2010) provides translations for roughly 5700 languages. These training resource requirements suggest this method could be well-suited to lower-resource languages. However, this word-level approach is unable to form representations for out-of-vocabulary (OOV) words, which could be particularly common in the case of lowresource, and morphologically-rich, languages. Hakimi Parizi and Cook (2020b) propose an extension of Duong et al. (2016) that incorporates subword information during training and therefore can generate representations for OOVs in the shared cross-lingual space. This method also does not require parallel corpora for training, and could therefore be particularly well-suited to lower-resource, and morphologically-rich, languages. However, Hakimi Parizi and Cook only evaluate on synthetic low-resource languages. We refer to the methods of Duong et al. and Hakimi Parizi and Cook as DUONG 2016 and HAKIMI 2020, respectively. Most prior work on BLI focuses on invocabulary ("
2021.starsem-1.29,2020.starsem-1.5,1,0.813802,"to learn cross-lingual word embeddings from modest size monolingual corpora, using a bilingual dictionary as the cross-lingual signal. Bilingual dictionaries are available for many language pairs, e.g., Panlex (Baldwin et al., 2010) provides translations for roughly 5700 languages. These training resource requirements suggest this method could be well-suited to lower-resource languages. However, this word-level approach is unable to form representations for out-of-vocabulary (OOV) words, which could be particularly common in the case of lowresource, and morphologically-rich, languages. Hakimi Parizi and Cook (2020b) propose an extension of Duong et al. (2016) that incorporates subword information during training and therefore can generate representations for OOVs in the shared cross-lingual space. This method also does not require parallel corpora for training, and could therefore be particularly well-suited to lower-resource, and morphologically-rich, languages. However, Hakimi Parizi and Cook only evaluate on synthetic low-resource languages. We refer to the methods of Duong et al. and Hakimi Parizi and Cook as DUONG 2016 and HAKIMI 2020, respectively. Most prior work on BLI focuses on invocabulary ("
2021.starsem-1.29,P19-1492,0,0.0967842,"gual embeddings is to learn a matrix to map the embeddings of one language to another using supervised (e.g., Mikolov et al., 2013b), semi-supervised (Artetxe et al., 2017), or unsupervised (e.g., Lample et al., 2018) methods. These methods rely on the assumption that the geometric arrangement of embeddings in different languages is the same. However, it has been shown that this assumption does not always hold, and that methods which instead jointly train embeddings for two languages produce embeddings that are more isomorphic and achieve stronger results for bilingual lexicon induction (BLI, Ormazabal et al., 2019), a well-known intrinsic evaluation for cross-lingual word representations (Ruder et al., 2019; Anastasopoulos and Neubig, 2020). The approach of Ormazabal et al. uses a parallel corpus as a cross-lingual signal. Parallel corpora are, however, unavailable for many language pairs, particularly low-resource languages. Duong et al. (2016) introduce a joint training approach that extends CBOW (Mikolov et al., 2013a) to learn cross-lingual word embeddings from modest size monolingual corpora, using a bilingual dictionary as the cross-lingual signal. Bilingual dictionaries are available for many lan"
2021.starsem-1.29,D14-1162,0,0.0874565,"-suited to lower-resource and morphologically-rich languages because it can be trained on modest size monolingual corpora, and is able to represent out-of-vocabulary words (OOVs). We consider bilingual lexicon induction, including an evaluation focused on OOVs. We find that this method achieves improvements over previous approaches, particularly for OOVs. 1 Introduction Word embeddings are an essential component in systems for many natural language processing tasks such as part-of-speech tagging (Al-Rfou’ et al., 2013), dependency parsing (Chen and Manning, 2014) and named entity recognition (Pennington et al., 2014). Cross-lingual word representations provide a shared space for word embeddings of two languages, and make it possible to transfer information between languages (Ruder et al., 2019). A common approach to learn cross-lingual embeddings is to learn a matrix to map the embeddings of one language to another using supervised (e.g., Mikolov et al., 2013b), semi-supervised (Artetxe et al., 2017), or unsupervised (e.g., Lample et al., 2018) methods. These methods rely on the assumption that the geometric arrangement of embeddings in different languages is the same. However, it has been shown that this"
2021.starsem-1.29,P16-1024,0,0.059628,"Missing"
2021.starsem-1.29,2020.acl-main.201,0,0.0247969,"training and therefore can generate representations for OOVs in the shared cross-lingual space. This method also does not require parallel corpora for training, and could therefore be particularly well-suited to lower-resource, and morphologically-rich, languages. However, Hakimi Parizi and Cook only evaluate on synthetic low-resource languages. We refer to the methods of Duong et al. and Hakimi Parizi and Cook as DUONG 2016 and HAKIMI 2020, respectively. Most prior work on BLI focuses on invocabulary (IV) words and well-resourced languages (e.g., Artetxe et al., 2017; Ormazabal et al., 2019; Zhang et al., 2020), although there has been some work on OOVs (Hakimi Parizi and Cook, 2020a) and low-resource languages (Anastasopoulos and Neubig, 2020). In this paper, we evaluate HAKIMI 2020 on BLI for twelve lower-resource 302 Proceedings of the 10th Conference on Lexical and Computational Semantics, pages 302–307 August 5–6, 2021, Bangkok, Thailand (online) ©2021 Association for Computational Linguistics Language Afrikaans Albanian Azerbaijani Bengali Bosnian Croatian Estonian Greek Hebrew Hindi Hungarian Turkish languages, and also consider an evaluation focused on OOVs. Our results indicate that HAKIMI"
C12-1064,W03-0109,0,0.0523152,"onger or more homogeneous document sets. In contrast, social media data consists of terse noisy texts, presenting a challenge for these approaches. For instance, any reliance on named entity recognition is thwarted by the unedited nature of social media data, where spelling and capitalisation are much more ad hoc than in edited document collections. The spatial data mining community has tended to approach the task via identifying geographical references in documents (also known as geoparsing: Leidner and Lieberman (2011)). Methods range from naive gazetteer matching and rule-based approaches (Bilhaut et al., 2003), to machine learning-based methods (mainly based on named entity recognition: Qin et al. (2010); Gelernter and Mushegian (2011)). The principal drawback of these methods is that they rely on explicit mentions of addresses or formal placenames in the text, rather than words which are more informally associated with a place. In social media data, we can’t rely on a given user mentioning an address or formal placename, severely limiting the coverage of such methods.3 There has been a limited amount of work on geolocation prediction based on social network analysis (Backstrom et al., 2010), but s"
C12-1064,D10-1124,0,0.559157,"Missing"
C12-1064,P12-3005,1,0.118954,"Missing"
C12-1064,D12-1137,0,0.504242,"kstrom et al., 2010), but social networks are dynamic and the data is often hard to obtain. In terms of text-based geolocation prediction, Cheng et al. (2010) estimate the city-level user geolocation for the continental US with a simple probabilistic model, which they complement with strictly local words and smoothing. Compared with their approach, our LIW selection requires no explicit training data and is more flexible. Wing and Baldridge (2011) use KL-divergence (Kullback and Leibler, 1951) to measure the similarity between different geo-grids specified by geospatial coordinates. Recently, Roller et al. (2012) extend this idea using a KD-tree-based adaptive grid and grid centroids, achieving state-of-the-art geolocation prediction results. Li et al. (2011) investigated the prediction of Places of Interest (POIs) based on linear rank combination of content and temporal factors. Kinsella et al. (2011) compare a variety of geolocation prediction classification models at different location granularities. Adams and Janowicz (2012) utilise external geo-reference data to infer locations. Mahmud et al. (2012) combine timezone information and content-based classifiers in a hierarchical model for geolocation"
C12-1064,P11-1096,0,0.497895,"rowing popularity of social media, massive volumes of user-generated data are produced everyday, e.g. in the form of Twitter messages (tweets) and Facebook updates.1 This data provides many new opportunities and challenges for natural language processing. One such challenge is geolocation prediction: predicting the geolocation of a message or user based on their social media posts. In this paper, we focus on user-level geolocation based on the aggregated body of tweets from a user, and estimate the user’s location at the city level. As is well established in previous work (Cheng et al., 2010; Wing and Baldridge, 2011; Kinsella et al., 2011), it is reasonable to assume that user posts in social media reflect their geospatial locum, because lexical priors differ from region to region. For example, a user in London is much more likely to talk about Piccadilly and British than a user in New York or Beijing. That is not to say that those terms are uniquely associated with London, of course: British could be used by a user outside of the UK to discuss something relating to the UK. However, the use of a range of such terms with high relative frequency is strongly indicative of the fact that a user is located in"
C14-1154,E09-1013,0,0.0784519,"Missing"
C14-1154,P13-1141,0,0.0154691,"pproaches there is a clear connection between (induced) word-senses and tokens, making it possible to identify usages of a specific (new) sense. Other work has focused on sense differences between dialects and domains. Peirsman et al. (2010) consider the identification of words that are typical of Belgian and Netherlandic Dutch, due to either marked frequency or sense. McCarthy et al. (2007) consider the identification of predominant wordsenses in corpora, including differences between domains. However, this approach does not identify new senses as it relies on a pre-existing sense inventory. Carpuat et al. (2013) identify words in a domainspecific parallel corpus with novel translations. The method proposed by Lau et al. (2012), and extended by Cook et al. (2013), identifies novel wordsenses using a state-of-the-art word-sense induction (WSI) system. This token-based approach offers a natural account of polysemy and not only identifies word types that have a novel sense, but identifies the token instances of the hypothesized novel senses, without reliance on parallel text or a pre-existing sense inventory. We therefore adopt this method for evaluation on our new dataset, and propose further extensions"
C14-1154,Y11-1028,1,0.838437,"about the expected domain(s) of novel senses. 2 Related work Identifying diachronic changes in word-sense is a challenge that has only been considered rather recently in computational linguistics. Sagi et al. (2009) and Cook and Stevenson (2010) propose methods to identify specific types of semantic change — widening and narrowing, and amelioration and pejoration, respectively — based on specific properties of these phenomena. Gulordava and Baroni (2011) identify diachronic sense change in an n-gram database, but using a model that is not restricted to any particular type of semantic change. Cook and Hirst (2011) consider the impact of sense frequency on methods for identifying novel senses. Crucially, all of the aforementioned approaches are type-based: they are able to identify words that have undergone a change in meaning, but not the token instances which give rise to these sense differences. Bamman and Crane (2011) use a parallel Latin–English corpus to induce word senses and build a WSD system, which they then apply to study diachronic variation in sense frequency. Rohrdantz et al. (2011) present a system for visualizing changes in word usage over time. Crucially, in these token-based approaches"
C14-1154,cook-stevenson-2010-automatically,1,0.937154,"d in research to date; (2) development and evaluation of a new baseline for novel sense detection, reformulations of the method of Lau et al., and a method that incorporates only the expected domain(s) of novel senses; (3) empirical evaluation of the method of Cook et al.; and (4) extension of the novel sense detection method of Cook et al. to automatically acquire information about the expected domain(s) of novel senses. 2 Related work Identifying diachronic changes in word-sense is a challenge that has only been considered rather recently in computational linguistics. Sagi et al. (2009) and Cook and Stevenson (2010) propose methods to identify specific types of semantic change — widening and narrowing, and amelioration and pejoration, respectively — based on specific properties of these phenomena. Gulordava and Baroni (2011) identify diachronic sense change in an n-gram database, but using a model that is not restricted to any particular type of semantic change. Cook and Hirst (2011) consider the impact of sense frequency on methods for identifying novel senses. Crucially, all of the aforementioned approaches are type-based: they are able to identify words that have undergone a change in meaning, but not"
C14-1154,W11-2508,0,0.203893,"senses; (3) empirical evaluation of the method of Cook et al.; and (4) extension of the novel sense detection method of Cook et al. to automatically acquire information about the expected domain(s) of novel senses. 2 Related work Identifying diachronic changes in word-sense is a challenge that has only been considered rather recently in computational linguistics. Sagi et al. (2009) and Cook and Stevenson (2010) propose methods to identify specific types of semantic change — widening and narrowing, and amelioration and pejoration, respectively — based on specific properties of these phenomena. Gulordava and Baroni (2011) identify diachronic sense change in an n-gram database, but using a model that is not restricted to any particular type of semantic change. Cook and Hirst (2011) consider the impact of sense frequency on methods for identifying novel senses. Crucially, all of the aforementioned approaches are type-based: they are able to identify words that have undergone a change in meaning, but not the token instances which give rise to these sense differences. Bamman and Crane (2011) use a parallel Latin–English corpus to induce word senses and build a WSD system, which they then apply to study diachronic"
C14-1154,S13-2049,0,0.0594711,"Missing"
C14-1154,H05-1053,1,0.75209,"nexpensively constructed in the future (Cook and Hirst, 2012). We conducted some preliminary experiments using domain-specific sports 1632 Method NoveltyDiff NoveltyLLR NoveltyRatio RelevanceAuto RelevanceManual Rank SumDiff,auto Rank SumDiff,manual Upper-bound Baseline F-score BNC–ukWaC SiBol/Port 0.57 0.29 0.67 0.28 0.66 0.28 0.48 0.24 0.45 0.27 0.72 0.30 0.72 0.29 0.72 0.42 0.36 0.20 Table 2: Token-level F-score for the BNC–ukWaC and SiBol/Port datasets using variants of Novelty, Relevance, and Rank Sum. The F-score of an oracle upper-bound and baseline are also shown. and finance corpora (Koeling et al., 2005) and the BNC. However, in these experiments we observed very high NoveltyRatio for many distractors (selected in a similar way to our other experiments). Unlike the case of time difference, in corpora from different domains, an arbitrarily chosen word will tend to cooccur with very different words in the corpora, and NoveltyRatio will consequently be high. To address vocabulary differences between corpora, in their experiments on identifying lexical semantic differences between Dutch dialects, Peirsman et al. (2010) restricted the context words used to represent a target word to those with mod"
C14-1154,S13-2051,1,0.89377,"Missing"
C14-1154,S13-2039,1,0.871209,"Missing"
C14-1154,E12-1060,1,0.877532,"s for identifying new word-senses could benefit applied NLP by helping to keep lexicons up-to-date. In revising dictionaries, lexicographers must identify new word-senses, in addition to new words themselves; methods which identify new word-senses could therefore also help to keep dictionaries current. In this paper, because of the need for lexicon maintenance, we focus on relatively-new word-senses. Specifically, we consider the identification of word-senses that are not attested in a reference corpus, taken to represent standard usage, but that are attested in a focus corpus of newer texts. Lau et al. (2012) introduced the task of novel sense identification. They presented a method for identifying novel word-senses — described here in Section 4 — and evaluated this method on a very small dataset consisting of just five lemmas having a novel sense in a single corpus pair. Cook et al. (2013) extended the method of Lau et al. to incorporate knowledge of the expected domains of new wordsenses, but did not conduct a rigorous empirical evaluation. The remainder of this paper is structured This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings"
C14-1154,J07-4005,1,0.808136,"d senses and build a WSD system, which they then apply to study diachronic variation in sense frequency. Rohrdantz et al. (2011) present a system for visualizing changes in word usage over time. Crucially, in these token-based approaches there is a clear connection between (induced) word-senses and tokens, making it possible to identify usages of a specific (new) sense. Other work has focused on sense differences between dialects and domains. Peirsman et al. (2010) consider the identification of words that are typical of Belgian and Netherlandic Dutch, due to either marked frequency or sense. McCarthy et al. (2007) consider the identification of predominant wordsenses in corpora, including differences between domains. However, this approach does not identify new senses as it relies on a pre-existing sense inventory. Carpuat et al. (2013) identify words in a domainspecific parallel corpus with novel translations. The method proposed by Lau et al. (2012), and extended by Cook et al. (2013), identifies novel wordsenses using a state-of-the-art word-sense induction (WSI) system. This token-based approach offers a natural account of polysemy and not only identifies word types that have a novel sense, but ide"
C14-1154,S13-2035,0,0.0486422,"Missing"
C14-1154,P11-2053,0,0.252002,"nge in an n-gram database, but using a model that is not restricted to any particular type of semantic change. Cook and Hirst (2011) consider the impact of sense frequency on methods for identifying novel senses. Crucially, all of the aforementioned approaches are type-based: they are able to identify words that have undergone a change in meaning, but not the token instances which give rise to these sense differences. Bamman and Crane (2011) use a parallel Latin–English corpus to induce word senses and build a WSD system, which they then apply to study diachronic variation in sense frequency. Rohrdantz et al. (2011) present a system for visualizing changes in word usage over time. Crucially, in these token-based approaches there is a clear connection between (induced) word-senses and tokens, making it possible to identify usages of a specific (new) sense. Other work has focused on sense differences between dialects and domains. Peirsman et al. (2010) consider the identification of words that are typical of Belgian and Netherlandic Dutch, due to either marked frequency or sense. McCarthy et al. (2007) consider the identification of predominant wordsenses in corpora, including differences between domains."
C14-1154,W09-0214,0,0.490803,"arger than has been used in research to date; (2) development and evaluation of a new baseline for novel sense detection, reformulations of the method of Lau et al., and a method that incorporates only the expected domain(s) of novel senses; (3) empirical evaluation of the method of Cook et al.; and (4) extension of the novel sense detection method of Cook et al. to automatically acquire information about the expected domain(s) of novel senses. 2 Related work Identifying diachronic changes in word-sense is a challenge that has only been considered rather recently in computational linguistics. Sagi et al. (2009) and Cook and Stevenson (2010) propose methods to identify specific types of semantic change — widening and narrowing, and amelioration and pejoration, respectively — based on specific properties of these phenomena. Gulordava and Baroni (2011) identify diachronic sense change in an n-gram database, but using a model that is not restricted to any particular type of semantic change. Cook and Hirst (2011) consider the impact of sense frequency on methods for identifying novel senses. Crucially, all of the aforementioned approaches are type-based: they are able to identify words that have undergon"
C14-1154,W11-1102,0,0.0956413,"Missing"
C16-1046,W02-2001,1,0.607985,"ining (but holding out the surprise language), we find that we are able to “smooth” annotation differences between languages. 2 Related Work There is a wealth of research on MWE identification (i.e. distinguishing MWEs from non-idiosyncratic combinations at the token level) and extraction (i.e. determining at the type level which word combinations in a corpus are MWEs). Many of these methods are customised to particular MWE constructions which are known to exist in a given corpus, e.g. noun compounds (Lapata and Lascarides, 2003; Tanaka and Baldwin, 2003), verb particle constructions (“VPCs”: Baldwin and Villavicencio (2002; Baldwin (2005)), determinerless prepositional phrases (Baldwin et al., 2004; van der Beek, 2005), or compound verbs (Breen and Baldwin, 2009). There is also a significant body of work on general-purpose MWE extraction, often based on statistical association measures applied to either a monolingual corpus (Evert and Krenn, 2005; Pecina, 2008; Ramisch, 2012) or a parallel corpus (Melamed, 1997; Moir´on and Tiedemann, 2006). Even here, however, POS-based constraints are generally applied on the types of MWE that are extracted (e.g. noun–noun or verb–noun bigrams). There are also methods for ide"
C16-1046,U09-1006,1,0.744627,"a wealth of research on MWE identification (i.e. distinguishing MWEs from non-idiosyncratic combinations at the token level) and extraction (i.e. determining at the type level which word combinations in a corpus are MWEs). Many of these methods are customised to particular MWE constructions which are known to exist in a given corpus, e.g. noun compounds (Lapata and Lascarides, 2003; Tanaka and Baldwin, 2003), verb particle constructions (“VPCs”: Baldwin and Villavicencio (2002; Baldwin (2005)), determinerless prepositional phrases (Baldwin et al., 2004; van der Beek, 2005), or compound verbs (Breen and Baldwin, 2009). There is also a significant body of work on general-purpose MWE extraction, often based on statistical association measures applied to either a monolingual corpus (Evert and Krenn, 2005; Pecina, 2008; Ramisch, 2012) or a parallel corpus (Melamed, 1997; Moir´on and Tiedemann, 2006). Even here, however, POS-based constraints are generally applied on the types of MWE that are extracted (e.g. noun–noun or verb–noun bigrams). There are also methods for identifying MWEs in context using supervised models (Diab and Bhutada, 2009; Li and Sporleder, 2010; Schneider et al., 2014), which require exhaus"
C16-1046,C14-1071,0,0.019567,"identifying MWEs in context using supervised models (Diab and Bhutada, 2009; Li and Sporleder, 2010; Schneider et al., 2014), which require exhaustive annotation of MWE token occurrences in a corpus. All of this research differs from our work in that it either assumes knowledge of the type(s) of MWE to extract for a given language, or requires explicitly annotated MWE data in that language. Closer to home, there has recently been work on general-purpose, unsupervised approaches to MWE extraction, making no assumptions about the types of MWE that exist in a given language (Newman et al., 2012; Brooke et al., 2014). Here, however, the definition of MWE tends to be blurred somewhat to focus on index terms or “formulaic language”, i.e. idiomatic expressions with statistically-marked properties in a given corpus — blurred in the sense that many MWEs are not statistically marked, and also that they include formulaic expressions such as in this paper that are not formally MWEs. Also related is recent work on resource development for low-resource languages, such as dependency parsing based on transfer learning from a higher-density language (Naseem et al., 2012; T¨ackstr¨om et al., 2013; Duong et al., 2015)."
C16-1046,D14-1082,0,0.00863418,"igher-density language (Naseem et al., 2012; T¨ackstr¨om et al., 2013; Duong et al., 2015). For example, Duong et al. (2015) proposed a neural network-based parser that transfers dependency relations across languages without requiring a parallel corpus. They learn syntactic cross-lingual word embeddings by training the skip-gram model (Mikolov et al., 2013) on a representation of the original text in which the context of each token is represented by its universal POS tags (Petrov et al., 2012). They then incorporate these word embeddings in a transition-based neural network dependency parser (Chen and Manning, 2014). Our proposed method is the first attempt to learn the MWE profile of a language with no knowledge of the target language except for POS tags (which themselves can be induced automatically, with little or no annotated data: Garrette and Baldridge (2013), Duong et al. (2014)), and no parallel corpus. We train a delexicalised dependency parser based on transfer learning (involving no syntactic annotations for the target language), and train a reranking model based on observed MWEs in only the source language(s). 472 id eu ga hi fa hu fi et hr fr es it no da nl sv de en he id eu fa fi hr bg fr e"
C16-1046,P12-1022,0,0.0203088,"ata, and not even a parallel corpus. Our proposed model is trained on a treebank with MWE relations of a source language, and can be applied to the monolingual corpus of the surprise language to identify its MWE construction types. 1 Introduction Multiword expressions (“MWEs”) are word combinations which have idiosyncratic properties relative to their component words (Sag et al., 2002; Baldwin and Kim, 2010), such as taken aback or red tape. The need for an explicit model of MWEs has been shown to be important in NLP tasks including machine translation (Venkatapathy and Joshi, 2006), parsing (Constant et al., 2012), and keyphrase/index term extraction (Newman et al., 2012). However, existing approaches to MWE identification/extraction typically target specific MWE types that are known to be prevalent in a given language, such as: (a) com´ S´eaghdha, 2008), German (Schulte im pound nouns in languages such as English (Copestake, 2003; O Walde et al., 2013) and Japanese (Tanaka and Baldwin, 2003); (b) light verb constructions (LVCs) in languages such as English (Butt, 2003), Persian (Karimi-Doostan, 1997) and Italian (Alba-Salas, 2002); and (c) compound verbs in languages such as Japanese (Uchiyama et al.,"
C16-1046,P11-1061,0,0.0194027,"ties across token instances. However, our model also predicts that hNOUN, ccomp, ADJi is an MWE. 6.2 Experiment II: Learning without gold standard dependency relations In our second experiment, we evaluate under the more realistic task setting of there being no gold standard treebank in the target language. Instead, we use the cross-lingual parser proposed by Duong et al. (2015) to parse the corpus in the target language (see Section 2). Note that we still use gold-standard POS tags, but this isn’t entirely unrealistic given the relative maturity of methods for inducing universal POS taggers (Das and Petrov, 2011; T¨ackstr¨om et al., 2013; Duong et al., 2014). Obviously, due to the fact that the parser has no access to dependency annotations in the target language, the parser output will be noisy. However, this emulates a true surprise language setup, where we 476 Language Baseline Reranker PMI Baseline + gold De 0.816 0.736 0.804 0.797 Sv 0.511 0.514 0.512 0.750 Da 0.481 0.428 0.567 0.846 It 0.637 0.631 0.803 0.885 Es 0.546 0.610 0.494 0.879 Fr 0.656 0.684 0.634 0.799 Hr 0.487 0.461 0.575 0.696 Bg 0.554 0.645 0.471 0.917 Hu 0.406 0.470 0.588 0.457 Fa 0.467 0.549 0.521 0.919 Ga 0.586 0.673 0.687 0.799"
C16-1046,W09-2903,0,0.0234383,"rases (Baldwin et al., 2004; van der Beek, 2005), or compound verbs (Breen and Baldwin, 2009). There is also a significant body of work on general-purpose MWE extraction, often based on statistical association measures applied to either a monolingual corpus (Evert and Krenn, 2005; Pecina, 2008; Ramisch, 2012) or a parallel corpus (Melamed, 1997; Moir´on and Tiedemann, 2006). Even here, however, POS-based constraints are generally applied on the types of MWE that are extracted (e.g. noun–noun or verb–noun bigrams). There are also methods for identifying MWEs in context using supervised models (Diab and Bhutada, 2009; Li and Sporleder, 2010; Schneider et al., 2014), which require exhaustive annotation of MWE token occurrences in a corpus. All of this research differs from our work in that it either assumes knowledge of the type(s) of MWE to extract for a given language, or requires explicitly annotated MWE data in that language. Closer to home, there has recently been work on general-purpose, unsupervised approaches to MWE extraction, making no assumptions about the types of MWE that exist in a given language (Newman et al., 2012; Brooke et al., 2014). Here, however, the definition of MWE tends to be blur"
C16-1046,D14-1096,1,0.933826,"s-lingual word embeddings by training the skip-gram model (Mikolov et al., 2013) on a representation of the original text in which the context of each token is represented by its universal POS tags (Petrov et al., 2012). They then incorporate these word embeddings in a transition-based neural network dependency parser (Chen and Manning, 2014). Our proposed method is the first attempt to learn the MWE profile of a language with no knowledge of the target language except for POS tags (which themselves can be induced automatically, with little or no annotated data: Garrette and Baldridge (2013), Duong et al. (2014)), and no parallel corpus. We train a delexicalised dependency parser based on transfer learning (involving no syntactic annotations for the target language), and train a reranking model based on observed MWEs in only the source language(s). 472 id eu ga hi fa hu fi et hr fr es it no da nl sv de en he id eu fa fi hr bg fr es it da nl sv de en 0.64 0.56 0.48 0.40 0.32 0.24 0.16 0.00 0.56 0.48 0.40 0.32 0.24 0.16 0.08 en de sv nl da it es fr bg hr fi fa eu id he en de sv nl da no it es fr hr et fi hu fa hi ga eu id 0.08 0.64 (a) compound 0.00 (b) mwe Figure 1: Cross-lingual similarity of MWE pat"
C16-1046,K15-1012,1,0.869192,"e able to inventorise the MWE types in the language (Oard, 2003; Maynard et al., 2003)? Here, there is little expectation of success without an automatic method for determining the inventory and relative frequency of MWEs in a given language. This provides the motivation for this paper: can we develop a method for automatically profiling the MWE inventory of a novel language based simply on a monolingual corpus of that language, and a treebank in a second language such as English? We carry out this research in the Universal Dependency (“UD”) framework (Nivre et al., 2016), using the method of Duong et al. (2015) to induce a delexicalised dependency parser for the surprise language, based on a supervised parsing model for a language such as English where we have a well-developed treebank in the UD. Given the parser output over a monolingual corpus in the surprise language, we then apply one of two methods to extract our MWE profile: (1) a baseline method, where we simply 471 Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 471–481, Osaka, Japan, December 11-17 2016. extract out delexicalised dependency tuples of relation type mwe or co"
C16-1046,N13-1014,0,0.0207183,"rpus. They learn syntactic cross-lingual word embeddings by training the skip-gram model (Mikolov et al., 2013) on a representation of the original text in which the context of each token is represented by its universal POS tags (Petrov et al., 2012). They then incorporate these word embeddings in a transition-based neural network dependency parser (Chen and Manning, 2014). Our proposed method is the first attempt to learn the MWE profile of a language with no knowledge of the target language except for POS tags (which themselves can be induced automatically, with little or no annotated data: Garrette and Baldridge (2013), Duong et al. (2014)), and no parallel corpus. We train a delexicalised dependency parser based on transfer learning (involving no syntactic annotations for the target language), and train a reranking model based on observed MWEs in only the source language(s). 472 id eu ga hi fa hu fi et hr fr es it no da nl sv de en he id eu fa fi hr bg fr es it da nl sv de en 0.64 0.56 0.48 0.40 0.32 0.24 0.16 0.00 0.56 0.48 0.40 0.32 0.24 0.16 0.08 en de sv nl da it es fr bg hr fi fa eu id he en de sv nl da no it es fr hr et fi hu fa hi ga eu id 0.08 0.64 (a) compound 0.00 (b) mwe Figure 1: Cross-lingual"
C16-1046,W03-1502,0,0.114981,"Missing"
C16-1046,E03-1073,0,0.065874,"g the supervised reranking method, however, and incorporating more and more languages for training (but holding out the surprise language), we find that we are able to “smooth” annotation differences between languages. 2 Related Work There is a wealth of research on MWE identification (i.e. distinguishing MWEs from non-idiosyncratic combinations at the token level) and extraction (i.e. determining at the type level which word combinations in a corpus are MWEs). Many of these methods are customised to particular MWE constructions which are known to exist in a given corpus, e.g. noun compounds (Lapata and Lascarides, 2003; Tanaka and Baldwin, 2003), verb particle constructions (“VPCs”: Baldwin and Villavicencio (2002; Baldwin (2005)), determinerless prepositional phrases (Baldwin et al., 2004; van der Beek, 2005), or compound verbs (Breen and Baldwin, 2009). There is also a significant body of work on general-purpose MWE extraction, often based on statistical association measures applied to either a monolingual corpus (Evert and Krenn, 2005; Pecina, 2008; Ramisch, 2012) or a parallel corpus (Melamed, 1997; Moir´on and Tiedemann, 2006). Even here, however, POS-based constraints are generally applied on the type"
C16-1046,C10-2078,0,0.0279912,"004; van der Beek, 2005), or compound verbs (Breen and Baldwin, 2009). There is also a significant body of work on general-purpose MWE extraction, often based on statistical association measures applied to either a monolingual corpus (Evert and Krenn, 2005; Pecina, 2008; Ramisch, 2012) or a parallel corpus (Melamed, 1997; Moir´on and Tiedemann, 2006). Even here, however, POS-based constraints are generally applied on the types of MWE that are extracted (e.g. noun–noun or verb–noun bigrams). There are also methods for identifying MWEs in context using supervised models (Diab and Bhutada, 2009; Li and Sporleder, 2010; Schneider et al., 2014), which require exhaustive annotation of MWE token occurrences in a corpus. All of this research differs from our work in that it either assumes knowledge of the type(s) of MWE to extract for a given language, or requires explicitly annotated MWE data in that language. Closer to home, there has recently been work on general-purpose, unsupervised approaches to MWE extraction, making no assumptions about the types of MWE that exist in a given language (Newman et al., 2012; Brooke et al., 2014). Here, however, the definition of MWE tends to be blurred somewhat to focus on"
C16-1046,W97-0311,0,0.272816,"articular MWE constructions which are known to exist in a given corpus, e.g. noun compounds (Lapata and Lascarides, 2003; Tanaka and Baldwin, 2003), verb particle constructions (“VPCs”: Baldwin and Villavicencio (2002; Baldwin (2005)), determinerless prepositional phrases (Baldwin et al., 2004; van der Beek, 2005), or compound verbs (Breen and Baldwin, 2009). There is also a significant body of work on general-purpose MWE extraction, often based on statistical association measures applied to either a monolingual corpus (Evert and Krenn, 2005; Pecina, 2008; Ramisch, 2012) or a parallel corpus (Melamed, 1997; Moir´on and Tiedemann, 2006). Even here, however, POS-based constraints are generally applied on the types of MWE that are extracted (e.g. noun–noun or verb–noun bigrams). There are also methods for identifying MWEs in context using supervised models (Diab and Bhutada, 2009; Li and Sporleder, 2010; Schneider et al., 2014), which require exhaustive annotation of MWE token occurrences in a corpus. All of this research differs from our work in that it either assumes knowledge of the type(s) of MWE to extract for a given language, or requires explicitly annotated MWE data in that language. Close"
C16-1046,W06-2405,0,0.0769814,"Missing"
C16-1046,P12-1066,0,0.0214297,"exist in a given language (Newman et al., 2012; Brooke et al., 2014). Here, however, the definition of MWE tends to be blurred somewhat to focus on index terms or “formulaic language”, i.e. idiomatic expressions with statistically-marked properties in a given corpus — blurred in the sense that many MWEs are not statistically marked, and also that they include formulaic expressions such as in this paper that are not formally MWEs. Also related is recent work on resource development for low-resource languages, such as dependency parsing based on transfer learning from a higher-density language (Naseem et al., 2012; T¨ackstr¨om et al., 2013; Duong et al., 2015). For example, Duong et al. (2015) proposed a neural network-based parser that transfers dependency relations across languages without requiring a parallel corpus. They learn syntactic cross-lingual word embeddings by training the skip-gram model (Mikolov et al., 2013) on a representation of the original text in which the context of each token is represented by its universal POS tags (Petrov et al., 2012). They then incorporate these word embeddings in a transition-based neural network dependency parser (Chen and Manning, 2014). Our proposed metho"
C16-1046,C12-1127,1,0.805624,"ained on a treebank with MWE relations of a source language, and can be applied to the monolingual corpus of the surprise language to identify its MWE construction types. 1 Introduction Multiword expressions (“MWEs”) are word combinations which have idiosyncratic properties relative to their component words (Sag et al., 2002; Baldwin and Kim, 2010), such as taken aback or red tape. The need for an explicit model of MWEs has been shown to be important in NLP tasks including machine translation (Venkatapathy and Joshi, 2006), parsing (Constant et al., 2012), and keyphrase/index term extraction (Newman et al., 2012). However, existing approaches to MWE identification/extraction typically target specific MWE types that are known to be prevalent in a given language, such as: (a) com´ S´eaghdha, 2008), German (Schulte im pound nouns in languages such as English (Copestake, 2003; O Walde et al., 2013) and Japanese (Tanaka and Baldwin, 2003); (b) light verb constructions (LVCs) in languages such as English (Butt, 2003), Persian (Karimi-Doostan, 1997) and Italian (Alba-Salas, 2002); and (c) compound verbs in languages such as Japanese (Uchiyama et al., 2005). Note here that the combination of highly-productive"
C16-1046,petrov-etal-2012-universal,0,0.313646,"nt work on resource development for low-resource languages, such as dependency parsing based on transfer learning from a higher-density language (Naseem et al., 2012; T¨ackstr¨om et al., 2013; Duong et al., 2015). For example, Duong et al. (2015) proposed a neural network-based parser that transfers dependency relations across languages without requiring a parallel corpus. They learn syntactic cross-lingual word embeddings by training the skip-gram model (Mikolov et al., 2013) on a representation of the original text in which the context of each token is represented by its universal POS tags (Petrov et al., 2012). They then incorporate these word embeddings in a transition-based neural network dependency parser (Chen and Manning, 2014). Our proposed method is the first attempt to learn the MWE profile of a language with no knowledge of the target language except for POS tags (which themselves can be induced automatically, with little or no annotated data: Garrette and Baldridge (2013), Duong et al. (2014)), and no parallel corpus. We train a delexicalised dependency parser based on transfer learning (involving no syntactic annotations for the target language), and train a reranking model based on obse"
C16-1046,W12-3311,0,0.0251613,"of these methods are customised to particular MWE constructions which are known to exist in a given corpus, e.g. noun compounds (Lapata and Lascarides, 2003; Tanaka and Baldwin, 2003), verb particle constructions (“VPCs”: Baldwin and Villavicencio (2002; Baldwin (2005)), determinerless prepositional phrases (Baldwin et al., 2004; van der Beek, 2005), or compound verbs (Breen and Baldwin, 2009). There is also a significant body of work on general-purpose MWE extraction, often based on statistical association measures applied to either a monolingual corpus (Evert and Krenn, 2005; Pecina, 2008; Ramisch, 2012) or a parallel corpus (Melamed, 1997; Moir´on and Tiedemann, 2006). Even here, however, POS-based constraints are generally applied on the types of MWE that are extracted (e.g. noun–noun or verb–noun bigrams). There are also methods for identifying MWEs in context using supervised models (Diab and Bhutada, 2009; Li and Sporleder, 2010; Schneider et al., 2014), which require exhaustive annotation of MWE token occurrences in a corpus. All of this research differs from our work in that it either assumes knowledge of the type(s) of MWE to extract for a given language, or requires explicitly annota"
C16-1046,Q14-1016,0,0.0125712,", or compound verbs (Breen and Baldwin, 2009). There is also a significant body of work on general-purpose MWE extraction, often based on statistical association measures applied to either a monolingual corpus (Evert and Krenn, 2005; Pecina, 2008; Ramisch, 2012) or a parallel corpus (Melamed, 1997; Moir´on and Tiedemann, 2006). Even here, however, POS-based constraints are generally applied on the types of MWE that are extracted (e.g. noun–noun or verb–noun bigrams). There are also methods for identifying MWEs in context using supervised models (Diab and Bhutada, 2009; Li and Sporleder, 2010; Schneider et al., 2014), which require exhaustive annotation of MWE token occurrences in a corpus. All of this research differs from our work in that it either assumes knowledge of the type(s) of MWE to extract for a given language, or requires explicitly annotated MWE data in that language. Closer to home, there has recently been work on general-purpose, unsupervised approaches to MWE extraction, making no assumptions about the types of MWE that exist in a given language (Newman et al., 2012; Brooke et al., 2014). Here, however, the definition of MWE tends to be blurred somewhat to focus on index terms or “formulai"
C16-1046,S13-1038,0,0.0601992,"Missing"
C16-1046,W04-2118,0,0.0183763,"e proportion of MWE tokens 3 We discarded Hindi despite the high proportion of MWEs because: (1) it only covered compound relations, and has no mwe relations, and (2) it has a low number of distinct MWE patterns (23), and as such appeared skewed in its annotation. 2 473 are: NOUN–NOUN; PROPN–PROPN (i.e. proper noun dependencies, which should be annotated with the name relation rather than compound, according to the annotation guidelines); and VERB– NOUN, which includes LVCs. There are also other noticeable patterns such as VERB–ADV(erb) and VERB–ADP(osition), corresponding to VPCs (Schulte im Walde, 2004; Baldwin, 2005). mwe patterns are more diverse than compound patterns: compound patterns mostly involve nouns and verbs, while mwe patterns involve a diverse range of POS types, such as ADP–ADP or ADV–ADV, and pairings including CONJ(unctions) or SCONJ (subordinating conjunctions). We additionally measured the similarity between the MWE pattern probability distribution of the different languages using Jensen–Shannon divergence, as shown in Figure 1 for all languages in UD. To make comparison between related languages easier, we clustered the languages by language family. According to Figure 1"
C16-1046,N13-1126,0,0.0328538,"Missing"
C16-1046,W03-1803,1,0.611439,"2002; Baldwin and Kim, 2010), such as taken aback or red tape. The need for an explicit model of MWEs has been shown to be important in NLP tasks including machine translation (Venkatapathy and Joshi, 2006), parsing (Constant et al., 2012), and keyphrase/index term extraction (Newman et al., 2012). However, existing approaches to MWE identification/extraction typically target specific MWE types that are known to be prevalent in a given language, such as: (a) com´ S´eaghdha, 2008), German (Schulte im pound nouns in languages such as English (Copestake, 2003; O Walde et al., 2013) and Japanese (Tanaka and Baldwin, 2003); (b) light verb constructions (LVCs) in languages such as English (Butt, 2003), Persian (Karimi-Doostan, 1997) and Italian (Alba-Salas, 2002); and (c) compound verbs in languages such as Japanese (Uchiyama et al., 2005). Note here that the combination of highly-productive MWE types can vary greatly across languages: English is rich with compound nouns and LVCs are also common, but lacks compound verbs; Persian is rich with LVCs and adjective–noun compounds, but has very few compound nouns and compound verbs; and Japanese is rich with LVCs and compound nouns and verbs, but adjective–noun MWEs"
C16-1046,W06-1204,0,0.0390327,"E patterns, certainly no annotated MWE data, and not even a parallel corpus. Our proposed model is trained on a treebank with MWE relations of a source language, and can be applied to the monolingual corpus of the surprise language to identify its MWE construction types. 1 Introduction Multiword expressions (“MWEs”) are word combinations which have idiosyncratic properties relative to their component words (Sag et al., 2002; Baldwin and Kim, 2010), such as taken aback or red tape. The need for an explicit model of MWEs has been shown to be important in NLP tasks including machine translation (Venkatapathy and Joshi, 2006), parsing (Constant et al., 2012), and keyphrase/index term extraction (Newman et al., 2012). However, existing approaches to MWE identification/extraction typically target specific MWE types that are known to be prevalent in a given language, such as: (a) com´ S´eaghdha, 2008), German (Schulte im pound nouns in languages such as English (Copestake, 2003; O Walde et al., 2013) and Japanese (Tanaka and Baldwin, 2003); (b) light verb constructions (LVCs) in languages such as English (Butt, 2003), Persian (Karimi-Doostan, 1997) and Italian (Alba-Salas, 2002); and (c) compound verbs in languages s"
C16-1046,L16-1262,0,\N,Missing
cook-stevenson-2010-automatically,W09-0214,0,\N,Missing
cook-stevenson-2010-automatically,J98-1004,0,\N,Missing
cook-stevenson-2010-automatically,D09-1062,0,\N,Missing
cook-stevenson-2010-automatically,D09-1063,0,\N,Missing
cook-stevenson-2010-automatically,P97-1023,0,\N,Missing
cook-stevenson-2010-automatically,esuli-sebastiani-2006-sentiwordnet,0,\N,Missing
D12-1039,P06-2005,0,0.594423,"andwidth. Here we exploit microblog data directly to derive (lexical variant, standard form) pairs, instead of relying on external resources. In morerecent work, Liu et al. (2012) endeavour to improve the accuracy of top-n normalisation candidates by integrating human cognitive inference, characterlevel transformations and spell checking in their normalisation model. The encouraging results shift the focus to reranking and promoting the correct normalisation to the top-1 position. However, like much previous work on lexical normalisation, this work 3 assumes perfect lexical variant detection. Aw et al. (2006) and Kaufmann and Kalita (2010) consider normalisation as a machine translation task from lexical variants to standard forms using off-theshelf tools. These methods do not assume that lexical variants have been pre-identified; however, these methods do rely on large quantities of labelled training data, which is not available for microblogs. Recently, Han and Baldwin (2011) and Gouws et al. (2011) propose two-step unsupervised approaches to normalisation, in which lexical variants are first identified, and then normalised. They approach lexical variant detection by using a context fitness clas"
D12-1039,P11-1040,0,0.00874909,"h other methods, this approach offers a fast, lightweight and easy-to-use solution, and is thus suitable for high-volume microblog pre-processing. 1 Lexical Normalisation A staggering number of short text “microblog” messages are produced every day through social media such as Twitter (Twitter, 2011). The immense volume of real-time, user-generated microblogs that flows through sites has been shown to have utility in applications such as disaster detection (Sakaki et al., 2010), sentiment analysis (Jiang et al., 2011; Gonz´alez-Ib´an˜ ez et al., 2011), and event discovery (Weng and Lee, 2011; Benson et al., 2011). However, due to the spontaneous nature of the posts, microblogs are notoriously noisy, containing many non-standard forms — e.g., tmrw “tomorrow” and 2day “today” — which degrade the performance of In this paper, we focus on the task of lexical normalisation of English Twitter messages, in which out-of-vocabulary (OOV) tokens are normalised to their in-vocabulary (IV) standard form, i.e., a standard form that is in a dictionary. Following other recent work on lexical normalisation (Liu et al., 2011a; Han and Baldwin, 2011; Gouws et al., 2011; Liu et al., 2012), we specifically focus on one-t"
D12-1039,P00-1037,0,0.0191066,"in lexical normalisation, t is assumed to be an 422 OOV token, relative to a fixed dictionary. In practice, not all OOV tokens should be normalised; i.e., only lexical variants (e.g., tmrw “tomorrow”) should be normalised and tokens that are OOV but otherwise not lexical variants (e.g., iPad “iPad”) should be unchanged. Most work in this area focuses only on the normalisation task itself, oftentimes assuming that the task of lexical variant detection has already been completed. Various approaches have been proposed to estimate the error model, P (t|s). For example, in work on spell-checking, Brill and Moore (2000) improve on a standard edit-distance approach by considering multi-character edit operations; Toutanova and Moore (2002) build on this by incorporating phonological information. Li et al. (2006) utilise distributional similarity (Lin, 1998) to correct misspelled search queries. In text message normalisation, Choudhury et al. (2007) model the letter transformations and emissions using a hidden Markov model (Rabiner, 1989). Cook and Stevenson (2009) and Xue et al. (2011) propose multiple simple error models, each of which captures a particular way in which lexical variants are formed, such as ph"
D12-1039,C10-2022,0,0.146671,"Twitter corpus, and the IV unigram frequencies in the Google Web 1T corpus (Brants and Franz, 2006) to get less-noisy frequency estimates. We also compared a variety of re-rankings based on a number of string similarity measures that have been previously considered in normalisation work (reviewed in Section 2). We experiment with standard edit distance (Levenshtein, 1966), edit distance over double metaphone codes (phonetic edit distance: (Philips, 2000)), longest common subsequence ratio over the consonant edit distance of the paired words (hereafter, denoted as 426 consonant edit distance: (Contractor et al., 2010)), and a string subsequence kernel (Lodhi et al., 2002). In Figure 1, we present the DCG@N results for each of our ranking methods at different rank cutoffs. Ranking by OOV frequency is motivated by the assumption that lexical variants are frequently used by social media users. This is confirmed by our findings that lexical pairs like (goin, going) and (nite, night) are at the top of the ranking. However, many proper nouns and named entities are also used frequently and ranked at the top, mixed with lexical variants like (Facebook, speech) and (Youtube, web). In ranking by IV word frequency, w"
D12-1039,W09-2010,1,0.797376,"ntities, e.g., are prevalent in microblogs, but not all named entities are included in our dictionary. One challenge for lexical normalisation is therefore to dis421 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural c Language Learning, pages 421–432, Jeju Island, Korea, 12–14 July 2012. 2012 Association for Computational Linguistics tinguish those OOV tokens that require normalisation from those that are well-formed. Recent unsupervised approaches have not attempted to distinguish such tokens from other types of OOV tokens (Cook and Stevenson, 2009; Liu et al., 2011a), limiting their applicability to real-world normalisation tasks. Other approaches (Han and Baldwin, 2011; Gouws et al., 2011) have followed a cascaded approach in which lexical variants are first identified, and then normalised. However, such two-step approaches suffer from poor lexical variant identification performance, which is propagated to the normalisation step. Motivated by the observation that most lexical variants have an unambiguous standard form (especially for longer tokens), and that a lexical variant and its standard form typically occur in similar contexts,"
D12-1039,P11-2008,0,0.0600356,"Missing"
D12-1039,P11-2102,0,0.00976165,"ore and word error rate on a standard dataset. Compared with other methods, this approach offers a fast, lightweight and easy-to-use solution, and is thus suitable for high-volume microblog pre-processing. 1 Lexical Normalisation A staggering number of short text “microblog” messages are produced every day through social media such as Twitter (Twitter, 2011). The immense volume of real-time, user-generated microblogs that flows through sites has been shown to have utility in applications such as disaster detection (Sakaki et al., 2010), sentiment analysis (Jiang et al., 2011; Gonz´alez-Ib´an˜ ez et al., 2011), and event discovery (Weng and Lee, 2011; Benson et al., 2011). However, due to the spontaneous nature of the posts, microblogs are notoriously noisy, containing many non-standard forms — e.g., tmrw “tomorrow” and 2day “today” — which degrade the performance of In this paper, we focus on the task of lexical normalisation of English Twitter messages, in which out-of-vocabulary (OOV) tokens are normalised to their in-vocabulary (IV) standard form, i.e., a standard form that is in a dictionary. Following other recent work on lexical normalisation (Liu et al., 2011a; Han and Baldwin, 2011; Gouws"
D12-1039,W11-2210,0,0.196949,"toria Research Laboratory ♥ Department of Computing and Information Systems, The University of Melbourne hanb@student.unimelb.edu.au, paulcook@unimelb.edu.au, tb@ldwin.net Abstract natural language processing (NLP) tools (Ritter et al., 2010; Han and Baldwin, 2011). To reduce this effect, attempts have been made to adapt NLP tools to microblog data (Gimpel et al., 2011; Foster et al., 2011; Liu et al., 2011b; Ritter et al., 2011). An alternative approach is to pre-normalise non-standard lexical variants to their standard orthography (Liu et al., 2011a; Han and Baldwin, 2011; Xue et al., 2011; Gouws et al., 2011). For example, se u 2morw!!! would be normalised to see you tomorrow! The normalisation approach is especially attractive as a preprocessing step for applications which rely on keyword match or word frequency statistics. For example, earthqu, eathquake, and earthquakeee — all attested in a Twitter corpus — have the standard form earthquake; by normalising these types to their standard form, better coverage can be achieved for keyword-based methods, and better word frequency estimates can be obtained. Microblog normalisation methods often utilise complex models and struggle to differentiate bet"
D12-1039,P11-1038,1,0.0891507,"Cook,♥ and Timothy Baldwin♠♥ ♠ NICTA Victoria Research Laboratory ♥ Department of Computing and Information Systems, The University of Melbourne hanb@student.unimelb.edu.au, paulcook@unimelb.edu.au, tb@ldwin.net Abstract natural language processing (NLP) tools (Ritter et al., 2010; Han and Baldwin, 2011). To reduce this effect, attempts have been made to adapt NLP tools to microblog data (Gimpel et al., 2011; Foster et al., 2011; Liu et al., 2011b; Ritter et al., 2011). An alternative approach is to pre-normalise non-standard lexical variants to their standard orthography (Liu et al., 2011a; Han and Baldwin, 2011; Xue et al., 2011; Gouws et al., 2011). For example, se u 2morw!!! would be normalised to see you tomorrow! The normalisation approach is especially attractive as a preprocessing step for applications which rely on keyword match or word frequency statistics. For example, earthqu, eathquake, and earthquakeee — all attested in a Twitter corpus — have the standard form earthquake; by normalising these types to their standard form, better coverage can be achieved for keyword-based methods, and better word frequency estimates can be obtained. Microblog normalisation methods often utilise complex m"
D12-1039,P11-1016,0,0.0175407,"-of-the-art performance for both F-score and word error rate on a standard dataset. Compared with other methods, this approach offers a fast, lightweight and easy-to-use solution, and is thus suitable for high-volume microblog pre-processing. 1 Lexical Normalisation A staggering number of short text “microblog” messages are produced every day through social media such as Twitter (Twitter, 2011). The immense volume of real-time, user-generated microblogs that flows through sites has been shown to have utility in applications such as disaster detection (Sakaki et al., 2010), sentiment analysis (Jiang et al., 2011; Gonz´alez-Ib´an˜ ez et al., 2011), and event discovery (Weng and Lee, 2011; Benson et al., 2011). However, due to the spontaneous nature of the posts, microblogs are notoriously noisy, containing many non-standard forms — e.g., tmrw “tomorrow” and 2day “today” — which degrade the performance of In this paper, we focus on the task of lexical normalisation of English Twitter messages, in which out-of-vocabulary (OOV) tokens are normalised to their in-vocabulary (IV) standard form, i.e., a standard form that is in a dictionary. Following other recent work on lexical normalisation (Liu et al., 2"
D12-1039,P06-1129,0,0.0608485,"Missing"
D12-1039,P98-2127,0,0.0455843,"therwise not lexical variants (e.g., iPad “iPad”) should be unchanged. Most work in this area focuses only on the normalisation task itself, oftentimes assuming that the task of lexical variant detection has already been completed. Various approaches have been proposed to estimate the error model, P (t|s). For example, in work on spell-checking, Brill and Moore (2000) improve on a standard edit-distance approach by considering multi-character edit operations; Toutanova and Moore (2002) build on this by incorporating phonological information. Li et al. (2006) utilise distributional similarity (Lin, 1998) to correct misspelled search queries. In text message normalisation, Choudhury et al. (2007) model the letter transformations and emissions using a hidden Markov model (Rabiner, 1989). Cook and Stevenson (2009) and Xue et al. (2011) propose multiple simple error models, each of which captures a particular way in which lexical variants are formed, such as phonetic spelling (e.g., epik “epic”) or clipping (e.g., walkin “walking”). Nevertheless, optimally weighting the various error models in these approaches is challenging. Without pre-categorising lexical variants into different types, Liu et"
D12-1039,P11-2013,0,0.380884,"logs Bo Han,♠♥ Paul Cook,♥ and Timothy Baldwin♠♥ ♠ NICTA Victoria Research Laboratory ♥ Department of Computing and Information Systems, The University of Melbourne hanb@student.unimelb.edu.au, paulcook@unimelb.edu.au, tb@ldwin.net Abstract natural language processing (NLP) tools (Ritter et al., 2010; Han and Baldwin, 2011). To reduce this effect, attempts have been made to adapt NLP tools to microblog data (Gimpel et al., 2011; Foster et al., 2011; Liu et al., 2011b; Ritter et al., 2011). An alternative approach is to pre-normalise non-standard lexical variants to their standard orthography (Liu et al., 2011a; Han and Baldwin, 2011; Xue et al., 2011; Gouws et al., 2011). For example, se u 2morw!!! would be normalised to see you tomorrow! The normalisation approach is especially attractive as a preprocessing step for applications which rely on keyword match or word frequency statistics. For example, earthqu, eathquake, and earthquakeee — all attested in a Twitter corpus — have the standard form earthquake; by normalising these types to their standard form, better coverage can be achieved for keyword-based methods, and better word frequency estimates can be obtained. Microblog normalisation methods"
D12-1039,P11-1037,0,0.607442,"logs Bo Han,♠♥ Paul Cook,♥ and Timothy Baldwin♠♥ ♠ NICTA Victoria Research Laboratory ♥ Department of Computing and Information Systems, The University of Melbourne hanb@student.unimelb.edu.au, paulcook@unimelb.edu.au, tb@ldwin.net Abstract natural language processing (NLP) tools (Ritter et al., 2010; Han and Baldwin, 2011). To reduce this effect, attempts have been made to adapt NLP tools to microblog data (Gimpel et al., 2011; Foster et al., 2011; Liu et al., 2011b; Ritter et al., 2011). An alternative approach is to pre-normalise non-standard lexical variants to their standard orthography (Liu et al., 2011a; Han and Baldwin, 2011; Xue et al., 2011; Gouws et al., 2011). For example, se u 2morw!!! would be normalised to see you tomorrow! The normalisation approach is especially attractive as a preprocessing step for applications which rely on keyword match or word frequency statistics. For example, earthqu, eathquake, and earthquakeee — all attested in a Twitter corpus — have the standard form earthquake; by normalising these types to their standard form, better coverage can be achieved for keyword-based methods, and better word frequency estimates can be obtained. Microblog normalisation methods"
D12-1039,P12-1109,0,0.520743,"scovery (Weng and Lee, 2011; Benson et al., 2011). However, due to the spontaneous nature of the posts, microblogs are notoriously noisy, containing many non-standard forms — e.g., tmrw “tomorrow” and 2day “today” — which degrade the performance of In this paper, we focus on the task of lexical normalisation of English Twitter messages, in which out-of-vocabulary (OOV) tokens are normalised to their in-vocabulary (IV) standard form, i.e., a standard form that is in a dictionary. Following other recent work on lexical normalisation (Liu et al., 2011a; Han and Baldwin, 2011; Gouws et al., 2011; Liu et al., 2012), we specifically focus on one-to-one normalisation in which one OOV token is normalised to one IV word. Naturally, not all OOV words in microblogs are lexical variants of IV words: named entities, e.g., are prevalent in microblogs, but not all named entities are included in our dictionary. One challenge for lexical normalisation is therefore to dis421 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural c Language Learning, pages 421–432, Jeju Island, Korea, 12–14 July 2012. 2012 Association for Computational Linguistics tingui"
D12-1039,I11-1062,1,0.735848,"ax. We also experiment with a number of simple but widely-used geometric and information theoretic distance/similarity measures. In particular, we use Kullback–Leibler (KL) divergence (Kullback and Leibler, 1951), Jensen–Shannon (JS) divergence (Lin, 1991), Euclidean distance and Cosine distance. We use a corpus of 10 million English tweets to do parameter tuning over, and a larger corpus of tweets in the final candidate ranking. All tweets were collected from September 2010 to January 2011 via the Twitter API.1 From the raw data we extract English tweets using a language identification tool (Lui and Baldwin, 2011), and then apply a simplified Twitter tokeniser (adapted from O’Connor et al. (2010)). We use the Aspell dictionary (v6.06)2 to determine whether a word is IV, and only include in our normalisation dictionary OOV tokens with at least 64 occurrences in the corpus and character length ≥ 4, both of which were determined through empirical observation. For each OOV word type in the corpus, we select the most similar IV type to form (OOV, IV) pairs. To further narrow the search space, we only consider IV words which are morphophonemically similar to the OOV type, following settings in Han and Baldwi"
D12-1039,N10-1020,0,0.0428163,"Missing"
D12-1039,D11-1141,0,0.0566316,"f representing context and different similarity measures we can use, which may influence the quality of generated normalisation pairs. In representing the context, we experimentally explore the following factors: (1) context window size (from 1 to 3 tokens on both sides); (2) n-gram order of the context tokens (unigram, bigram, trigram); (3) whether context words are indexed for relative position or not; and (4) whether we use all context tokens, or only IV words. Because high-accuracy linguistic processing tools for Twitter are still under exploration (Liu et al., 2011b; Gimpel et al., 2011; Ritter et al., 2011; Foster et al., 2011), we do not consider richer representations of context, for example, incorporating information about part-of-speech tags or syntax. We also experiment with a number of simple but widely-used geometric and information theoretic distance/similarity measures. In particular, we use Kullback–Leibler (KL) divergence (Kullback and Leibler, 1951), Jensen–Shannon (JS) divergence (Lin, 1991), Euclidean distance and Cosine distance. We use a corpus of 10 million English tweets to do parameter tuning over, and a larger corpus of tweets in the final candidate ranking. All tweets were"
D12-1039,P02-1019,0,0.0120776,"OOV tokens should be normalised; i.e., only lexical variants (e.g., tmrw “tomorrow”) should be normalised and tokens that are OOV but otherwise not lexical variants (e.g., iPad “iPad”) should be unchanged. Most work in this area focuses only on the normalisation task itself, oftentimes assuming that the task of lexical variant detection has already been completed. Various approaches have been proposed to estimate the error model, P (t|s). For example, in work on spell-checking, Brill and Moore (2000) improve on a standard edit-distance approach by considering multi-character edit operations; Toutanova and Moore (2002) build on this by incorporating phonological information. Li et al. (2006) utilise distributional similarity (Lin, 1998) to correct misspelled search queries. In text message normalisation, Choudhury et al. (2007) model the letter transformations and emissions using a hidden Markov model (Rabiner, 1989). Cook and Stevenson (2009) and Xue et al. (2011) propose multiple simple error models, each of which captures a particular way in which lexical variants are formed, such as phonetic spelling (e.g., epik “epic”) or clipping (e.g., walkin “walking”). Nevertheless, optimally weighting the various"
D12-1039,C00-2137,0,0.0417631,"onaries with the normalisation method of Gouws et al. (2011) (GHM-norm) and the combined unsupervised approach of Han and Baldwin (2011) (HB-norm). stantially over HB-dict and GHM-dict, respectively, indicating that S-dict contains markedly different entries to both HB-dict and GHM-dict. The best Fscore and WER are obtained using the combination of all three dictionaries, HB-dict+GHM-dict+S-dict. Furthermore, the difference between the results using HB-dict+GHM-dict+S-dict and HB-dict+GHMdict is statistically significant (p &lt; 0.01), based on the computationally-intensive Monte Carlo method of Yeh (2000), demonstrating the contribution of Sdict. 6.2.3 Hybrid Approaches The methods of Gouws et al. (2011) (i.e. GHM-dict+GHM-norm) and Han and Baldwin (2011) (i.e. HB-dict+HB-norm) have lower precision and higher false alarm rates than the dictionarybased approaches; this is largely caused by lexical variant detection errors.8 Using all dictionaries in combination with these methods — HB-dict+GHM-dict+S-dict+GHM-norm and HBdict+GHM-dict+S-dict+HB-norm — gives some improvements, but the false alarm rates remain high. Despite the limitations of a pure dictionary-based approach to normalisation — dis"
D12-1039,C98-2122,0,\N,Missing
D14-1096,N10-1083,0,0.0759904,"Missing"
D14-1096,J96-1002,0,0.0233732,"e DPM model while correcting for its inadequacies using direct supervision. We select only 1,000 annotated tokens to reflect a low resource scenario. A small supervised training sample is a more realistic form of supervision than a tag dictionary (noisy or otherwise). Although used in most prior work, a tag dictionary for a new language requires significant manual effort to construct. Garrette and Baldridge (2013) showed that a 1,000 token dataset could be collected very cheaply, requiring less than 2 hours of non-expert time. Our correction model makes use of a minimum divergence (MD) model (Berger et al., 1996), a variant of the maximum entropy model which biases the target distribution to be similar to a static reference distribution. The method has been used in several language applications including machine translation (Foster, 2000) and parsing (Plank and van Noord, 2008, Johnson and Riezler, 2000). These previous approaches have used various sources of reference distribution, e.g., incorporating information from a simpler model (Johnson and Riezler, 2000) or combining in- and outof-domain models (Plank and van Noord, 2008). Plank and van Noord (2008) concluded that this method for adding prior"
D14-1096,A00-1031,0,0.101349,"Missing"
D14-1096,W06-2920,0,0.500674,"than using a dictionary. We argue 1 http://www.wiktionary.org/ Das and Petrov (2011) Duong et al. (2013b) Li et al. (2012) T¨ackstr¨om et al. (2013) da 83.2 85.6 83.3 88.2 nl 79.5 84.0 86.3 85.9 de 82.8 85.4 85.4 90.5 el 82.5 80.4 79.2 89.5 it 86.8 81.4 86.5 89.3 pt 87.9 86.3 84.5 91.0 es 84.2 83.3 86.4 87.1 sv 80.5 81.0 86.1 88.9 Average 83.4 83.4 84.8 88.8 Table 1: Previously published token-level POS tagging accuracy for various models across 8 languages — Danish (da), Dutch (nl), German (de), Greek (el), Italian (it), Portuguese (pt), Spanish (es), Swedish (sv) — evaluated on CoNLL data (Buchholz and Marsi, 2006). that with a proper “guide”, we can take advantage of very limited annotated data. 2.1 Annotated data Our annotated data mainly comes from CoNLL shared tasks on dependency parsing (Buchholz and Marsi, 2006). The language specific tagsets are mapped into the universal tagset. We will use this annotated data mainly for evaluation. Table 2 shows the size of annotated data for each language. The 8 languages we are considering in this experiment are not actually resource-poor languages. However, running on these 8 languages makes our system comparable with previously proposed methods. Nevertheless"
D14-1096,D10-1056,0,0.024443,"Missing"
D14-1096,P11-1061,0,0.272017,"Missing"
D14-1096,I13-1177,1,0.896928,"Missing"
D14-1096,P13-2112,1,0.89119,"Missing"
D14-1096,P00-1006,0,0.0400144,"ictionary (noisy or otherwise). Although used in most prior work, a tag dictionary for a new language requires significant manual effort to construct. Garrette and Baldridge (2013) showed that a 1,000 token dataset could be collected very cheaply, requiring less than 2 hours of non-expert time. Our correction model makes use of a minimum divergence (MD) model (Berger et al., 1996), a variant of the maximum entropy model which biases the target distribution to be similar to a static reference distribution. The method has been used in several language applications including machine translation (Foster, 2000) and parsing (Plank and van Noord, 2008, Johnson and Riezler, 2000). These previous approaches have used various sources of reference distribution, e.g., incorporating information from a simpler model (Johnson and Riezler, 2000) or combining in- and outof-domain models (Plank and van Noord, 2008). Plank and van Noord (2008) concluded that this method for adding prior knowledge only works with high quality reference distributions, otherwise performance suffers. In contrast to these previous approaches, we consider the specific setting where both the learned model and the reference model so = P"
D14-1096,P08-1085,0,0.180683,"Missing"
D14-1096,W04-3229,0,0.406957,"Missing"
D14-1096,A00-2021,0,0.022186,"prior work, a tag dictionary for a new language requires significant manual effort to construct. Garrette and Baldridge (2013) showed that a 1,000 token dataset could be collected very cheaply, requiring less than 2 hours of non-expert time. Our correction model makes use of a minimum divergence (MD) model (Berger et al., 1996), a variant of the maximum entropy model which biases the target distribution to be similar to a static reference distribution. The method has been used in several language applications including machine translation (Foster, 2000) and parsing (Plank and van Noord, 2008, Johnson and Riezler, 2000). These previous approaches have used various sources of reference distribution, e.g., incorporating information from a simpler model (Johnson and Riezler, 2000) or combining in- and outof-domain models (Plank and van Noord, 2008). Plank and van Noord (2008) concluded that this method for adding prior knowledge only works with high quality reference distributions, otherwise performance suffers. In contrast to these previous approaches, we consider the specific setting where both the learned model and the reference model so = P (t|w) are both maximum entropy models. In this case we show that th"
D14-1096,2005.mtsummit-papers.11,0,0.110128,"Missing"
D14-1096,D12-1127,0,0.158782,"Missing"
D14-1096,J03-1002,0,0.00706494,"Missing"
D14-1096,E99-1010,0,0.0863202,"Missing"
D14-1096,petrov-etal-2012-universal,0,0.0846555,"the universal tagset. We will use this annotated data mainly for evaluation. Table 2 shows the size of annotated data for each language. The 8 languages we are considering in this experiment are not actually resource-poor languages. However, running on these 8 languages makes our system comparable with previously proposed methods. Nevertheless, we try to use as few resources as possible, in order to simulate the situation for resource-poor languages. Later in Section 6 we adapt the approach for Malagasy, a truly resource-poor language. 2.2 Universal tagset We employ the universal tagset from (Petrov et al., 2012) for our experiment. It consists of 12 common tags: NOUN, VERB, ADJ (adjective), ADV (adverb), PRON (pronoun), DET (determiner and article), ADP (preposition and postposition), CONJ (conjunctions), NUM (numerical), PRT (particle), PUNC (punctuation) and X (all other categories including foreign words and abbreviations). Petrov et al. (2012) provide the mapping from each language-specific tagset to the universal tagset. The idea of using the universal tagset is of great use in multilingual applications, enabling comparison across languages. However, the mapping is not always straightforward. Ta"
D14-1096,N13-1014,0,0.0990194,"cussed it makes many errors, due to invalid or inconsistent tag mappings, noisy alignments, and cross-linguistic syntactic divergence. However, our aim is to see how effectively we can exploit the strengths of the DPM model while correcting for its inadequacies using direct supervision. We select only 1,000 annotated tokens to reflect a low resource scenario. A small supervised training sample is a more realistic form of supervision than a tag dictionary (noisy or otherwise). Although used in most prior work, a tag dictionary for a new language requires significant manual effort to construct. Garrette and Baldridge (2013) showed that a 1,000 token dataset could be collected very cheaply, requiring less than 2 hours of non-expert time. Our correction model makes use of a minimum divergence (MD) model (Berger et al., 1996), a variant of the maximum entropy model which biases the target distribution to be similar to a static reference distribution. The method has been used in several language applications including machine translation (Foster, 2000) and parsing (Plank and van Noord, 2008, Johnson and Riezler, 2000). These previous approaches have used various sources of reference distribution, e.g., incorporating"
D14-1096,W08-1302,0,0.056461,"Missing"
D14-1096,P13-1057,0,0.203159,"Missing"
D14-1096,E14-1078,0,0.023072,"s. However, the mapping is not always straightforward. Table 2 shows the size of the annotated data for each language, the number of tags presented in the data, and the list of tags that are not matched. We can see that only 8 tags are presented in the annotated data for Danish, i.e, 4 tags (DET, PRT, PUNC, and NUM) are missing.2 Thus, a classifier using all 12 tags will be heavily penalized in the evaluation. Li et al. (2012) considered this problem and tried to manually modify the Danish mappings. Moreover, PRT is not really a universal tag since it only appears in 3 out of the 8 languages. Plank et al. (2014) pointed out that PRT often gets confused with ADP even in English. We will later show that the mapping problem causes substantial degradation in the performance of a POS tagger exploiting parallel data. The method we present here is more target-language oriented: our model is trained on the target language, in this way, only relevant information from the source language is retained. Thus, we automatically correct the mapping, and other incompatibilities arising from incorrect alignments and syntactic divergence between the source and target languages. Lang Size(k) # Tags da 94 8 nl 203 11 de"
D14-1096,W11-3603,0,0.0222103,"Missing"
D14-1096,Q13-1001,0,0.132253,"Missing"
D14-1096,N03-1033,0,0.0842473,"Missing"
D14-1096,P08-1086,0,0.101901,"Missing"
D14-1096,N01-1026,0,0.12215,"Missing"
D14-1189,W03-1812,1,0.691138,"between a compositional (generally non-MWE) and non-compositional MWE usage. Approaches have ranged from the unsupervised learning of type-level preferences (Fazly et al., 2009) to supervised methods specific to particular MWE constructions (Kim and Baldwin, 2007) or applicable across multiple constructions using features similar to those used in all-words word sense disambiguation (Forthergill and Baldwin, 2011; Muzny and Zettlemoyer, 2013). The prediction of the compositionality of MWE types has traditionally been couched as a binary classification task (compositional or non-compositional: Baldwin et al. (2003), Bannard (2006)), but more recent work has moved towards a regression setup, where the degree of the compositionality is predicted on a continuous scale (Reddy et al., 2011; Salehi and Cook, 2013; Salehi et al., 2014). In either case, the modelling has been done either over the whole MWE (Reddy et al., 2011; Salehi and Cook, 2013), or relative to each component within the MWE (Baldwin et al., 2003; Bannard, 2006). In this paper, we focus on the binary classification of MWE types relative to each component of the 1792 Proceedings of the 2014 Conference on Empirical Methods in Natural Language"
D14-1189,C10-3010,1,0.886304,"Missing"
D14-1189,J09-1005,1,0.884205,"ts about spelling, but not those which contain only bee. For research project, on the other hand, we are likely to be interested in documents which contain either research or project in isolation, and for swan song, we are only going to be interested in documents which contain the phrase swan song, and not just swan or song. In this paper, we propose an unsupervised approach based on Wikitionary for predicting which Related Work Previous studies which have considered MWE compositionality have focused on either the identification of non-compositional MWE token instances (Kim and Baldwin, 2007; Fazly et al., 2009; Forthergill and Baldwin, 2011; Muzny and Zettlemoyer, 2013), or the prediction of the compositionality of MWE types (Reddy et al., 2011; Salehi and Cook, 2013; Salehi et al., 2014). The identification of non-compositional MWE tokens is an important task when a word combination such as kick the bucket or saw logs is ambiguous between a compositional (generally non-MWE) and non-compositional MWE usage. Approaches have ranged from the unsupervised learning of type-level preferences (Fazly et al., 2009) to supervised methods specific to particular MWE constructions (Kim and Baldwin, 2007) or app"
D14-1189,S13-1039,1,0.885672,"ither research or project in isolation, and for swan song, we are only going to be interested in documents which contain the phrase swan song, and not just swan or song. In this paper, we propose an unsupervised approach based on Wikitionary for predicting which Related Work Previous studies which have considered MWE compositionality have focused on either the identification of non-compositional MWE token instances (Kim and Baldwin, 2007; Fazly et al., 2009; Forthergill and Baldwin, 2011; Muzny and Zettlemoyer, 2013), or the prediction of the compositionality of MWE types (Reddy et al., 2011; Salehi and Cook, 2013; Salehi et al., 2014). The identification of non-compositional MWE tokens is an important task when a word combination such as kick the bucket or saw logs is ambiguous between a compositional (generally non-MWE) and non-compositional MWE usage. Approaches have ranged from the unsupervised learning of type-level preferences (Fazly et al., 2009) to supervised methods specific to particular MWE constructions (Kim and Baldwin, 2007) or applicable across multiple constructions using features similar to those used in all-words word sense disambiguation (Forthergill and Baldwin, 2011; Muzny and Zett"
D14-1189,E14-1050,1,0.85897,"Missing"
D14-1189,I11-1102,1,0.882813,"Missing"
D14-1189,kamholz-etal-2014-panlex,0,0.0141321,"gs of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1792–1797, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics MWE. The work that is perhaps most closely related to this paper is that of Salehi and Cook (2013) and Salehi et al. (2014), who use translation data to predict the compositionality of a given MWE relative to each of its components, and then combine those scores to derive an overall compositionality score. In both cases, translations of the MWE and its components are sourced from PanLex (Baldwin et al., 2010; Kamholz et al., 2014), and if there is greater similarity between the translated components and MWE in a range of languages, the MWE is predicted to be more compositional. The basis of the similarity calculation is unsupervised, using either string similarity (Salehi and Cook, 2013) or distributional similarity (Salehi et al., 2014). However, the overall method is supervised, as training data is used to select the languages to aggregate scores across for a given MWE construction. To benchmark our method, we use two of the same datasets as these two papers, and repurpose the best-performing methods of Salehi and Co"
D14-1189,D13-1145,0,0.0604532,"bee. For research project, on the other hand, we are likely to be interested in documents which contain either research or project in isolation, and for swan song, we are only going to be interested in documents which contain the phrase swan song, and not just swan or song. In this paper, we propose an unsupervised approach based on Wikitionary for predicting which Related Work Previous studies which have considered MWE compositionality have focused on either the identification of non-compositional MWE token instances (Kim and Baldwin, 2007; Fazly et al., 2009; Forthergill and Baldwin, 2011; Muzny and Zettlemoyer, 2013), or the prediction of the compositionality of MWE types (Reddy et al., 2011; Salehi and Cook, 2013; Salehi et al., 2014). The identification of non-compositional MWE tokens is an important task when a word combination such as kick the bucket or saw logs is ambiguous between a compositional (generally non-MWE) and non-compositional MWE usage. Approaches have ranged from the unsupervised learning of type-level preferences (Fazly et al., 2009) to supervised methods specific to particular MWE constructions (Kim and Baldwin, 2007) or applicable across multiple constructions using features similar"
D14-1189,I11-1024,0,0.0698704,"ents which contain either research or project in isolation, and for swan song, we are only going to be interested in documents which contain the phrase swan song, and not just swan or song. In this paper, we propose an unsupervised approach based on Wikitionary for predicting which Related Work Previous studies which have considered MWE compositionality have focused on either the identification of non-compositional MWE token instances (Kim and Baldwin, 2007; Fazly et al., 2009; Forthergill and Baldwin, 2011; Muzny and Zettlemoyer, 2013), or the prediction of the compositionality of MWE types (Reddy et al., 2011; Salehi and Cook, 2013; Salehi et al., 2014). The identification of non-compositional MWE tokens is an important task when a word combination such as kick the bucket or saw logs is ambiguous between a compositional (generally non-MWE) and non-compositional MWE usage. Approaches have ranged from the unsupervised learning of type-level preferences (Fazly et al., 2009) to supervised methods specific to particular MWE constructions (Kim and Baldwin, 2007) or applicable across multiple constructions using features similar to those used in all-words word sense disambiguation (Forthergill and Baldwi"
D15-1040,D12-1001,0,0.060049,"s are highly informative of dependency relations, and that there exists shared dependency structures across languages. Building a dependency parser for a resourcepoor language usually starts with the delexicalized parser and then uses other resources to refine the model. McDonald et al. (2011) and Ma and Xia (2014) exploited parallel data as the bridge to transfer constraints from the source resourcerich language to the target resource-poor languages. T¨ackstr¨om et al. (2012) also used parallel data to induce cross-lingual word clusters which added as features for their delexicalized parser. Durrett et al. (2012) constructed the set of language-independent features and used a bilingual dictionary as the bridge to transfer these features from source to target language. T¨ackstr¨om et al. (2013) additionally used high-level linguistic features extracted from the World Atlas of Language Structures (WALS) (Dryer and Haspelmath, 2013). For low-resource languages, no large parallel corpus is available. Some linguists are dependency-annotating small amounts of field data, e.g. for Karuk, a nearly-extinct language of Northwest California (Garrett et al., 2013). Accordingly, we adopt a different resource requi"
D15-1040,D14-1034,0,0.007514,"Missing"
D15-1040,kamholz-etal-2014-panlex,0,0.0227054,"Missing"
D15-1040,H05-1091,0,0.019228,"Missing"
D15-1040,D14-1082,0,0.127244,"es from source to target language. T¨ackstr¨om et al. (2013) additionally used high-level linguistic features extracted from the World Atlas of Language Structures (WALS) (Dryer and Haspelmath, 2013). For low-resource languages, no large parallel corpus is available. Some linguists are dependency-annotating small amounts of field data, e.g. for Karuk, a nearly-extinct language of Northwest California (Garrett et al., 2013). Accordingly, we adopt a different resource require2.1 Supervised Neural Network Parser This section describes the monolingual neural network dependency parser structure of Chen and Manning (2014). This parser achieves excellent performance, and has a highly flexible formulation allowing auxilliary inputs. The model is based on a transition-based dependency parser (Nivre, 2006) formulated as a neural-network classifier to decide which transition to apply to each parsing state configuration.2 That is, for each configuration, the selected list of words, POS tags and labels from the Stack, Queue and Arcs are extracted. Each word, POS and label is mapped into a lowdimension vector representation using an embedding matrix, which is then fed into a two-layer neural network classifier to pred"
D15-1040,2005.mtsummit-papers.11,0,0.0112439,"Missing"
D15-1040,P14-1126,0,0.00966165,"parser is built without any lexical features and trained on a treebank for a resource-rich source language (Zeman et al., 2008). It is then applied directly to parse sentences in the target resource-poor languages. Delexicalized parsing relies on the fact that identical part-ofspeech (POS) inventories are highly informative of dependency relations, and that there exists shared dependency structures across languages. Building a dependency parser for a resourcepoor language usually starts with the delexicalized parser and then uses other resources to refine the model. McDonald et al. (2011) and Ma and Xia (2014) exploited parallel data as the bridge to transfer constraints from the source resourcerich language to the target resource-poor languages. T¨ackstr¨om et al. (2012) also used parallel data to induce cross-lingual word clusters which added as features for their delexicalized parser. Durrett et al. (2012) constructed the set of language-independent features and used a bilingual dictionary as the bridge to transfer these features from source to target language. T¨ackstr¨om et al. (2013) additionally used high-level linguistic features extracted from the World Atlas of Language Structures (WALS)"
D15-1040,D11-1006,0,0.168279,"alized parsing, in which a parser is built without any lexical features and trained on a treebank for a resource-rich source language (Zeman et al., 2008). It is then applied directly to parse sentences in the target resource-poor languages. Delexicalized parsing relies on the fact that identical part-ofspeech (POS) inventories are highly informative of dependency relations, and that there exists shared dependency structures across languages. Building a dependency parser for a resourcepoor language usually starts with the delexicalized parser and then uses other resources to refine the model. McDonald et al. (2011) and Ma and Xia (2014) exploited parallel data as the bridge to transfer constraints from the source resourcerich language to the target resource-poor languages. T¨ackstr¨om et al. (2012) also used parallel data to induce cross-lingual word clusters which added as features for their delexicalized parser. Durrett et al. (2012) constructed the set of language-independent features and used a bilingual dictionary as the bridge to transfer these features from source to target language. T¨ackstr¨om et al. (2013) additionally used high-level linguistic features extracted from the World Atlas of Langu"
D15-1040,P15-2139,1,0.534832,"e treebank covers 10 languages,6 with some languages very highly resourced—Czech, French and Spanish have 400k tokens—and only modest amounts of data for other languages—Hungarian and Irish have only around 25k tokens. Cross-lingual models assume English as the source language, for which we have a large treebank, and only a small treebank of 3k tokens exists in each target language, simulated by subsampling the corpus. 4.2 Baseline Cascade Model We compare our approach to a baseline interlingual model based on the same parsing algorithm as presented in section 2.1, but with cascaded training (Duong et al., 2015). This works by first learning the source language parser, and then training the target language parser using a regularization term to minimise the distance between the parameters of the target parser and the source parser (which is fixed). In this way, some structural information from the source parser can be used in the target parser, however it is likely that the representation will be overly biased towards the source language and consequently may not prove as useful for modelling the target. 4.3 Monolingual Word Embeddings While the E pos and E arc are randomly initialized, we initialize b"
D15-1040,N09-1028,0,0.0541894,"Missing"
D15-1040,skadins-etal-2014-billions,0,0.0228432,"Missing"
D15-1040,D13-1170,0,0.00392188,"Missing"
D15-1040,N12-1052,0,0.160554,"Missing"
D15-1040,N13-1126,0,0.0678031,"Missing"
D15-1040,I08-3008,0,\N,Missing
E12-1060,S07-1002,0,0.718641,"assignments — in the form of multinomial probability distributions over topics. LDA is appealing for WSI as it both assigns senses to words (in the form of topic allocation), and outputs a representation of each sense as a weighted list of words. LDA offers a solution to the question of sense granularity determination via non-parametric formulations, such as a Hierarchical Dirichlet Process (HDP: Teh et al. (2006), Yao and Durme (2011)). Our contributions in this paper are as follows. We first establish the effectiveness of HDP for WSI over both the SemEval-2007 and SemEval2010 WSI datasets (Agirre and Soroa, 2007; Manandhar et al., 2010), and show that the nonparametric formulation is superior to a standard LDA formulation with oracle determination of sense granularity for a given word. We next demonstrate that our interpretation of HDP-based WSI is superior to other topic model-based approaches to WSI, and indeed, better than the bestpublished results for both SemEval datasets. Finally, we apply our method to the novel sense detection task based on a dataset developed in this research, and achieve highly encouraging results. 2 Methodology In topic modelling, documents are assumed to exhibit multiple"
E12-1060,E09-1013,0,0.822927,"s a task, there have been no real examples of WSI being successfully deployed in end-user applications, other than work by Schutze (1998) and Navigli and Crisafulli (2010) in an information retrieval context. A key contribution of this paper is the successful application of WSI to the lexicographical task of novel sense detection, i.e. identifying words which have taken on new senses over time. One of the key challenges in WSI is learning the appropriate sense granularity for a given word, i.e. the number of senses that best captures the token occurrences of that word. Building on the work of Brody and Lapata (2009) and others, we approach WSI via topic modelling — using Latent Dirichlet Allocation (LDA: Blei et al. (2003)) and derivative approaches — and use the topic model to determine the appropriate sense granularity. Topic modelling is an unsupervised approach to jointly learn topics — in the form of multinomial probability distributions over words — and per-document topic assignments — in the form of multinomial probability distributions over topics. LDA is appealing for WSI as it both assigns senses to words (in the form of topic allocation), and outputs a representation of each sense as a weighte"
E12-1060,cook-stevenson-2010-automatically,1,0.882901,"hese senses is listed in Wordnet 3.0 (Fellbaum, 1998) — yet appear to be in regular usage, particularly in text related to pop culture and online media. The manual identification of such new wordsenses is a challenge in lexicography over and above identifying new words themselves, and is essential to keeping dictionaries up-to-date. Moreover, lexicons that better reflect contemporary usage could benefit NLP applications that use sense inventories. The challenge of identifying changes in word sense has only recently been considered in computational linguistics. For example, Sagi et al. (2009), Cook and Stevenson (2010), and Gulordava and Baroni (2011) propose type-based models of semantic change. Such models do not account for polysemy, and appear best-suited to identifying changes in predominant sense. Bamman and Crane (2011) use a parallel Latin– English corpus to induce word senses and build a WSD system, which they then apply to study diachronic variation in word senses. Crucially, in this token-based approach there is a clear connection between word senses and tokens, making it possible to identify usages of a specific sense. Based on the findings in Section 3.2, here we apply the HDP method for WSI to"
E12-1060,de-marneffe-etal-2006-generating,0,0.00362124,"Missing"
E12-1060,H92-1045,0,0.355926,"ruction of semantic space models, e.g. for WSD. Based on these findings, we include dependency relations as additional features in our topic models,2 but just for dependency relations that involve the target word. 2.2 Topic Modelling Topic models learn a probability distribution over topics for each document, by simply aggregating the distributions over topics for each word in the document. In WSI terms, we take this distribution over topics for each target word (“instance” in WSI parlance) as our distribution over senses for that word. 1 Notwithstanding the one sense per discourse heuristic (Gale et al., 1992). 2 We use the Stanford Parser to do part of speech tagging and to extract the dependency relations (Klein and Manning, 2003; De Marneffe et al., 2006). In our initial experiments, we use LDA topic modelling, which requires us to set T , the number of topics to be learned by the model. The LDA generative process is: (1) draw a latent topic z from a document-specific topic distribution P (t = z|d) then; (2) draw a word w from the chosen topic P (w|t = z). Thus, the probability of producing a single copy of word w given a document d is given by: P (w|d) = T ∑ P (w|t = z)P (t = z|d). z=1 In stand"
E12-1060,W11-2508,0,0.0612863,"t 3.0 (Fellbaum, 1998) — yet appear to be in regular usage, particularly in text related to pop culture and online media. The manual identification of such new wordsenses is a challenge in lexicography over and above identifying new words themselves, and is essential to keeping dictionaries up-to-date. Moreover, lexicons that better reflect contemporary usage could benefit NLP applications that use sense inventories. The challenge of identifying changes in word sense has only recently been considered in computational linguistics. For example, Sagi et al. (2009), Cook and Stevenson (2010), and Gulordava and Baroni (2011) propose type-based models of semantic change. Such models do not account for polysemy, and appear best-suited to identifying changes in predominant sense. Bamman and Crane (2011) use a parallel Latin– English corpus to induce word senses and build a WSD system, which they then apply to study diachronic variation in word senses. Crucially, in this token-based approach there is a clear connection between word senses and tokens, making it possible to identify usages of a specific sense. Based on the findings in Section 3.2, here we apply the HDP method for WSI to the task of 596 identifying new"
E12-1060,S10-1079,0,0.195775,"Missing"
E12-1060,S10-1011,0,0.457638,"rm of multinomial probability distributions over topics. LDA is appealing for WSI as it both assigns senses to words (in the form of topic allocation), and outputs a representation of each sense as a weighted list of words. LDA offers a solution to the question of sense granularity determination via non-parametric formulations, such as a Hierarchical Dirichlet Process (HDP: Teh et al. (2006), Yao and Durme (2011)). Our contributions in this paper are as follows. We first establish the effectiveness of HDP for WSI over both the SemEval-2007 and SemEval2010 WSI datasets (Agirre and Soroa, 2007; Manandhar et al., 2010), and show that the nonparametric formulation is superior to a standard LDA formulation with oracle determination of sense granularity for a given word. We next demonstrate that our interpretation of HDP-based WSI is superior to other topic model-based approaches to WSI, and indeed, better than the bestpublished results for both SemEval datasets. Finally, we apply our method to the novel sense detection task based on a dataset developed in this research, and achieve highly encouraging results. 2 Methodology In topic modelling, documents are assumed to exhibit multiple topics, with each documen"
E12-1060,D10-1012,0,0.0734893,"sk. 1 Introduction Word sense induction (WSI) is the task of automatically inducing the different senses of a given word, generally in the form of an unsupervised learning task with senses represented as clusters of token instances. It contrasts with word sense disambiguation (WSD), where a fixed sense inventory is assumed to exist, and token instances of a given word are disambiguated relative to the sense inventory. While WSI is intuitively appealing as a task, there have been no real examples of WSI being successfully deployed in end-user applications, other than work by Schutze (1998) and Navigli and Crisafulli (2010) in an information retrieval context. A key contribution of this paper is the successful application of WSI to the lexicographical task of novel sense detection, i.e. identifying words which have taken on new senses over time. One of the key challenges in WSI is learning the appropriate sense granularity for a given word, i.e. the number of senses that best captures the token occurrences of that word. Building on the work of Brody and Lapata (2009) and others, we approach WSI via topic modelling — using Latent Dirichlet Allocation (LDA: Blei et al. (2003)) and derivative approaches — and use t"
E12-1060,S07-1037,0,0.0562894,"Missing"
E12-1060,J07-2002,0,0.0570721,"Missing"
E12-1060,W09-0214,0,0.117489,"Missing"
E12-1060,J98-1004,0,0.680031,"sense detection task. 1 Introduction Word sense induction (WSI) is the task of automatically inducing the different senses of a given word, generally in the form of an unsupervised learning task with senses represented as clusters of token instances. It contrasts with word sense disambiguation (WSD), where a fixed sense inventory is assumed to exist, and token instances of a given word are disambiguated relative to the sense inventory. While WSI is intuitively appealing as a task, there have been no real examples of WSI being successfully deployed in end-user applications, other than work by Schutze (1998) and Navigli and Crisafulli (2010) in an information retrieval context. A key contribution of this paper is the successful application of WSI to the lexicographical task of novel sense detection, i.e. identifying words which have taken on new senses over time. One of the key challenges in WSI is learning the appropriate sense granularity for a given word, i.e. the number of senses that best captures the token occurrences of that word. Building on the work of Brody and Lapata (2009) and others, we approach WSI via topic modelling — using Latent Dirichlet Allocation (LDA: Blei et al. (2003)) and"
E12-1060,W11-1102,0,0.612441,"Missing"
E12-2014,P11-1038,1,0.769199,"roblem. At present, lexical normalisation is an optional plug-in for post-processing messages. A further issue related to noisy tokens is that it is possible that a relevant tweet might contain a variant of a query term, but not that query term itself. In future versions of the system we therefore aim to use query expansion to generate noisy versions of query terms to retrieve additional relevant tweets. We subsequently intend to perform lexical normalisation to evaluate the precision of the returned data. The present lexical normalisation used by our system is the dictionary lookup method of Han and Baldwin (2011) which normalises noisy tokens only when the normalised form is known with high confidence (e.g. you for u). Ultimately, however, we are interested in performing contextsensitive lexical normalisation, based on a reimplementation of the method of Han and Baldwin (2011). This method will allow us to target a wider variety of noisy tokens such as typos (e.g. earthquak “earthquake”), abbreviations (e.g. lv “love”), phonetic substitutions (e.g. b4 “before”) and vowel lengthening (e.g. goooood “good”). 3.4 Geolocation A vital component of event detection is the determination of where the event is h"
E12-2014,I11-1062,1,0.814867,"led and asynchronous. Below, we describe details of the various modules of the system. 3.1 Twitter Querying When the user inputs a set of keywords, this is issued as a disjunctive query to the Twitter Streaming API, which returns a streamed set of results in JSON format. The results are parsed, and piped through to the language filtering, lexical normalisation, and geolocation modules, and finally stored in a flat file, which the GUI interacts with. 3.2 Language Filtering For language identification, we use langid.py, a language identification toolkit developed at The University of Melbourne (Lui and Baldwin, 2011).3 langid.py combines a naive Bayes classifier with cross-domain feature selection to provide domain-independent language identification. It is available under a FOSS license as a stand-alone module pre-trained over 97 languages. langid.py has been developed specifically to be able to keep pace with the speed of messages through the Twitter “garden hose” feed on a single-CPU machine, making it particularly attractive for this project. Additionally, in an in-house evaluation over three separate corpora of Twitter data, we have found langid.py to be overall more accurate than other state-ofthe-a"
E12-2014,D11-1141,0,0.0183463,"ion, by combining the total set of messages from a given user into a single combined message. Given a message m, the task is to find arg maxi P (loci |m) where each loci is a grid cell on the map. Based on Bayes’ theorem and standard assumptions in the naive Bayes formulation, this is transformed into: arg max P (loci ) i v Y P (wj |loci ) j To avoid zero probabilities, we only consider tokens that occur at least twice in the training data, and ignore unseen words. A probability is calculated for the most-probable location by normalising over the scores for each loci . We employ the method of Ritter et al. (2011) to tokenise messages, and use token unigrams as features, including any hashtags, but ignoring twitter mentions, URLs and purely numeric tokens. We 6 Alternatively, we could consider a hybrid approach of user- and message-level geolocation prediction, especially for users where we have sufficient training data, which we plan to incorporate into a future version of the system. 71 0.40 Prediction Accuracy 0.35 • the probability of the predicated geolocation ● ● ● • the text of the tweet ● ● ● ● 0.30 ● ● ● ● 0.15 0.20 0.25 ● 10000 20000 30000 40000 Feature Number Figure 2: Accuracy of geolocatio"
E14-1050,W11-0815,0,0.161446,"Missing"
E14-1050,E03-1073,0,0.0405212,"ce language distributional similarity, but when combined with Results All experiments are carried out using 10 iterations of 10-fold cross validation, randomly partitioning the data independently on each of the 10 iterations, and averaging across all 100 test partitions in our presented results. In the case of CS L2N and other methods that make use of it (i.e. CS L1 +L2N and CS all ), the languages selected for a given training fold are then used to compute the compositionality scores for the instances in the test set. Figures 3a, 3b and 3c are histograms of the number of times 7 Although see Lapata and Lascarides (2003) for discussion of the difficulty of reliably identifying low-frequency English noun compounds. 477 25 20 18 20 16 Frequency Frequency 14 15 10 12 10 8 6 5 4 2 0 0 5 10 15 20 0 0 25 5 10 bestN 15 20 25 best N (a) ENC (b) EVPC 20 18 16 Frequency 14 12 10 8 6 4 2 0 0 5 10 15 20 25 best N (c) GNC Figure 3: Histograms displaying how many times a given N is selected as the best number of languages over each dataset. For example, according to the GNC chart, there is a peak for N = 2, which shows that over 100 folds, the best-2 languages achieved the highest correlation on 18 folds. Method CS L1 CS L"
E14-1050,W03-1812,1,0.673121,"ither the tokenlevel (over token occurrences of an MWE in a corpus) or type-level (over the MWE string, independent of usage). The bulk of work on compositionality has been language/construction-specific and operated at the token-level, using dedicated methods to identify instances of a given MWE, and specific properties of the MWE in that language to predict compositionality (Lin, 1999; Kim and Baldwin, 2007; Fazly et al., 2009). General-purpose token-level approaches such as distributional similarity have been commonly applied to infer the semantics of a word/MWE (Schone and Jurafsky, 2001; Baldwin et al., 2003; Reddy et al., 2011). These techniques are based on the assumption that the meaning of a word is predictable from its context of use, via the neighbouring words of token-level occurrences of the MWE. In order to predict the compositionality of a given MWE using distributional similarity, the different contexts of the MWE are compared with the contexts of its components, and the MWE is considered to be compositional if the MWE and component words occur in similar contexts. Identifying token instances of MWEs is not always easy, especially when the component words do not occur sequentially. For"
E14-1050,P99-1041,0,0.0380202,"te-of-the-art results over two datasets. 2 Related Work Most recent work on predicting the compositionality of MWEs can be divided into two categories: language/construction-specific and general-purpose. This can be at either the tokenlevel (over token occurrences of an MWE in a corpus) or type-level (over the MWE string, independent of usage). The bulk of work on compositionality has been language/construction-specific and operated at the token-level, using dedicated methods to identify instances of a given MWE, and specific properties of the MWE in that language to predict compositionality (Lin, 1999; Kim and Baldwin, 2007; Fazly et al., 2009). General-purpose token-level approaches such as distributional similarity have been commonly applied to infer the semantics of a word/MWE (Schone and Jurafsky, 2001; Baldwin et al., 2003; Reddy et al., 2011). These techniques are based on the assumption that the meaning of a word is predictable from its context of use, via the neighbouring words of token-level occurrences of the MWE. In order to predict the compositionality of a given MWE using distributional similarity, the different contexts of the MWE are compared with the contexts of its compone"
E14-1050,C10-3010,1,0.556517,"Missing"
E14-1050,W03-1810,0,0.829157,"Missing"
E14-1050,W03-1809,1,0.715081,"160 English verb particle constructions (VPCs), from the work of Bannard (2006). In this dataset, a verb particle construction consists of a verb (the head) and a prepositional particle (e.g. hand in, look up or battle on). For each component word (the verb and particle, respectively), multiple annotators were asked whether the VPC entails the component word. In order to translate the dataset into a regression task, we calculate the overall compositionality as the number of annotations of entailment for the verb, divided by the total number of verb annotations for that VPC. That is, following Bannard et al. (2003), we only consider the compositionality of the verb component in our experiments (and as such α = 1 in Equation 1). One area of particular interest with this dataset will be the robustness of the method to function words (the particles), both under translation and in terms of calculating distributional similarity, although the findings of Baldwin (2006) for English prepositions are at least encouraging in this respect. Additionally, English VPCs can occur in “split” form (e.g. put your jacket on, from our earlier example), which will complicate identification, and the verb component will often"
E14-1050,D13-1060,0,0.0109709,"ing MWE “identification” to be a token-level disambiguation task, and MWE “extraction” to be a type-level lexicon induction task. 472 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 472–481, c Gothenburg, Sweden, April 26-30 2014. 2014 Association for Computational Linguistics tional scores may not be reliable. Additionally, for morphologically-rich languages, it can be difficult to predict the different word forms a given MWE type will occur across, posing a challenge for our requirement of no language-specific preprocessing. Pichotta and DeNero (2013) proposed a tokenbased method for identifying English phrasal verbs based on parallel corpora for 50 languages. They show that they can identify phrasal verbs better when they combine information from multiple languages, in addition to the information they get from a monolingual corpus. This finding lends weight to our hypothesis that using translation data and distributional similarity from each of a range of target languages, can improve compositionality prediction. Having said that, the general applicability of the method is questionable — there are many parallel corpora involving English,"
E14-1050,W12-3311,0,0.167418,"Missing"
E14-1050,N10-1029,0,0.0743601,"Missing"
E14-1050,I11-1024,0,0.72515,"over token occurrences of an MWE in a corpus) or type-level (over the MWE string, independent of usage). The bulk of work on compositionality has been language/construction-specific and operated at the token-level, using dedicated methods to identify instances of a given MWE, and specific properties of the MWE in that language to predict compositionality (Lin, 1999; Kim and Baldwin, 2007; Fazly et al., 2009). General-purpose token-level approaches such as distributional similarity have been commonly applied to infer the semantics of a word/MWE (Schone and Jurafsky, 2001; Baldwin et al., 2003; Reddy et al., 2011). These techniques are based on the assumption that the meaning of a word is predictable from its context of use, via the neighbouring words of token-level occurrences of the MWE. In order to predict the compositionality of a given MWE using distributional similarity, the different contexts of the MWE are compared with the contexts of its components, and the MWE is considered to be compositional if the MWE and component words occur in similar contexts. Identifying token instances of MWEs is not always easy, especially when the component words do not occur sequentially. For example consider put"
E14-1050,W03-1806,0,0.0157759,"Missing"
E14-1050,S13-1039,1,0.760884,"rasal verbs based on parallel corpora for 50 languages. They show that they can identify phrasal verbs better when they combine information from multiple languages, in addition to the information they get from a monolingual corpus. This finding lends weight to our hypothesis that using translation data and distributional similarity from each of a range of target languages, can improve compositionality prediction. Having said that, the general applicability of the method is questionable — there are many parallel corpora involving English, but for other languages, this tends not to be the case. Salehi and Cook (2013) proposed a generalpurpose type-based approach using translation data from multiple languages, and string similarity between the MWE and each of the component words. They use training data to identify the best-10 languages for a given family of MWEs, on which to base the string similarity, and once again find that translation data improves their results substantially. Among the four string similarity measures they experimented with, longest common substring was found to perform best. Their proposed method is general and applicable to different families of MWEs in different languages. In this p"
E14-1050,W01-0513,0,0.131714,"and German noun compounds. We show that the estimation of compositionality is improved when using translations into multiple languages, as compared to simply using distributional similarity in the source language. We further find that string similarity complements distributional similarity. 1 Compositionality of MWEs Multiword expressions (hereafter MWEs) are combinations of words which are lexically, syntactically, semantically or statistically idiosyncratic (Sag et al., 2002; Baldwin and Kim, 2009). Much research has been carried out on the extraction and identification of MWEs1 in English (Schone and Jurafsky, 2001; Pecina, 2008; Fazly et al., 2009) and other languages (Dias, 2003; Evert and Krenn, 2005; Salehi et al., 2012). However, considerably less work has addressed the task of predicting the meaning of MWEs, especially in non-English languages. As a step in this direction, the focus of this study is on predicting the compositionality of MWEs. An MWE is fully compositional if its meaning is predictable from its component words, and it is non-compositional (or idiomatic) if not. For example, stand up “rise to one’s feet” is composi1 In this paper, we follow Baldwin and Kim (2009) in considering MWE"
E14-1050,S13-1038,0,0.403379,"Missing"
E14-4042,I13-1041,1,0.827887,"ultiple usages in a single document (Gale Introduction Social media applications such as Twitter enable users from all over the world to create and share web content spontaneously. The resulting usergenerated content has been identified as having potential in a myriad of applications including real-time event detection (Petrovi´c et al., 2010), trend analysis (Lau et al., 2012) and natural disaster response co-ordination (Earle et al., 2010). However, the dynamism and conversational nature of the text contained in social media can cause problems for traditional NLP approaches such as parsing (Baldwin et al., 2013), meaning that most content-based approaches use simple keyword search or a bag-of-words representation of the text. This paper is a first step towards full lexical semantic analysis of social media text, in investigating the sense distribution of a range of polysemous words in Twitter and a generalpurpose web corpus. The primary finding of this paper is that there are strong user-level lexical semantic priors in Twitter, equivalent in strength to document-level 215 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 215–220, c Got"
E14-4042,S13-2049,0,0.0833979,"word are mapped onto discrete “senses” in a preexisting sense inventory (Navigli, 2009). The most popular sense inventory used in WSD research has been WordNet (Fellbaum, 1998), although its finegrained sense distinctions have proven to be difficult to make for human annotators and WSD systems alike. This has resulted in a move towards more coarse-grained sense inventories (Palmer et al., 2004; Hovy et al., 2006; Navigli et al., 2007), or alternatively away from pre-existing sense inventories altogether, towards joint word sense induction (WSI) and disambiguation (Navigli and Vannella, 2013; Jurgens and Klapaftis, 2013). Two heuristics that have proven highly powerful in WSD and WSI research are: (1) first sense tagging, and (2) one sense per discourse. First sense tagging is based on the observation that sense distributions tend to be Zipfian, such that if the predominant or “first” sense can be identified, simply tagging all occurrences of a given word with this sense can achieve high WSD accuracy (McCarthy et al., 2007). Unsurprisingly, there are significant differences in sense distributions across domains (cf. cloud in the COMPUTING and METEOROLOG ICAL domains), motivating the need for unsupervised firs"
E14-4042,2011.mtsummit-papers.59,0,0.0862472,"Missing"
E14-4042,H05-1053,0,0.11944,"ul in WSD and WSI research are: (1) first sense tagging, and (2) one sense per discourse. First sense tagging is based on the observation that sense distributions tend to be Zipfian, such that if the predominant or “first” sense can be identified, simply tagging all occurrences of a given word with this sense can achieve high WSD accuracy (McCarthy et al., 2007). Unsurprisingly, there are significant differences in sense distributions across domains (cf. cloud in the COMPUTING and METEOROLOG ICAL domains), motivating the need for unsupervised first sense learning over domain-specific corpora (Koeling et al., 2005). One sense per discourse is the observation that a given word will often occur with a single sense across multiple usages in a single document (Gale Introduction Social media applications such as Twitter enable users from all over the world to create and share web content spontaneously. The resulting usergenerated content has been identified as having potential in a myriad of applications including real-time event detection (Petrovi´c et al., 2010), trend analysis (Lau et al., 2012) and natural disaster response co-ordination (Earle et al., 2010). However, the dynamism and conversational natu"
E14-4042,N13-1121,0,0.0640041,"Missing"
E14-4042,C12-1093,1,0.84043,"OLOG ICAL domains), motivating the need for unsupervised first sense learning over domain-specific corpora (Koeling et al., 2005). One sense per discourse is the observation that a given word will often occur with a single sense across multiple usages in a single document (Gale Introduction Social media applications such as Twitter enable users from all over the world to create and share web content spontaneously. The resulting usergenerated content has been identified as having potential in a myriad of applications including real-time event detection (Petrovi´c et al., 2010), trend analysis (Lau et al., 2012) and natural disaster response co-ordination (Earle et al., 2010). However, the dynamism and conversational nature of the text contained in social media can cause problems for traditional NLP approaches such as parsing (Baldwin et al., 2013), meaning that most content-based approaches use simple keyword search or a bag-of-words representation of the text. This paper is a first step towards full lexical semantic analysis of social media text, in investigating the sense distribution of a range of polysemous words in Twitter and a generalpurpose web corpus. The primary finding of this paper is th"
E14-4042,P12-3005,1,0.763775,"er text. 3 case form match rule charge function panel sign deal issue paper track Table 1: The 20 target nouns used in this research 3.1 Data Sampling We sampled tweets from a crawl made using the Twitter Streaming API from January 3, 2012 to February 29, 2012. The web corpus was built from ukWaC (Ferraresi et al., 2008), which was based on a crawl of the .uk domain from 2007. In contrast to ukWaC, the tweets are not restricted to documents from any particular country. For both corpora, we first selected only the English documents using langid.py, an off-theshelf language identification tool (Lui and Baldwin, 2012). We next identified documents which contained nominal usages of the target words, based on the POS tags supplied with the corpus in the case of ukWaC, and the output of the CMU ARK Twitter POS tagger v2.0 (Owoputi et al., 2012) in the case of Twitter. For Twitter, we are interested in not just the overall lexical distribution of each target noun, but also per-user lexical distributions. As such, we construct two Twitter-based datasets: (1) T WITTER RAND , a random sample of 100 usages of each target noun; and (2) T WITTER USER , 5 usages of each target noun from each member of a random sample"
E14-4042,P09-1002,0,0.161799,"Missing"
E14-4042,U12-1006,1,0.822745,"Missing"
E14-4042,J07-4005,0,0.0941266,"Navigli et al., 2007), or alternatively away from pre-existing sense inventories altogether, towards joint word sense induction (WSI) and disambiguation (Navigli and Vannella, 2013; Jurgens and Klapaftis, 2013). Two heuristics that have proven highly powerful in WSD and WSI research are: (1) first sense tagging, and (2) one sense per discourse. First sense tagging is based on the observation that sense distributions tend to be Zipfian, such that if the predominant or “first” sense can be identified, simply tagging all occurrences of a given word with this sense can achieve high WSD accuracy (McCarthy et al., 2007). Unsurprisingly, there are significant differences in sense distributions across domains (cf. cloud in the COMPUTING and METEOROLOG ICAL domains), motivating the need for unsupervised first sense learning over domain-specific corpora (Koeling et al., 2005). One sense per discourse is the observation that a given word will often occur with a single sense across multiple usages in a single document (Gale Introduction Social media applications such as Twitter enable users from all over the world to create and share web content spontaneously. The resulting usergenerated content has been identifie"
E14-4042,W04-0807,0,0.0528238,"Missing"
E14-4042,H92-1045,0,0.674404,"ristic (Gale et al., 1992). This has potential implications for future applications over Twitter which attempt to move beyond a simple string-based meaning representation to explicit lexical semantic analysis. In recent years, microblogs such as Twitter have emerged as a new communication channel. Twitter in particular has become the target of a myriad of content-based applications including trend analysis and event detection, but there has been little fundamental work on the analysis of word usage patterns in this text type. In this paper — inspired by the one-sense-perdiscourse heuristic of Gale et al. (1992) — we investigate user-level sense distributions, and detect strong support for “one sense per tweeter”. As part of this, we construct a novel sense-tagged lexical sample dataset based on Twitter and a web corpus. 1 2 Related Work The traditional approach to the analysis of wordlevel lexical semantics is via word sense disambiguation (WSD), where usages of a given word are mapped onto discrete “senses” in a preexisting sense inventory (Navigli, 2009). The most popular sense inventory used in WSD research has been WordNet (Fellbaum, 1998), although its finegrained sense distinctions have proven"
E14-4042,S13-2035,0,0.0612337,"d predominantly to comment on a favourite sports team or political events, and as such is domain-driven. Alternatively, it can perhaps be explained by the “reactive” nature of Twitter, in that posts are often emotive responses to happenings in a user’s life, and while different things excite different individuals, a given individual will tend to be excited by events of similar kinds. Clearly more research is required to test these hypotheses. One highly promising direction for this research would be to overlay analysis of sense distributions with analysis of user profiles (e.g. Bergsma et al. (2013)), and test the impact of geospatial and sociolinguistic factors on sense preferences. We would also like to consider the impact of time on the one sense per tweeter heuristic, and consider whether “one sense per Twitter conversation” also holds. To summarise, we have investigated sense distributions in Twitter and a general web corpus, over both a random sample of usages and a sample of usages from a single user/document. We found strong evidence for Twitter users to use a given word with a single sense, and also that individual first sense preferences differ between users, suggesting that me"
E14-4042,S13-1036,1,0.890686,"Missing"
E14-4042,S07-1006,0,0.0735875,"on Twitter and a web corpus. 1 2 Related Work The traditional approach to the analysis of wordlevel lexical semantics is via word sense disambiguation (WSD), where usages of a given word are mapped onto discrete “senses” in a preexisting sense inventory (Navigli, 2009). The most popular sense inventory used in WSD research has been WordNet (Fellbaum, 1998), although its finegrained sense distinctions have proven to be difficult to make for human annotators and WSD systems alike. This has resulted in a move towards more coarse-grained sense inventories (Palmer et al., 2004; Hovy et al., 2006; Navigli et al., 2007), or alternatively away from pre-existing sense inventories altogether, towards joint word sense induction (WSI) and disambiguation (Navigli and Vannella, 2013; Jurgens and Klapaftis, 2013). Two heuristics that have proven highly powerful in WSD and WSI research are: (1) first sense tagging, and (2) one sense per discourse. First sense tagging is based on the observation that sense distributions tend to be Zipfian, such that if the predominant or “first” sense can be identified, simply tagging all occurrences of a given word with this sense can achieve high WSD accuracy (McCarthy et al., 2007)"
E14-4042,N09-4006,0,0.0347805,"Missing"
E14-4042,W04-2807,0,0.529119,"ense-tagged lexical sample dataset based on Twitter and a web corpus. 1 2 Related Work The traditional approach to the analysis of wordlevel lexical semantics is via word sense disambiguation (WSD), where usages of a given word are mapped onto discrete “senses” in a preexisting sense inventory (Navigli, 2009). The most popular sense inventory used in WSD research has been WordNet (Fellbaum, 1998), although its finegrained sense distinctions have proven to be difficult to make for human annotators and WSD systems alike. This has resulted in a move towards more coarse-grained sense inventories (Palmer et al., 2004; Hovy et al., 2006; Navigli et al., 2007), or alternatively away from pre-existing sense inventories altogether, towards joint word sense induction (WSI) and disambiguation (Navigli and Vannella, 2013; Jurgens and Klapaftis, 2013). Two heuristics that have proven highly powerful in WSD and WSI research are: (1) first sense tagging, and (2) one sense per discourse. First sense tagging is based on the observation that sense distributions tend to be Zipfian, such that if the predominant or “first” sense can be identified, simply tagging all occurrences of a given word with this sense can achieve"
E14-4042,N10-1021,0,0.110433,"Missing"
E14-4042,N06-2015,0,\N,Missing
I13-1041,P13-4002,1,0.499896,"ols. Most research to date on social media text has used very shallow text processing (such as Background Natural language processing (NLP) has been applied to a wide range of applications on social media, especially Twitter. Numerous studies have attempted to go beyond simple keyword and burstiness models to identify real-world events from Twitter (Benson et al., 2011; Ritter et al., 2012; Petrovic et al., 2012). Recent efforts have considered identifying user location based on the textual content of tweets (Wing and Baldridge, 2011; Roller et al., 2012; Han et al., 2012b) and user metadata (Han et al., 2013). Related work has examined models of the relationships between words and locations for the purpose of identifying and studying regional linguistic variation (Eisenstein et al., 2010; Eisenstein et al., 2012). Given the abundance of non-standard language 356 International Joint Conference on Natural Language Processing, pages 356–364, Nagoya, Japan, 14-18 October 2013. following datasets from across the spectrum of popular social media sites, varying in terms of document length, the number of authors/editors per document, and the level of text editing: on social media, including lexical varian"
I13-1041,adolphs-etal-2008-fine,0,0.0360756,"Missing"
I13-1041,P12-3005,1,0.204428,"Missing"
I13-1041,N13-1039,0,0.0249366,"respectively), in the hopes of making text more tractable to NLP (Eisenstein, 2013). Approaches to normalisation have exploited various sources of information including the context in which a given instance of a lexical variant occurs (Gouws et al., 2011; Han and Baldwin, 2011), although the best results to date have been achieved by automatically discovering lexical variant–standard form pairs from a large Twitter corpus (Han et al., 2012a). This latter approach is particularly appealing because it allows for very fast normalisation, suitable for processing large volumes of text. Conversely, Owoputi et al. (2013) and Ritter et al. (2011) developed part-of-speech (POS) taggers for Twitter that are better able to handle properties of this text type such as the higher outof-vocabulary rate compared to conventional text. Ritter et al. further developed a Twitter shallow parser and named-entity recogniser. Foster et al. (2011) evaluated standard parsers on social media data, and found them to perform particularly poorly on Twitter, but showed that their performance can be improved through a retraining strategy. Another natural question to ask is how similar the characteristics of social media text are to t"
I13-1041,P11-1040,0,0.0125344,"as Twitter or blogs. A natural question to ask is how different the textual content of the myriad of social media types are from one another. This is an important first step towards building a generalpurpose suite of social media text processing tools. Most research to date on social media text has used very shallow text processing (such as Background Natural language processing (NLP) has been applied to a wide range of applications on social media, especially Twitter. Numerous studies have attempted to go beyond simple keyword and burstiness models to identify real-world events from Twitter (Benson et al., 2011; Ritter et al., 2012; Petrovic et al., 2012). Recent efforts have considered identifying user location based on the textual content of tweets (Wing and Baldridge, 2011; Roller et al., 2012; Han et al., 2012b) and user metadata (Han et al., 2013). Related work has examined models of the relationships between words and locations for the purpose of identifying and studying regional linguistic variation (Eisenstein et al., 2010; Eisenstein et al., 2012). Given the abundance of non-standard language 356 International Joint Conference on Natural Language Processing, pages 356–364, Nagoya, Japan, 14"
I13-1041,N12-1034,0,0.00934521,"ask is how different the textual content of the myriad of social media types are from one another. This is an important first step towards building a generalpurpose suite of social media text processing tools. Most research to date on social media text has used very shallow text processing (such as Background Natural language processing (NLP) has been applied to a wide range of applications on social media, especially Twitter. Numerous studies have attempted to go beyond simple keyword and burstiness models to identify real-world events from Twitter (Benson et al., 2011; Ritter et al., 2012; Petrovic et al., 2012). Recent efforts have considered identifying user location based on the textual content of tweets (Wing and Baldridge, 2011; Roller et al., 2012; Han et al., 2012b) and user metadata (Han et al., 2013). Related work has examined models of the relationships between words and locations for the purpose of identifying and studying regional linguistic variation (Eisenstein et al., 2010; Eisenstein et al., 2012). Given the abundance of non-standard language 356 International Joint Conference on Natural Language Processing, pages 356–364, Nagoya, Japan, 14-18 October 2013. following datasets from acr"
I13-1041,D10-1124,0,0.0437419,"Missing"
I13-1041,C12-2096,0,0.0361344,"nalysis of these Corpus Pre-processing We first pre-process each dataset using the following standardised methodology.3 In the case that the corpus comes with tokenisation and POS information, we strip this and perform automatic preprocessing to ensure consistency in the quality and composition of the tokens/tags. We first apply langid.py (Lui and Baldwin, 2012) — an off-the-shelf language identifier — to each document to detect its majority language. We then extract all documents identified as English for further processing. We next perform sentence tokenisation. In line with the findings of Read et al. (2012a) based on experimentation with a selection of sentence tokenisers over user-generated content, we sentencetokenise with tokenizer.4 Finally, we tokenise and POS tag the datasets using TweetNLP 0.3 (Owoputi et al., 2013). One particularly important property of TweetNLP is that it identifies content such as mentions, URLs, and emoticons that aren’t typically syntactic elements of a sentence. More3 Acknowledging that superior domain-specific approaches exist, e.g. for Wikipedia sentence tokenisation using markup (Flickinger et al., 2010). 4 http://www.cis.uni-muenchen.de/ wastl/misc/ ˜ 5 Specif"
I13-1041,read-etal-2012-wesearch,0,0.0651736,"nalysis of these Corpus Pre-processing We first pre-process each dataset using the following standardised methodology.3 In the case that the corpus comes with tokenisation and POS information, we strip this and perform automatic preprocessing to ensure consistency in the quality and composition of the tokens/tags. We first apply langid.py (Lui and Baldwin, 2012) — an off-the-shelf language identifier — to each document to detect its majority language. We then extract all documents identified as English for further processing. We next perform sentence tokenisation. In line with the findings of Read et al. (2012a) based on experimentation with a selection of sentence tokenisers over user-generated content, we sentencetokenise with tokenizer.4 Finally, we tokenise and POS tag the datasets using TweetNLP 0.3 (Owoputi et al., 2013). One particularly important property of TweetNLP is that it identifies content such as mentions, URLs, and emoticons that aren’t typically syntactic elements of a sentence. More3 Acknowledging that superior domain-specific approaches exist, e.g. for Wikipedia sentence tokenisation using markup (Flickinger et al., 2010). 4 http://www.cis.uni-muenchen.de/ wastl/misc/ ˜ 5 Specif"
I13-1041,N13-1037,0,0.200271,"which we compare to a reference corpus of edited English text. We first extract out various descriptive statistics from each data type (including the distribution of languages, average sentence length and proportion of out-ofvocabulary words), and then investigate the proportion of grammatical sentences in each, based on a linguistically-motivated parser. We also investigate the relative similarity between different data types. 1 Introduction 2 Various claims have been made about social media text being “noisy” (Java, 2007; Becker et al., 2009; Yin et al., 2012; Preotiuc-Pietro et al., 2012; Eisenstein, 2013, inter alia). However, there has been little effort to quantify the extent to which social media text is more noisy than conventional, edited text types. Moreover, social media comes in many flavours — including microblogs, blogs, and user-generated comments — and research has tended to focus on a specific data source, such as Twitter or blogs. A natural question to ask is how different the textual content of the myriad of social media types are from one another. This is an important first step towards building a generalpurpose suite of social media text processing tools. Most research to dat"
I13-1041,D11-1141,0,0.0294364,"s of making text more tractable to NLP (Eisenstein, 2013). Approaches to normalisation have exploited various sources of information including the context in which a given instance of a lexical variant occurs (Gouws et al., 2011; Han and Baldwin, 2011), although the best results to date have been achieved by automatically discovering lexical variant–standard form pairs from a large Twitter corpus (Han et al., 2012a). This latter approach is particularly appealing because it allows for very fast normalisation, suitable for processing large volumes of text. Conversely, Owoputi et al. (2013) and Ritter et al. (2011) developed part-of-speech (POS) taggers for Twitter that are better able to handle properties of this text type such as the higher outof-vocabulary rate compared to conventional text. Ritter et al. further developed a Twitter shallow parser and named-entity recogniser. Foster et al. (2011) evaluated standard parsers on social media data, and found them to perform particularly poorly on Twitter, but showed that their performance can be improved through a retraining strategy. Another natural question to ask is how similar the characteristics of social media text are to those of other domains. Mo"
I13-1041,flickinger-etal-2010-wikiwoods,0,0.00721079,"next perform sentence tokenisation. In line with the findings of Read et al. (2012a) based on experimentation with a selection of sentence tokenisers over user-generated content, we sentencetokenise with tokenizer.4 Finally, we tokenise and POS tag the datasets using TweetNLP 0.3 (Owoputi et al., 2013). One particularly important property of TweetNLP is that it identifies content such as mentions, URLs, and emoticons that aren’t typically syntactic elements of a sentence. More3 Acknowledging that superior domain-specific approaches exist, e.g. for Wikipedia sentence tokenisation using markup (Flickinger et al., 2010). 4 http://www.cis.uni-muenchen.de/ wastl/misc/ ˜ 5 Specifically, we remove any token tagged as #, @, ˜, U, or E. 358 T WITTER -1 en .406 ja .144 pt .098 es .093 id .031 nl .025 ms .016 ko .015 de .015 it .013 T WITTER -2 en .439 ja .124 es .091 pt .072 id .029 nl .022 ar .019 ko .018 ms .015 fr .015 C OMMENTS en .757 de .034 es .028 fr .023 ru .023 pt .020 pl .012 ar .011 it .011 nl .006 F ORUMS en .914 de .016 es .011 ro .009 it .007 nl .007 fr .006 pl .003 da .002 sv .002 B LOGS en .784 ru .050 fr .025 zh .022 de .019 es .017 ja .010 it .010 pt .009 sv .008 W IKIPEDIA en .998 la .000 de .00"
I13-1041,I11-1100,0,0.0599393,"Missing"
I13-1041,D12-1137,0,0.00547494,"g a generalpurpose suite of social media text processing tools. Most research to date on social media text has used very shallow text processing (such as Background Natural language processing (NLP) has been applied to a wide range of applications on social media, especially Twitter. Numerous studies have attempted to go beyond simple keyword and burstiness models to identify real-world events from Twitter (Benson et al., 2011; Ritter et al., 2012; Petrovic et al., 2012). Recent efforts have considered identifying user location based on the textual content of tweets (Wing and Baldridge, 2011; Roller et al., 2012; Han et al., 2012b) and user metadata (Han et al., 2013). Related work has examined models of the relationships between words and locations for the purpose of identifying and studying regional linguistic variation (Eisenstein et al., 2010; Eisenstein et al., 2012). Given the abundance of non-standard language 356 International Joint Conference on Natural Language Processing, pages 356–364, Nagoya, Japan, 14-18 October 2013. following datasets from across the spectrum of popular social media sites, varying in terms of document length, the number of authors/editors per document, and the level o"
I13-1041,W11-2210,0,0.00887458,"ard NLP tools cannot be immediately applied. Efforts to address this problem have taken two main approaches: modifying social media data to more closely resemble standard text, and building social media-specific tools. Lexical normalisation is the task of converting non-standard forms such as tlkin and touchdooown to their standard forms (talking and touchdown, respectively), in the hopes of making text more tractable to NLP (Eisenstein, 2013). Approaches to normalisation have exploited various sources of information including the context in which a given instance of a lexical variant occurs (Gouws et al., 2011; Han and Baldwin, 2011), although the best results to date have been achieved by automatically discovering lexical variant–standard form pairs from a large Twitter corpus (Han et al., 2012a). This latter approach is particularly appealing because it allows for very fast normalisation, suitable for processing large volumes of text. Conversely, Owoputi et al. (2013) and Ritter et al. (2011) developed part-of-speech (POS) taggers for Twitter that are better able to handle properties of this text type such as the higher outof-vocabulary rate compared to conventional text. Ritter et al. further de"
I13-1041,P11-1038,1,0.696697,"be immediately applied. Efforts to address this problem have taken two main approaches: modifying social media data to more closely resemble standard text, and building social media-specific tools. Lexical normalisation is the task of converting non-standard forms such as tlkin and touchdooown to their standard forms (talking and touchdown, respectively), in the hopes of making text more tractable to NLP (Eisenstein, 2013). Approaches to normalisation have exploited various sources of information including the context in which a given instance of a lexical variant occurs (Gouws et al., 2011; Han and Baldwin, 2011), although the best results to date have been achieved by automatically discovering lexical variant–standard form pairs from a large Twitter corpus (Han et al., 2012a). This latter approach is particularly appealing because it allows for very fast normalisation, suitable for processing large volumes of text. Conversely, Owoputi et al. (2013) and Ritter et al. (2011) developed part-of-speech (POS) taggers for Twitter that are better able to handle properties of this text type such as the higher outof-vocabulary rate compared to conventional text. Ritter et al. further developed a Twitter shallo"
I13-1041,D12-1039,1,0.81612,"ite of social media text processing tools. Most research to date on social media text has used very shallow text processing (such as Background Natural language processing (NLP) has been applied to a wide range of applications on social media, especially Twitter. Numerous studies have attempted to go beyond simple keyword and burstiness models to identify real-world events from Twitter (Benson et al., 2011; Ritter et al., 2012; Petrovic et al., 2012). Recent efforts have considered identifying user location based on the textual content of tweets (Wing and Baldridge, 2011; Roller et al., 2012; Han et al., 2012b) and user metadata (Han et al., 2013). Related work has examined models of the relationships between words and locations for the purpose of identifying and studying regional linguistic variation (Eisenstein et al., 2010; Eisenstein et al., 2012). Given the abundance of non-standard language 356 International Joint Conference on Natural Language Processing, pages 356–364, Nagoya, Japan, 14-18 October 2013. following datasets from across the spectrum of popular social media sites, varying in terms of document length, the number of authors/editors per document, and the level of text editing: on"
I13-1041,P11-1096,0,0.0308249,"first step towards building a generalpurpose suite of social media text processing tools. Most research to date on social media text has used very shallow text processing (such as Background Natural language processing (NLP) has been applied to a wide range of applications on social media, especially Twitter. Numerous studies have attempted to go beyond simple keyword and burstiness models to identify real-world events from Twitter (Benson et al., 2011; Ritter et al., 2012; Petrovic et al., 2012). Recent efforts have considered identifying user location based on the textual content of tweets (Wing and Baldridge, 2011; Roller et al., 2012; Han et al., 2012b) and user metadata (Han et al., 2013). Related work has examined models of the relationships between words and locations for the purpose of identifying and studying regional linguistic variation (Eisenstein et al., 2010; Eisenstein et al., 2012). Given the abundance of non-standard language 356 International Joint Conference on Natural Language Processing, pages 356–364, Nagoya, Japan, 14-18 October 2013. following datasets from across the spectrum of popular social media sites, varying in terms of document length, the number of authors/editors per docu"
I13-1041,C12-1064,1,0.362465,"ite of social media text processing tools. Most research to date on social media text has used very shallow text processing (such as Background Natural language processing (NLP) has been applied to a wide range of applications on social media, especially Twitter. Numerous studies have attempted to go beyond simple keyword and burstiness models to identify real-world events from Twitter (Benson et al., 2011; Ritter et al., 2012; Petrovic et al., 2012). Recent efforts have considered identifying user location based on the textual content of tweets (Wing and Baldridge, 2011; Roller et al., 2012; Han et al., 2012b) and user metadata (Han et al., 2013). Related work has examined models of the relationships between words and locations for the purpose of identifying and studying regional linguistic variation (Eisenstein et al., 2010; Eisenstein et al., 2012). Given the abundance of non-standard language 356 International Joint Conference on Natural Language Processing, pages 356–364, Nagoya, Japan, 14-18 October 2013. following datasets from across the spectrum of popular social media sites, varying in terms of document length, the number of authors/editors per document, and the level of text editing: on"
I13-1177,A00-1031,0,0.153464,"seed model tagger T0 is built on just the high scoring sentences. By applying self-training with revision, a series of new models T1 , T2 , . . . , Tm is constructed where Ti is the tagger after i iterations. The target language tagger, T agger(t), is then the last model, Tm . T agger(s) is trained from manually annotated data Data(s) which is mainly derived from the CoNLL 2006 and CoNLL 2007 Shared Tasks. Using the matching provided by Petrov et al., we map the individual tagsets to the Universal Tagset. We train a supervised POS tagger T agger(s) on the annotated data using the TNT tagger (Brants, 2000). Table 3 shows the source and size of annotated data, and the 5 fold cross-validation accuracy of T agger(s), for each language. We evaluate each T agger(t) using Data(t); results are shown in Table 4. The average tagger per3 NOUN, VERB, ADJ, ADV, PRON (pronouns), DET (determiners and articles), ADP (prepositions and postpositions), NUM (numerals), CONJ (conjunctions), PRT (particles), “.” (punctuation), and X (all other categories, e.g., foreign words, abbreviations). 1245 Source language en da nl pt sv el it es de Baseline en 55.73 75.70 72.40 66.56 47.67 74.50 68.76 72.24 30.28 da 76.17 76"
I13-1177,P11-1061,0,0.31602,"ta is not available for our target language, and we need to choose which data to collect – although further improvements can be obtained using features based on parallel corpora. We then show that if multiple source languages are available, even better accuracy can be obtained by combining information from them. 2 Related work One approach to build an unsupervised POS tagger is to project tag information from a resourcerich source language to a resource-poor target lan1243 International Joint Conference on Natural Language Processing, pages 1243–1249, Nagoya, Japan, 14-18 October 2013. guage. Das and Petrov (2011) and Duong et al. (2013) both achieve state-of-the-art performance on eight European languages using this crosslingual approach. The two approaches are similar in the following respects. First, both project tag information from source to target language, applying some kind of noise reduction along the way: Das and Petrov use high confidence alignments, while Duong et al. use high confidence sentences. Second, both use a semi-supervised method to obtain more labeled data: Das and Petrov use graph based label propagation, while Duong et al. use self-training. Finally, both apply noise reduction/"
I13-1177,P13-2112,1,0.741841,"r target language, and we need to choose which data to collect – although further improvements can be obtained using features based on parallel corpora. We then show that if multiple source languages are available, even better accuracy can be obtained by combining information from them. 2 Related work One approach to build an unsupervised POS tagger is to project tag information from a resourcerich source language to a resource-poor target lan1243 International Joint Conference on Natural Language Processing, pages 1243–1249, Nagoya, Japan, 14-18 October 2013. guage. Das and Petrov (2011) and Duong et al. (2013) both achieve state-of-the-art performance on eight European languages using this crosslingual approach. The two approaches are similar in the following respects. First, both project tag information from source to target language, applying some kind of noise reduction along the way: Das and Petrov use high confidence alignments, while Duong et al. use high confidence sentences. Second, both use a semi-supervised method to obtain more labeled data: Das and Petrov use graph based label propagation, while Duong et al. use self-training. Finally, both apply noise reduction/filtering on the (automa"
I13-1177,W11-3603,0,0.403144,"Missing"
I13-1177,D08-1109,0,0.072329,"Missing"
I13-1177,steinberger-etal-2006-jrc,0,0.0711628,"Missing"
I13-1177,W04-3229,0,0.801596,"Missing"
I13-1177,2005.mtsummit-papers.11,0,0.0294774,"the source or target language role. From Table 4, it seems that taggers perform better if the source and target language are in the same language family. For example, the top four source languages for Danish are Dutch, English, Swedish, and German, and the top two source languages for Portuguese are Italian and Spanish. This confirms the intuition in adding language relatedness features in section 4. Duong et al. (2013) used English as the source language to build taggers for the same eight other languages. The only difference between these two experiments is that Duong et al. used Europarl (Koehn, 2005) data instead of JRC-Acquis. Table 2 also compares the size of parallel data with JRC-Acquis 76.2 73.0 79.6 73.8 50.4 72.2 75.4 74.0 71.8 Europarl 85.6 84.0 86.3 81.0 80.0 81.4 83.3 85.4 83.4 Table 5: Accuracy on JRC-Acquis and Europarl using English as the source language. English as the source language for JRC-Acquis and Europarl. Given that Europarl is larger, higher performance is expected. Table 5 compares the tagger accuracy for each target language using English as the source language, for the two datasets. As expected, the accuracies are higher for Europarl. However, there is a strong"
I13-1177,2009.mtsummit-papers.7,0,0.0300127,"Missing"
I13-1177,petrov-etal-2012-universal,0,0.0786767,"he entropy for each lexical entry is calculated as X H(s) = − p(t|s) × log2 p(t|s) t∈T where T is the set of possible translations of word s, and t is a translation. For each language, we pick a fixed amount of text (1 million words) and calculate the average entropy for all words. 5 Build taggers In this section we construct 72 taggers, using parallel data for 72 language pairs, and then evaluate the performance of each pair. We use an open source unsupervised cross-lingual POS tagger (UMPOS) from Duong et al. (2013), a stateof-the-art system. UMPOS employs the consensus 12 Universal Tagset (Petrov et al., 2012),3 to avoid the problem of transliterating between different tagsets for different languages, and to enable comparison across languages. The input for UMPOS is a tagger for the source language, T agger(s), along with parallel data (s–t). The source language s is tagged using T agger(s), and then the tagged labels are projected to the target language t. Sentences are then ranked, and a seed model tagger T0 is built on just the high scoring sentences. By applying self-training with revision, a series of new models T1 , T2 , . . . , Tm is constructed where Ti is the tagger after i iterations. The"
J09-1005,W03-1812,0,0.710876,"Missing"
J09-1005,W07-1101,0,0.295213,"Missing"
J09-1005,W03-1809,0,0.219673,"Missing"
J09-1005,E06-1042,0,0.259961,"asks, such as semantic parsing and machine translation, which require the identiﬁcation of multiword semantic units. Most recent studies focusing on the identiﬁcation of idiomatic and non-idiomatic tokens either assume the existence of manually annotated data for a supervised classiﬁcation (Patrick and Fletcher 2005; Katz and Giesbrecht 2006), or rely on manually encoded linguistic knowledge about idioms (Uchiyama, Baldwin, and Ishizaki 2005; Hashimoto, Sato, and Utsuro 2006), or even ignore the speciﬁc properties of nonliteral language and rely mainly on general purpose methods for the task (Birke and Sarkar 2006). We propose unsupervised methods that rely on automatically acquired knowledge about idiom types to identify their token occurrences as idiomatic or literal (Section 6). More speciﬁcally, we explore the hypothesis that the type-based knowledge we automatically acquire about an idiomatic expression can be used to determine whether an instance of the expression is used literally or idiomatically (token-based knowledge). Our experimental results show that the performance of the token-based idiom identiﬁcation methods proposed here is comparable to that of existing supervised techniques (Section"
J09-1005,W07-1106,1,0.861217,"Missing"
J09-1005,copestake-etal-2002-multiword,0,0.0168772,"y idiomatic expression as either idiomatic or literal in order to handle a given sequence of words appropriately. For example, a machine translation system must translate held ﬁre differently in The army held their ﬁre and The worshippers held the ﬁre up to the idol. Previous studies focusing on the automatic identiﬁcation of idiom types have often recognized the importance of drawing on their linguistic properties, such as their semantic idiosyncrasy or their restricted ﬂexibility, pointed out earlier. Some researchers have relied on a manual encoding of idiom-speciﬁc knowledge in a lexicon (Copestake et al. 2002; Odijk 2004; Villavicencio et al. 2004), whereas others have presented approaches for the automatic acquisition of more general (hence less distinctive) knowledge from corpora (Smadja 1993; McCarthy, Keller, and Carroll 2003). Recent work that looks into the acquisition of the distinctive properties of idioms has been limited, both in scope and in the evaluation of the methods proposed (Lin 1999; Evert, Heid, and Spranger 2004). Our goal is to develop unsupervised means for the automatic acquisition of lexical, syntactic, and semantic knowledge about a broadly documented class of idiomatic ex"
J09-1005,P94-1038,0,0.0471208,"Missing"
J09-1005,evert-etal-2004-identifying,0,0.0607398,"Missing"
J09-1005,P01-1025,0,0.0483953,"Missing"
J09-1005,E06-1043,1,0.723279,"ure reﬂecting the degree of lexical ﬁxedness for the target pair. We assume that the target pair is lexically ﬁxed to the extent that its PMI deviates from the average PMI of its variants. By our measure, the target pair is considered lexically ﬁxed (i.e., is given a high ﬁxedness score) only if the difference between its PMI value and that of most of its variants—not necessarily all, as in the method of Lin (1999)—is high.4 Our measure calculates this deviation, normalized using the sample’s standard deviation: . PMI(v, n) − PMI Fixednesslex (v, n) = s (2) 3 In an early version of this work (Fazly and Stevenson 2006), only the noun constituent was varied because we expected replacing the verb constituent with a related verb to be more likely to yield another VNIC, as in keep/lose one’s cool, give/get the bird, crack/break the ice (Nunberg, Sag, and Wasow 1994; Grant 2005). Later experiments on the development data showed that variants generated by replacing both constituents, one at a time, produce better results. 4 This way, even if an idiom has a few frequently used variants (e.g., break the ice and crack the ice), it may still be assigned a high ﬁxedness score if most other variants are uncommon. Note"
J09-1005,W07-1102,1,0.82404,"Missing"
J09-1005,P06-2046,0,0.0396623,"Missing"
J09-1005,W06-1203,0,0.72055,"Missing"
J09-1005,E03-1073,0,0.104902,"Missing"
J09-1005,P98-2127,0,0.133518,"easure to determine); (ii) it can only measure the lexical ﬁxedness of idiomatic combinations, and so could not apply to literal combinations. We thus interpret this property statistically in the following way: We expect a lexically ﬁxed verb+noun combination to appear much more frequently than its variants in general. Speciﬁcally, we examine the strength of association between the verb and the noun constituent of a combination (the target expression or its lexical variants) as an indirect cue to its idiomaticity, an approach inspired by Lin (1999). We use the automatically built thesaurus of Lin (1998) to ﬁnd words similar to each constituent, in order to automatically generate variants.2 Variants are generated by replacing either 2 We also replicated our experiments with an automatically built thesaurus created from the British National Corpus (BNC) in a similar fashion, and kindly provided to us by Diana McCarthy. Results were similar, hence we do not report them here. 66 Fazly, Cook, and Stevenson Unsupervised Idiom Identiﬁcation the noun or the verb constituent of a pair with a semantically (and syntactically) similar word.3 Examples of automatically generated variants for the pair spi"
J09-1005,P99-1041,0,0.779836,"erties, such as their semantic idiosyncrasy or their restricted ﬂexibility, pointed out earlier. Some researchers have relied on a manual encoding of idiom-speciﬁc knowledge in a lexicon (Copestake et al. 2002; Odijk 2004; Villavicencio et al. 2004), whereas others have presented approaches for the automatic acquisition of more general (hence less distinctive) knowledge from corpora (Smadja 1993; McCarthy, Keller, and Carroll 2003). Recent work that looks into the acquisition of the distinctive properties of idioms has been limited, both in scope and in the evaluation of the methods proposed (Lin 1999; Evert, Heid, and Spranger 2004). Our goal is to develop unsupervised means for the automatic acquisition of lexical, syntactic, and semantic knowledge about a broadly documented class of idiomatic expressions. Speciﬁcally, we focus on a cross-linguistically prominent class of phrasal idioms which are commonly and productively formed from the combination of a frequent verb and a noun in its direct object position (Cowie, Mackin, and McCaig 1983; Nunberg, Sag, and Wasow 1994; Fellbaum 2002), for example, shoot the breeze, make a face, and push one’s luck. We refer to these as verb+noun idiomat"
J09-1005,W03-1810,0,0.533617,"Missing"
J09-1005,W97-0311,0,0.146044,"Missing"
J09-1005,W97-0207,0,0.186513,"Missing"
J09-1005,ritz-heid-2006-extraction,0,0.0284773,"Missing"
J09-1005,J93-1007,0,0.366692,"army held their ﬁre and The worshippers held the ﬁre up to the idol. Previous studies focusing on the automatic identiﬁcation of idiom types have often recognized the importance of drawing on their linguistic properties, such as their semantic idiosyncrasy or their restricted ﬂexibility, pointed out earlier. Some researchers have relied on a manual encoding of idiom-speciﬁc knowledge in a lexicon (Copestake et al. 2002; Odijk 2004; Villavicencio et al. 2004), whereas others have presented approaches for the automatic acquisition of more general (hence less distinctive) knowledge from corpora (Smadja 1993; McCarthy, Keller, and Carroll 2003). Recent work that looks into the acquisition of the distinctive properties of idioms has been limited, both in scope and in the evaluation of the methods proposed (Lin 1999; Evert, Heid, and Spranger 2004). Our goal is to develop unsupervised means for the automatic acquisition of lexical, syntactic, and semantic knowledge about a broadly documented class of idiomatic expressions. Speciﬁcally, we focus on a cross-linguistically prominent class of phrasal idioms which are commonly and productively formed from the combination of a frequent verb and a noun in"
J09-1005,W07-1104,0,0.0398115,"Missing"
J09-1005,H05-1113,0,0.108608,"Missing"
J09-1005,W06-2405,0,0.0263079,"Missing"
J09-1005,H05-1106,0,0.0466439,"Missing"
J09-1005,W05-1006,0,0.141505,"Missing"
J09-1005,J09-2001,0,\N,Missing
J09-1005,J03-4003,0,\N,Missing
J09-1005,C98-2122,0,\N,Missing
J10-1005,alex-2008-comparing,0,0.0258709,"at these studies rely on limit their applicability to general text. Techniques for inferring lexical properties of neologisms can make use of information that is typically not available in other lexical acquisition tasks—speciﬁcally, knowledge of the processes through which neologisms are formed. Computational work on neologisms has tended to focus on tasks pertaining to a speciﬁc type of neologism, such as identifying and inferring the long form of acronyms (Schwartz and Hearst 2003; Nadeau and Turney 2005; Okazaki and Ananiadou 2006, for example), recognizing loanwords (Baker and Brew 2008; Alex 2008, for example), and identifying and expanding clippings (Means 1988, for example). This study focuses on the tasks of identifying, and inferring the source words of, lexical blends, a common type of neologism, which have been previously unaddressed except for our preliminary work in Cook and Stevenson (2007). In addition to knowledge about a word’s formation process, for many types of neologism, information about its phonological and orthographic content can be used to 146 Cook and Stevenson Automatically Identifying the Source Words of Lexical Blends in English infer aspects of its syntactic"
J10-1005,baker-brew-2008-statistical,0,0.0652231,"Missing"
J10-1005,P08-1065,0,0.0466999,"Missing"
J10-1005,P90-1034,0,0.0660105,"the blend, whereas Lehrer’s subjects were not. 145 Computational Linguistics Volume 36, Number 1 Figure 1 ROC curves for blend identiﬁcation. In future work, we plan to re-examine this task and develop methods speciﬁcally for identifying blends and other types of neologism. 7. Related Work As discussed in Section 1, techniques generally used in the automatic acquisition of syntactic and semantic properties of words are not applicable here, because they use corpus statistics that cannot be accurately estimated for low frequency items, such as the novel lexical blends considered in this study (Hindle 1990; Lapata and Brew 2004; Joanis, Stevenson, and James 2008, for example). Other work has used the context in which an unknown word occurs, along with domain-speciﬁc knowledge, to infer aspects of its meaning and syntax (Granger 1977; Cardie 1993; Hastings and Lytinen 1994, for example). These studies have been able to learn properties of an unknown word from just one usage, or a small number of usages; however, the domain-speciﬁc resources that these studies rely on limit their applicability to general text. Techniques for inferring lexical properties of neologisms can make use of information t"
J10-1005,O97-1002,0,0.0232708,"Missing"
J10-1005,A88-1013,0,0.67328,"ction into the language. Fortunately, linguistic observations regarding neologisms— namely, that they are formed through speciﬁc word formation processes—can give insights for automatically learning their lexical properties. New words come about through a variety of means, including derivational morphology, compounding, and borrowing from another language (Algeo 1980; Bauer 1983; Plag 2003). Computational work on neologisms has largely focused on particular word formation processes, and has exploited information about the formation process to learn aspects of the semantic properties of words (Means 1988; Nadeau and Turney 2005; Baker and Brew 2008, for example). Subtractive word formations—words formed from partial orthographic or phonological content from existing words—have received a fair amount of attention recently in computational linguistics, particularly under the heading of inferring the long form of acronyms, especially in the bio-medical domain (e.g., Schwartz and Hearst 2003; Nadeau and Turney 2005; Okazaki and Ananiadou 2006, for example). Lexical blends—the focus of this study—also known as blends, are another common type of subtractive word formation. Most blends are formed by"
J10-1005,J97-3003,0,0.0680422,"al and orthographic content can be used to 146 Cook and Stevenson Automatically Identifying the Source Words of Lexical Blends in English infer aspects of its syntactic and semantic properties. This is the case for neologisms that are composed of existing words or afﬁxes (e.g., compounds and derivations) or partial orthographic or phonological material from existing words or afﬁxes (e.g., acronyms, clippings, and blends). For example, in the case of part-of-speech tagging, information about the sufﬁx of an unknown word can be used to determine its part-of-speech (Brill 1994; Ratnaparkhi 1996; Mikheev 1997, for example). For the task of inferring the long form of an acronym, the letters which compose a given acronym can be used to determine the most likely long form (Schwartz and Hearst 2003; Nadeau and Turney 2005; Okazaki and Ananiadou 2006, for example). The latter approach to acronyms is somewhat similar to the way in which we use knowledge of the letters that make up a blend to form candidate sets and determine the most likely source words. However, in the case of acronyms, each word in a long form typically contributes only one letter to the acronym, while for blends, a source word usuall"
J10-1005,W06-1605,0,0.0244025,"Missing"
J10-1005,P06-2083,0,0.0754098,"has largely focused on particular word formation processes, and has exploited information about the formation process to learn aspects of the semantic properties of words (Means 1988; Nadeau and Turney 2005; Baker and Brew 2008, for example). Subtractive word formations—words formed from partial orthographic or phonological content from existing words—have received a fair amount of attention recently in computational linguistics, particularly under the heading of inferring the long form of acronyms, especially in the bio-medical domain (e.g., Schwartz and Hearst 2003; Nadeau and Turney 2005; Okazaki and Ananiadou 2006, for example). Lexical blends—the focus of this study—also known as blends, are another common type of subtractive word formation. Most blends are formed by combining a preﬁx of one source word with a sufﬁx of another source word, as in brunch (breakfast and lunch). There may be overlap in the contribution of the source words, as in fantabulous ( fantastic and fabulous). It is also possible that one or both source words are entirely present, for example, gaydar ( gay radar) and jetiquette ( jet etiquette). We refer to blends such as these as simple two-word sequential blends, and focus on thi"
J10-1005,W96-0213,0,0.22287,"out its phonological and orthographic content can be used to 146 Cook and Stevenson Automatically Identifying the Source Words of Lexical Blends in English infer aspects of its syntactic and semantic properties. This is the case for neologisms that are composed of existing words or afﬁxes (e.g., compounds and derivations) or partial orthographic or phonological material from existing words or afﬁxes (e.g., acronyms, clippings, and blends). For example, in the case of part-of-speech tagging, information about the sufﬁx of an unknown word can be used to determine its part-of-speech (Brill 1994; Ratnaparkhi 1996; Mikheev 1997, for example). For the task of inferring the long form of an acronym, the letters which compose a given acronym can be used to determine the most likely long form (Schwartz and Hearst 2003; Nadeau and Turney 2005; Okazaki and Ananiadou 2006, for example). The latter approach to acronyms is somewhat similar to the way in which we use knowledge of the letters that make up a blend to form candidate sets and determine the most likely source words. However, in the case of acronyms, each word in a long form typically contributes only one letter to the acronym, while for blends, a sour"
J10-1005,J04-1003,0,\N,Missing
J10-1005,N04-3012,0,\N,Missing
J11-2007,J90-1003,0,\N,Missing
K15-1012,I13-1177,1,0.9016,"ngual transfer (Zeman et al., 2008; Søgaard, 2011; McDonald et al., 2011; Ma and Xia, 2014). In this setting, a delexicalized parser is trained on a resource-rich source language, and is then applied directly to a resource-poor target language. The only requirement here is that the source and target languages are POS tagged must use the same tagset. This assumption is pertinent for resourcepoor languages since it is relatively quick to manually POS tag the data. Moreover, there are many reports of high accuracy POS tagging for resourcepoor languages (Duong et al., 2014; Garrette et al., 2013; Duong et al., 2013b). The cross-lingual delexicalized approach has been shown to significantly outperform unsupervised approaches (McDonald et al., 2011; Ma and Xia, 2014). Parallel data can be used to boost the performance of a cross-lingual parser (McDonald et al., 2011; Ma and Xia, 2014). However, parallel data may be hard to acquire for truly resource-poor languages.1 Accordingly, we propose a method to improve the performance of a cross-lingual delexicalized parser using only monolingual data. Our approach is based on augmenting the delexicalized parser using syntactic word embeddings. Words from both sour"
K15-1012,P14-2133,0,0.0176224,"2014). However, parallel data may be hard to acquire for truly resource-poor languages.1 Accordingly, we propose a method to improve the performance of a cross-lingual delexicalized parser using only monolingual data. Our approach is based on augmenting the delexicalized parser using syntactic word embeddings. Words from both source and target language are mapped to a shared low-dimensional space based on their syntactic context, without recourse to parallel data. While prior work has struggled to efficiently incorporate word embedding information into the parsing model (Bansal et al., 2014; Andreas and Klein, 2014; Chen et al., 2014), we present a method for doing so using a neural netCross-lingual transfer has been shown to produce good results for dependency parsing of resource-poor languages. Although this avoids the need for a target language treebank, most approaches have still used large parallel corpora. However, parallel data is scarce for low-resource languages, and we report a new method that does not need parallel data. Our method learns syntactic word embeddings that generalise over the syntactic contexts of a bilingual vocabulary, and incorporates these into a neural network parser. We sho"
K15-1012,P13-2112,1,0.9306,"ngual transfer (Zeman et al., 2008; Søgaard, 2011; McDonald et al., 2011; Ma and Xia, 2014). In this setting, a delexicalized parser is trained on a resource-rich source language, and is then applied directly to a resource-poor target language. The only requirement here is that the source and target languages are POS tagged must use the same tagset. This assumption is pertinent for resourcepoor languages since it is relatively quick to manually POS tag the data. Moreover, there are many reports of high accuracy POS tagging for resourcepoor languages (Duong et al., 2014; Garrette et al., 2013; Duong et al., 2013b). The cross-lingual delexicalized approach has been shown to significantly outperform unsupervised approaches (McDonald et al., 2011; Ma and Xia, 2014). Parallel data can be used to boost the performance of a cross-lingual parser (McDonald et al., 2011; Ma and Xia, 2014). However, parallel data may be hard to acquire for truly resource-poor languages.1 Accordingly, we propose a method to improve the performance of a cross-lingual delexicalized parser using only monolingual data. Our approach is based on augmenting the delexicalized parser using syntactic word embeddings. Words from both sour"
K15-1012,P14-2131,0,0.131008,"l., 2011; Ma and Xia, 2014). However, parallel data may be hard to acquire for truly resource-poor languages.1 Accordingly, we propose a method to improve the performance of a cross-lingual delexicalized parser using only monolingual data. Our approach is based on augmenting the delexicalized parser using syntactic word embeddings. Words from both source and target language are mapped to a shared low-dimensional space based on their syntactic context, without recourse to parallel data. While prior work has struggled to efficiently incorporate word embedding information into the parsing model (Bansal et al., 2014; Andreas and Klein, 2014; Chen et al., 2014), we present a method for doing so using a neural netCross-lingual transfer has been shown to produce good results for dependency parsing of resource-poor languages. Although this avoids the need for a target language treebank, most approaches have still used large parallel corpora. However, parallel data is scarce for low-resource languages, and we report a new method that does not need parallel data. Our method learns syntactic word embeddings that generalise over the syntactic contexts of a bilingual vocabulary, and incorporates these into a neur"
K15-1012,D14-1096,1,0.840123,"idea of delexicalized parsing and cross-lingual transfer (Zeman et al., 2008; Søgaard, 2011; McDonald et al., 2011; Ma and Xia, 2014). In this setting, a delexicalized parser is trained on a resource-rich source language, and is then applied directly to a resource-poor target language. The only requirement here is that the source and target languages are POS tagged must use the same tagset. This assumption is pertinent for resourcepoor languages since it is relatively quick to manually POS tag the data. Moreover, there are many reports of high accuracy POS tagging for resourcepoor languages (Duong et al., 2014; Garrette et al., 2013; Duong et al., 2013b). The cross-lingual delexicalized approach has been shown to significantly outperform unsupervised approaches (McDonald et al., 2011; Ma and Xia, 2014). Parallel data can be used to boost the performance of a cross-lingual parser (McDonald et al., 2011; Ma and Xia, 2014). However, parallel data may be hard to acquire for truly resource-poor languages.1 Accordingly, we propose a method to improve the performance of a cross-lingual delexicalized parser using only monolingual data. Our approach is based on augmenting the delexicalized parser using synt"
K15-1012,P13-1057,0,0.0152204,"ed parsing and cross-lingual transfer (Zeman et al., 2008; Søgaard, 2011; McDonald et al., 2011; Ma and Xia, 2014). In this setting, a delexicalized parser is trained on a resource-rich source language, and is then applied directly to a resource-poor target language. The only requirement here is that the source and target languages are POS tagged must use the same tagset. This assumption is pertinent for resourcepoor languages since it is relatively quick to manually POS tag the data. Moreover, there are many reports of high accuracy POS tagging for resourcepoor languages (Duong et al., 2014; Garrette et al., 2013; Duong et al., 2013b). The cross-lingual delexicalized approach has been shown to significantly outperform unsupervised approaches (McDonald et al., 2011; Ma and Xia, 2014). Parallel data can be used to boost the performance of a cross-lingual parser (McDonald et al., 2011; Ma and Xia, 2014). However, parallel data may be hard to acquire for truly resource-poor languages.1 Accordingly, we propose a method to improve the performance of a cross-lingual delexicalized parser using only monolingual data. Our approach is based on augmenting the delexicalized parser using syntactic word embeddings."
K15-1012,J92-4003,0,0.0677996,"next subsection, we review some cross-lingual word embedding methods and propose our syntactic word embeddings. Section 4 empirically compares these word embeddings when incorporated into a dependency parser. 3.1 Tu mascota Pronoun Noun The weather Det Noun parece adorable Verb Adj is horrible today Verb Adj Noun Figure 1: Examples of the syntactic word embeddings for Spanish and English. In each case, the highlighted tags are predicted by the highlighted word. The Spanish sentence means “your pet looks lovely”. build cross-lingual word representations using a variant of the Brown clusterer (Brown et al., 1992) applied to parallel data. Bansal et al. (2014) and Turian et al. (2010) showed that for monolingual dependency parsing, the simple Brown clustering based algorithm outperformed many word embedding techniques. In this paper we compare our approach to forming cross-lingual word embeddings with those of both Hermann and Blunsom (2014a) and T¨ackstr¨om et al. (2012). Cross-lingual word embeddings We review methods that can represent words in both source and target languages in a lowdimensional space. There are many benefits of using a low-dimensional space. Instead of the traditional “one-hot” re"
K15-1012,W06-2920,0,0.0231945,"e start by building syntactic word embeddings between source and target languages as shown in algorithm 1. Next we incorporate syntactic word embeddings using the algorithm proposed in Section 3.3. The third step is to substitute source- with target-language syntactic word embeddings. Finally, we parse the target language using this substituted model. In this way, the model will recognize lexical items for the target language. 4 Experiments on CoNLL Data Experiments We test our method of incorporating syntactic word embeddings into a neural network parser, for both the existing CoNLL dataset (Buchholz and Marsi, 2006; Nivre et al., 2007) and the newlyreleased Universal Dependency Treebank (Nivre et al., 2015). We employed the Unlabeled Attachment Score (UAS) without punctuation for comparison with prior work on the CoNLL dataset. Where possible we also report Labeled Attachment Score (LAS) without punctuation. We use English as the source language for this experiment. 5 This is a consequence of only training the parser on the source language. If we were to update embeddings during parser training this would mean they no longer align with the target language embeddings. 6 117 All performance comparisons in"
K15-1012,W12-1909,1,0.714748,"9), text classification (Ozg¨ and G¨ung¨or, 2010), and question answering (Cui et al., 2005). Supervised approaches to dependency parsing have been very successful for many resource-rich languages, where relatively large treebanks are available (McDonald et al., 2005a). However, for many languages, annotated treebanks are not available, and are very costly to create (B¨ohmov´a et al., 2001). This motivates the development of unsupervised approaches that can make use of unannotated, monolingual data. However, purely unsupervised approaches have relatively low accuracy (Klein and Manning, 2004; Gelling et al., 2012). 1 Note that most research in this area (as do we) evaluates on simulated low-resource languages, through selective use of data in high-resource languages. Consequently parallel data is plentiful, however this is often not the case in the real setting, e.g., for Tagalog, where only scant parallel data exists (e.g., dictionaries, Wikipedia and the Bible). 113 Proceedings of the 19th Conference on Computational Language Learning, pages 113–122, c Beijing, China, July 30-31, 2015. 2015 Association for Computational Linguistics language. Delexicalized parsing relies on the fact that parts-of-spee"
K15-1012,H05-1091,0,0.0444913,"eed parallel data. Our method learns syntactic word embeddings that generalise over the syntactic contexts of a bilingual vocabulary, and incorporates these into a neural network parser. We show empirical improvements over a baseline delexicalised parser on both the CoNLL and Universal Dependency Treebank datasets. We analyse the importance of the source languages, and show that combining multiple source-languages leads to a substantial improvement. 1 Introduction Dependency parsing is a crucial component of many natural language processing (NLP) systems for tasks such as relation extraction (Bunescu and Mooney, 2005), statistical machine transla¨ ur tion (Xu et al., 2009), text classification (Ozg¨ and G¨ung¨or, 2010), and question answering (Cui et al., 2005). Supervised approaches to dependency parsing have been very successful for many resource-rich languages, where relatively large treebanks are available (McDonald et al., 2005a). However, for many languages, annotated treebanks are not available, and are very costly to create (B¨ohmov´a et al., 2001). This motivates the development of unsupervised approaches that can make use of unannotated, monolingual data. However, purely unsupervised approaches h"
K15-1012,P14-1006,0,0.0667528,"xamples of the syntactic word embeddings for Spanish and English. In each case, the highlighted tags are predicted by the highlighted word. The Spanish sentence means “your pet looks lovely”. build cross-lingual word representations using a variant of the Brown clusterer (Brown et al., 1992) applied to parallel data. Bansal et al. (2014) and Turian et al. (2010) showed that for monolingual dependency parsing, the simple Brown clustering based algorithm outperformed many word embedding techniques. In this paper we compare our approach to forming cross-lingual word embeddings with those of both Hermann and Blunsom (2014a) and T¨ackstr¨om et al. (2012). Cross-lingual word embeddings We review methods that can represent words in both source and target languages in a lowdimensional space. There are many benefits of using a low-dimensional space. Instead of the traditional “one-hot” representation with the number of dimensions equal to vocabulary size, words are represented using much fewer dimensions. This confers the benefit of generalising over the vocabulary to alleviate issues of data sparsity, through learning representations encoding lexical relations such as synonymy. Several approaches have sought to le"
K15-1012,D14-1082,0,0.0364157,"terising dependency relations. 115 Algorithm 1 Syntactic word embedding 1: Match the source and target tagsets to the Universal Tagset. 2: Extract word n-gram sequences for both the source and target language. 3: For each n-gram, keep the middle word, and replace the other words by their POS. 4: Train a skip-gram word embedding model on the resulting list of word and POS sequences from both the source and target language SOFT-MAX LAYER W2 HIDDEN LAYER W1 WORDS POS TAGS Eword ARC LABELS Epos Earc MAPPING LAYER CONFIGURATION (STACK, QUEUE, ARCS) Figure 2: Neural Network Parser Architecture from Chen and Manning (2014) We assume the same POS tagset is used for both the source and target language,2 and learn word embeddings for each word type in both languages into the same syntactic space of nearby POS contexts. In particular, we develop a predictive model of the tags to the left and right of a word, as illustrated in Figure 1 and outlined in Algorithm 1. Figure 1 illustrates two training contexts extracted from our English source and Spanish target language, where the highlighted fragments reflect the tags being predicted around each focus word. Note that for this example, the POS contexts for the English"
K15-1012,C14-1078,0,0.0261532,"Missing"
K15-1012,P04-1061,0,0.111529,"¨ ur tion (Xu et al., 2009), text classification (Ozg¨ and G¨ung¨or, 2010), and question answering (Cui et al., 2005). Supervised approaches to dependency parsing have been very successful for many resource-rich languages, where relatively large treebanks are available (McDonald et al., 2005a). However, for many languages, annotated treebanks are not available, and are very costly to create (B¨ohmov´a et al., 2001). This motivates the development of unsupervised approaches that can make use of unannotated, monolingual data. However, purely unsupervised approaches have relatively low accuracy (Klein and Manning, 2004; Gelling et al., 2012). 1 Note that most research in this area (as do we) evaluates on simulated low-resource languages, through selective use of data in high-resource languages. Consequently parallel data is plentiful, however this is often not the case in the real setting, e.g., for Tagalog, where only scant parallel data exists (e.g., dictionaries, Wikipedia and the Bible). 113 Proceedings of the 19th Conference on Computational Language Learning, pages 113–122, c Beijing, China, July 30-31, 2015. 2015 Association for Computational Linguistics language. Delexicalized parsing relies on the"
K15-1012,2005.mtsummit-papers.11,0,0.0892953,"Missing"
K15-1012,petrov-etal-2012-universal,0,0.0247689,"Missing"
K15-1012,P14-1126,0,0.13511,"r for Unsupervised Dependency Parsing Without Parallel Data Long Duong,12 Trevor Cohn,1 Steven Bird,1 and Paul Cook3 1 Department of Computing and Information Systems, University of Melbourne 2 National ICT Australia, Victoria Research Laboratory 3 Faculty of Computer Science, University of New Brunswick lduong@student.unimelb.edu.au {t.cohn,sbird}@unimelb.edu.au paul.cook@unb.ca Abstract Most recent work on unsupervised dependency parsing for low-resource languages has used the idea of delexicalized parsing and cross-lingual transfer (Zeman et al., 2008; Søgaard, 2011; McDonald et al., 2011; Ma and Xia, 2014). In this setting, a delexicalized parser is trained on a resource-rich source language, and is then applied directly to a resource-poor target language. The only requirement here is that the source and target languages are POS tagged must use the same tagset. This assumption is pertinent for resourcepoor languages since it is relatively quick to manually POS tag the data. Moreover, there are many reports of high accuracy POS tagging for resourcepoor languages (Duong et al., 2014; Garrette et al., 2013; Duong et al., 2013b). The cross-lingual delexicalized approach has been shown to significan"
K15-1012,P11-2120,0,0.0252529,"Missing"
K15-1012,P05-1012,0,0.0170725,"se the importance of the source languages, and show that combining multiple source-languages leads to a substantial improvement. 1 Introduction Dependency parsing is a crucial component of many natural language processing (NLP) systems for tasks such as relation extraction (Bunescu and Mooney, 2005), statistical machine transla¨ ur tion (Xu et al., 2009), text classification (Ozg¨ and G¨ung¨or, 2010), and question answering (Cui et al., 2005). Supervised approaches to dependency parsing have been very successful for many resource-rich languages, where relatively large treebanks are available (McDonald et al., 2005a). However, for many languages, annotated treebanks are not available, and are very costly to create (B¨ohmov´a et al., 2001). This motivates the development of unsupervised approaches that can make use of unannotated, monolingual data. However, purely unsupervised approaches have relatively low accuracy (Klein and Manning, 2004; Gelling et al., 2012). 1 Note that most research in this area (as do we) evaluates on simulated low-resource languages, through selective use of data in high-resource languages. Consequently parallel data is plentiful, however this is often not the case in the real s"
K15-1012,N12-1052,0,0.17988,"Missing"
K15-1012,H05-1066,0,0.0270963,"Missing"
K15-1012,D11-1006,0,0.194472,"r is trained on a resource-rich source language, and is then applied directly to a resource-poor target language. The only requirement here is that the source and target languages are POS tagged must use the same tagset. This assumption is pertinent for resourcepoor languages since it is relatively quick to manually POS tag the data. Moreover, there are many reports of high accuracy POS tagging for resourcepoor languages (Duong et al., 2014; Garrette et al., 2013; Duong et al., 2013b). The cross-lingual delexicalized approach has been shown to significantly outperform unsupervised approaches (McDonald et al., 2011; Ma and Xia, 2014). Parallel data can be used to boost the performance of a cross-lingual parser (McDonald et al., 2011; Ma and Xia, 2014). However, parallel data may be hard to acquire for truly resource-poor languages.1 Accordingly, we propose a method to improve the performance of a cross-lingual delexicalized parser using only monolingual data. Our approach is based on augmenting the delexicalized parser using syntactic word embeddings. Words from both source and target language are mapped to a shared low-dimensional space based on their syntactic context, without recourse to parallel dat"
K15-1012,N13-1126,0,0.143921,"Missing"
K15-1012,P10-1040,0,0.0215477,"nd propose our syntactic word embeddings. Section 4 empirically compares these word embeddings when incorporated into a dependency parser. 3.1 Tu mascota Pronoun Noun The weather Det Noun parece adorable Verb Adj is horrible today Verb Adj Noun Figure 1: Examples of the syntactic word embeddings for Spanish and English. In each case, the highlighted tags are predicted by the highlighted word. The Spanish sentence means “your pet looks lovely”. build cross-lingual word representations using a variant of the Brown clusterer (Brown et al., 1992) applied to parallel data. Bansal et al. (2014) and Turian et al. (2010) showed that for monolingual dependency parsing, the simple Brown clustering based algorithm outperformed many word embedding techniques. In this paper we compare our approach to forming cross-lingual word embeddings with those of both Hermann and Blunsom (2014a) and T¨ackstr¨om et al. (2012). Cross-lingual word embeddings We review methods that can represent words in both source and target languages in a lowdimensional space. There are many benefits of using a low-dimensional space. Instead of the traditional “one-hot” representation with the number of dimensions equal to vocabulary size, wor"
K15-1012,P12-1066,0,0.143232,"Missing"
K15-1012,N09-1028,0,0.00887976,"at generalise over the syntactic contexts of a bilingual vocabulary, and incorporates these into a neural network parser. We show empirical improvements over a baseline delexicalised parser on both the CoNLL and Universal Dependency Treebank datasets. We analyse the importance of the source languages, and show that combining multiple source-languages leads to a substantial improvement. 1 Introduction Dependency parsing is a crucial component of many natural language processing (NLP) systems for tasks such as relation extraction (Bunescu and Mooney, 2005), statistical machine transla¨ ur tion (Xu et al., 2009), text classification (Ozg¨ and G¨ung¨or, 2010), and question answering (Cui et al., 2005). Supervised approaches to dependency parsing have been very successful for many resource-rich languages, where relatively large treebanks are available (McDonald et al., 2005a). However, for many languages, annotated treebanks are not available, and are very costly to create (B¨ohmov´a et al., 2001). This motivates the development of unsupervised approaches that can make use of unannotated, monolingual data. However, purely unsupervised approaches have relatively low accuracy (Klein and Manning, 2004; Ge"
K15-1012,I08-3008,0,0.0208359,"ds for improving the delexicalized parser using syntactic word embeddings. Section 4 describes experiments on the CoNLL dataset and Universal Dependency Treebank. Section 5 presents methods for selecting the best source language given a target language. 2 Unsupervised Cross-lingual Dependency Parsing There are two main approaches for building dependency parsers for resource-poor languages without using target-language treebanks: delexicalized parsing and projection (Hwa et al., 2005; Ma and Xia, 2014; T¨ackstr¨om et al., 2013; McDonald et al., 2011). The delexicalized approach was proposed by Zeman et al. (2008). They built a delexicalized parser from a treebank in a resource-rich source language. This parser can be trained using any standard supervised approach, but without including any lexical features, then applied directly to parse sentences from the resource-poor 3 Improving Delexicalized Parsing We propose a novel method to improve the performance of a delexicalized cross-lingual parser without recourse to parallel data. Our method uses no additional resources and is designed to com114 plement other methods. The approach is based on syntactic word embeddings where a word is represented as a lo"
K15-1012,D13-1141,0,\N,Missing
K15-1012,D07-1096,0,\N,Missing
L16-1042,E09-1013,0,0.0301409,"BNC. Here we too consider known-similarity corpora from the BNC. We additionally construct corpora that are known to differ specifically with respect to genre, topic, time, and region, to examine the extent to which measures of corpus similarity can detect these types of differences between corpora. We further consider known-similarity corpora that are much larger than those used by Kilgarriff (2001) to consider the effect of corpus size on measures of corpus similarity. Our findings are somewhat surprising. Although topic modelling has been successfully applied to a wide range of NLP tasks (Brody and Lapata, 2009; Haghighi and Vanderwende, 2009; Hardisty et al., 2010; Lau et al., 2014), we find that, overall, our topic modelling-based approach to measuring corpus similarity is not an improvement over the χ2 method of Kilgarriff (2001). The current best approach to measuring corpus similarity thus remains χ2 . 2. 2.1. Related Work Comparing Corpora Corpora can be compared in a variety of ways. In perhaps the simplest case, the most frequent words, possibly restricted to a particular part-of-speech, can be compared for two corpora (Sch¨afer and Bildhauer, 2013). Lists of keywords — i.e., words that are"
L16-1042,N09-1041,0,0.0423929,"er known-similarity corpora from the BNC. We additionally construct corpora that are known to differ specifically with respect to genre, topic, time, and region, to examine the extent to which measures of corpus similarity can detect these types of differences between corpora. We further consider known-similarity corpora that are much larger than those used by Kilgarriff (2001) to consider the effect of corpus size on measures of corpus similarity. Our findings are somewhat surprising. Although topic modelling has been successfully applied to a wide range of NLP tasks (Brody and Lapata, 2009; Haghighi and Vanderwende, 2009; Hardisty et al., 2010; Lau et al., 2014), we find that, overall, our topic modelling-based approach to measuring corpus similarity is not an improvement over the χ2 method of Kilgarriff (2001). The current best approach to measuring corpus similarity thus remains χ2 . 2. 2.1. Related Work Comparing Corpora Corpora can be compared in a variety of ways. In perhaps the simplest case, the most frequent words, possibly restricted to a particular part-of-speech, can be compared for two corpora (Sch¨afer and Bildhauer, 2013). Lists of keywords — i.e., words that are marked with respect to frequency"
L16-1042,D10-1028,0,0.0316568,"the BNC. We additionally construct corpora that are known to differ specifically with respect to genre, topic, time, and region, to examine the extent to which measures of corpus similarity can detect these types of differences between corpora. We further consider known-similarity corpora that are much larger than those used by Kilgarriff (2001) to consider the effect of corpus size on measures of corpus similarity. Our findings are somewhat surprising. Although topic modelling has been successfully applied to a wide range of NLP tasks (Brody and Lapata, 2009; Haghighi and Vanderwende, 2009; Hardisty et al., 2010; Lau et al., 2014), we find that, overall, our topic modelling-based approach to measuring corpus similarity is not an improvement over the χ2 method of Kilgarriff (2001). The current best approach to measuring corpus similarity thus remains χ2 . 2. 2.1. Related Work Comparing Corpora Corpora can be compared in a variety of ways. In perhaps the simplest case, the most frequent words, possibly restricted to a particular part-of-speech, can be compared for two corpora (Sch¨afer and Bildhauer, 2013). Lists of keywords — i.e., words that are marked with respect to frequency in one corpus compared"
L16-1042,P14-1025,1,0.843838,"ly construct corpora that are known to differ specifically with respect to genre, topic, time, and region, to examine the extent to which measures of corpus similarity can detect these types of differences between corpora. We further consider known-similarity corpora that are much larger than those used by Kilgarriff (2001) to consider the effect of corpus size on measures of corpus similarity. Our findings are somewhat surprising. Although topic modelling has been successfully applied to a wide range of NLP tasks (Brody and Lapata, 2009; Haghighi and Vanderwende, 2009; Hardisty et al., 2010; Lau et al., 2014), we find that, overall, our topic modelling-based approach to measuring corpus similarity is not an improvement over the χ2 method of Kilgarriff (2001). The current best approach to measuring corpus similarity thus remains χ2 . 2. 2.1. Related Work Comparing Corpora Corpora can be compared in a variety of ways. In perhaps the simplest case, the most frequent words, possibly restricted to a particular part-of-speech, can be compared for two corpora (Sch¨afer and Bildhauer, 2013). Lists of keywords — i.e., words that are marked with respect to frequency in one corpus compared to another — compu"
L16-1042,C10-1078,0,0.0670489,"Missing"
L16-1042,D14-1162,0,0.0831919,"Missing"
L16-1042,read-etal-2012-wesearch,0,0.0241819,"ere the number of KSC varies between 9 and 11 (as shown in Table 1). 3.2.1. K ILGARRIFF KSC We evaluated each of our measures on a subset of the KSC used in Kilgarriff (2001), referred to as K ILGARRIFF, comprising the text type pairs indicated in Table 1. Within each KSC set, the number of words in the corpora varies between 111k and 114k. The three letter codes refer to subsets of the BNC, and are described in Kilgarriff (2001). 3.2.2. WDC KSC The WeSearch Data Collection (“WDC”) is a collection of user-generated text designed to capture differences in both subject matter and writing style (Read et al., 2012). It contains text on the separate topics of NLP and the Linux operating system taken from blogs, Wikipedia, software reviews and forums. Using Linux as a fixed topic and varying the writing style by varying the source through blogs, reviews and forums, we created three KSC with differences in genre. Then, using forums as a fixed source, we created additional KSC by mixing the topics of NLP and Linux. Table 2 shows details of each WDC KSC we constructed. 3.2.3. G IGAWORD Corpus KSC The Gigaword corpus (Parker et al., 2009, “G IGAWORD”) is a collection of date-stamped newswire text. We used the"
L16-1042,baroni-bernardini-2004-bootcat,0,\N,Missing
L16-1474,I13-1041,1,0.783593,"informative for this task. The categories that we predict could serve as a preliminary, automatically-generated source of lexical knowledge about out-of-vocabulary terms. Furthermore, we show that this approach can be adapted to give a semi-automated method for identifying out-of-vocabulary terms of a particular category, automotive named entities, that is of particular interest to us. Keywords: Lexical acquisition, social media text, out-of-vocabulary words 1. Domain-specific OOV Classification 2. Out-of-vocabulary terms are more common in social media text than more-conventional text types (Baldwin et al., 2013). Moreover, many domain-specific technical terms are not included in general-purpose dictionaries and lexical resources. Domain-specific social media corpora are therefore particularly rife with out-of-vocabulary terms. Many natural language processing (NLP) systems for tasks including sentiment analysis and question answering rely on lexical knowledge. In the case that a text being processed contains out-of-vocabulary terms, system performance suffers because lexical knowledge is not available for these words. Much research in NLP has therefore focused on lexical acquisition — automatically l"
L16-1474,C92-2082,0,0.40615,"d in general-purpose dictionaries and lexical resources. Domain-specific social media corpora are therefore particularly rife with out-of-vocabulary terms. Many natural language processing (NLP) systems for tasks including sentiment analysis and question answering rely on lexical knowledge. In the case that a text being processed contains out-of-vocabulary terms, system performance suffers because lexical knowledge is not available for these words. Much research in NLP has therefore focused on lexical acquisition — automatically learning syntactic or semantic properties of words from corpora (Hearst, 1992; Lin, 1998; Turney and Littman, 2003, for example). In this work we focus specifically on out-of-vocabulary (OOV) terms in web forum text from the automotive domain. The focus on the automotive domain is motivated by the business interests of VerticalScope, Inc. (the industrial collaborator in this research) in being able to moreintelligently analyze this text. VerticalScope, Inc. is a Canadian company that owns and operates one of the most highly visited automotive networks of online forums. The goal of this research is to automatically classify OOVs as one of a predefined set of domain- and"
L16-1474,P98-2127,0,0.0480249,"urpose dictionaries and lexical resources. Domain-specific social media corpora are therefore particularly rife with out-of-vocabulary terms. Many natural language processing (NLP) systems for tasks including sentiment analysis and question answering rely on lexical knowledge. In the case that a text being processed contains out-of-vocabulary terms, system performance suffers because lexical knowledge is not available for these words. Much research in NLP has therefore focused on lexical acquisition — automatically learning syntactic or semantic properties of words from corpora (Hearst, 1992; Lin, 1998; Turney and Littman, 2003, for example). In this work we focus specifically on out-of-vocabulary (OOV) terms in web forum text from the automotive domain. The focus on the automotive domain is motivated by the business interests of VerticalScope, Inc. (the industrial collaborator in this research) in being able to moreintelligently analyze this text. VerticalScope, Inc. is a Canadian company that owns and operates one of the most highly visited automotive networks of online forums. The goal of this research is to automatically classify OOVs as one of a predefined set of domain- and applicatio"
L16-1474,I11-1062,0,0.0306763,"kagvjfcjfx, kzvddzfv52 heyyaa, lol2 youll, genericfor Table 1: The categories of OOVs, along with an explanation of, and examples of, each. 3.1. Character N -grams Certain character n-grams are more frequent in some categories than others. For example, spelling errors and nonstandard social media forms contain character sequences that are uncommon in standard English due to character deletion or repetition. In drug names, character sequences such as word-final ne and an (e.g., as in ketamine and niaspan) are particularly common. Character n-grams are often applied in language identification (Lui and Baldwin, 2011, for example) and could therefore be particularly informative for identifying foreign terms. Our first set of features therefore consists of the character n-grams in a given OOV, for n = 1–3. 3.2. Character N -gram Models Many NE - AUTO OOVs contain character sequences that are rare in standard English (e.g., ls2 an engine model name). Moreover, many FOREIGN OOVs contain character sequences that are uncommon in English. We construct character-level bigram and trigram models from corpora of English, German, and Spanish. For English we used the Brown Corpus (Francis and Kucera, 1979), while for"
L16-1474,P14-5010,0,0.00289801,"s from a variety of English web forums such as talkford.com, watchfreeks.com, and samsunggalaxyreviews.com corresponding to verticals such as automotive, collectibles, and technology. Wikipedia A dump of English Wikipedia from 1 September 2015. Twitter A sample of English tweets collected from the Twitter Streaming APIs3 from November 2014 to March 2015. The Wikipedia corpus was pre-processed to remove meta-data, such as wiki markup, using WikiExtractor.4 The forum corpus was similarly pre-processed to remove forum tags. The Wikipedia corpus was tokenized using the Stanford CoreNLP tokenizer (Manning et al., 2014). The forum and Twitter corpora were tokenized using a tokenizer developed for tweets, adapted from (O’Connor et al., 2010). Table 2 shows the number of documents, tokens, and OOVs from our dataset, in each corpus. These corpora were used to calculate the frequency features (Section 3.3.). The forums corpus was used for training word2vec to calculate the word embeddings features (Section 3.4.). 3 4 https://dev.twitter.com/ https://github.com/attardi/wikiextractor Experimental Setup 10x10-fold stratified cross-validation experiments were carried out on the dataset of OOVs using the features des"
L16-1474,N13-1090,0,0.0114283,"embedding (which happens when the OOV does not occur in the corpus, or when it does occur in the corpus but has frequency below a threshold), we represent it as the average of the vectors for all other OOVs that do have word embeddings. We use the following settings for word2vec: the skipgram model, a vector dimensionality of 200, a window size of 5, and a minimum frequency of 10. To select these parameters, we trained word2vec with a variety of parameter settings for the model (skipgram and cbow), number of dimensions, and window size. We evaluated the vectors obtained on the analogy task of Mikolov et al. (2013); the selected parameters gave the best results on this task. 3.5. Frequency We hypothesize that categories such as SLANG and SPELLING - ERROR will tend to be infrequent in well-edited text, and relatively frequent in text types such as social media text. Moreover, categories such as NE - AUTO and AUTO will tend to be relatively frequent in text from that domain — whether social media text or not — and relatively infrequent in other domains. We obtain corpora corresponding to a variety of text types (described in Section 4.1. below). For these features, for a given OOV, we calculate its freque"
L18-1653,E17-1088,0,0.0313684,"identification are able to recognize which portions of text correspond to a particular language (Jurgens et al., 2017). Extending such methods to recognize Indigenous languages, and Mi’kmaq in particular, is an important area of future work. Future work on Mi’kmaq language identification should also take into account the variation in Mi’kmaq writing systems. In Section 3 we have identified some potential future directions with respect to language modelling. Recent approaches to low-resource language modelling have incorporated cross-lingual word embeddings learned from bilingual dictionaries (Adams et al., 2017), which are available for Mi’kmaq. We also intend to evaluate such approaches to language modelling in future work. Finally, because of the morphological complexity of Mi’kmaq, we are particularly interested in morphological analyzers for Mi’kmaq. As a first step, we intend to consider evaluating unsupervised approaches to morphological analysis (Smit et al., 2014) on Mi’kmaq. 4142 5. Acknowledgements We thank Starlit Simon for her annotation work which enabled the evaluation of the language identification. We thank David Perley for his input on this research. This research is financially supp"
L18-1653,baroni-bernardini-2004-bootcat,0,0.656179,"ment/2011/as-sa/98-314-x/ 98-314-x2011003_3-eng.cfm 3 http://crubadan.org/ 4 The number of writing systems included in this project has since climbed to over 2000. 5 The Truth and Reconciliation Commission of Canada’s calls to action also state that “The preservation, revitalization, and strengthening of Aboriginal languages and cultures are best managed by Aboriginal people and communities.” None of the authors of this work are Aboriginal people. We have consulted with the Mi’kmaq-Wolastoqey Centre at the University of New Brunswick in carrying out this research. 4139 2. A Mi’kmaq Web Corpus Baroni and Bernardini (2004) describe an approach to automatically creating topically-focused corpora from the web, whereby tuples are randomly formed from a user-created keyword list for the topic of interest, and these tuples are then sent as queries to a search engine. The top-n results for these queries are downloaded, and then post-processed to remove mark-up and boilerplate content, documents in unwanted languages, and duplicate documents, with the final result being a topically-focused corpus. This approach to corpus construction remains widely-used, and is incorporated into commercial lexicographical tools (Baron"
L18-1653,D14-1069,0,0.0500035,"Canada 2011 Census.2 Mi’kmaq is the most widely spoken Indigenous language in the province of New Brunswick. Although there exist Mi’kmaq dictionaries (Rand, 1888; DeBlois, 1996) and translated texts (DeBlois, 1990), Mi’kmaq remains a low-resource language. In particular, no (large) machine-readable corpora are available for Mi’kmaq. There has been very little work in computational linguistics or NLP on Mi’kmaq. The Cr´ubad´an project (Scannell, 2007)3 built web corpora for over 400 writing systems (as proxies for languages) of which Mi’kmaq was one.4 These corpora are not publicly available. Brown (2014) studied language identification for over 1300 languages, with Mi’kmaq being included amongst these. To the best of our knowledge, this is the first computational work to focus specifically on Mi’kmaq.5 The long-term goals of this research are to 1.) create a large Mi’kmaq corpus to support corpus linguistic studies of, lexicographical analysis of, and training NLP systems for, Mi’kmaq; and 2.) to build a suite of NLP tools for Mi’kmaq, which could potentially contribute to language preservation. In this preliminary work, in Section 2 we first build a web corpus of Mi’kmaq, using methods simil"
L18-1653,P13-2121,0,0.097233,"Missing"
L18-1653,P17-2009,0,0.0185464,"ries is important for building corpora for low-resource languages. Although the corpus created in this paper includes more documents than the Mi’kmaq corpus of Scannell (2007), crawling could still be useful to find more Mi’kmaq documents, and we intend to explore this in future work. Another important area for future work is language identification. Our analysis in Section 2 2 suggests that other languages are often present along with Mi’kmaq within a single paragraph. Some approaches to language identification are able to recognize which portions of text correspond to a particular language (Jurgens et al., 2017). Extending such methods to recognize Indigenous languages, and Mi’kmaq in particular, is an important area of future work. Future work on Mi’kmaq language identification should also take into account the variation in Mi’kmaq writing systems. In Section 3 we have identified some potential future directions with respect to language modelling. Recent approaches to low-resource language modelling have incorporated cross-lingual word embeddings learned from bilingual dictionaries (Adams et al., 2017), which are available for Mi’kmaq. We also intend to evaluate such approaches to language modelling"
L18-1653,N06-2001,0,0.114909,"Missing"
L18-1653,P12-3005,0,0.0389455,"(Pomik´alek, 2011), which preserves paragraph structure present in the HTML in the extracted text. JusText is able to incorporate information from a languagedependent stopword list in determining which document portions are boilerplate and which are more-conventional text. However, in preliminary experiments we observed that using a stopword list derived from the UDHR resulted in many portions of Mi’kmaq text not being recognized as such, and therefore chose to disable this feature.11 Language identification Although many language identification tools are available (Cavnar and Trenkle, 1994; Lui and Baldwin, 2012, for example), very little language identification research has considered Mi’kmaq, with Scannell (2007) and Brown (2014) being notable exceptions, and even these have not focused specifically on this language. Here we implement a simple approach to language identification. We represent each language for which the UDHR is available in NLTK as a vector of the character trigram frequencies in the corresponding version of the UDHR. For each document in our corpus, we similarly represent it as a vector of character trigram frequencies, and then compute its cosine similarity with the vector repres"
L18-1653,E14-2006,0,0.125843,"Missing"
N15-1099,W03-1812,1,0.8105,"h to predicting MWE compositionality. 3 Methodology In this work, we estimate the compositionality of an MWE based on the similarity between the expression and its component words in vector space. We use three different vector-space models: (1) a simple count-based model of distributional similarity; (2) word embeddings based on WORD 2 VEC; and (3) a multi-sense skip-gram model that, unlike the previous two models, is able to learn multiple embeddings per target word (or MWE). For all three models, we first greedily pre-tokenise the corpus to represent each MWE as a single token, similarly to Baldwin et al. (2003). In this, we apply the constraint that no language-specific pre-processing can be applied to the training corpus, in order to make the method maximally language independent. As such, we cannot perform any form of lemmatisation, and MWE identification takes the form of simple string match for concatenated instances of the component words, naively assuming that all occurrences of that word combination are MWEs. We detail each of the distributional similarity methods below. 978 Count-Based Distributional Similarity Our first method for building vectors is that of Salehi et al. (2014b): the top 5"
N15-1099,P14-2131,0,0.0131551,"mple approaches to composition, single word embeddings are empirically slightly superior to multi-prototype word embeddings overall. 977 Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 977–983, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics 2 Related Work 3.1 Recent work on distributed approaches to distributional semantics has demonstrated their utility in a wide range of NLP tasks, including identifying various morphosyntactic and semantic relations (Mikolov et al., 2013a), dependency parsing (Bansal et al., 2014), sentiment analysis (Socher et al., 2013), named-entity recognition (Collobert and Weston, 2008; Passos et al., 2014), and machine translation (Zou et al., 2013; Devlin et al., 2014). Despite the wealth of research applying word embeddings within NLP, they have not yet been considered for predicting the compositionality of MWEs. Much prior work on MWEs has been tailored to specific kinds of MWEs in particular languages (e.g. English verb–noun combinations (Fazly et al., 2009)). There has however been recent interest in approaches to MWEs that are more broadly applicable to a wider range of la"
N15-1099,P14-1023,0,0.0657342,"Missing"
N15-1099,W11-1304,0,0.016365,"g pool); (2) English verb particle constructions (“EVPCs”, e.g. stand up and give away); and (3) German noun compounds (“GNCs”, e.g. ahornblatt “maple leaf” and eidechse “lizard”). The ENC dataset consists of 90 binary English noun compounds, and is annotated on a continuous [0, 5] scale for both overall compositionality and the component-wise compositionality of each of the modifier and head noun (Reddy et al., 2011). The state-of-the-art method for this dataset (Salehi et al., 2014b) is a supervised support vector regression 3 We also considered using the dataset from the DisCo shared task (Biemann and Giesbrecht, 2011), but ultimately excluded it because it includes different types of MWEs without indication of the syntactic type of a given MWE, preventing us from carrying out construction-specific parameter tuning. 979 model, trained over the distributional method from Section 3.1 as applied to both English and 51 target languages (under word and MWE translation). The EVPC dataset consists of 160 English verb particle constructions, and is manually annotated for compositionality on a binary scale for each of the head verb and particle (Bannard, 2006). In order to translate the dataset into a regression tas"
N15-1099,C14-1071,0,0.0244373,"ocher et al., 2013), named-entity recognition (Collobert and Weston, 2008; Passos et al., 2014), and machine translation (Zou et al., 2013; Devlin et al., 2014). Despite the wealth of research applying word embeddings within NLP, they have not yet been considered for predicting the compositionality of MWEs. Much prior work on MWEs has been tailored to specific kinds of MWEs in particular languages (e.g. English verb–noun combinations (Fazly et al., 2009)). There has however been recent interest in approaches to MWEs that are more broadly applicable to a wider range of languages and MWE types (Brooke et al., 2014; Salehi et al., 2014b; Schneider et al., 2014). Word embeddings could form the basis for such an approach to predicting MWE compositionality. 3 Methodology In this work, we estimate the compositionality of an MWE based on the similarity between the expression and its component words in vector space. We use three different vector-space models: (1) a simple count-based model of distributional similarity; (2) word embeddings based on WORD 2 VEC; and (3) a multi-sense skip-gram model that, unlike the previous two models, is able to learn multiple embeddings per target word (or MWE). For all three"
N15-1099,P14-1129,0,0.0601437,"Missing"
N15-1099,J09-1005,1,0.804368,"ks, including identifying various morphosyntactic and semantic relations (Mikolov et al., 2013a), dependency parsing (Bansal et al., 2014), sentiment analysis (Socher et al., 2013), named-entity recognition (Collobert and Weston, 2008; Passos et al., 2014), and machine translation (Zou et al., 2013; Devlin et al., 2014). Despite the wealth of research applying word embeddings within NLP, they have not yet been considered for predicting the compositionality of MWEs. Much prior work on MWEs has been tailored to specific kinds of MWEs in particular languages (e.g. English verb–noun combinations (Fazly et al., 2009)). There has however been recent interest in approaches to MWEs that are more broadly applicable to a wider range of languages and MWE types (Brooke et al., 2014; Salehi et al., 2014b; Schneider et al., 2014). Word embeddings could form the basis for such an approach to predicting MWE compositionality. 3 Methodology In this work, we estimate the compositionality of an MWE based on the similarity between the expression and its component words in vector space. We use three different vector-space models: (1) a simple count-based model of distributional similarity; (2) word embeddings based on WOR"
N15-1099,D14-1113,0,0.0815431,"ogle.com/p/word2vec/ (“C-S KIP”), whereby a given word in context is projected onto a projection layer, and used to predict its immediate context (preceding and following words). WORD 2 VEC generates a vector of fixed dimensionality d for each pre-tokenised word/MWE type with frequency above a certain threshold in the training corpus. We again use comp 1 and comp 2 to estimate compositionality from these vectors. 3.3 Multi-Sense Skip-gram Model One potential shortcoming of WORD 2 VEC is that it generates a single word embedding for each word, irrespective of the relative polysemy of the word. Neelakantan et al. (2014) proposed a method motivated by WORD 2 VEC, which efficiently learns multiple embeddings per word/MWE. We refer to this approach as the multi-sense skip-gram (MSSG) model. We once again compose the resultant vectors with comp 1 and comp 2 , but modify the formulation slightly to handle the variable number of vectors for each word/MWE, by searching over the cross-product of vectors in each sim calculation and taking the maximum in each case. We initially set the number of embeddings to 2 in our MSSG experiments — in keeping with the findings in Neelakantan et al. (2014) — but come back to exami"
N15-1099,W14-1609,0,0.0130291,"dings overall. 977 Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 977–983, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics 2 Related Work 3.1 Recent work on distributed approaches to distributional semantics has demonstrated their utility in a wide range of NLP tasks, including identifying various morphosyntactic and semantic relations (Mikolov et al., 2013a), dependency parsing (Bansal et al., 2014), sentiment analysis (Socher et al., 2013), named-entity recognition (Collobert and Weston, 2008; Passos et al., 2014), and machine translation (Zou et al., 2013; Devlin et al., 2014). Despite the wealth of research applying word embeddings within NLP, they have not yet been considered for predicting the compositionality of MWEs. Much prior work on MWEs has been tailored to specific kinds of MWEs in particular languages (e.g. English verb–noun combinations (Fazly et al., 2009)). There has however been recent interest in approaches to MWEs that are more broadly applicable to a wider range of languages and MWE types (Brooke et al., 2014; Salehi et al., 2014b; Schneider et al., 2014). Word embeddings could form"
N15-1099,I11-1024,0,0.847727,"rch questions here are: Introduction Multiword expressions (MWEs) are word combinations that display some form of idiomaticity (Baldwin and Kim, 2009), including semantic idiomaticity, wherein the semantics of the MWE (e.g. ivory tower) cannot be predicted from the semantics of the component words (e.g. ivory and tower). Recent NLP work on semantic idiomaticity has focused on the task of “compositionality prediction”, in the form of a regression task whereby a given MWE is mapped onto a continuous-valued compositionality score, either for the MWE as a whole or for each of its component words (Reddy et al., 2011; Schulte im Walde et al., 2013; Salehi et al., 2014b). Separately in NLP, there has been a recent surge of interest in learning distributed representations of word meaning, in the form of “word embeddings” (Collobert and Weston, 2008; Mikolov et al., RQ1: Are word embeddings superior to conventional count-based models of distributional similarity? RQ2: How sensitive to parameter optimisation are different word embedding approaches? RQ3: Are multi-prototype word embeddings empirically superior to single-prototype word embeddings? We explore these questions relative to three compositionality pr"
N15-1099,S13-1039,1,0.827666,"cale for each of the head verb and particle (Bannard, 2006). In order to translate the dataset into a regression task, we calculate the overall compositionality as the number of annotations of entailment for the verb, divided by the total number of verb annotations for that VPC. The state-of-the-art method for this dataset (Salehi et al., 2014b) is a linear combination of: (1) the distributional method from Section 3.1; (2) the same method applied to 10 target languages (under word and MWE translation, selecting the languages using supervised learning); and (3) the string similarity method of Salehi and Cook (2013). The GNC dataset consists of 246 German noun compounds, and is annotated on a continuous [1, 7] scale (von der Heide and Borgwaldt, 2009; Schulte im Walde et al., 2013). The state-of-the-art method for this dataset is a distributional similarity method applied to part-of-speech tagged and lemmatised data (Schulte im Walde et al., 2013). 5 Experiments For all experiments, we train our models over raw text Wikipedia corpora for either English or German, depending on the language of the dataset. The raw English and German corpora were preprocessed using the WP2TXT toolbox4 to eliminate XML and H"
N15-1099,D14-1189,1,0.891505,"Missing"
N15-1099,E14-1050,1,0.506823,"essions (MWEs) are word combinations that display some form of idiomaticity (Baldwin and Kim, 2009), including semantic idiomaticity, wherein the semantics of the MWE (e.g. ivory tower) cannot be predicted from the semantics of the component words (e.g. ivory and tower). Recent NLP work on semantic idiomaticity has focused on the task of “compositionality prediction”, in the form of a regression task whereby a given MWE is mapped onto a continuous-valued compositionality score, either for the MWE as a whole or for each of its component words (Reddy et al., 2011; Schulte im Walde et al., 2013; Salehi et al., 2014b). Separately in NLP, there has been a recent surge of interest in learning distributed representations of word meaning, in the form of “word embeddings” (Collobert and Weston, 2008; Mikolov et al., RQ1: Are word embeddings superior to conventional count-based models of distributional similarity? RQ2: How sensitive to parameter optimisation are different word embedding approaches? RQ3: Are multi-prototype word embeddings empirically superior to single-prototype word embeddings? We explore these questions relative to three compositionality prediction datasets spanning two MWE construction type"
N15-1099,Q14-1016,0,0.0499337,"on (Collobert and Weston, 2008; Passos et al., 2014), and machine translation (Zou et al., 2013; Devlin et al., 2014). Despite the wealth of research applying word embeddings within NLP, they have not yet been considered for predicting the compositionality of MWEs. Much prior work on MWEs has been tailored to specific kinds of MWEs in particular languages (e.g. English verb–noun combinations (Fazly et al., 2009)). There has however been recent interest in approaches to MWEs that are more broadly applicable to a wider range of languages and MWE types (Brooke et al., 2014; Salehi et al., 2014b; Schneider et al., 2014). Word embeddings could form the basis for such an approach to predicting MWE compositionality. 3 Methodology In this work, we estimate the compositionality of an MWE based on the similarity between the expression and its component words in vector space. We use three different vector-space models: (1) a simple count-based model of distributional similarity; (2) word embeddings based on WORD 2 VEC; and (3) a multi-sense skip-gram model that, unlike the previous two models, is able to learn multiple embeddings per target word (or MWE). For all three models, we first greedily pre-tokenise the cor"
N15-1099,S13-1038,0,0.198581,"Missing"
N15-1099,D12-1110,0,0.0704611,"Missing"
N15-1099,D13-1170,0,0.00176361,"d embeddings are empirically slightly superior to multi-prototype word embeddings overall. 977 Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 977–983, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics 2 Related Work 3.1 Recent work on distributed approaches to distributional semantics has demonstrated their utility in a wide range of NLP tasks, including identifying various morphosyntactic and semantic relations (Mikolov et al., 2013a), dependency parsing (Bansal et al., 2014), sentiment analysis (Socher et al., 2013), named-entity recognition (Collobert and Weston, 2008; Passos et al., 2014), and machine translation (Zou et al., 2013; Devlin et al., 2014). Despite the wealth of research applying word embeddings within NLP, they have not yet been considered for predicting the compositionality of MWEs. Much prior work on MWEs has been tailored to specific kinds of MWEs in particular languages (e.g. English verb–noun combinations (Fazly et al., 2009)). There has however been recent interest in approaches to MWEs that are more broadly applicable to a wider range of languages and MWE types (Brooke et al., 2014"
N15-1099,D13-1141,0,0.0333351,"The 2015 Annual Conference of the North American Chapter of the ACL, pages 977–983, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics 2 Related Work 3.1 Recent work on distributed approaches to distributional semantics has demonstrated their utility in a wide range of NLP tasks, including identifying various morphosyntactic and semantic relations (Mikolov et al., 2013a), dependency parsing (Bansal et al., 2014), sentiment analysis (Socher et al., 2013), named-entity recognition (Collobert and Weston, 2008; Passos et al., 2014), and machine translation (Zou et al., 2013; Devlin et al., 2014). Despite the wealth of research applying word embeddings within NLP, they have not yet been considered for predicting the compositionality of MWEs. Much prior work on MWEs has been tailored to specific kinds of MWEs in particular languages (e.g. English verb–noun combinations (Fazly et al., 2009)). There has however been recent interest in approaches to MWEs that are more broadly applicable to a wider range of languages and MWE types (Brooke et al., 2014; Salehi et al., 2014b; Schneider et al., 2014). Word embeddings could form the basis for such an approach to predictin"
P13-2112,P11-2000,0,0.650374,"her language (e.g., Hana et al., 2004; Feldman et al., 2006; Reddy and Sharoff, 2011). Other approaches have simultaneously tagged two languages based on alignments in a parallel corpus (e.g., Snyder et al., 2008). A number of studies have used tag projection to copy tag information from a resource-rich to a resource-poor language, based on word alignments in a parallel corpus. After alignment, the resource-rich language is tagged, and tags are projected from the source language to the target language based on the alignment (e.g., Yarowsky and Ngai, 2001; Das and Petrov, 2011). Das and Petrov (2011) achieved the current state-of-the-art for unsupervised tagging by exploiting high conﬁdence alignments to copy tags from the source language to the target language. Graph-based label propagation was used to automatically produce more labelled training data. First, a graph was constructed in which each vertex corresponds to a unique trigram, and edge weights represent the syntactic similarity between vertices. Labels were then propagated by optimizing a convex function to favor the same tags for closely related nodes Unsupervised part-of-speech tagging Currently, part-of-speech (POS) taggers a"
P13-2112,2005.mtsummit-papers.11,0,0.351921,"it achieves comparable results. 3 We eliminate many-to-one alignments (Step 2). Keeping these would give more POS-tagged tokens for the target side, but also introduce noise. For example, suppose English and French were the source and target language, respectively. In this case alignments such as English laws (NNS) to French les (DT) lois (NNS) would be expected (Yarowsky and Ngai, 2001). However, in Step 3, where tags are projected from the source to target language, this would incorrectly tag French les as NN. We build a French tagger based on English– French data from the Europarl Corpus (Koehn, 2005). We also compare the accuracy and coverage of the tags obtained through direct projection using the French Melt POS tagger (Denis and Sagot, 2009). Table 1 conﬁrms that the one-to-one alignments indeed give higher accuracy but lower coverage than the many-to-one alignments. At this stage of the model we hypothesize that highconﬁdence tags are important, and hence eliminate the many-to-one alignments. In Step 4, in an effort to again obtain higher quality target language tags from direct projection, we eliminate all but the top n sentences based on their alignment scores, as provided by the al"
P13-2112,A00-1031,0,0.840096,"Missing"
P13-2112,N06-1020,0,0.0488021,"stimated emission and transition probabilities into the TNT tagger (Brants, 2000), an implementation of a trigram HMM tagger. 4.2 language word wis is aligned with target language word wjt with probability p(wjt |wis ), Tis is the tag for wis using the tagger available for the source language, and Tjt is the tag for wjt using the tagger learned for the target language. If p(wjt |wis ) &gt; S, where S is a threshold which we heuristically set to 0.7, we replace Tjt by Tis . Self-training can suffer from over-ﬁtting, in which errors in the original model are repeated and ampliﬁed in the new model (McClosky et al., 2006). To avoid this, we remove the tag of any token that the model is uncertain of, i.e., if p(wjt |wis ) &lt; S and Tjt �= Tis then Tjt = Null. So, on the target side, aligned words have a tag from direct projection or no tag, and unaligned words have a tag assigned by our model. Step 4 estimates the emission and transition probabilities as in Algorithm 1. In Step 5, emission probabilities for lexical items in the previous model, but missing from the current model, are added to the current model. Later models therefore take advantage of information from earlier models, and have wider coverage. Self"
P13-2112,P11-1061,0,0.687686,"ne language based on a corpus for another language (e.g., Hana et al., 2004; Feldman et al., 2006; Reddy and Sharoff, 2011). Other approaches have simultaneously tagged two languages based on alignments in a parallel corpus (e.g., Snyder et al., 2008). A number of studies have used tag projection to copy tag information from a resource-rich to a resource-poor language, based on word alignments in a parallel corpus. After alignment, the resource-rich language is tagged, and tags are projected from the source language to the target language based on the alignment (e.g., Yarowsky and Ngai, 2001; Das and Petrov, 2011). Das and Petrov (2011) achieved the current state-of-the-art for unsupervised tagging by exploiting high conﬁdence alignments to copy tags from the source language to the target language. Graph-based label propagation was used to automatically produce more labelled training data. First, a graph was constructed in which each vertex corresponds to a unique trigram, and edge weights represent the syntactic similarity between vertices. Labels were then propagated by optimizing a convex function to favor the same tags for closely related nodes Unsupervised part-of-speech tagging Currently, part-of"
P13-2112,D08-1109,0,0.0462258,"(Das and Petrov, 2011), but is substantially less-sophisticated (speciﬁcally not requiring convex optimization or a feature-based HMM). The complexity of our algorithm is O(nlogn) compared to O(n2 ) for that of Das and Petrov 637 (2011) where n is the size of training data.3 We made our code are available for download.4 In future work we intend to consider using a larger training corpus to reduce the proportion of unknown tokens and improve accuracy. Given the improvements of our model over that of Das and Petrov on languages from the same family as our source language, and the observation of Snyder et al. (2008) that a better tagger can be learned from a more-closely related language, we also plan to consider strategies for selecting an appropriate source language for a given target language. Using our ﬁnal model with unsupervised HMM methods might improve the ﬁnal performance too, i.e. use our ﬁnal model as the initial state for HMM, then experiment with different inference algorithms such as Expectation Maximization (EM), Variational Bayers (VB) or Gibbs sampling (GS).5 Gao and Johnson (2008) compare EM, VB and GS for unsupervised English POS tagging. In many cases, GS outperformed other methods, t"
P13-2112,N03-1033,0,0.239791,"Missing"
P13-2112,N01-1026,0,0.781294,"s for an HMM tagger for one language based on a corpus for another language (e.g., Hana et al., 2004; Feldman et al., 2006; Reddy and Sharoff, 2011). Other approaches have simultaneously tagged two languages based on alignments in a parallel corpus (e.g., Snyder et al., 2008). A number of studies have used tag projection to copy tag information from a resource-rich to a resource-poor language, based on word alignments in a parallel corpus. After alignment, the resource-rich language is tagged, and tags are projected from the source language to the target language based on the alignment (e.g., Yarowsky and Ngai, 2001; Das and Petrov, 2011). Das and Petrov (2011) achieved the current state-of-the-art for unsupervised tagging by exploiting high conﬁdence alignments to copy tags from the source language to the target language. Graph-based label propagation was used to automatically produce more labelled training data. First, a graph was constructed in which each vertex corresponds to a unique trigram, and edge weights represent the syntactic similarity between vertices. Labels were then propagated by optimizing a convex function to favor the same tags for closely related nodes Unsupervised part-of-speech tag"
P13-2112,Y09-1013,0,\N,Missing
P13-2112,petrov-etal-2012-universal,0,\N,Missing
P13-2112,feldman-etal-2006-cross,0,\N,Missing
P13-2112,W04-3229,0,\N,Missing
P13-2112,D08-1036,0,\N,Missing
P13-4002,D10-1124,0,0.508203,"Missing"
P13-4002,C12-1064,1,0.528301,"er data for experiments and exemplification in this study. 7 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 7–12, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics sized cell representation is that it introduces class imbalance: urban areas tend to contain far more tweets than rural areas. Based on this observation, Roller et al. (2012) introduced an adaptive grid representation in which cells contain approximately the same number of users, based on a KDtree partition. Given that most tweets are from urban areas, Han et al. (2012) consider a citybased class division, and explore different feature selection methods to extract “location indicative words”, which they show to improve prediction accuracy. Additionally, time zone information has been incorporated in a coarse-to-fine hierarchical model by first determining the time zone, and then disambiguating locations within it (Mahmud et al., 2012). Topic models have also been applied to the task, in capturing regional linguistic differences (Eisenstein et al., 2010; Yin et al., 2011; Hong et al., 2012). When designing a practical geolocation system, simple models such as"
P13-4002,P12-3005,1,0.810339,"90 Acc@161 .277 .262 .487 .492 .525 .171 .665 Median 793 913 181 170 92 1330 9 Table 1: Results over WORLD radius of the home location (Cheng et al., 2010), to capture near-misses (e.g., Edinburgh UK being predicted as Glasgow, UK). • Median : The median distance from the predicted city to the home location (Eisenstein et al., 2010). 4.2 Comparison with Benchmarks We base our evaluation on the publicly-available WORLD dataset of Han et al. (2012). The dataset contains 1.4M users whose tweets are primarily identified as English based on the output of the langid.py language identification tool (Lui and Baldwin, 2012), and who have posted at least 10 geotagged tweets. The city-level home location for a geotagged user is determined as follows. First, each of a user’s geotagged tweets is mapped to its nearest city (based on the same set of 3,709 cities used for the city-based location representation). Then, the most frequent city for a user is selected as the home location. To benchmark our method, we reimplement two recently-published state-of-the-art methods: (1) the KL-divergence nearest prototype method of Roller et al. (2012) based on KD-tree partitioned grid cells, which we denote as KL; and (2) the mu"
P13-4002,D12-1137,0,0.318331,"andall et al., 2009) and Wikipedia editors (Lieberman and Lin, 2009). Recently, a considerable amount of work has been devoted to extending geolocation prediction for Twitter 1 We only use public Twitter data for experiments and exemplification in this study. 7 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 7–12, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics sized cell representation is that it introduces class imbalance: urban areas tend to contain far more tweets than rural areas. Based on this observation, Roller et al. (2012) introduced an adaptive grid representation in which cells contain approximately the same number of users, based on a KDtree partition. Given that most tweets are from urban areas, Han et al. (2012) consider a citybased class division, and explore different feature selection methods to extract “location indicative words”, which they show to improve prediction accuracy. Additionally, time zone information has been incorporated in a coarse-to-fine hierarchical model by first determining the time zone, and then disambiguating locations within it (Mahmud et al., 2012). Topic models have also been"
P13-4002,P11-1096,0,0.165646,"erms which are associated with particular locations (e.g. ferry for Seattle or Sydney). Beyond identifying geographical references using off-the-shelf tools, more sophisticated methods have been introduced in the social media realm. Cheng et al. (2010) built a simple generative model based on tweet words, and further added words which are local to particular regions and applied smoothing to under-represented locations. Kinsella et al. (2011) applied different similarity measures to the task, and investigated the relative difficulty of geolocation prediction at city, state, and country levels. Wing and Baldridge (2011) introduced a grid-based representation for geolocation modeling and inference based on fixed latitude and longitude values, and aggregated all tweets in a single cell. Their approach was then based on lexical similarity using KL-divergence. One drawback to the uniform3 Methodology In this study, we adopt the same city-based representation and multinomial naive Bayes learner as Han et al. (2012). The city-based representation consists of 3,709 cities throughout the world, and is obtained by aggregating smaller cities with the largest nearby city. Han et al. (2012) found that using feature sele"
P14-1025,P13-1141,0,0.0173254,"ve of context, we will compare our model with that proposed by McCarthy et al. 260 Recent work on finding novel senses has tended to focus on comparing diachronic corpora (Sagi et al., 2009; Cook and Stevenson, 2010; Gulordava and Baroni, 2011) and has also considered topic models (Lau et al., 2012). In a similar vein, Peirsman et al. (2010) considered the identification of words having a sense particular to one language variety with respect to another (specifically Belgian and Netherlandic Dutch). In contrast to these studies, we propose a model for comparing a corpus with a sense inventory. Carpuat et al. (2013) exploit parallel corpora to identify words in domain-specific monolingual corpora with previously-unseen translations; the method we propose does not require parallel data. 3 marginal, we make no use of parser-based features in this paper.3 The induced topics take the form of word multinomials, and are often represented by the top-N words in descending order of conditional probability. We interpret each topic as a sense of the target lemma.4 To illustrate this, we give the example of topics induced by the HDP model for network in Table 1. We refer to this method as HDP-WSI henceforth.5 In pre"
P14-1025,W04-3204,0,0.195409,"Missing"
P14-1025,S07-1002,0,0.0143677,"nto a multinomial distribution over words, based on simple maximum likelihood estimation.6 We then calculate the Jensen– Shannon divergence between the multinomial distribution (over words) of the gloss and that of the topic, and convert the divergence value into a similarity score by subtracting it from 1. Formally, the similarity sense si and topic tj is: Methodology Our methodology is based on the WSI system described in Lau et al. (2012),1 which has been shown (Lau et al., 2012; Lau et al., 2013a; Lau et al., 2013b) to achieve state-of-the-art results over the WSI tasks from SemEval-2007 (Agirre and Soroa, 2007), SemEval-2010 (Manandhar et al., 2010) and SemEval-2013 (Navigli and Vannella, 2013; Jurgens and Klapaftis, 2013). The system is built around a Hierarchical Dirichlet Process (HDP: Teh et al. (2006)), a non-parametric variant of a Latent Dirichlet Allocation topic model (Blei et al., 2003) where the model automatically optimises the number of topics in a fully-unsupervised fashion over the training data. To learn the senses of a target lemma, we train a single topic model per target lemma. The system reads in a collection of usages of that lemma, and automatically induces topics (= senses) in"
P14-1025,P06-1012,0,0.0155071,"Missing"
P14-1025,E09-1005,0,0.0121583,"he flexibility and robustness of our methodology. Future work could pursue a more sophisticated methodology, using non-linear combinations of sim(si , tj ) for computing the affinity measures or multiple features in a supervised context. We contend, however, that these extensions are ultimately a preliminary demonstration to the flexibility and robustness of our methodology. A natural next step for this research would be to couple sense distribution estimation and the detection of unattested senses with evidence from the context, using topics or other information about the local context (e.g. Agirre and Soroa (2009)) to carry out unsupervised WSD of individual token occurrences of a given word. where f (tj ) is the frequency of topic tj in the corpus. The intuition behind novelty is that a target lemma with a novel sense should have a (somewhat-)frequent topic that has low association with any sense. That we use the frequency rather than the probability of the topic here is deliberate, as topics with a higher raw number of occurrences (whether as a low-probability topic for a high-frequency word, or a high-probability topic for a low-frequency word) are indicative of a novel word sense. For each of our t"
P14-1025,cook-stevenson-2010-automatically,1,0.840918,"Missing"
P14-1025,I13-1041,1,0.812251,"one dataset), but HDP-WSI is better at inducing the overall sense distribution. It is important to bear in mind that MKWC in these experiments makes use of full-text parsing in calculating the distributional similarity thesaurus, and the WordNet graph structure in calculating the similarity between associated words and different senses. Our method, on the other hand, uses no parsing, and only the synset definitions (and not the graph structure) of WordNet.8 The non-reliance on parsing is significant in terms of portability to text sources which are less amenable to parsing (such as Twitter: (Baldwin et al., 2013)), and the non-reliance on the graph structure of WordNet is significant in terms of portability to conventional “flat” sense inventories. While comparable results on a different dataset have been achieved with a proximity thesaurus (McCarthy et al., 2007) compared to a dependency one,9 it is not stated how both systems, as we see in the gap between the upper bound (based on perfect determination of the first sense) and the respective system accuracies. Given that both systems compute a continuousvalued prevalence score for each sense of a target lemma, a distribution of senses can be obtained"
P14-1025,N06-1017,0,0.0586004,"A system for automatically identifying such novel senses — i.e. senses that are attested in the corpus but not in the sense inventory — would be a very valuable lexicographical tool for keeping sense inventories up-to-date (Cook et al., 2013). We further propose an application of our proposed method to the identification of such novel senses. In contrast to McCarthy et al. (2004b), the use of topic models makes this possible, using topics as a proxy for sense (Brody and Lapata, 2009; Yao and Durme, 2011; Lau et al., 2012). Earlier work on identifying novel senses focused on individual tokens (Erk, 2006), whereas our approach goes further in identifying groups of tokens exhibiting the same novel sense. 2 As well as sense ranking for predominant sense acquisition, automatic estimates of sense frequency distribution can be very useful for WSD for training data sampling purposes (Agirre and Martinez, 2004), entropy estimation (Jin et al., 2009), and prior probability estimates, all of which can be integrated within a WSD system (Chan and Ng, 2005; Chan and Ng, 2006; Lapata and Brew, 2004). Various approaches have been adopted, such as normalizing sense ranking scores to obtain a probability dist"
P14-1025,S07-1060,0,0.466214,"Missing"
P14-1025,D07-1109,0,0.017322,"tems, The University of Melbourne ♦ University of Cambridge jeyhan.lau@gmail.com, paulcook@unimelb.edu.au, diana@dianamccarthy.co.uk, spandanagella@gmail.com, tb@ldwin.net Abstract and sense distributions for specific corpora are required (Koeling et al., 2005; Lapata and Brew, 2004). In this paper, we propose a method which uses topic models to estimate word sense distributions. This method is in principle applicable to all parts of speech, and moreover does not require a parser, a hierarchical sense representation or parallel text. Topic models have been used for WSD in a number of studies (Boyd-Graber et al., 2007; Li et al., 2010; Lau et al., 2012; Preiss and Stevenson, 2013; Cai et al., 2007; Knopp et al., 2013), but our work extends significantly on this earlier work in focusing on the acquisition of prior word sense distributions (and predominant senses). Because of domain differences and the skewed nature of word sense distributions, it is often the case that some senses in a sense inventory will not be attested in a given corpus. A system capable of automatically finding such senses could reduce ambiguity, particularly in domain adaptation settings, while retaining rare but nevertheless viable se"
P14-1025,E14-4042,1,0.869137,"Missing"
P14-1025,E09-1013,0,0.0180149,"let and swipe related to touchscreen computers) or domain-specific terms not being included in a more general-purpose sense inventory. A system for automatically identifying such novel senses — i.e. senses that are attested in the corpus but not in the sense inventory — would be a very valuable lexicographical tool for keeping sense inventories up-to-date (Cook et al., 2013). We further propose an application of our proposed method to the identification of such novel senses. In contrast to McCarthy et al. (2004b), the use of topic models makes this possible, using topics as a proxy for sense (Brody and Lapata, 2009; Yao and Durme, 2011; Lau et al., 2012). Earlier work on identifying novel senses focused on individual tokens (Erk, 2006), whereas our approach goes further in identifying groups of tokens exhibiting the same novel sense. 2 As well as sense ranking for predominant sense acquisition, automatic estimates of sense frequency distribution can be very useful for WSD for training data sampling purposes (Agirre and Martinez, 2004), entropy estimation (Jin et al., 2009), and prior probability estimates, all of which can be integrated within a WSD system (Chan and Ng, 2005; Chan and Ng, 2006; Lapata a"
P14-1025,W11-2508,0,0.125836,"Missing"
P14-1025,D07-1108,0,0.0240323,"k@unimelb.edu.au, diana@dianamccarthy.co.uk, spandanagella@gmail.com, tb@ldwin.net Abstract and sense distributions for specific corpora are required (Koeling et al., 2005; Lapata and Brew, 2004). In this paper, we propose a method which uses topic models to estimate word sense distributions. This method is in principle applicable to all parts of speech, and moreover does not require a parser, a hierarchical sense representation or parallel text. Topic models have been used for WSD in a number of studies (Boyd-Graber et al., 2007; Li et al., 2010; Lau et al., 2012; Preiss and Stevenson, 2013; Cai et al., 2007; Knopp et al., 2013), but our work extends significantly on this earlier work in focusing on the acquisition of prior word sense distributions (and predominant senses). Because of domain differences and the skewed nature of word sense distributions, it is often the case that some senses in a sense inventory will not be attested in a given corpus. A system capable of automatically finding such senses could reduce ambiguity, particularly in domain adaptation settings, while retaining rare but nevertheless viable senses. We further propose a method for applying our sense distribution acquisition"
P14-1025,S13-2039,1,0.835591,"we will compare our model with that proposed by McCarthy et al. 260 Recent work on finding novel senses has tended to focus on comparing diachronic corpora (Sagi et al., 2009; Cook and Stevenson, 2010; Gulordava and Baroni, 2011) and has also considered topic models (Lau et al., 2012). In a similar vein, Peirsman et al. (2010) considered the identification of words having a sense particular to one language variety with respect to another (specifically Belgian and Netherlandic Dutch). In contrast to these studies, we propose a model for comparing a corpus with a sense inventory. Carpuat et al. (2013) exploit parallel corpora to identify words in domain-specific monolingual corpora with previously-unseen translations; the method we propose does not require parallel data. 3 marginal, we make no use of parser-based features in this paper.3 The induced topics take the form of word multinomials, and are often represented by the top-N words in descending order of conditional probability. We interpret each topic as a sense of the target lemma.4 To illustrate this, we give the example of topics induced by the HDP model for network in Table 1. We refer to this method as HDP-WSI henceforth.5 In pre"
P14-1025,I08-1073,1,0.825054,"d in Table 3. HDP-WSI consistently achieves lower JS divergence, indicating that the distribution of senses that it finds is closer to the gold standard distribution. Testing for statistical significance over the paired JS divergence values for each lemma using the Wilcoxon signed-rank test, the result for FINANCE is significant (p &lt; 0.05) but the results for the other two datasets are not (p &gt; 0.1 in each case). 8 McCarthy et al. (2004b) obtained good results with definition overlap, but their implementation uses the relation structure alongside the definitions (Banerjee and Pedersen, 2002). Iida et al. (2008) demonstrate that further extensions using distributional data are required when applying the method to resources without hierarchical relations. 9 The thesauri used in the reimplementation of MKWC in this paper were obtained from http://webdocs.cs. ualberta.ca/˜lindek/downloads.htm. 263 wide a window is needed for the proximity thesaurus. This could be a significant issue with Twitter data, where context tends to be limited. In the next section, we demonstrate the robustness of the method in experimenting with two new datasets, based on Twitter and a web corpus, and the Macmillan English Dict"
P14-1025,O97-1002,0,0.0490006,"n for Computational Linguistics and Hirst, 2006), which is a very powerful heuristic approach to WSD. Most WSD systems rely upon this heuristic for back-off in the absence of strong contextual evidence (McCarthy et al., 2007). McCarthy et al. (2004b) proposed a method which relies on distributionally similar words (nearest neighbours) associated with the target word in an automatically acquired thesaurus (Lin, 1998). The distributional similarity scores of the nearest neighbours are associated with the respective target word senses using a WordNet similarity measure, such as those proposed by Jiang and Conrath (1997) and Banerjee and Pedersen (2002). The word senses are ranked based on these similarity scores, and the most frequent sense is selected for the corpus that the distributional similarity thesaurus was trained over. tively recent senses of tablet and swipe related to touchscreen computers) or domain-specific terms not being included in a more general-purpose sense inventory. A system for automatically identifying such novel senses — i.e. senses that are attested in the corpus but not in the sense inventory — would be a very valuable lexicographical tool for keeping sense inventories up-to-date ("
P14-1025,P10-1116,0,0.0142022,"lbourne ♦ University of Cambridge jeyhan.lau@gmail.com, paulcook@unimelb.edu.au, diana@dianamccarthy.co.uk, spandanagella@gmail.com, tb@ldwin.net Abstract and sense distributions for specific corpora are required (Koeling et al., 2005; Lapata and Brew, 2004). In this paper, we propose a method which uses topic models to estimate word sense distributions. This method is in principle applicable to all parts of speech, and moreover does not require a parser, a hierarchical sense representation or parallel text. Topic models have been used for WSD in a number of studies (Boyd-Graber et al., 2007; Li et al., 2010; Lau et al., 2012; Preiss and Stevenson, 2013; Cai et al., 2007; Knopp et al., 2013), but our work extends significantly on this earlier work in focusing on the acquisition of prior word sense distributions (and predominant senses). Because of domain differences and the skewed nature of word sense distributions, it is often the case that some senses in a sense inventory will not be attested in a given corpus. A system capable of automatically finding such senses could reduce ambiguity, particularly in domain adaptation settings, while retaining rare but nevertheless viable senses. We further"
P14-1025,N09-2059,1,0.960189,"senses. In contrast to McCarthy et al. (2004b), the use of topic models makes this possible, using topics as a proxy for sense (Brody and Lapata, 2009; Yao and Durme, 2011; Lau et al., 2012). Earlier work on identifying novel senses focused on individual tokens (Erk, 2006), whereas our approach goes further in identifying groups of tokens exhibiting the same novel sense. 2 As well as sense ranking for predominant sense acquisition, automatic estimates of sense frequency distribution can be very useful for WSD for training data sampling purposes (Agirre and Martinez, 2004), entropy estimation (Jin et al., 2009), and prior probability estimates, all of which can be integrated within a WSD system (Chan and Ng, 2005; Chan and Ng, 2006; Lapata and Brew, 2004). Various approaches have been adopted, such as normalizing sense ranking scores to obtain a probability distribution (Jin et al., 2009), using subcategorisation information as an indication of verb sense (Lapata and Brew, 2004) or alternatively using parallel text (Chan and Ng, 2005; Chan and Ng, 2006; Agirre and Martinez, 2004). Background and Related Work There has been a considerable amount of research on representing word senses and disambiguat"
P14-1025,P98-2127,0,0.0433121,"ing predominant senses 259 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 259–270, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics and Hirst, 2006), which is a very powerful heuristic approach to WSD. Most WSD systems rely upon this heuristic for back-off in the absence of strong contextual evidence (McCarthy et al., 2007). McCarthy et al. (2004b) proposed a method which relies on distributionally similar words (nearest neighbours) associated with the target word in an automatically acquired thesaurus (Lin, 1998). The distributional similarity scores of the nearest neighbours are associated with the respective target word senses using a WordNet similarity measure, such as those proposed by Jiang and Conrath (1997) and Banerjee and Pedersen (2002). The word senses are ranked based on these similarity scores, and the most frequent sense is selected for the corpus that the distributional similarity thesaurus was trained over. tively recent senses of tablet and swipe related to touchscreen computers) or domain-specific terms not being included in a more general-purpose sense inventory. A system for automa"
P14-1025,P12-3005,1,0.65009,"ary sense-tagging task that it better captured Twitter usages than WordNet (and also OntoNotes: Hovy et al. (2006)). The dataset is made up of 20 target nouns which were selected to span the high- to mid-frequency range in both Twitter and the ukWaC corpus, and have at least 3 Macmillan senses. The average sense ambiguity of the 20 target nouns in Macmillan is 5.6 (but 12.3 in WordNet). 100 usages of each target noun were sampled from each of Twitter (from a crawl over the time period Jan 3–Feb 28, 2013 using the Twitter Streaming API) and ukWaC, after language identification using langid.py (Lui and Baldwin, 2012) and POS tagging (based on the CMU ARK Twitter POS tagger v2.0 (Owoputi et al., 2012) for Twitter, and the POS tags provided with the corpus for ukWaC). Amazon Mechanical Turk (AMT) was then used to 5-way sense-tag each usage relative to Macmillan, including allowing the annotators the option to label a usage as “Other” in instances where the usage was not captured by any of the Macmillan senses. After quality control over the annotators/annotations (see 5.1 Learning Sense Distributions As in Section 4, we evaluate in terms of WSD accuracy (Table 4) and JS divergence over the gold-standard sen"
P14-1025,S13-2049,0,0.0136682,"the Jensen– Shannon divergence between the multinomial distribution (over words) of the gloss and that of the topic, and convert the divergence value into a similarity score by subtracting it from 1. Formally, the similarity sense si and topic tj is: Methodology Our methodology is based on the WSI system described in Lau et al. (2012),1 which has been shown (Lau et al., 2012; Lau et al., 2013a; Lau et al., 2013b) to achieve state-of-the-art results over the WSI tasks from SemEval-2007 (Agirre and Soroa, 2007), SemEval-2010 (Manandhar et al., 2010) and SemEval-2013 (Navigli and Vannella, 2013; Jurgens and Klapaftis, 2013). The system is built around a Hierarchical Dirichlet Process (HDP: Teh et al. (2006)), a non-parametric variant of a Latent Dirichlet Allocation topic model (Blei et al., 2003) where the model automatically optimises the number of topics in a fully-unsupervised fashion over the training data. To learn the senses of a target lemma, we train a single topic model per target lemma. The system reads in a collection of usages of that lemma, and automatically induces topics (= senses) in the form of a multinomial distribution over words, and per-usage topic assignments (= probabilistic sense assignm"
P14-1025,S10-1011,0,0.00982955,"ds, based on simple maximum likelihood estimation.6 We then calculate the Jensen– Shannon divergence between the multinomial distribution (over words) of the gloss and that of the topic, and convert the divergence value into a similarity score by subtracting it from 1. Formally, the similarity sense si and topic tj is: Methodology Our methodology is based on the WSI system described in Lau et al. (2012),1 which has been shown (Lau et al., 2012; Lau et al., 2013a; Lau et al., 2013b) to achieve state-of-the-art results over the WSI tasks from SemEval-2007 (Agirre and Soroa, 2007), SemEval-2010 (Manandhar et al., 2010) and SemEval-2013 (Navigli and Vannella, 2013; Jurgens and Klapaftis, 2013). The system is built around a Hierarchical Dirichlet Process (HDP: Teh et al. (2006)), a non-parametric variant of a Latent Dirichlet Allocation topic model (Blei et al., 2003) where the model automatically optimises the number of topics in a fully-unsupervised fashion over the training data. To learn the senses of a target lemma, we train a single topic model per target lemma. The system reads in a collection of usages of that lemma, and automatically induces topics (= senses) in the form of a multinomial distribution"
P14-1025,C04-1177,1,0.882843,". Because of domain differences and the skewed nature of word sense distributions, it is often the case that some senses in a sense inventory will not be attested in a given corpus. A system capable of automatically finding such senses could reduce ambiguity, particularly in domain adaptation settings, while retaining rare but nevertheless viable senses. We further propose a method for applying our sense distribution acquisition system to the task of finding unattested senses — i.e., senses that are in the sense inventory but not attested in a given corpus. In contrast to the previous work of McCarthy et al. (2004a) on this topic which uses the sense ranking score from McCarthy et al. (2004b) to remove low-frequency senses from WordNet, we focus on finding senses that are unattested in the corpus on the premise that, given accurate disambiguation, rare senses in a corpus contribute to correct interpretation. Corpus instances of a word can also correspond to senses that are not present in a given sense inventory. This can be due to, for example, words taking on new meanings over time (e.g. the relaUnsupervised word sense disambiguation (WSD) methods are an attractive approach to all-words WSD due to the"
P14-1025,H05-1053,1,0.623299,"r of usages assigned to topic tj ), and T is the number of topics. The intuition behind the approach is that the predominant sense should be the sense that has relatively high similarity (in terms of lexical overlap) with high-probability topic(s). 4 WordNet annotations. The authors evaluated their method in terms of WSD accuracy over a given corpus, based on assigning all instances of a target word with the predominant sense learned from that corpus. For the remainder of the paper, we denote their system as MKWC. To compare our system (HDP-WSI) with MKWC, we apply it to the three datasets of Koeling et al. (2005). For each dataset, we use HDP to induce topics for each target lemma, compute the similarity between the topics and the WordNet senses (Equation (1)), and rank the senses based on the prevalence scores (Equation (2)). In addition to the WSD accuracy based on the predominant sense inferred from a particular corpus, we additionally compute: (1) AccUB , the upper bound for the first sense-based WSD accuracy (using the gold standard predominant sense for disambiguation);7 and (2) ERR, the error rate reduction between the accuracy for a given system (Acc) and the upper bound (AccUB ), calculated a"
P14-1025,P04-1036,1,0.84242,". Because of domain differences and the skewed nature of word sense distributions, it is often the case that some senses in a sense inventory will not be attested in a given corpus. A system capable of automatically finding such senses could reduce ambiguity, particularly in domain adaptation settings, while retaining rare but nevertheless viable senses. We further propose a method for applying our sense distribution acquisition system to the task of finding unattested senses — i.e., senses that are in the sense inventory but not attested in a given corpus. In contrast to the previous work of McCarthy et al. (2004a) on this topic which uses the sense ranking score from McCarthy et al. (2004b) to remove low-frequency senses from WordNet, we focus on finding senses that are unattested in the corpus on the premise that, given accurate disambiguation, rare senses in a corpus contribute to correct interpretation. Corpus instances of a word can also correspond to senses that are not present in a given sense inventory. This can be due to, for example, words taking on new meanings over time (e.g. the relaUnsupervised word sense disambiguation (WSD) methods are an attractive approach to all-words WSD due to the"
P14-1025,J04-1003,0,0.0384531,"ta, 2009; Yao and Durme, 2011; Lau et al., 2012). Earlier work on identifying novel senses focused on individual tokens (Erk, 2006), whereas our approach goes further in identifying groups of tokens exhibiting the same novel sense. 2 As well as sense ranking for predominant sense acquisition, automatic estimates of sense frequency distribution can be very useful for WSD for training data sampling purposes (Agirre and Martinez, 2004), entropy estimation (Jin et al., 2009), and prior probability estimates, all of which can be integrated within a WSD system (Chan and Ng, 2005; Chan and Ng, 2006; Lapata and Brew, 2004). Various approaches have been adopted, such as normalizing sense ranking scores to obtain a probability distribution (Jin et al., 2009), using subcategorisation information as an indication of verb sense (Lapata and Brew, 2004) or alternatively using parallel text (Chan and Ng, 2005; Chan and Ng, 2006; Agirre and Martinez, 2004). Background and Related Work There has been a considerable amount of research on representing word senses and disambiguating usages of words in context (WSD) as, in order to produce computational systems that understand and produce natural language, it is essential to"
P14-1025,J07-4005,1,0.874451,"e learning and sense distribution acquisition, and also the novel tasks of detecting senses which aren’t attested in the corpus, and identifying novel senses in the corpus which aren’t captured in the sense inventory. 1 Introduction The automatic determination of word sense information has been a long-term pursuit of the NLP community (Agirre and Edmonds, 2006; Navigli, 2009). Word sense distributions tend to be Zipfian, and as such, a simple but surprisingly highaccuracy back-off heuristic for word sense disambiguation (WSD) is to tag each instance of a given word with its predominant sense (McCarthy et al., 2007). Such an approach requires knowledge of predominant senses; however, word sense distributions — and predominant senses too — vary from corpus to corpus. Therefore, methods for automatically learning predominant senses 259 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 259–270, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics and Hirst, 2006), which is a very powerful heuristic approach to WSD. Most WSD systems rely upon this heuristic for back-off in the absence of strong contextual evidence (McCarthy e"
P14-1025,E12-1060,1,0.881316,"ity of Cambridge jeyhan.lau@gmail.com, paulcook@unimelb.edu.au, diana@dianamccarthy.co.uk, spandanagella@gmail.com, tb@ldwin.net Abstract and sense distributions for specific corpora are required (Koeling et al., 2005; Lapata and Brew, 2004). In this paper, we propose a method which uses topic models to estimate word sense distributions. This method is in principle applicable to all parts of speech, and moreover does not require a parser, a hierarchical sense representation or parallel text. Topic models have been used for WSD in a number of studies (Boyd-Graber et al., 2007; Li et al., 2010; Lau et al., 2012; Preiss and Stevenson, 2013; Cai et al., 2007; Knopp et al., 2013), but our work extends significantly on this earlier work in focusing on the acquisition of prior word sense distributions (and predominant senses). Because of domain differences and the skewed nature of word sense distributions, it is often the case that some senses in a sense inventory will not be attested in a given corpus. A system capable of automatically finding such senses could reduce ambiguity, particularly in domain adaptation settings, while retaining rare but nevertheless viable senses. We further propose a method f"
P14-1025,H93-1061,0,0.648686,"Missing"
P14-1025,S13-2051,1,0.840589,"we will compare our model with that proposed by McCarthy et al. 260 Recent work on finding novel senses has tended to focus on comparing diachronic corpora (Sagi et al., 2009; Cook and Stevenson, 2010; Gulordava and Baroni, 2011) and has also considered topic models (Lau et al., 2012). In a similar vein, Peirsman et al. (2010) considered the identification of words having a sense particular to one language variety with respect to another (specifically Belgian and Netherlandic Dutch). In contrast to these studies, we propose a model for comparing a corpus with a sense inventory. Carpuat et al. (2013) exploit parallel corpora to identify words in domain-specific monolingual corpora with previously-unseen translations; the method we propose does not require parallel data. 3 marginal, we make no use of parser-based features in this paper.3 The induced topics take the form of word multinomials, and are often represented by the top-N words in descending order of conditional probability. We interpret each topic as a sense of the target lemma.4 To illustrate this, we give the example of topics induced by the HDP model for network in Table 1. We refer to this method as HDP-WSI henceforth.5 In pre"
P14-1025,E06-1016,0,0.357917,"Missing"
P14-1025,S13-2035,0,0.0261491,"we will compare our model with that proposed by McCarthy et al. 260 Recent work on finding novel senses has tended to focus on comparing diachronic corpora (Sagi et al., 2009; Cook and Stevenson, 2010; Gulordava and Baroni, 2011) and has also considered topic models (Lau et al., 2012). In a similar vein, Peirsman et al. (2010) considered the identification of words having a sense particular to one language variety with respect to another (specifically Belgian and Netherlandic Dutch). In contrast to these studies, we propose a model for comparing a corpus with a sense inventory. Carpuat et al. (2013) exploit parallel corpora to identify words in domain-specific monolingual corpora with previously-unseen translations; the method we propose does not require parallel data. 3 marginal, we make no use of parser-based features in this paper.3 The induced topics take the form of word multinomials, and are often represented by the top-N words in descending order of conditional probability. We interpret each topic as a sense of the target lemma.4 To illustrate this, we give the example of topics induced by the HDP model for network in Table 1. We refer to this method as HDP-WSI henceforth.5 In pre"
P14-1025,W11-1102,0,0.0307236,"Missing"
P14-1025,P10-4014,0,0.10309,"Missing"
P14-1025,S07-1006,0,0.0131677,"(Gella et al., to appear) based on text from ukWaC (Ferraresi et al., 2008) and Twitter, and annotated using the Macmillan English Dictionary10 (henceforth “Macmillan”). For the purposes of this research, the choice of Macmillan is significant in that it is a conventional dictionary with sense definitions and examples, but no linking between senses.11 In terms of the original research which gave rise to the sense-tagged dataset, Macmillan was chosen over WordNet for reasons including: (1) the well-documented difficulties of sense tagging with fine-grained WordNet senses (Palmer et al., 2004; Navigli et al., 2007); (2) the regular update cycle of Macmillan (meaning it contains many recently-emerged senses); and (3) the finding in a preliminary sense-tagging task that it better captured Twitter usages than WordNet (and also OntoNotes: Hovy et al. (2006)). The dataset is made up of 20 target nouns which were selected to span the high- to mid-frequency range in both Twitter and the ukWaC corpus, and have at least 3 Macmillan senses. The average sense ambiguity of the 20 target nouns in Macmillan is 5.6 (but 12.3 in WordNet). 100 usages of each target noun were sampled from each of Twitter (from a crawl ov"
P14-1025,W04-2807,0,0.0704841,"move to a new dataset (Gella et al., to appear) based on text from ukWaC (Ferraresi et al., 2008) and Twitter, and annotated using the Macmillan English Dictionary10 (henceforth “Macmillan”). For the purposes of this research, the choice of Macmillan is significant in that it is a conventional dictionary with sense definitions and examples, but no linking between senses.11 In terms of the original research which gave rise to the sense-tagged dataset, Macmillan was chosen over WordNet for reasons including: (1) the well-documented difficulties of sense tagging with fine-grained WordNet senses (Palmer et al., 2004; Navigli et al., 2007); (2) the regular update cycle of Macmillan (meaning it contains many recently-emerged senses); and (3) the finding in a preliminary sense-tagging task that it better captured Twitter usages than WordNet (and also OntoNotes: Hovy et al. (2006)). The dataset is made up of 20 target nouns which were selected to span the high- to mid-frequency range in both Twitter and the ukWaC corpus, and have at least 3 Macmillan senses. The average sense ambiguity of the 20 target nouns in Macmillan is 5.6 (but 12.3 in WordNet). 100 usages of each target noun were sampled from each of T"
P14-1025,N13-1079,0,0.0120313,"eyhan.lau@gmail.com, paulcook@unimelb.edu.au, diana@dianamccarthy.co.uk, spandanagella@gmail.com, tb@ldwin.net Abstract and sense distributions for specific corpora are required (Koeling et al., 2005; Lapata and Brew, 2004). In this paper, we propose a method which uses topic models to estimate word sense distributions. This method is in principle applicable to all parts of speech, and moreover does not require a parser, a hierarchical sense representation or parallel text. Topic models have been used for WSD in a number of studies (Boyd-Graber et al., 2007; Li et al., 2010; Lau et al., 2012; Preiss and Stevenson, 2013; Cai et al., 2007; Knopp et al., 2013), but our work extends significantly on this earlier work in focusing on the acquisition of prior word sense distributions (and predominant senses). Because of domain differences and the skewed nature of word sense distributions, it is often the case that some senses in a sense inventory will not be attested in a given corpus. A system capable of automatically finding such senses could reduce ambiguity, particularly in domain adaptation settings, while retaining rare but nevertheless viable senses. We further propose a method for applying our sense distri"
P14-1025,W09-0214,0,0.0845474,"Missing"
P14-1025,S07-1053,0,\N,Missing
P14-1025,N06-2015,0,\N,Missing
P14-1025,C98-2122,0,\N,Missing
P15-2139,D14-1096,1,0.401152,"Epos , Earc , W1 and W2 can be shared as indicated with dashed lines. In particular we expect this to be the case when languages use the same POS tagset and arc label sets, as we presume herein. This assumption is motivated by the development of unified annotation for many languages (Nivre et al., 2015; Petrov et al., 2012; McDonald et al., 2013). To allow parameter sharing between languages we could jointly train the parser on the source and target language simultaneously. However, we leave this for future work. Here we take an alternative approach, namely regularization in a similar vein to Duong et al. (2014). First we train a lexicalized neural network parser on the source resource-rich language (English), as described in Section 2. The learned parameters are en , E en , E en , W en , W en . Second, we incorEword pos arc 1 2 porate English parameters as a prior for the target language training. This is straightforward when we use the same architecture, such as a neural network parser, for the target language. All we need to do is modify the learning objective function so that it includes the regularization part. However, we don’t want to regularize the en part related to Eword since it will be ve"
P15-2139,2005.mtsummit-papers.11,0,0.0269162,"h language in the collection. Some languages have over 400k tokens such as cs, fr and es, meanwhile, hu and ga have only around 25k tokens. 4.2 Table 1: Number of tokens (× 1,000) for each language in the Universal Dependency Treebank collection. We initialize the target language word embeddings Eword of our neural network cross-lingual model with pre-trained embeddings. This is an advantage since we can incorporate monolingual data which is usually available for resource-poor languages. We collect monolingual data for each language from the Machine Translation Workshop (WMT) data,5 Europarl (Koehn, 2005) and EU Bookshop Corpus (Skadin¸sˇ et al., 2014). The size of monolingual data also varies significantly. There are languages such as English and German with more than 400 million words, whereas, Irish only has 4 million. We use the skip-gram model from word2vec to induce 50-dimension word embeddings (Mikolov et al., 2013). tagset and arc label annotation for the source and target language. The same POS tagset is required so that the source language parser has similar structure with the target language parser. The requirement of same arc label annotation is mainly needed for evaluation using t"
P15-2139,P14-1126,0,0.0386164,"Missing"
P15-2139,P05-1012,0,0.018181,"uide the learning process. Our model saves at least half of the annotation effort to reach the same accuracy compared with using the purely supervised method. 1 Introduction Dependency parsing is a crucial component of many natural language processing systems, for ¨ ur and tasks such as text classification (Ozg¨ G¨ung¨or, 2010), statistical machine translation (Xu et al., 2009), relation extraction (Bunescu and Mooney, 2005), and question answering (Cui et al., 2005). Supervised approaches to dependency parsing have been successful for languages where relatively large treebanks are available (McDonald et al., 2005). However, for many languages, annotated treebanks are not available. They are costly to create, requiring careful design, testing and subsequent refinement of annotation guidelines, along with assessment and management of annotator quality (B¨ohmov´a et al., 2001). The Universal Treebank Annotation Guidelines aim at providing unified annotation for many languages enabling cross-lingual comparison (Nivre et al., 2015). This project provides a starting point for developing a treebank for resource-poor languages. However, a mature parser requires a large treebank for training, and this is still"
P15-2139,N09-1028,0,0.0172191,"costly and time-consuming to build. We propose a learning method that needs less data, based on the observation that there are underlying shared structures across languages. We exploit cues from a different source language in order to guide the learning process. Our model saves at least half of the annotation effort to reach the same accuracy compared with using the purely supervised method. 1 Introduction Dependency parsing is a crucial component of many natural language processing systems, for ¨ ur and tasks such as text classification (Ozg¨ G¨ung¨or, 2010), statistical machine translation (Xu et al., 2009), relation extraction (Bunescu and Mooney, 2005), and question answering (Cui et al., 2005). Supervised approaches to dependency parsing have been successful for languages where relatively large treebanks are available (McDonald et al., 2005). However, for many languages, annotated treebanks are not available. They are costly to create, requiring careful design, testing and subsequent refinement of annotation guidelines, along with assessment and management of annotator quality (B¨ohmov´a et al., 2001). The Universal Treebank Annotation Guidelines aim at providing unified annotation for many l"
P15-2139,D11-1006,0,0.0235477,"Missing"
P15-2139,P13-2017,0,0.0373468,"Missing"
P15-2139,I08-3008,0,0.0233446,"Missing"
P15-2139,petrov-etal-2012-universal,0,0.0150699,". 3 Cross-lingual parser Our model takes advantage of underlying structure shared between languages. Given the source language parsing structure as in Figure 1 (left), the set of parameters Eword will be different for the target language parser shown in Figure 1 (right) but we hypothesize that Epos , Earc , W1 and W2 can be shared as indicated with dashed lines. In particular we expect this to be the case when languages use the same POS tagset and arc label sets, as we presume herein. This assumption is motivated by the development of unified annotation for many languages (Nivre et al., 2015; Petrov et al., 2012; McDonald et al., 2013). To allow parameter sharing between languages we could jointly train the parser on the source and target language simultaneously. However, we leave this for future work. Here we take an alternative approach, namely regularization in a similar vein to Duong et al. (2014). First we train a lexicalized neural network parser on the source resource-rich language (English), as described in Section 2. The learned parameters are en , E en , E en , W en , W en . Second, we incorEword pos arc 1 2 porate English parameters as a prior for the target language training. This is stra"
P15-2139,skadins-etal-2014-billions,0,0.024592,"Missing"
P15-2139,N13-1126,0,0.0257058,"Missing"
P15-2139,D14-1082,0,\N,Missing
P15-2139,H05-1091,0,\N,Missing
P18-2055,D12-1091,0,0.0285564,"odel by concatenating the embedding representing each VNC token instance with a one-dimensional vector which is one if the VNC occurs in its canonical form, and zero otherwise. We first consider results for the −CF setup. On both DEV and TEST, the accuracy achieved by each supervised model is higher than that of the unsupervised CForm approach, except for Siamese CBOW on TEST. The word2vec model achieves the highest accuracy on DEV and TEST of 0.830 and 0.804, respectively. The difference between the word2vec model and the next-best model, skip-thoughts, is significant using a bootstrap test (Berg-Kirkpatrick et al., 2012) with 10k repetitions for DEV (p = 0.006), but not for TEST (p = 0.051). Nevertheless, it is remarkable that the relatively simple approach to averaging word embeddings used by word2vec performs as well as, or better than, the much more complex skipthoughts model used by Salton et al. (2016).8 racies than those obtained by the skip-thoughts model. 9 In order to determine that this improvement is due to the information about canonical forms carried by the additional feature in the +CF setup, and not due to the increase in number of dimensions, we performed additional experiments in which we con"
P18-2055,E06-1042,0,0.0348141,"f MWEs (e.g., Patrick and Fletcher, 2005; Uchiyama et al., 2005), including English VNCs (e.g., Fazly et al., 2009; Salton et al., 2016), although some recent work has considered the identification of a broad range of kinds of MWEs (e.g., Schneider et al., 2014; Brooke et al., 2014; Savary et al., 2017). Work on MWE identification has leveraged rich linguistic knowledge of the constructions under consideration (e.g., Fazly et al., 2009; Fothergill and Baldwin, 2012), treated literal and idiomatic as two senses of an expression and applied approaches similar to word-sense disambiguation (e.g., Birke and Sarkar, 2006; Hashimoto and Kawahara, 2008), incorporated topic models (e.g., Li et al., 2010), and made use of distributed representations of words (Gharbieh et al., 2016). In the most closely related work to ours, Salton et al. (2016) represent token instances of VNCs by embedding the sentence that they occur in using skip-thoughts (Kiros et al., 2015) — an encoder– decoder model that can be viewed as a sentencelevel counterpart to the word2vec (Mikolov et al., 2013) skip-gram model. During training the target sentence is encoded using a recurrent neural network, and is used to predict the previous and"
P18-2055,C14-1071,0,0.0191877,"ng-based approaches to VNC token classification, and show that this linguistic knowledge can be leveraged to improve such approaches. ments, indicating that this rich linguistic knowledge is complementary to that available in distributed representations. 2 Related work Much research on MWE identification has focused on specific kinds of MWEs (e.g., Patrick and Fletcher, 2005; Uchiyama et al., 2005), including English VNCs (e.g., Fazly et al., 2009; Salton et al., 2016), although some recent work has considered the identification of a broad range of kinds of MWEs (e.g., Schneider et al., 2014; Brooke et al., 2014; Savary et al., 2017). Work on MWE identification has leveraged rich linguistic knowledge of the constructions under consideration (e.g., Fazly et al., 2009; Fothergill and Baldwin, 2012), treated literal and idiomatic as two senses of an expression and applied approaches similar to word-sense disambiguation (e.g., Birke and Sarkar, 2006; Hashimoto and Kawahara, 2008), incorporated topic models (e.g., Li et al., 2010), and made use of distributed representations of words (Gharbieh et al., 2016). In the most closely related work to ours, Salton et al. (2016) represent token instances of VNCs b"
P18-2055,J09-1005,1,0.381737,"Salton et al. (2016). VNCs exhibit lexico-syntactic fixedness. For example, the idiomatic interpretation in example 1 above is typically only accessible when the verb see has active voice, the determiner is null, and the noun star is in plural form, as in see stars or seeing stars. Usages with a determiner (as in example 2), a singular noun (e.g., see a star), or passive voice (e.g., stars were seen) typically only have the literal interpretation. In this paper we further incorporate knowledge of the lexico-syntactic fixedness of VNCs — automatically acquired from corpora using the method of Fazly et al. (2009) — into our various embedding-based approaches. Our experimental results show that this leads to substantial improveVerb–noun combinations (VNCs) — e.g., blow the whistle, hit the roof, and see stars — are a common type of English idiom that are ambiguous with literal usages. In this paper we propose and evaluate models for classifying VNC usages as idiomatic or literal, based on a variety of approaches to forming distributed representations. Our results show that a model based on averaging word embeddings performs on par with, or better than, a previously-proposed approach based on skip-thoug"
P18-2055,S12-1017,0,0.0187646,"knowledge is complementary to that available in distributed representations. 2 Related work Much research on MWE identification has focused on specific kinds of MWEs (e.g., Patrick and Fletcher, 2005; Uchiyama et al., 2005), including English VNCs (e.g., Fazly et al., 2009; Salton et al., 2016), although some recent work has considered the identification of a broad range of kinds of MWEs (e.g., Schneider et al., 2014; Brooke et al., 2014; Savary et al., 2017). Work on MWE identification has leveraged rich linguistic knowledge of the constructions under consideration (e.g., Fazly et al., 2009; Fothergill and Baldwin, 2012), treated literal and idiomatic as two senses of an expression and applied approaches similar to word-sense disambiguation (e.g., Birke and Sarkar, 2006; Hashimoto and Kawahara, 2008), incorporated topic models (e.g., Li et al., 2010), and made use of distributed representations of words (Gharbieh et al., 2016). In the most closely related work to ours, Salton et al. (2016) represent token instances of VNCs by embedding the sentence that they occur in using skip-thoughts (Kiros et al., 2015) — an encoder– decoder model that can be viewed as a sentencelevel counterpart to the word2vec (Mikolov"
P18-2055,W16-1817,1,0.866535,"ork has considered the identification of a broad range of kinds of MWEs (e.g., Schneider et al., 2014; Brooke et al., 2014; Savary et al., 2017). Work on MWE identification has leveraged rich linguistic knowledge of the constructions under consideration (e.g., Fazly et al., 2009; Fothergill and Baldwin, 2012), treated literal and idiomatic as two senses of an expression and applied approaches similar to word-sense disambiguation (e.g., Birke and Sarkar, 2006; Hashimoto and Kawahara, 2008), incorporated topic models (e.g., Li et al., 2010), and made use of distributed representations of words (Gharbieh et al., 2016). In the most closely related work to ours, Salton et al. (2016) represent token instances of VNCs by embedding the sentence that they occur in using skip-thoughts (Kiros et al., 2015) — an encoder– decoder model that can be viewed as a sentencelevel counterpart to the word2vec (Mikolov et al., 2013) skip-gram model. During training the target sentence is encoded using a recurrent neural network, and is used to predict the previous and next sentences. Salton et al. then use these sentence embeddings, representing VNC token instances, as features in a supervised classifier. We treat this skip-t"
P18-2055,D08-1104,0,0.0832679,"Missing"
P18-2055,N16-1162,0,0.0312575,"Missing"
P18-2055,P16-1019,0,0.134437,"combinations Milton King and Paul Cook Faculty of Computer Science, University of New Brunswick Fredericton, NB E3B 5A3 Canada milton.king@unb.ca, paul.cook@unb.ca Abstract MWE identification is the task of automatically determining which word combinations at the token-level form MWEs (Baldwin and Kim, 2010), and must be able to make such distinctions. This is particularly important for applications such as machine translation (Sag et al., 2002), where the appropriate meaning of word combinations in context must be preserved for accurate translation. In this paper, following prior work (e.g., Salton et al., 2016), we frame token-level identification of VNCs as a supervised binary classification problem, i.e., idiomatic vs. literal. We consider a range of approaches to forming distributed representations of the context in which a VNC occurs, including word embeddings (Mikolov et al., 2013), word embeddings tailored to representing sentences (Kenter et al., 2016), and skip-thoughts sentence embeddings (Kiros et al., 2015). We then train a support vector machine (SVM) on these representations to classify unseen VNC instances. Surprisingly, we find that an approach based on representing sentences as the a"
P18-2055,P16-1089,0,0.0446973,"Missing"
P18-2055,Q14-1016,0,0.0212707,"nical forms into embedding-based approaches to VNC token classification, and show that this linguistic knowledge can be leveraged to improve such approaches. ments, indicating that this rich linguistic knowledge is complementary to that available in distributed representations. 2 Related work Much research on MWE identification has focused on specific kinds of MWEs (e.g., Patrick and Fletcher, 2005; Uchiyama et al., 2005), including English VNCs (e.g., Fazly et al., 2009; Salton et al., 2016), although some recent work has considered the identification of a broad range of kinds of MWEs (e.g., Schneider et al., 2014; Brooke et al., 2014; Savary et al., 2017). Work on MWE identification has leveraged rich linguistic knowledge of the constructions under consideration (e.g., Fazly et al., 2009; Fothergill and Baldwin, 2012), treated literal and idiomatic as two senses of an expression and applied approaches similar to word-sense disambiguation (e.g., Birke and Sarkar, 2006; Hashimoto and Kawahara, 2008), incorporated topic models (e.g., Li et al., 2010), and made use of distributed representations of words (Gharbieh et al., 2016). In the most closely related work to ours, Salton et al. (2016) represent toke"
P18-2055,P10-1116,0,0.0253143,"(e.g., Fazly et al., 2009; Salton et al., 2016), although some recent work has considered the identification of a broad range of kinds of MWEs (e.g., Schneider et al., 2014; Brooke et al., 2014; Savary et al., 2017). Work on MWE identification has leveraged rich linguistic knowledge of the constructions under consideration (e.g., Fazly et al., 2009; Fothergill and Baldwin, 2012), treated literal and idiomatic as two senses of an expression and applied approaches similar to word-sense disambiguation (e.g., Birke and Sarkar, 2006; Hashimoto and Kawahara, 2008), incorporated topic models (e.g., Li et al., 2010), and made use of distributed representations of words (Gharbieh et al., 2016). In the most closely related work to ours, Salton et al. (2016) represent token instances of VNCs by embedding the sentence that they occur in using skip-thoughts (Kiros et al., 2015) — an encoder– decoder model that can be viewed as a sentencelevel counterpart to the word2vec (Mikolov et al., 2013) skip-gram model. During training the target sentence is encoded using a recurrent neural network, and is used to predict the previous and next sentences. Salton et al. then use these sentence embeddings, representing VNC"
S13-1030,J01-3001,0,0.0357172,"d petition, and case and verdict are also similar. One straightforward way of estimating semantic similarity of two texts is by using approaches based on the similarity of the surface forms of the words they contain. However, such methods are not capable of capturing similarity or relatedness at the lexical level, and moreover, they do not exploit the context in which individual words are used in a target text. Nevertheless, a variety of knowledge sources — including part-of-speech, collocations, syntax, and domain — can be used to identify the usage or sense of words in context (McRoy, 1992; Agirre and Martinez, 2001; Agirre and Stevenson, 2006) to address these issues. Despite their limitations, string similarity measures have been widely used in previous semantic similarity tasks (Agirre et al., 2012; Islam and Inkpen, 2008). Latent variable models have also been used to estimate the semantic similarity between words, word usages, and texts (Steyvers and Griffiths, 2007; Lui et al., 2012; Guo and Diab, 2012; Dinu and Lapata, 2010). In this paper, we consider three different ways of measuring semantic similarity based on word and word usage similarity: 1. String-based similarity to measure surfacelevel l"
S13-1030,S12-1051,0,0.15264,"e words they contain. However, such methods are not capable of capturing similarity or relatedness at the lexical level, and moreover, they do not exploit the context in which individual words are used in a target text. Nevertheless, a variety of knowledge sources — including part-of-speech, collocations, syntax, and domain — can be used to identify the usage or sense of words in context (McRoy, 1992; Agirre and Martinez, 2001; Agirre and Stevenson, 2006) to address these issues. Despite their limitations, string similarity measures have been widely used in previous semantic similarity tasks (Agirre et al., 2012; Islam and Inkpen, 2008). Latent variable models have also been used to estimate the semantic similarity between words, word usages, and texts (Steyvers and Griffiths, 2007; Lui et al., 2012; Guo and Diab, 2012; Dinu and Lapata, 2010). In this paper, we consider three different ways of measuring semantic similarity based on word and word usage similarity: 1. String-based similarity to measure surfacelevel lexical similarity, taking into account morphology and abbreviations (e.g., dismisses and dismissed, and government’s and govt’s); 207 Second Joint Conference on Lexical and Computational Se"
S13-1030,S13-1004,0,0.0474373,"Missing"
S13-1030,S12-1059,0,0.0950475,"Missing"
S13-1030,S12-1097,0,0.0219499,"y (SS) features. LEV2, SW, and NAW have not been previously considered for STS. 2.2 Topic Modelling Similarity Measures (TM) The topic modelling features (TM) are based on Latent Dirichlet Allocation (LDA), a generative probabilistic model in which each document is modeled as a distribution over a finite set of topics, and each topic is represented as a distribution over words (Blei et al., 2003). We build a topic model on a background corpus, and then for each target text we create a topic vector based on the topic allocations of its content words, based on the method developed by Lui et al. (2012) for predicting word usage similarity. The choice of the number of topics, T , can have a big impact on the performance of this method. Choosing a small T might give overlybroad topics, while a large T might lead to uninterpretable topics (Steyvers and Griffiths, 2007). Moreover smaller numbers of topics have been shown to perform poorly on both sentence similarity (Guo and Diab, 2012) and word usage similarity tasks (Lui et al., 2012). We therefore build topic models for 33 values of T in the range 2, 3, 5, 8, 10, 50, 80, 100, 150, 200, ...1350. The background corpus used for generating the t"
S13-1030,P06-4018,0,0.0677132,"res contains various string similarity measures (SS), which compare the target texts in terms of the words they contain and the order of the words (Islam and Inkpen, 2008). In the SemEval 2012 STS task (Agirre et al., 2012) such features were used by several participants (Biggins et al., 2012; B¨ar et al., 2012; Heilman and Madnani, 2012), including the first-ranked team (B¨ar et al., 2012) who considered string similarity measures alongside a wide range of other features. For our string similarity features, the texts were lemmatized using the implementation of Lancaster Stemming in NLTK 2.0 (Bird, 2006), and all punctuation was removed. Limited stopword removal was carried out by eliminating the words a, and, and the. The output of each string similarity measure is normalized to the range of [0, 1], where 0 indicates that the texts are completely different, while 1 means they are identical. The normalization method for each feature is described in Salehi and Cook (to 208 appear), wherein the authors applied string similarity measures successfully to the task of predicting the compositionality of multiword expressions. Identical Unigrams (IU): This feature measures the number of words shared"
S13-1030,D10-1113,0,0.032236,", a variety of knowledge sources — including part-of-speech, collocations, syntax, and domain — can be used to identify the usage or sense of words in context (McRoy, 1992; Agirre and Martinez, 2001; Agirre and Stevenson, 2006) to address these issues. Despite their limitations, string similarity measures have been widely used in previous semantic similarity tasks (Agirre et al., 2012; Islam and Inkpen, 2008). Latent variable models have also been used to estimate the semantic similarity between words, word usages, and texts (Steyvers and Griffiths, 2007; Lui et al., 2012; Guo and Diab, 2012; Dinu and Lapata, 2010). In this paper, we consider three different ways of measuring semantic similarity based on word and word usage similarity: 1. String-based similarity to measure surfacelevel lexical similarity, taking into account morphology and abbreviations (e.g., dismisses and dismissed, and government’s and govt’s); 207 Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference c and the Shared Task, pages 207–215, Atlanta, Georgia, June 13-14, 2013. 2013 Association for Computational Linguistics 2. Latent variable models of similarity to capture wo"
S13-1030,S12-1086,0,0.11977,"t text. Nevertheless, a variety of knowledge sources — including part-of-speech, collocations, syntax, and domain — can be used to identify the usage or sense of words in context (McRoy, 1992; Agirre and Martinez, 2001; Agirre and Stevenson, 2006) to address these issues. Despite their limitations, string similarity measures have been widely used in previous semantic similarity tasks (Agirre et al., 2012; Islam and Inkpen, 2008). Latent variable models have also been used to estimate the semantic similarity between words, word usages, and texts (Steyvers and Griffiths, 2007; Lui et al., 2012; Guo and Diab, 2012; Dinu and Lapata, 2010). In this paper, we consider three different ways of measuring semantic similarity based on word and word usage similarity: 1. String-based similarity to measure surfacelevel lexical similarity, taking into account morphology and abbreviations (e.g., dismisses and dismissed, and government’s and govt’s); 207 Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference c and the Shared Task, pages 207–215, Atlanta, Georgia, June 13-14, 2013. 2013 Association for Computational Linguistics 2. Latent variable models of"
S13-1030,S12-1076,0,0.0175684,"n previous research, rather than combine these features with the myriad of features that have been proposed by others for the task. 2 Text Similarity Measures In this section we describe the various features used in our system. 2.1 String Similarity Measures (SS) Our first set of features contains various string similarity measures (SS), which compare the target texts in terms of the words they contain and the order of the words (Islam and Inkpen, 2008). In the SemEval 2012 STS task (Agirre et al., 2012) such features were used by several participants (Biggins et al., 2012; B¨ar et al., 2012; Heilman and Madnani, 2012), including the first-ranked team (B¨ar et al., 2012) who considered string similarity measures alongside a wide range of other features. For our string similarity features, the texts were lemmatized using the implementation of Lancaster Stemming in NLTK 2.0 (Bird, 2006), and all punctuation was removed. Limited stopword removal was carried out by eliminating the words a, and, and the. The output of each string similarity measure is normalized to the range of [0, 1], where 0 indicates that the texts are completely different, while 1 means they are identical. The normalization method for each f"
S13-1030,U12-1006,1,0.90757,"re used in a target text. Nevertheless, a variety of knowledge sources — including part-of-speech, collocations, syntax, and domain — can be used to identify the usage or sense of words in context (McRoy, 1992; Agirre and Martinez, 2001; Agirre and Stevenson, 2006) to address these issues. Despite their limitations, string similarity measures have been widely used in previous semantic similarity tasks (Agirre et al., 2012; Islam and Inkpen, 2008). Latent variable models have also been used to estimate the semantic similarity between words, word usages, and texts (Steyvers and Griffiths, 2007; Lui et al., 2012; Guo and Diab, 2012; Dinu and Lapata, 2010). In this paper, we consider three different ways of measuring semantic similarity based on word and word usage similarity: 1. String-based similarity to measure surfacelevel lexical similarity, taking into account morphology and abbreviations (e.g., dismisses and dismissed, and government’s and govt’s); 207 Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference c and the Shared Task, pages 207–215, Atlanta, Georgia, June 13-14, 2013. 2013 Association for Computational Linguistics 2. Latent"
S13-1030,U12-1019,1,0.837479,"nce is drawn from. For test data, the entire vector consists of 0s. The second approach we considered is based on metalearning, and we will refer to it as domain stacking. In domain stacking, we train a regressor for each domain (the level 0 regressors (Wolpert, 1992)). Each of these regressors is then applied to a test instance to produce a predicted value (the level 0 prediction). These predictions are then combined using a second regressor (the level 1 regressor), to produce a final prediction for each instance (the level 1 prediction). This approach is closely related to feature stacking (Lui, 2012) and stacked generalization (Wolpert, 1992). A general principle of metalearning is to combine multiple weaker (“less accurate”) predictors — termed level 0 predictors — to produce a stronger (“more accurate”) predictor — the level 1 predictor. In stacked generalization, the level 0 predictors are different learning algorithms. In feature stacking, they are the same algorithm trained on different subsets of features, in this work corresponding to different methods for es210 timating STS (Section 2). In domain stacking, the level 0 predictions are obtained from subsets of the training data, whe"
S13-1030,J92-1001,0,0.356307,"s) of plea and petition, and case and verdict are also similar. One straightforward way of estimating semantic similarity of two texts is by using approaches based on the similarity of the surface forms of the words they contain. However, such methods are not capable of capturing similarity or relatedness at the lexical level, and moreover, they do not exploit the context in which individual words are used in a target text. Nevertheless, a variety of knowledge sources — including part-of-speech, collocations, syntax, and domain — can be used to identify the usage or sense of words in context (McRoy, 1992; Agirre and Martinez, 2001; Agirre and Stevenson, 2006) to address these issues. Despite their limitations, string similarity measures have been widely used in previous semantic similarity tasks (Agirre et al., 2012; Islam and Inkpen, 2008). Latent variable models have also been used to estimate the semantic similarity between words, word usages, and texts (Steyvers and Griffiths, 2007; Lui et al., 2012; Guo and Diab, 2012; Dinu and Lapata, 2010). In this paper, we consider three different ways of measuring semantic similarity based on word and word usage similarity: 1. String-based similarit"
S13-1030,S13-1039,1,0.771798,"Missing"
S13-1036,S12-1051,0,0.0483651,"8 0.09 0.29 R AND E XPANDED 0.09 0.06 0.18 Original Expanded RandExpanded ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● 2 3 5 8 10 20 30 50 100 150 200 250 300 350 400 450 500 Table 2: Spearman rank correlation (ρ) for each method based on each background corpus. The best result for each corpus is shown in bold. Spearman correlation ρ −0.1 0.0 0.1 0.2 0.3 Model Baseline WTMF LDA T models by predicting “missing” words on the basis of the message content, and including them in the vector representation. Guo and Diab showed WTMF to outperform LDA on the SemEval-2012 semantic textual similarity task (STS) (Agirre et al., 2012). The semantic space required for this model as applied here is built from the background tweets corresponding to the target word. We experimented with the missing weight parameter wm of WTMF in the range [0.05, 0.01, 0.005, 0.0005] and with dimensions K=100 and report the best results (wm = 0.0005). 4.3 Topic Modelling Latent Dirichlet Allocation (LDA) (Blei et al., 2003) is a generative model in which a document is modeled as a finite mixture of topics, where each topic is represented as a multinomial distribution of words. We treat each tweet as a document. Topics sensitive to each target w"
S13-1036,P12-1091,0,0.175991,"icon or sense inventory. The task is to rate on an ordinal scale the 2. @USER has his number on a piece of paper and I walkd off! The task is to predict a real-valued number in the range [1, 5] for the similarity in the respective usages of paper, where 1 indicates the usages are completely different and 5 indicates they are identical. In this paper we develop a new U SIM dataset based on Twitter data. In experiments on this dataset we demonstrate that an LDA-based topic modelling approach outperforms a baseline distributional semantic approach and Weighted Textual Matrix Factorization (WTMF: Guo and Diab (2012a)). We further show that context expansion using a novel hashtag-based strategy improves both the LDAbased method and WTMF. 2 Related Work Word sense disambiguation (WSD) is the task of determining the particular sense of a word from a given set of pre-defined senses (Navigli, 2009). It 248 Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference c and the Shared Task, pages 248–253, Atlanta, Georgia, June 13-14, 2013. 2013 Association for Computational Linguistics contrasts with word sense induction (WSI), where the senses of a given"
S13-1036,S12-1086,0,0.0849459,"icon or sense inventory. The task is to rate on an ordinal scale the 2. @USER has his number on a piece of paper and I walkd off! The task is to predict a real-valued number in the range [1, 5] for the similarity in the respective usages of paper, where 1 indicates the usages are completely different and 5 indicates they are identical. In this paper we develop a new U SIM dataset based on Twitter data. In experiments on this dataset we demonstrate that an LDA-based topic modelling approach outperforms a baseline distributional semantic approach and Weighted Textual Matrix Factorization (WTMF: Guo and Diab (2012a)). We further show that context expansion using a novel hashtag-based strategy improves both the LDAbased method and WTMF. 2 Related Work Word sense disambiguation (WSD) is the task of determining the particular sense of a word from a given set of pre-defined senses (Navigli, 2009). It 248 Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference c and the Shared Task, pages 248–253, Atlanta, Georgia, June 13-14, 2013. 2013 Association for Computational Linguistics contrasts with word sense induction (WSI), where the senses of a given"
S13-1036,D12-1039,1,0.926167,"ty of social media applications, there has been a steep rise in the amount of “post”-based user-generated text (including microblog posts, status updates and comments) (Bennett, 2012). This data has been identified as having potential for applications ranging from trend analysis (Lau et al., 2012a) and event detection (Osborne et al., 2012) to election outcome prediction (O’Connor et al., 2010). However, given that posts are generally very short, noisy and lacking in context, traditional NLP approaches tend to perform poorly over social media data (Hong and Davison, 2010; Ritter et al., 2011; Han et al., 2012). This is the first paper to address the task of lexical semantic interpretation in microblog data based on word usage similarity. Word usage similarity (U SIM: Erk et al. (2009)) is a relatively new paradigm for capturing similarity in the usages of a given word independently of any lexicon or sense inventory. The task is to rate on an ordinal scale the 2. @USER has his number on a piece of paper and I walkd off! The task is to predict a real-valued number in the range [1, 5] for the similarity in the respective usages of paper, where 1 indicates the usages are completely different and 5 indi"
S13-1036,C12-1093,0,0.290231,"irs annotated for usage similarity for a set of 10 nouns. The results show that our topic modelling approach outperforms the other two methods. 1 1. Deportation of Afghan Asylum Seekers from Australia : This paper aims to critically evaluate a newly signed agree. Introduction In recent years, with the growing popularity of social media applications, there has been a steep rise in the amount of “post”-based user-generated text (including microblog posts, status updates and comments) (Bennett, 2012). This data has been identified as having potential for applications ranging from trend analysis (Lau et al., 2012a) and event detection (Osborne et al., 2012) to election outcome prediction (O’Connor et al., 2010). However, given that posts are generally very short, noisy and lacking in context, traditional NLP approaches tend to perform poorly over social media data (Hong and Davison, 2010; Ritter et al., 2011; Han et al., 2012). This is the first paper to address the task of lexical semantic interpretation in microblog data based on word usage similarity. Word usage similarity (U SIM: Erk et al. (2009)) is a relatively new paradigm for capturing similarity in the usages of a given word independently of"
S13-1036,E12-1060,1,0.928201,"irs annotated for usage similarity for a set of 10 nouns. The results show that our topic modelling approach outperforms the other two methods. 1 1. Deportation of Afghan Asylum Seekers from Australia : This paper aims to critically evaluate a newly signed agree. Introduction In recent years, with the growing popularity of social media applications, there has been a steep rise in the amount of “post”-based user-generated text (including microblog posts, status updates and comments) (Bennett, 2012). This data has been identified as having potential for applications ranging from trend analysis (Lau et al., 2012a) and event detection (Osborne et al., 2012) to election outcome prediction (O’Connor et al., 2010). However, given that posts are generally very short, noisy and lacking in context, traditional NLP approaches tend to perform poorly over social media data (Hong and Davison, 2010; Ritter et al., 2011; Han et al., 2012). This is the first paper to address the task of lexical semantic interpretation in microblog data based on word usage similarity. Word usage similarity (U SIM: Erk et al. (2009)) is a relatively new paradigm for capturing similarity in the usages of a given word independently of"
S13-1036,P12-3005,0,0.0370235,"based topic modeling and WTMF to estimate word similarities in twitter data. LDA based topic modeling has been earlier studied on Twitter data for tweet classification (Ramage et al., 2010) and tweet clustering (Jin et al., 2011). 3 Data Preparation This section describes the construction of the U SIMtweet dataset based on microblog posts (“tweets”) from Twitter. We describe the pre-processing steps taken to sample the tweets in our datasets, outline the annotation process, and then describe the background corpora used in our experiments. 3.1 Data preprocessing English tweets using langid.py (Lui and Baldwin, 2012). We next performed lexical normalization using the dictionary of Han et al. (2012) to convert lexical variants (e.g., tmrw) to their standard forms (e.g., tomorrow) and reduce data sparseness. As our target words, we chose the 10 nouns from the original U SIM dataset of Erk et al. (2009) (bar, charge, execution, field, figure, function, investigator, match, paper, post), and identified tweets containing the target words as nouns using the CMU Twitter POS tagger (Owoputi et al., 2012). 3.2 To collect word usage similarity scores for Twitter message pairs, we used a setup similar to that of Erk"
S13-1036,U12-1006,0,0.364261,"au et al., 2012b). WSD and WSI have been the predominant paradigms for capturing and evaluating lexical semantics, and both assume that each usage corresponds to exactly one of a set of discrete senses of the target word, and that any prediction other than the “correct” sense is equally wrong. Erk et al. (2009) showed that, given a sense inventory, there is a high likelihood of multiple senses being compatible with a given usage, and proposed U SIM as a means of capturing the similarity in usage between a pairing of usages of a given word. As part of their work, they released a dataset, which Lui et al. (2012) recently developed a topic modelling approach over. Based on extensive experimentation, they demonstrated the best results with a single topic model for all target words based on full document context. Our topic modelling-based approach to U SIM builds off the approach of Lui et al. (2012). Guo and Diab (2012a) observed that, when applied to short texts, the effectiveness of latent semantic approaches can be boosted by expanding the text to include “missing” words. Based on this, they proposed Weighted Textual Matrix Factorization (WTMF), based on weighted matrix factorization (Srebro and Jaa"
S13-1036,S10-1011,0,0.0328392,"ense disambiguation (WSD) is the task of determining the particular sense of a word from a given set of pre-defined senses (Navigli, 2009). It 248 Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference c and the Shared Task, pages 248–253, Atlanta, Georgia, June 13-14, 2013. 2013 Association for Computational Linguistics contrasts with word sense induction (WSI), where the senses of a given target word are induced from an unannotated corpus of usages, and the induced senses are then used to disambiguate each token usage of the word (Manandhar et al., 2010; Lau et al., 2012b). WSD and WSI have been the predominant paradigms for capturing and evaluating lexical semantics, and both assume that each usage corresponds to exactly one of a set of discrete senses of the target word, and that any prediction other than the “correct” sense is equally wrong. Erk et al. (2009) showed that, given a sense inventory, there is a high likelihood of multiple senses being compatible with a given usage, and proposed U SIM as a means of capturing the similarity in usage between a pairing of usages of a given word. As part of their work, they released a dataset, whi"
S13-1036,D11-1141,0,0.00945957,"the growing popularity of social media applications, there has been a steep rise in the amount of “post”-based user-generated text (including microblog posts, status updates and comments) (Bennett, 2012). This data has been identified as having potential for applications ranging from trend analysis (Lau et al., 2012a) and event detection (Osborne et al., 2012) to election outcome prediction (O’Connor et al., 2010). However, given that posts are generally very short, noisy and lacking in context, traditional NLP approaches tend to perform poorly over social media data (Hong and Davison, 2010; Ritter et al., 2011; Han et al., 2012). This is the first paper to address the task of lexical semantic interpretation in microblog data based on word usage similarity. Word usage similarity (U SIM: Erk et al. (2009)) is a relatively new paradigm for capturing similarity in the usages of a given word independently of any lexicon or sense inventory. The task is to rate on an ordinal scale the 2. @USER has his number on a piece of paper and I walkd off! The task is to predict a real-valued number in the range [1, 5] for the similarity in the respective usages of paper, where 1 indicates the usages are completely d"
S13-1036,J98-1004,0,0.452511,"Missing"
S13-1036,P09-1002,0,\N,Missing
S13-1039,W11-0815,0,0.113252,"earchers, with research on binary compositional/non-compositional MWE clas1 The example is taken thefreedictionary.com from http://www. sification (Lin, 1999; Baldwin et al., 2003), or a three-way compositional/semi-compositional/noncompositional distinction (Fazly and Stevenson, 2007). There has also been research to suggest that MWEs span the entire continuum from full compositionality to full non-compositionality (McCarthy et al., 2003; Reddy et al., 2011). Investigating the degree of MWE compositionality has been shown to have applications in information retrieval and machine translation (Acosta et al., 2011; Venkatapathy and Joshi, 2006). As an example of an information retrieval system, if we were looking for documents relating to rat race (meaning “an exhausting routine that leaves no time for relaxation”2 ), we would not be interested in documents on rodents. These results underline the need for methods for broad-coverage MWE compositionality prediction. In this research, we investigate the possibility of using an MWE’s translations in multiple languages to measure the degree of the MWE’s compositionality, and investigate how literal the semantics of each component is within the MWE. We use P"
S13-1039,W03-1812,0,0.170066,"rds. For example, with ad hoc, the fact that neither ad nor hoc are standalone English words, makes ad hoc a lexicallyidiosyncratic MWE; with shoot the breeze, on the other hand, we have semantic idiosyncrasy, as the meaning of “to chat” in usages such as It was good to shoot the breeze with you1 cannot be predicted from the meanings of the component words shoot and breeze. Semantic idiosyncrasy has been of particular interest to NLP researchers, with research on binary compositional/non-compositional MWE clas1 The example is taken thefreedictionary.com from http://www. sification (Lin, 1999; Baldwin et al., 2003), or a three-way compositional/semi-compositional/noncompositional distinction (Fazly and Stevenson, 2007). There has also been research to suggest that MWEs span the entire continuum from full compositionality to full non-compositionality (McCarthy et al., 2003; Reddy et al., 2011). Investigating the degree of MWE compositionality has been shown to have applications in information retrieval and machine translation (Acosta et al., 2011; Venkatapathy and Joshi, 2006). As an example of an information retrieval system, if we were looking for documents relating to rat race (meaning “an exhausting"
S13-1039,C10-3010,0,0.0224098,"since they use statistical measures, they are not suitable for measuring the compositionality of MWEs with low frequency. And finally, most experiments have been carried out on English paired with other European languages, and it is not clear whether the results translate across to other language pairs. 3 Resources In this research, we use the translations of MWEs and their components to estimate the relative degree of compositionality of a MWE. There are several resources available to translate words into various languages such as Babelnet (Navigli and Ponzetto, 2010),4 Wiktionary,5 Panlex (Baldwin et al., 2010) and Google Translate.6 As we are ideally after broad coverage over multiple languages and MWEs/component words in a given language, we exclude Babelnet and Wiktionary from our current research. Babelnet covers only six languages at the time of writing this paper, and in Wiktionary, because it is constantly being updated, words and MWEs do not have translations into the same languages. This leaves translation resources such as Panlex and Google Translate. However, after manually analysing the two resources for a range of MWEs, we decided not to use Google Translate for two reasons: (1) we cons"
S13-1039,W03-1809,0,0.402839,"ion: each type of MWE has its own characteristics, and these characteristics differ from one language to another. Moreover, some MWEs (such as noun compounds) are not flexible syntactically, no matter whether they are compositional or non-compositional (Reddy et al., 2011). Much of the recent work on MWEs focuses on their semantic properties, measuring the semantic similarity between the MWE and its components using different resources, such as WordNet (Kim and Baldwin, 2007) or distributional similarity relative to a corpus (e.g. based on Latent Semantic Analysis: Schone and Jurafsky (2001), Bannard et al. (2003), Reddy et al. (2011)). The size of the corpus is important in methods based on distributional similarity. Unfortunately, however, large corpora are not available for all languages. Reddy et al. (2011) hypothesize that the number of common co-occurrences between a given MWE and its component words indicates the de267 gree of compositionality of that MWE. First, the cooccurrences of a given MWE/word are considered as the values of a vector. They then measure the Cosine similarity between the vectors of the MWE and its components. Bannard et al. (2003) presented four methods to measure the compo"
S13-1039,J93-2003,0,0.0625184,"al. (2012) tried to solve this problem with high frequency MWEs by using word alignment in both directions.3 They computed backward and forward entropy to try to remedy the problem with especially high-frequency phrases. However, their assumptions were not easily generalisable across languages, e.g., they assume that the relative frequency of a specific type of MWE (light verb constructions) in Persian is much greater than in English. Although methods using bilingual corpora are intuitively appealing, they have a number of drawbacks. The first and the most important problem 3 The IBM models (Brown et al., 1993), e.g., are not bidirectional, which means that the alignments are affected by the alignment direction. is data: they need large-scale parallel bilingual corpora, which are available for relatively few language pairs. Second, since they use statistical measures, they are not suitable for measuring the compositionality of MWEs with low frequency. And finally, most experiments have been carried out on English paired with other European languages, and it is not clear whether the results translate across to other language pairs. 3 Resources In this research, we use the translations of MWEs and the"
S13-1039,W07-1102,0,0.017993,"d hoc a lexicallyidiosyncratic MWE; with shoot the breeze, on the other hand, we have semantic idiosyncrasy, as the meaning of “to chat” in usages such as It was good to shoot the breeze with you1 cannot be predicted from the meanings of the component words shoot and breeze. Semantic idiosyncrasy has been of particular interest to NLP researchers, with research on binary compositional/non-compositional MWE clas1 The example is taken thefreedictionary.com from http://www. sification (Lin, 1999; Baldwin et al., 2003), or a three-way compositional/semi-compositional/noncompositional distinction (Fazly and Stevenson, 2007). There has also been research to suggest that MWEs span the entire continuum from full compositionality to full non-compositionality (McCarthy et al., 2003; Reddy et al., 2011). Investigating the degree of MWE compositionality has been shown to have applications in information retrieval and machine translation (Acosta et al., 2011; Venkatapathy and Joshi, 2006). As an example of an information retrieval system, if we were looking for documents relating to rat race (meaning “an exhausting routine that leaves no time for relaxation”2 ), we would not be interested in documents on rodents. These"
S13-1039,J09-1005,1,0.912728,"h, we investigate the possibility of using an MWE’s translations in multiple languages to measure the degree of the MWE’s compositionality, and investigate how literal the semantics of each component is within the MWE. We use Panlex to translate the MWE and its components, and compare the translations of the MWE with the translations of its components using string similarity measures. The greater the string similarity, the more compositional the MWE is. Whereas past research on MWE compositionality has tended to be tailored to a specific MWE type (McCarthy et al., 2007; Kim and Baldwin, 2007; Fazly et al., 2009), our method is applicable to any MWE type in any language. Our experiments 2 This definition is from WordNet 3.1. 266 Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference c and the Shared Task, pages 266–275, Atlanta, Georgia, June 13-14, 2013. 2013 Association for Computational Linguistics over two English MWE types demonstrate that our method is competitive with state-of-the-art methods over standard datasets. 2 Related Work Most previous work on measuring MWE compositionality makes use of lexical, syntactic or semantic properti"
S13-1039,P99-1041,0,0.518422,"omponent words. For example, with ad hoc, the fact that neither ad nor hoc are standalone English words, makes ad hoc a lexicallyidiosyncratic MWE; with shoot the breeze, on the other hand, we have semantic idiosyncrasy, as the meaning of “to chat” in usages such as It was good to shoot the breeze with you1 cannot be predicted from the meanings of the component words shoot and breeze. Semantic idiosyncrasy has been of particular interest to NLP researchers, with research on binary compositional/non-compositional MWE clas1 The example is taken thefreedictionary.com from http://www. sification (Lin, 1999; Baldwin et al., 2003), or a three-way compositional/semi-compositional/noncompositional distinction (Fazly and Stevenson, 2007). There has also been research to suggest that MWEs span the entire continuum from full compositionality to full non-compositionality (McCarthy et al., 2003; Reddy et al., 2011). Investigating the degree of MWE compositionality has been shown to have applications in information retrieval and machine translation (Acosta et al., 2011; Venkatapathy and Joshi, 2006). As an example of an information retrieval system, if we were looking for documents relating to rat race ("
S13-1039,W03-1810,0,0.490698,"of MWE has its own characteristics, and these characteristics differ from one language to another. Moreover, some MWEs (such as noun compounds) are not flexible syntactically, no matter whether they are compositional or non-compositional (Reddy et al., 2011). Much of the recent work on MWEs focuses on their semantic properties, measuring the semantic similarity between the MWE and its components using different resources, such as WordNet (Kim and Baldwin, 2007) or distributional similarity relative to a corpus (e.g. based on Latent Semantic Analysis: Schone and Jurafsky (2001), Bannard et al. (2003), Reddy et al. (2011)). The size of the corpus is important in methods based on distributional similarity. Unfortunately, however, large corpora are not available for all languages. Reddy et al. (2011) hypothesize that the number of common co-occurrences between a given MWE and its component words indicates the de267 gree of compositionality of that MWE. First, the cooccurrences of a given MWE/word are considered as the values of a vector. They then measure the Cosine similarity between the vectors of the MWE and its components. Bannard et al. (2003) presented four methods to measure the compo"
S13-1039,D07-1039,0,0.107404,"E compositionality prediction. In this research, we investigate the possibility of using an MWE’s translations in multiple languages to measure the degree of the MWE’s compositionality, and investigate how literal the semantics of each component is within the MWE. We use Panlex to translate the MWE and its components, and compare the translations of the MWE with the translations of its components using string similarity measures. The greater the string similarity, the more compositional the MWE is. Whereas past research on MWE compositionality has tended to be tailored to a specific MWE type (McCarthy et al., 2007; Kim and Baldwin, 2007; Fazly et al., 2009), our method is applicable to any MWE type in any language. Our experiments 2 This definition is from WordNet 3.1. 266 Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference c and the Shared Task, pages 266–275, Atlanta, Georgia, June 13-14, 2013. 2013 Association for Computational Linguistics over two English MWE types demonstrate that our method is competitive with state-of-the-art methods over standard datasets. 2 Related Work Most previous work on measuring MWE compositionality makes us"
S13-1039,W97-0311,0,0.498209,"f that MWE. First, the cooccurrences of a given MWE/word are considered as the values of a vector. They then measure the Cosine similarity between the vectors of the MWE and its components. Bannard et al. (2003) presented four methods to measure the compositionality of English verb particle constructions. Their best result is based on the previously-discussed method of Lin (1999) for measuring compositionality, but uses a more-general distributional similarity model to identify synonyms. Recently, a few studies have investigated using parallel corpora to detect the degree of compositionality (Melamed, 1997; Moir´on and Tiedemann, 2006; de Caseli et al., 2010; Salehi et al., 2012). The general approach is to word-align the source and target language sentences and analyse alignment patterns for MWEs (e.g. if the MWE is always aligned as a single “phrase”, then it is a strong indicator of non-compositionality). de Caseli et al. (2010) consider non-compositional MWEs to be those candidates that align to the same target language unit, without decomposition into word alignments. Melamed (1997) suggests using mutual information to investigate how well the translation model predicts the distribution of"
S13-1039,W06-2405,0,0.237243,"Missing"
S13-1039,P10-1023,0,0.0209527,"vailable for relatively few language pairs. Second, since they use statistical measures, they are not suitable for measuring the compositionality of MWEs with low frequency. And finally, most experiments have been carried out on English paired with other European languages, and it is not clear whether the results translate across to other language pairs. 3 Resources In this research, we use the translations of MWEs and their components to estimate the relative degree of compositionality of a MWE. There are several resources available to translate words into various languages such as Babelnet (Navigli and Ponzetto, 2010),4 Wiktionary,5 Panlex (Baldwin et al., 2010) and Google Translate.6 As we are ideally after broad coverage over multiple languages and MWEs/component words in a given language, we exclude Babelnet and Wiktionary from our current research. Babelnet covers only six languages at the time of writing this paper, and in Wiktionary, because it is constantly being updated, words and MWEs do not have translations into the same languages. This leaves translation resources such as Panlex and Google Translate. However, after manually analysing the two resources for a range of MWEs, we decided not to use"
S13-1039,I11-1024,0,0.762458,"ze with you1 cannot be predicted from the meanings of the component words shoot and breeze. Semantic idiosyncrasy has been of particular interest to NLP researchers, with research on binary compositional/non-compositional MWE clas1 The example is taken thefreedictionary.com from http://www. sification (Lin, 1999; Baldwin et al., 2003), or a three-way compositional/semi-compositional/noncompositional distinction (Fazly and Stevenson, 2007). There has also been research to suggest that MWEs span the entire continuum from full compositionality to full non-compositionality (McCarthy et al., 2003; Reddy et al., 2011). Investigating the degree of MWE compositionality has been shown to have applications in information retrieval and machine translation (Acosta et al., 2011; Venkatapathy and Joshi, 2006). As an example of an information retrieval system, if we were looking for documents relating to rat race (meaning “an exhausting routine that leaves no time for relaxation”2 ), we would not be interested in documents on rodents. These results underline the need for methods for broad-coverage MWE compositionality prediction. In this research, we investigate the possibility of using an MWE’s translations in mul"
S13-1039,W01-0513,0,0.0637642,"is their lack of generalization: each type of MWE has its own characteristics, and these characteristics differ from one language to another. Moreover, some MWEs (such as noun compounds) are not flexible syntactically, no matter whether they are compositional or non-compositional (Reddy et al., 2011). Much of the recent work on MWEs focuses on their semantic properties, measuring the semantic similarity between the MWE and its components using different resources, such as WordNet (Kim and Baldwin, 2007) or distributional similarity relative to a corpus (e.g. based on Latent Semantic Analysis: Schone and Jurafsky (2001), Bannard et al. (2003), Reddy et al. (2011)). The size of the corpus is important in methods based on distributional similarity. Unfortunately, however, large corpora are not available for all languages. Reddy et al. (2011) hypothesize that the number of common co-occurrences between a given MWE and its component words indicates the de267 gree of compositionality of that MWE. First, the cooccurrences of a given MWE/word are considered as the values of a vector. They then measure the Cosine similarity between the vectors of the MWE and its components. Bannard et al. (2003) presented four metho"
S13-1039,W06-1204,0,0.122756,"ch on binary compositional/non-compositional MWE clas1 The example is taken thefreedictionary.com from http://www. sification (Lin, 1999; Baldwin et al., 2003), or a three-way compositional/semi-compositional/noncompositional distinction (Fazly and Stevenson, 2007). There has also been research to suggest that MWEs span the entire continuum from full compositionality to full non-compositionality (McCarthy et al., 2003; Reddy et al., 2011). Investigating the degree of MWE compositionality has been shown to have applications in information retrieval and machine translation (Acosta et al., 2011; Venkatapathy and Joshi, 2006). As an example of an information retrieval system, if we were looking for documents relating to rat race (meaning “an exhausting routine that leaves no time for relaxation”2 ), we would not be interested in documents on rodents. These results underline the need for methods for broad-coverage MWE compositionality prediction. In this research, we investigate the possibility of using an MWE’s translations in multiple languages to measure the degree of the MWE’s compositionality, and investigate how literal the semantics of each component is within the MWE. We use Panlex to translate the MWE and"
S13-2039,S07-1002,0,0.060288,"model? and (4) given the rapid pace of language evolution on real-time social media such as Twitter and Facebook, is it reasonable to assume a static sense inventory? Given this backdrop, there has been a recent growth of interest in the task of word sense induction (WSI), where the word sense representation for a given word is automatically inferred from a given data source, and word usages are labelled (often probabilistically) according to that data source. While WSI has considerable appeal as a task, intrinsic cross-comparison of WSI systems is fraught with many of the same issues as WSD (Agirre and Soroa, 2007; Manandhar et al., 2010), leading to a move towards task-based WSI evaluation, such as in Task 11 of SemEval-2013, titled “Evaluating Word Sense Induction & Disambiguation within an End-User Application”. This paper presents the UNIMELB system entry to SemEval-2013 Task 11. Our method is based heavily on the WSI methodology proposed by Lau et al. (2012) for novel word sense detection. Largely the same methodology was also applied to SemEval2013 Task 13 on WSI (Lau et al., to appear). 2 System Description Our system is based on the WSI methodology proposed by Lau et al. (2012) for the task of"
S13-2039,E12-1060,1,0.867931,"iven data source, and word usages are labelled (often probabilistically) according to that data source. While WSI has considerable appeal as a task, intrinsic cross-comparison of WSI systems is fraught with many of the same issues as WSD (Agirre and Soroa, 2007; Manandhar et al., 2010), leading to a move towards task-based WSI evaluation, such as in Task 11 of SemEval-2013, titled “Evaluating Word Sense Induction & Disambiguation within an End-User Application”. This paper presents the UNIMELB system entry to SemEval-2013 Task 11. Our method is based heavily on the WSI methodology proposed by Lau et al. (2012) for novel word sense detection. Largely the same methodology was also applied to SemEval2013 Task 13 on WSI (Lau et al., to appear). 2 System Description Our system is based on the WSI methodology proposed by Lau et al. (2012) for the task of novel word sense detection. The core machinery of our system is driven by a Latent Dirichlet Allocation (LDA) topic model (Blei et al., 2003). In LDA, the model learns latent topics for a collection of documents, and associates these latent topics with every document in the collection. A topic is represented by a multinomial distribution of words, and th"
S13-2039,S13-2051,1,0.837682,"Missing"
S13-2039,S10-1011,0,0.0389883,"rapid pace of language evolution on real-time social media such as Twitter and Facebook, is it reasonable to assume a static sense inventory? Given this backdrop, there has been a recent growth of interest in the task of word sense induction (WSI), where the word sense representation for a given word is automatically inferred from a given data source, and word usages are labelled (often probabilistically) according to that data source. While WSI has considerable appeal as a task, intrinsic cross-comparison of WSI systems is fraught with many of the same issues as WSD (Agirre and Soroa, 2007; Manandhar et al., 2010), leading to a move towards task-based WSI evaluation, such as in Task 11 of SemEval-2013, titled “Evaluating Word Sense Induction & Disambiguation within an End-User Application”. This paper presents the UNIMELB system entry to SemEval-2013 Task 11. Our method is based heavily on the WSI methodology proposed by Lau et al. (2012) for novel word sense detection. Largely the same methodology was also applied to SemEval2013 Task 13 on WSI (Lau et al., to appear). 2 System Description Our system is based on the WSI methodology proposed by Lau et al. (2012) for the task of novel word sense detectio"
S13-2039,S13-2035,0,0.122581,"induce the ranking based on the sense probabilities assigned to the senses, such that snippets that have the highest probability of the induced sense are ranked highest, and snippets with lower sense probabilities 3 Our implementation can be accessed via https:// github.com/jhlau/hdp-wsi. 219 are ranked lower. Two classes of evaluation are used in the shared task: 1. cluster quality measures: Jaccard Index (JI), RandIndex (RI), Adjusted RandIndex (ARI) and F1; 2. diversification of search results: Subtopic Recall@K and Subtopic Precision@r. Details of the evaluation measures are described in Navigli and Vannella (2013). The idea behind the second form of evaluation (i.e. diversification of search results) is that search engine results should cluster the results based on senses (of the query term in the documents) given an ambiguous query. For example, if a user searches for apple, the search engine may return results related to both the computer brand sense and the fruit sense of apple. Given this assumption, the best WSI/WSD system is the one that can correctly identify the diversity of senses in the snippets. Figure 1: Subtopic Recall@K for all participating systems. Cluster quality, subtopic recall@K and"
S13-2051,S12-1027,0,0.0225917,"he weights of the induced senses and that of the gold senses. For clustering comparison, fuzzy normalised mutual information (FNMI) and fuzzy b-cubed (FBC) are used. Note that the WSD systems participating in this shared task are not evaluated with clustering comparison metrics, as they do not induce senses/clusters in the same manner as WSI systems. WSI systems produce senses that are different to the gold standard sense inventory (WordNet 3.1), and the induced senses are mapped to the gold standard senses using the 80/20 validation setting. Details of this mapping procedure are described in Jurgens (2012). Results for all test instances are presented in Table 3. Note that many baselines are used, only some of which we present in this paper, namely: (1) R AN DOM — label instances with one of three random induced senses; (2) S EMCOR MFS — label instances with the most frequently occurring sense in Semcor; (3) T EST MFS — label instances with the most frequently occurring sense in the test dataset. To benchmark our method, we present one or two of the best 309 systems from each team. Looking at Table 3, our system performs encouragingly well. Although not the best system, we achieve results close"
S13-2051,E12-1060,1,0.721911,"ed to grade senses rather than selecting a single sense like most word sense disambiguation (WSD) settings. The evaluation measures are designed to assess how well a system perceives the different senses in a contextual instance. We adopt a previously-proposed WSI methodology for the task, which is based on a Hierarchical Dirichlet Process (HDP), a nonparametric topic model. Our system requires no parameter tuning, uses the English ukWaC as an external resource, and achieves encouraging results over the shared task. 1 2 System Description Our system is based on the WSI methodology proposed by Lau et al. (2012), and also applied to SemEval-2013 Task 11 on WSI for web snippet clustering (Lau et al., to appear). The core machinery of our system is driven by a Latent Dirichlet Allocation (LDA) topic model (Blei et al., 2003). In LDA, the model learns latent topics for a collection of documents, and associates these latent topics with every document in the collection. A topic is represented by a multinomial distribution of words, and the association of topics with documents is represented by a multinomial distribution of topics, a distribution for each document. The generative process of LDA for drawing"
S13-2051,S13-2039,1,0.837606,"Missing"
S16-1113,W14-4012,0,0.0831763,"Missing"
S16-1113,W11-2207,0,0.0146586,"ds the tf-idf weight for the corresponding type in the sentence. Idf values were calculated over a 2015 dump of English Wikipedia from 1 September 2015, which was pre-processed using wp2txt1 to remove markup. 1 https://github.com/yohasebe/wp2txt 732 Proceedings of SemEval-2016, pages 732–735, c San Diego, California, June 16-17, 2016. 2016 Association for Computational Linguistics For all baseline methods, the similarity between two sentences is calculated as the cosine between the vectors representing them. In these baseline methods, the documents are tokenized using an approach suggested by Speriosu et al. (2011) — the text is first split based on whitespace; for each token, if it contains at least one alphanumeric character, then all leading and trailing non-alphanumeric characters are stripped. Stopwords are removed based on a stopword list,2 and case folding is applied. 2.1.2 Word2vec We considered two methods based on word embeddings from word2vec (Mikolov et al., 2013). For each sentence, we formed a vector corresponding to the element-wise summation, and product, of the word embeddings for each token in that sentence. We then measure the similarity of two sentences as the cosine between their ve"
S17-1006,W02-1001,0,0.143911,"ldwin, 2012). Crucially, this work has typically focused on specific kinds of MWEs, and has not considered identification of the full spectrum of MWEs. More-recent work has considered the identification of a wider range of types of MWEs. Brooke et al. (2014) present an unsupervised learning approach to segment a corpus into multiword units based on their predictability. Schneider et al. (2014a) propose methods for broad-coverage MWE identification, and evaluate them on a sizeable corpus (Schneider et al., 2014b). They proposed a supervised learning approach based on the structured perceptron (Collins, 2002). The system labels tokens using the BIO convention, where B indicates the beginning of an MWE, I indicates the continuation of an MWE, and O indicates that the token is not part of an MWE. The model includes features based on part-of-speech tags, MWE lexicons, and Brown clusters (Brown et al., 1992). Qu et al. (2015) later improved upon that system by using skip-gram embeddings (Mikolov et al., 2013) instead of Brown clusters with a variant of conditional random fields. More recently, Constant and Nivre (2016) incorporate MWE identification along with dependency parsing by forming two represe"
S17-1006,I11-1130,0,0.0611504,"Missing"
S17-1006,P16-1016,0,0.0732021,"al., 2014b). They proposed a supervised learning approach based on the structured perceptron (Collins, 2002). The system labels tokens using the BIO convention, where B indicates the beginning of an MWE, I indicates the continuation of an MWE, and O indicates that the token is not part of an MWE. The model includes features based on part-of-speech tags, MWE lexicons, and Brown clusters (Brown et al., 1992). Qu et al. (2015) later improved upon that system by using skip-gram embeddings (Mikolov et al., 2013) instead of Brown clusters with a variant of conditional random fields. More recently, Constant and Nivre (2016) incorporate MWE identification along with dependency parsing by forming two representations for a sentence: a tree that represents the syntactic dependencies, and a forest of lexical trees that includes the MWEs identified in the sentence. 3 Neural Network Models In this section, we discuss the features extracted for the neural network models, and the model architectures. Schneider et al. (2014b) extracted roughly 320k sparse features. Because of the large input feature space, the only feasible way to train a model on those features is by using a linear classifier. In contrast to Schneider et"
S17-1006,E06-1042,0,0.015313,"nce The staff leaves a lot to be desired (also used in Figure 1) a lot and leaves to be desired are MWEs. An important part of MWE identification is to be able to distinguish between MWEs and literal combinations that have the same surface form; e.g., kick the bucket is ambiguous between an idiomatic usage — meaning roughly ‘die’ — which is an MWE, and a literal one which is not. Many earlier studies on MWE identification have focused on this type of ambiguity, and treated the problem as one of word sense disambiguation, where literal and idiomatic usages are considered different word senses (Birke and Sarkar, 2006; Katz and Giesbrecht, 2006; Li et al., 2010). Other work has leveraged linguistic knowledge of properties of MWEs in order to make these distinctions (Uchiyama et al., 2005; Fazly et al., 2009; Fothergill and Baldwin, 2012). Crucially, this work has typically focused on specific kinds of MWEs, and has not considered identification of the full spectrum of MWEs. More-recent work has considered the identification of a wider range of types of MWEs. Brooke et al. (2014) present an unsupervised learning approach to segment a corpus into multiword units based on their predictability. Schneider et al"
S17-1006,J09-1005,1,0.913468,"lly, we consider a layered feedforward network, a recurrent neural network, and convolutional neural networks. In experimental results we show that convolutional neural networks are able to outperform the previous state-of-the-art for MWE identification, with a convolutional neural network with three hidden layers giving the best performance. 1 Recent work on token-level MWE identification has focused on methods that are applicable to the full spectrum of kinds of MWEs (Schneider et al., 2014a), in contrast to earlier work that tended to focus on specific kinds of MWEs (Uchiyama et al., 2005; Fazly et al., 2009; Fothergill and Baldwin, 2012). Deep learning is an emerging class of machine learning models that have recently achieved promising results on a range of NLP tasks such as machine translation (Bahdanau et al., 2015; Sutskever et al., 2014), named entity recognition (Lample et al., 2016), natural language generation (Li et al., 2015), and sentence classification (Kim, 2014). Such models have, however, not yet been applied to broad-coverage MWE identification. Introduction Multiword expressions (MWEs) are lexical items that can be decomposed into multiple component words, but have properties th"
S17-1006,C14-1071,0,0.0141544,"eated the problem as one of word sense disambiguation, where literal and idiomatic usages are considered different word senses (Birke and Sarkar, 2006; Katz and Giesbrecht, 2006; Li et al., 2010). Other work has leveraged linguistic knowledge of properties of MWEs in order to make these distinctions (Uchiyama et al., 2005; Fazly et al., 2009; Fothergill and Baldwin, 2012). Crucially, this work has typically focused on specific kinds of MWEs, and has not considered identification of the full spectrum of MWEs. More-recent work has considered the identification of a wider range of types of MWEs. Brooke et al. (2014) present an unsupervised learning approach to segment a corpus into multiword units based on their predictability. Schneider et al. (2014a) propose methods for broad-coverage MWE identification, and evaluate them on a sizeable corpus (Schneider et al., 2014b). They proposed a supervised learning approach based on the structured perceptron (Collins, 2002). The system labels tokens using the BIO convention, where B indicates the beginning of an MWE, I indicates the continuation of an MWE, and O indicates that the token is not part of an MWE. The model includes features based on part-of-speech ta"
S17-1006,S12-1017,0,0.155871,"ayered feedforward network, a recurrent neural network, and convolutional neural networks. In experimental results we show that convolutional neural networks are able to outperform the previous state-of-the-art for MWE identification, with a convolutional neural network with three hidden layers giving the best performance. 1 Recent work on token-level MWE identification has focused on methods that are applicable to the full spectrum of kinds of MWEs (Schneider et al., 2014a), in contrast to earlier work that tended to focus on specific kinds of MWEs (Uchiyama et al., 2005; Fazly et al., 2009; Fothergill and Baldwin, 2012). Deep learning is an emerging class of machine learning models that have recently achieved promising results on a range of NLP tasks such as machine translation (Bahdanau et al., 2015; Sutskever et al., 2014), named entity recognition (Lample et al., 2016), natural language generation (Li et al., 2015), and sentence classification (Kim, 2014). Such models have, however, not yet been applied to broad-coverage MWE identification. Introduction Multiword expressions (MWEs) are lexical items that can be decomposed into multiple component words, but have properties that are idiomatic, i.e., marked"
S17-1006,J92-4003,0,0.717288,"Missing"
S17-1006,N10-1029,0,0.170472,"Missing"
S17-1006,S14-1001,0,0.0272803,"Missing"
S17-1006,2012.eamt-1.60,0,0.0173272,"Missing"
S17-1006,W06-1203,0,0.0905723,"ot to be desired (also used in Figure 1) a lot and leaves to be desired are MWEs. An important part of MWE identification is to be able to distinguish between MWEs and literal combinations that have the same surface form; e.g., kick the bucket is ambiguous between an idiomatic usage — meaning roughly ‘die’ — which is an MWE, and a literal one which is not. Many earlier studies on MWE identification have focused on this type of ambiguity, and treated the problem as one of word sense disambiguation, where literal and idiomatic usages are considered different word senses (Birke and Sarkar, 2006; Katz and Giesbrecht, 2006; Li et al., 2010). Other work has leveraged linguistic knowledge of properties of MWEs in order to make these distinctions (Uchiyama et al., 2005; Fazly et al., 2009; Fothergill and Baldwin, 2012). Crucially, this work has typically focused on specific kinds of MWEs, and has not considered identification of the full spectrum of MWEs. More-recent work has considered the identification of a wider range of types of MWEs. Brooke et al. (2014) present an unsupervised learning approach to segment a corpus into multiword units based on their predictability. Schneider et al. (2014a) propose methods f"
S17-1006,D14-1181,0,0.020166,"fication has focused on methods that are applicable to the full spectrum of kinds of MWEs (Schneider et al., 2014a), in contrast to earlier work that tended to focus on specific kinds of MWEs (Uchiyama et al., 2005; Fazly et al., 2009; Fothergill and Baldwin, 2012). Deep learning is an emerging class of machine learning models that have recently achieved promising results on a range of NLP tasks such as machine translation (Bahdanau et al., 2015; Sutskever et al., 2014), named entity recognition (Lample et al., 2016), natural language generation (Li et al., 2015), and sentence classification (Kim, 2014). Such models have, however, not yet been applied to broad-coverage MWE identification. Introduction Multiword expressions (MWEs) are lexical items that can be decomposed into multiple component words, but have properties that are idiomatic, i.e., marked or unpredictable, with respect to properties of their component words (Baldwin and Kim, 2010). MWEs include a wide range of phenomena such as noun compounds (e.g., speed limit and monkey business), verb–particle constructions (e.g., clean up and throw out), and verb–noun idiomatic combinations (e.g., hit the roof and blow the whistle), as well"
S17-1006,2014.iwslt-papers.16,0,0.049113,"Missing"
S17-1006,C12-1127,0,0.026311,"Missing"
S17-1006,S16-1144,0,0.0994097,"ssing (NLP) is MWE identification — i.e., to identify which tokens in running text correspond to MWEs so that they can be analyzed accordingly. The challenges posed by MWEs have led to them to be referred to as a “pain in the neck” for NLP (Sag et al., 2002); nevertheless, incorporating knowledge of MWEs In this paper we propose the first deep learning models for broad-coverage MWE identification. Specifically, we propose and evaluate a layered feedforward network, a recurrent neural network, and two convolutional neural networks. We compare these models against the previous state-of-the-art (Kirilin et al., 2016) and several more-traditional supervised machine learning approaches. We show that the convolutional neural networks outperform the previous state-of-the-art. This finding is particularly remarkable given the relatively small size of the training data available, and demonstrates that deep learning models are able to learn well from small datasets. Moreover, we show that our proposed deep learning models are able to generalize more-effectively than previous approaches, based on comparisons between the models’ performances on validation and test data. 54 Proceedings of the 6th Joint Conference o"
S17-1006,D14-1108,0,0.0380706,"Missing"
S17-1006,D14-1162,0,0.0925215,"ted Work The recent SemEval shared task on Detecting Minimal Semantic Units and their Meanings (DiMSUM) focused on MWE identification along with supersense tagging (Schneider et al., 2016). The best performing system for MWE identification for this shared task was that of Kirilin et al. (2016) which took into consideration all of the basic features used by Schneider et al. (2014a) and two novel feature sets. The first one is based on the YAGO ontology (Suchanek et al., 2007), where heuristics were applied to extract potential named entities from the ontology. The second feature set was GloVe (Pennington et al., 2014) word embeddings, with the word vectors scaled by a constant and divided by the standard deviation of each of its dimensions. None of the systems that participated in the DiMSUM shared task considered deep learning approaches. In this paper we propose the first deep learning approaches to MWE identification. We use the DiMSUM data for training and evaluating our models, and compare against the state-of-the-art method of Kirilin et al. (2016). Here we focus solely on the MWE identification task, leaving supersense tagging for future work. MWE identification is the task of determining, at the to"
S17-1006,K15-1009,0,0.0178367,"pus into multiword units based on their predictability. Schneider et al. (2014a) propose methods for broad-coverage MWE identification, and evaluate them on a sizeable corpus (Schneider et al., 2014b). They proposed a supervised learning approach based on the structured perceptron (Collins, 2002). The system labels tokens using the BIO convention, where B indicates the beginning of an MWE, I indicates the continuation of an MWE, and O indicates that the token is not part of an MWE. The model includes features based on part-of-speech tags, MWE lexicons, and Brown clusters (Brown et al., 1992). Qu et al. (2015) later improved upon that system by using skip-gram embeddings (Mikolov et al., 2013) instead of Brown clusters with a variant of conditional random fields. More recently, Constant and Nivre (2016) incorporate MWE identification along with dependency parsing by forming two representations for a sentence: a tree that represents the syntactic dependencies, and a forest of lexical trees that includes the MWEs identified in the sentence. 3 Neural Network Models In this section, we discuss the features extracted for the neural network models, and the model architectures. Schneider et al. (2014b) ex"
S17-1006,N16-1030,0,0.0560727,"work with three hidden layers giving the best performance. 1 Recent work on token-level MWE identification has focused on methods that are applicable to the full spectrum of kinds of MWEs (Schneider et al., 2014a), in contrast to earlier work that tended to focus on specific kinds of MWEs (Uchiyama et al., 2005; Fazly et al., 2009; Fothergill and Baldwin, 2012). Deep learning is an emerging class of machine learning models that have recently achieved promising results on a range of NLP tasks such as machine translation (Bahdanau et al., 2015; Sutskever et al., 2014), named entity recognition (Lample et al., 2016), natural language generation (Li et al., 2015), and sentence classification (Kim, 2014). Such models have, however, not yet been applied to broad-coverage MWE identification. Introduction Multiword expressions (MWEs) are lexical items that can be decomposed into multiple component words, but have properties that are idiomatic, i.e., marked or unpredictable, with respect to properties of their component words (Baldwin and Kim, 2010). MWEs include a wide range of phenomena such as noun compounds (e.g., speed limit and monkey business), verb–particle constructions (e.g., clean up and throw out),"
S17-1006,N16-1000,0,0.603726,"ly than previous approaches, based on comparisons between the models’ performances on validation and test data. 54 Proceedings of the 6th Joint Conference on Lexical and Computational Semantics (*SEM 2017), pages 54–64, c Vancouver, Canada, August 3-4, 2017. 2017 Association for Computational Linguistics 2 Related Work The recent SemEval shared task on Detecting Minimal Semantic Units and their Meanings (DiMSUM) focused on MWE identification along with supersense tagging (Schneider et al., 2016). The best performing system for MWE identification for this shared task was that of Kirilin et al. (2016) which took into consideration all of the basic features used by Schneider et al. (2014a) and two novel feature sets. The first one is based on the YAGO ontology (Suchanek et al., 2007), where heuristics were applied to extract potential named entities from the ontology. The second feature set was GloVe (Pennington et al., 2014) word embeddings, with the word vectors scaled by a constant and divided by the standard deviation of each of its dimensions. None of the systems that participated in the DiMSUM shared task considered deep learning approaches. In this paper we propose the first deep lea"
S17-1006,P15-1107,0,0.0801781,"Missing"
S17-1006,D11-1141,0,0.149698,"Missing"
S17-1006,P10-1116,0,0.0290126,"in Figure 1) a lot and leaves to be desired are MWEs. An important part of MWE identification is to be able to distinguish between MWEs and literal combinations that have the same surface form; e.g., kick the bucket is ambiguous between an idiomatic usage — meaning roughly ‘die’ — which is an MWE, and a literal one which is not. Many earlier studies on MWE identification have focused on this type of ambiguity, and treated the problem as one of word sense disambiguation, where literal and idiomatic usages are considered different word senses (Birke and Sarkar, 2006; Katz and Giesbrecht, 2006; Li et al., 2010). Other work has leveraged linguistic knowledge of properties of MWEs in order to make these distinctions (Uchiyama et al., 2005; Fazly et al., 2009; Fothergill and Baldwin, 2012). Crucially, this work has typically focused on specific kinds of MWEs, and has not considered identification of the full spectrum of MWEs. More-recent work has considered the identification of a wider range of types of MWEs. Brooke et al. (2014) present an unsupervised learning approach to segment a corpus into multiword units based on their predictability. Schneider et al. (2014a) propose methods for broad-coverage"
S17-1006,P16-1019,0,0.0542664,"th two and three fully connected hidden layers, which we refer to as CNN2 and CNN3, respectively. We observed that CNNs with 2 and 3 hidden layers performed well on the validation set but adding more layers resulted in overfitting. Similarly, adding more hidden layers to the LFN and RNN also resulted in overfitting. Schneider et al. (2014a) include features based on MWE lexicons that represent which tokens and lemmas are potentially part of an MWE and according to which lexicon. We use a script provided by Schneider et al. (2014a) to include these same features in our representation. Finally, Salton et al. (2016) showed that embedding the entire sentence in which a target MWE occurs was helpful for distinguishing idiomatic from literal verb–noun idiomatic combinations. 56 4 Data and Evaluation This section presents the statistics and structure of the dataset used for this task, as well as the evaluation methodology. 4.1 Dataset We use the DiMSUM dataset (Schneider et al., 2016) for our experiments, which allows for direct comparison with previous results. Table 1 displays the source corpora from which the dataset was constructed; their domain (i.e., reviews, tweets, or TED talks); the number of senten"
S17-1006,Q14-1016,0,0.0716426,"ir component words. In this paper we propose the first deep learning models for token-level identification of MWEs. Specifically, we consider a layered feedforward network, a recurrent neural network, and convolutional neural networks. In experimental results we show that convolutional neural networks are able to outperform the previous state-of-the-art for MWE identification, with a convolutional neural network with three hidden layers giving the best performance. 1 Recent work on token-level MWE identification has focused on methods that are applicable to the full spectrum of kinds of MWEs (Schneider et al., 2014a), in contrast to earlier work that tended to focus on specific kinds of MWEs (Uchiyama et al., 2005; Fazly et al., 2009; Fothergill and Baldwin, 2012). Deep learning is an emerging class of machine learning models that have recently achieved promising results on a range of NLP tasks such as machine translation (Bahdanau et al., 2015; Sutskever et al., 2014), named entity recognition (Lample et al., 2016), natural language generation (Li et al., 2015), and sentence classification (Kim, 2014). Such models have, however, not yet been applied to broad-coverage MWE identification. Introduction Mu"
S17-1006,S16-1084,0,0.530769,"rn well from small datasets. Moreover, we show that our proposed deep learning models are able to generalize more-effectively than previous approaches, based on comparisons between the models’ performances on validation and test data. 54 Proceedings of the 6th Joint Conference on Lexical and Computational Semantics (*SEM 2017), pages 54–64, c Vancouver, Canada, August 3-4, 2017. 2017 Association for Computational Linguistics 2 Related Work The recent SemEval shared task on Detecting Minimal Semantic Units and their Meanings (DiMSUM) focused on MWE identification along with supersense tagging (Schneider et al., 2016). The best performing system for MWE identification for this shared task was that of Kirilin et al. (2016) which took into consideration all of the basic features used by Schneider et al. (2014a) and two novel feature sets. The first one is based on the YAGO ontology (Suchanek et al., 2007), where heuristics were applied to extract potential named entities from the ontology. The second feature set was GloVe (Pennington et al., 2014) word embeddings, with the word vectors scaled by a constant and divided by the standard deviation of each of its dimensions. None of the systems that participated"
S17-1006,schneider-etal-2014-comprehensive,0,0.0680005,"ir component words. In this paper we propose the first deep learning models for token-level identification of MWEs. Specifically, we consider a layered feedforward network, a recurrent neural network, and convolutional neural networks. In experimental results we show that convolutional neural networks are able to outperform the previous state-of-the-art for MWE identification, with a convolutional neural network with three hidden layers giving the best performance. 1 Recent work on token-level MWE identification has focused on methods that are applicable to the full spectrum of kinds of MWEs (Schneider et al., 2014a), in contrast to earlier work that tended to focus on specific kinds of MWEs (Uchiyama et al., 2005; Fazly et al., 2009; Fothergill and Baldwin, 2012). Deep learning is an emerging class of machine learning models that have recently achieved promising results on a range of NLP tasks such as machine translation (Bahdanau et al., 2015; Sutskever et al., 2014), named entity recognition (Lample et al., 2016), natural language generation (Li et al., 2015), and sentence classification (Kim, 2014). Such models have, however, not yet been applied to broad-coverage MWE identification. Introduction Mu"
S17-1006,N15-1177,0,0.0356073,"Missing"
S17-1006,W17-1726,0,\N,Missing
S18-1168,O97-1002,0,0.656232,"Missing"
S18-1168,P14-5010,0,0.00271944,"ption of our models we refer to the words in the triples in the dataset as word1, word2, and attribute, respectively. 2.1 Word2vec If an attribute is a discriminative attribute for word1, then we hypothesize that word1 and the attribute will be more semantically similar than word2 and the attribute. We use similarity of word embeddings as a proxy for semantic similarity. We train word2vec’s skip-gram model (Mikolov et al., 2013) on a snapshot of English Wikipedia from 1 September 2015 containing roughly 2.6 billion tokens, tokenized using the tokenizer available in the Stanford CoreNLP tools (Manning et al., 2014).1 We use a window size of ± 8 and 300 dimensions. We remove all words that occur less than 15 times in the corpus. We did not set a maximum vocabulary size. We train our model using negative sampling, and set the number of training epochs to 5. We then calculate the cosine similarity between the word embeddings for word1 and the attribute (cos(word1, attribute)), and word2 and the attribute (cos(word2, attribute)). We label the instance as a discriminative at1 http://nlp.stanford.edu/software/ corenlp.shtml 1013 Proceedings of the 12th International Workshop on Semantic Evaluation (SemEval-20"
S18-1168,J90-1003,0,0.550996,"Missing"
S18-1168,J93-1003,0,0.53866,"Missing"
S19-2092,S19-2007,0,0.0494565,"tuned the parameters for the LSTM model on the English development dataset using grid search. Specifically we considered the following settings for the embedding size (256, 512, 1024), number of hidden units (128, 256, 512), and number of epochs (1,2,3,4,5). The final parameters for the LSTMs used on the test datasets were 1 hidden layer, an embedding size of 1024, 128 hidden units, and they were trained using a batch size of 2 and 1 epoch. 3.1.2 Neural LMs with Class Token 3.1.4 Character-level LM Baselines In addition to the most-frequent class and SVC baselines provided by the shared task (Basile et al., 2019), we also compare our approaches against multinomial naive Bayes2 and fastText (Joulin et al., 2017). We use the default settings for fastText, and do not attempt to tune it to this task. This approach is the same as the previous wordlevel LM approach, except that character-level, as opposed to word-level, LMs are trained. We use a publicly available TensorFlow implementation of a character-level RNN language model.1 The following parameters are used: a two-layer GRU with one-hot character embeddings and a hidden layer 2 Note that the likelihood term in multinomial naive Bayes corresponds to a"
S19-2092,P13-2121,0,0.0842986,"Missing"
S19-2092,P18-1031,0,0.0297114,"mar et al., 2018) observed there is no significant difference between the performance of neural networks and linear classifiers. Furthermore, this shared task received just one lexicon-based approach, and its performance was not promising. Moreover, all of these approaches required expensive feature engineering and pre-processing. Instead of engineering specific features to use in supervised classifiers, we can instead employ language models to model the type of text we want to detect. Language models have previously been applied for the purpose of text classification (e.g., Bai et al., 2004; Howard and Ruder, 2018). Among different types of language models, recurrent neural network (RNN) language models with LSTM and GRU units have shown promising results for sequence modeling (Mikolov et al., 2012). To the best of our knowledge, RNN language models have not been widely used for detecting hate speech or offensive language. The most closely related work is that of Mehdad and Tetreault (2016), which used both word-level and character-level RNNs to detect abusive language. In their experiments, Mehdad and Tetreault found that character-level language models outperformed word-level language models. A furthe"
S19-2092,E17-2068,0,0.257234,"ng word- and character-level neural language models, as well as more-conventional (word-level) n-gram language models — are able to distinguish between hateful and not hateful, and offensive and not offensive, language. We find that these approaches outperform a most-frequent class baseline, indicating that language models can capture some knowledge of whether text is hateful or offensive. However, for task 5 — for which the testing data was made available for follow up experiments — we also find that more-conventional approaches to supervised classification, such as naive Bayes and fastText (Joulin et al., 2017), often give similar or better results. 514 Proceedings of the 13th International Workshop on Semantic Evaluation (SemEval-2019), pages 514–518 Minneapolis, Minnesota, USA, June 6–7, 2019. ©2019 Association for Computational Linguistics words (Mikolov et al., 2012). 3 size of 64 dimensions. The batch size, learning rate, and dropout are set to 20, 0.002, and 0, respectively. The hidden layer size and unit were tuned on the development data. Specifically we considered hidden layer sizes of 64, 128, and 256, and an LSTM and GRU for the unit. The other parameters are their default settings. Task"
S19-2092,W18-4401,0,0.0222685,"mputer Science, University of New Brunswick Fredericton, NB E3B 5A3, Canada ahakimi@unb.ca, milton.king@unb.ca, paul.cook@unb.ca Abstract 2 Social media such as Twitter and Facebook are widely used, and hate speech has become prevalent in these platforms. In response to this, a substantial amount of research has recently focused on detecting such disturbing comments. Prior work on hate speech and offensive language detection has mostly focused on supervised machine learning techniques (Mathur et al., 2018; Davidson et al., 2017). A recent shared task on identifying aggression in social media (Kumar et al., 2018) observed there is no significant difference between the performance of neural networks and linear classifiers. Furthermore, this shared task received just one lexicon-based approach, and its performance was not promising. Moreover, all of these approaches required expensive feature engineering and pre-processing. Instead of engineering specific features to use in supervised classifiers, we can instead employ language models to model the type of text we want to detect. Language models have previously been applied for the purpose of text classification (e.g., Bai et al., 2004; Howard and Ruder,"
S19-2092,W18-3504,0,0.0252651,"odels to Detect Hate Speech and Offensive Language Ali Hakimi Parizi, Milton King and Paul Cook Faculty of Computer Science, University of New Brunswick Fredericton, NB E3B 5A3, Canada ahakimi@unb.ca, milton.king@unb.ca, paul.cook@unb.ca Abstract 2 Social media such as Twitter and Facebook are widely used, and hate speech has become prevalent in these platforms. In response to this, a substantial amount of research has recently focused on detecting such disturbing comments. Prior work on hate speech and offensive language detection has mostly focused on supervised machine learning techniques (Mathur et al., 2018; Davidson et al., 2017). A recent shared task on identifying aggression in social media (Kumar et al., 2018) observed there is no significant difference between the performance of neural networks and linear classifiers. Furthermore, this shared task received just one lexicon-based approach, and its performance was not promising. Moreover, all of these approaches required expensive feature engineering and pre-processing. Instead of engineering specific features to use in supervised classifiers, we can instead employ language models to model the type of text we want to detect. Language models h"
S19-2092,W16-3638,0,0.0265176,"e in supervised classifiers, we can instead employ language models to model the type of text we want to detect. Language models have previously been applied for the purpose of text classification (e.g., Bai et al., 2004; Howard and Ruder, 2018). Among different types of language models, recurrent neural network (RNN) language models with LSTM and GRU units have shown promising results for sequence modeling (Mikolov et al., 2012). To the best of our knowledge, RNN language models have not been widely used for detecting hate speech or offensive language. The most closely related work is that of Mehdad and Tetreault (2016), which used both word-level and character-level RNNs to detect abusive language. In their experiments, Mehdad and Tetreault found that character-level language models outperformed word-level language models. A further advantage of character-level language models is that they are able to model out-of-vocabulary In this paper we apply a range of approaches to language modeling — including wordlevel n-gram and neural language models, and character-level neural language models — to the problem of detecting hate speech and offensive language. Our findings indicate that language models are able to"
U12-1014,D07-1016,0,0.226736,"-processing. This is not a criticism of these projects — their goals were to build useful language resources, not speciﬁcally to study the impact of document post-processing on corpora. Nevertheless, because of the immediate opportunities for improving LT by building larger Web corpora, and the importance of post-processing on the quality of the resulting corpora, there appear to be potential opportunities to improve LT by improving Web corpus construction methods. In this paper we consider the importance of language identiﬁcation — which has already been shown to beneﬁt other LT tasks (e.g., Alex et al., 2007) — in Web corpus construction. We build corpora of varying sizes from a readily-available Web crawl (the English portion of ClueWeb09) using a standard corpus construction methodology. This dataset contains only documents classiﬁed as English according to a commonly-used language identiﬁcation tool (T EXT C AT).3 We then produce versions of these corpora from which non-English documents according to a state-ofthe-art language identiﬁcation tool (langid.py, Lui and Baldwin, 2012) are ﬁltered. In this preliminary work, we measure the impact of language identiﬁcation in a task-based evaluation. S"
U12-1014,P01-1005,0,0.0438698,"standard English corpus is lower under a language model trained from a Web corpus built with this extra language identiﬁcation step, demonstrating the importance of state-of-the-art language identiﬁcation in Web corpus construction. 1 The need for large corpora Corpora are essential resources for building language technology (LT) systems for a variety of applications. For example, frequency estimates for n-grams — which can be used to build a language model, a key component of many contemporary LT systems — are typically derived from corpora. Furthermore, bigger corpora are typically better. Banko and Brill (2001) show that for a classiﬁcation task central to many LT problems, performance increases as a variety of models are trained on increasingly large corpora. The Web is a source of vast amounts of linguistic data, and the need for large corpora has motivated a wide range of research into techniques for building corpora of various types from the Web (e.g., Baroni and Bernardini, 2004; Ferraresi et al., 2008; Kilgarriff et al., 2010; Murphy and Stemle, 2011). In stark contrast to manual corpus construction, such automatic methods enable large corpora to be built quickly and inexpensively. Moreover, l"
U12-1014,baroni-bernardini-2004-bootcat,0,0.278729,"example, frequency estimates for n-grams — which can be used to build a language model, a key component of many contemporary LT systems — are typically derived from corpora. Furthermore, bigger corpora are typically better. Banko and Brill (2001) show that for a classiﬁcation task central to many LT problems, performance increases as a variety of models are trained on increasingly large corpora. The Web is a source of vast amounts of linguistic data, and the need for large corpora has motivated a wide range of research into techniques for building corpora of various types from the Web (e.g., Baroni and Bernardini, 2004; Ferraresi et al., 2008; Kilgarriff et al., 2010; Murphy and Stemle, 2011). In stark contrast to manual corpus construction, such automatic methods enable large corpora to be built quickly and inexpensively. Moreover, large Web crawls have recently been produced which are readily-available to the LT community (e.g., ClueWeb091 and CommonCrawl2 ) and can easily be exploited to build corpora much larger than those currently available (and indeed Pomik´alek et al. (2012) have already done so); based on the ﬁndings of Banko and Brill, such corpora could be exploited to improve LT systems. Despite"
U12-1014,baroni-etal-2008-cleaneval,0,0.0247066,"herwise be, potentially having a negative 1 2 http://lemurproject.org/clueweb09/ http://commoncrawl.org/ Paul Cook and Marco Lui. 2012. langid.py for better language modelling. In Proceedings of Australasian Language Technology Association Workshop, pages 107−112. impact on LT systems trained on such a corpus. Similar problems are encountered with the presence of boilerplate text, and duplicate or nearduplicate documents or text segments. Although document post-processing is clearly important to corpus construction, little work has studied it directly, with the notable exception of CleanEval (Baroni et al., 2008), a shared task on cleaning webpages by removing boilerplate and markup. Liu and Curran (2006) and Versley and Panchenko (2012) compare Web corpora with standard corpora in task-based evaluations, but do not speciﬁcally consider the impact of document post-processing. Web corpus construction projects have tended to rely on readily-available tools, or simple heuristics, to accomplish this post-processing. This is not a criticism of these projects — their goals were to build useful language resources, not speciﬁcally to study the impact of document post-processing on corpora. Nevertheless, becau"
U12-1014,kilgarriff-etal-2010-corpus,0,0.035829,"Missing"
U12-1014,E06-1030,0,0.0215814,"ncrawl.org/ Paul Cook and Marco Lui. 2012. langid.py for better language modelling. In Proceedings of Australasian Language Technology Association Workshop, pages 107−112. impact on LT systems trained on such a corpus. Similar problems are encountered with the presence of boilerplate text, and duplicate or nearduplicate documents or text segments. Although document post-processing is clearly important to corpus construction, little work has studied it directly, with the notable exception of CleanEval (Baroni et al., 2008), a shared task on cleaning webpages by removing boilerplate and markup. Liu and Curran (2006) and Versley and Panchenko (2012) compare Web corpora with standard corpora in task-based evaluations, but do not speciﬁcally consider the impact of document post-processing. Web corpus construction projects have tended to rely on readily-available tools, or simple heuristics, to accomplish this post-processing. This is not a criticism of these projects — their goals were to build useful language resources, not speciﬁcally to study the impact of document post-processing on corpora. Nevertheless, because of the immediate opportunities for improving LT by building larger Web corpora, and the imp"
U12-1014,I11-1062,1,0.8824,"Missing"
U12-1014,P12-3005,1,0.868237,"Missing"
U12-1014,W11-2603,0,0.0113387,"age model, a key component of many contemporary LT systems — are typically derived from corpora. Furthermore, bigger corpora are typically better. Banko and Brill (2001) show that for a classiﬁcation task central to many LT problems, performance increases as a variety of models are trained on increasingly large corpora. The Web is a source of vast amounts of linguistic data, and the need for large corpora has motivated a wide range of research into techniques for building corpora of various types from the Web (e.g., Baroni and Bernardini, 2004; Ferraresi et al., 2008; Kilgarriff et al., 2010; Murphy and Stemle, 2011). In stark contrast to manual corpus construction, such automatic methods enable large corpora to be built quickly and inexpensively. Moreover, large Web crawls have recently been produced which are readily-available to the LT community (e.g., ClueWeb091 and CommonCrawl2 ) and can easily be exploited to build corpora much larger than those currently available (and indeed Pomik´alek et al. (2012) have already done so); based on the ﬁndings of Banko and Brill, such corpora could be exploited to improve LT systems. Despite the importance of large Web corpora, the issue of how to best derive a cor"
U12-1014,pomikalek-etal-2012-building,0,0.0189552,"Missing"
U13-1003,I13-1041,1,0.794098,"ugh our features are clearly informative for the task (L ANG ID results comfortably exceed the baseline), there may be useful information that is lost when a document is mapped into this reduced feature space. L ANG ID performs exceptionally poorly when applied to T WITTER in a cross-domain setting, because the classifier predicts a minority class ‘Australian’ for almost all documents. This is likely due to the lack of national corpus training data for ‘Australian’, as Table 4 suggests that national corpus data are an especially poor proxy for Twitter (a result consistent with the findings of Baldwin et al. (2013)). The poor performance of the G EOLOCATION is perhaps more surprising, as like T EXT C ATEGO RIZATION this approach makes use of the full bagof-words feature set. However, in the geolocation task of Wing and Baldridge (2011), the class space is much larger, and furthermore it is structured; classes correspond to regions of the Earth’s surface, and the distance of the predicted region to the goldstandard region is taken into account in evaluation. The national dialect identification task is much more coarse-grained, potentially making it a poor match for geolocation methods. VARIANT PAIR perfo"
U13-1003,D10-1124,0,0.0957795,"Missing"
U13-1003,baroni-bernardini-2004-bootcat,0,0.0332025,"e can build corpora. However, the top-level domain for the United States, .us, is primarily used for morespecialized purposes, such as government, and so a similar Web corpus cannot easily be built for American English. Here we build English Web corpora from .au, .ca, and .uk which — based on the findings of Cook and Hirst (2012) — we assume to represent Australian, Canadian, and British English, respectively. One common method for corpus construction is to issue a large number of queries to a search engine, download the resulting URLs, and postprocess the documents to produce a corpus (e.g., Baroni and Bernardini, 2004; Sharoff, 2006; Kilgarriff et al., 2010). Cook and Hirst (2012) use such a method to build corpora from the .ca and .uk domains; we follow their approach here. Specifically, we select alphabetic types in the BNC with character length greater than 2 and frequency rank 1001–5000 in the BNC as seed words. We then use Baroni and Bernardini’s (2004) BootCaT tools to form 18k random 3-tuples from these seeds. We use the BootCaT tools to issue search engine queries for these tuples in the .au, .ca, and .uk domains. Using the BootCaT tools we 4.3 W EB G OV (Government) The government of each of the c"
U13-1003,C12-1064,1,0.846683,"Missing"
U13-1003,C12-1025,0,0.0405072,"ty of sources, 1 We don’t consider American English because of a rather surprising lack of available resources for this national dialect, discussed in Section 4. Marco Lui and Paul Cook. 2013. Classifying English Documents by National Dialect. In Proceedings of Australasian Language Technology Association Workshop, pages 5−15. this is not the language that the document is written in. One approach to NLI is to capture grammatical errors made by authors, through the use of contrastive analysis (Wong and Dras, 2009), parse structures (Wong and Dras, 2011) or adaptor grammars (Wong et al., 2012). Brooke and Hirst (2012) test a broad array of approaches to NLI, and specifically highlight issues with in-domain evaluation thereof. (4) we empirically evaluate a number of text classification methods for national dialect identification, and (5) we find that we can train classifiers that are able to predict the national dialect of documents across data sources. 2 Related Work National dialect identification is conceptually related to a range of established text classification tasks. In this section, we give some background on related areas, deferring the description of the specific methods we implement to Section 3"
U13-1003,kilgarriff-etal-2010-corpus,0,0.0215484,"Missing"
U13-1003,I11-1062,1,0.903511,"two corpora in this study. Appropriate resources are not available for American or Australian English. The Corpus of Contemporary American English (COCA, Davies, 2009) currently consists of over 450 million words of American English, but can only be accessed through a web interface; the full text form is unavailable. The American National Corpus (ANC, Ide, 2009) is much smaller than the BNC and Strathy Corpus at approximately only 11 million words.4 In the case of Australian English, the AusL ANG ID We treat each dialect as a distinct language, and apply the language identification method of Lui and Baldwin (2011) in which documents are represented using a mixture of specially-selected byte sequences. The method specifically exploits differences in data sources to learn a set of byte sequences that is representative of languages (or in our case, dialects) across all the data sources. This feature selection is done by scoring each sequence using information gain (IG, Quinlan, 1993), with respect to each dialect as well as with each data source. This representation is then combined with a multinomial naive Bayes classifier. 2 http://wordlist.sourceforge.net http://www.queensu.ca/strathy/ 4 This figure re"
U13-1003,P12-2052,0,0.0175461,"andidate authors (Stamatatos, 2009), and is sometimes incorrectly conflated with authorship profiling. Mosteller and Wallace (1964) used a set of function words to attribute papers of disputed authorship. Other stylometric features used to identify authors include average sentence and word length (Yule, 1939). Modern features used for authorship attribution include distributions over function words (Zhao and Zobel, 2005), as well as features derived from parsing and part-of-speech tagging (Hirst and Feiguina, 2007). Author-aware topic models have also been proposed for authorship attribution (Seroussi et al., 2012). 2.1 Text Categorization Text categorization has been described as the intersection of machine learning and information retrieval (Sebastiani, 2005), and is focused on tasks such as mapping newswire documents onto the topics they discuss (Debole and Sebastiani, 2005). A large variety of methods have been examined in the literature, due to the large overlap with the machine learning community (Sebastiani, 2002). One approach that has been shown to consistently perform well is the use of Support Vector Machines (SVM, Cortes and Vapnik, 1995). Joachims (1998) argued for their use in text categor"
U13-1003,P12-3005,1,0.847648,"stralia µ σ – – 2111.7 3261.5 1237.2 2706.3 12.1 6.3 # 10000 10000 10000 3598 Canada µ σ 2415.8 2750.4 2459.4 3839.5 3980.4 4522.4 11.8 6.3 United Kingdom # µ σ 10000 2742.3 2692.9 10000 2098.1 3527.4 10000 2558.1 3327.4 24047 12.0 6.5 Table 1: Characteristics of the E N D IALECT dataset. # is the document count, µ and σ are the mean and standard deviation of document length (in words). tralian Corpus of English (Green and Peters, 1991) consists of just 1 million words.5 4.2 then download the resulting URLs, and eliminate duplicates. We further eliminate non-English documents using langid.py (Lui and Baldwin, 2012). Following Cook and Hirst we only retain up to three randomly-selected documents per domain (e.g., www.cbc.ca). The final corpora consist of roughly 77, 96, and 115 million tokens for the .au, .ca, and .uk domains, respectively. W EB The Web has been widely used for building corpora (e.g., Baroni et al., 2009; Kilgarriff et al., 2010) with Cook and Hirst (2012) presenting preliminary results suggesting that English corpora from top-level domains might represent corresponding national dialects of English. Australia, Canada, and the United Kingdom all have corresponding top-level domains that c"
U13-1003,P11-1096,0,0.368716,"of studies (e.g., Yang and Liu, 1999; Drucker et al., 1999). 2.5 Text-based Geolocation Social media has recently exploded in popularity, with Twitter reporting that roughly 500 million tweets are sent each day (Twitter, 2013). There is a relationship between textual content and geolocation, with for example, texts containing words such as streetcar, Maple Leafs, and DVP likely being related to Toronto, Canada (Han et al., 2012). Eisenstein et al. (2010) apply techniques from topic modeling to study variation in word usage on Twitter in the United States. Of particular relevance to our work, Wing and Baldridge (2011) and Roller et al. (2012) aggregate the tweets of users to predict their physical location in grid-based representations of the continental United States. These methods consider the KL-divergence between the distribution of words in a user’s aggregated tweets and that of the tweets known to originate from each grid cell, with the most-similar cell being selected as the target user’s most-likely location. 2.2 Language Identification Language identification is the task of classifying a document according to the natural language it is written in. Recent work has applied language identification te"
U13-1003,U09-1008,0,0.146201,"assification, (3) we assemble a dataset for national dialect identification using corpora from a variety of sources, 1 We don’t consider American English because of a rather surprising lack of available resources for this national dialect, discussed in Section 4. Marco Lui and Paul Cook. 2013. Classifying English Documents by National Dialect. In Proceedings of Australasian Language Technology Association Workshop, pages 5−15. this is not the language that the document is written in. One approach to NLI is to capture grammatical errors made by authors, through the use of contrastive analysis (Wong and Dras, 2009), parse structures (Wong and Dras, 2011) or adaptor grammars (Wong et al., 2012). Brooke and Hirst (2012) test a broad array of approaches to NLI, and specifically highlight issues with in-domain evaluation thereof. (4) we empirically evaluate a number of text classification methods for national dialect identification, and (5) we find that we can train classifiers that are able to predict the national dialect of documents across data sources. 2 Related Work National dialect identification is conceptually related to a range of established text classification tasks. In this section, we give some"
U13-1003,D11-1148,0,0.0163752,"for national dialect identification using corpora from a variety of sources, 1 We don’t consider American English because of a rather surprising lack of available resources for this national dialect, discussed in Section 4. Marco Lui and Paul Cook. 2013. Classifying English Documents by National Dialect. In Proceedings of Australasian Language Technology Association Workshop, pages 5−15. this is not the language that the document is written in. One approach to NLI is to capture grammatical errors made by authors, through the use of contrastive analysis (Wong and Dras, 2009), parse structures (Wong and Dras, 2011) or adaptor grammars (Wong et al., 2012). Brooke and Hirst (2012) test a broad array of approaches to NLI, and specifically highlight issues with in-domain evaluation thereof. (4) we empirically evaluate a number of text classification methods for national dialect identification, and (5) we find that we can train classifiers that are able to predict the national dialect of documents across data sources. 2 Related Work National dialect identification is conceptually related to a range of established text classification tasks. In this section, we give some background on related areas, deferring"
U13-1003,D12-1064,0,0.0191023,"corpora from a variety of sources, 1 We don’t consider American English because of a rather surprising lack of available resources for this national dialect, discussed in Section 4. Marco Lui and Paul Cook. 2013. Classifying English Documents by National Dialect. In Proceedings of Australasian Language Technology Association Workshop, pages 5−15. this is not the language that the document is written in. One approach to NLI is to capture grammatical errors made by authors, through the use of contrastive analysis (Wong and Dras, 2009), parse structures (Wong and Dras, 2011) or adaptor grammars (Wong et al., 2012). Brooke and Hirst (2012) test a broad array of approaches to NLI, and specifically highlight issues with in-domain evaluation thereof. (4) we empirically evaluate a number of text classification methods for national dialect identification, and (5) we find that we can train classifiers that are able to predict the national dialect of documents across data sources. 2 Related Work National dialect identification is conceptually related to a range of established text classification tasks. In this section, we give some background on related areas, deferring the description of the specific methods"
W06-1207,W04-2608,0,0.0560858,"se compound verbs (similar to VPCs) as aspectual, spatial, or adverbial. In the future, we aim to extend the scope of our work, to determine the meaning of a particle in a VPC token, along the lines of our sense classes here. This will almost certainly require semantic classification of the verb token (Lapata and Brew, 2004), similar to our approach here of using the semantic class of a verb type as indicative of the meaning of a particle type. Particle semantics has clear relations to preposition semantics. Some research has focused on the sense disambiguation of specific prepositions (e.g., Alam, 2004), while other work has classified preposition tokens according to their semantic role (O’Hara and Wiebe, 2003). Moreover, two large lexical resources of preposition senses are currently under construction, The Preposition Project (Litkowski, 2005) and PrepNet (SaintDizier, 2005). These resources were not suitable as the basis for our sense classes because they do not address the range of metaphorical extensions that a preposition/particle can take on, but future work may enable larger scale studies of the type 7 Conclusions While progress has recently been made in techniques for assessing the"
W06-1207,W03-1809,0,0.362002,"Missing"
W06-1207,W05-1005,1,0.867879,"n a goal-oriented sense as in A challenge in learning the semantics of multiword expressions (MWEs) is their varying degrees of compositionality—the contribution of each component word to the overall semantics of the expression. MWEs fall on a range from fully compositional (i.e., each component contributes its meaning, as in frying pan) to noncompositional or idiomatic (as in hit the roof ). Because of this variation, researchers have explored automatic methods for learning whether, or the degree to which, an MWE is compositional (e.g., Lin, 1999; Bannard et al., 2003; McCarthy et al., 2003; Fazly et al., 2005). However, such work leaves unaddressed the basic issue of which of the possible meanings of a component word is contributed when the MWE is (at least partly) compositional. Words are notoriously ambiguous, so that even if it can be determined that an MWE is compositional, its meaning 45 Proceedings of the Workshop on Multiword Expressions: Identifying and Exploiting Underlying Properties, pages 45–53, c Sydney, July 2006. 2006 Association for Computational Linguistics 2003). The features are motivated by the fact that semantic properties of a verb are reflected in the syntactic expression of"
W06-1207,E03-1040,1,0.900401,"Missing"
W06-1207,J04-1003,0,0.0241316,"trick and Fletcher (2005) classify VPC tokens, considering each as compositional, non-compositional or not a VPC. Again, however, it is important to recognize which of the possible meaning components is being contributed. In this vein, Uchiyama et al. (2005) tackle token classification of Japanese compound verbs (similar to VPCs) as aspectual, spatial, or adverbial. In the future, we aim to extend the scope of our work, to determine the meaning of a particle in a VPC token, along the lines of our sense classes here. This will almost certainly require semantic classification of the verb token (Lapata and Brew, 2004), similar to our approach here of using the semantic class of a verb type as indicative of the meaning of a particle type. Particle semantics has clear relations to preposition semantics. Some research has focused on the sense disambiguation of specific prepositions (e.g., Alam, 2004), while other work has classified preposition tokens according to their semantic role (O’Hara and Wiebe, 2003). Moreover, two large lexical resources of preposition senses are currently under construction, The Preposition Project (Litkowski, 2005) and PrepNet (SaintDizier, 2005). These resources were not suitable"
W06-1207,P99-1041,0,0.0612925,"han one sense; in contrast to (1a), come up may use up in a goal-oriented sense as in A challenge in learning the semantics of multiword expressions (MWEs) is their varying degrees of compositionality—the contribution of each component word to the overall semantics of the expression. MWEs fall on a range from fully compositional (i.e., each component contributes its meaning, as in frying pan) to noncompositional or idiomatic (as in hit the roof ). Because of this variation, researchers have explored automatic methods for learning whether, or the degree to which, an MWE is compositional (e.g., Lin, 1999; Bannard et al., 2003; McCarthy et al., 2003; Fazly et al., 2005). However, such work leaves unaddressed the basic issue of which of the possible meanings of a component word is contributed when the MWE is (at least partly) compositional. Words are notoriously ambiguous, so that even if it can be determined that an MWE is compositional, its meaning 45 Proceedings of the Workshop on Multiword Expressions: Identifying and Exploiting Underlying Properties, pages 45–53, c Sydney, July 2006. 2006 Association for Computational Linguistics 2003). The features are motivated by the fact that semantic"
W06-1207,W03-1810,0,0.753485,"), come up may use up in a goal-oriented sense as in A challenge in learning the semantics of multiword expressions (MWEs) is their varying degrees of compositionality—the contribution of each component word to the overall semantics of the expression. MWEs fall on a range from fully compositional (i.e., each component contributes its meaning, as in frying pan) to noncompositional or idiomatic (as in hit the roof ). Because of this variation, researchers have explored automatic methods for learning whether, or the degree to which, an MWE is compositional (e.g., Lin, 1999; Bannard et al., 2003; McCarthy et al., 2003; Fazly et al., 2005). However, such work leaves unaddressed the basic issue of which of the possible meanings of a component word is contributed when the MWE is (at least partly) compositional. Words are notoriously ambiguous, so that even if it can be determined that an MWE is compositional, its meaning 45 Proceedings of the Workshop on Multiword Expressions: Identifying and Exploiting Underlying Properties, pages 45–53, c Sydney, July 2006. 2006 Association for Computational Linguistics 2003). The features are motivated by the fact that semantic properties of a verb are reflected in the syn"
W06-1207,P04-1036,0,0.0117226,"ure its semantics when used in VPCs in general). The deadline is coming up. While our long-term goal is token classification (disambiguation) of a VPC in context, following other work on VPCs (e.g., Bannard et al., 2003; McCarthy et al., 2003), we begin here with the task of type classification. Given our use of features which capture the statistical behaviour relevant to a VPC across a corpus, we assume that the outcome of type classification yields the predominant sense of the particle in the VPC. Predominant sense identification is a useful component of sense disambiguation of word tokens (McCarthy et al., 2004), and we presume our VPC type classification work will form the basis for later token disambiguation. Section 2 continues the paper with a discussion of the features we developed for particle sense classification. Section 3 first presents some brief cognitive linguistic background, followed by the sense classes of up used in our experiments. Sections 4 and 5 discuss our experimental set-up and results, Section 6 related work, and Section 7 our conclusions. 2.1.2 Particle Features Two types of features are motivated by properties specific to the semantics and syntax of particles and VPCs. First"
W06-1207,W03-0411,0,0.174508,"Missing"
W07-1106,W03-1812,0,0.345058,"s. 1 Introduction Identification of multiword expressions (MWEs), such as car park, make a decision, and kick the bucket, is extremely important for accurate natural language processing (NLP) (Sag et al., 2002). Most MWEs need to be treated as single units of meaning, e.g., make a decision roughly means “decide”. Nonetheless, the components of an MWE can be separated, making it hard for an NLP system to identify the expression as a whole. Many researchers have recently developed methods for the automatic acquisition of various properties of MWEs from corpora (Lin, 1999; Krenn and Evert, 2001; Baldwin et al., 2003; McCarthy et al., 2003; Venkatapathy and Joshi, 2005; Villada Moir´on and Tiedemann, 2006; Most of these methods have been aimed at recognizing MWE types; less attention has been paid to the identification of instances (tokens) of MWEs in context. For example, most such techniques (if successful) would identify make a face as a potential MWE. This expression is, however, ambiguous between an idiom, as in The little girl made a funny face at her mother, and a literal combination, as in She made a face on the snowman using a carrot and two buttons. Despite the common perception that phrases tha"
W07-1106,E06-1042,0,0.147767,"zly and Stevenson, 2006). Much research has addressed the non-compositionality of MWEs as an important property related to their idiomaticity, and has used it in the classification of both MWE types and tokens (Baldwin et al., 2003; McCarthy et al., 2003; Katz and Giesbrecht, 2006). We also make use of this property in an MWE token classification task, but in addition, we draw on other salient characteristics of MWEs which have been previously shown to be useful for their type classification (Evert et al., 2004; Fazly and Stevenson, 2006). The idiomatic/literal token classification methods of Birke and Sarkar (2006) and Katz and Giesbrecht (2006) rely primarily on the local context of a token, and fail to exploit specific linguistic properties of non-literal language. Our results suggest that such properties are often more informative than the local 47 context, in determining the class of an MWE token. The supervised classifier of Patrick and Fletcher (2005) distinguishes between compositional and non-compositional English verb-particle construction tokens. Their classifier incorporates linguistically-motivated features, such as the degree of separation between the verb and particle. Here, we focus on a"
W07-1106,evert-etal-2004-identifying,0,0.220131,"tion, and draw on the local context of an expression to disambiguate it. Such techniques either do not use any information regarding the linguistic properties of MWEs (Birke and Sarkar, 2006), or mainly focus on their noncompositionality (Katz and Giesbrecht, 2006). Pre41 Proceedings of the Workshop on A Broader Perspective on Multiword Expressions, pages 41–48, c Prague, June 2007. 2007 Association for Computational Linguistics vious work on the identification of MWE types, however, has found other properties of MWEs, such as their syntactic fixedness, to be relevant to their identification (Evert et al., 2004; Fazly and Stevenson, 2006). In this paper, we propose techniques that draw on this property to classify individual tokens of a potentially idiomatic phrase as literal or idiomatic. We also put forward classification techniques that combine such information with evidence from the local context of an MWE. We explore the hypothesis that informative prior knowledge about the overall syntactic behaviour of an idiomatic expression (type-based knowledge) can be used to determine whether an instance of the expression is used literally or idiomatically (tokenbased knowledge). Based on this hypothesis"
W07-1106,E06-1043,1,0.917289,"e local context of an expression to disambiguate it. Such techniques either do not use any information regarding the linguistic properties of MWEs (Birke and Sarkar, 2006), or mainly focus on their noncompositionality (Katz and Giesbrecht, 2006). Pre41 Proceedings of the Workshop on A Broader Perspective on Multiword Expressions, pages 41–48, c Prague, June 2007. 2007 Association for Computational Linguistics vious work on the identification of MWE types, however, has found other properties of MWEs, such as their syntactic fixedness, to be relevant to their identification (Evert et al., 2004; Fazly and Stevenson, 2006). In this paper, we propose techniques that draw on this property to classify individual tokens of a potentially idiomatic phrase as literal or idiomatic. We also put forward classification techniques that combine such information with evidence from the local context of an MWE. We explore the hypothesis that informative prior knowledge about the overall syntactic behaviour of an idiomatic expression (type-based knowledge) can be used to determine whether an instance of the expression is used literally or idiomatically (tokenbased knowledge). Based on this hypothesis, we develop unsupervised me"
W07-1106,P06-2046,0,0.119525,"ocal 47 context, in determining the class of an MWE token. The supervised classifier of Patrick and Fletcher (2005) distinguishes between compositional and non-compositional English verb-particle construction tokens. Their classifier incorporates linguistically-motivated features, such as the degree of separation between the verb and particle. Here, we focus on a different class of English MWEs, verb+noun combinations. Moreover, by making a more direct use of their syntactic behaviour, we develop unsupervised token classification methods that perform well. The unsupervised token classifier of Hashimoto et al. (2006) uses manually-encoded information about allowable and non-allowable syntactic transformations of Japanese idioms—that are roughly equivalent to our notions of canonical and non-canonical forms. The rule-based classifier of Uchiyama et al. (2005) incorporates syntactic information about Japanese compound verbs (JCVs), a type of MWE composed of two verbs. In both cases, although the classifiers incorporate syntactic information about MWEs, their manual development limits the scalability of the approaches. Uchiyama et al. (2005) also propose a statistical token classification method for JCVs. Th"
W07-1106,W06-1203,0,0.751428,"as a word frequency vector ~ve where each dimension i of ~ve is the frequency with which e co-occurs with word i across the usages of e. We similarly estimate the meaning of a single token of an expression t as a vector ~vt capturing that usage. To determine if an instance of an expression is literal or idiomatic, we compare its co-occurrence vector to the co-occurrence vectors representing each of the literal and idiomatic meanings of the expression. We use a standard measure of distributional similarity, 43 cosine, to compare co-occurrence vectors. In supervised approaches, such as that of Katz and Giesbrecht (2006), co-occurrence vectors for literal and idiomatic meanings are formed from manuallyannotated training data. Here, we propose unsupervised methods for estimating these vectors. We use one way of estimating the idiomatic meaning of an expression, and two ways for estimating its literal meaning, yielding two methods for token classification. Our first Diff method draws further on our expectation that canonical forms are more likely idiomatic usages, and non-canonical forms are more likely literal usages. We estimate the idiomatic meaning of an expression by building a co-occurrence vector, ~ vI -"
W07-1106,P99-1041,0,0.846778,"te-of-the-art supervised techniques. 1 Introduction Identification of multiword expressions (MWEs), such as car park, make a decision, and kick the bucket, is extremely important for accurate natural language processing (NLP) (Sag et al., 2002). Most MWEs need to be treated as single units of meaning, e.g., make a decision roughly means “decide”. Nonetheless, the components of an MWE can be separated, making it hard for an NLP system to identify the expression as a whole. Many researchers have recently developed methods for the automatic acquisition of various properties of MWEs from corpora (Lin, 1999; Krenn and Evert, 2001; Baldwin et al., 2003; McCarthy et al., 2003; Venkatapathy and Joshi, 2005; Villada Moir´on and Tiedemann, 2006; Most of these methods have been aimed at recognizing MWE types; less attention has been paid to the identification of instances (tokens) of MWEs in context. For example, most such techniques (if successful) would identify make a face as a potential MWE. This expression is, however, ambiguous between an idiom, as in The little girl made a funny face at her mother, and a literal combination, as in She made a face on the snowman using a carrot and two buttons. D"
W07-1106,W03-1810,0,0.196615,"tification of multiword expressions (MWEs), such as car park, make a decision, and kick the bucket, is extremely important for accurate natural language processing (NLP) (Sag et al., 2002). Most MWEs need to be treated as single units of meaning, e.g., make a decision roughly means “decide”. Nonetheless, the components of an MWE can be separated, making it hard for an NLP system to identify the expression as a whole. Many researchers have recently developed methods for the automatic acquisition of various properties of MWEs from corpora (Lin, 1999; Krenn and Evert, 2001; Baldwin et al., 2003; McCarthy et al., 2003; Venkatapathy and Joshi, 2005; Villada Moir´on and Tiedemann, 2006; Most of these methods have been aimed at recognizing MWE types; less attention has been paid to the identification of instances (tokens) of MWEs in context. For example, most such techniques (if successful) would identify make a face as a potential MWE. This expression is, however, ambiguous between an idiom, as in The little girl made a funny face at her mother, and a literal combination, as in She made a face on the snowman using a carrot and two buttons. Despite the common perception that phrases that can be idioms are mai"
W07-1106,H05-1113,0,0.0624968,"Missing"
W07-1106,W06-2405,0,0.133843,"Missing"
W07-1106,J03-4003,0,\N,Missing
W09-2010,P06-2005,0,0.62746,"tivity, and hence many novel lexical items, in the language of text messaging, or texting language. Normalization of non-standard forms— converting non-standard forms to their standard forms—is a challenge that must be tackled before other types of natural language processing can take place (Sproat et al., 2001). In the case of text messages, text-to-speech synthesis may be 1 The number of characters in a text message may also be limited to 160 characters, although this is not always the case. particularly useful for the visually impaired; automatic translation has also been considered (e.g., Aw et al., 2006). For texting language, given the abundance of creative forms, and the wide-ranging possibilities for creating new forms, normalization is a particularly important problem, and has indeed received some attention in computational linguistics (e.g., Aw et al., 2006; Choudhury et al., 2007; Kobus et al., 2008). In this paper we propose an unsupervised noisy channel method for texting language normalization, that gives performance on par with that of a supervised system. We pursue unsupervised approaches to this problem, as large collections of text messages, and their corresponding standard forms"
W09-2010,fairon-paumier-2006-translated,0,0.031005,"ve phenomena similar to text messaging, although at a lower frequency (Ling and Baron, 2007). Moreover, technological changes, such as new input devices, are likely to have an impact on the language of such media (Thurlow, 2003).3 An unsupervised approach, drawing on linguistic properties of creative word formations, has the potential to be adapted for normalization of text in other similar genres—such as Internet discussion forums—without the cost of developing a large training corpus. Moreover, normalization may be particularly important for such genres, given the 2 One notable exception is Fairon and Paumier (2006), although this resource is in French. The resource used in our study, Choudhury et al. (2007), is quite small in comparison. 3 The rise of other technology, such as word prediction, could reduce the use of abbreviations, although it’s not clear such technology is widely used (Grinter and Eldridge, 2001). 71 Proceedings of the NAACL HLT Workshop on Computational Approaches to Linguistic Creativity, pages 71–78, c Boulder, Colorado, June 2009. 2009 Association for Computational Linguistics Formation type Stylistic variation Subseq. abbrev. Prefix clipping Syll. letter/digit G-clipping Phonetic"
W09-2010,N07-1047,0,0.035286,"d of the word; P (ht |ps , hs , pos) is the probability of texting form graphemes ht given the standard form phonemes ps and graphemes hs at position pos. ht , ps , and hs can be a single grapheme or phoneme, or a bigram. We compute edit-probability between the graphemes of si and ti . When filling each cell in the chart, we consider edit operations between segments of si and ti of length 0–2, referred to as a and b, respectively. If a aligns with phonemes in si , we also consider those phonemes, p. In our lexicon, the graphemes and phonemes of each word are aligned according to the method of Jiampojamarn et al. (2007). For example, the alignment for without is given in Table 2. The probability of each edit operation is then determined by three properties—the length of a, whether a aligns with any phonemes in si , and if so, p—as shown below: |a|= 0 or 1, not aligned w/ si phonemes: P (b|a, pos ) |a|= 2, not aligned w/ si phonemes: 0 |a|= 1 or 2, aligned w/ si phonemes: P (b|p, a, pos ) 3.1.2 Subsequence Abbreviations We model subsequence abbreviations according to the equation below: ( c if ti is a subseq of si P (ti |si , subseq abrv) = 0 otherwise where c is a constant. Note that this is similar to the e"
W09-2010,C08-1056,0,0.458132,"., 2001). In the case of text messages, text-to-speech synthesis may be 1 The number of characters in a text message may also be limited to 160 characters, although this is not always the case. particularly useful for the visually impaired; automatic translation has also been considered (e.g., Aw et al., 2006). For texting language, given the abundance of creative forms, and the wide-ranging possibilities for creating new forms, normalization is a particularly important problem, and has indeed received some attention in computational linguistics (e.g., Aw et al., 2006; Choudhury et al., 2007; Kobus et al., 2008). In this paper we propose an unsupervised noisy channel method for texting language normalization, that gives performance on par with that of a supervised system. We pursue unsupervised approaches to this problem, as large collections of text messages, and their corresponding standard forms, are not readily available.2 Furthermore, other forms of computer-mediated communication, such as Internet messaging, exhibit creative phenomena similar to text messaging, although at a lower frequency (Ling and Baron, 2007). Moreover, technological changes, such as new input devices, are likely to have an"
W09-2010,P02-1019,0,0.501736,"th machine translation and automatic speech recognition for text message normalization. However, both of these approaches are supervised, and have only limited means for normalizing texting forms that do not occur in the training data. Our work, like that of Choudhury et al. (2007), can be viewed as a noisy-channel model for spelling error correction (e.g., Mays et al., 1991; Brill and Moore, 2000), in which texting forms are seen as a kind of spelling error. Furthermore, like our approach to text message normalization, approaches to spelling correction have incorporated phonemic information (Toutanova and Moore, 2002). The word model of the supervised approach of Choudhury et al. consists of hidden Markov models, which capture properties of texting language similar to those of our stylistic variation model. We propose multiple word models—corresponding to frequent texting language formation processes—and an unsupervised method for parameter estimation. 7 Conclusions We analyze a sample of texting forms to determine frequent word formation processes in creative texting language. Drawing on these observations, we construct an unsupervised noisy-channel model for text message normalization. On an unseen test"
W09-2010,P00-1037,0,\N,Missing
W10-2109,P07-2009,0,0.018145,"date the factors influencing interpretation of such sentences across a collection of actual usages. The second reason for our interest in this construction is that it illustrates a complex ambiguity that can cause difficulty for natural language processing applications that seek to semantically interpret text. Faced with the above two sentences, a parsing system (in the absence of specific knowledge of this construction) will presumably find the exact same structure for each, giving no basis on which to determine the correct meaning from the parse. (Unsurprisingly, when we run the C&C Parser (Curran et al., 2007) on (1) and (2) it assigns the same structure to each sentence.) Our second goal in this work is thus to explore whether increased linguistic understanding of this phenomenon could be used to disambiguate such examples automatically. Specifically, we use this construction as an example of the kind of difficulties faced in semantic interpretation when meaning may be determined by pragmatic or other extra-syntactic factors, in order to explore whether We consider sentences of the form No X is too Y to Z, in which X is a noun phrase, Y is an adjective phrase, and Z is a verb phrase. Such construc"
W10-2109,D08-1027,0,0.011597,"Missing"
W10-2109,H05-1044,0,0.0208223,"ebatable paths we’ve taken in our war against terror? (7) No neighborhood is too remote to diminish Mr. Levine’s determination to discover and announce some previously unheralded treat. Polarity Because of the observation that the verb in the target construction, in particular, has some property of negativity in the “no” interpretation, we also use features representing the semantic polarity of the noun, adjective, and verb in each instance. The features are tertiary, representing positive, neutral, or negative polarity. We obtain polarity information from the subjectivity lexicon provided by Wilson et al. (2005), and consider words to be neutral if they have both positive and negative polarity, or are not in the lexicon. (8) No one is too remote anymore to be concerned about style, Ms. Hansen suggested. In example (6) the author is using the target construction to express somebody else’s viewpoint that “any amount should be spent on the war against terror”. Therefore the literal reading of the target construction appears to be the “every” interpretation. However, this construction is being used rhetorically (as part of the overall sentence) to express the author’s belief that “too much money is being"
W10-2109,C00-1028,0,0.0390032,"Missing"
W10-2109,J02-2003,0,0.0250003,"Missing"
W10-2109,J03-4003,0,0.00650375,"ated to a verb, as in No interest is too narrow for attention. (We would only extract the latter if there were an infinitive verb embedded in or following the NP.) In the present study we limit our consideration to sentences of the form discussed by Wason and Reich (1979), but intend to consider related constructions such as these—which appear to exhibit the same ambiguity as the target construction—in the future. We next manually identify the noun, adjective, and verb that participate in the target construction in each sentence. Although this could be done automatically using a parser (e.g., Collins, 2003) or chunker (e.g., Abney, 1991), here we want to ensure error-free identification. We also note a number of sentences containing co-ordination, such as in the following example. 3.2 Annotation We used Amazon Mechanical Turk (AMT, https://www.mturk.com/) to obtain judgements as to the correct interpretation of each instance of the target construction in both the development and testing datasets. For each instance, we generated two paraphrases, one corresponding to each of the interpretations discussed in Section 1. We then presented the given instance of the target construction along with its t"
W13-1008,J09-1005,1,0.826814,"o identify many fixed and semifixed clich´es. Nevertheless, an appropriate clich´e lexicon would be required for this approach. Moreover, because of the relationship between clich´es and culture, to be applicable to historical texts, such as for the literary analysis of interest to us, a lexicon for the appropriate time period would be required. Techniques for MWE extraction could potentially be used to (semi-) automatically build a clich´e lexicon. Much work in this area has again focused on specific types of MWEs — e.g., verb–particle constructions (Baldwin, 2005) or verb–noun combinations (Fazly et al., 2009) — but once more the heterogeneity of clich´es limits the applicability of such approaches for extracting them. Methods based on strength of association — applied to n-grams or words co-occurring through some other relation such as syntactic dependency (see Evert, 2008, for an overview) — could be applied to extract a wider range of MWEs, although here most research has focused on two-word co-occurrences, with considerably less attention paid to longer MWEs. Even if general-purpose MWE extraction were a solved problem, methods would still be required to distinguish between MWEs that are and ar"
W13-1008,U12-1012,0,0.0400271,"be strongly context dependent. This paper identifies clich´es as an under-studied problem closely related to many issues of interest to the MWE community. We propose a preliminary method for assessing the degree to which a text is clich´ed, and then show how such a method can contribute to literary analysis. Specifically, we apply this approach to James Joyce’s novel Ulysses to offer insight into the ongoing literary debate about the use of clich´es in this work. 2 Related work Little research in computational linguistics has specifically addressed clich´es. The most relevant work is that of Smith et al. (2012) who propose a method for identifying clich´es in song lyrics, and determining the extent to which a song is clich´ed. Their method combines information about rhymes and the df-idf of trigrams (tf-idf, but using document frequency instead of term frequency) in song 53 lyrics. However, this method isn’t applicable for our goal of determining how clich´ed an arbitrary text is with a focus on literary analysis, because in this case rhyming is not a typical feature of the texts. Moreover, repetition in song lyrics motivated their df-idf score, but this is not a salient feature of the texts we cons"
W14-5315,N10-1027,1,0.833342,"gual POS tagging as well as fine-grained tags extracted from a deep grammar of English, and discuss additional data we collected for the open submissions, utilizing custombuilt web corpora based on top-level domains as well as existing corpora. 1 Introduction Language identification (LangID) is the problem of determining what natural language a document is written in. Studies in the area often report high accuracy (Cavnar and Trenkle, 1994; Dunning, 1994; Grefenstette, 1995; Prager, 1999; Teahan, 2000). However, recent work has shown that high accuracy is only achieved under ideal conditions (Baldwin and Lui, 2010), and one area that needs further work is accurate discrimination between closely-related languages (Ljubeˇsi´c et al., 2007; Tiedemann and Ljubeˇsi´c, 2012). The problem has been explored for specific groups of confusable languages, such as Malay/Indonesian (Ranaivo-Malancon, 2006), South-Eastern European languages (Tiedemann and Ljubeˇsi´c, 2012), as well as varieties of English (Lui and Cook, 2013), Portuguese (Zampieri and Gebre, 2012), and Spanish (Zampieri et al., 2013). The Discriminating Similar Language (DSL) shared task (Zampieri et al., 2014) was hosted at the VarDial workshop at CO"
W14-5315,C10-3010,1,0.824871,"ciated with those sub-languages. Based on the findings of Cook and Hirst (2012), the assumption underlying this approach is that text found in the top-level domains (TLDs) of those countries will primarily be of the sub-language dominant in that country. For instance, we assume that Portuguese text found when crawling the .pt TLD will primarily be European Portuguese, while the Portuguese found in .br will be primarily Brazilian Portuguese. The process of creating a corpus for each sub-language involved translating a sample of 200 of the original ukWaC queries into each language using Panlex (Baldwin et al., 2010).5 These queries were then submitted to the Bing Search API using the BootCaT tools (Baroni and Bernardini, 2004), constraining results to the relevant TLD. For each query, we took the first 10 URLs yielded by Bing and appended them to a list of seed URLs for that language. After deduplication, the seed URLs were then fed to a Heritrix 3.1.16 instance with default settings other than constraining the crawled content to the relevant TLD. Corpora were then created from the data gathered by Heritrix. Following the ukWaC approach, only documents with a MIME type of HTML and size between 5k and 200"
W14-5315,baroni-bernardini-2004-bootcat,0,0.0821185,"this approach is that text found in the top-level domains (TLDs) of those countries will primarily be of the sub-language dominant in that country. For instance, we assume that Portuguese text found when crawling the .pt TLD will primarily be European Portuguese, while the Portuguese found in .br will be primarily Brazilian Portuguese. The process of creating a corpus for each sub-language involved translating a sample of 200 of the original ukWaC queries into each language using Panlex (Baldwin et al., 2010).5 These queries were then submitted to the Bing Search API using the BootCaT tools (Baroni and Bernardini, 2004), constraining results to the relevant TLD. For each query, we took the first 10 URLs yielded by Bing and appended them to a list of seed URLs for that language. After deduplication, the seed URLs were then fed to a Heritrix 3.1.16 instance with default settings other than constraining the crawled content to the relevant TLD. Corpora were then created from the data gathered by Heritrix. Following the ukWaC approach, only documents with a MIME type of HTML and size between 5k and 200k bytes were used. Justext (Pomik´alek, 2011) was used to extract text from the selected documents. langid.py (Lu"
W14-5315,D13-1120,0,0.0477973,"Missing"
W14-5315,P13-2112,1,0.893753,"Missing"
W14-5315,2005.mtsummit-papers.11,0,0.0102014,"Missing"
W14-5315,I11-1062,1,0.875837,"proceedings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/ 129 Proceedings of the First Workshop on Applying NLP Tools to Similar Languages, Varieties and Dialects, pages 129–138, Dublin, Ireland, August 23 2014. 2 Overview Our main focus was to explore novel methods and sources of training data for discriminating similar languages. In this section, we describe techniques and text representations that we tested, as well as the external data sources that we used to build language identifiers for this task. 2.1 Language-Indicative Byte Sequences Lui and Baldwin (2011) introduced the LD feature set, a document representation for LangID that is robust to variation in languages across different sources of text. The LD feature set can be thought of as language-indicative byte sequences, i.e. sequences of 1 to 4 bytes that have been selected to be strongly characteristic of a particular language or set of languages regardless of the text source. Lui and Baldwin (2012) present langid.py,1 an off-the-shelf LangID system that utilizes the LD feature set. In this work, we re-train langid.py using the training data provided by the shared task organizers, and use thi"
W14-5315,P12-3005,1,0.938117,"e describe techniques and text representations that we tested, as well as the external data sources that we used to build language identifiers for this task. 2.1 Language-Indicative Byte Sequences Lui and Baldwin (2011) introduced the LD feature set, a document representation for LangID that is robust to variation in languages across different sources of text. The LD feature set can be thought of as language-indicative byte sequences, i.e. sequences of 1 to 4 bytes that have been selected to be strongly characteristic of a particular language or set of languages regardless of the text source. Lui and Baldwin (2012) present langid.py,1 an off-the-shelf LangID system that utilizes the LD feature set. In this work, we re-train langid.py using the training data provided by the shared task organizers, and use this as a baseline result representative of the state-of-the-art in LangID. 2.2 Hierarchical LangID In LangID research to date, systems generally do not take into account any form of structure in the class space. In this shared task, languages are explicitly grouped into 6 disjoint groups. We make use of this structure by introducing a two-level LangID model. The first level implements a single grouplev"
W14-5315,U13-1003,1,0.927895,"h accuracy (Cavnar and Trenkle, 1994; Dunning, 1994; Grefenstette, 1995; Prager, 1999; Teahan, 2000). However, recent work has shown that high accuracy is only achieved under ideal conditions (Baldwin and Lui, 2010), and one area that needs further work is accurate discrimination between closely-related languages (Ljubeˇsi´c et al., 2007; Tiedemann and Ljubeˇsi´c, 2012). The problem has been explored for specific groups of confusable languages, such as Malay/Indonesian (Ranaivo-Malancon, 2006), South-Eastern European languages (Tiedemann and Ljubeˇsi´c, 2012), as well as varieties of English (Lui and Cook, 2013), Portuguese (Zampieri and Gebre, 2012), and Spanish (Zampieri et al., 2013). The Discriminating Similar Language (DSL) shared task (Zampieri et al., 2014) was hosted at the VarDial workshop at COLING 2014, and brings together the work on these various language groups by proposing a task on a single dataset containing text from 13 languages in 6 groups, drawn from a variety of news text datasets (Tan et al., 2014). In this paper, we describe the entries made by team UniMelb NLP to the DSL shared task. We took part in both the closed and the open categories, submitting to the main component (Gr"
W14-5315,J03-1002,0,0.00581025,"Missing"
W14-5315,petrov-etal-2012-universal,0,0.0912549,"Missing"
W14-5315,skadins-etal-2014-billions,0,0.0262828,"Missing"
W14-5315,C12-1160,0,0.370094,"Missing"
W14-5315,tiedemann-2012-parallel,0,0.0311357,"of participation: (1) Closed, using only training data provided by the organizers (Tan et al., 2014); and (2) Open, using any training data available to participants. To participate in the latter category, we sourced additional training data through: (1) collection of data relevant to this task from existing text corpora; and (2) automatic construction of web corpora. The information about the additional training data is shown in Table 2. 2.4.1 Existing Corpora We collected training data from a number of existing corpora, as shown in Table 3. Many of the corpora that we used are part of OPUS (Tiedemann, 2012), which is a collection of sentence-aligned text corpora commonly used for research in machine translation. The exceptions are: (1) debian, which was constructed using translations of message strings from the Debian operating system,3 ; (2) BNC — the British National Corpus (Burnard, 2000); (3) OANC — the open component of the Second Release of the American National Corpus (Ide and Macleod, 2001), and (4) Reuters Corpus Volume 2 (RCV2);4 a corpus of news stories by local reporters in 13 languages. We sampled approximately 19000 sentences from each of the BNC and OANC, which we used as training"
W14-5315,N03-1033,0,0.120728,"Missing"
W14-5315,U09-1008,0,0.0384822,"Missing"
W14-5315,W14-5307,0,0.184466,"Missing"
W15-0909,W11-0815,0,0.0498963,"egrating compositionality scores for English noun compounds into the T ESLA machine translation evaluation metric. The attempt is marginally successful, and we speculate on whether a larger-scale attempt is likely to have greater impact. 1 Introduction While the explicit identification of multiword expressions (“MWEs”: Sag et al. (2002), Baldwin and Kim (2009)) has been shown to be useful in various NLP applications (Ramisch, 2012), recent work has shown that automatic prediction of the degree of compositionality of MWEs also has utility, in applications including information retrieval (“IR”: Acosta et al. (2011)) and machine translation (“MT”: Weller et al. (2014), Carpuat and Diab (2010) and Venkatapathy and Joshi (2006)). For instance, Acosta et al. (2011) showed that by considering non-compositional MWEs as a single unit, the effectiveness of document ranking in an IR system improves, and Carpuat and Diab (2010) showed that by adding compositionality scores to the Moses SMT system (Koehn et al., 2007), they could improve translation quality. This paper presents the first attempt to use MWE compositionality scores for the evaluation of MT system outputs. The basic intuition underlying this work is"
W15-0909,C10-3010,1,0.827638,"dataset, and the subset of the dataset that contains noun compounds SS+DS: the arithmetic mean of DS and string similarity (“SS”), based on the findings of Salehi et al. (2014). SS is calculated for each component using the LCS-based string similarity between the MWE and each of its components in the original language as well as a number of translations (Salehi and Cook, 2013), under the hypothesis that compositional MWEs are more likely to be word-forword translations in a given language than noncompositional MWEs. Following Salehi and Cook (2013), the translations were sourced from PanLex (Baldwin et al., 2010; Kamholz et al., 2014). In Salehi and Cook (2013), the best translation languages are selected based on the training data. Since, we focus on NCs in this paper, we use the translation languages reported in that paper to work best for English noun compounds, namely: Czech, Norwegian, Portuguese, Thai, French, Chinese, Dutch, Romanian, Hindi and Russian. 4 tions for five to-English language pairs (Bojar et al., 2013). As our judgements, we used: (1) the original pairwise preference judgements from WMT 2013 (i.e. which of translation A and B is better?); and (2) continuous-valued adequacy judgem"
W15-0909,W05-0909,0,0.680956,"ative compositionality of MWEs, focusing on compound nouns, and the T ESLA machine translation metric (Liu et al., 2010). 2 Related Work In this section, we overview previous work on MT evaluation and measuring the compositionality of MWEs. 2.1 Machine Translation Evaluation Automatic MT evaluation methods score MT system outputs based on similarity with reference translations provided by human translators. This scoring can be based on: (1) simple string similarity (Papineni et al., 2002; Snover et al., 2006); (2) shallow linguistic information such as lemmatisation, POS tagging and synonyms (Banerjee and Lavie, 2005; Liu et al., 2010); or (3) deeper linguistic information such as semantic roles (Gim´enez and M`arquez, 2008; Pad´o et al., 2009). In this research, we focus on the T ESLA MT eval54 Proceedings of NAACL-HLT 2015, pages 54–59, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics uation metric (Liu et al., 2010), which falls into the second group and uses a linear programming framework to automatically learn weights for matching n-grams of different types, making it easy to incorporate continuous-valued compositionality scores of MWEs. 2.2 Compositionality o"
W15-0909,N10-1029,0,0.0448741,"machine translation evaluation metric. The attempt is marginally successful, and we speculate on whether a larger-scale attempt is likely to have greater impact. 1 Introduction While the explicit identification of multiword expressions (“MWEs”: Sag et al. (2002), Baldwin and Kim (2009)) has been shown to be useful in various NLP applications (Ramisch, 2012), recent work has shown that automatic prediction of the degree of compositionality of MWEs also has utility, in applications including information retrieval (“IR”: Acosta et al. (2011)) and machine translation (“MT”: Weller et al. (2014), Carpuat and Diab (2010) and Venkatapathy and Joshi (2006)). For instance, Acosta et al. (2011) showed that by considering non-compositional MWEs as a single unit, the effectiveness of document ranking in an IR system improves, and Carpuat and Diab (2010) showed that by adding compositionality scores to the Moses SMT system (Koehn et al., 2007), they could improve translation quality. This paper presents the first attempt to use MWE compositionality scores for the evaluation of MT system outputs. The basic intuition underlying this work is that we should sensitise the relative reward associated with partial mismatche"
W15-0909,W08-0332,0,0.083093,"Missing"
W15-0909,D14-1020,1,0.828248,"oerover, the N→P sentences for DS are a subset of the N→P sentences for 0/1; the same is true for the P→N sentences. 6 Table 3: The number of judgements that were ranked correctly by T ESLA originally, but incorrectly with the incorporation of compositionality scores (“P→N”) and vice versa (“N→P”), and the absolute improvement with compositionality scores (“∆”) judgements, as shown in Table 2, again over the full dataset and also the subset of data containing compound nouns. The improvement here is slightly greater than for our first experiment, but not at a level of statistical significance (Graham and Baldwin, 2014). Perhaps surprisingly, the exact compositionality predictions produce a higher correlation than the continuous-valued compositionality predictions, but again, even with the inclusion of the compositionality features, T ESLA is outperformed by M ETEOR. The correlation over the subset of the data containing compound nouns is markedly higher than that over the full dataset, but the r values with the inclusion of compositionality values are actually all slightly below those for the basic T ESLA. As a final analysis, we examine the relative impact on T ESLA of the three compositionality methods, i"
W15-0909,E14-1047,1,0.848875,"ok (2013), the best translation languages are selected based on the training data. Since, we focus on NCs in this paper, we use the translation languages reported in that paper to work best for English noun compounds, namely: Czech, Norwegian, Portuguese, Thai, French, Chinese, Dutch, Romanian, Hindi and Russian. 4 tions for five to-English language pairs (Bojar et al., 2013). As our judgements, we used: (1) the original pairwise preference judgements from WMT 2013 (i.e. which of translation A and B is better?); and (2) continuous-valued adequacy judgements for each MT output, as collected by Graham et al. (2014). We used the Stanford CoreNLP parser (Klein and Manning, 2003) to identify English noun compounds in the translations. Among the 3000 sentences, 579 sentences contain at least one noun compound. Dataset We evaluate our method over the data from WMT 2013, which is made up of a total of 3000 transla56 Results We performed two evaluations, based on the two sets of judgements (pairwise preference or continuousvalued judgement for each MT output). In each case, we use three baselines (each applied at the segment level, meaning that individual sentences get a score): (1) M ETEOR (Banerjee and Lavie"
W15-0909,I11-1024,0,0.255284,"for Computational Linguistics uation metric (Liu et al., 2010), which falls into the second group and uses a linear programming framework to automatically learn weights for matching n-grams of different types, making it easy to incorporate continuous-valued compositionality scores of MWEs. 2.2 Compositionality of MWEs Earlier work on MWE compositionality (Bannard, 2006) approached the task via binary classification (compositional or non-compositional). However, there has recently been a shift towards regression analysis of the task, and prediction of a continuousvalued compositionality score (Reddy et al., 2011; Salehi and Cook, 2013; Salehi et al., 2014). This is the (primary) approach we take in this paper, as outlined in Section 3.2. 3 3.1 Methodology Using compositionality scores in T ESLA In this section, we introduce T ESLA and our method for integrating compositionality scores into the method. Firstly, T ESLA measures the similarity between the unigrams of the two given sentences (MT output and reference translation) based on the following three terms for each pairing of unigrams x and y: T ESLA uses an integer linear program to find the phrase alignment that maximizes the similarity scores o"
W15-0909,kamholz-etal-2014-panlex,0,0.0128974,"et of the dataset that contains noun compounds SS+DS: the arithmetic mean of DS and string similarity (“SS”), based on the findings of Salehi et al. (2014). SS is calculated for each component using the LCS-based string similarity between the MWE and each of its components in the original language as well as a number of translations (Salehi and Cook, 2013), under the hypothesis that compositional MWEs are more likely to be word-forword translations in a given language than noncompositional MWEs. Following Salehi and Cook (2013), the translations were sourced from PanLex (Baldwin et al., 2010; Kamholz et al., 2014). In Salehi and Cook (2013), the best translation languages are selected based on the training data. Since, we focus on NCs in this paper, we use the translation languages reported in that paper to work best for English noun compounds, namely: Czech, Norwegian, Portuguese, Thai, French, Chinese, Dutch, Romanian, Hindi and Russian. 4 tions for five to-English language pairs (Bojar et al., 2013). As our judgements, we used: (1) the original pairwise preference judgements from WMT 2013 (i.e. which of translation A and B is better?); and (2) continuous-valued adequacy judgements for each MT output"
W15-0909,P03-1040,0,0.0329895,"dgements that were ranked correctly originally, but incorrectly when the compositionality score was incorporated (“P→N”); and also the number of pairwise judgements that were ranked incorrectly originally, and corrected with the incorpo57 Discussion As shown in the previous section, the incorporation of compositionality scores can improve the quality of MT evaluation based on T ESLA. However, the improvements are very small and not statistically significant. Part of the reason is that we focus exclusively on noun compounds, which are contiguous and relatively easy to translate for MT systems (Koehn and Knight, 2003). Having said that, preliminary error analysis would suggest that most MT systems have difficulty translating non-compositional noun compounds, although then again, most noun compounds in the WMT 2013 shared task are highly compositional, limiting the impact of compositionality scores. We speculate that, for the method to have greater impact, we would need to target a larger set of MWEs, including non-contiguous MWEs such as split verb particle constructions (Kim and Baldwin, 2010). Further error analysis suggests that incorrect identification of noun compounds in a reference sentence can have"
W15-0909,P07-2045,0,0.00407743,"s NLP applications (Ramisch, 2012), recent work has shown that automatic prediction of the degree of compositionality of MWEs also has utility, in applications including information retrieval (“IR”: Acosta et al. (2011)) and machine translation (“MT”: Weller et al. (2014), Carpuat and Diab (2010) and Venkatapathy and Joshi (2006)). For instance, Acosta et al. (2011) showed that by considering non-compositional MWEs as a single unit, the effectiveness of document ranking in an IR system improves, and Carpuat and Diab (2010) showed that by adding compositionality scores to the Moses SMT system (Koehn et al., 2007), they could improve translation quality. This paper presents the first attempt to use MWE compositionality scores for the evaluation of MT system outputs. The basic intuition underlying this work is that we should sensitise the relative reward associated with partial mismatches between MT outputs and the reference translations, based on compositionality. For example, an MT output of white tower should not be rewarded for partial overlap with ivory tower in the reference translation, as tower here is most naturally interpreted compositionally in the MT output, but non-compositionally in the re"
W15-0909,W10-1754,0,0.0176187,"for partial overlap with ivory tower in the reference translation, as tower here is most naturally interpreted compositionally in the MT output, but non-compositionally in the reference translation. On the other hand, a partial mismatch between traffic signal and traffic light should be rewarded, as the usage of traffic is highly compositional in both cases. That is, we ask the question: can we better judge the quality of translations if we have some means of automatically estimating the relative compositionality of MWEs, focusing on compound nouns, and the T ESLA machine translation metric (Liu et al., 2010). 2 Related Work In this section, we overview previous work on MT evaluation and measuring the compositionality of MWEs. 2.1 Machine Translation Evaluation Automatic MT evaluation methods score MT system outputs based on similarity with reference translations provided by human translators. This scoring can be based on: (1) simple string similarity (Papineni et al., 2002; Snover et al., 2006); (2) shallow linguistic information such as lemmatisation, POS tagging and synonyms (Banerjee and Lavie, 2005; Liu et al., 2010); or (3) deeper linguistic information such as semantic roles (Gim´enez and M"
W15-0909,P09-1034,0,0.0605029,"Missing"
W15-0909,P02-1040,0,0.0974854,"s, we ask the question: can we better judge the quality of translations if we have some means of automatically estimating the relative compositionality of MWEs, focusing on compound nouns, and the T ESLA machine translation metric (Liu et al., 2010). 2 Related Work In this section, we overview previous work on MT evaluation and measuring the compositionality of MWEs. 2.1 Machine Translation Evaluation Automatic MT evaluation methods score MT system outputs based on similarity with reference translations provided by human translators. This scoring can be based on: (1) simple string similarity (Papineni et al., 2002; Snover et al., 2006); (2) shallow linguistic information such as lemmatisation, POS tagging and synonyms (Banerjee and Lavie, 2005; Liu et al., 2010); or (3) deeper linguistic information such as semantic roles (Gim´enez and M`arquez, 2008; Pad´o et al., 2009). In this research, we focus on the T ESLA MT eval54 Proceedings of NAACL-HLT 2015, pages 54–59, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics uation metric (Liu et al., 2010), which falls into the second group and uses a linear programming framework to automatically learn weights for matching"
W15-0909,W12-3311,0,0.0149819,"Abstract In this paper, we present the first attempt to integrate predicted compositionality scores of multiword expressions into automatic machine translation evaluation, in integrating compositionality scores for English noun compounds into the T ESLA machine translation evaluation metric. The attempt is marginally successful, and we speculate on whether a larger-scale attempt is likely to have greater impact. 1 Introduction While the explicit identification of multiword expressions (“MWEs”: Sag et al. (2002), Baldwin and Kim (2009)) has been shown to be useful in various NLP applications (Ramisch, 2012), recent work has shown that automatic prediction of the degree of compositionality of MWEs also has utility, in applications including information retrieval (“IR”: Acosta et al. (2011)) and machine translation (“MT”: Weller et al. (2014), Carpuat and Diab (2010) and Venkatapathy and Joshi (2006)). For instance, Acosta et al. (2011) showed that by considering non-compositional MWEs as a single unit, the effectiveness of document ranking in an IR system improves, and Carpuat and Diab (2010) showed that by adding compositionality scores to the Moses SMT system (Koehn et al., 2007), they could im"
W15-0909,S13-1039,1,0.903598,"nguistics uation metric (Liu et al., 2010), which falls into the second group and uses a linear programming framework to automatically learn weights for matching n-grams of different types, making it easy to incorporate continuous-valued compositionality scores of MWEs. 2.2 Compositionality of MWEs Earlier work on MWE compositionality (Bannard, 2006) approached the task via binary classification (compositional or non-compositional). However, there has recently been a shift towards regression analysis of the task, and prediction of a continuousvalued compositionality score (Reddy et al., 2011; Salehi and Cook, 2013; Salehi et al., 2014). This is the (primary) approach we take in this paper, as outlined in Section 3.2. 3 3.1 Methodology Using compositionality scores in T ESLA In this section, we introduce T ESLA and our method for integrating compositionality scores into the method. Firstly, T ESLA measures the similarity between the unigrams of the two given sentences (MT output and reference translation) based on the following three terms for each pairing of unigrams x and y: T ESLA uses an integer linear program to find the phrase alignment that maximizes the similarity scores over the three terms (Sm"
W15-0909,E14-1050,1,0.897614,"(Liu et al., 2010), which falls into the second group and uses a linear programming framework to automatically learn weights for matching n-grams of different types, making it easy to incorporate continuous-valued compositionality scores of MWEs. 2.2 Compositionality of MWEs Earlier work on MWE compositionality (Bannard, 2006) approached the task via binary classification (compositional or non-compositional). However, there has recently been a shift towards regression analysis of the task, and prediction of a continuousvalued compositionality score (Reddy et al., 2011; Salehi and Cook, 2013; Salehi et al., 2014). This is the (primary) approach we take in this paper, as outlined in Section 3.2. 3 3.1 Methodology Using compositionality scores in T ESLA In this section, we introduce T ESLA and our method for integrating compositionality scores into the method. Firstly, T ESLA measures the similarity between the unigrams of the two given sentences (MT output and reference translation) based on the following three terms for each pairing of unigrams x and y: T ESLA uses an integer linear program to find the phrase alignment that maximizes the similarity scores over the three terms (Sms , Slem and Spos ) fo"
W15-0909,2006.amta-papers.25,0,0.0599367,"can we better judge the quality of translations if we have some means of automatically estimating the relative compositionality of MWEs, focusing on compound nouns, and the T ESLA machine translation metric (Liu et al., 2010). 2 Related Work In this section, we overview previous work on MT evaluation and measuring the compositionality of MWEs. 2.1 Machine Translation Evaluation Automatic MT evaluation methods score MT system outputs based on similarity with reference translations provided by human translators. This scoring can be based on: (1) simple string similarity (Papineni et al., 2002; Snover et al., 2006); (2) shallow linguistic information such as lemmatisation, POS tagging and synonyms (Banerjee and Lavie, 2005; Liu et al., 2010); or (3) deeper linguistic information such as semantic roles (Gim´enez and M`arquez, 2008; Pad´o et al., 2009). In this research, we focus on the T ESLA MT eval54 Proceedings of NAACL-HLT 2015, pages 54–59, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics uation metric (Liu et al., 2010), which falls into the second group and uses a linear programming framework to automatically learn weights for matching n-grams of different"
W15-0909,W06-1204,0,0.120698,"ion metric. The attempt is marginally successful, and we speculate on whether a larger-scale attempt is likely to have greater impact. 1 Introduction While the explicit identification of multiword expressions (“MWEs”: Sag et al. (2002), Baldwin and Kim (2009)) has been shown to be useful in various NLP applications (Ramisch, 2012), recent work has shown that automatic prediction of the degree of compositionality of MWEs also has utility, in applications including information retrieval (“IR”: Acosta et al. (2011)) and machine translation (“MT”: Weller et al. (2014), Carpuat and Diab (2010) and Venkatapathy and Joshi (2006)). For instance, Acosta et al. (2011) showed that by considering non-compositional MWEs as a single unit, the effectiveness of document ranking in an IR system improves, and Carpuat and Diab (2010) showed that by adding compositionality scores to the Moses SMT system (Koehn et al., 2007), they could improve translation quality. This paper presents the first attempt to use MWE compositionality scores for the evaluation of MT system outputs. The basic intuition underlying this work is that we should sensitise the relative reward associated with partial mismatches between MT outputs and the refer"
W15-0909,W14-5709,0,0.151776,"Missing"
W15-0909,W13-2201,0,\N,Missing
W16-1817,E06-1042,0,0.141005,"are MWEs (Baldwin and Kim, 2010). Although there has been relatively less work on MWE identification than other type-level MWE prediction tasks, it is nevertheless important for NLP applications such as machine translation that must be able to distinguish MWEs from literal combinations in context. Some recent work has focused on token-level identification of a wide range of types of MWEs and other multiword units (e.g., Newman et al., 2012; Schneider et al., 2014; Brooke et al., 2014). Many studies, however, have taken a word sense disambiguation–inspired approach to MWE identification (e.g., Birke and Sarkar, 2006; Katz and Giesbrecht, 2006; Li et al., 2010), treating literal combinations and MWEs as different word senses, and have exploited linguistic knowledge of MWEs (e.g., Patrick and Fletcher, 2005; Uchiyama et al., 2005; Hashimoto and Kawahara, 2008; Fazly et al., 2009; Fothergill and Baldwin, 2012). In this study we focus on English verb–noun idiomatic combinations (VNICs). VNICs are formed from a verb with a noun in its direct object position. They are a common and productive type of English idiom, and occur cross-lingually (Fazly et al., 2009). VNICs tend to be relatively lexico-syntactically"
W16-1817,C14-1071,0,0.345702,"Brunswick Fredericton, NB E3B 5A3 Canada {waseem.gharbieh,bhavsar,paul.cook}@unb.ca Abstract which token instances in running text are MWEs (Baldwin and Kim, 2010). Although there has been relatively less work on MWE identification than other type-level MWE prediction tasks, it is nevertheless important for NLP applications such as machine translation that must be able to distinguish MWEs from literal combinations in context. Some recent work has focused on token-level identification of a wide range of types of MWEs and other multiword units (e.g., Newman et al., 2012; Schneider et al., 2014; Brooke et al., 2014). Many studies, however, have taken a word sense disambiguation–inspired approach to MWE identification (e.g., Birke and Sarkar, 2006; Katz and Giesbrecht, 2006; Li et al., 2010), treating literal combinations and MWEs as different word senses, and have exploited linguistic knowledge of MWEs (e.g., Patrick and Fletcher, 2005; Uchiyama et al., 2005; Hashimoto and Kawahara, 2008; Fazly et al., 2009; Fothergill and Baldwin, 2012). In this study we focus on English verb–noun idiomatic combinations (VNICs). VNICs are formed from a verb with a noun in its direct object position. They are a common an"
W16-1817,J90-1003,0,0.518798,"object position. Usages of these expressions can be ambiguous between an idiomatic usage and a literal combination. In this paper we propose supervised and unsupervised approaches, based on word embeddings, to identifying token instances of VNICs. Our proposed supervised and unsupervised approaches perform better than the supervised and unsupervised approaches of Fazly et al. (2009), respectively. 1 Verb–noun Idiomatic Combinations Much research on multiword expressions (MWEs) in natural language processing (NLP) has focused on various type-level prediction tasks, e.g., MWE extraction (e.g., Church and Hanks, 1990; Smadja, 1993; Lin, 1999) — i.e., determining which MWE types are present in a given corpus (Baldwin and Kim, 2010) — and compositionality prediction (e.g., McCarthy et al., 2003; Reddy et al., 2011; Salehi et al., 2014). However, word combinations can be ambiguous between literal combinations and MWEs. For example, consider the following two usages of the expression hit the roof : 1. I think Paula might hit the roof if you start ironing. 2. When the blood hit the roof of the car I realised it was serious. The first example of hit the roof is an idiomatic usage, while the second is a literal"
W16-1817,P15-1026,0,0.0179058,"ns in a corpus.2 They then classify a given token instance of a VNIC as idiomatic if it occurs in its canonical form, and as literal otherwise. Fazly et al. also consider a supervised approach that classifies a given VNIC instance based on the similarity of its context to that of idiomatic and literal instances of the same expression seen during training. Distributed representations of word meaning in the form of word embeddings (Mikolov et al., 2013) have recently been demonstrated to benefit a wide range of NLP tasks including POS tagging (e.g., Ling et al., 2015), question answering (e.g., Dong et al., 2015), and machine translation (e.g., Zou et al., 2013). Moreover, word embeddings have been shown to improve over count-based models of distributional similarity for predicting MWE compositionality (Salehi et al., 2015). In this work we first propose a supervised approach to identifying VNIC token instances based on word embeddings that outperforms the supervised method of Fazly et al. (2009). We then propose an unsupervised approach to this task, that combines word embeddings with Fazly et al.’s unsupervised CF ORM approach, that improves over CF ORM. Original text: You can see the stars, now, in"
W16-1817,J09-1005,1,0.178295,"ral combinations in context. Some recent work has focused on token-level identification of a wide range of types of MWEs and other multiword units (e.g., Newman et al., 2012; Schneider et al., 2014; Brooke et al., 2014). Many studies, however, have taken a word sense disambiguation–inspired approach to MWE identification (e.g., Birke and Sarkar, 2006; Katz and Giesbrecht, 2006; Li et al., 2010), treating literal combinations and MWEs as different word senses, and have exploited linguistic knowledge of MWEs (e.g., Patrick and Fletcher, 2005; Uchiyama et al., 2005; Hashimoto and Kawahara, 2008; Fazly et al., 2009; Fothergill and Baldwin, 2012). In this study we focus on English verb–noun idiomatic combinations (VNICs). VNICs are formed from a verb with a noun in its direct object position. They are a common and productive type of English idiom, and occur cross-lingually (Fazly et al., 2009). VNICs tend to be relatively lexico-syntactically fixed, e.g., whereas hit the roof is ambiguous between literal and idiomatic meanings, hit the roofs and a roof was hit are most likely to be literal usages. Fazly et al. (2009) exploit this property in their unsupervised approach, referred to as CF ORM. They define"
W16-1817,W03-1810,0,0.0738867,", based on word embeddings, to identifying token instances of VNICs. Our proposed supervised and unsupervised approaches perform better than the supervised and unsupervised approaches of Fazly et al. (2009), respectively. 1 Verb–noun Idiomatic Combinations Much research on multiword expressions (MWEs) in natural language processing (NLP) has focused on various type-level prediction tasks, e.g., MWE extraction (e.g., Church and Hanks, 1990; Smadja, 1993; Lin, 1999) — i.e., determining which MWE types are present in a given corpus (Baldwin and Kim, 2010) — and compositionality prediction (e.g., McCarthy et al., 2003; Reddy et al., 2011; Salehi et al., 2014). However, word combinations can be ambiguous between literal combinations and MWEs. For example, consider the following two usages of the expression hit the roof : 1. I think Paula might hit the roof if you start ironing. 2. When the blood hit the roof of the car I realised it was serious. The first example of hit the roof is an idiomatic usage, while the second is a literal combination.1 MWE identification is the task of determining 1 These examples, and idiomaticity judgements, are taken from Cook et al. (2008). 112 Proceedings of the 12th Workshop"
W16-1817,D08-1104,0,0.222086,"Missing"
W16-1817,J07-4005,0,0.0740875,"Missing"
W16-1817,W06-1203,0,0.181068,"m, 2010). Although there has been relatively less work on MWE identification than other type-level MWE prediction tasks, it is nevertheless important for NLP applications such as machine translation that must be able to distinguish MWEs from literal combinations in context. Some recent work has focused on token-level identification of a wide range of types of MWEs and other multiword units (e.g., Newman et al., 2012; Schneider et al., 2014; Brooke et al., 2014). Many studies, however, have taken a word sense disambiguation–inspired approach to MWE identification (e.g., Birke and Sarkar, 2006; Katz and Giesbrecht, 2006; Li et al., 2010), treating literal combinations and MWEs as different word senses, and have exploited linguistic knowledge of MWEs (e.g., Patrick and Fletcher, 2005; Uchiyama et al., 2005; Hashimoto and Kawahara, 2008; Fazly et al., 2009; Fothergill and Baldwin, 2012). In this study we focus on English verb–noun idiomatic combinations (VNICs). VNICs are formed from a verb with a noun in its direct object position. They are a common and productive type of English idiom, and occur cross-lingually (Fazly et al., 2009). VNICs tend to be relatively lexico-syntactically fixed, e.g., whereas hit th"
W16-1817,P14-1025,1,0.912693,"Missing"
W16-1817,C12-1127,0,0.0266377,"culty of Computer Science, University of New Brunswick Fredericton, NB E3B 5A3 Canada {waseem.gharbieh,bhavsar,paul.cook}@unb.ca Abstract which token instances in running text are MWEs (Baldwin and Kim, 2010). Although there has been relatively less work on MWE identification than other type-level MWE prediction tasks, it is nevertheless important for NLP applications such as machine translation that must be able to distinguish MWEs from literal combinations in context. Some recent work has focused on token-level identification of a wide range of types of MWEs and other multiword units (e.g., Newman et al., 2012; Schneider et al., 2014; Brooke et al., 2014). Many studies, however, have taken a word sense disambiguation–inspired approach to MWE identification (e.g., Birke and Sarkar, 2006; Katz and Giesbrecht, 2006; Li et al., 2010), treating literal combinations and MWEs as different word senses, and have exploited linguistic knowledge of MWEs (e.g., Patrick and Fletcher, 2005; Uchiyama et al., 2005; Hashimoto and Kawahara, 2008; Fazly et al., 2009; Fothergill and Baldwin, 2012). In this study we focus on English verb–noun idiomatic combinations (VNICs). VNICs are formed from a verb with a noun in it"
W16-1817,P10-1116,0,0.0716022,"s been relatively less work on MWE identification than other type-level MWE prediction tasks, it is nevertheless important for NLP applications such as machine translation that must be able to distinguish MWEs from literal combinations in context. Some recent work has focused on token-level identification of a wide range of types of MWEs and other multiword units (e.g., Newman et al., 2012; Schneider et al., 2014; Brooke et al., 2014). Many studies, however, have taken a word sense disambiguation–inspired approach to MWE identification (e.g., Birke and Sarkar, 2006; Katz and Giesbrecht, 2006; Li et al., 2010), treating literal combinations and MWEs as different word senses, and have exploited linguistic knowledge of MWEs (e.g., Patrick and Fletcher, 2005; Uchiyama et al., 2005; Hashimoto and Kawahara, 2008; Fazly et al., 2009; Fothergill and Baldwin, 2012). In this study we focus on English verb–noun idiomatic combinations (VNICs). VNICs are formed from a verb with a noun in its direct object position. They are a common and productive type of English idiom, and occur cross-lingually (Fazly et al., 2009). VNICs tend to be relatively lexico-syntactically fixed, e.g., whereas hit the roof is ambiguou"
W16-1817,P99-1041,0,0.196364,"essions can be ambiguous between an idiomatic usage and a literal combination. In this paper we propose supervised and unsupervised approaches, based on word embeddings, to identifying token instances of VNICs. Our proposed supervised and unsupervised approaches perform better than the supervised and unsupervised approaches of Fazly et al. (2009), respectively. 1 Verb–noun Idiomatic Combinations Much research on multiword expressions (MWEs) in natural language processing (NLP) has focused on various type-level prediction tasks, e.g., MWE extraction (e.g., Church and Hanks, 1990; Smadja, 1993; Lin, 1999) — i.e., determining which MWE types are present in a given corpus (Baldwin and Kim, 2010) — and compositionality prediction (e.g., McCarthy et al., 2003; Reddy et al., 2011; Salehi et al., 2014). However, word combinations can be ambiguous between literal combinations and MWEs. For example, consider the following two usages of the expression hit the roof : 1. I think Paula might hit the roof if you start ironing. 2. When the blood hit the roof of the car I realised it was serious. The first example of hit the roof is an idiomatic usage, while the second is a literal combination.1 MWE identifi"
W16-1817,D15-1161,0,0.0207646,"sociation for Computational Linguistics patterns in a corpus.2 They then classify a given token instance of a VNIC as idiomatic if it occurs in its canonical form, and as literal otherwise. Fazly et al. also consider a supervised approach that classifies a given VNIC instance based on the similarity of its context to that of idiomatic and literal instances of the same expression seen during training. Distributed representations of word meaning in the form of word embeddings (Mikolov et al., 2013) have recently been demonstrated to benefit a wide range of NLP tasks including POS tagging (e.g., Ling et al., 2015), question answering (e.g., Dong et al., 2015), and machine translation (e.g., Zou et al., 2013). Moreover, word embeddings have been shown to improve over count-based models of distributional similarity for predicting MWE compositionality (Salehi et al., 2015). In this work we first propose a supervised approach to identifying VNIC token instances based on word embeddings that outperforms the supervised method of Fazly et al. (2009). We then propose an unsupervised approach to this task, that combines word embeddings with Fazly et al.’s unsupervised CF ORM approach, that improves over CF ORM."
W16-1817,I11-1024,0,0.0388431,"ngs, to identifying token instances of VNICs. Our proposed supervised and unsupervised approaches perform better than the supervised and unsupervised approaches of Fazly et al. (2009), respectively. 1 Verb–noun Idiomatic Combinations Much research on multiword expressions (MWEs) in natural language processing (NLP) has focused on various type-level prediction tasks, e.g., MWE extraction (e.g., Church and Hanks, 1990; Smadja, 1993; Lin, 1999) — i.e., determining which MWE types are present in a given corpus (Baldwin and Kim, 2010) — and compositionality prediction (e.g., McCarthy et al., 2003; Reddy et al., 2011; Salehi et al., 2014). However, word combinations can be ambiguous between literal combinations and MWEs. For example, consider the following two usages of the expression hit the roof : 1. I think Paula might hit the roof if you start ironing. 2. When the blood hit the roof of the car I realised it was serious. The first example of hit the roof is an idiomatic usage, while the second is a literal combination.1 MWE identification is the task of determining 1 These examples, and idiomaticity judgements, are taken from Cook et al. (2008). 112 Proceedings of the 12th Workshop on Multiword Express"
W16-1817,P14-5010,0,0.0035072,"es. Fazly et al. (2009) focus primarily on DEV and TEST; we therefore only consider these subsets here. DEV and TEST consist of a total of 597 and 613 VNIC tokens, respectively, that are annotated as either literal or idiomatic usages.7 Word embeddings The word embeddings required by our proposed methods were trained using the gensim5 implementation of the skip gram version of word2vec (Mikolov et al., 2013). The model was trained on a snapshot of English Wikipedia from 1 September 2015. The text was pre-processed using wp2txt6 to remove markup, and then tokenized with the Stanford tokenizer (Manning et al., 2014). Tokens occurring less than 15 times were removed, and the negative sampling parameter was set to 5. 3.2 Dev 87.3 88.2 86.3 86.4 86.7 86.5 86.0 85.9 87.3 85.5 85.6 85.8 62.1 72.3 80.1 Table 1: Percent accuracy using a linear SVM for different word2vec parameters. Results for a most-frequent class baseline, and the CF ORM and supervised methods from Fazly et al. (2009), are also shown. 3 Materials and Methods 3.1 Dimensions 50 100 300 50 100 300 50 100 300 50 100 300 4 Experimental Results In the following subsections we describe the results of experiments using our supervised approach, the ab"
W16-1817,N15-1099,1,0.832791,"ven VNIC instance based on the similarity of its context to that of idiomatic and literal instances of the same expression seen during training. Distributed representations of word meaning in the form of word embeddings (Mikolov et al., 2013) have recently been demonstrated to benefit a wide range of NLP tasks including POS tagging (e.g., Ling et al., 2015), question answering (e.g., Dong et al., 2015), and machine translation (e.g., Zou et al., 2013). Moreover, word embeddings have been shown to improve over count-based models of distributional similarity for predicting MWE compositionality (Salehi et al., 2015). In this work we first propose a supervised approach to identifying VNIC token instances based on word embeddings that outperforms the supervised method of Fazly et al. (2009). We then propose an unsupervised approach to this task, that combines word embeddings with Fazly et al.’s unsupervised CF ORM approach, that improves over CF ORM. Original text: You can see the stars, now, in the city Context tokens for verb (see): you, can, the, now Context tokens for noun (stars): can, the, now, in ~cverb = vec(you)+vec(can)+vec(the)+vec(now) 4 ~cnoun = vec(can)+vec(the)+vec(now)+vec(in) 4 ~c = Figure"
W16-1817,Q14-1016,0,0.0534245,"ence, University of New Brunswick Fredericton, NB E3B 5A3 Canada {waseem.gharbieh,bhavsar,paul.cook}@unb.ca Abstract which token instances in running text are MWEs (Baldwin and Kim, 2010). Although there has been relatively less work on MWE identification than other type-level MWE prediction tasks, it is nevertheless important for NLP applications such as machine translation that must be able to distinguish MWEs from literal combinations in context. Some recent work has focused on token-level identification of a wide range of types of MWEs and other multiword units (e.g., Newman et al., 2012; Schneider et al., 2014; Brooke et al., 2014). Many studies, however, have taken a word sense disambiguation–inspired approach to MWE identification (e.g., Birke and Sarkar, 2006; Katz and Giesbrecht, 2006; Li et al., 2010), treating literal combinations and MWEs as different word senses, and have exploited linguistic knowledge of MWEs (e.g., Patrick and Fletcher, 2005; Uchiyama et al., 2005; Hashimoto and Kawahara, 2008; Fazly et al., 2009; Fothergill and Baldwin, 2012). In this study we focus on English verb–noun idiomatic combinations (VNICs). VNICs are formed from a verb with a noun in its direct object position"
W16-1817,J93-1007,0,0.544942,"of these expressions can be ambiguous between an idiomatic usage and a literal combination. In this paper we propose supervised and unsupervised approaches, based on word embeddings, to identifying token instances of VNICs. Our proposed supervised and unsupervised approaches perform better than the supervised and unsupervised approaches of Fazly et al. (2009), respectively. 1 Verb–noun Idiomatic Combinations Much research on multiword expressions (MWEs) in natural language processing (NLP) has focused on various type-level prediction tasks, e.g., MWE extraction (e.g., Church and Hanks, 1990; Smadja, 1993; Lin, 1999) — i.e., determining which MWE types are present in a given corpus (Baldwin and Kim, 2010) — and compositionality prediction (e.g., McCarthy et al., 2003; Reddy et al., 2011; Salehi et al., 2014). However, word combinations can be ambiguous between literal combinations and MWEs. For example, consider the following two usages of the expression hit the roof : 1. I think Paula might hit the roof if you start ironing. 2. When the blood hit the roof of the car I realised it was serious. The first example of hit the roof is an idiomatic usage, while the second is a literal combination.1"
W16-1817,D13-1141,0,0.0252586,"nstance of a VNIC as idiomatic if it occurs in its canonical form, and as literal otherwise. Fazly et al. also consider a supervised approach that classifies a given VNIC instance based on the similarity of its context to that of idiomatic and literal instances of the same expression seen during training. Distributed representations of word meaning in the form of word embeddings (Mikolov et al., 2013) have recently been demonstrated to benefit a wide range of NLP tasks including POS tagging (e.g., Ling et al., 2015), question answering (e.g., Dong et al., 2015), and machine translation (e.g., Zou et al., 2013). Moreover, word embeddings have been shown to improve over count-based models of distributional similarity for predicting MWE compositionality (Salehi et al., 2015). In this work we first propose a supervised approach to identifying VNIC token instances based on word embeddings that outperforms the supervised method of Fazly et al. (2009). We then propose an unsupervised approach to this task, that combines word embeddings with Fazly et al.’s unsupervised CF ORM approach, that improves over CF ORM. Original text: You can see the stars, now, in the city Context tokens for verb (see): you, can,"
W17-1906,P09-1002,0,0.303669,"-purpose sense inventories. This issue of coverage is particularly relevant for social media text, which contains a higher rate of outof-vocabulary words than more-conventional text types (Baldwin et al., 2013). These issues pose problems for natural language processing tasks such as word sense disambiguation and induction, which rely on, and seek to induce, respectively, sense inventories, and have traditionally assumed that each instance of a word can be assigned one sense.1 In response to this, alternative approaches to word meaning have been proposed that do not rely on sense inventories. Erk et al. (2009) carried out an annotation task on “usage similarity” (USim), in which the similarity of the meanings of two usages of a given word are rated on a five-point scale. Lui et al. (2012) proposed the first computational approach to USim. They considered approaches based on topic modelling (Blei et al., 2003), under a wide range of parameter settings, and found that a single topic model for all target lemmas (as opposed to one topic model per target lemma) performed best on the dataset of Erk et al. (2009). Gella et al. (2013) considered USim on Twitter text, noting that this model of word meaning"
W17-1906,I13-1041,1,0.817074,"ng and Paul Cook Faculty of Computer Science, University of New Brunswick Fredericton, NB E3B 5A3, Canada milton.king@unb.ca, paul.cook@unb.ca Abstract to doubt the existence of word senses (Kilgarriff, 1997). Sense inventories also suffer from a lack of coverage. New words regularly come into usage, as do new senses for established words. Furthermore, domain-specific senses are often not included in general-purpose sense inventories. This issue of coverage is particularly relevant for social media text, which contains a higher rate of outof-vocabulary words than more-conventional text types (Baldwin et al., 2013). These issues pose problems for natural language processing tasks such as word sense disambiguation and induction, which rely on, and seek to induce, respectively, sense inventories, and have traditionally assumed that each instance of a word can be assigned one sense.1 In response to this, alternative approaches to word meaning have been proposed that do not rely on sense inventories. Erk et al. (2009) carried out an annotation task on “usage similarity” (USim), in which the similarity of the meanings of two usages of a given word are rated on a five-point scale. Lui et al. (2012) proposed t"
W17-1906,S13-1036,1,0.74254,"to word meaning have been proposed that do not rely on sense inventories. Erk et al. (2009) carried out an annotation task on “usage similarity” (USim), in which the similarity of the meanings of two usages of a given word are rated on a five-point scale. Lui et al. (2012) proposed the first computational approach to USim. They considered approaches based on topic modelling (Blei et al., 2003), under a wide range of parameter settings, and found that a single topic model for all target lemmas (as opposed to one topic model per target lemma) performed best on the dataset of Erk et al. (2009). Gella et al. (2013) considered USim on Twitter text, noting that this model of word meaning seems particularly well-suited to this text type because of the prevalence of out-ofvocabulary words. Gella et al. (2013) also considered topic modelling-based approaches, achieving their best results using one topic model per Usage similarity (USim) is an approach to determining word meaning in context that does not rely on a sense inventory. Instead, pairs of usages of a target lemma are rated on a scale. In this paper we propose unsupervised approaches to USim based on embeddings for words, contexts, and sentences, and"
W17-1906,P12-1092,0,0.149465,"Missing"
W17-1906,S13-2049,0,0.0231509,"approaches to USim based on embeddings for words, contexts, and sentences, and achieve state-of-the-art results over two USim datasets. We further consider supervised approaches to USim, and find that although they outperform unsupervised approaches, they are unable to generalize to lemmas that are unseen in the training data. 1 Usage similarity Word senses are not discrete. In many cases, for a given instance of a word, multiple senses from a sense inventory are applicable, and to varying degrees (Erk et al., 2009). For example, consider the usage of wait in the following sentence taken from Jurgens and Klapaftis (2013): 1. And is now the time to say I can hardly wait for your impending new novel about the Alamo? Annotators judged the WordNet (Fellbaum, 1998) senses glossed as ‘stay in one place and anticipate or expect something’ and ‘look forward to the probable occurrence of’, to have applicability ratings of 4 out of 5, and 2 out of 5, respectively, for this usage of wait. Moreover, Erk et al. (2009) also showed that this issue cannot be addressed simply by choosing a coarser-grained sense inventory. That a clear line cannot be drawn between the various senses of a word has been observed as far back as J"
W17-1906,P14-2050,0,0.0426388,"iven fold consists of SPairs for one lemma, and the training data consists of SPairs for all other lemmas. 3.3 W 2 5 8 2 5 8 2 5 8 4 Experimental results We first consider the unsupervised approach using word2vec for a variety of window sizes and number of dimensions. Results are shown in Table 1. All correlations are significant (p &lt; 0.05). On both O RIGINAL and T WITTER, for a given number of dimensions, as the window size is increased, ρ increases. Embeddings for larger window sizes tend to better capture semantics, whereas embeddings for smaller window sizes tend to better reflect syntax (Levy and Goldberg, 2014); the Embeddings We train word2vec’s skipgram model on two corpora:3 (1) a corpus of English tweets collected from the Twitter Streaming APIs4 from November 2014 to March 2015 containing 1.3 billion tokens; and (2) an English Wikipedia dump from 1 September 2015 containing 2.6 billion tokens. Because of the relatively-low cost of training word2vec, we consider several settings of 5 http://nlp.stanford.edu/projects/ glove/ 6 https://github.com/orenmel/ context2vec 7 https://github.com/ryankiros/ skip-thoughts 3 In preliminary experiments the alternative word2vec CBOW model achieved substantiall"
W17-1906,U12-1006,0,0.402646,"types (Baldwin et al., 2013). These issues pose problems for natural language processing tasks such as word sense disambiguation and induction, which rely on, and seek to induce, respectively, sense inventories, and have traditionally assumed that each instance of a word can be assigned one sense.1 In response to this, alternative approaches to word meaning have been proposed that do not rely on sense inventories. Erk et al. (2009) carried out an annotation task on “usage similarity” (USim), in which the similarity of the meanings of two usages of a given word are rated on a five-point scale. Lui et al. (2012) proposed the first computational approach to USim. They considered approaches based on topic modelling (Blei et al., 2003), under a wide range of parameter settings, and found that a single topic model for all target lemmas (as opposed to one topic model per target lemma) performed best on the dataset of Erk et al. (2009). Gella et al. (2013) considered USim on Twitter text, noting that this model of word meaning seems particularly well-suited to this text type because of the prevalence of out-ofvocabulary words. Gella et al. (2013) also considered topic modelling-based approaches, achieving"
W17-1906,S07-1009,0,0.0500405,"ntence encoder that can be viewed as a sentencelevel version of word2vec’s skipgram model, i.e., 3 Materials and methods 3.1 USim Datasets We evaluate our methods on two USim datasets representing two different text types: O RIGINAL, the USim dataset of Erk et al. (2009), and T WITTER from Gella et al. (2013). Both USim datasets contain pairs of sentences; each sentence in each pair includes a usage of a particular target lemma. Each sentence pair is rated on a scale of 1–5 for how similar in meaning the usages of the target words are in the two sentences. O RIGINAL consists of sentences from McCarthy and Navigli (2007), which were drawn from a web corpus (Sharoff, 2006). This dataset contains 34 lemmas, including nouns, verbs, adjectives, and adverbs. Each lemma is the target 2 Inference requires only a single sentence, so the model can infer skip-thought vectors for sentences taken out-ofcontext, as in the USim datasets. 48 D 50 50 50 100 100 100 300 300 300 word in 10 sentences. For each lemma, sentence pairs (SPairs) are formed based on all pairwise comparisons, giving 45 SPairs per lemma. Annotations were provided by three native English speakers, with the average taken as the final gold standard simila"
W17-1906,K16-1006,0,0.0891072,"asuring sentence-level relatedness. Although our goal is to determine the meaning of a word in context, the meaning of a sentence could be a useful proxy for this.2 target word, and a document expansion strategy based on medium frequency hashtags to combat the data sparsity of tweets due to their relatively short length. The methods of Lui et al. (2012) and Gella et al. (2013) are unsupervised; they do not rely on any gold standard USim annotations. In this paper we propose unsupervised approaches to USim based on embeddings for words (Mikolov et al., 2013; Pennington et al., 2014), contexts (Melamud et al., 2016), and sentences (Kiros et al., 2015), and achieve state-of-the-art results over the USim datasets of both Erk et al. (2009) and Gella et al. (2013). We then consider supervised approaches to USim based on these same methods for forming embeddings, which outperform the unsupervised approaches, but perform poorly on lemmas that are unseen in the training data. 2.2 In the unsupervised setup, we measure the similarity between two usages of a target word as the cosine similarity between their vector representations, obtained by one of the methods described in Section 2.1. This method does not requi"
W17-1906,D14-1113,0,0.0705644,"Missing"
W17-1906,D14-1162,0,0.0763677,"t-performs previous approaches to measuring sentence-level relatedness. Although our goal is to determine the meaning of a word in context, the meaning of a sentence could be a useful proxy for this.2 target word, and a document expansion strategy based on medium frequency hashtags to combat the data sparsity of tweets due to their relatively short length. The methods of Lui et al. (2012) and Gella et al. (2013) are unsupervised; they do not rely on any gold standard USim annotations. In this paper we propose unsupervised approaches to USim based on embeddings for words (Mikolov et al., 2013; Pennington et al., 2014), contexts (Melamud et al., 2016), and sentences (Kiros et al., 2015), and achieve state-of-the-art results over the USim datasets of both Erk et al. (2009) and Gella et al. (2013). We then consider supervised approaches to USim based on these same methods for forming embeddings, which outperform the unsupervised approaches, but perform poorly on lemmas that are unseen in the training data. 2.2 In the unsupervised setup, we measure the similarity between two usages of a target word as the cosine similarity between their vector representations, obtained by one of the methods described in Sectio"
W18-4920,W03-1812,0,0.0601263,"s is important for natural language processing (NLP) tasks such as parsing (Korkontzelos and Manandhar, 2010) and machine translation (Carpuat and Diab, 2010). In the case of translation, compositionality is particularly important because a word-for-word translation would typically be incorrect for a non-compositional expression. Much research has therefore focused on compositionality prediction of MWEs, primarily at the type level. One common approach to measuring compositionality is to compare distributional representations of an MWE and its component words (e.g., Schone and Jurafsky, 2001; Baldwin et al., 2003; Katz and Giesbrecht, 2006; Reddy et al., 2011; Schulte im Walde et al., 2013; Salehi et al., 2015). The hypothesis behind this line of work is that the representation of a compositional MWE will be more similar to the representations of its component words than the representation of a non-compositional MWE will be to those of its component words. One issue faced by such approaches is that token-level instances of MWEs must be identified in a corpus in order to form distributional representations of them. Token-level MWE identification has been studied for specific types of MWEs such as verb-"
W18-4920,D07-1090,0,0.0198675,"ense. creativecommons.org/licenses/by/4.0/. 1 These expressions and compositionality judgements are taken from Reddy et al. (2011). License details: http:// 185 Proceedings of the Joint Workshop on Linguistic Annotation, Multiword Expressions and Constructions (LAW-MWE-CxG-2018), pages 185–192 Santa Fe, New Mexico, USA, August 25-26, 2018. verb–noun idioms (e.g., Salton et al., 2016). Broad coverage MWE identification has also been studied, and remains a challenge (Schneider et al., 2014; Gharbieh et al., 2017). Language models are common throughout NLP in tasks including machine translation (Brants et al., 2007), speech recognition (Collins et al., 2005), and question answering (Chen et al., 2006). Although word-level language models are widely used, and their performance can be higher than character-level language models, character-level models have the advantage that they can model out-of-vocabulary words (Mikolov et al., 2012). Owing to this advantage, character-level language models have been applied in a range of NLP tasks, including authorship attribution, (Peng et al., 2003), part-of-speech tagging (Santos and Zadrozny, 2014), case restoration (Susanto et al., 2016), and stock price prediction"
W18-4920,N10-1029,0,0.0371341,"uch as rush hour and fine line falling somewhere in between as semi-compositional.1 Compositionality can also be viewed with respect to an individual component word of an MWE, where an MWE component word is compositional if its meaning is reflected in the meaning of the expression. For example, in spelling bee and grandfather clock, the first and second component words, respectively, are compositional, while the others are not. Knowledge of multiword expressions is important for natural language processing (NLP) tasks such as parsing (Korkontzelos and Manandhar, 2010) and machine translation (Carpuat and Diab, 2010). In the case of translation, compositionality is particularly important because a word-for-word translation would typically be incorrect for a non-compositional expression. Much research has therefore focused on compositionality prediction of MWEs, primarily at the type level. One common approach to measuring compositionality is to compare distributional representations of an MWE and its component words (e.g., Schone and Jurafsky, 2001; Baldwin et al., 2003; Katz and Giesbrecht, 2006; Reddy et al., 2011; Schulte im Walde et al., 2013; Salehi et al., 2015). The hypothesis behind this line of w"
W18-4920,P06-1136,0,0.0566401,"ements are taken from Reddy et al. (2011). License details: http:// 185 Proceedings of the Joint Workshop on Linguistic Annotation, Multiword Expressions and Constructions (LAW-MWE-CxG-2018), pages 185–192 Santa Fe, New Mexico, USA, August 25-26, 2018. verb–noun idioms (e.g., Salton et al., 2016). Broad coverage MWE identification has also been studied, and remains a challenge (Schneider et al., 2014; Gharbieh et al., 2017). Language models are common throughout NLP in tasks including machine translation (Brants et al., 2007), speech recognition (Collins et al., 2005), and question answering (Chen et al., 2006). Although word-level language models are widely used, and their performance can be higher than character-level language models, character-level models have the advantage that they can model out-of-vocabulary words (Mikolov et al., 2012). Owing to this advantage, character-level language models have been applied in a range of NLP tasks, including authorship attribution, (Peng et al., 2003), part-of-speech tagging (Santos and Zadrozny, 2014), case restoration (Susanto et al., 2016), and stock price prediction (dos Santos Pinheiro and Dras, 2017). Moreover, character-level information can be com"
W18-4920,P05-1063,0,0.0265239,". 1 These expressions and compositionality judgements are taken from Reddy et al. (2011). License details: http:// 185 Proceedings of the Joint Workshop on Linguistic Annotation, Multiword Expressions and Constructions (LAW-MWE-CxG-2018), pages 185–192 Santa Fe, New Mexico, USA, August 25-26, 2018. verb–noun idioms (e.g., Salton et al., 2016). Broad coverage MWE identification has also been studied, and remains a challenge (Schneider et al., 2014; Gharbieh et al., 2017). Language models are common throughout NLP in tasks including machine translation (Brants et al., 2007), speech recognition (Collins et al., 2005), and question answering (Chen et al., 2006). Although word-level language models are widely used, and their performance can be higher than character-level language models, character-level models have the advantage that they can model out-of-vocabulary words (Mikolov et al., 2012). Owing to this advantage, character-level language models have been applied in a range of NLP tasks, including authorship attribution, (Peng et al., 2003), part-of-speech tagging (Santos and Zadrozny, 2014), case restoration (Susanto et al., 2016), and stock price prediction (dos Santos Pinheiro and Dras, 2017). More"
W18-4920,U17-1001,0,0.027747,"gnition (Collins et al., 2005), and question answering (Chen et al., 2006). Although word-level language models are widely used, and their performance can be higher than character-level language models, character-level models have the advantage that they can model out-of-vocabulary words (Mikolov et al., 2012). Owing to this advantage, character-level language models have been applied in a range of NLP tasks, including authorship attribution, (Peng et al., 2003), part-of-speech tagging (Santos and Zadrozny, 2014), case restoration (Susanto et al., 2016), and stock price prediction (dos Santos Pinheiro and Dras, 2017). Moreover, character-level information can be composed to form representations of words (Ling et al., 2015). In this paper we consider whether character-level neural network language models capture knowledge of MWE compositionality. We train character-level language models based on recurrent neural networks — including long short-term memory (LSTM, Hochreiter and Schmidhuber, 1997) and gated recurrent unit (GRU, Cho et al., 2014). We then use these language models to form continuous vector representations of MWEs and their component words. Following prior work, we then use these representatio"
W18-4920,S17-1006,1,0.765078,"im and Baldwin, 2010) and This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/. 1 These expressions and compositionality judgements are taken from Reddy et al. (2011). License details: http:// 185 Proceedings of the Joint Workshop on Linguistic Annotation, Multiword Expressions and Constructions (LAW-MWE-CxG-2018), pages 185–192 Santa Fe, New Mexico, USA, August 25-26, 2018. verb–noun idioms (e.g., Salton et al., 2016). Broad coverage MWE identification has also been studied, and remains a challenge (Schneider et al., 2014; Gharbieh et al., 2017). Language models are common throughout NLP in tasks including machine translation (Brants et al., 2007), speech recognition (Collins et al., 2005), and question answering (Chen et al., 2006). Although word-level language models are widely used, and their performance can be higher than character-level language models, character-level models have the advantage that they can model out-of-vocabulary words (Mikolov et al., 2012). Owing to this advantage, character-level language models have been applied in a range of NLP tasks, including authorship attribution, (Peng et al., 2003), part-of-speech"
W18-4920,W06-1203,0,0.0563999,"ural language processing (NLP) tasks such as parsing (Korkontzelos and Manandhar, 2010) and machine translation (Carpuat and Diab, 2010). In the case of translation, compositionality is particularly important because a word-for-word translation would typically be incorrect for a non-compositional expression. Much research has therefore focused on compositionality prediction of MWEs, primarily at the type level. One common approach to measuring compositionality is to compare distributional representations of an MWE and its component words (e.g., Schone and Jurafsky, 2001; Baldwin et al., 2003; Katz and Giesbrecht, 2006; Reddy et al., 2011; Schulte im Walde et al., 2013; Salehi et al., 2015). The hypothesis behind this line of work is that the representation of a compositional MWE will be more similar to the representations of its component words than the representation of a non-compositional MWE will be to those of its component words. One issue faced by such approaches is that token-level instances of MWEs must be identified in a corpus in order to form distributional representations of them. Token-level MWE identification has been studied for specific types of MWEs such as verb-particle constructions (e.g"
W18-4920,N10-1089,0,0.0692583,"ional ends of the spectrum, respectively, and expressions such as rush hour and fine line falling somewhere in between as semi-compositional.1 Compositionality can also be viewed with respect to an individual component word of an MWE, where an MWE component word is compositional if its meaning is reflected in the meaning of the expression. For example, in spelling bee and grandfather clock, the first and second component words, respectively, are compositional, while the others are not. Knowledge of multiword expressions is important for natural language processing (NLP) tasks such as parsing (Korkontzelos and Manandhar, 2010) and machine translation (Carpuat and Diab, 2010). In the case of translation, compositionality is particularly important because a word-for-word translation would typically be incorrect for a non-compositional expression. Much research has therefore focused on compositionality prediction of MWEs, primarily at the type level. One common approach to measuring compositionality is to compare distributional representations of an MWE and its component words (e.g., Schone and Jurafsky, 2001; Baldwin et al., 2003; Katz and Giesbrecht, 2006; Reddy et al., 2011; Schulte im Walde et al., 2013; Salehi et"
W18-4920,D15-1176,0,0.0366066,"e widely used, and their performance can be higher than character-level language models, character-level models have the advantage that they can model out-of-vocabulary words (Mikolov et al., 2012). Owing to this advantage, character-level language models have been applied in a range of NLP tasks, including authorship attribution, (Peng et al., 2003), part-of-speech tagging (Santos and Zadrozny, 2014), case restoration (Susanto et al., 2016), and stock price prediction (dos Santos Pinheiro and Dras, 2017). Moreover, character-level information can be composed to form representations of words (Ling et al., 2015). In this paper we consider whether character-level neural network language models capture knowledge of MWE compositionality. We train character-level language models based on recurrent neural networks — including long short-term memory (LSTM, Hochreiter and Schmidhuber, 1997) and gated recurrent unit (GRU, Cho et al., 2014). We then use these language models to form continuous vector representations of MWEs and their component words. Following prior work, we then use these representations to predict the compositionality of MWEs. This method overcomes the limitation of previous work in this ve"
W18-4920,I11-1024,0,0.1601,"LP) tasks such as parsing (Korkontzelos and Manandhar, 2010) and machine translation (Carpuat and Diab, 2010). In the case of translation, compositionality is particularly important because a word-for-word translation would typically be incorrect for a non-compositional expression. Much research has therefore focused on compositionality prediction of MWEs, primarily at the type level. One common approach to measuring compositionality is to compare distributional representations of an MWE and its component words (e.g., Schone and Jurafsky, 2001; Baldwin et al., 2003; Katz and Giesbrecht, 2006; Reddy et al., 2011; Schulte im Walde et al., 2013; Salehi et al., 2015). The hypothesis behind this line of work is that the representation of a compositional MWE will be more similar to the representations of its component words than the representation of a non-compositional MWE will be to those of its component words. One issue faced by such approaches is that token-level instances of MWEs must be identified in a corpus in order to form distributional representations of them. Token-level MWE identification has been studied for specific types of MWEs such as verb-particle constructions (e.g., Kim and Baldwin,"
W18-4920,E14-1050,1,0.891023,"Missing"
W18-4920,N15-1099,1,0.253559,"ar, 2010) and machine translation (Carpuat and Diab, 2010). In the case of translation, compositionality is particularly important because a word-for-word translation would typically be incorrect for a non-compositional expression. Much research has therefore focused on compositionality prediction of MWEs, primarily at the type level. One common approach to measuring compositionality is to compare distributional representations of an MWE and its component words (e.g., Schone and Jurafsky, 2001; Baldwin et al., 2003; Katz and Giesbrecht, 2006; Reddy et al., 2011; Schulte im Walde et al., 2013; Salehi et al., 2015). The hypothesis behind this line of work is that the representation of a compositional MWE will be more similar to the representations of its component words than the representation of a non-compositional MWE will be to those of its component words. One issue faced by such approaches is that token-level instances of MWEs must be identified in a corpus in order to form distributional representations of them. Token-level MWE identification has been studied for specific types of MWEs such as verb-particle constructions (e.g., Kim and Baldwin, 2010) and This work is licensed under a Creative Comm"
W18-4920,P16-1019,0,0.027238,"of them. Token-level MWE identification has been studied for specific types of MWEs such as verb-particle constructions (e.g., Kim and Baldwin, 2010) and This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/. 1 These expressions and compositionality judgements are taken from Reddy et al. (2011). License details: http:// 185 Proceedings of the Joint Workshop on Linguistic Annotation, Multiword Expressions and Constructions (LAW-MWE-CxG-2018), pages 185–192 Santa Fe, New Mexico, USA, August 25-26, 2018. verb–noun idioms (e.g., Salton et al., 2016). Broad coverage MWE identification has also been studied, and remains a challenge (Schneider et al., 2014; Gharbieh et al., 2017). Language models are common throughout NLP in tasks including machine translation (Brants et al., 2007), speech recognition (Collins et al., 2005), and question answering (Chen et al., 2006). Although word-level language models are widely used, and their performance can be higher than character-level language models, character-level models have the advantage that they can model out-of-vocabulary words (Mikolov et al., 2012). Owing to this advantage, character-level"
W18-4920,Q14-1016,0,0.0765924,"e constructions (e.g., Kim and Baldwin, 2010) and This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/. 1 These expressions and compositionality judgements are taken from Reddy et al. (2011). License details: http:// 185 Proceedings of the Joint Workshop on Linguistic Annotation, Multiword Expressions and Constructions (LAW-MWE-CxG-2018), pages 185–192 Santa Fe, New Mexico, USA, August 25-26, 2018. verb–noun idioms (e.g., Salton et al., 2016). Broad coverage MWE identification has also been studied, and remains a challenge (Schneider et al., 2014; Gharbieh et al., 2017). Language models are common throughout NLP in tasks including machine translation (Brants et al., 2007), speech recognition (Collins et al., 2005), and question answering (Chen et al., 2006). Although word-level language models are widely used, and their performance can be higher than character-level language models, character-level models have the advantage that they can model out-of-vocabulary words (Mikolov et al., 2012). Owing to this advantage, character-level language models have been applied in a range of NLP tasks, including authorship attribution, (Peng et al."
W18-4920,W01-0513,0,0.156515,"dge of multiword expressions is important for natural language processing (NLP) tasks such as parsing (Korkontzelos and Manandhar, 2010) and machine translation (Carpuat and Diab, 2010). In the case of translation, compositionality is particularly important because a word-for-word translation would typically be incorrect for a non-compositional expression. Much research has therefore focused on compositionality prediction of MWEs, primarily at the type level. One common approach to measuring compositionality is to compare distributional representations of an MWE and its component words (e.g., Schone and Jurafsky, 2001; Baldwin et al., 2003; Katz and Giesbrecht, 2006; Reddy et al., 2011; Schulte im Walde et al., 2013; Salehi et al., 2015). The hypothesis behind this line of work is that the representation of a compositional MWE will be more similar to the representations of its component words than the representation of a non-compositional MWE will be to those of its component words. One issue faced by such approaches is that token-level instances of MWEs must be identified in a corpus in order to form distributional representations of them. Token-level MWE identification has been studied for specific types"
W18-4920,S13-1038,0,0.0618753,"Missing"
W18-4920,D16-1225,0,0.0247248,"cluding machine translation (Brants et al., 2007), speech recognition (Collins et al., 2005), and question answering (Chen et al., 2006). Although word-level language models are widely used, and their performance can be higher than character-level language models, character-level models have the advantage that they can model out-of-vocabulary words (Mikolov et al., 2012). Owing to this advantage, character-level language models have been applied in a range of NLP tasks, including authorship attribution, (Peng et al., 2003), part-of-speech tagging (Santos and Zadrozny, 2014), case restoration (Susanto et al., 2016), and stock price prediction (dos Santos Pinheiro and Dras, 2017). Moreover, character-level information can be composed to form representations of words (Ling et al., 2015). In this paper we consider whether character-level neural network language models capture knowledge of MWE compositionality. We train character-level language models based on recurrent neural networks — including long short-term memory (LSTM, Hochreiter and Schmidhuber, 1997) and gated recurrent unit (GRU, Cho et al., 2014). We then use these language models to form continuous vector representations of MWEs and their compo"
Y11-1028,W11-2508,0,0.476424,"I gave up. [text = ‘text message’] Furthermore, new word senses are not necessarily frequent. For example, the above usages are taken from the enTenTen corpus,1 a very large corpus containing a wide variety of text types, but the corresponding senses of these words appear to be rare in this corpus. The identification of new word senses is an important task in lexicography, and is necessary to keep dictionaries up-to-date. But lexical semantic change has only recently been studied from a computational perspective, and only to a limited extent (e.g., Sagi et al., 2009; Cook and Stevenson, 2010; Gulordava and Baroni, 2011). Furthermore, despite the low frequency of many novel word senses, no work to date has specifically considered the identification of words with novel infrequent senses. In contrast, in this paper we focus specifically on this issue The manual identification of novel senses is becoming increasingly difficult nowadays due to the vast quantities of text being produced (e.g., through online social media) that must be searched. ⋆ 1 We thank Afsaneh Fazly, Diana McCarthy, Suzanne Stevenson, and the members of the University of Melbourne Language Technology Group for their feedback on earlier versio"
Y11-1028,P98-2127,0,0.815347,"ither nouns or verbs—that are hypothesized by the method to have different senses in one of the corpora compared to the other. We consider a statistical model similar to one that Peirsman et al. (2010) used for automatically identifying lectal markers. This approach assumes that usages of different senses of a word will occur in different contexts, and that the aggregated contexts of a word in two corpora will differ if the senses of that word differ in those corpora. The model is based on a distributional representation of meaning that draws on work on automatically clustering similar words (Lin, 1998) that has been incorporated into tools used by lexicographers to identify word senses (Kilgarriff and Tugwell, 2002). Specifically, this method measures the similarity of two lexico-syntactic representations of the aggregated contexts of a target word; these two representations would typically come from different corpora representing, for example, different time periods. The lexico-syntactic representations capture the association of a target word with dependency triples, and the similarity between two target word representations is determined with a number of metrics. We propose some variatio"
Y11-1028,J07-4005,0,0.0604068,"vely—and exploit properties of these phenomena in their methods for identifying them. Gulordava and Baroni (2011) consider the identification of diachronic changes in meaning from an n-gram database, but in contrast to Sagi et al. and Cook and Stevenson, do not focus on specific types of semantic change. Others have studied differences in meaning between dialects and domains, instead of over time. Peirsman et al. (2010) consider the identification of lectal markers—words typical of one dialect versus another, either because of their marked frequency or sense—in Belgian and Netherlandic Dutch. McCarthy et al. (2007) consider the identification of predominant word senses in corpora, focusing on differences between domains. This method can be applied to not only identify the words that differ in predominant sense in two corpora, but also the specific predominant senses of those words. Nevertheless, none of these studies has specifically considered the identification of words with novel infrequent senses, the focus of this study. Given the challenges of evaluating methods for identifying semantic change—namely a lack of suitable resources—we propose the use of synthetic examples of semantic change for evalu"
Y11-1028,W04-0807,0,0.0913346,"Missing"
Y11-1028,N03-2023,0,0.0247257,"fication of words with novel infrequent senses, the focus of this study. Given the challenges of evaluating methods for identifying semantic change—namely a lack of suitable resources—we propose the use of synthetic examples of semantic change for evaluation. Gaustad (2001) showed that evaluations using pseudowords (artificially-ambiguous words) can over-estimate the accuracy of a word sense disambiguation system on real data. Gaustad suggests this is because word senses are typically related (i.e., words are polysemous) whereas pseudowords are usually created from words with distinct senses. Nakov and Hearst (2003) and Otrusina and Smrz (2010) propose methods for constructing more-realistic pseudowords by taking into account information about lexical categories, and lexical or distributional information, respectively. In this work we propose a new use for pseudowords—evaluating methods for identifying semantic change—and attempt to address concerns about the use of pseudowords, such as those raised by Gaustad, by creating our pseudowords (discussed in Sections 4 and 5) from real word senses and words with related senses. 266 3 Model The input to our method is two corpora which represent different text v"
Y11-1028,otrusina-smrz-2010-new,0,0.0302482,"infrequent senses, the focus of this study. Given the challenges of evaluating methods for identifying semantic change—namely a lack of suitable resources—we propose the use of synthetic examples of semantic change for evaluation. Gaustad (2001) showed that evaluations using pseudowords (artificially-ambiguous words) can over-estimate the accuracy of a word sense disambiguation system on real data. Gaustad suggests this is because word senses are typically related (i.e., words are polysemous) whereas pseudowords are usually created from words with distinct senses. Nakov and Hearst (2003) and Otrusina and Smrz (2010) propose methods for constructing more-realistic pseudowords by taking into account information about lexical categories, and lexical or distributional information, respectively. In this work we propose a new use for pseudowords—evaluating methods for identifying semantic change—and attempt to address concerns about the use of pseudowords, such as those raised by Gaustad, by creating our pseudowords (discussed in Sections 4 and 5) from real word senses and words with related senses. 266 3 Model The input to our method is two corpora which represent different text varieties, e.g., different tim"
Y11-1028,W09-0214,0,0.434278,"ignored the first few texts and phone calls, I gave up. [text = ‘text message’] Furthermore, new word senses are not necessarily frequent. For example, the above usages are taken from the enTenTen corpus,1 a very large corpus containing a wide variety of text types, but the corresponding senses of these words appear to be rare in this corpus. The identification of new word senses is an important task in lexicography, and is necessary to keep dictionaries up-to-date. But lexical semantic change has only recently been studied from a computational perspective, and only to a limited extent (e.g., Sagi et al., 2009; Cook and Stevenson, 2010; Gulordava and Baroni, 2011). Furthermore, despite the low frequency of many novel word senses, no work to date has specifically considered the identification of words with novel infrequent senses. In contrast, in this paper we focus specifically on this issue The manual identification of novel senses is becoming increasingly difficult nowadays due to the vast quantities of text being produced (e.g., through online social media) that must be searched. ⋆ 1 We thank Afsaneh Fazly, Diana McCarthy, Suzanne Stevenson, and the members of the University of Melbourne Languag"
Y11-1028,cook-stevenson-2010-automatically,1,\N,Missing
Y11-1028,J98-1004,0,\N,Missing
Y11-1028,C98-2122,0,\N,Missing
