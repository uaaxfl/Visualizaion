2020.bionlp-1.11,W19-1909,0,0.0696569,"Missing"
2020.bionlp-1.11,S17-2001,0,0.0674623,") have been shown to benefit from pre-training over large corpora followed by fine tuning over specific tasks. However, for small-scale datasets, only limited finetuning can be done. For example, GPT-2 achieved strong results across four large natural language inference (NLI) datasets, but was less successful over the small-scale RTE corpus (Bentivogli et al., 2009), performing below a multi-task biLSTM model. Similarly, while the large-scale pre-training of BERT has led to impressive improvements on a range of tasks, only very modest improvements have been achieved on STS tasks such as STSB (Cer et al., 2017) and MRPC (Dolan and Brockett, 2005) (with 5.7k and 3.6k training instances, resp.). Compared to general-domain STS benchmarks, labeled clinical STS data is more scarce, which tends to cause overfitting during fine-tuning. Moreover, further model scaling is a challenge due to GPU/TPU memory limitations and longer training time (Lan et al., 2019). This motivates us to search for model configurations which strike a balance between model flexibility and overfitting. In this paper, we study the impact of a number of model design choices. First, following Reimers and Gurevych (2019), we study the i"
2020.bionlp-1.11,W05-1203,0,0.151145,"ely different fine-tuning and pooling strategies. We observe that the impact of domain-specific fine-tuning on clinical STS is much less than that in the general domain, likely due to the concept richness of the domain. Based on this, we propose two data augmentation techniques. Experimental results on N2C2-STS1 demonstrate substantial improvements, validating the utility of the proposed methods. 1 Introduction Semantic Textual Similarity (STS) is a language understanding task, involving assessing the degree of semantic equivalence between two pieces of text based on a graded numerical score (Corley and Mihalcea, 2005). It has application in tasks such as information retrieval (Hliaoutakis et al., 2006), question answering (Hoogeveen et al., 2018), and summarization (AL-Khassawneh et al., 2016). In this paper, we focus on STS in the clinical domain, in the context of a recent task within the framework of N2C2 (the National NLP Clinical Challenges)1 , which makes use of the extended MedSTS data set (Wang et al., 2018), referring to N2C2-STS, with limited annotated sentences pairs (1.6K) that are rich in domain terms. Neural STS models typically consist of encoders to generate text representations, and a regr"
2020.bionlp-1.11,N19-1423,0,0.214256,"referring to N2C2-STS, with limited annotated sentences pairs (1.6K) that are rich in domain terms. Neural STS models typically consist of encoders to generate text representations, and a regression layer to measure the similarity score (He et al., 2015; Mueller and Thyagarajan, 2016; He and Lin, 1 https://portal.dbmi.hms.harvard.edu/ projects/n2c2-2019-t1/ 2016; Reimers and Gurevych, 2019). These architectures require a large amount of training data, an unrealistic requirement in low resource settings. Recently, pre-trained language models (LMs) such as GPT-2 (Radford et al., 2018) and BERT (Devlin et al., 2019) have been shown to benefit from pre-training over large corpora followed by fine tuning over specific tasks. However, for small-scale datasets, only limited finetuning can be done. For example, GPT-2 achieved strong results across four large natural language inference (NLI) datasets, but was less successful over the small-scale RTE corpus (Bentivogli et al., 2009), performing below a multi-task biLSTM model. Similarly, while the large-scale pre-training of BERT has led to impressive improvements on a range of tasks, only very modest improvements have been achieved on STS tasks such as STSB (C"
2020.bionlp-1.11,I05-5002,0,0.0220521,"from pre-training over large corpora followed by fine tuning over specific tasks. However, for small-scale datasets, only limited finetuning can be done. For example, GPT-2 achieved strong results across four large natural language inference (NLI) datasets, but was less successful over the small-scale RTE corpus (Bentivogli et al., 2009), performing below a multi-task biLSTM model. Similarly, while the large-scale pre-training of BERT has led to impressive improvements on a range of tasks, only very modest improvements have been achieved on STS tasks such as STSB (Cer et al., 2017) and MRPC (Dolan and Brockett, 2005) (with 5.7k and 3.6k training instances, resp.). Compared to general-domain STS benchmarks, labeled clinical STS data is more scarce, which tends to cause overfitting during fine-tuning. Moreover, further model scaling is a challenge due to GPU/TPU memory limitations and longer training time (Lan et al., 2019). This motivates us to search for model configurations which strike a balance between model flexibility and overfitting. In this paper, we study the impact of a number of model design choices. First, following Reimers and Gurevych (2019), we study the impact of various pooling methods on"
2020.bionlp-1.11,D15-1181,0,0.0230962,"ion retrieval (Hliaoutakis et al., 2006), question answering (Hoogeveen et al., 2018), and summarization (AL-Khassawneh et al., 2016). In this paper, we focus on STS in the clinical domain, in the context of a recent task within the framework of N2C2 (the National NLP Clinical Challenges)1 , which makes use of the extended MedSTS data set (Wang et al., 2018), referring to N2C2-STS, with limited annotated sentences pairs (1.6K) that are rich in domain terms. Neural STS models typically consist of encoders to generate text representations, and a regression layer to measure the similarity score (He et al., 2015; Mueller and Thyagarajan, 2016; He and Lin, 1 https://portal.dbmi.hms.harvard.edu/ projects/n2c2-2019-t1/ 2016; Reimers and Gurevych, 2019). These architectures require a large amount of training data, an unrealistic requirement in low resource settings. Recently, pre-trained language models (LMs) such as GPT-2 (Radford et al., 2018) and BERT (Devlin et al., 2019) have been shown to benefit from pre-training over large corpora followed by fine tuning over specific tasks. However, for small-scale datasets, only limited finetuning can be done. For example, GPT-2 achieved strong results across f"
2020.bionlp-1.11,D17-1082,0,0.0158979,"ng the capability and generalizability of LMs, while adapting a single fully-connected layer to capture task features. Sentence-BERT (Reimers and Gurevych, 2019) makes use of task-specific structures to optimize STS, concentrating on computational and time efficiency, and is evaluated on relatively larger datasets in the general domain. For evaluating the impact of number of layers transferred to the supervised target task from the pre-trained language model, GPT-2 has been analyzed on two datasets. However, they are both large: MultiNLI (Williams et al., 2018) with &gt;390k instances, and RACE (Lai et al., 2017) with &gt;97k instances. These tasks also both involve reasoning-related classification, as opposed to the nuanced regression task of STS. 2.2 Data Augmentation Synonym replacement is one of the most commonly used data augmentation methods to simulate linguistic diversity, but it introduces ambiguity if accurate context-dependent disambiguation is not performed. Moreover, random selection and replacement of a single word used in general texts is not plausible for term-rich clinical text, resulting in too much semantic divergence (e.g patient to affected role and discharge to home to spark to home"
2020.bionlp-1.11,2021.ccl-1.108,0,0.124035,"Missing"
2020.bionlp-1.11,D19-1410,0,0.243996,"2016). In this paper, we focus on STS in the clinical domain, in the context of a recent task within the framework of N2C2 (the National NLP Clinical Challenges)1 , which makes use of the extended MedSTS data set (Wang et al., 2018), referring to N2C2-STS, with limited annotated sentences pairs (1.6K) that are rich in domain terms. Neural STS models typically consist of encoders to generate text representations, and a regression layer to measure the similarity score (He et al., 2015; Mueller and Thyagarajan, 2016; He and Lin, 1 https://portal.dbmi.hms.harvard.edu/ projects/n2c2-2019-t1/ 2016; Reimers and Gurevych, 2019). These architectures require a large amount of training data, an unrealistic requirement in low resource settings. Recently, pre-trained language models (LMs) such as GPT-2 (Radford et al., 2018) and BERT (Devlin et al., 2019) have been shown to benefit from pre-training over large corpora followed by fine tuning over specific tasks. However, for small-scale datasets, only limited finetuning can be done. For example, GPT-2 achieved strong results across four large natural language inference (NLI) datasets, but was less successful over the small-scale RTE corpus (Bentivogli et al., 2009), perf"
2020.bionlp-1.11,P16-1009,0,0.0325008,"n. Random insertion, deletion, and swapping of words have been demonstrated to be effective on five text classification tasks (Wei and Zou, 2019). But those experiments targeted topic prediction, in contrast to semantic reasoning such as STS and MultiNLI. Intuitively, they do not change the overall topic of a text, but can skew the meaning of a sentence, undermining the STS task. Swapping an entire semantic segment may mitigate the risk of introducing label noise to the STS task. Compared to semantic and syntactic distortion potentially caused by aforementioned methods, back translation (BT) (Sennrich et al., 2016) — translating to a target language then back to the original language — presents fluent augmented data and reliable improvements for tasks demanding for adequate semantic understanding, such as low-resource machine translation (Xia et al., 2019) and question answering (Yu et al., 2019). This motivates our application of BT on low-resource clinical STS, to bridge linguistic variation between two sentences. This work represents the first exploration of applying BT for STS. 3 STS Model Configurations In this section, we study the impact of a number of model design choices on BERT for STS, using"
2020.bionlp-1.11,N16-1108,0,0.0388325,"Missing"
2020.bionlp-1.11,D19-1670,0,0.122152,"introduces ambiguity if accurate context-dependent disambiguation is not performed. Moreover, random selection and replacement of a single word used in general texts is not plausible for term-rich clinical text, resulting in too much semantic divergence (e.g patient to affected role and discharge to home to spark to home). By contrast, replacing a complete mention of the concept can increase error propagation due to the prerequisite concept extraction and normalization. Random insertion, deletion, and swapping of words have been demonstrated to be effective on five text classification tasks (Wei and Zou, 2019). But those experiments targeted topic prediction, in contrast to semantic reasoning such as STS and MultiNLI. Intuitively, they do not change the overall topic of a text, but can skew the meaning of a sentence, undermining the STS task. Swapping an entire semantic segment may mitigate the risk of introducing label noise to the STS task. Compared to semantic and syntactic distortion potentially caused by aforementioned methods, back translation (BT) (Sennrich et al., 2016) — translating to a target language then back to the original language — presents fluent augmented data and reliable improv"
2020.bionlp-1.11,N18-1101,0,0.0136766,"ed at improving downstream tasks indirectly by optimizing the capability and generalizability of LMs, while adapting a single fully-connected layer to capture task features. Sentence-BERT (Reimers and Gurevych, 2019) makes use of task-specific structures to optimize STS, concentrating on computational and time efficiency, and is evaluated on relatively larger datasets in the general domain. For evaluating the impact of number of layers transferred to the supervised target task from the pre-trained language model, GPT-2 has been analyzed on two datasets. However, they are both large: MultiNLI (Williams et al., 2018) with &gt;390k instances, and RACE (Lai et al., 2017) with &gt;97k instances. These tasks also both involve reasoning-related classification, as opposed to the nuanced regression task of STS. 2.2 Data Augmentation Synonym replacement is one of the most commonly used data augmentation methods to simulate linguistic diversity, but it introduces ambiguity if accurate context-dependent disambiguation is not performed. Moreover, random selection and replacement of a single word used in general texts is not plausible for term-rich clinical text, resulting in too much semantic divergence (e.g patient to af"
2020.bionlp-1.11,P19-1579,0,0.0156264,"NLI. Intuitively, they do not change the overall topic of a text, but can skew the meaning of a sentence, undermining the STS task. Swapping an entire semantic segment may mitigate the risk of introducing label noise to the STS task. Compared to semantic and syntactic distortion potentially caused by aforementioned methods, back translation (BT) (Sennrich et al., 2016) — translating to a target language then back to the original language — presents fluent augmented data and reliable improvements for tasks demanding for adequate semantic understanding, such as low-resource machine translation (Xia et al., 2019) and question answering (Yu et al., 2019). This motivates our application of BT on low-resource clinical STS, to bridge linguistic variation between two sentences. This work represents the first exploration of applying BT for STS. 3 STS Model Configurations In this section, we study the impact of a number of model design choices on BERT for STS, using a 12-layer base model initialized with pretrained weights. 3.1 Hierarchical Convolution (HConv) The resource-poor and concept-rich nature of clinical STS makes it difficult to train a large model endto-end on sentence pairs. To address this, most"
2020.bionlp-1.17,C08-1003,0,0.109274,"Missing"
2020.bionlp-1.17,W19-1909,0,0.0481317,"Missing"
2020.bionlp-1.17,W06-1615,0,0.168252,"l domains with unique vocabularies (Alsentzer et al., 2019; Lee et al., 2019). These models can also accomplish many tasks in an unsupervised manner. For example, Radford et al. (2019) showed that free text questions could be fed through a language model and generate the correct answer in many cases. In our experiments, we demonstrate the usefulness of contextualized language models by pre-training BERT on a large set of veterinary clinical records, and further explore its usefulness for domain adaptation through instance selection. Domain adaptation is a task which has a long history in NLP (Blitzer et al., 2006; Jiang and Zhai, 2007; Agirre and De Lacalle, 2008; Daum´e III, 2007). There has been further work demonstrating the usefulness of reducing the covariance between domains through adversarial learning (Li et al., 2018b). More recently, it has been shown that domain adversarial training can be effectively done using contextualized models, such as BERT, through using a two-step domain-discriminative data selection (Ma et al., 2019). We adapt these methods to our task to create a more generalizable SOURCE ( Ce f ov e c i n) TARGETY ( Ce p ha l e x i n ) TARGETZ ( Amoc y c i l l i nCl av u l on at"
2020.bionlp-1.17,U17-1008,1,0.832458,"ne with a variety of methods (Kiritchenko and Cherry, 2011; Goldstein et al., 2007; Li et al., 2018a). Additionally, classifying diseases and medications in clinical text has been addressed in shared tasks for human texts (Uzuner et al., 2010). Previous methods have also been explored for extracting the antimicrobials used, out of veterinary prescription labels, associated with the clinical records (Hur et al., 2019), and labeling of diseases in veterinary clinical records (Zhang et al., 2019; Nie et al., 2018) as well exploring methods for negation of diseases for addressing false positives (Cheng et al., 2017; Kennedy et al., 2019). Our work expands on this work by linking the indication of use to an antimicrobial being administered for that diagnosis. Contextualized language models have recently gained much popularity due to their ability to greatly improve the representation of texts with fewer training instances, thereby transferring more efficiently between domains (Devlin et al., 2018; Howard and Ruder, 2018). Pre-training these language models on large amounts of text data specific to a given domain, such as clinical records or biomedical literature, has also been shown to further improve th"
2020.bionlp-1.17,P07-1033,0,0.354009,"Missing"
2020.bionlp-1.17,N19-1423,0,0.0691654,"Missing"
2020.bionlp-1.17,N13-1014,0,0.118762,"e used, with the form of drug administration (oral, injected, etc.) and different indications of use creating distinct contexts that can be seen as sub-domains. Therefore, models that allow for the transfer of knowledge between the sub-domains of the various antimicrobials are required to effectively label the indication of use. To explore the interaction between learning methods and the resource constraints on labeling, we develop models using the complete set of labels we had available, but also models derived using only labels that can be created within two hours, following the paradigm of Garrette and Baldridge (2013). Specifically, our work explores methods to improve the performance of classifying the indication for an antibiotic administration in veterinary records of dogs and cats. In addition to classifying the indication of use, we explore how data selection can be used to improve the transfer of knowledge derived from labeled data of a single antimicrobial agent to the context of other agents. We also release our code, and select pre-trained models used in this study at: https://github. com/havocy28/VetBERT. 157 Related Work Clinical coding of medical documents has been previously done with a variet"
2020.bionlp-1.17,P18-1031,0,0.0291686,"t al., 2019), and labeling of diseases in veterinary clinical records (Zhang et al., 2019; Nie et al., 2018) as well exploring methods for negation of diseases for addressing false positives (Cheng et al., 2017; Kennedy et al., 2019). Our work expands on this work by linking the indication of use to an antimicrobial being administered for that diagnosis. Contextualized language models have recently gained much popularity due to their ability to greatly improve the representation of texts with fewer training instances, thereby transferring more efficiently between domains (Devlin et al., 2018; Howard and Ruder, 2018). Pre-training these language models on large amounts of text data specific to a given domain, such as clinical records or biomedical literature, has also been shown to further improve the performance in biomedical domains with unique vocabularies (Alsentzer et al., 2019; Lee et al., 2019). These models can also accomplish many tasks in an unsupervised manner. For example, Radford et al. (2019) showed that free text questions could be fed through a language model and generate the correct answer in many cases. In our experiments, we demonstrate the usefulness of contextualized language models b"
2020.bionlp-1.17,P07-1034,0,0.309145,"Missing"
2020.bionlp-1.17,S18-1190,0,0.0267164,"ained the prefixes clav or amoxyclav for amoxycillin clavulanate, ceph, rilex or kflex for cephalexin, and conv or cefov for cefovecin. These prefixes were sourced from a previous study exploring mention detection of antimicrobials (Hur et al., 2019). We signal the use of mention boundary embeddings with “+M” in the results tables. 4.2.2 Data augmentation Synonym-based data augmentation has been successfully applied to contexts including word sense disambiguation (Leacock and Chodorow, 1998), sentiment analysis (Li et al., 2017), text classification (Wei and Zou, 2019), and argument analysis (Joshi et al., 2018). We perform data augmentation on clinical notes by replacing synonyms using WordNet (Fellbaum, 160 S OURCE TARGET Y TARGET Z VetBERT+rank[linear] VetBERT+rank[linear]+A VetBERT+rank[linear]+M VetBERT+rank[linear]+M+A 74.3±0.2 75.8±1.3 73.4±0.9 75.7±0.8 76.6±3.0 81.0±2.6 77.1±1.9 81.0±2.8 66.9±2.2 63.7±1.4 65.9±2.4 63.8±3.5 VetBERT+rank[exp] VetBERT+rank[exp]+A VetBERT+rank[exp]+M VetBERT+rank[exp]+M+A 68.3±2.1 76.6±0.3 68.9±2.0 76.9±0.2 66.5±2.1 76.7±2.4 66.7±1.5 77.3±2.3 58.1±1.5 65.4±1.0 57.9±2.1 64.4±1.5 VetBERT+rank[rand] VetBERT+rank[rand]+A VetBERT+rank[rand]+M VetBERT+rank[rand]+M+A 73"
2020.bionlp-1.17,P11-1075,0,0.0329658,", our work explores methods to improve the performance of classifying the indication for an antibiotic administration in veterinary records of dogs and cats. In addition to classifying the indication of use, we explore how data selection can be used to improve the transfer of knowledge derived from labeled data of a single antimicrobial agent to the context of other agents. We also release our code, and select pre-trained models used in this study at: https://github. com/havocy28/VetBERT. 157 Related Work Clinical coding of medical documents has been previously done with a variety of methods (Kiritchenko and Cherry, 2011; Goldstein et al., 2007; Li et al., 2018a). Additionally, classifying diseases and medications in clinical text has been addressed in shared tasks for human texts (Uzuner et al., 2010). Previous methods have also been explored for extracting the antimicrobials used, out of veterinary prescription labels, associated with the clinical records (Hur et al., 2019), and labeling of diseases in veterinary clinical records (Zhang et al., 2019; Nie et al., 2018) as well exploring methods for negation of diseases for addressing false positives (Cheng et al., 2017; Kennedy et al., 2019). Our work expand"
2020.bionlp-1.17,N18-2076,1,0.897088,"Missing"
2020.bionlp-1.17,E17-2004,1,0.874917,"Missing"
2020.bionlp-1.17,P19-1335,0,0.0420086,"Missing"
2020.bionlp-1.17,D19-6109,0,0.0362407,"Missing"
2020.bionlp-1.17,D19-1670,0,0.0288211,"re created by identifying strings that contained the prefixes clav or amoxyclav for amoxycillin clavulanate, ceph, rilex or kflex for cephalexin, and conv or cefov for cefovecin. These prefixes were sourced from a previous study exploring mention detection of antimicrobials (Hur et al., 2019). We signal the use of mention boundary embeddings with “+M” in the results tables. 4.2.2 Data augmentation Synonym-based data augmentation has been successfully applied to contexts including word sense disambiguation (Leacock and Chodorow, 1998), sentiment analysis (Li et al., 2017), text classification (Wei and Zou, 2019), and argument analysis (Joshi et al., 2018). We perform data augmentation on clinical notes by replacing synonyms using WordNet (Fellbaum, 160 S OURCE TARGET Y TARGET Z VetBERT+rank[linear] VetBERT+rank[linear]+A VetBERT+rank[linear]+M VetBERT+rank[linear]+M+A 74.3±0.2 75.8±1.3 73.4±0.9 75.7±0.8 76.6±3.0 81.0±2.6 77.1±1.9 81.0±2.8 66.9±2.2 63.7±1.4 65.9±2.4 63.8±3.5 VetBERT+rank[exp] VetBERT+rank[exp]+A VetBERT+rank[exp]+M VetBERT+rank[exp]+M+A 68.3±2.1 76.6±0.3 68.9±2.0 76.9±0.2 66.5±2.1 76.7±2.4 66.7±1.5 77.3±2.3 58.1±1.5 65.4±1.0 57.9±2.1 64.4±1.5 VetBERT+rank[rand] VetBERT+rank[rand]+A Ve"
2020.clinicalnlp-1.25,S15-2045,0,0.0502991,"Missing"
2020.clinicalnlp-1.25,S14-2010,0,0.0574759,"Missing"
2020.clinicalnlp-1.25,S16-1081,0,0.0499427,"Missing"
2020.clinicalnlp-1.25,S13-1004,0,0.0496199,"Missing"
2020.clinicalnlp-1.25,S12-1051,0,0.039632,"n STS-G + N2C2-STS train .894 .902 .830 .836 Eval set / Model Data STS-B dev: CLS-BERT CLS-BERT N2C2-STS test: HConvBERT HConvBERT Table 2: Pearson’s r and Spearman’s ρ evaluation on STS-B dev (upper half) and N2C2-STS test (bottom half), based on fine-tuning over STS-B train (5,749) and STS-G (28,518), for CLS-BERT and HConvBERT. clinical STS models? RQ2 How does low-quality training data impact clinical STS performance, vs. high-quality labelled data or no labelled data? Effect of Larger General STS Corpus. We source general-domain labelled data from: (1) SemEval-STS shared tasks 2012–2017 (Agirre et al., 2012, 2013, 2014, 2015, 2016; Cer et al., 2017); and SICK-R (Marelli et al., 2014). This results in a total of 28,518 labelled sentence pairs, which we refer to as “STS-G”. We adapt a BERT encoder connected to a linear regression layer to fine-tune a general-domain STS model using STS-G, where the CLS-vector is used to represent the sentence pair (CLS-BERT). We compare this with a model trained only on STSB. We evaluate both models on STS-B dev (same setup as Section 6.1). For clinical STS, we employ a hierarchical convolution (HConv) model based on BERT (updating parameters of the last four layer"
2020.clinicalnlp-1.25,2020.acl-main.692,0,0.0204454,"or averaged last-layer embeddings, but this tends to perform poorly, even worse than averaged GloVe (Pennington et al., 2014) embeddings. SBERT (Reimers and Gurevych, 2019) proposed to use a Siamese structure based on BERT to learn sentence representations, where they fine-tuned the model over general NLI data, and continued to fine-tune on general STS data (STS-B) (Cer et al., 2017). In this work, we experiment with this approach specifically in the clinical context. 3 4 Observations In modern NLP, large amounts of high-quality training data are a key element in building successful systems (Aharoni and Goldberg, 2020). This is also the case with STS, where additional training data has been shown to improve accuracy (Wang et al., 2020b). However, domain shifts inevitably lead to performance drops (Gururangan et al., 2020). Therefore, we ask: RQ1 Can large-scale generaldomain labelled STS data be transferred to train Len Train Size Test Size MedSTS N2C2-STS 25.4 19.3 750 1642 318 412 Table 1: Clinical STS datasets. Train and Test Size = number of text pairs. Len = mean sentence length in tokens. Datasets and Tasks We select two available clinical STS benchmark datasets for evaluation: MedSTS (Wang et al., 20"
2020.clinicalnlp-1.25,W19-1909,0,0.0489185,"Missing"
2020.clinicalnlp-1.25,S17-2001,0,0.122684,"shop, pages 227–233 c November 19, 2020. 2020 Association for Computational Linguistics For STS, in the absence of labelled data, the simplest approach is to calculate the cosine similarity between the CLS-vectors of two sentences or averaged last-layer embeddings, but this tends to perform poorly, even worse than averaged GloVe (Pennington et al., 2014) embeddings. SBERT (Reimers and Gurevych, 2019) proposed to use a Siamese structure based on BERT to learn sentence representations, where they fine-tuned the model over general NLI data, and continued to fine-tune on general STS data (STS-B) (Cer et al., 2017). In this work, we experiment with this approach specifically in the clinical context. 3 4 Observations In modern NLP, large amounts of high-quality training data are a key element in building successful systems (Aharoni and Goldberg, 2020). This is also the case with STS, where additional training data has been shown to improve accuracy (Wang et al., 2020b). However, domain shifts inevitably lead to performance drops (Gururangan et al., 2020). Therefore, we ask: RQ1 Can large-scale generaldomain labelled STS data be transferred to train Len Train Size Test Size MedSTS N2C2-STS 25.4 19.3 750 1"
2020.clinicalnlp-1.25,N19-1423,0,0.221766,"er to the test data, we can obtain better performance. By leveraging a large general-purpose STS dataset and small-scale in-domain training data, we obtain further improvements to r = 0.90, a new SOTA. 1 Introduction Semantic textual similarity (STS) measures the degree of semantic equivalence between two text snippets, based on a graded numerical value, with applications including question answering (Yadav et al., 2020), duplicate detection (Poerner and Sch¨utze, 2019), and entity linking (Zhou et al., 2020). Modern pretrained language models have achieved impressive results for general STS (Devlin et al., 2019). However in low-resource domains without in-domain labelled data, results are generally lower (Wang et al., 2020b). In the clinical domain in particular, annotation requires medical experts (Wang et al., 2018; Romanov and Shivade, 2018), meaning that labelled datasets are generally small, hampering clinical STS. We address the question of how to apply pretrained language models to such domain-specific tasks where there is little or no labelled data, focusing specifically on the task of clinical STS. Employing a general STS model generally yields poor results over technical domains due to cova"
2020.clinicalnlp-1.25,2020.acl-main.740,0,0.0414559,"Missing"
2020.clinicalnlp-1.25,2021.ccl-1.108,0,0.0796948,"Missing"
2020.clinicalnlp-1.25,S14-2001,0,0.0322862,": CLS-BERT CLS-BERT N2C2-STS test: HConvBERT HConvBERT Table 2: Pearson’s r and Spearman’s ρ evaluation on STS-B dev (upper half) and N2C2-STS test (bottom half), based on fine-tuning over STS-B train (5,749) and STS-G (28,518), for CLS-BERT and HConvBERT. clinical STS models? RQ2 How does low-quality training data impact clinical STS performance, vs. high-quality labelled data or no labelled data? Effect of Larger General STS Corpus. We source general-domain labelled data from: (1) SemEval-STS shared tasks 2012–2017 (Agirre et al., 2012, 2013, 2014, 2015, 2016; Cer et al., 2017); and SICK-R (Marelli et al., 2014). This results in a total of 28,518 labelled sentence pairs, which we refer to as “STS-G”. We adapt a BERT encoder connected to a linear regression layer to fine-tune a general-domain STS model using STS-G, where the CLS-vector is used to represent the sentence pair (CLS-BERT). We compare this with a model trained only on STSB. We evaluate both models on STS-B dev (same setup as Section 6.1). For clinical STS, we employ a hierarchical convolution (HConv) model based on BERT (updating parameters of the last four layers), where the model is first fine-tuned with STS-B, then N2C2STS is augmented"
2020.clinicalnlp-1.25,W19-5006,0,0.0717902,"al STS. We address the question of how to apply pretrained language models to such domain-specific tasks where there is little or no labelled data, focusing specifically on the task of clinical STS. Employing a general STS model generally yields poor results over technical domains due to covariate shift. To bridge this gap, a standard approach is to pretrain the LM on in-domain text, such as ClinicalBERT (Alsentzer et al., 2019) using MIMICIII (Johnson et al., 2016). However, existing research has tended to estimate effectiveness under the fine-tuning setting, rather than via inference tasks (Peng et al., 2019; Wang et al., 2020b). In this paper, we first evaluate domain pretraining approaches for clinical STS, with no labelled data. Based on the assumption that general STS models trained on large-scale STS datasets will perform reasonably well on clinical sentence pairs (Section 4), we then experiment with learning from the pseudo-labelled data (Section 5). Experimental results show both domain pretraining and pseudo-labelled data fine-tuning improve clinical STS, and the combination of the two achieves the best performance of r = 0.80 on N2C2-STS (Section 6.3). Further analysis shows that the sco"
2020.clinicalnlp-1.25,D14-1162,0,0.0835444,"specific text and task-specific labelled data (Gururangan et al., 2020; Peng et al., 2019). For this approach, however, domain-specific labelled data is required, an assumption that we seek to relax. 227 Proceedings of the 3rd Clinical Natural Language Processing Workshop, pages 227–233 c November 19, 2020. 2020 Association for Computational Linguistics For STS, in the absence of labelled data, the simplest approach is to calculate the cosine similarity between the CLS-vectors of two sentences or averaged last-layer embeddings, but this tends to perform poorly, even worse than averaged GloVe (Pennington et al., 2014) embeddings. SBERT (Reimers and Gurevych, 2019) proposed to use a Siamese structure based on BERT to learn sentence representations, where they fine-tuned the model over general NLI data, and continued to fine-tune on general STS data (STS-B) (Cer et al., 2017). In this work, we experiment with this approach specifically in the clinical context. 3 4 Observations In modern NLP, large amounts of high-quality training data are a key element in building successful systems (Aharoni and Goldberg, 2020). This is also the case with STS, where additional training data has been shown to improve accuracy"
2020.clinicalnlp-1.25,D19-1173,0,0.0340273,"Missing"
2020.clinicalnlp-1.25,D19-1410,0,0.0111701,"a (Gururangan et al., 2020; Peng et al., 2019). For this approach, however, domain-specific labelled data is required, an assumption that we seek to relax. 227 Proceedings of the 3rd Clinical Natural Language Processing Workshop, pages 227–233 c November 19, 2020. 2020 Association for Computational Linguistics For STS, in the absence of labelled data, the simplest approach is to calculate the cosine similarity between the CLS-vectors of two sentences or averaged last-layer embeddings, but this tends to perform poorly, even worse than averaged GloVe (Pennington et al., 2014) embeddings. SBERT (Reimers and Gurevych, 2019) proposed to use a Siamese structure based on BERT to learn sentence representations, where they fine-tuned the model over general NLI data, and continued to fine-tune on general STS data (STS-B) (Cer et al., 2017). In this work, we experiment with this approach specifically in the clinical context. 3 4 Observations In modern NLP, large amounts of high-quality training data are a key element in building successful systems (Aharoni and Goldberg, 2020). This is also the case with STS, where additional training data has been shown to improve accuracy (Wang et al., 2020b). However, domain shifts i"
2020.clinicalnlp-1.25,D18-1187,0,0.0306164,"Missing"
2020.clinicalnlp-1.25,2020.bionlp-1.11,1,0.903317,"cale in-domain training data, we obtain further improvements to r = 0.90, a new SOTA. 1 Introduction Semantic textual similarity (STS) measures the degree of semantic equivalence between two text snippets, based on a graded numerical value, with applications including question answering (Yadav et al., 2020), duplicate detection (Poerner and Sch¨utze, 2019), and entity linking (Zhou et al., 2020). Modern pretrained language models have achieved impressive results for general STS (Devlin et al., 2019). However in low-resource domains without in-domain labelled data, results are generally lower (Wang et al., 2020b). In the clinical domain in particular, annotation requires medical experts (Wang et al., 2018; Romanov and Shivade, 2018), meaning that labelled datasets are generally small, hampering clinical STS. We address the question of how to apply pretrained language models to such domain-specific tasks where there is little or no labelled data, focusing specifically on the task of clinical STS. Employing a general STS model generally yields poor results over technical domains due to covariate shift. To bridge this gap, a standard approach is to pretrain the LM on in-domain text, such as ClinicalBER"
2020.clinicalnlp-1.25,2020.acl-main.414,0,0.0130279,"al model. We evaluate the approach on two clinical STS datasets, and achieve r = 0.80 on N2C2-STS. Further investigation reveals that if the data distribution of unlabelled sentence pairs is closer to the test data, we can obtain better performance. By leveraging a large general-purpose STS dataset and small-scale in-domain training data, we obtain further improvements to r = 0.90, a new SOTA. 1 Introduction Semantic textual similarity (STS) measures the degree of semantic equivalence between two text snippets, based on a graded numerical value, with applications including question answering (Yadav et al., 2020), duplicate detection (Poerner and Sch¨utze, 2019), and entity linking (Zhou et al., 2020). Modern pretrained language models have achieved impressive results for general STS (Devlin et al., 2019). However in low-resource domains without in-domain labelled data, results are generally lower (Wang et al., 2020b). In the clinical domain in particular, annotation requires medical experts (Wang et al., 2018; Romanov and Shivade, 2018), meaning that labelled datasets are generally small, hampering clinical STS. We address the question of how to apply pretrained language models to such domain-specifi"
2020.clinicalnlp-1.25,2020.tacl-1.8,0,0.0188378,"C2-STS. Further investigation reveals that if the data distribution of unlabelled sentence pairs is closer to the test data, we can obtain better performance. By leveraging a large general-purpose STS dataset and small-scale in-domain training data, we obtain further improvements to r = 0.90, a new SOTA. 1 Introduction Semantic textual similarity (STS) measures the degree of semantic equivalence between two text snippets, based on a graded numerical value, with applications including question answering (Yadav et al., 2020), duplicate detection (Poerner and Sch¨utze, 2019), and entity linking (Zhou et al., 2020). Modern pretrained language models have achieved impressive results for general STS (Devlin et al., 2019). However in low-resource domains without in-domain labelled data, results are generally lower (Wang et al., 2020b). In the clinical domain in particular, annotation requires medical experts (Wang et al., 2018; Romanov and Shivade, 2018), meaning that labelled datasets are generally small, hampering clinical STS. We address the question of how to apply pretrained language models to such domain-specific tasks where there is little or no labelled data, focusing specifically on the task of cl"
2020.coling-main.523,D19-1352,0,0.0214971,"KB1 is used as a query against all the entities in KB2 using a scoring method such as cosine similarity or BM25. Entities are represented by bag of words or bag of character n-grams. The result of these cheap methods can be further improved by supervised (neural) ranking models (Guo et al., 2016; Rao et al., 2019; Wang et al., 2018). Particularly when query and candidate documents are represented by an encoder such as BERT (Devlin et al., 2019), pre-trained on massive amounts of text data, neural rerankers perform substantially better than IR-based methods (Nogueira and Cho, 2019; Akkalyoncu Yilmaz et al., 2019). Our methods fit within this group. 3 Method n Given a UMLS concept ci represented by query qi = {t1i , . . . , tN i }, where ti is an alias term for ci in UMLS, we use the English Wikipedia augmented with multilingual Wikidata as a document collection D = {d1 , . . . , d|D |}, to retrieve page dj matching concept ci . Each page is represented by its title, text (only for candidate generation), and multilingual aliases from Wikidata. We follow a two-stage retrieval procedure: (1) candidate generation, where an IR method (e.g. BM25) is used to retrieve related documents; and (2) reranking of t"
2020.coling-main.523,N19-1423,0,0.0533084,"and entity variety. String and semantic matching methods are based on similarity between the entity names or descriptions in the two knowledge-bases (KBs). Each entity in KB1 is used as a query against all the entities in KB2 using a scoring method such as cosine similarity or BM25. Entities are represented by bag of words or bag of character n-grams. The result of these cheap methods can be further improved by supervised (neural) ranking models (Guo et al., 2016; Rao et al., 2019; Wang et al., 2018). Particularly when query and candidate documents are represented by an encoder such as BERT (Devlin et al., 2019), pre-trained on massive amounts of text data, neural rerankers perform substantially better than IR-based methods (Nogueira and Cho, 2019; Akkalyoncu Yilmaz et al., 2019). Our methods fit within this group. 3 Method n Given a UMLS concept ci represented by query qi = {t1i , . . . , tN i }, where ti is an alias term for ci in UMLS, we use the English Wikipedia augmented with multilingual Wikidata as a document collection D = {d1 , . . . , d|D |}, to retrieve page dj matching concept ci . Each page is represented by its title, text (only for candidate generation), and multilingual aliases from"
2020.coling-main.523,P19-1335,0,0.176791,"Missing"
2020.coling-main.523,P18-1010,0,0.0261336,"an IR method (e.g. BM25) is used to retrieve related documents; and (2) reranking of the top k candidates via a learn-to-rank method (Liu, 2009). 3.1 Candidate Generation We index Wikipedia collection D using Lucene, and build query qi from UMLS to retrieve the top k=64 relevant pages. We use a Boolean disjunction between all alias terms in UMLS, and search in the title, text, and multilingual aliases fields in D. BM25 relies on exact term matches, and small variations can result in a mismatch. As a result, we also experimented with a character n-gram method (TFIDFchar ) successfully used in Murty et al. (2018) for candidate generation in medical entity linking. We build a bag of character n-grams (n ∈ [1, 5])) weighted by TF-IDF within term boundaries, and use cosine similarity between qi and each d ∈ D (excluding page text) to generate the top k=64 candidates. 3.2 Reranking We formulate the reranking task as passage pair binary classification (Nogueira and Cho, 2019), where the first passage is qi for concept ci from UMLS, and the second passage is the set of Wikipedia alias 5958 UMLS Concept UMLS Query Gold Wiki Candidate CUI: C0017168 GERD, Acid Reflux, oesofagusaandoening, . . . disorder of the"
2020.coling-main.523,P19-1492,0,0.0170411,"in more than 134 languages, compared to only a dozen languages in UMLS. Knowledge-base alignment: There are two main approaches for aligning knowledge bases such as Wikipedia and UMLS: (1) embedding-based alignment, and (2) string and semantic matching. In embedding-based methods, entity embeddings are learnt from text co-occurrence statistics or knowledgegraph (KG) relations separately, and are then aligned using a seed alignment dictionary (Mikolov et al., 2013; Chen et al., 2017), or adversarial learning (Qu et al., 2019). However, the text-based methods suffer from non-comparable corpora (Ormazabal et al., 2019), and the KG methods have not been tried on Wikidata because of its large scale and entity variety. String and semantic matching methods are based on similarity between the entity names or descriptions in the two knowledge-bases (KBs). Each entity in KB1 is used as a query against all the entities in KB2 using a scoring method such as cosine similarity or BM25. Entities are represented by bag of words or bag of character n-grams. The result of these cheap methods can be further improved by supervised (neural) ranking models (Guo et al., 2016; Rao et al., 2019; Wang et al., 2018). Particularly"
2020.coling-main.523,D19-1540,0,0.0219568,"rom non-comparable corpora (Ormazabal et al., 2019), and the KG methods have not been tried on Wikidata because of its large scale and entity variety. String and semantic matching methods are based on similarity between the entity names or descriptions in the two knowledge-bases (KBs). Each entity in KB1 is used as a query against all the entities in KB2 using a scoring method such as cosine similarity or BM25. Entities are represented by bag of words or bag of character n-grams. The result of these cheap methods can be further improved by supervised (neural) ranking models (Guo et al., 2016; Rao et al., 2019; Wang et al., 2018). Particularly when query and candidate documents are represented by an encoder such as BERT (Devlin et al., 2019), pre-trained on massive amounts of text data, neural rerankers perform substantially better than IR-based methods (Nogueira and Cho, 2019; Akkalyoncu Yilmaz et al., 2019). Our methods fit within this group. 3 Method n Given a UMLS concept ci represented by query qi = {t1i , . . . , tN i }, where ti is an alias term for ci in UMLS, we use the English Wikipedia augmented with multilingual Wikidata as a document collection D = {d1 , . . . , d|D |}, to retrieve pag"
2020.coling-main.523,P16-1162,0,0.133624,"Missing"
2020.coling-main.523,W18-2306,0,0.109033,"corpora (Ormazabal et al., 2019), and the KG methods have not been tried on Wikidata because of its large scale and entity variety. String and semantic matching methods are based on similarity between the entity names or descriptions in the two knowledge-bases (KBs). Each entity in KB1 is used as a query against all the entities in KB2 using a scoring method such as cosine similarity or BM25. Entities are represented by bag of words or bag of character n-grams. The result of these cheap methods can be further improved by supervised (neural) ranking models (Guo et al., 2016; Rao et al., 2019; Wang et al., 2018). Particularly when query and candidate documents are represented by an encoder such as BERT (Devlin et al., 2019), pre-trained on massive amounts of text data, neural rerankers perform substantially better than IR-based methods (Nogueira and Cho, 2019; Akkalyoncu Yilmaz et al., 2019). Our methods fit within this group. 3 Method n Given a UMLS concept ci represented by query qi = {t1i , . . . , tN i }, where ti is an alias term for ci in UMLS, we use the English Wikipedia augmented with multilingual Wikidata as a document collection D = {d1 , . . . , d|D |}, to retrieve page dj matching concep"
2020.coling-main.523,E14-1049,0,\N,Missing
2020.coling-main.523,D19-1274,0,\N,Missing
2020.nlpcovid19-2.12,2020.nlpcovid19-acl.1,0,0.0599788,"Missing"
2020.nlpcovid19-2.12,N10-1012,1,0.61303,"coherent topics were further subdivided into specific and generic. This distinction is important as some topics can be highly coherent, but not informative. This is especially visible in datasets where the documents are homogeneous both in terms of style (scientific articles) and content (related to coronaviruses). For example, such topics as [research, study, approach ] or [coronavirus, virus, disease ] are coherent, but not representative of the content of the paper. In line with this, each topic was assigned one of three labels by the annotators: incoherent, specific, or generic. Following Newman et al. (2010), to evaluate coherence, annotators were asked to decide if each topic was meaningful and interpretable. To judge specificity, they were instructed to decide if a particular set of words is Word tokens Concepts Non-generic concepts Incoherent Generic Specific 11 3 2 7 6 3 7 16 20 Table 1: Number of incoherent, generic and specific topics identified in topic models of 25 topics built over different representations of the CORD-19 corpus likely to occur in the majority of COVID-19 related studies or not. Annotators were provided examples of incoherent, specific and generic topics related to COVID"
2020.nlpcovid19-2.12,W00-0901,0,0.519699,"Missing"
2020.nlpcovid19-2.12,C16-1050,0,0.0283846,"5 for each model, as the coherence structure (boilerplate sentences, section headings scores are close enough to each other at this point. such as Discussion, phrases such as in conclusion), or be included in informative sentences but not be 4.2 Document representation meaningful for the purposes of topic modelling. As We consider three different input representations adding all such words to a stop-word list would of the text for inferring the models: not be feasible, we filter the concepts based on their semantic type as defined in UMLS. Following • Word tokens: The input text was tokenised ShafieiBavani et al. (2016) and Plaza et al. (2011), using the NLTK Tokeniser6 . who used a similar approach to filter concepts for • Concepts: Documents are transformed into graph-based summarisation of medical documents, an unordered set of Unified Medical Language we exclude terms based on broad semantic types 4 5 including QUANTITATIVE CONCEPT (rate, unit), pypi.org/project/pycld2/ ncbi.nlm.nih.gov/books/NBK3827/table/pubmedhelp.T.stopwords/ 6 nltk.org 7 metamap.nlm.nih.gov Figure 1: Coherence scores for different representations of the CORD-19 corpus. (characteristics, different), TEMPORAL CONCEPT (year, recent), F"
2021.eacl-main.113,D13-1160,0,0.0343665,"on the same task and data are common in NLP. Held-out test sets are typically provided, enabling assessment of the generalizability of different methods to previously unseen data. These datasets have played a key role in driving progress in NLP, by defining focus tasks and by making annotated data available to the broader community, in particular in specialized domains such as biomedicine where data can be difficult to obtain, and quality data annotations require the detailed work of domain experts. Examples of tasks where benchmark data sets exist include open domain question answering (QA) (Berant et al., 2013; Joshi et al., 2017) and biomedical named entity recognition (Smith et al., 2008) . In the context of machine learning models, effectiveness is typically determined by the model’s ability to both memorize and generalize (Chatterjee, 2018). A model that has huge capacity to memorize will often work well in real world applications, particularly where large amounts of training data are available (Daelemans et al., 2005). The ability of a model to generalize relates to how well the model performs when it is applied on data that may be different from the data used to train the model, in terms of e"
2021.eacl-main.113,S17-2001,0,0.0314477,"ST SST SST SST Q 1 2 3 4 1 2 3 4 1 2 3 4 1 2 3 4 Min Max 0.0 26.3 26.3 31.6 31.6 38.3 38.3 100.0 37.9 56.7 56.8 68.2 68.2 78.5 78.6 99.8 6.3 20.1 20.1 25.7 25.7 31.9 31.9 75.0 0.0 36.5 36.5 43.6 43.6 53.5 53.5 100.0 P 69.8 74.5 78.3 83.0 90.8 93.3 95.1 97.1 44.5 42.3 46.5 46.1 90.8 91.3 91.2 88.0 R 82.0 85.9 86.4 88.9 91.8 94.4 96.1 97.4 81.4 82.5 85.3 85.2 95.2 96.2 97.3 98.1 F1 75.4 79.8 82.1 85.9 91.3 93.8 95.6 97.3 57.6 55.9 60.2 59.8 92.9 93.7 94.2 92.8 A surement is a challenging task in its own right, with a large body of literature and a number of shared tasks organized to address it (Cer et al., 2017; Wang et al., 2020; Karimi et al., 2015). More sophisticated methods for similarity measurement developed in these contexts could be incorporated into the framework for measuring similarity of data set splits; for simple leakage detection it is arguably adequate. However, sophisticated methods can also potentially lead to a chicken and egg problem, if we use a machine learning model to compute semantic similarity. The question of what level of similarity is acceptable is highly data and task-dependent. If the training data has good volume and variety, the trainingtest similarity will naturall"
2021.eacl-main.113,I17-2041,0,0.0278848,"g germany and eastern european jews might have changed 20th-century history is undermined by ahola ’s inadequate performance . of the unsung heroes of 20th century Table 1: Examples of train-test matches and the corresponding unigram similarity score. • BC3ACT - Biocreative III protein interaction classification (CLS) (Arighi et al., 2011) • SST2 - Stanford Sentiment Analysis Treebank (Socher et al., 2013) used to classify sentiments (CLS) in Glue (Wang et al., 2018) The AIMed dataset does not explicitly provide a test set and 10-fold cross validation is used for evaluation in previous works (Hsieh et al., 2017; Zhang et al., 2019). In this paper, we use two types of splits of AIMed to evaluate the impact of data leakage: AIMed (R) which Randomly splits the dataset into 10 folds; and AIMed (U) which splits the dataset into 10 folds such that the documents within each resultant split are Unique (according to the document ID) to other splits across each split. The document ID refers to the source document of a data instance, and data instances from the same source document have the same document ID, see example in Appendix A 4.2 Similarity measurement The pseudo code for measuring similarity is shown"
2021.eacl-main.113,P17-1147,0,0.0254427,"data are common in NLP. Held-out test sets are typically provided, enabling assessment of the generalizability of different methods to previously unseen data. These datasets have played a key role in driving progress in NLP, by defining focus tasks and by making annotated data available to the broader community, in particular in specialized domains such as biomedicine where data can be difficult to obtain, and quality data annotations require the detailed work of domain experts. Examples of tasks where benchmark data sets exist include open domain question answering (QA) (Berant et al., 2013; Joshi et al., 2017) and biomedical named entity recognition (Smith et al., 2008) . In the context of machine learning models, effectiveness is typically determined by the model’s ability to both memorize and generalize (Chatterjee, 2018). A model that has huge capacity to memorize will often work well in real world applications, particularly where large amounts of training data are available (Daelemans et al., 2005). The ability of a model to generalize relates to how well the model performs when it is applied on data that may be different from the data used to train the model, in terms of e.g. the distribution"
2021.eacl-main.113,J15-3006,0,0.0293138,"3 4 1 2 3 4 Min Max 0.0 26.3 26.3 31.6 31.6 38.3 38.3 100.0 37.9 56.7 56.8 68.2 68.2 78.5 78.6 99.8 6.3 20.1 20.1 25.7 25.7 31.9 31.9 75.0 0.0 36.5 36.5 43.6 43.6 53.5 53.5 100.0 P 69.8 74.5 78.3 83.0 90.8 93.3 95.1 97.1 44.5 42.3 46.5 46.1 90.8 91.3 91.2 88.0 R 82.0 85.9 86.4 88.9 91.8 94.4 96.1 97.4 81.4 82.5 85.3 85.2 95.2 96.2 97.3 98.1 F1 75.4 79.8 82.1 85.9 91.3 93.8 95.6 97.3 57.6 55.9 60.2 59.8 92.9 93.7 94.2 92.8 A surement is a challenging task in its own right, with a large body of literature and a number of shared tasks organized to address it (Cer et al., 2017; Wang et al., 2020; Karimi et al., 2015). More sophisticated methods for similarity measurement developed in these contexts could be incorporated into the framework for measuring similarity of data set splits; for simple leakage detection it is arguably adequate. However, sophisticated methods can also potentially lead to a chicken and egg problem, if we use a machine learning model to compute semantic similarity. The question of what level of similarity is acceptable is highly data and task-dependent. If the training data has good volume and variety, the trainingtest similarity will naturally be higher and so will the acceptable si"
2021.eacl-main.113,N19-1423,0,0.0298205,"rms of unigrams): [0-0.25),[0.250.50), [0.50-0.75), and [0.75-1.0]. For example, the test instances in the first interval are most different from the training set with a similarity less than 0.25. This method allows full control of the similarity of instances within each interval, but results in a different number of instances in each interval. Thus, we consider another scenario where we split the test set into 4 quartiles based on similarity ranking, so that the number of samples remain the same in each quartile but the threshold varies as a result. We finetune a BERT (base and cased) model (Devlin et al., 2019) for each dataset using their own training set and compare the performance of the finetuned BERT model on the four different test intervals and test quartiles. We compare the performances of AIMed (R) with AIMed (U) using 3 different models—Zhang et al. (2019) convolutional residual network, Hsieh et al. (2017) Bi-LSTM, and BioBERT (Lee et al., 2019). Following previous works, we preprocess the dataset and replace all non-participating proteins with neutral name PROTEIN, the participating entity pairs with PROTEIN1 and PROTEIN2, so the model only ever sees the pseudo protein names. 5 5.1 Resul"
2021.eacl-main.113,W18-5446,0,0.0737261,"Missing"
2021.eacl-main.113,P15-1129,0,0.0750966,"Missing"
2021.eacl-main.116,D16-1245,0,0.0492171,"Missing"
2021.eacl-main.116,P16-1061,0,0.0711216,"Missing"
2021.eacl-main.116,D14-1162,0,0.0838093,"Missing"
2021.eacl-main.116,N18-1202,0,0.0683807,"Missing"
D14-1096,N10-1083,0,0.0759904,"Missing"
D14-1096,J96-1002,0,0.0233732,"e DPM model while correcting for its inadequacies using direct supervision. We select only 1,000 annotated tokens to reflect a low resource scenario. A small supervised training sample is a more realistic form of supervision than a tag dictionary (noisy or otherwise). Although used in most prior work, a tag dictionary for a new language requires significant manual effort to construct. Garrette and Baldridge (2013) showed that a 1,000 token dataset could be collected very cheaply, requiring less than 2 hours of non-expert time. Our correction model makes use of a minimum divergence (MD) model (Berger et al., 1996), a variant of the maximum entropy model which biases the target distribution to be similar to a static reference distribution. The method has been used in several language applications including machine translation (Foster, 2000) and parsing (Plank and van Noord, 2008, Johnson and Riezler, 2000). These previous approaches have used various sources of reference distribution, e.g., incorporating information from a simpler model (Johnson and Riezler, 2000) or combining in- and outof-domain models (Plank and van Noord, 2008). Plank and van Noord (2008) concluded that this method for adding prior"
D14-1096,A00-1031,0,0.101349,"Missing"
D14-1096,W06-2920,0,0.500674,"than using a dictionary. We argue 1 http://www.wiktionary.org/ Das and Petrov (2011) Duong et al. (2013b) Li et al. (2012) T¨ackstr¨om et al. (2013) da 83.2 85.6 83.3 88.2 nl 79.5 84.0 86.3 85.9 de 82.8 85.4 85.4 90.5 el 82.5 80.4 79.2 89.5 it 86.8 81.4 86.5 89.3 pt 87.9 86.3 84.5 91.0 es 84.2 83.3 86.4 87.1 sv 80.5 81.0 86.1 88.9 Average 83.4 83.4 84.8 88.8 Table 1: Previously published token-level POS tagging accuracy for various models across 8 languages — Danish (da), Dutch (nl), German (de), Greek (el), Italian (it), Portuguese (pt), Spanish (es), Swedish (sv) — evaluated on CoNLL data (Buchholz and Marsi, 2006). that with a proper “guide”, we can take advantage of very limited annotated data. 2.1 Annotated data Our annotated data mainly comes from CoNLL shared tasks on dependency parsing (Buchholz and Marsi, 2006). The language specific tagsets are mapped into the universal tagset. We will use this annotated data mainly for evaluation. Table 2 shows the size of annotated data for each language. The 8 languages we are considering in this experiment are not actually resource-poor languages. However, running on these 8 languages makes our system comparable with previously proposed methods. Nevertheless"
D14-1096,D10-1056,0,0.024443,"Missing"
D14-1096,P11-1061,0,0.272017,"Missing"
D14-1096,I13-1177,1,0.896928,"Missing"
D14-1096,P13-2112,1,0.89119,"Missing"
D14-1096,P00-1006,0,0.0400144,"ictionary (noisy or otherwise). Although used in most prior work, a tag dictionary for a new language requires significant manual effort to construct. Garrette and Baldridge (2013) showed that a 1,000 token dataset could be collected very cheaply, requiring less than 2 hours of non-expert time. Our correction model makes use of a minimum divergence (MD) model (Berger et al., 1996), a variant of the maximum entropy model which biases the target distribution to be similar to a static reference distribution. The method has been used in several language applications including machine translation (Foster, 2000) and parsing (Plank and van Noord, 2008, Johnson and Riezler, 2000). These previous approaches have used various sources of reference distribution, e.g., incorporating information from a simpler model (Johnson and Riezler, 2000) or combining in- and outof-domain models (Plank and van Noord, 2008). Plank and van Noord (2008) concluded that this method for adding prior knowledge only works with high quality reference distributions, otherwise performance suffers. In contrast to these previous approaches, we consider the specific setting where both the learned model and the reference model so = P"
D14-1096,P08-1085,0,0.180683,"Missing"
D14-1096,W04-3229,0,0.406957,"Missing"
D14-1096,A00-2021,0,0.022186,"prior work, a tag dictionary for a new language requires significant manual effort to construct. Garrette and Baldridge (2013) showed that a 1,000 token dataset could be collected very cheaply, requiring less than 2 hours of non-expert time. Our correction model makes use of a minimum divergence (MD) model (Berger et al., 1996), a variant of the maximum entropy model which biases the target distribution to be similar to a static reference distribution. The method has been used in several language applications including machine translation (Foster, 2000) and parsing (Plank and van Noord, 2008, Johnson and Riezler, 2000). These previous approaches have used various sources of reference distribution, e.g., incorporating information from a simpler model (Johnson and Riezler, 2000) or combining in- and outof-domain models (Plank and van Noord, 2008). Plank and van Noord (2008) concluded that this method for adding prior knowledge only works with high quality reference distributions, otherwise performance suffers. In contrast to these previous approaches, we consider the specific setting where both the learned model and the reference model so = P (t|w) are both maximum entropy models. In this case we show that th"
D14-1096,2005.mtsummit-papers.11,0,0.110128,"Missing"
D14-1096,D12-1127,0,0.158782,"Missing"
D14-1096,J03-1002,0,0.00706494,"Missing"
D14-1096,E99-1010,0,0.0863202,"Missing"
D14-1096,petrov-etal-2012-universal,0,0.0846555,"the universal tagset. We will use this annotated data mainly for evaluation. Table 2 shows the size of annotated data for each language. The 8 languages we are considering in this experiment are not actually resource-poor languages. However, running on these 8 languages makes our system comparable with previously proposed methods. Nevertheless, we try to use as few resources as possible, in order to simulate the situation for resource-poor languages. Later in Section 6 we adapt the approach for Malagasy, a truly resource-poor language. 2.2 Universal tagset We employ the universal tagset from (Petrov et al., 2012) for our experiment. It consists of 12 common tags: NOUN, VERB, ADJ (adjective), ADV (adverb), PRON (pronoun), DET (determiner and article), ADP (preposition and postposition), CONJ (conjunctions), NUM (numerical), PRT (particle), PUNC (punctuation) and X (all other categories including foreign words and abbreviations). Petrov et al. (2012) provide the mapping from each language-specific tagset to the universal tagset. The idea of using the universal tagset is of great use in multilingual applications, enabling comparison across languages. However, the mapping is not always straightforward. Ta"
D14-1096,N13-1014,0,0.0990194,"cussed it makes many errors, due to invalid or inconsistent tag mappings, noisy alignments, and cross-linguistic syntactic divergence. However, our aim is to see how effectively we can exploit the strengths of the DPM model while correcting for its inadequacies using direct supervision. We select only 1,000 annotated tokens to reflect a low resource scenario. A small supervised training sample is a more realistic form of supervision than a tag dictionary (noisy or otherwise). Although used in most prior work, a tag dictionary for a new language requires significant manual effort to construct. Garrette and Baldridge (2013) showed that a 1,000 token dataset could be collected very cheaply, requiring less than 2 hours of non-expert time. Our correction model makes use of a minimum divergence (MD) model (Berger et al., 1996), a variant of the maximum entropy model which biases the target distribution to be similar to a static reference distribution. The method has been used in several language applications including machine translation (Foster, 2000) and parsing (Plank and van Noord, 2008, Johnson and Riezler, 2000). These previous approaches have used various sources of reference distribution, e.g., incorporating"
D14-1096,W08-1302,0,0.056461,"Missing"
D14-1096,P13-1057,0,0.203159,"Missing"
D14-1096,E14-1078,0,0.023072,"s. However, the mapping is not always straightforward. Table 2 shows the size of the annotated data for each language, the number of tags presented in the data, and the list of tags that are not matched. We can see that only 8 tags are presented in the annotated data for Danish, i.e, 4 tags (DET, PRT, PUNC, and NUM) are missing.2 Thus, a classifier using all 12 tags will be heavily penalized in the evaluation. Li et al. (2012) considered this problem and tried to manually modify the Danish mappings. Moreover, PRT is not really a universal tag since it only appears in 3 out of the 8 languages. Plank et al. (2014) pointed out that PRT often gets confused with ADP even in English. We will later show that the mapping problem causes substantial degradation in the performance of a POS tagger exploiting parallel data. The method we present here is more target-language oriented: our model is trained on the target language, in this way, only relevant information from the source language is retained. Thus, we automatically correct the mapping, and other incompatibilities arising from incorrect alignments and syntactic divergence between the source and target languages. Lang Size(k) # Tags da 94 8 nl 203 11 de"
D14-1096,W11-3603,0,0.0222103,"Missing"
D14-1096,Q13-1001,0,0.132253,"Missing"
D14-1096,N03-1033,0,0.0842473,"Missing"
D14-1096,P08-1086,0,0.101901,"Missing"
D14-1096,N01-1026,0,0.12215,"Missing"
K18-2008,D15-1159,0,0.0496596,"Missing"
K18-2008,K17-3002,0,0.0999746,"Missing"
K18-2008,P16-1231,0,0.0582228,"Missing"
K18-2008,P15-1033,0,0.0752376,"Missing"
K18-2008,D15-1041,0,0.219879,"re and combined features (McDonald and Pereira, 2006; Nivre et al., 2007b; Bohnet, 2010; Zhang and Nivre, 2011), while recent stateof-the-art models propose neural network architectures to handle feature-engineering (Dyer et al., 2015; Kiperwasser and Goldberg, 2016; Dozat and Manning, 2017; Ma and Hovy, 2017). Most traditional and neural network-based parsing models use automatically predicted POS tags as essential features. However, POS taggers are not perfect, resulting in error propagation problems. Some work has attempted to avoid using POS tags for dependency parsing (Dyer et al., 2015; Ballesteros et al., 2015; de Lhoneux et al., 2017), however, to achieve the strongest parsing scores these methods still require automatically assigned POS tags. Alternatively, joint POS tagging and dependency parsing has also attracted a lot of attention in NLP community as it could help improve both tagging and parsing results over independent modeling (Li et al., 2011; Hatori et al., 2011; Lee et al., 2011; Bohnet and Nivre, 2012; Zhang et al., 2015; Zhang and Weiss, 2016; Yang et al., 2018). In this paper, we present a novel neural network-based model for jointly learning POS tagging and dependency paring. Our jo"
K18-2008,C96-1058,0,0.666084,") = LSTMf (c1:k ) ◦ LSTMr (ck:1 ) 2.2 scorearc (i, j) Tagging component = MLParc We feed the sequence of vectors e1:n with an additional context position index i into another BiLSTM (BiLSTMpos ), resulting in latent feature vec(pos) tors v i each representing the ith word wi in s: (pos) vi = BiLSTMpos (e1:n , i) where (v i ∗ v j ) and |v i − v j |denote the elementwise product and the absolute element-wise difference, respectively; and v i and v j are correspondingly the latent feature vectors associating to the ith and j th words in s, computed by Equation 5. Given the arc scores, we use the Eisner (1996)’s decoding algorithm to find the highest scoring projective parse tree: X score(s) = argmax scorearc (h, m) (7) (2) We use a MLP with softmax output (MLPpos ) on top of the BiLSTMpos to predict POS tag of each word in s. The number of nodes in the output layer of this MLPpos is the number of POS tags. (pos) Given v i , we compute an output vector as: ϑi = (pos) MLPpos (v i ) yˆ∈Y(s) (h,m)∈ˆ y where Y(s) is the set of all possible dependency trees for the input sentence s while scorearc (h, m) measures the score of the arc between the head hth word and the modifier mth word in s. Following Kip"
K18-2008,K18-2002,0,0.041237,"Missing"
K18-2008,P09-1087,0,0.158772,"Missing"
K18-2008,D12-1133,0,0.0664697,"Missing"
K18-2008,W06-2920,0,0.235373,"Missing"
K18-2008,H05-1091,0,0.364375,"Missing"
K18-2008,D14-1082,0,0.0650247,"computational resource, for experiments presented in Section 3, we perform a minimal grid search of hyper-parameters to select the number of BiLSTMpos and BiLSTMdep layers from {1, 2} and the size of LSTM hidden states in each layer from {128, 256}. For experiments presented in sections 4 and 5, we fix the number of BiLSTM layers at 2 and the size of hidden states at 128. 2.5 Experimental setup: We evaluate our model using the English WSJ Penn treebank (Marcus et al., 1993). We follow a standard data split to use sections 02-21 for training, Section 22 for development and Section 23 for test (Chen and Manning, 2014), employing the Stanford conversion toolkit v3.3.0 to generate dependency trees with Stanford basic dependencies (de Marneffe and Manning, 2008). Word embeddings are initialized by 100dimensional GloVe word vectors pre-trained on Wikipedia and Gigaword (Pennington et al., 2014).2 As mentioned in Section 2.5, we perform a minimal grid search of hyper-parameters and find that the highest mixed accuracy on the development set is obtained when using 2 BiLSTM layers and 256-dimensional LSTM hidden states (in Table 1, we present scores obtained on the development set when using 2 BiLSTM layers). 3 I"
K18-2008,D16-1257,0,0.0553598,"Missing"
K18-2008,D17-1206,0,0.0633592,"Missing"
K18-2008,P04-1054,0,0.263556,"Missing"
K18-2008,K17-3022,0,0.0964793,"Missing"
K18-2008,I11-1136,0,0.0682876,"Missing"
K18-2008,P15-1162,0,0.0672387,"Missing"
K18-2008,K17-3014,1,0.812718,"Missing"
K18-2008,Q16-1023,0,0.59467,"Australia {dqnguyen, karin.verspoor}@unimelb.edu.au Abstract (Reddy et al., 2016) and machine translation (Galley and Manning, 2009). In general, dependency parsing models can be categorized as graph-based (McDonald et al., 2005) and transition-based (Yamada and Matsumoto, 2003; Nivre, 2003). Most traditional graph- or transition-based models define a set of core and combined features (McDonald and Pereira, 2006; Nivre et al., 2007b; Bohnet, 2010; Zhang and Nivre, 2011), while recent stateof-the-art models propose neural network architectures to handle feature-engineering (Dyer et al., 2015; Kiperwasser and Goldberg, 2016; Dozat and Manning, 2017; Ma and Hovy, 2017). Most traditional and neural network-based parsing models use automatically predicted POS tags as essential features. However, POS taggers are not perfect, resulting in error propagation problems. Some work has attempted to avoid using POS tags for dependency parsing (Dyer et al., 2015; Ballesteros et al., 2015; de Lhoneux et al., 2017), however, to achieve the strongest parsing scores these methods still require automatically assigned POS tags. Alternatively, joint POS tagging and dependency parsing has also attracted a lot of attention in NLP com"
K18-2008,W03-3017,0,0.364583,"Missing"
K18-2008,D16-1180,0,0.02384,"Missing"
K18-2008,P11-1089,0,0.126896,"Missing"
K18-2008,D11-1109,0,0.0792604,"Missing"
K18-2008,I17-1007,0,0.0345348,"Missing"
K18-2008,D14-1162,0,0.0813878,"n sections 4 and 5, we fix the number of BiLSTM layers at 2 and the size of hidden states at 128. 2.5 Experimental setup: We evaluate our model using the English WSJ Penn treebank (Marcus et al., 1993). We follow a standard data split to use sections 02-21 for training, Section 22 for development and Section 23 for test (Chen and Manning, 2014), employing the Stanford conversion toolkit v3.3.0 to generate dependency trees with Stanford basic dependencies (de Marneffe and Manning, 2008). Word embeddings are initialized by 100dimensional GloVe word vectors pre-trained on Wikipedia and Gigaword (Pennington et al., 2014).2 As mentioned in Section 2.5, we perform a minimal grid search of hyper-parameters and find that the highest mixed accuracy on the development set is obtained when using 2 BiLSTM layers and 256-dimensional LSTM hidden states (in Table 1, we present scores obtained on the development set when using 2 BiLSTM layers). 3 Implementation details Our model is released as jPTDP v2.0, available at https://github.com/datquocnguyen/ jPTDP. Our jPTDP v2.0 is implemented using DY N ET v2.0 (Neubig et al., 2017) with a fixed random seed.1 Word embeddings are initialized either randomly or by pre-trained w"
K18-2008,J93-2004,0,0.0605241,"dimensional character embeddings and 100dimensional POS tag embeddings. We also fix the number of hidden nodes in MLPs at 100. Due to limited computational resource, for experiments presented in Section 3, we perform a minimal grid search of hyper-parameters to select the number of BiLSTMpos and BiLSTMdep layers from {1, 2} and the size of LSTM hidden states in each layer from {128, 256}. For experiments presented in sections 4 and 5, we fix the number of BiLSTM layers at 2 and the size of hidden states at 128. 2.5 Experimental setup: We evaluate our model using the English WSJ Penn treebank (Marcus et al., 1993). We follow a standard data split to use sections 02-21 for training, Section 22 for development and Section 23 for test (Chen and Manning, 2014), employing the Stanford conversion toolkit v3.3.0 to generate dependency trees with Stanford basic dependencies (de Marneffe and Manning, 2008). Word embeddings are initialized by 100dimensional GloVe word vectors pre-trained on Wikipedia and Gigaword (Pennington et al., 2014).2 As mentioned in Section 2.5, we perform a minimal grid search of hyper-parameters and find that the highest mixed accuracy on the development set is obtained when using 2 BiL"
K18-2008,P16-2067,0,0.09422,"e s consisting of n word tokens w1 , w2 , ..., wn , we represent each ith word wi in s by a vector ei . We obtain ei by concate(W) nating word embedding ewi and character-level (C) word embedding ewi : ei = e(wWi ) ◦ e(wCi) Our joint model (1) Here, each word type w in the training data is rep(W) resented by a real-valued word embedding ew . Given the word type w consisting of k characters w = c1 c2 ...ck where each jth character in w is represented by a character embedding cj , we use a sequence BiLSTM (BiLSTMseq ) to learn its character-level vector representation (Ballesteros et al., 2015; Plank et al., 2016). The input to BiLSTMseq is the sequence of k character embeddings c1:k , and the output is a concatenation of outputs of a forward LSTM (LSTMf ) reading This section presents our model for joint POS tagging and graph-based dependency parsing. Figure 1 illustrates the architecture of our joint model which can be viewed as a two-component mixture of a tagging component and a parsing component. Given word tokens in an input sentence, the tagging component uses a BiLSTM to learn “latent” feature vectors representing these word tokens. Then the tagging component feeds these feature vectors into a"
K18-2008,P05-1012,0,0.198355,"n, using only the gold labeled parse tree. Our parsing component can be viewed as an extension of the BIST graph-based dependency model (Kiperwasser and Goldberg, 2016), where we additionally incorporate the character-level vector representations of words. (4) We feed the sequence of vectors x1:n with an additional index i into a BiLSTM (BiLSTMdep ), resulting in latent feature vectors v i as follows: v i = BiLSTMdep (x1:n , i) (6)  v i ◦ v j ◦ (v i ∗ v j ) ◦ |v i − v j | (5) 2.4 Based on latent feature vectors v i , we follow a common arc-factored parsing approach to decode dependency arcs (McDonald et al., 2005). In particular, a dependency tree can be formalized as a directed graph. An arc-factored parsing approach learns the scores of the arcs in the graph (K¨ubler Joint model training The training objective loss of our joint model is the sum of the POS tagging loss LPOS , the structure loss LARC and the relation labeling loss LREL : L = LPOS + LARC + LREL 83 (9) The model parameters, including word embeddings, character embeddings, POS embeddings, three one-hidden-layer MLPs and three BiLSTMs, are learned to minimize the sum L of the losses. Most neural network-based joint models for POS tagging a"
K18-2008,E06-1011,0,0.117076,"Missing"
K18-2008,N15-1005,0,0.207378,"Missing"
K18-2008,P16-1147,0,0.0779833,"Missing"
K18-2008,P11-2033,0,0.139561,"Missing"
K18-2008,K17-3009,0,0.0588227,"Missing"
K18-2008,I05-2038,0,0.0287423,"odel is trained using a fixed set of hyper-parameters as mentioned in Section 2.5. 5 UniMelb in the EPE 2018 campaign scores. General background can be also found in the first EPE edition 2017 (Oepen et al., 2017). Unlike EPE 2017, the EPE 2018 campaign limited the training data to the English UD treebanks only. We unfortunately were unaware of this restriction during development of our model. Thus, we trained a jPTDP v2.0 model on dependency trees generated with the Stanford basic dependencies on a combination of the WSJ treebank, sections 02-21, and the training split of the GENIA treebank (Tateisi et al., 2005). We used the fixed set of hyper-parameters as used for the CoNLL 2018 shared task as mentioned in Section 2.5.11 We then submitted the parsing outputs by runOur UniMelb team also participated with jPTDP v2.0 in the 2018 Extrinsic Parser Evaluation (EPE) campaign (Fares et al., 2018).10 The EPE 2018 campaign runs in collaboration with the CoNLL 2018 shared task, which aims to evaluate dependency parsers by comparing their performance on three downstream tasks: biomedical event extraction (Bj¨orne et al., 2017), negation resolution (Lapponi et al., 2017) and opinion analysis (Johansson, 2017)."
K18-2008,N03-1033,0,0.344113,"Missing"
K18-2008,P15-1032,0,0.0383261,"Missing"
K18-2008,W03-3023,0,0.434999,"Missing"
K18-2008,K18-2001,0,0.0668916,"Missing"
K18-2008,K17-3001,0,0.0683429,"Missing"
K18-2008,E17-1063,0,0.0398596,"Missing"
L18-1043,W14-3302,0,0.0869344,"Missing"
L18-1043,federmann-2010-appraise,0,0.42498,"slation, others were created more freely and the content in one language could be structured differently in the other language. We made the hypothesis that documents could nonetheless be aligned at the sentence level and we relied on automatic tools for performing the alignment. We identified alignment tools based on an evaluation of alignment for literary texts which is a genre that also features fuzzy alignment(Xu et al., 2015). Quality checking. After automatically aligning the sentences of the documents, we manually checked a sample of our corpora. This was carried out using the Appraise (Federmann, 2010) tool, and we evaluated whether the aligned sentences where correct or whether more information was available in one language or the other. Native speakers of each foreign language were responsible for this task. 4. Application to Three Biomedical Corpora Here we describe the three corpora that we developed and highlight the differences regarding the particular tools that we used for the various steps above. EDP We identified five open access CC-BY journals, referenced EDP Sciences4 as having content in French and in English: the articles were originally written in French but the journals also"
L18-1043,hellrich-etal-2014-collaboratively,0,0.0539987,"Work One of the first efforts that involved the development of large-scale shareable parallel corpora for the biomedical domain was the OPUS collection that contained medical documents from the European Medicines Agency (EMEA) (Tiedemann, 2012)2 . A number of biomedical parallel (Widdows et al., 2002; Ozdowska et al., 2005; Deleger et al., 2009) and comparable corpora (Chiao and Zweigenbaum, 2004) have been used for terminology translation only. Similarly, the Mantra project (Kors et al., 2015; 1 2 286 http://github.com/biomedical-translation-corpora/corpora http://opus.lingfil.uu.se/EMEA.php Hellrich et al., 2014) provided corpora of biomedical articles automatically annotated for named-entity recognition for English, Spanish, French, German and Dutch. The corpora included MEDLINE titles, EMEA documents and patents in the biomedical field. While the goal of this project was to leverage annotation transfer from English to other languages to expand terminology coverage in languages other than English, to our knowledge, the corpus has not been used for machine translation. After general purpose machine translation systems were found to perform poorly on medical text (Zeng-Treitler et al., 2010), the use o"
L18-1043,2013.mtsummit-papers.10,0,0.0134367,"MT17). Then, we manually validated sentence segmentation in both languages in order to create a reference corpus that may be used to train and evaluate sentence segmentation tools. Therefore, Updated versions of the corpus reflect the manual sentence segmentation. Table 2 presents detailed statistics of the contents of the biomedical parallel corpora that we developped. Table 3 presents an overview of the corpora with the training and test set splits that were offered thoroughout the WMT campaigns. Sentence Alignment: GMA was used for Scielo and ReBEC. Due to difficulties to install GMA, Yasa(Lamraoui and Langlais, 2013) was used for EDP; however, Yasa may be limited to the language pair en/fr. We can refer readers to (Xu et al., 2015) for a discussion and evaluation of alignment tools for a specialized domain (literary texts). Nevertheless, both tools provided good automatic alignments (?). Additionally, GMA was used for two languages (es and pt) and two document types (scientific publications and clinical trials). 5.2. 6.2. 5. 5.1. Results Datasets Descriptive Statistics Quality Assessment We also provide a summary of the correct alignment rate for the various corpora, as shown in Table 4. The alignment was"
L18-1043,L16-1470,1,0.833842,"Missing"
L18-1043,W17-2507,1,0.921227,"ument across languages. However, despite its importance for the general population and researchers, there are very few parallel and comparable corpora specific for this domain. In this paper, we present an overview of the stateof-the-art on parallel and comparable corpora for the biomedical domain. In a scoping review of existing resources, we characterize the resources available by language pairs and document type and provide pointers to more in-depth descriptions of the resources. Additionally, we present the parallel corpora that we assembled and built, such as EDP (French/English), ReBEC (Neves, 2017) (Portuguese/English) and Scielo (Neves et al., 2016) (French/English, Portuguese/English and Spanish/English). For the latter, we provide details on the corpus construction, insights on the data and their utilization for the biomedical task (Bojar et al., 2016; Jimeno Yepes et al., 2017) of the Conference for Machine Translation (WMT). All corpora are available in our repository in GitHub1 . 2. Related Work One of the first efforts that involved the development of large-scale shareable parallel corpora for the biomedical domain was the OPUS collection that contained medical documents from the"
L18-1043,tiedemann-2012-parallel,0,0.541082,"eves et al., 2016) (French/English, Portuguese/English and Spanish/English). For the latter, we provide details on the corpus construction, insights on the data and their utilization for the biomedical task (Bojar et al., 2016; Jimeno Yepes et al., 2017) of the Conference for Machine Translation (WMT). All corpora are available in our repository in GitHub1 . 2. Related Work One of the first efforts that involved the development of large-scale shareable parallel corpora for the biomedical domain was the OPUS collection that contained medical documents from the European Medicines Agency (EMEA) (Tiedemann, 2012)2 . A number of biomedical parallel (Widdows et al., 2002; Ozdowska et al., 2005; Deleger et al., 2009) and comparable corpora (Chiao and Zweigenbaum, 2004) have been used for terminology translation only. Similarly, the Mantra project (Kors et al., 2015; 1 2 286 http://github.com/biomedical-translation-corpora/corpora http://opus.lingfil.uu.se/EMEA.php Hellrich et al., 2014) provided corpora of biomedical articles automatically annotated for named-entity recognition for English, Spanish, French, German and Dutch. The corpora included MEDLINE titles, EMEA documents and patents in the biomedica"
L18-1043,widdows-etal-2002-using,0,0.393247,"h and Spanish/English). For the latter, we provide details on the corpus construction, insights on the data and their utilization for the biomedical task (Bojar et al., 2016; Jimeno Yepes et al., 2017) of the Conference for Machine Translation (WMT). All corpora are available in our repository in GitHub1 . 2. Related Work One of the first efforts that involved the development of large-scale shareable parallel corpora for the biomedical domain was the OPUS collection that contained medical documents from the European Medicines Agency (EMEA) (Tiedemann, 2012)2 . A number of biomedical parallel (Widdows et al., 2002; Ozdowska et al., 2005; Deleger et al., 2009) and comparable corpora (Chiao and Zweigenbaum, 2004) have been used for terminology translation only. Similarly, the Mantra project (Kors et al., 2015; 1 2 286 http://github.com/biomedical-translation-corpora/corpora http://opus.lingfil.uu.se/EMEA.php Hellrich et al., 2014) provided corpora of biomedical articles automatically annotated for named-entity recognition for English, Spanish, French, German and Dutch. The corpora included MEDLINE titles, EMEA documents and patents in the biomedical field. While the goal of this project was to leverage a"
N19-3007,J92-4003,0,0.145983,"the choice of models and the features considered. In model selection, the multi-type relation extraction task can be assigned to several independent binary classifiers, each making the decision of whether a certain type of relation exist or not. A basic binary classifier such as logistic regression with ridge regularization is capable of performing relation extraction in this scenario. Panyam et al. (2016) used support vector machines (SVM) with a dependency graph kernel to perform relation extraction on two biomedical relation extraction tasks, showing competitive results. Brown Clustering (Brown et al., 1992) is a hierarchical approach to clustering words into classes through maximizing mutual information of bi-grams; it showed competitive performances in many NLP tasks (Turian et al., 2010).Nguyen and Verspoor (2018) implemented a method using character-based word embeddings which can capture unknown words within the context, coupled with CNN and LSTM neural network models. This approach obtained state-of-the-art performance in extracting chemical-disease relations on the BioCreative-V CDR corpus (Li et al., 2016). For feature engineering, text features can generally be divided into the two categ"
N19-3007,D17-1191,0,0.0176613,"uting and Information Systems The University of Melbourne, Australia jiyuc@student.unimelb.edu.au, {karin.verspoor, zenan.zhai}@unimelb.edu.au Abstract if two entities with type TestName and TestResult, respectively, are observed in a given sentence, it is likely that a relation of type TestFinding exists between them. However, construction of highprecision rules defining relevant contexts is timeconsuming and expensive, requiring extensive effort from domain experts. The state-of-the-art machine learning algorithms such as neural network models (Nguyen and Grishman, 2016; Ammar et al., 2017; Huang and Wang, 2017) may over-fit in performing relation extraction in this context, due to a limited quantity of training instances. In this work, we experiment with two automatic approaches to semantic relation extraction applied to a small corpus consisting of breast cancer follow-up treatment letters (Pitson et al., 2017), comparing a simple rule-based co-occurrence approach to machine learning classifiers. The first approach, simple co-occurrence (Verspoor et al., 2016), is based on the assumption that most relevant relations are intra-sentential, that is, the relation between a pair of named entities is exp"
N19-3007,S17-2097,0,0.0238696,"Zhai School of Computing and Information Systems The University of Melbourne, Australia jiyuc@student.unimelb.edu.au, {karin.verspoor, zenan.zhai}@unimelb.edu.au Abstract if two entities with type TestName and TestResult, respectively, are observed in a given sentence, it is likely that a relation of type TestFinding exists between them. However, construction of highprecision rules defining relevant contexts is timeconsuming and expensive, requiring extensive effort from domain experts. The state-of-the-art machine learning algorithms such as neural network models (Nguyen and Grishman, 2016; Ammar et al., 2017; Huang and Wang, 2017) may over-fit in performing relation extraction in this context, due to a limited quantity of training instances. In this work, we experiment with two automatic approaches to semantic relation extraction applied to a small corpus consisting of breast cancer follow-up treatment letters (Pitson et al., 2017), comparing a simple rule-based co-occurrence approach to machine learning classifiers. The first approach, simple co-occurrence (Verspoor et al., 2016), is based on the assumption that most relevant relations are intra-sentential, that is, the relation between a pair o"
N19-3007,P14-5010,0,0.00955529,"of-words (count-based), lemmas (base, uninflected form of a noun or verb), algebraic expressions, named entity type (derived from the Data Preparation Considering the semantic variation in the texts, and the small number of examples, training several independent binary classifiers is more robust for mining individual type of semantic relation patterns. Therefore, we transform the original dataset into 16 independent subsets, by grouping 1 http://anoncvs.postgresql.org/ cvsweb.cgi/pgsql/src/backend/snowball/ stopwords/ 46 gold-standard), POS tags and dependency parse based on Stanford CoreNLP (Manning et al., 2014), and a transformation from dependency parse tree to graph using NetworkX (Hagberg et al., 2008) where edges are dependencies and nodes are tokens/labels. similarity based on shared surrounding context. The size of the surrounding context, known as window size control, varies the representation of word embeddings from more semantic (shorter window size) to more syntactic (longer window size). Synonyms can be identified by identifying two words with similar embeddings, based on cosine similarity measurement. Overfitting can occur for word embeddings, where a training corpus is not large enough"
N19-3007,C04-1070,0,0.170937,"Missing"
N19-3007,P10-1040,0,0.0823282,"decision of whether a certain type of relation exist or not. A basic binary classifier such as logistic regression with ridge regularization is capable of performing relation extraction in this scenario. Panyam et al. (2016) used support vector machines (SVM) with a dependency graph kernel to perform relation extraction on two biomedical relation extraction tasks, showing competitive results. Brown Clustering (Brown et al., 1992) is a hierarchical approach to clustering words into classes through maximizing mutual information of bi-grams; it showed competitive performances in many NLP tasks (Turian et al., 2010).Nguyen and Verspoor (2018) implemented a method using character-based word embeddings which can capture unknown words within the context, coupled with CNN and LSTM neural network models. This approach obtained state-of-the-art performance in extracting chemical-disease relations on the BioCreative-V CDR corpus (Li et al., 2016). For feature engineering, text features can generally be divided into the two categories of lexical features and syntactic features. Typical features used in other relation extraction tasks are summarized here. binary tasks. We build on a bag-of-concepts (BoC) (Sahlgre"
N19-3007,W18-2314,1,0.910737,"a certain type of relation exist or not. A basic binary classifier such as logistic regression with ridge regularization is capable of performing relation extraction in this scenario. Panyam et al. (2016) used support vector machines (SVM) with a dependency graph kernel to perform relation extraction on two biomedical relation extraction tasks, showing competitive results. Brown Clustering (Brown et al., 1992) is a hierarchical approach to clustering words into classes through maximizing mutual information of bi-grams; it showed competitive performances in many NLP tasks (Turian et al., 2010).Nguyen and Verspoor (2018) implemented a method using character-based word embeddings which can capture unknown words within the context, coupled with CNN and LSTM neural network models. This approach obtained state-of-the-art performance in extracting chemical-disease relations on the BioCreative-V CDR corpus (Li et al., 2016). For feature engineering, text features can generally be divided into the two categories of lexical features and syntactic features. Typical features used in other relation extraction tasks are summarized here. binary tasks. We build on a bag-of-concepts (BoC) (Sahlgren and C¨oster, 2004) approa"
N19-3007,U16-1007,1,0.769547,"017), comparing a simple rule-based co-occurrence approach to machine learning classifiers. The first approach, simple co-occurrence (Verspoor et al., 2016), is based on the assumption that most relevant relations are intra-sentential, that is, the relation between a pair of named entities is expressed within the scope of a single sentence. However, some relations may be expressed across sentence boundaries, and thus a single sentence may not be the ideal choice of scope, as shown in prior work that considers inter-sentential relations (also known as non-sentence or cross-sentence relations) (Panyam et al., 2016; Peng et al., 2017). We extend the co-occurrence approach to allow explicit adjustment of context window size, from one to two sentences, a method called WindowBounded Co-occurrence (WBC). The best window size for a given relation is identified by choosing the one which produces the highest score under F1 -measure on a development set. The second approach is based on supervised binary classification. We transform the multirelation extraction task into several independent This paper focuses on a traditional relation extraction task in the context of limited annotated data and a narrow knowledg"
N19-3007,Q17-1008,0,0.0166572,"ple rule-based co-occurrence approach to machine learning classifiers. The first approach, simple co-occurrence (Verspoor et al., 2016), is based on the assumption that most relevant relations are intra-sentential, that is, the relation between a pair of named entities is expressed within the scope of a single sentence. However, some relations may be expressed across sentence boundaries, and thus a single sentence may not be the ideal choice of scope, as shown in prior work that considers inter-sentential relations (also known as non-sentence or cross-sentence relations) (Panyam et al., 2016; Peng et al., 2017). We extend the co-occurrence approach to allow explicit adjustment of context window size, from one to two sentences, a method called WindowBounded Co-occurrence (WBC). The best window size for a given relation is identified by choosing the one which produces the highest score under F1 -measure on a development set. The second approach is based on supervised binary classification. We transform the multirelation extraction task into several independent This paper focuses on a traditional relation extraction task in the context of limited annotated data and a narrow knowledge domain. We explore"
N19-3007,D14-1162,0,0.086481,"rrounding context. The size of the surrounding context, known as window size control, varies the representation of word embeddings from more semantic (shorter window size) to more syntactic (longer window size). Synonyms can be identified by identifying two words with similar embeddings, based on cosine similarity measurement. Overfitting can occur for word embeddings, where a training corpus is not large enough or a corpus is limited to a narrow domain of knowledge. Therefore, instead of training word embeddings on our corpus, we use two publicly available pre-trained word embeddings, GloVe (Pennington et al., 2014) and a Wikipedia-PubMedPMC embedding (Moen and Ananiadou, 2013) to capture more clinically relevant vocabulary. The vocabulary of word embeddings denotes the total number of words that are represented. In our experiment, only the top 20,000 most frequent lemmas are selected. Gensim (Rehurek and Sojka, 2010) is used to find the synonyms of a lemma from the vocabulary by measuring similarity between GloVe word vectors. • ASM features The classical ASM measurement was developed by Liu et al. (2013), and was later extended to kernel method by Panyam et al. (2016). The ASM kernel was applied to the"
S16-1120,P03-1054,0,0.029583,"LexiM works to align chunks which have lexical overlap, while string similarity and semantic distance work in two ways. The first way is to align chunks that that cannot be handled with those rules and follow by assining labels to the aligned chunks. The second way is for the chunks that are aligned through LexiM, the string distance and semantic similarity rule will provide similarity and relatedness labels. There are existing tools which perform tasks such as string distance measurement like SecondString (Cohen et al., 2003) and stringmetric (Madden, 2013), PoS taggers like Stanford Parser (Klein and Manning, 2003), and dictionary for synonymous words like WordNet (Fellbaum, 1998). Cohen et al. (2003) shows that Jaro-Winkler is an effective string distance metric for name matching task. Liu et al. (2010) has shown that TESLA, a similarity metric that considers both PoS tags and semantic equivalence (based on WordNet synsets), is effective in the task of automatic evaluation of machine translation in English language. We used Jaro-Winkler proximity (SecondString implementation) for string distance measurement, and TESLA for semantic similarity measurement. We analyzed the chunks in terms of their string"
S16-1120,W10-1754,0,0.027913,"d follow by assining labels to the aligned chunks. The second way is for the chunks that are aligned through LexiM, the string distance and semantic similarity rule will provide similarity and relatedness labels. There are existing tools which perform tasks such as string distance measurement like SecondString (Cohen et al., 2003) and stringmetric (Madden, 2013), PoS taggers like Stanford Parser (Klein and Manning, 2003), and dictionary for synonymous words like WordNet (Fellbaum, 1998). Cohen et al. (2003) shows that Jaro-Winkler is an effective string distance metric for name matching task. Liu et al. (2010) has shown that TESLA, a similarity metric that considers both PoS tags and semantic equivalence (based on WordNet synsets), is effective in the task of automatic evaluation of machine translation in English language. We used Jaro-Winkler proximity (SecondString implementation) for string distance measurement, and TESLA for semantic similarity measurement. We analyzed the chunks in terms of their string distance and semantic similarity scores, relating the scores to the similarity and relatedness types and scores in the annotated data. We identified the lowest, average and highest values for t"
S17-2003,S17-2044,0,0.0542572,"Missing"
S17-2003,N10-1145,0,0.016031,"es used by these systems and provides further discussion. Finally, Section 6 presents the main conclusions. 2 Question-answer similarity has been a subtask (subtask A) of our task in its two previous editions (Nakov et al., 2015, 2016b). This is a wellresearched problem in the context of general question answering. One research direction has been to try to match the syntactic structure of the question to that of the candidate answer. For example, Wang et al. (2007) proposed a probabilistic quasi-synchronous grammar to learn syntactic transformations from the question to the candidate answers. Heilman and Smith (2010) used an algorithm based on Tree Edit Distance (TED) to learn tree transformations in pairs. Wang and Manning (2010) developed a probabilistic model to learn tree-edit operations on dependency parse trees. Yao et al. (2013) applied linear chain conditional random fields (CRFs) with features derived from TED to learn associations between questions and candidate answers. Moreover, syntactic structure was central for some of the top systems that participated in SemEval-2016 Task 3 (Filice et al., 2016; Barr´on-Cede˜no et al., 2016). Related Work The first step to automatically answer questions on"
S17-2003,C16-2001,1,0.881977,"Missing"
S17-2003,S15-2035,0,0.0226369,"andidate answer. Similarly, (Guzm´an et al., 2016a,b) ported an entire machine translation evaluation framework (Guzm´an et al., 2015) to the CQA problem. Using information about the answer thread is another important direction, which has been explored mainly to address Subtask A. In the 2015 edition of the task, the top participating systems used thread-level features, in addition to local features that only look at the question–answer pair. For example, the second-best team, HITSZ-ICRC, used as a feature the position of the comment in the thread, such as whether the answer is first or last (Hou et al., 2015). Similarly, the third-best team, QCRI, used features to model a comment in the context of the entire comment thread, focusing on user interaction (Nicosia et al., 2015). Finally, the fifth-best team, ICRC-HIT, treated the answer selection task as a sequence labeling problem and proposed recurrent convolutional neural networks to recognize good comments (Zhou et al., 2015b). In follow-up work, Zhou et al. (2015a) included long-short term memory (LSTM) units in their convolutional neural network to model the classification sequence for the thread, and Barr´on-Cede˜no et al. (2015) exploited the"
S17-2003,K15-1032,1,0.0248911,"to make more consistent global decisions about the goodness of the answers in the thread. They modeled the relations between pairs of comments at any distance in the thread, and combined the predictions of local classifiers using graph-cut and Integer Linear Programming. In follow up work, Joty et al. (2016) proposed joint learning models that integrate inference within the learning process using global normalization and an Ising-like edge potential. 5 https://github.com/tbmihailov/ semeval2016-task3-cqa 6 Using a heuristic that if several users call somebody a troll, then s/he should be one (Mihaylov et al., 2015a,b; Mihaylov and Nakov, 2016a; Mihaylov et al., 2017b). 30 Category Original Questions Train+Dev+Test from SemEval-2015 – Train(1,2)+Dev+Test from SemEval-2016 (200+67)+50+70 2,480+291+319 – – – (1,999+670)+500+700 (181+54)+59+81 (606+242)+155+152 (1,212+374)+286+467 880 24 139 717 – (19,990+6,700)+5,000+7,000 8,800 – – – (1,988+849)+345+654 (16,319+5,154)+4,061+5,943 (1,683+697)+594+403 246 8,291 263 (14,110+3,790)+2,440+3,270 2,930 (5,287+1,364)+818+1,329 (6,362+1,777)+1,209+1,485 (2,461+649)+413+456 1,523 1,407 0 Related Questions – Perfect Match – Relevant – Irrelevant Related Comments (w"
S17-2003,S17-2009,0,0.0610799,"Missing"
S17-2003,S15-2036,1,0.824235,"Missing"
S17-2003,J11-2003,0,0.0485113,"ting systems across all three subtasks. This includes fine-tuned word embeddings5 (Mihaylov and Nakov, 2016b); features modeling text complexity, veracity, and user trollness6 (Mihaylova et al., 2016); sentiment polarity features (Nicosia et al., 2015); and PMI-based goodness polarity lexicons (Balchev et al., 2016; Mihaylov et al., 2017a). Yet another research direction has been on using machine translation models as features for question-answer similarity (Berger et al., 2000; Echihabi and Marcu, 2003; Jeon et al., 2005; Soricut and Brill, 2006; Riezler et al., 2007; Li and Manandhar, 2011; Surdeanu et al., 2011; Tran et al., 2015; Hoogeveen et al., 2016a; Wu and Zhang, 2016), e.g., a variation of IBM model 1 (Brown et al., 1993), to compute the probability that the question is a “translation” of the candidate answer. Similarly, (Guzm´an et al., 2016a,b) ported an entire machine translation evaluation framework (Guzm´an et al., 2015) to the CQA problem. Using information about the answer thread is another important direction, which has been explored mainly to address Subtask A. In the 2015 edition of the task, the top participating systems used thread-level features, in addition to local features tha"
S17-2003,D16-1244,0,0.0146703,"Missing"
S17-2003,S17-2059,0,0.0505879,"Missing"
U13-1012,W07-1013,0,0.0267274,"spawned an increasing number of NLP tools for content analysis that help researchers and practitioners access the latest developments in their fields. Examples of these tools include: BANNER – a Named Entity Recognizer (NER) for the biomedical domain (Leaman and Gonzalez, 2008); ABNER – a NER for molecular biology (Settles, 2004); and Whatizit – a Web service which provides functionality to perform text-mining tasks (RebholzSchuhmann et al., 2008). These tools in turn require the development of annotated training corpora, e.g., (Kim et al., 2003; Rosario and Hearst, 2004; Kulick et al., 2004; Pestian et al., 2007; Jimeno-Yepes et al., 2008; Bada et al., 2012). Studies have been conducted to examine the performance of different NLP tools on a single corpus, e.g., (Jacob et al., 2013; Verspoor et al., 2012). However, experience shows that the characteristics of a corpus influence performance, e.g., (Cao and Zukerman, 2012) for sentiment analysis and (Pyysalo et al., 2008) in the biomedical space. In this paper, we analyze how the characteristics and annotation schemas of two corpora influence BANNER’s performance on the recognition of diseases (note that BANNER outperforms ABNER in the recognition of di"
U13-1012,U12-1008,1,0.830608,"logy (Settles, 2004); and Whatizit – a Web service which provides functionality to perform text-mining tasks (RebholzSchuhmann et al., 2008). These tools in turn require the development of annotated training corpora, e.g., (Kim et al., 2003; Rosario and Hearst, 2004; Kulick et al., 2004; Pestian et al., 2007; Jimeno-Yepes et al., 2008; Bada et al., 2012). Studies have been conducted to examine the performance of different NLP tools on a single corpus, e.g., (Jacob et al., 2013; Verspoor et al., 2012). However, experience shows that the characteristics of a corpus influence performance, e.g., (Cao and Zukerman, 2012) for sentiment analysis and (Pyysalo et al., 2008) in the biomedical space. In this paper, we analyze how the characteristics and annotation schemas of two corpora influence BANNER’s performance on the recognition of diseases (note that BANNER outperforms ABNER in the recognition of diseases (Leaman and Gonzalez, 2008)). The corpora in question are the Human Variome Project Corpus (HVPC) developed at NICTA (Verspoor et al., 2013), and the Arizona Disease Corpus (AZDC) – a popular medical resource developed at the University of Arizona (Leaman et al., 2009).1 Our results show that BANNER’s perf"
U13-1012,P04-1055,0,0.0127345,"nt growth of on-line biomedical literature has spawned an increasing number of NLP tools for content analysis that help researchers and practitioners access the latest developments in their fields. Examples of these tools include: BANNER – a Named Entity Recognizer (NER) for the biomedical domain (Leaman and Gonzalez, 2008); ABNER – a NER for molecular biology (Settles, 2004); and Whatizit – a Web service which provides functionality to perform text-mining tasks (RebholzSchuhmann et al., 2008). These tools in turn require the development of annotated training corpora, e.g., (Kim et al., 2003; Rosario and Hearst, 2004; Kulick et al., 2004; Pestian et al., 2007; Jimeno-Yepes et al., 2008; Bada et al., 2012). Studies have been conducted to examine the performance of different NLP tools on a single corpus, e.g., (Jacob et al., 2013; Verspoor et al., 2012). However, experience shows that the characteristics of a corpus influence performance, e.g., (Cao and Zukerman, 2012) for sentiment analysis and (Pyysalo et al., 2008) in the biomedical space. In this paper, we analyze how the characteristics and annotation schemas of two corpora influence BANNER’s performance on the recognition of diseases (note that BANNER"
U13-1012,W04-1221,0,0.0274044,"ona Disease Corpus. Our analysis of the performance of a state-of-the-art NER tool in terms of the characteristics and annotation schema of these corpora shows that these factors significantly affect performance. 1 Introduction The recent growth of on-line biomedical literature has spawned an increasing number of NLP tools for content analysis that help researchers and practitioners access the latest developments in their fields. Examples of these tools include: BANNER – a Named Entity Recognizer (NER) for the biomedical domain (Leaman and Gonzalez, 2008); ABNER – a NER for molecular biology (Settles, 2004); and Whatizit – a Web service which provides functionality to perform text-mining tasks (RebholzSchuhmann et al., 2008). These tools in turn require the development of annotated training corpora, e.g., (Kim et al., 2003; Rosario and Hearst, 2004; Kulick et al., 2004; Pestian et al., 2007; Jimeno-Yepes et al., 2008; Bada et al., 2012). Studies have been conducted to examine the performance of different NLP tools on a single corpus, e.g., (Jacob et al., 2013; Verspoor et al., 2012). However, experience shows that the characteristics of a corpus influence performance, e.g., (Cao and Zukerman, 20"
U13-1012,E12-2021,0,0.0140726,"els to input tokens, and considers the following features: (1) lemma for a token; (2) part of speech; (3) orthographic features, such as capitalization, presence of digits, prefixes and suffixes, and 2 and 3-character n-grams. • HVPC annotates only the last and most complete part of a disease coordination2 (e.g., in “breast and ovarian cancer”, “breast” is annotated as a body part3 ), while AZDC annotates a coordination as separate but overlapping mentions of a disease (e.g., “breast and ovarian cancer” and “ovarian cancer”). 2 This was originally done in response to the BRAT annotation tool (Stenetorp et al., 2012) not allowing annotation of discontinuous entities (since rectified). 3 A refinement is to consider (body-part, disease) related pairs as multi-word disease names, which would boost the mention-length counts for HVPC in Figure 2. 92 Parameter # of sentences # of tokens Total # of disease mentions # of unique disease mentions HVPC AZDC 2116 52454 1552 130 2783 79950 3228 1202 Parameter Frequency mean Frequency standard deviation Ratio of top N frequent mentions to all mentions N = 10 N = 20 N = 30 Table 1: Various quantitative parameters of HVPC and AZDC. Unique mentions refer to all (casesensi"
U13-1012,W04-3111,0,0.0455948,"dical literature has spawned an increasing number of NLP tools for content analysis that help researchers and practitioners access the latest developments in their fields. Examples of these tools include: BANNER – a Named Entity Recognizer (NER) for the biomedical domain (Leaman and Gonzalez, 2008); ABNER – a NER for molecular biology (Settles, 2004); and Whatizit – a Web service which provides functionality to perform text-mining tasks (RebholzSchuhmann et al., 2008). These tools in turn require the development of annotated training corpora, e.g., (Kim et al., 2003; Rosario and Hearst, 2004; Kulick et al., 2004; Pestian et al., 2007; Jimeno-Yepes et al., 2008; Bada et al., 2012). Studies have been conducted to examine the performance of different NLP tools on a single corpus, e.g., (Jacob et al., 2013; Verspoor et al., 2012). However, experience shows that the characteristics of a corpus influence performance, e.g., (Cao and Zukerman, 2012) for sentiment analysis and (Pyysalo et al., 2008) in the biomedical space. In this paper, we analyze how the characteristics and annotation schemas of two corpora influence BANNER’s performance on the recognition of diseases (note that BANNER outperforms ABNER in"
U13-1021,U13-1019,0,0.0263724,".edu.au Abstract The 2013 ALTA Shared Task was utilised as a class project for a subject taught at The University of Melbourne in the second semester of 2013. This paper reviews the experience of using an on-line, Kaggle in Class-based shared task for class work. Adoption of the shared task enables a blended learning paradigm that engages students in problem-based learning in a shared and open context. 1 Introduction As in recent years, the Australasian Language Technology Association sponsored a shared task in 2013 to stimulate interest in language technology tasks among university students (Molla, 2013). This year’s task was primarily organised by Diego Molla of Macquarie University and addressed the restoration of normal case (capitalisation) and punctuation to a noisy text input not conforming to conventional use of case and punctuation. As described in (Molla, 2013) the task is framed as a simplification of the general task explored by (Baldwin and Joseph, 2009). Because the task is specifically aimed at university students with programming skills, and as it can be approached as a classification task, it is appropriate to consider as a project for a university subject that addresses machi"
U14-1004,W02-1502,0,0.0310493,"tional Center for Biomedical Ontology’s BioPortal, http://bioportal. bioontology.org (Whetzel et al., 2011). 2 24 work allows multiple systems to be compared over the same data, producing quantitative results in terms of precision, recall, and F-score, as well as supports visual inspection of annotations and annotation differences (Kano et al., 2011). However, there is no direct support for quantitative error analysis. (Oepen et al., 1998; Oepen, 1999), as well as from systematic organisation of grammatical phenomena along typological dimensions. The LinGO Grammar Matrix (Bender et al., 2010; Bender et al., 2002) captures linguistic variation along a number of defined dimensions, and enables creation of an initial grammar based on a library of syntactic structures. One of the key elements of the Matrix is support for regression testing via automated tests, such that any change to a grammar or the linguistic phenomena captured in the system can be automatically assessed for impact to the performance on previously existing phenomena. Such test suites are used for validation and exploration of changes to a grammar, during grammar engineering (Bender et al., 2008). 3 A Framework for Ontology Test Suite ge"
U14-1004,D12-1096,0,0.0240207,", and allow analysis to be performed along particular dimensions of variation. This is in stark contrast to standard annotated corpora that reflect natural linguistic variation and natural distribution of entities, which is dependent on the collection strategy for the corpus. In error analysis of a task using an annotated corpus, the categorisation of annotations and errors into coherent groups is typically done in post-hoc analysis. It has been demonstrated that this can be both challenging to implement and insightful with regards to the generalisability of algorithms (Stoyanov et al., 2009; Kummerfeld et al., 2012; Kummerfeld and Klein, 2013). Using a test suite, it is done a priori through the test suite construction. The use of test suites has long benefited development of NLP systems for syntactic analysis Background 2.1 Concept Recognition Systems The class of NLP system that we are primarily concerned with testing is the concept recognition system. These are systems that aim to detect mentions of terms corresponding to concepts from an ontology or controlled vocabulary in natural language text. These could be named entity recognition systems, where the set of named entities is defined by a target"
U14-1004,W04-3101,0,0.293663,"tions. Evaluation of the performance of NLP methods is typically done with respect to annotated training data. Methods are assessed based on their ability to reproduce human performance on a task, as measured in terms of the standard metrics of precision, recall, and F-score. Such metrics provide a quantitative basis for comparing performance of different methods. However, they are by their nature aggregative, considering all annotations in the corpus as equal for evaluation purposes. Furthermore, such metrics do not provide insight into the nature of errors made by the methods. As stated by (Cohen et al., 2004), testing a system on an annotated corpus “tells you how often the system failed, but not what it failed at; it tells you how often the system succeeds, but not where its strengths are.” Yet investigation of the strengths and failures of a system can reveal information meaningful for improving system performance, and is a critical component of error analysis. This approach is commonly applied in software testing. The methodology of equivalence partitioning (Myers, 1979) explicitly involves partitioning the input into equivalence classes that are representative of a range of possible test cases"
U14-1004,W08-0506,0,0.017187,"building test suites, we aim to enable more systematic investigation of errors. We focus in this initial work on ontology concept recognition systems, that is, systems that aim to detect concepts defined in an ontology in natural language text. Prior work has demonstrated substantial differences in the performance of such systems, due to linguistic variability in the expression of ontology concepts (Funk et al., 2014). The use of structured test suites has been shown to enable identification of performance errors of such systems (Cohen et al., 2010), as well as being useful for finding bugs (Cohen et al., 2008). Structured test suites enable systematic evaluation, exhaustivity, inclusion of negative data, and control over data (Oepen et al., 1998). They can focus on specific linguistic phenomena, that can be presented in isolation and in controlled combinations. Evaluation of the performance of NLP methods is typically done with respect to annotated training data. Methods are assessed based on their ability to reproduce human performance on a task, as measured in terms of the standard metrics of precision, recall, and F-score. Such metrics provide a quantitative basis for comparing performance of di"
U14-1004,cohen-etal-2010-test,1,0.863238,"unsystematic manner. By providing a framework for automatically building test suites, we aim to enable more systematic investigation of errors. We focus in this initial work on ontology concept recognition systems, that is, systems that aim to detect concepts defined in an ontology in natural language text. Prior work has demonstrated substantial differences in the performance of such systems, due to linguistic variability in the expression of ontology concepts (Funk et al., 2014). The use of structured test suites has been shown to enable identification of performance errors of such systems (Cohen et al., 2010), as well as being useful for finding bugs (Cohen et al., 2008). Structured test suites enable systematic evaluation, exhaustivity, inclusion of negative data, and control over data (Oepen et al., 1998). They can focus on specific linguistic phenomena, that can be presented in isolation and in controlled combinations. Evaluation of the performance of NLP methods is typically done with respect to annotated training data. Methods are assessed based on their ability to reproduce human performance on a task, as measured in terms of the standard metrics of precision, recall, and F-score. Such metri"
U14-1004,P09-1074,0,0.0215241,"f the linguistic inputs, and allow analysis to be performed along particular dimensions of variation. This is in stark contrast to standard annotated corpora that reflect natural linguistic variation and natural distribution of entities, which is dependent on the collection strategy for the corpus. In error analysis of a task using an annotated corpus, the categorisation of annotations and errors into coherent groups is typically done in post-hoc analysis. It has been demonstrated that this can be both challenging to implement and insightful with regards to the generalisability of algorithms (Stoyanov et al., 2009; Kummerfeld et al., 2012; Kummerfeld and Klein, 2013). Using a test suite, it is done a priori through the test suite construction. The use of test suites has long benefited development of NLP systems for syntactic analysis Background 2.1 Concept Recognition Systems The class of NLP system that we are primarily concerned with testing is the concept recognition system. These are systems that aim to detect mentions of terms corresponding to concepts from an ontology or controlled vocabulary in natural language text. These could be named entity recognition systems, where the set of named entitie"
U14-1004,tanenblatt-etal-2010-conceptmapper,0,0.167099,"2011), which involves entity linking of gene/protein mentions to biological data bases. In the context of large structured vocabularies, the CR task involves mapping of terms to specific vocabulary identifiers. The set of NER categories in CR is therefore effectively as large as the number of primary terms in the vocabulary. Our test suite generation framework consists of 3 main components: cently been developed, or deployed, to address this domain, including the US National Library of Medicine’s MetaMap tool (Aronson and Lang, 2010), the NCBO Annotator (Jonquet et al., 2009), ConceptMapper (Tanenblatt et al., 2010; Funk et al., 2014), WhatIzIt (Rebholz-Schuhmann and others, 2008) and Neji (Campos et al., 2013). These systems could equally make use of machine learning, or rule-based methods. Rule-based systems have the advantage of being flexibly redeployable to new ontologies or vocabularies that might be defined, as they do not require training data. Furthermore, in a normalisation context in which specific vocabulary items must be detected and normalised to an identifier (e.g., not just recognising a US corporation mention, but mapping that mention to a specific register ID), the number of target cla"
U14-1004,D13-1027,0,0.0141646,"e performed along particular dimensions of variation. This is in stark contrast to standard annotated corpora that reflect natural linguistic variation and natural distribution of entities, which is dependent on the collection strategy for the corpus. In error analysis of a task using an annotated corpus, the categorisation of annotations and errors into coherent groups is typically done in post-hoc analysis. It has been demonstrated that this can be both challenging to implement and insightful with regards to the generalisability of algorithms (Stoyanov et al., 2009; Kummerfeld et al., 2012; Kummerfeld and Klein, 2013). Using a test suite, it is done a priori through the test suite construction. The use of test suites has long benefited development of NLP systems for syntactic analysis Background 2.1 Concept Recognition Systems The class of NLP system that we are primarily concerned with testing is the concept recognition system. These are systems that aim to detect mentions of terms corresponding to concepts from an ontology or controlled vocabulary in natural language text. These could be named entity recognition systems, where the set of named entities is defined by a target resource (e.g., the set of al"
U14-1004,C96-2120,0,\N,Missing
U14-1015,E14-1056,0,0.0757105,"Missing"
U14-1019,W11-1828,0,0.0298942,"Missing"
U14-1019,W06-3316,0,0.0604373,"Missing"
U14-1019,W11-1801,0,0.0691896,"Missing"
U14-1019,W11-1813,0,0.0159907,"roteins, genes, chemical compounds and drugs. Biological molecular pathways, for example, integrated with knowledge of relevant protein-protein interactions, are used to understand complex biological processes. Coreference resolution is an essential task in information extraction, because it can automatically provide links between entities, and as well can facilitate better indexing for medical information search with rich semantic information. A key obstacle is the low detection reliability of hidden or complex mentions of entities involving coreference expressions in natural language texts (Kim et al., 2011a; Miwa et al., 2010). Such In this paper, we investigate the challenges of biomedical coreference resolution, and provide an evaluation of general domain coreference resolution system on biomedical texts. Prior work demonstrated the importance of domain-specific knowledge for coreference (Choi et al., 2014). We extend that work with a detailed analysis of features of coreference relations with respect to the type of the anaphor defined by a previously proposed framework (Nguyen and Kim, 2008), and propose an efficient strategy towards improved anaphoric coreference resolution in the biomedica"
U14-1019,W11-1902,0,0.117501,"nformation for resolving pronominal expressions (Kehler, 1995), and a framework based Miji Choi, Karin Verspoor and Justin Zobel. 2014. Analysis of Coreference Relations in the Biomedical Literature. In Proceedings of Australasian Language Technology Association Workshop, pages 134−138. on the Centering theory was developed for the interpretation of pronouns by identifying patterns of coreference (Gordon and Hendrick, 1997). An unsupervised system was developed to determine coreference links with a collection of rulebased models (Raghunathan et al., 2010), and the system has been extended by (Lee et al., 2011) with additional processes such as mention detection, discourse processing and semantic-similarity processing. The system was developed targeting to the newswire domain, but has been adopted for the clinical domain (Jindal and Roth, 2013; Jonnalagadda et al., 2012; Dai et al., 2012). The rulebased approach has been demonstrated to slightly outperform a machine learning approach for coreference resolution related to treatment, test and person (Jonnalagadda et al., 2012). Recently, there was a community-wide shared task for coreference resolution in biomedical literature, the Protein Coreference"
U14-1019,O04-1011,0,0.0921228,"Missing"
U14-1019,C08-1079,0,0.0317493,"ity of hidden or complex mentions of entities involving coreference expressions in natural language texts (Kim et al., 2011a; Miwa et al., 2010). Such In this paper, we investigate the challenges of biomedical coreference resolution, and provide an evaluation of general domain coreference resolution system on biomedical texts. Prior work demonstrated the importance of domain-specific knowledge for coreference (Choi et al., 2014). We extend that work with a detailed analysis of features of coreference relations with respect to the type of the anaphor defined by a previously proposed framework (Nguyen and Kim, 2008), and propose an efficient strategy towards improved anaphoric coreference resolution in the biomedical literature building on that framework. 2 Background Related Work For general coreference resolution, several strategies and methodologies have been developed since 1990’s. Centering theory was studied based on syntactic information for resolving pronominal expressions (Kehler, 1995), and a framework based Miji Choi, Karin Verspoor and Justin Zobel. 2014. Analysis of Coreference Relations in the Biomedical Literature. In Proceedings of Australasian Language Technology Association Workshop, pa"
U14-1019,W11-1811,0,0.0313683,"Missing"
U14-1019,D10-1048,0,0.0926542,"Missing"
U14-1019,W13-2002,0,\N,Missing
U15-1010,W06-1615,0,0.150222,"r annotation, we explore domain adaptation of an information extraction system using out-of-domain data and a small amount of in-domain data. Domain adaptation for named entity recognition techniques has been explored widely in recent years. For instance, Jiang and Zhai (2006) approached the problem by generalizing features across the source and target domain to way avoid overfitting. Mohit and Hwa (2005) proposed a semi-supervised method combining a naive Bayes classifier with the EM algorithm, applied to features extracted from a parser, and showed that the method is robust over novel data. Blitzer et al. (2006) induced a correspondence between features from a source and target domain based on structural correspondence learn4 4.1 exp( t k wk fk (st−1 , st , o, t)) (1) Zw (o) Methods Data In order to evaluate NER over financial agreements, we annotated a dataset of financial agreements made public through U.S. Security and Exchange Commission (SEC) filings. Eight documents (totalling 54,256 words) were randomly selected for manual annotation, based on the four NE types provided in the CoNLL-2003 dataset: LOCATION ( LOC ), ORGANISATION (ORG), PERSON (PER), and MISCELLANEOUS (MISC). The annotation was c"
U15-1010,D14-1096,1,0.827424,"of ORG, though in the gold standard they don’t belong to any entity type. This error was reduced drastically through the addition of the in-domain financial data in training, improving the overall performance of the model. Ultimately, the purely in-domain training stratagem in Experiment4 outperforms the mixed data setup (Experiment3), indicating that domain context is critical for the task. Having said that, the results of our study inform the broader question of out-of-domain applicability of NER models. Furthermore, they point to the value of even a small amount of in-domain training data (Duong et al., 2014). 6 Conclusions Risk assessment is a crucial task for financial institutions such as banks because it helps to estimate the amount of capital they should hold to promote their stability and protect their clients. Manual extraction of relevant information from text88 LOC MISC Actual ORG PER NA NE Precision LOC 20 0 0 0 12 0.625 Predicted MISC ORG PER 0 3 2 7 0 0 0 16 0 0 0 202 2 24 8 0.778 0.372 0.953 O 14 0 40 14 – Recall 0.513 1.000 0.286 0.935 Table 3: Confusion matrix for the predictions over F IN 3 using the model from Experiment3, including the precision and recall for each class (“NA NE”"
U15-1010,demiros-etal-2000-named,0,0.0734988,"d, but the number of words in the corpus is 30,000 words for training and 140,000 for testing. The approach involved the creation of rules by hand; this is a time-consuming task, and the overall recall is low compared to other extraction methods. Related Work Another rule-based approach was proposed by Sheikh and Conlon (2012) for extracting information from financial data (combined quarterly reports from companies and financial news) with the aim of assisting in investment decision-making. Most prior approaches to information extraction in the financial domain make use of rule-based methods. Farmakiotou et al. (2000) extract entities from financial news using grammar rules 85 ing over unlabelled target domain data. Qu et al. (2015) showed that a graph transformer NER model trained over word embeddings is more robust cross-domain than a model based on simple lexical features. Our approach is based on large amounts of labelled data from a source domain and small amounts of labelled data from the target domain (i.e. financial agreements), drawing inspiration from previous research that has shown that using a modest amount of labelled in-domain data to perform transfer learning can substantially improve class"
U15-1010,N06-1010,0,0.0316701,"of the text and linguistic forms, and then creates text grammars. Finally, the approach uses a parser to process the document content. Although the authors do not present results, they argue that when applied to a test set of 1,000 criminal cases, they were able to identify the required information. P P p(s|o) = In order to reduce the need for annotation, we explore domain adaptation of an information extraction system using out-of-domain data and a small amount of in-domain data. Domain adaptation for named entity recognition techniques has been explored widely in recent years. For instance, Jiang and Zhai (2006) approached the problem by generalizing features across the source and target domain to way avoid overfitting. Mohit and Hwa (2005) proposed a semi-supervised method combining a naive Bayes classifier with the EM algorithm, applied to features extracted from a parser, and showed that the method is robust over novel data. Blitzer et al. (2006) induced a correspondence between features from a source and target domain based on structural correspondence learn4 4.1 exp( t k wk fk (st−1 , st , o, t)) (1) Zw (o) Methods Data In order to evaluate NER over financial agreements, we annotated a dataset o"
U15-1010,P05-3015,0,0.0421595,"t. Although the authors do not present results, they argue that when applied to a test set of 1,000 criminal cases, they were able to identify the required information. P P p(s|o) = In order to reduce the need for annotation, we explore domain adaptation of an information extraction system using out-of-domain data and a small amount of in-domain data. Domain adaptation for named entity recognition techniques has been explored widely in recent years. For instance, Jiang and Zhai (2006) approached the problem by generalizing features across the source and target domain to way avoid overfitting. Mohit and Hwa (2005) proposed a semi-supervised method combining a naive Bayes classifier with the EM algorithm, applied to features extracted from a parser, and showed that the method is robust over novel data. Blitzer et al. (2006) induced a correspondence between features from a source and target domain based on structural correspondence learn4 4.1 exp( t k wk fk (st−1 , st , o, t)) (1) Zw (o) Methods Data In order to evaluate NER over financial agreements, we annotated a dataset of financial agreements made public through U.S. Security and Exchange Commission (SEC) filings. Eight documents (totalling 54,256 w"
U15-1010,K15-1009,1,0.84904,"Missing"
U15-1010,E12-2021,0,0.0977471,"Missing"
U15-1012,E12-1036,0,0.0172074,"rate on identification of significant revision changes, or revision changes that have higher impact of meaning change for the purpose of prioritising revision changes, especially in multi-author revision. Nevertheless, the work by Zhang and Litman (2014; 2015) provides insights to revisions from a different perspective. Research has shown that predefined edit categories such as fluency edits (i.e. edits to improve on style and readability) and factual edits (i.e. edits that alter the meaning) in Wikipedia, where revision history data is abundant, can be classified using a supervised approach (Bronner and Monz, 2012; Daxenberger and Gurevych, 2013). The distinction of the edits can be linked to Faigley and Witte’s (1981) taxonomy: fluency edits to surface changes and factual edits to text-base changes. Supervised classification would be difficult to apply to other types of revised documents, due to more limited training data in most domain-specific contexts. They too did not consider the significance of edits. As our task is to align words between versioned sentences to assist in identification of significant changes between versioned texts, it is important to consider the semantics of sentences. Lee et."
U15-1012,D13-1055,0,0.0125488,"f significant revision changes, or revision changes that have higher impact of meaning change for the purpose of prioritising revision changes, especially in multi-author revision. Nevertheless, the work by Zhang and Litman (2014; 2015) provides insights to revisions from a different perspective. Research has shown that predefined edit categories such as fluency edits (i.e. edits to improve on style and readability) and factual edits (i.e. edits that alter the meaning) in Wikipedia, where revision history data is abundant, can be classified using a supervised approach (Bronner and Monz, 2012; Daxenberger and Gurevych, 2013). The distinction of the edits can be linked to Faigley and Witte’s (1981) taxonomy: fluency edits to surface changes and factual edits to text-base changes. Supervised classification would be difficult to apply to other types of revised documents, due to more limited training data in most domain-specific contexts. They too did not consider the significance of edits. As our task is to align words between versioned sentences to assist in identification of significant changes between versioned texts, it is important to consider the semantics of sentences. Lee et. al. (2014) reviewed the limitati"
U15-1012,W03-1608,0,0.0598411,"et. al. (2014) reviewed the limitations of information retrieval methods (i.e., the Boolean model, the vector space model and the statistical probability model) that calculate the similarity of natural language sentences, but did not consider the meaning of the sentences. Their proposal was to use link grammar to measure similarity based on grammatical structures, combined with the use of an ontology to measure the similarity of the meaning. Their method was shown to be effective for the problem of paraphrase. Paraphrase addresses detecting alternative ways of conveying the same information (Ibrahim et al., 2003) and we observe paraphrase problem as a subset to our task because sentence re-phrasing is part of revision. However, the paraphrase problem effectively try to normalize away differences, while versioned sentences analysis focuses more directly on evaluating the meaning impact of differences. Original Sentence, SO 3 Calculate Offset of Non-Destroyed Hip. Dataset The dataset that we study is a set of revised software requirements documents, the Orthopedic Workstation (OWS) Use Case Specifications (UCS) for Pre-Operative Planning for the Hip. We were provided with two versions, version 0.9 (orig"
U15-1012,P03-1054,0,0.0103153,"vily. As there is no previous work on the optimal weight to use for aligning versioned sentences, we experimented with a weight value of +2. • SAVeS: SAVeS is implemented based on the algorithm in Figure 4. The updated tokens are Approach String Similarity Semantic Similarity Tokenization approaches: Baseline Glossary Terms Weighted Glossary Terms SAVeS r -0.34 -0.59 0.63 0.66 0.68 0.58 Table 5: Correlation coefficient (r) values between similarity measurement and significant changes, using various approaches to similarity assessment. re-aligned based on the noun phrases. The Stanford parser (Klein and Manning, 2003) we used produced parse trees with minor errors in some sentences. To eliminate issues in the results related to the incorrect parsing, we manually corrected errors in the parse trees, thus assuming the existence of a ‘perfect’ parser. 7 Results and Discussion Table 5 shows that semantic similarity has a stronger negative correlation to significant changes when compared to string similarity but the baseline approach of single word token alignment correlates better to significant changes. This result shows that semantic similarity could be used to filter out non-significant revised sentences be"
U15-1012,P13-1127,0,0.023254,"Missing"
U15-1012,W10-1754,0,0.0272514,"stance between SO and SR , W(P) is the sum of the edit operations of P, where weight is added for edit operation involving word in the glossary for the weighted glossary experiment. Winkler proximity (Cohen et al., 2003). Automatic machine translation evaluation metrics, which normally integrate with linguistics knowledge, is used to measure how semantically similar between the translation output of a system to the parallel corpus without human judgement. This approach is also used for paraphrase evaluation (Madnani et al., 2012). For semantic similarity, we adopted one of the metrics, Tesla (Liu et al., 2010), which is linked to WordNet as our semantic similarity measurement between versioned sentences. 6.4 Figure 2: Example how SAVeS capture the context surrounding the edited word 6.2 Annotation Before we can consider a suitable measurement for revision changes between versioned sentences, manual intuitive annotation is performed by an annotator, with review from one other. The versioned sentences are annotated based on significance of the changes, framed by Faigley and Witte’s (1981) revision analysis taxonomy. We compared the original sentence, SO , to the revised sentence, SR , and for each se"
U15-1012,N12-1019,0,0.0128156,"R is. W ER(SO , SR ) = W (P ) maximum length(SO , SR ) (1) Where: P is minimum edit distance between SO and SR , W(P) is the sum of the edit operations of P, where weight is added for edit operation involving word in the glossary for the weighted glossary experiment. Winkler proximity (Cohen et al., 2003). Automatic machine translation evaluation metrics, which normally integrate with linguistics knowledge, is used to measure how semantically similar between the translation output of a system to the parallel corpus without human judgement. This approach is also used for paraphrase evaluation (Madnani et al., 2012). For semantic similarity, we adopted one of the metrics, Tesla (Liu et al., 2010), which is linked to WordNet as our semantic similarity measurement between versioned sentences. 6.4 Figure 2: Example how SAVeS capture the context surrounding the edited word 6.2 Annotation Before we can consider a suitable measurement for revision changes between versioned sentences, manual intuitive annotation is performed by an annotator, with review from one other. The versioned sentences are annotated based on significance of the changes, framed by Faigley and Witte’s (1981) revision analysis taxonomy. We"
U15-1012,W14-1818,0,0.342176,"isions is required for meaning changes assessment. We will present our proposed method, structural alignment of versioned sentences, SAVeS that addresses this requirement. We provide a performance comparison to three other word segmentation approaches. The broader aim of this research is to develop a computational approach to automatically identifying significant changes between versions of a text document. 2 Related Works Research on revision concentrates on detecting edits and aligning sentences between versioned text documents. Considering sentences from the first and last draft of essays, Zhang and Litman (2014; 2015) proposed an automated approach to detect whether a sentence has been edited between these versions. Their proposed method starts with senFigure 1: Taxonomy for revision analysis (Faigley and Witte, 1981) 102 tence alignment, and then identifies the sequence of edits (i.e., the edit operations of Add, Modify, Delete and Keep) between the two sentences. They further consider automated classification of the reason for a revision (i.e., claim, evidence, rebuttal, etc.), which they hypothesised can help writers to improve their writing. Classifying revisions based on the reasons of revision"
U15-1012,W15-0616,0,0.0336604,"Missing"
U16-1005,J93-1003,0,0.376964,"us how different the actual frequency of the term is from the expected frequency of the same term. For this, the log-likelihood is calculated using Equation 5. LL = 2 X i (Otk ,ci ln( Otk ,ci )) Etk ,ci (5) Figure 1: The analysis of the effect of different log-likelihood threshold values on the changes in the JSD distances between consecutive chief complaint corpora in the SynSurv data set An alternative to the log-likelihood measure for statistical analysis of textual corpora is Pearson’s χ2 statistic. This measure assumes a normal distribution of terms in the corpora and has been shown in (Dunning, 1993) to be less reliable especially in the case of small textual corpora with rare terms. Given the relatively small size of the chief complaint corpora for each time-frame, the log-likelihood analysis was preferred here. Once the log-likelihood of each term was calculated, all of the terms with the log-likelihood below a set threshold were filtered out. The texts of the two corpora now contained only the most important terms that participated in the calculation of probability distributions using JSD. To estimate the best log-likelihood threshold, we calculated the JSD between consecutive timefram"
U16-1005,W00-0901,0,0.210113,"be an effective method, as all the terms will have equal importance. In this case, there is a need for filtering out the terms that do not distinguish the two corpora well (i.e., terms that are common in both corpora). Textual Modeling of Chief Complaints We examine the distribution of lexical items in chief complaints to find the differences in the terminology used in consecutive time-frames. For this, statistical methods were utilized, as will be discussed in the following sections. Such statistical methods have been previously used for corpus analysis and comparison (Verspoor et al., 2009; Rayson and Garside, 2000). We follow that prior 47 The log-likelihood score of a term represents the relative frequency difference of that term in the two different corpora under comparison (Rayson and Garside, 2000). This measure is calculated based on the expected value for term tk ∈ V using the total frequency of all terms in the corpus and the actual frequency (or the sum of occurrences) of term tk in the same corpus. Equation 4 shows how the log-likelihood score is calculated for tk ∈ V where Nci is the total frequency of all terms in corpus ci , and Otk ,ci represents the observed frequency of term tk in the sam"
U16-1005,I13-1041,0,0.0734631,"o may come in contact with the infected person. Chief complaints are readily available in a digital format and can therefore be easily processed using natural language processing algorithms. In the past, various work has been done to perform syndromic surveillance using supervised machine learning and statistical algorithms (Tsui et al., 2003; Espino et al., 2007; Chapman et al., 2005; Bradley et al., 2005). The major drawback of machine learning methods is the requirement of historical data that can be used to train the system, and a sensitivity to the characteristics of specific text types (Baldwin et al., 2013). In the case of syndromic surveillance, there is evidence of a need for localized training data; as Ofoghi and Verspoor (2015) found, a machine learning classifier trained on an American data set may not be effective on an Australian data set. The authors found that the American off-the-shelf syndromic classifier (CoCo) achieved a lower F-score on the Australian data set compared with another classifier (SyCo) that was trained with the Australian data set. Moreover, there may be a lack of resources to collect ongoing data for chief complaints, especially in remote areas. Therefore, there is a"
U16-1007,W08-0601,0,0.0298256,"Missing"
U16-1007,P14-5010,0,0.0334903,"arning algorithms such as SVM. Deep learning based approches (Zeng et al., 2014; Xu et al., 2015) are other alternatives to eliminate the manual feature engineering efforts. However, in this work we are primarily focussed on kernel methods. In NLP, kernel methods have been effectively used for relation extraction and sentence classification. Subset tree kernels (SSTK) and partial tree kernels (PTK) were developed to work with constituency parse trees and basic dependency parse trees. However, these kernels are not suitable for arbitrary graph structures such as the enhanced dependency parses (Manning et al., 2014). Secondly, tree kernels can only handle node labels and not edge labels. As a work around, these kernels require that the original dependency graphs be heuristically altered to translate edge labels into special nodes to create different syntactic representations such as the grammatical relation centered tree (Croce et al., 2011). These limitations were overcome with the Approximate Subgraph Nagesh C Panyam, Karin Verspoor, Trevor Cohn and Rao Kotagiri. 2016. ASM Kernel: Graph Kernel using Approximate Subgraph Matching for Relation Extraction. In Proceedings of Australasian Language Technolog"
U16-1007,W16-3001,0,0.0476605,"Missing"
U16-1007,W11-0216,0,0.0199374,"he graph with at most n nodes. Enumerating the features of this graph involves a single traversal of each such path, which translates to a complexity bound of O(n · m2 ) or simply O(n3 ) (a looser upper bound). Finding the shortest paths across all node pairs can be done in O(n3 ) time using standard graph algorithms (Seidel, 1995). F1 49.0 55.5 53.7 58.1 with ASM kernel. We extend the comparison to two well known tree kernels, namely Subset Tree Kernel (SSTK) and the Partial Tree Kernel (PTK) that have been shown to be effective for relation extraction (Zelenko et al., 2002; Moschitti, 2006; Chowdhury et al., 2011). Note that unlike constituency parse trees, dependency trees have edge labels which cannot be handled by these tree kernels. Therefore, the edge labels are converted into node labels of specially inserted nodes in the original dependency graph, to get a modified structure referred to as the Location Centered Tree (LCT) (Lan et al., 2009). Finally, we compare the ASM kernel with tree kernels in a sentence classification task. This is a straightforward application of kernels in a graph classification problem, over the unmodified dependency graphs of the corpus. We used the Java based Kelp frame"
U16-1007,D14-1050,0,0.0579519,"Missing"
U16-1007,D11-1096,0,0.0234242,"fication. Subset tree kernels (SSTK) and partial tree kernels (PTK) were developed to work with constituency parse trees and basic dependency parse trees. However, these kernels are not suitable for arbitrary graph structures such as the enhanced dependency parses (Manning et al., 2014). Secondly, tree kernels can only handle node labels and not edge labels. As a work around, these kernels require that the original dependency graphs be heuristically altered to translate edge labels into special nodes to create different syntactic representations such as the grammatical relation centered tree (Croce et al., 2011). These limitations were overcome with the Approximate Subgraph Nagesh C Panyam, Karin Verspoor, Trevor Cohn and Rao Kotagiri. 2016. ASM Kernel: Graph Kernel using Approximate Subgraph Matching for Relation Extraction. In Proceedings of Australasian Language Technology Association Workshop, pages 65−73. annotations, which are related entity pairs (metoclopramide, dyskinesia). We assume that the relation (causation) is implied by the training sentence and then to try to infer a similar relation or its absence in the test sentence. Matching (ASM) (Liu et al., 2013), that was designed to be a fle"
U16-1007,P15-4004,0,0.0376669,"Missing"
U16-1007,D15-1206,0,0.0267186,"uch as a graph, by transforming it into a flat array of features is inherently harder. This problem of constructing explicit feature sets for complex objects is generally overcome by kernel methods for classification. Kernel methods allow for an implicit exploration of a vast high dimensional feature space and shift the focus from feature engineering to similarity score design. Importantly, such a kernel must be shown to be symmetric and positive semi-definite (Burges, 1998), to be valid for use with kernelized learning algorithms such as SVM. Deep learning based approches (Zeng et al., 2014; Xu et al., 2015) are other alternatives to eliminate the manual feature engineering efforts. However, in this work we are primarily focussed on kernel methods. In NLP, kernel methods have been effectively used for relation extraction and sentence classification. Subset tree kernels (SSTK) and partial tree kernels (PTK) were developed to work with constituency parse trees and basic dependency parse trees. However, these kernels are not suitable for arbitrary graph structures such as the enhanced dependency parses (Manning et al., 2014). Secondly, tree kernels can only handle node labels and not edge labels. As"
U16-1007,W02-1010,0,0.0184454,"h label pair corresponds to a path in the graph with at most n nodes. Enumerating the features of this graph involves a single traversal of each such path, which translates to a complexity bound of O(n · m2 ) or simply O(n3 ) (a looser upper bound). Finding the shortest paths across all node pairs can be done in O(n3 ) time using standard graph algorithms (Seidel, 1995). F1 49.0 55.5 53.7 58.1 with ASM kernel. We extend the comparison to two well known tree kernels, namely Subset Tree Kernel (SSTK) and the Partial Tree Kernel (PTK) that have been shown to be effective for relation extraction (Zelenko et al., 2002; Moschitti, 2006; Chowdhury et al., 2011). Note that unlike constituency parse trees, dependency trees have edge labels which cannot be handled by these tree kernels. Therefore, the edge labels are converted into node labels of specially inserted nodes in the original dependency graph, to get a modified structure referred to as the Location Centered Tree (LCT) (Lan et al., 2009). Finally, we compare the ASM kernel with tree kernels in a sentence classification task. This is a straightforward application of kernels in a graph classification problem, over the unmodified dependency graphs of the"
U16-1007,W11-1801,0,0.0734385,"Missing"
U16-1007,C14-1220,0,0.0647168,"ressive structure such as a graph, by transforming it into a flat array of features is inherently harder. This problem of constructing explicit feature sets for complex objects is generally overcome by kernel methods for classification. Kernel methods allow for an implicit exploration of a vast high dimensional feature space and shift the focus from feature engineering to similarity score design. Importantly, such a kernel must be shown to be symmetric and positive semi-definite (Burges, 1998), to be valid for use with kernelized learning algorithms such as SVM. Deep learning based approches (Zeng et al., 2014; Xu et al., 2015) are other alternatives to eliminate the manual feature engineering efforts. However, in this work we are primarily focussed on kernel methods. In NLP, kernel methods have been effectively used for relation extraction and sentence classification. Subset tree kernels (SSTK) and partial tree kernels (PTK) were developed to work with constituency parse trees and basic dependency parse trees. However, these kernels are not suitable for arbitrary graph structures such as the enhanced dependency parses (Manning et al., 2014). Secondly, tree kernels can only handle node labels and n"
U16-1007,C02-1150,0,0.236301,"Missing"
U16-1007,H05-1091,0,\N,Missing
U17-1008,W17-2320,0,0.0276588,"and Daelemans, 2009b). Other approaches that use machine learning include the work of Agarwal and Yu (2010a,b) that uses conditional random fields (CRFs) to detect negation and speculation, and Cruz D´ıaz et al. (2012) who experimented with the use of decision trees and support vector machines. Most work on negation and speculation detection has focused on a specific corpus and domain, with some exceptions. Wu et al. (2014) investigated the generalisability of different negation detection methods over different domains, and found that performance often suffers without in-domain training data. Miller et al. (2017) also investigated the use of different unsupervised domain adaptation algorithms for negation detection in the clinical domain and found that such algorithms only achieved marginal increase in performance compared to systems that use in-domain training data. ]], adv [[SPEC Such differences in usage between veterinary clinicians and other medical professionals such as radiologists are a major focus of this work, in adapting the annotation framework from BioScope to this new domain. This paper attempts to address the following research questions: (1) Can the task of negation/speculation detecti"
U17-1008,W09-1304,0,0.0263696,"al infection can be allergy in origin SPEC ]] SPEC More recently, machine learning approaches have become popular. Morante et al. (2008) proposed a machine learning approach that consists of two phases: (1) classification of whether each token in a sentence is a negation cue, and (2) classification of whether each token is part of the negation scope of a given cue. Both phases used a memory-based classifier using features such as the the wordform of the token, part-of-speech (POS) tag, and chunk tags of the token and neighbouring tokens. The approach was also applied to speculation detection (Morante and Daelemans, 2009a), and incorporated into a meta-learning approach to the second phase of negation scope detection (Morante and Daelemans, 2009b). Other approaches that use machine learning include the work of Agarwal and Yu (2010a,b) that uses conditional random fields (CRFs) to detect negation and speculation, and Cruz D´ıaz et al. (2012) who experimented with the use of decision trees and support vector machines. Most work on negation and speculation detection has focused on a specific corpus and domain, with some exceptions. Wu et al. (2014) investigated the generalisability of different negation detectio"
U17-1008,W09-1105,0,0.0264146,"al infection can be allergy in origin SPEC ]] SPEC More recently, machine learning approaches have become popular. Morante et al. (2008) proposed a machine learning approach that consists of two phases: (1) classification of whether each token in a sentence is a negation cue, and (2) classification of whether each token is part of the negation scope of a given cue. Both phases used a memory-based classifier using features such as the the wordform of the token, part-of-speech (POS) tag, and chunk tags of the token and neighbouring tokens. The approach was also applied to speculation detection (Morante and Daelemans, 2009a), and incorporated into a meta-learning approach to the second phase of negation scope detection (Morante and Daelemans, 2009b). Other approaches that use machine learning include the work of Agarwal and Yu (2010a,b) that uses conditional random fields (CRFs) to detect negation and speculation, and Cruz D´ıaz et al. (2012) who experimented with the use of decision trees and support vector machines. Most work on negation and speculation detection has focused on a specific corpus and domain, with some exceptions. Wu et al. (2014) investigated the generalisability of different negation detectio"
U17-1008,D08-1075,0,0.0312125,"ncratic in nature. Second, while radiology clinical notes are often professionally transcribed from an oral account by the clinician, in the veterinary general practice context, notes are authored directly by the clinician as text. Inevitably, this is done under time pressure, meaning that the text is often ungrammatical and lacks punctuation. Examples (3) and (4) exemplify negation and speculation in VetCompass: (3) Mm - moist [[NEG no skin tent NEG ]] (4) Adv [[SPEC poss bacterial infection can be allergy in origin SPEC ]] SPEC More recently, machine learning approaches have become popular. Morante et al. (2008) proposed a machine learning approach that consists of two phases: (1) classification of whether each token in a sentence is a negation cue, and (2) classification of whether each token is part of the negation scope of a given cue. Both phases used a memory-based classifier using features such as the the wordform of the token, part-of-speech (POS) tag, and chunk tags of the token and neighbouring tokens. The approach was also applied to speculation detection (Morante and Daelemans, 2009a), and incorporated into a meta-learning approach to the second phase of negation scope detection (Morante a"
U17-1008,C12-2096,0,0.0600722,"Missing"
U17-1008,N15-1168,0,0.0224595,"otes? This paper describes the process of annotating negation and speculation in veterinary clinical records. We then demonstrate that the task of negation and speculation detection can be successfully applied to veterinary clinical notes using a simple conditional random field (CRF) model. We additionally show that models trained on a related out-of-domain corpus such as the BioScope have utility over veterinary clinical records, in particular for negation detection. 2 2.2 Veterinary NLP We are only aware of a few papers that have applied natural language processing in the veterinary domain. Ding and Riloff (2015) conducted work on detecting mentions of medication usage in a discussion forum for veterinarians, and categorizing the usage of the medication. A classifier determines whether each word is part of a medication mention using features such as the POS tags and neighbouring words The output of the medication mention detector is used by another classifier to determine its usage category such as whether the clinician prescribed the medication or changed it. Text classification is a task that had been previously applied to veterinary clinical records. Anholt et al. (2014) performed classification of"
U17-1008,E12-2021,0,0.115484,"Missing"
U17-1008,W08-0606,0,0.567458,"data, in the veterinary clinical note domain, and describes a series of experiments whereby we port a CRF-based method across from the BioScope corpus to this novel domain. 1 Introduction Negation and speculation are common in clinical texts, yet pose a challenge for natural language processing of these texts. Negation indicates the absence or opposite of something, and is defined within the previously released BioScope corpus (a collection of biomedical and clinical documents annotated for the task of negation/speculation detection) to be the “implication of the non-existence of something” (Szarvas et al., 2008). For example, the statement no abnormalities were found in the patient indicates the absence of abnormalities in the patient. Speculation is used to indicate uncertainty or the possibility of something, and is defined within BioScope to be statements of “the possible existence of something”. For example, there is possible bacterial infection indicates that an infection might be present, without any certainty that it is. Both are commonly used in clinical texts as a means of ruling out diagnostic possibilities and hypothesising. This paper will discuss a method for detecting negation and specu"
U19-1014,N18-2075,0,0.0279898,"zed as part of CLEF-IP 2012 (Piroi et al., 2012). In the sub-task titled “Passage Retrieval Starting From Claims”, participants were required to extract passages from chemical patents that are relevant to a given claim. The difference between our task and theirs is that the output of our task is all chemical reactions mentioned in a given patent, independent of any claim. In addition, the CLEF-IP task does not require the identification of reaction spans. That is, they deal with each passage independently, ignoring ordering. The proposed task can also be viewed as a text segmentation problem. Koshorek et al. (2018) formulated the text segmentation task on general domain corpora such as Wikipedia as a supervised learning problem, and proposed a twolevel bidirectional LSTM model to learn to detect text spans. In particular, they used a softmax layer on top of a standard BiLTSM architecture for segmentation prediction. We experiment with a BiLTSM-CRF architecture as a document-level training method as described in Section 4, i.e. we use a CRF layer to obtain the document-level label sequence, instead of applying a softmax classifier on top of the BiLSTM. 3 3.1 Task and Dataset Task Formulation A patent doc"
U19-1014,P09-1113,0,0.0277244,"kground knowledge. Indeed, the BiLSTM-CRF model trained at the documentlevel performed much better than the paragraphlevel classification methods. The performance of the baseline methods presented in this paper is still not satisfactory considering the complex downstream tasks such as event extraction. We believe that both the models and the corpus have potential to be improved. As future work, we plan to explore more efficient document-level training methods, and, in particular, methods that work well on noisy training sets. For instance, techniques successfully used for distant supervision (Mintz et al., 2009) may be effective. Furthermore, although we used only textual information, patent documents contain substantial visual information (e.g., images of compounds, or tables) that may be helpful to properly understand a reaction description. Longer term, we will also tackle finer-grained information extraction for chemical reactions utilizing the output of this task. This step involves extracting the details of the detected reactions, that is, inferring the underlying structure of the reactions themselves. Acknowledgments We would like to thank the anonymous reviewers for their valuable comments. T"
U19-1014,N18-1202,0,0.0449915,"Missing"
U19-1014,E99-1023,0,0.134349,"copper(I)bromide dimethyl sulfide (2.17 g, 10.56 mmol) was dissolved ... I Figure 2: Illustration of our reaction span detection task. would not be able to detect reactions as a whole or capture reaction substructure. Figure 2 shows an example of an input and goldstandard output of the reaction span detection task. A patent document is given as a sequence of paragraphs. The task is to detect a span of contiguous paragraphs that describe a single chemical reaction. In our corpus, we provide paragraph-level label sequences over paragraphs in patent documents, following the IOB2 tagging scheme (Tjong et al., 1999). The definition of “reaction spans” in our dataset follows the extraction rules of the original database. In principle, a reaction is extracted from a patent if the requisite information about the reaction (e.g., starting materials, reaction conditions and target compounds) is provided within the patent document and there is no obvious error or inconsistency in the description. Typically a reaction constitutes an example section or a subsection beginning with a title paragraph such as Example 1, Step 1 and Preparation of [product name], as shown in Figure 1. However, it is also commonly the c"
U19-1014,W19-5035,1,0.833679,"xt of each paragraph as input, with a maximum length of 128 tokens.5 For tokenization, we used the OSCAR4 tokenizer (Jessop et al., 2011), as it is customized to chemical text mining. Equation (1) formulates the input token-level representation for the BiLSTM paragraph encoder in the form of (context-insensitive) word embeddings, contextualized word embeddings, and feature embeddings. For the word embeddings eWE wi and contextualized embeddings eCW wi |p , we employ Word2Vec (Mikolov et al., 2013) and ELMo (Peters et al., 2018), respectively, both pre-trained on chemical patent documents from Zhai et al. (2019). These embeddings are fixed during training. We denote our encoder employing only the pre-trained word and contextualized embeddings CW (i.e. ei = eWE wi ⊕ ewi |p ) as W 2 V +ELM O . We also explore additional learnable feature embeddings eFT fi (in Equation 1) based on the output of a chemical named entity recognizer (Zhai et al., 2019). This named entity recognizer was trained on a patent corpus named Reaxys® Gold data (Akhondi et al., 2019). For self-containment purpose we show the entity label set of Reaxys® Gold data in Table 4 in the Appendix. As the label set has two levels of granular"
W09-1407,W04-3101,1,0.814783,"hop on BioNLP: Shared Task, pages 50–58, c Boulder, Colorado, June 2009. 2009 Association for Computational Linguistics quality ontologies available in the biomedical domain to formally define entities, events, and constraints on slots within events and to develop patterns for how concepts can be expressed in text that take advantage of both semantic and linguistic characteristics of the text. We manually built patterns for each event type by examining the training data and by using native speaker intuitions about likely ways of expressing relationships, similar to the technique described in (Cohen et al., 2004). The patterns characterize the linguistic expression of that event and identify the arguments (participants) of the events according to (a) occurrence in a relevant linguistic context and (b) satisfaction of appropriate semantic constraints, as defined by our ontology. Our solution results in very high precision information extraction, although the current rule set has limited recall. 3.1 The reference ontology The central organizing structure of an OpenDMAP project is an ontology. We built the ontology for this project by combining elements of several community-consensus ontologies—the Gene"
W09-1407,W09-1401,0,0.043756,"Missing"
W11-0205,cohen-etal-2010-test,1,0.43761,"Missing"
W11-0205,W04-1213,0,0.0428319,"Missing"
W11-0205,W09-1401,0,0.0303816,"Missing"
W11-0205,J03-4003,0,\N,Missing
W11-0205,M98-1001,0,\N,Missing
W11-1826,W09-1403,0,0.104442,"s the frequency of the event trigger ti P of the event type E in the training data, and i f (ti , E) calculates the total frequency of all event triggers of the event type E in the training data. P (ti |E) evaluates the degree of the importance of a trigger to an event type. When the dependency graphs of two rules of different event types are isomorphic to each other, and two rules share a same event trigger, we examine the P (ti |E) of each event type, and only retain the rule for which the P (ti |E) is higher. Compared to the “once a trigger, always a trigger” method employed in other work (Buyko et al., 2009; Kilicoglu and Bergler, 2009), triggers are treated in a more flexible way in our work. A token is not necessarily always a trigger unless it appears in the appropriate context. Also, the same token can serve as trigger for different event types as long as it appears in the different context. A trigger will only be classified into a fixed event type when it could serve as trigger for different event types in the same context. 5.1.3 Performance-based rule ranking In addition to the process of refining rules across event types, we proposed a performance-based rule ranking method to evaluate eac"
W11-1826,C08-1033,0,0.0454183,"Missing"
W11-1826,W09-1418,0,0.0422725,"he event trigger ti P of the event type E in the training data, and i f (ti , E) calculates the total frequency of all event triggers of the event type E in the training data. P (ti |E) evaluates the degree of the importance of a trigger to an event type. When the dependency graphs of two rules of different event types are isomorphic to each other, and two rules share a same event trigger, we examine the P (ti |E) of each event type, and only retain the rule for which the P (ti |E) is higher. Compared to the “once a trigger, always a trigger” method employed in other work (Buyko et al., 2009; Kilicoglu and Bergler, 2009), triggers are treated in a more flexible way in our work. A token is not necessarily always a trigger unless it appears in the appropriate context. Also, the same token can serve as trigger for different event types as long as it appears in the different context. A trigger will only be classified into a fixed event type when it could serve as trigger for different event types in the same context. 5.1.3 Performance-based rule ranking In addition to the process of refining rules across event types, we proposed a performance-based rule ranking method to evaluate each rule under one event type. W"
W11-1826,W11-1802,0,0.0904283,"ng biological data such as pathways and protein interaction networks (Tian et al., 2007; Yan et al., 2006). More recently, the dependency representations obtained from full syntactic parsing, with its ability to reveal longrange dependencies, has shown an advantage in biological relation extraction over the traditional Penn Treebank-style phrase structure trees (Miyao et al., 2009). Since the dependency representation maps straightforwardly onto a directed graph, operations on graphs can be naturally applied to the problem of biological event extraction. We participated in the BioNLP-ST 2011 (Kim et al., 2011a), and applied a graph matching-based approach (Liu et al., 2010) to tackling the Task 1 of the GENIA event extraction (GE) task (Kim et al., 2011b), and the core task of the Epigenetics and Post-translational Modifications (EPI) task (Ohta et al., 2011), two main tasks of the BioNLP-ST 2011. Event recognition is performed by searching for an isomorphism between dependency representations of automatically learned event rules and complete sentences in the input texts. This process is treated as a subgraph matching problem, which corresponds to the search for a subgraph isomorphic to a rule gra"
W11-1826,I05-2038,0,0.00846753,"okens. Biological events are then extracted by applying the event descriptions of tokens in each matched rule consisting of the type, the trigger and the arguments to the corresponding tokens of the sentence. 4 Implementation 4.1 Preprocessing The same preprocessing steps as in (Liu et al., 2010) are completed on the datasets of the GE and the EPI tasks before performing text mining strategies. These include sentence segmentation and tokenization, Partof-Speech tagging, and sentence parsing. The Stanford unlexicalized natural language parser (version 1.6.5), which includes Genia Treebank 1.0 (Ohta et al., 2005) as training material, is used to analyze the syntactic structure of the sentences. The parser returns a dependency graph for each sentence. 4.2 Rule Induction and Sentence Matching For each gold event, the shortest path in the undirected graph connecting the event trigger to each event argument is extracted using Dijkstra’s algorithm (Cormen et al., 2001) with equal weight for edges. Sentence matching is performed and the raw matching results are then postprocessed based on the specifications of the shared task, such as event trigger cannot be a protein name or another event. 5 Results and Ev"
W11-1826,W11-1803,0,0.0543293,"Missing"
W11-1826,W09-1401,0,\N,Missing
W11-1826,W11-1801,0,\N,Missing
W12-3610,denecke-2002-signatures,0,0.0409403,"odels currently under development, a few changes would be required. A key difference is the separation in the Open Annotation models of three distinct elements: a target, a body, and the annotation itself, relating the previous two. These distinctions allow relations between any two elements to be made explicit and unambiguous, and further allow more detailed provenance tracking (Livingston et al. 2011). 5.2.1 Annotation content In the LAF model, feature structures can be added to any node in the annotation graph. It has been shown that feature structures can be losslessly represented in RDF (Denecke 2002; Krieger & Schäfer 2010). In the XML serialization of LAF, GrAF (Ide & Suderman 2007), feature structures are represented within an annotation. An example of a LAF annotation from that paper is in Figure 2. In an Open Annotation model, the LAF feature structure corresponds to the body of the annotation. Figures 3 and 4 show several possibilities for representing the information in Figure 2 in a model compatible with the Open Annotation proposals. The most literal transformation for the part of speech annotation msd:16, Figure 3:OAa, utilizes an explicit feature structure representation in the"
W12-3610,ide-romary-2006-representing,0,0.0322964,"tituency relationship is assumed. For transparency, an edge type that specifically defines the semantics of the relationship would be preferable to avoid any potential ambiguity. Furthermore, the LAF model allows feature structures to be added to edges, as well as nodes. We agree with Cassidy (Cassidy 2010) that the intended use of this is likely to produce typed edges, and not to produce unique instance data for 81 each edge. However, this is another source of ambiguity in the LAF representation. For example, annotations are sometimes directly connected to edges in the segmentation document (Ide & Romary 2006). In the LAF model, the body and the annotation itself can at times appear conflated. When an edge connects two nodes it is unclear if that edge contains information that relates to the body of the annotation or metadata about the annotation itself. In LAF it sometimes appears to be both. There is a single link in the LAF representation in Figure 2 from ptb:23 to msd:16. This link simultaneously encodes information about the target of the annotation, the representation of the body of the annotation, and the provenance of the annotation. The Open Annotation models provide for more explicit and"
W12-3610,C10-2067,0,0.0495898,"y under development, a few changes would be required. A key difference is the separation in the Open Annotation models of three distinct elements: a target, a body, and the annotation itself, relating the previous two. These distinctions allow relations between any two elements to be made explicit and unambiguous, and further allow more detailed provenance tracking (Livingston et al. 2011). 5.2.1 Annotation content In the LAF model, feature structures can be added to any node in the annotation graph. It has been shown that feature structures can be losslessly represented in RDF (Denecke 2002; Krieger & Schäfer 2010). In the XML serialization of LAF, GrAF (Ide & Suderman 2007), feature structures are represented within an annotation. An example of a LAF annotation from that paper is in Figure 2. In an Open Annotation model, the LAF feature structure corresponds to the body of the annotation. Figures 3 and 4 show several possibilities for representing the information in Figure 2 in a model compatible with the Open Annotation proposals. The most literal transformation for the part of speech annotation msd:16, Figure 3:OAa, utilizes an explicit feature structure representation in the body, consistent with au"
W12-3610,J93-2004,0,0.0469352,"resentation of annotations in the Semantic Web, referred to here as the Open Annotation models. We argue that the adapted model, in addition to being interoperable with other annotations and annotation tools, also resolves some representational limitations and semantic ambiguity of the original data model. 1 Introduction Formal annotation of language data is an activity that dates back at least to the classic work of Kucera and Francis on the Brown Corpus (Kucera 1967). Many annotation representations have been developed; some proposals are specific to a given corpus, e.g., the Penn Treebank (Marcus et al. 1993)) or type of annotation, e.g., CONLL dependency parse representation 1 ), while others aim towards standardization and interoperability, most recently the Linguistic Annotation Framework 2 (LAF) (ISO 2008). All such proposals, however, are closely tied to the requirements of linguistic annotation. Annotation, however, is not an activity limited to language data but rather is a general scholarly activity used both by the humanist and the scientist. It is a method by which scholars organize 1 2 http://conll.cemantix.org/2012/data.html http://www.cs.vassar.edu/~ide/papers/LAF.pdf existing knowled"
W12-3610,kemps-snijders-etal-2008-isocat,0,\N,Missing
W12-3610,W07-1501,0,\N,Missing
W13-2005,W13-2010,1,0.893498,"with Noisy Training Data Andrew MacKinlay♦ , David Martinez♦ , Antonio Jimeno Yepes♦ , Haibin Liu♠ , W. John Wilbur♠ and Karin Verspoor♦ ♦ NICTA Victoria Research Laboratory, University of Melbourne, Australia {andrew.mackinlay, david.martinez}@nicta.com.au {antonio.jimeno, karin.verspoor}@nicta.com.au ♠ National Center for Biotechnology Information, Bethesda, MD, USA haibin.liu@nih.gov, wilbur@ncbi.nlm.nih.gov Abstract itive regulation of gene expression). In our submission, we built on a system originally developed for the BioNLP-ST 2011 (Liu et al., 2011) and extended in more recent work (Liu et al., 2013a; Liu et al., 2013b). This system learns to recognise subgraphs of syntactic dependency parse graphs that express a given bio-molecular event, and matches those subgraphs to new text using an algorithm called Approximate Subgraph Matching. Due to the method’s fundamental dependency on the syntactic dependency parse of the text, in this work we set out to explore the impact of substituting the previously employed dependency parsers with a different parser which has been demonstrated to achieve higher performance than other commonly used parsers for full-text biomedical literature (Verspoor et"
W13-2005,P13-1104,0,0.0214701,"op-k events overall, versus choosing the top-k events for each event type. We also tested adding as many instances per event-type as there were in the manually-annotated dataset, with different multiplying factors. Finally, we evaluated the effect of using different splits of the data for the evaluation and optimisation steps of ASM. This is the full list of parameters that we tested over held-out data: 2.1.2 Parsing Pipeline In our parsing pipeline, we first split sentences using the JULIE Sentence Boundary Detector, or JSBD (Tomanek et al., 2007). We then parse using a version of clearnlp1 (Choi and McCallum, 2013), a successor to ClearParser (Choi and Palmer, 2011), which was shown to have stateof-the-art performance over the CRAFT corpus of full-text biomedical articles (Verspoor et al., 2012). We use dependency and POS-tagging models trained on the CRAFT corpus (except where noted); these pre-trained models are provided with clearnlp. Our fork of clearnlp integrates token span marking into the parsing process, so the dependency nodes can easily be matched to the standoff annotations provided with the shared task data. This pipeline is not dependent on any preannotated data, so can thus be trivially a"
W13-2005,P08-2026,0,0.147871,"edings of the BioNLP Shared Task 2013 Workshop, pages 35–44, c Sofia, Bulgaria, August 9 2013. 2013 Association for Computational Linguistics nent of an event rule. The rule also consists of an event type, and a mapping from event arguments to nodes from the pattern graph, or to an event type/node pair for nested event arguments. After processing all training documents, we get on the order of a few thousand rules; this can be decreased slightly by removing rules with subgraphs that are isomorphic to those of other rules. very effective for improving parsing performance (McClosky et al., 2006; McClosky and Charniak, 2008). Self-training of the TEES system has been previously explored (Bjorne et al., 2012), with somewhat mixed results, but with evidence suggesting it could be useful with an appropriate strategy for selecting training examples. Here, rather than training our system with its own output over external data, we explore a semi-supervised learning approach in which we train our system with the outputs of a different system (TEES) over external data. 2 In principle, this set of rules could then be directly applied to the test documents, by searching for any matching subgraphs. However, in practice doin"
W13-2005,P11-2121,0,0.0309941,"for each event type. We also tested adding as many instances per event-type as there were in the manually-annotated dataset, with different multiplying factors. Finally, we evaluated the effect of using different splits of the data for the evaluation and optimisation steps of ASM. This is the full list of parameters that we tested over held-out data: 2.1.2 Parsing Pipeline In our parsing pipeline, we first split sentences using the JULIE Sentence Boundary Detector, or JSBD (Tomanek et al., 2007). We then parse using a version of clearnlp1 (Choi and McCallum, 2013), a successor to ClearParser (Choi and Palmer, 2011), which was shown to have stateof-the-art performance over the CRAFT corpus of full-text biomedical articles (Verspoor et al., 2012). We use dependency and POS-tagging models trained on the CRAFT corpus (except where noted); these pre-trained models are provided with clearnlp. Our fork of clearnlp integrates token span marking into the parsing process, so the dependency nodes can easily be matched to the standoff annotations provided with the shared task data. This pipeline is not dependent on any preannotated data, so can thus be trivially applied to extra data not provided as part of the sha"
W13-2005,N06-1020,0,0.0244081,"ents (e.g., pos35 Proceedings of the BioNLP Shared Task 2013 Workshop, pages 35–44, c Sofia, Bulgaria, August 9 2013. 2013 Association for Computational Linguistics nent of an event rule. The rule also consists of an event type, and a mapping from event arguments to nodes from the pattern graph, or to an event type/node pair for nested event arguments. After processing all training documents, we get on the order of a few thousand rules; this can be decreased slightly by removing rules with subgraphs that are isomorphic to those of other rules. very effective for improving parsing performance (McClosky et al., 2006; McClosky and Charniak, 2008). Self-training of the TEES system has been previously explored (Bjorne et al., 2012), with somewhat mixed results, but with evidence suggesting it could be useful with an appropriate strategy for selecting training examples. Here, rather than training our system with its own output over external data, we explore a semi-supervised learning approach in which we train our system with the outputs of a different system (TEES) over external data. 2 In principle, this set of rules could then be directly applied to the test documents, by searching for any matching subgra"
W13-2005,W00-0901,0,0.0142391,"e basic set of cue lemmas came from a variety of sources. Some were manually specified and some were derived from previous work on modification detection (Cohen et al., 2011; MacKinlay et al., 2012). We manually expanded this cue list to include obvious derivational variants. This gave us a basic set of 34 S PECULA TION and 21 N EGATION cues. (Genetic Phenomena[MH] OR Metabolic Phenomena[MH] OR Cell Physiological Phenomena[MH] OR Biochemical Processes[MH]) AND open access[filter] We also used a data-driven strategy to find additional lemmas indicative of modification. We adapted the method of Rayson and Garside (2000) which uses log-likelihood for finding words that characterise differences between corpora. Here, the “corpora” are sentences attached to all events in the training set, and sentences attached to events which are subject to N EGATION or S PECULATION (treated separately). We build a frequency distribution over lemmas in each set of sentences, and calculate the log-likelihood for all lemmas, usFurthermore, the articles were split into sections and specific sections from the full text like Introduction, Background and Methods were removed to reduce the quantity of text to be annotated by TEES. Th"
W13-2005,W08-1301,0,0.02079,"Missing"
W13-2005,W09-1401,0,0.130196,"Introduction In this paper, we describe our submission to the Genia Event (GE) information extraction subtask of the BioNLP Shared Task. This task requires the development of systems that are capable of identifying bio-molecular events as those events are expressed in full-text publications. The task represents an important contribution to the broader problem of converting unstructured information captured in the biomedical literature into structured information that can be used to index and analyse bio-molecular relationships. This year’s task builds on previous instantiations of this task (Kim et al., 2009; Kim et al., 2012), with only minor changes in the task definition introduced for 2011. The task organisers provided full text publications annotated with mentions of biological entities including proteins and genes, and asked participants to provide annotations of simple events including gene expression, binding, localization, and protein modification, as well as higher-order regulation events (e.g., pos35 Proceedings of the BioNLP Shared Task 2013 Workshop, pages 35–44, c Sofia, Bulgaria, August 9 2013. 2013 Association for Computational Linguistics nent of an event rule. The rule also cons"
W13-2005,W11-0204,0,0.0426084,"Missing"
W13-2005,W11-1826,1,\N,Missing
W13-2005,W11-1828,0,\N,Missing
W13-2010,W11-1801,0,0.299573,"Missing"
W13-2010,P03-1054,0,0.00376253,"word individually on event extraction. 3.2 4 4.1 Implementation Preprocessing We employed the preprocessed data in the BioC (Comeau et al., 2013) compliant XML format provided by the shared task organizers as supporting resources. The BioC project attempts to address the interoperability among existing natural language processing tools by providing a unified BioC XML format. The supporting analyses include tokenization, sentence segmentation, POS tagging and lemmatization. Different syntactic parsers analyze text based on different underlying methodologies, for instances, the Stanford parser (Klein and Manning, 2003) performs joint inference over the product of an unlexicalized Probabilistic Context-Free Grammar (PCFG) parser and a lexicalized dependency parser while the McClosky-Charniak-Johnson (Charniak) parser (McClosky and Charniak, 2008) is based on N -best parse reranking over a lexicalized PCFG model. In order to take advantage of multiple aspects of structural analysis of sentences, both Stanford parser and Charniak parser, which are among the best performing parsers trained on the GENIA Treebank corpus, are used to parse the training sentences and produce dependency graphs for learning event rul"
W13-2010,W11-1826,1,0.862651,"Missing"
W13-2010,H05-1091,0,0.268346,"vent, the shortest dependency path connecting the event trigger to each event argument in the undirected version of the graph is selected. While additional information such as individual words in each sentence (bag-of-words), sequences of words (n-grams) and semantic concepts is typically used in the state-ofthe-art supervised learning-based systems to cover a broader context (Airola et al., 2008; Buyko et al., 2009; Bj¨orne et al., 2012), the shortest path between two tokens in the dependency graph is particularly likely to carry the most valuable information about their mutual relationship (Bunescu and Mooney, 2005a; Thomas et al., 2011b; Rinaldi et al., 2010). In case there exists more than one shortest path, all of them are considered. For multi-token event triggers, the shortest path connecting every trigger token to each event argument is extracted, and the union of the paths is then computed for each trigger. For regulatory events that take a sub-event as an argument, the shortest path is extracted so as to connect the trigger of the main event to that of the sub-event. For complex events that involve multiple arguments, we computed the dependency path union of all shortest paths from trigger to ea"
W13-2010,W09-1403,0,0.17202,"ased Event Extraction Framework 2.1 Rule Induction Event rules are learned automatically using the following method. Starting with the dependency graph of each training sentence, for each annotated event, the shortest dependency path connecting the event trigger to each event argument in the undirected version of the graph is selected. While additional information such as individual words in each sentence (bag-of-words), sequences of words (n-grams) and semantic concepts is typically used in the state-ofthe-art supervised learning-based systems to cover a broader context (Airola et al., 2008; Buyko et al., 2009; Bj¨orne et al., 2012), the shortest path between two tokens in the dependency graph is particularly likely to carry the most valuable information about their mutual relationship (Bunescu and Mooney, 2005a; Thomas et al., 2011b; Rinaldi et al., 2010). In case there exists more than one shortest path, all of them are considered. For multi-token event triggers, the shortest path connecting every trigger token to each event argument is extracted, and the union of the paths is then computed for each trigger. For regulatory events that take a sub-event as an argument, the shortest path is extracte"
W13-2010,P08-2026,0,0.0425141,"es. The BioC project attempts to address the interoperability among existing natural language processing tools by providing a unified BioC XML format. The supporting analyses include tokenization, sentence segmentation, POS tagging and lemmatization. Different syntactic parsers analyze text based on different underlying methodologies, for instances, the Stanford parser (Klein and Manning, 2003) performs joint inference over the product of an unlexicalized Probabilistic Context-Free Grammar (PCFG) parser and a lexicalized dependency parser while the McClosky-Charniak-Johnson (Charniak) parser (McClosky and Charniak, 2008) is based on N -best parse reranking over a lexicalized PCFG model. In order to take advantage of multiple aspects of structural analysis of sentences, both Stanford parser and Charniak parser, which are among the best performing parsers trained on the GENIA Treebank corpus, are used to parse the training sentences and produce dependency graphs for learning event rules. Only the Charniak parser is used on the testing sentences in the event extraction phase. Adopting All-paths for Event Rules Airola et al. proposed an all-paths graph (APG) kernel for extracting protein-protein interactions (PPI"
W13-2010,W09-1401,0,0.402246,"Missing"
W13-2010,W11-0201,0,0.0956249,"cy path connecting the event trigger to each event argument in the undirected version of the graph is selected. While additional information such as individual words in each sentence (bag-of-words), sequences of words (n-grams) and semantic concepts is typically used in the state-ofthe-art supervised learning-based systems to cover a broader context (Airola et al., 2008; Buyko et al., 2009; Bj¨orne et al., 2012), the shortest path between two tokens in the dependency graph is particularly likely to carry the most valuable information about their mutual relationship (Bunescu and Mooney, 2005a; Thomas et al., 2011b; Rinaldi et al., 2010). In case there exists more than one shortest path, all of them are considered. For multi-token event triggers, the shortest path connecting every trigger token to each event argument is extracted, and the union of the paths is then computed for each trigger. For regulatory events that take a sub-event as an argument, the shortest path is extracted so as to connect the trigger of the main event to that of the sub-event. For complex events that involve multiple arguments, we computed the dependency path union of all shortest paths from trigger to each event argument, res"
W14-5202,cassidy-etal-2014-alveo,1,0.742094,"te data. Workflows in Galaxy can be stored, shared and published, and we hope this will also become a way for human communication science researchers to codify and exchange common analyses. A number of the tools listed in Table 2 have been packaged as Python scripts, for instance NLTK 13stemming and parsing. Other tools are implemented based scripts to carry out part-of-speech tagging, in R, e.g. EMU/R and ParseEval. An API is provided to mediate access to data, ensuring that permissions are respected, and providing a way to access individual items, and &apos;mount&apos; datasets for fast access (Steve Cassidy, Estival, Jones, Burnham, & Burghold, 2014). An instance of the Galaxy Workflow engine is run on a virtual machine in the NeCTAR Research Cloud, a secure platform for Australian research, funded by the same government program (https://www.nectar.org.au/research-cloud). Finally, a UIMA interface has been developed to enable the conversion of Alveo items, as well as their associated annotations, into UIMA CAS documents, for analysis in a conventional UIMA pipeline. Conversely annotations from a UIMA pipeline can be associated with a document in Alveo. Figure 1 gives an overview of the architecture. Figure 1: The architecture of the Alve"
W14-5202,cassidy-etal-2012-australian,1,0.834685,"on are also functionalities which are available in a consistent manner across the data collections through the web-based Discovery Interface. The first phase of the project (December 2012 – June 2014) saw the inclusion of the collections shown in Table 1. 1. PARADISEC (Pacific and Regional Archive for Digital Sources in Endangered Cultures): audio, video, text and image resources for Australian and Pacific Island languages (Thieberger, Barwick, Billington, & Vaughan, 2011) 2. AusTalk, audio-visual speech corpus of Australian English (Burnham et al., 2011) 3. The Australian National Corpus (S. Cassidy, Haugh, Peters, & Fallu, 2012) comprising: Australian Corpus of English (ACE); Australian Radio Talkback (ART); AustLit; Braided Channels; Corpus of Oz Early English (COOEE); Griffith Corpus of Spoken English (GCSAusE); International Corpus of English (ICEAUS); Mitchell & Delbridge corpus; Monash Corpus of Spoken English (Musgrave & Haugh, 2009). 4. AVOZES, a visual speech corpus (Goecke & Millar, 2004) 5. UNSW Pixar Emotional Music Excerpts: Pixar movie theme music expressing different emotions 6. Sydney University Room Impulse Responses: environmental audio samples which, through convolution with speech or music, can cr"
W14-5202,kano-etal-2010-u,0,0.0308406,"hical interface. In contrast to Alveo, documents are uploaded to the system within an individual user space, and resulting annotations are not persisted outside of the UIMA data structures; although they can be serialised and stored for subsequent re-use in processing pipelines, or exported as RDF, they are not directly accessible within the framework itself. The repository contains a wide range of NLP components, e.g., modules to perform sentence splitting, POS tagging, parsing, and a number of information extraction tasks targeted to biomedical text. The U-Compare system (Kano et al., 2009; Kano, Dorado, McCrohon, Ananiadou, & Tsujii, 2010) also supports evaluation and performance comparison of UIMA-based automated annotation tools. It was designed with UIMA in mind from the ground up, enabling UIMA workflow creation and execution through a GUI. Therefore it assumes that all analysis of collections is performed with a set of UIMA components, and indeed provides a substantial number of such components in their repository, although other components can be added. The system is launched locally via Java Web Start; given recent changes to how browsers interact with Java, this no longer works reliably and off-line use (after download"
W14-5202,rak-etal-2012-collaborative,0,0.0175847,"standard Collection Reader from Alveo-UIMA, a basic POS-tagging CAS annotator from DKProCore, 6 and the annotation uploading CAS annotator from Alveo-UIMA. An advanced version also demonstrates implementing an interface which remaps the POS tag types from those automaticallyderived from DKPro.outputs. 5 5.1 Discussion Related Work There are several frameworks that have been developed to enable development and evaluation of text processing workflows, and UIMA has been used as the backbone for a few such frameworks due to its support for processing module interoperability. The Argo web service (Rak, Rowley, & Ananiadou, 2012; Rak, Rowley, Carter, & Ananiadou, 2013) is a recent web application that enables development of UIMA-based text processing workflows through an on-line graphical interface. In contrast to Alveo, documents are uploaded to the system within an individual user space, and resulting annotations are not persisted outside of the UIMA data structures; although they can be serialised and stored for subsequent re-use in processing pipelines, or exported as RDF, they are not directly accessible within the framework itself. The repository contains a wide range of NLP components, e.g., modules to perform"
W14-5202,P13-4020,0,0.0335049,"Missing"
W14-5202,P13-4004,0,0.0670606,"Missing"
W14-5202,P05-1022,0,\N,Missing
W16-2301,W13-3520,0,0.0148252,"Missing"
W16-2301,2011.mtsummit-papers.35,0,0.440323,"Missing"
W16-2301,W15-3001,1,0.419439,"Missing"
W16-2301,W16-2305,0,0.0273073,"Missing"
W16-2301,L16-1662,0,0.00889007,"introduce a new set of features for the sentence-Level QE task. The features extracted include three alignment-based features, three bilingual embedding-based features, two embeddingbased features constrained on alignment links, as well as a set of 74 bigrams used as boolean features. The set of bigrams represents the most frequent bigrams in translations that have changed after the post-edition, and they are compiled by aligning translations to their post-editions provided in the WMT QE datasets. To produce these features, GIZA++ (Och and Ney, 2003) was used for word alignment and Multivec (Berard et al., 2016) was used for the bilingual model, which jointly learns distributed representations for source and target languages using a parallel corpus. To build the bilingual model, domain-specific data compiled from the resources made available for the WMT 16 IT-Domain shared task was used. As prediction model, a Linear Regression model using scikit-learn was built using a combination of QuEst++ baseline features and the new features proposed. tional features for both tasks. UNBABEL (Task 2): Two systems were submitted for the word-level task. UNBABEL 2 linear is a feature-based linear sequential model."
W16-2301,W11-2101,1,0.851508,"Missing"
W16-2301,W16-2306,0,0.0263259,"Missing"
W16-2301,W16-2382,0,0.0451767,"Missing"
W16-2301,W16-2302,1,0.346609,"Missing"
W16-2301,W16-2307,1,0.756182,"Missing"
W16-2301,W16-2308,0,0.0373021,"Missing"
W16-2301,buck-etal-2014-n,0,0.0150538,"Missing"
W16-2301,W13-2201,1,0.287085,"Missing"
W16-2301,W14-3302,1,0.399147,"Missing"
W16-2301,W14-3340,1,0.419045,"Missing"
W16-2301,W07-0718,1,0.759293,"oth automatically and manually. The human evaluation (§3) involves asking human judges to rank sentences output by anonymized systems. We obtained large numbers of rankings from researchers who contributed The quality estimation task had three subtasks, with a total of 14 teams, submitting 39 entries. The automatic post-editing task had a total of 6 teams, submitting 11 entries. 1 Introduction We present the results of the shared tasks of the First Conference on Statistical Machine Translation (WMT) held at ACL 2016. This conference builds on nine previous WMT workshops (Koehn and Monz, 2006; Callison-Burch et al., 2007, 2008, 2009, 2010, 2011, 2012; Bojar et al., 2013, 2014, 2015). 131 Proceedings of the First Conference on Machine Translation, Volume 2: Shared Task Papers, pages 131–198, c Berlin, Germany, August 11-12, 2016. 2016 Association for Computational Linguistics to refine evaluation and estimation methodologies for machine translation. As before, all of the data, translations, and collected human judgments are publicly available.1 We hope these datasets serve as a valuable resource for research into statistical machine translation and automatic evaluation or prediction of translation quality. New"
W16-2301,W15-3025,1,0.888663,"used for the Task 2p. The submissions to the word-level task are modified in order to comply with the phrase-level task. sequences. Nevertheless, it is worth noticing the phraselevel QE systems introduced a number of interesting strategies that allowed them to outperform a strong baseline phrase-level model. Finally, we recall that the evaluation metric – word-level F1 mult – has difficulties to distinguish phrase-level systems. This suggests that we may need to find a different metric for evaluation of the phrase-level task, with phrase-level F1 -mult one of the candidates. 7 pointed out by Chatterjee et al. (2015b), from the application point of view the task is motivated by its possible uses to: • Improve MT output by exploiting information unavailable to the decoder, or by performing deeper text analysis that is too expensive at the decoding stage; • Cope with systematic errors of an MT system whose decoding process is not accessible; • Provide professional translators with improved MT output quality to reduce (human) post-editing effort; Automatic Post-editing Task This year WMT hosted the second round of the shared task on MT automatic post-editing (APE), which consists in automatically correcting"
W16-2301,P15-2026,1,0.895785,"used for the Task 2p. The submissions to the word-level task are modified in order to comply with the phrase-level task. sequences. Nevertheless, it is worth noticing the phraselevel QE systems introduced a number of interesting strategies that allowed them to outperform a strong baseline phrase-level model. Finally, we recall that the evaluation metric – word-level F1 mult – has difficulties to distinguish phrase-level systems. This suggests that we may need to find a different metric for evaluation of the phrase-level task, with phrase-level F1 -mult one of the candidates. 7 pointed out by Chatterjee et al. (2015b), from the application point of view the task is motivated by its possible uses to: • Improve MT output by exploiting information unavailable to the decoder, or by performing deeper text analysis that is too expensive at the decoding stage; • Cope with systematic errors of an MT system whose decoding process is not accessible; • Provide professional translators with improved MT output quality to reduce (human) post-editing effort; Automatic Post-editing Task This year WMT hosted the second round of the shared task on MT automatic post-editing (APE), which consists in automatically correcting"
W16-2301,W08-0309,1,0.809107,"Missing"
W16-2301,W16-2309,0,0.0305938,"Missing"
W16-2301,W10-1703,1,0.312318,"Missing"
W16-2301,W16-2336,0,0.0356983,"Missing"
W16-2301,W12-3102,1,0.235434,"ir native language. Consequently, health professionals may use the translated information to make clinical decisions impacting patients care. It is vital that translation systems do not contribute to the dissemination of incorrect clinical information. Therefore, the evaluation of biomedical translation systems should include an assessment at the document level indicating whether a translation conveyed erroneous clinical information. 6 Quality Estimation The fifth edition of the WMT shared task on quality estimation (QE) of machine translation (MT) builds on the previous editions of the task (Callison-Burch et al., 2012; Bojar et al., 2013, 2014, 2015), with “traditional” tasks at sentence and word levels, a new task for entire documents quality prediction, and a variant of the word-level task: phrase-level estimation. The goals of this year’s shared task were: • To advance work on sentence and wordlevel quality estimation by providing domainspecific, larger and professionally annotated datasets. • To analyse the effectiveness of different types of quality labels provided by humans for longer texts in document-level prediction. • To investigate quality estimation at a new level of granularity: phrases. Plan"
W16-2301,W16-2330,0,0.0328047,"Missing"
W16-2301,W16-2310,1,0.835279,"Missing"
W16-2301,W11-2103,1,0.65163,"Missing"
W16-2301,W16-2331,0,0.0203904,"cance test results for pairs of systems competing in the news domain translation task (en-ru), where a green cell denotes a significantly higher DA adequacy score for the system in a given row over the system in a given column. cs-en fi-en tr-en de-en ru-en ro-en 0.997 0.996 0.988 0.964 0.961 0.920 en-ru 0.975 DA Correlation with RR Table 11: Correlation between overall DA standardized mean adequacy scores and RR Trueskill scores. 150 4 IT Translation Task from the QTLeap project: HF&FCUL for Portuguese, UPV/EHU for Spanish and Basque, IICT-BAS for Bulgarian, CUNI for Czech and UG for Dutch). Duma and Menzel (2016) describe UHDS- DOC 2 VEC and UHBS- LMI (University of Hamburg). Pahari et al. (2016) describe JU-USAAR (Jadavpur University & Saarland University). Cuong et al. (2016) describe ILLC-U VA-S CORPIO (University of Amsterdam). IILC-U VA-DS is based on Hoang and Sima’an (2014). PROMT-RULE - BASED and PROMT-H YBRID systems were submitted by the PROMT LLC company and they are not described in any paper. QTL -M OSES is the standard Moses setup (MERT-tuned on the in-domain training data, but otherwise without any domain-adaptation) and serves as a baseline. The IT-domain translation task introduced th"
W16-2301,W15-3009,1,0.788321,"Missing"
W16-2301,P15-1174,1,0.28154,"endently inserted in another part of this sentence, i.e. to correct an unrelated error. The statistics of the datasets are outlined in Table 20. Evaluation Evaluation was performed against the true HTER label and/or ranking, using the following metrics: • Scoring: Pearson’s r correlation score (primary metric, official score for ranking submissions), Mean Average Error (MAE) and Root Mean Squared Error (RMSE). • Ranking: Spearman’s ρ rank correlation and DeltaAvg. Statistical significance on Pearson r and Spearman rho was computed using the William’s test, following the approach suggested in (Graham, 2015). Results Table 19 summarises the results for Task 1, ranking participating systems best to worst using Pearson’s r correlation as primary key. Spearman’s ρ correlation scores should be used to rank systems according to the ranking variant. We note that three systems have not submitted results ranking evaluation variant. 6.4 Task 2: Predicting word-level quality The goal of this task is to evaluate the extent to which we can detect word-level errors in MT output. Various classes of errors can be found in translations, but for this task we consider all error types together, aiming at making a b"
W16-2301,W16-2311,0,0.0122367,"016) PROMT Automated Translation Solutions (Molchanov and Bykov, 2016) QT21 System Combination (Peter et al., 2016b) RWTH Aachen (Peter et al., 2016a) ¨ TUBITAK (Bektas¸ et al., 2016) University of Edinburgh (Sennrich et al., 2016) University of Edinburgh (Williams et al., 2016) UEDIN - SYNTAX UEDIN - LMU UH -* USFD - RESCORING UUT YSDA ONLINE -[ A , B , F, G ] University of Edinburgh / University of Munich (Huck et al., 2016) University of Helsinki (Tiedemann et al., 2016) University of Sheffield (Blain et al., 2016) Uppsala University (Tiedemann et al., 2016) Yandex School of Data Analysis (Dvorkovich et al., 2016) Four online statistical machine translation systems Table 2: Participants in the shared translation task. Not all teams participated in all language pairs. The translations from the commercial and online systems were not submitted by their respective companies but were obtained by us, and are therefore anonymized in a fashion consistent with previous years of the workshop. 136 Figure 2: Screenshot of the Appraise interface used in the human evaluation campaign. The annotator is presented with a source segment, a reference translation, and up to five outputs from competing systems (anonymized"
W16-2301,W13-2305,1,0.509145,"Missing"
W16-2301,W15-3036,0,0.0603245,"Missing"
W16-2301,E14-1047,1,0.831614,"Missing"
W16-2301,W16-2378,0,0.168663,"ts for pairs of systems competing in the news domain translation task (en-ru), where a green cell denotes a significantly higher DA adequacy score for the system in a given row over the system in a given column. cs-en fi-en tr-en de-en ru-en ro-en 0.997 0.996 0.988 0.964 0.961 0.920 en-ru 0.975 DA Correlation with RR Table 11: Correlation between overall DA standardized mean adequacy scores and RR Trueskill scores. 150 4 IT Translation Task from the QTLeap project: HF&FCUL for Portuguese, UPV/EHU for Spanish and Basque, IICT-BAS for Bulgarian, CUNI for Czech and UG for Dutch). Duma and Menzel (2016) describe UHDS- DOC 2 VEC and UHBS- LMI (University of Hamburg). Pahari et al. (2016) describe JU-USAAR (Jadavpur University & Saarland University). Cuong et al. (2016) describe ILLC-U VA-S CORPIO (University of Amsterdam). IILC-U VA-DS is based on Hoang and Sima’an (2014). PROMT-RULE - BASED and PROMT-H YBRID systems were submitted by the PROMT LLC company and they are not described in any paper. QTL -M OSES is the standard Moses setup (MERT-tuned on the in-domain training data, but otherwise without any domain-adaptation) and serves as a baseline. The IT-domain translation task introduced th"
W16-2301,W11-2123,0,0.0116622,"s were also evaluated against a reimplementation of the approach firstly proposed by Simard et al. (2007).36 Last year, in fact, this statistical post-editing approach represented the common backbone of all submissions (this is also reflected by the close results achieved by participants in the pilot task). For this purpose, a phrasebased SMT system based on Moses (Koehn et al., 2007) was used. Translation and reordering models were estimated following the Moses protocol with default setup using MGIZA++ (Gao and Vogel, 2008) for word alignment. For language modeling we used the KenLM toolkit (Heafield, 2011) for standard n-gram modeling with an n-gram length of 5. Finally, the APE system was tuned on Adam Mickiewicz University. This system is among the very first ones exploring the application of neural translation models to the APE task. In particular, it investigates the following aspects: i) the use of artificially-created postedited data to train the neural models, ii) the loglinear combination of monolingual and bilingual models in an ensemble-like manner, iii) the addition of task-specific features in the log-linear model to control the final output quality. Concerning the data, in addition"
W16-2301,W16-2384,0,0.0416841,"Missing"
W16-2301,W13-0805,0,0.0110515,"of words in the reference. Lower TER values indicate lower distance from the reference as a proxy for higher MT quality. 33 http://www.cs.umd.edu/˜snover/tercom/ 34 https://github.com/moses-smt/mosesdecoder/ blob/master/scripts/generic/multi-bleu.perl 27 The source sentences (together with their reference translations which were not used for the task) were provided by TAUS (https://www.taus.net/) and originally come from a unique IT vendor. 28 It consists of a phrase-based machine translation system leveraging generic and in-domain parallel training data and using a pre-reordering technique (Herrmann et al., 2013). It takes also advantages of POS and word class-based language models. 29 German native speakers working at Text&Form https: //www.textform.com/. 176 Train (12,000) Dev (1,000) Test (2,000) SRC 201,505 17,827 31,477 Tokens TGT 210,573 19,355 34,332 PE 214,720 19,763 35,276 Table 31: Types SRC TGT 9,328 14,185 2,931 3,333 3,908 4,695 Data statistics. APE@WMT15 APE@WMT16 (EN-ES, news, crowd) (EN-DE, IT, prof.) SRC 2.905 6.616 TGT 3.312 8.845 PE 3.085 8.245 Table 32: Repetition Rate (RR) of the WMT15 (EnglishSpanish, news domain, crowdsourced post-edits) and WMT16 (English-German, IT domain, pro"
W16-2301,W16-2315,1,0.824148,"Missing"
W16-2301,P07-2045,1,0.017773,"gets unmodified.35 Baseline results are reported in Table 34. 7.2 Participants Monolingual translation as another term of comparison. To get some insights about the progress with respect to the first pilot task, participating systems were also evaluated against a reimplementation of the approach firstly proposed by Simard et al. (2007).36 Last year, in fact, this statistical post-editing approach represented the common backbone of all submissions (this is also reflected by the close results achieved by participants in the pilot task). For this purpose, a phrasebased SMT system based on Moses (Koehn et al., 2007) was used. Translation and reordering models were estimated following the Moses protocol with default setup using MGIZA++ (Gao and Vogel, 2008) for word alignment. For language modeling we used the KenLM toolkit (Heafield, 2011) for standard n-gram modeling with an n-gram length of 5. Finally, the APE system was tuned on Adam Mickiewicz University. This system is among the very first ones exploring the application of neural translation models to the APE task. In particular, it investigates the following aspects: i) the use of artificially-created postedited data to train the neural models, ii)"
W16-2301,W16-2337,0,0.0376798,"Missing"
W16-2301,W16-2303,1,0.821661,"Missing"
W16-2301,L16-1582,1,0.790825,"Missing"
W16-2301,W16-2385,0,0.0304494,"Missing"
W16-2301,P16-2095,1,0.885979,"e package to extract these and other quality estimation features and perform model learning, with suggested methods for all levels of prediction. Participants, described in Section 6.2, could submit up to two systems for each task. Data used to build MT systems or internal system information (such as model scores or n-best lists) were made available on request for Tasks 1 and 2. 6.1 The γ,  and C parameters were optimised via grid search with 5-fold cross validation on the training set. Word-level baseline system: For Tasks 2 and 2p, the baseline features were extracted with the Marmot tool (Logacheva et al., 2016b). For the baseline system we used a number of features that have been found the most informative in previous research on word-level QE. Our baseline set of features is loosely based on the one described in (Luong et al., 2014). It contains the following 22 features: • Word count in the source and target sentences, source and target token count ratio. Although these features are sentence-level (i.e. their values will be the same for all words in a sentence), the length of a sentence might influence the probability of a word being wrong. Baseline systems Sentence-level baseline system: For Tas"
W16-2301,W15-3037,0,0.00849978,"stems were submitted for the word-level task. UNBABEL 2 linear is a feature-based linear sequential model. It uses the baseline features provided by the shared task organisers (with slight changes) conjoined with individual labels and pairs of consecutive labels. It also uses various syntactic dependency-based features (dependency relations, heads, and second-order structures like siblings and grandparents). The syntactic dependencies are predicted with TurboParser trained on the TIGER German treebank. UNBABEL 2 ensemble uses a stacked architecture, inspired by the last year’s QUETCH+ system (Kreutzer et al., 2015), which combines three neural systems: one feedforward and two recurrent ones. The predictions of these systems are added as additional features in the linear system above. The following external resources were used: part-of-speech tags and extra syntactic dependency information obtained with TurboTagger and TurboParser (Martins et al., 2013), trained on the Penn Treebank (for English) and on the version of the German TIGER corpus used in the SPMRL shared task (Seddah et al., 2014) for German. For the neural models, pretrained word embeddings from Polyglot (AlRfou et al., 2013) and those produ"
W16-2301,W14-3342,0,0.0237077,"Data used to build MT systems or internal system information (such as model scores or n-best lists) were made available on request for Tasks 1 and 2. 6.1 The γ,  and C parameters were optimised via grid search with 5-fold cross validation on the training set. Word-level baseline system: For Tasks 2 and 2p, the baseline features were extracted with the Marmot tool (Logacheva et al., 2016b). For the baseline system we used a number of features that have been found the most informative in previous research on word-level QE. Our baseline set of features is loosely based on the one described in (Luong et al., 2014). It contains the following 22 features: • Word count in the source and target sentences, source and target token count ratio. Although these features are sentence-level (i.e. their values will be the same for all words in a sentence), the length of a sentence might influence the probability of a word being wrong. Baseline systems Sentence-level baseline system: For Task 1, QuEst++13 (Specia et al., 2015) was used to extract 17 features from the SMT source/target language training corpus: • Number of tokens in source & target sentences. • Target token, its left and right contexts of one word."
W16-2301,W16-2318,0,0.0145349,"Missing"
W16-2301,N06-1014,0,0.0215966,"d on n previous translation and reordering decisions. This technique is able to model both local and long-range reorderings that are quite useful when dealing with the German language. To improve the capability of choosing the correct edit to process, eight new features are added to the loglinear model. These features capture the cost of deleting a phrase and different information on possible gaps in reordering operations. The monolingual alignments between the MT outputs and their post-edits are computed using different methods based on TER, METEOR (Snover et al., 2006) and Berkeley Aligner (Liang et al., 2006). Only the task data is used for these submissions. 7.3 179 TER/BLEU results ID AMU Primary AMU Contrastive FBK Contrastive FBK Primary USAAR Primary USAAR Constrastive CUNI Primary (Simard et al., 2007) Baseline DCU Contrastive JUSAAR Primary JUSAAR Contrastive DCU Primary Avg. TER 21.52 23.06 23.92 23.94 24.14 24.14 24.31 24.64 24.76 26.79 26.92 26.97 28.97 BLEU 67.65 66.09 64.75 64.75 64.10 64.00 63.32 63.47 62.11 58.60 59.44 59.18 55.19 tained this year by the top runs can only be reached by moving from the basic statistical MT backbone shared by all last year’s participants to new and mor"
W16-2301,P13-2109,0,0.0179517,"eads, and second-order structures like siblings and grandparents). The syntactic dependencies are predicted with TurboParser trained on the TIGER German treebank. UNBABEL 2 ensemble uses a stacked architecture, inspired by the last year’s QUETCH+ system (Kreutzer et al., 2015), which combines three neural systems: one feedforward and two recurrent ones. The predictions of these systems are added as additional features in the linear system above. The following external resources were used: part-of-speech tags and extra syntactic dependency information obtained with TurboTagger and TurboParser (Martins et al., 2013), trained on the Penn Treebank (for English) and on the version of the German TIGER corpus used in the SPMRL shared task (Seddah et al., 2014) for German. For the neural models, pretrained word embeddings from Polyglot (AlRfou et al., 2013) and those produced with a neural MT system (Bahdanau et al., 2014) were used. UGENT-LT3 (Task 1, Task 2): The submissions for the word-level task use 41 features in combination with the baseline feature set to train binary classifiers. The 41 additional features attempt to capture accuracy errors (concerned with the meaning transfer from the source to targe"
W16-2301,W16-2387,0,0.0253511,"Missing"
W16-2301,W16-2317,0,0.0414065,"Missing"
W16-2301,N13-1090,0,0.00833788,"Missing"
W16-2301,2015.iwslt-papers.4,1,0.737239,"Missing"
W16-2301,W16-2319,0,0.0339318,"Missing"
W16-2301,W16-2386,1,0.814188,"e package to extract these and other quality estimation features and perform model learning, with suggested methods for all levels of prediction. Participants, described in Section 6.2, could submit up to two systems for each task. Data used to build MT systems or internal system information (such as model scores or n-best lists) were made available on request for Tasks 1 and 2. 6.1 The γ,  and C parameters were optimised via grid search with 5-fold cross validation on the training set. Word-level baseline system: For Tasks 2 and 2p, the baseline features were extracted with the Marmot tool (Logacheva et al., 2016b). For the baseline system we used a number of features that have been found the most informative in previous research on word-level QE. Our baseline set of features is loosely based on the one described in (Luong et al., 2014). It contains the following 22 features: • Word count in the source and target sentences, source and target token count ratio. Although these features are sentence-level (i.e. their values will be the same for all words in a sentence), the length of a sentence might influence the probability of a word being wrong. Baseline systems Sentence-level baseline system: For Tas"
W16-2301,L16-1470,1,0.826657,"Missing"
W16-2301,P03-1021,0,0.0340784,"7 Tokens TGT 210,573 19,355 34,332 PE 214,720 19,763 35,276 Table 31: Types SRC TGT 9,328 14,185 2,931 3,333 3,908 4,695 Data statistics. APE@WMT15 APE@WMT16 (EN-ES, news, crowd) (EN-DE, IT, prof.) SRC 2.905 6.616 TGT 3.312 8.845 PE 3.085 8.245 Table 32: Repetition Rate (RR) of the WMT15 (EnglishSpanish, news domain, crowdsourced post-edits) and WMT16 (English-German, IT domain, professional posteditors) APE Task data. 7.1.3 PE 16,388 3,506 5,047 SRC 5,628 1,922 2,479 Lemmas TGT PE 11,418 13,244 2,686 2,806 3,753 4,050 the development set, optimizing TER/BLEU with Minimum Error Rate Training (Och, 2003). The results of this additional term of comparison are also reported in Table 34. For each submitted run, the statistical significance of performance differences with respect to the baselines and the re-implementation of Simard et al. (2007) was calculated with the bootstrap test (Koehn, 2004). Baseline The official baseline results are the TER and BLEU scores calculated by comparing the raw MT output with the human post-edits. In practice, the baseline APE system is a system that leaves all the test targets unmodified.35 Baseline results are reported in Table 34. 7.2 Participants Monolingual"
W16-2301,W16-2338,0,0.0374669,"Missing"
W16-2301,J03-1002,0,0.0268639,"word alignments and bilingual distributed representations to introduce a new set of features for the sentence-Level QE task. The features extracted include three alignment-based features, three bilingual embedding-based features, two embeddingbased features constrained on alignment links, as well as a set of 74 bigrams used as boolean features. The set of bigrams represents the most frequent bigrams in translations that have changed after the post-edition, and they are compiled by aligning translations to their post-editions provided in the WMT QE datasets. To produce these features, GIZA++ (Och and Ney, 2003) was used for word alignment and Multivec (Berard et al., 2016) was used for the bilingual model, which jointly learns distributed representations for source and target languages using a parallel corpus. To build the bilingual model, domain-specific data compiled from the resources made available for the WMT 16 IT-Domain shared task was used. As prediction model, a Linear Regression model using scikit-learn was built using a combination of QuEst++ baseline features and the new features proposed. tional features for both tasks. UNBABEL (Task 2): Two systems were submitted for the word-level tas"
W16-2301,W16-2321,0,0.0372138,"Missing"
W16-2301,W16-2388,1,0.748184,"Missing"
W16-2301,W16-2333,0,0.0259256,"Missing"
W16-2301,W15-3026,0,0.0339859,"Missing"
W16-2301,W16-2379,1,0.819672,"Missing"
W16-2301,W16-2334,1,0.811656,"Missing"
W16-2301,P02-1040,0,0.105557,", eventually, make the APE task more feasible by automatic systems. Other changes concern the language combination and the evaluation mode. As regards the languages, we moved from English-Spanish to English-German, which is one of the language pairs covered by the QT21 Project26 that supported data collection and post-editing. Concerning the evaluation, we changed from TER scores computed both in case-sensitive and caseinsensitive mode to a single ranking based on case sensitive measurements. Besides these changes the new round of the APE task included some extensions in the evaluation. BLEU (Papineni et al., 2002) has been introduced as a secondary evaluation metric to measure the improvements over the rough MT output. In addition, to gain further insights on final output quality, a subset of the outputs of the submitted systems has also been manually evaluated. Based on these changes and extensions, the goals of this year’s shared task were to: i) improve and stabilize the evaluation framework in view of future rounds, ii) analyze the effect on task 26 feasibility of data coming from a narrow domain, iii) analyze the effect of post-edits collected from professional translators, iv) analyze how humans"
W16-2301,W16-2389,0,0.0210658,"Missing"
W16-2301,W16-2390,0,0.0283246,"Missing"
W16-2301,W16-2322,0,0.0139204,"ng and evaluating the manual translations has settled into the following pattern. We ask human annotators to rank the outputs of five systems. From these rankings, we produce pairwise translation comparisons, and then evaluate them with a version of the TrueSkill algorithm adapted to our task. We refer to this approach (described in Section 3.4) as the relative ranking approach (RR), so named because the pairwise comparisons denote only relative ability between a pair of systems, and cannot be used to infer their absolute quality. These results are used to produce the official ranking for the WMT 2016 tasks. However, work in evaluation over the past few years has provided fresh insight into ways to collect direct assessments (DA) of machine translation quality. In this setting, annotators are asked to provide an assessment of the direct quality of the output of a system relative to a reference translation. In order to evaluate the potential of this approach for future WMT evaluations, we conducted a direct assessment evaluation in parallel. This evaluation, together with a comparison of the official results, is described in Section 3.5. 2.3 3.1 2.2 Training data As in past years we provide"
W16-2301,W16-2392,1,0.772695,"Missing"
W16-2301,L16-1649,1,0.704564,"): one for the 17 baseline features and another for the 500 features from the embeddings. Since each kernel has its own set of hyperparameters, the full model can leverage the contributions from the two different sets. The second system (GRAPH-DISC) combines the baseline features with discourse-aware features. The discourse aware features are the same as the ones used by Scarton et al. (2015a) plus Latent Semantic Analysis (LSA) cohesion features (Scarton and Specia, 2014), number of subtrees and height of the Rhetorical Structure Theory (RST) tree and entity graph-based coherence scores (Sim Smith et al., 2016). Discourseaware and RST tree features were extracted only for English (tools are only available for this language), LSA features were extracted for both languages, and entity graph-based coherence scores were extracted for the target language only (Spanish), as the source documents are expected to be coherent. This QE model was trained with an SVR algorithm. YSDA (Task 1): The YSDA submission is based on a simple idea that the more complex the sentence is the more difficult it is to translate. For this purpose, it uses information provided by syntactic parsing (information from parsing trees,"
W16-2301,W16-2391,1,0.796337,"Missing"
W16-2301,2014.eamt-1.21,1,0.643048,"e extracted by averaging word embeddings in the document. The GP model was trained with two Rational Quadratic kernels (Rasmussen and Williams, 2006): one for the 17 baseline features and another for the 500 features from the embeddings. Since each kernel has its own set of hyperparameters, the full model can leverage the contributions from the two different sets. The second system (GRAPH-DISC) combines the baseline features with discourse-aware features. The discourse aware features are the same as the ones used by Scarton et al. (2015a) plus Latent Semantic Analysis (LSA) cohesion features (Scarton and Specia, 2014), number of subtrees and height of the Rhetorical Structure Theory (RST) tree and entity graph-based coherence scores (Sim Smith et al., 2016). Discourseaware and RST tree features were extracted only for English (tools are only available for this language), LSA features were extracted for both languages, and entity graph-based coherence scores were extracted for the target language only (Spanish), as the source documents are expected to be coherent. This QE model was trained with an SVR algorithm. YSDA (Task 1): The YSDA submission is based on a simple idea that the more complex the sentence"
W16-2301,W15-3040,1,0.909092,"on-word corpus,18 with a vocabulary size of 527K words. Document embeddings are extracted by averaging word embeddings in the document. The GP model was trained with two Rational Quadratic kernels (Rasmussen and Williams, 2006): one for the 17 baseline features and another for the 500 features from the embeddings. Since each kernel has its own set of hyperparameters, the full model can leverage the contributions from the two different sets. The second system (GRAPH-DISC) combines the baseline features with discourse-aware features. The discourse aware features are the same as the ones used by Scarton et al. (2015a) plus Latent Semantic Analysis (LSA) cohesion features (Scarton and Specia, 2014), number of subtrees and height of the Rhetorical Structure Theory (RST) tree and entity graph-based coherence scores (Sim Smith et al., 2016). Discourseaware and RST tree features were extracted only for English (tools are only available for this language), LSA features were extracted for both languages, and entity graph-based coherence scores were extracted for the target language only (Spanish), as the source documents are expected to be coherent. This QE model was trained with an SVR algorithm. YSDA (Task 1)"
W16-2301,2006.amta-papers.25,0,0.850121,"ombinations of several features. A regression model was training to predict BLEU as target metric instead HTER. The machine learning pipeline uses an SVR with RBF kernel to predict BLEU scores, followed by a linear SVR to predict HTER scores from BLEU scores. As external resources, the system uses a syntactic parser, pseudo-references and back-translation from web-scale MT system, and a web-scale language model. 6.3 Task 1: Predicting sentence-level quality This task consists in scoring (and ranking) translation sentences according to the percentage of their words that need to be fixed. HTER (Snover et al., 2006) is used as quality score, i.e. the minimum edit distance between the machine translation and its manually post-edited version in [0,1]. As in previous years, two variants of the results could be submitted: • Scoring: An absolute HTER score for each sentence translation, to be interpreted as an error metric: lower scores mean better translations. • Ranking: A ranking of sentence translations for all source sentences from best to worst. For this variant, it does not matter how the ranking is produced (from HTER predictions or by other means). The reference ranking is defined based on the true H"
W16-2301,W15-4916,1,0.824025,"Missing"
W16-2301,W16-2346,1,0.0602876,"Missing"
W16-2301,W16-2325,1,0.816227,"Missing"
W16-2301,W16-2393,0,0.033041,"Missing"
W16-2301,W16-2326,0,0.0221449,"Missing"
W16-2301,W16-2327,1,0.737603,"Missing"
W16-2301,W16-2328,0,0.0403946,"Missing"
W16-2301,C00-2137,0,0.0384283,"Missing"
W16-2301,D07-1091,1,\N,Missing
W16-2301,W09-0401,1,\N,Missing
W16-2301,P11-1105,0,\N,Missing
W16-2301,N07-1064,0,\N,Missing
W16-2301,W04-3250,1,\N,Missing
W16-2301,P15-4020,1,\N,Missing
W16-2301,W16-2377,1,\N,Missing
W16-2301,aziz-etal-2012-pet,1,\N,Missing
W16-2301,2012.eamt-1.31,0,\N,Missing
W16-2301,C14-1182,0,\N,Missing
W16-2301,W16-2316,0,\N,Missing
W16-2301,W16-2332,1,\N,Missing
W16-2301,W16-2320,1,\N,Missing
W16-2301,W16-2335,0,\N,Missing
W16-2301,W16-2329,0,\N,Missing
W16-2301,W16-2383,0,\N,Missing
W16-2301,W16-2381,1,\N,Missing
W16-2301,W16-2312,0,\N,Missing
W16-2301,P16-1162,1,\N,Missing
W16-2301,W13-2243,1,\N,Missing
W16-2301,W08-0509,0,\N,Missing
W16-2301,2015.eamt-1.17,1,\N,Missing
W16-2301,W14-6111,0,\N,Missing
W16-2301,2012.tc-1.5,1,\N,Missing
W16-2301,W16-2347,1,\N,Missing
W16-2301,W16-2314,0,\N,Missing
W16-3010,H05-1091,0,0.0976369,"oach of combining many subclasses into one negative class reduced precision and hence overall performance. 3. Co-occurrence: A simple approach to relation extraction is to consider all event pairs that occur within a sentence as related. We tried using this cooccurrence strategy for relation types for which SVM or Naive Bayes classifiers did not work effectively. We abandoned this strategy as we observed that the overall F1 score reduced over the development dataset, even as the recall at the relation level improved. 4. Kernel methods: We experimented with the shortest dependency path kernel (Bunescu and Mooney, 2005) and the subset tree kernels (Moschitti, 2006) for classification with SVMs. However their performance was quite low (F1 score &lt; 0.20). It is likely that small training set sizes and multiple entity pairs in most sentences affect the performance of these kernel methods. 5. Dominant class types : In our system we adopted the strategy of only accepting predictions of the dominant class type from each classifier. That is, we filter out predictions of type rj from classifier Ci when j 6= i. This strategy proved very effective when tested over the development dataset. Without this filtering step, w"
W16-3010,W16-3001,0,0.0394085,"Missing"
W16-3010,W09-1401,0,0.049046,"t. Different tissues involving complex genetics and various environmental factors are responsible for the healthy development of a seed. A large body of research literature is available containing this knowledge. The SeeDev binary relation extraction subtask of the BioNLP Shared Task 2016 (Chaix et al., 2016) focuses on extracting relations or events that involve two biological entities as expressed in full-text publication articles. The task represents an important contribution to the broader problem of biomedical relation extraction. Similar to previous BioNLP shared tasks in 2009 and 2011 (Kim et al., 2009; Kim et al., 2011), this task focuses on molecular information extraction. The task organisers provided paragraphs from manually selected full text publications on seed development of Arabidopsis thaliana 2 Approach The seedev task involves extraction of 22 different binary events over 16 entity types. Entity mentions within a sentence and the events between them are provided in the gold standard annotations. In the rest of the article, we refer to an event with two entity arguments as simply a binary relation. We treat relation identification as a supervised 1 Source: https://github.com/unim"
W16-3010,W11-1801,0,0.0753552,"Missing"
W16-3010,W13-2010,1,0.850749,"Missing"
W16-3010,P14-5010,0,0.0079723,"ea and eb are known to be related by the type rc , from the relation annotations, we set label = rc . If they are not related, we set label =NR. NR is a special label to denote no relation. 2. Add the triple t = (ea , eb , label) to the training set of Ci , if (ea , eb ) satisifies the type signature for relation ri , i ∈ [1, 22]. 2.4 Feature Engineering We developed a set of common lexical, syntactic and dependency parse based features. Relation specific features were also developed. For part of speech tagging and dependency parsing of the text, 83 we used the toolset from Stanford CoreNLP (Manning et al., 2014). These features are described in detail below. sists of a total of 7, 082 entities and 3, 575 binary relations and is partitioned into training, development and test datasets. Gold standard entity and relation annotations are provided for training and development data and for test data only entity annotations have been released. The given set of 16 entity types are categorized into 7 different entity groups and 22 different relation types are defined. Pre-defined event signatures constrain the types of entity arguments for each relation. 1. Stop word removal: For some relations (“Has Sequence"
W16-3010,E06-1015,0,0.0370004,"ss reduced precision and hence overall performance. 3. Co-occurrence: A simple approach to relation extraction is to consider all event pairs that occur within a sentence as related. We tried using this cooccurrence strategy for relation types for which SVM or Naive Bayes classifiers did not work effectively. We abandoned this strategy as we observed that the overall F1 score reduced over the development dataset, even as the recall at the relation level improved. 4. Kernel methods: We experimented with the shortest dependency path kernel (Bunescu and Mooney, 2005) and the subset tree kernels (Moschitti, 2006) for classification with SVMs. However their performance was quite low (F1 score &lt; 0.20). It is likely that small training set sizes and multiple entity pairs in most sentences affect the performance of these kernel methods. 5. Dominant class types : In our system we adopted the strategy of only accepting predictions of the dominant class type from each classifier. That is, we filter out predictions of type rj from classifier Ci when j 6= i. This strategy proved very effective when tested over the development dataset. Without this filtering step, we found that our system gets a high recall as"
W17-4719,L16-1470,1,0.868974,"Missing"
W17-4719,C16-2064,0,0.0199749,"ach language pair, and implemented byte pair encoding (BPE) (subword units) in their systems (Wolk and Marasek, 2017). Only the official parallel text corpora and monolingual models for the challenge evaluation campaign were used to train language models, and to develop, tune, and test their system. PJIIT explored the use of domain adaptation techniques, symmetrized word alignment models, the unsupervised transliteration models and the KenLM language modeling tool. kyoto (Kyoto University). The system from the team from Kyoto University is based on two previous papers (Cromieres et al., 2016; Cromieres, 2016). The participants describe it as a classic neural machine translation (NMT) system, however, we do not have further information regarding the datasets that have been used to train and tune the system for the WMT challenge. uedin-nmt (University of Edinburgh). The systems from the University of Edinburgh used a NMT trained with Nematus, an attentional encoder-decoder (Sennrich et al., 2017). Their setup follows the one from last year. This team again built BPE-based models with parallel and back-translated monolingual training data. New approaches this year included the use of deep architectur"
W17-4719,W16-4616,0,0.0207529,"training settings for each language pair, and implemented byte pair encoding (BPE) (subword units) in their systems (Wolk and Marasek, 2017). Only the official parallel text corpora and monolingual models for the challenge evaluation campaign were used to train language models, and to develop, tune, and test their system. PJIIT explored the use of domain adaptation techniques, symmetrized word alignment models, the unsupervised transliteration models and the KenLM language modeling tool. kyoto (Kyoto University). The system from the team from Kyoto University is based on two previous papers (Cromieres et al., 2016; Cromieres, 2016). The participants describe it as a classic neural machine translation (NMT) system, however, we do not have further information regarding the datasets that have been used to train and tune the system for the WMT challenge. uedin-nmt (University of Edinburgh). The systems from the University of Edinburgh used a NMT trained with Nematus, an attentional encoder-decoder (Sennrich et al., 2017). Their setup follows the one from last year. This team again built BPE-based models with parallel and back-translated monolingual training data. New approaches this year included the use o"
W17-4719,W17-4754,0,0.123112,"Munich has participated with an en2de NMT system (Huck and Fraser, 2017). A distinctive feature of their system is a linguistically informed, cascaded target word segmentation approach. Fine-tuning for the domain of health texts was done using in-domain sections of the UFAL Medical Corpus v.1.0 as a training corpus. The learning rate was set to 0.00001, initialized with a pre-trained model, and optimized using only the in-domain medical data. The HimL tun13 UHH (University of Hamburg). All SMT models were developed using the Moses phrase-based MT toolkit and the Experiment Management System (Duma and Menzel, 2017). The preprocessing of the data consisted of tokenization, cleaning (6-80), lowercasing and normalizing punctuation. The tuning and the test sets were derived from WMT 2016 and WMT 2017. The SRILM toolkit https://lilt.com/ 236 LIMSI baseline. For additional comparison, we also provided the results of an en2fr Moses-based system prepared by Ive et al. for their participation in the WMT16 biomedical track, which reflects the state of the art for this language pair (Ive et al., 2016a). The system uses in-domain parallel data provided for the biomedical task in 2016, as well as additional in-domai"
W17-4719,federmann-2010-appraise,0,0.0291939,"run2 LMU PJIIT run1 PJIIT run2 PJIIT run3 uedin-nmt run1 uedin-nmt run2 UHH run1 UHH run2 UHH run3 cs 15.93* 22.79* - de 20.45* 27.57* 26.79 29.46* 21.88* 33.06* 18.71 19.80 19.66* fr 22.99* 31.79 31.89 33.36* pl 14.09* 14.32 10.75 14.34* 23.15* 19.87 - es 40.97 41.20 41.22* ro 10.56* 18.10* 29.32* 27.32 - Table 7: Results for the NHS test sets. * indicates the primary run as informed by the participants. native speakers of the languages and were either members of the participating teams or colleagues from the research community. The validation task was carried out using the Appraise tool15 (Federmann, 2010). For each pairwise comparison, we validated a total of 100 randomly-chosen sentence pairs. The validation consisted of reading the two sentences (A and B), i.e., translations from two systems or from the reference, and choosing one of the options below: The manual validation for the Scielo test sets is presented in Table 8, for the comparison of the only participating team (UHH) to the reference translation. For en2es, the automatic translation scored lower than the reference one in 53 out of 100 pairs, but could still beat the reference translation in 23 pairs. For en2pt, the automatic trans"
W17-4719,W17-4730,0,0.0242129,"uilt BPE-based models with parallel and back-translated monolingual training data. New approaches this year included the use of deep architectures, layer normalization, and more compact models due to weight-tying and improvements in BPE segmentations. Lilt (Lilt Inc.). The system from the Lilt Inc.13 uses an in-house implementation of a sequenceto-sequence model with Bahdanau-style attention. The final submissions are ensembles between models fine-tuned on different parts of the available data. LMU (Ludwig Maximilian University of Munich). LMU Munich has participated with an en2de NMT system (Huck and Fraser, 2017). A distinctive feature of their system is a linguistically informed, cascaded target word segmentation approach. Fine-tuning for the domain of health texts was done using in-domain sections of the UFAL Medical Corpus v.1.0 as a training corpus. The learning rate was set to 0.00001, initialized with a pre-trained model, and optimized using only the in-domain medical data. The HimL tun13 UHH (University of Hamburg). All SMT models were developed using the Moses phrase-based MT toolkit and the Experiment Management System (Duma and Menzel, 2017). The preprocessing of the data consisted of tokeni"
W17-4719,W16-2337,0,0.097915,". All SMT models were developed using the Moses phrase-based MT toolkit and the Experiment Management System (Duma and Menzel, 2017). The preprocessing of the data consisted of tokenization, cleaning (6-80), lowercasing and normalizing punctuation. The tuning and the test sets were derived from WMT 2016 and WMT 2017. The SRILM toolkit https://lilt.com/ 236 LIMSI baseline. For additional comparison, we also provided the results of an en2fr Moses-based system prepared by Ive et al. for their participation in the WMT16 biomedical track, which reflects the state of the art for this language pair (Ive et al., 2016a). The system uses in-domain parallel data provided for the biomedical task in 2016, as well as additional in-domain data14 and out-ofdomain data. However, we did not perform SOUL re-scoring. and Kneser-Ney discounting were used to estimate 5-gram language models (LM). For word alignment, GIZA++ with the default grow-diag-finaland alignment symmetrization method was used. Tuning of the SMT systems was performed with MERT. Commoncrawl and Wikipedia were used as general domain data for all language pairs except for EN/PT, where no Commoncrawl data was provided by WMT. As for the in-domain corpo"
W17-4719,W17-4739,1,\N,Missing
W17-4719,W17-4743,0,\N,Missing
W18-2314,P05-1053,0,0.526405,"Missing"
W18-2314,D17-1191,0,0.0371973,"for relation extraction are feature-based and kernel-based supervised learning approaches which utilize various lexical and syntactic features as well as knowledge base resources; see the comprehensive survey of these traditional approaches in Pawar et al. (2017). Recent research has shown that neural network (NN) models for relation extraction obtain state-of-theart performance. Two major neural architectures for the task include the convolutional neural networks, CNNs, (Zeng et al., 2014; Nguyen and Grishman, 2015; Zeng et al., 2015; Lin et al., 2016; Jiang et al., 2016; Zeng et al., 2017; Huang and Wang, 2017) and long short-term memory networks, LSTMs (Miwa and Bansal, 2016; Zhang et al., 2017; Katiyar and Cardie, 2017; Ammar et al., 2017). We also find combinations of those two architectures (Nguyen and Grishman, 2016; Raj et al., 2017). 129 Proceedings of the BioNLP 2018 workshop, pages 129–136 c Melbourne, Australia, July 19, 2018. 2018 Association for Computational Linguistics 2.1 there is no prior work addressing this. We experiment with two common neural architectures of CNN and LSTM for learning the character-based embeddings, and evaluate the models on the benchmark BioCreative-V CDR corpu"
W18-2314,P16-1200,0,0.129137,"cations (Bach and Badaskar, 2007). Traditional approaches for relation extraction are feature-based and kernel-based supervised learning approaches which utilize various lexical and syntactic features as well as knowledge base resources; see the comprehensive survey of these traditional approaches in Pawar et al. (2017). Recent research has shown that neural network (NN) models for relation extraction obtain state-of-theart performance. Two major neural architectures for the task include the convolutional neural networks, CNNs, (Zeng et al., 2014; Nguyen and Grishman, 2015; Zeng et al., 2015; Lin et al., 2016; Jiang et al., 2016; Zeng et al., 2017; Huang and Wang, 2017) and long short-term memory networks, LSTMs (Miwa and Bansal, 2016; Zhang et al., 2017; Katiyar and Cardie, 2017; Ammar et al., 2017). We also find combinations of those two architectures (Nguyen and Grishman, 2016; Raj et al., 2017). 129 Proceedings of the BioNLP 2018 workshop, pages 129–136 c Melbourne, Australia, July 19, 2018. 2018 Association for Computational Linguistics 2.1 there is no prior work addressing this. We experiment with two common neural architectures of CNN and LSTM for learning the character-based embeddings, an"
W18-2314,P16-1101,0,0.031943,"relations can be expressed across sentence boundaries; they can extend over distances of hundreds of word tokens. As LSTM models can be difficult to apply to very long word sequences (Bradbury et al., 2017), CNN models may be better suited for this task. New domain-specific terms arise frequently in biomedical text data, requiring the capture of unknown words in practical relation extraction applications in this context. Recent research has shown that character-based word embeddings enable capture of unknown words, helping to improve performance on many NLP tasks (dos Santos and Gatti, 2014; Ma and Hovy, 2016; Lample et al., 2016; Plank et al., 2016; Nguyen et al., 2017). This may be particularly relevant for terms such as gene or chemical names, which often have identifiable morphological structure (Krallinger et al., 2017). We investigate the value of character-based word embeddings in a standard CNN model for relation extraction (Zeng et al., 2014; Nguyen and Grishman, 2015). To the best of our knowledge, We investigate the incorporation of character-based word representations into a standard CNN-based relation extraction model. We experiment with two common neural architectures, CNN and LSTM,"
W18-2314,J93-2004,0,0.0608846,"+CNNchar CNN+LSTMchar Linear+TK (Panyam et al., 2016) SVM (Peng et al., 2016) SVM (+dev.) (Peng et al., 2016) SVM (+dev.+18K) (Peng et al., 2016) SVM (+dev.) (Xu et al., 2016) SVM (+dev.) (Pons et al., 2016) P 62.0 59.3 64.9 55.6 60.9 55.7 55.6 53.2 54.8 57.0 56.8 63.6 62.1 68.2 71.1 65.8 73.1 R 55.1 62.3 49.3 68.4 59.5 68.1 70.8 69.7 69.0 68.6 68.8 59.8 64.2 66.0 72.6 68.6 67.6 F1 58.3 60.8 56.0 61.3 60.2 61.3 62.1 60.3 61.1 62.3 62.2 61.7 63.1 67.1 71.8 67.2 70.2 dency parser (Chen and Manning, 2014). However, this dependency parser was trained on the Penn Treebank (in the newswire domain) (Marcus et al., 1993); training on a domain-specific treebank such as CRAFT (Bada et al., 2012) should help to improve results (Verspoor et al., 2012). We also achieve slightly better scores than the more complex model BRAN (Verga et al., 2017), the Biaffine Relation Attention Network, based on the Transformer self-attention model (Vaswani et al., 2017). BRAN additionally uses byte pair encoding (Gage, 1994) to construct a vocabulary of subword units for tokenization. Using subword tokens to capture rare or unknown words has been demonstrated to be useful in machine translation (Sennrich et al., 2016) and likely c"
W18-2314,P16-2067,0,0.0267964,"nce boundaries; they can extend over distances of hundreds of word tokens. As LSTM models can be difficult to apply to very long word sequences (Bradbury et al., 2017), CNN models may be better suited for this task. New domain-specific terms arise frequently in biomedical text data, requiring the capture of unknown words in practical relation extraction applications in this context. Recent research has shown that character-based word embeddings enable capture of unknown words, helping to improve performance on many NLP tasks (dos Santos and Gatti, 2014; Ma and Hovy, 2016; Lample et al., 2016; Plank et al., 2016; Nguyen et al., 2017). This may be particularly relevant for terms such as gene or chemical names, which often have identifiable morphological structure (Krallinger et al., 2017). We investigate the value of character-based word embeddings in a standard CNN model for relation extraction (Zeng et al., 2014; Nguyen and Grishman, 2015). To the best of our knowledge, We investigate the incorporation of character-based word representations into a standard CNN-based relation extraction model. We experiment with two common neural architectures, CNN and LSTM, to learn word vector representations from"
W18-2314,P09-1113,0,0.385071,"Missing"
W18-2314,P16-1105,0,0.0328705,"ised learning approaches which utilize various lexical and syntactic features as well as knowledge base resources; see the comprehensive survey of these traditional approaches in Pawar et al. (2017). Recent research has shown that neural network (NN) models for relation extraction obtain state-of-theart performance. Two major neural architectures for the task include the convolutional neural networks, CNNs, (Zeng et al., 2014; Nguyen and Grishman, 2015; Zeng et al., 2015; Lin et al., 2016; Jiang et al., 2016; Zeng et al., 2017; Huang and Wang, 2017) and long short-term memory networks, LSTMs (Miwa and Bansal, 2016; Zhang et al., 2017; Katiyar and Cardie, 2017; Ammar et al., 2017). We also find combinations of those two architectures (Nguyen and Grishman, 2016; Raj et al., 2017). 129 Proceedings of the BioNLP 2018 workshop, pages 129–136 c Melbourne, Australia, July 19, 2018. 2018 Association for Computational Linguistics 2.1 there is no prior work addressing this. We experiment with two common neural architectures of CNN and LSTM for learning the character-based embeddings, and evaluate the models on the benchmark BioCreative-V CDR corpus for chemical-induced disease relation extraction (Li et al., 201"
W18-2314,E17-1110,0,0.187073,"Missing"
W18-2314,D15-1203,0,0.186853,"of practical applications (Bach and Badaskar, 2007). Traditional approaches for relation extraction are feature-based and kernel-based supervised learning approaches which utilize various lexical and syntactic features as well as knowledge base resources; see the comprehensive survey of these traditional approaches in Pawar et al. (2017). Recent research has shown that neural network (NN) models for relation extraction obtain state-of-theart performance. Two major neural architectures for the task include the convolutional neural networks, CNNs, (Zeng et al., 2014; Nguyen and Grishman, 2015; Zeng et al., 2015; Lin et al., 2016; Jiang et al., 2016; Zeng et al., 2017; Huang and Wang, 2017) and long short-term memory networks, LSTMs (Miwa and Bansal, 2016; Zhang et al., 2017; Katiyar and Cardie, 2017; Ammar et al., 2017). We also find combinations of those two architectures (Nguyen and Grishman, 2016; Raj et al., 2017). 129 Proceedings of the BioNLP 2018 workshop, pages 129–136 c Melbourne, Australia, July 19, 2018. 2018 Association for Computational Linguistics 2.1 there is no prior work addressing this. We experiment with two common neural architectures of CNN and LSTM for learning the character-ba"
W18-2314,K17-1032,0,0.0180127,"ches in Pawar et al. (2017). Recent research has shown that neural network (NN) models for relation extraction obtain state-of-theart performance. Two major neural architectures for the task include the convolutional neural networks, CNNs, (Zeng et al., 2014; Nguyen and Grishman, 2015; Zeng et al., 2015; Lin et al., 2016; Jiang et al., 2016; Zeng et al., 2017; Huang and Wang, 2017) and long short-term memory networks, LSTMs (Miwa and Bansal, 2016; Zhang et al., 2017; Katiyar and Cardie, 2017; Ammar et al., 2017). We also find combinations of those two architectures (Nguyen and Grishman, 2016; Raj et al., 2017). 129 Proceedings of the BioNLP 2018 workshop, pages 129–136 c Melbourne, Australia, July 19, 2018. 2018 Association for Computational Linguistics 2.1 there is no prior work addressing this. We experiment with two common neural architectures of CNN and LSTM for learning the character-based embeddings, and evaluate the models on the benchmark BioCreative-V CDR corpus for chemical-induced disease relation extraction (Li et al., 2016a), obtaining state-of-theart results. 2 Convolutional layer: This layer uses different filters to extract features from the input matrix S = [v 1 , v 2 , ..., v n ]T"
W18-2314,C14-1220,0,0.898304,"unknown words in practical relation extraction applications in this context. Recent research has shown that character-based word embeddings enable capture of unknown words, helping to improve performance on many NLP tasks (dos Santos and Gatti, 2014; Ma and Hovy, 2016; Lample et al., 2016; Plank et al., 2016; Nguyen et al., 2017). This may be particularly relevant for terms such as gene or chemical names, which often have identifiable morphological structure (Krallinger et al., 2017). We investigate the value of character-based word embeddings in a standard CNN model for relation extraction (Zeng et al., 2014; Nguyen and Grishman, 2015). To the best of our knowledge, We investigate the incorporation of character-based word representations into a standard CNN-based relation extraction model. We experiment with two common neural architectures, CNN and LSTM, to learn word vector representations from character embeddings. Through a task on the BioCreative-V CDR corpus, extracting relationships between chemicals and diseases, we show that models exploiting the character-based word representations improve on models that do not use this information, obtaining state-of-the-art result relative to previous"
W18-2314,P16-1162,0,0.0511875,"swire domain) (Marcus et al., 1993); training on a domain-specific treebank such as CRAFT (Bada et al., 2012) should help to improve results (Verspoor et al., 2012). We also achieve slightly better scores than the more complex model BRAN (Verga et al., 2017), the Biaffine Relation Attention Network, based on the Transformer self-attention model (Vaswani et al., 2017). BRAN additionally uses byte pair encoding (Gage, 1994) to construct a vocabulary of subword units for tokenization. Using subword tokens to capture rare or unknown words has been demonstrated to be useful in machine translation (Sennrich et al., 2016) and likely captures similar information to character embeddings. However, Verga et al. (2017) do not provide comparative results using only original word tokens. Therefore, it is difficult to assess the usefulness specifically of using byte-pair encoded subword tokens in the CID relation extraction task, as compared to the impact of the full model architecture. We also plan to explore the usefulness of subword tokens in the baseline CNN for future work, to enable comparison with the improvement when using the character-based word embeddings. It is worth noting that both CNN+CNNchar and CNN+LS"
W18-2314,D17-1186,0,0.157191,"ditional approaches for relation extraction are feature-based and kernel-based supervised learning approaches which utilize various lexical and syntactic features as well as knowledge base resources; see the comprehensive survey of these traditional approaches in Pawar et al. (2017). Recent research has shown that neural network (NN) models for relation extraction obtain state-of-theart performance. Two major neural architectures for the task include the convolutional neural networks, CNNs, (Zeng et al., 2014; Nguyen and Grishman, 2015; Zeng et al., 2015; Lin et al., 2016; Jiang et al., 2016; Zeng et al., 2017; Huang and Wang, 2017) and long short-term memory networks, LSTMs (Miwa and Bansal, 2016; Zhang et al., 2017; Katiyar and Cardie, 2017; Ammar et al., 2017). We also find combinations of those two architectures (Nguyen and Grishman, 2016; Raj et al., 2017). 129 Proceedings of the BioNLP 2018 workshop, pages 129–136 c Melbourne, Australia, July 19, 2018. 2018 Association for Computational Linguistics 2.1 there is no prior work addressing this. We experiment with two common neural architectures of CNN and LSTM for learning the character-based embeddings, and evaluate the models on the benchmark"
W18-2314,D17-1182,0,0.0342266,"Missing"
W18-2314,P04-1054,0,\N,Missing
W18-2314,D14-1082,0,\N,Missing
W18-2314,W15-1506,0,\N,Missing
W18-2314,C14-1008,0,\N,Missing
W18-2314,N16-1030,0,\N,Missing
W18-2314,W16-2922,0,\N,Missing
W18-2314,K17-3014,1,\N,Missing
W18-2314,P17-1085,0,\N,Missing
W18-2314,H05-1091,0,\N,Missing
W18-2314,N16-1034,0,\N,Missing
W18-2314,C16-1139,0,\N,Missing
W18-5605,P16-1101,0,0.379418,"ng state-of-art performance that outperformed traditional feature-based models. Luo et al. (2018) further improved on this result on a chemical NER task by adding an attention layer between the BiLSTM and CRF layers (Att-BiLSTM-CRF). In an experiment by Reimers and Gurevych (2017b), optimal hyper-parameters for LSTM networks in sequence tagging tasks were explored, with the finding that incorporation of characterlevel word embeddings significantly improved performance on NER tasks on general datasets including CoNLL 2003 (Tjong Kim Sang and De Meulder, 2003). However, the choice of CNN-based (Ma and Hovy, 2016) or LSTM-based character-level word embeddings (Lample et al., 2016) did not affect the performance significantly. Since the CNN has fewer parameters to train than BiLSTM network, it is better in terms of training efficiency, and was recommended as the preferred approach. In this paper, we implement and compare models with each type of word embedding to generate empirical results for the tasks of chemical and disease NER, using the BioCreative V CDR corpus (Li et al., 2016). These BNER categories are the most searched entities in the biomedical literature (Islamaj Dogan et al., 2009), and henc"
W18-5605,D17-1035,0,0.0949137,"morphology such as common prefixes (e.g., di-) or suffixes (e.g., -ase). Features that capture word-internal characteristics have been shown to be effective for BNER tasks in CRF models (Klinger et al., 2008). Lyu et al. (2017) applied a BiLSTM-CRF model with LSTM-based character-level word embeddings to a gene and protein NER task, demonstrating state-of-art performance that outperformed traditional feature-based models. Luo et al. (2018) further improved on this result on a chemical NER task by adding an attention layer between the BiLSTM and CRF layers (Att-BiLSTM-CRF). In an experiment by Reimers and Gurevych (2017b), optimal hyper-parameters for LSTM networks in sequence tagging tasks were explored, with the finding that incorporation of characterlevel word embeddings significantly improved performance on NER tasks on general datasets including CoNLL 2003 (Tjong Kim Sang and De Meulder, 2003). However, the choice of CNN-based (Ma and Hovy, 2016) or LSTM-based character-level word embeddings (Lample et al., 2016) did not affect the performance significantly. Since the CNN has fewer parameters to train than BiLSTM network, it is better in terms of training efficiency, and was recommended as the preferred"
W18-5605,N16-1030,0,0.797536,"based models. Luo et al. (2018) further improved on this result on a chemical NER task by adding an attention layer between the BiLSTM and CRF layers (Att-BiLSTM-CRF). In an experiment by Reimers and Gurevych (2017b), optimal hyper-parameters for LSTM networks in sequence tagging tasks were explored, with the finding that incorporation of characterlevel word embeddings significantly improved performance on NER tasks on general datasets including CoNLL 2003 (Tjong Kim Sang and De Meulder, 2003). However, the choice of CNN-based (Ma and Hovy, 2016) or LSTM-based character-level word embeddings (Lample et al., 2016) did not affect the performance significantly. Since the CNN has fewer parameters to train than BiLSTM network, it is better in terms of training efficiency, and was recommended as the preferred approach. In this paper, we implement and compare models with each type of word embedding to generate empirical results for the tasks of chemical and disease NER, using the BioCreative V CDR corpus (Li et al., 2016). These BNER categories are the most searched entities in the biomedical literature (Islamaj Dogan et al., 2009), and hence particularly important to study. The results show that models with"
W18-6403,W18-6444,0,0.0369107,"a. Terminological resources such as the Unified Medical Language System (UMLS) (Bodenreider, 2004) were used as well. TFG TALP UPC. Each two submissions for language pairs es/en, fr/en and pt/en utilized either multi-source (run1, primary run) or the singlesource (run2) training. UFRGS. The two submissions from the UFRGS teams seem to have differed only on the MT tool that they used, i.e., either OpenNMT (run1, primary run) or Moses (run2). UHH-DS (University of Hamburg, Germany). The UHH-DS team utilized Moses (Koehn et al., 2007) trained on a variety of in-domain and general domain corpora (Duma and Menzel, 2018). The main feature of their system was the development of an unsupervised method to automatically under-sample sentences from the general domain collection that were better suited for the biomedical domain. Their under-sampling algorithm can be applied either on the source or target side of the corpora, as well as on both sides. 4 UHH-DS. The three submissions for each of the language pairs (en/es, en/pt, en/ro, es/en and pt/en) differed on whether the under-sampling algorithm was applied only on the English side (run1), on the non-English side (run2) or on both sides (run3, primary run). 4.2"
W18-6403,federmann-2010-appraise,0,0.736408,"de) • Portuguese-English (pt/en); Eng.-Port. (en/pt) • English-Romanian (en/ro) • Spanish-English (es/en); Eng.-Span. (en/es) Test sets Test sets were obtained from Medline and EDP. In these sources, text for both languages is readily available from the authors of the publications. Manual evaluation of the automatic alignment After compiling the Medline test sets, we manually checked the totality of the abstracts to assess the quality of the automatic alignment (cf. results shown in Table 2). We utilized a modified version of the Quality Checking task of our installation of the Appraise tool (Federmann, 2010, 2018) and one native speaker of each non-English language carried out the validation (cf. Figure 1). The only exception were the Chinese abstracts which were manually checked without the use of the Appraise tool. For each language pair, we checked the totality of the abstracts for both translation directions, e.g., en/de and de/en, which was later randomly EDP. This year’s test set was derived from last year’s processing of publications. We kept one extra test set for this year’s challenge. It can be noted that the sentence segmentation offered for the EDP corpus this year was performed manu"
W18-6403,C18-2019,0,0.042363,"Missing"
W18-6403,W18-6445,1,0.860902,"Missing"
W18-6403,W18-6446,0,0.0543218,"lti-source systems utilized a concatenation of training data from es/en, fr/en and pt/en. Hunter MT (Hunter College, USA). The Hunter team (Khan et al., 2018) used different transfer learning methods and trained different indomain biomedical data sets one after another. Their system was set up using parameters of previous training as the initialization of the following training. A News based model was used as pretraining. LMU (Ludwig Maximilian University of Munich, Germany). The LMU team implementated various neural network models and trained and tuned the models on parallel biomedical data (Huck et al., 2018). They experimented with implementations of the Transformer architecture (Sockeye implementation) and the encoderdecoder models (Nematus toolkit). The authors highlight that the word segmentation used on the German language for both translation directions were responsible for the good performance of the system in the human evaluation. Participating teams and systems We received submissions from six teams, as summarized in Table 3. The teams came from research and academic institutions of four countries (Brazil, Germany, Spain and USA) and from three continents. An overview of the teams and the"
W18-6403,ma-2006-champollion,0,0.0427762,"models for each languages, i.e., Chinese, French, German and Spanish (Manning et al., 2014).2 Since for Portuguese and Romanian no models are available in the Stanford CoreNLP tools, we used models for other similar Roman languages (Spanish for Portuguese and French for Romanian). The sentences were then automatically aligned using the GMA tool for which we provided a list of stopwords for each language.3 After a short analysis of the alignment of the Chinese/English abstracts, and given the bad alignments that we obtained, we carried out a new automatic alignment using the Champollion tool (Ma, 2006).4 The resulting aligned sentences were then manually checked for assessing their quality. 2 2.1 language pairs and from EDP for one language pair, as detailed below: • Chinese-English (zh/en); Eng.-Chinese (en/zh) • French-English (fr/en); Eng.-French (en/fr) • German-English (de/en); Eng.-German (en/de) • Portuguese-English (pt/en); Eng.-Port. (en/pt) • English-Romanian (en/ro) • Spanish-English (es/en); Eng.-Span. (en/es) Test sets Test sets were obtained from Medline and EDP. In these sources, text for both languages is readily available from the authors of the publications. Manual evaluat"
W18-6403,P14-5010,0,0.00380103,"d for which we have native speakers of the foreign languages. The text of the abstracts were extracted from the XML files and 120 abstracts were randomly selected, excepted for Romanian whose total of parallel documents in Medline was less than 50. The number 120 accounts for possible errors in the preprocessing of the abstract in order to have a final test set of 100 abstracts to be split into the two translation directions. The documents were automatically split using the Stanford CoreNLP tool and the respective available models for each languages, i.e., Chinese, French, German and Spanish (Manning et al., 2014).2 Since for Portuguese and Romanian no models are available in the Stanford CoreNLP tools, we used models for other similar Roman languages (Spanish for Portuguese and French for Romanian). The sentences were then automatically aligned using the GMA tool for which we provided a list of stopwords for each language.3 After a short analysis of the alignment of the Chinese/English abstracts, and given the bad alignments that we obtained, we carried out a new automatic alignment using the Champollion tool (Ma, 2006).4 The resulting aligned sentences were then manually checked for assessing their q"
W18-6403,L18-1191,0,0.135912,"(Prieto, 2018). Therefore, biomedicine is a domain for which suitable parallel corpora, official evaluation test sets and machine translation (MT) systems are in high demand. There is active development of parallel corpora in this domain (see the recent survey in (N´ev´eol et al., 2018)). In this year alone, three new corpora have been published in a single conference: a compilation of full texts from the Scielo database for English, Portuguese, and Spanish (Soares et al., 2018), medical documents and glossaries for Spanish/English (Villegas et al., 2018) and a biomedical corpus for Romanian (Mitrofan and Tufis, 2018). However, in spite of the growing number of parallel corpora and the many open source tools for MT (e.g., Moses (Koehn et al., 2007), OpenNMT (Klein et al., 2017) and Marian (Junczys-Dowmunt et al., 2018)), there is still no ready-to-use tool for automatic translation of biomedical publications for any language pair. With the aim of fostering advances in this field, we organized the third edition of the Biomedical Translation Task in the Conference for Machine Translation (WMT).1 It builds on the two previous editions (Bojar et al., 2016; Jimeno Yepes et al., 2017) by offering test sets from"
W18-6403,L18-1043,1,0.564419,"Missing"
W18-6403,P18-4020,0,0.0427834,"Missing"
W18-6403,L16-1470,1,0.656371,"Missing"
W18-6403,W18-6447,0,0.0591703,"nges in the Romanian language. 3 TFG TALP UPC (Technical University of Catalunya, Spain). For their system that provides translations into English, the TGF TALP UPC team participated with a Transformer architecture (Kaiser et al., 2017; Vaswani et al., 2017) using both single-language and multi-source systems (Tubay and Costa-Juss`a, 2018). The systems were trained on the Scielo and Medline titles made available by the shared task in the last years. The multi-source systems utilized a concatenation of training data from es/en, fr/en and pt/en. Hunter MT (Hunter College, USA). The Hunter team (Khan et al., 2018) used different transfer learning methods and trained different indomain biomedical data sets one after another. Their system was set up using parameters of previous training as the initialization of the following training. A News based model was used as pretraining. LMU (Ludwig Maximilian University of Munich, Germany). The LMU team implementated various neural network models and trained and tuned the models on parallel biomedical data (Huck et al., 2018). They experimented with implementations of the Transformer architecture (Sockeye implementation) and the encoderdecoder models (Nematus too"
W18-6403,W18-6448,0,0.167561,"FOKUS (Germany) Hunter College (USA) Ludwig Maximilian University of Munich (Germany) Technical University of Catalunya (Spain) Universidade Federal do Rio Grande do Sul (Brazil) University of Hamburg (Germany) Table 3: List of the participating teams. 327 LMU. The three en/de submissions from the LMU team were the following: a right-to-left reranked Transformer (run1, primary run), a Transformer ensemble without re-ranking (run2) and the encoder-decoder built with Nematus (run3). The only submission for de/en was a Transformer without ensemble. 2007) or OpenNMT (Klein et al., 2017) systems (Soares and Becker, 2018). Training data was prepared by concatenating several in-domain and outof-domain resources. The in-domain corpora included scientific articles (full texts) from Scielo, the UFAL medical corpus, the EMEA corpus and Brazilian theses and dissertations. Due to possible overlap with the test sets from Medline, the team applied some procedures to automatically exclude some publications from the Scielo training data. Terminological resources such as the Unified Medical Language System (UMLS) (Bodenreider, 2004) were used as well. TFG TALP UPC. Each two submissions for language pairs es/en, fr/en and"
W18-6403,L18-1546,0,0.166247,"ation of the best Chinese papers (Tao et al., 2018) and the development of automatic tools for the automatic translation of publications (Prieto, 2018). Therefore, biomedicine is a domain for which suitable parallel corpora, official evaluation test sets and machine translation (MT) systems are in high demand. There is active development of parallel corpora in this domain (see the recent survey in (N´ev´eol et al., 2018)). In this year alone, three new corpora have been published in a single conference: a compilation of full texts from the Scielo database for English, Portuguese, and Spanish (Soares et al., 2018), medical documents and glossaries for Spanish/English (Villegas et al., 2018) and a biomedical corpus for Romanian (Mitrofan and Tufis, 2018). However, in spite of the growing number of parallel corpora and the many open source tools for MT (e.g., Moses (Koehn et al., 2007), OpenNMT (Klein et al., 2017) and Marian (Junczys-Dowmunt et al., 2018)), there is still no ready-to-use tool for automatic translation of biomedical publications for any language pair. With the aim of fostering advances in this field, we organized the third edition of the Biomedical Translation Task in the Conference for"
W18-6403,P17-4012,0,0.124259,"mand. There is active development of parallel corpora in this domain (see the recent survey in (N´ev´eol et al., 2018)). In this year alone, three new corpora have been published in a single conference: a compilation of full texts from the Scielo database for English, Portuguese, and Spanish (Soares et al., 2018), medical documents and glossaries for Spanish/English (Villegas et al., 2018) and a biomedical corpus for Romanian (Mitrofan and Tufis, 2018). However, in spite of the growing number of parallel corpora and the many open source tools for MT (e.g., Moses (Koehn et al., 2007), OpenNMT (Klein et al., 2017) and Marian (Junczys-Dowmunt et al., 2018)), there is still no ready-to-use tool for automatic translation of biomedical publications for any language pair. With the aim of fostering advances in this field, we organized the third edition of the Biomedical Translation Task in the Conference for Machine Translation (WMT).1 It builds on the two previous editions (Bojar et al., 2016; Jimeno Yepes et al., 2017) by offering test sets from Medline for six Machine translation enables the automatic translation of textual documents between languages and can facilitate access to information only availabl"
W19-5035,D14-1012,0,0.023505,"tion performance of these systems and their ensembles (Habibi et al., 2016). The application of the tmChem model trained on chemical literature corpora of the BioCreative IV CHEMDNER task (Krallinger et al., 2015) and the ChemSpot model trained on a subset of the SCAI corpus (Klinger et al., 2008) resulted in a significant performance drop over chemical patent corpora. Zhang et al. (2016) compared the performance of CRF- and Support Vector Machine (SVM)based models on the CHEMDNER-patents corpus (Krallinger et al., 2015). The features constructed in that work included the binarized embedding (Guo et al., 2014), Brown clustering (Brown et al., 1992) and domain-specific features extracted by The morphological structures within words are also important clues for identifying named entities in biological domain. Such morphological structures are widely used in systematic chemical name formats (e.g. IUPAC names) and hence particularly informative for chemical NER (Klinger et al., 2008). Character-level word representations have been developed to leverage information from these structures by encoding the character sequences within tokens. Ma and Hovy (2016) uses Convolutional Neural Networks (CNNs) to enc"
W19-5035,J92-4003,0,0.261664,"their ensembles (Habibi et al., 2016). The application of the tmChem model trained on chemical literature corpora of the BioCreative IV CHEMDNER task (Krallinger et al., 2015) and the ChemSpot model trained on a subset of the SCAI corpus (Klinger et al., 2008) resulted in a significant performance drop over chemical patent corpora. Zhang et al. (2016) compared the performance of CRF- and Support Vector Machine (SVM)based models on the CHEMDNER-patents corpus (Krallinger et al., 2015). The features constructed in that work included the binarized embedding (Guo et al., 2014), Brown clustering (Brown et al., 1992) and domain-specific features extracted by The morphological structures within words are also important clues for identifying named entities in biological domain. Such morphological structures are widely used in systematic chemical name formats (e.g. IUPAC names) and hence particularly informative for chemical NER (Klinger et al., 2008). Character-level word representations have been developed to leverage information from these structures by encoding the character sequences within tokens. Ma and Hovy (2016) uses Convolutional Neural Networks (CNNs) to encode character sequences while Lample et"
W19-5035,N19-1149,0,0.0159323,"extended version as EBC-CRF as illustrated in Figure 1. In particular, for EBC-CRF, we use a concatenation of pretrained word embeddings, CNN-based characterlevel word embeddings and ELMo-based contextualized word embeddings as the input of a BiLSTM encoder. The BiLSTM encoder learns a latent feature vector for each word in the input. Then each latent feature vector is linearly transformed before being fed into a linear-chain CRF layer (Lafferty et al., 2001) for NER tag prediction. We assume binary potential between tags and unary potential between tags and words. Pre-trained word embeddings Dai et al. (2019) showed that NER performance is significantly affected by the overlap between pretrained word embedding vocabulary and the target NER data. Therefore, we explore the effects of different sets of pre-trained word embeddings on the NER performance. We use 200-dimensional pre-trained PubMedPMC and Wiki-PubMed-PMC word embeddings (Pyysalo et al., 2013), which are widely used for NLP tasks in biomedical domain. Both the PubMed-PMC and Wiki-PubMed-PMC embeddings word embeddings were generated by training the Word2Vec skip-gram model (Mikolov et al., 2013) on a collection of PubMed abstracts and PubM"
W19-5035,N19-1423,0,0.0142134,"Hence, we fix the hyper-parameters shown in Table 2 to the suggested values in our experiments, which means that only models with 2stacked LSTM of size 250 are evaluated. In this study, we also consider the choice of tokenizer and word embedding source as hyperparameters. To compare the performance of different tokenizers, we tokenize the same split of datasets with different tokenizers and evaluate the overall F1 score over development set. After the best tokenizer for pre-processing patent corpus is determined, we use datasets tokenized by the best ELMo ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019) can be used to generate contextualized word representations by combining internal states of different layers in neural language models. Contextualized word representation can help to improve performance in various NLP tasks by incorporating contextual information, essentially allowing for the same word to have distinct context-dependent meanings. This could be particularly powerful for chemical NER since generic chemical names (e.g. salts, acid) may have different meanings in other domains. We therefore explore the impact of using contextualized word representations for chemical patents. We t"
W19-5035,N16-1030,0,0.775717,"l., 1992) and domain-specific features extracted by The morphological structures within words are also important clues for identifying named entities in biological domain. Such morphological structures are widely used in systematic chemical name formats (e.g. IUPAC names) and hence particularly informative for chemical NER (Klinger et al., 2008). Character-level word representations have been developed to leverage information from these structures by encoding the character sequences within tokens. Ma and Hovy (2016) uses Convolutional Neural Networks (CNNs) to encode character sequences while Lample et al. (2016) 329 CRF-based models (Section 3.3) with pre-trained word embeddings (Section 3.4), character-level word embeddings (Section 3.5), contextualized word embeddings (Section 3.6) and implementation details (Section 3.7). developed a LSTM-based approach for encoding character level information. Habibi et al. (2017) presented an empirical study comparing three NER models on a large collection of biomedical corpora including the BioSemantics patent corpus: (1) tmChem–the CRFbased model with hand-crafted features–used as the baseline; (2) a second CRF model based on CRFSuite (Okazaki, 2007) using pre"
W19-5035,D17-1035,0,0.0122379,"sets. We do not update weights for word embeddings if pre-trained word embeddings were used. Character-level representation The BiLSTM-CRF model with character-level word representations (Lample et al., 2016; Ma and Hovy, 2016) has been shown to have state-of-theart performance in NER tasks on chemical patent datasets (Habibi et al., 2017). It has been shown that the choice of using LSTM-based or CNNbased character-level word representation has little effect on final NER performance in both general and biomedical domain while the CNN-based approach has the advantage of reduced training time (Reimers and Gurevych, 2017b; Zhai et al., 2018). Hence, we use the CNN-based approach with the same hyper-parameter settings of Reimers and Gurevych (2017b) for capturing characterlevel information (see Table 2 for details). 3.6 Hyper-para. charEmbedSize filter length # of filters output size (a) BiLSTM-CRF Table 1: Statistics of the unannotated patent corpus used for training ChemPatent embeddings and ELMo. 3.5 Value Adam 0.001 16 1 [0.25, 0.25] 3.7 Implementation details Our NER model implementation is based on the AllenNLP system (Gardner et al., 2017). We learn model parameters using the training set, and we use th"
W19-5035,P16-1101,0,0.294108,"ed in that work included the binarized embedding (Guo et al., 2014), Brown clustering (Brown et al., 1992) and domain-specific features extracted by The morphological structures within words are also important clues for identifying named entities in biological domain. Such morphological structures are widely used in systematic chemical name formats (e.g. IUPAC names) and hence particularly informative for chemical NER (Klinger et al., 2008). Character-level word representations have been developed to leverage information from these structures by encoding the character sequences within tokens. Ma and Hovy (2016) uses Convolutional Neural Networks (CNNs) to encode character sequences while Lample et al. (2016) 329 CRF-based models (Section 3.3) with pre-trained word embeddings (Section 3.4), character-level word embeddings (Section 3.5), contextualized word embeddings (Section 3.6) and implementation details (Section 3.7). developed a LSTM-based approach for encoding character level information. Habibi et al. (2017) presented an empirical study comparing three NER models on a large collection of biomedical corpora including the BioSemantics patent corpus: (1) tmChem–the CRFbased model with hand-crafte"
W19-5035,D14-1162,0,0.0868215,"3) were also explored. The results showed that the BiLSTMCRF model with the combination of domainspecific pre-trained word embedding and LSTMbased character-level word embeddings outperformed the two CRF-based models on chemical NER tasks in both chemical literature and chemical patent corpora. However, this work used only a general tokenizer (i.e. OpenNLP) and word embeddings pre-trained on biomedical corpora. Corbett and Boyle (2018) presented word-level and character-level BiLSTM networks for chemical NER in literature domain. The word-level model employed word embeddings learned by GloVe (Pennington et al., 2014) on a corpus of patent titles and abstracts. The character-level model used two different transfer learning approaches to pre-train its character-level encoder. The first approach attempts to predict neighbor characters at each time step, while the other tries to predict whether a given character sequence is an entry in the chemical database ChEBI (Degtyarenko et al., 2007). Experimental results show that the character-level model can produce better NER performance than word-level model by leveraging transfer learning. In addition, for the wordlevel model, using pre-trained word embeddings lea"
W19-5035,N18-1202,0,0.19692,"ddings are fixed during training of the NER models. For a more concrete comparison, a set of 200-dimensional trainable word embeddings initialized from normal distribution is used as a baseline. The 200-dimensional baseline word embeddings contain all words in the vocabulary of the dataset and are initialized from a normal distribution, the baseline word embeddings are learned during training process. The vocabulary of models Models We use the BiLSTM-CNN-CRF model (Ma and Hovy, 2016) as our baseline. We extend the baseline by adding the contextualized word representations generated from ELMo (Peters et al., 2018). 1 NBIC UMLSGeneChemTokenizer is developed by the Netherlands Bioinformatics Center, available at https:// trac.nbic.nl/data-mining/wiki. 331 Patent Office AU CA EP GB IN US WO Total Document 7,743 1,962 19,274 918 1,913 41,131 11,135 84,076 Sentence 4,662,375 463,123 3,478,258 182,627 261,260 19,800,123 4,830,708 33,687,474 Hyper-para. Optimizer Learning rate Mini-batch size Clip Norm(L2) Dropout Tokens 156,137,670 16,109,776 117,992,191 6,038,837 9,015,238 628,256,609 159,286,325 1,092,836,646 Value 50 3 30 30 (b) CNN-char Table 2: Fixed hyper-parameter configurations. patents (detailed in"
W19-5035,W18-5605,1,0.80271,"ts for word embeddings if pre-trained word embeddings were used. Character-level representation The BiLSTM-CRF model with character-level word representations (Lample et al., 2016; Ma and Hovy, 2016) has been shown to have state-of-theart performance in NER tasks on chemical patent datasets (Habibi et al., 2017). It has been shown that the choice of using LSTM-based or CNNbased character-level word representation has little effect on final NER performance in both general and biomedical domain while the CNN-based approach has the advantage of reduced training time (Reimers and Gurevych, 2017b; Zhai et al., 2018). Hence, we use the CNN-based approach with the same hyper-parameter settings of Reimers and Gurevych (2017b) for capturing characterlevel information (see Table 2 for details). 3.6 Hyper-para. charEmbedSize filter length # of filters output size (a) BiLSTM-CRF Table 1: Statistics of the unannotated patent corpus used for training ChemPatent embeddings and ELMo. 3.5 Value Adam 0.001 16 1 [0.25, 0.25] 3.7 Implementation details Our NER model implementation is based on the AllenNLP system (Gardner et al., 2017). We learn model parameters using the training set, and we use the overall F1 score ov"
W19-5035,W18-2501,0,\N,Missing
W19-5403,W16-2331,0,0.0602264,"Missing"
W19-5403,federmann-2010-appraise,0,0.0998932,"primary runs that we considered from each team. We performed a total of 62 validations of pairwise datasets. We relied on human validators who were native speakers of the target languages and who were either members of the participating teams or colleagues from the research community. We also preferred to use validators who were familiar enough with the source language so that the original text could be consulted in case of questions about the translations, and for most language pairs this was the case. We carried out the so-called 3-way ranking task in our installation of the Appraise tool (Federmann, 2010).14 . For each pairwise dataset, we checked a total of 100 randomly-chosen sentence pairs. The validation consisted of reading the two translation sentences (A and B) and choosing one of the options listed below: en/pt. Results for en/pt from the BSC were almost 10 points higher than the ones for pt/en. The run from the BSC team based on OpenNMT outperfomed with some difference the baseline based on Marian NMT, maybe because of the many resources that the team trained its system on. Further, they were much superior to the baselines 2 and 3 also based on OpenNMT but only trained on the Medline"
W19-5403,P18-4020,0,0.0226832,"Missing"
W19-5403,P17-4012,0,0.0423458,"Missing"
W19-5403,P07-2045,0,0.0097248,"Terms 6,624 - Table 1: Number of documents, sentences, and terms in the training and test sets. the Appraise tool. We present statistics concerning the quality of the test set alignments in Table 2. than narrative. Two of the targeted languages, Portuguese and Chinese, are not present in UFAL. For Portuguese we therefore trained our model on the Scielo corpus (Neves et al., 2016) and tested on the Brazilian thesis corpus (Soares et al., 2018b). For Chinese we used the United Nations Parallel Corpus (Ziemski et al., 2016). The data was preprocessed using standard tools from the Moses toolkit (Koehn et al., 2007): tokenisation, cleaning of training data and truecasing. Subword segmentation (Sennrich et al., 2015) was then trained jointly over both source and target languages and applied using FastBPE.10 The number of merge operations for BPE was set to 85000. The models trained were shallow RNN encoderdecoders.11 They were trained on a GTX 1080 Ti with 8 GPUs. Validation using cross-entropy and BLEU was performed every 10,000 updates, and models were trained until there was no improvement on either metric for 5 consecutive updates. Training of a single model took approximately 2 days. Discussion. Comp"
W19-5403,W08-0336,0,0.0823727,"Missing"
W19-5403,W19-5418,0,0.251422,"Missing"
W19-5403,W19-5421,0,0.16707,"Missing"
W19-5403,L18-1043,1,0.820309,"Missing"
W19-5403,W17-2507,1,0.907231,"Missing"
W19-5403,W18-6403,1,0.767844,"Missing"
W19-5403,L18-1546,1,0.919338,"o improve patient-provider communication (Turner et al., 2019). However, the recurring conclusion of practical studies is that progress is still needed. The goal of this shared task is to bring machine translation of biomedical text to a level of performance that can help with these medical challenges. In recent years, many parallel corpora in the biomedical domain have been made available, which are valuable resources for training and evaluating MT systems. Examples of such corpora include Khresmoi (Duˇsek et al., 2017), Scielo (Neves et al., 2016), Full-Text Scientific Articles from Scielo (Soares et al., 2018a), MeSpEn (Villegas et al., 2018), thesis and dissertations (Soares et al., 2018b), and clinical trials (Neves, 2017). These corpora cover a variety of language pairs and document types, such as scientific articles, clinical trials, and academic dissertations. Many previous efforts have addressed MT for the biomedical domain. Interesting previous work includes a comparison of performance in biomedical MT to Google Translate for English, French, German, and Spanish (Wu et al., 2011). Pecina et al. applied MT for the task of multilingual information retrieval in the medical domain (Pecina et al"
W19-5403,L16-1470,1,0.901668,"Missing"
W19-5403,tiedemann-2012-parallel,0,0.252445,"Missing"
W19-5403,W18-1819,0,0.0299267,"summary paragraph. Below we provide a short description of the systems for which a corresponding paper is available or for which we received a description from the participants. Two teams (‘peace’ and ‘Radiant’) did not provide system descriptions. Table 5 provides an overview of the methods, implementations and training corpora used by the participants. While two teams used the statistical machine translation toolkit Moses (MT-UOCUPF and UHH-DS), the most popular translation 13 KU. The KU team’s systems were based on the Transformer-big architecture, trained using the Tensor2Tensor toolkit (Vaswani et al., 2018). Training data was carefully cleaned to remove encoding errors, bad translations, etc. They did not perform standard ensemble translation, but obtained a small BLEU improvement by taking a “majority vote” on the final translations for different checkpoints. MT-UOC-UPF. The MT-UOC-UPF team’s systems were deep RNN-based encoder-decoder models with attention, trained using Marian (and with layer normalisation, tied embeddings and 10.6084/m9.figshare.8094119 33 Team ID ARC BSC KU MT-UOC-UPF NRPU OOM peace Radiant Talp upc UCAM UHH-DS Institution Huawei Technologies (China), Barcelona Supercomputi"
W19-5403,W18-6429,0,0.02331,"to those from UCAM. Both teams used Transformer models but the ARC also used BERT multilingual embeddings. We observed no significant difference between the submissions from team ARC but runs based on the ensemble of models from team UCAM (i.e. runs 2 and 3) obtained a higher score than their single best systems. characters, achieving such an ideal tokenization requires a sophisticated dictionary (Chang et al., 2008) – including biomedical terms – and is beyond the scope of this shared task. Further, using character-level tokenization for BLEU purposes is in accordance with current practice (Wang et al., 2018; Xu and Carpuat, 2018). Table 6 shows BLEU scores for all language pairs when considering all sentences in our test sets. Table 7 only considers the sentences that have been manually classified as being correctly aligned (cf. Section 2). As expected, certain results improve considerably (by more than 10 BLEU points) when only considering the sentences that are correctly aligned. Most teams outperformed the three baselines, except the NRPU team’s submissions for en/fr and fr/en. Baseline1, trained using Marian NMT, obtained results not far behind the best performing team, while the two other b"
W19-5403,W19-5420,0,0.097457,"Missing"
W19-5403,W18-6431,0,0.0160294,". Both teams used Transformer models but the ARC also used BERT multilingual embeddings. We observed no significant difference between the submissions from team ARC but runs based on the ensemble of models from team UCAM (i.e. runs 2 and 3) obtained a higher score than their single best systems. characters, achieving such an ideal tokenization requires a sophisticated dictionary (Chang et al., 2008) – including biomedical terms – and is beyond the scope of this shared task. Further, using character-level tokenization for BLEU purposes is in accordance with current practice (Wang et al., 2018; Xu and Carpuat, 2018). Table 6 shows BLEU scores for all language pairs when considering all sentences in our test sets. Table 7 only considers the sentences that have been manually classified as being correctly aligned (cf. Section 2). As expected, certain results improve considerably (by more than 10 BLEU points) when only considering the sentences that are correctly aligned. Most teams outperformed the three baselines, except the NRPU team’s submissions for en/fr and fr/en. Baseline1, trained using Marian NMT, obtained results not far behind the best performing team, while the two other baselines were not very"
W19-5403,L16-1561,0,0.0276925,"ntences 50 589 50 719 50 526 50 599 50 486 50 593 50 491 50 589 50 283 50 351 Terminology test Terms 6,624 - Table 1: Number of documents, sentences, and terms in the training and test sets. the Appraise tool. We present statistics concerning the quality of the test set alignments in Table 2. than narrative. Two of the targeted languages, Portuguese and Chinese, are not present in UFAL. For Portuguese we therefore trained our model on the Scielo corpus (Neves et al., 2016) and tested on the Brazilian thesis corpus (Soares et al., 2018b). For Chinese we used the United Nations Parallel Corpus (Ziemski et al., 2016). The data was preprocessed using standard tools from the Moses toolkit (Koehn et al., 2007): tokenisation, cleaning of training data and truecasing. Subword segmentation (Sennrich et al., 2015) was then trained jointly over both source and target languages and applied using FastBPE.10 The number of merge operations for BPE was set to 85000. The models trained were shallow RNN encoderdecoders.11 They were trained on a GTX 1080 Ti with 8 GPUs. Validation using cross-entropy and BLEU was performed every 10,000 updates, and models were trained until there was no improvement on either metric for 5"
